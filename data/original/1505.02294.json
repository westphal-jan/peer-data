{"id": "1505.02294", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2015", "title": "Estimation with Norm Regularization", "abstract": "Analysis of non-asymptotic estimation error and structured statistical recovery based on norm regularized regression, such as Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise model. This paper presents generalizations of such estimation error analysis on all four aspects compared to the existing literature. We characterize the restricted error set where the estimation error vector lies, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to any norm. Precise characterizations of the bound is presented for a variety of design matrices, including sub-Gaussian, anisotropic, and correlated designs, noise models, including both Gaussian and sub-Gaussian noise, and loss functions, including least squares and generalized linear models. A key result from the analysis is that the sample complexity of all such estimators depends on the Gaussian width of the spherical cap corresponding to the restricted error set. Further, once the number of samples $n$ crosses the required sample complexity, the estimation error decreases as $\\frac{c}{\\sqrt{n}}$, where $c$ depends on the Gaussian width of the unit dual norm ball.", "histories": [["v1", "Sat, 9 May 2015 17:25:14 GMT  (1218kb,D)", "https://arxiv.org/abs/1505.02294v1", null], ["v2", "Mon, 18 May 2015 16:24:01 GMT  (643kb,D)", "http://arxiv.org/abs/1505.02294v2", "Fixed minor typos"], ["v3", "Mon, 30 Nov 2015 20:47:14 GMT  (660kb,D)", "http://arxiv.org/abs/1505.02294v3", "Fixed technical issues. Generalized some results"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["arindam banerjee", "sheng chen", "farideh fazayeli", "vidyashankar sivakumar"], "accepted": true, "id": "1505.02294"}, "pdf": {"name": "1505.02294.pdf", "metadata": {"source": "CRF", "title": "Estimation with Norm Regularization", "authors": ["Arindam Banerjee", "Sheng Chen", "Farideh Fazayeli", "Vidyashankar Sivakumar"], "emails": ["banerjee@cs.umn.edu}", "shengc@cs.umn.edu}", "farideh@cs.umn.edu}", "sivakuma@cs.umn.edu}"], "sections": [{"heading": null, "text": "n , where c depends on the Gaussian width of the unit norm ball."}, {"heading": "1 Introduction", "text": "Over the past decade, progress has been made in developing non-asymptotic bounds on the estimation error of structured parameters based on norm regularized regression. Such estimators are usually of the form [39, 29, 9]:\n\u03b8\u0302\u03bbn = argmin \u03b8\u2208Rp\nL(\u03b8;Zn) + \u03bbnR(\u03b8) , (1)\nwhere R(\u03b8) is a suitable norm, L(\u00b7) is a suitable loss function, Zn = {(yi, Xi)}ni=1 where yi \u2208 R, Xi \u2208 Rp is the training set, and \u03bbn > 0 is a regularization parameter. The optimal parameter \u03b8\u2217 is often assumed to be \u2018structured,\u2019 usually characterized or approximated as a small value according to some norm R(\u00b7). Recent work has viewed such characterizations in terms of atomic norms, which give the tightest convex relaxation of a structured set of atoms in which \u03b8\u2217 belongs [14]. Since \u03b8\u0302\u03bbn is an estimate of the optimal structure \u03b8\n\u2217, the focus has been on bounding a suitable measure of the error vector \u2206\u0302n = (\u03b8\u0302\u03bbn \u2212 \u03b8\u2217), e.g., the L2 norm \u2016\u2206\u0302n\u20162. To understand the state-of-the-art on non-asymptotic bounds on the estimation error for norm-regularized regression, four aspects of (1) need to be considered: (i) the norm R(\u03b8), (ii) properties of the design matrix X = [X1 \u00b7 \u00b7 \u00b7Xn]T \u2208 Rn\u00d7p, (iii) the loss function L(\u00b7), and (iv) the noise model, typically in terms of \u03c9i = yi \u2212 E[y|Xi]. Most of the literature has focused on a linear model: y = X\u03b8 + \u03c9, and a squared-loss function: L(\u03b8;Zn) = 1n\u2016y \u2212X\u03b8\u2016 2 2 = 1 n \u2211n i=1(yi \u2212 \u3008\u03b8,Xi\u3009)2. Early work on such estimators focussed on\nar X\niv :1\n50 5.\n02 29\n4v 3\n[ st\nat .M\nL ]\n3 0\nN ov\nthe L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33]. While much of the development has focussed on isotropic Gaussian design matrices, recent work has extended the analysis for L1 norm to correlated Gaussian designs [33] as well as anisotropic sub-Gaussian design matrices [34].\nBuilding on such development, [29] presents a unified framework for the case of decomposable norms and also considers generalized linear models (GLMs) for certain norms such as L1. Two key insights are offered in [29]: first, the error vector \u2206\u0302n lies in a restricted set, a cone or a star, for suitably large \u03bbn, and second, the loss function needs to satisfy restricted strong convexity (RSC), a generalization of the RE condition, on the restricted error set for the analysis to work out.\nFor isotropic Gaussian design matrices, additional progress has been made. [14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon\u2019s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere. [31] considers three related formulations for generalized Lasso problems, establish recovery guarantees based on Gordon\u2019s inequality, and quantities related to the Gaussian width. Sharper analysis for recovery has been considered in [1], yielding a precise characterization of phase transition behavior using quantities related to the Gaussian width. [32] consider a linear programming estimator in a 1-bit compressed sensing setting and, interestingly, the concept of Gaussian width shows up in the analysis. In spite of the advances, with a few notable exceptions [40, 42], most existing results are restricted to isotropic Gaussian design matrices. Further, while a suitable scale for \u03bbn is known for special cases such as the L1, a general analysis applicable to any normR(\u00b7) has not been explored in the literature.\nIn this paper, we consider structured estimation problems with norm regularization of the form (1), and present a unified analysis which substantially generalizes existing results on all four pertinent aspects: the norm, the design matrix, the loss, and the noise model. The analysis we present applies to all norms, and the results can be divided into three groups: characterization of the error set and recovery guarantees, characterization of the regularization parameter \u03bbn, and characterization of the restricted eigenvalue conditions or restricted strong convexity. We provide a summary of the key results below.\nRestricted error set: We start with a characterization of the error set Er in which the error vector \u2206\u0302n belongs. For a suitably large \u03bbn, we show that \u2206\u0302n belongs to the restricted error set\nEr =\n{ \u2206 \u2208 Rp \u2223\u2223\u2223\u2223 R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217) + 1\u03b2R(\u2206) } , (2)\nwhere \u03b2 > 1 is a constant. The restricted error set has interesting structure, and forms the basis of subsequent analysis for bounds on \u2016\u2206\u0302n\u20162. Regularized vs. constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(\u03b8) under suitable constraints determined by the noise (y \u2212 X\u03b8) and/or the design matrix X [13, 6, 14, 15]. A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15], which generalizes the Dantzig selector [11] corresponding to the L1 norm, and is given by:\n\u03b8\u0302\u03b3n = argmin \u03b8\u2208Rp\nR(\u03b8) s.t. R\u2217(XT (y \u2212X\u03b8\u2217)) \u2264 \u03b3n , (3)\nwhere R\u2217(\u00b7) denotes the dual norm of R(\u00b7). One can show [14, 15] that the restricted error set for such constrained estimators are of the form:\nEc = {\u2206 \u2208 Rp | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} . (4)\nOne can readily see that Er is larger than Ec, i.e., Ec \u2286 Er, and Er approaches Ec as \u03b2 increases. We establish a geometric relationship between the two sets, which will possibly help in transforming analysis done on regularized estimators as in (1) to corresponding constrained estimators as in (3) and vice versa. Let Bp2 be a L2 ball of radius 1 in Rp. Then, with Ar = Er \u2229 B p 2 , Ac = Ec \u2229 B p 2 , and A\u0304c = cone(Ec) \u2229 B p 2 , assuming \u2016\u03b8\u2217\u20162 = 1, \u03b2 = 2, we show that\nw(Ac) \u2264 w(Ar) \u2264 3w(A\u0304c) , (5)\nwhere w(A) = Eg[supa\u2208A\u3008a, g\u3009], with g \u223c N(0, Ip\u00d7p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37]. Note that Ar corresponds to the spherical cap of the error set Er, and Ac corresponds to the spherical cap of the error cone cone(Ec) at the unit ball. Interestingly, the above relationship between the widths of these spherical caps is geometric, and applies for any norm R(\u00b7). We establish a more general version of the above relationship. Let \u03c1Bp2 denote a L2 ball of any radius \u03c1 in Rp. Then, with A(\u03c1)r = Er \u2229 \u03c1Bp2 , A (\u03c1) c = Ec \u2229 \u03c1Bp2 , and A\u0304 (\u03c1) c = cone(Ec) \u2229 \u03c1Bp2 , we show that\nw(A(\u03c1)c ) \u2264 w(A(\u03c1)r ) \u2264 ( 1 + 2 \u03b2 \u2212 1 \u2016\u03b8\u2217\u20162 \u03c1 ) w(A\u0304(\u03c1)c ) . (6)\nAs before, except for the scaling constants, the relationship between the restricted error sets is geometric, and does not change based on the choice of the norm R(\u00b7). For the special case of L1 norm, [6] considered a simultaneous analysis of the Lasso and the Dantzig selector, and characterized the structure of the error sets for regularized and constrained sets for the special case of L1 norm. Further, while the characterization in [6] was also geometric, it was not based on Gaussian widths. In contrast, our results apply to any norm, not just L1, and the geometric characterization is based on Gaussian widths. The utility of the Gaussian width based characterization becomes evident later when we establish sample complexity results for Gaussian and sub-Gaussian random matrices in terms of Gaussian widths of spherical caps.\nBounds on estimation error: We establish bounds on the estimation error \u2206\u0302n under two assumptions, which are subsequently shown to hold with high probability for sub-Gaussian designs and noise models. The first assumption is that the regularization parameter \u03bbn is suitably large. In particular, for any \u03b2 > 1, the regularization parameter \u03bbn needs to satisfy\n\u03bbn \u2265 \u03b2R\u2217(\u2207L(\u03b8\u2217;Zn)) , (7)\nwhere R\u2217(\u00b7) denotes the dual norm of R(\u00b7). The second assumption is that the design matrix X \u2208 Rn\u00d7p satisfies the restricted strong convexity (RSC) condition [6, 29] in the error set Er, in particular, there exists a suitable constant \u03ba > 0 so that\n\u03b4L(\u2206, \u03b8\u2217) , L(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217)\u2212 \u3008\u2207L(\u03b8\u2217),\u2206\u3009 \u2265 \u03ba\u2016\u2206\u201622 \u2200\u2206 \u2208 Er . (8)\nWith such suitably large \u03bbn and L satisfying the RSC condition, we establish the following bound:\n\u2016\u2206\u0302n\u20162 \u2264 c\u03c8(Er) \u03bbn \u03ba , (9)\nwhere \u03c8(Er) = supu\u2208Er R(u) \u2016u\u20162 is a norm compatibility constant [29], and c > 0 is a constant. Note that the above bound is deterministic, but relies on assumptions on \u03bbn and \u03ba. So, we focus on characterizations of \u03bbn and \u03ba which hold with high probability for sub-Gaussian design matrices X and sub-Gaussian noise \u03c9. Recent work in [36] has extended the analyses for sub-exponential distributions.\n1A gentle exposition to Gaussian width and some of its properties is given in Appendix A.\nBounds on the regularization parameter \u03bbn: From (7) above, for the analysis to work, one needs to have \u03bbn \u2265 \u03b2R\u2217(\u2207L(\u03b8\u2217;Zn)). There are a few challenges in getting a suitable bound for \u03bbn. First, the bound depends on \u03b8\u2217, but \u03b8\u2217 is unknown and is the quantity one is interested in estimating. Second, the bound depends on Zn, the samples, and is hence random. The goal will be to bound the expectation E[R\u2217(\u2207L(\u03b8\u2217;Zn))] over all samples of size n, and obtain high-probability deviation bounds. Third, since the bound relies on the (dual) norm R\u2217(\u00b7) of a p-dimensional random vector, without proper care, the lower bound on \u03bbn may end up having a large scaling dependency, say \u221a p, on the ambient dimensionality. Since the error bound in (9) is directly proportional to \u03bbn, such dependencies will lead to weak bounds. In Section 3, we characterize the expectationE[R\u2217(\u2207L(\u03b8\u2217;Zn))] in terms of the geometry of the unit normball of R, which leads to a sharp bound. Let \u2126R = {u \u2208 Rp|R(u) \u2264 1} denote the unit norm-ball. Then, for sub-Gaussian design matrices and squared loss, we show that\nE[R\u2217(\u2207L(\u03b8\u2217;Zn))] \u2264 c\u221a n w(\u2126R) , (10)\nwhich scales as the Gaussian width of \u2126R. Interestingly, for sub-Gaussian designs, one obtains the results in terms of the \u2018sub-Gaussian width\u2019 of the unit norm-ball, which can be upper bounded by a constant times the Gaussian width using generic chaining [37]. The result can be extended to the case of anisotropic subGaussian designs, where the constant c starts depending on the maximum eigenvalue (operator norm) of the corresponding covariance matrix. Further, one can get high-probability versions of these bounds using related advances in generic chaining [37, 38]. The results can also be extended to general convex losses, such as those from generalized linear models.\nThe above characterization allows one to choose \u03bbn \u2265 c\u221anw(\u2126R). For the special case of L1 regularization, \u2126R is the unit L1 norm ball, and the corresponding Gaussian width w(\u2126R) \u2264 c1 \u221a log p, which explains the\u221a\nlog p term one finds in existing bounds for Lasso [29, 9]. When working with other norms, one simply needs to get an upper bound on the corresponding w(\u2126R).\nRestricted eigenvalue conditions: When the loss function under consideration is the squared loss, the RSC condition in (8) reduces to the restricted eigenvalue (RE) condition on the design matrix. Our analysis focuses on establishing the RE condition on A\u0304r = cone(Er) \u2229 Sp\u22121, the spherical cap obtained by intersecting the cone of the error set with the unit hypersphere, since it implies the RE condition on Er. For isotropic sub-Gaussian design matrices, a stronger two-sided restricted isometry property (RIP) holds, i.e., with high probability, for any A \u2286 Sp\u22121, we have\n1\u2212 cw(A)\u221a n \u2264 inf u\u2208A 1 n \u2016Xu\u20162 \u2264 sup\nu\u2208A\n1 n \u2016Xu\u20162 \u2264 1 + cw(A)\u221a n (11)\nwhere w(A) is the Gaussian width of A. Thus, for say n0 = 4c2w2(A\u0304r), and for n > n0, an RE condition of the form\ninf u\u2208A\u0304r\n1 n \u2016Xu\u201622 \u2265 1/2 , (12)\nis satisfied with high probability. Instead of the constant to be 1/2, one can have any constant less than 1, with suitable increase in n0. Thus, one does not need to treat the RE condition as an assumption for isotropic subGaussian designs\u2014it always holds with high probability, with the phase transition happening at O(w2(A\u0304r)) samples. The RIP results can be generalized to anisotropic sub-Gaussian designs, where additional constants depending on the restricted eigenvalues of the anisotropic covariance matrix \u03a3 show up, but the form of the bound stays similar. Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].\nGeneralized linear models and restricted strong convexity: For convex loss functions, such as those coming from generalized linear models (GLMs), the sample complexity and associated phase transition behavior\nis determined by the Restricted Strong Convexity (RSC) condition [29]. By generalizing our argument for RE conditions corresponding to square loss, we show that the RSC conditions are going to be satisfied for convex losses for subGaussian designs at the same order of sample complexity as that for squared loss. In particular, for we show a high probability lower bound of the form\ninf u\u2208A \u03b4L(u, \u03b8\u2217) \u2265 c1 \u2212 c2 w(A)\u221a n , (13)\nwhere the constants c1, c2 > 0 depend on the tail probabilities of the design matrix distribution. Specializing the result to A\u0304r = cone(Er) \u2229 Sp\u22121, we note that the sample complexity still scales as O(w2(A\u0304r)), similar to the case of squared loss. The result is thus a considerable generalization of earlier results on convex losses, such as GLMs, which had looked at specific norms and associated cones and/or did not express the results in terms of the Gaussian width of A [29].\nPutting everything together: With the above results in place, from (9), the main bound takes the form\n\u2016\u2206\u0302n\u20162 \u2264 c \u03c8(Er)[ c1 \u2212 c2w(A\u0304r)\u221an ]\n+\nw(\u2126R)\u221a n\n(14)\nwith high probability, wherew(\u2126R) is the Gaussian width of the unit norm ball, w(A\u0304r) is the Gaussian width of the spherical cap corresponding to the error set cone(Er), and the result is valid only when n > n0 = O(w2(A\u0304r)) which corresponds to the sample complexity. For the special case of L1 norm, i.e., Lasso, the sample complexity n0 is of the order w2(A\u0304r) = O(s log p). Further, w(\u2126R) = \u221a log p and \u03c8(Er) = \u221a s.\nPlugging in these values, choosing \u03b2 = 2, for n > c3s log p, the bound \u2016\u2206\u0302n\u20162 \u2264 c \u221a s log p n holds with probability. For other norms, one can simply plug-in the widths to get the corresponding sample complexity and non-asymptotic error bounds.\nThe rest of the paper is organized as follows: Section 2 presents results on the restricted error set and deterministic error bounds under suitable bounds on the regularization parameter \u03bbn and RSC assumptions. Section 3 presents a characterization of \u03bbn in terms of the Gaussian width of the unit norm ball for Gaussian as well as sub-Gaussian designs and noise. Section 4 proves RE conditions and associated sample complexity results corresponding to squared loss functions. Results are presented for subGaussian designs, including anisotropic and correlated cases, and always in terms of the Gaussian width of the spherical cap corresponding to the error set. Section 6 presents RSC conditions corresponding to general convex losses arising from generalized linear models, and the results are again in terms of the Gaussian width of the spherical cap corresponding to the error set. We conclude in Section 7. All technical arguments and proofs are in the appendix, along with a gentle exposition to Gaussian widths and related results.\nA brief word on the notation used. We denote random matrices as X , and random vectors as Xi where i may be an index to a row or column of a random matrix. Vector norms are denoted as \u2016 \u00b7 \u2016, e.g., \u2016Xi\u20162 for a (random) vector Xi, and norms of random variables are denoted as |||\u00b7|||, e.g., |||X|||2 = E[\u2016X\u20162]."}, {"heading": "2 Restricted Error Set and Recovery Guarantees", "text": "In this section, we give a characterization of the restricted error setEr in which the error vector \u2206\u0302n = (\u03b8\u0302\u03bbn\u2212 \u03b8\u2217) lies, establish clear relationships between the error sets for the regularized and constrained problems, and finally establish upper bounds on the estimation error. The error bound is deterministic, but has quantities which involve \u03b8\u2217, X, \u03c9, for which we develop high probability bounds in Sections 3, 4, and 6."}, {"heading": "2.1 The Restricted Error Set and the Error Cone", "text": "We start with a characterization of the restricted error set Er where \u2206\u0302n will belong.\nLemma 1 For any \u03b2 > 1, assuming\n\u03bbn \u2265 \u03b2R\u2217(\u2207L(\u03b8\u2217;Zn)) , (15)\nwhere R\u2217(\u00b7) is the dual norm of R(\u00b7). Then the error vector \u2206\u0302n = \u03b8\u0302\u03bbn \u2212 \u03b8\u2217 belongs to the set\nEr = Er(\u03b8 \u2217, \u03b2) = { \u2206 \u2208 Rp \u2223\u2223\u2223\u2223 R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217) + 1\u03b2R(\u2206) } . (16)\nThe restricted error set Er need not be convex for general norms. Interestingly, for \u03b2 = 1, the inequality in (16) is just the triangle inequality, and is satisfied by all \u2206. Note that \u03b2 > 1 restricts the set of \u2206 which satisfy the inequality, yielding the restricted error set. In particular, \u2206 cannot go in the direction of \u03b8\u2217, i.e., \u2206 6= \u03b1\u03b8\u2217 for any \u03b1 > 0. Further, note that the condition in (15) is similar to that in [29] for \u03b2 = 2, but the above characterization holds for any norm, not just decomposable norms [29].\nWhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15]. A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15] given by:\n\u03b8\u0302\u03b3n = argmin \u03b8\u2208Rp\nR(\u03b8) s.t. R\u2217(XT (y \u2212X\u03b8\u2217)) \u2264 \u03b3n , (17)\nwhere R\u2217(\u00b7) denotes the dual norm of R(\u00b7). One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form:\nEc = {\u2206 \u2208 Rp | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} . (18)\nBy definition, it is easy to see that Ec is always convex, and that Ec \u2286 Er, as shown schematically in Figure 1.\nThe following results establishes a relationship between Er and Ec in terms of their Gaussian widths.\nTheorem 1 Let A(\u03c1)c = Ec \u2229 \u03c1Bp2 , A (\u03c1) r = Er \u2229 \u03c1Bp2 , and A\u0304 (\u03c1) c = Cc \u2229 \u03c1Bp2 , where \u03c1B p 2 = {u|\u2016u\u20162 \u2264 \u03c1} is the L2 ball of any radius \u03c1 > 0. Then, for any \u03b2 > 1 we have\nw(A(\u03c1)c ) \u2264 w(A(\u03c1)r ) \u2264 ( 1 + 2 \u03b2 \u2212 1 \u2016\u03b8\u2217\u20162 \u03c1 ) w(A\u0304(\u03c1)c ) , (19)\nwhere w(A) denotes the Gaussian width of any set A given by: w(A) = Eg [ sup a\u2208A \u3008a, g\u3009 ] , where g is an isotropic Gaussian random vector, i.e., g \u223c N(0, Ip\u00d7p).\nThus, the Gaussian width of the error sets of regularized and constrained problems are closely related. See Figure 1 for more details. In particular, for \u2016\u03b8\u2217\u20162 = 1, with \u03c1 = 1, \u03b2 = 2, we have w(Ac) \u2264 w(Ar) \u2264 3w(A\u0304c) as introduced in Section 1. Related observations have been made for the special case of the L1 norm [6], although past work did not provide an explicit characterization in terms of Gaussian widths. The result also suggests that it is possible to move between the error analysis of the regularized and the constrained versions of the estimation problem."}, {"heading": "2.2 Recovery Guarantees", "text": "In order to establish recovery guarantees, we start by assuming that restricted strong convexity (RSC) is satisfied by the loss function in Er, the error set, so that for any \u2206 \u2208 Er, there exists a suitable constant \u03ba so that\n\u03b4L(\u2206, \u03b8\u2217) , L(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217)\u2212 \u3008\u2207L(\u03b8\u2217),\u2206\u3009 \u2265 \u03ba\u2016\u2206\u201622 . (20)\nIn Sections 4 and 6, we establish precise forms of the RSC condition for a wide variety of design matrices and loss functions. In order to establish recovery guarantees, we focus on the quantity\nF(\u2206) = L(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217) + \u03bbn(R(\u03b8\u2217 + \u2206)\u2212R(\u03b8\u2217)) . (21)\nSince \u03b8\u0302\u03bbn = \u03b8 \u2217 + \u2206\u0302n is the estimated parameter, i.e., \u03b8\u0302\u03bbn is the minimum of the objective, we clearly have F(\u2206\u0302n) \u2264 0, which implies a bound on \u2016\u2206\u0302n\u20162. Unlike previous analysis, the bound can be established without making any additional assumptions on the norm R(\u03b8). We start with the following result, which expresses the upper bound on \u2016\u2206\u0302n\u20162 in terms of the gradient of the objective at \u03b8\u2217.\nLemma 2 Assume that the RSC condition is satisfied in Er by the loss L(\u00b7) with parameter \u03ba. With \u2206\u0302n = \u03b8\u0302\u03bbn \u2212 \u03b8\u2217, for any norm R(\u00b7), we have\n\u2016\u2206\u0302n\u20162 \u2264 1 \u03ba \u2016\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217)\u20162 , (22)\nwhere\u2207R(\u00b7) is any sub-gradient of the norm R(\u00b7).\nFigure 3 illustrates the above results. Note that the right hand side is simply the L2 norm of the gradient of the objective evaluated at \u03b8\u2217. For the special case when \u03b8\u0302\u03bbn = \u03b8\n\u2217, the gradient of the objective is zero, implying correctly that \u2016\u2206\u0302n\u20162 = 0. While the above result provides useful insights about the bound on \u2016\u2206\u0302n\u20162, the quantities on the right hand side depend on \u03b8\u2217, which is unknown. We present another form of the result in terms of quantities such as \u03bbn, \u03ba, and the norm compatibility constant \u03c8(Er) = supu\u2208Er R(u) \u2016u\u20162 , which are often easier to compute or bound.\nTheorem 2 Assume that the RSC condition is satisfied in Er by the loss L(\u00b7) with parameter \u03ba. With \u2206\u0302n = \u03b8\u0302\u03bbn \u2212 \u03b8\u2217, for any norm R(\u00b7), we have\n\u2016\u2206\u0302n\u20162 \u2264 \u03c8(Er) 1 + \u03b2\n\u03b2 \u03bbn \u03ba . (23)\nThe above result is deterministic, but contains \u03bbn and \u03ba. In Section 3, we give precise characterizations of \u03bbn, which needs to satisfy (15). In Sections 4 and 6, we characterize the RSC condition constant \u03ba for different losses and a variety of design matrices."}, {"heading": "2.3 A Special Case: Decomposable Norms", "text": "In recent work, [29] considered regularized regression with the special case of decomposable norms, defined in terms of a pair of subspacesM \u2286 M\u0304 of Rp. The model is assumed to be in the subspaceM, and the\ndefinition considers the so-called perturbation subspace M\u0304\u22a5 which is the orthogonal complement of M\u0304. A norm R(\u00b7) is considered decomposable with respect to subspaces (M,M\u0304\u22a5) if R(\u03b8+\u03b3) = R(\u03b8) +R(\u03b3) for all \u03b8 \u2208M and \u03b3 \u2208 M\u0304\u22a5. We show that for decomposable norms, the error set Er in our analysis is included in the error cone defined in [29]. In the current context, let \u03b2 = 2, \u03b8\u2217 \u2208M, then for any \u2206 = \u2206M\u0304\u22a5 + \u2206M\u0304 \u2208 Er, we have\nR(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217) + 1 2 R(\u2206) (24)\n\u21d2 R(\u03b8\u2217 + \u2206M\u0304\u22a5 + \u2206M\u0304) \u2264 R(\u03b8\u2217) + 1\n2 R(\u2206M\u0304\u22a5 + \u2206M\u0304) (25)\n\u21d2 R(\u03b8\u2217 + \u2206M\u0304\u22a5)\u2212R(\u2206M\u0304) (a) \u2264 R(\u03b8\u2217) + 1 2 R(\u2206M\u0304\u22a5) + 1 2 R(\u2206M\u0304) (26) \u21d2 R(\u03b8\u2217) +R(\u2206M\u0304\u22a5)\u2212R(\u2206M\u0304) (b)\n\u2264 R(\u03b8\u2217) + 1 2 R(\u2206M\u0304\u22a5) + 1 2 R(\u2206M\u0304) (27)\n\u21d2 R(\u2206M\u0304\u22a5) \u2264 3R(\u2206M\u0304). (28)\nwhere inequality (a) follows from the triangle inequality and (b) follows from decomposability of the norm. The last inequality is precisely the error cone in [29] for \u03b8\u2217 \u2208 M. As a result, for any \u2206 \u2208 Er, for decomposable norms we have\nR(\u2206) = R(\u2206M\u0304\u22a5 + \u2206M\u0304) \u2264 R(\u2206M\u0304\u22a5) +R(\u2206M\u0304) \u2264 4R(\u2206M\u0304) (29)\nHence, the norm compatibility constant can be bounded as\n\u03c8(Er) = sup \u2206\u2208Er\nR(\u2206) \u2016\u2206\u20162 \u2264 4 sup \u2206\u2208Er R(\u2206M\u0304) \u2016\u2206\u20162 \u2264 4 sup u\u2208M\u0304\\{0} R(u) \u2016u\u20162 = 4\u03a8(M\u0304). (30)\nwhere \u03a8(M\u0304) is the subspace compatibility in M\u0304, as used in [29]."}, {"heading": "3 Bounds on the Regularization Parameter", "text": "Recall that the parameter \u03bbn needs to satisfy the inequality\n\u03bbn \u2265 \u03b2R\u2217(\u2207L(\u03b8\u2217;Zn)) . (31)\nThe right hand side of the inequality has two issues: the expression depends on \u03b8\u2217, the optimal parameter which is unknown, and the expression is a random variable, since it depends on Zn. In this section, we characterize the expectation E[R\u2217(\u2207L(\u03b8\u2217;Zn))] in terms of the Gaussian width of the unit norm ball \u2126R = {u : R(u) \u2264 1}, and further discuss its upper bounds. For ease of exposition, we present results for the case of squared loss, i.e., L(\u03b8\u2217;Zn) = 12n\u2016y\u2212X\u03b8\n\u2217\u20162 with the linear model y = X\u03b8+\u03c9, where \u03c9 is noise vector with i.i.d. entries. Under this setting,\n\u2207L(\u03b8\u2217;Zn) = 1 n XT (y \u2212X\u03b8\u2217) = 1 n XT\u03c9 , (32)\nwhich eliminates the dependency on the unknown \u03b8\u2217. Before presenting the results, we introduce a few notations. We let \u039bmax(\u00b7) denote the largest eigenvalue of a square matrix. We also recall the definition of the sub-Gaussian norm for a sub-Gaussian variable x, |||x|||\u03c82 = supp\u22651 1\u221a p(E[|x| p])1/p [41]. From this section onwards, the analysis will take into account the randomness of the design X and the noise \u03c9. Here we give a brief description of our assumptions on X and \u03c9 as follows,\nIsotropic Sub-Gaussian Designs: the design matrix X \u2208 Rn\u00d7p has independent sub-Gaussian rows where each row satisfies |||Xi|||\u03c82 \u2264 \u03ba and E[XiX T i ] = Ip\u00d7p. Thus, the measure \u00b5 from which the rows Xi are sampled independently is an isotropic sub-Gaussian measure. Anisotropic Sub-Gaussian Designs: the design matrix X \u2208 Rn\u00d7p has independent rows, and each row Xi is anisotropic sub-Gaussian with E[XTi Xi] = \u03a3. Further, we assume that corresponding isotropic random\nvector X\u0303i = Xi\u03a3\u22121/2 satisfies \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223X\u0303i\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u03c82 \u2264 \u03ba. A simple special case of such an anisotropic sub-Gaussian design is when Xi \u223c N(0,\u03a3), where X\u0303i = Xi\u03a3\u22121/2 \u223c N(0, I) so that \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223X\u0303i\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u03c82 = 1.\nSub-Gaussian Noise: the noise \u03c9 has i.i.d. centered unit-variance sub-Gaussian entries with |||\u03c9i|||\u03c82 \u2264 K. For convenience, we only use the shorthand in bold font to specify the assumptions. In the following theorem, we characterize the expectation ofR\u2217(\u2207L(\u03b8\u2217;Zn)) in terms of Gaussian width of the unit norm ballw(\u2126R).\nTheorem 3 Let \u2126R = {u : R(u) \u2264 1}, and L be the squared loss. For sub-Gaussian design X and noise \u03c9, we have\nE [R\u2217(\u2207L(\u03b8\u2217;Zn))] \u2264 \u03b7\u03be \u00b7 \u03baw(\u2126R)\u221a n , (33)\nwhere the expectation is taken over both X and \u03c9. The constant \u03be is given by\n\u03be =\n{ 1 if X is isotropic\u221a\n\u039bmax(\u03a3) if X is anisotropic .\nBounding the expectation of R\u2217(\u2207L(\u03b8\u2217;Zn)) gives us a rough scale of the regularization parameter \u03bbn. In the next theorem, we present a high-probability upper bound for R\u2217(\u2207L(\u03b8\u2217;Zn)).\nTheorem 4 Let designX and noise \u03c9 be sub-Gaussian, andL be squared loss. Define \u03c6 = supR(u)\u22641 \u2016u\u20162, then for any \u03c4 > 0, with probability at least 1\u2212 c1 exp ( \u2212min ( ( \u03c4c2\u03be\u03ba\u03c6) 2, c0n )) , we have\nR\u2217 (\u2207L(\u03b8\u2217;Zn)) \u2264 \u221a 2K2 + 1\nn (c\u03be\u03ba \u00b7 w(\u2126R) + \u03c4) , (34)\nwhere c, c0, c1 and c2 are all absolute constants, and \u03be is the same as in Theorem 3.\nBounding the Gaussian width w(\u2126R): In certain cases, one may be able to directly obtain a bound on the Gaussian width w(\u2126R). Here, we provide a mechanism for bounding the Gaussian width w(\u2126R) of the unit norm ball in terms of the Gaussian width of a suitable cone, obtained by shifting or translating the norm ball. In particular, the result involves taking any point on the boundary of the unit norm ball, considering that as the origin, and constructing a cone using the norm ball. Since such a construction can be done with any point on the boundary, the tightest bound is obtained by taking the infimum over all points on the boundary. The motivation behind getting an upper bound of the Gaussian width w(\u2126R) of the unit norm ball in terms of the Gaussian width of such a cone is because considerable advances have been made in recent years in upper bounding Gaussian widths of such cones [14, 1].\nLemma 3 Let \u2126R = {u : R(u) \u2264 1} be the unit norm ball and \u0398R = {u : R(u) = 1} be the boundary. For any \u03b8\u0303 \u2208 \u0398R, \u03c1(\u03b8\u0303) = sup\u03b8:R(\u03b8)\u22641 \u2016\u03b8 \u2212 \u03b8\u0303\u20162 is the diameter of \u2126R measured with respect to \u03b8\u0303. Let G(\u03b8\u0303) = cone(\u2126R \u2212 \u03b8\u0303) \u2229 \u03c1(\u03b8\u0303)Bp2 , i.e., the cone of (\u2126R \u2212 \u03b8\u0303) intersecting the ball of radius \u03c1(\u03b8\u0303). Then\nw(\u2126R) \u2264 inf \u03b8\u0303\u2208\u0398R w(G(\u03b8\u0303)) . (35)\nThe analysis and results for \u03bbn presented above can be extended to general convex losses arising in the context of GLMs for sub-Gussian designs and sub-Gaussian noise (see Section 6). ."}, {"heading": "4 Least Squares Models: Restricted Eigenvalue Conditions", "text": "The error bound analysis in Theorem 2 depends on the restricted strong convexity (RSC) assumption. In this section, we establish RSC conditions for sub-Gaussian design matrices when the loss function is the squared loss. For squared loss, i.e., L(\u03b8;Zn) = 1n\u2016y \u2212 X\u03b8\u2016\n2, the RSC condition (20) becomes equivalent to the Restricted Eigenvalue (RE) condition [6, 29], since\n\u03b4L(\u2206, \u03b8\u2217) = 1 n \u2016y \u2212X(\u03b8\u2217 + \u2206)\u20162 \u2212 1 n \u2016y \u2212X\u03b8\u2217\u20162 + 1 n \u3008XT (y \u2212X\u03b8\u2217),\u2206\u3009\n= 1 n \u2016X\u2206\u20162 = 1 n n\u2211 i=1 \u3008Xi,\u2206\u30092 . (36)\nso that the condition simplifies to\n\u03b4L(\u2206, \u03b8\u2217) = 1 n n\u2211 i=1 \u3008Xi,\u2206\u30092 \u2265 \u03ba\u2016\u2206\u201622 , (37)\nfor all \u2206 \u2208 Er. We make two simplifications which lets us develop the RE results in terms of widths of spherical caps rather than over the error set Er. Let nEr be the sample complexity for the RE condition over the set Er, so that for n > nEr samples, with high probability\ninf \u2206\u2208Er\n1 n \u2016X\u2206\u201622 \u2265 \u03baEr\u2016\u2206\u201622 , (38)\nfor some \u03baEr > 0. Let Cr = cone(Er) and let nCr be the sample complexity for the RE condition over the cone Cr, so that for n > nCr samples, with high probability\ninf \u2206\u2208Cr\n1 n \u2016X\u2206\u201622 \u2265 \u03baCr\u2016\u2206\u201622 , (39)\nfor some \u03baCr > 0. Since Er \u2286 Cr, we have nEr \u2264 nCr . Thus, it is sufficient to obtain (an upper bound to) the sample complexity nCr , since that will serve as an upper bound to nEr , the sample complexity over Er. Further, since Cr is a cone, the absolute magnitude \u2016\u2206\u20162 does not affect the sample complexity. As a result, it is sufficient to focus on a spherical cap A = Cr \u2229Sp\u22121. In particular, if nA denotes the sample complexity for the RE condition over the spherical cap A, so that for n > nA samples, with high probability\ninf u\u2208A\n1 n \u2016Xu\u201622 \u2265 \u03ba\u0304A\u2016u\u201622 , (40)\nfor some \u03ba\u0304A > 0, then nA = nCr \u2265 nEr . Noting that \u2016u\u20162 = 1 for u \u2208 Cr \u2229 Sp\u22121, we consider sample complexity results for RE conditions the form\ninf u\u2208A\n1 n \u2016Xu\u201622 = inf u\u2208A 1 n n\u2211 i=1 \u3008Xi, u\u30092 \u2265 \u03baA(n, p) (41)\nwhere \u03baA(n, p) > 0 with high probability for n > nA. In this section, we characterize sample complexity nA over any given spherical cap A, and establish RE conditions for isotropic and anisotropic sub-Gaussian design matrices X in terms of the Gaussian width w(A).\nAnalysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34]. The RE/RIP conditions for independent isotropic Gaussian designs have been widely studied for the case of L1 norm [10, 6]. The generalization to RE condition for correlated Gaussian designs for the special of L1 norm was studied in [33]. [14] consider the more general context of atomic norms, and RE condition analysis applies to any spherical cap A, with sample complexity results in terms of w(A), the Gaussian width of A. However, the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices. Progress has been made on establishing RE conditions for sub-Gaussian designs for error sets/caps corresponding to specific norms such as L1 [46]. In recent work, RE conditions were developed for anisotropic sub-Gaussian designs for the L1 norm [34]. Further, recent work have pointed out the differences between the RE and the RIP condition, which gives a two-sided bound on quadratic forms of random matrices [29]. In particular, while the RE condition is sufficient for structured estimation, the RIP results are stronger and may have higher sample complexity.\nIn the following, we establish the stronger RIP results for any spherical cap A and any sub-Gaussian design matrix, handling the isotropic and anisotropic cases separately. The special case of Gaussian design matrices are automatically covered by the sub-Gaussian results, and results such as Gordon\u2019s inequality can be viewed as a special case. All results are in terms of w(A), the Gaussian width of A, even for sub-Gaussian designs. In fact, all existing RE results do implicitly have the width term, but in a form specific to the chosen norm [33, 34]. The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.\nThe proof technique we use is an application of generic chaining [37, 38]. The specific form we utilize was originally developed in [21, 28]. The main idea is to pose the RIP condition as a bound on the supremum of a suitable stochastic process, so that generic chaining can be invoked to obtain a bound. The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use\nof generic chaining which simplifies the analysis considerably. Further, the RIP results can be viewed as a generalization of the celebrated Johnson-Lindenstrauss (JL) lemma [17], and the interested reader can explore these connections in [21]. Isotropic Sub-Gaussian Designs: We first consider the case where the design matrix X \u2208 Rn\u00d7p has independent sub-Gaussian rows where each row satisfies |||Xi|||\u03c82 \u2264 \u03ba and E[XiX T i ] = Ip\u00d7p. Thus, the measure \u00b5 from which the rows Xi are sampled independently is an isotropic sub-Gaussian measure.\nTheorem 5 Let X be a design matrix with independent isotropic sub-Gaussian rows, i.e., |||Xi|||\u03c82 \u2264 \u03ba and E[XiX T i ] = Ip\u00d7p. Then, for absolute constants \u03b7, c > 0, with probability at least (1 \u2212 2 exp(\u2212\u03b7w2(A))), we have\nsup u\u2208A \u2223\u2223\u2223\u2223 1n ||Xu||2 \u2212 1 \u2223\u2223\u2223\u2223 = sup u\u2208A \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 \u3008Xi, u\u30092 \u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2264 cw(A)\u221an , (42) or, equivalently,\n1\u2212 cw(A)\u221a n \u2264 inf u\u2208A 1 n ||Xu||2 \u2264 sup\nu\u2208A\n1 n ||Xu||2 \u2264 1 + cw(A)\u221a n . (43)\nAs a result, for n > c2w2(A), the RE condition: infu\u2208A \u2016Xu\u20162 \u2265 1 \u2212 cw(A)/ \u221a n > 0 is satisfied with high probability for any sub-Gaussian design matrix. More generally, choosing = cw(A)/ \u221a n, one can write the result in a traditional RIP form [10]. Anisotropic Sub-Gaussian Designs: We now consider the case where the design matrix X \u2208 Rn\u00d7p has independent rows, and each row Xi is anisotropic sub-Gaussian with E[XTi Xi] = \u03a3. Further, we assume\nthat corresponding isotropic random vector X\u0303i = Xi\u03a3\u22121/2 satisfies \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223X\u0303i\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u03c82 \u2264 \u03ba. A simple special case\nof such an anisotropic sub-Gaussian design is when Xi \u223c N(0,\u03a3), where X\u0303i = Xi\u03a3\u22121/2 \u223c N(0, I) so that \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223X\u0303i\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u03c82 = 1. The result below characterizes RIP-style property of any such anisotropic sub-Gaussian designs.\nTheorem 6 Let X be a design matrix with independent anisotropic sub-Gaussian rows, i.e., E[XTi Xi] = \u03a3 and \u2223\u2223\u2223\u2223\u2223\u2223Xi\u03a3\u22121/2\u2223\u2223\u2223\u2223\u2223\u2223\u03c82 \u2264 \u03ba. Then, for absolute constants \u03b7, c > 0, with probability at least (1\u22122 exp(\u2212\u03b7w2(A))), we have\nsup u\u2208A \u2223\u2223\u2223\u2223 1n 1uT\u03a3u ||Xu||2 \u2212 1 \u2223\u2223\u2223\u2223 = sup u\u2208A \u2223\u2223\u2223\u2223\u2223 1n 1uT\u03a3u n\u2211 i=1 \u3008Xi, u\u30092 \u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2264 cw(A)\u221an . (44) Further,\n\u03bbmin(\u03a3|A) (\n1\u2212 cw(A)\u221a n\n) \u2264 inf\nu\u2208A\n1 n ||Xu||2 \u2264 sup\nu\u2208A\n1 n ||Xu||2 \u2264 \u03bbmax(\u03a3|A)\n( 1 + c\nw(A)\u221a n\n) ,\n(45) where\n\u03bbmin(\u03a3|A) = inf u\u2208A uT\u03a3u , and \u03bbmax(\u03a3|A) = sup u\u2208A uT\u03a3u (46)\nare the restricted minimum and maximum eigenvalues of \u03a3 restricted to A \u2286 Sp\u22121.\nThus, for the anisotropic case, the RIP is with respect to the restricted minimum and maximum eigenvalues corresponding to A \u2286 Sp\u22121. For the special case when A = Sp\u22121, we have \u03bbmin(\u03a3|A) = \u03bbmin(\u03a3), the minimum eigenvalue, and \u03bbmax(\u03a3|A) = \u03bbmax(\u03a3), the maximum eigenvalue of \u03a3. Further, when \u03a3 = I,\nwe get back the result in Theorem 11. Finally, it is instructive to compare the above result to existing characterizations of the RE condition for anisotropic Gaussian [33] and anisotropic sub-Gaussian [34] designs, focused on the L1 norm. The result in Theorem 12 is a more general RIP result, applies to any spherical cap A, and is in terms of w(A), the Gaussian width of the spherical cap A."}, {"heading": "5 Examples and Applications", "text": "In this section, we give examples of the analysis from previous sections for three norms: L1 norm, group sparse norm, andL2 norm. The summary of the results is given in Table 1. Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16]. L1 Norm: Assume that the statistical parameter \u03b8\u2217 is s-sparse, and note that \u2016\u03b8\u2217\u20161 \u2264 \u221a s\u2016\u03b8\u2217\u20162. Since L1 norm is a decomposable norm following the result in (28), we have \u03a8(Er) \u2264 4\u03a8(M\u0304) = 4 \u221a s.\nApplying Lemma 3, let \u03b8\u0304 be a 1-sparse vector, and \u03c1(\u03b8\u0304) = 2, then w(\u2126R) can be bounded by\nw(\u2126R) \u2264 inf \u03b8\u0303\u2208\u0398R\nw(G(\u03b8\u0303)) = w(G(\u03b8\u0304)) (a) = O (\u221a log p ) , (47)\nwhere (a) is obtained from the fact that Gaussian width ofG(\u03b8\u0303) with \u03b8\u0303 be a s-sparse vector is \u221a\n2s log(ps ) + 5 4s\n[14]. See Figure 4 for more details. From Theorem 4 and (47), the bound on \u03bbn is\n\u03bbn \u2264 c w(\u2126R)\u221a\nn = O\n(\u221a log p\nn\n) . (48)\nHence, the recovery error is bounded by\n\u2016\u2206\u0302n\u20162 \u2264 c3 \u03a8(Er)\u03bbn\n\u03ba = O\n(\u221a s log p\nn\n) , (49)\nwhich is similar to the results obtained in well known results [14, 29].\nGroup Sparse Norm: Suppose that the index set {1, 2, \u00b7 \u00b7 \u00b7 , p} can be partitioned into a set of T disjoint groups, say G = {G1,G2, \u00b7 \u00b7 \u00b7 ,GT }. Define (1, \u03bd)-group norm for a given vector \u03bd = (\u03bd1, \u00b7 \u00b7 \u00b7 , \u03bdT ) \u2208 [1,\u221e]T as\n\u2016\u03b1\u2016G,\u03bd = T\u2211 t=1 \u2016\u03b1Gt\u2016\u03bdt (50)\nAs shown in [29] Group norm is a decomposable norm. For a given subset SG \u2282 {1, . . . , T} with cardinality |SG |, define the subspace A(SG) = {\u03b1 \u2208 Rp |\u03b1Gt = 0, \u2200t /\u2208 SG }. Let \u03bdt \u2265 2, then we have\n\u2016\u2206\u2016G,\u03bd = \u2211 t\u2208SG \u2016\u2206Gt\u2016\u03bdt \u2264 \u2211 t\u2208SG \u2016\u2206Gt\u20162 \u2264 \u221a sG\u2016\u2206\u20162. (51)\nHence, from (30) and (51) we have \u03a8(Er) \u2264 4 \u221a sG . (52)\nApplying Lemma 3, define \u03b8\u0304 with 1-active group, and \u03c1(\u03b8\u0304) = 2, then w(\u2126R) can be bounded by\nw(\u2126R) \u2264 inf \u03b8\u0303\u2208\u0398R\nw(G(\u03b8\u0303)) = w(G(\u03b8\u0304)) (a) = O (\u221a m+ log T ) , (53)\nwhere m = max t |Gt| and (a) is obtained from the fact that Gaussian width of G(\u03b8\u0303) where \u03b8\u0303 has k active group is \u221a 2k(m+ log(T \u2212 k)) + k [14]. From Theorem 4 and (53), the bound on \u03bbn is\n\u03bbn \u2264 c w(\u2126R)\u221a\nn = O\n(\u221a m+ log T\nn\n) . (54)\nHence, the recovery error is bounded by\n\u2016\u2206\u0302n\u20162 \u2264 c3 \u03a8(Er)\u03bbn\n\u03ba = O\n(\u221a sG(m+ log T )\nn\n) , (55)\nwhich is similar to the results obtained in previous works [14, 29].\nL2 Norm: With L2 norm as the regularizer, the norm constant is obtained as\n\u03a8(Er) = sup \u2206\u2208Er \u2016\u2206\u20162 \u2016\u2206\u20162 = 1. (56)\nApplying Lemma 3, set \u03c1(\u03b8\u0303) = 1, then w(\u2126R) can be bounded by\nw(\u2126R) \u2264 inf \u03b8\u0303\u2208\u0398R\nw(G(\u03b8\u0303)) = O ( \u221a p) . (57)\nFrom Theorem 4 and (57), the bound on \u03bbn is\n\u03bbn \u2264 c w(\u2126R)\u221a\nn = O\n(\u221a p\nn\n) . (58)\nHence, the recovery error is bounded by\n\u2016\u2206\u0302n\u20162 \u2264 c3 \u03a8(Er)\u03bbn\n\u03ba = O\n(\u221a p\nn\n) . (59)"}, {"heading": "6 Generalized Linear Models: Restricted Strong Convexity", "text": "In this section, we extend our results to estimation with norm regularization in the context of generalized linear models (GLMs) [4, 8]. Assume that the conditional distribution of the response yi conditioned on the covariates Xi is an exponential family distribution:\np(yi|Xi; \u03b8\u2217) = p(yi|\u3008Xi, \u03b8\u2217\u3009) = exp{yi\u3008Xi, \u03b8\u2217\u3009 \u2212 \u03d5(\u3008Xi, \u03b8\u2217\u3009)} . (60)\nwhere\n\u03d5(\u3008Xi, \u03b8\u2217\u3009) = log (\u222b\nyi\nexp{yi\u3008Xi, \u03b8\u2217\u3009} dyi )\n(61)\nis the log-partition function [8, 4, 44].2 In GLMs, the conditional distribution of the response yi is characterized by an exponential family distribution p(yi|\u03b7i) with natural parameter \u03b7i = \u3008Xi, \u03b8\u2217\u3009 determined by the covariatesXi and the parameter \u03b8\u2217. It is easy to verify that the gradient of the log-partition function w.r.t. the natural parameter \u03b7i = \u3008Xi, \u03b8\u2217\u3009 gives the expectation of the response [4, 8], i.e.,\n\u2207\u03b7i\u03d5(\u03b7i) = \u2207\u3008Xi,\u03b8\u2217\u3009\u03d5(\u3008Xi, \u03b8 \u2217\u3009) = E[yi|\u3008Xi, \u03b8\u2217\u3009] . (62)\nFor estimating \u03b8\u2217, the loss function corresponding to GLMs typically consider the negative log likelihood of the conditional distribution:\nL(\u03b8;Zn) = \u2212 1 n log p(yi|Xi; \u03b8\u2217) = 1 n n\u2211 i=1 (\u03d5(\u3008Xi, \u03b8\u3009)\u2212 yi\u3008Xi, \u03b8\u3009) . (63)\nIn the current context, we assume \u03b8\u2217 to be sparse/structured, and the structure can be suitably captured by a norm R(\u00b7). Then, the estimation of \u03b8\u2217 with norm regularization takes the form:\n\u03b8\u0302\u03bbn = argmin \u03b8\u2208Rp L(\u03b8;Zn) + \u03bbnR(\u03b8) = argmin \u03b8\u2208Rp\n1\nn n\u2211 i=1 (\u03d5(\u3008Xi, \u03b8\u3009)\u2212 yi\u3008Xi, \u03b8\u3009) + \u03bbnR(\u03b8) . (64)\nNoise in the context of GLMs is simply the deviation of a specific response yi from the conditional mean, i.e., \u03c9i = E[yi|Xi] \u2212 yi. Popular examples of GLMs come from suitable choices of the conditional distribution, e.g., when p(yi|\u3008Xi, \u03b8\u3009) is Gaussian so that \u03d5(\u3008Xi, \u03b8\u3009) = \u3008Xi,\u03b8\u3009 2\n2 , Bernoulli so that \u03d5(\u3008Xi, \u03b8\u3009) = log(1+exp(\u3008Xi, \u03b8\u3009)), and Poisson where \u03d5(\u3008Xi, \u03b8\u3009) = exp(\u3008Xi, \u03b8\u3009), respectively yielding least squares regression, logistic regression, and Poisson regression loss functions. Next, we provide the key results needed to characterize the regularization parameter \u03bbn and restricted strong convexity (RSC) in the context of GLMs. The non-asymptotic bound on the estimation error then follows from the general result in (23).\nBounds on the Regularization Parameter: Following the general analysis from Section 3, the regularization parameter needs to satisfy the condition: \u03bbn \u2265 \u03b2R\u2217(\u2207\u03b8L(\u03b8\u2217;Zn)) for any fixed \u03b2 > 1. For GLMs,\n\u2207\u03b8L(\u03b8\u2217;Zn) = \u2212 1\nn n\u2211 i=1 yiXi+ 1 n n\u2211 i=1 Xi\u2207\u3008Xi,\u03b8\u2217\u3009\u03d5(\u3008Xi, \u03b8 \u2217\u3009) = 1 n n\u2211 i=1 Xi(E[y|Xi]\u2212yi) = 1 n XT\u03c9 , (65)\nwhere we have used the fact, \u2207\u3008Xi,\u03b8\u2217\u3009\u03d5(\u3008Xi, \u03b8\u2217\u3009) = E[yi|\u3008Xi, \u03b8\u2217\u3009] and \u03c9 = E[yi|\u3008Xi, \u03b8\u2217\u3009] \u2212 yi. Thus, the form of \u2207\u03b8L(\u03b8\u2217;Zn) is the same as that in Section 3. Assuming the design matrix X and noise \u03c9 are sub-Gaussian, a characterization of \u03bbn follows from Theorems 3 and 4 in Section 3. In particular, E[\u2207\u03b8L(\u03b8\u2217;Zn)] = O(w(\u2126R)\u221an ), with corresponding high probability concentration results, and it suffices to have \u03bbn to be of this order.\n2Note that for GLMs over discrete responses yi, the integration needs to be suitably changed to summation [4, 8].\nRestricted Strong Convexity: By definition, the restricted strong convexity considers\n\u03b4L(u, \u03b8\u2217) = L(\u03b8\u2217 + u)\u2212 L(\u03b8\u2217)\u2212 \u3008\u2207L(\u03b8\u2217), u\u3009 = 1 n n\u2211 i=1 \u22072\u03d5(\u3008\u03b8\u2217, Xi\u3009+ \u03b3i\u3008u,Xi\u3009)\u3008u,Xi\u30092 ,\nwhere \u03b3i \u2208 [0, 1], and where the last equality follows from a direct application of the mean value theorem [35]. Since the log-partition function \u03d5 is of Legendre type [3, 4, 8], the second derivative \u22072\u03d5(\u00b7) is always positive. Since the RSC condition relies on a non-trivial lower bound for the above quantity, the analysis will consider suitable compact sets where\u22072\u03d5(\u00b7) is bounded away from zero by a constant. In particular, for a suitable constant T , we consider the sets {Xi||\u3008Xi, \u03b8\u2217\u3009| < T} and {Xi||\u3008Xi, u\u3009| < T}. For Xi lying in these sets, the argument a = \u3008Xi, \u03b8\u2217\u3009+ \u03b3i\u3008u,Xi\u3009 of the second derivative satisfies |a| \u2264 2T , which is the compact set of interest. Within the compact set, ` = `\u03d5(T ) = min|a|\u22642T \u22072\u03d5(a) is bounded away from zero. Outside the compact set, we will only assume \u22072\u03d5(\u00b7) > 0. Based on the above construction, we have\n\u03b4L(u, \u03b8\u2217) \u2265 ` n n\u2211 i=1 \u3008Xi, u\u30092 I[|\u3008Xi, \u03b8\u2217\u3009| < T ] I[|\u3008Xi, u\u3009| < T ] . (66)\nThe quadratic form based lower bound allows us to establish RSC conditions for GLMs with isotropic subGaussian design matrices by building on results from Section 4 for RE conditions for squared loss. As a result, the sample complexity of the RSC condition is also expressed in terms of the Gaussian width of the spherical cap A derived from the error set. The analysis can be suitably generalized to anisotropic design matrices using techniques discussed in Section 4. As before, we consider u \u2208 A \u2286 Sp\u22121 so that \u2016u\u20162 = 1. Assuming X has isotropic sub-Gaussian rows with |||Xi|||\u03d52 \u2264 \u03ba, \u3008Xi, \u03b8\n\u2217\u3009 and \u3008Xi, u\u3009 are sub-Gaussian random variables with sub-Gaussian norm at most C\u03ba [42]. Denote by \u03b51 and \u03b52 the probability that \u3008Xi, u\u3009 and \u3008Xi, \u03b8\u2217\u3009 exceeds some constant T , i.e., \u03b51(T ;u) = P{|\u3008Xi, u\u3009| > T} \u2264 e \u00b7 exp(\u2212c2T 2/C2\u03ba2) = \u03b5\u03041, and \u03b52(T ; \u03b8\u2217) = P{|\u3008Xi, \u03b8\u2217\u3009| > T} \u2264 e \u00b7 exp(\u2212c2T 2/C2\u03ba2) = \u03b5\u03042, where \u03b5\u03041 = \u03b5\u03041(T, \u03ba) and \u03b5\u03042 = \u03b5\u03042(T ;\u03ba) are uniform upper bounds on the individual tail probabilities. The result we present below is in terms of the above defined constants ` = `\u03d5(T ), \u03b5\u03041 = \u03b5\u03041(T, \u03ba) and \u03b5\u03042 = \u03b5\u03042(T, \u03ba) for any suitably chosen T .\nTheorem 7 Let X \u2208 Rn\u00d7p be a design matrix with independent isotropic sub-Gaussian rows such that |||Xi|||\u03d52 \u2264 \u03ba. Then, for any set A \u2286 S\np\u22121 for suitable constants \u03b7, c > 0, with probability at least 1\u2212 2 exp ( \u2212\u03b7w2(A) ) , we have\ninf u\u2208A\n\u2202L(u, \u03b8\u2217) \u2265 `\u03c12 (\n1\u2212 c\u03ba21 w(A)\u221a n\n) , (67)\nwhere \u03c12 = infu\u2208A \u03c12u, with \u03c1 2 u = E[\u3008Xi, u\u30092I[|\u3008Xi, \u03b8\u2217\u3009| < T ]I[|\u3008Xi, u\u3009| < T ]], and \u03ba1 = \u03ba1\u2212\u03b5\u03041\u2212\u03b5\u03042 .\nThe form of the result is closely related to the corresponding result for the RE condition infu\u2208A \u2016Xu\u20162 considered in Section 4. Note that RSC analysis for GLMs was considered in [29] for specific norms, especially L1, whereas our analysis applies to any set A \u2286 Sp\u22121, hence to any norm, and the result is in terms of the Gaussian width w(A) of A. Further, following arguments in Section 4, the RE analysis for GLMs can be extended to anisotropic subGaussian design matrices."}, {"heading": "7 Conclusions", "text": "The paper presents a general set of results and tools for characterizing non-asymptotic estimation error in norm regularized regression problems. The analysis holds for any norm, and subsumes much of existing\nliterature focused on structured sparsity and related themes. The work can be viewed as a direct generalization of results in [29], which presented related results for decomposable norms. Our analysis illustrates the important role Gaussian widths, as a measure of size of suitable sets, play in such results. Further, the error sets for regularized and constrained versions of such problems are shown to be closely related [6].\nWhile the paper presents a unified geometric treatment of non-asymptotic structured estimation with regularized estimators, several technical questions need further investigation. The focus of the analysis has been on thin-tailed distributions, and the RE/RSC type analysis presented really gives two sided bounds, i.e., RIP, showing that thin-tailed distributions do satisfy the RIP condition. For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27]. Further, the sample complexity of the phase transitions in the RE/RSC conditions for anisotropic designs depend on the largest eigenvalue (operator norm) of the covariance matrix, making the estimator sample inefficient for highly correlated designs. Since real-world several problems, including spatial and temporal problems, do have correlated observations, it will be important to investigate estimators which perform well in such settings [18]. Finally, the focus of the work is on parametric estimation, and it will be interesting to explore generalizations of the analysis to non-parametric settings."}, {"heading": "Appendix", "text": ""}, {"heading": "A Background and Preliminaries", "text": "We start with a review of some definitions and well-known results which will be used for our proofs."}, {"heading": "A.1 Gaussian Width", "text": "In several of our proofs, we use the concept of Gaussian width [20, 14], which is defined as follows.\nDefinition 1 (Gaussian width) For any set A \u2208 Rp, the Gaussian width of the set A is defined as:\nw(A) = Eg [ sup u\u2208A \u3008g, u\u3009 ] , (68)\nwhere the expectation is over g \u223c N(0, Ip\u00d7p), a vector of independent zero-mean unit-variance Gaussian random variable.\nThe Gaussian width w(A) provides a geometric characterization of the size of the set A. We consider three perspectives of the Gaussian width, and provide some properties which are used in our analysis. First, consider the Gaussian process {Zu} where the constituent Gaussian random variables Zu = \u3008t, g\u3009 are indexed by u \u2208 A, and g \u223c N(0, Ip\u00d7p). Then the Gaussian width w(A) can be viewed as the expectation of the supremum of the Gaussian process {Zt}. Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24]. Second, \u3008u, g\u3009 can be viewed as a Gaussian random projection of each u \u2208 A to one dimension, and the Gaussian width simply measures the expectation of largest value of such projections. Third, if A is the unit ball of any norm R(\u00b7), i.e., A = {x \u2208 Rp | R(x) \u2264 1}, then w(A) = Eg[R\u2217(g)] by definition of the dual norm. Thus, the Gaussian width is the expected value of the dual norm of a standard Gaussian random vector. For instance, if A is unit ball of L1 norm, w(A) = E[\u2016g\u2016\u221e]. Below we list some simple and useful properties of the Gaussian width of A \u2286 Rp:\nProperty 1: w(A) \u2264 w(B) for A \u2286 B.\nProperty 2: w(A) = w(conv(A)), where conv(\u00b7) denotes the convex hull of A.\nProperty 3: w(cA) = cw(A) for any positive scalar c, in which cA = {cx | x \u2208 A}.\nProperty 4: w(\u0393A) = w(A) for any orthogonal matrix \u0393 \u2208 Rp\u00d7p.\nProperty 5: w(A+ b) = w(A) for any A \u2286 Rp and fixed b \u2208 Rp. The last two properties illustrate the Gaussian width is rotation and translation invariant."}, {"heading": "A.2 Sub-Gaussian and Sub-exponential Random Variables (Vectors)", "text": "In the proof, we will also frequently use the properties of sub-Gaussian and sub-exponential random variables (vectors). In particular, we are interested in their definitions using moments.\nDefinition 2 Sub-Gaussian (sub-exponential) random variable: We say that a random variable x is subGaussian (sub-exponential) if the moments satisfies\n[E|x|p] 1 p \u2264 K2 \u221a p ([E|x|p] 1 p \u2264 K1p) (69)\nfor any p \u2265 1 with a constant K2 (K1). The minimum value of K2 (K1) is called sub-Gaussian (subexponential) norm of x, denoted by |||x|||\u03c82 (|||x|||\u03c81).\nDefinition 3 Sub-Gaussian (sub-exponential) random vector: We say that a random vector X in Rn is sub-Gaussian (sub-exponential) if the one-dimensional marginals \u3008X,x\u3009 are sub-Gaussian (sub-exponential) random variables for all x \u2208 Rn. The sub-Gaussian (sub-exponential) norm of X is defined as\n|||X|||\u03c82 = sup x\u2208Sn\u22121 \u2016\u3008X,x\u3009\u2016\u03c82 (|||X|||\u03c81 = sup x\u2208Sn\u22121 \u2016\u3008X,x\u3009\u2016\u03c81) (70)\nThe following definitions and lemmas are from [41].\nLemma 4 Consider a finite number of independent centered sub-Gaussian random variables Xi. Then\u2211 iXi is also a centered sub-Gaussian random variable. Moreover,\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2211 i Xi \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2\n\u03c82\n\u2264 C \u2211 i |||Xi|||2\u03c82 (71)\nLemma 5 LetX1, . . . , Xn be independent centered sub-Gaussian random variables. ThenX = (X1, . . . , Xn) is a centered sub-Gaussian random vector in Rn, and\n|||X|||\u03c82 \u2264 C maxi\u2264n |||Xi|||\u03c82 (72)\nwhere C is an absolute constant.\nLemma 6 Consider a sub-Gaussian random vector X with sub-Gaussian norm K = maxi |||Xi|||\u03c82 , then, Z = \u3008X, a\u3009 is a sub-Gaussian random variable with sub-Gaussian norm |||Z|||\u03c82 \u2264 CK\u2016a\u20162.\nLemma 7 A random variable X is sub-Gaussian if and only if X2 is sub-exponential. Moreover, |||X|||2\u03c82 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223X2\u2223\u2223\u2223\u2223\u2223\u2223 \u03c81 \u2264 2|||X|||2\u03c82 (73)\nLemma 8 If X is sub-Gaussian (or sub-exponential), then so is X \u2212 EX . Moreover, the following holds,\n|||X \u2212 EX|||\u03c82 \u2264 2|||X|||\u03c82 , |||X \u2212 EX|||\u03c81 \u2264 2|||X|||\u03c81 (74)"}, {"heading": "B Restricted Error Set and Recovery Guarantees", "text": "Section 2 is about the restricted error set. Lemma 1 characterizes the restricted error set. Theorem 1 establishes the relation between the constrained and restricted error sets. In particular, we prove that the Gaussian width of the regularized and constrained error sets (cone) are of the same order. Starting with the assumption that the RSC condition is satisfied Lemma 2 and Theorem 2 derive results on the upper bound on the L2 norm of the error.\nWe collect the proofs of the different results in this section."}, {"heading": "B.1 The Restricted Error Set", "text": "Lemma 1 in Section 2 characterizes the set to which the error vector belongs. We give the proof of Lemma 1 below:\nLemma 1 For any \u03b2 > 1, assuming\n\u03bbn \u2265 \u03b2R\u2217(\u2207L(\u03b8\u2217;Zn)) (75)\nwhere R\u2217(\u00b7) is the dual norm of R(\u00b7). Then the error vector \u2206\u0302n = \u03b8\u0302\u03bbn \u2212 \u03b8\u2217 belongs to the set:\nEr = Er(\u03b8 \u2217, \u03b2) = { \u2206 \u2208 Rp \u2223\u2223\u2223\u2223 R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217) + 1\u03b2R(\u2206) } . (76)\nProof: By the optimality of \u03b8\u0302\u03bbn = \u03b8 \u2217 + \u2206\u0302n, we have\nL(\u03b8\u2217 + \u2206\u0302n) + \u03bbnR(\u03b8\u2217 + \u2206\u0302n)\u2212 {L(\u03b8\u2217) + \u03bbnR(\u03b8\u2217)} \u2264 0 . (77)\nNow, since L is convex,\nL(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217) \u2265 \u3008\u2207L(\u03b8\u2217),\u2206\u3009 \u2265 \u2212|\u3008\u2207L(\u03b8\u2217),\u2206\u3009| . (78)\nFurther, by generalized Holder\u2019s inequality, we have\n|\u3008\u2207L(\u03b8\u2217),\u2206\u3009| \u2264 R\u2217(\u2207L(\u03b8\u2217))R(\u2206) \u2264 \u03bbn \u03b2 R(\u2206) , (79)\nwhere we have used \u03bbn \u2265 \u03b2R\u2217(\u2207L(\u03b8\u2217;Zn)). Hence, we have\nL(\u03b8\u2217 + \u2206\u0302n)\u2212 L(\u03b8\u2217) \u2265 \u2212 \u03bbn \u03b2 R(\u2206\u0302n) . (80)\nAs a result,\n\u03bbn { R(\u03b8\u2217 + \u2206\u0302n)\u2212R(\u03b8\u2217)\u2212 1\n\u03b2 R(\u2206\u0302n)\n} \u2264 0 . (81)\nNoting that \u03bbn > 0 and rearranging completes the proof."}, {"heading": "B.2 Relation between the Constrained and Regularized Error Cones", "text": "In this section we show that the sizes of the regularized and constrained error sets are of the same order. Recall from [14], that the error set for the constrained setting for atomic norms is a cone given by:\nCc = Cc(\u03b8 \u2217) = cone(Ec) = cone {\u2206 \u2208 Rp | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} . (82)\nThe error set Er is given by:\nEr = Er(\u03b8 \u2217, \u03b2) = { \u2206 \u2208 Rp \u2223\u2223\u2223\u2223 R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217) + 1\u03b2R(\u2206) } .\nBelow we provide the proof of Theorem 1.\nTheorem 1 Let A(\u03c1)c = Ec \u2229 \u03c1Bp2 , A (\u03c1) r = Er \u2229 \u03c1Bp2 , and A\u0304 (\u03c1) c = Cc \u2229 \u03c1Bp2 , where \u03c1B p 2 = {u|\u2016u\u20162 \u2264 \u03c1} is the L2 ball of any radius \u03c1 > 0. Then, for any \u03b2 > 1 we have\nw(A(\u03c1)c ) \u2264 w(A(\u03c1)r ) \u2264 ( 1 + 2 \u03b2 \u2212 1 \u2016\u03b8\u2217\u20162 \u03c1 ) w(A\u0304(\u03c1)c ) , (83)\nwhere w(A) denotes the Gaussian width of any set A given by: w(A) = Eg [ sup a\u2208A \u3008a, g\u3009 ] , where g is an isotropic Gaussian random vector, i.e., g \u223c N(0, Ip\u00d7p). Proof: The first inequality simply follows from the fact that Ec \u2286 Er and Property 1 of Gaussian width. For the second part, from triangle inequality, we have\nR(\u2206) \u2264 R(\u03b8\u2217 + \u2206) +R(\u03b8\u2217) . (84)\nThen,\nEr(\u03b8 \u2217, \u03b2) = { \u2206 \u2208 Rp \u2223\u2223\u2223\u2223R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217) + 1\u03b2R(\u2206) }\n\u2286 { \u2206 \u2208 Rp \u2223\u2223\u2223\u2223R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217) + 1\u03b2R(\u03b8\u2217 + \u2206) + 1\u03b2R(\u03b8\u2217) } = { \u2206 \u2208 Rp \u2223\u2223\u2223\u2223(1\u2212 1\u03b2 ) R(\u03b8\u2217 + \u2206) \u2264 ( 1 + 1 \u03b2 ) R(\u03b8\u2217)\n} = { \u2206 \u2208 Rp \u2223\u2223\u2223\u2223R(\u03b8\u2217 + \u2206) \u2264 \u03b2 + 1\u03b2 \u2212 1R(\u03b8\u2217) } = E\u0303r(\u03b8 \u2217, \u03b2) .\nLet C\u0303r(\u03b8\u2217, \u03b2) denote the following set\nC\u0303r = C\u0303r(\u03b8 \u2217, \u03b2) = cone { \u2206\u2212 2 \u03b2 \u2212 1 \u03b8\u2217 \u2223\u2223\u2206 \u2208 E\u0304r}+ 2 \u03b2 \u2212 1 \u03b8\u2217 . (85)\nIt follows naturally from the construction that Er \u2286 C\u0303r. Let A\u0303(\u03c1)r = C\u0303r(\u03b8\u2217, \u03b2) \u2229 \u03c1Bp2 . Since Er(\u03b8\u2217, \u03b2) \u2286 C\u0303r(\u03b8\u2217, \u03b2), we have w(A (\u03c1) r ) \u2264 w(A\u0303(\u03c1)r ). We define two additional sets for our analysis:\nB\u0303(\u03c1)r = A\u0303 (\u03c1) r \u2212\n2\n\u03b2 \u2212 1 \u03b8\u2217 =\n{ \u2206 \u2208 Rp \u2223\u2223\u2223\u2223\u2206 + 2\u03b2 \u2212 1\u03b8\u2217 \u2208 A\u0303(\u03c1)r } , (86)\nD\u0303(\u03c1)c = Cc(\u03b8 \u2217, \u03b2) \u2229\n( \u03c1+ 2\n\u03b2 \u2212 1 \u2016\u03b8\u2217\u20162\n) Bp2 = { \u2206 \u2208 Rp \u2223\u2223\u2223\u2223\u2206 \u2208 Cc, \u2016\u2206\u20162 \u2264 (\u03c1+ 2\u03b2 \u2212 1 ) \u2016\u03b8\u2217\u20162 } .\n(87)\nFollowing Property 3 of Gaussian width, we have\nw(D\u0303(\u03c1)c ) =\n( 1 + 2\n\u03b2 \u2212 1 \u2016\u03b8\u2217\u20162 \u03c1 ) w(A\u0304(\u03c1)c ) . (88)\nFurther, using Property 5 of Gaussian width, we have\nw(A\u0303(\u03c1)r ) = w(B\u0303 (\u03c1) r ) . (89)\nFrom the construction it is clear that B\u0303(\u03c1)r \u2282 D\u0303(\u03c1)c . Hence we have\nw(D\u0303(\u03c1)c ) \u2265 w(B\u0303(\u03c1)r ) (90)\nThen, we have\nw(A\u0303(\u03c1)r ) = w(B\u0303 (\u03c1) r ) \u2264 w(D\u0303(\u03c1)c ) =\n( 1 + 2\n\u03b2 \u2212 1 \u2016\u03b8\u2217\u20162 \u03c1 ) w(A\u0304(\u03c1)c )\nBy noting that w(A(\u03c1)r ) \u2264 w(A\u0303(\u03c1)r ), we complete the proof."}, {"heading": "B.3 Recovery Guarantees", "text": "Lemma 2 and Theorem 2 in the paper are results which establish recovery guarantees. The result in Lemma 2 depends on \u03b8\u2217, which is unknown. On the other hand Theorem 2 gives the result in terms of quantities like \u03bbn and the norm compatibility constant \u03a8(Er) = supu\u2208Er R(u) \u2016u\u20162 which are easier to compute or bound. In this section we give proofs of Lemma 2 and Theorem 2.\nLemma 2 Assume that the RSC condition is satisfied in Er by the loss L(\u00b7) with parameter \u03ba. With \u2206\u0302n = \u03b8\u0302\u03bbn \u2212 \u03b8\u2217, for any norm R(\u00b7), we have\n\u2016\u2206\u0302n\u20162 \u2264 1 \u03ba \u2016\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217)\u20162 , (91)\nwhere\u2207R(\u00b7) is any sub-gradient of the norm R(\u00b7). Proof: By the RSC property in Er, for any \u2206 \u2208 Er we have\nL(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217) \u2265 \u3008\u2207L(\u03b8\u2217),\u2206\u3009+ \u03ba\u2016\u2206\u201622 . (92)\nAlso, recall that any norm is convex, since by triangle inequality, for t \u2208 [0, 1], we have\nR(t\u03b81 + (1\u2212 t)\u03b82) \u2264 R(t\u03b81) +R((1\u2212 t)\u03b82) = tR(\u03b81) + (1\u2212 t)R(\u03b82) . (93)\nAs a result, for any sub-gradient\u2207R(\u03b8) of R(\u03b8), we have\nR(\u03b8\u2217 + \u2206)\u2212R(\u03b8\u2217) \u2265 \u3008\u2206,\u2207R(\u03b8\u2217)\u3009 . (94)\nAdding (92) and (94), we get\nL(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217) + \u03bbn(R(\u03b8\u2217 + \u2206)\u2212R(\u03b8\u2217)) \u2265 \u3008\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217),\u2206\u3009+ \u03ba\u2016\u2206\u201622 (95)\nNow, by Cauchy-Schwartz inequality, we have\n|\u3008\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217),\u2206\u3009| \u2264 \u2016\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217)\u20162\u2016\u2206\u20162 \u21d2 \u3008\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217),\u2206\u3009 \u2265 \u2212\u2016\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217)\u20162\u2016\u2206\u20162 . (96)\nUsing (96) in (95), we have\nF(\u2206) = L(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217) + \u03bbn(R(\u03b8\u2217 + \u2206)\u2212R(\u03b8\u2217)) \u2265 \u2212\u2016\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217)\u20162\u2016\u2206\u20162 + \u03ba\u2016\u2206\u201622\n= \u03ba\u2016\u2206\u20162 { \u2016\u2206\u20162 \u2212\n\u2016\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217)\u20162 \u03ba\n} . (97)\nNow, since F(\u2206\u0302n) \u2264 0, from (97), we have\n\u2016\u2206\u0302n\u20162 \u2264 \u2016\u2207L(\u03b8\u2217) + \u03bbn\u2207R(\u03b8\u2217)\u20162\n\u03ba , (98)\nwhich completes the proof.\nTheorem 2 Assume that the RSC condition is satisfied in Er by the loss L(\u00b7) with parameter \u03ba. With \u2206\u0302n = \u03b8\u0302\u03bbn \u2212 \u03b8\u2217, for any norm R(\u00b7), we have\n\u2016\u2206\u0302n\u20162 \u2264 1 + \u03b2\n\u03b2 \u03bbn \u03ba \u03a8(Er) . (99)\nProof: By the RSC property in Er, we have for any \u2206 \u2208 Er\nL(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217) \u2265 \u3008\u2207L(\u03b8\u2217),\u2206\u3009+ \u03ba\u2016\u2206\u201622 . (100)\nBy definition of a dual norm, we have\n|\u3008\u2207L(\u03b8\u2217),\u2206\u3009| \u2264 R\u2217(\u2207L(\u03b8\u2217))R(\u2206) . (101)\nFurther, by construction, R\u2217(\u2207L(\u03b8\u2217)) \u2264 \u03bbn\u03b2 , implying\n|\u3008\u2207L(\u03b8\u2217),\u2206\u3009| \u2264 \u03bbn \u03b2 R(\u2206)\n\u21d2 \u3008\u2207L(\u03b8\u2217),\u2206\u3009 \u2265 \u2212\u03bbn \u03b2 R(\u2206) . (102)\nFurther, from triangle inequality, we have\nR(\u03b8\u2217 + \u2206)\u2212R(\u03b8\u2217) \u2265 \u2212R(\u2206) (103)\nAdding (102) and (103), we have\nF(\u2206) = L(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217) + \u03bbn(R(\u03b8\u2217 + \u2206)\u2212R(\u03b8\u2217)) \u2265 \u2212 \u03bbn \u03b2 R(\u2206) + \u03ba\u2016\u2206\u201622 \u2212 \u03bbnR(\u2206)\n= \u03ba\u2016\u2206\u201622 \u2212 \u03bbn 1 + \u03b2\n\u03b2 R(\u2206) . (104)\nBy definition of the norm compatibility constant \u03c8Er , we have R(\u2206) \u2264 \u2016\u2206\u20162\u03c8Er implying \u2212R(\u2206) \u2265 \u2212\u2016\u2206\u20162\u03c8Er . Plugging the inequality back into (104), we have\nF(\u2206) \u2265 \u03ba\u2016\u2206\u20162 { \u2016\u2206\u20162 \u2212 1 + \u03b2\n\u03b2 \u03bbn \u03ba \u03c8Er\n} . (105)\nSince F(\u2206\u0302n) \u2264 0, we have\n\u2016\u2206\u0302n\u20162 \u2264 1 + \u03b2\n\u03b2 \u03bbn \u03ba \u03c8Er , (106)\nwhich completes the proof."}, {"heading": "C Bounds on the Regularization Parameter", "text": "In this section, we prove Theorem 3 and 4 in Section 3 of the paper. The regularization parameter should satisfy the condition \u03bbn \u2265 \u03b2R\u2217(\u2207L(\u03b8\u2217;Zn)). In Theorem 3 we establish the upper bound on the expectation E[R\u2217(\u2207L(\u03b8\u2217;Zn))] in terms of the Gaussian width of the unit norm ball for least squares loss and Gaussian designs. In Theorem 4 we show that R\u2217(\u2207L(\u03b8\u2217;Zn)) concentrates sharply around its expectation."}, {"heading": "C.1 Proof of Theorem 3", "text": "To prove Theorem 3, we first need the following theorem from generic chaining.\nTheorem 8 Let \u2126R = {u : R(u) \u2264 1} be the unit norm ball of R(\u00b7). Assuming h is any centered subGaussian random vector with |||h|||\u03c82 \u2264 \u03ba, then we have\nE [ sup\nR(u)\u22641 \u3008h, u\u3009\n] \u2264 \u03b70\u03baw (\u2126R) ,\nwhere \u03b70 is a universal constant.\nProof: The quantity E[supR(u)\u22641\u3008h, u\u3009] can be considered the \u201csub-Gaussian width\u201d of \u2126R, the unit norm ball, since it has the exact same form as the Gaussian width, with h being a sub-Gaussian vector instead of a Gaussian vector. Next, we show that the sub-Gaussian width is always bounded by the Gaussian width times a factor proportional to \u03ba.\nConsider the sub-Gaussian process Y = {Yu}, Yu = \u3008u, h\u3009 indexed by u \u2208 \u2126R, the unit norm ball. Consider the Gaussian process X = {Xu}, Xu = \u3008u, g\u3009, where g \u223c N(0, I), indexed by the same set, i.e., u \u2208 \u2126R, the unit norm ball. First, note that |Yu \u2212 Yv| = |\u3008h, u \u2212 v\u3009|, so that by the concentration of sub-Gaussian random variable [41, Equation 5.10], we have\nP (|Yu \u2212 Yv| \u2265 ) \u2264 e \u00b7 exp ( \u2212 c 2\n\u03ba2\u2016u\u2212 v\u20162\n) , (107)\nwhere c > 0 is an absolute constant. As a result, a direct application of the generic chaining argument for upper bounds on such empirical processes [37, Theorem 2.1.5] gives\nE [ sup u,v |Yu \u2212 Yv| ] \u2264 \u03b71E [ sup u Xu ] = \u03b71w(\u2126R) , (108)\nwhere \u03b71 is an absolute constant. Further, since {Yu} is a symmetric process, from [37, Lemma 1.2.8], we have\nE [ sup u,v |Yu \u2212 Yv| ] = 2E [ sup u Yu ] . (109)\nAs a result, with \u03b70 = \u03b71/2, we have\nE [ sup\nR(u)\u22641 \u3008h, u\u3009\n] = E [ sup u Yu ] \u2264 \u03b70w(\u2126R) . (110)\nThat completes the proof.\nNow we turn to the proof of Theorem 3. Theorem 3 Let \u2126R = {u : R(u) \u2264 1}, and L be the squared loss. For sub-Gaussian design X and noise \u03c9, we have\nE [R\u2217(\u2207L(\u03b8\u2217;Zn))] \u2264 \u03b7\u03be \u00b7 \u03baw(\u2126R)\u221a n , (111)\nwhere the expectation is taken over both X and \u03c9. The constant \u03be is given by\n\u03be =\n{ 1 if X is isotropic\u221a\n\u039bmax(\u03a3) if X is anisotropic .\nProof: For least squares loss, we first note that E [R\u2217(\u2207L(\u03b8\u2217;Zn))] = E [ R\u2217( 1\nn XT\u03c9)\n] = E [ sup\nR(u)\u22641\n\u2329 1 n XT\u03c9, u\n\u232a]\n= E\n[ 1\nn \u2016\u03c9\u20162 \u00b7 E\n[ sup\nR(u)\u22641\n\u2329 XT \u03c9 \u2016\u03c9\u20162 , u \u232a\u2223\u2223\u2223\u03c9]]\n\u2264 E\n[ 1\nn \u2016\u03c9\u20162 \u00b7 sup v\u2208Sn\u22121 E\n[ sup\nR(u)\u22641\n\u2329 XT v, u\n\u232a]]\n= 1\nn E [\u2016\u03c9\u20162] \u00b7 sup v\u2208Sn\u22121 E\n[ sup\nR(u)\u22641\n\u2329 XT v, u \u232a] .\n(112)\nE[\u2016\u03c9\u20162] is the expected length of a centered sub-Gaussian random vector, which can be easily bounded using Jensen\u2019s inequality,\nE[\u2016\u03c9\u20162] < \u221a E[\u2016\u03c9\u201622] = \u221a n . (113)\nThen we focus on E [ sup\nR(u)\u22641 \u3008XT v, u\u3009\n] for any fixed v \u2208 Sn\u22121. Let h = XT v be a random vector, and\nconsider the random variable \u3008h, z\u3009 for any fixed z \u2208 Sp\u22121. Note that\n\u3008h, z\u3009 = \u3008v,Xz\u3009 .\nCase 1. IfX is independent isotropic, thenXz has i.i.d. centered sub-Gaussian entries with \u03c82 norm at most \u03ba. By Lemma 4, we know that \u3008v,Xz\u3009 is sub-Gaussian with |||\u3008v,Xz\u3009|||\u03c82 \u2264 C\u03ba, where C is an absolute constant. Hence h is a sub-Gaussian random vector with |||h|||\u03c82 \u2264 C\u03ba. Using Theorem 8, we conclude that for any v \u2208 Sn\u22121\nE [ sup\nR(u)\u22641\n\u2329 XT v, u \u232a] \u2264 \u03b70C\u03ba \u00b7 w(\u2126R) . (114)\nCase 2. If X is independent anisotropic, then Xz has i.i.d. centered sub-Gaussian entries with \u03c82 norm at most \u03ba \u221a \u039bmax(\u03a3), where \u039bmax(\u03a3) is the largest eigenvalue of \u03a3. By the same argument as Case 1, we have\nE [ sup\nR(u)\u22641\n\u2329 XT v, u \u232a] \u2264 \u03b70C\u03ba \u221a \u039bmax(\u03a3) \u00b7 w(\u2126R) . (115)\nLetting \u03b7 = \u03b70C and combining (114), (113) and (115), we complete the proof."}, {"heading": "C.2 Proof of Theorem 4", "text": "To prove Theorem 4, we also need the following result from generic chaining.\nTheorem 9 Let \u2126R = {u : R(u) \u2264 1} be the unit norm ball of R(\u00b7). Assuming h is any centered subGaussian random vector with |||h|||\u03c82 \u2264 \u03ba, then we have for any \u03c4 > 0,\nP ( sup\nR(u)\u22641 \u3008h, u\u3009 \u2265 \u03bd0\u03baw(\u2126R) + \u03c4\n) \u2264 \u03bd1 exp ( \u2212 ( \u03c4\n\u03bd2\u03ba\u03c6\n)2) , (116)\nwhere \u03bd0, \u03bd1 and \u03bd2 are universal constants, and \u03c6 = supR(u)\u22641 \u2016u\u20162.\nProof: Consider the sub-Gaussian process Y = {Yu}, Yu = \u3008h, u\u3009 indexed by u \u2208 \u2126R. By the same argument in the proof of Theorem 8, we have\nP (|Yu \u2212 Yv| \u2265 ) \u2264 e \u00b7 exp ( \u2212 c 2\n\u03ba2\u2016u\u2212 v\u20162\n) , (117)\nand\nE [ sup u,v |Yu \u2212 Yv| ] = 2E [ sup u Yu ] . (118)\nThen a direct application of [37, Theorem 2.1.5] and [38, Theorem 2.2.27] gives us (116).\nTheorem 4 Let designX and noise \u03c9 be sub-Gaussian, and L be squared loss. Define \u03c6 = supR(u)\u22641 \u2016u\u20162, then for any \u03c4 > 0, with probability at least 1\u2212 c1 exp ( \u2212min ( ( \u03c4c2\u03be\u03ba\u03c6) 2, c0n )) , we have\nR\u2217 (\u2207L(\u03b8\u2217;Zn)) \u2264 \u221a 2K2 + 1\nn (c\u03be\u03ba \u00b7 w(\u2126R) + \u03c4) , (119)\nwhere c, c0, c1 and c2 are all absolute constants, and \u03be is the same as in Theorem 3. Proof: We only show the case for isotropic X , where \u03be = 1. Note that\nP ( R\u2217 (\u2207L(\u03b8\u2217;Zn)) \u2265 \u221a 2K2 + 1\nn (c\u03be\u03ba \u00b7 w(\u2126R) + \u03c4)\n)\n= P ( \u2016\u03c9\u20162 \u00b7R\u2217 ( XT\u03c9\n\u2016\u03c9\u20162\n) \u2265 \u221a (2K2 + 1)n (c\u03ba \u00b7 w(\u2126R) + \u03c4) )\n\u2264 P ( \u2016\u03c9\u20162 > \u221a (2K2 + 1)n ) + sup v\u2208Sn\u22121 P ( R\u2217 ( XT v ) \u2265 c\u03ba \u00b7 w(\u2126R) + \u03c4 ) ,\n(120)\nwhere the last inequality uses the union bound. We first prove the bound for \u2016\u03c9\u20162. Since \u03c9 consists of i.i.d. centered unit-variance sub-Gaussian elements with |||\u03c9i|||\u03c82 < K, \u03c9 2 i is sub-exponential with |||\u03c9i|||\u03c81 <\n2K2. By applying Bernstein\u2019s inequality to \u2016\u03c9\u201622 = \u2211n i=1 \u03c9 2 i , we obtain\nP (\u2223\u2223\u2223\u2016\u03c9\u201622 \u2212 E[\u2016\u03c9\u201622]\u2223\u2223\u2223 \u2265 \u03c4) \u2264 2 exp [\u2212c0 min( \u03c424K4n, \u03c42K2 )] ,\nwhere c0 is an absolute constant. Setting \u03c4 = 2K2n and using (113), we get P ( \u2016\u03c9\u20162 \u2265 \u221a (2K2 + 1)n ) \u2264 2 exp(\u2212c0n) (121)\nNext we bound R\u2217(XT v) for any v \u2208 Sn\u22121. Given any fixed v \u2208 Sn\u22121, we note that\nR\u2217(XT v) = sup R(u)\u22641 \u3008XT v, u\u3009 ,\nand XT v is a sub-Gaussian random vector with \u2223\u2223\u2223\u2223\u2223\u2223XT v\u2223\u2223\u2223\u2223\u2223\u2223\n\u03c82 \u2264 C\u03ba as shown in the proof of Theorem 3.\nUsing Theorem 9, we have\nP ( R\u2217(XT v) \u2265 \u03bd0C\u03ba \u00b7 w(\u2126R) + \u03c4 ) \u2264 \u03bd1 exp ( \u2212 ( \u03c4\n\u03bd2C\u03ba\u03c6\n)2) . (122)\nLetting c = \u03bd0C, c1 = \u03bd1 + 2 and c2 = \u03bd2C, and combining (121) and (122), we complete the proof."}, {"heading": "C.3 Proof of Lemma 3", "text": "Lemma 3 Let \u2126R = {u : R(u) \u2264 1} be the unit norm ball and \u0398R = {u : R(u) = 1} be the boundary. For any \u03b8\u0303 \u2208 \u0398R define \u03c1(\u03b8\u0303) = sup\u03b8:R(\u03b8)\u22641 \u2016\u03b8 \u2212 \u03b8\u0303\u20162 is the diameter of \u2126R measured with respect to \u03b8\u0303. If G(\u03b8\u0303) = cone(\u2126R \u2212 \u03b8\u0303) \u2229 \u03c1(\u03b8\u0303)Bp2 , i.e., the cone of (\u2126R \u2212 \u03b8\u0303) intersecting the ball of radius \u03c1(\u03b8\u0303). Then\nw(\u2126R) \u2264 inf \u03b8\u0303\u2208\u0398R w(G(\u03b8\u0303)) (123)\nProof: For any \u03b8\u0303 \u2208 \u0398R, consider the set FR(\u03b8\u0303) = \u2126R \u2212 \u03b8\u0303 = {u : R(u + \u03b8\u0303) \u2264 1}. Since Gaussian width is translation invariant, the Gaussian width of \u2126R and FR are the same, i.e., w(\u2126R) = w(FR(\u03b8\u0303)). Since, \u03c1(\u03b8\u0303) = sup\u03b8:R(\u03b8)\u22641 \u2016\u03b8\u2212 \u03b8\u0303\u20162 is the diameter of \u2126R as well as FR(\u03b8\u0303), a ball of radius \u03c1(\u03b8\u0303) will include FR(\u03b8\u0303), so that FR(\u03b8\u0303) \u2286 \u03c1(\u03b8\u0303)Bp2 . Further, by definition, FR(\u03b8\u0303) \u2286 cone(FR(\u03b8\u0303)) = cone(\u2126R \u2212 \u03b8\u0303). Let G(\u03b8\u0303) = cone(\u2126R \u2212 \u03b8\u0303) \u2229 \u03c1(\u03b8\u0303)Bp2 . By construction, FR(\u03b8\u0303) \u2286 G(\u03b8\u0303). Then,\nw(\u2126R) = w(FR(\u03b8\u0303)) \u2264 w(G(\u03b8\u0303)) .\nNoting the analysis holds for any \u03b8\u0303 \u2208 \u0398R, completes the proof."}, {"heading": "D Restricted Eigenvalue Conditions: Sub-Gaussian Designs", "text": "We focus on results in Section 4. In particular we consider RE conditions for sub-Gaussian design matrices for three different cases: (i) the design matrix has independent sub-Gaussian rows Xi with |||Xi|||\u03c82 \u2264 \u03ba, (ii) the design matrix has independent rows with subGaussian elements xij so that |||xij |||\u03c82 \u2264 \u03ba and the\ncolumns are correlated, and (iii) the columns are independent subGaussian but the rows are correlated, i.e., correlated samples. One can view (ii) as a special case of (i), but we highlight this special case because of its practical importance and past literature on RE conditions for anisotropic subGaussian designs [33, 34].\nOur results simply use a general treatment developed in [28], building on [21], based on Talagrand\u2019s generic chaining [37, 38]. We specifically focus on results in [28] which provide uniform bounds on the supremum of certain empirical processes. RE results for the specific cases of interest in the current paper will then be established by suitable choices of these empirical processes. The results in [28], and more generally in generic chaining [37, 38], are based on certain \u03b3-functionals which we briefly introduce below.\nConsider a metric space (T, d) and for a finite set A \u2282 T , let |A| denote its cardinality. An admissible sequence is an increasing sequence of subsets {An, n \u2265 0} of T , such that |A0| = 1 and for n \u2265 1, |An| = 22 n . Given \u03b1 > 0, we define the \u03b3\u03b1-functional as\n\u03b3\u03b1(T, d) = inf sup t\u2208T \u221e\u2211 n=0 Diam(An(t)) , (124)\nwhere An(t) is the unique element ofAn that contains t, Diam(An(t)) is the diameter of An according to d, and the infimum is over all admissible sequences of T . To get the desired RIP results in terms of Gaussian widths, we start with the following key result, originally [28, Theorem D].\nTheorem 10 (Mendelson, Pajor, Tomczak-Jaegermann [28]) There exist absolute constants c1, c2, c3 for which the following holds. Let (\u2126, \u00b5) be a probability space, set F be a subset of the unit sphere of L2(\u00b5), i.e., F \u2286 SL2 = {f : |||f |||L2 = 1}, and assume that supf\u2208F |||f |||\u03c82 \u2264 \u03ba. Then, for any \u03b8 > 0 and n \u2265 1 satisfying\nc1\u03ba\u03b32(F, |||\u00b7|||\u03c82) \u2264 \u03b8 \u221a n , (125)\nwith probability at least 1\u2212 exp(\u2212c2\u03b82n/\u03ba4),\nsup f\u2208F \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 f2(Xi)\u2212 E [ f2 ]\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b8 . (126)"}, {"heading": "Further, if F is symmetric, then", "text": "E [ sup f\u2208F \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 f2(Xi)\u2212 E [ f2 ]\u2223\u2223\u2223\u2223\u2223 ] \u2264 c3 max { 2\u03ba \u03b32(F, |||\u00b7|||\u03c82)\u221a n , \u03b322(F, |||\u00b7|||\u03c82) n } (127)\nWe use the above result and related arguments to establish RIP conditions for the cases of interest.\nD.1 Isotropic Sub-Gaussian Designs\nWe consider the case where the design matrix X \u2208 Rn\u00d7p has independent subGaussian rows where each row satisfies |||Xi|||\u03c82 \u2264 \u03ba and E[XiX T i ] = Ip\u00d7p. Thus, the measure \u00b5 from which the rows Xi are sampled independently is an isotropic sub-Gaussian measure.\nTheorem 11 Let X be a design matrix with independent isotropic subGaussian rows, i.e., |||Xi|||\u03c82 \u2264 \u03ba and E[XiX T i ] = I. Then, for absolute constants \u03b7, c > 0, with probability at least (1 \u2212 2 exp(\u2212\u03b7w2(A))), we have\nsup u\u2208A \u2223\u2223\u2223\u2223 1n ||Xu||2 \u2212 1 \u2223\u2223\u2223\u2223 = sup u\u2208A \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 \u3008Xi, u\u30092 \u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2264 cw(A)\u221an , (128)\nor, equivalently,\n1\u2212 cw(A)\u221a n \u2264 inf u\u2208A 1 n ||Xu||2 \u2264 sup\nu\u2208A\n1 n ||Xu||2 \u2264 1 + cw(A)\u221a n . (129)\nProof: The result essentially follows from an application of Theorem 10. For convenience of notation, let X0 be i.i.d. as the rows Xi, i = 1, . . . , n, thus distributed following \u00b5. To apply Theorem 10, we choose any A \u2286 Sp\u22121 consider the following class of functions: F = {\u3008\u00b7, u\u3009 : u \u2208 A}. Then, f(X0) = \u3008X0, u\u3009 and F is a subset of the unit sphere, i.e., F \u2286 SL2 , since |||f |||L2 = E[u\nTXT0 X0u] = \u2016u\u20162 = 1. Further, supf\u2208F |||f |||\u03c82 = supu\u2208A |||\u3008X0, u\u3009|||\u03c82 \u2264 |||X0|||\u03c82 \u2264 \u03ba/2. Next, we show that for the current setting, the \u03b32-functional can be upper bounded by w(A), the Gaussian width of A. Since \u00b5 is isotropic subGaussian with \u03c82-norm bounded by \u03ba, we have\n\u03b32(F \u2229 SL2 , |||\u00b7|||\u03c82) \u2264 \u03ba\u03b32(F \u2229 SL2 , |||\u00b7|||L2) \u2264 \u03bac4w(A) , (130)\nwhere the last inequality follows from generic chaining, in particular [37, Theorem 2.1.1], for an absolute constant c4 > 0.\nIn the context of Theorem 10, we choose\n\u03b8 = c1c4\u03ba 2w(A)\u221a\nn \u2265 c1\u03ba \u03b32(F \u2229 SL2 , |||\u00b7|||\u03c82)\u221a n ,\nso that the condition on \u03b8 is satisfied. With this choice of \u03b8, we have\n\u03b82n/\u03ba4 = c21c 2 4w 2(A) .\nThen, from Theorem 10, it follows that with probability at least 1\u2212 exp(\u2212\u03b7w2(A)), we have\nsup u\u2208A \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 \u3008Xi, u\u30092 \u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2264 cw(A)\u221an , (131) where \u03b7 = c2c21c 2 4 and c = c1c2\u03ba 2 are absolute constants. As a result, we have\nsup u\u2208A\n( 1\nn n\u2211 i=1 \u3008Xi, u\u30092 \u2212 1\n) \u2264 cw(A)\u221a\nn , and sup\nu\u2208A\n( 1\u2212 1\nn n\u2211 i=1\n\u3008Xi, u\u30092 ) \u2264 cw(A)\u221a\nn ,\nyielding\n1\u2212 cw(A)\u221a n \u2264 inf u\u2208A 1 n ||Xu||2 \u2264 sup\nu\u2208A\n1 n ||Xu||2 \u2264 1 + cw(A)\u221a n . (132)\nThat completes the proof."}, {"heading": "D.2 Anisotropic Sub-Gaussian Designs", "text": "We now consider the case where the design matrix X \u2208 Rn\u00d7p has independent rows, and each row Xi is anisotropic subGaussian with E[XTi Xi] = \u03a3. Further, we assume that corresponding isotropic random\nvector X\u0303i = Xi\u03a3\u22121/2 satisfies \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223X\u0303i\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u03c82 \u2264 \u03ba. A simple special case of such an anisotropic subGaussian design is when Xi \u223c N(0,\u03a3), where X\u0303i = Xi\u03a3\u22121/2 \u223c N(0, I) so that \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223X\u0303i\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u03c82 = 1. The result below\ncharacterizes RIP-style property of any such anisotropic subGaussian designs.\nTheorem 12 LetX be a design matrix with independent anisotropic subGaussian rows, i.e., E[XTi Xi] = \u03a3 and \u2223\u2223\u2223\u2223\u2223\u2223Xi\u03a3\u22121/2\u2223\u2223\u2223\u2223\u2223\u2223\u03c82 \u2264 \u03ba. Then, for absolute constants \u03b7, c > 0, with probability at least (1\u22122 exp(\u2212\u03b7w2(A))), we have\nsup u\u2208A \u2223\u2223\u2223\u2223 1n 1uT\u03a3u ||Xu||2 \u2212 1 \u2223\u2223\u2223\u2223 = sup u\u2208A \u2223\u2223\u2223\u2223\u2223 1n 1uT\u03a3u n\u2211 i=1 \u3008Xi, u\u30092 \u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2264 cw(A)\u221an . (133) Further,\n\u03bbmin(\u03a3|A) (\n1\u2212 cw(A)\u221a n\n) \u2264 inf\nu\u2208A\n1 n ||Xu||2 \u2264 sup\nu\u2208A\n1 n ||Xu||2 \u2264 \u03bbmax(\u03a3|A)\n( 1 + c\nw(A)\u221a n\n) ,\n(134) where\n\u03bbmin(\u03a3|A) = inf u\u2208A uT\u03a3u , and \u03bbmax(\u03a3|A) = sup u\u2208A uT\u03a3u (135)\nare the restricted minimum and maximum eigenvalues of \u03a3 restricted to A \u2286 Sp\u22121.\nProof: The result also follows from an application of Theorem 10. For convenience of notation, let X0 be i.i.d. as the rows Xi, i = 1, . . . , n, thus distributed following \u00b5. To apply Theorem 10, we choose any A \u2286 Sp\u22121 consider the following class of functions:\nF = {fu, u \u2208 A : fu(\u00b7) = 1\u221a uT\u03a3u \u3008\u00b7, u\u3009} . (136)\nThen, fu(X0) = 1\u221a uT \u03a3u \u3008X0, u\u3009 and F is a subset of the unit sphere, i.e., F \u2286 SL2 , since for fu \u2208 F\n|||fu|||2L2 = 1 uT\u03a3u E[uTXT0 X0u] = 1 .\nNext, we focus on getting an upper bound on supfu\u2208F |||fu|||\u03c82 = supu\u2208A \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 1\u221a uT \u03a3u \u3008X0, u\u3009 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u03c82 . Let X\u03030 = X0\u03a3 \u22121/2 so that X\u03030 is a isotropic vector with \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223X\u03030\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u03c82 \u2264 \u03ba. Noting that\n\u3008X0, u\u3009 = \u3008X\u03030,\u03a31/2u\u3009 = \u221a uT\u03a3u\u3008X\u03030, \u03a31/2u\n\u2016\u03a31/2u\u20162 \u3009 ,\nwe have\nsup u |||fu|||\u03c82 = sup u\u2208A \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 1\u221a uT\u03a3u \u3008X0, u\u3009 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u03c82 = sup u\u2208A \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u3008X\u03030, \u03a31/2u\u2016\u03a31/2u\u20162 \u3009 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 \u03c82 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223X\u03030\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u03c82 \u2264 \u03ba .\nAs a result, we have\n\u03b32(F \u2229 SL2 , |||\u00b7|||\u03c82) \u2264 \u03ba \u03b32(F \u2229 SL2 , |||\u00b7|||L2) \u2264 \u03bac4w(A) , (137)\nwhere the last inequality follows from [37, Theorem 2.1.1], for an absolute constant c4 > 0.\nIn the context of Theorem 10, we choose\n\u03b8 = c1c4\u03ba 2w(A)\u221a\nn \u2265 c1\u03ba \u03b32(F \u2229 SL2 , |||\u00b7|||\u03c82)\u221a n ,\nso that the lower bound condition on \u03b8 is satisfied. With this choice of \u03b8, we have\n\u03b82n/\u03ba4 = c21c 2 4w 2(A) .\nThen, from Theorem 10, it follows that with probability at least 1\u2212 exp(\u2212\u03b7w2(A)), we have\nsup u\u2208A \u2223\u2223\u2223\u2223\u2223 1n 1uT\u03a3u n\u2211 i=1 \u3008Xi, u\u30092 \u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2264 cw(A)\u221an , (138) where \u03b7 = c2c21c 2 4 and c = c1c2\u03ba 2 are absolute constants. As a result, we have\nsup u\u2208A\n( 1\nn\n1\nuT\u03a3u n\u2211 i=1 \u3008Xi, u\u30092 \u2212 1\n) \u2264 cw(A)\u221a\nn , (139)\nsup u\u2208A\n( 1\u2212 1\nn\n1\nuT\u03a3u n\u2211 i=1\n\u3008Xi, u\u30092 ) \u2264 cw(A)\u221a\nn . (140)\nFrom (139), we have\n1\n\u03bbmax(\u03a3|A) sup u\u2208A\n1 n \u2016Xu\u20162 \u2264 sup\nu\u2208A\n1\nn\n1 uT\u03a3u \u2016Xu\u20162 \u2264 1 + cw(A)\u221a n ,\nso that\nsup u\u2208A\n1 n \u2016Xu\u20162 \u2264 \u03bbmax(\u03a3|A) + c\u03bbmax(\u03a3|A) w(A)\u221a n . (141)\nSimilarly, from (140), we have\n1\u2212 cw(A)\u221a n \u2264 inf u\u2208A 1 n 1 uT\u03a3u \u2016Xu\u20162 \u2264 1 \u03bbmin(\u03a3|A) inf u\u2208A 1 n \u2016Xu\u20162 ,\nimplying\n\u03bbmin(\u03a3|A)\u2212 c\u03bbmin(\u03a3|A) w(A)\u221a n \u2264 inf u\u2208A 1 n \u2016Xu\u20162 . (142)\nPutting (141) and (142) together completes the proof."}, {"heading": "E Generalized Linear Models: Restricted Strong Convexity", "text": "We establish bounds on the regularization parameter and RSC condition for GLMs as discussed in Section 6, along with a few specific examples."}, {"heading": "E.1 Generalized Linear Models", "text": "Loss functions for GLMs are derived as maximum likelihood estimators for the family of exponential distributions. The canonical density function of exponential family distributions is given by [4, 8, 44]:\nP (y|\u03b7) \u221d exp{\u03b7y \u2212 \u03d5(\u03b7)} , (143)\nwhere \u03b7 is the natural parameter and has a one-to-one function mapping with the mean parameter \u00b5 = E[y] of the distribution, \u03d5(\u03b7) is the log-partition function which ensures that P (y|\u03b7) remains a probability distribution. The gradient of the log-partition function is the response function, i.e., g(\u00b7) = \u03d5\u2032(\u00b7), which is monotonic by construction. The inverse of the response function is the so-called link function h(\u00b7) = g\u22121(\u00b7).\nThe mean of the distribution can be obtained from the gradient of the log-partition function at the natural parameter, i.e.,\n\u00b5 = \u03d5 \u2032 (\u03b7) = g(\u03b7) . (144)\nThe interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44]. Examples of distributions from the exponential family include the Gaussian, multinomial, exponential, Dirichlet, Poisson, Gamma, etc.\nGLMs are obtained from conditional exponential family distributions by assuming a suitable parametric form of the natural parameter \u03b7 in terms of X and \u03b8\u2217, in particular \u03b7i = \u3008Xi, \u03b8\u2217\u3009. Then, the conditional distribution is given by\nP (yi|Xi, \u03b8\u2217) \u221d exp{\u03b7iyi \u2212 \u03d5(\u03b7i)} = exp{\u3008Xi, \u03b8\u2217\u3009yi \u2212 \u03d5(\u3008Xi, \u03b8\u2217\u3009)} . (145)\nThe loss function for GLMs simply consider the negative log likelihood of such conditional exponential family forms. Assuming samples to be independent, we have\nL(\u03b8;Zn) = \u2212 1 n n\u2211 i=1 {\u03b7iyi \u2212 \u03d5(\u03b7i)} = \u2212 1 n n\u2211 i=1 {\u3008Xiyi, \u03b8\u3009 \u2212 \u03d5(\u3008Xi, \u03b8\u3009)} . (146)\nUsing chain rule, the first derivative of the loss function evaluated at \u03b8\u2217 is\n\u2207\u03b8L(\u03b8\u2217;Zn) = \u2212 1\nn n\u2211 i=1 yiXi + 1 n n\u2211 i=1 Xi \u2202\u03d5(\u3008\u03b8\u2217, Xi\u3009) \u2202\u03b7i = 1 n n\u2211 i=1 Xi(E[yi|Xi]\u2212 yi) = 1 n XT\u03c9 ,\nwhere each element of \u03c9 \u2208 Rn is given as \u03c9i = E(y|Xi) \u2212 yi. Next we look at some specific examples of exponential families and corresponding GLMs.\n1. Gaussian distribution: If the variance of the Gaussian distribution P (y|Xi) is assumed to be 1, then we have\nP (yi|Xi; \u03b8\u2217) \u221d exp { yi\u3008Xi, \u03b8\u2217\u3009 \u2212 \u3008Xi, \u03b8\u2217\u30092\n2\n} .\nComparing it with the canonical form given earlier, the natural parameter is \u03b7i = \u3008Xi, \u03b8\u2217\u3009, log-partition function is \u03d5(\u3008Xi, \u03b8\u2217\u3009) = \u3008Xi,\u03b8 \u2217\u30092 2 and hence E[y|Xi] = \u03d5 \u2032 (\u3008Xi, \u03b8\u2217\u3009) = \u3008Xi, \u03b8\u2217\u3009. The noise \u03c9i = yi \u2212 E(yi|Xi) is Gaussian. Considering the negative log-likelihood, the GLM corresponding to the Gaussian distribution yields least squares regression [8].\n2. Bernoulli distribution: Assuming the conditional distribution of yi|Xi, \u03b8\u2217 to have a Bernoulli distribution with conditional mean parameter pi, which is a suitable function of \u3008Xi, \u03b8\u2217\u3009, the likelihood of the observations is given by\nP (yi|pi) = pyii (1\u2212 pi) (1\u2212yi) = exp(yi log pi + (1\u2212 yi) log(1\u2212 pi)) = exp ( yi log ( pi\n1\u2212 pi\n) + log(1\u2212 pi) ) Therefore, the natural parameter \u03b7i = \u3008Xi, \u03b8\u2217\u3009 = log ( pi\n1\u2212pi\n) giving pi =\nexp(\u3008Xi,\u03b8\u2217\u3009) 1+exp(\u3008Xi,\u03b8\u2217\u3009) . It can be\nverified from the fact that the probability density function P (yi|pi) adds to 1 and the log-partition function evaluates to \u03d5(\u03b7i) = log(1\u2212pi) = log(1+exp(\u3008Xi, \u03b8\u2217\u3009)). The noise in the model corresponds to random draws from a Bernoulli distribution, and each element of \u03c9 is \u03c9i = pi \u2212 yi = exp(\u3008Xi,\u03b8\n\u2217\u3009) 1+exp(\u3008Xi,\u03b8\u2217\u3009) \u2212 yi, which\nis bounded and hence sub-Gaussian. Considering the negative log-likelihood, the GLM corresponding to the Bernoulli distribution yields logistic regression [8].\n3. Poisson distribution: Assuming the conditional distribution of yi|Xi, \u03b8\u2217 to have a Poisson distribution with conditional mean parameter \u03bbi, which is a suitable function of \u3008Xi, \u03b8\u2217\u3009, the likelihood of the observations is given by\nP (yi|\u03bbi) = \u03bbyii exp(\u2212\u03bbi)\nyi! \u221d exp{log(\u03bbyii exp(\u2212\u03bbi))} = exp{yi log \u03bbi \u2212 \u03bbi} ,\nwhere the 1/yi! term constitutes the base measure for the distribution. Based on the form, the natural parameter \u03b7i = \u3008Xi, \u03b8\u2217\u3009 = log \u03bbi giving \u03bbi = exp(\u03b7i) = exp(\u3008Xi, \u03b8\u2217\u3009). Also it can be verified that the log-partition function \u03d5(\u03b7i) = exp(\u03b7i) = \u03bbi. Each element of \u03c9 is \u03c9i = \u03bbi \u2212 yi = exp(\u3008Xi, \u03b8\u2217\u3009) \u2212 yi. Considering the negative log-likelihood, the GLM corresponding to the Poisson distribution yields Poisson regression [8].\nAs discussed in Section 6, if the design matrix X and the noise is assumed to be sub-Gaussian, then the regularization parameter \u03bbn needs to be O(\u2126R\u221an), following the analysis and results in Section 3. In the rest of this section, we focus on proving the GLMs with sub-Gaussian designs satisfy the RSC condition with sample complexity depending on the width of the spherical cap corresponding to the error set, as discussed in Section 2."}, {"heading": "E.2 RSC condition for GLMs", "text": "For any convex loss function to satisfy the RSC condition on any A \u2286 Sp\u22121, the following inequality\n\u03b4L(\u03b8\u2217, u;Zn) = L(\u03b8\u2217 + u;Zn)\u2212 L(\u03b8\u2217;Zn)\u2212 \u3008\u2207L(\u03b8\u2217;Zn), u\u3009 \u2265 \u03ba\u2016u\u201622 (147)\nneeds to hold \u2200u \u2208 Ar. For the general formulation of GLM discussed earlier, we have\n\u03b4L(\u03b8\u2217, u;Zn) = \u2212\u3008\u03b8\u2217 + u, 1 n n\u2211 i=1 yiXi\u3009+ 1 n n\u2211 i=1 \u03d5(\u3008\u03b8\u2217 + u,Xi\u3009) + \u3008\u03b8\u2217, 1 n n\u2211 i=1 yiXi\u3009\n\u2212 1 n n\u2211 i=1 \u03d5(\u3008\u03b8\u2217, Xi\u3009)\u2212 \u3008\u2212 1 n n\u2211 i=1 yiXi + 1 n n\u2211 i=1 Xi\u03d5 \u2032 (\u3008\u03b8\u2217, xi\u3009), u\u3009 .\nSimplifying the expression and applying mean value theorem twice we get the following\n\u03b4L(\u03b8\u2217, u;Zn) = 1 n n\u2211 i=1 \u03d5 \u2032\u2032 (\u3008\u03b8\u2217, Xi\u3009+ \u03b3i\u3008u,Xi\u3009) \u3008u,Xi\u30092 , (148)\nfor suitable \u03b3i \u2208 [0, 1]. The RSC condition for GLMs then needs to consider lower bounds for\n\u03b4L(\u03b8\u2217, u;Zn) = 1 n n\u2211 i=1 \u03d5 \u2032\u2032 (\u3008\u03b8\u2217, Xi\u3009+ \u03b3i\u3008u,Xi\u3009)\u3008u,Xi\u30092 (149)\nwhere \u03b3i \u2208 [0, 1]. The second derivative of the log-partition function is always positive. Since the RSC condition relies on a non-trivial lower bound for the above quantity, the analysis will suitably consider a compact set where ` = `\u03d5(T ) = min|a|\u22642T \u03d5 \u2032\u2032 (a) is bounded away from zero. The only assumption outside this compact set {a : |a| \u2264 2T} is that the second derivative is greater than 0. Further, we assume \u2016\u03b8\u2217\u20162 \u2264 c1 for some constant c1. With these assumptions\n\u03b4L(\u03b8\u2217, u;Zn) \u2265 ` n n\u2211 i=1 \u3008Xi, u\u30092I[|\u3008Xi, \u03b8\u2217\u3009| < T ]I[|\u3008Xi, u\u3009| < T ] . (150)\nWe give a characterization of the RSC condition for isotropic sub-Gaussian design matrices X \u2208 Rn\u00d7p. We consider u \u2208 A \u2286 Sp\u22121 so that \u2016u\u20162 = 1. Further, we assume \u2016\u03b8\u2217\u20162 \u2264 c1 for some constant c1. Assuming X has sub-Gaussian rows with |||Xi|||\u03d52 \u2264 \u03ba, \u3008Xi, \u03b8\n\u2217\u3009 and \u3008Xi, u\u3009 are sub-Gaussian random variables with sub-Gaussian norm at most C\u03ba. Let \u03b51 and \u03b52 denote the tail probability that \u3008Xi, u\u3009 and \u3008Xi, \u03b8\u2217\u3009 exceeds some constant T , i.e., \u03b51(T ;u) = P{|\u3008Xi, u\u3009| > T} \u2264 e\u00b7exp(\u2212c2T 2/C2\u03ba2) = \u03b5\u03041, and \u03b52(T ; \u03b8\u2217) = P{|\u3008Xi, \u03b8\u2217\u3009| > T} \u2264 e\u00b7exp(\u2212c2T 2/C2\u03ba2) = \u03b5\u03042, where \u03b5\u03041 = \u03b5\u03041(T, \u03ba) and \u03b5\u03042 = \u03b5\u03042(T ;\u03ba) are uniform upper bounds on the individual tail probabilities. The result we present below is in terms of the above defined constants ` = `\u03d5(T ), \u03b5\u03041 = \u03b5\u03041(T, \u03ba) and \u03b5\u03042 = \u03b5\u03042(T, \u03ba) for any suitably chosen T . Theorem 7 Let X \u2208 Rn\u00d7p be a design matrix with independent isotropic sub-Gaussian rows such that |||Xi|||\u03d52 \u2264 \u03ba. Then, for any set A \u2286 S\np\u22121 for suitable constants \u03b7, c > 0, with probability at least 1\u2212 2 exp ( \u2212\u03b7w2(A) ) , we have\ninf u\u2208A\n\u2202L(\u03b8\u2217;u,X) \u2265 `\u03c12 (\n1\u2212 c\u03ba21 w(A)\u221a n\n) . (151)\nwhere \u03c12 = infu\u2208A \u03c12u, with \u03c1 2 u = E[\u3008Xi, u\u30092I[|\u3008Xi, \u03b8\u2217\u3009| < T ]I[|\u3008Xi, u\u3009| < T ]], and \u03ba1 = \u03ba(1\u2212\u03b5\u03041\u2212\u03b5\u03042)2 . Proof: For any fixed T , let Z\u0304i = Z\u0304ui = \u3008Xi, u\u3009I(|\u3008Xi, u\u3009| \u2264 T )I(|\u3008Xi, \u03b8\u2217\u3009| \u2264 T ). Then, the probability distribution over Z\u0304i can be written as:3\nP (Z\u0304i = z) = P (\u3008Xi, u\u3009 = z)I(|\u3008Xi, u\u3009| \u2264 T )I(|\u3008Xi, \u03b8\u2217\u3009| \u2264 T )\nP (|\u3008Xi, u\u3009| \u2264 T, |\u3008Xi, \u03b8\u2217\u3009| \u2264 T ) \u2264 1 1\u2212 \u03b5\u03041 \u2212 \u03b5\u03042 P (\u3008Xi, u\u3009 = z) . (152)\nAs a result, \u2223\u2223\u2223\u2223\u2223\u2223Z\u0304i\u2223\u2223\u2223\u2223\u2223\u2223\u03c82 \u2264 \u03ba1\u2212\u03b5\u03041\u2212\u03b5\u03042 = \u03ba1. Thus, Z\u0304i = Z\u0304ui is a sub-Gaussian random variable for any u \u2208 A. Let \u03c12u = E[(Z\u0304i u )2] > 0. Let X0 be i.i.d. as the rows Xi, i = 1, . . . , n. Let A \u2286 Sp\u22121 and consider the following class of functions: F = { 1\u03c1u \u3008\u00b7, u\u3009I(|\u3008\u00b7, u\u3009| \u2264 T )I(|\u3008\u00b7, \u03b8 \u2217\u3009| \u2264 T ) : u \u2208 A}. Then for any f \u2208 F , f(X0) = 1\u03c1u \u3008X0, u\u3009I(|\u3008X0, u\u3009| \u2264 T )I(|\u3008X0, \u03b8 \u2217\u3009| \u2264 T ) and, by construction, F is a subset of the unit sphere, i.e., F \u2286 SL2 . Further, supf\u2208F |||f |||\u03c82 \u2264 \u03ba1/2. Next, we show that for the current setting, the \u03b32-functional can be upper bounded by w(A), the Gaussian width of A. Since the process is sub-Gaussian with \u03d52-norm bounded by \u03ba1, we have\n\u03b32(F \u2229 SL2 , |||\u00b7|||\u03c82) \u2264 \u03ba1\u03b32(F \u2229 SL2 , |||\u00b7|||L2) \u2264 \u03ba1c4w(A) , (153)\nwhere the last inequality follows from generic chaining, in particular [37, Theorem 2.1.1], for an absolute constant c4 > 0.\nIn the context of Theorem 10, we choose\n\u03b8 = c1c4\u03ba 2 1 w(A)\u221a n \u2265 c1\u03ba1 \u03b32(F \u2229 SL2 , |||\u00b7|||\u03d52)\u221a n , (154)\nso that the condition on \u03b8 is satisfied. With this choice of \u03b8, we have\n\u03b82n/\u03ba41 = c 2 1c 2 4w 2(A) . (155)\nThen, from Theorem 10, it follows that with probability at least 1\u2212 exp(\u2212\u03b7w2(A)), we have\nsup u\u2208A \u2223\u2223\u2223\u2223\u2223 1\u03c1un n\u2211 i=1 \u3008Xi, u\u30092I(|\u3008X0, u\u3009| \u2264 T )I(|\u3008X0, \u03b8\u2217\u3009| \u2264 T )\u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2264 c\u03ba21w(A)\u221an (156) 3With abuse of notation, we treat the distribution over Z\u0304i as discrete for ease of notation. A similar argument applies for the true\ncontinuous distribution, but more notation is needed.\nwhere \u03b7 = c2c21c 2 4 and c = c1c2 are absolute constants. Thus, with probability at least 1\u2212 exp(\u2212\u03b7w2(A)),\ninf u\u2208A\n1\nn n\u2211 i=1 \u3008Xi, u\u30092I(|\u3008X0, u\u3009| \u2264 T )I(|\u3008X0, \u03b8\u2217\u3009| \u2264 T ) \u2265 inf u\u2208A \u03c12u ( 1\u2212 c\u03ba21 w(A)\u221a n ) . (157)\nDenoting \u03c12 = infu\u2208A \u03c12u, with probability at least 1\u2212 exp(\u2212\u03b7w2(A)), we have\ninf u\u2208A \u2202L(\u03b8\u2217;u,X) \u2265 inf u\u2208A\n`\nn n\u2211 i=1 \u3008Xi, u\u30092I[|\u3008Xi, \u03b8\u2217\u3009| < T ]I[|\u3008Xi, u\u3009| < T ] \u2265 `\u03c12 ( 1\u2212 c\u03ba21 w(A)\u221a n ) .\n(158) That completes the proof.\nAcknowledgements: We thank the reviewers of the conference version [2] for helpful comments and suggestions on related work. We thank Sergey Bobkov, Snigdhansu Chatterjee, and Pradeep Ravikumar for helpful discussions related to the paper. The research was supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A."}], "references": [{"title": "Living on the edge: Phase transitions in convex programs with random data", "author": ["D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp"], "venue": "Inform. Inference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Estimation with norm regularization", "author": ["A. Banerjee", "S. Chen", "F. Fazayeli", "V. Sivakumar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Clustering with Bregman Divergences", "author": ["A Banerjee", "S Merugu", "I Dhillon", "J Ghosh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Information and Exponential Families in Statistical Theory", "author": ["O Barndorff-Nielsen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1978}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Fundamentals of Statistical Exponential Families", "author": ["L Brown"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Statistics for High Dimensional Data: Methods, Theory and Applications", "author": ["P. Buhlmann", "S. van de Geer"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "The restricted isometry property and its implications for compressed sensing", "author": ["E. Candes"], "venue": "Comptes Rendus Mathematique,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["E. Candes", "T Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Candes", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Decoding by linear programming", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "The convex geometry of linear inverse problems", "author": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Generalized dantzig selector: Application to the k-support norm", "author": ["S. Chatterjee", "S. Chen", "A. Banerjee"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Structured estimation with atomic norms: General bounds and applications", "author": ["S. Chen", "A. Banerjee"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Struct. Algorithms,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Sparse estimation with strongly correlated variables using ordered weighted l1 regularization", "author": ["M.A.T. Figueiredo", "R.D. Nowak"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Some inequalities for gaussian processes and applications", "author": ["Y. Gordon"], "venue": "Israel Journal of Mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1985}, {"title": "On milmans inequality and random subspaces which escape through a mesh in rn", "author": ["Y. Gordon"], "venue": "In Geometric Aspects of Functional Analysis,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Empirical processes and random projections", "author": ["B. Klartag", "S. Mendelson"], "venue": "Journal of Functional Analysis,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Bounding the smallest singular value of a random matrix without concentration", "author": ["V. Koltchinskii", "S. Mendelson"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Sparse recovery under weak moment assumptions", "author": ["G. Lecu\u00e9", "S. Mendelson"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "author": ["M. Ledoux", "M. Talagrand"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning", "author": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Lasso-type recovery of sparse representations for high-dimensional data", "author": ["N. Meinshausen", "B. Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Learning Without Concentration", "author": ["S. Mendelson"], "venue": "In Journal of the ACM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Reconstruction and subGaussian operators in asymptotic geometric analysis", "author": ["S. Mendelson", "A. Pajor", "N. Tomczak-Jaegermann"], "venue": "Geometric and Functional Analysis,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "A unified framework for the analysis of regularized M -estimators", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Statistical Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "The lower tail of random quadratic forms, with applications to ordinary least squares and restricted eigenvalue properties", "author": ["R.I. Oliveira"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "The Squared-Error of Generalized Lasso: A Precise Analysis", "author": ["S. Oymak", "C. Thrampoulidis", "B. Hassibi"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach", "author": ["Y. Plan", "R. Vershynin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Restricted Eigenvalue Properties for Correlated Gaussian Designs", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Reconstruction from anisotropic random measurements", "author": ["Z. Rudelson", "S. Zhou"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Principles of Mathematical Analysis", "author": ["Walter Rudin"], "venue": "International Series in Pure & Applied Mathematics. McGraw-Hill,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1976}, {"title": "Beyond sub-gaussian measurements: High-dimensional structured estimation with sub-exponential designs", "author": ["V. Sivakumar", "A. Banerjee", "P. Ravikumar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "The Generic Chaining", "author": ["M. Talagrand"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Upper and Lower Bounds for Stochastic Processes", "author": ["M. Talagrand"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1996}, {"title": "Convex recovery of a structured signal from independent random linear measurements. In Sampling Theory, a Renaissance", "author": ["J.A. Tropp"], "venue": "(To Appear),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Compressed Sensing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Estimation in high dimensions: A geometric perspective", "author": ["R. Vershynin"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Sharp thresholds for noisy and high-dimensional recovery of sparsity using `1constrained quadratic programming(Lasso)", "author": ["M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M J Wainwright", "M I Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2006}, {"title": "Restricted eigenvalue conditions on subgaussian random matrices", "author": ["S. Zhou"], "venue": "Technical report, Department of Mathematics, ETH Zurich, December", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}], "referenceMentions": [{"referenceID": 38, "context": "Such estimators are usually of the form [39, 29, 9]: \u03b8\u0302\u03bbn = argmin \u03b8\u2208Rp L(\u03b8;Z) + \u03bbnR(\u03b8) , (1)", "startOffset": 40, "endOffset": 51}, {"referenceID": 28, "context": "Such estimators are usually of the form [39, 29, 9]: \u03b8\u0302\u03bbn = argmin \u03b8\u2208Rp L(\u03b8;Z) + \u03bbnR(\u03b8) , (1)", "startOffset": 40, "endOffset": 51}, {"referenceID": 8, "context": "Such estimators are usually of the form [39, 29, 9]: \u03b8\u0302\u03bbn = argmin \u03b8\u2208Rp L(\u03b8;Z) + \u03bbnR(\u03b8) , (1)", "startOffset": 40, "endOffset": 51}, {"referenceID": 13, "context": "Recent work has viewed such characterizations in terms of atomic norms, which give the tightest convex relaxation of a structured set of atoms in which \u03b8\u2217 belongs [14].", "startOffset": 163, "endOffset": 167}, {"referenceID": 44, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 12, "endOffset": 24}, {"referenceID": 42, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 12, "endOffset": 24}, {"referenceID": 25, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 12, "endOffset": 24}, {"referenceID": 12, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 134, "endOffset": 142}, {"referenceID": 11, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 134, "endOffset": 142}, {"referenceID": 5, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 185, "endOffset": 196}, {"referenceID": 28, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 185, "endOffset": 196}, {"referenceID": 32, "context": "the L1 norm [45, 43, 26], and led to sufficient conditions on the design matrix X , including the restrictedisometry properties (RIP) [13, 12] and restricted eigenvalue (RE) conditions [6, 29, 33].", "startOffset": 185, "endOffset": 196}, {"referenceID": 32, "context": "While much of the development has focussed on isotropic Gaussian design matrices, recent work has extended the analysis for L1 norm to correlated Gaussian designs [33] as well as anisotropic sub-Gaussian design matrices [34].", "startOffset": 163, "endOffset": 167}, {"referenceID": 33, "context": "While much of the development has focussed on isotropic Gaussian design matrices, recent work has extended the analysis for L1 norm to correlated Gaussian designs [33] as well as anisotropic sub-Gaussian design matrices [34].", "startOffset": 220, "endOffset": 224}, {"referenceID": 28, "context": "Building on such development, [29] presents a unified framework for the case of decomposable norms and also considers generalized linear models (GLMs) for certain norms such as L1.", "startOffset": 30, "endOffset": 34}, {"referenceID": 28, "context": "Two key insights are offered in [29]: first, the error vector \u2206\u0302n lies in a restricted set, a cone or a star, for suitably large \u03bbn, and second, the loss function needs to satisfy restricted strong convexity (RSC), a generalization of the RE condition, on the restricted error set for the analysis to work out.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon\u2019s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon\u2019s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.", "startOffset": 157, "endOffset": 169}, {"referenceID": 19, "context": "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon\u2019s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.", "startOffset": 157, "endOffset": 169}, {"referenceID": 23, "context": "[14] considers a constrained estimation formulation for all atomic norms, where the gain condition, equivalent to the RE condition, uses Gordon\u2019s inequality [19, 20, 24] and is succinctly represented in terms of the Gaussian width of the intersection of the cone of the error set and a unit ball/sphere.", "startOffset": 157, "endOffset": 169}, {"referenceID": 30, "context": "[31] considers three related formulations for generalized Lasso problems, establish recovery guarantees based on Gordon\u2019s inequality, and quantities related to the Gaussian width.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Sharper analysis for recovery has been considered in [1], yielding a precise characterization of phase transition behavior using quantities related to the Gaussian width.", "startOffset": 53, "endOffset": 56}, {"referenceID": 31, "context": "[32] consider a linear programming estimator in a 1-bit compressed sensing setting and, interestingly, the concept of Gaussian width shows up in the analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "In spite of the advances, with a few notable exceptions [40, 42], most existing results are restricted to isotropic Gaussian design matrices.", "startOffset": 56, "endOffset": 64}, {"referenceID": 41, "context": "In spite of the advances, with a few notable exceptions [40, 42], most existing results are restricted to isotropic Gaussian design matrices.", "startOffset": 56, "endOffset": 64}, {"referenceID": 12, "context": "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(\u03b8) under suitable constraints determined by the noise (y \u2212 X\u03b8) and/or the design matrix X [13, 6, 14, 15].", "startOffset": 249, "endOffset": 264}, {"referenceID": 5, "context": "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(\u03b8) under suitable constraints determined by the noise (y \u2212 X\u03b8) and/or the design matrix X [13, 6, 14, 15].", "startOffset": 249, "endOffset": 264}, {"referenceID": 13, "context": "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(\u03b8) under suitable constraints determined by the noise (y \u2212 X\u03b8) and/or the design matrix X [13, 6, 14, 15].", "startOffset": 249, "endOffset": 264}, {"referenceID": 14, "context": "constrained estimators: As an alternative to regularized estimators, the literature has considered constrained estimators which directly focus on minimizing R(\u03b8) under suitable constraints determined by the noise (y \u2212 X\u03b8) and/or the design matrix X [13, 6, 14, 15].", "startOffset": 249, "endOffset": 264}, {"referenceID": 14, "context": "A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15], which generalizes the Dantzig selector [11] corresponding to the L1 norm, and is given by: \u03b8\u0302\u03b3n = argmin \u03b8\u2208Rp R(\u03b8) s.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15], which generalizes the Dantzig selector [11] corresponding to the L1 norm, and is given by: \u03b8\u0302\u03b3n = argmin \u03b8\u2208Rp R(\u03b8) s.", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "One can show [14, 15] that the restricted error set for such constrained estimators are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 13, "endOffset": 21}, {"referenceID": 14, "context": "One can show [14, 15] that the restricted error set for such constrained estimators are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 13, "endOffset": 21}, {"referenceID": 23, "context": "Then, with Ar = Er \u2229 B p 2 , Ac = Ec \u2229 B p 2 , and \u0100c = cone(Ec) \u2229 B p 2 , assuming \u2016\u03b8\u20162 = 1, \u03b2 = 2, we show that w(Ac) \u2264 w(Ar) \u2264 3w(\u0100c) , (5) where w(A) = Eg[supa\u2208A\u3008a, g\u3009], with g \u223c N(0, Ip\u00d7p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].", "startOffset": 271, "endOffset": 286}, {"referenceID": 4, "context": "Then, with Ar = Er \u2229 B p 2 , Ac = Ec \u2229 B p 2 , and \u0100c = cone(Ec) \u2229 B p 2 , assuming \u2016\u03b8\u20162 = 1, \u03b2 = 2, we show that w(Ac) \u2264 w(Ar) \u2264 3w(\u0100c) , (5) where w(A) = Eg[supa\u2208A\u3008a, g\u3009], with g \u223c N(0, Ip\u00d7p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].", "startOffset": 271, "endOffset": 286}, {"referenceID": 13, "context": "Then, with Ar = Er \u2229 B p 2 , Ac = Ec \u2229 B p 2 , and \u0100c = cone(Ec) \u2229 B p 2 , assuming \u2016\u03b8\u20162 = 1, \u03b2 = 2, we show that w(Ac) \u2264 w(Ar) \u2264 3w(\u0100c) , (5) where w(A) = Eg[supa\u2208A\u3008a, g\u3009], with g \u223c N(0, Ip\u00d7p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].", "startOffset": 271, "endOffset": 286}, {"referenceID": 36, "context": "Then, with Ar = Er \u2229 B p 2 , Ac = Ec \u2229 B p 2 , and \u0100c = cone(Ec) \u2229 B p 2 , assuming \u2016\u03b8\u20162 = 1, \u03b2 = 2, we show that w(Ac) \u2264 w(Ar) \u2264 3w(\u0100c) , (5) where w(A) = Eg[supa\u2208A\u3008a, g\u3009], with g \u223c N(0, Ip\u00d7p) being an isotropic Gaussian vector, denotes the Gaussian width1 of the set A [24, 5, 14, 37].", "startOffset": 271, "endOffset": 286}, {"referenceID": 5, "context": "For the special case of L1 norm, [6] considered a simultaneous analysis of the Lasso and the Dantzig selector, and characterized the structure of the error sets for regularized and constrained sets for the special case of L1 norm.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Further, while the characterization in [6] was also geometric, it was not based on Gaussian widths.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "The second assumption is that the design matrix X \u2208 Rn\u00d7p satisfies the restricted strong convexity (RSC) condition [6, 29] in the error set Er, in particular, there exists a suitable constant \u03ba > 0 so that \u03b4L(\u2206, \u03b8\u2217) , L(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217)\u2212 \u3008\u2207L(\u03b8\u2217),\u2206\u3009 \u2265 \u03ba\u2016\u2206\u20162 \u2200\u2206 \u2208 Er .", "startOffset": 115, "endOffset": 122}, {"referenceID": 28, "context": "The second assumption is that the design matrix X \u2208 Rn\u00d7p satisfies the restricted strong convexity (RSC) condition [6, 29] in the error set Er, in particular, there exists a suitable constant \u03ba > 0 so that \u03b4L(\u2206, \u03b8\u2217) , L(\u03b8\u2217 + \u2206)\u2212 L(\u03b8\u2217)\u2212 \u3008\u2207L(\u03b8\u2217),\u2206\u3009 \u2265 \u03ba\u2016\u2206\u20162 \u2200\u2206 \u2208 Er .", "startOffset": 115, "endOffset": 122}, {"referenceID": 28, "context": "where \u03c8(Er) = supu\u2208Er R(u) \u2016u\u20162 is a norm compatibility constant [29], and c > 0 is a constant.", "startOffset": 65, "endOffset": 69}, {"referenceID": 35, "context": "Recent work in [36] has extended the analyses for sub-exponential distributions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 36, "context": "Interestingly, for sub-Gaussian designs, one obtains the results in terms of the \u2018sub-Gaussian width\u2019 of the unit norm-ball, which can be upper bounded by a constant times the Gaussian width using generic chaining [37].", "startOffset": 214, "endOffset": 218}, {"referenceID": 36, "context": "Further, one can get high-probability versions of these bounds using related advances in generic chaining [37, 38].", "startOffset": 106, "endOffset": 114}, {"referenceID": 37, "context": "Further, one can get high-probability versions of these bounds using related advances in generic chaining [37, 38].", "startOffset": 106, "endOffset": 114}, {"referenceID": 28, "context": "For the special case of L1 regularization, \u03a9R is the unit L1 norm ball, and the corresponding Gaussian width w(\u03a9R) \u2264 c1 \u221a log p, which explains the \u221a log p term one finds in existing bounds for Lasso [29, 9].", "startOffset": 200, "endOffset": 207}, {"referenceID": 8, "context": "For the special case of L1 regularization, \u03a9R is the unit L1 norm ball, and the corresponding Gaussian width w(\u03a9R) \u2264 c1 \u221a log p, which explains the \u221a log p term one finds in existing bounds for Lasso [29, 9].", "startOffset": 200, "endOffset": 207}, {"referenceID": 36, "context": "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].", "startOffset": 74, "endOffset": 82}, {"referenceID": 37, "context": "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].", "startOffset": 74, "endOffset": 82}, {"referenceID": 20, "context": "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].", "startOffset": 127, "endOffset": 135}, {"referenceID": 27, "context": "Our analysis techniques for the RIP results are based on generic chaining [37, 38], in particular a specific form developed in [21, 28].", "startOffset": 127, "endOffset": 135}, {"referenceID": 28, "context": "is determined by the Restricted Strong Convexity (RSC) condition [29].", "startOffset": 65, "endOffset": 69}, {"referenceID": 28, "context": "The result is thus a considerable generalization of earlier results on convex losses, such as GLMs, which had looked at specific norms and associated cones and/or did not express the results in terms of the Gaussian width of A [29].", "startOffset": 227, "endOffset": 231}, {"referenceID": 28, "context": "Further, note that the condition in (15) is similar to that in [29] for \u03b2 = 2, but the above characterization holds for any norm, not just decomposable norms [29].", "startOffset": 63, "endOffset": 67}, {"referenceID": 28, "context": "Further, note that the condition in (15) is similar to that in [29] for \u03b2 = 2, but the above characterization holds for any norm, not just decomposable norms [29].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].", "startOffset": 132, "endOffset": 147}, {"referenceID": 5, "context": "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].", "startOffset": 132, "endOffset": 147}, {"referenceID": 13, "context": "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].", "startOffset": 132, "endOffset": 147}, {"referenceID": 14, "context": "WhileEr need not be a convex set, we establish a relationship betweenEr and the error setEc corresponding to constrained estimators [13, 6, 14, 15].", "startOffset": 132, "endOffset": 147}, {"referenceID": 14, "context": "A recent example of such a constrained estimator is the generalized Dantzig selector (GDS) [15] given by: \u03b8\u0302\u03b3n = argmin \u03b8\u2208Rp R(\u03b8) s.", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 13, "endOffset": 21}, {"referenceID": 14, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 13, "endOffset": 21}, {"referenceID": 13, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 84, "endOffset": 96}, {"referenceID": 14, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 84, "endOffset": 96}, {"referenceID": 39, "context": "One can show [14, 15] that the restricted error set for such constrained estimators [14, 15, 40] are of the form: Ec = {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 84, "endOffset": 96}, {"referenceID": 5, "context": "Related observations have been made for the special case of the L1 norm [6], although past work did not provide an explicit characterization in terms of Gaussian widths.", "startOffset": 72, "endOffset": 75}, {"referenceID": 28, "context": "In recent work, [29] considered regularized regression with the special case of decomposable norms, defined in terms of a pair of subspacesM \u2286 M\u0304 of Rp.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "We show that for decomposable norms, the error set Er in our analysis is included in the error cone defined in [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "The last inequality is precisely the error cone in [29] for \u03b8\u2217 \u2208 M.", "startOffset": 51, "endOffset": 55}, {"referenceID": 28, "context": "where \u03a8(M\u0304) is the subspace compatibility in M\u0304, as used in [29].", "startOffset": 60, "endOffset": 64}, {"referenceID": 40, "context": "We also recall the definition of the sub-Gaussian norm for a sub-Gaussian variable x, |||x|||\u03c82 = supp\u22651 1 \u221a p(E[|x| p])1/p [41].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "The motivation behind getting an upper bound of the Gaussian width w(\u03a9R) of the unit norm ball in terms of the Gaussian width of such a cone is because considerable advances have been made in recent years in upper bounding Gaussian widths of such cones [14, 1].", "startOffset": 253, "endOffset": 260}, {"referenceID": 0, "context": "The motivation behind getting an upper bound of the Gaussian width w(\u03a9R) of the unit norm ball in terms of the Gaussian width of such a cone is because considerable advances have been made in recent years in upper bounding Gaussian widths of such cones [14, 1].", "startOffset": 253, "endOffset": 260}, {"referenceID": 5, "context": ", L(\u03b8;Zn) = 1 n\u2016y \u2212 X\u03b8\u2016 2, the RSC condition (20) becomes equivalent to the Restricted Eigenvalue (RE) condition [6, 29], since", "startOffset": 113, "endOffset": 120}, {"referenceID": 28, "context": ", L(\u03b8;Zn) = 1 n\u2016y \u2212 X\u03b8\u2016 2, the RSC condition (20) becomes equivalent to the Restricted Eigenvalue (RE) condition [6, 29], since", "startOffset": 113, "endOffset": 120}, {"referenceID": 5, "context": "Analysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34].", "startOffset": 147, "endOffset": 158}, {"referenceID": 32, "context": "Analysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34].", "startOffset": 147, "endOffset": 158}, {"referenceID": 33, "context": "Analysis of RE conditions for certain types of design matrices for certain types of norms, especially the L1 norm, have appeared in the literature [6, 33, 34].", "startOffset": 147, "endOffset": 158}, {"referenceID": 9, "context": "The RE/RIP conditions for independent isotropic Gaussian designs have been widely studied for the case of L1 norm [10, 6].", "startOffset": 114, "endOffset": 121}, {"referenceID": 5, "context": "The RE/RIP conditions for independent isotropic Gaussian designs have been widely studied for the case of L1 norm [10, 6].", "startOffset": 114, "endOffset": 121}, {"referenceID": 32, "context": "The generalization to RE condition for correlated Gaussian designs for the special of L1 norm was studied in [33].", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "[14] consider the more general context of atomic norms, and RE condition analysis applies to any spherical cap A, with sample complexity results in terms of w(A), the Gaussian width of A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "However, the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 52, "endOffset": 64}, {"referenceID": 19, "context": "However, the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 52, "endOffset": 64}, {"referenceID": 23, "context": "However, the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 52, "endOffset": 64}, {"referenceID": 45, "context": "Progress has been made on establishing RE conditions for sub-Gaussian designs for error sets/caps corresponding to specific norms such as L1 [46].", "startOffset": 141, "endOffset": 145}, {"referenceID": 33, "context": "In recent work, RE conditions were developed for anisotropic sub-Gaussian designs for the L1 norm [34].", "startOffset": 98, "endOffset": 102}, {"referenceID": 28, "context": "Further, recent work have pointed out the differences between the RE and the RIP condition, which gives a two-sided bound on quadratic forms of random matrices [29].", "startOffset": 160, "endOffset": 164}, {"referenceID": 32, "context": "In fact, all existing RE results do implicitly have the width term, but in a form specific to the chosen norm [33, 34].", "startOffset": 110, "endOffset": 118}, {"referenceID": 33, "context": "In fact, all existing RE results do implicitly have the width term, but in a form specific to the chosen norm [33, 34].", "startOffset": 110, "endOffset": 118}, {"referenceID": 13, "context": "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 113, "endOffset": 125}, {"referenceID": 19, "context": "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 113, "endOffset": 125}, {"referenceID": 23, "context": "The analysis on atomic norm in [14] has the w(A) term explicitly, but the analysis relies on Gordon\u2019s inequality [19, 20, 24], which is applicable only for isotropic Gaussian design matrices.", "startOffset": 113, "endOffset": 125}, {"referenceID": 36, "context": "The proof technique we use is an application of generic chaining [37, 38].", "startOffset": 65, "endOffset": 73}, {"referenceID": 37, "context": "The proof technique we use is an application of generic chaining [37, 38].", "startOffset": 65, "endOffset": 73}, {"referenceID": 20, "context": "The specific form we utilize was originally developed in [21, 28].", "startOffset": 57, "endOffset": 65}, {"referenceID": 27, "context": "The specific form we utilize was originally developed in [21, 28].", "startOffset": 57, "endOffset": 65}, {"referenceID": 32, "context": "The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use", "startOffset": 165, "endOffset": 173}, {"referenceID": 28, "context": "The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use", "startOffset": 165, "endOffset": 173}, {"referenceID": 33, "context": "The key difference between our RIP analysis and much of the existing literature on RE conditions, which use specialized tools such as Gaussian comparison principles [33, 29] or analysis geared to a particular norm [34], is the use", "startOffset": 214, "endOffset": 218}, {"referenceID": 16, "context": "Further, the RIP results can be viewed as a generalization of the celebrated Johnson-Lindenstrauss (JL) lemma [17], and the interested reader can explore these connections in [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "Further, the RIP results can be viewed as a generalization of the celebrated Johnson-Lindenstrauss (JL) lemma [17], and the interested reader can explore these connections in [21].", "startOffset": 175, "endOffset": 179}, {"referenceID": 9, "context": "More generally, choosing = cw(A)/ \u221a n, one can write the result in a traditional RIP form [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 32, "context": "Finally, it is instructive to compare the above result to existing characterizations of the RE condition for anisotropic Gaussian [33] and anisotropic sub-Gaussian [34] designs, focused on the L1 norm.", "startOffset": 130, "endOffset": 134}, {"referenceID": 33, "context": "Finally, it is instructive to compare the above result to existing characterizations of the RE condition for anisotropic Gaussian [33] and anisotropic sub-Gaussian [34] designs, focused on the L1 norm.", "startOffset": 164, "endOffset": 168}, {"referenceID": 13, "context": "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].", "startOffset": 133, "endOffset": 141}, {"referenceID": 28, "context": "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].", "startOffset": 133, "endOffset": 141}, {"referenceID": 24, "context": "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].", "startOffset": 250, "endOffset": 258}, {"referenceID": 15, "context": "Other examples can be constructed for norms and error sets with known bounds on the Gaussian widths and norm compatibility constants [14, 29], and more general ways to bound the Gaussian widths and norm compatibility constants have been developed in [25, 16].", "startOffset": 250, "endOffset": 258}, {"referenceID": 13, "context": "where (a) is obtained from the fact that Gaussian width ofG(\u03b8\u0303) with \u03b8\u0303 be a s-sparse vector is \u221a 2s log(ps ) + 5 4s [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "which is similar to the results obtained in well known results [14, 29].", "startOffset": 63, "endOffset": 71}, {"referenceID": 28, "context": "which is similar to the results obtained in well known results [14, 29].", "startOffset": 63, "endOffset": 71}, {"referenceID": 28, "context": "As shown in [29] Group norm is a decomposable norm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "where m = max t |Gt| and (a) is obtained from the fact that Gaussian width of G(\u03b8\u0303) where \u03b8\u0303 has k active group is \u221a 2k(m+ log(T \u2212 k)) + k [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "which is similar to the results obtained in previous works [14, 29].", "startOffset": 59, "endOffset": 67}, {"referenceID": 28, "context": "which is similar to the results obtained in previous works [14, 29].", "startOffset": 59, "endOffset": 67}, {"referenceID": 3, "context": "In this section, we extend our results to estimation with norm regularization in the context of generalized linear models (GLMs) [4, 8].", "startOffset": 129, "endOffset": 135}, {"referenceID": 7, "context": "In this section, we extend our results to estimation with norm regularization in the context of generalized linear models (GLMs) [4, 8].", "startOffset": 129, "endOffset": 135}, {"referenceID": 7, "context": "is the log-partition function [8, 4, 44].", "startOffset": 30, "endOffset": 40}, {"referenceID": 3, "context": "is the log-partition function [8, 4, 44].", "startOffset": 30, "endOffset": 40}, {"referenceID": 43, "context": "is the log-partition function [8, 4, 44].", "startOffset": 30, "endOffset": 40}, {"referenceID": 3, "context": "the natural parameter \u03b7i = \u3008Xi, \u03b8\u2217\u3009 gives the expectation of the response [4, 8], i.", "startOffset": 74, "endOffset": 80}, {"referenceID": 7, "context": "the natural parameter \u03b7i = \u3008Xi, \u03b8\u2217\u3009 gives the expectation of the response [4, 8], i.", "startOffset": 74, "endOffset": 80}, {"referenceID": 3, "context": "Note that for GLMs over discrete responses yi, the integration needs to be suitably changed to summation [4, 8].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "Note that for GLMs over discrete responses yi, the integration needs to be suitably changed to summation [4, 8].", "startOffset": 105, "endOffset": 111}, {"referenceID": 0, "context": "i=1 \u22072\u03c6(\u3008\u03b8\u2217, Xi\u3009+ \u03b3i\u3008u,Xi\u3009)\u3008u,Xi\u3009 , where \u03b3i \u2208 [0, 1], and where the last equality follows from a direct application of the mean value theorem [35].", "startOffset": 47, "endOffset": 53}, {"referenceID": 34, "context": "i=1 \u22072\u03c6(\u3008\u03b8\u2217, Xi\u3009+ \u03b3i\u3008u,Xi\u3009)\u3008u,Xi\u3009 , where \u03b3i \u2208 [0, 1], and where the last equality follows from a direct application of the mean value theorem [35].", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "Since the log-partition function \u03c6 is of Legendre type [3, 4, 8], the second derivative \u22072\u03c6(\u00b7) is always positive.", "startOffset": 55, "endOffset": 64}, {"referenceID": 3, "context": "Since the log-partition function \u03c6 is of Legendre type [3, 4, 8], the second derivative \u22072\u03c6(\u00b7) is always positive.", "startOffset": 55, "endOffset": 64}, {"referenceID": 7, "context": "Since the log-partition function \u03c6 is of Legendre type [3, 4, 8], the second derivative \u22072\u03c6(\u00b7) is always positive.", "startOffset": 55, "endOffset": 64}, {"referenceID": 41, "context": "Assuming X has isotropic sub-Gaussian rows with |||Xi|||\u03c62 \u2264 \u03ba, \u3008Xi, \u03b8 \u2217\u3009 and \u3008Xi, u\u3009 are sub-Gaussian random variables with sub-Gaussian norm at most C\u03ba [42].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "Note that RSC analysis for GLMs was considered in [29] for specific norms, especially L1, whereas our analysis applies to any set A \u2286 Sp\u22121, hence to any norm, and the result is in terms of the Gaussian width w(A) of A.", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "The work can be viewed as a direct generalization of results in [29], which presented related results for decomposable norms.", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "Further, the error sets for regularized and constrained versions of such problems are shown to be closely related [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 29, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 95, "endOffset": 103}, {"referenceID": 26, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 95, "endOffset": 103}, {"referenceID": 21, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 263, "endOffset": 275}, {"referenceID": 22, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 263, "endOffset": 275}, {"referenceID": 26, "context": "For heavy tailed measurements, the lower and upper tails of quadratic forms behave differently [30, 27], and it may be possible to establish geometric estimation error analysis for general norms, some special cases of which have been investigated in recent years [22, 23, 27].", "startOffset": 263, "endOffset": 275}, {"referenceID": 17, "context": "Since real-world several problems, including spatial and temporal problems, do have correlated observations, it will be important to investigate estimators which perform well in such settings [18].", "startOffset": 192, "endOffset": 196}, {"referenceID": 19, "context": "In several of our proofs, we use the concept of Gaussian width [20, 14], which is defined as follows.", "startOffset": 63, "endOffset": 71}, {"referenceID": 13, "context": "In several of our proofs, we use the concept of Gaussian width [20, 14], which is defined as follows.", "startOffset": 63, "endOffset": 71}, {"referenceID": 36, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].", "startOffset": 179, "endOffset": 194}, {"referenceID": 37, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].", "startOffset": 179, "endOffset": 194}, {"referenceID": 6, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].", "startOffset": 179, "endOffset": 194}, {"referenceID": 23, "context": "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [37, 38, 7, 24].", "startOffset": 179, "endOffset": 194}, {"referenceID": 40, "context": "The following definitions and lemmas are from [41].", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "Recall from [14], that the error set for the constrained setting for atomic norms is a cone given by: Cc = Cc(\u03b8 \u2217) = cone(Ec) = cone {\u2206 \u2208 R | R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} .", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "Also, recall that any norm is convex, since by triangle inequality, for t \u2208 [0, 1], we have R(t\u03b81 + (1\u2212 t)\u03b82) \u2264 R(t\u03b81) +R((1\u2212 t)\u03b82) = tR(\u03b81) + (1\u2212 t)R(\u03b82) .", "startOffset": 76, "endOffset": 82}, {"referenceID": 32, "context": "One can view (ii) as a special case of (i), but we highlight this special case because of its practical importance and past literature on RE conditions for anisotropic subGaussian designs [33, 34].", "startOffset": 188, "endOffset": 196}, {"referenceID": 33, "context": "One can view (ii) as a special case of (i), but we highlight this special case because of its practical importance and past literature on RE conditions for anisotropic subGaussian designs [33, 34].", "startOffset": 188, "endOffset": 196}, {"referenceID": 27, "context": "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand\u2019s generic chaining [37, 38].", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand\u2019s generic chaining [37, 38].", "startOffset": 74, "endOffset": 78}, {"referenceID": 36, "context": "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand\u2019s generic chaining [37, 38].", "startOffset": 118, "endOffset": 126}, {"referenceID": 37, "context": "Our results simply use a general treatment developed in [28], building on [21], based on Talagrand\u2019s generic chaining [37, 38].", "startOffset": 118, "endOffset": 126}, {"referenceID": 27, "context": "We specifically focus on results in [28] which provide uniform bounds on the supremum of certain empirical processes.", "startOffset": 36, "endOffset": 40}, {"referenceID": 27, "context": "The results in [28], and more generally in generic chaining [37, 38], are based on certain \u03b3-functionals which we briefly introduce below.", "startOffset": 15, "endOffset": 19}, {"referenceID": 36, "context": "The results in [28], and more generally in generic chaining [37, 38], are based on certain \u03b3-functionals which we briefly introduce below.", "startOffset": 60, "endOffset": 68}, {"referenceID": 37, "context": "The results in [28], and more generally in generic chaining [37, 38], are based on certain \u03b3-functionals which we briefly introduce below.", "startOffset": 60, "endOffset": 68}, {"referenceID": 27, "context": "Theorem 10 (Mendelson, Pajor, Tomczak-Jaegermann [28]) There exist absolute constants c1, c2, c3 for which the following holds.", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "The canonical density function of exponential family distributions is given by [4, 8, 44]:", "startOffset": 79, "endOffset": 89}, {"referenceID": 7, "context": "The canonical density function of exponential family distributions is given by [4, 8, 44]:", "startOffset": 79, "endOffset": 89}, {"referenceID": 43, "context": "The canonical density function of exponential family distributions is given by [4, 8, 44]:", "startOffset": 79, "endOffset": 89}, {"referenceID": 7, "context": "(144) The interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44].", "startOffset": 125, "endOffset": 135}, {"referenceID": 3, "context": "(144) The interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44].", "startOffset": 125, "endOffset": 135}, {"referenceID": 43, "context": "(144) The interested reader can study details and additional properties of exponential families from the existing literature [8, 4, 44].", "startOffset": 125, "endOffset": 135}, {"referenceID": 7, "context": "Considering the negative log-likelihood, the GLM corresponding to the Gaussian distribution yields least squares regression [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "Considering the negative log-likelihood, the GLM corresponding to the Bernoulli distribution yields logistic regression [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "Considering the negative log-likelihood, the GLM corresponding to the Poisson distribution yields Poisson regression [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 0, "context": "for suitable \u03b3i \u2208 [0, 1].", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": "where \u03b3i \u2208 [0, 1].", "startOffset": 11, "endOffset": 17}, {"referenceID": 1, "context": "Acknowledgements: We thank the reviewers of the conference version [2] for helpful comments and suggestions on related work.", "startOffset": 67, "endOffset": 70}], "year": 2015, "abstractText": "Analysis of non-asymptotic estimation error and structured statistical recovery based on norm regularized regression, such as Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise model. This paper presents generalizations of such estimation error analysis on all four aspects compared to the existing literature. We characterize the restricted error set where the estimation error vector lies, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to any norm. Precise characterizations of the bound is presented for isotropic as well as anisotropic subGaussian design matrices, subGaussian noise models, and convex loss functions, including least squares and generalized linear models. Generic chaining and associated results play an important role in the analysis. A key result from the analysis is that the sample complexity of all such estimators depends on the Gaussian width of a spherical cap corresponding to the restricted error set. Further, once the number of samples n crosses the required sample complexity, the estimation error decreases as c \u221a n , where c depends on the Gaussian width of the unit norm ball.", "creator": "LaTeX with hyperref package"}}}