{"id": "1505.03783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2015", "title": "Rank diversity of languages: Generic behavior in computational linguistics", "abstract": "Statistical studies of languages have focused on the rank-frequency distribution of words. Instead, we introduce here a measure of how word ranks change in time and call this distribution \\emph{rank diversity}. We calculate this diversity for books published in six European languages since 1800, and find that it follows a universal lognormal distribution. Based on the mean and standard deviation associated with the lognormal distribution, we define three different word regimes of languages: \"heads\" consist of words which almost do not change their rank in time, \"bodies\" are words of general use, while \"tails\" are comprised by context-specific words and vary their rank considerably in time. The heads and bodies reflect the size of language cores identified by linguists for basic communication. We propose a Gaussian random walk model which reproduces the rank variation of words in time and thus the diversity. Rank diversity of words can be understood as the result of random variations in rank, where the size of the variation depends on the rank itself. We find that the core size is similar for all languages studied.", "histories": [["v1", "Thu, 14 May 2015 16:21:02 GMT  (5034kb,D)", "http://arxiv.org/abs/1505.03783v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["germinal cocho", "jorge flores", "carlos gershenson", "carlos pineda", "sergio s\\'anchez"], "accepted": false, "id": "1505.03783"}, "pdf": {"name": "1505.03783.pdf", "metadata": {"source": "CRF", "title": "Rank diversity of languages: Generic behavior in computational linguistics", "authors": ["Germinal Cocho", "Jorge Flores", "Carlos Gershenson", "Carlos Pineda", "Sergio S\u00e1nchez"], "emails": ["cgg@unam.mx"], "sections": [{"heading": "Introduction", "text": "Statistical studies of languages have become popular since the work of George Zipf [1] and have been refined with the availability of large data sets and the introduction of novel analytical models [2\u20137]. Zipf found that when words of large corpora are ranked according to their frequency, there seems to be a universal tendency across texts and languages. He proposed that ranked words follow a power law f \u223c 1/k, where k is the rank of the word\u2014the higher ranks corresponding to the least frequent words\u2014and f is the relative frequency of each word [8, 9]. This regularity of languages and other social and physical phenomena had been noticed beforehand, at least by Jean-Baptiste Estoup [10] and Felix Auerbach [11], but it is now known as Zipf\u2019s law.\nZipf\u2019s law is a rough approximation of the precise statistics of rank-frequency distributions of languages. As a consequence, several variations have been proposed [12\u201315]. We compared Zipf\u2019s law with four other models, all of them behaving as 1/ka for a small k, with a \u2248 1, as detailed in the SI. We found that all models have systematic errors so it was difficult to choose one over the other.\nStudies based on rank-frequency distributions of languages have proposed two word regimes [15, 16]: a \u201ccore\u201d where the most common words occur, which behaves as 1/ka for small k, and another region for large k, which is identified by a change of exponent a in the distribution fit. Unfortunately, the point where exponent a changes varies widely across texts and languages, from 5000 [16] to 62,000 [15]. A recent study [17] measures the number of most frequent words which account for 75% of the Google books corpus. Differences of an order of magnitude across languages were obtained, from 2365 to 21077 words (including inflections of the same stems). This illustrates the variability of rank-frequency distributions. The core of human languages can be considered to be between 1500 and 3000 words (not counting different inflections of the same stems), based on basic vocabularies for foreigners [18], creole [19], and pidgin languages [20]. For example, Voice of America\u2019s Special English [21] and Wikipedia in Simple English use about 1500 and\nar X\niv :1\n50 5.\n03 78\n3v 1\n[ cs\n.C L\n] 1\n4 M\nay 2\n2 2000 words, respectively (not counting inflections). The Oxford Advanced Learner\u2019s Dictionary lists 3000 priority lexical entries [22]. This suggests that the change of exponent a or another arbitrary cutoff in rank-frequency distributions does not reflect the size of the core of languages.\nIn view of these problems with rank-frequency distributions, we propose a novel measure to characterize statistical properties of languages. We have called this measure rank diversity and it tells us how words change their rank in time. With rank diversity, three regimes of words are identified: \u201cheads\u201d, \u201cbodies\u201d and \u201ctails\u201d. This measure of rank diversity follows the same simple functional law with similar parameters for all data analyzed. In particular, this is so for the six European languages studied here using a large data set of more than 6.4 \u00d71011 words from Google Books [23], which contains about 4% of all books written until 2008. It should be noted that this data set includes all different inflected forms (such as plural, different tense/aspect forms, etc.) found in the book corpus. Data sets such as this have allowed the study of \u201cculturomics\u201d: how cultural traits such as language have changed in time [24\u201330].\nThe rank diversity follows a scale-invariant behavior regarding its fluctuations, which inspires a model based on random walks, with scale-invariant random steps. This model reproduces the behavior of diversity and thus captures the essence of the evolution of word rank across different languages."}, {"heading": "Rank diversity of words", "text": "In what follows we shall consider six European languages from the Indo-European family. They are English and German; Spanish, French and Italian; and Russian. They belong to different linguistic branches: Germanic, Romance, and Slavic, respectively. The native speakers of these languages account for approximately 17% of the world population.\nWe shall start by taking into account the 20, say, most used words in the six languages, that is, the lowest-ranked words. Using, for the sake of uniformity, the first sense or first meaning given by Google Translate, once these words are translated into English, the coincidences in all six languages are remarkable (see Table S1 in File S1). This could have been foreseen, since most of the lowest-ranked words are articles, prepositions or conjunctions, i.e. what is called function words. A different matter, as we shall see, would result if we had considered only nouns, verbs, adverbs or adjectives, known as content words.\nIn order to quantify this fact, we present in Fig. 1 the time evolution of the overlap of the first 20 lowest-ranked words in the five languages with respect to the corresponding list of English. From the upper part of this figure we can see that along two centuries this overlap fluctuates around 0.9, a rather large number, except for Russian, since this language does not have articles. These data reveal that these Indo-European languages have shared structural properties, notwithstanding that they belong to distinct linguistic branches.\nThe lowest-ranked words used to construct the upper part of Fig. 1 are essentially the same along centuries (See Figs S3-S8 in SI). But this is not the case for content words, as can be seen in Table S2 in File S1. First, and as also shown by the dashed curves in Fig. 1, the overlap of these words with respect to English for the other five languages (including Russian) is of the order of 0.5. These values are much lower than the overlap of function words. Second, the most common nouns vary considerably with time. On the one hand, nouns like time, man, life and their translation to the other languages are present independently of the century. On the other hand, words like god and king have a low rank in the eighteenth century but have a larger rank in the last century. The rank change in time of these nouns reflect cultural facts.\nWhat is discussed in the previous paragraph is an example of what could be called rank diversity d(k). This is, in the present study, the number of different words occurring at a specific rank k over a given period of time \u2206t. We found that the resulting rank diversity curves for the six languages studied between 1800 and 2008 are similar to each other, as shown in Figs. 2 and 6. Low ranks have a very low diversity, as few words appear in the same ranks for the years we have studied.\nAs shown by the continuous lines in Fig. 2, the sigmoid curve fits very well d(k) for all languages considered, except for low k where the statistical fluctuations are larger due to the small sample size. The sigmoid is the cumulative of a Gaussian distribution, i.e.\n\u03a6\u00b5,\u03c3(log10 k) = 1\n\u03c3 \u221a 2\u03c0 \u222b log10 k \u2212\u221e e\u2212 (y\u2212\u00b5)2 2\u03c32 dy, (1)\nand is given as a function of log k. The values of \u00b5 and \u03c3 reported in Fig. 2 were obtained adjusting equation 1 to the rank diversity calculated for each individual language. The mean value \u00b5 identifies the point where d(k) \u2248 0.5, while the width \u03c3 gives the scale in which d(k) gets close to its extremal values. When log k is much larger than \u00b5+ \u03c3, \u03a6\u00b5,\u03c3(log k) gets exponentially close to one, whereas when log k is much smaller than \u00b5\u2212 \u03c3 it gets exponentially close to zero. It is customary in statistics to define a bulk of the Gaussian between \u00b5\u00b1 2\u03c3, where 95% of the population lies. Along the same lines, we define three regions, marked by\nlog10 k\u00b1 = \u00b5\u00b1 2\u03c3. (2)\nFirst, we find what we shall call the head of the language, distributed with ranks between 1 and k\u2212; a second region, identified as the body of the language, lies between k\u2212 and k+; and finally the tail, beyond k+. From the values reported in Fig. 2, we see that 9 < k\u2212 < 22, while k+ lies between 1832 and 3099. As shown in Fig. 3, these regions are robust to changes in the historical period considered and to the data set size (larger for recent years).\nThe bodies of languages consist of words that have limited change in time. Based on the size of basic vocabularies, it can be argued that the \u201ccore\u201d of English is between 1500 and 3000 words, as mentioned in the introduction, which is consistent with our results. If we agree that the rank diversity identifies the core (head and body) of English, then it can be argued that the size of the core of the other five languages studied is similar [31], which is also supported by the high similarity across languages in Fig 2.\nThe tails of languages are formed by words which vary their rank considerably in time. This implies that they are more dependent on the text and its domain than words from the core. It can be assumed that words belonging to the head and body of languages have a high probability of being used in any text, while words from the tail would appear only in specific texts and domains.\nNote that we obtain language cores slightly larger than those proposed by linguists. This is to be expected, as the Google Books data set treats words forms inflected for different persons, tenses, genders, numbers, cases, and so forth, as distinct items, while dictionaries count only stems (presented as citation forms, i.e. the basic form that users are most likely to look up). For example, the core for English obtained using rank diversity consists of 2448 words, but within these there are only 1760 different stems in the year 2008. Moreover, the studied data set contains several proper names which are not included in basic vocabulary lists. For English, 55 out of 2448 are proper names in 2008.\nThe rank evolution of particular words in time, belonging to the head, body, and tail of English is shown in Fig. 4a. This ratifies the results shown in Fig. 2, where low-ranked words exhibit little variation in time and this variation increases with the rank. More trajectories are presented in the SI. As mentioned above, words from the head vary little over time. However, the way in which words from the body or tail vary their rank in time appears to be similar, although at a different scale. This similarity leads us to propose a model of rank diversity where the amount of rank variation depends only on the rank."}, {"heading": "A random walk model for rank diversity", "text": "We consider the relative size of frequency changes, or flights as they are sometimes called in statistical physics, defined as (kt+1 \u2212 kt) /kt where kt is the rank at discrete time t of a given element. We present in Fig. 5 the distribution of these frequency changes for English, our largest data set, and in Fig. 17 in File S1 for all languages. Notice that, on average, the relative jumps seem to be largely independent of the value of the rank. We propose, based on this fact, a simple model to understand the evolution of rank diversity of words.\nWe shall call this model a scale-invariant random Gaussian walk, since a word with rank kt, is converted to rank kt+1 according to the following procedure: One defines an auxiliary variable st+1 at time t+ 1 by the relation st+1 = kt +G (0, kt\u03c3\u0302) , (3) where G(0, \u03c3\u0303) is a Gaussian random number generator of width \u03c3\u0303 and mean 0. This means that the random variable st+1 has a width distribution proportional to kt. Words with very low ranks will change very slowly or not at all, while those with higher k have a larger rank variation in time, as reflected by d(k). Once the values of st+1 for all words are obtained, they are ordered according to their magnitude. This new order gives new rankings, i.e. the k values at time t + 1. There is a small correlation of the jumps between different times in this model. This is consistent with the observed behavior of the six languages dealt with here, as can be seen in Fig. 18 in File S1. The only parameter in the model is the width \u03c3\u0302, which is the most common standard deviation of the relative frequency changes of each data set.\nA word of caution must be said. In Fig. 5, two curves are plotted. In green, a Lorentzian distribution, and in red a Gaussian distribution, both centered at zero, and with a width obtained by best fit to the data presented here. Although the Lorentzian fits these data somewhat better than the Gaussian, we use the latter in our model, since the long tails of the Lorentzian would yield long flights in words (not observed in the historical data) and a very different function d(k). One should recall that the Lorentzian does not have a finite second moment, so this might be the reason for this distribution to be inadequate. It is probable that a truncated Lorentzian could be a better choice, but we leave this detail open as a possible refinement to our model.\nWith this model we have produced the evolution of a random simulated language; see [32] for other approaches. Fig. 4b shows examples of rank trajectories at different scales, exhibiting similarities with\n8 those of actual words shown in Fig. 4a. Moreover, if its diversity d(k) is calculated with the \u03c3\u0302 corresponding to the most popular width of the distribution of relative size of flights for all words in the English language from 1800 to 2008, the results coincide with the sigmoid obtained for all six languages analyzed, as shown in Fig. 6."}, {"heading": "Discussion", "text": "Within statistical linguistics, the frequency-rank distributions of several languages of European origin have been analyzed for many years now. However, no simple model can reproduce the detailed properties of this distribution (see SI). In particular, there has been the proposal that there exist two different regimes for ranks, but these regimes have not been satisfactorily validated in the empirical data. Due to these difficulties we have been led to introduce a statistical measure, which we have called rank diversity, to describe the statistical properties of natural languages. A simulated random language was generated which reproduces the observed features quite well.\nOur random walk model mimics the evolution of languages to produce a simulated rank diversity which closely matches that of historical data. We consider that statistical similarities across languages and the simplicity of the model to reproduce them sufficient evidence to claim that rank diversity of words is universal. This does not imply that all languages have the same rank diversity curves, but that the rank diversity distribution of all the languages studied here can be fitted properly with equation 1. Certainly, different languages have different curves that fit them better, just as different exponents fit better a Zipf distribution of different languages. For the languages studied, 1.6 \u2264 \u00b5 \u2264 2.1 and 0.4 \u2264 \u03c3 \u2264 0.6.\nThis universality could be used to favor nativist explanations of human language [33, 34], where language is claimed to be determined by innate constraints. However, the high-ranked diversity of language tails could be used in favor of adaptationist explanations as well [35], as the precise rank of tail\n9 words is highly contingent. In recent years, explanations of human language relating biological evolution (genetically encoded innate properties) and learning (epigenetical adaptation) with culture have gained strength [36\u201338]. Even so, few assumptions are necessary to explain some general aspects of the evolution of human languages [39]. The present work shows that the evolution of word frequency can be explained with Gaussian random walks, where the size of the change in word frequency is proportional to its rank, i.e. frequent words change less than infrequent words. This explanation does not require innate properties, adaptive advantages, nor culture. This does not imply that the latter are irrelevant for other aspects of language evolution. Note that our study is carried out at a statistical level. We do not address syntactic, semantic, and grammatical aspects of human language [40\u201343], which are certainly important.\nWhy does the rank diversity approach a lognormal distribution? Which processes and mechanisms are required for this? There is one condition for a variable to have a lognormal distribution. This condition is that the variable should be the result of a high number of different and independent causes which produce positive effects composed multiplicatively. Thus, each cause has a negligible effect on the global result [44]. Our Gaussian random walk model supports this as a suitable explanation: the statistical distribution of d is always lognormal, there is a high number of components (words), each word has a negligible effect compared to the language properties, i.e. large changes in word frequency (ranking) do not cause large changes in the statistical properties of each language, and the rank of each word is partially a cumulative product of its rank in previous times, as expressed in equation 3. Languages statistically comply with these dynamics, and that serves as an explanation for their evolution and structure.\nIn future work, it will be relevant to study the rank diversity of n-grams with n > 1 [45], other linguistic corpora and phenomena with dynamic rank distributions [27,46\u201348] and more generally with temporal networks [49\u201352]. A specific example would be the ranking of chess players, given by the World Chess Federation (Fe\u0301de\u0301ration Internationale des E\u0301checs). The rank diversity in this case is provided in figure 7, which shows that the sigmoid is appropriate also for this case.\n10"}, {"heading": "S1 Models for rank-frequency distributions", "text": "The rank-frequency distributions of words for different languages are very similar to each other, as shown in Fig. 8a. The distributions are also similar across centuries, as shown in Fig. 8b.\nWe present five different distributions with distinct origins, though all of them containing the common factor 1ka . The distributions are:\nm1(k) = N1 1\nka , (S4)\nm2(k) = N2 e\u2212b(k\u22121)\nka , (S5)\nm3(k) = N3 (N\u0304 + 1\u2212 k)\u03b1\nka , (S6)\nm4(k) = N4 (N\u0304 + 1\u2212 k)\u03b1e\u2212b(k\u22121)\nka , (S7) m5(k) = N5 {\n1 k k \u2264 kc ka\u22121c ka k > kc\n(S8)\nwhere Ni are normalization factors, depending on the parameters a, b, and \u03b1 of the different models, and N\u0304 is the total number of words.\nIn Fig. 9 we compare the fit of these distributions with the observed curves. It can be seen that none of the distributions reproduces closely the dataset. We calculated for all fits the \u03c72 test with similar results. The best value corresponds to the fit proposed in [15], namely the double Zipf model (equation (S8)). In all cases we studied the p-value of the data, needed for an appropriate interpretation of the goodness of the fit. In all cases, that is for all years, all languages and all models, this number was smaller than machine precision. This shows that none of these models captures satisfactorily the data behavior.\nThe origin of some of these models is similar. The following discussion shows how they can be encompassed in a common formulation.\n15\nGiven a set of words forming a text, one can evaluate the number of times N(k, t) that a certain word appears with the rank k at time t. If B(k)and D(k) denote, respectively, the probability per unit time that a word enters or leaves the rank k, we have:\n\u2202 \u2202t N(k, t) = {\u03be(k)\u2212 F [\u03a3(t)]}N(k, t)\n+ { D(k + 1)N(k + 1, t) +B(k \u2212 1)N(k \u2212 1, t)\u2212 [D(k) +B(k)]N(k, t) } . (S9)\nHere the two terms on the r.h.s. within the first curly brackets describe, respectively, the local growth rate and the overall decrease rate acting on N(k, t). The total number of words at a given time t is \u03a3(t) and F is a function that determines global constraint features that refer to the total number of words. The terms within the second curly brackets, indicate the balance arising from the birth B(k) and death D(k) contributions of first neighbor words with k \u00b1 1 ranks at time t. If we consider the total number of words at a given time t to be a fixed quantity,\n\u03a3(t) = \u2211 k N(k, t) (S10)\nwe can define the probability density of finding a word with rank k, or relative frequency distribution, by\nn(k, t) \u2261 N(k, t) \u03a3(t) . (S11)\nSubstitution of equation (S10) and equation (S11) into equation (S9) leads to\nd dt \u03a3(t) = \u3008\u03be(k)\u3009 \u2212 F [\u03a3(t)]\u03a3(t), (S12)\n16\nwhere the bracket indicates a sum over all k weighted by n(k, t). We assume, for simplicity, that \u3008\u03be(k)\u3009 is a linear function of the number of edges k, so that \u3008\u03be(k)\u3009 = \u03be0 + \u03be1\u3008k\u3009, where \u03be0 and \u03be1 are constants. Then equation (S9) reduces to the following master equation for a one step process:\nn\u0307(k, t) = V (k)n(k, t)\n+ {D(k + 1)n(k + 1, t) +B(k \u2212 1)n(k \u2212 1, t)\u2212 [D(k) +B(k)]n(k, t)} , (S13)\nwhere n\u0307(k, t) \u2261 \u2202n(k, t)/\u2202t and the effective potential V (k) \u2261 \u03be(k)\u2212 \u3008\u03be(k)\u3009 has the property\n\u3008V (k)\u3009 = 0. (S14)\nIn what follows we shall only consider the case \u03be(k) = \u03be0, so equation (S13) reduces to the general form of the master equation for a one step process,\nn\u0307 (k, t) = D (k + 1)n (k + 1, t) +B (k \u2212 1)n (k \u2212 1, t)\u2212 [D(k) +B(k)]n(k, t). (S15)\nIf the changes in k are small and we are only interested in solutions n (k, t) that vary slowly with k, then k may be treated as a continuous variable and we obtain the Fokker-Planck equation:\n\u2202n (k, t) \u2202t = \u2212 \u2202 \u2202k [g (k)n (k, t)] + 1 2\n\u22022\n\u2202k2 [f (k)n (k, t)] , (S16)\nwhere f(k) = B(k) + D(k) and g(k) = B(k) \u2212 D(k). For the stationary solutions m(k), we have the equation\ng(k)m(k) = 1\n2\nd\ndk [f(k)m(k)] . (S17)\nIf we approximate g(k)/f(k) by Pade\u0301 approximants gn(k)/fn(k) = A0 + \u2211n k=1 Ak (k+ck)\n, the stationary solution becomes\nm(k) = N exp (A0k) n\u220f k=1 (k + ck) \u2212Ak . (S18)\nIf we assume the simplest expression for D(k) and B (k) transition probabilities\nD(k) = \u03bb1(c1 + k)(N1 \u2212 k), (S19) B(k) = \u03bb2(c2 + k)(N2 \u2212 k) (S20)\nthen\nm(k) = N exp(A0k) ( N\u0304 \u2212 k )b (c\u0304+ k)a , (S21)\nwhere N\u0304 = 12 (N1 +N2), c\u0304 = 1 2 (c1 + c2) = 1, a = c1 \u2212 c2 + 1, and b = N1 \u2212 N2 \u2212 1. Also we must remember that in our case k starts at one. Then if A0 = b = 0, we have the Zipf model; when A0 6= 0 and b = 0, the \u03b3 model is gotten (equation (S5)); if A0 = 0 but b 6= 0 the \u03b2 model is obtained (equation (S6)); finally, if A0 and b are different from 0, we have the general \u03b2\u03b3 model (equation (S7)).\nThese and additional results could be obtained using the complex network language [53\u201355]. With respect to the distribution of equation (S8), the derivation given in [15] is based on the following assumptions. The existence of two word regimes: A language core containing words with low rank and do not affect the birth of new words, and the remaining high ranked words which reduce the probability of new words to be used.\n17\nS2 Variation of words in time Table S1 shows the most frequent words for the year 2000 with their translation and relative frequency. Notice that these are very similar across languages. Table S2 shows the most frequent nouns for the years 1700, 1800, 1900, and 2000. There are similarities across languages and across centuries, but also important differences.\n18\nT ab\nle 1.\nLo w es t\u2013 ra nk\nw or ds\nfo r se ve ra ll an\ngu ag\nes in\nbo ok\ns pu\nbl is he d du\nri ng\nth e ye ar\n20 00\n,t og\net he r w it h th ei r tr an\nsl at io n to\nE ng\nlis h an d th ei r re la ti ve fr eq ue nc y.\nra nk\nE ng\nlis h\nG er m an\nFr en ch\nIt al ia n\nSp an\nis h\nR us si an\n1 th e,\n0. 06 55 30\nde r,\nth e,\n0. 03 85 12\nde ,o\nf, 0. 05 72 25\ndi ,o\nf, 0. 04 15 18\nde ,o\nf, 0. 07 30 63\n\u0438, an\nd, 0. 05 39 61\n2 of ,0\n.0 36 76 9\ndi e,\nth e,\n0. 03 60 10\nla ,t he ,0\n.0 35 22 2\ne, an\nd, 0. 02 81 07\nla ,t he ,0\n.0 43 29 7\n\u0432, in ,0\n.0 53 92 2\n3 an\nd, 0. 02 92 89\nun d,\nan d,\n0. 02 80 87\net ,a\nnd ,0\n.0 24 46 6\nla ,t he ,0\n.0 20 30 8\nen ,i n,\n0. 02 90 59\n\u043d\u0430 ,o\nn, 0. 02 01 90\n4 to ,0\n.0 25 26 4\nin ,i n,\n0. 02 06 07\nle ,t he ,0\n.0 22 38 4\nch e,\nth at ,0\n.0 17 86 1\ny, an\nd, 0. 02 89 08\n\u043d\u0435 ,n\not ,0\n.0 17 33 4\n5 in ,0\n.0 21 76 9\nvo n,\nof ,0\n.0 11 27 7\nle s,\nth e,\n0. 02 10 76\nil, th e,\n0. 01 77 02\nel ,t he ,0\n.0 27 77 1\n\u0447\u0442 \u043e,\nw ha\nt, 0. 01 17 70\n6 a,\n0. 02 07 15\nde n,\nth e,\n0. 01 10 12\na\u0300, to ,0\n.0 19 95 1\nin ,i n,\n0. 01 73 57\nqu e,\nth at ,0\n.0 26 71 3\n\u043f\u043e ,b\ny, 0. 01 02 02\n7 is ,0\n.0 10 71 2\nzu ,t o,\n0. 01 04 88\nde s,\nof ,0\n.0 19 21 2\na, to ,0\n.0 14 06 7\na, to ,0\n.0 19 70 6\n\u043a, to ,0\n.0 08 55 9\n8 th at ,0\n.0 10 52 9\nde s,\nof ,0\n.0 10 10 2\nen ,i n,\n0. 01 43 34\nde l, of ,0\n.0 13 40 3\nlo s,\nth e,\n0. 01 80 39\n\u043a\u0430 \u043a,\nas ,0\n.0 08 02 7\n9 fo r,\n0. 00 89 75\nda s,\nth e,\n0. 00 98 06\ndu ,o\nf, 0. 01 29 91\nde lla\n,o f, 0. 01 08 76\nde l, of ,0\n.0 13 49 2\n\u0430, an\nd, 0. 00 77 45\n10 as ,0\n.0 07 39 6\nim ,i n th e,\n0. 00 74 18\nun ,a\n,0 .0 11 11 2\npe r,\nfo r,\n0. 01 04 80\nse ,o\nne se lf,\n0. 01 24 48\n\u043e, ab\nou t, 0. 00 68 24\n11 it ,0\n.0 06 83 2\nm it ,w\nit h,\n0. 00 74 03\nun e,\na, 0. 01 08 25\nun ,a\n,0 .0 09 94 9\nla s,\nth e,\n0. 01 22 94\n\u0438\u0437 ,o\nf, 0. 00 63 56\n12 w it h,\n0. 00 67 07\nsi ch ,i ts el f, 0. 00 73 37\nda ns ,i n,\n0. 01 01 45\nno n,\nno t,\n0. 00 86 45\npo r,\nby ,0\n.0 09 90 8\n\u0435\u0433 \u043e,\nhi s,\n0. 00 59 11\n13 w as ,0\n.0 06 57 6\nis t, is ,0\n.0 07 19 7\nqu e,\nth at ,0\n.0 09 89 6\nsi ,o\nne se lf,\n0. 00 85 15\nun ,a\n,0 .0 08 82 4\n\u0434\u043b \u044f,\nfo r,\n0. 00 58 22\n14 on\n,0 .0 06 28 9\nau f, on\n,0 .0 07 04 7\nqu i, w ho\n,0 .0 08 60 9\ne\u0300, is ,0\n.0 08 50 1\nco n,\nw it h,\n0. 00 84 69\n\u043e\u0442 ,f ro m ,0\n.0 05 76 9\n15 no\nt, 0. 00 59 70\nni ch t, no\nt, 0. 00 68 75\npa r,\nby ,0\n.0 07 49 4\nun a,\na, 0. 00 78 91\nun a,\na, 0. 00 78 63\n\u043e\u043d ,h\ne, 0. 00 55 38\n16 be\n,0 .0 05 67 1\nfu\u0308 r,\nfo r,\n0. 00 68 74\nes t, is ,0\n.0 07 25 8\nle ,t he ,0\n.0 07 85 2\nno ,n\no, 0. 00 75 47\n\u043d\u043e ,b\nut ,0\n.0 05 32 4\n17 by ,0\n.0 05 44 0\nei ne ,a\n,0 .0 06 75 7\npo ur ,f or ,0\n.0 07 02 7\ni, th e,\n0. 00 76 26\npa ra ,f or ,0\n.0 06 87 7\n\u044f, I, 0. 00 50 97\n18 i, 0. 00 52 12\nal s,\nas ,0\n.0 06 52 1\nil, it ,0\n.0 06 74 9\nco n,\nw it h,\n0. 00 67 34\nsu ,i ts ,0\n.0 06 59 7\n\u044d\u0442 \u043e,\nth is ,0\n.0 04 92 5\n19 ar e,\n0. 00 49 28\nde m ,t he ,0\n.0 05 72 3\nau ,t o th e,\n0. 00 64 29\nda ,f ro m ,0\n.0 06 25 8\nes ,i s,\n0. 00 60 86\n\u0437\u0430 ,f or ,0\n.0 04 62 3\n20 th is ,0\n.0 04 91 6\nau ch ,a\nls o,\n0. 00 56 30\na, ha\ns, 0. 00 55 04\nne l, in ,0\n.0 05 18 4\nal ,t o th e,\n0. 00 58 55\n\u0443, at ,0\n.0 03 86 2\n19\nFigs. 10\u201316 show rank trajectories of words for the languages studied, including our simulated language. It can be seen that the behavior is similar for all languages: words with low rank (heads) almost do not vary in time. Afterwards the variation in rank depends on the rank itself, approximating a scale-invariant random walk. Notice that there is a higher variation at all scales before 1850. Further work is required to measure how much this variation depends on having less data before 1850 and how much on language properties of the time.\nFig. 17 shows the distribution of relative flights for all languages. See main text for details."}, {"heading": "S3 Correlation of relative frequency changes", "text": "We studied the correlations of the relative frequency changes (flights), defined in the main text as\n\u2206t = (kt+1 \u2212 kt) /kt. (S22)\nWe shall use a normalized version of it:\ndt = \u2206t \u2212 \u3008\u2206t\u3009\u221a \u3008(\u2206t \u2212 \u3008\u2206t\u3009)2\u3009 , (S23)\nwhere \u3008\u00b7\u3009 denotes average over time. This normalization ensures that both \u3008dt\u3009 = 0 and \u3008d2t \u3009 = 1. The time correlation is given by C\u03c4 = \u3008dtdt+\u03c4 \u3009. (S24) In principle, this quantity also depends on t, but usually this dependence is very weak, as in this case, and one can ignore it.\nIn Fig. 18 we show the average of C\u03c4 , of 50 different ranks chosen randomly, for different languages, as well as for the simulated language. We note that the correlation is very small, except for \u03c4 = 0, where it is 1, due to the normalization chosen, and for \u03c4 = 1 where a negative value, typical of bounded sequences, is observed for the six languages studied here. The random Gaussian model reproduces well these correlations except at \u03c4 = 1.\n20\n21\n22\n23\n24\n25\n26\n27\n28"}], "references": [{"title": "Selective Studies and the Principle of Relative Frequency in Language", "author": ["GK Zipf"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1932}, {"title": "An informational theory of the statistical structure of language", "author": ["B Mandelbrot"], "venue": "Communication Theory, the Second London Symposium, London: Betterworth,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1953}, {"title": "The Evolution of Human Languages", "author": ["JA Hawkins", "M Gell-Mann"], "venue": "Proceedings of the Workshop on the Evolution of Human Languages, Held August,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Zipf\u2019s law and random texts", "author": ["R Ferrer i Cancho", "RV Sol\u00e9"], "venue": "Advances in Complex Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Zipf\u2019s law unzipped", "author": ["SK Baek", "S Bernhardsson", "P Minnhagen"], "venue": "New Journal of Physics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Emergence of Zipf\u2019s law in the evolution of communication", "author": ["B Corominas-Murtra", "J Fortuny", "RV Sol\u00e9"], "venue": "Phys Rev E", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Evolution of the most common English words and phrases over the centuries", "author": ["M Perc"], "venue": "Journal of The Royal Society Interface", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Power laws, Pareto distributions and Zipf\u2019s law", "author": ["ME Newman"], "venue": "Contemporary Physics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Power-law distributions in empirical data", "author": ["A Clauset", "CR Shalizi", "ME Newman"], "venue": "SIAM Review", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Das gesetz der bev\u00f6lkerungskonzentration", "author": ["F Auerbach"], "venue": "Petermanns Geographische Mitteilungen", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1913}, {"title": "A \u201claw\u201d of occurrences for words of low frequency", "author": ["AD Booth"], "venue": "Information and Control", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1967}, {"title": "Beyond the Zipf\u2013Mandelbrot law in quantitative linguistics", "author": ["MA Montemurro"], "venue": "Physica A: Statistical Mechanics and its Applications", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "A (2013) A scaling law beyond Zipf\u2019s law and its relation to Heaps", "author": ["F Font-Clos", "G Boleda", "Corral"], "venue": "law. New Journal of Physics", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Stochastic model for the vocabulary growth in natural languages", "author": ["M Gerlach", "EG Altmann"], "venue": "Phys Rev X", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Two regimes in the frequency of words and the origins of complex lexicons: Zipf\u2019s law revisited", "author": ["R Ferrer i Cancho", "RV Sol\u00e9"], "venue": "Journal of Quantitative Linguistics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Universals versus historical contingencies in lexical evolution", "author": ["V Bochkarev", "V Solovyev", "S Wichmann"], "venue": "Journal of The Royal Society Interface", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Estimating students\u2019 vocabulary sizes in foreign language teaching", "author": ["S Takala"], "venue": "Practice and Problems in Language Testing, Afinla,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1985}, {"title": "Haitian Creole: Grammar, Texts, Vocabulary. Philadelphia: American Folklore Society", "author": ["RA Hall"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1953}, {"title": "Pidgin and Creole Languages", "author": ["S Romaine"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Voice of America Special English Dictionary. About.com English as 2nd Language. URL http://esl.about.com/cs/reference/a/aavoa.htm", "author": ["K Beare"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Oxford Advanced Learner\u2019s Dictionary. Oxford, UK: Oxford University Press. URL http://www.oxfordlearnersdictionaries.com/wordlist/english/oxford3000/ ox3k_A-B", "author": ["AS Hornby"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Quantitative analysis of culture using millions of digitized", "author": ["JB Michel", "YK Shen", "AP Aiden", "A Veres", "MK Gray"], "venue": "books. Science", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Understanding semantic change of words over centuries", "author": ["DT Wijaya", "R Yeniterzi"], "venue": "Proceedings of the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Measuring the evolution of contemporary western popular music", "author": ["J Serr\u00e0", "\u00c1 Corral", "M Bogu\u00f1\u00e1", "M Haro", "JL Arcos"], "venue": "Scientific Reports", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Statistical laws governing fluctuations in word use from word birth to word death", "author": ["AM Petersen", "J Tenenbaum", "S Havlin", "HE Stanley"], "venue": "Scientific Reports", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Dynamics of ranking processes in complex systems", "author": ["N Blumm", "G Ghoshal", "Z Forr\u00f3", "M Schich", "G Bianconi"], "venue": "Physical Review Letters", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "The expression of emotions in 20th century books", "author": ["A Acerbi", "V Lampos", "P Garnett", "RA Bentley"], "venue": "PLoS ONE", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Self-organization of progress across the century of physics", "author": ["M Perc"], "venue": "Scientific Reports", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Complexity measurement of natural and artificial languages. Complexity Early View", "author": ["G Febres", "K Jaffe", "C Gershenson"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Hacia un modelo de diccionario monoling\u00fce del espa\u00f1ol para usuarios extranjeros", "author": ["H Hern\u00e1ndez"], "venue": "Actas del Primer Congreso Nacional de ASELE. pp. 159\u2013166. URL http://cvc.cervantes. es/ensenanza/biblioteca_ele/asele/pdf/01/01_0307.pdf", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1988}, {"title": "The synthetic modeling of language origins", "author": ["L Steels"], "venue": "Evolution of Communication", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1997}, {"title": "Natural language and natural selection", "author": ["S Pinker", "P Bloom"], "venue": "Behavioral and Brain Sciences", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1990}, {"title": "Function, Selection, and Innateness: The Emergence of Language Universals", "author": ["S Kirby"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1999}, {"title": "Innateness and culture in the evolution of language", "author": ["S Kirby", "M Dowman", "TL Griffiths"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Restrictions on biological adaptation in language evolution", "author": ["N Chater", "F Reali", "MH Christiansen"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "The evolution of language", "author": ["MA Nowak", "DC Krakauer"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1999}, {"title": "A self-organizing spatial vocabulary", "author": ["L Steels"], "venue": "Artificial Life", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1995}, {"title": "The emergence of grammar: Systematic structure in a new language", "author": ["W Sandler", "I Meir", "C Padden", "M Aronoff"], "venue": "Proceedings of the National Academy of Sciences of the United States of America", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "The origin and evolution of word order", "author": ["M Gell-Mann", "M Ruhlen"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Agent-Based Models of Strategies for the Emergence and Evolution of Grammatical Agreement", "author": ["K Beuls", "L Steels"], "venue": "PLoS ONE", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "The hidden geometry of complex, network-driven contagion phenomena", "author": ["D Brockmann", "D Helbing"], "venue": "Science", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Extension of Zipf\u2019s law to words and phrases", "author": ["LQ Ha", "EI Sicilia-Garcia", "J Ming", "FJ Smith"], "venue": "Proceedings of the 19th International Conference on Computational Linguistics (COLING", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2002}, {"title": "From centrality to temporary fame: Dynamic centrality in complex", "author": ["D Braha", "Y Bar-Yam"], "venue": "networks. Complexity", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2006}, {"title": "The Atlas of Economic Complexity: Mapping Paths to Prosperity", "author": ["R Hausmann", "CA Hidalgo", "S Bustos", "M Coscia", "A Simoes"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Adaptive networks: Theory, Models and Applications", "author": ["T Gross", "H Sayama"], "venue": "Understanding Complex Systems. Berlin Heidelberg: Springer. doi:10.1007/978-3-642-01284-6. URL http://dx.doi.org/10.1007/978-3-642-01284-6", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2009}, {"title": "Microdynamics in stationary complex networks", "author": ["A Gautreau", "A Barrat", "M Barth\u00e9lemy"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2009}, {"title": "A (2012) Activity driven modeling of time varying networks", "author": ["N Perra", "B Gon\u00e7alves", "R Pastor-Satorras", "Vespignani"], "venue": "Scientific Reports", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Statistical mechanics of complex networks", "author": ["R Albert", "AL Barab\u00e1si"], "venue": "Rev Mod Phys", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2002}, {"title": "Emergence of network structure in models of collective evolution and evolutionary dynamics", "author": ["HJ Jensen"], "venue": "Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Statistical studies of languages have become popular since the work of George Zipf [1] and have been refined with the availability of large data sets and the introduction of novel analytical models [2\u20137].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Introduction Statistical studies of languages have become popular since the work of George Zipf [1] and have been refined with the availability of large data sets and the introduction of novel analytical models [2\u20137].", "startOffset": 211, "endOffset": 216}, {"referenceID": 2, "context": "Introduction Statistical studies of languages have become popular since the work of George Zipf [1] and have been refined with the availability of large data sets and the introduction of novel analytical models [2\u20137].", "startOffset": 211, "endOffset": 216}, {"referenceID": 3, "context": "Introduction Statistical studies of languages have become popular since the work of George Zipf [1] and have been refined with the availability of large data sets and the introduction of novel analytical models [2\u20137].", "startOffset": 211, "endOffset": 216}, {"referenceID": 4, "context": "Introduction Statistical studies of languages have become popular since the work of George Zipf [1] and have been refined with the availability of large data sets and the introduction of novel analytical models [2\u20137].", "startOffset": 211, "endOffset": 216}, {"referenceID": 5, "context": "Introduction Statistical studies of languages have become popular since the work of George Zipf [1] and have been refined with the availability of large data sets and the introduction of novel analytical models [2\u20137].", "startOffset": 211, "endOffset": 216}, {"referenceID": 6, "context": "Introduction Statistical studies of languages have become popular since the work of George Zipf [1] and have been refined with the availability of large data sets and the introduction of novel analytical models [2\u20137].", "startOffset": 211, "endOffset": 216}, {"referenceID": 7, "context": "He proposed that ranked words follow a power law f \u223c 1/k, where k is the rank of the word\u2014the higher ranks corresponding to the least frequent words\u2014and f is the relative frequency of each word [8, 9].", "startOffset": 194, "endOffset": 200}, {"referenceID": 8, "context": "He proposed that ranked words follow a power law f \u223c 1/k, where k is the rank of the word\u2014the higher ranks corresponding to the least frequent words\u2014and f is the relative frequency of each word [8, 9].", "startOffset": 194, "endOffset": 200}, {"referenceID": 9, "context": "This regularity of languages and other social and physical phenomena had been noticed beforehand, at least by Jean-Baptiste Estoup [10] and Felix Auerbach [11], but it is now known as Zipf\u2019s law.", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "As a consequence, several variations have been proposed [12\u201315].", "startOffset": 56, "endOffset": 63}, {"referenceID": 11, "context": "As a consequence, several variations have been proposed [12\u201315].", "startOffset": 56, "endOffset": 63}, {"referenceID": 12, "context": "As a consequence, several variations have been proposed [12\u201315].", "startOffset": 56, "endOffset": 63}, {"referenceID": 13, "context": "As a consequence, several variations have been proposed [12\u201315].", "startOffset": 56, "endOffset": 63}, {"referenceID": 13, "context": "Studies based on rank-frequency distributions of languages have proposed two word regimes [15, 16]: a \u201ccore\u201d where the most common words occur, which behaves as 1/k for small k, and another region for large k, which is identified by a change of exponent a in the distribution fit.", "startOffset": 90, "endOffset": 98}, {"referenceID": 14, "context": "Studies based on rank-frequency distributions of languages have proposed two word regimes [15, 16]: a \u201ccore\u201d where the most common words occur, which behaves as 1/k for small k, and another region for large k, which is identified by a change of exponent a in the distribution fit.", "startOffset": 90, "endOffset": 98}, {"referenceID": 14, "context": "Unfortunately, the point where exponent a changes varies widely across texts and languages, from 5000 [16] to 62,000 [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Unfortunately, the point where exponent a changes varies widely across texts and languages, from 5000 [16] to 62,000 [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "A recent study [17] measures the number of most frequent words which account for 75% of the Google books corpus.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "The core of human languages can be considered to be between 1500 and 3000 words (not counting different inflections of the same stems), based on basic vocabularies for foreigners [18], creole [19], and pidgin languages [20].", "startOffset": 179, "endOffset": 183}, {"referenceID": 17, "context": "The core of human languages can be considered to be between 1500 and 3000 words (not counting different inflections of the same stems), based on basic vocabularies for foreigners [18], creole [19], and pidgin languages [20].", "startOffset": 192, "endOffset": 196}, {"referenceID": 18, "context": "The core of human languages can be considered to be between 1500 and 3000 words (not counting different inflections of the same stems), based on basic vocabularies for foreigners [18], creole [19], and pidgin languages [20].", "startOffset": 219, "endOffset": 223}, {"referenceID": 19, "context": "For example, Voice of America\u2019s Special English [21] and Wikipedia in Simple English use about 1500 and ar X iv :1 50 5.", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "The Oxford Advanced Learner\u2019s Dictionary lists 3000 priority lexical entries [22].", "startOffset": 77, "endOffset": 81}, {"referenceID": 21, "context": "4 \u00d71011 words from Google Books [23], which contains about 4% of all books written until 2008.", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "Data sets such as this have allowed the study of \u201cculturomics\u201d: how cultural traits such as language have changed in time [24\u201330].", "startOffset": 122, "endOffset": 129}, {"referenceID": 23, "context": "Data sets such as this have allowed the study of \u201cculturomics\u201d: how cultural traits such as language have changed in time [24\u201330].", "startOffset": 122, "endOffset": 129}, {"referenceID": 24, "context": "Data sets such as this have allowed the study of \u201cculturomics\u201d: how cultural traits such as language have changed in time [24\u201330].", "startOffset": 122, "endOffset": 129}, {"referenceID": 25, "context": "Data sets such as this have allowed the study of \u201cculturomics\u201d: how cultural traits such as language have changed in time [24\u201330].", "startOffset": 122, "endOffset": 129}, {"referenceID": 26, "context": "Data sets such as this have allowed the study of \u201cculturomics\u201d: how cultural traits such as language have changed in time [24\u201330].", "startOffset": 122, "endOffset": 129}, {"referenceID": 27, "context": "Data sets such as this have allowed the study of \u201cculturomics\u201d: how cultural traits such as language have changed in time [24\u201330].", "startOffset": 122, "endOffset": 129}, {"referenceID": 28, "context": "Data sets such as this have allowed the study of \u201cculturomics\u201d: how cultural traits such as language have changed in time [24\u201330].", "startOffset": 122, "endOffset": 129}, {"referenceID": 29, "context": "If we agree that the rank diversity identifies the core (head and body) of English, then it can be argued that the size of the core of the other five languages studied is similar [31], which is also supported by the high similarity across languages in Fig 2.", "startOffset": 179, "endOffset": 183}, {"referenceID": 30, "context": "With this model we have produced the evolution of a random simulated language; see [32] for other approaches.", "startOffset": 83, "endOffset": 87}, {"referenceID": 31, "context": "However, the high-ranked diversity of language tails could be used in favor of adaptationist explanations as well [35], as the precise rank of tail", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "In recent years, explanations of human language relating biological evolution (genetically encoded innate properties) and learning (epigenetical adaptation) with culture have gained strength [36\u201338].", "startOffset": 191, "endOffset": 198}, {"referenceID": 33, "context": "In recent years, explanations of human language relating biological evolution (genetically encoded innate properties) and learning (epigenetical adaptation) with culture have gained strength [36\u201338].", "startOffset": 191, "endOffset": 198}, {"referenceID": 34, "context": "In recent years, explanations of human language relating biological evolution (genetically encoded innate properties) and learning (epigenetical adaptation) with culture have gained strength [36\u201338].", "startOffset": 191, "endOffset": 198}, {"referenceID": 35, "context": "Even so, few assumptions are necessary to explain some general aspects of the evolution of human languages [39].", "startOffset": 107, "endOffset": 111}, {"referenceID": 36, "context": "We do not address syntactic, semantic, and grammatical aspects of human language [40\u201343], which are certainly important.", "startOffset": 81, "endOffset": 88}, {"referenceID": 37, "context": "We do not address syntactic, semantic, and grammatical aspects of human language [40\u201343], which are certainly important.", "startOffset": 81, "endOffset": 88}, {"referenceID": 38, "context": "We do not address syntactic, semantic, and grammatical aspects of human language [40\u201343], which are certainly important.", "startOffset": 81, "endOffset": 88}, {"referenceID": 39, "context": "We do not address syntactic, semantic, and grammatical aspects of human language [40\u201343], which are certainly important.", "startOffset": 81, "endOffset": 88}, {"referenceID": 40, "context": "Thus, each cause has a negligible effect on the global result [44].", "startOffset": 62, "endOffset": 66}, {"referenceID": 41, "context": "In future work, it will be relevant to study the rank diversity of n-grams with n > 1 [45], other linguistic corpora and phenomena with dynamic rank distributions [27,46\u201348] and more generally with temporal networks [49\u201352].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "In future work, it will be relevant to study the rank diversity of n-grams with n > 1 [45], other linguistic corpora and phenomena with dynamic rank distributions [27,46\u201348] and more generally with temporal networks [49\u201352].", "startOffset": 163, "endOffset": 173}, {"referenceID": 42, "context": "In future work, it will be relevant to study the rank diversity of n-grams with n > 1 [45], other linguistic corpora and phenomena with dynamic rank distributions [27,46\u201348] and more generally with temporal networks [49\u201352].", "startOffset": 163, "endOffset": 173}, {"referenceID": 43, "context": "In future work, it will be relevant to study the rank diversity of n-grams with n > 1 [45], other linguistic corpora and phenomena with dynamic rank distributions [27,46\u201348] and more generally with temporal networks [49\u201352].", "startOffset": 163, "endOffset": 173}, {"referenceID": 44, "context": "In future work, it will be relevant to study the rank diversity of n-grams with n > 1 [45], other linguistic corpora and phenomena with dynamic rank distributions [27,46\u201348] and more generally with temporal networks [49\u201352].", "startOffset": 216, "endOffset": 223}, {"referenceID": 45, "context": "In future work, it will be relevant to study the rank diversity of n-grams with n > 1 [45], other linguistic corpora and phenomena with dynamic rank distributions [27,46\u201348] and more generally with temporal networks [49\u201352].", "startOffset": 216, "endOffset": 223}, {"referenceID": 46, "context": "In future work, it will be relevant to study the rank diversity of n-grams with n > 1 [45], other linguistic corpora and phenomena with dynamic rank distributions [27,46\u201348] and more generally with temporal networks [49\u201352].", "startOffset": 216, "endOffset": 223}, {"referenceID": 13, "context": "The best value corresponds to the fit proposed in [15], namely the double Zipf model (equation (S8)).", "startOffset": 50, "endOffset": 54}, {"referenceID": 47, "context": "These and additional results could be obtained using the complex network language [53\u201355].", "startOffset": 82, "endOffset": 89}, {"referenceID": 48, "context": "These and additional results could be obtained using the complex network language [53\u201355].", "startOffset": 82, "endOffset": 89}, {"referenceID": 13, "context": "With respect to the distribution of equation (S8), the derivation given in [15] is based on the following assumptions.", "startOffset": 75, "endOffset": 79}], "year": 2015, "abstractText": "Rank diversity of languages: Generic behavior in computational linguistics Germinal Cocho, Jorge Flores, Carlos Gershenson3,2,\u2217, Carlos Pineda, Sergio S\u00e1nchez 1 Instituto de F\u0301isica, Universidad Nacional Aut\u00f3noma de M\u00e9xico 2 Centro de Ciencias de la Complejidad, Universidad Nacional Aut\u00f3noma de M\u00e9xico 3 Instituto de Investigaciones en Matem\u00e1ticas Aplicadas y en Sistemas, Universidad Nacional Aut\u00f3noma de M\u00e9xico 4 Facultad de Ciencias, Universidad Nacional Aut\u00f3noma de M\u00e9xico \u2217 E-mail: cgg@unam.mx", "creator": "LaTeX with hyperref package"}}}