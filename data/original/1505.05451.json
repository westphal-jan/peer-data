{"id": "1505.05451", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2015", "title": "Fuzzy Least Squares Twin Support Vector Machines", "abstract": "Least Squares Twin Support Vector Machine (LSTSVM) is an extremely efficient and fast version of SVM algorithm for binary classification. LSTSVM combines the idea of Least Squares SVM and Twin SVM in which two non-parallel hyperplanes are found by solving two systems of linear equations. Although, the algorithm is very fast and efficient in many classification tasks, it is unable to cope with two features of real-world problems. First, in many real-world classification problems, it is almost impossible to assign data points to a single class. Second, data points in real-world problems may have different importance. In this study, we propose a novel version of LSTSVM based on fuzzy concepts to deal with these two characteristics of real-world data. The algorithm is called Fuzzy LSTSVM (FLSTSVM) which provides more flexibility than binary classification of LSTSVM. Two models are proposed for the algorithm. In the first model, a fuzzy membership value is assigned to each data point and the hyperplanes are optimized based on these fuzzy samples. In the second model we construct fuzzy hyperplanes to classify data. Finally, we apply our proposed FLSTSVM to an artificial as well as three real-world datasets. Results demonstrate that FLSTSVM obtains better performance than SVM and LSTSVM.", "histories": [["v1", "Wed, 20 May 2015 16:57:02 GMT  (1439kb,D)", "http://arxiv.org/abs/1505.05451v1", null], ["v2", "Fri, 22 Jul 2016 08:59:40 GMT  (10356kb,D)", "http://arxiv.org/abs/1505.05451v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["javad salimi sartakhti", "nasser ghadiri", "homayun afrabandpey", "narges yousefnezhad"], "accepted": false, "id": "1505.05451"}, "pdf": {"name": "1505.05451.pdf", "metadata": {"source": "CRF", "title": "Fuzzy Least Squares Twin Support Vector Machines", "authors": ["Javad Salimi Sartakhtia", "Nasser Ghadiri", "Homayun Afrabandpey", "Narges Yousefnezhad"], "emails": ["j.salimi@ec.iut.ac.ir"], "sections": [{"heading": null, "text": "Least Squares Twin Support Vector Machine (LSTSVM) is an extremely efficient and fast version of SVM algorithm for binary classification. LSTSVM combines the idea of Least Squares SVM and Twin SVM in which two nonparallel hyperplanes are found by solving two systems of linear equations. Although, the algorithm is very fast and efficient in many classification tasks, it is unable to cope with two features of real-world problems. First, in many realworld classification problems, it is almost impossible to assign data points to a single class. Second, data points in real-world problems may have different importance. In this study, we propose a novel version of LSTSVM based on fuzzy concepts to deal with these two characteristics of real-world data. The algorithm is called Fuzzy LSTSVM (FLSTSVM) which provides more flexibility than binary classification of LSTSVM. Two models are proposed for the algorithm. In the first model, a fuzzy membership value is assigned to each data point and the hyperplanes are optimized based on these fuzzy samples. In the second model we construct fuzzy hyperplanes to classify data. Finally, we apply our proposed FLSTSVM to an artificial as well as three real-world datasets. Results demonstrate that FLSTSVM obtains better performance than SVM and LSTSVM.\nKeywords: Machine learning, Fuzzy least squares twin support vector machine, Fuzzy hyperplane, SVM."}, {"heading": "1. Inroduction", "text": "Support Vector Machine (SVM) is a classification technique based on the idea of Structural Risk Minimization (SRM). It is a kernel-based classifier which was first introduced in 1995 by Vapnik and his colleagues, at AT&T Bell Laboratories [1]. The algorithm has been used in many classification tasks due to its\n\u2217Corresponding Author Email address: j.salimi@ec.iut.ac.ir (Javad Salimi Sartakhti)\nPreprint submitted to Elsevier\nar X\niv :1\n50 5.\n05 45\n1v 1\n[ cs\n.A I]\n2 0\nM ay\nsuccess in recognizing handwritten characters in which it outperformed precisely trained neural networks. Some of these tasks are: text classification [2], image classification [3], and bioinformatics [4, 5].\nOne of the newest versions of SVM is Least Squares Twin Support Vector Machine (LSTSVM) introduced in 2009 [6]. The algorithm combines the idea of Least Squares SVM (LSSVM) [7] and Twin SVM (TSVM) [8]. Although, in some classification tasks LSTSVM provides high accuracies [6] it still suffers from two main drawbacks. (I) In real-world applications, the data points may not be fully assigned to a class, while LSTSVM strictly assigns each data point to a class, (II) Although, in many classification tasks data points have different importance; LSTSVM considers the data points to have same priorities.\nMany real-world applications require different values of importance for input data. In such cases, the main concern is how to determine the final classes by assigning different importance degrees to training data. Moreover, the classifier should be designed in a way that it has the ability to separate the noises from data. A good approach to cope with these challenges is to use the concept of fuzzy functions.\nThe fuzzy theory is very useful for analyzing complex processes using standard quantitative methods or when the available information is interpreted uncertainly. A fuzzy function can represent uncertainty in data structures using fuzzy parameters. In the literature, the concepts of fuzzy function and fuzzy operations are introduced by different researchers [9, 10, 11, 12, 13]. A fuzzy function offers an efficient way of capturing the inexact nature of real-world problems.\nIn this paper we incorporate the concept of fuzzy set theory into the LSTSVM model. Unlike the standard LSTSVM, in the training phase the proposed fuzzy LSTSVM treats training data points according to their importance degrees. In the literature several approaches of applying fuzzy sets in SVM have been proposed [14, 15, 16, 17, 18]. The key feature of the proposed fuzzy LSTSVM is that it assigns fuzzy membership values to data points based on their importance degrees. In addition, we use fuzzy numbers to set the parameters of the fuzzy LSTSVM model such as the weight vector and the bias term. Using these two features, we proposed two models for fuzzy LSTSVM.\nThe rest of this paper is organized as follows. A brief review of basic concepts including the SVM, TSVM, and LSTSVM is presented in Section 2. The proposed models for fuzzy LSTSVM are introduced in Section 3. In section 4 we evaluate the proposed models, and finally section 5 concludes the paper."}, {"heading": "2. Basic Concepts", "text": "In this section a quick review of different versions of SVM is presented, namely the standard SVM, TSVM, and LSTSVM."}, {"heading": "2.1. Support Vector Machine", "text": "The main idea behind SVM is to minimize the classification error while preserving the maximum possible margin between classes. Suppose we are given a set of training data points xi \u2208 Rd, i = 1, \u00b7 \u00b7 \u00b7 ,n with labels yi \u2208 {\u22121,+1}. SVM seeks for a hyperplane with equation w.x+ b = 0 with the following constraints:\nyi(w.xi + b) \u2265 1, \u2200i. (1)\nwhere w is the weight vector. Such a hyperplane could be obtained by solving Eq. (2):\nMinimize f(x) = \u2016w\u20162\n2 (2)\nsubject to yi(w.xi + b)\u2212 1 \u2265 0\nThe geometric interpretation of this formulation is depicted in Fig. 1 for a toy example."}, {"heading": "2.2. Twin Support Vector Machine", "text": "In SVM only one hyperplane does the task of partitioning the samples into two groups of positive and negative classes. For the first time in 2007, Jayadeva et al. [8] proposed TSVM with the idea of using two hyperplanes in which samples are assigned to a class according to their distance from the hyperplanes. The main equations of TSVM are as follows:\nxiw (1) + b(1) = 0 (3)\nxiw (2) + b(2) = 0\nwhere w(i) and b(i) are the weight vector and bias term of the ith hyperplane, respectively. Each hyperplane represents the samples of its class. This concept is geometrically depicted in Fig. 2 for a toy example. In TSVM, the two hyperplanes are non-parallel. Each of them is closest to the samples of its own class and farthest from the samples of the opposite class [19, 20].\nLet us assume that A and B indicate two data points of class +1 and class \u22121, respectively. The two hyperplanes are obtained by solving Eq. (4) and Eq. (5).\nMinimize 1\n2 (Aw(1) + e1b (1))T (Aw(1) + e1b (1)) + p1e T 2 \u03be (4)\nw.r.t w(1), b(1), \u03be\nsubject to \u2212 (Bw(1) + e2b(1)) + \u03be \u2265 e2, \u03be \u2265 0\nMinimize 1\n2 (Bw(2) + e2b (2))T (Bw(2) + e2b (2)) + p2e T 1 \u03be (5)\nw.r.t w(2), b(2), \u03be\nsubject to Aw(2) + e1b (2) + \u03be \u2265 e1, \u03be \u2265 0\nIn these equations \u03be represents the slack variables, ei (i \u2208 {1, 2}) is a column vector of ones with desirable length, and p1 and p2 are penalty parameters."}, {"heading": "2.3. Least Squares Twin Support Vector Machine", "text": "LSTSVM [6, 21] is a binary classifier which combines the idea of LSSVM and TSVM. In other words, LSTSVM employs least squares of errors to modify the inequality constraints in TSVM to equality constraints and solves a set of linear equations rather than two Quadratic Programming Problems (QPPs). Experiments have shown that LSTSVM can considerably reduce the training time, while preserving competitive classification accuracy [7, 22]. Furthermore, since the time complexity of SVM is of order m3, where m is the number of constraints, theoretically when there are equal number of positive and negative samples, the speed of the algorithm increases by the factor of four compared to\nthe standard SVM.\nLSTSVM finds its hyperplanes by minimizing Eq. (6) and Eq. (7) which are linearly solvable. By solving Eq. (6) and Eq. (7), the values of w and b for each hyperplane are obtained according to Eq. (8) and Eq. (9).\nMinimize 1\n2 (Aw(1) + eb(1))T (Aw(1) + eb(1)) + p1 2 \u03beT \u03be (6)\nw.r.t w(1), b(1)\nsubject to \u2212 (Bw(1) + eb(1)) + \u03be = e\nMinimize 1\n2 (Bw(2) + eb(2))T (Bw(2) + eb(2)) + p2 2 \u03beT \u03be (7)\nw.r.t w(2), b(2)\nsubject to (Aw(2) + eb(2)) + \u03be = e\n[ w(1)\nb(1)\n] = \u2212(FTF + 1\np1 ETE)\u22121FT e (8)\n[ w(2)\nb(2)\n] = (ETE + 1\np2 FTF )\u22121ET e (9)\nwhere E = [ A e ] and F = [ B e ] and A, B, e and \u03be are already introduced in Section 2.2."}, {"heading": "3. Fuzzy Least Squares Twin Support Vector Machine", "text": "In this section, first we explain the importance of fuzzy classification and then we introduce two approaches for improving LSTSVM using the fuzzy sets theory. Basic notations used in this section are as follows: samples of the positive and negative classes are represented by matrices A and B, respectively. A contains m1 positive samples and B contains m2 negative samples. Membership degrees are represented by \u00b5 and slack variables are represented by vector \u03be. All equations will be presented in matrix form where for each matrix M , its transpose is represented by MT . e is a vector with arbitrary size and all its elements are equal to 1."}, {"heading": "3.1. Fuzzy Classification", "text": "In many real-world applications a sample in the training data does not belong exactly to a single class. Furthermore, in some applications it would be desirable for the new training samples to have higher importance than older ones. Given\nthe uncertainty of assigning such importance values, the fuzzy sets provide an elegant way to cope with this problem. We can define a fuzzy membership degree \u00b5i for each sample in the training data. The membership degree is a number between 0 and 1 which can be considered as a measure of influence of the sample on the final class. Therefore, a training sample with membership degree of \u00b5i influences class +1 by \u00b5i and influences class \u22121 by (1\u2212\u00b5i). In addition, using fuzzy membership functions, it is possible to assign a membership degree to each sample based on its entry time. Sequential learning [23] is another application which induces applying fuzzy concepts in classification algorithms such as SVM.\nIn 2008 Pei-Yi Hao introduced fuzzy SVM [18]. In his paper, he introduced two approaches, M1 and M2for applying fuzzy sets in SVM. In the first model, M1, he constructed a crisp hyperplane, and he also assigned a fuzzy membership to each data point. In the second model, M2, he constructed a fuzzy hyperplane to discriminate classes. In the following sections, we integrated the fuzzy set theory into the LSTSVM algorithm in accordance with [18]."}, {"heading": "3.2. Fuzzy LSTSVM: Model M1", "text": "In this model, fuzzy memberships values are assigned to data points such that noises and outliers get smaller memberships. Our goal is to construct two crisp hyperplanes to distinguish target classes. In order to use this model in LSTSVM algorithm, we rewrote Eq. (6) and Eq. (7) in the form of Eq. (10) and Eq. (11):\nMinimize J1 = 1\n2 (Aw(1) + eb(1))T (Aw(1) + eb(1)) + p1 2 \u00b5\u03beT \u03be (10)\nw.r.t w(1), b(1)\nsubject to \u2212 (Bw(1) + eb(1)) + \u03be = 0e\nMinimize J2 = 1\n2 (Bw(2) + eb(2))T (Bw(2) + eb(2)) + p2 2 \u00b5\u03beT \u03be (11)\nw.r.t w(2), b(2)\nsubject to \u2212 (Aw(2) + eb(2)) + \u03be = 0e\nEq. (10) and Eq. (11) represent equations of the positive and the negative class hyperplanes, respectively. In these two equations the membership degree \u00b5 appears only as error coefficient.\nBy obtaining \u03be and substituting it in Eq. (10) and Eq. (11), the two equations are reformulated as Eq. (12) and Eq. (13).\nMinimize J1 = 1 2 \u2016Aw(1) + eb(1)\u20162 + p1 2 \u00b5\u2016Bw(1) + eb(1) + e\u20162 (12) w.r.t w(1), b(1)\nMinimize J2 = 1 2 \u2016Bw(2) + eb(2)\u20162 + p2 2 \u00b5\u2016Aw(2) + eb(2) + e\u20162 (13) w.r.t w(2), b(2)\nBy differentiating Eq. (12) and Eq. (13) with respect to w and b, we have:\n\u2202J1 \u2202w(1) = AT (Aw(1) + eb(1)) + p1\u00b5B T (Bw(1) + eb(1) + e) = 0e\n\u2202J1 \u2202b(1) = eT (Aw(1) + eb(1)) + p1\u00b5e T (Bw(1) + eb(1) + e) = 0\n\u2202J2 \u2202w(2) = BT (Bw(2) + eb(2)) + p2\u00b5A T (Aw(2) + eb(2) + e) = 0e\n\u2202J2 \u2202b(2) = eT (Bw(2) + eb(2)) + p2\u00b5e T (Aw(2) + eb(2) + e) = 0\nBy solving the above equations using some matrix algebra, we would have Eq. (14) and Eq. (15) as the equations of the hyperplanes J1 and J2, respectively.[ \u00b5BTB \u00b5BT e \u00b5eTB \u00b5m2 ] [ w(1) b(1) ] + 1 p1 [ ATA AT e eTA m1 ] [ w(1) b(1) ] = 0e (14) [ \u00b5ATA \u00b5AT e \u00b5eTA \u00b5m1 ] [ w(2) b(2) ] + 1 p2 [ BTB BT e eTB m2 ] [ w(2) b(2) ] = 0e (15)\nThese two equations can be represented as Eq. (16) and Eq. (17), respectively.[ w(1)\nb(1)\n] = [ \u00b5BTB + 1p1A TA \u00b5BT e+ 1p1A T e\n\u00b5eTB + 1p1e TA \u00b5m2 + 1 p1 m1e\n]\u22121 [ \u2212BT e \u2212m2 ] (16)\n[ w(2)\nb(2)\n] = [ \u00b5ATA+ 1p2B TB \u00b5AT e+ 1p2B T e\n\u00b5eTA+ 1p2e TB \u00b5m1 + 1 p2 m2e\n]\u22121 [ \u2212AT e \u2212m1 ] (17)\nOnce the values of w(1), b(1), w(2) and b(2) are obtained, a new data point is assigned to a class based on its distance from the hyperplane of the corresponding class."}, {"heading": "3.3. Fuzzy LSTSVM: Model M2", "text": "In this model, we construct fuzzy hyperplanes to discriminate the classes. In M2, all parameters of the model, even the components of weight vector w, are fuzzy numbers. For computational simplicity all parameters used in this work are restricted to a class of \u201dtriangular\u201d symmetric membership functions. For a symmetric triangular fuzzy number X = (o, r), o is the center and r is the width of the corresponding membership function.\nLet us assume W and B are the fuzzy weight vector and fuzzy bias term, respectively, where each component of W is shown by Wi = (wi, ci) and B = (b, d). Then the equation of a fuzzy hyperplane is defined as follows:\nW.x+B =< w1, c1 > .x1 + \u00b7 \u00b7 \u00b7+ < wn, cn > .xn+ < b, d >= 0 (18)\nTo find the fuzzy hyperplane for class +1 of our fuzzy LSTSVM, we rewrite Eq. (6) as:\nMinimize J = 1\n2 (Aw(1) + eb(1))T (Aw(1) + eb(1)) + p1 2 \u00b5\u03beT \u03be+ (19)\nM( 1\n2 \u2016c(1)\u20162 + d(1))\nw.r.t w(1), b(1), c(1), d(1)\nsubject to (\u3008Bw(1)\u3009+ eb(1)) = e\u2212 \u03be\nIn this equation, 12\u2016c (1)\u20162 + d(1) measures the vagueness of the model. As the vagueness of the model increases, the results would be more inexact. In Eq. (19) the parameter M is a control parameter chosen by the user. Also p12 \u00b5\u03be\nT \u03be determines the amount of least squares error, where \u00b5 is the membership degree of the positive sample and the vector \u03be is the slack variable vector. p1 is a trade-off parameter which controls the effect of the least squares error on the hyperplane.\nEq. (19) can be rewritten as Eq. (20).\nMinimize J = 1\n2 (Aw(1) + eb(1))T (Aw(1) + eb(1))+ (20)\np1 2 \u00b5(\u03be1 + \u03be2) T (\u03be1 + \u03be2) +M( 1 2 \u2016c(1)\u20162 + d(1))\nw.r.t w(1), b(1), c(1), d(1)\nsubject to (\u3008Bw(1)\u3009+ eb(1)) + (\u3008Bc(1)\u3009+ ed(1)) = 0e\u2212 \u03be1 (\u3008Bw(1)\u3009+ eb(1))\u2212 (\u3008Bc(1)\u3009+ ed(1)) = 0e\u2212 \u03be2\nEq. (20) is rewritten as Eq. (21)\nMinimize J = 1\n2 \u2016\u3008Aw(1)\u3009+ eb(1) + \u3008Ac(1)\u3009+ ed(1)\u20162+ (21)\np1\u00b5\u2016\u3008Bw(1)\u3009+ eb(1) + e\u2016+M( 1\n2 \u2016c(1)\u20162 + d(1))\nw.r.t w(1), b(1), c(1), d(1)\nSetting the derivation of Eq. (21) with respect to w(1), b(1), c(1) and d(1) equal to zero, one gets\n\u2202J\n\u2202w(1) = AT (\u3008Aw(1)\u3009+ eb(1) + \u3008Ac(1)\u3009+ ed(1)) + p1\u00b5BT (\u3008Bw(1)\u3009+ eb(1) + e) = 0e\n\u2202J\n\u2202b(1) = eT (\u3008Aw(1)\u3009+ eb(1) + \u3008Ac(1)\u3009+ ed(1)) + p1\u00b5eT (\u3008Bw(1)\u3009+ eb(1) + e) = 0\n\u2202J\n\u2202c(1) = AT (\u3008Aw(1)\u3009+ eb(1) + \u3008Ac(1)\u3009+ ed(1)) +Mc(1) = 0\n\u2202J\n\u2202d(1) = AT (\u3008Aw(1)\u3009+ eb(1) + \u3008Ac(1)\u3009+ ed(1)) +M = 0e\nAfter solving the above equations, the below system would appear: 1 p1 ATA 1p1A T e 1p1A TA 1p1A T e 1 p1 eTA 1p1m1 1 p1 eTA 1p1m1\nATA AT e ATA AT e eTA m1 e TA m1\n  w(1) b(1)\nc(1) d(1)\n (22)\n+  \u00b5BTB \u00b5BT e 0 0 \u00b5eTB \u00b5m2 0 0\n0 0 eMeT 0 0 0 0 0\n  w(1) b(1)\nc(1) d(1)  = 0e Eq. (22) can be rewritten in the form of Eq. (23).  w(1) b(1)\nc(1) d(1)  = (23)  1 p1 ATA+ \u00b5BTB 1p1A T e+ \u00b5BT e 1p1A TA 1p1A T e 1 p1 eTA+ \u00b5eTB 1p1m1 + \u00b5m2 1 p1 eTA 1p1m1\nATA AT e ATA+ eMeT AT e eTA m1 e TA m1\n \u22121 \n\u00b5BT e \u00b5 0 M  Up to now we have found all the necessary parameters of the first fuzzy hyperplane. By substituting values of these parameters in Eq. (18), we can obtain\nthe equation of the first fuzzy hyperplane.\nFor the second hyperplane, the equations are as follows:\nMinimize J = 1\n2 (Bw(2) + eb(2))T (Bw(2) + eb(2)) + p2 2 \u00b5\u03beT \u03be (24)\n+M( 1\n2 \u2016c(2)\u20162 + d(2))\nw.r.t w(2), b(2), c(2), d(2)\nsubject to (\u3008Aw(2)\u3009+ eb(2)) + \u03be = 0\nMinimize J = 1\n2 (Bw(2) + eb(2))T (Bw(2) + eb(2))+ (25)\np2 2 \u00b5(\u03be1 + \u03be2) T (\u03be1 + \u03be2) +M( 1 2 \u2016c(2)\u20162 + d(2))\nw.r.t w(2), b(2), c(2), d(2)\nsubject to (\u3008Aw(2)\u3009+ eb(2)) + (\u3008Ac(2)\u3009+ ed(2)) + \u03be1 = 0e (\u3008Aw(2)\u3009+ eb(2)) + (\u3008Ac(2)\u3009+ ed(2))\u2212 \u03be2 = 0e\nMinimize J = 1\n2 \u2016\u3008Bw(2)\u3009+ eb(2) + \u3008Bc(2)\u3009+ ed(2)\u20162+ (26)\np2\u00b5\u2016\u3008Aw(2)\u3009+ eb(2) + e\u2016+M( 1\n2 \u2016c(2)\u20162 + d(2))\nw.r.t w(2), b(2), c(2), d(2)\n\u2202J\n\u2202w(2) = BT (\u3008Bw(2)\u3009+ eb(2) + \u3008Bc(2)\u3009+ ed(2)) + p2\u00b5AT (\u3008Aw(2)\u3009+ eb(2) + e) = 0e\n\u2202J\n\u2202b(2) = eT (\u3008Bw(2)\u3009+ eb(2) + \u3008Bc(2)\u3009+ ed(2)) + p2\u00b5eT (\u3008Aw(2)\u3009+ eb(2) + e) = 0\n\u2202J\n\u2202c(2) = BT (\u3008Bw(2)\u3009+ eb(2) + \u3008Bc(2)\u3009+ ed(2)) +Mc(2) = 0\n\u2202J\n\u2202d(2) = BT (\u3008Bw(2)\u3009+ eb(2) + \u3008Bc(2)\u3009+ ed(2)) +M = 0e  1 p2 BTB 1p2B T e 1p2B TB 1p2B T e 1 p2 eTB 1p2m2 1 p2 eTB 1p2m2\nBTB BT e BTB BT e eTB m2 e TB m2\n  w(2) b(2)\nc(2) d(2)\n (27)\n+  \u00b5ATA \u00b5AT e 0 0 \u00b5eTA \u00b5m1 0 0\n0 0 eMeT 0 0 0 0 0\n  w(2) b(2)\nc(2) d(2)\n = 0e\n w(2) b(2)\nc(2) d(2)  = (28)  \u00b5ATA+ 1p2B TB \u00b5AT e+ 1p2B T e 1p2B TB 1p2B T e \u00b5eTA+ 1p2 e TB \u00b5m1 + 1 p2 m2 1 p2 eTB 1p2m2\nBTB BT e eMeT +BTB BT e eTB m2 e TB m2\n \u22121 \n\u00b5AT e \u00b5 0 M  By solving Eq. (28) and finding the values of the parameters w(2), b(2), c(2) and d(2), the equation of the second fuzzy hyperplane can be obtained using Eq. (18). After finding the equations of the two fuzzy hyperplanes, the fuzzy distance between a given test data point and the fuzzy hyperplanes should be calculated. Definition 1 shows how find the fuzzy distance between a data point and a fuzzy hyperplane.\nDefinition 1: \u2206 = (\u03b4, \u03b3) is the fuzzy distance between a data point x0 = (x01, \u00b7 \u00b7 \u00b7 , x0n) and the fuzzy hyperplane W.x+B, where \u03b4 = |w1x01+\u00b7\u00b7\u00b7+wnx0n+b|\u2016W\u2016 and \u03b3 = |(w1+c1)x01+\u00b7\u00b7\u00b7+(wn+cn)x0n|\u2016W\u2016 .\nBy finding fuzzy distances between the data point and the fuzzy hyperplanes, it is necessary to define a fuzzy membership function which determines the membership degree of the data point in each fuzzy hyperplane. Let us assume that \u22061 = (\u03b41, \u03b31) and \u22062 = (\u03b42, \u03b32) are fuzzy distances between a data point and the two hyperplanes H1 and H2, respectively. Then for an input data x0, the degree that x0 belongs to hyperplane H1 is defined by the following membership function (by finding membership degrees for H1, membership degrees for H2 are easily obtainable):\n\u00b51(x0) =  1\u2212 \u03b41+\u03b31\u03b41+\u03b31+\u03b42+\u03b32 \u03b41 \u2265 \u03b31, \u03b42 \u2265 \u03b32, 1\u2212 \u03b41\u03b41+\u03b42+\u03b32 \u03b41 < \u03b31, \u03b42 \u2265 \u03b32, 1\u2212 \u03b41+\u03b31\u03b41+\u03b31+\u03b42 \u03b41 \u2265 \u03b31, \u03b42 < \u03b32 1\u2212 \u03b41\u03b41+\u03b42 \u03b41 < \u03b31, \u03b42 < \u03b32,\n(29)"}, {"heading": "4. Numerical Experiments", "text": "To evaluate the performance of our proposed algorithm, we investigate its classification accuracy on both artificial and benchmark datasets. All experiments are carried out in Matlab 7.9 (R2009b) environment on a PC with Intel processor (2.9 GHz) with 2 GB RAM. The Accuracy used to evaluate a classifier is defined as: (TP + TN)/(TP + FP + TN + FN), where TP , TN ,\nFP and FN are the number of true positive, true negative, false positive and false negative, respectively. Also the accuracies are measured by the standard 10-fold cross-validation methodology [24]. In our implementation, we focus on comparison between SVM, LSTSVM and FLSTSVM with model M2."}, {"heading": "4.1. Experiment on Artificial Dataset", "text": "We first consider a two dimensional \u201cXor\u201d dataset, which is a very common dataset for evaluating the effectiveness of SVM based algorithms, shown in Fig. 3. This hand-made dataset consists of 132 records belonging to two classes. Each record has two features: a class and a value which determines how much the record belongs to the class. The red circles denote the data points of positive class, while the blue circles belong to the negative class.\nTable 1 shows the results of applying SVM, LSTSVM and FLSTSVM algorithms to the dataset. It should be noted that in this paper, only the linear version of all the three algorithms are studied. The obtained values for accuracies of all three algorithms are fully justifiable. In SVM there is only one hyperplane responsible of classifying data and because the dataset is defined as a two-dimensional space, this hyperplane would be a line which is shown in figure 4a. In LSTSVM algorithm we have two lines for classification. As mentioned in Section 2.3, these lines should be the nearest to their corresponding class records and farthest from the opposite class records. Figure 4b shows these lines for LSTSVM. Because the data points overlap and\nthey don\u2019t exactly lie on a line, the LSTSVM algorithm has still a large amount of error although it has higher accuracy compared to the SVM. FLSTSVM also has two lines responsible for classifying data with the difference that these two lines are not crisp. Figure 4c shows the fuzzy lines of FLSTSVM. To show the fuzzy nature of each line, we have used multiple lines. As shown in the figure, these fuzzy lines discriminate the data points better than SVM and LSTSVM. Therefore FLSTSVM provides higher accuracy compared to the other two algorithms."}, {"heading": "4.2. Experiments on Benchmark Datasets", "text": "We also performed experiments on a collection of four benchmark datasets form UCI machine learning repository [25]. These datasets are Heart-Statlog, Australian Credit Approval, Liver Disorder and Breast Cancer Wisconsin. These datasets represent a wide range of size (from 198 to 690) and features (from 7 to 34). Details of the four datasets are listed in Table 2. Also, Table 3 lists the results of each algorithm. As it is shown in the talbe, FLSTSVM has higher accuracies compared to the other two algorithms. It should be noted once more that, in these experiments only the linear version of the FLSTSVM is considered (and so for the other two algorithms). We claim that the non-linear version of the proposed algorithm would outperform the non-linear version of SVM and LSTSVM with more meaningful differences."}, {"heading": "5. Conclusion", "text": "In this paper, we enriched LSTSVM classifier by incorporating the theory of fuzzy sets. We proposed two novel models for fuzzy LSTSVM. In the first\nmodel, M1, a fuzzy membership was assigned to each input point and the hyperplanes were optimized based on fuzzy importance degrees of samples. In the second model, M2, all parameters to be identified in LSTSVM are considered to be fuzzy. Also to discriminate the target class in M2, we construct two fuzzy hyperplanes. We carried out a series of experiments to analyze our classifier against SVM and LSTSVM. The results demonstrate that FLSTSVM obtains better accuracies that the other two algorithms. As our future work, we want to concentrate on non-linear version of fuzzy LSTSVM."}], "references": [{"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning 20 (3) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Vision-based rock-type classification of limestone using multi-class support vector machine", "author": ["S. Chatterjee"], "venue": "Applied Intelligence 39 (1) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine learning 46 (1-3) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Hepatitis disease diagnosis using a novel hybrid method based on support vector machine and simulated annealing (svm-sa)", "author": ["J.S. Sartakhti", "M.H. Zangooei", "K. Mozafari"], "venue": "Computer methods and programs in biomedicine 108 (2) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Least squares twin support vector machines for pattern classification", "author": ["M. Arun Kumar", "M. Gopal"], "venue": "Expert Systems with Applications 36 (4) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Least squares support vector machine classifiers", "author": ["J.A. Suykens", "J. Vandewalle"], "venue": "Neural processing letters 9 (3) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "S", "author": ["R. Khemchandani"], "venue": "Chandra, et al., Twin support vector machines for pattern classification, Pattern Analysis and Machine Intelligence, IEEE Transactions on 29 (5) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "The concept of a linguistic variable and its application to approximate reasoningi", "author": ["L.A. Zadeh"], "venue": "Information sciences 8 (3) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1975}, {"title": "Operations on fuzzy numbers", "author": ["D. Dubois", "H. Prade"], "venue": "International Journal of systems science 9 (6) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1978}, {"title": "On solving fuzzy mathematical relationships", "author": ["R.R. Yager"], "venue": "Information and Control 41 (1) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1979}, {"title": "Fuzzy sets and systems: theory and applications", "author": ["D.J. Dubois"], "venue": "Vol. 144, Academic press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1980}, {"title": "Fuzzy support vector machines for pattern classification", "author": ["T. Inoue", "S. Abe"], "venue": "in: Neural Networks, 2001. Proceedings. IJCNN\u201901. International Joint Conference on, Vol. 2, IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Fuzzy support vector machines", "author": ["C.-F. Lin", "S.-D. Wang"], "venue": "Neural Networks, IEEE Transactions on 13 (2) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Fuzzy rule extraction from support vector machines", "author": ["A.C. Chaves", "M.M.B. Vellasco", "R. Tanscheit"], "venue": "in: Hybrid Intelligent Systems, 2005. HIS\u201905. Fifth International Conference on, IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Interval type-2 fuzzy weighted support vector machine learning for energy efficient biped walking", "author": ["L. Wang", "Z. Liu", "C. Chen", "Y. Zhang"], "venue": "Applied Intelligence 40 (3) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Fuzzy one-class support vector machines", "author": ["P.-Y. Hao"], "venue": "Fuzzy Sets and Systems 159 (18) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse least square twin support vector machine with adaptive norm", "author": ["Z. Zhang", "L. Zhen", "N. Deng", "J. Tan"], "venue": "Applied Intelligence 41 (4) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "An overview on twin support vector machines", "author": ["S. Ding", "J. Yu", "B. Qi", "H. Huang"], "venue": "Artificial Intelligence Review 42 (2) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Least squares recursive projection twin support vector machine for classification", "author": ["Y.-H. Shao", "N.-Y. Deng", "Z.-M. Yang"], "venue": "Pattern Recognition 45 (6) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "1-norm least squares twin support vector machines", "author": ["S. Gao", "Q. Ye", "N. Ye"], "venue": "Neurocomputing 74 (17) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequence learning", "author": ["B.A. Clegg", "G.J. DiGirolamo", "S.W. Keele"], "venue": "Trends in cognitive sciences 2 (8) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Pattern classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "John Wiley & Sons", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "It is a kernel-based classifier which was first introduced in 1995 by Vapnik and his colleagues, at AT&T Bell Laboratories [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "Some of these tasks are: text classification [2], image classification [3], and bioinformatics [4, 5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "Some of these tasks are: text classification [2], image classification [3], and bioinformatics [4, 5].", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "Some of these tasks are: text classification [2], image classification [3], and bioinformatics [4, 5].", "startOffset": 95, "endOffset": 101}, {"referenceID": 4, "context": "Some of these tasks are: text classification [2], image classification [3], and bioinformatics [4, 5].", "startOffset": 95, "endOffset": 101}, {"referenceID": 5, "context": "One of the newest versions of SVM is Least Squares Twin Support Vector Machine (LSTSVM) introduced in 2009 [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "The algorithm combines the idea of Least Squares SVM (LSSVM) [7] and Twin SVM (TSVM) [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "The algorithm combines the idea of Least Squares SVM (LSSVM) [7] and Twin SVM (TSVM) [8].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "Although, in some classification tasks LSTSVM provides high accuracies [6] it still suffers from two main drawbacks.", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "In the literature, the concepts of fuzzy function and fuzzy operations are introduced by different researchers [9, 10, 11, 12, 13].", "startOffset": 111, "endOffset": 130}, {"referenceID": 9, "context": "In the literature, the concepts of fuzzy function and fuzzy operations are introduced by different researchers [9, 10, 11, 12, 13].", "startOffset": 111, "endOffset": 130}, {"referenceID": 10, "context": "In the literature, the concepts of fuzzy function and fuzzy operations are introduced by different researchers [9, 10, 11, 12, 13].", "startOffset": 111, "endOffset": 130}, {"referenceID": 11, "context": "In the literature, the concepts of fuzzy function and fuzzy operations are introduced by different researchers [9, 10, 11, 12, 13].", "startOffset": 111, "endOffset": 130}, {"referenceID": 12, "context": "In the literature several approaches of applying fuzzy sets in SVM have been proposed [14, 15, 16, 17, 18].", "startOffset": 86, "endOffset": 106}, {"referenceID": 13, "context": "In the literature several approaches of applying fuzzy sets in SVM have been proposed [14, 15, 16, 17, 18].", "startOffset": 86, "endOffset": 106}, {"referenceID": 14, "context": "In the literature several approaches of applying fuzzy sets in SVM have been proposed [14, 15, 16, 17, 18].", "startOffset": 86, "endOffset": 106}, {"referenceID": 15, "context": "In the literature several approaches of applying fuzzy sets in SVM have been proposed [14, 15, 16, 17, 18].", "startOffset": 86, "endOffset": 106}, {"referenceID": 16, "context": "In the literature several approaches of applying fuzzy sets in SVM have been proposed [14, 15, 16, 17, 18].", "startOffset": 86, "endOffset": 106}, {"referenceID": 7, "context": "[8] proposed TSVM with the idea of using two hyperplanes in which samples are assigned to a class according to their distance from the hyperplanes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Each of them is closest to the samples of its own class and farthest from the samples of the opposite class [19, 20].", "startOffset": 108, "endOffset": 116}, {"referenceID": 18, "context": "Each of them is closest to the samples of its own class and farthest from the samples of the opposite class [19, 20].", "startOffset": 108, "endOffset": 116}, {"referenceID": 5, "context": "Least Squares Twin Support Vector Machine LSTSVM [6, 21] is a binary classifier which combines the idea of LSSVM and TSVM.", "startOffset": 49, "endOffset": 56}, {"referenceID": 19, "context": "Least Squares Twin Support Vector Machine LSTSVM [6, 21] is a binary classifier which combines the idea of LSSVM and TSVM.", "startOffset": 49, "endOffset": 56}, {"referenceID": 6, "context": "Experiments have shown that LSTSVM can considerably reduce the training time, while preserving competitive classification accuracy [7, 22].", "startOffset": 131, "endOffset": 138}, {"referenceID": 20, "context": "Experiments have shown that LSTSVM can considerably reduce the training time, while preserving competitive classification accuracy [7, 22].", "startOffset": 131, "endOffset": 138}, {"referenceID": 21, "context": "Sequential learning [23] is another application which induces applying fuzzy concepts in classification algorithms such as SVM.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "In 2008 Pei-Yi Hao introduced fuzzy SVM [18].", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "In the following sections, we integrated the fuzzy set theory into the LSTSVM algorithm in accordance with [18].", "startOffset": 107, "endOffset": 111}, {"referenceID": 22, "context": "Also the accuracies are measured by the standard 10-fold cross-validation methodology [24].", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "Least Squares Twin Support Vector Machine (LSTSVM) is an extremely efficient and fast version of SVM algorithm for binary classification. LSTSVM combines the idea of Least Squares SVM and Twin SVM in which two nonparallel hyperplanes are found by solving two systems of linear equations. Although, the algorithm is very fast and efficient in many classification tasks, it is unable to cope with two features of real-world problems. First, in many realworld classification problems, it is almost impossible to assign data points to a single class. Second, data points in real-world problems may have different importance. In this study, we propose a novel version of LSTSVM based on fuzzy concepts to deal with these two characteristics of real-world data. The algorithm is called Fuzzy LSTSVM (FLSTSVM) which provides more flexibility than binary classification of LSTSVM. Two models are proposed for the algorithm. In the first model, a fuzzy membership value is assigned to each data point and the hyperplanes are optimized based on these fuzzy samples. In the second model we construct fuzzy hyperplanes to classify data. Finally, we apply our proposed FLSTSVM to an artificial as well as three real-world datasets. Results demonstrate that FLSTSVM obtains better performance than SVM and LSTSVM.", "creator": "TeX"}}}