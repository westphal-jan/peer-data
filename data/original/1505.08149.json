{"id": "1505.08149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2015", "title": "Modeling meaning: computational interpreting and understanding of natural language fragments", "abstract": "In this introductory article we present the basics of an approach to implementing computational interpreting of natural language aiming to model the meanings of words and phrases. Unlike other approaches, we attempt to define the meanings of text fragments in a composable and computer interpretable way. We discuss models and ideas for detecting different types of semantic incomprehension and choosing the interpretation that makes most sense in a given context. Knowledge representation is designed for handling context-sensitive and uncertain / imprecise knowledge, and for easy accommodation of new information. It stores quantitative information capturing the essence of the concepts, because it is crucial for working with natural language understanding and reasoning. Still, the representation is general enough to allow for new knowledge to be learned, and even generated by the system. The article concludes by discussing some reasoning-related topics: possible approaches to generation of new abstract concepts, and describing situations and concepts in words (e.g. for specifying interpretation difficulties).", "histories": [["v1", "Fri, 29 May 2015 19:06:42 GMT  (1464kb)", "http://arxiv.org/abs/1505.08149v1", "26 pages"], ["v2", "Wed, 29 Jun 2016 10:35:11 GMT  (1493kb)", "http://arxiv.org/abs/1505.08149v2", "26 pages"]], "COMMENTS": "26 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael kapustin", "pavlo kapustin"], "accepted": false, "id": "1505.08149"}, "pdf": {"name": "1505.08149.pdf", "metadata": {"source": "CRF", "title": "Modeling of the meaning: computational interpreting and understanding of natural language fragments", "authors": ["Michael Kapustin", "Pavlo Kapustin"], "emails": ["michael.kapustin@gmail.com", "pkapustin@gmail.com"], "sections": [{"heading": null, "text": "computational interpreting of natural language aiming to model the meanings of words and phrases. Unlike other approaches, we attempt to define the meanings of text fragments in a composable and computer interpretable way. We discuss models and ideas for detecting different types of semantic incomprehension and choosing the interpretation that makes most sense in a given context. Knowledge representation is designed for handling context-sensitive and uncertain / imprecise knowledge, and for easy accommodation of new information. It stores quantitative information capturing the essence of the concepts, because it is crucial for working with natural language understanding and reasoning. Still, the representation is general enough to allow for new knowledge to be learned, and even generated by the system. The article concludes by discussing some reasoning-related topics: possible approaches to generation of new abstract concepts, and describing situations and concepts in words (e.g. for specifying interpretation difficulties)."}, {"heading": "1 Introduction", "text": "Knowledge representations based on semantic networks and frames have been criticized for imprecision and ambiguity since [1]. Formal logic-based approaches, on the other hand, do not really allow for learning new knowledge easily, as they are dependent on people codifying it [2:5-7]. Neither semantic networks nor formal logic approaches handle uncertainty or fuzzy concepts like perceptions well [3]. More importantly, mentioned representations do not contain any quantitative, computer interpretable information about the concept and relation internal structure, or definitions1, making it virtually impossible to work with the meanings of concepts and phrases, and thus, severely restricting capabilities of the software.\nWe, on the other hand, tend to agree with the words of T. Winograd [4:1]: \u201cWe assume that a computer cannot deal reasonably with language unless it can understand the subject it is discussing\u201d. We do not believe it is possible to create a system with understanding abilities unless we can computationally distinguish comprehension from incomprehension. We won\u2019t be able to make the system express itself meaningfully unless we have a model for the meaning itself.\n1 By internal structure (or definitions) here we mean something allowing computer to understand what makes this concept different (or similar) to/from other concept (and how).\nThe ultimate problem of our interest (the one we obviously do not attempt to fully solve, but consider useful to guide the design of the framework) is conducting a dialog with a system using natural language (not necessarily syntactically and grammatically correct). We understand that one needs to be more realistic than many AI researchers were in the early seventies, but, on the other hand, some steps towards modeling of the meaning were already taken (e.g. L. Zadeh, starting from [3]), and we see no other choice as to continue small steps in this direction.\nThis introductory article presents the basics of our approach aiming to express natural language concepts in a quantitative, computer interpretable way, and discusses some examples. Primary focus of the approach is the ability to computationally analyze the meaning of natural language statements, so that the system can analyze whether they \u201cmake sense\u201d, and interpret them. We discuss modeling different parts of speech, working with different contexts, and draft the overall phrase interpretation process, including the choice of the most sensible interpretation (when several interpretations are possible). Further, we provide some examples of criteria for assessing comprehensibility. In the last chapter, we mention some other reasoning-related aspects: first, we discuss working with abstract concepts, and then conclude with a draft of an algorithm allowing to describe situations (or concepts known to the system) in \u201cown words\u201d. One practical use case for such description is communicating interpretation difficulties to the user."}, {"heading": "2 Related work", "text": "\u201cThe meaning of meaning and how to deal with meaning in formal and natural systems has been one of the great mysteries of intelligence - artificial or otherwise. It has been an issue from the earliest days of philosophy and logic, and it has become an engineering issue with the advent of computerized question answering systems, information retrieval systems, machine translation, speech understanding, intelligent agents, and other applications of natural language processing, knowledge representation, and artificial intelligence in general\u201d [5:75].\nComputational aspect of meaning seems to remain a mystery for the most part of it. There is fairly little research around quantitative modeling of the meaning of natural language constructs.\nIn Quantitative Fuzzy Semantics [3], L. Zadeh asks: \u201cCan the fuzziness of meaning be treated quantitatively, at least in principle?\u201d He suggests modeling concepts like \u201cyoung\u201d, \u201cclose to middleage\u201d and \u201cmiddle-aged\u201d as fuzzy sets.\n[6] discusses using linguistic variables whose values are words or sentences in a natural language.\n[7] suggests modeling linguistic hedges (e.g. \u201cvery\u201d, \u201cmore or less\u201d, \u201cmuch\u201d, \u201cessentially\u201d, \u201cslightly\u201d) as operators that act on the fuzzy set representing the meaning of its operand (e.g. operator \u201cvery\u201d acting on the meaning of operand \u201ctall man\u201d). Several operations for manipulating these fuzzy set representations are introduced, e.g. complementation, intersection, normalization, concentration, dilation, fuzzification, etc.\n[8] discusses using fuzzy sets for modeling natural language quantifiers like \u201cseveral\u201d, \u201cmost\u201d, \u201cnot many\u201d, \u201cclose to five\u201d, \u201capproximately ten\u201d, etc.\nTest-score semantics [9] and knowledge representation based on it [10] propose modeling the meaning of a proposition as a composition of meanings of words-elements. However, the semantics does not formalize the meaning enough (it is represented by a so-called test procedure) to allow handling it computationally: \u201cWhat is much more difficult, however, is to write a program which could construct an explanatory database and a test procedure without human assistance. This is a longer range problem whose complete solution must await the development of a substantially better\nunderstanding of natural languages and knowledge representation than we have at this juncture\u201d [9:33].\nGeneralized Constraint Language (GCL) [11, 12] is an evolution of test-score semantics and a basis for Computing With Words (CWW) [13, 14]. CWW aims to allow computations with words from natural language (instead of numbers), and it requires precisiation step to rewrite natural language elements in GCL. This step includes identification of constrained variables and constraining relations (and their types), together defining the meaning of the proposition [13:66-67]. This step is \u201cgenerally done by inspection\u201d [13:67]. In particular, no attempt is made to automatically relate the variables that are implicit in the proposition to the words in natural language. Also, same as in test-score semantics, there is no known way to programmatically generate explanatory database, that serves as the basis for precisiation [13:185].\nIn other words, CWW does not provide a way to handle the meaning computationally before it is precisiated. This may not be required for Computing With Words, but it is necessary for interpreting and understanding natural language, and this is what we are focusing on in this article."}, {"heading": "3 Approach overview", "text": "Very briefly (and informally), the approach could be outlined like the following.\nThe approach is based on fuzzy logic, as it is a very good instrument for working with different levels of truthness, and concepts with unclear boundaries, phenomena commonly occurring in knowledge coming from natural language, and commonsense knowledge in particular [10].\nAll knowledge is encoded in fuzzy properties (with values ranging from zero to one), each of them encoding an independent piece of information.\nContexts and context hierarchies are used for structuring knowledge and modeling its context sensitivity. Context is defined as a coordinate system: If N is the number of independent properties in a given context, then it is said that the context contains N axes, and the knowledge in this context is described as a fuzzy region in a N-dimensional unit hypercube.\nWe model phrases, words and other natural language fragments as region transforms that we call meaning-operators. For example, a specific interpretation of a phrase is a transformation of the source region (the region before interpretation) by the corresponding phrase operator. The result of the transformation is what we call resulting region (the region after interpretation).\nWe see natural language understanding as choosing the interpretation that makes most sense, using different heuristics.\nWe can assess the meaning of an operator during its composition. We can evaluate overall phrase comprehension when the phrase operator is being applied. Then we are considering both source and resulting regions, and potentially other factors (e.g. phrase mood).\nSimplified schematics of phrase interpretation is shown on Fig. 3-1."}, {"heading": "4 Basic concepts", "text": "This chapter is an informal introduction to the concepts and ideas that we will later use to define what we mean by \u201cmeaning\u201d, and how it can be used in the phrase interpretation process."}, {"heading": "4.1 Properties, context and regions", "text": "Let\u2019s assume that the state, or knowledge of a system can be completely described with a finite number of independent real parameters, or properties. Values of each property range from zero to one, so that one corresponds to maximal presence of the property, and zero corresponds to its complete absence. For example, if we would like to model vehicle speed, we could use property \u201cquickness\u201d2. \u201cZero\u201d would mean \u201cnot fast at all\u201d and \u201cone\u201d would mean \u201cas fast as it gets\u201d).\nWe are going to call some of the properties basic properties. Many of the basic properties will include values that can be directly \u201cperceived\u201d by the system. For example, if we were developing a robot with a built-in rangefinder, the property \u201crelative distance\u201d would have been perceived directly. The same is true for the property \u201crelative time\u201d, as long as the system has a built-in clock.\nThe rest of the properties we are going to call derived, with their meaning defined via other properties using the model described below.\n2 For simplicity of the examples, we are avoiding direct use of \u201cspeed\u201d, as it is a more complex concept.\nContext is a coordinate system consisting of axes that represent values of currently relevant properties, one axis per property. For example, when talking about movement speed, two of the relevant properties could be \u201crelative distance\u201d and \u201crelative time\u201d.\nIn the context\u2019s coordinate system we can define a fuzzy (in terms of fuzzy logic) shape: a region. Each point of the region is assigned a value between zero and one, describing this point\u2019s degree of membership.\nWe can use regions to express meaning of different concepts. For example, using axes t and s (\u201crelative time\u201d and \u201crelative distance\u201d), we can express regions, describing concepts \u201cfast\u201d and \u201cslow\u201d (Fig. 4-1).\nBased on existing regions, we can introduce new (derived) properties. For example, based on the region corresponding to the concept \u201cfast\u201d (Fig. 4-1), we can introduce a new property \u201cquickness\u201d.\nIt is natural to assume the values of \u201cquickness\u201d (x-axis on Fig. 4-2) be equal to the degrees of membership of its corresponding region\u2019s (\u201cfast\u201d) points (color intensity on Fig. 4-1, left). As long as this relation holds, we are going to call such region reference region of a property.\nLet\u2019s now introduce a new concept \u201cmoderatelyPaced\u201d containing only one axis \u201cquickness\u201d (q), and define a region in this context, described by the function moderatelyPaced(q) (Fig. 4-3).\nPlease note that as long as we only have one property in this context, we are using Y-axis for the degree of membership (instead of using color intensity, as in the previous example).\nLet\u2019s now see how this region looks in coordinates s, t. Remember, we assumed values of property \u201cquickness\u201d (x-axis on Fig. 4-2) to be drawn from the degrees of membership of the concept \u201cfast\u201d (color intensity on Fig. 4-1, left). Because of this, \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc66\ud835\udc43\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc51(\ud835\udc53\ud835\udc4e\ud835\udc60\ud835\udc61(\ud835\udc60, \ud835\udc61)) will be function composition of \ud835\udc53\ud835\udc4e\ud835\udc60\ud835\udc61(\ud835\udc60, \ud835\udc61) and \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc66\ud835\udc43\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc51(\ud835\udc5e), yielding a function like \u201c\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc66\ud835\udc43\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc51(\ud835\udc60, \ud835\udc61)\u201d. This function will transform membership degree of each point of the concept \ud835\udc53\ud835\udc4e\ud835\udc60\ud835\udc61(\ud835\udc60, \ud835\udc61) in accordance with the rule given by \ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc66\ud835\udc43\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc51(\ud835\udc5e), and this corresponds to the concept \u201cmoderately paced\u201d in the context with axes \ud835\udc60, \ud835\udc61 (Fig. 4-4).\nThis kind of mapping is general operation, allowing us to \u201cexpand\u201d any axis via its reference axes, thus, mapping different pieces of information into the same (reference) context for processing. We will come back to axis expansion again in 5.4."}, {"heading": "4.2 Operators", "text": "Region transforms, or operators, are going to play key role in our model. This section is an example of two simple operators, visualizing how we can use operators for creating new concepts.\nWe can define the operator corresponding to the word \u201cnot\u201d like this: \ud835\udc5b\ud835\udc5c\ud835\udc61(\ud835\udc65) = 1 \u2212 \ud835\udc65, where x is the value of the property (Fig. 4-5). This is similar to Zadeh\u2019s \u201ccomplementation\u201d [7:10].\nUsing operator \u201cnot\u201d, we can define a new concept \u201cslow\u201d as not(fast) (Fig. 4-6)\nMapping this back to our reference context, we get: \ud835\udc5b\ud835\udc5c\ud835\udc61(\ud835\udc53\ud835\udc4e\ud835\udc60\ud835\udc61(\ud835\udc60, \ud835\udc61)) (Fig. 4-7)\nWe can define \u201cvery\u201d approximately like this: \ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc66(\ud835\udc65) = \ud835\udc652 (Fig. 4-9). This is similar to Zadeh\u2019s \u201cvery\u201d [7:23].\nFig. 4-8\nApplied to region \u201cfast\u201d, we\u2019ll get region corresponding to \u201cvery fast\u201d (Fig. 4-10).\nMapping this back to our reference context, we get: \ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc66(\ud835\udc53\ud835\udc4e\ud835\udc60\ud835\udc61(\ud835\udc60, \ud835\udc61)) (Fig. 4-11)."}, {"heading": "5 Meaning operators", "text": ""}, {"heading": "5.1 Parameter space", "text": "Let\u2019s now take a look at how we can use these ideas to work with meanings.\nWe assume that for our goals we can use systems having their state completely described with a finite number (N) of independent real parameters (properties) 3 ranging from zero to one. In other words \u2013 a point in an N-dimensional unit hypercube. In practice, as long as the state is never known exactly, we are going to deal with a fuzzy region in this cube instead of a point.\nHow much information do we need to encode these regions? Well, even one real parameter (by real, we mean a real number in mathematics, not a data type in programming) already carries infinite amount of information. However, we intend to manage with relatively small amount of information, encoding the meaning of a typical phrase. To ensure adequate performance on today\u2019s computers, we believe that it would be acceptable if this information measures some kilobytes, maybe tenths or hundreds of kilobytes, but hardly \u2013 megabytes.\nIn this case, we could estimate the number of allowed parameters, if we assume for simplicity that a typical region is simply connected and its typical shape is a polyhedron obtained from a parallelepiped by tilting its faces (we are never going to use this assumption later, except for this estimate). We need to store the position for each face and its tilt with respect to all the axes: this gives approximately 2N numbers per face. Total amount of faces is also 2N at max. This way, we are going to need around 4N2 real numbers to describe such a region. This means that if we are not willing to exceed 100 KB per region, we should not work with regions needing more than 30-50 parameters to describe.\n3 Here we should mention that neither choice of independent parameters, nor choice of basic parameters appropriate to describe any given fragment of the environment is trivial. Luckily, we can hopefully benefit from the other side of natural language complexity, and trust that the concepts suitable for deriving appropriate parameters and useful words-elements have already been worked out as a result of long running-in in natural language.\nWe assume that the system should understand the meaning of a phrase based on the available information about the environment around. Though, it is obvious that no simply shaped 30- dimensional region can describe a complex enough picture of the world. But a specific phrase should not change the whole picture of the world, either. On the contrary, we expect that each phrase should be allocated a subspace of relatively small dimensionality (say, 30 to 50), accommodating all of the modifications, while projection of the region onto the remaining dimensions will stay the same.\nWe are going to call the subspace containing region modifications phrase context. In practice, we are going to allocate the context with a certain excess; and if we pick a 30-dimensional subspace, but in reality the region only changes with respect to 10 dimensions, there is nothing wrong about it.\nLet\u2019s note that normally we can store multi-dimensional regions as a set of independent regions in low-dimensional (one- and two-dimensional) spaces. This is described in more details in 5.5 and 5.6, including the mechanism for merging such regions into one multidimensional region. In the case when we need to project a multidimensional region onto a subspace of smaller dimensionality, we can just discard corresponding low-dimensional regions (that are not a part of the projection)."}, {"heading": "5.2 Meaning of phrases and words", "text": "So, before interpreting the phrase we have a certain projection of the system state onto a given context: let us call it source region S1. After interpreting, the state will change, and so will its projection onto that context. Let us call this new projection resulting region R1. Now, if the source state were S2, the resulting state after interpretation would also be different, say, R2. Thus, each set of possible source regions {Si} has a corresponding set of resulting regions {Ri}.\nNow we are getting ready for a key definition: phrase meaning in a given context is an operator transforming every source region to a corresponding resulting region. In other words, phrase meaning is simply a mapping {Si} -> {Ri}.\nObviously, we are not going to define such operators by specifying the sets of source and resulting regions. We will use more practical and descriptive ways, e.g. composition of several basic operators of different kinds.\nSo, we made an attempt to define the meaning of a phrase. But phrases are composed from words. How can we model the meaning of separate words, is there any conceptual difference between word and phrase meaning? Intuition is suggesting that it is different: many of the words cannot form complete phrases, and can only be used together with other words. We can formulate the most important distinction like this: generally speaking, words are operators that depend on some parameters. For example, let\u2019s take a command \u201cwalk\u201d. A robot receiving this command should start moving with some average speed. If it receives command \u201cwalk fast\u201d, the speed would be different: word \u201cfast\u201d modifies a parameter of the operator \u201cwalk\u201d. In case of the command \u201cwalk very fast\u201d, word \u201cvery\u201d modifies a parameter of the operator \u201cfast\u201d, that, in its turn, modifies the parameter of the operator \u201cwalk\u201d (Fig. 5-1).\nNow we should mention that a word can have many such parameters, and that they are, of course, fuzzy (as long as we are talking about natural language). In other words, we can say that a word operator has its own internal context4, and the region, defined in this context, affects the operator. The operator resulting from such interaction of two or several modifying operators, we are going to call block-operator.\n4 It is important not to mix axes of this context (that can have separate, utility meaning) and axes of the contexts of the main message, in which the operators of complete phrases work."}, {"heading": "5.3 Different contexts", "text": "We have previously mentioned phrase contexts. However, much larger narratives (from a paragraph to a multi-volume novel) also have certain meaning. Variant reading is common when interpreting background, underplot, etc., but on the level \u201cwho went were, who did what and what were the consequences\u201d even such large narratives can be fairly understandable. In our terminology this means that there is an operator transforming system state before interpreting the writing to the state after interpretation. Well, even if saying this is valid, in practice such representation is extremely redundant. In this case there will be hundreds of thousands of parameters characterizing the context, if not millions, and any practical work with operators in the space of such dimensionality becomes impossible.\nIn addition, as we mentioned before, the meaning is not obvious immediately, it is chosen for each individual phrase as result of comparing meaningfulness (comprehensibility) of the interpretations. And, if using brute force search through all phrase interpretations is perfectly possible, for large discourses it would be unfeasible.\nThe solution is that in case of independent statements (having to do with different objects, characters etc.) we can trust that the meaning-operators will be working in different subspaces. So, if we have two phrases A and B, their contexts are \ud835\udc36\ud835\udc34and \ud835\udc36\ud835\udc35, and their meanings are \ud835\udc34: \ud835\udc4b \u2192 \ud835\udc34(\ud835\udc4b) and \ud835\udc35: \ud835\udc4c \u2192 \ud835\udc35(\ud835\udc4c), then the meaning of the statement including both phrases is \ud835\udc34 \u2295 \ud835\udc35: \ud835\udc4b \u2295 \ud835\udc4c \u2192 \ud835\udc34(\ud835\udc4b) \u2295 \ud835\udc35(\ud835\udc4c). And even if we can work with each of these phrases in the \u201clarge\u201d context, \ud835\udc36\ud835\udc34 \u2295 \ud835\udc36\ud835\udc35, in which their meanings are expressed with operators \ud835\udc34 \u2295 \ud835\udc3c: \ud835\udc4b \u2295 \ud835\udc4c \u2192 \ud835\udc34(\ud835\udc4b) \u2295 \ud835\udc4c and \ud835\udc3c \u2295 \ud835\udc35: \ud835\udc4b \u2295 \ud835\udc4c \u2192 \ud835\udc4b \u2295 \ud835\udc35(\ud835\udc4c), it is obvious that it is just a waste of resources. Thus, in practice we are working separately with independent contexts, remembering that we can consider all transforms happening in one large space.\nNow, when we have defined what we mean when talking about several contexts, let\u2019s discuss in more details how these contexts are brought into focus. They may be either found among the ones used before (i.e. we have already worked in this subspace), or created anew (i.e. we have not yet paid attention to this subspace of the \u201cglobal context\u201d). A typical situation can be described like this: first, we try to work in the same context we used for the previous phrase, but if we are running into understanding issues, we look for another suitable context, and if that fails \u2013 we create a new one. Ways of searching is a separate topic, for now let\u2019s just mention that the contexts can be organized in some kind of hierarchy for convenience. Moreover, we could create several hierarchical indexes: hierarchy of narrative parts, time intervals, spatial locations, objects, events, actions, etc."}, {"heading": "5.4 Axis expansion", "text": "Let\u2019s assume that we have a property \ud835\udc34 given by a reference region that is defined in paramers \ud835\udc65 and \u0443. Parameter \ud835\udc65, in its turn, is defined by a region with coordinates \ud835\udc651 and \ud835\udc652, and parameter \ud835\udc66, respectively \u2013 by a region with parameters \ud835\udc661 \u0438 \ud835\udc662. Is there an automatic way to express property \ud835\udc34 in coordinates \ud835\udc651, \ud835\udc652, \ud835\udc661 and \ud835\udc662? It turns out that yes, and it is very simple. If region \ud835\udc34 is expressed by the function \ud835\udc34(\ud835\udc65,\ud835\udc66), and regions \ud835\udc4b and \ud835\udc4c \u2013 by functions \ud835\udc4b(\ud835\udc651,\ud835\udc652) and \ud835\udc4c(\ud835\udc661,\ud835\udc662), respectively, then the expression sought for is \ud835\udc34(\ud835\udc4b(\ud835\udc651,\ud835\udc652),\ud835\udc4c(\ud835\udc661,\ud835\udc662)). Really, the value of membership function for the region \ud835\udc4b shows the truth degree of parameter \ud835\udc65, and this is exactly what needs to be provided as the first argument of function \ud835\udc34. The same is also true for the truth degree of parameter \ud835\udc66. In this way, we can express any regions via basic parameters. Some examples of axis expansion were given in 4.1 and 4.2.\nAn important comment should be made here: in this case certain basic parameters (e.g. related to time and space) may appear several times in the context, and with different scales (in the sence that the value on the time axis may in one case mean \u201chour\u201d, and in another case \u2013 year). This should not worry us too much: for example, color of different objects may just as well be represented by two different axes, even if they have similar meaning. Additional information on the hierarchy of scales, objects, etc. can be stored in earlier mentioned (5.3) hierarchical stores of events, objects and contexts. As we commented previously, these are a practical way to optimize work, while keeping the information (that is not used in the context at this very moment) for later."}, {"heading": "5.5 Specifying regions", "text": "It may be convenient to store regions using reference points, that are used as a basis for interpolation. That is why it is also practical to define regions in the same way. The difficulty here is that, even though we are going to store multidimensional regions, it is only natural (from the user interface point of view) to define one-dimensional and two-dimensional regions. On the other hand, we can always define a multidimensional region as a sort of product of one-dimensional and two-dimensional regions. More precisely, our context would then be a direct sum of low-dimensional (one- and two-dimensional) contexts \ud835\udc341 \u2295 \ud835\udc342 \u2295 \u2026 \u2295 \ud835\udc34\ud835\udc5b, and our membership degree for region points would be the geometric mean of the point membership degrees in each of the low-dimensional regions. The choice of geometric mean is motivated by the following:\na) if in any of the combined regions we have zero (\u201cclearly does not belong\u201d), it means that the result should be zero as well, b) only if all of the regions have one (\u201cclearly belongs\u201d), the total membership degree would be one, c) surfaces with equal membership degree (e.g. equal to \ud835\udc65) include the points, whose membership degree in all of the combined regions is also equal to \ud835\udc65, d) increase in the number of axes being combined does not result in radical decrease of membership function values in all places where its value is different from one.\nHere, especially in the case with many axes, there may occur a need to avoid \u201cequalization\u201d of their relative weights. Really, it may happen that a region is defined in a space of 30-50 axes, but the most important axes are only one or two. In this case, instead of geometric mean we can take\n\u220f(\ud835\udc65\ud835\udc56) \ud835\udefc\ud835\udc56\n\ud835\udc5b\n\ud835\udc56=1\nwhere \ud835\udc65\ud835\udc56 are point membership degrees in each of the combined regions, and \ud835\udefc\ud835\udc56 \u2013 a set of numbers (exponents) such that \ud835\udefc\ud835\udc56 > 0 and \u2211 \ud835\udefc\ud835\udc56 = 1. However, the last condition (normalization of the\nexponents) may be dropped \u2013 in this case, we would lose property c), but this may be convenient for definition of some regions.\nIt is worth noting that we do not always need to immediately combine low-dimensional regions, creating one region in a multidimensional context. As long as this is possible, it may be convenient to store the region as a set of several independent low-dimensional regions. This may be especially relevant when it comes to subsequent application of meaning-operators, for example qualitative adjectives (5.6).\nSometimes, we may need to define an unspecified (empty) region that corresponds to zero information. The values of the corresponding membership function will be equal to one for all the points of the region. It is easy to see that such membership function can be \u201cadded\u201d to an existing region as a subspace, without changing existing properties of the region / its membership function.\nThus, we have a way of defining a region, and therefore, a projection operator (transforming any region into the given one). If we need to define a non-trivial operator describing region transformations, then, in addition to the source region, we can specify the trajectory of movement of reference points (e.g. using a spline), and the membership function degree change along the trajectory (e.g. using a different spline)."}, {"heading": "5.6 Parts of speech", "text": "The problem of selecting suitable and universal \u201cbuilding blocks\u201d for constructing all possible kinds of meanings is an extremely complex one. However, each natural language is one solution to this problem. Let us look at some of the most basic \u201cbuilding blocks\u201d normally used in natural language.\nQualitative adjectives (\u201clarge\u201d, \u201ctall\u201d, \u201csimple\u201d). Operator, corresponding to a qualitative adjective, may be modeled as a projection operator. More concretely, \ud835\udc3c(\ud835\udc4b)\u2a01\ud835\udc43(\ud835\udc4c), where \ud835\udc3c(\ud835\udc4b) is identity operator, \ud835\udc43(\ud835\udc4c) is projection operator, and \ud835\udc4b(\ud835\udc65) and \ud835\udc4c(\ud835\udc66) are the membership functions in corresponding region subspaces. \ud835\udc43(\ud835\udc4c) substitutes \ud835\udc4c(\ud835\udc66) for some particular membership \ud835\udc4c\u2217(\ud835\udc66). Here we assume that the source region is either given as a set of independent membership functions \ud835\udc4b and \ud835\udc4c (as it was mentioned in 5.5), or may be decomposed into a set of such functions. In the case when such decomposition is not possible with reasonable accuracy, we should treat this as an ambiguity within this interpretation (this is described in more details in 6.1). If the context has an axis directly corresponding to the adjective, then dimensionality of y will be one, otherwise \u2013 more than one.\nWe considered some examples of modeling qualitative adjectives in 4.1.\nComparative adjectives (\u201clarger\u201d, \u201ctaller\u201d, \u201csimpler\u201d) can be modeled as a direct sum of some general operator \ud835\udc3a(\ud835\udc4c) and identity operator: \ud835\udc3c(\ud835\udc4b)\u2a01\ud835\udc3a(\ud835\udc4c), where \ud835\udc4b(\ud835\udc65) and \ud835\udc4c(\ud835\udc66) are membership functions in corresponding region subspaces. Here, same as in the previous paragraph, we also assume decomposability of the source region into independent membership functions.\nIn the end of 5.5 we briefly described how such operators (corresponding to comparative adjectives) could be defined.\nWhen it comes to nouns, they either create a new object in the object hierarchy or \u201cactualize\u201d an already existing one. If a new object is being created, then the axes specific for this word are added to its context, and the membership is formed with the spatial and other important parameters of the object. We would have obtained the same region if we first created a default (empty) context and then applied a number of adjectives, describing the properties of this noun.\nVerbs are special, because they always work with the time axis, and often \u2013 with its small part. Therefore, if nouns creates or finds special object context, then verbs create or find action context.\nSome conjunctions like \u201cand\u201d and \u201cor\u201d can be defined fairly simply: we can combine resulting regions in the same context analogous to the way we previously combined low-dimensional regions into a region, defined in a multidimensional space.\nFor example, for conjunction \u201cand\u201d, instead of obvious multiplication, we can take the geometric mean\nof two functions: \u210e(\ud835\udc65) = \u221a\ud835\udc53(\ud835\udc65) \ud835\udc54(\ud835\udc65).\nFor \u201cor\u201d, in its turn, we can take \u210e(\ud835\udc65) = 1 \u2212 \u221a(1 \u2212 \ud835\udc53(\ud835\udc65))(1 \u2212 \ud835\udc54(\ud835\udc65)), where \ud835\udc53(\ud835\udc65) and \ud835\udc54(\ud835\udc65) are\nmembership functions in the source contexts, and \u210e(\ud835\udc65) \u2013 membership function in the resulting context. Such definition, as we already mentioned previously in 5.5, allows to avoid significant decrease in the membership function values, when membership degrees in the source contexts are lower, than one.\nSimple illustrations to \u201cand\u201d and \u201cor\u201d are given on Fig. 6-1 and Fig. 6-2.\n\u201cNot\u201d, obviously, should work like \u210e(\ud835\udc65) = 1 \u2212 \ud835\udc53(\ud835\udc65). This is similar to Zadeh\u2019s \u201ccomplementation\u201d [7:10]. An illustration to \u201cnot\u201d was given on Fig. 4-5.\n\u201cBut\u201d, on the other hand, prevents \u201ccontext interpenetration\u201d. What we mean by this is that the second phrase of the two, connected with the conjunction, is interpreted like if the first one was not there. More formally, if the source region was \ud835\udc34, and the first part of the phrase transformed it into \ud835\udc35, then the meaning-operator of the second phrase part \ud835\udc39 performs transformation \ud835\udc39(\ud835\udc34) (and not \ud835\udc39(\ud835\udc35)). After that the results \ud835\udc35 and \ud835\udc39(\ud835\udc34) are stored separately in the context hierarchy, becoming different subspaces of some conceptual \u201cglobal context\u201d, that contains all the information known to the system.\nWe gave some examples of how different parts of speech and concrete words can be modeled with operators of different kinds. Now let us take a look at some other problems: analyzing comprehensibility of the interpretations, and the problem of describing situations and concepts known to the system with words."}, {"heading": "6 Interpreting phrases", "text": ""}, {"heading": "6.1 Process", "text": "Let\u2019s now briefly discuss the phrase interpretation process.\nWhen a new phrase comes in, we can do the following. First, as much as it is possible, the syntactic structure of the phrase is obtained from the grammatical forms of words. In practice, this means determining the order of application of operators and block-operators to one another. In case when this structure is ambiguous, we need to remember the possible options so we can later compare them based on their comprehensibility (6.3), and select the best option.\nAfter, we need to do sequential application of operators to one another. More precisely, the operators that modify the internal context of another operator, do that (yielding a block-operator), while the operators working only with the external context of the narrative (e.g. phrase context), form a line, in the order of application. The result is phrase operator: composition of several block-operators. This operator is later applied to the narrative context (by sequential application of block-operators from the line).\nAfter this step, we need to resolve the ambiguities of the structure. More concretely, we need to assess the meaningfulness of the resulting construction given the context of the narrative. If none of the options reach the threshold, we try using one of the \u201cspare contexts\u201d (see below). If that fails as well \u2013\nwe need to ask for clarifications. If several options are above threshold, we choose the best one, and save the rest (or some of them) as \u201cspare contexts\u201d.\nThe number of \u201cspare contexts\u201d kept can be an adjustable parameter. When a phrase is unclear, and asking for clarifications is not an option, we can increase this parameter and reinterpret some of the recent phrases again. If we still fail to understand the phrase, we can increase both this parameter, and the number of recent phrases to reinterpret.\nOur approach allows to handle situations when the syntactic structure of the sentence cannot be completely restored from word order and grammatical forms, and for restoring this structure people would need to resort to semantics. This situation may often occur when using a voice interface. In our approach, we can try different variants of the structure (there are not too many of these because of grammar and syntax rules), calculate the comprehension level (6.3), and choose the interpretation that makes most sense.\nAfter the understanding of the phrase is achieved, we move on to the next phrase."}, {"heading": "6.2 Polysemy and homonyms", "text": "Let\u2019s say some words about working with polysemy, and homonyms in particular. In case of essentially different word meanings, we can store several different operators corresponding to the same word. In case of homonyms (when words are used in very different contexts), the right meaning can be detected almost immediately, by comparing axes of word\u2019s internal context with axes of the current context.\nIn the case of more similar meanings the internal contexts of the words will be fairly similar. Then we will need to take all possible meaning-operators for the word and \u201ctest\u201d them as part of the phrase operator, choosing the one resulting in the best satisfaction of comprehension criteria. It would be best if we normally store such similar meanings as one \u201cfuzzier\u201d operator, as opposed to storing separate meanings as separate operators, otherwise there is a risk of ending up with too many variants of phrase meaning (if the phrase contains several such words with involved polysemy)."}, {"heading": "6.3 Assessing comprehension", "text": "In this section we describe some techniques and ideas that can be used to determine whether a statement or a phrase makes sense, and to choose which of the possible interpretations makes the most of it.\nWe can start with trying to define several heuristics to help identify vague, contradictory and other incomprehensible phrases.\nA region that has no points with high enough degree of membership (say, 0.95) can correspond to a contradiction. Indeed, this situation would mean there is no property combination in the context that definitely corresponds to our concept. For example, if we consider a hypothetical concept and(slow, fast), we get a typical contradiction (Fig. 6-1).\nOn the other hand, if almost all the points of the context belong to a region, this usually does not make much sense either, as it provides no information. For example, if we consider hypothetical concept or(slow, fast), we get into this situation (Fig. 6-2).\nAnother useful heuristics has to do with the fact that normally we expect new information to change something in our knowledge. Let\u2019s compare the regions, describing verb-like concepts \u201cwalk\u201d and \u201cstand still\u201d (Fig. 6-3).\nIf we apply operator \u201cfaster\u201d to these concepts, \u201cwalk faster\u201d will result in a different region, while \u201cstand still faster\u201d will remain the same, because the \u201ccompression\u201d over time axis done by operator \u201cfaster\u201d will have no effect (Fig. 6-4). So, after comparing the regions, we can conclude that phrase \u201cstand still faster\u201d makes no sense, indeed.\nIn many cases, new information not only changes our knowledge, but often is expected to precisiate it rather than making it more vague. This is especially the case when a system is receiving instructions.\nFor example, if we are trying to direct a robot, and the region describing it is transformed from \u201cSomewhere NE\u201d to \u201cAnywhere except SW\u201d, it may be a sign of misunderstanding (Fig. 6-5). An exception to this could be a special phrase \u201cforget everything, I will try to explain from the beginning\u201d.\nAn important type of sensibility assessment is to analyze the correspondence between statement\u2019s assumed goal, and the information it actually conveys. At this stage of model development, we can, for example, check whether the grammatical mood of the message (like realis, imperative, conditional etc.) corresponds to the resulting region. Grammatical mood can be obtained from the grammatical form of the words or syntax (from parser).\nWhen it comes to the region shape, we need to note that when giving natural language commands to the system, some parameters may be connected to system effectors. Like, say, parameter \u201cmovement speed\u201d can be connected to a controller altering speed. Then, if the region with membership value above certain threshold is small and simply connected (say, region, corresponding to a command \u201cdrive fast\u201d), we could find the center C of this region and output command \u201capply value C\u201d to the controller\u2019s effector. However, if we give a command \u201cdrive very fast or very slowly\u201d, it would be natural for the system to ask for clarifications, even though the phrase itself is clear (unlike phrase \u201cdrive very fast AND very slowly\u201d). On the other hand, when we are in realis or conditional mood (\u201cbut if I was driving very fast or very slowly\u201d), such region shape is perfectly acceptable and does not require any clarifications.\nIt should be noted that many other, more specific heuristics can be created for estimating the comprehension level."}, {"heading": "7 Composing descriptions", "text": ""}, {"heading": "7.1 Abstract concepts", "text": "In 4.1 and 5.4 we considered how derived concepts (concepts defined via other concepts) are expanded, resulting in simpler concepts (defined via basic system parameters). Let\u2019s now take a look at how the system may derive new concepts, performing the operation of abstracting.\nLet us consider two operations that we could perform with a region (or an operator). The first one is \u201cblurring\u201d that reduces boundary definition accuracy for the region (or image and preimage in case of operator). The other operation is dimensionality reduction: projection of this region onto a certain\nsubspace (or restriction of an operator to this subspace). In both cases we reduce the amount of information used for defining the region (or operator).\nIt may happen that under \u201cblurring\u201d and/or dimensionality reduction, several regions (operators) become indistinguishable. And if we talk about an operator, the transformation defined by the operator can be correlated with a certain meaning. Then it may be possible to define a generalizing concept (that is described by the operator obtained with \u201cblurring\u201d or reducing dimensionality of the original operators), that will express certain common essence present in all of the considered operators.\nMore formally, let \ud835\udc4b\ud835\udc56 be a family of subspaces with a common subspace \ud835\udc4c, \ud835\udc34\ud835\udc56 \u2013 a family of operators in these spaces, \ud835\udc43\ud835\udc66: \ud835\udc4b \u2192 \ud835\udc4c \u2013 a projection operator onto subspace \ud835\udc4c, and \ud835\udeff \u0438 \ud835\udf00 \u2013 accuracy levels with which we define parameter values and membership degree of region points.\nThen, if an operator \ud835\udc35 in a space \ud835\udc4c is satisfying\n\u2200\ud835\udc56, \ud835\udc65 \u2203 \u2016\u2206\ud835\udc66\u2016 < \ud835\udeff: \u2016\ud835\udc35 (\ud835\udc43\ud835\udc66(\ud835\udc65)) \u2212 \ud835\udc43\ud835\udc66(\ud835\udc34\ud835\udc56(\ud835\udc65)) \u2212 \u2206\ud835\udc66\u2016 < \ud835\udf00,\nwe are going to call it abstracting with respect to meaning-operators \ud835\udc34\ud835\udc56."}, {"heading": "7.2 Describing with words", "text": "The problem of finding families of operators that have non-trivial abstracting meaning-operators is computationally quite complex. Despite being solvable in principle, building new meaningful abstractions in reality is going to be fairly difficult. However, here we can rely on natural language, hoping that all necessary abstractions are already available. Now we are going to talk about how these abstractions can be used in the problem of describing a given situation in words. Let\u2019s define the problem in more details.\nWe are going to distinguish three types of \u201cdescribe in words\u201d problem. First one \u2013 when there are many parameters known to the system, but unknown to the user. Possibly, the parameters are organized in a whole hierarchy of contexts, containing many hundreds and thousands of them. The problem is to convey this information to the user using minimal (or close to minimal) amount of words. Second type is similar to the first one, but in this case the information is not organized in contexts and parameters (in terms of our model). For example, it can be obtained from external sensors or be the result of solving some problem with a certain algorithm. In any case, it needs to be described in words. The third subtype of this problem is \u201cinterpretation crisis\u201d: none of the interpretations of what is said by the user does not meet the comprehension criteria. In this case, all of them should be described to the user, including the \u201cfailing\u201d comprehension criterion (for each of them).\nIn all these cases the approach is similar: we need to obtain a meaning-operator transforming the source region (reflecting some knowledge taken as a starting point) into the resulting region that corresponds to the state being described. This correspondence may be partial or imprecise (correspondence of only a certain projection of the resulting region onto some subspace). In any case, we can define a measure of this correspondence, that is adequate for each concrete problem, using a special test operator that verifies the degree of goal satisfaction. And now we can define the problem of describing with words like the following: assume we have a certain original context \ud835\udc46 with a defined in it region \ud835\udc34 and a (large) set of meaning-operators \ud835\udc35\ud835\udc56. These can be separate word operators, large block-operators, and even phrase and multi-phrase operators. Also, we have a test operator \ud835\udc3a that verifies goal satisfaction: \ud835\udc3a: \ud835\udc39 \u2192 \ud835\udc43, transforming the region in the resulting context \ud835\udc39 to the region in a one-dimensional context \ud835\udc43 with the only parameter: goal satisfaction degree. The problem of describing in words is then composing (using the set \ud835\udc35\ud835\udc56) such meaning-operator \ud835\udc37: \ud835\udc46 \u2192 \ud835\udc39, that\nregion \ud835\udc3a(\ud835\udc37(\ud835\udc34)) has values of membership degrees close to one when and only when the parameter \u201cgoal satisfaction degree\u201d is also close to one (concrete values of proximity to one should be given separately, and may depend a lot on the problem in question).\nSo, we have defined what we understand by \u201cdescribing with words\u201d problem. But, are there any ways to carry out this description, except for brute forcing all possible combinations of meaning-operators in \ud835\udc35\ud835\udc56? We believe that yes, and that the hierarchy of abstract concepts already available in the natural languages may help us.\nWe saw that meaning-operators that are more abstract work in subspaces (contexts) of smaller dimensionality. That\u2019s why a suitable way of composing the description would be first taking the most abstract operators in the family \ud835\udc35\ud835\udc56, making sure that at least some of them have parameters from the context \ud835\udc46, and some \u2013 from the context F. There will be much fewer of such abstract operators, compared to the total number of operators. We can organize them in compositions \ud835\udc37\ud835\udc58 = \ud835\udc37\ud835\udc580 \u2218 \ud835\udc37\ud835\udc581 \u2218 \u2026 \u2218 \ud835\udc37\ud835\udc58\ud835\udc5a , such that when adding a new operator to the composition, the region \ud835\udc3a(\ud835\udc37\ud835\udc58(\ud835\udc34)) would be shifting further towards one (Fig. 7-2). But we also expect, that when using the most abstract operators from \ud835\udc35\ud835\udc56, the region \ud835\udc3a(\ud835\udc37\ud835\udc58(\ud835\udc34)) would be fairly fuzzy; this means that we cannot be sure that the goal satisfaction degree would close to one.\nof the result of application of the first operator \ud835\udc37\ud835\udc580, and the whole composition \ud835\udc37\ud835\udc58 to the region A. High abstraction level,\nhigh fuzziness.\nThat\u2019s why we continue composing our description by making each of the composition components more specific. This means, that we move from composition \ud835\udc37\ud835\udc58 = \ud835\udc37\ud835\udc580 \u2218 \ud835\udc37\ud835\udc581 \u2218 \u2026 \u2218 \ud835\udc37\ud835\udc58\ud835\udc5a to the composition \ud835\udc37\ud835\udc59 = \ud835\udc37\ud835\udc590 \u2218 \ud835\udc37\ud835\udc591 \u2218 \u2026 \u2218 \ud835\udc37\ud835\udc59\ud835\udc5b , replacing each of the elements of the previous composition \ud835\udc37\ud835\udc58\ud835\udc57 with the sequence \ud835\udc37\ud835\udc59\ud835\udc60 \u2218 \u2026 \u2218 \ud835\udc37\ud835\udc59\ud835\udc61 , such that operators \ud835\udc37\ud835\udc59\ud835\udc60 , \u2026 , \ud835\udc37\ud835\udc59\ud835\udc61 are less abstract, than \ud835\udc37\ud835\udc58\ud835\udc57 (have less fuzzier regions and operate in subspaces of larger dimensionality), while \ud835\udc37\ud835\udc58\ud835\udc57 is an abstracting operator with respect to the combined operator \ud835\udc37\ud835\udc59\ud835\udc60 \u2218 \u2026 \u2218 \ud835\udc37\ud835\udc59\ud835\udc61 (Fig. 7-3). Roughly speaking, operator\n\ud835\udc37\ud835\udc59\ud835\udc60 \u2218 \u2026 \u2218 \ud835\udc37\ud835\udc59\ud835\udc61 does the same as \ud835\udc37\ud835\udc58\ud835\udc57 , but with some additional details. We expect that, as the\nabstraction level decreases, the test operator, applied to the region in the final context, would be yielding less fuzzier results, as shown on Fig. 7-4. If this is really the case, the process may be regarded as successful.\n(sufficiently low abstraction level)\nIt remains to discuss why we believe that such algorithm may lead to sufficient decrease in the total size of the enumeration. The idea is that at the highest level of abstraction, there are fairly few relevant operators (with parameters matching either source or resulting context, or \u2013 any other parameters in the operator composition so far). With each decrease in the abstraction level, we only talk about expressing (with less abstract operators) each of the more abstract operators in the composition so far \u2013 for what, again, we will only be attempting operators with sufficiently fitting parameter set. With each step of decreasing abstraction level, the fraction of \u201calready defined\u201d context parameters for the operators considered during the enumeration will be high \u2013 enough to decrease the enumeration to acceptable levels.\nThus, we demonstrated a draft of an algorithm for describing situations in words, based on the system of abstractions, available in the natural language. It is interesting to note that we can use this algorithm to describe the meaning of any operator or concept known to the system in \u201cown words\u201d. For that, we need to exclude that operator (concept) from the set \ud835\udc35\ud835\udc56, from which the meaning-operators used in the description algorithm are drawn. Such a description may turn out to be less precise, but possibly more compact (if it was originally given in words).\nIt remains to mention that when the system is experiencing difficulties with text comprehension, it may describe in words each of the interpretations, and more concretely: meaning of the largest narrative fragment, for which this interpretation succeeds the comprehension tests, and meaning of the smallest narrative fragment, for which the tests start failing. When the person communicating with the system is provided this data, she would have enough information to adjust the situation and rephrase her explanations or commands in a different, clearer way."}, {"heading": "8 Conclusions", "text": "We described our approach to working with semantic information, that allows us to give definitions to word and phrase meaning, and formulate criteria for text comprehensibility. This possibility potentially allows our system to learn new words with help of texts meant for people (dictionaries, encyclopedias etc.), as long as certain minimal vocabulary is accumulated. Then, using the same types of sources, verification of the learning may be performed. For example, if a word just learned has a known synonym, we may verify closeness of the corresponding meaning-operators. In a similar way, meaningoperators obtained from the definitions given in different dictionaries may be verified.\nA separate advantage of the method is the possibility to express a situation or a concept known to the system in words. It may allow for more productive dialog with a user, and her more complete awareness of the difficulties the system is experiencing.\nFinally, storing often used block-operators, along with the ability to partially or fully \u201cexpand\u201d them into more basic (or even built-in) operators, plus the abstracting procedure allows the system to obtain new and useful for it concepts. The procedure of \u201cdescribing in words\u201d may be applied to such concepts, followed by a \u201cdiscussion\u201d with the user, or a search in dictionaries. It may turn out that the system has discovered a concept that exists in natural language, or \u2013 come up with a new one. In principle, both possibilities are interesting."}, {"heading": "9 Summary", "text": "The field of formalizing meaning and quantifying understanding is still much unexplored. In this introductory article we tried to make some humble steps in this field, describing our approach and ideas. Obviously, a lot of research and experimenting remains to be done before it is possible to evaluate any results. Having said that, we would like to emphasize that the approach described in this article has certain advantages, compared to those known and widely used. Semantic network-like representations can fairly easily absorb new knowledge, but the relations between words contain very little quantitative information. Other systems, like those based on different logic formalisms, encode information in details, but require human work for rule input and editing, and do not really handle uncertainty and imprecision well. Neither of the representations attempt to model the concepts for \u201cwhat they are\u201d, and are not capturing their essence in quantitative form. This restricts capabilities of those approaches when it comes to complex understanding and reasoning related tasks (like engaging in a dialog). This is what our approach is focusing on, and we hope that its features will let it take its rightful place together with the other systems that obtain knowledge from natural language sources and allow communication with natural language."}], "references": [{"title": "What's in a Link: Foundations for Semantic Networks", "author": ["W. Woods"], "venue": "DTIC Document,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1975}, {"title": "Commonsense Reasoning in and over natural language", "author": ["H. Liu", "P. Singh"], "venue": "MIT Media Lab,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Quantitative Fuzzy Semantics", "author": ["L. Zadeh"], "venue": "Information Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1971}, {"title": "Understanding Natural Language", "author": ["T. Winograd"], "venue": "Cognitive Psychology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1972}, {"title": "The concept of a linguistic variable and its application to approximate reasoning", "author": ["L. Zadeh"], "venue": "Information Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1975}, {"title": "A Fuzzy-Set-Theoretic Interpretation of Linguistic Hedges", "author": ["L. Zadeh"], "venue": "Journal of Cybernetics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1972}, {"title": "A computational approach to fuzzy quantifiers in natural languages", "author": ["L. Zadeh"], "venue": "Computers & Mathematics with Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Test-Score Semantics as a Basis for a Computational Approach to the Representation of Meaning", "author": ["L. Zadeh"], "venue": "Literary and Linguistic Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1986}, {"title": "Knowledge representation in fuzzy logic", "author": ["L. Zadeh"], "venue": "IEEE Trans. on Knowledge and Data Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Precisiated Natural Language (PNL)", "author": ["L. Zadeh"], "venue": "AI Magazine,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "From imprecise to granular probabilities", "author": ["L. Zadeh"], "venue": "Fuzzy Sets and Systems, Elsevier,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Computing with Words\u2014Principal Concepts and Ideas", "author": ["L. Zadeh"], "venue": "Lecture Notes,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Computing with Words\u2014Principal Concepts and Ideas", "author": ["L. Zadeh"], "venue": "Studies in Fuzziness and Soft Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Knowledge representations based on semantic networks and frames have been criticized for imprecision and ambiguity since [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Neither semantic networks nor formal logic approaches handle uncertainty or fuzzy concepts like perceptions well [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Zadeh, starting from [3]), and we see no other choice as to continue small steps in this direction.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "In Quantitative Fuzzy Semantics [3], L.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "[6] discusses using linguistic variables whose values are words or sentences in a natural language.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] suggests modeling linguistic hedges (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] discusses using fuzzy sets for modeling natural language quantifiers like \u201cseveral\u201d, \u201cmost\u201d, \u201cnot many\u201d, \u201cclose to five\u201d, \u201capproximately ten\u201d, etc.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Test-score semantics [9] and knowledge representation based on it [10] propose modeling the meaning of a proposition as a composition of meanings of words-elements.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "Test-score semantics [9] and knowledge representation based on it [10] propose modeling the meaning of a proposition as a composition of meanings of words-elements.", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "Generalized Constraint Language (GCL) [11, 12] is an evolution of test-score semantics and a basis for Computing With Words (CWW) [13, 14].", "startOffset": 38, "endOffset": 46}, {"referenceID": 10, "context": "Generalized Constraint Language (GCL) [11, 12] is an evolution of test-score semantics and a basis for Computing With Words (CWW) [13, 14].", "startOffset": 38, "endOffset": 46}, {"referenceID": 11, "context": "Generalized Constraint Language (GCL) [11, 12] is an evolution of test-score semantics and a basis for Computing With Words (CWW) [13, 14].", "startOffset": 130, "endOffset": 138}, {"referenceID": 12, "context": "Generalized Constraint Language (GCL) [11, 12] is an evolution of test-score semantics and a basis for Computing With Words (CWW) [13, 14].", "startOffset": 130, "endOffset": 138}, {"referenceID": 8, "context": "The approach is based on fuzzy logic, as it is a very good instrument for working with different levels of truthness, and concepts with unclear boundaries, phenomena commonly occurring in knowledge coming from natural language, and commonsense knowledge in particular [10].", "startOffset": 268, "endOffset": 272}], "year": 2015, "abstractText": "In this introductory article we present the basics of an approach to implementing computational interpreting of natural language aiming to model the meanings of words and phrases. Unlike other approaches, we attempt to define the meanings of text fragments in a composable and computer interpretable way. We discuss models and ideas for detecting different types of semantic incomprehension and choosing the interpretation that makes most sense in a given context. Knowledge representation is designed for handling context-sensitive and uncertain / imprecise knowledge, and for easy accommodation of new information. It stores quantitative information capturing the essence of the concepts, because it is crucial for working with natural language understanding and reasoning. Still, the representation is general enough to allow for new knowledge to be learned, and even generated by the system. The article concludes by discussing some reasoning-related topics: possible approaches to generation of new abstract concepts, and describing situations and concepts in words (e.g. for specifying interpretation difficulties).", "creator": "Microsoft\u00ae Word 2013"}}}