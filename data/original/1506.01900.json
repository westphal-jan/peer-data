{"id": "1506.01900", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2015", "title": "Communication Complexity of Distributed Convex Learning and Optimization", "abstract": "We study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered. We identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible. Among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power.", "histories": [["v1", "Fri, 5 Jun 2015 13:24:17 GMT  (29kb)", "http://arxiv.org/abs/1506.01900v1", null], ["v2", "Wed, 28 Oct 2015 19:02:22 GMT  (33kb)", "http://arxiv.org/abs/1506.01900v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["yossi arjevani", "ohad shamir"], "accepted": true, "id": "1506.01900"}, "pdf": {"name": "1506.01900.pdf", "metadata": {"source": "CRF", "title": "Communication Complexity of Distributed Convex Learning and Optimization", "authors": ["Yossi Arjevani"], "emails": ["yossi.arjevani@weizmann.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n01 90\n0v 1\n[ cs\n.L G\n] 5\nJ un"}, {"heading": "1 Introduction", "text": "We consider the problem of distributed convex learning and optimization, where a set of m machines, each with access to a different local convex function Fi : Rd 7\u2192 R and a convex domain W \u2286 Rd, attempt to solve the optimization problem\nmin w\u2208W\nF (w) where F (w) = 1\nm\nm\u2211\ni=1\nFi(w). (1)\nA prominent application is empirical risk minimization, where the goal is to minimize the average loss over some dataset, where each machine has access to a different subset of the data. Letting {z1, . . . , zm} be the dataset composed of N examples, and assuming the loss function \u2113(w, z) is convex in w, then the empirical risk minimization problem minw\u2208W 1N \u2211N i=1 \u2113(w, zi) can be written as in Eq. (1), where Fi(w) is the average loss over the subset of examples to which machine i has access. The main challenge in solving such problems is that communication between the different machines is usually slow and constrained, at least compared to the speed of local processing. On the other hand, the datasets involved in distributed learning are usually large and high-dimensional. Therefore, machines cannot simply communicate their entire data to each other, and the question is how well can we solve problems such as Eq. (1) using as little communication as possible.\nAs datasets continue to increase in size, and parallel computing platforms becoming more and more common (from multiple cores on a single CPU to large-scale and geographically distributed computing grids), distributed learning and optimization methods have been the focus of much research in recent years, with just a few examples including [23, 4, 2, 25, 1, 5, 12, 22, 15, 16, 8, 7, 9, 11, 19, 18, 3, 24]. Most of this work studied algorithms for this problem, which provide upper bounds on the time and communication complexity required.\nIn this paper, we take the opposite direction, and study what are the fundamental performance limitations in solving Eq. (1), under several different sets of assumptions. We identify cases where existing algorithms are already optimal (at least in the worst-case), as well as the cases where room for further improvement is still possible.\nSince a major constraint in distributed learning is communication, we focus on studying the amount of communication required to optimize Eq. (1) up to some desired accuracy \u01eb. More precisely, we consider the number of communication rounds that are required, where in each communication round the machines can generally broadcast to each other information linear in the problem\u2019s dimension d (e.g. a point in W or a gradient). This applies to virtually all algorithms for large-scale learning we are aware of, where sending vectors and gradients is feasible, but computing and sending larger objects, such as Hessians (d\u00d7d matrices) is not.\nOur results pertain to several possible settings (see Sec. 2 for precise definitions). First, we distinguish between the local functions being merely convex or strongly-convex, and whether they are smooth or not. These distinctions are standard in studying optimization algorithms for learning, and capture important properties such as the regularization and the type of loss function used. Second, we distinguish between a setting where the local functions are related \u2013 e.g., because they reflect the average loss of a dataset split uniformly at random between the machines \u2013 and a setting where no relationship is assumed. In the former case, one can show that quantities such as the values, gradients and Hessians of the local functions differ by \u03b4 = O(1/\u221an), where n is the sample size per machine, and this can be used to speed up the optimization/learning process (as was done in e.g. [19, 24]). Both of these settings can be studied in a unified way, by letting \u03b4 be a parameter and studying the attainable lower bounds as a function of \u03b4. Our results can be summarized as follows:\n\u2022 First, we define a mild structural assumption on the algorithm (which is satisfied by reasonable approaches we are aware of), which allows us to provide the lower bounds described below on the number of communication rounds required to reach a given suboptimality \u01eb.\n\u2013 When the local functions can be unrelated, we prove a lower bound of \u2126( \u221a\n1/\u03bb log(1/\u01eb)) for smooth and \u03bb-strongly convex functions, and \u2126( \u221a\n1/\u01eb) for smooth convex functions. These lower bounds are matched by a straightforward distributed implementation of accelerated gradient descent. In particular, the results imply that many communication rounds may be required to get a high-accuracy solution, and moreover, that no algorithm satisfying our structural assumption would be better, even if we endow the local machines with unbounded computational power. For non-smooth functions, we show a lower bound of \u2126( \u221a\n1/\u03bb\u01eb) for \u03bb-strongly convex functions, and \u2126(1/\u01eb) for general convex functions. We conjecture that the optimal algorithm in this case is again a straightforward distributed implementation of the subgradient method.\n\u2013 When the local functions are related (as quantified by the parameter \u03b4), we prove a communication round lower bound of \u2126( \u221a\n\u03b4/\u03bb log(1/\u01eb)) for smooth and \u03bb-strongly convex functions. For quadratics, this bound is matched by (up to constants and logarithmic factors) by the recently-proposed DISCO algorithm [24]. However, getting an optimal algorithm for general strongly convex and smooth functions in the \u03b4-related setting, let alone for non-smooth or non-strongly convex functions, remains open.\n\u2022 We also study the attainable performance without posing any structural assumptions on the algorithm, but in the more restricted case where only a single round of communication is allowed. We prove that in a broad regime, the performance of any distributed algorithm may be no better than a \u2018trivial\u2019 algorithm which returns the minimizer of one of the local functions, as long as the number of bits communicated is\nless than \u2126(d2). Therefore, in our setting, no communication-efficient 1-round distributed algorithm can provide non-trivial performance in the worst case.\nRelated Work\nThere have been several previous works which considered lower bounds in the context of distributed learning and optimization, but to the best of our knowledge, none of them provide a similar type of results. Perhaps the most closely-related paper is [21], which studies the communication complexity of distributed optimization, and showed that \u2126(d log(1/\u01eb)) bits of communication are necessary between the machines, for d-dimensional convex problems. However, in our setting this does not lead to any non-trivial lower bound on the number of communication rounds (indeed, just specifying a d-dimensional vector up to accuracy \u01eb required O(d log(1/\u01eb)) bits). More recently, [2] considered lower bounds for certain types of distributed learning problems, but not convex ones in an agnostic distribution-free framework. In the context of lower bounds for one-round algorithms, the results of [6] imply that \u2126(d2) bits of communication are required to solve linear regression in one round of communication. However, that paper assumes a different model than ours, where the function to be optimized is not split among the machines as in Eq. (1), where each Fi is convex. Moreover, issues such as strong convexity and smoothness are not considered. [19] proves an impossibility result for a one-round distributed learning scheme, even when the local functions are not merely related, but actually result from splitting data uniformly at random between machines. On the flip side, that result is for a particular algorithm, and doesn\u2019t apply to any possible method.\nFinally, we emphasize that distributed learning and optimization can be studied under many settings, including ones different than those studied here. For example, one can consider distributed learning on a stream of i.i.d. data [18, 7, 10, 8], or settings where the computing architecture is different, e.g. where the machines have a shared memory, or the function to be optimized is not split as in Eq. (1). Studying lower bounds in such settings is an interesting topic for future work."}, {"heading": "2 Notation and Framework", "text": "The only vector and matrix norms used in this paper are the Euclidean norm and the spectral norm, respectively. ej denotes the j-th standard unit vector. We let \u2207G(w) and \u22072G(w) denote the gradient and Hessians of a function G at w, if they exist. G is smooth (with parameter L) if it is differentiable and the gradient is L-Lipschitz. In particular, if w\u2217 = argminw\u2208W G(w), then G(w) \u2212G(w\u2217) \u2264 L2 \u2016w \u2212w\u2217\u2016\n2. G is strongly convex (with parameter \u03bb) if for any w,w\u2032 \u2208 W , G(w\u2032) \u2265 G(w)+ \u3008g,w\u2032\u2212w\u3009+ \u03bb2 \u2016w\u2032 \u2212w\u2016 2 where g \u2208 \u2202G(w\u2032) is a subgradient of G at w. In particular, if w\u2217 = argminw\u2208W G(w), then G(w) \u2212 G(w\u2217) \u2265 \u03bb2 \u2016w \u2212w\u2217\u2016\n2. Any convex function is also strongly-convex with \u03bb = 0. A special case of smooth convex functions are quadratics, where G(w) = w\u22a4Aw + b\u22a4w + c for some positive semidefinite matrix A, vector b and scalar c. In this case, \u03bb and L correspond to the smallest and largest eigenvalues of A.\nWe model the distributed learning algorithm as an iterative process, where in each round the machines may perform some local computations, followed by a communication round where each machine broadcasts a message to all other machines. We make no assumptions on the computational complexity of the local computations. After all communication rounds are completed, a designated machine provides the algorithm\u2019s output (possibly after additional local computation).\nClearly, without any assumptions on the number of bits communicated, the problem can be trivially solved in one round of communication (e.g. each machine communicates the function Fi to the designated machine, which then solves Eq. (1). However, in practical large-scale scenarios, this is non-feasible, and the\nsize of each message (measured by the number of bits) is typically on the order of O\u0303(d), enough to send a d-dimensional real-valued vector1, such as points in the optimization domain or gradients, but not larger objects such as d\u00d7 d Hessians.\nIn this model, our main question is the following: How many rounds of communication are necessary in order to solve problems such as Eq. (1) to some given accuracy \u01eb?\nAs discussed in the introduction, we first need to distinguish between different assumptions on the possible relation between the local functions. One natural situation is when no relationship whatsoever is assumed. In a learning context, this corresponds to a situation where data is arbitrarily split between the different machines, or when no statistical relationships can be assumed on the data at different machines. We denote this as the unrelated setting. However, this assumption is often unnecessarily pessimistic. For example, sometimes the data is assigned to machines randomly, or data at different machines are collected from statistically similar sources. In that case, the local functions aren\u2019t arbitrary, and in fact, due to concentration of measure effects, they can be quite similar to each other. For example, in the context of quadratic functions, we can capture this similarity using the following definition:\nDefinition 1. We say that a set of quadratic functions\nFi(w) := w \u22a4Aiw + biw + ci, Ai \u2208 Rd\u00d7d, bi \u2208 Rd, ci \u2208 R\nare \u03b4-related, if for any i, j \u2208 {1 . . . k}, it holds that\n\u2016Ai \u2212Aj\u2016 \u2264 \u03b4, \u2016bi \u2212 bj\u2016 \u2264 \u03b4, |ci \u2212 cj | \u2264 \u03b4\nFor example, in the context of linear regression with the squared loss over a bounded subset of Rd, and assuming mn data points with bounded norm are randomly and equally split among m machines, it can be shown that the conditions above hold with \u03b4 = O(1/\u221an) [19]. The choice of \u03b4 provides us with a spectrum of learning problems ranked by difficulty: When \u03b4 = \u2126(1), this generally corresponds to the unrelated setting discussed earlier. When \u03b4 = O(1/\u221an), we get the situation typical of randomly partitioned data. When \u03b4 = 0, then all the local functions have essentially the same minimizers, in which case Eq. (1) can be trivially solved with zero communication, just by letting one machine optimize its own local function. We note that although Definition 1 can be generalized to non-quadratic functions, we do not need it for the results presented here.\nWe end this section with an important remark. In this paper, we prove lower bounds for the \u03b4-related setting, which also apply when \u03b4 = O(1/\u221an), the typical regime for e.g. distributed learning on randomly partitioned data. However, we emphasize that our constructions do not actually correspond to some randomly partitioned datasets. Thus, the lower bounds apply to any algorithm for randomly-partitioned data, which works only by assuming that the difference in the gradients or Hessians of the local functions are bounded by some \u03b4, such as [19, 24]. In fact, we are not aware of any existing method, using multiple communication rounds, which handles this setting in a different way, and we suspect this may be inevitable: After multiple communication rounds, the initial statistical independence between the data points (conditioned on the algorithm\u2019s state) gets quickly destroyed, so it seems we are only left with deterministic assumptions on the similarity of the local functions.\n1The O\u0303 hides constants and factors logarithmic in the required accuracy of the solution. The idea is that we can represent real numbers up to some arbitrarily high machine precision, enough so that finite-precision issues are not a problem."}, {"heading": "3 Lower Bounds Using a Structural Assumption", "text": "In this section, we present lower bounds on the number of communication rounds, where we impose a certain mild structural assumption on the operations performed by the algorithm. Roughly speaking, our lower bounds pertain to a very large class of algorithms, which are based on linear operations involving points, gradients, and vector products with local Hessians and their inverses, as well as solving local optimization problems involving such quantities. At each communication round, the machines can share any of the vectors they have computed so far. Formally, we consider algorithms which satisfy the assumption stated below. For convenience, we state it for smooth functions (which are differentiable) and discuss the case of non-smooth functions in Sec. 3.2.\nAssumption 1. For each machine j, define a set Wj \u2282 Rd, initially Wj = {0}. Between communication rounds, each machine j iteratively computes and adds to Wj some finite number of points w, each satisfying\n\u03b3w + \u03bd\u2207Fj(w) \u2208 span { w\u2032 , \u2207Fj(w\u2032) , (\u22072Fj(w\u2032) +D)w\u2032\u2032 , (\u22072Fj(w\u2032) +D)\u22121w\u2032\u2032 \u2223 \u2223 \u2223\nw\u2032,w\u2032\u2032 \u2208 Wj , D diagonal , \u22072Fj(w\u2032) exists , (\u22072Fj(w\u2032) +D)\u22121 exists } . (2)\nfor some \u03b3, \u03bd \u2265 0 such that \u03b3 + \u03bd > 0. After every communication round, let Wj := \u222ami=1Wi for all j. The algorithm\u2019s final output (provided by the designated machine j) is a point in the span of Wj .\nThis assumption requires several remarks:\n\u2022 Note that Wj is not an explicit part of the algorithm: It simply includes all points computed by machine j so far, or communicated to it by other machines, and is used to define the set of new points which the machine is allowed to compute.\n\u2022 The assumption bears some resemblance \u2013 but is far weaker \u2013 than standard assumptions used to provide lower bounds for iterative optimization algorithms. For example, a common assumption (see [13]) is that each computed point w must lie in the span of the previous gradients. This corresponds to a special case of Assumption 1, where \u03b3 = 1, \u03bd = 0, and the span is only over gradients of previously computed points. However, it also allows (for instance) exact optimization of each local function, which is a subroutine in some distributed algorithms (e.g. [25, 23]), by setting \u03b3 = 0, \u03bd = 1 and computing a point w satisfying \u03b3w+\u03bd\u2207Fj(w) = 0. By allowing the span to include previous gradients, we also incorporate algorithms which perform optimization of the local function plus terms involving previous gradients and points, such as [19], as well as algorithms which rely on local Hessian information and preconditioning, such as [24]. In summary, the assumption is satisfied by most techniques for black-box convex optimization that we are aware of. Finally, we emphasize that we do not restrict the number or computational complexity of the operations performed between communication rounds.\n\u2022 The requirement that \u03b3, \u03bd \u2265 0 is to exclude algorithms which solve non-convex local optimization problems of the form minw Fj(w) + \u03b3 \u2016w\u20162 + \u00b7 \u00b7 \u00b7 with \u03b3 < 0, which are unreasonable in practice and can sometimes break our lower bounds.\n\u2022 The assumption that Wj is initially {0} (namely, that the algorithm starts from the origin) is purely for convenience, and our results can be easily adapted to any other starting point by shifting all functions accordingly.\nThe techniques we employ in this section are inspired by lower bounds on the iteration complexity of first-order methods for standard (non-distributed) optimization (see for example [13]). These are based on the construction of \u2018hard\u2019 functions, where each gradient (or subgradient) computation can only provide a small improvement in the objective value. In our setting, the dynamics are roughly similar, but the necessity of many gradient computations is replaced by many communication rounds. This is achieved by constructing suitable local functions, where at any time point no individual machine can \u2018progress\u2019 on its own, without information from other machines."}, {"heading": "3.1 Smooth Local Functions", "text": "We begin by presenting a lower bound when the local functions Fi are strongly-convex and smooth:\nTheorem 1. For any even number m of machines, any distributed algorithm which satisfies Assumption 1, and for any \u03bb \u2208 [0, 1), \u03b4 \u2208 (0, 1), there exist m local quadratic functions over Rd (where d is sufficiently large) which are 1-smooth, \u03bb-strongly convex, and \u03b4-related, such that if w\u2217 = argmin\nw\u2208Rd F (w), then the number of communication rounds required to obtain w\u0302 satisfying F (w\u0302) \u2212 F (w\u2217) \u2264 \u01eb (for any \u01eb > 0) is at least\n1\n4\n(\u221a\n1 + \u03b4\n( 1\n\u03bb \u2212 1\n) \u2212 1 ) log (\n\u03bb \u2016w\u2217\u20162 4\u01eb\n)\n\u2212 1 2 = \u2126\n(\u221a\n\u03b4 \u03bb log\n(\n\u03bb \u2016w\u2217\u20162 \u01eb\n))\nif \u03bb > 0, and at least \u221a\n3\u03b4 32\u01eb \u2016w\u2217\u2016 \u2212 2 if \u03bb = 0.\nThe assumption of m being even is purely for technical convenience, and can be discarded at the cost of making the proof slightly more complex. Also, note that m does not appear explicitly in the bound, but does appear implicitly, via \u03b4: As discussed previously, in a statistical setting \u03b4 decays with \u221a\n1/n, where n is the number of examples per machine. Hence, for the same total number of examples, more machines m implies that \u03b4 is larger.\nLet us contrast our lower bound with some existing algorithms and guarantees in the literature. First, regardless of whether the local functions are similar or not, we can always simulate any gradient-based method designed for a single machine, by iteratively computing gradients of the local functions, and performing a communication round to compute their average. Clearly, this will be a gradient of the objective function F (\u00b7) = 1m \u2211m i=1 Fi(\u00b7), which can be fed into any gradient-based method such as gradient descent or accelerated gradient descent [13]. The resulting number of required communication rounds is then equal to the number of iterations. In particular, using accelerated gradient descent for smooth and \u03bb-strongly convex functions yields a round complexity of O( \u221a 1/\u03bb log(\u2016w\u2217\u20162 /\u01eb)), and O(\u2016w\u2217\u20162 \u221a\n1/\u01eb) for smooth convex functions. This matches our lower bound (up to constants and log factors) when the local functions are unrelated (\u03b4 = \u2126(1)).\nWhen the functions are related, however, the upper bounds above are highly sub-optimal: Even if the local functions are completely identical, and \u03b4 = 0, the number of communication rounds will remain the same as when \u03b4 = \u2126(1). To utilize function similarity while guaranteeing arbitrary small \u01eb, the two most relevant algorithms are DANE [19], and the more recent DISCO [24]. For smooth and \u03bb-strongly convex functions, which are either quadratic or satisfy a certain self-concordance condition, DISCO achieves O\u0303(1 + \u221a\n\u03b4/\u03bb) round complexity ([24, Thm.2]), which matches our lower bound in terms of dependence on \u03b4, \u03bb. As discussed in those papers, in statistical learning problems, where each machine is given n random data points, \u03b4 usually scales as 1/ \u221a n, and \u03bb as 1/ \u221a mn, so with the bound above, the total number of\ncommunication rounds is only O\u0303( 4\u221am). However, for non-quadratic losses, the round complexity bounds are somewhat worse, and there are no guarantees for strongly convex and smooth functions which are not self-concordant. Thus, the question of the optimal round complexity for such functions remains open.\nThe full proof of Thm. 1 appears in the appendix, and is based on the following idea: For simplicity, suppose we have two machines, with local functions F1, F2 defined as follows:\nF1(w) = \u03b4(1 \u2212 \u03bb)\n4 w\u22a4A1w \u2212 \u03b4(1 \u2212 \u03bb) 2 e\u22a41 w + \u03bb 2 \u2016w\u20162 (3)\nF2(w) = \u03b4(1 \u2212 \u03bb)\n4 w\u22a4A2w +\n\u03bb 2 \u2016w\u20162 , where\nA1 =\n\n        1 0 0 0 0 0 . . . 0 1 \u22121 0 0 0 . . . 0 \u22121 1 0 0 0 . . . 0 0 0 1 \u22121 0 . . . 0 0 0 \u22121 1 0 . . . ... ... ... ... ... ... ...\n\n        , A2 =\n\n          1 \u22121 0 0 0 0 . . . \u22121 1 0 0 0 0 . . . 0 0 1 \u22121 0 0 . . . 0 0 \u22121 1 0 0 . . . 0 0 0 0 1 \u22121 . . . 0 0 0 0 \u22121 1 . . . ... ... ... ... ... ... ...\n\n         \nIt is easy to verify that for \u03b4, \u03bb \u2264 1, both F1(w) and F2(w) are 1-smooth and \u03bb-strongly convex, as well as \u03b4-related. Moreover, the optimum of their average is a point w\u2217 with non-zero entries at all coordinates. However, since each local functions has a block-diagonal quadratic term, it can be shown that for any algorithm satisfying Assumption 1, after T communication rounds, the points computed by the two machines can only have the first T+1 coordinates non-zero. No machine will be able to further \u2018progress\u2019 on its own, and cause additional coordinates to become non-zero, without another communication round. This leads to a lower bound on the optimization error which depends on T , resulting in the theorem statement after a few computations."}, {"heading": "3.2 Non-smooth Local Functions", "text": "Remaining in the framework of algorithms satisfying Assumption 1, we now turn to discuss the situation where the local functions are not necessarily smooth or differentiable. For simplicity, our formal results here will be in the unrelated setting, and we only informally discuss their extension to a \u03b4-related setting (in a sense relevant to non-smooth functions). Formally defining \u03b4-related non-smooth functions is possible but not altogether trivial, and is therefore left to future work.\nWe adapt Assumption 1 to the non-smooth case, by allowing gradients to be replaced by arbitrary subgradients at the same points. Namely, we replace Eq. (2) by the requirement that for some g \u2208 \u2202Fj(w), and \u03b3, \u03bd \u2265 0, \u03b3 + \u03bd > 0,\n\u03b3w + \u03bdg \u2208 span { w\u2032 , g\u2032 , (\u22072Fj(w\u2032) +D)w\u2032\u2032 , (\u22072Fj(w\u2032) +D)\u22121w\u2032\u2032 \u2223 \u2223 \u2223\nw\u2032,w\u2032\u2032 \u2208 Wj , g\u2032 \u2208 \u2202Fj(w\u2032) , D diagonal , \u22072Fj(w\u2032) exists , (\u22072Fj(w\u2032) +D)\u22121 exists } .\nThe lower bound for this setting is stated in the following theorem.\nTheorem 2. For any even number m of machines, any distributed optimization algorithm which satisfies Assumption 1, and for any \u03bb \u2265 0, there exist (1 + \u03bb)-Lipschitz continuous convex local functions F1(w) and F2(w) over the unit Euclidean ball in Rd (where d is sufficiently large), such that if\nw\u2217 = argmin w:\u2016w\u2016\u22641 F (w), the number of communication rounds required to obtain w\u0302 satisfying F (w\u0302)\u2212 F (w\u2217) \u2264 \u01eb (for any sufficiently small \u01eb > 0) is 18\u01eb \u2212 2 for \u03bb = 0, and \u221a 1 16\u03bb\u01eb \u2212 2 for \u03bb > 0.\nAs in Thm. 1, we note that the assumption of even m is for technical convenience. This theorem, together with Thm. 1, implies that both strong convexity and smoothness are necessary for the number of communication rounds to scale logarithmically with the required accuracy \u01eb. We emphasize that this is true even if we allow the machines unbounded computational power, to perform arbitrarily many operations satisfying Assumption 1. In terms of tightness, we are not aware of algorithms matching this lower bound. One natural candidate is a distributed implementation of the subgradient method [13], where each iteration we use a communication round to compute a subgradient with respect to the average function F . The number of communication rounds is then O(1/\u01eb2) in the general convex case [13], and O(1/\u03bb\u01eb) in the strongly convex case [14]. We conjecture that this approach is actually optimal when the local functions are unrelated, and that it should be possible to strengthen our lower bounds. For the \u03b4-related setting, and non-smooth functions, we are not aware of any relevant algorithms in the literature adapted to such a setting, and the question of attainable performance there remains wide open.\nThe full proof of Thm. 2 appears in the appendix. The proof idea relies on the following construction: Assume that we fix the number of communication rounds to be T , and (for simplicity) that T is even and the number of machines is 2. Then we use local functions of the form\nF1(w) = 1\u221a 2 |b\u2212 w1|+ 1 \u221a 2(T + 2) (|w2 \u2212 w3|+ |w4 \u2212 w5|+ \u00b7 \u00b7 \u00b7+ |wT \u2212wT+1|) + \u03bb 2 \u2016w\u20162\nF2(w) = 1 \u221a\n2(T + 2) (|w1 \u2212 w2|+ |w3 \u2212 w4|+ \u00b7 \u00b7 \u00b7 + |wT+1 \u2212 wT+2|) +\n\u03bb 2 \u2016w\u20162 ,\nwhere b is a suitably chosen parameter. Being a sum of convex functions, both local functions are convex, and can be shown to be 1-Lipschitz. Similar to the smooth case, we argue that after T communication rounds, the resulting points w computed by machine 1 will be non-zero only on the first T + 1 coordinates, and the points w computed by machine 2 will be non-zero only on the first T coordinates. Intuitively, this is because for any such w, there exist subgradients of the local function which will be zero on the zero coordinates, and the Hessians of F1, F2 (where they exist) are always zero, so no algorithm satisfying Assumption 1 will allow machines to \u2018progress\u2019 on their own without a communication round.\nFinally, although the result is in the unrelated setting, it is straightforward to have a similar construction in a \u2018\u03b4-related\u2019 setting, by multiplying F1 and F2 by \u03b4. The resulting two functions have their gradients and subgradients at most \u03b4-different from each other, and the construction above leads to a lower bound of \u2126(\u03b4/\u01eb) for convex Lipschitz functions, and \u2126(\u03b4 \u221a 1/\u03bb\u01eb) for \u03bb-strongly convex Lipschitz functions."}, {"heading": "4 One Round of Communication", "text": "In this section, we study what lower bounds are attainable without any kind of structural assumption (such as Assumption 1). This is a more challenging setting, and the result we present will be limited to algorithms using a single round of communication round. We note that this still captures a realistic non-interactive distributed computing scenario, where we want each machine to broadcast a single message, and a designated machine is then required to produce an output. In the context of distributed optimization, a natural example is a one-shot averaging algorithm, where each machine optimizes its own local data, and the resulting points are averaged (e.g. [25, 23]).\nIntuitively, with only a single round of communication, getting an arbitrarily small error \u01eb may be infeasible. The following theorem establishes a lower bound on the attainable error, depending on the strong convexity parameter \u03bb and the similarity measure \u03b4 between the local functions, and compares this with a \u2018trivial\u2019 zero-communication algorithm, which just returns the optimum of a single local function:\nTheorem 3. For any even number m of machines, any dimension d larger than some numerical constant, any \u03b4 \u2265 3\u03bb > 0, and any (possibly randomized) algorithm which communicates at most d2/128 bits in a single round of communication, there exist m quadratic functions over Rd, which are \u03b4-related, \u03bb-strongly convex and 9\u03bb-smooth, for which the following hold for some positive numerical constants c, c\u2032:\n\u2022 The point w\u0302 returned by the algorithm satisfies\nE\n[\nF (w\u0302)\u2212 min w\u2208Rd F (w)\n]\n\u2265 c\u03b4 2\n\u03bb\nin expectation over the algorithm\u2019s randomness.\n\u2022 For any machine j, if w\u0302j = argminw\u2208Rd Fj(w), then F (w\u0302j)\u2212minw\u2208Rd F (w) \u2264 c\u2032\u03b42/\u03bb.\nThe theorem shows that unless the communication budget is extremely large (quadratic in the dimension), there are functions which cannot be optimized to non-trivial accuracy in one round of communication, in the sense that the same accuracy (up to a universal constant) can be obtained with a \u2018trivial\u2019 solution where we just return the optimum of a single local function. This complements an earlier result in [19], which showed that a particular one-round algorithm is no better than returning the optimum of a local function, under the stronger assumption that the local functions are not merely \u03b4-related, but are actually the average loss over some randomly partitioned data.\nThe full proof appears in the appendix, but we sketch the main ideas below. As before, focusing on the case of two machines, and assuming machine 2 is responsible for providing the output, we use\nF1(w) = 3\u03bbw \u22a4 (( I + 1\n2c \u221a d M\n)\u22121 \u2212 1\n2 I\n)\nw\nF2(w) = 3\u03bb\n2 \u2016w\u20162 \u2212 \u03b4ej ,\nwhere M is essentially a randomly chosen {\u22121,+1}-valued d \u00d7 d symmetric matrix with spectral norm at most c \u221a d, and c is a suitable constant. These functions can be shown to be \u03b4-related as well as \u03bb-strongly convex. Moreover, the optimum of F (w) = 12(F1(w) + F2(w)) equals\nw\u2217 = \u03b4\n6\u03bb\n(\nI + 1\n2c \u221a d M\n)\nej .\nThus, we see that the optimal point w\u2217 depends on the j-th column of M . Intuitively, the machines need to approximate this column, and this is the source of hardness in this setting: Machine 1 knows M but not j, yet needs to communicate to machine 2 enough information to construct its j-th column. However, given a communication budget much smaller than the size of M (which is d2), it is difficult to convey enough information on the j-th column without knowing what j is. Carefully formalizing this intuition, and using some information-theoretic tools, allows us to prove the first part of Thm. 3. Proving the second part of Thm. 3 is straightforward, using a few computations."}, {"heading": "5 Summary and Open Questions", "text": "In this paper, we studied lower bounds on the number of communication rounds needed to solve distributed convex learning and optimization problems, under several different settings. Our results indicate that when the local functions are unrelated, then regardless of the local machines\u2019 computational power, many communication rounds may be necessary (scaling polynomially with 1/\u01eb or 1/\u03bb), and that the worst-case optimal algorithm (at least for smooth functions) is just a straightforward distributed implementation of accelerated gradient descent. When the functions are related, we show that the optimal performance is achieved by the algorithm of [24] for quadratic and strongly convex functions, but designing optimal algorithms for more general functions remains open. Beside these results, which required a certain mild structural assumption on the algorithm employed, we also provided an assumption-free lower bound for one-round algorithms, which implies that even for strongly convex quadratic functions, such algorithms can sometimes only provide trivial performance.\nBesides the question of designing optimal algorithms for the remaining settings, several additional questions remain open. First, it would be interesting to get assumption-free lower bounds for algorithms with multiple rounds of communication. Second, our work focused on communication complexity, but in practice the computational complexity of the local computations is no less important. Thus, it would be interesting to understand what is the attainable performance with simple, runtime-efficient algorithms. Finally, it would be interesting to study lower bounds for other distributed learning and optimization scenarios."}, {"heading": "Acknowledgments", "text": "This research is supported by the Intel ICRI-CI Institute, Israel Science Foundation grant 425/13, and an FP7 Marie Curie CIG grant."}, {"heading": "A Proofs", "text": "A.1 Proof of Thm. 1\nThe proof of the theorem is based on splitting the machines into two sub-groups of the same size, each of which is assigned with a finite dimensional restriction of F1 and F2 (see Eq. (3)), and tracing the maximal number of non-zero coordinates for vectors in Wj , the set of feasible points.\nRecall that Fi are defined as follows:\nF1(w) = \u03b4(1 \u2212 \u03bb)\n4 w\u22a4A1w \u2212 \u03b4(1 \u2212 \u03bb) 2 e\u22a41 w + \u03bb 2 \u2016w\u20162\nF2(w) = \u03b4(1 \u2212 \u03bb)\n4 w\u22a4A2w +\n\u03bb 2 \u2016w\u20162 , where\nA1 =\n\n        1 0 0 0 0 0 . . . 0 1 \u22121 0 0 0 . . . 0 \u22121 1 0 0 0 . . . 0 0 0 1 \u22121 0 . . . 0 0 0 \u22121 1 0 . . . ... ... ... ... ... ... ...\n\n        , A2 =\n\n          1 \u22121 0 0 0 0 . . . \u22121 1 0 0 0 0 . . . 0 0 1 \u22121 0 0 . . . 0 0 \u22121 1 0 0 . . . 0 0 0 0 1 \u22121 . . . 0 0 0 0 \u22121 1 . . . ... ... ... ... ... ... ...\n\n         \nFormally speaking, we consider the matrices A1, A2 as infinite in size, so that each Fi is defined over \u21132(R), the space of square-summable sequences. To derive lower bounds in Rd, we consider the following restrictions of Fi and F :\n[Fi]d(w) := Fi(w1, w2, . . . , wd, 0, 0, . . . ), w \u2208 Rd\n[F ]d(w) := [F1]d(w) + [F2]d(w)\n2\nNote that [Fi]d(w) and [F ]d(w) produce the same values as Fi(w) and F (w) do for vectors such that wi = 0 for all i \u2265 d. Similarly, we define the d\u00d7 d leading principal submatrices of Ai by [Ai]d.\nWe assign half of the machines with [F1]d, and the other half with [F2]d. To prove the theorem, we need the following lemma, which formalizes the intuition described in the main paper. Let\nE0,d = {0} , ET,d = span{e1,d, . . . , eT,d},\nwhere ei,d \u2208 Rd denote the standard unit vectors. Then, the following holds:\nLemma 1. Suppose all the sets of feasible points satisfy Wj \u2286 ET,d for some T \u2264 d \u2212 1, then under assumption 1, right after the next communication round we have Wj \u2286 ET+1,d.\nProof. Recall that by Assumption 1, each machine can compute new points w that satisfy the following for some \u03b3, \u03bd \u2265 0 such that \u03b3 + \u03bd > 0:\n\u03b3w + \u03bd\u2207[Fi]d(w) \u2208 span { w\u2032 , \u2207[Fi]d(w\u2032) , (\u22072[Fi]d(w\u2032) +D)w\u2032\u2032 , (\u22072[Fi]d(w\u2032) +D)\u22121w\u2032\u2032 \u2223 \u2223 \u2223\nw\u2032,w\u2032\u2032 \u2208 Wj , D diagonal , \u22072[Fi]d(w\u2032) exists , (\u22072[Fi]d(w\u2032) +D)\u22121 exists } .\nWe now analyze the state of the sets of feasible points prior to the next communication round. Assume that T is an odd number, i.e., assume T = 2k+1 for some k = 0, 1, . . . . The proof for the case where T is even follows similar lines. Note that for any w\u2032,w\u2032\u2032 \u2208 Wj , we have\n\u2207[F1]d(w\u2032) = \u03b4(1 \u2212 \u03bb)\n2 [A1]dw \u2032 \u2212 \u03b4(1 \u2212 \u03bb) 2 e1 + \u03bb 2 w\u2032 \u2286 E2k+1,d\n(\u22072[F1]d(w\u2032) +D)w\u2032\u2032 = ( \u03b4(1 \u2212 \u03bb)\n2 [A1]d +D + \u03bbI\n)\nw\u2032\u2032 \u2286 E2k+1,d\n(\u22072[F1]d(w\u2032) +D)\u22121w\u2032\u2032 = ( \u03b4(1 \u2212 \u03bb)\n2 [A1]d +D + \u03bbI\n)\u22121 w\u2032\u2032 \u2286 E2k+1,d\nFor any viable diagonal matrix D. Therefore, since Wj \u2286 E2k+1,d, we have that the first point generated by machines which hold [F1]d(w) must satisfy\n\u03b3w + \u03bd\u2207[F1]d(w) \u2208 E2k+1,d\nfor \u03b3, \u03bd as stated in the assumption. That is, ( \u03b4(1 \u2212 \u03bb)\n2 \u03bd[A1]d + (\u03b3 +\n\u03bd\u03bb\n2 )I\n)\nw \u2212 \u03b4(1 \u2212 \u03bb) 2 e1 \u2208 E2k+1,d\nWhich implies, ( \u03b4(1 \u2212 \u03bb)\n2 \u03bd[A1]d + (\u03b3 +\n\u03bd\u03bb\n2 )I\n)\n\ufe38 \ufe37\ufe37 \ufe38\nH\nw \u2208 E2k+1,d\nSince [A1]d is positive semidefinite, it holds that H is invertible. Also, [A1]d,H and H\u22121 admit the same partitions into 1\u00d7 1 and 2\u00d7 2 blocks on the diagonal, thus H\u22121E2k+1,d \u2286 E2k+1,d, yielding w \u2208 E2k+1,d. Inductively extending the latter argument shows that, in the absence of any communication rounds, all the machines whose local function is [F1]d(w) are \u2018stuck\u2019 in E2k+1,d.\nAs for machines which contain [F2]d(w), we have that for all w\u2032,w\u2032\u2032 \u2208 Wj\n\u2207[F2]d(w\u2032) = \u03b4(1 \u2212 \u03bb)\n2 [A2]dw\n\u2032 + \u03bb 2 w\u2032 \u2286 E2k+2,d\n(\u22072[F2]d(w\u2032) +D)w\u2032\u2032 = ( \u03b4(1 \u2212 \u03bb)\n2 [A2]d +D + \u03bbI\n)\nw\u2032\u2032 \u2286 E2k+2,d\n(\u22072[F2]d(w\u2032) +D)\u22121w\u2032\u2032 = ( \u03b4(1 \u2212 \u03bb)\n2 [A2]d +D + \u03bbI\n)\u22121 w\u2032\u2032 \u2286 E2k+2,d\nFor any viable diagonal matrix D. Therefore, the first generated point by these machines must satisfy,\n\u03b3w + \u03bd\u2207[F1]d(w) \u2208 E2k+2,d\nfor appropriate \u03b3, \u03bd. Hence, ( \u03b4(1 \u2212 \u03bb)\n2 \u03bd[A2]d + (\u03b3 +\n\u03bd\u03bb\n2 )I\n)\nw \u2208 E2k+2,d\nSimilarly to the previous case this implies that w \u2208 E2k+2,d. It is now left to show that these machines cannot make further progress beyond E2k+2,d without communicating. To see this, note that for all w\u2032,w\u2032\u2032 \u2208 E2k+2,d we have,\n\u2207[F2]d(w\u2032) = \u03b4(1\u2212 \u03bb)\n2 [A2]dw\n\u2032 + \u03bb 2 w\u2032 \u2286 E2k+2,d\n(\u22072[F2]d(w\u2032) +D)w\u2032\u2032 = ( \u03b4(1 \u2212 \u03bb)\n2 [A2]d +D + \u03bbI\n)\nw\u2032\u2032 \u2286 E2k+2,d\n(\u22072[F2]d(w\u2032) +D)\u22121w\u2032\u2032 = ( \u03b4(1 \u2212 \u03bb)\n2 [A2]d +DI + \u03bbI\n)\u22121 w\u2032\u2032 \u2286 E2k+2,d\nThis means that all the points which are generated subsequently also lie in E2k+2,d, i.e., without communicating , machines whose local function is [F2]d(w) are stuck in E2k+2,d. Finally, executing a communication round updates all the sets of feasible points to be Wj := E2k+2,d.\nThe following is a direct consequence of a recursive application of Lemma 1.\nCorollary 1. Under assumption 1, after T \u2264 d\u2212 1 communication rounds we have\nWj \u2286 ET+1, j \u2208 {1, . . . ,m}\nWith this corollary in hand, we now turn to prove the main result. First, we compute the minimizer of\nthe average function F (w) = m 2 F1(w)+ m 2 F2(w) m in \u2113 2(R), denoted by w\u2217, whose form for even number of machines is simply:\nF (w) = \u03b4(1 \u2212 \u03bb)\n8 w\u22a4 (A1 +A2)w \u2212 \u03b4(1 \u2212 \u03bb) 4 e\u22a41 w + \u03bb 2 \u2016w\u20162\nBy first-order optimality condition for smooth convex functions, we have ( \u03b4(1 \u2212 \u03bb)\n4 (A1 +A2) + \u03bbI\n)\nw\u2217 \u2212 \u03b4(1 \u2212 \u03bb) 4 e1 = 0,\nor equivalently, (\nA1 +A2 + 4\u03bb \u03b4(1 \u2212 \u03bb)I ) w\u2217 = e1\nwhose coordinate form is as follows (\n2 + 4\u03bb\n\u03b4(1 \u2212 \u03bb)\n)\nw\u2217[1]\u2212w\u2217[2] = 1 (4)\n\u2200 k , w\u2217[k + 1]\u2212 ( 2 + 4\u03bb\n\u03b4(1 \u2212 \u03bb)\n)\nw\u2217[k] +w\u2217[k \u2212 1] = 0. (5)\nThe optimal solution can be now realized as a geometric sequence (\u03b6k)\u221ek=1 for some \u03b6 as follows: By Eq. (5), we must have\n\u03b62 \u2212 ( 2 + 4\u03bb\n\u03b4(1 \u2212 \u03bb)\n)\n\u03b6 + 1 = 0,\nwith the smallest root being\n\u03b6 = 2 + 4\u03bb\u03b4(1\u2212\u03bb) \u2212\n\u221a (\n2 + 4\u03bb\u03b4(1\u2212\u03bb)\n)2 \u2212 4\n2 = 1 +\n2\u03bb\n\u03b4(1 \u2212 \u03bb) \u2212\n\u221a (\n1 + 2\u03bb\n\u03b4(1 \u2212 \u03bb)\n)2\n\u2212 1\n(6)\nTherefore, this choice of \u03b6 satisfies Eq. (5), and it is straightforward to verify that it also satisfies Eq. (4), hence w\u2217 indeed equals (\u03b6k)\u221ek=1. It will be convenient to denote a continuous range of coordinates of (\u03b6k)\u221ek=1 by \u03b6a:b where a \u2208 N and b \u2208 N \u222a\u221e. Also, using the following inequality which holds for x > 1\nx\u2212 \u221a x2 \u2212 1 \u2265 exp\n  \u22122\n\u221a x+1 x\u22121 \u2212 1\n\n , x > 1\ntogether with Eq. (6) yields\n\u03b6 \u2265 exp ( \u22122 \u221a\n\u03b4(1/\u03bb \u2212 1) + 1\u2212 1\n)\n(7)\nWe now use this computation (with respect to F1, F2) to find the minimizer of [F ]d, defined as the average function of the finite-dimensional restrictions [F1]d, [F2]d actually handed to the machines. Fix d \u2208 N and denote the corresponding minimizer by\nw\u2217d = arg min w\u2208Rd [F ]d(w)\nLet wT be some point which was obtained after T \u2264 d \u2212 2 communication rounds. To bound the suboptimality of wT from below, observe that\n[F ]d(wT )\u2212 [F ]d(w\u2217d) \u2265 [F ]d(wT )\u2212 [F ]d(\u03b61:d\u22121) = [F ]d(wT )\u2212 F (w\u2217) + F (w\u2217)\u2212 [F ]d(\u03b61:d\u22121) = F (wT )\u2212 F (w\u2217)\n\ufe38 \ufe37\ufe37 \ufe38\nA\n+F (w\u2217)\u2212 F (\u03b61:d\u22121) \ufe38 \ufe37\ufe37 \ufe38\nB\nwhere the last equality follows from Corollary (1), according to which all the coordinates of wT , except for the first T + 1 \u2264 d\u2212 1, must vanish. To bound the A term, note that\n\u2016wT \u2212w\u2217\u20162 \u2265 \u221e\u2211\nt=T+2\n\u03b62t = \u03b62(T+1) \u221e\u2211\nt=1\n\u03b62t = \u03b62(T+1) \u2016w\u2217\u20162\nThe fact that F (w) is \u03bb-strongly convex implies\nF (wT )\u2212 F (w\u2217) \u2265 \u03bb 2 \u2016wT \u2212w\u2217\u20162 \u2265\n\u03bb\u03b62(T+1)\n2 \u2016w\u2217\u20162 .\nInequality (7) yields\nF (wT )\u2212 F (w\u2217) \u2265 \u03bb\n2 exp\n(\n\u22124T \u2212 4 \u221a\n\u03b4(1/\u03bb \u2212 1) + 1\u2212 1\n)\n\u2016w\u2217\u20162\nTo bound the B term from below, note that since F is 1-smooth we have\nF (w\u2217)\u2212 F (\u03b61:d\u22121) \u2265 \u2212 1\n2\n\u2225 \u2225w\u2217 \u2212 \u03b61:d\u22121 \u2225 \u22252 = \u2212\u03b6\n2(d\u22121)\n2\n\u221e\u2211\nt=1\n\u03b62t = \u2212\u03b6 2(d\u22121)\n2 \u2016w\u2217\u20162\nCombining both lower bounds for the terms A and B, we get for any T \u2264 d\u2212 2\n[F ]d(wT )\u2212 [F ]d(w\u2217d) \u2265 ( \u03bb\n2 exp\n(\n\u22124T \u2212 4 \u221a\n\u03b4(1/\u03bb \u2212 1) + 1\u2212 1\n)\n\u2212 \u03b6 2(d\u22121)\n2\n)\n\u2016w\u2217\u20162\nPicking d sufficiently large, and considering how large the number of communication rounds T must be to make this lower bound less than \u01eb, we get\nT \u2265 \u221a\n\u03b4(1/\u03bb \u2212 1) + 1\u2212 1 4 ln\n(\n\u03bb \u2016w\u2217\u20162 4\u01eb\n)\n\u2212 1.\nIt is worth mentioning that by computing the exact minimizers of [Fi]d one may derive a lower bound such that the choice of d does not depend on the parameters of the problem, except for the number of communication rounds. Nevertheless, such analysis requires a more involved reasoning which we find unnecessary for stating our results.\nFor the non-strongly convex case, where \u03bb = 0, using Corollary 1 and a similar analysis (virtually identical to the proof of Theorem 2.1.7 in [13]), we have that if T \u2264 12 (d\u2212 1), then\nF (wT )\u2212 F (w\u2217) \u2265 3\u03b4\u2016w\u2217\u20162\n32(T + 2)2\nTherefore, to obtain an \u01eb-suboptimal solution for this case, we must have at least\n\u221a\n3\u03b4\n32\u01eb \u2016w\u2217\u2016 \u2212 2\ncommunication rounds, for sufficiently small \u01eb.\nA.2 Proof of Thm. 2\nWe construct two types of local functions, and provide one of them to m/2 of the machines, and the other function to the other m/2 machines, in some arbitrary order. In this case, the average function is simply the average of the two types of local functions.\nWe will first prove the theorem statement in the strongly convex case, where \u03bb > 0 is given, and then explain how to extract from it the result in the non strongly convex case.\nFix a natural number k and some b \u2208 [0, 1/ \u221a k], to be specified later. We define the following local\nfunction over the unit ball:\nF1,k(w) = 1\u221a 2 |b\u2212 w[1]|+ 1\u221a 2k (|w[2] \u2212 w[3]| + |w[4] \u2212 w[5]| + \u00b7 \u00b7 \u00b7+ |w[k \u2212 2]\u2212 w[k \u2212 1]|) + \u03bb 2 \u2016w\u20162 F2,k(w) = 1\u221a 2k (|w[1] \u2212 w[2]| + |w[3]\u2212 w[4]| + \u00b7 \u00b7 \u00b7+ |w[k \u2212 1]\u2212w[k]|) + \u03bb 2 \u2016w\u20162 (8)\nFor even k \u2264 d, and\nF1,k(w) = 1\u221a 2 |b\u2212 w[1]| + 1\u221a 2k (|w[2] \u2212w[3]| + |w[4]\u2212 w[5]| + \u00b7 \u00b7 \u00b7+ |w[k \u2212 1]\u2212 w[k]|) + \u03bb 2 \u2016w\u20162 F2,k(w) = 1\u221a 2k (|w[1] \u2212 w[2]| + |w[3] \u2212 w[4]| + \u00b7 \u00b7 \u00b7 + |w[k \u2212 2]\u2212 w[k \u2212 1]|) + \u03bb 2 \u2016w\u20162\notherwise. Being a sum of convex functions, both local functions are convex, and in fact \u03bb-strongly convex due to the \u03bb2 \u2016w\u2016\n2 term. Furthermore, both function are (1+\u03bb)-Lipschitz continuous over the unit Euclidean ball. To see this, let \u2202G(w) denote the subgradient set of a convex function G at a point w, and note that\ng \u2208 \u2202 |b\u2212 w[1]| =\u21d2 g \u2208 conv{\u03c30,\u2212\u03c30} g \u2208 \u2202 |w[l]\u2212 w[l + 1]| =\u21d2 g \u2208 conv{\u03c3l,\u2212\u03c3l}\nwhere\n\u03c30 = (1, 0, . . . , 0)\n\u03c3l = (0, . . . , 0, 1\ufe38\ufe37\ufe37\ufe38 l\n, \u22121 \ufe38\ufe37\ufe37\ufe38\nl+1\n, 0, . . . , 0).\nAssume for the moment that \u03bb = 0, then by the linearity of the sub-differential operator that\n\u2200g \u2208 \u2202F1,k(w), \u2016g\u2016 \u2264 \u221a 1\n2 + k \u2212 1 2k \u2264 1\n\u2200g \u2208 \u2202F2,k(w), \u2016g\u2016 \u2264 \u221a 1\n2\nwhich shows that, for \u03bb = 0, both functions are 1-Lipschitz. For \u03bb > 0, note that \u03bb2 \u2016w\u2016 2 is \u03bb-Lipschitz over the unit ball and \u03bb-strongly convex. Therefore, using the linearity of the sub-differential operator again, we see that both Fi are (1 + \u03bb)-Lipschitz and \u03bb-strongly convex functions over the unit ball.\nSimilar to the smooth case, the following lemma shows that, no matter how the subgradients are chosen, at each iteration at most one non-zero coordinate may be gained.\nLemma 2. Suppose all the sets of feasible points satisfy Wj \u2286 ET,d for some T \u2264 d \u2212 1. Then under assumption 1, right after the next communication round we have Wj \u2286 ET+1,d.\nProof. Recall that by Assumption 1 (modified for the non-differentiable case), each machine can compute new points w that satisfy the following for some \u03b3, \u03bd \u2265 0 such that \u03b3 + \u03bd > 0:\n\u03b3w + \u03bdgi,k(w) \u2208 span { w\u2032 , gi,k(w \u2032) , (\u22072Fi,k(w\u2032) +D)w\u2032\u2032 , (\u22072Fi,k(w\u2032) +D)\u22121w\u2032\u2032 \u2223 \u2223 \u2223\nw\u2032,w\u2032\u2032 \u2208 Wj , gi,k(w\u2032) \u2208 \u2202Fi,k(w\u2032) , D diagonal , \u22072Fi,k(w\u2032) exists , (\u22072Fi,k(w\u2032) +D)\u22121 exists } .\nWe now analyze the state of the sets of feasible points prior to the next communication round. Assume that T is an odd number, i.e., assume T = 2p + 1 for some p \u2208 N \u222a {0}. We show that as long as no communication round has been executed, it must hold that Wj \u2286 ET,d for machines whose local function is F1, and that Wj \u2286 ET+1,d for machines whose local function is F2. The case where T is even follows a similar line.\nLet E0,d = {0} , ET,d = span{e1,d, . . . , eT,d}\nwhere ei,d \u2208 Rd denote the standard unit vectors. First, we prove the claim for machines whose local function is F1,k. In which case, for any w\u2032,w\u2032\u2032 \u2208 E2p+1,d, it holds that\ng1,k(w \u2032) \u2286 conv{\u00b1\u03c32l | l = 0, . . . , p} \u2286 E2p+1,d (\u22072F1,k(w\u2032) +D)w\u2032\u2032 = (\u03bbI +D)w\u2032\u2032 \u2286 E2p+1,d (\u22072F1,k(w\u2032) +D)\u22121w\u2032\u2032 = (\u03bbI +D)\u22121 w\u2032\u2032 \u2286 E2p+1,d\nFor any viable diagonal matrix D. Therefore, we have that the first point generated by machines which hold F1,k must satisfy\n\u03b3w + \u03bdg1,k(w) \u2208 E2p+1,d (9)\nfor \u03b3, \u03bd as stated in Assumption (1). Note that, if \u03bd = 0 (which by assumption means that \u03b3 > 0) then clearly w \u2208 E2p+1,d. As for \u03bd 6= 0, suppose by contradiction that w /\u2208 E2p+1,d. That is, assume that there exists some j > 2p + 1 such that w[j] 6= 0. First, if j = d and d is even, then the absolute value terms in F1,k do not involve w[j], and therefore g1,k(w)[j] = \u03bbw[j]. In this case, by Eq. (9) we have\n\u03b3w[j] + \u03bd\u03bbw[j] = (\u03b3 + \u03bd\u03bb)w[j] = 0,\nand since \u03bd\u03bb > 0, this implies that w[j] = 0 \u2013 a contradiction! Thus, it remains to consider the cases of either odd d or j 6= d. In both of these cases, w[j] appears in one of the absolute value terms in F1,k, either as |w[j \u2212 1]\u2212 w[j]| or |w[j] \u2212 w[j + 1]| (depending on whether j is odd or even).\nLet l > p be such that either 2l = j or 2l + 1 = j, depending on the parity of j. We note that any valid subgradient must satisfy\ng1,k(w)[2l] = 2\u03b1\u2212 1\u221a\n2k + \u03bbw[2l]\ng1,k(w)[2l + 1] = 1\u2212 2\u03b1\u221a\n2k + \u03bbw[2l + 1]\nfor some \u03b1 \u2208 [0, 1], such that if w[2l] \u2212w[2l + 1] 6= 0 then\nsgn (w[2l]\u2212 w[2l + 1]) = sgn ( 2\u03b1\u2212 1\u221a\n2k\n)\n, (10)\nwhere sgn() is the sign function. Rearranging terms in Eq. (9) and using the facts that coordinates 2l, 2l+1 are always zero in E2p+1,d, as well as \u03b3 + \u03bd\u03bb \u2265 \u03bd\u03bb > 0, we get\nw[2l] = \u2212\u03bd(2\u03b1\u2212 1)\u221a 2k(\u03b3 + \u03bd\u03bb)\n(11)\nw[2l + 1] = \u03bd(2\u03b1\u2212 1)\u221a 2k(\u03b3 + \u03bd\u03bb)\nTherefore,\nw[2l]\u2212 w[2l + 1] = \u22122\u03bd(2\u03b1\u2212 1)\u221a 2k(\u03b3 + \u03bd\u03bb)\n(12)\nwhich implies\nsgn (w[2l] \u2212 w[2l + 1]) = sgn (\u2212(2\u03b1\u2212 1)\u221a\n2k\n)\n,\ncontradicting Eq. (10). Hence, we must have w[2l]\u2212w[2l+1] = 0, in which case Eq. (12) implies \u03b1 = 1/2. Thus, by Eq. (11)\nw[2l] = w[2l + 1] = 0,\nwhich contradicts the assumption that either w[j] (and hence w[2l] or w[2l + 1]) is not zero. Thus, we have shown that w \u2208 E2p+1,d, for the first point generated by machines holding F1,k. Repeating the argument, we get that any point generated by those machines, in the absence of any communication rounds, is \u2018stuck\u2019 in E2p+1,d.\nWe now turn to prove the claim for machines whose local function is F2,k, using an almost identical argument, which we provide below for completeness. For these functions, we assume that initially Wj \u2286 E2p+1,d, and will show any additional points computed locally by the machines must be in E2p+2,d. We begin by noting that for any w\u2032,w\u2032\u2032 in E2p+2,d (and in particular E2p+1,d), it holds that\ng2,k(w \u2032) \u2286 conv{\u00b1\u03c32l+1 | l = 0, . . . , p} \u2286 E2p+2,d (\u22072F2,k(w\u2032) +D)w\u2032\u2032 = (\u03bbI +D)w\u2032\u2032 \u2286 E2p+2,d (\u22072F2,k(w\u2032) +D)\u22121w\u2032\u2032 = (\u03bbI +D)\u22121 w\u2032\u2032 \u2286 E2p+2,d\nFor any viable diagonal matrix D. Therefore, we have that the first point generated by machines which hold F2,k must satisfy\n\u03b3w + \u03bdg2,k(w) \u2208 E2p+2,d (13)\nfor \u03b3, \u03bd as stated in the assumption. Note that, if \u03bd = 0 then clearly w \u2208 E2p+2,d. As for \u03bd 6= 0, suppose by contradiction that w /\u2208 E2p+2,d. That is, assume that there exists some j > 2p + 2 such that w[j] 6= 0. First, if j = d and d is odd, then the absolute value terms in F2,k do not involve w[j], and therefore g2,k(w)[j] = \u03bbw[j]. In this case, by Eq. (13) we have\n\u03b3w[j] + \u03bd\u03bbw[j] = (\u03b3 + \u03bd\u03bb)w[j] = 0,\nand since \u03bd\u03bb > 0, this implies that w[j] = 0 \u2013 a contradiction! Thus, it remains to consider the cases of either even d or j 6= d. In both of these cases, w[j] appears in one of the absolute value terms in F2,k, either as |w[j \u2212 1]\u2212 w[j]| or |w[j] \u2212 w[j + 1]| (depending on whether j is odd or even).\nLet l > p be such that either 2l + 1 = j or 2l + 2 = j, depending on the parity of j. We note that any valid subgradient must satisfy Any valid subgradient must satisfy\ng2,k(w)[2l + 1] = 2\u03b1\u2212 1\u221a\n2k + \u03bbw[2l + 1]\ng2,k(w)[2l + 2] = 1\u2212 2\u03b1\u221a\n2k + \u03bbw[2l + 2]\nfor some \u03b1 \u2208 [0, 1], such that if w[2l + 1]\u2212 w[2l + 2] 6= 0 then\nsgn (w[2l + 1]\u2212 w[2l + 2]) = sgn ( 2\u03b1\u2212 1\u221a\n2k\n)\n(14)\nRearranging terms in Eq. (13) and using the fact that \u03b3 + \u03bd\u03bb \u2265 \u03bd\u03bb > 0, we get\nw[2l + 1] = \u2212\u03bd(2\u03b1\u2212 1)\u221a 2k(\u03b3 + \u03bd\u03bb)\n(15)\nw[2l + 2] = \u03bd(2\u03b1\u2212 1)\u221a 2k(\u03b3 + \u03bd\u03bb)\nTherefore,\nw[2l + 1]\u2212w[2l + 2] = \u22122\u03bd(2\u03b1\u2212 1)\u221a 2k(\u03b3 + \u03bd\u03bb)\n(16)\nwhich implies\nsgn (w[2l + 1]\u2212 w[2l + 2]) = sgn (\u2212(2\u03b1 \u2212 1)\u221a\n2k\n)\na contradiction to Eq. (14). Hence, we must have w[2l+1]\u2212w[2l+2] = 0, in which case Eq. (16) implies \u03b1 = 1/2. Thus, by Eq. (15)\nw[2l + 1] = w[2l + 2] = 0\nwhich contradicts our assumption that w[j] (and hence either w[2l + 1] or w[2l + 2] is not zero). Thus, we have shown that w \u2208 E2p+2,d. As before, repeating the argument together with the assumption that Wj \u2286 E2p+2,d shows that, in the absence of any communication rounds, all the machines whose local function is F2,k are \u2019stuck\u2019 in E2p+2,d. Therefore, before the next communication round, Wj \u2286 E2p+2,d for all machines j holding F2,j . Moreover, as shown earlier, Wj \u2286 E2p+1,d for all machines holding F1,k. Therefore, after the next communication round, Wj \u2286 E2p+2,d for any machine j.\nRepeatedly applying Lemma 2, we get the following corollary:\nCorollary 2. Under assumption 1, after T \u2264 d\u2212 1 communication rounds we have\nWj \u2286 ET+1, j \u2208 {1, . . . ,m}\nWith this corollary in hand, we now turn to establish the main result, namely, bounding from below the optimality of points in Wj after T communication rounds. Choosing the dimension d such that T \u2264 d\u2212 2, we employ the local functions defined in Eq. (8) with k = T + 2. In which case, the average function is\nF (w) = 1\n2 F1,T+2(w) +\n1 2 F2,T+2(w) = 1 2 \u221a 2 |b\u2212 w[1]| + 1 2 \u221a 2(T + 2)\nT+1\u2211\ni=1\n|w[i] \u2212 w[i+ 1]|+ \u03bb 2 \u2016w\u20162\nThe key ingredient in deriving the lower bound is Corollary (2), according to which after T communication rounds, all but the first T + 1 coordinates must be zero, in particular w[T + 2] = 0. Using this and the triangle inequality, we have\nF (w) \u2265 1 2 \u221a 2 |b\u2212 w[1]| + 1 2 \u221a 2(T + 2) |w[1]\u2212 w[T + 2]|+ \u03bb 2 \u2016w\u20162\n= 1 2 \u221a 2 |b\u2212 w[1]| + 1 2 \u221a 2(T + 2) |w[1]| + \u03bbw[1]\n2\n2\nfor all w in Wj . Therefore, we can lower bound the objective value of the algorithm\u2019s output by\nmin w\u2208R\n(\n1 2 \u221a 2 |b\u2212w|+ 1 2 \u221a 2(T + 2) |w|+ \u03bbw\n2\n2\n)\nOn the flip side, the minimal value of F (w) over the unit Euclidean ball can be upper bounded by F (wb) for some b \u2264 1\u221a\nT+2 , where\nwb = (b, . . . , b \ufe38 \ufe37\ufe37 \ufe38\nT+2 times\n, 0, . . . , 0)\nPutting both bounds together yields,\nmin w\u2208Wj F (w)\u2212 min \u2016w\u2016\u22641 F (w) \u2265 min w\u2208Wj F (w)\u2212 F (wb)\n\u2265 min w\u2208R\n(\n1 2 \u221a 2 |b\u2212 w|+ 1 2 \u221a 2(T + 2) |w|+ \u03bbw\n2\n2\n)\n\u2212 \u03bb(T + 2)b 2\n2 (17)\nAssuming T \u2265 12\u03bb \u2212 2 (so that \u03bb \u2265 12(T+2) ), we take\nb = 1\n2\u03bb(T + 2) \u221a 2(T + 2)\n(note again that wb is indeed in the unit ball for this regime of \u03bb and T ). In this case, the minimal w in Eq. (17) is 1\n2\u03bb(T+2) \u221a 2(T+2) , so we get a suboptimality lower bound of\n(\n0 + 1\n2 \u221a 2(T + 2) \u2223 \u2223 \u2223 \u2223 \u2223\n1\n2\u03bb(T + 2) \u221a 2(T + 2) \u2223 \u2223 \u2223 \u2223 \u2223 +\n1\n8\u03bb((T + 2) \u221a 2(T + 2) 2 )\n)\n\u2212 1 16\u03bb(T + 2)2\n\u2265 (\n1\n8\u03bb(T + 2)2 + 0\n)\n\u2212 1 16\u03bb(T + 2)2\n= 1\n16\u03bb(T + 2)2 (18)\nThis bound holds in particular for any T \u2265 \u2308\n1 2\u03bb \u2212 2\n\u2309 . If the number of communication rounds T is less than\n\u2308 1 2\u03bb \u2212 2 \u2309 , then clearly we cannot do better than with \u2308 1 2\u03bb \u2212 2 \u2309 communication rounds. Therefore, for any number of communication rounds T , the suboptimality is at least\nmin\n{\n1\n16\u03bb (\u2308\n1 2\u03bb \u2212 2\n\u2309 + 2\n)2 , 1\n16\u03bb(T + 2)2\n}\nTherefore, for any \u01eb \u2208 (\n0, 1 16\u03bb(\u2308 1\n2\u03bb \u22122\u2309+2)2\n]\n, we would need at least T \u2265 \u221a\n1 16\u03bb\u01eb\u22122 communication rounds\nto get an \u01eb-suboptimal solution. This implies the theorem statement for \u03bb-strongly convex functions.\nFinally, we treat the case where the local functions are not required to be strongly convex. In this setting, for proving a lower bound, we can use the same construction as in Eq. (8), where we are free to choose any\n\u03bb. In particular, let us choose \u03bb = 12(T+2) , and apply the lower bound derived above (note that in this case the condition T \u2265 12\u03bb \u2212 2 trivially holds). Plugging in it into (18), we establish that for any number of communication rounds T , the suboptimality is at least\n1\n8(T + 2) .\nConsidering how large T must be to make this smaller than some \u01eb, we get that T must be at least 18\u01eb \u2212 2.\nA.3 Proof of Thm. 3\nAs usual, we construct two functions F1, F2, and provide F1 to m/2 of the machines, and F2 to the other m/2 machines, in some arbitrary order, such that the machine designated to provide the output receives F2. Note that the average function F is simply 12(F1(w) + F2(w)).\nLet c be a certain positive numerical constant (whose value corresponds to c in Lemma 6 below). Given some symmetric M \u2208 {\u22121,+1}d\u00d7d, where \u2016M\u2016 \u2264 c \u221a d, and j \u2208 {\u2308d/2\u2309, . . . , d}, define\nF1(w) = 3\u03bbw \u22a4 (( I + 1\n2c \u221a d M\n)\u22121 \u2212 1\n2 I\n)\nw\nF2(w) = 3\u03bb\n2 \u2016w\u20162 \u2212 \u03b4ej ,\nThe average F of F1, F2 equals\nF (w) = 1\n2 (F1(w) + F2(w)) =\n3\u03bb\n2 w\u22a4\n(\nI + 1\n2c \u221a d M\n)\u22121 w \u2212 \u03b4\n2 ej,\nwith an optimum at\nw\u2217 = \u03b4\n6\u03bb\n(\nI + 1\n2c \u221a d M\n)\nej .\nThe following lemma establishes that the functions satisfy the strong convexity, smoothness and relatedness requirements of the theorem. The proof also establishes that the inverse in the definition of F1 indeed exists.\nLemma 3. F1 and F2 are \u03bb strongly-convex, 9\u03bb smooth, and \u03b4-related.\nProof. The Hessian of F2 is 3\u03bbI , which implies that F2 is 3\u03bb smooth and strongly convex (and in particular, \u03bb-strongly convex). As to F1, note that since \u2016M\u2016 \u2264 c \u221a d, then\n\u2225 \u2225 \u2225 \u2225 1\n2c \u221a d M\n\u2225 \u2225 \u2225 \u2225 \u2264 1\n2 ,\nThe fact that the spectral radius and spectral norm of symmetric matrices coincide implies that the eigenvalues of the matrix I + 1\n2c \u221a d M lie between 1 \u2212 12 = 12 and 1 + 12 = 32 . Thus, all the eigenvalues are\nstrictly positive, hence the matrix is indeed invertible as in the definition of F1. Moreover, the eigenvalues of the inverse lie in [\n1 3/2 , 1 1/2\n]\n= [ 2 3 , 2 ] , and therefore those of 3\u03bb\n((\nI + 1 2c \u221a d M )\u22121 \u2212 12I\n)\nlie in\n[ 3\u03bb ( 2 3 \u2212 12 ) , 3\u03bb ( 2\u2212 12 )] = [ \u03bb 2 , 9\u03bb 2 ] . Thus, the spectrum of the Hessian of F1 lie in [\u03bb, 9\u03bb], which implies that F1 is \u03bb-strongly convex and 9\u03bb smooth. To show \u03b4-relatedness, the only non-trivial part is upper-bounding the norm of the difference of the quadratic terms, which equals the following: \u2225 \u2225 \u2225 \u2225 \u2225 3\u03bb (( I + 1 2c \u221a d M )\u22121 \u2212 1 2 I ) \u2212 3\u03bb 2 I \u2225 \u2225 \u2225 \u2225 \u2225\n= 3\u03bb \u2225 \u2225 \u2225 \u2225 \u2225 ( I + 1 2c \u221a d M )\u22121 \u2212 I \u2225 \u2225 \u2225 \u2225 \u2225 . (19)\nSince \u2016M\u2016 \u2264 c \u221a d, the eigenvalues of (\nI + 1 2c \u221a d M )\u22121 \u2212I lie between 11+1/2\u22121 = \u221213 and 11\u22121/2\u22121 = 1,\nwhich implies that Eq. (19) can be upper bounded by 3\u03bb \u2264 \u03b4.\nThe next lemma proves the second part of the theorem, namely an upper bound on the suboptimality of any local function optimizer.\nLemma 4. For any w\u0302j = argminw\u2208Rd Fj(w), it holds that F (w\u0302j) \u2212minw\u2208Rd F (w) \u2264 c\u03b42/\u03bb for some numerical positive constant c.\nProof. The optimum of any quadratic and strongly-convex function w\u22a4Aw + b\u22a4w + c equals 12A \u22121b. Therefore, if w\u2217 is the optimizer of F , and we denote the parameters of F and Fj by A,b, c and Aj ,bj , cj respectively, then\n\u2016w\u0302j \u2212w\u2217\u2016 = 1\n2\n\u2225 \u2225 \u2225A\u22121j bj \u2212A\u22121b \u2225 \u2225 \u2225\n= 1\n2\n\u2225 \u2225 \u2225A\u22121j bj \u2212A\u22121bj +A\u22121bj \u2212A\u22121b \u2225 \u2225 \u2225\n\u2264 1 2\n(\u2225 \u2225 \u2225 ( A\u22121j \u2212A\u22121 ) bj \u2225 \u2225 \u2225+ \u2225 \u2225A\u22121 (bj \u2212 b) \u2225 \u2225 )\n\u2264 1 2 (\u2225 \u2225 \u2225A\u22121j \u2212A\u22121 \u2225 \u2225 \u2225 \u2016bj\u2016+ \u2225 \u2225A\u22121 \u2225 \u2225 \u2016bj \u2212 b\u2016 ) .\nBy definition of F1, F2 and the average function F , this is at most\n1\n2\n(\u2225 \u2225 \u2225A\u22121j \u2212A\u22121 \u2225 \u2225 \u2225 \u03b4 + \u2225 \u2225A\u22121 \u2225 \u2225 \u03b4\n2\n)\n. (20)\nIn Lemma 3, we showed that F1, F2 are \u03bb-strongly convex and 9\u03bb smooth, which implies that the eigenvalues of Aj as well as A lie in [ \u03bb 2 , 9\u03bb 2 ] . Therefore, the eigenvalues of A\u22121j and A \u22121 lie in [ 2 9\u03bb , 2 \u03bb ] , so \u2225 \u2225A\u22121\n\u2225 \u2225 \u2264 2\u03bb and \u2225 \u2225 \u2225A\u22121j \u2212A\u22121 \u2225 \u2225 \u2225 \u2264 2\u03bb . Substituting this back into Eq. (20), we get\n\u2016w\u0302j \u2212w\u2217\u2016 \u2264 1\n\u03bb\n(\n\u03b4 + \u03b4\n2\n)\n= 3\u03b4\n2\u03bb .\nFinally, since F is 9\u03bb-smooth, and its minimizer is w\u2217,\nF (w\u0302j)\u2212 F (w\u2217) \u2264 9\u03bb 2 \u2016w\u0302j \u2212w\u2217\u20162 \u2264 9\u03bb 2\n( 3\u03b4\n2\u03bb\n)2\n,\nwhich equals 81\u03b42/8\u03bb as required.\nWe now turn to derive the lower bound in the theorem statement. As discussed earlier, the intuition is that the optimal point w\u2217 is a function of the j-th column of M , so the machines holding F1 must broadcast enough information on M to the designated machine producing the algorithm\u2019s output (the machine, by construction, holds F2, and hence knows j but not M ). As long as the communication budget is smaller than the size of M , this will be difficult to achieve. This intuition is formalized in the following lemma, which is based on information-theoretic tools:\nLemma 5. For any dimension d \u2265 c (where c is the same constant as in Lemma 6 and the definition of F1), and for any (possibly randomized) 1-round algorithm using at most d2/128 bits of communication, there exists a valid choice of M, j for the functions F1, F2 defined above, such that the vector w\u0302 returned by the algorithm satisfies\nE\n[ \u2016w\u0302 \u2212w\u2217\u20162 ] \u2265 c\u2032 ( \u03b4\n\u03bb\n)2\n,\nwhere the expectation is over the algorithm\u2019s randomness, and c\u2032 is a positive numerical constant.\nUsing the lemma and the \u03bb-strong convexity of F1, F2 (and hence their average F ),\nE[F (w\u0302)\u2212 F (w\u2217)] \u2265 \u03bb 2 E[\u2016w \u2212w\u2217\u20162] \u2265 c\n\u2032\n2\n\u03b42 \u03bb ,\nhence proving the theorem. It now remains to prove Lemma 5:\nProof of Lemma 5. By definition of w\u2217, we have that the j-th column of M , designated as Mj , satisfies\nMj = 2c \u221a d ( 6\u03bb\n\u03b4 w\u2217 \u2212 ej\n)\n.\nGiven the predictor w\u0302 returned by the algorithm, define\nM\u0302j = 2c \u221a d ( 6\u03bb\n\u03b4 w\u0302 \u2212 ej\n)\n.\nThis can be thought of as the algorithm\u2019s \u2018estimate\u2019 of the j-th column of M , based on the returned predictor. Define [w] = min{1,max{\u22121, w}} as the clipping operation of a scalar w to [\u22121,+1], and for a vector w = (w1, . . . , wd), define [w] = ([w1], [w2], . . . , [wd]). By the expressions for Mj , M\u0302j above, we have\n\u2225 \u2225 \u2225[M\u0302j \u2212Mj] \u2225 \u2225 \u2225 2 = \u2225 \u2225 \u2225 \u2225 [ 2c \u221a d 6\u03bb\n\u03b4 (w\u0302 \u2212w\u2217)\n]\u2225 \u2225 \u2225 \u2225 2 = d\u2211\ni=1\n[ 12c\u03bb \u221a d\n\u03b4 (w\u0302i \u2212 w\u2217i )\n]2\n\u2264 ( 12c\u03bb \u221a d\n\u03b4\n)2 d\u2211\ni=1\n(w\u0302i \u2212 w\u2217i )2,\nwhich implies that\n\u2016w\u0302 \u2212w\u2217\u20162 \u2265 (\n\u03b4\n12c\u03bb \u221a d\n)2 \u2225 \u2225 \u2225[M\u0302j \u2212Mj ] \u2225 \u2225 \u2225 2 , (21)\nTo get the lemma statement, it is enough to show that for some M, j, one can lower bound E [\u2225 \u2225 \u2225[M\u0302j \u2212Mj] \u2225 \u2225 \u2225 2 ]\n(where the expectation is over the algorithm\u2019s randomness) by some constant multiple of d.\nBelow, we will prove that if M (in the definition of F1) is chosen uniformly at random from all {\u22121,+1}-valued d \u00d7 d symmetric matrices, and j (in the definition of F2) is chosen uniformly at random from {\u2308d/2\u2309, . . . , d}, then for any deterministic algorithm,\nEM,j [\u2225 \u2225 \u2225[M\u0302j \u2212Mj ] \u2225 \u2225 \u2225 2 ]\n\u2265 d 8\n(22)\nLet us first show how this can be used to prove the lemma. To do so, we will need the following lemma on the concentration of the spectral norm of random symmetric matrices.\nLemma 6 ([20], Corollary 2.3.6). There exist positive numerical constants c, c\u2032, such that if M is a d \u00d7 d symmetric matrix, where each entry Mj,i, j \u2265 i is chosen independently and uniformly from {\u22121,+1}, and d \u2265 c, then Pr(\u2016M\u2016 > c \u221a d) \u2264 c exp(\u2212c\u2032d).\nFirst, we note that the expectation in Eq. (22) is over all symmetric {\u22121,+1}-valued matrices, including those whose spectral norm may be larger than c \u221a d. However, by Lemma 6, Pr(\u2016M\u2016 > c \u221a d) \u2264 c exp(\u2212c\u2032d) for some absolute constant c\u2032. Letting E be the event that \u2016M\u2016 > c \u221a d, and noting that \u2016[w]\u20162 \u2264 d for any vector w, we have\nE [\u2225 \u2225 \u2225[M\u0302j \u2212Mj ] \u2225 \u2225 \u2225 2 ] = E [\u2225 \u2225 \u2225[M\u0302j \u2212Mj ] \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 E ] Pr(E) + E [\u2225 \u2225 \u2225[M\u0302j \u2212Mj ] \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 \u00acE ] Pr(\u00acE)\n\u2264 dPr(E) + E [\u2225 \u2225 \u2225[M\u0302j \u2212Mj ] \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 \u00acE ] \u2264 cd exp(\u2212c\u2032d) + E [\u2225 \u2225 \u2225[M\u0302j \u2212Mj ] \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 \u00acE ] .\nPlugging back into Eq. (22), we get that\nE [\u2225 \u2225 \u2225[M\u0302j \u2212Mj] \u2225 \u2225 \u2225 2 \u2223 \u2223 \u2223 \u2223 \u00acE ] \u2265 d 8 \u2212 cd exp(\u2212c1d),\nwhich is at least d/16 for any d larger than some constant. Combining with Eq. (21), we get\nE\n[ \u2016w\u0302 \u2212w\u2217\u20162 \u2223 \u2223 \u2223\u00acE ]\n\u2265 1 16\n( \u03b4\n12c\u03bb\n)2\n.\nThis inequality implies that for any deterministic algorithm, in expectation over the random draw of j and a {\u22121,+1}-valued matrix M with spectral norm at most c \u221a d, \u2016w\u0302 \u2212w\u2217\u20162 will be at least c\u2032\n( \u03b4 \u03bb )2 for some\nsuitable constant c\u2032. By Yao\u2019s minimax principle, this implies that for any (possibly randomized) algorithm, there will be some deterministic choice of M, j such that \u2016M\u2016 \u2264 c \u221a d, and for which\nE\n[ \u2016w\u0302 \u2212w\u2217\u20162 ] \u2265 c\u2032 ( \u03b4\n\u03bb\n)2\n(in expectation over the algorithm\u2019s randomness), yielding the lemma\u2019s statement. It now remains to prove Eq. (22), assuming j is chosen uniformly at random from {\u2308d/2\u2309, . . . , d}, and M is chosen at random (i.e. each entry at or above the main diagonal is chosen independently and uniformly from {\u22121,+1}). Roughly speaking, the proof idea is to reduce this to an upper bound on how much information the machines holding M can send on M \u2019s entries (and more particularly, on the entries\nin the upper-right quadrant of M ). Since this quadrant is composed of \u0398(d2) random variables, and the machines can send much less than d2 bits, this information is necessarily restricted.\nLet Pr(\u00b7) denote probability with respect to the random choice of M, j, and let Prj(\u00b7) denote probability conditioned on the choice of j. Recalling that any entry Mj,i in the j-th column has values in {\u22121,+1}, it follows that either Mj,i has the same sign as M\u0302j,i, or that ([Mj,i \u2212 M\u0302j,i])2 is at least 1. Therefore, we have the following:\nE [\u2225 \u2225 \u2225[Mj \u2212 M\u0302j] \u2225 \u2225 \u2225 2 ] = d\u2211\ni=1\nE[([Mj,i \u2212 M\u0302j,i])2] \u2265 \u2308d/2\u2309 \u2211\ni=1\nE[([Mj,i \u2212 M\u0302j,i])2]\n\u2265 \u2308d/2\u2309 \u2211\ni=1\nE\n[ ([Mj,i \u2212 M\u0302j,i])2 \u2223 \u2223 \u2223Mj,iM\u0302j,i \u2264 0 ] Pr ( Mj,iM\u0302j,i \u2264 0 ) + 0\n\u2265 \u2308d/2\u2309 \u2211\ni=1\nPr ( Mj,iM\u0302j,i \u2264 0 ) =\n\u2308d/2\u2309 \u2211\ni=1\n\n 1\n1 + \u230ad/2\u230b\nd\u2211\nj=\u2308d/2\u2309 Prj\n( Mj,iM\u0302j,i \u2264 0 )\n\n\n= 1\n1 + \u230ad/2\u230b\n\u2308d/2\u2309 \u2211\ni=1\nd\u2211\nj=\u2308d/2\u2309\n( 1\n2 Prj\n( M\u0302j,i \u2264 0|Mj,i > 0 ) + 1\n2 Prj\n( M\u0302j,i \u2265 0|Mj,i < 0 ))\n\u2265 1/2 1 + \u230ad/2\u230b\n\u2308d/2\u2309 \u2211\ni=1\nd\u2211\nj=\u2308d/2\u2309\n( 1\u2212 (\nPrj\n( M\u0302j,i \u2265 0|Mj,i > 0 ) \u2212 Prj ( M\u0302j,i \u2265 0|Mj,i < 0 )))\n\u2265 1/2 1 + \u230ad/2\u230b\n\u2308d/2\u2309 \u2211\ni=1\nd\u2211\nj=\u2308d/2\u2309\n( 1\u2212 \u2223 \u2223 \u2223Prj ( M\u0302j,i \u2265 0|Mj,i < 0 ) \u2212 Prj ( M\u0302j,i \u2265 0|Mj,i > 0 )\u2223 \u2223 \u2223 )\n= \u2308d/2\u2309 2 \u2212 1/2 1 + \u230ad/2\u230b\n\u2308d/2\u2309 \u2211\ni=1\nd\u2211\nj=\u2308d/2\u2309\n\u2223 \u2223 \u2223Prj ( M\u0302j,i \u2265 0|Mj,i < 0 ) \u2212 Prj ( M\u0302j,i \u2265 0|Mj,i > 0 )\u2223 \u2223 \u2223 . (23)\nLet S be the vector of bits broadcasted by the machines holding F1, and received by the machine designated with providing the output (recalling that it only holds F2). Note that conditioned on S and j, the algorithm\u2019s output (and hence M\u0302j,i) is independent of M . Therefore, we have \u2223 \u2223 \u2223Prj ( M\u0302j,i \u2265 0|Mj,i < 0 ) \u2212 Prj ( M\u0302j,i \u2265 0|Mj,i > 0 )\u2223 \u2223 \u2223\n= \u2223 \u2223 \u2223 \u2223 \u2223 \u2211\nS\nPrj\n( M\u0302j,i \u2265 0|S,Mj,i < 0 ) Pr(S|Mj,i < 0)\u2212 \u2211\nS\nPrj\n( M\u0302j,i \u2265 0|S,Mj,i > 0 ) Pr(S|Mj,i > 0) \u2223 \u2223 \u2223 \u2223 \u2223\n= \u2223 \u2223 \u2223 \u2223 \u2223 \u2211\nS\nPrj\n( M\u0302j,i \u2265 0|S ) Pr(S|Mj,i < 0)\u2212 \u2211\nS\nPrj\n( M\u0302j,i \u2265 0|S ) Pr(S|Mj,i > 0) \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u2211\nS\n\u2223 \u2223 \u2223Prj ( M\u0302j,i \u2265 0|S ) (Pr(S|Mj,i < 0)\u2212 Pr(S|Mj,i > 0)) \u2223 \u2223 \u2223\n\u2264 \u2211\nS\n|Prj(S|Mj,i < 0)\u2212 Prj(S|Mj,i > 0)|\n\u2264 \u2211\nS\n|Prj(S|Mj,i < 0)\u2212 Prj(S)|+ \u2211\nS\n|Prj(S|Mj,i > 0)\u2212 Prj(S)| .\nSince S is sent by the machines holding F1 (and not F2), it is independent of j. Therefore, we can write the above as \u2211\nS\n|Pr(S|Mj,i < 0)\u2212 Pr(S)|+ \u2211\nS\n|Pr(S|Mj,i > 0)\u2212 Pr(S)|\nwhere j in the conditioning is a fixed index. Using Pinsker\u2019s inequality, we can upper bound the above by \u221a\n2Dkl (p(S|Mj,i < 0)||p(S)) + \u221a 2Dkl (p(S|Mj,i > 0)||p(S))\nwhere p is the probability distribution of S, and Dkl is the Kullback-Leibler divergence. By the elementary inequality \u221a a+ \u221a b \u2264 \u221a\n2(a+ b) for all non-negative a, b, we can upper bound the above by \u221a\n4 (Dkl (p(S|Mj,i < 0)||p(S)) +Dkl (p(S|Mj,i > 0)||p(S)))\n= \u221a 8\n\u221a\n1 2 (Dkl (p(S|Mj,i < 0)||p(S)) +Dkl (p(S|Mj,i > 0)||p(S))).\nUsing the fact that Mj,i (for some fixed j, i) is uniformly distributed in {\u22121,+1}, and that the mutual information I(X;Y ) between random variables X,Y equals EY [Dkl(p(X|Y = y)||p(X))], the above equals\n\u221a 8 \u221a\nI(S;Mj,i).\nRecalling that this is an upper bound on \u2223 \u2223 \u2223Prj ( M\u0302j,i \u2265 0|Mj,i < 0 ) \u2212 Prj ( M\u0302j,i \u2265 0|Mj,i > 0 )\u2223 \u2223 \u2223, we have\n1/2\n1 + \u230ad/2\u230b\n\u2308d/2\u2309 \u2211\ni=1\nd\u2211\nj=\u2308d/2\u2309\n\u2223 \u2223 \u2223Prj ( M\u0302j,i \u2265 0|Mj,i < 0 ) \u2212 Prj ( M\u0302j,i \u2265 0|Mj,i > 0 )\u2223 \u2223 \u2223\n\u2264 \u221a 2\n1 + \u230ad/2\u230b\n\u2308d/2\u2309 \u2211\ni=1\nd\u2211\nj=\u2308d/2\u2309\n\u221a I(S;Mj,i) = \u221a 2\u2308d/2\u2309 1\u2308d/2\u2309 (1 + \u230ad/2\u230b)\n\u2308d/2\u2309 \u2211\ni=1\nd\u2211\nj=\u2308d/2\u2309\n\u221a\nI(S;Mj,i)\n\u2264 \u221a 2\u2308d/2\u2309 \u221a \u221a \u221a \u221a \u221a\n1\n\u2308d/2\u2309 (1 + \u230ad/2\u230b)\n\u2308d/2\u2309 \u2211\ni=1\nd\u2211\nj=\u2308d/2\u2309 I(S;Mj,i) (24)\nwhere the last step is by Jensen\u2019s inequality (i.e. the average of square roots is upper bounded by the square root of the average). The expression in the square root equals the average mutual information between a random variable S (composed of at most d2/128 bits), and \u2308d/2\u2309 (1 + \u230ad/2\u230b) binary random variables Mj,i, where i \u2208 {1, . . . , \u2308d/2\u2309}, j \u2208 {\u2308d/2\u2309, . . . , d}, which are all independent by construction. By Lemma 6 in [17], it is at most (d2/128)/ (\u2308d/2\u2309 (1 + \u230ad/2\u230b)) \u2264 1/32, so we have\n\u221a 2\u2308d/2\u2309 \u221a \u221a \u221a \u221a \u221a\n1\n\u2308d/2\u2309 (1 + \u230ad/2\u230b)\n\u2308d/2\u2309 \u2211\ni=1\nd\u2211\nj=\u2308d/2\u2309 I(S;Mj,i) \u2264\n\u221a 2\u2308d/2\u2309\n\u221a\n1\n32 = \u2308d/2\u2309 4 .\nRecalling this is an upper bound on Eq. (24), which is the second term in Eq. (23), we get that\nEM,j\n[ [Mj \u2212 M\u0302j ]2 ] \u2265 \u2308d/2\u2309 2 \u2212 \u2308d/2\u2309 4 = \u2308d/2\u2309 4 \u2265 d 8 ,\nhence justifying Eq. (22)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We study the fundamental limits to communication-efficient distributed methods for convex learning<lb>and optimization, under different assumptions on the information available to individual machines, and<lb>the types of functions considered. We identify cases where existing algorithms are already worst-case<lb>optimal, as well as cases where room for further improvement is still possible. Among other things, our<lb>results indicate that without similarity between the local objective functions (due to statistical data simi-<lb>larity or otherwise) many communication rounds may be required, even if the machines have unbounded<lb>computational power.", "creator": "LaTeX with hyperref package"}}}