{"id": "1506.02544", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Learning with Group Invariant Features: A Kernel Perspective", "abstract": "We analyze in this paper a random feature map based on a theory of invariance I-theory introduced recently. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of $N$ points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.", "histories": [["v1", "Mon, 8 Jun 2015 15:19:30 GMT  (2724kb,D)", "http://arxiv.org/abs/1506.02544v1", null], ["v2", "Fri, 4 Dec 2015 20:49:25 GMT  (76kb)", "http://arxiv.org/abs/1506.02544v2", "NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["youssef mroueh", "stephen voinea", "tomaso a poggio"], "accepted": true, "id": "1506.02544"}, "pdf": {"name": "1506.02544.pdf", "metadata": {"source": "CRF", "title": "Learning with Group Invariant Features: A Kernel Perspective", "authors": ["Youssef Mroueh", "Stephen Voinea", "Tomaso Poggio"], "emails": ["mroueh@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4]. Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc. In this work, we adopt the approach of [1] where the representation of the signal is designed to reflect the invariant properties and model the world symmetries with group actions. The ultimate aim is to bridge unsupervised learning of invariant representations with invariant kernel methods, where we can easily address the statistical consistency and sample complexity questions, using tools from classical supervised learning [9, 10]. Indeed many invariant kernel methods and related invariant kernel networks have been proposed. We refer the reader to the related work section for a review (Section 5) and we start by showing how to accomplish this invariance through Haar-integration group-invariant kernels [11], and then show how random features derived from a memory based theory of invariances introduced in [1] approximate such a kernel."}, {"heading": "1.1 Group Invariant Kernels", "text": "We start by reviewing Haar-integration group-invariant kernels introduced in [11], and their use in a binary classification problem. This section highlights the conceptual advantages of such kernels as well as their practical inconvenience, putting into perspective the advantage of approximating such kernels with explicit and invariant random feature maps.\nar X\niv :1\n50 6.\n02 54\n4v 1\n[ cs\n.L G\n] 8\nJ un\nInvariant Haar-Integration Kernels. We consider a subset X of the hypersphere in d dimensions Sd\u22121. Let \u03c1X be a measure on X . Consider a kernel k0 on X , such as a radial basis function kernel. Let G be a group acting on X , with a normalized Haar measure \u00b5. G is assumed to be a compact and unitary group. Define an invariant kernel K between x, z \u2208 X through Haar-integration [11] as follows:\nK(x, z) = \u222b G \u222b G k0(gx, g \u2032z)d\u00b5(g)d\u00b5(g\u2032). (1)\nAs we are integrating over the entire group, it is easy to see that: K(g\u2032x, gz) = K(x, z), \u2200g, g\u2032 \u2208 G,\u2200x, z \u2208 X . Hence the Haar-integration kernel is invariant to the group action. The symmetry of K is obvious. Moreover, if k0 is a positive definite kernel, it follows that K is positive definite as well [11]. One can see the Haar-integration kernel framework as another form of data augmentation, since we have to produce group transformed points in order to compute the kernel.\nInvariant Decision Boundary. Turning now to a pattern classification problem, we assume that we are given a training set S of labeled data points among two classes: S = {(xi, yi), i = 1 . . . N | xi \u2208 X , yi \u2208 Y = {\u00b11}}. In order to learn a decision function f : X \u2192 Y , we minimize the following empirical risk induced by an L-Lipschitz and convex loss function V , with V \u2032(0) < 0 [12]: minf\u2208HK E\u0302V (f) := 1N \u2211N i=1 V (yif(xi)), where we restrict f to belong to a hypothesis class induced by the invariant kernel K, the so called reproducing kernel hilbert space HK. The representer theorem [13] shows that the solution of such a problem, or the optimal decision boundary f\u2217N has the following form: f \u2217 N (x) = \u2211N i=1 \u03b1 \u2217 iK(x, xi). Since the kernel K is group-invariant it\nfollows that : f\u2217N (gx) = \u2211N i=1 \u03b1iK(gx, xi) = \u2211N i=1 \u03b1iK(x, xi) = f\u2217N (x), \u2200g \u2208 G. Hence the the decision boundary f\u2217is group-invariant as well, and we have: f\u2217N (gx) = f \u2217 N (x),\u2200g \u2208 G,\u2200x \u2208 X .\nReduced Sample Complexity. We have shown that a group-invariant kernel induces a groupinvariant decision boundary, but how does this translate to the sample complexity of the learning algorithm? To answer this question, we will assume that the input set X has the following structure: X = X0 \u222a GX0, GX0 = {z|z = gx, x \u2208 X0, g \u2208 G/ {e}}, where e is the identity group element. This structure implies that for a function f in the invariant RKHSHK, we have:\n\u2200z \u2208 GX0,\u2203 x \u2208 X0,\u2203 g \u2208 G such that, z = gx, and f(z) = f(x). Let \u03c1y(x) = P(Y = y|x) be the label posteriors. We assume that \u03c1y(gx) = \u03c1y(x),\u2200g \u2208 G,. This is a natural assumption since the label is unchanged given the group action. Assume that the set X is endowed with a measure \u03c1X that is also group-invariant. Let f be the group-invariant decision function , i.e ( such as f\u2217N \u2208 HK). Consider the expected risk induced by the loss V , EV (f), defined as follows:\nEV (f) = \u222b X \u2211 y\u2208Y V (yf(x))\u03c1y(x)\u03c1X (x)dx, (2)\nEV (f) is a proxy to the misclassification risk [12]. Using the invariant properties of the function class and the data distribution we have by invariance of f , \u03c1y , and \u03c1:\nEV (f) = \u222b X0 \u2211 y\u2208Y V (yf(x))\u03c1y(x)\u03c1X (x)dx+ \u222b GX0 \u2211 y\u2208Y V (yf(z))\u03c1y(z)\u03c1X (z)dz\n= \u222b G d\u00b5(g) \u222b X0 \u2211 y\u2208Y V (yf(gx))\u03c1y(gx)\u03c1X (x)dx\n= \u222b G d\u00b5(g) \u222b X0 \u2211 y\u2208Y V (yf(x))\u03c1y(x)\u03c1X (x)dx (By invariance of f , \u03c1y , and \u03c1 )\n= \u222b X0 \u2211 y\u2208Y V (yf(x))\u03c1y(x)\u03c1X (x)dx.\nHence, given an invariant kernel to a group action that is identity preserving, it is sufficient to minimize the empirical risk on the core set X0, and it generalizes to samples in GX0. Let us imagine that X was finite, and of cardinality |X |, the cardinality of the core set X0 is a small fraction of the cardinality of X : |X0| = \u03b1|X |, where 0 < \u03b1 < 1. Hence when we sample the training set from X0 the maximum size of the training set isN = \u03b1|X | << |X |, hence the reduction in the sample complexity."}, {"heading": "1.2 Contributions", "text": "We have just reviewed the Haar-integration group-invariant kernel setup. In summary, a groupinvariant kernel implies the existence of a decision function that is invariant to the group action, as well as a reduction in the sample complexity as we have to sample the training set in a reduced set, aka the core set X0. Kernel methods with Haar-integration kernels come at a very expensive computational price at both training and test time: computing the Kernel is computationally cumbersome as we have to integrate over the group and produce virtual examples by transforming points explicitly through the group action, moreover the training complexity of kernel methods scales cubicly in the sample size. Those practical considerations make the usefulness of such kernels very limited. The contributions of this paper are on three folds:\n1. We first show that a non linear random feature map \u03a6 : X \u2192 RD derived from a memory based theory of invariances introduced in [1] induces an expected Haar-integration groupinvariant kernel K. For fixed points x, z \u2208 X , we have: E \u3008\u03a6(x),\u03a6(z)\u3009 = K(x, z), where K satisfies: K(gx, g\u2032z) = K(x, z),\u2200g, g\u2032 \u2208 G, x, z \u2208 X .\n2. We show a Johnson Lindenstrauss type result that holds uniformly on a set of N points that assess the concentration of this random feature map around its expected induced kernel. For sufficiently large D, we have \u3008\u03a6(x),\u03a6(z)\u3009 \u2248 K(x, z), uniformly on an N points set.\n3. We show that, with a linear model, an invariant decision function can be learned in this random feature space by sampling points from the core set X0 i.e: f\u2217N (x) \u2248 \u3008w\u2217,\u03a6(x)\u3009 , and generalizes to unseen points in GX0 and reducing the sample complexity. Moreover, we show that those features define a function space that approximates a dense subset of the Invariant RKHS, and assess the error rates of the empirical risk minimization using such random features.\n4. We demonstrate the validity of these claims on three datasets: text (artificial), vision (MNIST), and speech (TIDIGITS)."}, {"heading": "2 From Group Invariant Kernels to Feature Maps", "text": "In this paper we show that a random feature map based on I-theory [1]: \u03a6 : X \u2192 RD approximates a Haar-integration group-invariant kernel K having the form given in Equation (1):\n\u3008\u03a6(x),\u03a6(z)\u3009 \u2248 K(x, z). We start with some notation that will be useful for defining the feature map. Denote the cumulative distribution function of a random variable X by,\nFX(\u03c4) = P(X \u2264 \u03c4), Fix x \u2208 X , Let g \u2208 G be a random variable drawn according to the normalized Haar measure \u00b5 and let t be a random template whose distribution will be defined latter. For s > 0, define the following truncated cumulative distribution function (CDF) of the dot product \u3008x, gt\u3009:\n\u03c8(x, t, \u03c4) = Pg(\u3008x, gt\u3009 \u2264 \u03c4) = F\u3008x,gt\u3009(\u03c4), \u03c4 \u2208 [\u2212s, s], x \u2208 X ,\nLet \u03b5 \u2208 (0, 1). We consider the following Gaussian vectors (sampling with rejection) for the templates t: t = n \u223c N ( 0, 1\nd Id\n) , if \u2016n\u201622 < 1 + \u03b5, t =\u22a5 else .\nThe reason behind this sampling is to keep the range of \u3008x, gt\u3009 under control: The squared norm \u2016n\u201622 will be bounded by 1 + \u03b5 with high probability by a classical concentration result (See proof of Theorem 1 for more details). The group being unitary and x \u2208 Sd\u22121, we know that : | \u3008x, gt\u3009 | \u2264 \u2016n\u2016 < \u221a 1 + \u03b5 \u2264 1 + \u03b5, for \u03b5 \u2208 (0, 1).\nRemark 1. We can also consider templates t, drawn uniformly on the unit sphere Sd\u22121. Uniform templates on the sphere can be drawn as follows:\nt = \u03bd\n\u2016\u03bd\u20162 , \u03bd \u223c N (0, Id),\nsince the norm of a gaussian vector is highly concentrated around its mean \u221a d, we can use the gaussian sampling with rejection. Results proved for gaussian templates (with rejection) will hold true for templates drawn at uniform on the sphere with different constants.\nDefine the following kernel function, Ks(x, z) = Et \u222b s \u2212s \u03c8(x, t, \u03c4)\u03c8(z, t, \u03c4)d\u03c4,\nwhere swill be fixed throughout the paper to be s = 1+\u03b5 since the gaussian sampling with rejection controls the dot product to be in that range. Let g\u0304 \u2208 G. As the group is closed, we have \u03c8(t, g\u0304x, \u03c4) = \u222b G\n1I\u3008gg\u0304x,t\u3009\u2264\u03c4d\u00b5(g) =\u222b G\n1I\u3008gx,t\u3009\u2264\u03c4d\u00b5(g) = \u03c8(t, x, \u03c4) and hence K(gx, g\u2032z) = K(x, z), for all g, g\u2032 \u2208 G. It is clear now that K is a group-invariant kernel. In order to approximate K, we sample |G| elements uniformly and independently form the group G, i.e. gi, i = 1 . . . |G|, and define the normalized empirical CDF :\n\u03c6(x, t, \u03c4) = 1\n|G| \u221a m |G|\u2211 i=1 1I\u3008git,x\u3009\u2264\u03c4 ,\u2212s \u2264 \u03c4 \u2264 s.\nWe discretize the continuous threshold \u03c4 as follows:\n\u03c6 ( x, t, sk\nn\n) = \u221a s\u221a\nnm|G| |G|\u2211 i=1 1I\u3008git,x\u3009\u2264k sn ,\u2212n \u2264 k \u2264 n.\nWe sample m templates independently according to the Gaussian sampling with rejection, tj , j = 1 . . .m. We are now ready to define the random feature map \u03a6:\n\u03a6(x) = [ \u03c6 ( x, tj , sk\nn )] j=1...m,k=\u2212n...n \u2208 R(2n+1)\u00d7m.\nIt is easy to see that:\nlim n\u2192\u221e Et,g \u3008\u03a6(x),\u03a6(z)\u3009R(2n+1)\u00d7m = limn\u2192\u221eEt,g m\u2211 j=1 n\u2211 k=\u2212n \u03c6 ( x, tj , sk n ) \u03c6 ( z, tj , sk n ) = Ks(x, z).\nIn Section 3 we study the geometric information captured by this kernel by stating explicitly the similarity it computes. Remark 2 (Efficiency of the representation). 1) The main advantage of such a feature map as outlined in [1], is that we store transformed templates in order to compute \u03a6, while if we needed to compute an invariant kernel of typeK (Equation (1)), we need to expliclitly transform the points. The latter is computationally expensive. Storing transformed templates and computing the signature \u03a6 is much more efficient. It falls in the category of memory-based learning, and is biologically plausible [1]. 2) As |G|,m,n get large enough the feature map \u03a6, approximates a group-invariant Kernel, as we will see in next section."}, {"heading": "3 An Equivalent Expected Kernel and a Uniform Concentration Result", "text": "In this section we present our main results, with proofs given in the supplementary material . Theorem 1 shows that the random feature map \u03a6, defined in the previous section, corresponds in expectation to a group-invariant, Haar-integration kernel Ks(x, z). Moreover s\u2212Ks(x, z) computes the average pairwise distance between all points in the orbits of x and z, where the orbit is defined as the collection of all group-transformed points of a given point x : Ox = {gx, g \u2208 G}. Theorem 1 (Expectation). Let \u03b5 \u2208 (0, 1) and x, z \u2208 X . Define the distance dG between the orbits Ox and Oz:\ndG(x, z) = 1\u221a 2\u03c0d \u222b G \u222b G \u2016gx\u2212 g\u2032z\u20162 d\u00b5(g)d\u00b5(g \u2032),\nand the group-invariant expected kernel\nKs(x, z) = lim n\u2192\u221e Et,g \u3008\u03a6(x),\u03a6(z)\u3009R(2n+1)\u00d7m = Et \u222b s \u2212s \u03c8(x, t, \u03c4)\u03c8(z, t, \u03c4)d\u03c4, s = 1 + \u03b5.\n1. The following inequality holds with probability 1:\n\u03b5\u2212 \u03b42(d, \u03b5) \u2264 Ks(x, z)\u2212 (1\u2212 dG(x, z)) \u2264 \u03b5+ \u03b41(d, \u03b5), (3)\nwhere \u03b41(\u03b5, d) = e \u2212d\u03b52/16 \u221a d \u2212 12 e\u2212\u03b5d/2(1+\u03b5) d 2\u221a d and \u03b42(\u03b5, \u03b4) = e \u2212d\u03b52/16 \u221a d + (1 + \u03b5)e\u2212d\u03b5 2/8.\n2. For any \u03b5 \u2208 (0, 1) as the dimension d \u2192 \u221e we have \u03b41(\u03b5, d) \u2192 0 and \u03b42(\u03b5, d) \u2192 0, and we have asymptotically Ks(x, z)\u2192 1\u2212 dG(x, z) + \u03b5 = s\u2212 dG(x, z).\n3. Ks is symmetric and Ks is positive semi definite. Remark 3. 1) \u03b5, \u03b41(d, \u03b5), and \u03b42(d, \u03b5) are due to the truncation and are a technical artifact of the proof, and are not errors due to results holding with high probability. 2) Local invariance can be defined by restricting the sampling of the group elements to a subset G \u2282 G, and the equivalent kernel has asymptotically the following form:\nKs(x, z) \u2248 s\u2212 1\u221a 2\u03c0d \u222b G \u222b G \u2016gx\u2212 g\u2032z\u20162 d\u00b5(g)d\u00b5(g \u2032).\n3) The norm-one constraint can be relaxed , let R = supx\u2208X \u2016x\u20162 < \u221e, hence we can set s = R(1 + \u03b5), and\n\u2212\u03b42(d, \u03b5) \u2264 Ks(x, z)\u2212 (R(1 + \u03b5)\u2212 dG(x, z)) \u2264 \u03b41(d, \u03b5), (4)\nwhere \u03b41(\u03b5, d) = R e \u2212d\u03b52/16 \u221a d \u2212 R2 e\u2212\u03b5d/2(1+\u03b5) d 2\u221a d and \u03b42(\u03b5, \u03b4) = R e \u2212d\u03b52/16 \u221a d +R(1 + \u03b5)e\u2212d\u03b5 2/8.\nTheorem 2 is in a sense an invariant Johnson Lindenstrauss [14] type result where we show that the dot product defined by the random feature map \u03a6 , i.e \u3008\u03a6(x),\u03a6(z)\u3009 is concentrated around the invariant expected kernel, uniformly on a data set of N points, given a sufficiently large number of templates m, a large number of sampled group elements |G|, and a large bin number n. The error naturally decomposes to a numerical error \u03b50 and statistical errors \u03b51, \u03b52 due to the sampling of the templates and the group elements respectively. Theorem 2. [Johnson Lindenstrauss type Theorem- N point Set] Let D = {x1 . . . xN |xi \u2208 X , i = 1 . . . N}, be a finite dataset. Fix \u03b50, \u03b51, \u03b52, \u03b41, \u03b42 \u2208 (0, 1). For a number of bins n \u2265 1\u03b50 , a number of templates m \u2265 C1\n\u03b521 log(N\u03b41 ), and a number of group elements |G| \u2265 C2 \u03b522 log(Nm\u03b42 ), where C1, C2 are universal numeric constants, we have:\n|\u3008\u03a6(xi),\u03a6(xj)\u3009 \u2212Ks(xi, xj)| \u2264 \u03b50 + \u03b51 + \u03b52, i = 1 . . . N, j = 1 . . . N, (5) with probability 1\u2212 \u03b41 \u2212 \u03b42.\nPutting together Theorems 1 and 2, the following Corollary shows how the random, group-invariant feature map \u03a6 captures the invariant distance between points uniformly on a dataset of N points. Corollary 1 (Invariant Features Maps and Distances between Orbits). Let D = {x1 . . . xN |xi \u2208 X , i = 1 . . . N}, be a finite dataset. Fix \u03b50, \u03b4 \u2208 (0, 1). For a number of bins n \u2265 3\u03b50 , a number of templates m \u2265 9C1\n\u03b520 log(N\u03b4 ), and a number of group elements |G| \u2265 9C2 \u03b520 log(Nm\u03b4 ), where C1, C2 are universal numeric constants, we have:\n\u03b5\u2212 \u03b42(d, \u03b5)\u2212 \u03b50 \u2264 \u3008\u03a6(xi),\u03a6(xj)\u3009 \u2212 (1\u2212 dG(xi, xj)) \u2264 \u03b50 + \u03b5+ \u03b41(d, \u03b5), (6) i = 1 . . . N, j = 1 . . . N , with probability 1\u2212 2\u03b4. Remark 4. Assuming that the templates are unitary and drawn form a general distribution p(t), the equivalent kernel has the following form:\nKs(x, z) = \u222b G \u222b G d\u00b5(g)d\u00b5(g\u2032) (\u222b s\u2212max(\u3008x, gt\u3009 , \u3008z, g\u2032t\u3009)p(t)dt ) .\nIndeed when we use the gaussian sampling with rejection for the templates, the integral\u222b max(\u3008x, gt\u3009 , \u3008z, g\u2032t\u3009)p(t)dt is asymptotically proportional to \u2225\u2225\u2225g\u22121x\u2212 g\u2032,\u22121z\u2225\u2225\u2225 2\n. It is interesting to consider different distributions that are domain-specific for the templates and assess the number of the templates needed to approximate such kernels. It is also interesting to find the optimal templates that achieve the minimum distortion in equation 6, in a data dependent way, but we will address these points in future work."}, {"heading": "4 Learning with Group Invariant Random Features", "text": "In this section, we show that learning a linear model in the invariant, random feature space, on a training set sampled from the reduced core setX0, has a low expected risk, and generalizes to unseen test points generated from the distribution on X = X0 \u222a GX0. The architecture of the proof follows ideas from [15] and [16]. Recall that given an L-Lipschitz convex loss function V , our aim is to minimize the expected risk given in Equation (2). Denote the CDF by \u03c8(x, t, \u03c4) = P(\u3008gt, x\u3009 \u2264 \u03c4), and the empirical CDF by \u03c8\u0302(x, t, \u03c4) = 1|G| \u2211|G| i=1 1I\u3008git,x\u3009\u2264\u03c4 . Let p(t) be the distribution of templates\nt. The RKHS defined by the invariant kernel Ks, Ks(x, z) = \u222b \u222b s \u2212s \u03c8(x, t, \u03c4)\u03c8(z, t, \u03c4)p(t)dtd\u03c4 denotedHKs , is the completion of the set of all finite linear combinations of the form:\nf(x) = \u2211 i \u03b1iKs(x, xi), xi \u2208 X , \u03b1i \u2208 R. (7)\nSimilarly to [16], we define the following infinite-dimensional function space: Fp = { f(x) = \u222b \u222b s \u2212s w(t, \u03c4)\u03c8(x, t, \u03c4)dtd\u03c4 | sup \u03c4,t |w(t, \u03c4)| p(t) \u2264 C } .\nLemma 1. Fp is dense in HKs . For f \u2208 Fp we have EV (f) = \u222b X0 V (yf(x))\u03c1y(x)d\u03c1X (x), where X0 is the reduced core set.\nSince Fp is dense in HKs , we can learn an invariant decision function in the space Fp, instead of learning in HKs . Let \u03a8(x) = [ \u03c8\u0302 ( x, tj , sk n )] j=1...m,k=\u2212n...n . \u03a8, and \u03a6 are equivalent up to constants. We will approximate the set Fp as follows:\nF\u0303 = f(x) = \u3008w,\u03a8(x)\u3009 = sn m\u2211 j=1 n\u2211 k=\u2212n wj,k\u03c8\u0302 ( x, tj , sk n ) , tj \u223c p, j = 1 . . .m | \u2016w\u2016\u221e \u2264 C m  . Hence, we learn the invariant decision function via empirical risk minimization where we restrict the function to belong to F\u0303 , and the sampling in the training set is restricted to the core set X0. Note that with this function space we are regularizing for convenience the norm infinity of the weights but this can be relaxed in practice to a classical Tikhonov regularization. Theorem 3 (Learning with Group invariant features). Let S = {(xi, yi) | xi \u2208 X0, yi \u2208 Y, i = 1 . . . N}, a training set sampled from the core set X0. Let f\u2217N = arg minf\u2208F\u0303 E\u0302V (f) = 1 N \u2211N i=1 V (yif(xi)).Fix \u03b4 > 0, then\nEV (f\u2217N ) \u2264 min f\u2208Fp EV (f) + 2 1\u221a N\n( 4LsC + 2V (0) + LC \u221a 1\n2 log\n( 1\n\u03b4\n))\n+ 2sLC\u221a m\n( 1 + \u221a 2 log ( 1\n\u03b4\n)) + L ( 2sC\u221a |G| ( 1 + \u221a 2 log (m \u03b4 )) + 2sC n ) ,\nwith probability at least 1\u2212 3\u03b4 on the training set and the choice of templates and group elements.\nThe proof of Theorem 3 is given in Appendix B. Theorem 3 shows that learning a linear model in the invariant random feature space defined by \u03a6 (or equivalently \u03a8), has a low expected risk. More importantly, this risk is arbitrarily close to the optimal risk achieved in an infinitedimensional class of functions, namely Fp. The training set is sampled from the reduced core set X0, and invariant learning generalizes to unseen test points generated from the distribution on X = X0 \u222a GX0, hence the reduction in the sample complexity. Recall that Fp is dense in the RKHS of the Haar-integration invariant Kernel, and so the expected risk achieved by a linear model in the invariant random feature space is not far from the one attainable in the invariant RKHS. Note that the error decomposes into two terms. The first, O( 1\u221a\nN ), is statistical and it\ndepends on the training sample complexity N . The other is governed by the approximation error of functions Fp, with functions in F\u0303 , and depends on the number of templates m, number of group elements sampled |G|, the number of bins n, and has the following formO( 1\u221a\nm )+O (\u221a logm |G| ) + 1n ."}, {"heading": "5 Relation to Previous Work", "text": "We now put our contributions in perspective by outlining some of the previous work on invariant kernels and approximating kernels with random features. Approximating Kernels. Several schemes have been proposed for approximating a non-linear kernel with an explicit non-linear feature map in conjunction with linear methods, such as the Nystro\u0308m method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels [15]. Our features fall under the random sampling techniques where, unlike previous work, we sample both projections and group elements to induce invariance with an integral representation. We note that the relation between random features and quadrature rules has been thoroughly studied in [18], where sharper bounds and error rates are derived, and can apply to our setting. Invariant Kernels. We focused in this paper on Haar-integration kernels [11], since they have an integral representation and hence can be represented with random features [18]. Other invariant kernels have been proposed: In [19] authors introduce transformation invariant kernels, but unlike our general setting, the analysis is concerned with dilation invariance. In [20], multilayer arccosine kernels are built by composing kernels that have an integral representation, but does not explicitly induce invariance. More closely related to our work is [21], where kernel descriptors are built for visual recognition by introducing a kernel view of histogram of gradients that corresponds in our case to the cumulative distribution on the group variable. Explicit feature maps are obtained via kernel PCA, while our features are obtained via random sampling. Finally the convolutional kernel network of [22] builds a sequence of multilayer kernels that have an integral representation, by convolution, considering spatial neighborhoods in an image. Our future work will consider the composition of Haar-integration kernels, where the convolution is applied not only to the spatial variable but to the group variable akin to [2]."}, {"heading": "6 Numerical Evaluation", "text": "In this paper, and specifically with Theorems 2 and 3, we showed that the random, group-invariant feature map \u03a6 captures the invariant distance between points, and that learning a linear model trained in the invariant, random feature space will generalize well to unseen test points. In this section, we validate these claims through three experiments. For the claims of Theorem 2, we will use a nearest neighbor classifier, while for Theorem 3, we will rely on the regularized least squares (RLS) classifier, one of the simplest algorithms for supervised learning. While our proofs focus on norm-infinity regularization, RLS corresponds to Tikhonov regularization with square loss. Specifically, for performing T\u2212way classification on a batch of N training points in Rd, summarized in the data matrix X \u2208 RN\u00d7d and label matrix Y \u2208 RN\u00d7T , RLS will perform the optimization, minW\u2208Rm\u00d7T { 1 N ||Y \u2212 \u03a6(X)W || 2 F + \u03bb||W ||2F } , where || \u00b7 ||F is the Frobenius norm, \u03bb is the regularization parameter, and \u03a6 is the feature map, which for the representation described in this paper will be a CDF pooling of the data projected onto group transformed random templates. All RLS experiments in this paper were completed with the GURLS toolbox [23]. The three datasets we explore are: Xperm (Figure 1): An artificial dataset consisting of all sequences of length 5 whose elements come from an alphabet of 8 characters. We want to learn a function which assigns a positive value to any sequence that contains a target set of characters (in our case, two of them) regardless of their position. Thus, the function label is globally invariant to permutation, and so we project our data onto all permuted versions of our random template sequences. MNIST (Figure 2): We seek local invariance to translation and rotation, and so all random templates are translated by up to 3 pixels in all directions and rotated between -20 and 20 degrees. TIDIGITS (Figure 3): We use a subset of TIDIGITS consisting of 326 speakers (men, women, children) reading the digits 0-9 in isolation, and so each datapoint is a waveform of a single word. We seek local invariance to pitch and speaking rate [25], and so all random templates are pitch shifted up and down by 400 cents and warped to play at half and double speed. The task is 10-way classification with on class-per-digit. See [24] for more detail."}, {"heading": "A Proofs of Theorems 1 and 2", "text": "Proof of Theorem 1. 1) Ks(x, z) = Et \u222b s \u2212s Eg [ 1I\u3008x,gt\u3009\u2264\u03c4 ] Eg\u2032 [ 1I\u3008z,g\u2032t\u3009\u2264\u03c4 ] d\u03c4\n= Et \u222b d\u00b5(g)d\u00b5(g\u2032) \u222b s \u2212s 1I\u3008x,gt\u3009\u2264\u03c41I\u3008x,g\u2032t\u3009\u2264\u03c4d\u03c4\n= \u222b d\u00b5(g)d\u00b5(g\u2032)Et (s\u2212max(\u3008x, gt\u3009 , \u3008z, g\u2032t\u3009)) .\nwhere the second equality is by Fubini theorem and the last one holds since for a, b \u2208 [\u2212s, s] :\u222b s \u2212s 1Ia\u2264\u03c41Ib\u2264\u03c4d\u03c4 = s\u2212max(a, b).\nRecall that the sampling of t is the following for \u03b5 \u2208 (0, 1) let : t = n \u223c N ( 0, 1\nd Id\n) , if \u2016n\u201622 < 1 + \u03b5, t =\u22a5 else ,\nsince our group is unitary, x being norm one, and by virtue of this sampling the dot product |\u3008x, gt\u3009| \u2264 \u2016n\u20162 \u2264 \u221a 1 + \u03b5 \u2264 1 + \u03b5 . Hence \u3008x, gt\u3009 \u2208 [\u2212(1 + \u03b5), 1 + \u03b5], and we can choose s = 1 + \u03b5. Using again the fact the group is unitary and compact we have:\nKs(x, z) = \u222b d\u00b5(g)d\u00b5(g\u2032)Et(s\u2212max (\u2329 g\u22121x, t \u232a , \u2329 g \u2032,\u22121z, t \u232a) .\nNow using this particular sampling of templates we have:\nKs(x, z) = \u222b G \u222b G d\u00b5(g)d\u00b5(g\u2032)En ( 1I\u2016n\u201622<1+\u03b5 [ 1 + \u03b5\u2212max (\u2329 g\u22121x, n \u232a , \u2329 g\u2032\u22121z, n \u232a)]) .\nLet Zx,z(n, g, g \u2032) = max (\u2329 g\u22121x, n \u232a , \u2329 g\u2032\u22121z, n \u232a) ,\nIt follows that:\nKs(x, z) = \u222b G \u222b G d\u00b5(g)d\u00b5(g\u2032)En ( 1I\u2016n\u201622<1+\u03b5 [1 + \u03b5\u2212 Zx,z(n, g, g \u2032)] )\n= (1 + \u03b5)P(\u2016n\u201622 < 1 + \u03b5)\u2212 \u222b G \u222b G d\u00b5(g)d\u00b5(g\u2032)En ( 1I\u2016n\u201622<1+\u03b5Zx,z(n, g, g \u2032) )\n= (1 + \u03b5)P(\u2016n\u201622 < 1 + \u03b5)\u2212 \u222b G \u222b G d\u00b5(g)d\u00b5(g\u2032)En ( (1\u2212 1I\u2016n\u201622\u22651+\u03b5)Zx,z(n, g, g \u2032) )\n= (1 + \u03b5)P(\u2016n\u201622 < 1 + \u03b5)\u2212 \u222b G \u222b G d\u00b5(g)d\u00b5(g\u2032)EnZx,z(n, g, g\u2032)\n+ \u222b G \u222b G d\u00b5(g)d\u00b5(g\u2032)En ( 1I\u2016n\u201622\u22651+\u03b5Zx,z(n, g, g \u2032) )\n(8)\nWe are left with evaluating or bounding two expectations: I1 = EnZx,z(n, g, g\u2032), and I2 = En ( 1I\u2016n\u201622\u22651+\u03b5Zx,z(n, g, g \u2032) ) , that involve the maximum of correlated gaussian variables as we will see in the following.\nBy rotation invariance of Gaussians we have that \u2329 g\u22121x, n \u232a , and \u2329 g\u2032\u22121z, n \u232a are two correlated\nrandom gaussian variables with correllation coefficient that we note by cos(\u03b8g,g\u2032) = \u2329 g\u22121x, g,\u22121z \u232a . Hence by a change of a basis we can write:\u2329 g\u22121x, n \u232a =\n1\u221a d u, \u2329 g\u2032\u22121z, n \u232a = 1\u221a d cos(\u03b8g,g\u2032)u+ 1\u221a d \u221a 1\u2212 cos2(\u03b8g,g\u2032)v\nwhere cos(\u03b8g,g\u2032) = \u2329 g\u22121x, g\u2032\u22121z \u232a , and u, v \u223c N (0, 1) iids.\nHence,\nI1 = 1\u221a d Eu,v max\n( u, cos(\u03b8g,g\u2032)u+ \u221a 1\u2212 cos2(\u03b8g,g\u2032)v ) .\nThe following Lemma from [26] gives the expectation and the variance of the maximum of two gaussians with correllation coefficient \u03c1.\nLemma 2 (Mean and Variance of Maximum of Correlated Gaussians [26] ). Let X \u223c N (\u00b5X , \u03c32X) and Y \u223c N (\u00b5Y , \u03c32Y ), two correlated gaussians with correllation coefficient \u03c1. Define \u03c6N (x) = 1\u221a 2\u03c0 exp(\u2212x2/2), and \u03a6N (y) = \u222b y \u2212\u221e \u03c6N (x)dx. Let a = \u221a \u03c32X + \u03c3 2 Y \u2212 2\u03c1\u03c3X\u03c3Y , and \u03b1 = \u00b5X\u2212\u00b5Y a . The mean \u00b5Z and variance \u03c32Z of Z = max(X,Y ) are expressed analytically as follows:\n\u00b5Z = \u00b5X\u03a6N (\u03b1) + \u00b5Y \u03a6N (\u2212\u03b1) + a\u03c6N (\u03b1). (9) \u03c32Z = ( \u03c32X + \u00b5 2 X ) \u03a6N (\u03b1) + ( \u03c32Y + \u00b5 2 Y ) \u03a6N (\u2212\u03b1) + (\u00b5X + \u00b5Y ) a\u03c6N (\u03b1)\ufe38 \ufe37\ufe37 \ufe38\nEZ2\n\u2212\u00b52Z . (10)\nApplying Lemma 2 to our case (\u00b5X = \u00b5Y = 0, \u03c3X = \u03c3Y = 1, \u03c1 = cos(\u03b8g,g\u2032)). We have: a = \u221a 2(1\u2212 cos(\u03b8g,g\u2032)) and \u03b1 = 0.\nI1 = 1\u221a d a\u03c6N (0)\n= 1\u221a 2\u03c0d\n\u221a 2(1\u2212 cos(\u03b8g,g\u2032))\n= 1\u221a 2\u03c0d\n\u2225\u2225g\u22121x\u2212 g\u2032\u22121z\u2225\u2225 2 . (11)\nWe turn now to I2 that we bound using Cauchy-Schwarz inequality: |I2| = \u2223\u2223\u2223En (1I\u2016n\u201622\u22651+\u03b5Zx,z(n, g, g\u2032))\u2223\u2223\u2223\n\u2264 \u221a E(1I\u2016n\u201622\u22651+\u03b5) \u221a E(Z2x,z(n, g, g \u2032))\n= \u221a P ( \u2016n\u201622 \u2265 1 + \u03b5 )\u221a E(Z2x,z(n, g, g \u2032)). (12)\nOn the first hand, applying again Lemma 2 (for EZ2) we have:\nE(Z2x,z(n, g, g \u2032) =\n1 d Eu,v\n( max ( u, cos(\u03b8g,g\u2032)u+ \u221a 1\u2212 cos2(\u03b8g,g\u2032)v ))2 = 1\nd (2\u03a6N (0))\n= 1\nd . (13)\nOn the other hand, note that \u2016n\u201622 has a (normalized) chi squared distribution with d degree of freedom \u03c72d , with mean 1 . The following Lemma gives upper bounds for the upper and lower tails of a chi square distribution.\nLemma 3 (\u03c72 tail bounds). Let X \u223c \u03c72k, a chi squared random variable with k degree of freedom. The following hold true for any \u03b5 \u2208 (0, 1):\n\u2022 Upper Bound for the upper tail [27]: P (\n1 kX \u2265 1 + \u03b5\n) \u2264 e\u2212k\u03b52/8.\n\u2022 Upper Bound for the lower tail [28]: For all k \u2265 2, u \u2265 k \u2212 1 we have:\nP (X < u) \u2264 1\u2212 1 2 exp\n( \u22121\n2 (u\u2212 k \u2212 (k \u2212 2) log(u/k) + log(k))\n) .\nMore specifically for u = k(1 + \u03b5) we have:\nP ( 1\nk X < 1 + \u03b5\n) \u2264 1\u2212 1\n2\ne\u2212\u03b5k/2 (1 + \u03b5) k\u22122 2\n\u221a k\n.\nApplying Lemma 3, for \u2016n\u201622. We have \u2016n\u2016 2 2 = 1 dX , where X \u223c \u03c7 2 d, hence:\nP ( \u2016n\u201622 \u2265 1 + \u03b5 ) \u2264 e\u2212d\u03b5 2/8, (14)\nPutting together Equations (12),(14), (13) we have finally:\n|I2| \u2264 e\u2212d\u03b5 2/16\n\u221a d . (15)\nPutting together Equations (8), (11), and (15), and using upper and lower bounds for P(\u2016n\u201622 < 1+\u03b5) from Lemma 3:\nKs(x, z) \u2264 (1 + \u03b5)P(\u2016n\u201622 < 1 + \u03b5)\u2212 1\u221a 2\u03c0d \u222b G \u222b G \u2225\u2225g\u22121x\u2212 g\u2032\u22121z\u2225\u2225 2 d\u00b5(g)d\u00b5(g\u2032) + e\u2212d\u03b5 2/16 \u221a d\n\u2264 (1 + \u03b5) ( 1\u2212 1\n2\ne\u2212\u03b5d/2 (1 + \u03b5) d\u22122 2\n\u221a d\n) \u2212 1\u221a\n2\u03c0d \u222b G \u222b G \u2225\u2225g\u22121x\u2212 g\u2032\u22121z\u2225\u2225 2 d\u00b5(g)d\u00b5(g\u2032)\n+ e\u2212d\u03b5 2/16\n\u221a d .\nKs(x, z) \u2265 P(\u2016n\u201622 < 1 + \u03b5)\u2212 1\u221a 2\u03c0d \u222b G \u222b G \u2225\u2225g\u22121x\u2212 g\u2032\u22121z\u2225\u2225 2 d\u00b5(g)d\u00b5(g\u2032)\u2212 e \u2212d\u03b52/16 \u221a d\n\u2265 (1 + \u03b5) ( 1\u2212 e\u2212d\u03b5 2/8 ) \u2212 1\u221a\n2\u03c0d \u222b G \u222b G \u2225\u2225g\u22121x\u2212 g\u2032\u22121z\u2225\u2225 2 d\u00b5(g)d\u00b5(g\u2032)\u2212 e \u2212d\u03b52/16 \u221a d .\nNoting by dG the integral and using that the group is compact and unitary:\ndG(x, z) = 1\u221a 2\u03c0d \u222b G \u222b G \u2225\u2225g\u22121x\u2212 g\u2032\u22121z\u2225\u2225 2 d\u00b5(g)d\u00b5(g\u2032)\n= 1\u221a 2\u03c0d \u222b G \u222b G \u2016gx\u2212 g\u2032z\u20162 d\u00b5(g)d\u00b5(g \u2032).\nWe finally have:\n\u2212e \u2212d\u03b52/16 \u221a d \u2212(1+\u03b5)e\u2212d\u03b5 2/8 +\u03b5 \u2264 Ks(x, z)\u2212(1\u2212 dG(x, z)) \u2264 e\u2212d\u03b5 2/16 \u221a d \u2212 1 2 e\u2212\u03b5d/2 (1 + \u03b5) d 2 \u221a d +\u03b5. (16) For any \u03b5 \u2208 (0, 1) , as the dimension d\u2192\u221e, we have asymptotically:\nKs(x, z)\u2192 1\u2212 dG(x, z) + \u03b5 = s\u2212 dG(x, z).\n2) The symmetry of K is obvious. Let p(t) be the distribution of the templates t. Define the following weighted dot product: \u3008f(x, ., .), g(z, ., .)\u3009 = \u222b t p(t) \u222b s \u2212s d\u03c4f(x, t, \u03c4)g(z, t, \u03c4). Recall that:\nKs(x, z) =\n\u222b p(t)dt \u222b s \u2212s \u03c8(x, t, \u03c4)\u03c8(z, t, \u03c4)d\u03c4\n= \u3008\u03c8(x, ., .), \u03c8(z, ., .)\u3009 .\nHence K is symmetric and positive semidefinite.\nProof of Theorem 2. In the following we fix two points x and z in X and a random template t. Let Xj = \u222b s \u2212s P(\u3008gtj , x\u3009 \u2264 \u03c4)P(\u3008gtj , z\u3009 \u2264 \u03c4)d\u03c4 , we have 0 \u2264 Xj \u2264 2s, where s = 1 + \u03b5. Recall that Ks(x, z) = 1 mEt( \u2211m j=1Xj). By Hoeffding\u2019s inequality we have:\nPt  \u2223\u2223\u2223\u2223\u2223\u2223 1m m\u2211 j=1 Xj \u2212Ks(x, z) \u2223\u2223\u2223\u2223\u2223\u2223 >  \u2264 2 exp ( \u22122m 2 (2s)2 )\nTurning now to the CDF \u03c8(x, t, \u03c4) = P(\u3008gt, x\u3009 \u2264 \u03c4), and the empirical CDF \u03c8\u0302(x, t, \u03c4) = 1 |G| \u2211|G| i=1 1I\u3008git,x\u3009\u2264\u03c4 . By the theorem on convergence of the empirical CDF [29] (Theorem 4 given in Appendix D ) we have, for \u03b3 > 0:\nPg {\nsup \u03c4 \u2223\u2223\u2223\u03c8\u0302(x, t, \u03c4)\u2212 \u03c8(x, t, \u03c4)\u2223\u2223\u2223 > \u03b3} \u2264 2 exp(\u22122|G|\u03b32) Hence we have \u2200\u03c4 \u2208 [\u2212s, s]:\u2223\u2223\u2223\u03c8\u0302(x, t, \u03c4)\u2212 \u03c8(x, t, \u03c4)\u2223\u2223\u2223 \u2264 \u03b3 and \u2223\u2223\u2223\u03c8\u0302(x, t, \u03c4)\u2212 \u03c8(z, t, \u03c4)\u2223\u2223\u2223 \u2264 \u03b3 with a probability at least 1\u2212 4 exp(\u22122|G|\u03b32). Define X = \u222b s \u2212s \u03c8(x, t, \u03c4)\u03c8(z, t, \u03c4)d\u03c4 , X\u0302 = \u222b s \u2212s \u03c8\u0302(x, t, \u03c4)\u03c8\u0302(z, t, \u03c4)d\u03c4 , and X\u0303 = (2s) n \u2211n k=\u2212n \u03c8\u0302(x, t, ks n )\u03c8\u0302(z, t, ks n ), choose 0 < \u03b3 < 1:\n|X\u0302 \u2212X| = \u2223\u2223\u2223\u2223\u222b s \u2212s ( \u03c8\u0302(x, t, \u03c4)\u03c8\u0302(z, t, \u03c4)\u2212 \u03c8(x, t, \u03c4)\u03c8(z, t, \u03c4) ) d\u03c4 \u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u222b s \u2212s ( \u03c8\u0302(x, t, \u03c4)\u2212 \u03c8(x, t, \u03c4) + \u03c8(x, t, \u03c4) )( \u03c8\u0302(z, t, \u03c4)\u2212 \u03c8(z, t, \u03c4) + \u03c8(z, t, \u03c4) ) \u2212 \u03c8(x, t, \u03c4)\u03c8(z, t, \u03c4)d\u03c4 \u2223\u2223\u2223\u2223 \u2264 (2\u03b3 + \u03b32)2s \u2264 6s\u03b3,\nwith probability 1 \u2212 4 exp(\u22122|G|\u03b32). Define Xj = \u222b s \u2212s \u03c8(x, tj , \u03c4)\u03c8(z, tj , \u03c4)d\u03c4 , X\u0302j =\u222b s\n\u2212s \u03c8\u0302(x, tj , \u03c4)\u03c8\u0302(z, tj , \u03c4)d\u03c4 , and X\u0303j = (2s) n \u2211n k=\u2212n \u03c8\u0302(x, tj , ks n )\u03c8\u0302(z, tj , ks n ), Then for all j =\n1 . . .m, we have |X\u0302j \u2212Xj | \u2264 6s\u03b3\nwith probability 1\u2212 4m exp(\u22122|G|\u03b32)\u2212 2 exp ( \u22122m 2 (2s)2 ) .\nNow we turn to the numerical approximation of the integra by a Riemann sum, we have for all j = 1 . . .m : \u2223\u2223\u2223X\u0302j \u2212 X\u0303j\u2223\u2223\u2223 \u2264 s\nn .\nHence the error decomposes in the following way:\n|\u3008\u03a6(x),\u03a6(z)\u3009 \u2212Ks(x, z)| = \u2223\u2223\u2223\u2223\u2223\u2223 1m m\u2211 j=1 X\u0303j \u2212Ks(x, z) \u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223  1 m m\u2211 j=1 X\u0303j \u2212 1 m m\u2211 j=1 X\u0302j +  1 m m\u2211 j=1 X\u0302j \u2212 1 m m\u2211 j=1 Xj +  1 m m\u2211 j=1 Xj \u2212Ks(x, z)\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223\u2223 1m m\u2211 j=1 X\u0303j \u2212 1 m m\u2211 j=1 X\u0302j \u2223\u2223\u2223\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38 Numerical Binning Error + \u2223\u2223\u2223\u2223\u2223\u2223 1m m\u2211 j=1 X\u0302j \u2212 1 m m\u2211 j=1 Xj \u2223\u2223\u2223\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38 Group CDF Approximation Error + \u2223\u2223\u2223\u2223\u2223\u2223 1m m\u2211 j=1 Xj \u2212Ks(x, z) \u2223\u2223\u2223\u2223\u2223\u2223\ufe38 \ufe37\ufe37 \ufe38 Templates Concentration Error\n\u2264 s n + 6s\u03b3 + .\nwith probability 1\u2212 4m exp(\u22122|G|\u03b32)\u2212 2 exp ( \u22122m 2 (2s)2 ) . For this to hold on all pairs of points in a set of cardinality N we have:\n|\u3008\u03a6(xi),\u03a6(xj)\u3009 \u2212K(xi, xj)| \u2264 s\nn + 6s\u03b3 + , i = 1 . . . N, j = 1 . . . N, with probability 1\u2212 4mN(N \u2212 1) exp(\u22122|G|\u03b32)\u2212 2N(N \u2212 1) exp ( \u2212m 2 2(s)2 ) . Hence we have for numerical constants C1, and C2, 0 < \u03b41, \u03b42 < 1, and 0 < \u03b50, \u03b51, \u03b52 < 1, for n \u2265 s\u03b50 , m \u2265 C1 \u03b521 log(N\u03b41 ),|G| \u2265 C2 \u03b522 log(Nm\u03b42 ), :\n|\u3008\u03a6(xi),\u03a6(xj)\u3009 \u2212Ks(xi, xj)| \u2264 \u03b50 + \u03b51 + \u03b52, i = 1 . . . N, j = 1 . . . N, with probability 1\u2212 \u03b41 \u2212 \u03b42."}, {"heading": "B Proof of Theorem 3", "text": "Proof of Lemma 1. Our proof parallels similar proofs in [16]. Note that functions of the form (7) are dense inHK . f(x) = \u2211 i \u03b1iKs(x, xi) = \u2211 i \u03b1i \u222b \u222b s \u2212s \u03c8(x, t, \u03c4)\u03c8(xi, t, \u03c4)p(t)dtd\u03c4\n= \u222b \u222b s \u2212s (p(t) \u2211 i \u03b1i\u03c8(xi, t, \u03c4))\u03c8(x, t, \u03c4)dtd\u03c4. Let \u03b2(t, \u03c4) = p(t) \u2211 i \u03b1i\u03c8(xi, t, \u03c4), since 0 \u2264\n\u03c8(x, t, \u03c4) \u2264 1, \u2200x, t, \u03c4 , we have |\u03b2(t,\u03c4)|p(t) \u2264 \u2211 i |\u03b1i| < \u221e, since \u03b1i are finite. Hence f can be written in the form:\nf(x) = \u222b \u222b s \u2212s \u03b2(t, \u03c4)\u03c8(x, t, \u03c4)dtd\u03c4, sup \u03c4,t |\u03b2(t, \u03c4)| p(t) <\u221e,\nand f \u2208 Fp.\nIn order to prove Theorem 3, we need some preliminary lemmas. The following Lemma assess the approximation of any function f \u2208 Fp, by a certain f\u0303 \u2208 F\u0303 .\nLemma 4 (F\u0303 Approximation of Fp). Let f be a function in Fp. Then for \u03b41, \u03b42 > 0, there exists a function f\u0303 \u2208 F\u0303 such that:\u2225\u2225\u2225f\u0303 \u2212 f\u2225\u2225\u2225\nL2(X ,\u03c1X ) \u2264 2sC\u221a m\n( 1 + \u221a 2 log ( 1\n\u03b41\n)) +\n2sC\u221a |G|\n( 1 + \u221a 2 log ( m\n\u03b42\n)) + 2sC\nn ,\nwith probability at least 1\u2212 \u03b41 \u2212 \u03b42.\nProof of Lemma 4. Let f \u2208 Fp, f(x) = \u222b \u222b s \u2212s w(t, \u03c4)\u03c8(x, t, \u03c4)d\u03c4dt.\nLet fj(x) = \u222b s \u2212s w(tj ,\u03c4) p(tj) \u03c8(x, tj , \u03c4)d\u03c4, f\u0302j(x) = \u222b s \u2212s w(tj ,\u03c4) p(tj)\n\u03c8\u0302(x, tj , \u03c4)d\u03c4, and f\u0303j(x) = s n \u2211n k=\u2212n w(tj , ks n ) p(tj) \u03c8\u0302(x, tj , ks n ). We have the following: Et(fj) = f , and 1 mEt( \u2211m j=1 fj) = f .\nConsider the Hilbert space L2(X , \u03c1X ), with dot product: \u3008f, g\u3009L2(X ,\u03c1X ) = \u222b X f(x)g(x)d\u03c1X (x).\n||fj ||L2(X ,\u03c1X ) = \u221a\u222b X \u222b s \u2212s \u222b s \u2212s w(tj , \u03c4 \u2032)w(tj , \u03c4) p2(tj) \u03c8(x, tj , \u03c4)\u03c8(x, tj , \u03c4 \u2032)d\u03c4d\u03c4 \u2032d\u03c1X (x) \u2264 (2sC),\nFix \u03b41 > 0, applying Lemma 7 we have therefore with probability 1\u2212 \u03b41:\u2225\u2225\u2225\u2225\u2225\u2225 1m m\u2211 j=1 fj \u2212 f \u2225\u2225\u2225\u2225\u2225\u2225 L2(X ,\u03c1X ) \u2264 2sC\u221a m ( 1 + \u221a 2 log ( 1 \u03b41 )) , (17)\nNow turn to: \u2225\u2225\u2225\u2225\u2225\u2225 1m m\u2211 j=1 (f\u0302j \u2212 fj) \u2225\u2225\u2225\u2225\u2225\u2225 L2(X ,\u03c1X ) \u2264 1\u221a m m\u2211 j=1 \u2225\u2225\u2225f\u0302j \u2212 fj\u2225\u2225\u2225 L2(X ,\u03c1X ) ,\n\u2225\u2225\u2225f\u0302j \u2212 fj\u2225\u2225\u22252 = \u222b X \u222b s \u2212s w2(tj , \u03c4) p2(tj) (\u03c8\u0302(x, tj , \u03c4)\u2212 \u03c8(x, tj , \u03c4))2d\u03c4d\u03c1X (x)\n\u2264 C2 \u222b s \u2212s \u2225\u2225\u2225\u03c8\u0302(., tj , \u03c4)\u2212 \u03c8(., tj , \u03c4)\u2225\u2225\u22252 L2(X ,\u03c1X ) d\u03c4.\nRecall that: \u03c8\u0302(x, t, \u03c4) = 1|G| \u2211|G| i=1 1I\u3008git,x\u3009\u2264\u03c4 , and \u03c8(x, t, \u03c4) = Eg\u03c8\u0302(x, t, \u03c4).\nClearly \u2225\u22251I\u3008.,gt\u3009\u2264\u03c4\u2225\u2225L2(X ,\u03c1X ) \u2264 1, hence applying again Lemma 7, for \u03b42 > 0 we have with probability 1\u2212 \u03b42: \u2225\u2225\u2225\u03c8\u0302(., tj , \u03c4)\u2212 \u03c8(., tj , \u03c4)\u2225\u2225\u22252 L2(X ,\u03c1X ) \u2264 1 |G| ( 1 + \u221a 2 log ( 1 \u03b42 ))2 , It follows that: \u2200j = 1 . . .m, \u2225\u2225\u2225f\u0302j \u2212 fj\u2225\u2225\u2225 \u2264 2Cs\u221a|G| ( 1 + \u221a 2 log ( 1 \u03b42 )) , with probability 1 \u2212m\u03b42. Hence with probability 1\u2212m\u03b42, we have:\u2225\u2225\u2225\u2225\u2225\u2225 1m m\u2211 j=1 (f\u0302j \u2212 fj) \u2225\u2225\u2225\u2225\u2225\u2225 L2(X ,\u03c1X ) \u2264 2Cs\u221a |G| ( 1 + \u221a 2 log ( 1 \u03b42 )) . (18)\nand by the approximation of a Riemann sum we have that:\u2225\u2225\u2225\u2225\u2225\u2225 1m m\u2211 j=1 (f\u0302j \u2212 f\u0303j) \u2225\u2225\u2225\u2225\u2225\u2225 L2(X ,\u03c1X ) \u2264 2sC n . (19)\nIt is clear that f\u0303 = 1m \u2211m j=1 f\u0303j \u2208 F\u0303 , hence, putting together equations (17),(18), and (19) we finally have:\u2225\u2225\u2225\u2225\u2225\u2225 1m m\u2211 j=1 f\u0303j \u2212 f \u2225\u2225\u2225\u2225\u2225\u2225 L2(X ,\u03c1X ) \u2264 \u2225\u2225\u2225\u2225\u2225\u2225 1m m\u2211 j=1 (f\u0303j \u2212 f\u0302j) \u2225\u2225\u2225\u2225\u2225\u2225 L2(X ,\u03c1X ) + \u2225\u2225\u2225\u2225\u2225\u2225 1m m\u2211 j=1 (f\u0302j \u2212 fj) \u2225\u2225\u2225\u2225\u2225\u2225 L2(X ,\u03c1X ) + \u2225\u2225\u2225\u2225\u2225\u2225 1m m\u2211 j=1 fj \u2212 f \u2225\u2225\u2225\u2225\u2225\u2225 L2(X ,\u03c1X )\n\u2264 2sC n + 2Cs\u221a |G|\n( 1 + \u221a 2 log ( 1\n\u03b42\n)) +\n2sC\u221a m\n( 1 + \u221a 2 log ( 1\n\u03b41 )) with probability 1\u2212 \u03b41 \u2212m\u03b42.\nThe following Lemma shows how the approximation of functions inFp, by functions in F\u0303 , translates to the expected Risk:\nLemma 5 (Bound on the Approximation Error). Let f \u2208 Fp, fix \u03b41, \u03b42 > 0. There exists a function f\u0303 \u2208 F\u0303 , such that:\nEV (f\u0303) \u2264 EV (f) + 2sLC\u221a m\n( 1 + \u221a 2 log ( 1\n\u03b41\n)) + L ( 2sC\u221a |G| ( 1 + \u221a 2 log ( m \u03b42 )) + 2sC n ) ,\nwith probability at least 1\u2212 \u03b41 \u2212 \u03b42. Proof of Lemma 5. EV (f\u0303) \u2212 EV (f) \u2264 \u222b X \u2223\u2223\u2223V (yf\u0303(x))\u2212 V (yf(x))\u2223\u2223\u2223 d\u03c1X (x) \u2264 L \u222bX |f\u0303(x) \u2212 f(x)|d\u03c1X (x) \u2264 L \u221a\u222b X (f\u0303(x)\u2212 f(x))2d\u03c1X (x) = L \u2225\u2225\u2225f\u0303 \u2212 f\u2225\u2225\u2225 L2(X ,\u03c1X ) , where we used the Lipschitz condition and Jensen inequality. The rest of the proof follows from Lemma 4.\nThe following Lemma gives a bound on the estimation of the expected Risk with finite training samples:\nLemma 6 (Bound on the Estimation Error). Fix \u03b4 > 0, then\nsup f\u2208F\u0303 \u2223\u2223\u2223EV (f)\u2212 E\u0302V (f)\u2223\u2223\u2223 \u2264 1\u221a N ( 4LsC + 2V (0) + LC \u221a 1 2 log ( 1 \u03b4 )) ,\nwith probability 1\u2212 \u03b4.\nProof. The proof follows from Theorem 5 given in Appendix D. It is sufficient to bound the Rademacher complexity of the class F\u0303 :\nRN (F\u0303) = Ex,\u03c3 [ sup f\u2208F\u0303 \u2223\u2223\u2223\u2223\u2223 1N N\u2211 i=1 \u03c3if(xi) \u2223\u2223\u2223\u2223\u2223 ] = Ex,\u03c3 sup f\u2208F\u0303 \u2223\u2223\u2223\u2223\u2223\u2223 sNn N\u2211 i=1 \u03c3i  m\u2211 j=1 n\u2211 k=\u2212n wj,k\u03c8\u0302 ( xi, tj , sk n )\u2223\u2223\u2223\u2223\u2223\u2223 \n= Ex,\u03c3 sup f\u2208F\u0303 \u2223\u2223\u2223\u2223\u2223\u2223 sNn m\u2211 j=1 n\u2211 k=\u2212n wj,k N\u2211 i=1 \u03c3i\u03c8\u0302 ( xi, tj , sk n )\u2223\u2223\u2223\u2223\u2223\u2223 \n\u2264 Ex,\u03c3 sC\nmNn m\u2211 j=1 n\u2211 k=\u2212n \u2223\u2223\u2223\u2223\u2223 N\u2211 i=1 \u03c3i\u03c8\u0302 ( xi, tj , sk n )\u2223\u2223\u2223\u2223\u2223 By Holder inequality: \u3008a, b\u3009 \u2264 \u2016a\u2016\u221e \u2016b\u20161 \u2264 sC mNn Ex m\u2211 j=1 n\u2211 k=\u2212n \u221a\u221a\u221a\u221aE\u03c3 ( N\u2211 i=1 \u03c3i\u03c8\u0302 ( xi, tj , sk n ))2 Jensen inequality, concavity of square root\nNote that E(\u03c3i\u03c3j) = 0, for i 6= j it follows that: E\u03c3 (\u2211N i=1 \u03c3i\u03c8\u0302 ( xi, tj , sk n ))2 = E\u03c3 \u2211N i=1 \u2211N `=1 \u03c3i\u03c3`\u03c8\u0302 ( xi, tj , sk n ) \u03c8\u0302 ( x`, tj , sk n ) =\u2211N\ni=1 \u03c8\u0302 2 ( xi, tj , sk n ) \u2264 N , since \u03c8\u0302(., ., .) \u2264 1. Finally:\nRm(F\u0303) \u2264 Cs\u221a N .\nWe are now ready to prove Theorem 3:\nProof of Theorem 3. Let f\u2217N = arg minf\u2208F\u0303 E\u0302V (f), f\u0303 = arg minf\u2208F\u0303 EV (f), fp = arg minf\u2208Fp EV (f).\nEV (f\u2217N )\u2212 min f\u2208Fp\nEV (f) = ( EV (f\u2217N )\u2212 EV (f\u0303) ) \ufe38 \ufe37\ufe37 \ufe38\nStatistical Error\n+ ( EV (f\u0303)\u2212 EV (fp) ) \ufe38 \ufe37\ufe37 \ufe38\nApproximation Error\nThe first term is the usual estimation or statistical error than we can bound using Lemma 6, we have: EV (f\u2217N )\u2212 EV (f\u0303) = ( EV (f\u2217N )\u2212 E\u0302V (f\u2217N ) ) + ( E\u0302V (f\u2217N )\u2212 E\u0302V (f\u0303) ) \ufe38 \ufe37\ufe37 \ufe38 \u22640,by optimality of f\u2217N + ( E\u0302V (f\u0303)\u2212 EV (f\u0303) )\n\u2264 2 sup f\u2208F\u0303 \u2223\u2223\u2223EV (f)\u2212 E\u0302V (f)\u2223\u2223\u2223 \u2264 2 1\u221a\nN\n( 4LsC + 2V (0) + LC \u221a 1\n2 log\n( 1\n\u03b4\n)) ,\nwith probability 1 \u2212 \u03b4 over the training samples. Let f\u0303p, the function defined in Lemma 4, that approximates fp in F\u0303 . By Lemma 5 we know that:\nEV (f\u0303p) \u2264 EV (fp) + 2sLC\u221a m\n( 1 + \u221a 2 log ( 1\n\u03b41\n)) + L ( 2sC\u221a |G| ( 1 + \u221a 2 log ( m \u03b42 )) + 2sC n ) ,\nwith probability 1 \u2212 \u03b41 \u2212 \u03b42, on the choice of the templates and the sampled group elements. By optimality of f\u0303 \u2208 F\u0303 , we have\nEV (f\u0303) \u2264 EV (f\u0303p) \u2264 EV (fp)+ 2sLC\u221a m\n( 1 + \u221a 2 log ( 1\n\u03b41\n)) +L ( 2sC\u221a |G| ( 1 + \u221a 2 log ( m \u03b42 )) + 2sC n ) Hence by a union bound with probability 1\u2212 \u03b4\u2212 \u03b41 \u2212 \u03b42, on the training set , the templates and the group elements we have:\nEV (f\u2217N )\u2212 min f\u2208Fp EV (f) \u2264 2 1\u221a N\n( 4LsC + 2V (0) + LC \u221a 1\n2 log\n( 1\n\u03b4\n))\n+ 2sLC\u221a m\n( 1 + \u221a 2 log ( 1\n\u03b41\n)) + L ( 2sC\u221a |G| ( 1 + \u221a 2 log ( m \u03b42 )) + 2sC n ) ."}, {"heading": "C Technical tools", "text": "Theorem 4. [29] Let X1, X2, ..., Xm be i.i.d. random variables with cumulative distribution function F , and let F\u0302m be the associated empirical cumulative density function F\u0302m = 1m \u2211m i=1 1IXi\u2264\u03c4 . Then for any \u03b3 > 0\nP {\nsup \u03c4 \u2223\u2223\u2223F\u0302m(\u03c4)\u2212 F (\u03c4)\u2223\u2223\u2223 > \u03b3} \u2264 2 exp (\u22122m\u03b32) . Lemma 7 ([15],Concentration of the mean of bounded random variables in a Hilbert Space). Let (H, \u3008., .\u3009H) be a Hilbert space. Let Xj , j = 1 . . .K, be iid random, such that ||Xj ||H \u2264 M . Then for any \u03b4 > 0, with probability 1\u2212 \u03b4,\u2225\u2225\u2225\u2225\u2225\u2225 1K K\u2211 j=1 Xj \u2212 1 K E K\u2211 j=1 Xj \u2225\u2225\u2225\u2225\u2225\u2225 H \u2264 M\u221a K ( 1 + \u221a 2 log ( 1 \u03b4 )) .\nTheorem 5 ([15]). Let F be a bounded class of function, supx\u2208X |f(x)| \u2264 C for all f \u2208 F . Let V be an L-Lipschitz loss. Then with probability 1 \u2212 \u03b4, with respect to training samples {xi, yi}i=1...N ,every f satisfies:\nEV (f) \u2264 E\u0302V (f) + 4LRN (F) + 2V (0)\u221a N + LC\n\u221a 1\n2N log\n1 \u03b4 ,\nwhereRN (F) is the Rademacher complexity of the class F:\nRN (F) = Ex,\u03c3 [ sup f\u2208F \u2223\u2223\u2223\u2223\u2223 1N N\u2211 i=1 \u03c3if(xi) \u2223\u2223\u2223\u2223\u2223 ] ,\nthe variables \u03c3i are iid symmetric Bernoulli random variables taking value in {\u22121, 1}, with equal probability and are independent form xi."}, {"heading": "D Numerical Evaluation", "text": "D.1 Permutation Invariance Experiment\nFor our first experiment, we created an artificial dataset which was designed to exploit permutation invariance, providing us with a finite group to which we had complete access. The dataset Xperm consists of all sequences of length L = 5, where each element of the sequence is taken from an alphabet A of 8 characters, giving us a total of 32,768 data points. Two characters c1, c2 \u2208 A were randomly chosen and designated as targets, so that a sequence x \u2208 Xperm is labeled positive if it contains both c1 and c2, where the position of these characters in the sequence does not matter.\nLikewise, any sequence that does not contain both characters is labeled negative. This provides us with a binary classification problem (positive sequences vs. negative sequences), for which the label is preserved by permutations of the sequence indices, i.e. two sequences will belong to the same orbit if and only if they are permuted versions of one another. The ith character in A is encoded as an 8-dimensional vector which is 0 in every position but the ith, where it is 1. Each sequence x \u2208 Xperm is formed by concatenating the 5 such vectors representing its characters, resulting in a binary vector of length 40. To build the permutation-invariant representation, we project a binary sequences onto an equal-length sequence consisting of standard-normal gaussian vectors, as well as all of its permutations, and then pool over the projections with a CDF. As a baseline, we also used a bag-of-words representation, where each x \u2208 Xperm was encoded with an 8-dimensional vector with ith element equal to the count of how many times character i appears in x. Note that this representation is also invariant to permutations, and so should share many of the benefits of our feature map. For all classification results, 4000 points were randomly chosen fromXperm to form the training set, with an even split of 2000 positive points and 2000 negative points. The remaining 28,768 points formed the test set. We know from Theorem 3 that the expected risk is dependent on the number of templates used to encode our data and on the number of bins used in the CDF-pooling step. The right panel of Figure 1 shows RLS classification accuracy on Xperm for different numbers of templates and bins. We see that, for a fixed number of templates, increasing the number of bins will improve accuracy, and for a fixed number of bins, adding more templates will improve accuracy. We also know there is a further dependence on the number of transformation samples from the group G. The left panel of figure 1 shows how classification accuracy, for a fixed number of training points, bins, and templates, depends on the number of transformation we have access to. We see the curve is rather flat, and there is a very graceful degradation in performance. In Figure 2, we include the sample complexity plot (for RLS) with the error bars added.\nD.2 TIDIGITS Experiment\nHere, we add plots showing performance as a function of number of templates and bins for some other splits of the TIDIGITS data."}], "references": [{"title": "Unsupervised learning of invariant representations in hierarchical architectures", "author": ["F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": "CoRR, vol. abs/1311.4158, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "CoRR, vol. abs/1203.1513, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A.C. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1828}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, pp. 2278\u20132324, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pp. 1106\u20131114, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Incorporating prior information in machine learning by creating virtual examples", "author": ["P. Niyogi", "F. Girosi", "T. Poggio"], "venue": "Proceedings of the IEEE, pp. 2196\u20132209, 1998.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning from hints in neural networks", "author": ["Y.-A. Mostafa"], "venue": "Journal of complexity, vol. 6, pp. 192\u2013198, June 1990.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Statistical learning theory", "author": ["V.N. Vapnik"], "venue": "A Wiley-Interscience Publication", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "Information Science and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Invariance in kernel methods by haar-integration kernels", "author": ["B. Haasdonk", "A. Vossen", "H. Burkhardt"], "venue": "SCIA , Springer, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Convexity, classification, and risk bounds", "author": ["P.L. Bartlett", "M.I. Jordan", "J.D. McAuliffe"], "venue": "Journal of the American Statistical Association, vol. 101, no. 473, pp. 138\u2013156, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Spline models for observational data, vol. 59 of CBMS-NSF", "author": ["G. Wahba"], "venue": "Regional Conference Series in Applied Mathematics. Philadelphia, PA: SIAM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Extensions of lipschitz mappings into a hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Conference in modern analysis and probability, 1984.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1984}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Uniform approximation of functions with random bases", "author": ["A. Rahimi", "B. Recht"], "venue": "Proceedings of the 46th Annual Allerton Conference, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Using the nystrm method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "NIPS, 2001.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "On the equivalence between quadrature rules and random features", "author": ["F.R. Bach"], "venue": "CoRR, vol. abs/1502.06800, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with transformation invariant kernels", "author": ["C. Walder", "O. Chapelle"], "venue": "NIPS, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel methods for deep learning", "author": ["Y. Cho", "L.K. Saul"], "venue": "NIPS, pp. 342\u2013350, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernel descriptors for visual recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "NIPS., 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid"], "venue": "NIPS, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Gurls: a least squares library for supervised learning", "author": ["A. Tacchetti", "P.K. Mallapragada", "M. Santoro", "L. Rosasco"], "venue": "CoRR, vol. abs/1303.0934, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Word-level invariant representations from acoustic waveforms", "author": ["S. Voinea", "C. Zhang", "G. Evangelopoulos", "L. Rosasco", "T. Poggio"], "venue": "vol. 14, pp. 3201\u20133205, September 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic speech recognition and speech variability: A review", "author": ["M. Benzeghiba", "R. De Mori", "O. Deroo", "S. Dupont", "T. Erbes", "D. Jouvet", "L. Fissore", "P. Laface", "A. Mertins", "C. Ris", "R. Rose", "V. Tyagi", "C. Wellekens"], "venue": "Speech Communication, vol. 49, pp. 763\u2013786, 01 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "The greatest of a finite set of random variables", "author": ["C.E. Clark"], "venue": "Operations Research, vol. 9, pp. 145\u2013162, Mar-Apr 1961.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1961}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Compressed Sensing: Theory and Applications, Y. Eldar and G. Kutyniok, Eds. Cambridge University Press., 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Inequalities for quantiles of the chi-square distribution", "author": ["T.Inglot"], "venue": "Probability and Mathematical Statistics, vol. 30(2):339351, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"], "venue": "Ann. Math. Statist., vol. 27, pp. 642\u2013669, 09 1956. 20", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1956}], "referenceMentions": [{"referenceID": 0, "context": "We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 0, "context": "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].", "startOffset": 374, "endOffset": 386}, {"referenceID": 1, "context": "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].", "startOffset": 374, "endOffset": 386}, {"referenceID": 2, "context": "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].", "startOffset": 374, "endOffset": 386}, {"referenceID": 3, "context": "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.", "startOffset": 30, "endOffset": 36}, {"referenceID": 4, "context": "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.", "startOffset": 267, "endOffset": 273}, {"referenceID": 6, "context": "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.", "startOffset": 267, "endOffset": 273}, {"referenceID": 0, "context": "In this work, we adopt the approach of [1] where the representation of the signal is designed to reflect the invariant properties and model the world symmetries with group actions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "The ultimate aim is to bridge unsupervised learning of invariant representations with invariant kernel methods, where we can easily address the statistical consistency and sample complexity questions, using tools from classical supervised learning [9, 10].", "startOffset": 248, "endOffset": 255}, {"referenceID": 8, "context": "The ultimate aim is to bridge unsupervised learning of invariant representations with invariant kernel methods, where we can easily address the statistical consistency and sample complexity questions, using tools from classical supervised learning [9, 10].", "startOffset": 248, "endOffset": 255}, {"referenceID": 9, "context": "We refer the reader to the related work section for a review (Section 5) and we start by showing how to accomplish this invariance through Haar-integration group-invariant kernels [11], and then show how random features derived from a memory based theory of invariances introduced in [1] approximate such a kernel.", "startOffset": 180, "endOffset": 184}, {"referenceID": 0, "context": "We refer the reader to the related work section for a review (Section 5) and we start by showing how to accomplish this invariance through Haar-integration group-invariant kernels [11], and then show how random features derived from a memory based theory of invariances introduced in [1] approximate such a kernel.", "startOffset": 284, "endOffset": 287}, {"referenceID": 9, "context": "1 Group Invariant Kernels We start by reviewing Haar-integration group-invariant kernels introduced in [11], and their use in a binary classification problem.", "startOffset": 103, "endOffset": 107}, {"referenceID": 9, "context": "Define an invariant kernel K between x, z \u2208 X through Haar-integration [11] as follows: K(x, z) = \u222b", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "Moreover, if k0 is a positive definite kernel, it follows that K is positive definite as well [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "In order to learn a decision function f : X \u2192 Y , we minimize the following empirical risk induced by an L-Lipschitz and convex loss function V , with V \u2032(0) < 0 [12]: minf\u2208HK \u00caV (f) := 1 N \u2211N i=1 V (yif(xi)), where we restrict f to belong to a hypothesis class induced by the invariant kernel K, the so called reproducing kernel hilbert space HK.", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "The representer theorem [13] shows that the solution of such a problem, or the optimal decision boundary f\u2217 N has the following form: f \u2217 N (x) = \u2211N i=1 \u03b1 \u2217 iK(x, xi).", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "EV (f) is a proxy to the misclassification risk [12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "We first show that a non linear random feature map \u03a6 : X \u2192 R derived from a memory based theory of invariances introduced in [1] induces an expected Haar-integration groupinvariant kernel K.", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "2 From Group Invariant Kernels to Feature Maps In this paper we show that a random feature map based on I-theory [1]: \u03a6 : X \u2192 R approximates a Haar-integration group-invariant kernel K having the form given in Equation (1): \u3008\u03a6(x),\u03a6(z)\u3009 \u2248 K(x, z).", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "1) The main advantage of such a feature map as outlined in [1], is that we store transformed templates in order to compute \u03a6, while if we needed to compute an invariant kernel of typeK (Equation (1)), we need to expliclitly transform the points.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "It falls in the category of memory-based learning, and is biologically plausible [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "Theorem 2 is in a sense an invariant Johnson Lindenstrauss [14] type result where we show that the dot product defined by the random feature map \u03a6 , i.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "The architecture of the proof follows ideas from [15] and [16].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "The architecture of the proof follows ideas from [15] and [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "Similarly to [16], we define the following infinite-dimensional function space: Fp = { f(x) = \u222b \u222b s", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "Several schemes have been proposed for approximating a non-linear kernel with an explicit non-linear feature map in conjunction with linear methods, such as the Nystr\u00f6m method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels [15].", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "Several schemes have been proposed for approximating a non-linear kernel with an explicit non-linear feature map in conjunction with linear methods, such as the Nystr\u00f6m method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels [15].", "startOffset": 267, "endOffset": 271}, {"referenceID": 16, "context": "We note that the relation between random features and quadrature rules has been thoroughly studied in [18], where sharper bounds and error rates are derived, and can apply to our setting.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "We focused in this paper on Haar-integration kernels [11], since they have an integral representation and hence can be represented with random features [18].", "startOffset": 53, "endOffset": 57}, {"referenceID": 16, "context": "We focused in this paper on Haar-integration kernels [11], since they have an integral representation and hence can be represented with random features [18].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "Other invariant kernels have been proposed: In [19] authors introduce transformation invariant kernels, but unlike our general setting, the analysis is concerned with dilation invariance.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "In [20], multilayer arccosine kernels are built by composing kernels that have an integral representation, but does not explicitly induce invariance.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "More closely related to our work is [21], where kernel descriptors are built for visual recognition by introducing a kernel view of histogram of gradients that corresponds in our case to the cumulative distribution on the group variable.", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "Finally the convolutional kernel network of [22] builds a sequence of multilayer kernels that have an integral representation, by convolution, considering spatial neighborhoods in an image.", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "Our future work will consider the composition of Haar-integration kernels, where the convolution is applied not only to the spatial variable but to the group variable akin to [2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 21, "context": "All RLS experiments in this paper were completed with the GURLS toolbox [23].", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "We seek local invariance to pitch and speaking rate [25], and so all random templates are pitch shifted up and down by 400 cents and warped to play at half and double speed.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "See [24] for more detail.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The following Lemma from [26] gives the expectation and the variance of the maximum of two gaussians with correllation coefficient \u03c1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "Lemma 2 (Mean and Variance of Maximum of Correlated Gaussians [26] ).", "startOffset": 62, "endOffset": 66}, {"referenceID": 25, "context": "\u2022 Upper Bound for the upper tail [27]: P ( 1 kX \u2265 1 + \u03b5 ) \u2264 e\u2212k\u03b52/8.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "\u2022 Upper Bound for the lower tail [28]: For all k \u2265 2, u \u2265 k \u2212 1 we have: P (X < u) \u2264 1\u2212 1 2 exp ( \u2212 2 (u\u2212 k \u2212 (k \u2212 2) log(u/k) + log(k)) ) .", "startOffset": 33, "endOffset": 37}, {"referenceID": 27, "context": "By the theorem on convergence of the empirical CDF [29] (Theorem 4 given in Appendix D ) we have, for \u03b3 > 0:", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Our proof parallels similar proofs in [16].", "startOffset": 38, "endOffset": 42}, {"referenceID": 27, "context": "[29] Let X1, X2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Lemma 7 ([15],Concentration of the mean of bounded random variables in a Hilbert Space).", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "Theorem 5 ([15]).", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1]. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of N points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.", "creator": "LaTeX with hyperref package"}}}