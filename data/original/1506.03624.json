{"id": "1506.03624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "Bootstrapping Skills", "abstract": "The monolithic approach to policy representation in Markov Decision Processes (MDPs) looks for a single policy that can be represented as a function from states to actions. For the monolithic approach to succeed (and this is not always possible), a complex feature representation is often necessary since the policy is a complex object that has to prescribe what actions to take all over the state space. This is especially true in large domains with complicated dynamics. It is also computationally inefficient to both learn and plan in MDPs using a complex monolithic approach. We present a different approach where we restrict the policy space to policies that can be represented as combinations of simpler, parameterized skills---a type of temporally extended action, with a simple policy representation. We introduce Learning Skills via Bootstrapping (LSB) that can use a broad family of Reinforcement Learning (RL) algorithms as a \"black box\" to iteratively learn parametrized skills. Initially, the learned skills are short-sighted but each iteration of the algorithm allows the skills to bootstrap off one another, improving each skill in the process. We prove that this bootstrapping process returns a near-optimal policy. Furthermore, our experiments demonstrate that LSB can solve MDPs that, given the same representational power, could not be solved by a monolithic approach. Thus, planning with learned skills results in better policies without requiring complex policy representations.", "histories": [["v1", "Thu, 11 Jun 2015 11:06:40 GMT  (1157kb,D)", "http://arxiv.org/abs/1506.03624v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel j mankowitz", "timothy a mann", "shie mannor"], "accepted": false, "id": "1506.03624"}, "pdf": {"name": "1506.03624.pdf", "metadata": {"source": "CRF", "title": "Bootstrapping Skills", "authors": ["Daniel J. Mankowitz", "Timothy A. Mann"], "emails": ["danielm@tx.technion.ac.il", "timothymann@google.com", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "State-of-the-art Reinforcement Learning (RL) algorithms need to produce compact solutions to large or continuous state Markov Decision Processes (MDPs), where a solution, called a policy, generates an action when presented with the current state. One such approach to producing compact solutions is linear function approximation.\nMDPs are important for both planning and learning in Reinforcement Learning (RL). The RL planning problem uses an MDP model to derive a policy that maximizes the sum of rewards received, while the RL learning problem learns an MDP model from experience (because the MDP model is unknown in advance). In this paper, we focus on RL planning, and use insights from RL that could be used to scale up to problems that are unsolvable with traditional planning approaches (such as Value Iteration and Policy Iteration (c.f., Puterman [1994]). A general result from ma-\nar X\niv :1\n50 6.\n03 62\n4v 1\n[ cs\n.A I]\nchine learning is that the sample complexity of learning increases with the complexity of the representation Vapnik [1998]. In a planning scenario, increased sample complexity directly translates to an increase in computational complexity. Thus monolithic approaches, which learn a single parametric policy that solves the entire MDP, scale poorly. This is because they often require highly complex feature representations, especially in high-dimensional domains with complicated dynamics, to support near-optimal policies. Instead, we investigate learning a collection of policies over a much simpler feature representation (compact policies) and combine those policies hierarchically.\nGeneralization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009].\nTemporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al. [2011], and options Sutton et al. [1999]. TEAs are known to speed up the convergence rate of MDP planning algorithms Sutton et al. [1999], Mann and Mannor [2014]. However, the effectiveness of planning with TEAs depends critically on the given actions. For example, Figure 1a depicts an episodic MDP with a single goal region and skills {\u03c31, \u03c32, . . . , \u03c35}. In this 2D setting, each skill represents a simple movement in a single, linear direction. Most of the TEAs move towards the goal region, but \u03c35 moves in the opposite direction of the goal making it impossible to reach. With these TEAs, we cannot hope to derive a satisfactory solution. On the other hand, if one of the TEAs takes the agent directly to the goal (Figure 1b, the monolithic approach), then planning becomes trivial. Notice, however, that this TEA may be quite complex, and therefore difficult to learn since, in this 2D setting, it represents non-linear movements in multiple directions.\nLearning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al. [1998]. However, prior work suffers from the following drawbacks: (1) lack of theoretical analysis guaranteeing that the derived policy will be near-optimal in continuous state MDPs, (2) the process of learning TEAs is so expensive that it needs to be ammortized over a sequence of MDPs, (3) the approach is not applicable to MDPs with large or continuous state-spaces, or (4) the learned TEAs do not generalize over the state-space. We provide the first theoretical guarantees for iteratively learning a set of simple, generalizable parametric TEAs (skills) in a continuous state MDP. The learned TEAs solve the given tasks in a near-optimal manner.\nSkills: Generalization & Temporal Abstraction: Skills are TEAs defined over a parametrized policy. Thus, they incorporate both temporal abstraction and generalization. As TEAs, skills are closely related to options Sutton et al. [1999] developed in the RL literature. In fact, skills, as defined here, are a special case of options. Therefore, skills inherit many of the useful theoretical properties of options (e.g., Precup et al. [1998]). The main difference between skills and more general options is that skills are based on parametric policies that can be initialized and reused in any region of the state space.\nWe introduce a novel meta-algorithm, Learning Skills via Bootstrapping (LSB), that uses an RL algorithm as a \u201cblack box\u201d to iteratively learn parametrized skills. The learning algorithm is given a partition of the state-space, and one skill is created for each class in the partition. This is a very weak requirement since any partition could be used, such as a grid. During an iteration, an RL algorithm is used to update each skill. The skills may be initialized arbitrarily, but\nafter the first iteration skills with access to a goal region or non-zero rewards will learn how to exploit those rewards (e.g., Figure 2, Iteration 1). On further iterations, the newly acquired skills propagate reward back to other regions of the state-space. Thus, skills that previously had no reward signal bootstrap off of the rewards of other skills (e.g., Figure 2, Iterations 2 and 5). Although each skill is only learned over a single partition class, it can be initialized in any state.\nIt is important to note that this paper deals primarily with learning TEAs or Skills that aid in both speeding up the convergence rate of RL planning algorithms Sutton et al. [1999], Mann and Mannor [2014], as well as enabling larger problems to be solved using skills with simple policy representations. Utilizing simple policy representations is advantageous since this results in better generalization and better sample efficiency. These skills represent a misspecified model of the problem since they are not known in advance. By learning skills, we are also therefore inherently tackling the learning problem as we are iteratively correcting a misspecified model.\nContributions: Our main contributions are (1) The introduction of Learning Skills via Bootstrapping (LSB), which requires no additional prior knowledge apart from a partition over the state-space. (2) LSB is the first algorithm for learning skills in continuous state-spaces with theoretical convergence guarantees. (3) Theorem 1, which relates the quality of the policy returned by LSB to the quality of the skills learned by the \u201cblack box\u201d RL algorithm. (4) Experiments demonstrating that LSB can solve MDPs that, given the same representational power, can not be solved by a policy derived from a monolithic approach. Thus, planning with learned skills allows us to work with simpler representations Barto et al. [2013], which ultimately allows us to solve larger MDPs."}, {"heading": "2 Background", "text": "Let M = \u3008S,A, P,R, \u03b3\u3009 be an MDP, where S is a (possibly infinite) set of states, A is a finite set of actions, P is a mapping from state-action pairs to probability distributions over next states, R maps each state-action pair to a reward in [0, 1], and \u03b3 \u2208 [0, 1) is the discount factor. While assuming the rewards are in [0, 1] may seem restrictive, any bounded space can be rescaled so that this assumption holds. A policy \u03c0(a|s) gives the probability of executing action a \u2208 A from state s \u2208 S. Let M be an MDP. The value function of a policy \u03c0 with respect to a state s \u2208 S is V \u03c0M (s) = E [\u2211\u221e\nt=1 \u03b3 t\u22121R(st, at)|s0 = s\n] where the expectation is taken with respect to the trajectory produced by following\npolicy \u03c0. The value function of a policy \u03c0 can also be written recursively as\nV \u03c0M (s) = Ea\u223c\u03c0(\u00b7|s) [R(s, a)] + \u03b3Es\u2032\u223cP (\u00b7|s,\u03c0) [V \u03c0(s\u2032)] , (1)\nwhich is known as the Bellman equation. The optimal Bellman equation can be written as V \u2217M (s) = maxa E [R(s, a)] + \u03b3Es\u2032\u223cP (\u00b7|s,\u03c0) [V \u2217(s\u2032)] . Let \u03b5 > 0. We say that a policy \u03c0 is \u03b5-optimal if V \u03c0M (s) \u2265 V \u2217M (s) \u2212 \u03b5 for all s \u2208 S. The action-value function of a policy \u03c0 can be defined by Q\u03c0M (s, a) = Ea\u223c\u03c0(\u00b7|s) [R(s, a)] + \u03b3Es\u2032\u223cP (\u00b7|s,\u03c0) [V \u03c0(s\u2032)] , for a state s \u2208 S and an action a \u2208 A, and the optimal action-value function is denoted by Q\u2217M (s, a). Throughout this paper, we will drop the dependence on M when it is clear from context."}, {"heading": "3 Skills", "text": "One of the key ideas behind skills is that they may be learned locally, but they can be used throughout the entire state-space. We present a new formal definition for skills and a skill policy.\nDefinition 1. A skill \u03c3 is defined by a pair \u3008\u03c0\u03b8, \u03b2\u3009, where \u03c0\u03b8 is a parametric policy with parameter vector \u03b8 and \u03b2 : S \u2192 {0, 1} indicates whether the skill has finished (i.e., \u03b2(s) = 1) or not (i.e., \u03b2(s) = 0) given the current state s \u2208 S. Definition 2. Let \u03a3 be a set of m \u2265 1 skills. A skill policy \u00b5 is a mapping \u00b5 : S \u2192 [m] where S is the state-space and [m] is the index set over skills.\nA skill policy selects which skill to initialize from the current state by returning the index of one of the skills. By defining skill policies to select an index (rather than the skill itself), we can use the same policy even as the set of skills is adapting. Next we define a Skill MDP, which is a sub-partition of a target MDP as shown in Figure 3.\nDefinition 3. Given a target MDP M = \u3008S,A, P,R, \u03b3\u3009 and value function VM , a Skill MDP for partition Pi is an MDP defined by M \u2032i = \u3008S\u2032, A, P \u2032, R\u2032, \u03b3\u3009 where S\u2032 = Pi \u222a {sT } where sT is a terminal state and A is the action set from M . The transition probability function P \u2032(s\u2032|s, a) and reward function R\u2032(s, a) are defined below. P \u2032(s\u2032|s, a) = R\u2032(s, a) = P (s\u2032|s, a) if s \u2208 Pi \u2227 s\u2032 \u2208 Pi\u2211 y\u2208S\\Pi P (y|s, a) if s \u2208 Pi \u2227 s\u2032 = sT\n1 if s = sT \u2227 s\u2032 = sT 0 if s = sT \u2227 s\u2032 6= sT\n,  0 if s = sT\u2211\ns\u2032\u2208Pi P (s\u2032|s, a)R(s, a) if s 6= sT \u2227 s\u2032 6= sT\u2211 y\u2208S\\Pi \u03c8(s, a, y) if s 6= sT \u2227 s\u2032 = sT ,\nwhere \u03c8(s, a, y) = P (y|s, a) (R(s, a) + \u03b3VM (y)), and \u03b3 is the discount factor from M .\nA Skill MDP M \u2032i is an episodic MDP that terminates once the agent escapes from Pi and upon terminating receives a reward equal to the value of the state the agent would have transitioned to in the target MDP. Therefore, we construct a modified MDP called a Skill MDP and apply a planning or RL algorithm to solve it. The resulting solution is a skill. Each Skill MDP M \u2032i is defined within the partition Pi. Given a good set of skills, planning can be significantly faster Sutton et al. [1999], Mann and Mannor [2014]. However, in many domains we may not be given a good set of skills. Therefore it is necessary to learn this set of skills given the unsatisfactory skill set. In the next section, we introduce an algorithm for dynamically improving skills via bootstrapping."}, {"heading": "4 Learning Skills via Bootstrapping (LSB) Algorithm", "text": "Algorithm 1: Learning Skills via Bootstrapping (LSB)\nRequire: M {Target MDP}, P {Partitioning of S}, K {# Iterations}\n1: m\u2190 |P| {# of partitions.} 2: \u00b5(s) = arg maxi\u2208[m] I{s \u2208 Pi} 3: Initialize \u03a3 with m skills. {1 skill per partition.} 4: for k = 1, 2, . . . ,K do {Do K iterations.} 5: for i = 1, 2, . . . ,m do {One update per skill.} 6: Policy Evaluation: 7: Evaluate \u00b5 with \u03a3 to obtain V \u3008\u00b5,\u03a3\u3009M 8: Skill Update: 9: Construct Skill MDP M \u2032i from M & V \u3008\u00b5,\u03a3\u3009 M\n10: Solve M \u2032i obtaining policy \u03c0\u03b8 11: \u03c3\u2032i \u2190 \u3008\u03c0\u03b8, \u03b2i\u3009 12: Replace \u03c3i in \u03a3 by \u03c3\u2032i 13: end for 14: end for 15: return \u3008\u00b5,\u03a3\u3009\nTarget MDP M\nSkill MDP M'1 Skill MDP M'2 Skill MDP ...\nSkill MDP M'9\nFigure 3: A partitioning of a target MDP in the pinball domain. Each sub-partition (partition class) i represents the skill MDP M \u2032i . Note that, so long as the classes overlap one another and the goal region is within one of the classes, near-optimal convergence is guaranteed. Therefore, the entire state-space does not have to be partitioned.\nLearning Skills via Bootstrapping (LSB, Algorithm 1) takes a target MDP M , a partition P over the state-space and a number of iterations K \u2265 1 and returns a pair \u3008\u00b5,\u03a3\u3009 containing a skill policy \u00b5 and a set of skills \u03a3. The number of skills m = |P| is equal to the number of classes in the partition P (line 1). The skill policy \u00b5 returned by LSB is defined (line 2) by\n\u00b5(s) = arg max i\u2208[m]\nI {s \u2208 Pi} , (2)\nwhere I{\u00b7} is the indicator function returning 1 if its argument is true and 0 otherwise and Pi denotes the ith class in the partition P . Thus \u00b5 simply returns the index of the skill associated with the partition class containing the current state. On line 3, LSB could either initialize \u03a3 with skills that we believe might be useful or initialize them arbitrarily, depending on our level of prior knowledge.\nNext (lines 4 \u2013 14), LSB performsK iterations. In each iteration, LSB updates the skills in \u03a3 (lines 5 \u2013 13). Remember that the value of a skill depends on how it is combined with other skills (e.g., Figure 1a failed because a single TEA prevented reaching the goal). If we allowed all skills to change simultaneously, the skills could not reliably bootstrap off of each other. Therefore, LSB updates each skill individually. Multiple iterations are needed so that the skill set can converge (Figure 2).\nThe process of updating a skill (lines 6 \u2013 12) starts by evaluating \u00b5 with the current skill set \u03a3 (line 6). Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation Sutton and Barto [1998] or LSTD Boyan [2002], modified to be used with skills. In our experiments, we used a straighforward variant of LSTD Sorg and Singh [2010]. Then we use the target MDP M to construct a Skill MDP M \u2032 (line 9). Next, LSB uses a planning or RL algorithm to approximately solve the Skill MDP M \u2032 returning a parametrized policy \u03c0\u03b8 (line 10). Any planning or RL algorithm for regular MDPs could fill this role provided that it produces a parametrized policy. However, in our experiments, we used a simple actor-critic PG algorithm, unless otherwise stated. Then a new\nskill \u03c3\u2032i = \u3008\u03c0\u03b8, \u03b2i\u3009 is created (line 11) where \u03c0\u03b8 is the policy derived on line 10 and \u03b2i(s) = { 0 if s \u2208 Pi 1 otherwise . The\ndefinition of \u03b2i means that the skill will terminate only if it leaves the ith partition. Finally, we update the skill set \u03a3 by replacing the ith skill with \u03c3\u2032i (line 12). It is important to note that in LSB, updating a skill is equivalent to solving a Skill MDP."}, {"heading": "5 Analysis of LSB", "text": "We provide the first convergence guarantee for iteratively learning skills in a continuous state MDP using LSB (Lemma 1 and Lemma 2, proven in the supplementary material). We use this guarantee as well as Lemma 2 to prove Theorem 1. This theorem enables us to analyze the quality of the policy returned by LSB. It turns out that the quality of the policy depends critically on the quality of the skill learning algorithm. An important parameter for determining the quality of a policy returned by LSB is the skill learning error defined below. Definition 4. Let P be a partition over the target MDP\u2019s state-space. The skill learning error is\n\u03b7P = max i\u2208[m] \u03b7i , (3)\nwhere \u03b7i is the smallest \u03b7i \u2265 0, such that V \u2217M \u2032i (s)\u2212 V \u03c0\u03b8M \u2032i (s) \u2264 \u03b7i , for all s \u2208 Pi and \u03c0\u03b8 is the policy returned by the skill learning algorithm executed on M \u2032 i .\nThe skill learning error quantifies the quality of the Skill MDP solutions returned by our skill learning algorithm. If we used an exact solver to learn skills, then \u03b7P = 0. However, if we use an approximate solver, then \u03b7P will be non-zero and the quality will depend on the partition P . Generally, using finer grain partitions will decrease \u03b7P . However, Theorem 1 reveals that adding too many skills can also negatively impact the returned policy\u2019s quality. Theorem 1. Let \u03b5 > 0. If we run LSB with partition P for K \u2265 log\u03b3 (\u03b5(1\u2212 \u03b3)) iterations, then the algorithm returns policy \u03d5 = \u3008\u00b5,\u03a3\u3009 such that\n\u2016V \u2217M \u2212 V \u03d5M\u2016\u221e \u2264 m\u03b7P\n(1\u2212 \u03b3)2 + \u03b5 , (4)\nwhere m is the number of classes in P .\nThe proof of Theorem 1 is divided into three parts (a complete proof is given in the supplementary material). The main challenge to proving Theorem 1 is that updating one skill can have a significant impact on the value of other skills. Our analysis starts by bounding the impact of updating one skill. Note that \u03a3 represents a skill set and \u03a3i represents a skill set where we have updated the ith skill (corresponding to the ith partition class Pi) in the set. (1) First, we show that error between V \u2217M , the globally optimal value function, and V \u3008\u00b5,\u03a3i\u3009 M , is a contraction when s \u2208 Pi and is bound by \u2016V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009 M \u2016\u221e + \u03b7P1\u2212\u03b3 otherwise (Lemma 1). (2) Next we apply an inductive argument to show that updating all m skills results in a \u03b3 contraction over the entire state space (Lemma 2). (3) Finally, we apply this contraction recursively, which proves Theorem 1.\nThis provides the first theoretical guarantees of convergence to a near optimal solution when iteratively learning a set of skills \u03a3 in a continuous state space. Theorem 1 tells us that when the skill learning error is small, LSB returns a near-optimal policy. The first term on the right hand side of (4) is the approximation error. This is the loss we pay for\nthe parametrized class of policies that we learn skills over. Since m represents the number of classes defined by the partition, we now have a formal way of analysing the effect of the partitioning structure. In addition, complex skills do not need to be designed by a domain expert; only the partitioning needs to be provided a-priori. The second term is the convergence error. It goes to 0 as the number of iterations K increases.\nAt first, the guarantee provided by Theorem 1 may appear similar to (Hauskrecht et al. [1998], Theorem 1). However, Hauskrecht et al. [1998] derive TEAs only at the beginning of the learning process and do not update them. On the other hand, LSB updates its skill set dynamically via bootstrapping. Thus, LSB does not require prior knowledge of the optimal value function.\nTheorem 1 does not explicitly present the effect of policy evaluation error, which occurs with any approximate policy evaluation technique. However, if the policy evaluation error is bounded by \u03bd > 0, then we can simply replace \u03b7P in (4) with (\u03b7P + \u03bd). Again, smaller policy evaluation error leads to smaller approximation error."}, {"heading": "6 Experiments and Results", "text": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain has similar results to PW and therefore has been moved to the supplementary material. We use two variations for the Pinball domain, namely maze-world, which we created, and pinball-world which is one of the standard pinball benchmark domains. Our experiments show that, using a simple policy representation, the monolithic approach is unable to adequately solve the tasks in each case as the policy representation is not complex enough. However, LSB can solve these tasks with the same simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations.\nRecall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC and PW domains, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al. [2009] for skill learning (see supplementary material for details). In the Pinball domains, we used Nearest-Neighbor Function Approximation (NN-FA) for PE and UCB Random Policy Search (UCB-RPS) for skill learning.\nIn our experiments, for the MC and PW domains, each skill is simply represented as a probability distribution over actions (independent of the state). We compare their performance to a policy using the same representation that has been derived using the monolithic approach. Each experiment is run for 10 independent trials. A 2\u00d72 grid partitioning is used for the skill partition in these domains, unless otherwise stated. Binary-grid features are used to estimate the value function. In the pinball domains, each skill is represented by 5 polynomial features corresponding to each state dimension and a bias term. A 4\u00d7 1\u00d7 1\u00d7 1 grid-partitioning is used for maze-world and a 4\u00d7 3\u00d7 1\u00d7 1 partitioning is used for pinball-world. The value function is represented by a KD-Tree containing 1000 state-value pairs uniformly sampled in the domain. A value for a particular state is obtained by assigning the value of the nearest neighbor to that state that is contained within the KD-tree. Each experiment in the pinball domain has been run for 5 independent trials. These are example representations. In principal, any value function and policy representation that is representative of the domain can be utilized."}, {"heading": "6.1 Puddle World", "text": "Puddle World is a 2-dimensional world containing two puddles. A successful agent should navigate to the goal location, avoiding the puddles. The state space is the \u3008x, y\u3009 location of the agent. Figure 4a compares the monolithic approach with LSB (for a 2\u00d7 2 grid partition). The monolithic approach achieves low average reward. However, with the same restricted policy representation, LSB combines a set of skills, resulting in a richer solution space and a higher average reward as seen in Figure 4a. This is comparable to the approximately optimal average reward attained by executing Approximate Value Iteration (AVI) for a huge number of iterations. In this experiment LSB is not initiated in the partition class containing the goal state but still achieves near-optimal convergence after only 2 iterations.\nFigure 4b compares the performance of different partitions where a 1 \u00d7 1 grid represents the monolithic approach. The skill learning error \u03b7P is significantly smaller for all the partitions greater than 1\u00d7 1, resulting in lower cost. On the other hand, according to Theorem 1, adding more skills m increases the cost. A tradeoff therefore exists between \u03b7P and m. In practice, \u03b7P tends to dominate m. In addition to the tradeoff, the importance of the partition design is"}, {"heading": "Maze-world Pinball-world", "text": "evident when analyzing the cost of the 3\u00d7 3 and 4\u00d7 4 grids. In this scenario, the 3\u00d7 3 partition design is better suited to Puddle World than the 4\u00d7 4 partition, resulting in lower cost."}, {"heading": "6.2 Skill Generalization", "text": "In the worst case, the number of skills learned by (LSB) is based on the partition. However, LSB may learn similar skills in different partition classes, adding redundancy to the skill set. This suggests that skills can be reused in different parts of the state-space, resulting in less skills compared to the number of partition classes. To validate this intuition, a 4 \u00d7 4 grid was created for both the Mountain Car and Puddle World domains. We ran LSB using this grid on each domain. Since it is more intuitive to visualize and analyze the reusable skills generated for the 2D Puddle World, we present these skills in a quiver plot superimposed on the Puddle World (Figure 4c). For each skill, the direction (red arrows in Figure 4c) is determined by sampling and averaging actions from the skill\u2019s probability distribution. As can be seen in Figure 4c, many of the learned skills have the same direction. These skills can therefore be combined into a single skill and reused throughout the state-space. In this case, the skill-set consisting of 16 skills can be reduced to a reusable skill-set of 5 skills (the four cardinal directions, including two skills that are in the approximately north direction). Therefore, skill reuse may further reduce the complexity of a solution."}, {"heading": "6.3 Pinball", "text": "These experiments have been performed in domains with simple dynamics. We decided to test LSB on a domain with significantly more complicated dynamics, namely Pinball Konidaris and Barto [2009]. The goal in Pinball is to direct an agent (the blue ball) to the goal location (the red region). The Pinball domain provides a sterner test for LSB as the velocity at which the agent is travelling needs to be taken into account to circumnavigate obstacles. In addition, collisions with obstacles in the environment are non-linear at obstacle vertices. The state space is the four-tuple \u3008x, y, x\u0307, y\u0307\u3009 where x, y represents the 2D location of the agent, and x\u0307, y\u0307 represents the velocities in each direction.\nTwo domains have been utilized, namely maze-world and pinball-world (Figure 5a and Figure 5d respectively). For maze-world, a 4 \u00d7 1 \u00d7 1 \u00d7 1 grid partitioning has been utilized and therefore 4 skills need to be learned using LSB. After running LSB on the maze-world domain, it can be seen in Figure 5b that LSB significantly outperforms the monolithic approach. Note that each skill in LSB has the same parametric representation as the monolithic approach. That is, a five-tuple \u30081, x, y, x\u0307, y\u0307\u3009. This simple parametric representation does not have the power to consistently solve maze-world using the monolithic approach. However, using LSB this simple representation is capable of solving the task in a near-optimal fashion as indicated on the average reward graph (Figure 5b) and resulting value function (Figure 5c).\nWe also tested LSB on the more challenging pinball-world domain (5d). The same LSB parameters were used as in maze-world, but the provided partitioning was a 4 \u00d7 3 \u00d7 1 \u00d7 1 grid. Therefore, 12 skills needed to be learned in this domain. More skills were utilized for this domain since the domain is significantly more complicated than maze-world and a more refined skill-set is required to solve the task. As can be seen in the average reward graph in Figure 5e, LSB clearly outperforms the monolithic approach in this domain. It is less than optimal but still manages to sufficiently perform the task (see value function, Figure 5f ). The drop in performance is due to the complicated obstacle setup, the non-linear dynamics when colliding with obstacle edges and the partition design."}, {"heading": "7 Discussion", "text": "In this paper, we introduced an iterative bootstrapping procedure for learning skills. This approach is similar to (and partly inspired by) skill chaining Konidaris and Barto [2009]. However, the heuristic approach applied by skill chaining may not produce a near-optimal policy even when the skill learning error is small. We provide theoretical results for LSB that directly relate the quality of the final policy to the skill learning error. LSB is the first algorithm that provides theoretical convergence guarantees whilst iteratively learning a set of skills in a continuous state space. In addition, the theoretical guarantees for LSB enable us to interlace skill learning with Policy Evaluation (PE). We can therefore perform PE whilst learning skills and still converge to a near-optimal solution.\nIn each of the experiments, LSB converges in very few iterations. This is because we perform policy evaluation in between each skill update, causing the global value function to converge at a fast pace. Initializing LSB in the partition class containing the goal state also results in value being propagated quickly to subsequent partition classes and therefore fast convergence. However, LSB can be initialized from any partition class.\nOne limitation of LSB is that it learns skills for all partition classes. This is a problem in high-dimensional statespaces. However, the problem can be overcome, by focusing only on the most important regions of the state-space. One way to identify these regions is by observing an expert\u2019s demonstrations Abbeel and Ng [2005], Argall et al.\n[2009]. In addition, we could apply self-organizing approaches to facilitate skill reuse Moerman [2009]. Skill reuse can be especially useful for transfer learning. Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set. In this environment, skill reuse can facilitate learning complex multi-agent policies (co-learning) with very few samples.\nGiven a task, LSB can learn and combine skills, based on a set of rules, to solve the task. This structure of learned skills and combination rules forms a generative action grammar Summers-Stay et al. [2012] which paves the way for building advanced skill structures that are capable of solving complex tasks in different environments and conditions.\nOne exciting extension of our work would be to incorporate skill interruption, similar to option interruption. Option interruption involves terminating an option based on an adaptive interruption rule Sutton et al. [1999]. Options are terminated when the value of continuing the current option is lower than the value of switching to a new option. This also implies that partition classes can overlap one another, as the option interruption rule ensures that the option with the best long term value is always being executed. Mankowitz et al. [2014] interlaced Sutton\u2019s interruption rule between iterations of value iteration and proved convergence to a global optimum. In addition, they take advantage of faster convergence rates due to temporal extension by adding a time-based regularization term resulting in a new option interruption rule. However, their results have not yet been extended to use with function approximation. Comanici and Precup [2010] have developed a policy gradient technique for learning the termination conditions of options. Their method involves augmentation of the state-space. However, the overall solution converges to a local optimum."}, {"heading": "A Appendix", "text": ""}, {"heading": "A.1 LSB Skill MDP", "text": "The formal definition of a Skill MDP is provided here for completeness.\nDefinition 5. Given a target MDP M = \u3008S,A, P,R, \u03b3\u3009 and value function VM , a Skill MDP for partition Pi is an MDP defined by M \u2032i = \u3008S\u2032, A, P \u2032, R\u2032, \u03b3\u3009 where S\u2032 = Pi \u222a {sT } where sT is a terminal state and A is the action set from M . The transition probabilities\nP \u2032(s\u2032|s, a) =  P (s\u2032|s, a) if s \u2208 Pi \u2227 s\u2032 \u2208 Pi\u2211\ny\u2208S\\Pi P (y|s, a) if s \u2208 Pi \u2227 s\u2032 = sT 1 if s = sT \u2227 s\u2032 = sT 0 if s = sT \u2227 s\u2032 6= sT ,\nthe reward function\nR\u2032(s, a) =  0 if s = sT\u2211\ns\u2032\u2208Pi P (s\u2032|s, a)R(s, a) if s 6= sT \u2227 s\u2032 6= sT\u2211 y\u2208S\\Pi \u03c8(s, a, y) if s 6= sT \u2227 s\u2032 = sT ,\nwhere \u03c8(s, a, y) = P (y|s, a) (R(s, a) + \u03b3VM (y)), and \u03b3 is the discount factor from M ."}, {"heading": "A.2 Proof of Theorem 1", "text": "In this section, we prove Theorem 1.\nWe will make use of the following notations. Form \u2265 1, we will denote by [m] the set {1, 2, . . . ,m}. Let \u03c3 = \u3008\u03c0\u03b8, \u03b2\u3009 be a skill. Suppose the skill \u03c3 is initialized from a state s.\n1. P\u03c0\u03b8\u03b2 (s \u2032|s, t) denotes the probability that the skill will terminate (i.e., return control to the agent) in state s\u2032\nexactly t \u2265 1 timesteps after being initialized. 2. R\u0303\u03c0\u03b8\u03b2,s denotes the expected, discounted sum of rewards received during \u03c3\u2019s execution. We use the \u00b7\u0303 notation\nto emphasize that this quantity is discounted.\nThe proof of Theorem 1 will make use of two lemmas. The first lemma (Lemma 1) demonstrates a relationship between the value of a skill policy before and after replacing a single skill. Within the skill\u2019s partition class there is a \u03b3-contraction (plus some error), but outside the skill\u2019s partition class the value may become worse by a bounded amount. The second lemma (Lemma 2) uses Lemma 1 to prove that after a complete iteration (each skill has been update once), there is a \u03b3-contraction (plus some error) over the entire state-space. We then prove Theorem 1 by applying the result of Lemma 2 recursively.\nLemma 1. Let\n1. M be the target MDP,\n2. P a partition of the state-space, 3. \u00b5 be the skill policy defined by P (i.e., \u00b5(s) = arg maxi\u2208[m] I{s \u2208 Pi}), 4. \u03a3 be an ordered set of m \u2265 1 skills, and 5. i \u2208 [m] be the index of the ith skill in \u03a3.\nSuppose we apply A to the Skill MDP M \u2032i defined by M and V \u3008\u00b5,\u03a3\u3009 M , obtain \u03c0\u03b8, construct a new skill \u03c3 \u2032 i = \u3008\u03c0\u03b8, \u03b2i\u3009, and create a new skill set \u03a3\u2032 = (\u03a3\\{\u03c3i}) \u222a {\u03c3\u2032i} by replacing the ith skill with the new skill, then\n\u2200s\u2208S , V \u3008\u00b5,\u03a3\u3009M (s)\u2212 V \u3008\u00b5,\u03a3\u2032\u3009 M (s) \u2264 \u03b7\n1\u2212 \u03b3 (5)\nand\nV \u2217M (s)\u2212 V \u3008\u00b5,\u03a3 \u2032\u3009 M (s) \u2264  \u03b3 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + \u03b71\u2212\u03b3 if s \u2208 Pi, and\u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + \u03b71\u2212\u03b3 otherwise, (6)\nwhere \u03b7 is the skill learning error."}, {"heading": "Proof.", "text": ""}, {"heading": "Proving that (5) holds:", "text": "First, we show that (5) holds. For each skill \u03c3i \u2208 \u03a3, we will denote the skill\u2019s policy and termination rule by \u03c0i and \u03b2i, respectively. If s \u2208 Pj where j 6= i, then\nV \u3008\u00b5,\u03a3\u3009 M (s)\u2212 V \u3008\u00b5,\u03a3\u2032\u3009 M (s) = ( R\u0303 \u03c0j \u03b2j ,s + \u221e\u2211 t=1 \u03b3t \u2211 s\u2032 P \u03c0j \u03b2j (s\u2032|s, t)V \u3008\u00b5,\u03a3\u3009(s\u2032) ) \u2212 ( R\u0303 \u03c0j \u03b2j ,s + \u221e\u2211 t=1 \u03b3t \u2211 s\u2032 P \u03c0j \u03b2j (s\u2032|s, t)V \u3008\u00b5,\u03a3\u2032\u3009(s\u2032) )\n\u2264 \u03b3 \u2225\u2225\u2225V \u3008\u00b5,\u03a3\u3009M \u2212 V \u3008\u00b5,\u03a3\u2032\u3009M \u2225\u2225\u2225\u221e .\nOn the other hand, if s \u2208 Pi, then\nV \u3008\u00b5,\u03a3\u3009 M (s)\u2212 V \u3008\u00b5,\u03a3\u2032\u3009 M (s) = V \u3008\u00b5,\u03a3\u3009 M (s) + ( V \u2217M \u2032i (s)\u2212 V \u2217 M \u2032i (s) ) \u2212 V \u3008\u00b5,\u03a3 \u2032\u3009 M (s) By inserting 0 = ( V \u2217M \u2032i (s)\u2212 V \u2217 M \u2032i (s) ) .\n= ( V \u3008\u00b5,\u03a3\u3009 M (s)\u2212 V \u2217M \u2032i (s) ) + ( V \u2217M \u2032i (s)\u2212 V \u3008\u00b5,\u03a3\u2032\u3009 M (s) ) Regrouping terms.\n\u2264 0 + ( V \u2217M \u2032i (s)\u2212 V \u3008\u00b5,\u03a3\u2032\u3009 M (s) ) The definition of M \u2032i \u21d2 V \u2217M \u2032i (s) \u2265 V \u3008\u00b5,\u03a3\u3009 M (s) .\n\u2264 \u03b7 By Definition 4. In either case,\nV \u3008\u00b5,\u03a3\u3009 M (s)\u2212 V \u3008\u00b5,\u03a3\u2032\u3009 M (s) \u2264 \u03b3 \u2225\u2225\u2225V \u3008\u00b5,\u03a3\u3009M \u2212 V \u3008\u00b5,\u03a3\u2032\u3009M \u2225\u2225\u2225\u221e + \u03b7 , which leads to (5) by recursing on this inequality."}, {"heading": "Proving that (6) holds:", "text": "If s /\u2208 Pi, then by (5), we have\nV \u2217M (s)\u2212 V \u3008\u00b5,\u03a3\u2032\u3009 M (s) \u2264 V \u2217M (s)\u2212 ( V \u3008\u00b5,\u03a3\u3009 M (s)\u2212 \u03b71\u2212\u03b3 ) \u2264\n\u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + \u03b71\u2212\u03b3 . Now we consider the case where s \u2208 Pi. Let \u03c3\u2032i = \u3008\u03c0\u03b8, \u03b2i\u3009 be the newly introduced skill. We will denote by \u03c3\u2032i\u3008\u00b5,\u03a3\u2032\u3009 the policy that first executes \u03c3\u2032i from a state s \u2208 Pi and then follows the policy \u3008\u00b5,\u03a3\u3009 thereafter.\n\u2200s\u2208Pi , V \u2217M (s)\u2212 V \u3008\u00b5,\u03a3\u2032\u3009 M (s) = V \u2217 M (s)\u2212 V \u03c3\u2032i\u3008\u00b5,\u03a3 \u2032\u3009 M \u2264 V \u2217M (s)\u2212 ( V \u03c3\u2217i \u3008\u00b5,\u03a3 \u2032\u3009 M \u2212 \u03b7 ) = ( R\u0303\u03c0 \u2217\n\u03b2i + \u221e\u2211 t=1 \u03b3t \u2211 s\u2032 P\u03c0 \u2217 \u03b2i (s\u2032|s, t)V \u2217M (s\u2032) ) \u2212 ( R\u0303\u03c0 \u2217\n\u03b2i + \u221e\u2211 t=1 \u03b3t \u2211 s\u2032 P\u03c0 \u2217 \u03b2i (s\u2032|s, t)V \u3008\u00b5,\u03a3 \u2032\u3009 M (s \u2032)\n) + \u03b7\n= \u221e\u2211 t=1 \u03b3t \u2211 s\u2032 P\u03c0 \u2217 \u03b2i (s\u2032|s, t) ( V \u2217M (s \u2032)\u2212 V \u3008\u00b5,\u03a3 \u2032\u3009 M (s \u2032) ) + \u03b7\n\u2264 \u221e\u2211 t=1 \u03b3t \u2211 s\u2032 P\u03c0 \u2217 \u03b2i (s\u2032|s, t) ( V \u2217M (s \u2032)\u2212 ( V \u3008\u00b5,\u03a3\u3009 M (s \u2032)\u2212 \u03b71\u2212\u03b3 )) + \u03b7\n\u2264 \u03b3 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + \u03b3 ( \u03b71\u2212\u03b3)+ \u03b7\n\u2264 \u03b3 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + \u03b71\u2212\u03b3 .\nLemma 2. Suppose we execute LSB for a single iteration. Let \u03a3 be the set of skills at the beginning of the iteration and \u03a3\u2032 be the set of skills after each skill has been updated and the iteration has completed, then\u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u2032\u3009M \u2225\u2225\u2225\u221e \u2264 \u03b3 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + m\u03b71\u2212 \u03b3 . (7) Proof. Without loss of generality, we assume that the skills are updated in order of increasing index. We denote the skill set at the beginning of the iteration by \u03a3 and the skill set at the end of the iteration by \u03a3\u2032 after all of the skills have been updated once. It will be convenient to refer to the intermediate skill sets that are created during an iteration. Therefore, we denote by \u03a3\u20321,\u03a3 \u2032 2, . . . ,\u03a3 \u2032 m = \u03a3\n\u2032, the set of skills after the first skill was replaced, the second skill was replaced, . . . , and after the mth skill was replaced, respectively.\nWe will proceed by induction on the skill updates. As the base case, notice that by Lemma 1, we have that\nV \u2217M (s)\u2212 V \u3008\u00b5,\u03a3\u20321\u3009 M (s) \u2264  \u03b3 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + \u03b71\u2212\u03b3 if s \u2208 P1, and\u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + \u03b71\u2212\u03b3 otherwise.\nLet 1 \u2264 i < m. Now suppose for \u03a3\u2032i, we have that\nV \u2217M (s)\u2212 V \u3008\u00b5,\u03a3\u2032i\u3009 M (s) \u2264  \u03b3 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + i\u03b71\u2212\u03b3 if s \u2208 \u22c3j\u2208[i]Pj , and\u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + i\u03b71\u2212\u03b3 otherwise.\nNow for \u03a3\u2032i+1, we have several cases:\n1. s \u2208 Pi+1: By applying Lemma 2, we see that\nV \u2217M (s)\u2212 V \u3008\u00b5,\u03a3\u2032i+1\u3009 M (s) \u2264 \u03b3 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u2032i\u3009M \u2225\u2225\u2225\u221e + \u03b71\u2212\u03b3 \u2264 \u03b3\n(\u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + i\u03b71\u2212\u03b3)+ \u03b71\u2212\u03b3 \u2264 \u03b3\n\u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + (i+1)\u03b71\u2212\u03b3 . 2. s \u2208 \u22c3\nj\u2208[i] Pj :\nBy applying Lemma 2, we see that\nV \u2217M (s)\u2212 V \u3008\u00b5,\u03a3\u2032i+1\u3009 M (s) \u2264 V \u2217M (s)\u2212 V \u3008\u00b5,\u03a3\u2032i\u3009 M (s) + V \u3008\u00b5,\u03a3\u2032i\u3009 M (s)\u2212 V \u3008\u00b5,\u03a3\u2032i+1\u3009 M (s)\n\u2264 V \u2217M (s)\u2212 V \u3008\u00b5,\u03a3\u2032i\u3009 M (s) + \u03b7 1\u2212\u03b3 \u2264 ( \u03b3 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + i\u03b71\u2212\u03b3)+ \u03b71\u2212\u03b3\n= \u03b3 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + (i+1)\u03b71\u2212\u03b3 .\n3. s /\u2208 \u22c3 j\u2208[i+1] Pj :\nAgain, by Lemma 2, we see that\nV \u2217M (s)\u2212 V \u3008\u00b5,\u03a3\u2032i+1\u3009 M (s) \u2264 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u2032i\u3009M \u2225\u2225\u2225\u221e + \u03b71\u2212\u03b3 \u2264\n(\u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + i\u03b71\u2212\u03b3)+ \u03b71\u2212\u03b3 \u2264 \u2225\u2225\u2225V \u2217M \u2212 V \u3008\u00b5,\u03a3\u3009M \u2225\u2225\u2225\u221e + (i+1)\u03b71\u2212\u03b3 .\nThus by the principle of mathematical induction the statement is true for i = 1, 2, . . . ,m \u2212 1. After performing m updates, \u22c3 j\u2208[m] Pj \u2261 S. Thus, we obtain the \u03b3-contraction over the entire state-space."}, {"heading": "A.2.1 Proof of Theorem 1", "text": "Proof. (of Theorem 1)\nThe loss of all policies is bounded by 11\u2212\u03b3 . Therefore, by applying Lemma 2 and recursing on (7) for K \u2265 log\u03b3 (\u03b5(1\u2212 \u03b3)) iterations, we obtain\n\u2016V \u2217M \u2212 V \u03d5M\u2016\u221e \u2264 \u03b3K ( 1 1\u2212\u03b3 ) + m\u03b7(1\u2212\u03b3)2\n\u2264 \u03b3log\u03b3(\u03b5(1\u2212\u03b3)) (\n1 1\u2212\u03b3 ) + m\u03b7(1\u2212\u03b3)2\n= (\u03b5(1\u2212 \u03b3)) (\n1 1\u2212\u03b3 ) + m\u03b7(1\u2212\u03b3)2\n= m\u03b7(1\u2212\u03b3)2 + \u03b5 ."}, {"heading": "A.3 Experiments", "text": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain is discussed here. The PW and Pinball domains are found in the main paper. The purpose of our experiments is to show that LSB can solve a complicated task with a simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations.\nRecall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC domain, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al. [2009] for skill learning.\nIn our experiments, for the MC domain, each skill is simply represented as a probability distribution over actions (independent of the state). We compare the performance to a policy using the same representation that has been derived using the monolithic approach. Each experiment is run for 10 independent trials. A 2\u00d7 2 grid partitioning is used for the skill partition in this domain, unless otherwise stated. Binary-grid features are used to estimate the value function.\nThese are example representations. In principal, any value function and policy representation that is representative of the domain can be utilized."}, {"heading": "A.3.1 Mountain Car", "text": "The Mountain Car domain consists of an under-powered car situated in a valley. The car has to leverage potential energy to propel itself up to the goal, which is the top of the rightmost hill. The state-space is the car\u2019s position and velocity \u3008p, v\u3009. Figure 6a compares the monolithic approach with LSB (for a 2\u00d7 2 grid partition). The monolithic approach achieves low average reward. However, with the same restricted policy representation, LSB combines a set of skills, resulting in a richer solution space and a higher average reward as seen in Figure 6a. This is comparable to the approximately optimal average reward. Convergence is achieved after a single iteration since LSB is initiated from the partition containing the goal location causing value to be instantaneously propagated to subsequent skills.\nFigure 6b compares the performance of different partitions where a 1\u00d7 1 grid represents the monolithic approach. As seen in the figure, the cost is lower for all partitions greater than 1\u00d7 1 which is consistent with the results in the main\npaper. Figure 7 indicates the resulting value functions for various grid sizes. The value function from the monolithic approach is not capable of solving the task whereas the 4\u00d7 4 grid partition\u2019s value function is near-optimal."}, {"heading": "A.3.2 Puddle World", "text": "Figure 8 compares the value functions for different grid sizes in Puddle World. The monolithic approach (1 \u00d7 1 partition) provides a highly sub-optimal solution since, according to its value function, the agent must travel directly through the puddles to reach the goal location. The 3\u00d7 3 grid provides a near-optimal solution."}, {"heading": "A.4 Modified Regular-Gradient ActorCritic", "text": "We used a very simple policy gradient algorithm (Algorithm 2) for skill learning. The algorithm is based on RegularGradient ActorCritic Bhatnagar et al. [2009]. The algorithm differs from Regular-Gradient ActorCritic because it uses different representations for approximating the value function and the policy. For a state action pair (s, a) \u2208 S \u00d7A, a functions \u03c6(s, a) \u2208 Rd and \u03b6(s, a) \u2208 Rd\u2032 mapped (s, a) to a vector with dimension d and d\u2032, respectively. We use the representation given by \u03c6 to approximate the value function and the representation given by \u03b6 to represent the policy.\nThe parametrized policy was defined by\n\u03c0\u03b8(a|s) = exp\n( \u03b8T \u03b6(s, a) )\u2211 a\u2032\u2208A exp (\u03b8T \u03b6(s, a\u2032)) , (8)\nwhere \u03b8 \u2208 Rd\u2032 are the learned policy parameters. We used representations such that d\u2032 d, meaning that the policy parametrization was much simpler than the representation used to approximate the value function. This allowed us to get an accurate representation of the value function, but restrict the policy space to very simple policies.\nAlgorithm 2 Modified Regular-Gradient ActorCritic"}, {"heading": "Require:", "text": "1. \u03c6 : mapping from states to a vector representation used to approximate the value, 2. \u03c9 : value function approximation parameters, 3. \u03b6 : mapping from states to a vector representation used to approximate the policy, 4. \u03b8 : policy parameters, 5. \u03b1 : the value learning rate (fast learning rate), 6. \u03b2 : the policy learning rate (slow learning rate, i.e., \u03b2 < \u03b1), and 7. (s, a, s\u2032, r) : a state-action-next-state-reward tuple.\n1: V\u0302NEW \u2190 ( r + \u03b3 \u2211 a\u2032\u2208A \u03c0\u03b8(a \u2032|s\u2032)\u03c9T\u03c6(s, a\u2032) ) {Estimate V \u03c0\u03b8 given the new sample.} 2: V\u0302OLD \u2190 \u03c9T\u03c6(s, a) {Use the current value function approximation to estimate the value.} 3: \u03b4 \u2190 ( V\u0302NEW \u2212 V\u0302OLD ) {Compute the temporal difference error.} 4: \u03c9\u2032 \u2190 \u03c9 + \u03b1\u03b4 { Update the value function weights using the fast learning rate \u03b1. } 5: \u03c8s,a \u2190 \u03b6(s, a)\u2212\n\u2211 a\u2032\u2208A \u03c0\u03b8(a \u2032, s)\u03b6(s, a\u2032) {Compute the \u201ccompatible features\u201d Bhatnagar et al. [2009]. }\n6: \u03b8\u2032 \u2190 \u03b8 + \u03b2\u03b4\u03c8s,a { Update the policy parameters using the slow learning rate \u03b2. } 7: Return \u3008\u03c9\u2032, \u03b8\u2032\u3009 { Updated value function and policy parameters. }\nAlthough using different representations for approximating the value function and the policy strictly violates the policy gradient theorem Sutton et al. [2000], it still tends to work well in practice.\nIn our experiments, we used a fast learning rate of \u03b1 = 0.1 and a slow learning rate of \u03b2 = 0.2\u03b1. Value function and policy parameters were initialized to zero vectors."}, {"heading": "A.5 Pinball Demonstration Videos", "text": "There are two videos attached showing a demonstration of a policy learned by LSB for the Pinball domain Konidaris and Barto [2009]. Both of these domains are analyzed and discussed in the main paper. The first video shows a policy learned for Maze-world and the second video shows a policy learned for Pinball-world, one of the standard pinball benchmark domains. The objective of the agent (blue ball) is to circumnavigate the obstacles and reach the goal region\n(red ball). A colored square is superimposed onto the active skill indicating the current skill or partition class being executed."}], "references": [{"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Abbeel and Ng.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2005}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Behavioral hierarchy: Exploration and representation", "author": ["Andrew Barto", "George Konidaris", "C.M. Vigorito"], "venue": "In Computational and Robotic Models of the Hierarchical Organization of Behavior,", "citeRegEx": "Barto et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barto et al\\.", "year": 2013}, {"title": "Dynamic programming and optimal control, volume 1", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "Bertsekas.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas.", "year": 1995}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["Justin A Boyan"], "venue": "Machine Learning,", "citeRegEx": "Boyan.,? \\Q2002\\E", "shortCiteRegEx": "Boyan.", "year": 2002}, {"title": "PAC-inspired option discovery in lifelong reinforcement learning", "author": ["Emma Brunskill", "Lihong Li"], "venue": "JMLR, 1:316\u2013324,", "citeRegEx": "Brunskill and Li.,? \\Q2014\\E", "shortCiteRegEx": "Brunskill and Li.", "year": 2014}, {"title": "Optimal policy switching algorithms for reinforcement learning", "author": ["Gheorghe Comanici", "Doina Precup"], "venue": "In Proceedings of the 9 th AAMAS,", "citeRegEx": "Comanici and Precup.,? \\Q2010\\E", "shortCiteRegEx": "Comanici and Precup.", "year": 2010}, {"title": "Accelerating Multi-agent Reinforcement Learning with Dynamic Co-learning", "author": ["Daniel Garant", "Bruno C. da Silva", "Victor Lesser", "Chongjie Zhang"], "venue": "Technical report,", "citeRegEx": "Garant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garant et al\\.", "year": 2015}, {"title": "Hierarchical solution of markov decision processes using macro-actions", "author": ["Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In Proceedings of the 14th Conference on Uncertainty in AI,", "citeRegEx": "Hauskrecht et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht et al\\.", "year": 1998}, {"title": "Efficient planning under uncertainty with macro-actions", "author": ["Ruijie He", "Emma Brunskill", "Nicholas Roy"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "He et al\\.,? \\Q2011\\E", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Skill discovery in continuous reinforcement learning domains using skill chaining", "author": ["George Konidaris", "Andrew G Barto"], "venue": "In NIPS", "citeRegEx": "Konidaris and Barto.,? \\Q2009\\E", "shortCiteRegEx": "Konidaris and Barto.", "year": 2009}, {"title": "Time regularized interrupting options", "author": ["Daniel J Mankowitz", "Timothy A Mann", "Shie Mannor"], "venue": null, "citeRegEx": "Mankowitz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mankowitz et al\\.", "year": 2014}, {"title": "Scaling up approximate value iteration with options: Better policies with fewer iterations", "author": ["Timothy A Mann", "Shie Mannor"], "venue": "In Proceedings of the 31 st ICML,", "citeRegEx": "Mann and Mannor.,? \\Q2014\\E", "shortCiteRegEx": "Mann and Mannor.", "year": 2014}, {"title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density", "author": ["Amy McGovern", "Andrew G Barto"], "venue": "In Proceedings of the 18th ICML,", "citeRegEx": "McGovern and Barto.,? \\Q2001\\E", "shortCiteRegEx": "McGovern and Barto.", "year": 2001}, {"title": "Hierarchical reinforcement learning: Assignment of behaviours to subpolicies by self-organization", "author": ["Wilco Moerman"], "venue": "PhD thesis,", "citeRegEx": "Moerman.,? \\Q2009\\E", "shortCiteRegEx": "Moerman.", "year": 2009}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Jan Peters", "Stefan Schaal"], "venue": "Neural Networks,", "citeRegEx": "Peters and Schaal.,? \\Q2008\\E", "shortCiteRegEx": "Peters and Schaal.", "year": 2008}, {"title": "Theoretical results on reinforcement learning with temporally abstract options", "author": ["Doina Precup", "Richard S Sutton", "Satinder Singh"], "venue": "In ECML-98,", "citeRegEx": "Precup et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Precup et al\\.", "year": 1998}, {"title": "Markov Decision Processes - Discrete Stochastic Dynamic Programming", "author": ["Martin L Puterman"], "venue": "Jonathan Sorg and Satinder Singh. Linear options. In Proceedings 9 AAMAS,", "citeRegEx": "Puterman.,? \\Q1994\\E", "shortCiteRegEx": "Puterman.", "year": 1994}, {"title": "Using a minimal action grammar for activity understanding in the real world", "author": ["Douglas Summers-Stay", "Ching Lik Teo", "Yezhou Yang", "C Fermuller", "Yiannis Aloimonos"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "Summers.Stay et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Summers.Stay et al\\.", "year": 2012}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["Richard Sutton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutton.,? \\Q1996\\E", "shortCiteRegEx": "Sutton.", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David McAllester", "Satindar Singh", "Yishay Mansour"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Statistical learning theory, volume 2", "author": ["Vladimir Naumovich Vapnik"], "venue": "Wiley New York,", "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}], "referenceMentions": [{"referenceID": 17, "context": ", Puterman [1994]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 14, "context": "chine learning is that the sample complexity of learning increases with the complexity of the representation Vapnik [1998]. In a planning scenario, increased sample complexity directly translates to an increase in computational complexity.", "startOffset": 109, "endOffset": 123}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996].", "startOffset": 223, "endOffset": 240}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al.", "startOffset": 223, "endOffset": 255}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al.", "startOffset": 223, "endOffset": 451}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al.", "startOffset": 223, "endOffset": 476}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al.", "startOffset": 223, "endOffset": 774}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al.", "startOffset": 223, "endOffset": 814}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al. [2011], and options Sutton et al.", "startOffset": 223, "endOffset": 832}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al. [2011], and options Sutton et al. [1999]. TEAs are known to speed up the convergence rate of MDP planning algorithms Sutton et al.", "startOffset": 223, "endOffset": 866}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al. [2011], and options Sutton et al. [1999]. TEAs are known to speed up the convergence rate of MDP planning algorithms Sutton et al. [1999], Mann and Mannor [2014].", "startOffset": 223, "endOffset": 963}, {"referenceID": 3, "context": "Generalization: the ability of a system to perform accurately on unseen data, is important for machine learning in general, and can be achieved in this context by restricting the policy space, resulting in compact policies Bertsekas [1995], Sutton [1996]. Policy Search (PS) algorithms, a form of generalization, learn and maintain a compact policy representation so that the policy generates similar actions in nearby states Peters and Schaal [2008], Bhatnagar et al. [2009]. Temporally Extended Actions [TEAs, Sutton et al., 1999]: Compact policies can be represented and combined hierarchically as TEAs. TEAs are control structures that execute for multiple timesteps. They have been extensively studied under different names, including skills Konidaris and Barto [2009], macro-actions Hauskrecht et al. [1998], He et al. [2011], and options Sutton et al. [1999]. TEAs are known to speed up the convergence rate of MDP planning algorithms Sutton et al. [1999], Mann and Mannor [2014]. However, the effectiveness of planning with TEAs depends critically on the given actions.", "startOffset": 223, "endOffset": 987}, {"referenceID": 10, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al.", "startOffset": 67, "endOffset": 93}, {"referenceID": 10, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al.", "startOffset": 67, "endOffset": 109}, {"referenceID": 8, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al.", "startOffset": 110, "endOffset": 137}, {"referenceID": 5, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al.", "startOffset": 138, "endOffset": 162}, {"referenceID": 5, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al. [1998]. However, prior work suffers from the following drawbacks: (1) lack of theoretical analysis guaranteeing that the derived policy will be near-optimal in continuous state MDPs, (2) the process of learning TEAs is so expensive that it needs to be ammortized over a sequence of MDPs, (3) the approach is not applicable to MDPs with large or continuous state-spaces, or (4) the learned TEAs do not generalize over the state-space.", "startOffset": 138, "endOffset": 188}, {"referenceID": 5, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al. [1998]. However, prior work suffers from the following drawbacks: (1) lack of theoretical analysis guaranteeing that the derived policy will be near-optimal in continuous state MDPs, (2) the process of learning TEAs is so expensive that it needs to be ammortized over a sequence of MDPs, (3) the approach is not applicable to MDPs with large or continuous state-spaces, or (4) the learned TEAs do not generalize over the state-space. We provide the first theoretical guarantees for iteratively learning a set of simple, generalizable parametric TEAs (skills) in a continuous state MDP. The learned TEAs solve the given tasks in a near-optimal manner. Skills: Generalization & Temporal Abstraction: Skills are TEAs defined over a parametrized policy. Thus, they incorporate both temporal abstraction and generalization. As TEAs, skills are closely related to options Sutton et al. [1999] developed in the RL literature.", "startOffset": 138, "endOffset": 1068}, {"referenceID": 5, "context": "Learning a useful set of TEAs has been a topic of intense research McGovern and Barto [2001], Moerman [2009], Konidaris and Barto [2009], Brunskill and Li [2014], Hauskrecht et al. [1998]. However, prior work suffers from the following drawbacks: (1) lack of theoretical analysis guaranteeing that the derived policy will be near-optimal in continuous state MDPs, (2) the process of learning TEAs is so expensive that it needs to be ammortized over a sequence of MDPs, (3) the approach is not applicable to MDPs with large or continuous state-spaces, or (4) the learned TEAs do not generalize over the state-space. We provide the first theoretical guarantees for iteratively learning a set of simple, generalizable parametric TEAs (skills) in a continuous state MDP. The learned TEAs solve the given tasks in a near-optimal manner. Skills: Generalization & Temporal Abstraction: Skills are TEAs defined over a parametrized policy. Thus, they incorporate both temporal abstraction and generalization. As TEAs, skills are closely related to options Sutton et al. [1999] developed in the RL literature. In fact, skills, as defined here, are a special case of options. Therefore, skills inherit many of the useful theoretical properties of options (e.g., Precup et al. [1998]).", "startOffset": 138, "endOffset": 1272}, {"referenceID": 17, "context": "It is important to note that this paper deals primarily with learning TEAs or Skills that aid in both speeding up the convergence rate of RL planning algorithms Sutton et al. [1999], Mann and Mannor [2014], as well as enabling larger problems to be solved using skills with simple policy representations.", "startOffset": 161, "endOffset": 182}, {"referenceID": 11, "context": "[1999], Mann and Mannor [2014], as well as enabling larger problems to be solved using skills with simple policy representations.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": "Thus, planning with learned skills allows us to work with simpler representations Barto et al. [2013], which ultimately allows us to solve larger MDPs.", "startOffset": 82, "endOffset": 102}, {"referenceID": 18, "context": "Given a good set of skills, planning can be significantly faster Sutton et al. [1999], Mann and Mannor [2014].", "startOffset": 65, "endOffset": 86}, {"referenceID": 12, "context": "[1999], Mann and Mannor [2014]. However, in many domains we may not be given a good set of skills.", "startOffset": 8, "endOffset": 31}, {"referenceID": 18, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation Sutton and Barto [1998] or LSTD Boyan [2002], modified to be used with skills.", "startOffset": 105, "endOffset": 129}, {"referenceID": 4, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation Sutton and Barto [1998] or LSTD Boyan [2002], modified to be used with skills.", "startOffset": 137, "endOffset": 150}, {"referenceID": 4, "context": "Any number of policy evaluation algorithms could be used here, such as TD(\u03bb) with function approximation Sutton and Barto [1998] or LSTD Boyan [2002], modified to be used with skills. In our experiments, we used a straighforward variant of LSTD Sorg and Singh [2010]. Then we use the target MDP M to construct a Skill MDP M \u2032 (line 9).", "startOffset": 137, "endOffset": 267}, {"referenceID": 8, "context": "At first, the guarantee provided by Theorem 1 may appear similar to (Hauskrecht et al. [1998], Theorem 1).", "startOffset": 69, "endOffset": 94}, {"referenceID": 8, "context": "At first, the guarantee provided by Theorem 1 may appear similar to (Hauskrecht et al. [1998], Theorem 1). However, Hauskrecht et al. [1998] derive TEAs only at the beginning of the learning process and do not update them.", "startOffset": 69, "endOffset": 141}, {"referenceID": 18, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009].", "startOffset": 97, "endOffset": 111}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain has similar results to PW and therefore has been moved to the supplementary material.", "startOffset": 134, "endOffset": 161}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain has similar results to PW and therefore has been moved to the supplementary material. We use two variations for the Pinball domain, namely maze-world, which we created, and pinball-world which is one of the standard pinball benchmark domains. Our experiments show that, using a simple policy representation, the monolithic approach is unable to adequately solve the tasks in each case as the policy representation is not complex enough. However, LSB can solve these tasks with the same simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations. Recall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC and PW domains, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al.", "startOffset": 134, "endOffset": 1218}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain has similar results to PW and therefore has been moved to the supplementary material. We use two variations for the Pinball domain, namely maze-world, which we created, and pinball-world which is one of the standard pinball benchmark domains. Our experiments show that, using a simple policy representation, the monolithic approach is unable to adequately solve the tasks in each case as the policy representation is not complex enough. However, LSB can solve these tasks with the same simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations. Recall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC and PW domains, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al. [2009] for skill learning (see supplementary material for details).", "startOffset": 134, "endOffset": 1305}, {"referenceID": 8, "context": "We decided to test LSB on a domain with significantly more complicated dynamics, namely Pinball Konidaris and Barto [2009]. The goal in Pinball is to direct an agent (the blue ball) to the goal location (the red region).", "startOffset": 96, "endOffset": 123}, {"referenceID": 8, "context": "We decided to test LSB on a domain with significantly more complicated dynamics, namely Pinball Konidaris and Barto [2009]. The goal in Pinball is to direct an agent (the blue ball) to the goal location (the red region). The Pinball domain provides a sterner test for LSB as the velocity at which the agent is travelling needs to be taken into account to circumnavigate obstacles. In addition, collisions with obstacles in the environment are non-linear at obstacle vertices. The state space is the four-tuple \u3008x, y, \u1e8b, \u1e8f\u3009 where x, y represents the 2D location of the agent, and \u1e8b, \u1e8f represents the velocities in each direction. Two domains have been utilized, namely maze-world and pinball-world (Figure 5a and Figure 5d respectively). For maze-world, a 4 \u00d7 1 \u00d7 1 \u00d7 1 grid partitioning has been utilized and therefore 4 skills need to be learned using LSB. After running LSB on the maze-world domain, it can be seen in Figure 5b that LSB significantly outperforms the monolithic approach. Note that each skill in LSB has the same parametric representation as the monolithic approach. That is, a five-tuple \u30081, x, y, \u1e8b, \u1e8f\u3009. This simple parametric representation does not have the power to consistently solve maze-world using the monolithic approach. However, using LSB this simple representation is capable of solving the task in a near-optimal fashion as indicated on the average reward graph (Figure 5b) and resulting value function (Figure 5c). We also tested LSB on the more challenging pinball-world domain (5d). The same LSB parameters were used as in maze-world, but the provided partitioning was a 4 \u00d7 3 \u00d7 1 \u00d7 1 grid. Therefore, 12 skills needed to be learned in this domain. More skills were utilized for this domain since the domain is significantly more complicated than maze-world and a more refined skill-set is required to solve the task. As can be seen in the average reward graph in Figure 5e, LSB clearly outperforms the monolithic approach in this domain. It is less than optimal but still manages to sufficiently perform the task (see value function, Figure 5f ). The drop in performance is due to the complicated obstacle setup, the non-linear dynamics when colliding with obstacle edges and the partition design. 7 Discussion In this paper, we introduced an iterative bootstrapping procedure for learning skills. This approach is similar to (and partly inspired by) skill chaining Konidaris and Barto [2009]. However, the heuristic approach applied by skill chaining may not produce a near-optimal policy even when the skill learning error is small.", "startOffset": 96, "endOffset": 2429}, {"referenceID": 0, "context": "One way to identify these regions is by observing an expert\u2019s demonstrations Abbeel and Ng [2005], Argall et al.", "startOffset": 77, "endOffset": 98}, {"referenceID": 11, "context": "In addition, we could apply self-organizing approaches to facilitate skill reuse Moerman [2009]. Skill reuse can be especially useful for transfer learning.", "startOffset": 81, "endOffset": 96}, {"referenceID": 6, "context": "Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set.", "startOffset": 35, "endOffset": 56}, {"referenceID": 6, "context": "Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set. In this environment, skill reuse can facilitate learning complex multi-agent policies (co-learning) with very few samples. Given a task, LSB can learn and combine skills, based on a set of rules, to solve the task. This structure of learned skills and combination rules forms a generative action grammar Summers-Stay et al. [2012] which paves the way for building advanced skill structures that are capable of solving complex tasks in different environments and conditions.", "startOffset": 35, "endOffset": 479}, {"referenceID": 6, "context": "Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set. In this environment, skill reuse can facilitate learning complex multi-agent policies (co-learning) with very few samples. Given a task, LSB can learn and combine skills, based on a set of rules, to solve the task. This structure of learned skills and combination rules forms a generative action grammar Summers-Stay et al. [2012] which paves the way for building advanced skill structures that are capable of solving complex tasks in different environments and conditions. One exciting extension of our work would be to incorporate skill interruption, similar to option interruption. Option interruption involves terminating an option based on an adaptive interruption rule Sutton et al. [1999]. Options are terminated when the value of continuing the current option is lower than the value of switching to a new option.", "startOffset": 35, "endOffset": 844}, {"referenceID": 6, "context": "Consider a multi-agent environment Garant et al. [2015] where many of the agents may be performing similar tasks which require a similar skill-set. In this environment, skill reuse can facilitate learning complex multi-agent policies (co-learning) with very few samples. Given a task, LSB can learn and combine skills, based on a set of rules, to solve the task. This structure of learned skills and combination rules forms a generative action grammar Summers-Stay et al. [2012] which paves the way for building advanced skill structures that are capable of solving complex tasks in different environments and conditions. One exciting extension of our work would be to incorporate skill interruption, similar to option interruption. Option interruption involves terminating an option based on an adaptive interruption rule Sutton et al. [1999]. Options are terminated when the value of continuing the current option is lower than the value of switching to a new option. This also implies that partition classes can overlap one another, as the option interruption rule ensures that the option with the best long term value is always being executed. Mankowitz et al. [2014] interlaced Sutton\u2019s interruption rule between iterations of value iteration and proved convergence to a global optimum.", "startOffset": 35, "endOffset": 1172}, {"referenceID": 6, "context": "Comanici and Precup [2010] have developed a policy gradient technique for learning the termination conditions of options.", "startOffset": 0, "endOffset": 27}, {"referenceID": 18, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009].", "startOffset": 97, "endOffset": 111}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain is discussed here.", "startOffset": 134, "endOffset": 161}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain is discussed here. The PW and Pinball domains are found in the main paper. The purpose of our experiments is to show that LSB can solve a complicated task with a simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations. Recall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC domain, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al.", "startOffset": 134, "endOffset": 886}, {"referenceID": 10, "context": "We performed experiments on three well-known RL benchmarks: Mountain Car (MC), Puddle World (PW) Sutton [1996] and the Pinball domain Konidaris and Barto [2009]. The MC domain is discussed here. The PW and Pinball domains are found in the main paper. The purpose of our experiments is to show that LSB can solve a complicated task with a simple policy representation by combining bootstrapped skills. These domains are simple enough that we can still solve them using richer representations. This allows us to compare LSB to a policy that is very close to optimal. Our experiments demonstrate potential to scale up to higher dimensional domains by combining skills over simple representations. Recall that LSB is a meta-algorithm. We must provide an algorithm for Policy Evaluation (PE) and skill learning. In our experiments, for the MC domain, we used SMDP-LSTD Sorg and Singh [2010] for PE and a modified version of Regular-Gradient Actor-Critic Bhatnagar et al. [2009] for skill learning.", "startOffset": 134, "endOffset": 973}, {"referenceID": 19, "context": "Although using different representations for approximating the value function and the policy strictly violates the policy gradient theorem Sutton et al. [2000], it still tends to work well in practice.", "startOffset": 139, "endOffset": 160}, {"referenceID": 10, "context": "There are two videos attached showing a demonstration of a policy learned by LSB for the Pinball domain Konidaris and Barto [2009]. Both of these domains are analyzed and discussed in the main paper.", "startOffset": 104, "endOffset": 131}], "year": 2015, "abstractText": "The monolithic approach to policy representation in Markov Decision Processes (MDPs) looks for a single policy that can be represented as a function from states to actions. For the monolithic approach to succeed (and this is not always possible), a complex feature representation is often necessary since the policy is a complex object that has to prescribe what actions to take all over the state space. This is especially true in large domains with complicated dynamics. It is also computationally inefficient to both learn and plan in MDPs using a complex monolithic approach. We present a different approach where we restrict the policy space to policies that can be represented as combinations of simpler, parameterized skills\u2014a type of temporally extended action, with a simple policy representation. We introduce Learning Skills via Bootstrapping (LSB) that can use a broad family of Reinforcement Learning (RL) algorithms as a \u201cblack box\u201d to iteratively learn parametrized skills. Initially, the learned skills are short-sighted but each iteration of the algorithm allows the skills to bootstrap off one another, improving each skill in the process. We prove that this bootstrapping process returns a near-optimal policy. Furthermore, our experiments demonstrate that LSB can solve MDPs that, given the same representational power, could not be solved by a monolithic approach. Thus, planning with learned skills results in better policies without requiring complex policy representations.", "creator": "LaTeX with hyperref package"}}}