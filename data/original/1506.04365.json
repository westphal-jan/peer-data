{"id": "1506.04365", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2015", "title": "Leveraging Word Embeddings for Spoken Document Summarization", "abstract": "Owing to the rapidly growing multimedia content available on the Internet, extractive spoken document summarization, with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document, has been an active area of research and experimentation. On the other hand, word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing (NLP)-related tasks. However, as far as we are aware, there are relatively few studies investigating its use in extractive text or speech summarization. A common thread of leveraging word embeddings in the summarization process is to represent the document (or sentence) by averaging the word embeddings of the words occurring in the document (or sentence). Then, intuitively, the cosine similarity measure can be employed to determine the relevance degree between a pair of representations. Beyond the continued efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech summarization. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.", "histories": [["v1", "Sun, 14 Jun 2015 09:18:36 GMT  (369kb)", "http://arxiv.org/abs/1506.04365v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["kuan-yu chen", "shih-hung liu", "hsin-min wang", "berlin chen", "hsin-hsi chen"], "accepted": false, "id": "1506.04365"}, "pdf": {"name": "1506.04365.pdf", "metadata": {"source": "CRF", "title": "Leveraging Word Embeddings for Spoken Document Summarization", "authors": ["Kuan-Yu Chen", "Shih-Hung Liu", "Hsin-Min Wang", "Berlin Chen", "Hsin-Hsi Chen"], "emails": ["whm}@iis.sinica.edu.tw,", "berlin@ntnu.edu.tw,", "hhchen@csie.ntu.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "Owing to the popularity of various Internet applications, rapidly growing multimedia content, such as music video, broadcast news programs, and lecture recordings, has been continuously filling our daily life [1-3]. Obviously, speech is one of the most important sources of information about multimedia. By virtue of spoken document summarization (SDS), one can efficiently digest multimedia content by listening to the associated speech summary. Extractive SDS manages to select a set of indicative sentences from a spoken document according to a target summarization ratio and concatenate them together to form a summary [4-7]. The wide spectrum of extractive SDS methods developed so far may be divided into three categories [4, 7]: 1) methods simply based on the sentence position or structure information, 2) methods based on unsupervised sentence ranking, and 3) methods based on supervised sentence classification.\nFor the first category, the important sentences are selected from some salient parts of a spoken document [8], such as the introductory and/or concluding parts. However, such methods can be only applied to some specific domains with limited document structures. Unsupervised sentence ranking methods attempt to select important sentences based on the statistical features of the sentences or of the words in the sentences\nwithout human annotations involved. Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15]. Statistical features may include the term (word) frequency, linguistic score, recognition confidence measure, and prosodic information. In contrast, supervised sentence classification methods, such as the Gaussian mixture model (GMM) [9], the Bayesian classifier (BC) [16], the support vector machine (SVM) [17], and the conditional random fields (CRFs) [18], usually formulate sentence selection as a binary classification problem, i.e., a sentence can either be included in a summary or not. Interested readers may refer to [4-7] for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of text and speech summarization tasks.\nDifferent from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26]. The central idea of these methods is to learn continuously distributed vector representations of words using neural networks, which can probe latent semantic and/or syntactic cues that can in turn be used to induce similarity measures among words, sentences, and documents. A common thread of leveraging word embedding methods to NLP-related tasks is to represent the document (or query and sentence) by averaging the word embeddings corresponding to the words occurring in the document (or query and sentence). Then, intuitively, the cosine similarity measure can be applied to determine the relevance degree between a pair of representations. However, such a framework ignores the inter-dimensional correlation between the two vector representations. To mitigate this deficiency, we further propose a novel use of the triplet learning model to enhance the estimation of the similarity degree between a pair of representations. In addition, since most word embedding methods are founded on a probabilistic objective function, a probabilistic similarity measure might be a more natural choice than non-probabilistic ones. Consequently, we also propose a new language model-based framework, which incorporates the word embedding methods with the document likelihood measure. To recapitulate, beyond the continued and tremendous efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models on top of the general word embedding methods for extractive SDS."}, {"heading": "2. Review of Word Embedding Methods", "text": "Perhaps one of the most-known seminal studies on developing word embedding methods was presented in [19]. It estimated a statistical (N-gram) language model, formalized as a feedforward neural network, for predicting future words in context while inducing word embeddings (or representations) as a byproduct. Such an attempt has already motivated many followup extensions to develop similar methods for probing latent semantic and syntactic regularities in the representation of words. Representative methods include, but are not limited to, the continuous bag-of-words (CBOW) model [21], the skipgram (SG) model [21, 27], and the global vector (GloVe) model [22]. As far as we are aware, there is little work done to contextualize these methods for use in speech summarization."}, {"heading": "2.1. The Continuous Bag-of-Words (CBOW) Model", "text": "Rather than seeking to learn a statistical language model, the CBOW model manages to obtain a dense vector representation (embedding) of each word directly [21]. The structure of CBOW is similar to a feed-forward neural network, with the exception that the non-linear hidden layer in the former is removed. By getting around the heavy computational burden incurred by the non-linear hidden layer, the model can be trained on a large corpus efficiently, while still retains good performance. Formally, given a sequence of words, w1,w2,\u2026,wT, the objective function of CBOW is to maximize the log-probability,\n,),...,,,...,|(log1 11\u2211 = ++\u2212\u2212T t ctttctt wwwwwP (1)\nwhere c is the window size of context words being considered for the central word wt, T denotes the length of the training corpus, and\n, )exp(\n)exp( ),...,,,...,|(\n1 11 \u2211 = ++\u2212\u2212 \u22c5\n\u22c5 = V\ni ww\nwwctttctt\nit\ntt wwwwwP\nvv\nvv\n(2)\nwhere \ud835\udc2f\ud835\udc2f\ud835\udc64\ud835\udc64\ud835\udc61\ud835\udc61 denotes the vector representation of the word w at position t; V is the size of the vocabulary; and \ud835\udc2f\ud835\udc2f\ud835\udc64\ud835\udc64\ufffd\ud835\udc61\ud835\udc61 denotes the (weighted) average of the vector representations of the context words of wt [21, 26]. The concept of CBOW is motivated by the distributional hypothesis [27], which states that words with similar meanings often occur in similar contexts, and it is thus suggested to look for wt whose word representation can capture its context distributions well."}, {"heading": "2.2. The Skip-gram (SG) Model", "text": "In contrast to the CBOW model, the SG model employs an inverse training objective for learning word representations with a simplified feed-forward neural network [21, 28]. Given the word sequence, w1,w2,\u2026,wT, the objective function of SG is to maximize the following log-probability,\n,)|(log1 0,\u2211 = \u2211 \u2260\u2212= +T t c jcj tjt wwP (3)\nwhere c is the window size of the context words for the central word wt, and the conditional probability is computed by\n, )exp(\n)exp( )|(\n1\u2211 =\n+ \u22c5\n\u22c5 = +\nV i ww\nwwtjt\nti\ntjt wwP\nvv\nvv (4)\nwhere \ud835\udc2f\ud835\udc2f\ud835\udc64\ud835\udc64\ud835\udc61\ud835\udc61+\ud835\udc57\ud835\udc57 and \ud835\udc2f\ud835\udc2f\ud835\udc64\ud835\udc64\ud835\udc61\ud835\udc61 denote the word representations of the words at positions t+j and t, respectively. In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the negative sampling algorithm [28, 30] can make the training process more efficient and effective."}, {"heading": "2.3. The Global Vector (GloVe) Model", "text": "The GloVe model suggests that an appropriate starting point for word representation learning should be associated with the ratios of co-occurrence probabilities rather than the prediction probabilities [22]. More precisely, GloVe makes use of weighted least squares regression, which aims at learning word representations by preserving the co-occurrence frequencies between each pair of words:\n,)log)((1 1 2\u2211 = \u2211 = \u2212++\u22c5 V i V j wwwwwwww jijijiji XbbXf vv (5)\nwhere \ud835\udc4b\ud835\udc4b\ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56\ud835\udc64\ud835\udc64\ud835\udc57\ud835\udc57 denotes the number of times words wi and wj cooccur in a pre-defined sliding context window; f( \u00b7 ) is a monotonic smoothing function used to modulate the impact of each pair of words involved in the model training; and vw and bw denote the word representation and the bias term of word w, respectively."}, {"heading": "2.4. Analytic Comparisons", "text": "There are several analytic comparisons can be made among the above three word embedding methods. First, they have different model structures and learning strategies. CBOW and SG adopt an on-line learning strategy, i.e., the parameters (word representations) are trained sequentially. Therefore, the order that the training samples are used may change the resulting models dramatically. In contrast, GloVe uses a batch learning strategy, i.e., it accumulates the statistics over the entire training corpus and updates the model parameters at once. Second, it is worthy to note that SG (trained with the negative sampling algorithm) and GloVe have an implicit/explicit relation with the classic weighted matrix factorization approach, while the major difference is that SG and GloVe concentrate on rendering the word-by-word cooccurrence matrix but weighted matrix factorization is usually concerned with decomposing the word-by-document matrix [9, 31, 32].\nThe observations made above on the relation between word embedding methods and matrix factorization bring us to the notion of leveraging the singular value decomposition (SVD) method as an alternative mechanism to derive the word embeddings in this paper. Given a training text corpus, we have a word-by-word co-occurrence matrix A. Each element Aij of A is the log-frequency of times words wi and wj co-occur in a pre-defined sliding context window in the corpus. Subsequently, SVD decomposes A into three sub-matrices:\n,~T AVUA =\u2211\u2248 (6)\nwhere U and V are orthogonal matrices, and \u03a3 is a diagonal matrix. Finally, each row vector of matrix U (or the column vector of matrix VT, U=V since A is a symmetric matrix) designates the word embedding of a specific word in the vocabulary. It is worthy to note that using SVD to derive the word representations is similar in spirit to latent semantic analysis (LSA) but using the word-word co-occurrence matrix instead of the word-by-document co-occurrence matrix [33]."}, {"heading": "3. Sentence Ranking based on Word Embeddings", "text": ""}, {"heading": "3.1. The Triplet Learning Model", "text": "Inspired by the vector space model (VSM), a straightforward way to leverage the word embedding methods for extractive SDS is to represent a sentence Si (and a document D to be summarized) by averaging the vector representations of words occurring in the sentence Si (and the document D) [23, 25]:\n. ||\n),( \u2211 \u2208= ii Sw w\ni\ni S S\nSwn vv (7)\nBy doing so, the document D and each sentence Si of D will have a respective fixed-length dense vector representation, and their relevance degree can be evaluated by the cosine similarity measure.\nHowever, such an approach ignores the inter-dimensional correlation between two vector representations. To mitigate the deficiency of the cosine similarity measure, we employ a triplet learning model to enhance the estimation of the similarity degree between a pair of representations [34-36]. Without loss of generality, our goal is to learn a similarity function, R( \u00b7 , \u00b7 ), which assigns higher similarity scores to summary sentences than to non-summary sentences, i.e.,\n),,(),( \u2212+ > ji SDSD RR vvvv\n(8)\nwhere \ud835\udc2f\ud835\udc2f\ud835\udc60\ud835\udc60\ud835\udc56\ud835\udc56 + denotes the sentence representation (in the form of a column vector) for a summary sentence Si, while \ud835\udc2f\ud835\udc2f\ud835\udc60\ud835\udc60\ud835\udc57\ud835\udc57 \u2212 is the representation for a non-summary sentence Sj. The parametric ranking function has a bi-linear form as follows:\nSDSDR Wvvvv T),( \u2261 (9)\nwhere \ud835\udc16\ud835\udc16 \u2208 \u211d\ud835\udc51\ud835\udc51\u00d7\ud835\udc51\ud835\udc51 , and d is the dimension of the vector representation. By applying the passive-aggressive learning algorithm presented in [34], we can derive the similarity function R such that all triplets obey\n.),(),( \u03b4+> \u2212+ ji SDSD RR vvvv\n(10)\nThat is, not only the similarity function will distinguish summary and non-summary sentences, but also there is a safety margin of \ud835\udeff\ud835\udeff between them. With \ud835\udeff\ud835\udeff , a hinge loss function can be defined as\n)}.,(),(,0max{),,( \u2212+\u2212+ +\u2212= jiji SDSDSSD RRloss vvvvvvv \u03b4 (11)\nThen, W can be obtained by applying an efficient sequential learning algorithm iteratively over the triplets [34, 35]. With W, sentences can be ranked in descending order of similarity measure, and the top sentences will be selected and sequenced to form a summary according to a target summarization ratio."}, {"heading": "3.2. The Document Likelihood Measure", "text": "A recent school of thought for extractive SDS is to employ a language modeling (LM) approach for the selection of important sentences. A principal realization is to use a probabilistic generative paradigm for ranking each sentence S of a document D, which can be expressed by P(D|S). The simplest way is to estimate a unigram language model (ULM) based on the frequency of each distinct word w occurring in S, with the maximum likelihood (ML) criterion [37, 38]:\n, || ),()|( S SwnSwP = (12)\nwhere n(w,S) is the number of times that word w occurs in S and |S| is the length of S. Obviously, one major challenge facing the LM approach is how to accurately estimate the model parameters for each sentence.\nStimulated by the document likelihood measure adopted by the ULM method, for the various word representation methods studied in this paper, we can construct a new word-based language model for predicting the occurrence probability of any arbitrary word wj. Taking CBOW as an example, the\nprobability of word wj given another word wi can be calculated by\n. )exp(\n)exp( )|(\n\u2211 \u2208 \u22c5\n\u22c5 =\nVw ww\nww ij\nl il\nijwwP vv\nvv (13)\nAs such, we can linearly combine the associated word-based language models of the words occurring in sentence S to form a composite sentence-specific language model for S, and express the document likelihood measure as\n,)|()|( ),(\n\u220f \u2208 \u2211 \u2208   \n\n  \n \u22c5= Dw\nDwn\nSw ijwj\nj\ni i\nwwPSDP \u03b1 (14)\nwhere the weighting factor \ud835\udefc\ud835\udefc\ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56 is set to be proportional to the frequency of word wi occurring in sentence S, subject to \u2211 \ud835\udefc\ud835\udefc\ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56\ud835\udc64\ud835\udc64\ud835\udc56\ud835\udc56\u2208\ud835\udc46\ud835\udc46 = 1 . The sentences offering higher document likelihoods will be selected and sequenced to form the summary according to a target summarization ratio."}, {"heading": "4. Experimental Setup", "text": "The dataset used in this study is the MATBN broadcast news corpus collected by the Academia Sinica and the Public Television Service Foundation of Taiwan between November 2001 and April 2003 [39]. The corpus has been segmented into separate stories and transcribed manually. Each story contains the speech of one studio anchor, as well as several field reporters and interviewees. A subset of 205 broadcast news documents compiled between November 2001 and August 2002 was reserved for the summarization experiments. We chose 20 documents as the test set while the remaining 185 documents as the held-out development set. The reference summaries were generated by ranking the sentences in the manual transcript of a spoken document by importance without assigning a score to each sentence. Each document has three reference summaries annotated by three subjects. For the assessment of summarization performance, we adopted the widely-used ROUGE metrics [40]. All the experimental results reported hereafter are obtained by calculating the Fscores [17] of these ROUGE metrics. The summarization ratio was set to 10%. A corpus of 14,000 text news documents, compiled during the same period as the broadcast news documents, was used to estimate related models compared in this paper, which are CBOW, SG, GloVe, and SVD. A subset of 25-hour speech data from MATBN compiled from November 2001 to December 2002 was used to bootstrap the acoustic training with the minimum phone error rate (MPE) criterion and a training data selection scheme [41]. The vocabulary size is about 72 thousand words. The average word error rate of automatic transcription is about 40%."}, {"heading": "5. Experimental Results", "text": "At the outset, we assess the performance levels of several well-practiced or/and state-of-the-art summarization methods for extractive SDS, which will serve as the baseline systems in this paper, including the LM-based summarization method (i.e., ULM, c.f. Eq. (12)), the vector-space methods (i.e., VSM, LSA, and MMR), the graph-based methods (i.e., MRW and LexRank), the submodularity method (SM), and the integer linear programming (ILP) method. The results are illustrated in Table 1, where TD denotes the results obtained based on the manual transcripts of spoken documents and SD denotes the results using the speech recognition transcripts that may contain recognition errors. Several noteworthy observations can be drawn from Table 1. First, the two graph-based methods (i.e., MRW and LexRank) are quite competitive with each other and perform better than the vector-space methods\n(i.e., VSM, LSA, and MMR) for the TD case. However, for the SD case, the situation is reversed. It reveals that imperfect speech recognition may affect the graph-based methods more than the vector-space methods; a possible reason for such a phenomenon is that the speech recognition errors may lead to inaccurate similarity measures between each pair of sentences. The PageRank-like procedure of the graph-based methods, in turn, will be performed based on these problematic measures, potentially leading to degraded results. Second, LSA, which represents the sentences of a spoken document and the document itself in the latent semantic space instead of the index term (word) space, performs slightly better than VSM in both the TD and SD cases. Third, SM and ILP achieve the best results in the TD case, but only have comparable performance to other methods in the SD case. Finally, ULM shows competitive results compared to other state-of-the-art methods, confirming the applicability of the language modeling approach for speech summarization.\nWe now turn to investigate the utilities of three state-of-theart word embedding methods (i.e., CBOW, SG, and GloVe) and the proposed SVD method (c.f. Section 2.4), working in conjunction with the cosine similarity measure for speech summarization. The results are shown in Table 2. From the results, several observations can be made. First, the three stateof-the-art word embedding methods (i.e., CBOW, SG, and GloVe), though with disparate model structures and learning strategies, achieve comparable results to each other in both the TD and SD cases. Although these methods outperform the conventional VSM model, they only achieve almost the same level of performance as LSA and MMR, two improved versions of VSM, and perform worse than MRW, LexRank, SM, and ILP in the TD case. To our surprise, the proposed SVD method outperforms other word embedding methods by a substantial margin in the TD case and slightly in the SD case. It should be noted that the SVD method outperforms not only CBOW, SG, and GloVe, but also LSA and MMR. It even outperforms all the methods compared in Table 1 in the SD case.\nIn the next set of experiments, we evaluate the capability of the triplet learning model for improving the measurement of similarity when applying word embedding methods in speech summarization. The results are shown in Table 3. From the table, two observations can be drawn. First, it is clear that the triplet learning model outperforms the baseline cosine similarity measure (c.f. Table 2) in all cases. This indicates that triplet learning is able to improve the measurement of the similarity degree for sentence ranking and considering the inter-dimensional correlation in the similarity measure between two vector representations is indeed beneficial. Second, \u201cCBOW with triplet learning\u201d outperforms all the methods compared in Table 1 in both the TD and SD cases. However, we have to note that learning W in Eq. (9) has to resort to a set of documents with reference summaries; thus the comparison is unfair since all the methods in Table 1 are unsupervised ones. We used the development set (c.f. Section 4) to learn W. So far, we have not figured out systematic and\neffective ways to incorporate word embeddings into existing supervised speech summarization methods. We leave this as our future work.\nIn the last set of experiments, we pair the word embedding methods with the document likelihood measure for extractive SDS. The deduced sentence-based language models were linearly combined with ULM in computing the document likelihood using Eq. (14) [40]. The results are shown in Table 4. Comparing the results to that of the word embedding methods paired with the cosine similarity measure (c.f. Table 2), it is evident that the document likelihood measure works pretty well as a vehicle to leverage word embedding methods for speech summarization. We also notice that CBOW outperforms the other three word embedding methods in both the TD and SD cases, just as it had done previously in Table 3 when combined with triplet learning, whereas \u201cSVD with document likelihood measure\u201d does not preserve the superiority as \u201cSVD with triplet learning\u201d (c.f. Table 3). Moreover, comparing the results with that of various state-ofthe-art methods (c.f. Table 1), the word embedding methods with the document likelihood measure are quite competitive in most cases."}, {"heading": "6. Conclusions & Future Work", "text": "In this paper, both the triplet learning model and the document likelihood measure have been proposed to leverage the word embeddings learned by various word embedding methods for speech summarization. In addition, a new SVD-based word embedding method has also been proposed and proven efficient and as effective as existing word embedding methods. Experimental evidence supports that the proposed summarization methods are comparable to several state-of-theart methods, thereby indicating the potential of the new word embedding-based speech summarization framework. For future work, we will explore other effective ways to enrich the representations of words and integrate extra cues, such as speaker identities or prosodic (emotional) information, into the proposed framework. We are also interested in investigating more robust indexing techniques to represent spoken documents in an elegant way."}, {"heading": "SM 0.414 0.286 0.363 0.332 0.204 0.303", "text": ""}, {"heading": "7. References", "text": "[1] S. Furui et al., \u201cFundamental technologies in modern speech\nrecognition,\u201d IEEE Signal Processing Magazine, 29(6), pp. 16\u2013 17, 2012.\n[2] M. Ostendorf, \u201cSpeech technology and information access,\u201d IEEE Signal Processing Magazine, 25(3), pp. 150\u2013152, 2008.\n[3] L. S. Lee and B. Chen, \u201cSpoken document understanding and organization,\u201d IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 42\u201360, 2005.\n[4] Y. Liu and D. Hakkani-Tur, \u201cSpeech summarization,\u201d Chapter 13 in Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, G. Tur and R. D. Mori (Eds), New York: Wiley, 2011.\n[5] G. Penn and X. Zhu, \u201cA critical reassessment of evaluation baselines for speech summarization,\u201d in Proc. of ACL, pp. 470\u2013 478, 2008.\n[6] A. Nenkova and K. McKeown, \u201cAutomatic summarization,\u201d Foundations and Trends in Information Retrieval, vol. 5, no. 2\u20133, pp. 103\u2013233, 2011.\n[7] I. Mani and M. T. Maybury (Eds.), Advances in automatic text summarization, Cambridge, MA: MIT Press, 1999.\n[8] P. B. Baxendale, \u201cMachine-made index for technical literaturean experiment,\u201d IBM Journal, October, 1958.\n[9] Y. Gong and X. Liu, \u201cGeneric text summarization using relevance measure and latent semantic analysis,\u201d in Proc. of SIGIR, pp. 19\u201325, 2001.\n[10] X. Wan and J. Yang, \u201cMulti-document summarization using cluster-based link analysis,\u201d in Proc. of SIGIR, pp. 299\u2013306, 2008.\n[11] J. Carbonell and J. Goldstein, \u201cThe use of MMR, diversity based reranking for reordering documents and producing summaries,\u201d in Proc. of SIGIR, pp. 335\u2013336, 1998.\n[12] S. Furui et al., \u201cSpeech-to-text and speech-to-speech summarization of spontaneous speech\u201d, IEEE Transactions on Speech and Audio Processing, vol. 12, no. 4, pp. 401\u2013408, 2004.\n[13] G. Erkan and D. R. Radev, \u201cLexRank: Graph-based lexical centrality as salience in text summarization\u201d, Journal of Artificial Intelligent Research, vol. 22, no. 1, pp. 457\u2013479, 2004.\n[14] H. Lin and J. Bilmes, \u201cMulti-document summarization via budgeted maximization of submodular functions,\u201d in Proc. of NAACL HLT, pp. 912\u2013920, 2010.\n[15] K. Riedhammer et al., \u201cLong story short - Global unsupervised models for keyphrase based meeting summarization,\u201d Speech Communication, vol. 52, no. 10, pp. 801\u2013815, 2010.\n[16] J. Kupiec et al., \u201cA trainable document summarizer,\u201d in Proc. of SIGIR, pp. 68\u201373, 1995.\n[17] J. Zhang and P. Fung, \u201cSpeech summarization without lexical features for Mandarin broadcast news\u201d, in Proc. of NAACL HLT, Companion Volume, pp. 213\u2013216, 2007.\n[18] M. Galley, \u201cSkip-chain conditional random field for ranking meeting utterances by importance,\u201d in Proc. of EMNLP, pp. 364\u2013372, 2006.\n[19] Y. Bengio et al., \u201cA neural probabilistic language model,\u201d Journal of Machine Learning Research (3), pp. 1137\u20131155, 2003.\n[20] A. Mnih and G. Hinton, \u201cThree new graphical models for statistical language modeling,\u201d in Proc. of ICML, pp. 641\u2013648, 2007.\n[21] T. Mikolov et al., \u201cEfficient estimation of word representations in vector space,\u201d in Proc. of ICLR, pp. 1\u201312, 2013.\n[22] J. Pennington et al., \u201cGloVe: Global vector for word representation,\u201d in Proc. of EMNLP, pp. 1532\u20131543, 2014.\n[23] D. Tang et al., \u201cLearning sentiment-specific word embedding for twitter sentiment classification\u201d in Proc. of ACL, pp. 1555\u20131565, 2014.\n[24] R. Collobert and J. Weston, \u201cA unified architecture for natural language processing: deep neural networks with multitask learning,\u201d in Proc. of ICML, pp. 160\u2013167, 2008\n[25] M. Kageback et al., \u201cExtractive summarization using continuous vector space models,\u201d in Proc. of CVSC, pp. 31\u201339, 2014.\n[26] L. Qiu et al., \u201cLearning word representation considering proximity and ambiguity,\u201d in Proc. of AAAI, pp. 1572\u20131578, 2014.\n[27] G. Miller and W. Charles, \u201cContextual correlates of semantic similarity,\u201d Language and Cognitive Processes, 6(1), pp. 1\u201328, 1991.\n[28] T. Mikolov et al., \u201cDistributed representations of words and phrases and their compositionality,\u201d in Proc. of ICLR, pp. 1\u20139, 2013.\n[29] F. Morin and Y. Bengio, \u201cHierarchical probabilistic neural network language model,\u201d in Proc. of AISTATS, pp. 246\u2013252, 2005.\n[30] A. Mnih and K. Kavukcuoglu, \u201cLearning word embeddings efficiently with noise-contrastive estimation,\u201d in Proc. of NIPS, pp. 2265\u20132273, 2013.\n[31] O. Levy and Y. Goldberg, \u201cNeural word embedding as implicit matrix factorization,\u201d in Proc. of NIPS, pp. 2177\u20132185, 2014.\n[32] K. Y. Chen et al., \u201cWeighted matrix factorization for spoken document retrieval,\u201d in Proc. of ICASSP, pp. 8530\u20138534, 2013.\n[33] M. Afify et al., \u201cGaussian mixture language models for speech recognition,\u201d in Proc. of ICASSP, pp. IV-29\u2013IV-32, 2007.\n[34] K. Crammer et al., \u201cOnline passive-aggressive algorithms,\u201d Journal of Machine Learning Research (7), pp. 551\u2013585, 2006.\n[35] G. Chechik et al., \u201cLarge scale online learning of image similarity through ranking,\u201d Journal of Machine Learning Research (11), pp. 1109\u20131135, 2010.\n[36] M. Norouzi et al., \u201cHamming distance metric learning,\u201d in Proc. of NIPS, pp. 1070\u20131078, 2012.\n[37] Y. T. Chen et al., \u201cA probabilistic generative framework for extractive broadcast news speech summarization, \u201d IEEE Transactions on Audio, Speech and Language Processing, vol. 17, no. 1, pp. 95\u2013106, 2009.\n[38] C. Zhai and J. Lafferty, \u201cA study of smoothing methods for language models applied to ad hoc information retrieval,\u201d in Proc. of SIGIR, pp. 334\u2013342, 2001.\n[39] H. M. Wang et al., \u201cMATBN: A Mandarin Chinese broadcast news corpus,\u201d International Journal of Computational Linguistics and Chinese Language Processing, vol. 10, no. 2, pp. 219\u2013236, 2005.\n[40] C. Y. Lin, \u201cROUGE: Recall-oriented understudy for gisting evaluation.\u201d 2003 [Online]. Available: http://haydn.isi.edu/ROUGE/.\n[41] G. Heigold et al., \u201cDiscriminative training for automatic speech recognition: Modeling, criteria, optimization, implementation, and performance,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 58\u201369, 2012."}], "references": [{"title": "Fundamental technologies in modern speech recognition", "author": ["S. Furui"], "venue": "IEEE Signal Processing Magazine, 29(6), pp. 16\u2013 17, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech technology and information access", "author": ["M. Ostendorf"], "venue": "IEEE Signal Processing Magazine, 25(3), pp. 150\u2013152, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Spoken document understanding and organization", "author": ["L.S. Lee", "B. Chen"], "venue": "IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 42\u201360, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Speech summarization", "author": ["Y. Liu", "D. Hakkani-Tur"], "venue": "Chapter 13 in Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, G. Tur and R. D. Mori (Eds), New York: Wiley, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A critical reassessment of evaluation baselines for speech summarization", "author": ["G. Penn", "X. Zhu"], "venue": "Proc. of ACL, pp. 470\u2013 478, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic summarization", "author": ["A. Nenkova", "K. McKeown"], "venue": "Foundations and Trends in Information Retrieval, vol. 5, no. 2\u20133, pp. 103\u2013233, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Machine-made index for technical literaturean experiment", "author": ["P.B. Baxendale"], "venue": "IBM Journal, October, 1958.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1958}, {"title": "Generic text summarization using relevance measure and latent semantic analysis", "author": ["Y. Gong", "X. Liu"], "venue": "Proc. of SIGIR, pp. 19\u201325, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-document summarization using cluster-based link analysis", "author": ["X. Wan", "J. Yang"], "venue": "Proc. of SIGIR, pp. 299\u2013306, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "The use of MMR, diversity based reranking for reordering documents and producing summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "Proc. of SIGIR, pp. 335\u2013336, 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Speech-to-text and speech-to-speech summarization of spontaneous speech", "author": ["S. Furui"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "LexRank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligent Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["H. Lin", "J. Bilmes"], "venue": "Proc. of NAACL HLT, pp. 912\u2013920, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Long story short - Global unsupervised models for keyphrase based meeting summarization", "author": ["K. Riedhammer"], "venue": "Speech Communication, vol. 52, no. 10, pp. 801\u2013815, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "A trainable document summarizer", "author": ["J. Kupiec"], "venue": "Proc. of SIGIR, pp. 68\u201373, 1995.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Speech summarization without lexical features for Mandarin broadcast news", "author": ["J. Zhang", "P. Fung"], "venue": "in Proc. of NAACL HLT, Companion Volume,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Skip-chain conditional random field for ranking meeting utterances by importance", "author": ["M. Galley"], "venue": "Proc. of EMNLP, pp. 364\u2013372, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio"], "venue": "Journal of Machine Learning Research (3), pp. 1137\u20131155, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Three new graphical models for statistical language modeling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proc. of ICML, pp. 641\u2013648, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov"], "venue": "Proc. of ICLR, pp. 1\u201312, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "GloVe: Global vector for word representation", "author": ["J. Pennington"], "venue": "Proc. of EMNLP, pp. 1532\u20131543, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["D. Tang"], "venue": "in Proc. of ACL, pp", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proc. of ICML, pp. 160\u2013167, 2008", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Extractive summarization using continuous vector space models", "author": ["M. Kageback"], "venue": "Proc. of CVSC, pp. 31\u201339, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["L. Qiu"], "venue": "Proc. of AAAI, pp. 1572\u20131578, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual correlates of semantic similarity", "author": ["G. Miller", "W. Charles"], "venue": "Language and Cognitive Processes, 6(1), pp. 1\u201328, 1991.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1991}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov"], "venue": "Proc. of ICLR, pp. 1\u20139, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "Proc. of AISTATS, pp. 246\u2013252, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Proc. of NIPS, pp. 2265\u20132273, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Proc. of NIPS, pp. 2177\u20132185, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Weighted matrix factorization for spoken document retrieval", "author": ["K.Y. Chen"], "venue": "Proc. of ICASSP, pp. 8530\u20138534, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian mixture language models for speech recognition", "author": ["M. Afify"], "venue": "Proc. of ICASSP, pp. IV-29\u2013IV-32, 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Online passive-aggressive algorithms", "author": ["K. Crammer"], "venue": "Journal of Machine Learning Research (7), pp. 551\u2013585, 2006.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Large scale online learning of image similarity through ranking", "author": ["G. Chechik"], "venue": "Journal of Machine Learning Research (11), pp. 1109\u20131135, 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Hamming distance metric learning", "author": ["M. Norouzi"], "venue": "Proc. of NIPS, pp. 1070\u20131078, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "A probabilistic generative framework for extractive broadcast news speech summarization,  ", "author": ["Y.T. Chen"], "venue": "IEEE Transactions on Audio, Speech and Language Processing,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "Proc. of SIGIR, pp. 334\u2013342, 2001.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2001}, {"title": "MATBN: A Mandarin Chinese broadcast news corpus", "author": ["H.M. Wang"], "venue": "International Journal of Computational Linguistics and Chinese Language Processing, vol. 10, no. 2, pp. 219\u2013236, 2005.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "ROUGE: Recall-oriented understudy for gisting evaluation.", "author": ["C.Y. Lin"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Discriminative training for automatic speech recognition: Modeling, criteria, optimization, implementation, and performance", "author": ["G. Heigold"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 58\u201369, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Owing to the popularity of various Internet applications, rapidly growing multimedia content, such as music video, broadcast news programs, and lecture recordings, has been continuously filling our daily life [1-3].", "startOffset": 209, "endOffset": 214}, {"referenceID": 1, "context": "Owing to the popularity of various Internet applications, rapidly growing multimedia content, such as music video, broadcast news programs, and lecture recordings, has been continuously filling our daily life [1-3].", "startOffset": 209, "endOffset": 214}, {"referenceID": 2, "context": "Owing to the popularity of various Internet applications, rapidly growing multimedia content, such as music video, broadcast news programs, and lecture recordings, has been continuously filling our daily life [1-3].", "startOffset": 209, "endOffset": 214}, {"referenceID": 3, "context": "Extractive SDS manages to select a set of indicative sentences from a spoken document according to a target summarization ratio and concatenate them together to form a summary [4-7].", "startOffset": 176, "endOffset": 181}, {"referenceID": 4, "context": "Extractive SDS manages to select a set of indicative sentences from a spoken document according to a target summarization ratio and concatenate them together to form a summary [4-7].", "startOffset": 176, "endOffset": 181}, {"referenceID": 5, "context": "Extractive SDS manages to select a set of indicative sentences from a spoken document according to a target summarization ratio and concatenate them together to form a summary [4-7].", "startOffset": 176, "endOffset": 181}, {"referenceID": 3, "context": "The wide spectrum of extractive SDS methods developed so far may be divided into three categories [4, 7]: 1) methods simply based on the sentence position or structure information, 2) methods based on unsupervised sentence ranking, and 3) methods based on supervised sentence classification.", "startOffset": 98, "endOffset": 104}, {"referenceID": 6, "context": "For the first category, the important sentences are selected from some salient parts of a spoken document [8], such as the introductory and/or concluding parts.", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 235, "endOffset": 239}, {"referenceID": 3, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 287, "endOffset": 290}, {"referenceID": 11, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 311, "endOffset": 315}, {"referenceID": 12, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 348, "endOffset": 352}, {"referenceID": 13, "context": "Popular methods include the vector space model (VSM) [9], the latent semantic analysis (LSA) method [9], the Markov random walk (MRW) method [10], the maximum marginal relevance (MMR) method [11], the sentence significant score method [12], the unigram language model-based (ULM) method [4], the LexRank method [13], the submodularity-based method [14], and the integer linear programming (ILP) method [15].", "startOffset": 402, "endOffset": 406}, {"referenceID": 7, "context": "In contrast, supervised sentence classification methods, such as the Gaussian mixture model (GMM) [9], the Bayesian classifier (BC) [16], the support vector machine (SVM) [17], and the conditional random fields (CRFs) [18], usually formulate sentence selection as a binary classification problem, i.", "startOffset": 98, "endOffset": 101}, {"referenceID": 14, "context": "In contrast, supervised sentence classification methods, such as the Gaussian mixture model (GMM) [9], the Bayesian classifier (BC) [16], the support vector machine (SVM) [17], and the conditional random fields (CRFs) [18], usually formulate sentence selection as a binary classification problem, i.", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "In contrast, supervised sentence classification methods, such as the Gaussian mixture model (GMM) [9], the Bayesian classifier (BC) [16], the support vector machine (SVM) [17], and the conditional random fields (CRFs) [18], usually formulate sentence selection as a binary classification problem, i.", "startOffset": 171, "endOffset": 175}, {"referenceID": 16, "context": "In contrast, supervised sentence classification methods, such as the Gaussian mixture model (GMM) [9], the Bayesian classifier (BC) [16], the support vector machine (SVM) [17], and the conditional random fields (CRFs) [18], usually formulate sentence selection as a binary classification problem, i.", "startOffset": 218, "endOffset": 222}, {"referenceID": 3, "context": "Interested readers may refer to [4-7] for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of text and speech summarization tasks.", "startOffset": 32, "endOffset": 37}, {"referenceID": 4, "context": "Interested readers may refer to [4-7] for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of text and speech summarization tasks.", "startOffset": 32, "endOffset": 37}, {"referenceID": 5, "context": "Interested readers may refer to [4-7] for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of text and speech summarization tasks.", "startOffset": 32, "endOffset": 37}, {"referenceID": 17, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 18, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 19, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 20, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 90, "endOffset": 97}, {"referenceID": 21, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 320, "endOffset": 327}, {"referenceID": 22, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 320, "endOffset": 327}, {"referenceID": 23, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 320, "endOffset": 327}, {"referenceID": 24, "context": "Different from the above methods, we explore in this paper various word embedding methods [19-22] for use in extractive SDS, which have recently demonstrated excellent performance in many natural language processing (NLP)-related tasks, such as relational analogy prediction, sentiment analysis, and sentence completion [23-26].", "startOffset": 320, "endOffset": 327}, {"referenceID": 17, "context": "Perhaps one of the most-known seminal studies on developing word embedding methods was presented in [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Representative methods include, but are not limited to, the continuous bag-of-words (CBOW) model [21], the skipgram (SG) model [21, 27], and the global vector (GloVe) model [22].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "Representative methods include, but are not limited to, the continuous bag-of-words (CBOW) model [21], the skipgram (SG) model [21, 27], and the global vector (GloVe) model [22].", "startOffset": 127, "endOffset": 135}, {"referenceID": 25, "context": "Representative methods include, but are not limited to, the continuous bag-of-words (CBOW) model [21], the skipgram (SG) model [21, 27], and the global vector (GloVe) model [22].", "startOffset": 127, "endOffset": 135}, {"referenceID": 20, "context": "Representative methods include, but are not limited to, the continuous bag-of-words (CBOW) model [21], the skipgram (SG) model [21, 27], and the global vector (GloVe) model [22].", "startOffset": 173, "endOffset": 177}, {"referenceID": 19, "context": "Rather than seeking to learn a statistical language model, the CBOW model manages to obtain a dense vector representation (embedding) of each word directly [21].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "where vvwwtt denotes the vector representation of the word w at position t; V is the size of the vocabulary; and vvww\ufffdtt denotes the (weighted) average of the vector representations of the context words of wt [21, 26].", "startOffset": 209, "endOffset": 217}, {"referenceID": 24, "context": "where vvwwtt denotes the vector representation of the word w at position t; V is the size of the vocabulary; and vvww\ufffdtt denotes the (weighted) average of the vector representations of the context words of wt [21, 26].", "startOffset": 209, "endOffset": 217}, {"referenceID": 25, "context": "The concept of CBOW is motivated by the distributional hypothesis [27], which states that words with similar meanings often occur in similar contexts, and it is thus suggested to look for wt whose word representation can capture its context distributions well.", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "In contrast to the CBOW model, the SG model employs an inverse training objective for learning word representations with a simplified feed-forward neural network [21, 28].", "startOffset": 162, "endOffset": 170}, {"referenceID": 26, "context": "In contrast to the CBOW model, the SG model employs an inverse training objective for learning word representations with a simplified feed-forward neural network [21, 28].", "startOffset": 162, "endOffset": 170}, {"referenceID": 26, "context": "In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the negative sampling algorithm [28, 30] can make the training process more efficient and effective.", "startOffset": 75, "endOffset": 83}, {"referenceID": 27, "context": "In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the negative sampling algorithm [28, 30] can make the training process more efficient and effective.", "startOffset": 75, "endOffset": 83}, {"referenceID": 26, "context": "In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the negative sampling algorithm [28, 30] can make the training process more efficient and effective.", "startOffset": 120, "endOffset": 128}, {"referenceID": 28, "context": "In the implementations of CBOW and SG, the hierarchical soft-max algorithm [28, 29] and the negative sampling algorithm [28, 30] can make the training process more efficient and effective.", "startOffset": 120, "endOffset": 128}, {"referenceID": 20, "context": "The GloVe model suggests that an appropriate starting point for word representation learning should be associated with the ratios of co-occurrence probabilities rather than the prediction probabilities [22].", "startOffset": 202, "endOffset": 206}, {"referenceID": 7, "context": "Second, it is worthy to note that SG (trained with the negative sampling algorithm) and GloVe have an implicit/explicit relation with the classic weighted matrix factorization approach, while the major difference is that SG and GloVe concentrate on rendering the word-by-word cooccurrence matrix but weighted matrix factorization is usually concerned with decomposing the word-by-document matrix [9, 31, 32].", "startOffset": 396, "endOffset": 407}, {"referenceID": 29, "context": "Second, it is worthy to note that SG (trained with the negative sampling algorithm) and GloVe have an implicit/explicit relation with the classic weighted matrix factorization approach, while the major difference is that SG and GloVe concentrate on rendering the word-by-word cooccurrence matrix but weighted matrix factorization is usually concerned with decomposing the word-by-document matrix [9, 31, 32].", "startOffset": 396, "endOffset": 407}, {"referenceID": 30, "context": "Second, it is worthy to note that SG (trained with the negative sampling algorithm) and GloVe have an implicit/explicit relation with the classic weighted matrix factorization approach, while the major difference is that SG and GloVe concentrate on rendering the word-by-word cooccurrence matrix but weighted matrix factorization is usually concerned with decomposing the word-by-document matrix [9, 31, 32].", "startOffset": 396, "endOffset": 407}, {"referenceID": 31, "context": "It is worthy to note that using SVD to derive the word representations is similar in spirit to latent semantic analysis (LSA) but using the word-word co-occurrence matrix instead of the word-by-document co-occurrence matrix [33].", "startOffset": 224, "endOffset": 228}, {"referenceID": 21, "context": "Inspired by the vector space model (VSM), a straightforward way to leverage the word embedding methods for extractive SDS is to represent a sentence Si (and a document D to be summarized) by averaging the vector representations of words occurring in the sentence Si (and the document D) [23, 25]:", "startOffset": 287, "endOffset": 295}, {"referenceID": 23, "context": "Inspired by the vector space model (VSM), a straightforward way to leverage the word embedding methods for extractive SDS is to represent a sentence Si (and a document D to be summarized) by averaging the vector representations of words occurring in the sentence Si (and the document D) [23, 25]:", "startOffset": 287, "endOffset": 295}, {"referenceID": 32, "context": "To mitigate the deficiency of the cosine similarity measure, we employ a triplet learning model to enhance the estimation of the similarity degree between a pair of representations [34-36].", "startOffset": 181, "endOffset": 188}, {"referenceID": 33, "context": "To mitigate the deficiency of the cosine similarity measure, we employ a triplet learning model to enhance the estimation of the similarity degree between a pair of representations [34-36].", "startOffset": 181, "endOffset": 188}, {"referenceID": 34, "context": "To mitigate the deficiency of the cosine similarity measure, we employ a triplet learning model to enhance the estimation of the similarity degree between a pair of representations [34-36].", "startOffset": 181, "endOffset": 188}, {"referenceID": 32, "context": "By applying the passive-aggressive learning algorithm presented in [34], we can derive the similarity function R such that all triplets obey", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "Then, W can be obtained by applying an efficient sequential learning algorithm iteratively over the triplets [34, 35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 33, "context": "Then, W can be obtained by applying an efficient sequential learning algorithm iteratively over the triplets [34, 35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 35, "context": "The simplest way is to estimate a unigram language model (ULM) based on the frequency of each distinct word w occurring in S, with the maximum likelihood (ML) criterion [37, 38]:", "startOffset": 169, "endOffset": 177}, {"referenceID": 36, "context": "The simplest way is to estimate a unigram language model (ULM) based on the frequency of each distinct word w occurring in S, with the maximum likelihood (ML) criterion [37, 38]:", "startOffset": 169, "endOffset": 177}, {"referenceID": 37, "context": "The dataset used in this study is the MATBN broadcast news corpus collected by the Academia Sinica and the Public Television Service Foundation of Taiwan between November 2001 and April 2003 [39].", "startOffset": 191, "endOffset": 195}, {"referenceID": 38, "context": "For the assessment of summarization performance, we adopted the widely-used ROUGE metrics [40].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "All the experimental results reported hereafter are obtained by calculating the Fscores [17] of these ROUGE metrics.", "startOffset": 88, "endOffset": 92}, {"referenceID": 39, "context": "A subset of 25-hour speech data from MATBN compiled from November 2001 to December 2002 was used to bootstrap the acoustic training with the minimum phone error rate (MPE) criterion and a training data selection scheme [41].", "startOffset": 219, "endOffset": 223}, {"referenceID": 38, "context": "(14) [40].", "startOffset": 5, "endOffset": 9}], "year": 2015, "abstractText": "Owing to the rapidly growing multimedia content available on the Internet, extractive spoken document summarization, with the purpose of automatically selecting a set of representative sentences from a spoken document to concisely express the most important theme of the document, has been an active area of research and experimentation. On the other hand, word embedding has emerged as a newly favorite research subject because of its excellent performance in many natural language processing (NLP)-related tasks. However, as far as we are aware, there are relatively few studies investigating its use in extractive text or speech summarization. A common thread of leveraging word embeddings in the summarization process is to represent the document (or sentence) by averaging the word embeddings of the words occurring in the document (or sentence). Then, intuitively, the cosine similarity measure can be employed to determine the relevance degree between a pair of representations. Beyond the continued efforts made to improve the representation of words, this paper focuses on building novel and efficient ranking models based on the general word embedding methods for extractive speech summarization. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.", "creator": "Acrobat PDFMaker 11 Word \u7248"}}}