{"id": "1508.02375", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Aug-2015", "title": "Approximation-Aware Dependency Parsing by Belief Propagation", "abstract": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n^3) runtime. It outputs the parse with maximum expected recall -- but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by back-propagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as Stoyanov et al. (2011) and Domke (2010) did for loopy CRFs. The resulting trained parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood.", "histories": [["v1", "Mon, 10 Aug 2015 19:48:33 GMT  (143kb,D)", "http://arxiv.org/abs/1508.02375v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["matthew r gormley", "mark dredze", "jason eisner"], "accepted": true, "id": "1508.02375"}, "pdf": {"name": "1508.02375.pdf", "metadata": {"source": "CRF", "title": "Approximation-Aware Dependency Parsing by Belief Propagation", "authors": ["Matthew R. Gormley", "Mark Dredze", "Jason Eisner"], "emails": ["mrg@cs.jhu.edu", "mdredze@cs.jhu.edu", "jason@cs.jhu.edu"], "sections": [{"heading": "1 Introduction", "text": "Recent improvements to dependency parsing accuracy have been driven by higher-order features. Such a feature can look beyond just the parent and child words connected by a single edge to also consider siblings, grand-parents, etc. By including increasingly global information, these features provide more information for the parser\u2014but they also complicate inference. The resulting higher-order parsers depend on approximate inference and decoding procedures, which may prevent them from predicting the best parse.\nFor example, consider the dependency parser we will train in this paper, which is based on the work of Smith and Eisner (2008). Ostensibly, this parser\nfinds the minimum Bayes risk (MBR) parse under a probability distribution defined by a higher-order dependency parsing model. In reality, however, it achieves O(n3T ) runtime by relying on three approximations during inference: (1) variational inference by loopy belief propagation (BP) on a factor graph, (2) early stopping of inference after tmax iterations prior to convergence, and (3) a first-order pruning model to limit the number of edges considered in the higher-order model. Such parsers are traditionally trained as if the inference had been exact (Smith and Eisner, 2008).1\nIn contrast, we train the parser such that the approximate system performs well on the final evaluation function. Stoyanov and Eisner (2012) call this approach ERMA, for \u201cempirical risk minimization under approximations.\u201d We treat the entire parsing computation as a differentiable circuit, and backpropagate the evaluation function through our approximate inference and decoding methods to improve its parameters by gradient descent.\nOur primary contribution is the application of Stoyanov and Eisner\u2019s learning method in the parsing setting, for which the graphical model involves a global constraint. Smith and Eisner (2008) previously showed how to run BP in this setting (by calling the inside-outside algorithm as a subroutine). We must backpropagate the downstream objective function through their algorithm so that we can follow its gradient. We carefully define our objective function to be smooth and differentiable, yet equivalent to accuracy of the minimum Bayes risk (MBR) parse in the limit. Further we introduce a new simpler objective function based on the L2 dis-\n1For perceptron training, utilizing inexact inference as a drop-in replacement for exact inference can badly mislead the learner (Kulesza and Pereira, 2008).\nar X\niv :1\n50 8.\n02 37\n5v 1\n[ cs\n.C L\n] 1\n0 A\nug 2\n01 5\ntance between the approximate marginals and the \u201ctrue\u201d marginals from the gold data.\nThe goal of this work is to account for the approximations made by a system rooted in structured belief propagation. Taking such approximations into account during training enables us to improve the speed and accuracy of inference at test time. To this end, we compare our training method with the standard approach of conditional log-likelihood. We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks as well as the English Penn Treebank (Marcus et al., 1993). On English, the resulting parser obtains higher accuracy with fewer iterations of BP than standard conditional log-likelihood (CLL) training. On the CoNLL languages, we find that on average it yields higher accuracy parsers than CLL training, particularly when limited to few BP iterations."}, {"heading": "2 Dependency Parsing by Belief Propagation", "text": "This section describes the parser that we will train.\nModel A factor graph (Frey et al., 1997; Kschischang et al., 2001) is a bipartite graph between factors \u03b1 and variables yi, and defines the factorization of a probability distribution over a set of variables {y1, y2, . . .}. The factor graph contains edges between each factor \u03b1 and a subset of variables y\u03b1. Each factor has a local opinion about the possible assignments to its neighboring variables. Such opinions are given by the factor\u2019s potential function \u03c8\u03b1, which assigns a nonnegative score to each configuration of a subset of variables y\u03b1. We define the probability of a given assignment y to be proportional to a product of potential functions: p(y) = 1 Z \u220f \u03b1 \u03c8\u03b1(y\u03b1).\nSmith and Eisner (2008) define a factor graph for dependency parsing of a given n-word sentence: n2 binary variables {y1, y2, . . .} indicate which of the directed arcs are included (yi = ON) or excluded (yi = OFF) in the dependency parse. One of the factors plays the role of a hard global constraint: \u03c8PTREE(y) is 1 or 0 according to whether the assignment encodes a projective dependency tree. Another O(n2) factors (one per variable) evaluate the individual arcs given the sentence, so that p(y) describes a first-order dependency parser. A higherorder parsing model is achieved by including higherorder factors, each scoring configurations of two or more arcs, such as grandparent and sibling configurations. Higher-order factors add cycles to the factor graph. See Figure 1 for an example factor graph.\nWe define each potential function to have a loglinear form: \u03c8\u03b1(y\u03b1) = exp(\u03b8 \u00b7 f\u03b1(y\u03b1,x)). Here x is the vector of observed variables such as the sentence and its POS tags; f\u03b1 extracts a vector of features; and \u03b8 is our vector of model parameters. We write the resulting probability distribution over parses as p\u03b8(y), to indicate that it depends on \u03b8.\nLoss For dependency parsing, our loss function is the number of missing edges in the predicted parse y\u0302, relative to the reference (or \u201cgold\u201d) parse y\u2217:\n`(y\u0302,y\u2217) = \u2211\ni: y\u2217i=ON\n\u03b4(y\u0302i = OFF) (1)\nBecause y\u0302 and y\u2217 each specify exactly one parent for each word token, `(y\u0302,y\u2217) equals the number of word tokens whose parent is predicted incorrectly\u2014 that is, directed dependency error.\nDecoder To obtain a single parse as output, we use a minimum Bayes risk (MBR) decoder, which attempts to find the tree with minimum expected loss under the model\u2019s distribution (Bickel and Doksum, 1977). For our directed dependency error loss function, we obtain the following decision rule:\nh\u03b8(x) = argmin y\u0302 Ep\u03b8(y|x)[`(y\u0302,y)] (2)\n= argmax y\u0302 \u2211 i: y\u0302i=ON p\u03b8(yi = ON|x) (3)\nHere y\u0302 ranges over well-formed parses. Thus, our parser seeks a well-formed parse h\u03b8(x) whose individual edges have a high probability of being correct according to p\u03b8. MBR is the principled way to take a loss function into account under a probabilistic model. By contrast, maximum a posteriori (MAP) decoding does not consider the loss function. It would return the single highest-probability parse even if that parse, and its individual edges, were unlikely to be correct.2\nAll systems in this paper use MBR decoding to consider the loss function at test time. This implies that the ideal training procedure would be to find the true p\u03b8 so that its marginals can be used in (3). Our baseline system attempts this. In practice, however, we will not be able to find the true p\u03b8 (model misspecification) nor exactly compute the marginals of p\u03b8 (computational intractability). Thus, this paper proposes a training procedure that compensates for the system\u2019s approximations, adjusting \u03b8 to reduce the actual loss of h\u03b8(x) as measured at training time.\nTo find the MBR parse, we first run inference to compute the marginal probability p\u03b8(yi = ON) for each edge. Then we maximize (3) by running a firstorder dependency parser with edge scores equal to those probabilities.3 When our inference algorithm is approximate, we replace the exact marginal with its approximation\u2014the normalized belief from BP, given by bi(ON) in (6) below.\nInference Loopy belief propagation (BP) (Murphy et al., 1999) computes approximations to the\n2If we used a simple 0-1 loss function within (2), then MBR decoding would reduce to MAP decoding.\n3Prior work (Smith and Eisner, 2008; Bansal et al., 2014) used the log-odds ratio log p\u03b8(yi=ON)\np\u03b8(yi=OFF) as the edge scores for\ndecoding, but this yields a parse different from the MBR parse.\nvariable marginals p\u03b8(yi) and the factor marginals p\u03b8(y\u03b1). The algorithm proceeds by iteratively sending messages from variables, yi, to factors, \u03c8\u03b1:\nm (t) i\u2192\u03b1(yi) = \u220f \u03b2\u2208N (i)\\\u03b1 m (t\u22121) \u03b2\u2192i (yi) (4)\nand from factors to variables:\nm (t) \u03b1\u2192i(yi) = \u2211 y\u03b1\u223cyi \u03c8\u03b1(y\u03b1) \u220f j\u2208N (\u03b1)\\i m (t\u22121) j\u2192\u03b1 (yi) (5)\nwhere N (i) and N (\u03b1) denote the neighbors of yi and \u03c8\u03b1 respectively, and where y\u03b1 \u223c yi is standard notation to indicate that y\u03b1 ranges over all assignments to the variables participating in the factor \u03b1 provided that the ith variable has value yi. Note that the messages at time t are computed from those at time (t\u22121). Messages at the final time tmax are used to compute the beliefs at each factor and variable:\nbi(yi) = \u220f\n\u03b1\u2208N (i)\nm (tmax) \u03b1\u2192i (yi) (6)\nb\u03b1(y\u03b1) = \u03c8\u03b1(y\u03b1) \u220f\ni\u2208N (\u03b1)\nm (tmax) i\u2192\u03b1 (yi) (7)\nEach of the functions defined by equations (4)\u2013 (7) can be optionally rescaled by a constant at any time, e.g., to prevent overflow/underflow. Below, we specifically assume that each function bi has been rescaled such that \u2211 yi bi(yi) = 1. This bi approximates the marginal distribution over yi values. Messages continue to change indefinitely if the factor graph is cyclic, but in the limit, the rescaled messages may converge. Although the equations above update all messages in parallel, convergence is much faster if only one message is updated per timestep, in some well-chosen serial order.4\nFor the PTREE factor, the summation over variable assignments required for m(t)\u03b1\u2192i(yi) in Eq. (5) equates to a summation over exponentially many projective parse trees. However, we can use an inside-outside variant of the algorithm of Eisner\n4Following Dreyer and Eisner (2009, footnote 22), we choose an arbitrary directed spanning tree rooted at the PTREE factor. We visit the nodes in topologically sorted order (starting at the leaves) and update any message from the node being visited to a node that is later in the order (e.g., closer to the root). We then reverse this order and repeat, so that every message has been passed once. This constitutes one iteration of BP.\n(1996) to compute this in polynomial time (we describe this as hypergraph parsing in \u00a7 3). The resulting \u201cstructured BP\u201d inference procedure is exact for first-order dependency parsing, and approximate when high-order factors are incorporated. The advantage of BP is that it enables fast approximate inference when exact inference is too slow. See Smith and Eisner (2008) for details.5"}, {"heading": "3 Approximation-aware Learning", "text": "We aim to find the parameters \u03b8\u2217 that minimize a regularized objective function over the training sample of sentence/parse pairs {(x(d),y(d))}Dd=1.\n\u03b8\u2217 = argmin \u03b8\n\u03bb 2 ||\u03b8||22 + 1 D D\u2211 d=1 J(\u03b8;x(d),y(d)) (8)\nwhere \u03bb > 0 is the regularization coefficient and J(\u03b8;x,y) is a given differentiable function, possibly nonconvex. We locally minimize this objective using `2-regularized AdaGrad with Composite Mirror Descent (Duchi et al., 2011)\u2014a variant of stochastic gradient descent that uses mini-batches, an adaptive learning rate per dimension, and sparse lazy updates from the regularizer.6\nObjective Functions As in Stoyanov et al. (2011), our aim is to minimize expected loss on the true data distribution over sentence/parse pairs (X,Y ):\n\u03b8\u2217 = argmin\u03b8 E[`(h\u03b8(X), Y )] (9)\nSince the true data distribution is unknown, we substitute the expected loss over the training sample, and regularize our objective to reduce sampling variance. Specifically, we aim to minimize the regularized empirical risk, given by (8) with J(\u03b8;x(d),y(d)) set to `(h\u03b8(x(d)),y(d)). Using our MBR decoder h\u03b8 in (3), this loss function would\n5How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PTREE factor with a TREE factor that allows edges to cross.\n6\u03b8 is initialized to 0 when not otherwise specified.\nnot be differentiable because of the argmax in the definition of h\u03b8 (3). We will address this below by substituting a differentiable softmax. This is the \u201cERMA\u201d method of Stoyanov and Eisner (2012). We will also consider simpler choices of J(\u03b8;x(d),y(d)) that are commonly used in training neural networks. Finally, the standard convex objective is conditional log-likelihood (\u00a7 4).\nGradient Computation To compute the gradient \u2207\u03b8J(\u03b8;x,y\u2217) of the loss on a single sentence (x,y\u2217) = (x(d),y(d)), we apply automatic differentiation (AD) in the reverse mode (Griewank and Corliss, 1991). This yields the same type of \u201cbackpropagation\u201d algorithm that has long been used for training neural networks (Rumelhart et al., 1986). In effect, we are regarding (say) 5 iterations of the BP algorithm on sentence x, followed by (softened) MBR decoding and comparison to the target output y\u2217, as a kind of neural network that computes `(h\u03b8(x),y\n\u2217). It is important to note that the resulting gradient computation algorithm is exact up to floating-point error, and has the same asymptotic complexity as the original decoding algorithm, requiring only about twice the computation. The AD method applies provided that the original function is indeed differentiable with respect to \u03b8, an issue that we take up below.\nIn principle, it is possible to compute the gradient with minimal additional coding. There exists AD software (some listed at autodiff.org) that could be used to derive the necessary code automatically. Another option would be to use the perturbation method of Domke (2010). However, we implemented the gradient computation directly, and we describe it here."}, {"heading": "3.1 Inference, Decoding, and Loss as a Feedfoward Circuit", "text": "The backpropagation algorithm is often applied to neural networks, where the topology of a feedforward circuit is statically specified and can be applied to any input. Our BP algorithm, decoder, and loss function similarly define a feedfoward circuit that computes our function J . However, the circuit\u2019s topology is defined dynamically (per sentence x(d)) by \u201cunrolling\u201d the computation into a graph.\nFigure 2 shows this topology for one choice of ob-\njective function. The high level modules consist of (A) computing potential functions, (B) initializing messages, (C) sending messages, (D) computing beliefs, and (E) decoding and computing the loss. We zoom in on two submodules: the first computes messages from the PTREE factor efficiently (C.1\u2013C.3); the second computes a softened version of our loss function (E.1\u2013E.3). Both of these submodules are made efficient by the inside-outside algorithm.\nThe remainder of this section describes additional details of how we define the function J (the forward pass) and how we compute its gradient (the backward pass). Backpropagation computes the derivative of any given function specified by an arbitrary circuit consisting of elementary differentiable operations (e.g. +,\u2212,\u00d7,\u00f7, log, exp). This is accomplished by repeated application of the chain rule.\nBackpropagating through an algorithm proceeds by similar application of the chain rule, where the intermediate quantities are determined by the topology of the circuit. Doing so with the circuit from Figure 2 poses several challenges. Eaton and Ghahramani (2009) and Stoyanov et al. (2011) showed how to backpropagate through the basic BP algorithm, and we reiterate the key details below (\u00a7 3.3). The remaining challenges form the primary technical contribution of this paper:\n1. Our true loss function `(h\u03b8(x),y\u2217) by way of the decoder (3) contains an argmax over trees and is therefore not differentiable. We show how to soften this decoder, making it differentiable (\u00a7 3.2). 2. Empirically, we find the above objective difficult to optimize. To address this, we substitute a simpler L2 loss function (commonly used in neural networks). This is easier to optimize and yields our best parsers in practice (\u00a7 3.2). 3. We show how to run backprop through the inside-outside algorithm on a hypergraph (\u00a7 3.5), and thereby on the softened decoder and computation of messages from the PTREE factor. This allows us to go beyond Stoyanov et al. (2011) and train structured BP in an approximation-aware and loss-aware fashion."}, {"heading": "3.2 Differentiable Objective Functions", "text": "Annealed Risk Directed dependency error, `(h\u03b8(x),y\n\u2217), is not differentiable due to the argmax in the decoder h\u03b8. We therefore redefine J(\u03b8;x,y\u2217) to be a new differentiable loss function, the annealed risk R1/T\u03b8 (x,y\n\u2217), which approaches the loss `(h\u03b8(x),y\u2217) as the temperature T \u2192 0.\nThis is done by replacing our non-differentiable decoder h\u03b8 with a differentiable one (at training time). As input, it still takes the marginals p\u03b8(yi = ON | x), or in practice, their BP approximations bi(ON). We define a distribution over parse trees:\nq 1/T \u03b8 (y\u0302) \u221d exp  \u2211 i:y\u0302i=ON p\u03b8(yi = ON|x)/T  (10) Imagine that at training time, our decoder stochastically returns a parse y\u0302 sampled from this distribution. Our risk is the expected loss of that decoder:\nR 1/T \u03b8 (x,y \u2217) = E y\u0302\u223cq1/T\u03b8 [`(y\u0302,y\u2217)] (11)\nAs T \u2192 0 (\u201cannealing\u201d), the decoder almost always\nchooses the MBR parse,7 so our risk approaches the loss of the actual MBR decoder that will be used at test time. However, as a function of \u03b8, it remains differentiable (though not convex) for any T > 0.\nTo compute the annealed risk, observe that it simplifies to R1/T\u03b8 (x,y \u2217) = \u2212 \u2211 i:y\u2217i=ON q 1/T \u03b8 (y\u0302i = ON). This is the negated expected recall of a parse y\u0302 \u223c q1/T\u03b8 . We obtain the required marginals q 1/T \u03b8 (y\u0302i = ON) from (10) by running inside-outside where the edge weight for edge i is given by exp(p\u03b8(yi = ON|x)/T ).\nWith the annealed risk as our J function, we can compute\u2207\u03b8J by backpropagating through the computation in the previous paragraph. The computations of the edge weights and the expected recall are trivially differentiable. The only challenge is computing the partials of the marginals differentiating the function computed by this call to the insideoutside algorithm; we address this in Section 3.5. Figure 2 (E.1\u2013E.3) shows where these computations lie within the circuit.\nWhether our test-time system computes the marginals of p\u03b8 exactly or does so approximately via BP, our new training objective approaches (as T \u2192 0) the true empirical risk of the test-time parser that performs MBR decoding from the computed marginals. Empirically, however, we will find that it is not the most effective training objective (\u00a7 5.2). Stoyanov et al. (2011) postulate that the nonconvexity of empirical risk may make it a difficult function to optimize (even with annealing). Our next two objectives provide alternatives.\nL2 Distance We can view our inference, decoder, and loss as defining a form of deep neural network, whose topology is inspired by our linguistic knowledge of the problem (e.g., the edge variables should define a tree). This connection to deep learning allows us to consider training methods akin to supervised layer-wise training. We temporarily remove the top layers of our network (i.e. the decoder and loss module, Fig. 2 (E)) so that the output layer of our \u201cdeep network\u201d consists of the normalized vari-\n7Recall from (3) that the MBR parse is the tree y\u0302 that maximizes the sum \u2211 i:y\u0302i=ON\np\u03b8(yi = ON|x). As T \u2192 0, the right-hand side of (10) grows fastest for this y\u0302, so its probability under q1/T\u03b8 approaches 1 (or 1/k if there is a k-way tie for MBR parse).\nable beliefs bi(yi) from BP. We can then define a supervised loss function directly on these beliefs.\nWe don\u2019t have supervised data for this layer of beliefs, but we can create it artificially. Use the supervised parse y\u2217 to define \u201ctarget beliefs\u201d by b\u2217i (yi) = I(yi = y\u2217i ) \u2208 {0, 1}. To find parameters \u03b8 that make BP\u2019s beliefs close to these targets, we can minimize an L2 distance loss function:\nJ(\u03b8;x,y\u2217) = \u2211 i \u2211 yi (bi(yi)\u2212 b\u2217i (y\u2217i ))2 (12)\nWe can use this L2 distance objective function for training, adding the MBR decoder and loss evaluation back in only at test time.\nLayer-wise Training Just as in layer-wise training of neural networks, we can take a two-stage approach to training. First, we train to minimize the L2 distance. Then, we use the resulting \u03b8 as initialization to optimize the annealed risk, which does consider the decoder and loss function (i.e. the top layers of Fig. 2). Stoyanov et al. (2011) found mean squared error (MSE) to give a smoother training objective, though still non-convex, and similarly used it to find an initializer for empirical risk. Though their variant of the L2 objective did not completely dispense with the decoder as ours does, it is a similar approach to our proposed layer-wise training."}, {"heading": "3.3 Backpropagation through BP", "text": "Belief propagation proceeds iteratively by sending messages. We can label each message with a timestamp t (e.g. m(t)i\u2192\u03b1) indicating the time step at which it was computed. Figure 2 (B) shows the messages at time t = 0, denoted m(0)i\u2192\u03b1, which are initialized to the uniform distribution. Figure 2 (C) depicts the computation of all subsequent messages via Eqs. (4) and (5). Messages at time t are computed from messages at time t \u2212 1 or before and the potential functions \u03c8\u03b1. After the final iteration T , the beliefs bi(yi), b\u03b1(y\u03b1) are computed from the final messages m\n(T ) i\u2192\u03b1 using Eqs. (6) and (7)\u2014this is shown in Figure 2 (D). Optionally, we can normalize the messages after each step to avoid overflow (not shown in the figure) as well as the beliefs.\nExcept for the messages sent from the PTREE factor, each step of BP computes some value from\nearlier values using a simple formula. Backpropagation differentiates these simple formulas. This lets it compute J\u2019s partial derivatives with respect to the earlier values, once its partial derivatives have been computed with respect to later values. Explicit formulas can be found in the appendix of Stoyanov et al. (2011)."}, {"heading": "3.4 BP and backpropagation with PTREE", "text": "The PTREE factor has a special structure that we exploit for efficiency during BP. Stoyanov et al. (2011) assume that BP takes an explicit sum in (5). For the PTREE factor, this equates to a sum over all projective dependency trees (since \u03c8PTREE(y) = 0 for any assignment y which is not a tree). There are exponentially many such trees. However, Smith and Eisner (2008) point out that for \u03b1 = PTREE, the summation has a special structure that can be exploited by dynamic programming.\nTo compute the factor-to-variable messages from \u03b1 = PTREE, they first run the inside-outside algorithm where the edge weights are given by the ratios of the messages to PTREE: m (t) i\u2192\u03b1(ON)\nm (t) i\u2192\u03b1(OFF)\n. Then\nthey multiply each resulting edge marginal given by inside-outside by the product of all the OFF messages \u220f im (t) i\u2192\u03b1(OFF) to get the marginal factor belief b\u03b1(yi). Finally they divide the belief by the incoming message m(t)i\u2192\u03b1(ON) to get the corresponding outgoing message m(t+1)\u03b1\u2192i (ON).\nThese steps are shown in Figure 2 (C.1\u2013C.3), and are repeated each time we send a message from the PTree factor. The derivatives of the message ratios and products mentioned here are trivial. Though we focus here on projective dependency parsing, our techniques are also applicable to non-projective parsing and the TREE factor; we leave this to future work. In the next subsection, we explain how to backpropagate through the inside-outside algorithm."}, {"heading": "3.5 Backpropagation through Inside-Outside on a Hypergraph", "text": "Both the annealed risk loss function (\u00a7 3.2) and the computation of messages from the PTREE factor use the inside-outside algorithm for dependency parsing. Here we describe inside-outside and the accompanying backpropagation algorithm over a hypergraph. This more general treatment shows the ap-\nplicability of our method to other structured factors such as for CNF parsing, HMM forward-backward, etc. In the case of dependency parsing, the structure of the hypergraph is given by the dynamic programming algorithm of Eisner (1996).\nFor the forward pass of the inside-outside module, the input variables are the hyperedge weights we\u2200e and the outputs are the marginal probabilities pw(i)\u2200i of each node i in the hypergraph. The latter are a function of the inside \u03b2i and outside \u03b1j probabilities. We initialize \u03b1root = 1.\n\u03b2i = \u2211 e\u2208I(i) we \u220f j\u2208T (e) \u03b2j (13)\n\u03b1j = \u2211 e\u2208O(i) we \u03b1H(e) \u220f j\u2208T (e):j 6=i \u03b2j (14)\npw(i) = \u03b1i\u03b2i/\u03b2root (15)\nFor each node i, we define the set of incoming edges I(i) and outgoing edges O(i). The antecedents of the edge are T (e), the parent of the edge is H(e), and its weight is we.\nBelow we use the concise notation of an adjoint \u00f0y = \u2202J\u2202y , a derivative with respect to objective J . For the backward pass through the inside-outside AD module, the inputs are \u00f0pw(i)\u2200i and the outputs are \u00f0we\u2200e. We also compute the adjoints of the intermediate quantities \u00f0\u03b2j ,\u00f0\u03b1i. We first compute \u00f0\u03b1i bottom-up. Next \u00f0\u03b2j are computed top-down. The adjoints \u00f0we are then computed in any order.\n\u00f0\u03b1i = \u00f0pw(i)\u2202pw(i)\u2202\u03b1i + \u2211 e\u2208I(i) \u2211 j\u2208T (e) \u00f0\u03b1j \u2202\u03b1j\u2202\u03b1i (16)\n\u00f0\u03b2root = \u2211 i 6=root \u00f0pw(i)\u2202pw(i)\u2202\u03b2root (17)\n\u00f0\u03b2j = \u00f0pw(j)\u2202pw(j)\u2202\u03b2j + \u2211\ne\u2208O(j)\n\u00f0\u03b2H(e) \u2202\u03b2H(e) \u2202\u03b2j\n(18)\n+ \u2211\ne\u2208O(j) \u2211 k\u2208T (e):k 6=j \u00f0\u03b1k \u2202\u03b1k\u2202\u03b2j \u2200j 6= root (19)\n\u00f0we = \u00f0\u03b2H(e) \u2202\u03b2H(e) \u2202we + \u2211 j\u2208T (e) \u00f0\u03b1j \u2202\u03b1j\u2202we (20)\nBelow, we show the partial derivatives required for the adjoint computations.\n\u2202pw(i)\n\u2202\u03b1i = \u03b2i/\u03b2root,\n\u2202pw(i)\n\u2202\u03b2root = \u2212\u03b1i\u03b2i/(\u03b22root),\n\u2202pw(i)\n\u2202\u03b2i = \u03b1i/\u03b2root\nFor some edge e, let i = H(e) be the parent of the edge and j, k \u2208 T (e) be among its antecendents.\n\u2202\u03b2i \u2202\u03b2j\n= we \u220f\nk\u2208T (e):k 6=j\n\u03b2k, \u2202\u03b2H(e)\n\u2202we = \u220f j\u2208T (e) \u03b2j\n\u2202\u03b1j \u2202\u03b1i\n= we \u220f\nk\u2208T (e):k 6=j\n\u03b2k, \u2202\u03b1j \u2202we\n= \u03b1H(e) \u220f\nk\u2208T (e):k 6=j\n\u03b2k\n\u2202\u03b1k \u2202\u03b2j\n= we\u03b1H(e) \u220f\nl\u2208T (e):l 6=j,l 6=k\n\u03b2l\nThis backpropagation method is used for both Figure 2 C.2 and E.2."}, {"heading": "4 Other Learning Settings", "text": "Loss-aware Training with Exact Inference Backpropagating through inference, decoder, and loss need not be restricted to approximate inference algorithms. Li and Eisner (2009) optimize Bayes risk with exact inference on a hypergraph for machine translation. Each of our differentiable loss functions (\u00a7 3.2) can also be coupled with exact inference. For a first-order parser, BP is exact. Yet, in place of modules (B), (C), and (D) in Figure 2, we can use a standard dynamic programming algorithm for dependency parsing, which is simply another instance of inside-outside on a hypergraph (\u00a7 3.5). The exact marginals from inside-outside (15) are then fed forward into the decoder/loss module (E).\nConditional and Surrogate Log-likelihood The standard approach to training is conditional loglikelihood (CLL) maximization (Smith and Eisner, 2008), which does not take inexact inference into account. When inference is exact, this baseline computes the true gradient of CLL. When inference is approximate, this baseline uses the approximate marginals from BP in place of their exact values in the gradient. The literature refers to this approximation-unaware training method as surrogate likelihood training since it returns the \u201cwrong\u201d model even under the assumption of infinite training data (Wainwright, 2006). Despite this, the surrogate likelihood objective is commonly used to train CRFs. CLL and approximation-aware training are not mutually exclusive. Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011)."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Setup", "text": "Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design. We add O(n3) secondorder grandparent and arbitrary sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse POS tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Attenberg et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work.\nPruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train an (exact) first-order model and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token.8 On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009). The pruning model uses a simpler feature set as in Rush and Petrov (2012).\nData We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks. We also convert the English Penn Treebank (PTB) (Marcus et al., 1993) to dependencies using the head rules from Yamada and Matsumoto (2003) (PTB-YM). We evaluate unlabeled attachment accuracy (UAS) using gold POS tags for the CoNLL languages, and predicted tags from TurboTagger9 for the PTB. Unlike most prior work, we hold out 10% of each CoNLL training dataset as development data.\n8We expect this to be the least impactful of our approximations: Koo and Collins (2010) report 99.92% oracle accuracy for English.\n9 http://www.cs.cmu.edu/\u02dcafm/TurboParser\nSome of the CoNLL languages contain nonprojective edges. With the projectivity constraint, the model assigns zero probability to such trees. For approximation-aware training this is not a problem; however CLL training cannot handle such trees. For CLL only, we projectivize the training trees following (Carreras, 2007) by finding the maximum projective spanning tree under an oracle model which assigns score +1 to edges in the gold tree and 0 to the others. We always evaluate on the nonprojective trees for comparison with prior work.\nLearning Settings We compare three learning settings. The first, our baseline, is conditional loglikelihood training (CLL) (\u00a7 4). As is common in the literature, we conflate two distinct learning settings (conditional log-likelihood/surrogate loglikelihood) under the single name \u201cCLL\u201d allowing the inference method (exact/inexact) to differentiate them. The second learning setting is approximationaware learning (\u00a7 3) with either our L2 distance objective (L2) or our layer-wise training method (L2+AR) which takes the L2-trained model as an initializer for our annealed risk (\u00a7 3.2). The annealed risk objective requires an annealing schedule: over the course of training, we linearly anneal from initial temperature T = 0.1 to T = 0.0001, updating T at each iteration of stochastic optimization. The third\nuses the same two objectives, L2 and L2+AR, but with exact inference (\u00a7 4). The `2-regularizer weight is \u03bb = 10.1D . Each method is trained by AdaGrad for 10 epochs with early stopping (i.e. the model with the highest score on dev data is returned). The learning rate for each training run is dynamically tuned on a sample of the training data."}, {"heading": "5.2 Results", "text": "Our goal is to demonstrate that our approximationaware training method leads to improved parser accuracy as compared with the standard training approach of conditional log-likelihood (CLL) maximization (Smith and Eisner, 2008), which does not take inexact inference into account. The two key findings of our experiments are that our learning approach is more robust to (1) decreasing the number of iterations of BP and (2) adding additional cycles to the factor graph in the form of higher-order factors. In short: our approach leads to faster inference and creates opportunities for more accurate parsers.\nSpeed-Accuracy Tradeoff Our first experiment is on English dependencies. For English PTB-YM, Figure 3 shows accuracy as a function of the number of BP iterations for our second-order model with both arbitrary sibling and grandparent factors on English. We find that our training methods (L2 and L2+AR) obtain higher accuracy than standard training (CLL), particularly when a small number of BP iterations are used and the inference is a worse ap-\nproximation. Notice that with just two iterations of BP, the parsers trained by our approach obtain accuracy equal to the CLL-trained parser with four iterations. Contrasting the two objectives for our approximation-aware training, we find that our simple L2 objective performs very well. In fact, in only one case at 6 iterations, does the additional annealed risk (L2+AR) improve performance on test data. In our development experiments, we also evaluated AR without using L2 for initialization and we found that it performed worse than either of CLL and L2 alone. That AR performs only slightly better than L2 (and not worse) in the case of L2+AR is likely due to early stopping on dev data, which guards against selecting a worse model.\nIncreasingly Cyclic Models Figure 4 contrasts accuracy with the type of 2nd-order factors (grandparent, sibling, or both) included in the model for English, for a fixed budget of 4 BP iterations. As we add additional higher-order factors, the model has more loops thereby making the BP approximation more problematic for standard CLL training. By contrast, our training performs well even when the factor graphs have many cycles.\nNotice that our advantage is not restricted to the case of loopy graphs. Even when we use a 1storder model, for which BP inference is exact, our approach yields higher accuracy parsers than CLL training. We postulate that this improvement comes from our choice of the L2 objective function. Note the following subtle point: when inference is exact, the CLL estimator is actually a special case of our approximation-aware learner\u2014that is, CLL computes the same gradient that our training by backpropagation would if we used log-likelihood as the objective. Despite its appealing theoretical justification, the AR objective that approaches empirical risk minimization in the limit consistently provides no improvement over our L2 objective.\nExact Inference with Grandparents When our factor graph includes unary and grandparent factors, exact inference in O(n4) time is possible using the dynamic programming algorithm for Model 0 of Koo and Collins (2010). Table 1 compares four parsers, by considering two training approaches and two inference methods. The training approaches are CLL and approximation-aware inference with an L2\nobjective. The inference methods are BP with only four iterations or exact inference by dynamic programming. On test UAS, we find that both the CLL and L2 parsers with exact inference outperform approximate inference\u2014though the margin for CLL is much larger. Surprisingly, our L2-trained parser, which uses only 4 iterations of BP and O(n3) runtime, does just as well as CLL with exact inference. Our L2 parser with exact inference performs the best.\nOther Languages Our final experiments evaluate our approximation-aware learning approach across 19 languages from CoNLL-2006/2007 (Table 2). We find that, on average, approximation-aware training with an L2 objective obtains higher UAS than CLL training. This result holds for both 1stand 2nd-order models with grandparent and sibling factors with 1, 2, 4, or 8 iterations of BP. Table 2 also shows the relative improvement in UAS of L2 vs CLL training for each language as we vary the maximum number of iterations of BP. We find that the approximation-aware training doesn\u2019t always outperform CLL training\u2014only in the aggregate. Again, we see the trend that our training approach yields more significant gains when BP is restricted to a small number of maximum iterations."}, {"heading": "6 Discussion", "text": "The purpose of this work was to explore ERMA and related training methods for models which incorporate structured factors. We applied these methods to a basic higher-order dependency parsing model, because that was the simplest and first (Smith and Eisner, 2008) instance of structured BP. In future work, we hope to explore further models with structured factors\u2014particularly those which jointly account for multiple linguistic strata (e.g. syntax, semantics, and\ntopic). Another natural extension of this work is to explore other types of factors: here we considered only exponential-family potential functions (commonly used in CRFs), but any differentiable function would be appropriate, such as a neural network.\nOur primary contribution is approximation-aware training for structured BP. While our experiments only consider dependency parsing, our approach is applicable for any constraint factor which amounts to running the inside-outside algorithm on a hypergraph. Prior work has used this structured form of BP to do dependency parsing (Smith and Eisner, 2008), CNF grammar parsing (Naradowsky et al., 2012), TAG (Auli and Lopez, 2011), ITGconstraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009). Our training methods could be applied to such tasks as well."}, {"heading": "7 Conclusions", "text": "We introduce a new approximation-aware learning framework for belief propagation with structured factors. We present differentiable objectives for both\nempirical risk minimization (a la. ERMA) and a novel objective based on L2 distance between the inferred beliefs and the true edge indicator functions. Experiments on the English Penn Treebank and 19 languages from CoNLL-2006/2007 shows that our estimator is able to train more accurate dependency parsers with fewer iterations of belief propagation than standard conditional log-likelihood training, by taking approximations into account. Our code is available in a general-purpose library for structured BP, hypergraphs, and backprop.10"}], "references": [{"title": "Feature hashing for large scale multitask learning", "author": ["Josh Attenberg", "A Dasgupta", "J Langford", "A Smola", "K Weinberger."], "venue": "ICML.", "citeRegEx": "Attenberg et al\\.,? 2009", "shortCiteRegEx": "Attenberg et al\\.", "year": 2009}, {"title": "A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing", "author": ["Michael Auli", "Adam Lopez."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Auli and Lopez.,? 2011", "shortCiteRegEx": "Auli and Lopez.", "year": 2011}, {"title": "Structured learning for taxonomy induction with belief propagation", "author": ["Mohit Bansal", "David Burkett", "Gerard de Melo", "Dan Klein."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Mathematical Statistics: Basic Ideas and Selected Topics", "author": ["P.J. Bickel", "K.A. Doksum."], "venue": "HoldenDay Inc., Oakland, CA, USA.", "citeRegEx": "Bickel and Doksum.,? 1977", "shortCiteRegEx": "Bickel and Doksum.", "year": 1977}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Sabine Buchholz", "Erwin Marsi."], "venue": "In Proc. of CoNLL, pages 149\u2013164.", "citeRegEx": "Buchholz and Marsi.,? 2006", "shortCiteRegEx": "Buchholz and Marsi.", "year": 2006}, {"title": "Fast Inference in Phrase Extraction Models with Belief Propagation", "author": ["David Burkett", "Dan Klein."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Burkett and Klein.,? 2012", "shortCiteRegEx": "Burkett and Klein.", "year": 2012}, {"title": "Experiments with a higher-order projective dependency parser", "author": ["Xavier Carreras."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 957\u2013961.", "citeRegEx": "Carreras.,? 2007", "shortCiteRegEx": "Carreras.", "year": 2007}, {"title": "Implicit differentiation by perturbation", "author": ["J. Domke."], "venue": "Advances in Neural Information Processing Systems, pages 523\u2013531.", "citeRegEx": "Domke.,? 2010", "shortCiteRegEx": "Domke.", "year": 2010}, {"title": "Graphical Models over Multiple Strings", "author": ["Markus Dreyer", "Jason Eisner."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 101\u2013110. Association for Computational Linguistics.", "citeRegEx": "Dreyer and Eisner.,? 2009", "shortCiteRegEx": "Dreyer and Eisner.", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Choosing a variable to clamp", "author": ["Frederik Eaton", "Zoubin Ghahramani."], "venue": "International Conference on Artificial Intelligence and Statistics, pages 145\u2013 152.", "citeRegEx": "Eaton and Ghahramani.,? 2009", "shortCiteRegEx": "Eaton and Ghahramani.", "year": 2009}, {"title": "Parsing with soft and hard constraints on dependency length", "author": ["Jason Eisner", "Noah A. Smith."], "venue": "Proceedings of the International Workshop on Parsing Technologies (IWPT), pages 30\u201341, Vancouver, October.", "citeRegEx": "Eisner and Smith.,? 2005", "shortCiteRegEx": "Eisner and Smith.", "year": 2005}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason Eisner."], "venue": "Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 340\u2013345, Copenhagen, August.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "First- and second-order", "author": ["Zhifei Li", "Jason Eisner"], "venue": null, "citeRegEx": "Li and Eisner.,? \\Q2009\\E", "shortCiteRegEx": "Li and Eisner.", "year": 2009}, {"title": "Turbo Parsers: Depen", "author": ["Mario Figueiredo"], "venue": null, "citeRegEx": "Figueiredo.,? \\Q2010\\E", "shortCiteRegEx": "Figueiredo.", "year": 2010}, {"title": "Grammarless Parsing for Joint Inference", "author": ["Jason Naradowsky", "Tim Vieira", "David Smith."], "venue": "Proceedings of COLING 2012, pages 1995\u20132010, Mumbai, India, December. The COLING 2012 Organizing Committee.", "citeRegEx": "Naradowsky et al\\.,? 2012", "shortCiteRegEx": "Naradowsky et al\\.", "year": 2012}, {"title": "The CoNLL 2007 shared task on dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Sandra K\u00fcbler", "Ryan McDonald", "Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915\u2013932.", "citeRegEx": "Nivre et al\\.,? 2007", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "Proc. of LREC.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Relaxed Marginal Inference and its Application to Dependency Parsing", "author": ["Sebastian Riedel", "David A. Smith."], "venue": "Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Riedel and Smith.,? 2010", "shortCiteRegEx": "Riedel and Smith.", "year": 2010}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams."], "venue": "David E. Rumelhart and James L. McClelland, editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1, pages 318\u2013", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Vine pruning for efficient multi-pass dependency parsing", "author": ["Alexander M. Rush", "Slav Petrov."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 498\u2013", "citeRegEx": "Rush and Petrov.,? 2012", "shortCiteRegEx": "Rush and Petrov.", "year": 2012}, {"title": "Dependency parsing by belief propagation", "author": ["David A. Smith", "Jason Eisner."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Smith and Eisner.,? 2008", "shortCiteRegEx": "Smith and Eisner.", "year": 2008}, {"title": "Minimum-risk training of approximate CRF-Based NLP systems", "author": ["Veselin Stoyanov", "Jason Eisner."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "Stoyanov and Eisner.,? 2012", "shortCiteRegEx": "Stoyanov and Eisner.", "year": 2012}, {"title": "Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure", "author": ["Veselin Stoyanov", "Alexander Ropson", "Jason Eisner."], "venue": "Proceedings of the 14th International Conference on Artificial Intel-", "citeRegEx": "Stoyanov et al\\.,? 2011", "shortCiteRegEx": "Stoyanov et al\\.", "year": 2011}, {"title": "Estimating the wrong graphical model: Benefits in the computation-limited setting", "author": ["Martin J. Wainwright."], "venue": "The Journal of Machine Learning Research, 7:1829\u20131859.", "citeRegEx": "Wainwright.,? 2006", "shortCiteRegEx": "Wainwright.", "year": 2006}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proceedings of IWPT, volume 3.", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}], "referenceMentions": [{"referenceID": 11, "context": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy.", "startOffset": 61, "endOffset": 75}, {"referenceID": 11, "context": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n) runtime. It outputs the parse with maximum expected recall\u2014but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by backpropagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as Stoyanov et al. (2011) and Domke (2010) did for loopy CRFs.", "startOffset": 61, "endOffset": 739}, {"referenceID": 7, "context": "(2011) and Domke (2010) did for loopy CRFs.", "startOffset": 11, "endOffset": 24}, {"referenceID": 21, "context": "Such parsers are traditionally trained as if the inference had been exact (Smith and Eisner, 2008).", "startOffset": 74, "endOffset": 98}, {"referenceID": 12, "context": "For example, consider the dependency parser we will train in this paper, which is based on the work of Smith and Eisner (2008). Ostensibly, this parser finds the minimum Bayes risk (MBR) parse under a probability distribution defined by a higher-order dependency parsing model.", "startOffset": 113, "endOffset": 127}, {"referenceID": 12, "context": "Stoyanov and Eisner (2012) call this approach ERMA, for \u201cempirical risk minimization under approximations.", "startOffset": 13, "endOffset": 27}, {"referenceID": 12, "context": "Our primary contribution is the application of Stoyanov and Eisner\u2019s learning method in the parsing setting, for which the graphical model involves a global constraint. Smith and Eisner (2008) previously showed how to run BP in this setting (by calling the inside-outside algorithm as a subroutine).", "startOffset": 60, "endOffset": 193}, {"referenceID": 4, "context": "We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al.", "startOffset": 59, "endOffset": 85}, {"referenceID": 16, "context": "We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks as well as the English Penn Treebank (Marcus et al.", "startOffset": 101, "endOffset": 121}, {"referenceID": 12, "context": "Smith and Eisner (2008) define a factor graph for dependency parsing of a given n-word sentence: n2 binary variables {y1, y2, .", "startOffset": 10, "endOffset": 24}, {"referenceID": 3, "context": "Decoder To obtain a single parse as output, we use a minimum Bayes risk (MBR) decoder, which attempts to find the tree with minimum expected loss under the model\u2019s distribution (Bickel and Doksum, 1977).", "startOffset": 177, "endOffset": 202}, {"referenceID": 21, "context": "Prior work (Smith and Eisner, 2008; Bansal et al., 2014) used the log-odds ratio log p\u03b8(yi=ON) p\u03b8(yi=OFF) as the edge scores for decoding, but this yields a parse different from the MBR parse.", "startOffset": 11, "endOffset": 56}, {"referenceID": 2, "context": "Prior work (Smith and Eisner, 2008; Bansal et al., 2014) used the log-odds ratio log p\u03b8(yi=ON) p\u03b8(yi=OFF) as the edge scores for decoding, but this yields a parse different from the MBR parse.", "startOffset": 11, "endOffset": 56}, {"referenceID": 12, "context": "See Smith and Eisner (2008) for details.", "startOffset": 14, "endOffset": 28}, {"referenceID": 9, "context": "We locally minimize this objective using `2-regularized AdaGrad with Composite Mirror Descent (Duchi et al., 2011)\u2014a variant of stochastic gradient descent that uses mini-batches, an adaptive learning rate per dimension, and sparse lazy updates from the regularizer.", "startOffset": 94, "endOffset": 114}, {"referenceID": 23, "context": "Objective Functions As in Stoyanov et al. (2011), our aim is to minimize expected loss on the true data distribution over sentence/parse pairs (X,Y ):", "startOffset": 26, "endOffset": 49}, {"referenceID": 6, "context": "How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010).", "startOffset": 149, "endOffset": 211}, {"referenceID": 6, "context": ", 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PTREE factor with a TREE factor that allows edges to cross. \u03b8 is initialized to 0 when not otherwise specified. not be differentiable because of the argmax in the definition of h\u03b8 (3). We will address this below by substituting a differentiable softmax. This is the \u201cERMA\u201d method of Stoyanov and Eisner (2012). We will also consider simpler choices of J(\u03b8;x(d),y(d)) that are commonly used in training neural networks.", "startOffset": 8, "endOffset": 695}, {"referenceID": 19, "context": "This yields the same type of \u201cbackpropagation\u201d algorithm that has long been used for training neural networks (Rumelhart et al., 1986).", "startOffset": 110, "endOffset": 134}, {"referenceID": 7, "context": "Another option would be to use the perturbation method of Domke (2010). However, we implemented the gradient computation directly, and we describe it here.", "startOffset": 58, "endOffset": 71}, {"referenceID": 10, "context": "Eaton and Ghahramani (2009) and Stoyanov et al.", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "Eaton and Ghahramani (2009) and Stoyanov et al. (2011) showed how to backpropagate through the basic BP algorithm, and we reiterate the key details below (\u00a7 3.", "startOffset": 0, "endOffset": 55}, {"referenceID": 23, "context": "This allows us to go beyond Stoyanov et al. (2011) and train structured BP in an approximation-aware and loss-aware fashion.", "startOffset": 28, "endOffset": 51}, {"referenceID": 23, "context": "Stoyanov et al. (2011) postulate that the nonconvexity of empirical risk may make it a difficult function to optimize (even with annealing).", "startOffset": 0, "endOffset": 23}, {"referenceID": 23, "context": "Stoyanov et al. (2011) found mean squared error (MSE) to give a smoother training objective, though still non-convex, and similarly used it to find an initializer for empirical risk.", "startOffset": 0, "endOffset": 23}, {"referenceID": 23, "context": "Explicit formulas can be found in the appendix of Stoyanov et al. (2011).", "startOffset": 50, "endOffset": 73}, {"referenceID": 21, "context": "Stoyanov et al. (2011) assume that BP takes an explicit sum in (5).", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "However, Smith and Eisner (2008) point out that for \u03b1 = PTREE, the summation has a special structure that can be exploited by dynamic programming.", "startOffset": 19, "endOffset": 33}, {"referenceID": 12, "context": "In the case of dependency parsing, the structure of the hypergraph is given by the dynamic programming algorithm of Eisner (1996). For the forward pass of the inside-outside module, the input variables are the hyperedge weights we\u2200e and the outputs are the marginal probabilities pw(i)\u2200i of each node i in the hypergraph.", "startOffset": 116, "endOffset": 130}, {"referenceID": 12, "context": "Li and Eisner (2009) optimize Bayes risk with exact inference on a hypergraph for machine translation.", "startOffset": 7, "endOffset": 21}, {"referenceID": 21, "context": "Conditional and Surrogate Log-likelihood The standard approach to training is conditional loglikelihood (CLL) maximization (Smith and Eisner, 2008), which does not take inexact inference into account.", "startOffset": 123, "endOffset": 147}, {"referenceID": 24, "context": "The literature refers to this approximation-unaware training method as surrogate likelihood training since it returns the \u201cwrong\u201d model even under the assumption of infinite training data (Wainwright, 2006).", "startOffset": 188, "endOffset": 206}, {"referenceID": 23, "context": "Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011).", "startOffset": 95, "endOffset": 118}, {"referenceID": 6, "context": ", 2005) and second-order (Carreras, 2007) parsing.", "startOffset": 25, "endOffset": 41}, {"referenceID": 0, "context": "We use feature hashing (Ganchev and Dredze, 2008; Attenberg et al., 2009) and restrict to at most 20 million features.", "startOffset": 23, "endOffset": 73}, {"referenceID": 15, "context": "We add O(n3) secondorder grandparent and arbitrary sibling factors as in Riedel and Smith (2010) and Martins et al.", "startOffset": 73, "endOffset": 97}, {"referenceID": 15, "context": "We add O(n3) secondorder grandparent and arbitrary sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al.", "startOffset": 73, "endOffset": 123}, {"referenceID": 5, "context": ", 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse POS tags from Petrov et al.", "startOffset": 26, "endOffset": 84}, {"referenceID": 5, "context": ", 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse POS tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Attenberg et al.", "startOffset": 26, "endOffset": 204}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data.", "startOffset": 115, "endOffset": 139}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data.", "startOffset": 115, "endOffset": 173}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train an (exact) first-order model and for each token prune any parents for which the marginal probability is less than 0.", "startOffset": 115, "endOffset": 372}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train an (exact) first-order model and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token.8 On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009). The pruning model uses a simpler feature set as in Rush and Petrov (2012).", "startOffset": 115, "endOffset": 677}, {"referenceID": 11, "context": "Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train an (exact) first-order model and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token.8 On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009). The pruning model uses a simpler feature set as in Rush and Petrov (2012).", "startOffset": 115, "endOffset": 752}, {"referenceID": 4, "context": "Data We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al.", "startOffset": 49, "endOffset": 75}, {"referenceID": 16, "context": "Data We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks.", "startOffset": 91, "endOffset": 111}, {"referenceID": 4, "context": "Data We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks. We also convert the English Penn Treebank (PTB) (Marcus et al., 1993) to dependencies using the head rules from Yamada and Matsumoto (2003) (PTB-YM).", "startOffset": 50, "endOffset": 266}, {"referenceID": 6, "context": "For CLL only, we projectivize the training trees following (Carreras, 2007) by finding the maximum projective spanning tree under an oracle model which assigns score +1 to edges in the gold tree and 0 to the others.", "startOffset": 59, "endOffset": 75}, {"referenceID": 21, "context": "Our goal is to demonstrate that our approximationaware training method leads to improved parser accuracy as compared with the standard training approach of conditional log-likelihood (CLL) maximization (Smith and Eisner, 2008), which does not take inexact inference into account.", "startOffset": 202, "endOffset": 226}, {"referenceID": 21, "context": "We applied these methods to a basic higher-order dependency parsing model, because that was the simplest and first (Smith and Eisner, 2008) instance of structured BP.", "startOffset": 115, "endOffset": 139}, {"referenceID": 21, "context": "Prior work has used this structured form of BP to do dependency parsing (Smith and Eisner, 2008), CNF grammar parsing (Naradowsky et al.", "startOffset": 72, "endOffset": 96}, {"referenceID": 15, "context": "Prior work has used this structured form of BP to do dependency parsing (Smith and Eisner, 2008), CNF grammar parsing (Naradowsky et al., 2012), TAG (Auli and Lopez, 2011), ITGconstraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009).", "startOffset": 118, "endOffset": 143}, {"referenceID": 1, "context": ", 2012), TAG (Auli and Lopez, 2011), ITGconstraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009).", "startOffset": 13, "endOffset": 35}, {"referenceID": 5, "context": ", 2012), TAG (Auli and Lopez, 2011), ITGconstraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009).", "startOffset": 74, "endOffset": 99}, {"referenceID": 8, "context": ", 2012), TAG (Auli and Lopez, 2011), ITGconstraints for phrase extraction (Burkett and Klein, 2012), and graphical models over strings (Dreyer and Eisner, 2009).", "startOffset": 135, "endOffset": 160}], "year": 2015, "abstractText": "We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n) runtime. It outputs the parse with maximum expected recall\u2014but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by backpropagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as Stoyanov et al. (2011) and Domke (2010) did for loopy CRFs. The resulting trained parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood.", "creator": "LaTeX with hyperref package"}}}