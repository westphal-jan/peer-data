{"id": "1508.03812", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "Causal Decision Trees", "abstract": "Uncovering causal relationships in data is a major objective of data analytics. Causal relationships are normally discovered with designed experiments, e.g. randomised controlled trials, which, however are expensive or infeasible to be conducted in many cases. Causal relationships can also be found using some well designed observational studies, but they require domain experts' knowledge and the process is normally time consuming. Hence there is a need for scalable and automated methods for causal relationship exploration in data. Classification methods are fast and they could be practical substitutes for finding causal signals in data. However, classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones. In this paper, we develop a causal decision tree where nodes have causal interpretations. Our method follows a well established causal inference framework and makes use of a classic statistical test. The method is practical for finding causal signals in large data sets.", "histories": [["v1", "Sun, 16 Aug 2015 11:31:49 GMT  (3399kb,D)", "http://arxiv.org/abs/1508.03812v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jiuyong li", "saisai ma", "thuc duy le", "lin liu", "jixue liu"], "accepted": false, "id": "1508.03812"}, "pdf": {"name": "1508.03812.pdf", "metadata": {"source": "CRF", "title": "Causal Decision Trees", "authors": ["Jiuyong Li", "Saisai Ma", "Thuc Duy Le", "Lin Liu", "Jixue Liu"], "emails": [], "sections": [{"heading": null, "text": "Keywords\u2014Decision tree, Causal relationship, Potential outcome model, Partial association\nI. INTRODUCTION Detecting causal relationships in data is an important data analytics task as causal relationships can provide better insights into data, as well as actionable knowledge for correct decision making and timely intervening in processes at risk.\nCausal relationships are normally identified with experiments, such as randomised controlled trials [19], which are effective but expensive and often impossible to be conducted. Causal relationships can also be found by observational studies, such as cohort studies and case control studies [18]. An observational study takes a causal hypothesis and tests it using samples selected from historical data or collected passively over the period of time when observing the subjects of interest. Therefore observational studies need domain experts\u2019 knowledge and interactions in data selection or collection and the process is normally time consuming.\nCurrently there is a lack of scalable and automated methods for causal relationship exploration in data. These methods should be able to find causal signals in data without requiring domain knowledge or any hypothesis established beforehand. The methods must also be efficient to deal with the increasing amount of big data.\nClassification methods are fast and have the potential to become practical substitutes for finding causal signals in data since the discovery of causal relationships is a type of supervised learning when the target or outcome variable is fixed. Decision trees [9] are a good example of classification methods, and they have been widely used in many areas, including social and medical data analyses.\nHowever, classification methods are not designed with causal discovery in mind and a classification method may find false causal signals in data and miss true causal signals. For example, Figure 1 shows a decision tree built from a hypothesised data set of the recovery of a disease. Based on\nthe decision tree, we may conclude that the use of Tinder (a matchmaking mobile app) helps cure the disease. However, it is misleading since the majority of people using Tinder are young whereas most people not using Tinder are old. Young people will recover from the disease anyway and old people have a lower chance of recovery. This misleading decision tree is caused by an unfair comparison between the two different groups of people. It may be a good classification tree to predict the likelihood of recovery, but it does not imply the causes of recovery and its nodes do not have any causal interpretation.\nA classification method fails to take account of the effects of other variables on the class or outcome variable when examining the relationship between a variable and the class\nvariable, and this is the major reason for the false discoveries (of causal relationships). For example, when we study the relationship between using Tinder and the recovery of a disease, the effect of other variables such as age, gender, and health condition of patients (who may or may not use Tinder) should be considered. The objective is not simply to maximise the difference of the conditional probabilities of recovered and not recovered conditioning on the use of Tinder when a classifier is being sought.\nIn this paper, we design a causal decision tree (CDT) where nodes have causal interpretations. As presented in the following sections, our method follows a well established causal inference framework, the potential outcome model, and it makes use of a classic statistical test, Mantel-Haenszel test. The proposed CDT is practical for uncovering causal signals in large data.\nThe paths in a CDT are not interpreted as \u201cif - then\u201d first order logic rules as in a normal decision tree. For example, Figure 4 (left) shows a CDT learnt from the Titanic data set. It does not read as \u201cif female = n then survived = n\u201d. A node in a CDT indicates a causal factor of the outcome attribute. The node \u2018female\u2019 indicates that being female or not is causally related to survived or not; the node \u2018thirdClass\u2019 shows that in the female group (the context), staying in a third class cabin or not is causally related to survived or not.\nThe main contributions of this paper are as follows:\n\u2022 We systematically analyse the limitations of decision trees for causal discovery and identify the underlying reasons. \u2022 We propose the CDT method, which can be used to represent and identify simple and interpretable causal relationships in data, including big data.\nar X\niv :1\n50 8.\n03 81\n2v 1\n[ cs\n.A I]\n1 6\nA ug\n2 01\n5"}, {"heading": "II. CAUSE AND EFFECT IN THE POTENTIAL OUTCOME FRAMEWORK", "text": "Let X be a predictive attribute and Y the outcome attribute where x \u2208 {0, 1} and y \u2208 R\u22650 We aim to find out if there is a causal relationship between X and Y . For easy discussion, we consider that X = 1 is a treatment and Y = 1 the recovery. We will establish if the treatment is effective for the recovery.\nThe potential outcome or counterfactual model [17], [12] is a well established framework for causal inference. Here we introduce the basic concepts of the model and a principle for estimating the average causal effect, mainly following the introduction in [13].\nWith the potential outcome model, an individual i in a population has two potential outcomes for a treatment X: Y 1i when taking the treatment and Y 0 i when not taking the treatment. We say that Y 1i is the potential outcome in the treatment state and Y 0i is the potential outcome in the control state. Then we have the following definition.\nDefinition 1 (Individual level causal effect (ICE)) The individual level causal effect is defined as the difference of two potential outcomes of an individual, i.e. \u03b4i = Y 1i \u2212 Y 0i .\nIn practice we can only find out one outcome Y 1i or Y 0 i since one person can be placed in either the treatment group (X = 1) or the control group (X = 0). One of the two potential outcomes has to be estimated. So the potential outcome model is also called counterfactual model. For example, we know that Mary has a headache (the outcome) and she did not take aspirin (the treatment), i.e. we know Y 0i . The question is what the outcome would be if Mary took aspirin one hour ago, i.e. we want to know Y 1i and to estimate the ICE of aspirin on Mary\u2019s condition (having headache or not).\nIf we had both Y 1i and Y 0 i of an individual we would aggregate the causal effects of individuals in a population to get the average causal effect as defined below, where E[.] stands for expectation operator in probability theory.\nDefinition 2 (Average causal effect (ACE)) The average causal effect of a population is the average of the individual level causal effects in the population, i.e. E[\u03b4i] = E[Y 1 i ]\u2212 E[Y 0i ].\nNote that i is kept in the above formula as other work in the counterfactual framework to indicate individual level heterogeneity of potential outcomes and causal effects.\nAssuming that \u03c0 proportion of samples take the treatment and (1\u2212\u03c0) proportion do not, and the sample size is large so the error caused by sampling is negligible, given a data set D, the ACE, E[\u03b4i] can be estimated as:\nED[\u03b4i] = \u03c0(ED[Y 1 i |Xi = 1]\u2212 ED[Y 0i |Xi = 1]) +\n(1\u2212 \u03c0)(ED[Y 1i |Xi = 0]\u2212 ED[Y 0i |Xi = 0]) (1) That is, the ACE of the population is the ACE in the treatment group plus the ACE in the control group, where Xi = 1 indicates that an individual takes the treatment and the causal effect is (Y 1i |Xi = 1) \u2212 (Y 0i |Xi = 1). Similarly, Xi = 0 indicates that an individual does not take the treatment and the causal effect is (Y 1i |Xi = 0)\u2212 (Y 0i |Xi = 0).\nIn a data set, we can observe the potential outcomes in the treatment state for those treated, (Y 1i |Xi = 1), and the potential outcomes in the control state for those not treated, (Y 0i |Xi = 0). However, we cannot observe the potential outcomes in the\ncontrol state for those treated, (Y 0i |Xi = 1), or the potential outcomes in the treatment state for those not treated, (Y 1i |Xi = 0). We have to estimate what the potential outcome, (Y 0i |Xi = 1), would be if an individual did not take the treatment (in fact she has); and what potential outcome, (Y 1i |Xi = 0), would be if an individual took the treatment (in fact she has not).\nWith a data set D we can obtain the following \u201cna\u0131\u0308ve\u201d estimation of the ACE:\nEnaiveD [\u03b4i] = ED[Y 1 i |Xi = 1]\u2212 ED[Y 0i |Xi = 0] (2)\nThe question is when the na\u0131\u0308ve estimation (Equation (2)) will approach the true estimation (Equation (1)).\nIf the assignment of individuals to the treatment and control groups is purely random, the estimation in Equation (2) approaches the estimation in Equation (1). In an observational data set, however, the random assignment is not possible. How can we estimate the average causal effect? A solution is by perfect stratification. Let the differences of individuals in a data set be characterised by a set of attributes S (excluding X and Y ) and let the data set be perfectly stratified by S. In each stratum, apart from the fact of taking treatment or not, all individuals are indistinguishable from each other. Under the perfect stratification assumption, we have:\nE[Y 1i |Xi = 0,S = si] = E[Y 1i |Xi = 1,S = si] (3) E[Y 0i |Xi = 1,S = si] = E[Y 0i |Xi = 0,S = si] (4)\nwhere S = si indicates a stratum of perfect stratification. Since individuals are indistinguishable in the stratum, unobserved potential outcomes can be estimated by observed ones. Specifically, the mean potential outcome in the treatment state for those untreated is the same as that in the treatment state for those treated (Equation (3)), and the mean potential outcome in the control state for those treated is the same as that in the control state for those untreated (Equation (4)). By replacing Equation (1) with Equations (3) and (4), we have:\nED[\u03b4i|S = si] = \u03c0(ED[Y 1 i |Xi = 1,S = si]\u2212 ED[Y 0i |Xi = 1,S = si]) +\n(1\u2212 \u03c0)(ED[Y 1i |Xi = 0,S = si]\u2212 ED[Y 0i |Xi = 0,S = si]) = \u03c0(ED[Y 1 i |Xi = 1,S = si]\u2212 ED[Y 0i |Xi = 0,S = si]) +\n(1\u2212 \u03c0)(ED[Y 1i |Xi = 1,S = si]\u2212 ED[Y 0i |Xi = 0,S = si]) = ED[Y 1 i |Xi = 1,S = si]\u2212 ED[Y 0i |Xi = 0,S = si] = EnaiveD [\u03b4i|S = si] As a result, the na\u0131\u0308ve estimation approximates the true\naverage causal effect, and we have the following observation.\nObservation 1 [Principle for estimating average causal effect] The average causal effect can be estimated by taking weighted sum of na\u0131\u0308ve estimators in stratified sub data sets.\nThis principle ensures that each comparison is between individuals with no observable differences, and hence the estimated causal effect is not resulted from other factors than the studied one. In the following, we will use this principle to estimate causal effect in observational data sets."}, {"heading": "III. A DECISION TREE MAY NOT ENCODE CAUSAL RELATIONSHIPS", "text": "Decision trees are a popular classification model, with two types of nodes: branching and leaf nodes. A branching node represents a predictive attribute and each of its values denotes a choice and leads to another branching node or a leaf node\nrepresenting a class. Now we use the potential outcome model to explain why decision trees may not encode causality.\nExample 1 Given the data set and its corresponding decision tree as in Figure 2. The decision tree has perfectly explained the data set, but the tree does not represent causal relationships.\nPath A = 1\u2192 Y = 1, with the difference in probabilities, (prob(Y = 1|A = 1)\u2212 prob(Y = 0|A = 1)) = 1, represents a top quality discriminative rule. Let us assume that A = 1 is a treatment and Y represents the outcome. To derive the average causal effect of A on Y , we need to stratify the data set such that in each stratum the records are exchangeable. Here {B,C} are the stratifying attributes. The data set in Figure 2 is stratified into four strata and their summaries are as follows:\n{B,C} Y {0, 0} 1 0 A = 1 20 0 A = 0 0 0\n{B,C} Y {0, 1} 1 0 A = 1 10 0 A = 0 20 0\n{B,C} Y {1, 0} 1 0 A = 1 0 0 A = 0 0 20\n{B,C} Y {1, 1} 1 0 A = 1 10 0 A = 0 0 0\nIn the first stratum, all records have B = 0 and C = 0. There are no records from the control group (A = 0), hence we cannot estimate the average causal effect in this stratum. Similarly, we cannot estimate causal effects from the third (B = 1, C = 0) and fourth (B = 1, C = 1) strata. In the second stratum (B = 0, C = 1), ED[Y 1|A = 1]\u2212ED[Y 0|A = 0] = 1 \u2212 1 = 0. All cases regardless they are treated or not treated have the same outcome. So the causal relationship between A and Z cannot be established.\nFor paths (A = 0, B = 1) \u2192 Y = 0 and (A = 0, B = 0) \u2192 Y = 1, let us try to establish a causal relationship between B and Y in the sub data set where A = 0.\nThe two strata by attribute C are summarised as: C Y 0 1 0\nB = 1 0 20 B = 0 0 0\nC Y 1 1 0 B = 1 0 0 B = 0 20 0\nIn the two strata (C=0 and C=1) there are only cases in either the treatment group (B=1) or the control group (B=0). There is no way to estimate the average causal effect, so we cannot establish a causal relationship between B and Y .\nIn this example, we see that a perfect decision tree does not indicate any causal relationship. In other words, in this data set, there is not enough evidence to support causal relationships."}, {"heading": "IV. FROM NORMAL DECISION TREES TO CAUSAL DECISION TREES", "text": "Let X = {X1, X2, . . . , Xm} be a set of predictive attributes where xi \u2208 {0, 1} for 1 \u2264 i \u2264 m, and Y be an outcome attribute where y \u2208 {0, 1}. Data set D contains n records taking various assignments of values for X and Y , each of which represents the record of an observation. Let us\nassume that X includes all the attributes for characterising an individual, and the data set is large and thus there is no bias in the sampling process."}, {"heading": "A. Why a decision tree may not encode causal relationships?", "text": "The construction of a decision tree follows a divide and conquer strategy, and the most important decision to be made in the construction is to choose which attribute as a branching node. Information gain, information gain ratio or Gini index can be used to choose a branching node [9]. These criteria have slight differences, but they all aimed at finding a discriminative attribute in the context.\nDefinition 3 (Discriminative attribute) Given a data set D\u2032, a discriminative attribute is the attribute Xi such that |prob(Y = 1|Xi = 1)\u2212 prob(Y = 0|Xi = 1)| is maximised.\nNote that D\u2032 is a sub data set (of D) defined by the attribute values in the prefix path of the current branching node under consideration. It is a context specific data set (see Section IV-C for details).\nIn the following, we will discuss why a decision tree may not represent causal relationships.\nFirstly, The objective of a discriminative attribute (maximising prob(Y = 1|Xi = 1) \u2212 prob(Y = 0|Xi = 1)) is different from that of a causal factor (having significant causal effect prob(Y = 1|Xi = 1)\u2212 prob(Y = 1|Xi = 0)).\nSecondly, the estimation of prob(Y =1|Xi=1)\u2212prob(Y = 0|Xi = 1) for choosing a discriminative attribute is based on the data set D\u2032, while the estimation of the causal effect of Xi on Y is based on the stratified data set DS=si to avoid unfair comparison. For example, let Xi be a treatment and Y the recovery, the comparison between individuals with and without treatments have to be in the same age and gender groups and have similar medical conditions. Otherwise, the comparison is meaningless. In other words, when a comparison is within a stratified data set, the effect of other attributes on Y is eliminated and hence the difference (prob(Y = 1|Xi = 1)\u2212prob(Y =0|Xi=1)) reflects the causal effect of Xi on Y .\nEssentially the main limitation of a decision tree is that it does not consider other attributes in determining a branching attribute. The choice of a branching attribute does not rely on the causal effect of the attribute on the outcome attribute.\nFollowing Observation 1, the estimation of causal effect should be based on stratified data where the difference of individuals in a stratum is eliminated."}, {"heading": "B. A measure for causal effect", "text": "Based on the previous discussion, to estimate the causal effect of a predictive attribute Q on the outcome Y , we stratify a data set using X\\{Q} so that within each stratum there is no observable difference among the records.\nA measure of average causal effect should be able to quantify the difference of outcomes in two groups (treatment and control). For binary outcomes, odds ratio [6] is suitable for measuring the difference of two outcomes. Let the following table summarise the statistics of stratum sk where S = sk.\nsk Y = 1 Y = 0 total Q = 1 n11k n12k n1k Q = 0 n21k n22k n2k\ntotal n.1k n.2k n..k\nThe odds ratio (measuring the difference of Y between groups Q = 1 and Q = 0) of the stratum k is\n(n11kn22k)/(n12kn21k) or equivalently, ln(n11k)+ln(n22k)\u2212 ln(n12k)\u2212 ln(n21k).\nA question is how to get the aggregated difference over all the strata of a data set. Partial association test [3] is a means to achieve this. Over all the r strata of a data set, the difference can be summarised as:\nPAMH(Q,Y ) = (| \u2211r k=1 n11kn22k\u2212n21kn12k n..k | \u2212 12 ) 2\u2211r\nk=1 n1.kn2.kn.1kn.2k n2..k(n..k\u22121)\n(5)\nThis is the test statistic of the Mantel-Haenszel test [11], [3]. The purpose of the test is to see if the association between Q and Y is consistent in all conditions (strata). Such consistency is a strong indication of direct causal relationship between two variables [3]. The test statistic has a Chi-square distribution (degree of freedom=1). Given a significance level \u03b1, if PAMH(Q,Y ) \u2265 \u03c72\u03b1, the null hypothesis that Q and Y are independent in all strata is rejected and the partial association between Q and Y is significant. An example of Mantel-Haenszel test is given in Example 2 in the next section."}, {"heading": "C. Causal decision trees", "text": "Our aim is to build a causal decision tree (CDT) where a non-leaf node represents a causal attribute, an edge denotes an assignment of a value of a causal attribute, and a leaf represents an assignment of a value of the outcome. A path from the root to a leaf represents a series of assignments of values of the attributes and a highly probable outcome value as the leaf.\nA CDT differs from a normal decision tree in that each of its non-leaf nodes has a causal interpretation with respect to the outcome, i.e. a non-leaf node and the outcome attribute have a context specific causal relationship as defined below. Definition 4 (Context) Let P \u2282 X, then a value assignment of P, P = p, is called a context and (D|P = p) is a context specific data set where P = p holds for all records in D.\nDefinition 5 (Context specific causal relationship) Let P = p be a context and Q be a predictive attribute and P\u2229{Q} = \u2205. Q and the outcome attribute Y have a context specific causal relationship if PAMH(Q,Y ) is greater than a threshold in the context specific data set (D|P = p).\nA context specific causal relationship between the root node and the outcome attribute of a CDT is global or context free, i.e. the context attribute set P is empty, and a context specific causal relationships between a non-root node A and the outcome Y is a refinement of the causal relationship between A\u2019s parent and Y . For example, with the CDT in Figure 4 (right), the causal relationship between the root \u2018age<30\u2019 and the outcome \u2018>50K\u2019 is context free, while the causal relationship \u2018education-num>12\u2019 having with the outcome is in the context of \u2018age<30\u2019 being no, which is a refinement of the causal relationship \u2018age<30\u2019 (parent of \u2018education-num>12\u2019) having with the outcome and is a more specific but stronger relationship. Definition 6 (Causal decision tree (CDT)) In a causal decision tree, a non-leaf node Q represents a context specific causal relationship between Q and the outcome attribute Y where the context is a series of value assignments of the attributes along the path from the root and to the parent of Q. A leaf node represents a value assignment of Y , which is the most probable value of Y in the context specific data set where the context is a series of value assignments of the attributes along the path from the root to the leaf.\nWe use the following example to show that a CDT encodes causal relationships.\nExample 2 From the data set shown in Figure 3, for path A = 1\u2192 Y = 1 of the CDT, we have the following summaries of the strata in terms of attributes {B,C}:\n{B,C} Y {0, 0} 1 0 A = 1 10 0 A = 0 10 0\n{B,C} Y {0, 1} 1 0 A = 1 0 5 A = 0 20 0\n{B,C} Y {1, 0} 1 0 A = 1 10 0 A = 0 0 10\n{B,C} Y {1, 1} 1 0 A = 1 15 0 A = 0 0 20\nWe now calculate the Mantel-Haenszel test statistic (Equation (5)), PAMH(A, Y ). The first table above (for the stratum B = 0, C = 0) does not contribute to the calculation of PAMH(A, Y ) since it has one column of zero values.\nIn the stratum B = 0 and C = 1, n11kn22k \u2212 n21kn12k\nn..k = 0 \u2217 0\u2212 20 \u2217 5 25 = \u22124\nn1.kn2.kn.1kn.2k n2..k(n..k \u2212 1) = (20 \u2217 5 \u2217 5 \u2217 20) 252(25\u2212 1) = 0.667\nSimilarly, we compute the intermediate results for strata (B = 1, C = 0) and (B = 1, C = 1), and obtain PAMH(A, Y ) = 17.5 > 3.84. So A and Y have a causal relationship based on the test (for \u03b1 = 0.05 or \u03c72\u03b1 = 3.84).\nIn the context A = 0, we test if B and Y have a causal relationship, based on the following summaries of the data set:\nC Y 0 1 0 B = 1 0 10 B = 0 10 0\nC Y 1 1 0 B = 1 0 20 B = 0 20 0\nFrom the above tables, we have PAMH(B,Y )=49>3.84 in the context specific data set for A=0. So we can conclude that B and Y have a causal relationship in the context of A=0."}, {"heading": "V. CAUSAL DECISION TREE ALGORITHM", "text": "Normal decision trees have the following advantages: (1) The divide and conquer strategy of decision tree induction is very efficient. A decision tree construction algorithm is scalable to the data set size and the number of attributes. This is a major advantage in exploring big data; (2) Decision trees explore both global and context specific relationships, and the latter provide refined explanations for the former. They jointly provide comprehensive explanations for a data set.\nTherefore in this paper we exploit these advantages for exploring causal relationships. However, the challenges for\nbuilding a CDT include: (1) The criterion for choosing a branching attribute for a normal decision tree needs to be replaced by a causality based criterion. In our algorithm, we make use of the Mantel-Haenszel test, a statistically sound method for testing causal signals; (2) The time complexity for Mantel-Haenszel test is quadratic to the size of a data set since all strata must be found in the first place. We use quick sort to facilitate the discovery of strata, which reduces the time complexity greatly.\nBased on the above discussion, we present the CDT construction algorithm as shown in Algorithm 1.\nThe algorithm takes 3 inputs: the data set D for a set of predictive attributes X and one outcome attribute Y ; user specified confidence level for Mantel-Haenszel test and correlation test (to find relevant attributes); and the maximum height of the CDT. Having a maximum tree height makes the tree more interpretable. If we do not restrict the tree height, we can get a context which includes many attributes, and a causal relationship in such a context only explains a very specific scenario and has less interest to users. In our experiments, however, the height of a tree is short after pruning even we set the maximum height of a tree high. So in practice it is not necessary to set the maximum height high.\nAlgorithm 1 firstly initiates the CDT (i.e. T), and sets the count of the height of the tree (i.e. h) as zero in Line 1. Then the functions TreeConstruct and TreePruning are called subsequently. Finally, the CDT is returned.\nThe treeConstruct function uses a recursive procedure to construct a CDT and it takes 5 inputs: current node N to be expanded or terminated; the set of attributes Z \u2286 X to expand the current subtree (whose root is N ) and Z contains only the attributes that have not been used in the tree; the context specific data set D\u2032, where the context is the value assignments along the path from the root to N (inclusive); h, the current height of the tree up to N ; and e, the label of the edge from N to the next node to be expanded.\nLines 1 to 4 of the treeConstruct function terminate N if there is no attribute left in Z and/or the depth of N reaches the maximum height of the tree. N is terminated by attaching to it a pair of leaves with edges of 1 and 0 respectively and labelling the leaves with the most probable values in (D\u2032|1) and (D\u2032|0) respectively.\nIf N is not to be terminated, Line 5 finds a set of attributes correlated with Y in the current context specific data set D\u2032. This correlated attribute set is used to stratify D\u2032 for the Mantel-Haenszel test. The reason for choosing a set of correlated attributes for stratification is discussed in Section VII-B.\nIn Lines 6 to 8, the partial association between Y and each attribute in Z is tested. The attribute, W that has the most significant partial association with Y (i.e. has the largest Mantel-Haenszel test statistic) is selected in Line 9. If the partial association between W and Y is insignificant, in Lines 10 to 13 we terminate N by attaching a pair of leaves with edges of 1 and 0 respectively and labelling the leaves with the most probable values in data sets (D\u2032|1) and (D\u2032|0) correspondingly. If the partial association is significant, W is a context specific cause of Y and W is added to the tree in one of the following two ways. If e = null, W is set as the root of tree T; otherwise, W is added as a child node of N and the edge between N and W is labelled as e. Line 19 removes W from Z so it will not be used in the subtree again. In Lines\nAlgorithm 1 Causal Decision Tree (CDT) Input: D, a data set for the set of predictor attributes X = {X1, X2, . . . , Xm} and the outcome attribute Y , hmax the maximum height the tree, and \u03b1, significance level for the Mantel-Haenszel (partial association) test and correlation test. Output: T, causal decision tree\n1: let T = \u2205 and h = 0 2: TreeConstruct(T , X, D, h, null) // T is the root of T 3: TreePruning(T) 4: return T\nTreeConstruct(N , Z, D\u2032, h, e) 1: if Z = \u2205 OR ++ h = hmax then 2: add two leaf nodes to N with edges e \u2208 {1, 0} and label each with\nthe most probable value of Y in (D\u2032|N = e) 3: return 4: end if 5: find a set of correlated attributes in Z with Y in D\u2032 6: for each correlated attribute Xi do 7: compute PAMH(Xi, Y ) in D\u2032 stratified by the remaining correlated attributes 8: end for 9: find attribute W with the highest partial association 10: if partial association between W with Y is insignificant then 11: add two leaf nodes to N with edges e \u2208 {1, 0} and label each with the most probable value of Y in (D\u2032|N = e) 12: return 13: end if 14: if e = null then 15: let node W be the root of T 16: else 17: add node W as a child node of N and label the edge between N and W as e 18: end if 19: remove W from Z 20: for each w \u2208 {0, 1} do 21: call TreeConstruct(W , Z, (D\u2032|W = w), h, w) 22: end for TreePruning(T)\n1: for each leaf in T do 2: if its sibling leaf has the same label of Y value as its then 3: change their parent node as a leaf node and label the leaf node with the common label 4: remove both leaves 5: end if 6: end for\n20 to 22, TreeConstruct is called recursively for W with the context specific data sets (D\u2032|W = w) where w \u2208 {0, 1}.\nThe TreePruning function prunes leaves that do not have distinct labels. The function back traces the tree from the leaf nodes. When two sibling leaves of a parent node share the same label, their parent is converted to a leaf node and is labelled with the same label as their children in Line 3. Both leaves are then pruned in Line 4.\nThe time complexity of Algorithm 1 mainly attributes to 3 factors: tree construction, forming strata, and causal tests.\nFor tree construction, at each split, firstly, we test the correlation of each (unused) attribute with Y , and the complexity is O(mn) where m is the number of predictive attributes and n is the number of samples in the given data set. Then Mantel-Haenszel tests with Y are conducted for all (relevant) attributes, and this is the most expensive part of the algorithm. For each test, the context specific data set D\u2032 is sorted and strata are found in the data set, which has a complexity of O(n log n), and for all the tests at a split, the complexity is O(mn log n). At most we have 2hmax splits. When hmax is not\nbig, it is a small number and let it be a constant ns. Therefore the time complexity for tree construction is O(mn log n). For tree pruning, the algorithm traverses the tree once and merge the leaves with the same labels under a branching node, which takes a constant time (proportional to ns).\nOverall the time complexity for building a CDT is O(mn log n)."}, {"heading": "VI. EXPERIMENTS", "text": "To evaluate CDT, three sets of experiments are conducted. Firstly we experiment with 2 real world and 1 synthetic data sets to show that CDT is able to identify more interpretable relationships when comparing to normal decision trees. The normal decision trees are built using the C4.5 algorithm [9] implemented in Weka [8]. It is difficult to evaluate discovered causal relationships as for most real world data sets we do not have the ground truths (true causal relationships). It is also impossible to use a method for evaluating classifiers to assess causal discovery results, because a model containing no causal relationships may give accurate classification, such as the decision tree in Figure 2. Thus we take a common sense approach to do the evaluation by using two data sets from which the results could make sense to ordinary people. We examine the results to see if they are reasonable, and contrast the CDTs to normal decision trees built based on the data.\nSecondly experiments with synthetic data sets are carried out to demonstrate the ability of CDT in finding causal relationships comparing to the commonly used Bayesian network learning method, the PC algorithm [21].\nFinally we evaluate the scalability of CDT using synthetic data sets in comparison to C4.5 and PC.\nIn all the experiments, the significance level for MantelHaenszel tests is 0.05, and the maximum level of CDTs is 5."}, {"heading": "A. CDT finds more interpretable relationships", "text": "1) The Titanic data set: This is a builtin data set in R (https://stat.ethz.ch/R-manual/Rdevel/library/datasets/html/Titanic.html). It has ticket and gender information of passengers and crew on board Titanic. The outcome attribute is \u2018survived\u2019 (or not). A summary of the data set (converted to a binary data set) is given in Table I. We try to establish relationships between \u2018survived\u2019 and the other attributes.\nThe CDT built from the data set is shown in Figure 4 (left). At the first level, the tree reveals a causal relationship between \u2018female\u2019 (gender) and \u2018survived\u2019. This relationship is sensible as we know that if someone was a female, she was likely to have higher priority to board the limited number of lifeboats. At the second level, the tree gives a context specific causal relationship between \u2018thirdClass\u2019 and \u2018survived\u2019 in the female group, which is reasonable too as passengers in the lowest class cabins would have less chance to escape. Therefore the\ntree is simple but it gives insights about the causes of surviving for people on Titanic, and the results are logic.\n2) Adult data set - census income: The Adult data set (Table I) was retrieved from the UCI Machine Learning Repository [2] and it is an extraction of 1994 USA census database. It is a well known classification data set to predict whether a person earns over 50K or not in a year. We recoded the data set to make the causes for high/low income more clearly and easily understandable. The objective is to find the causal factors of high (or low) income.\nFrom the CDT built with the data set (Figure 4, right), there is a causal relationship between \u2018age<30\u2019 (or not) and income, i.e. young adults have lower income, which follows common knowledge. For older adults, year of education is causally related to income, i.e. adults with education shorter than 12 years would have low income, which makes good sense too. For older and highly educated adults, gender affects income such that females have lower income than male (an unfortunate finding but it could be true in reality). In the highly educated and older male group, occupation is a causal factor of income so that those in professional occupations earn more than those not in professional occupations. We see that the CDT gives sensible explanations for the causes of high or low income.\nFrom the normal decision tree from the Adult data set (Figure 5), we have observed that: (1) A normal decision\nmay be large for high classification accuracy but a large tree reduces its interoperability. The objectives of causal discovery and classification may not be consistent; (2) Causality based and classification based criteria do not make the same choice. In the top level, the branching attribute of the normal decision tree is \u2018education-num>12\u2019 while the branching attribute of the CDT is \u2018age<30\u2019. From common knowledge, age should have stronger influence on income than years of education since young adults usually get lower income in most cases, simply because of their lack of working experience. Formally, there are more strata (13.3% of all strata) violating the causal relationship between \u2018education-num>12\u2019 and \u2018>50K\u2019 than those (7.75% of all strata) violating the causal relationship between \u2018age <30\u2019 and \u2018< 50K\u2019. This is why the CDT chooses \u2018age <30\u2019 as the root. In contrast, since attribute \u2018educationnum > 12\u2019 has a higher information gain than \u2018age <30\u2019 it is chosen to split the data set firstly in a normal decision tree. Different choices lead to different trees. In this example, the different choices do not cause significant difference as \u2018age <30\u2019 is chosen immediately after \u2018education-num > 12\u2019, but in other data sets, the difference could be significant. For causal discoveries, it is better to choose a causal based criterion.\nWith the above data sets, the first few levels at the top of a normal decision tree is quite interpretable since the causal relationships are evident in data. A CDT and a normal decision tree will be different when the causal relationships are subtle or with noises. To demonstrate this point, we build a CDT and a normal decision tree with a randomised data set where there is no relationship at all. Values in each of 10 attributes were randomly drawn with 50% 1s and 50% 0s in the data set. When we tried to learn a CDT from the data, no tree was returned and this is expected. However, C4.5 grew a decision tree as in Figure 6.\nThis result shows that the relationships in a normal decision tree may not be meaningful at all and a more interpretable decision tree, like a CDT, is necessary."}, {"heading": "B. CDT identifies causal relationships", "text": "1) Finding global causal relationships: To show that CDT is competent to discover causal relationships, we use 5 groups of synthetic data sets, each group containing 10 data sets with the same number of variables, to compare the findings of CDT and the PC algorithm from the data. In total 50 data sets are used, and each data set contains 10k samples.\nThe data sets are generated using the TETRAD tool (http://www.phil.cmu.edu/tetrad/). To create a data set, in TETRAD we firstly generate randomly a causal Bayesian network structure with the specified number of variables (20, 40, 60, 80, or 100), and randomly select a node with a specified degree (i.e. number of parent and children nodes, which is\nin the range of 3 to 7) as the outcome attribute for the data set. The conditional probability tables of the causal Bayesian network are also randomly assigned. The data set is then generated using the built-in Bayes Instantiated Model (Bayes IM) based on the conditional probability tables. The ground truth of the data is the set of nodes directly connected to the outcome variable in the causal Bayesian network structure.\nWe then apply CDT and PC to each of the 50 data sets, and for each group of the data sets, the average recalls of the algorithms are shown in Table II (Part A). It can be seen that in general CDT can detect similar percentages of causal relationships as PC does, indicating that CDT has comparable ability and has obtained consistent results in discovering causal relationships as the commonly used approach. We are aware that the causal relationships identified by CDT are context specific while those discovered by PC is global or context free. However, it is reasonable to assume that if a causal relationship exists with no context, it should appear in the contexts too, and these relationships have been mostly picked up by the CDTs.\n2) Finding context specific causal relationships: In order to test the performance of CDT in finding context specific causal relationships, we also use 5 groups of synthetic data sets, each group containing 10 data sets with the same number of variables (20, 40, 60, 80 or 100).\nTo create a data set, e.g. with 20 binary variables, {v1, v2, . . . , v20}, we firstly create a causal Bayesian network structure that contains only one edge, e.g. between v1 and v20, and all other nodes are isolated nodes. Based on this structure, we use logistic regression to simulate the data set for the Bayesian network. One of the two causally related variables, e.g. v20 is chosen as the outcome variable, then v1 in this example is the ground truth of the global causal node of v20. However, we do not know the ground truth of the context specific causal relationships around v20. Our solution is to use v1 as the context variable, and apply PC-select [23] (also known as PC-simple [16]) to the two partitions of the data set respectively, one partition containing all the samples with (v1 = 0) and one containing all the samples with (v1 = 1) (while the v1 column is excluded). In this way, we identify the variables that are causally related to v20 within each of the two contexts, (v1 = 0) and (v1 = 1), and use the findings as the ground truth of the context specific causal relationships around v20 in the data set. PC-select is a simplified version of the PC algorithm for finding causal relationships around a given outcome variable.\nWe then apply CDT to each of the 50 data sets generated. The CDT built from each of the data sets always has the\nnode that is causally related to the output attribute as its root, i.e. the CDT correctly finds the global causal relationship. Moreover, each of the CDTs also contains context specific causal relationships. We did not prune CDT trees in these data sets since some randomly generated data sets have skewed distribution, which makes the pruning too aggressive. We will design a pruning strategy for skewed data sets in future work. Table II (Part B) summarises the average recall of CDT in finding the context specific causal relationships. From the table, CDT is able to discover the majority of the context specific causal relationships. PC, in contrast, does not find any context specific causal relationships in the data sets since it is not design for the purpose. If we want to use PC to find the context specific causal relationships, we have to run PC in each context specific data set, which is impractical. On the other hand, CDT can find context specific causal relationships in the complete data sets."}, {"heading": "C. Scalability of the CDT algorithm", "text": "We test the scalability of the CDT algorithm by comparing it with the C4.5 [9] algorithm implemented in Weka [8] and the PC Algorithm [21].\nWe use 12 synthetic data sets generated with the same procedure as for generating the data sets in Section VI-B-1). To be fair among data sets, we chose the nodes with the same degree as the target variables. The comparisons were carried out using the same desktop computer (Quad core CPU 3.4 GHz and 16 GB of memory).\nThe comparison results are shown in Figure 7. The run time of CDT is almost linear to the size of the data sets and the number of attributes. It is less efficient than C4.5 but more efficient than PC. The results have shown that the proposed CDT is practical for high dimensional and large data sets."}, {"heading": "VII. DISCUSSIONS", "text": ""}, {"heading": "A. Difference from other causal trees", "text": "In this section, we will differentiate our CDTs from other causal trees derived from causal Bayesian networks, including the conditional probability table tree (CPT-tree) [4] and causal explanation tree [15].\nA causal Bayesian network (CBN) [20] consists of a causal structure of a directed acyclic graph (DAG), with nodes and arcs representing random variables and causal relationships between the variables respectively, and a joint probability distribution of the variables. Given the DAG of a CBN, the joint probability distribution can be represented by a set of conditional probabilities attached to the corresponding nodes (given their parents). A CBN provides a graphical visualisation of causal relationships, a reasoning machinery for deriving new knowledge (effects) when evidence (changes of causes) is fed into the given network; as well as a mechanism for learning causal relationships in observational data. In recent decades, CBNs have emerged, especially in the areas of machine learning and data mining, as a core methodology for causal discovery and inference in data.\nA CBN depicts the relationships of all attributes under consideration, and it can be complex when the number of attributes is more than just a few. For example, it takes some effort to understand the CBN in Figure 8 learnt from the Adult data set, even though there are only 14 attributes in the data set. A CBN does not give a simple model to explain the causes of an outcome as our CDT does.\nThe conditional probability table tree (CPT-tree) [4] is designed to summarise the conditional probability tables of a CBN for concise presentation and fast inference. An example of CPT-trees is shown in Figure 9. The probabilistic dependence relationships among the outcome Y and its parent nodes X1, X2 and X3 (causes of Y ) are specified by a conditional probability table where the probabilities of Y given all value assignments of its parents are listed. The size of a conditional probability table is exponential to the number of parent nodes of Y and can be very large. For example, for 20 parent nodes, the conditional probability table will have 1,048,576 rows. This table will be difficult to display and the inference based on the table is inefficient too. Given a context, i.e. one or more parent nodes taking an assignment of a value, the probability of Y may be constant (without being affected by the values of other parents). So a conditional probability table can be represented clearly with a tree structure, called a conditional probability table tree (CPT-tree), as illustrated in Figure 9. In the CPT-tree, the causal semantics is naturally linked to the CBN where all parent nodes are direct causes of variable Y .\nThere are two major differences between a CPT-tree and a CDT. Firstly, CPT-trees are built from CBNs and CDTs are built from data sets directly. Before building the CPT-trees,\nwe already know the causal relationships, and a CPT tree specifies how the assignments of some causal variables link to outcome values. This is impractical in many real world applications since we do not know the CBN or we could not build a CBN from a data set, particularly a large data set, as existing algorithms for learning CBNs cannot handle a large number of variables and they often only present a partially oriented CBN. Secondly, in a CBN, the parents of a node Y are all global causes of Y . As a CPT-tree is derived from a CBN, all the variables included in a CPT-tree are all global causes. However, as discussed previously, it is possible that under a context, a variable becomes causally related to Y . For example, in the Titanic data set, \u2018thirdClass\u2019 cabin is not a causal factor in the whole data set (i.e. it is not a global cause of \u2018survived\u2019), but it becomes a causal factor in the context of female passengers/crew. So such causal relationships will not be discovered or represented by a CBN and thus not by the CPT-trees too, but they can be be revealed and represented by our CDTs.\nA causal explanation tree [15] aims at explaining the outcome values using a series of value assignments of a subset of attributes in a CBN. A series of value assignments of attributes form a path of a causal explanation tree, and a path is determined by a causal information flow. The assignment of a set of attributes along a path represents an intervention in the causal inference in a CBN. The causal interpretation is based on the causal information flow criterion used for building a causal explanation tree. However this method is impractical since we do not have a CBN in most real world applications as explained previously. Similarly a causal explanation tree cannot capture the context specific causal relationships encoded in a CDT, because the explanation tree is obtained from a CBN, which only encodes global causes."}, {"heading": "B. Assumptions and practical considerations", "text": "Causal discovery is based on assumptions. In the causal Bayesian network discovery framework, some assumptions, such as causal Markov condition, faithfulness and causal sufficiency [21], are used to ensure the causal semantics of the discoveries. Simply speaking, Markov condition requires that every edge in a causal Bayesian network implies a probabilistic dependence. The faithfulness assumption ensures that for two variables that are probabilistically dependent, there is a corresponding edge between the two variables in the causal Bayesian network. The causal sufficiency assumes that there is no unmeasured or hidden causes in data. Up to now, we have not explicitly discussed the causal assumptions. However, we do need certain assumptions which will be discussed in the following.\nThe causal interpretation of a CDT is ensured by the evaluation in the stratified data sets of the difference in the potential outcomes of a possible causal attribute Xi. In each\nstratum, the individuals are indistinguishable, or the attributes possibly affecting the outcome Y take the same values and they do not affect the estimation of the causal effect of Xi on Y . Therefore, the causal effect estimated using the stratified data sets approaches the true causal effect.\nAn assumption here is that the differences of individuals should be captured by the set of attributes used for stratification. This assumption implies causal sufficiency that all causes are measured and included in the data set. A na\u0131\u0308ve choice is to select all attributes other than the attribute being tested (Xi) and the outcome (Y ), for stratification. However, this is not workable for high dimensional data sets since many strata will contain very few or no samples when the number of attributes is large. As a result, the CDT algorithm may not find any causal relationship. For example, diverse information, such as demographic information, education, hobbies and liked movies, is collected as personal profile in a data set. However, if all the attributes are used for stratification, they reduce the chance of finding sizable strata for reliable discovery. In fact, it is unwise to use any irrelevant attributes, such as hobbies and liked movies, for stratification when the objective is to study, e.g. the causal effect of a treatment on a disease.\nA reasonable and practical choice of stratifying attributes is the set of attributes that may affect the outcome, called relevant attributes in this paper. Differences in irrelevant attributes that do not affect the outcome should not impact the estimation of the causal effect of the studied attribute on the outcome. For example, different hobbies and liked movies may not affect the estimation of the causal effect of a treatment on a disease. Therefore, only those relevant attributes should be used for stratifying a data set, and this is what we have used in the algorithm for building a CDT. In case there are many relevant variables, which may result in many small strata, we restrict the maximum number of relevant variables to ten according the strength of correlation. The purpose of this work is to design a fast algorithm to find causal signals in data automatically without user interactions. We do tolerate certain false positives and expect that a real causal relationship will be refined by a dedicated follow-up observational study.\nIn many real world studies, the stratification is based on a limited number of demographic attributes, e.g. gender, age group and residential areas. Thinking about a heath study, it is very difficult to recruit volunteers with the same background (age, diet, education, etc.), and stratification on more than a few attributes is just impractical. Considering some stratifying attributes is better than considering none.\nCDTs help practitioners with the discovery of causal relationships in the following ways although it may not confirm causal relationships: (1) Because of stratification, many spurious relationships that are definitely not causal will be excluded from the resulting CDTs, so practitioners will have a smaller set of quality hypotheses for further studying; (2) Context specific causal relationships are more difficult to be observed than global causal relationships. CDTs are useful for practitioners to find hidden context specific causal hypotheses."}, {"heading": "VIII. RELATED WORK", "text": "Discovering causal relationships in passively observed data has attracted enormous research efforts in the past decades, due to the high cost, low efficiency and unknown feasibility of experiments based approaches, as well as the increasing availability of observational data. To the credit of the theo-\nretical development by a group of statisticians, philosophers and computer scientists, including Pearl [17], Spirtes, Glymour [21] and others, we have seen graphical causal models playing dominant role in causality discovery. Among these graphical models, causal Bayesian networks (CBNs) [20] have been the most developed and used one.\nMany algorithms have been developed for learning CBNs [14], [20]. However in general learning a complete CBN is NP-hard [5] and the methods are able to handle a CBN with only tens of variables, or hundreds if the causal relationships are sparse [20].\nConsequently, local causal relationship discovery around a given target (outcome) variable has been actively explored recently as in practice we are often more interested in knowing the direct causes or effects of a variable, especially in the early stage of investigations. The work presented in this paper is along the line of local causal discovery.\nExisting methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.\nThe CDT proposed in this paper belongs to the second category, as it takes advantage of decision tree induction and partial association tests. Comparing to other methods in the category, however, the proposed CDT approach is distinct because it is aimed at finding a sequence of causal factors (variables along the path from the root to a leaf of a CDT) where a preceding factor is a context under which the following factors can have impact on the target, while the other methods identify a set of causal factors each being a cause or an effect of the given target, and they only discover global causal relationships. However, in practice, a variable may not be a cause of another variable globally, but under certain context, it may affect other variables. A CDT provides a way to identify such context specific causal relationships. Additionally because a context specific causal relationship contains information about the conditions in which a causal relationship holds, such relationships are more prescriptive and actionable and thus are more suitable for decision support and action planning.\nIn terms of using decision trees as a means for causality investigation, except from the above mentioned method for identifying Markov blankets [7], most existing work takes decision trees as a tool for causal relationship representation and/or inference, assuming that the causal relationships are known in advance. Examples include the CPT-trees [4] and causal explanation tree [15] introduced in the Discussions section, which are both derived from a known causal Bayesian network. Unlike these trees, our CDT is mainly used as a tool for detecting causal relationships in data, without any assumption of known causal relationships."}, {"heading": "IX. CONCLUSION", "text": "In this paper, we have proposed causal decision trees (CDTs), a novel model for representing and discovering causal relationships in data.\nA CDT provides a compact and precise graphical representation of the causal relationships between a set of predicate attributes and an outcome attribute. The context specific causal relationships represented by a CDT are of great practical use and they are not encoded by existing causal models.\nThe algorithm developed for constructing a CDT utilises the divide and conquer strategy for building a normal decision tree and thus is fast and scalable to large data sets. The criterion used for selecting branching attributes of a CDT is based on the well established potential outcome model and partial association tests, ensuring the causal semantics of the tree.\nGiven the increasing availability of big data, we believe that the proposed CDTs will be a promising tool for automated discovery of causal relationships in big data, thus to support better decision making and action planning in various areas."}], "references": [{"title": "Local causal and Markov blanket induction for causal discovery and feature selection for classification part I: Algorithms and empirical evaluation", "author": ["C.F. Aliferis", "A. Statnikov", "I. Tsamardinos", "S. Mani", "X.D. Koutsoukos"], "venue": "J. of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "The detection of partial association, I: The 2\u00d72 Case", "author": ["M.W. Birch"], "venue": "J. of the R. Stat. Soc.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1964}, {"title": "Contextspecific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "In The 12th Conf. on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Large-sample learning of Bayesian networks is NP-hard", "author": ["D. Chickering", "D. Heckerman", "C. Meek"], "venue": "J. of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Statistical Methods for Rates and Proportions", "author": ["J.L. Fleiss", "B. Levin", "M.C. Paik"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Identifying Markov blankets with decision tree induction", "author": ["L. Frey", "D. Fisher", "I. Tsamardinos", "C. Aliferis", "A. Statnikov"], "venue": "In Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "The WEKA data mining software: An update", "author": ["Q. Hall", "et. al"], "venue": "SIGKDD Explorations,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Data Mining: Concepts and Techniques", "author": ["J. Han", "M. Kamber", "J. Pei"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Mining causal association rules", "author": ["J. Li", "T. Le", "L. Liu", "J. Liu", "Z. Jin", "B. Sun"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Statistical aspects of the analysis of data from retrospective studies of disease", "author": ["N. Mantel", "W. Haenszel"], "venue": "J. of the National Cancer Institute,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1959}, {"title": "Counterfactuals and Causal Inference: Methods and Principles for Social Research", "author": ["S. Morgan", "C. Winship"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Matching estimators of causal effects: Prospects and pitfalls in theory and practice", "author": ["S.L. Morgan", "D.J. Harding"], "venue": "Sociological Methods & Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Learning Bayesian Networks", "author": ["R.E. Neapolitan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Explanation Trees for Causal Bayesian Networks", "author": ["U.H. Nielsen", "J. philippe Pellet", "A. Elisseeff"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Variable selection for highdimensional linear models: partially faithful distributions and the PCsimple", "author": ["M.K.P. B\u00fcehlmann", "M. Maathuis"], "venue": "algorithm. Biometrika,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Design of Observational Studies", "author": ["P.R. Rosenbaum"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Experimental and Quasi-Experimental Designs for Generalized Causal Inference", "author": ["W.R. Shadish", "T.D. Thomas", "D.T. Campbell"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Introduction to causal inference", "author": ["P. Spirtes"], "venue": "J. of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Causation, Predication, and Search", "author": ["P. Spirtes", "C.C. Glymour", "R. Scheines"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Discovery of causal rules using partial association", "author": ["Z.Jin", "J. Li", "L. Liu", "T.D. Le", "B. Sun", "R. Wang"], "venue": "In Data Mining (ICDM),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "Causal relationships are normally identified with experiments, such as randomised controlled trials [19], which are effective but expensive and often impossible to be conducted.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Causal relationships can also be found by observational studies, such as cohort studies and case control studies [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "Decision trees [9] are a good example of classification methods, and they have been widely used in many areas, including social and medical data analyses.", "startOffset": 15, "endOffset": 18}, {"referenceID": 15, "context": "The potential outcome or counterfactual model [17], [12] is a well established framework for causal inference.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "The potential outcome or counterfactual model [17], [12] is a well established framework for causal inference.", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": "Here we introduce the basic concepts of the model and a principle for estimating the average causal effect, mainly following the introduction in [13].", "startOffset": 145, "endOffset": 149}, {"referenceID": 7, "context": "Information gain, information gain ratio or Gini index can be used to choose a branching node [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "For binary outcomes, odds ratio [6] is suitable for measuring the difference of two outcomes.", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "Partial association test [3] is a means to achieve this.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "This is the test statistic of the Mantel-Haenszel test [11], [3].", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "This is the test statistic of the Mantel-Haenszel test [11], [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "Such consistency is a strong indication of direct causal relationship between two variables [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "5 algorithm [9] implemented in Weka [8].", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "5 algorithm [9] implemented in Weka [8].", "startOffset": 36, "endOffset": 39}, {"referenceID": 19, "context": "Secondly experiments with synthetic data sets are carried out to demonstrate the ability of CDT in finding causal relationships comparing to the commonly used Bayesian network learning method, the PC algorithm [21].", "startOffset": 210, "endOffset": 214}, {"referenceID": 14, "context": "Our solution is to use v1 as the context variable, and apply PC-select [23] (also known as PC-simple [16]) to the two partitions of the data set respectively, one partition containing all the samples with (v1 = 0) and one containing all the samples with (v1 = 1) (while the v1 column is excluded).", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "5 [9] algorithm implemented in Weka [8] and the PC Algorithm [21].", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "5 [9] algorithm implemented in Weka [8] and the PC Algorithm [21].", "startOffset": 36, "endOffset": 39}, {"referenceID": 19, "context": "5 [9] algorithm implemented in Weka [8] and the PC Algorithm [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "Difference from other causal trees In this section, we will differentiate our CDTs from other causal trees derived from causal Bayesian networks, including the conditional probability table tree (CPT-tree) [4] and causal explanation tree [15].", "startOffset": 206, "endOffset": 209}, {"referenceID": 13, "context": "Difference from other causal trees In this section, we will differentiate our CDTs from other causal trees derived from causal Bayesian networks, including the conditional probability table tree (CPT-tree) [4] and causal explanation tree [15].", "startOffset": 238, "endOffset": 242}, {"referenceID": 18, "context": "A causal Bayesian network (CBN) [20] consists of a causal structure of a directed acyclic graph (DAG), with nodes and arcs representing random variables and causal relationships between the variables respectively, and a joint probability distribution of the variables.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "The conditional probability table tree (CPT-tree) [4] is designed to summarise the conditional probability tables of a CBN for concise presentation and fast inference.", "startOffset": 50, "endOffset": 53}, {"referenceID": 13, "context": "A causal explanation tree [15] aims at explaining the outcome values using a series of value assignments of a subset of attributes in a CBN.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "In the causal Bayesian network discovery framework, some assumptions, such as causal Markov condition, faithfulness and causal sufficiency [21], are used to ensure the causal semantics of the discoveries.", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "retical development by a group of statisticians, philosophers and computer scientists, including Pearl [17], Spirtes, Glymour [21] and others, we have seen graphical causal models playing dominant role in causality discovery.", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "retical development by a group of statisticians, philosophers and computer scientists, including Pearl [17], Spirtes, Glymour [21] and others, we have seen graphical causal models playing dominant role in causality discovery.", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "Among these graphical models, causal Bayesian networks (CBNs) [20] have been the most developed and used one.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "Many algorithms have been developed for learning CBNs [14], [20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "Many algorithms have been developed for learning CBNs [14], [20].", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "However in general learning a complete CBN is NP-hard [5] and the methods are able to handle a CBN with only tens of variables, or hundreds if the causal relationships are sparse [20].", "startOffset": 54, "endOffset": 57}, {"referenceID": 18, "context": "However in general learning a complete CBN is NP-hard [5] and the methods are able to handle a CBN with only tens of variables, or hundreds if the causal relationships are sparse [20].", "startOffset": 179, "endOffset": 183}, {"referenceID": 14, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 222, "endOffset": 226}, {"referenceID": 19, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 286, "endOffset": 290}, {"referenceID": 0, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 322, "endOffset": 325}, {"referenceID": 20, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 617, "endOffset": 621}, {"referenceID": 8, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 626, "endOffset": 630}, {"referenceID": 5, "context": "Existing methods for local causal discovery around a given a target fall into two broad categories: (1) Methods that adapt the algorithms or ideas for learning a complete CBN into local causal discovery, such as PC-Simple [16], [23], a simplified version of the well-known PC algorithm [21] for CBN learning; and HITON-PC [1], which applies the basic idea of PC to find variables strongly (and causally) related to a given target; (2) Methods that are designed to exploit the high efficiency of popular data mining approaches and the causal discovery ability of traditional statistical methods, including the work in [22] and [10], both using association rule mining for identifying causal rules; and the decision tree based approach [7] for finding the Markov blanket of a given variable.", "startOffset": 734, "endOffset": 737}, {"referenceID": 5, "context": "In terms of using decision trees as a means for causality investigation, except from the above mentioned method for identifying Markov blankets [7], most existing work takes decision trees as a tool for causal relationship representation and/or inference, assuming that the causal relationships are known in advance.", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Examples include the CPT-trees [4] and causal explanation tree [15] introduced in the Discussions section, which are both derived from a known causal Bayesian network.", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "Examples include the CPT-trees [4] and causal explanation tree [15] introduced in the Discussions section, which are both derived from a known causal Bayesian network.", "startOffset": 63, "endOffset": 67}], "year": 2015, "abstractText": "Uncovering causal relationships in data is a major objective of data analytics. Causal relationships are normally discovered with designed experiments, e.g. randomised controlled trials, which, however are expensive or infeasible to be conducted in many cases. Causal relationships can also be found using some well designed observational studies, but they require domain experts\u2019 knowledge and the process is normally time consuming. Hence there is a need for scalable and automated methods for causal relationship exploration in data. Classification methods are fast and they could be practical substitutes for finding causal signals in data. However, classification methods are not designed for causal discovery and a classification method may find false causal signals and miss the true ones. In this paper, we develop a causal decision tree where nodes have causal interpretations. Our method follows a well established causal inference framework and makes use of a classic statistical test. The method is practical for finding causal signals in large data sets. Keywords\u2014Decision tree, Causal relationship, Potential outcome model, Partial association", "creator": "TeX"}}}