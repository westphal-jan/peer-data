{"id": "1508.04112", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2015", "title": "Molding CNNs for text: non-linear, non-consecutive convolutions", "abstract": "The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.4% accuracy on the fine-grained sentiment classification task.", "histories": [["v1", "Mon, 17 Aug 2015 19:02:45 GMT  (523kb,D)", "https://arxiv.org/abs/1508.04112v1", null], ["v2", "Tue, 18 Aug 2015 02:52:40 GMT  (524kb,D)", "http://arxiv.org/abs/1508.04112v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["tao lei", "regina barzilay", "tommi s jaakkola"], "accepted": true, "id": "1508.04112"}, "pdf": {"name": "1508.04112.pdf", "metadata": {"source": "CRF", "title": "Molding CNNs for text: non-linear, non-consecutive convolutions", "authors": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "emails": ["tommi}@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep learning methods and convolutional neural networks (CNNs) among them have become de facto top performing techniques across a range of NLP tasks such as sentiment classification, question-answering, and semantic parsing. As methods, they require only limited domain knowledge to reach respectable performance with increasing data and computation, yet permit easy architectural and operational variations so as to fine tune them to specific applications to reach top performance. Indeed, their success is often contingent on specific architectural and operational choices.\n1Our code and data are available at https://github. com/taolei87/text_convnet\nCNNs for text applications make use of temporal convolution operators or filters. Similar to image processing, they are applied at multiple resolutions, interspersed with non-linearities and pooling. The convolution operation itself is a linear mapping over \u201cn-gram vectors\u201d obtained by concatenating consecutive word (or character) representations. We argue that this basic building block can be improved in two important respects. First, the power of n-grams derives precisely from multi-way interactions and these are clearly missed (initially) with linear operations on stacked n-gram vectors. Non-linear interactions within a local context have been shown to improve empirical performance in various tasks (Mitchell and Lapata, 2008; Kartsaklis et al., 2012; Socher et al., 2013). Second, many useful patterns are expressed as non-consecutive phrases, such as semantically close multi-word expressions (e.g.,\u201cnot that good\u201d, \u201cnot nearly as good\u201d). In typical CNNs, such expressions would have to come together and emerge as useful patterns after several layers of processing.\nWe propose to use a feature mapping operation based on tensor products instead of linear operations on stacked vectors. This enables us to directly tap into non-linear interactions between adjacent word feature vectors (Socher et al., 2013; Lei et al., 2014). To offset the accompanying parametric explosion we maintain a low-rank representation of the tensor parameters. Moreover, we show that this feature mapping can be applied to all possible non-consecutive n-grams in the sequence with an exponentially decaying weight depending on the length of the span. Owing to the low rank representation of the tensor, this operation can be performed efficiently in linear time with respect to the sequence length via dynamic programming. Similar to traditional convolution operations, our non-linear feature mapping can be applied successively at multiple levels. ar X iv :1\n50 8.\n04 11\n2v 2\n[ cs\n.C L\n] 1\n8 A\nug 2\n01 5\nWe evaluate the proposed architecture in the context of sentence sentiment classification and news categorization. On the Stanford Sentiment Treebank dataset, our model obtains state-of-theart performance among a variety of neural networks in terms of both accuracy and training cost. Our model achieves 51.2% accuracy on finegrained classification and 88.6% on binary classification, outperforming the best published numbers obtained by a deep recursive model (Tai et al., 2015) and a convolutional model (Kim, 2014). On the Chinese news categorization task, our model achieves 80.0% accuracy, while the closest baseline achieves 79.2%."}, {"heading": "2 Related Work", "text": "Deep neural networks have recently brought about significant advancements in various natural language processing tasks, such as language modeling (Bengio et al., 2003; Mikolov et al., 2010), sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; Ku\u0308chler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).\nOur model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014). Kalchbrenner et al. (2014) instead propose (dynamic) k-max pooling operation for modeling sentences. In addition, Kim (2014) combines CNNs of different filter widths and either static or fine-tuned word vectors. In contrast to the traditional CNN models, our method considers non-consecutive n-\ngrams thereby expanding the representation capacity of the model. Moreover, our model captures non-linear interactions within n-gram snippets through the use of tensors, moving beyond direct linear projection operator used in standard CNNs. As our experiments demonstrate these advancements result in improved performance."}, {"heading": "3 Background", "text": "Let x \u2208 RL\u00d7d be the input sequence such as a document or sentence. Here L is the length of the sequence and each xi \u2208 Rd is a vector representing the ith word. The (consecutive) n-gram vector ending at position j is obtained by simply concatenating the corresponding word vectors\nvj = [xj\u2212n+1;xj\u2212n+2; \u00b7 \u00b7 \u00b7 ;xj ]\nOut-of-index words are simply set to all zeros. The traditional convolution operator is parameterized by filter matrix m \u2208 Rnd\u00d7h which can be thought of as n smaller filter matrices applied to each xi in vector vj . The operator maps each ngram vector vj in the input sequence to m>vj \u2208 Rh so that the input sequence x is transformed into a sequence of feature representations,[\nm>v1, \u00b7 \u00b7 \u00b7 ,m>vL ] \u2208 RL\u00d7h\nThe resulting feature values are often passed through non-linearities such as the hyper-tangent (element-wise) as well as aggregated or reduced by \u201csum-over\u201d or \u201cmax-pooling\u201d operations for later (similar stages) of processing.\nThe overall architecture can be easily modified by replacing the basic n-gram vectors and the convolution operation with other feature mappings. Indeed, we appeal to tensor algebra to introduce a non-linear feature mapping that operates on nonconsecutive n-grams."}, {"heading": "4 Model", "text": "N-gram tensor Typical n\u2212gram feature mappings where concatenated word vectors are mapped linearly to feature coordinates may be insufficient to directly capture relevant information in the n\u2212gram. As a remedy, we replace concatenation with a tensor product. Consider a 3-gram (x1,x2,x3) and the corresponding tensor product x1 \u2297 x2 \u2297 x3. The tensor product is a 3-way array of coordinate interactions such that each ijk\nentry of the tensor is given by the product of the corresponding coordinates of the word vectors\n(x1 \u2297 x2 \u2297 x3)ijk = x1i \u00b7 x2j \u00b7 x3k\nHere \u2297 denotes the tensor product operator. The tensor product of a 2-gram analogously gives a two-way array or matrix x1\u2297x2 \u2208 Rd\u00d7d. The ngram tensor can be seen as a direct generalization of the typical concatenated vector2.\nTensor-based feature mapping Since each ngram in the sequence is now expanded into a high-dimensional tensor using tensor products, the set of filters are analogously maintained as highorder tensors. In other words, our filters are linear mappings over the higher dimensional interaction terms rather than the original word coordinates.\nConsider again mapping a 3-gram (x1,x2,x3) into a feature representation. Each filter is a 3-way tensor with dimensions d\u00d7 d\u00d7 d. The set of h filters, denoted as T , is a 4-way tensor of dimension d \u00d7 d \u00d7 d \u00d7 h, where each d3 slice of T represents a single filter and h is the number of such filters, i.e., the feature dimension. The resulting h\u2212dimensional feature representation z \u2208 Rh for the 3-gram (x1,x2,x3) is obtained by multiplying the filter T and the 3-gram tensor as follows. The lth coordinate of z is given by\nzl = \u2211 ijk Tijkl \u00b7 (x1 \u2297 x2 \u2297 x3)ijk\n= \u2211 ijk Tijkl \u00b7 x1i \u00b7 x2j \u00b7 x3k (1)\nThe formula is equivalent to summing over all the third-order polynomial interaction terms where tensor T stores the coefficients.\nDirectly maintaining the filters as full tensors leads to parametric explosion. Indeed, the size of the tensor T (i.e. h\u00d7 dn) would be too large even for typical low-dimensional word vectors where, e.g., d = 300. To this end, we assume a low-rank factorization of the tensor T, represented in the Kruskal form. Specifically, T is decomposed into a sum of h rank-1 tensors\nT = h\u2211 i=1 Pi \u2297Qi \u2297Ri \u2297Oi\n2To see this, consider word vectors with a \u201cbias\u201d term xi\n\u2032 = [xi; 1]. The tensor product of n such vectors includes the concatenated vector as a subset of tensor entries but, in addition, contains all up to nth-order interaction terms.\nwhere P,Q,R \u2208 Rh\u00d7d and O \u2208 Rh\u00d7h are four smaller parameter matrices. Pi (similarly Qi, Ri and Oi) denotes the ith row of the matrix. Note that, for simplicity, we have assumed that the number of rank-1 components in the decomposition is equal to the feature dimension h. Plugging the low-rank factorization into Eq.(1), the featuremapping can be rewritten in a vector form as\nz = O> (Px1 Qx2 Rx3) (2)\nwhere is the element-wise product such that, e.g., (a b)k = ak \u00d7 bk for a, b \u2208 Rm. Note that while Px1 (similarly Qx2 and Rx3) is a linear mapping from each word x1 (similarly x2 and x3) into a h-dimensional feature space, higher order terms arise from the element-wise products.\nNon-consecutive n-gram features Traditional convolution uses consecutive n-grams in the feature map. Non-consecutive n-grams may nevertheless be helpful since phrases such as \u201cnot good\u201d, \u201cnot so good\u201d and \u201cnot nearly as good\u201d express similar sentiments but involve variable spacings between the key words. Variable spacings are not effectively captured by fixed n-grams.\nWe apply the feature-mapping in a weighted manner to all n-grams thereby gaining access to patterns such as \u201cnot ... good\u201d. Let z[i, j, k] \u2208 Rh denote the feature representation corresponding to a 3-gram (xi,xj ,xk) of words in positions i, j, and k along the sequence. This vector is calculated analogously to Eq.(2),\nz[i, j, k] = O> (Pxi Qxj Rxk)\nWe will aggregate these vectors into an h\u2212dimensional feature representation at each position in the sequence. The idea is similar to neural bag-of-words models where the feature representation for a document or sentence is obtained by averaging (or summing) of all the word vectors. In our case, we define the aggregate representation z3[k] in position k as the weighted sum of all 3-gram feature representations ending at position k, i.e.,\nz3[k] = \u2211\ni<j<k\nz[i, j, k] \u00b7 \u03bb(k\u2212j\u22121)+(j\u2212i\u22121)\n= \u2211\ni<j<k\nz[i, j, k] \u00b7 \u03bbk\u2212i\u22122 (3)\nwhere \u03bb \u2208 [0, 1) is a decay factor that downweights 3-grams with longer spans (i.e., 3-grams\nthat skip more in-between words). As \u03bb \u2192 0 all non-consecutive 3-grams are omitted, z3[k] = z[k \u2212 2, k \u2212 1, k], and the model acts like a traditional model with only consecutive n-grams. When \u03bb > 0, however, z3[k] is a weighted average of many 3-grams with variable spans.\nAggregating features via dynamic programming Directly calculating z3[\u00b7] according to Eq.(3) by enumerating all 3-grams would require O(L3) feature-mapping operations. We can, however, evaluate the features more efficiently by relying on the associative and distributive properties of the feature operation in Eq.(2).\nLet f3[k] be a dynamic programming table representing the sum of 3-gram feature representations before multiplying with matrix O. That is, z3[k] = O >f3[k] or, equivalently,\nf3[k] = \u2211\ni<j<k\n\u03bbk\u2212i\u22122 \u00b7 (Pxi Qxj Rxk)\nWe can analogously define f1[i] and f2[j] for 1- grams and 2-grams,\nf1[i] = Pxi f2[j] = \u2211 i<j \u03bbj\u2212i\u22121 \u00b7 (Pxi Qxj)\nThese dynamic programming tables can be calculated recursively according to the following formulas:\nf1[i] = Pxi s1[i] = \u03bb \u00b7 s1[i\u2212 1] + f1[i]\nf2[j] = s1[j \u2212 1] Qxj s2[j] = \u03bb \u00b7 s2[j \u2212 1] + f2[j]\nf3[k] = s2[k \u2212 1] Rxk\nz[k] = O> (f1[k] + f2[k] + f3[k])\nwhere s1[\u00b7] and s2[\u00b7] are two auxiliary tables. The resulting z[\u00b7] is the sum of 1, 2, and 3-gram features. We found that aggregating the 1,2 and 3- gram features in this manner works better than using 3-gram features alone. Overall, the n-gram feature aggregation can be performed in O(Ln) matrix multiplication/addition operations, and remains linear in the sequence length.\nThe overall architecture The dynamic programming algorithm described above maps the original input sequence to a sequence of feature representations z = z[1 : L] \u2208 RL\u00d7h. As in standard convolutional architectures, the resulting sequence can be used in multiple ways. One can directly aggregate it to a classifier or expose it to non-linear element-wise transformations and use it as an input to another sequence-to-sequence feature mapping.\nThe simplest strategy (adopted in neural bagof-words models) would be to average the feature representations and pass the resulting averaged vector directly to a softmax output unit\nz\u0304 = 1\nL L\u2211 i=1 z[i]\ny\u0303 = softmax ( W>z\u0304 ) Our architecture, as illustrated in Figure 1, includes two additional refinements. First, we add a non-linear activation function after each feature representation, i.e. z\u2032 = ReLU (z + b), where b is a bias vector and ReLU is the rectified linear unit function. Second, we stack multiple tensorbased feature mapping layers. That is, the input sequence x is first processed into a feature sequence and passed through the non-linear transformation to obtain z(1). The resulting feature sequence z(1) is then analogously processed by another layer, parameterized by a different set of feature-mapping matrices P, \u00b7 \u00b7 \u00b7 ,O, to obtain a higher-level feature sequence z(2), and so on. The output feature representations of all these layers are averaged within each layer and concatenated as shown in Figure 1. The final prediction is therefore obtained on the basis of features across the levels."}, {"heading": "5 Learning the Model", "text": "Following standard practices, we train our model by minimizing the cross-entropy error on a given training set. For a single training sequence x and the corresponding gold label y \u2208 [0, 1]m, the error is defined as,\nloss (x, y) = m\u2211 l=1 yl log (y\u0303l)\nwhere m is the number of possible output label. The set of model parameters (e.g. P, \u00b7 \u00b7 \u00b7 ,O in each layer) are updated via stochastic gradient\ndescent using AdaGrad algorithm (Duchi et al., 2011).\nInitialization We initialize matrices P,Q,R from uniform distribution [ \u2212 \u221a 3/d, \u221a 3/d ] and\nsimilarly O \u223c U [ \u2212 \u221a 3/h, \u221a 3/h ] . In this way,\neach row of the matrices is an unit vector in expectation, and each rank-1 filter slice has unit variance as well,\nE [ \u2016Pi \u2297Qi \u2297Ri \u2297Oi\u20162 ] = 1\nIn addition, the parameter matrix W in the softmax output layer is initialized as zeros, and the bias vectors b for ReLU activation units are initialized to a small positive constant 0.01.\nRegularization We apply two common techniques to avoid overfitting during training. First, we add L2 regularization to all parameter values with the same regularization weight. In addition, we randomly dropout (Hinton et al., 2012) units on the output feature representations z(i) at each level."}, {"heading": "6 Experimental Setup", "text": "Datasets We evaluate our model on sentence sentiment classification task and news categorization task. For sentiment classification, we use the Stanford Sentiment Treebank benchmark (Socher et al., 2013). The dataset consists of 11855 parsed English sentences annotated at both the root (i.e. sentence) level and the phrase level using 5-class fine-grained labels. We use the stan-\ndard 8544/1101/2210 split for training, development and testing respectively. Following previous work, we also evaluate our model on the binary classification variant of this benchmark, ignoring all neutral sentences. The binary version has 6920/872/1821 sentences for training, development and testing.\nFor the news categorization task, we evaluate on Sogou Chinese news corpora.3 The dataset contains 10 different news categories in total, including Finance, Sports, Technology and Automobile etc. We use 79520 documents for training, 9940 for development and 9940 for testing. To obtain Chinese word boundaries, we use LTP-Cloud4, an open-source Chinese NLP platform.\nBaselines We implement the standard SVM method and the neural bag-of-words model NBoW as baseline methods in both tasks. To assess the proposed tensor-based feature map, we also implement a convolutional neural network model CNN by replacing our filter with traditional linear filter. The rest of the framework (such as feature averaging and concatenation) remains the same.\nIn addition, we compare our model with a wide range of top-performing models on the sentence sentiment classification task. Most of these models fall into either the category of recursive neural networks (RNNs) or the category of convolutional neural networks (CNNs). The recursive neural\n3http://www.sogou.com/labs/dl/c.html 4http://www.ltp-cloud.com/intro/en/ https://github.com/HIT-SCIR/ltp\nnetwork baselines include standard RNN (Socher et al., 2011b), RNTN with a small core tensor in the composition function (Socher et al., 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al., 2015). These recursive models assume the input sentences are represented as parse trees. As a benefit, they can readily utilize annotations at the phrase level. In contrast, convolutional neural networks are trained on sequence-level, taking the original sequence and its label as training input. Such convolutional baselines include the dynamic CNN with k-max pooling DCNN (Kalchbrenner et al., 2014) and the convolutional model with multi-channel CNN-MC by Kim (2014). To leverage the phrase-level annotations in the Stanford Sentiment Treebank, all phrases and the corresponding labels are added as separate instances when training the sequence models. We follow this strategy and report results with and without phrase annotations.\nWord vectors The word vectors are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data (Turian et al., 2010). In particular, for the English sentiment classification task,\nwe use the publicly available 300-dimensional GloVe word vectors trained on the Common Crawl with 840B tokens (Pennington et al., 2014). This choice of word vectors follows most recent work, such as DAN (Iyyer et al., 2015) and RLSTM (Tai et al., 2015). For Chinese news categorization, there is no widely-used publicly available word vectors. Therefore, we run word2vec (Mikolov et al., 2013) to train 200-dimensional word vectors on the 1.6 million Chinese news articles. Both word vectors are normalized to unit norm (i.e. \u2016w\u201622 = 1) and are fixed in the experiments without fine-tuning.\nHyperparameter setting We perform an extensive search on the hyperparameters of our full model, our implementation of the CNN model (with linear filters), and the SVM baseline. For our model and the CNN model, the initial learning rate of AdaGrad is fixed to 0.01 for sentiment classification and 0.1 for news categorization, and the L2 regularization weight is fixed to 1e \u2212 5 and 1e\u22126 respectively based on preliminary runs. The rest of the hyperparameters are randomly chosen as follows: number of feature-mapping layers \u2208 {1, 2, 3}, n-gram order n \u2208 {2, 3}, hidden feature dimension h \u2208 {50, 100, 200}, dropout probability \u2208 {0.0, 0.1, 0.3, 0.5}, and length de-\ncay \u03bb \u2208 {0.0, 0.3, 0.5}. We run each configuration 3 times to explore different random initializations. For the SVM baseline, we tune L2 regularization weight C \u2208 {0.01, 0.1, 1.0, 10.0}, word cut-off frequency \u2208 {1, 2, 3, 5} (i.e. pruning words appearing less than this times) and n-gram feature order n \u2208 {1, 2, 3}.\nImplementation details The source code is implemented in Python using the Theano library (Bergstra et al., 2010), a flexible linear algebra compiler that can optimize userspecified computations (models) with efficient automatic low-level implementations, including (back-propagated) gradient calculation."}, {"heading": "7 Results", "text": ""}, {"heading": "7.1 Overall Performance", "text": "Table 1 presents the performance of our model and other baseline methods on Stanford Sentiment Treebank benchmark. Our full model obtains the highest accuracy on both the development and test sets. Specifically, it achieves 51.2% and 88.6% test accuracies on fine-grained and binary tasks respectively5. As shown in Table 2, our model performance is relatively stable \u2013 it remains high accuracies with around 0.5% standard deviation under different initializations and dropout rates.\nOur full model is also several times faster than other top-performing models. For example, the convolutional model with multi-channel (CNNMC) runs over 2400 seconds per training epoch. In contrast, our full model (with 3 feature layers) runs on average 28 seconds with only root labels and on average 445 seconds with all labels.\nOur results also show that the CNN model, where our feature map is replaced with traditional linear map, performs worse than our full model. This observation confirms the importance of the proposed non-linear, tensor-based feature mapping. The CNN model also lags behind the DCNN and CNN-MC baselines, since the latter two propose several advancements over standard CNN.\nTable 3 reports the results of SVM, NBoW and our model on the news categorization task. Since the dataset is much larger compared to the sentiment dataset (80K documents vs. 8.5K sentences), the SVM method is a competitive baseline. It achieves 78.5% accuracy compared to 74.4% and\n5Best hyperparameter configuration based on dev accuracy: 3 layers, 3-gram tensors (n=3), feature dimension d = 200 and length decay \u03bb = 0.5\n79.2% obtained by the neural bag-of-words model and CNN model. In contrast, our model obtains 80.0% accuracy on both the development and test sets, outperforming the three baselines by a 0.8% absolute margin. The best hyperparameter configuration in this task uses less feature layers and lower n-gram order (specifically, 2 layers and n = 2) compared to the sentiment classification task. We hypothesize that the difference is due to the nature of the two tasks: the document classification task requires to handle less compositions or context interactions than sentiment analysis."}, {"heading": "7.2 Hyperparameter Analysis", "text": "We next investigate the impact of hyperparameters in our model performance. We use the models trained on fine-grained sentiment classification task with only root labels.\nNumber of layers We plot the fine-grained sentiment classification accuracies obtained during hyperparameter grid search. Figure 2 illustrates how the number of feature layers impacts the model performance. As shown in the figure, adding higher-level features clearly improves the classification accuracy across various hyperparameter settings and initializations.\nNon-consecutive n-gram features We also analyze the effect of modeling non-consecutive n-\nTable 1 the movie is not good the 0.25 movie 0.25 is 0.08 not -1.91 good -1.00 the 0.25 movie 0.25 is 0.08 not -1.91 bad 1.99 the 0.25 movie 0.25 is 0.08 neither -1.99 good -0.95 nor -1.97 bad 1.75 the 0.25 movie 0.25 is 0.08 bad 12 the 0.25 movie 0.25 is 0.08 good 1.99\n-2 -1 0 1 2 the movie is not good -2 -1 0 1 2 the movie is not bad\n-2 -1 0 1 2 the movie is neither good nor bad -2 -1 0 1 2 the movie is bad -2 -1 0 1 2 the movie is good\nTable 1 the movie is not good the 0.25 movie 0.25 is 0.08 not -1.91 good -1.00 the 0.25 movie 0.25 is 0.08 not -1.91 bad 1.99 the 0.25 movie 0.25 is 0.08 neither -1.99 good -0.95 nor -1.97 bad 1.75 the 0.25 movie 0.25 is 0.08 bad 12 the 0.25 movie 0.25 is 0.08 good 1.99\n-2 -1 0 1 2 the movie s not good -2 -1 0 1 2 the movie s not bad\n-2 -1 0 1 2 the movie is neither good nor bad -2 -1 0 1 2 the movie s bad -2 -1 0 1 2 the movie s good\nTable 1 the movie is not good the 0.25 movie 0.25 is 0.08 not -1.91 good -1.00 the 0.25 movie 0.25 is 0.08 not -1.91 bad 1.99 the 0.25 movie 0.25 is 0.08 neither -1.99 good -0.95 nor -1.97 bad 1.75 the 0.25 movie 0.25 is 0.08 bad 12 the 0.25 movie 0.25 is 0.08 good 1.99 okay 10.34 but 0 not 10.51 good 10.04\n-2 -1 0 1 2 the movi is not good -2 -1 0 1 2 the movie is not bad\n-2 -1 0 1 2 the movie is neither good nor bad -2 -1 0 1 2 the movie is bad -2 -1 0 1 2 the movie is good -2 -1 0 1 2 okay but not good\nTable 1 the movie is not good the 0.25 movie 0.25 is 0.08 not -1.91 good -1.00 the 0.25 movie 0.25 is 0.08 not -1.91 bad 1.99 the 0.25 movie 0.25 is 0.08 neither -1.99 good -0.95 nor -1.97 bad 1.75 the 0.25 movie 0.25 is 0. 8 bad 12 the 0.25 movie 0.25 is 0. 8 good 1.99 okay 10.34 but 0 not 10.51 good 10. 4\n-2 -1 0 1 2 the movie is not good -2 -1 0 1 2 the movie is not bad\n-2 -1 0 1 2 the movie is neither good nor bad -2 -1 0 1 2 the movie is bad -2 -1 0 1 2 the movie is good -2 -1 0 1 2 okay but not good\n(1) positive prediction (2) negative prediction (3) negative prediction (4) positive prediction\nTable 1 the movie is not good the 0.25 movie 0.25 is 0.08 not -1.91 good -1.00 the 0.25 movie 0.25 is 0.08 not -1.91 bad 1.99 the 0.25 movie 0.25 is 0.08 neither -1.99 good -0.95 nor -1.97 bad 1.75 the 0.25 movie 0.25 is 0.08 bad 12 the 0.25 movie 0.25 is 0.08 good 1.99\nokay 10.34\nbut 0\nnot 10.51\ngood 10.04\n-2 -1 0 1 2 the movie is not good -2 -1 0 1 2 the movie is not bad\n-2 -1 0 1 2 the movie is neither good nor bad -2 -1 0 1 2 the movie is bad -2 -1 0 1 2 the movie is good -2 -1 0 1 2 okay but not good\nno #1.56\nmovement #0.81\n, 0.09\nno #1.97\nyuks 0.08\n, 0.46\nnot #0.58\nmuch #0.85\nof #1.01\nanything #1.43\n. #1.01\n-2 -1 0 1 2\nno movement , no yuks , not much of anything .\n(5) negative prediction (6) negative prediction (ground truth: negative)\ntoo #0.94 bad #1.99 , #0.88 but 0.06 thanks 1.39 to 1.16 some 1.07 lovely 1.93 .. 1.98 .. 1.79 and 1.91 several 1.77 fine 1.98 .. 2 , 2 it 0.42 \u2018s #0.72 not #0.99 a #1.02 total 0.7 loss 1.32 . 0.97\n-2\n-1\n0\n1\n2\ntoo bad , but thanks to some lovely .. .. and several fine .. , it \u2018s not a total loss .\n(7) positive prediction (ground truth: positive)\nFigure 5: Example sentences and their sentiments predicted by our model trained with root labels. The predicted sentiment scores at each word position are plotted. Examples (1)-(5) are synthetic inputs, (6) and (7) are two real inputs from the test set. Our model successfully identifies negation, double negation and phrases with different sentiment in one sentence.\nTable 1\n1 layer 2 layers 3 layers 0.445 0.4692 0.446 0.4738\n0.4469 0.4769 0.4478 0.4629 0.4487 0.4688 0.4505 0.4701 0.4505 0.4692 0.4505 0.4665 0.4514 0.4688 0.4523 0.4697 0.4523 0.462 0.4523 0.4706 0.4523 0.4652 0.4523 0.4688 0.4532 0.4665 0.4532 0.4756 0.4541 0.4534 0.4541 0.4652\n0.455 0.4652 0.455 0.4633\n0.4569 0.467 0.4569 0.476 0.4578 0.4679 0.4578 0.4692 0.4587 0.4538 0.4587 0.4801 0.4587 0.4701 0.4596 0.4588 0.4596 0.4715 0.4614 0.4557 0.4614 0.4719 0.4614 0.4751 0.4623 0.4575 0.4632 0.4747 0.4641 0.4679 0.4641 0.4729\n0.465 0.4701 0.465 0.471\n0.4659 0.4688 0.4659 0.4774 0.4659 0.4724 0.4659 0.4692 0.4668 0.4688 0.4678 0.4724 0.4678 0.4851 0.4678 0.4751 0.4687 0.4661 0.4732 0.4792 0.4632 0.481 0.4714 0.4842 0.4714 0.49 0.4714 0.4769 0.4714 0.4828 0.4723 0.4674 0.4723 0.4828 0.4732 0.4593 0.4732 0.4814 0.4732 0.4697\n0.475 0.4638 0.475 0.4778\n0.4759 0.4887 0.4759 0.4887 0.4759 0.4783 0.4759 0.4692 0.4768 0.4701 0.4768 0.49 0.4768 0.4842 0.4777 0.4824 0.4787 0.476 0.4796 0.4747 0.4796 0.4982 0.4796 0.486 0.4814 0.4742 0.4814 0.4959 0.4823 0.4729 0.4841 0.4801\n0.485 0.4719\n0.4859 0.4864 0.4877 0.4873 0.4877 0.5009 0.4668 0.4932 0.4696 0.4724 0.4696 0.4919 0.4705 0.4905 0.4714 0.4869 0.4723 0.4665 0.4723 0.481 0.4723 0.4873 0.4732 0.4905 0.4741 0.4783 0.4759 0.4769 0.4759 0.4778 0.4768 0.4896 0.4787 0.4851 0.4796 0.4801 0.4805 0.5081 0.4805 0.4869 0.4805 0.4964 0.4805 0.4882 0.4814 0.4932 0.4814 0.4955 0.4823 0.4661 0.4823 0.4652 0.4823 0.4819 0.4832 0.4719 0.4832 0.4787 0.4832 0.4891 0.4832 0.4769 0.4832 0.4986\n0.485 0.4964\n0.4859 0.476 0.4859 0.4986 0.4868 0.486 0.4877 0.4959 0.4886 0.4878 0.4896 0.4602 0.4896 0.4937 0.4914 0.4765 0.4923 0.4986\n0.495 0.5063\n44.5%\n46.1%\n47.8%\n49.4%\n51.0%\n43.5% 45.1% 46.8% 48.4% 50.0%\n1 layer 2 layers 3 layers\nFigure 2: Dev accuracy (x-axis) and test accuracy (y-axis) of independent runs of our model on finegrained sentiment classification task. Deeper architectures achieve better accuracies.\ngrams. Figure 3 splits the model accuracies according to the choice of span decaying factor \u03bb. Note when \u03bb = 0, the model applies feature extractions to consecutive n-grams only. As shown in Figure 3, this setting leads to consistent performance drop. This result confirms the importance of handling non-consecutive n-gram patterns.\nNon-linear activation Finally, we verify the effectiveness of rectified linear unit activation func-\ndecay=0.0 decay=0.3 decay=0.5\n0.4587 0.4724 0.4623 0.4774 0.4632 0.4593\n0.465 0.4864\n0.4659 0.4765 0.4668 0.4796 0.4678 0.4715 0.4687 0.4805 0.4696 0.4805 0.4696 0.4864 0.4705 0.4548 0.4705 0.4729 0.4714 0.4652 0.4714 0.4629 0.4723 0.4787 0.4723 0.4982 0.4723 0.4787\n0.4723 0.4606\n0.4732 0.4765\n0.4732 0.4701\n0.475 0.4801\n0.4759 0.4688\n0.4759 0.4661\n0.4768 0.4837 0.4768 0.4652 0.4777 0.4557 0.4777 0.4588 0.4777 0.4688 0.4777 0.4986 0.4787 0.4633 0.4796 0.471 0.4805 0.4891 0.4632 0.481 0.4668 0.4932 0.4696 0.4724 0.4705 0.4905 0.4714 0.4842 0.4714 0.49 0.4714 0.4869 0.4714 0.4769 0.4723 0.4665 0.4723 0.481 0.4723 0.4674 0.4723 0.4828 0.4732 0.4697 0.4732 0.4905\n0.475 0.4778\n0.4759 0.4692 0.4768 0.4896 0.4768 0.49 0.4768 0.4842 0.4787 0.476 0.4796 0.4747 0.4796 0.486 0.4805 0.5081 0.4805 0.4869 0.4805 0.4882 0.4814 0.4932 0.4823 0.4729 0.4823 0.4661 0.4823 0.4652 0.4832 0.4891 0.4859 0.476 0.4859 0.4864 0.4886 0.4878 0.4896 0.4602\n0.495 0.5063\n0.4696 0.4919 0.4714 0.4828 0.4723 0.4873 0.4732 0.4593 0.4732 0.4814 0.4741 0.4783\n0.475 0.4638\n0.4759 0.4769 0.4759 0.4887 0.4759 0.4887 0.4759 0.4783 0.4759 0.4778 0.4768 0.4701 0.4777 0.4824 0.4787 0.4851 0.4796 0.4982 0.4796 0.4801 0.4805 0.4964 0.4814 0.4742 0.4814 0.4959 0.4814 0.4955 0.4823 0.4819 0.4832 0.4719 0.4832 0.4787 0.4832 0.4769 0.4832 0.4986 0.4841 0.4801\n0.485 0.4719 0.485 0.4964\n0.4859 0.4986 0.4868 0.486 0.4877 0.4873 0.4877 0.4959 0.4877 0.5009 0.4896 0.4937 0.4914 0.4765 0.4923 0.4986\ntion (ReLU) by comparing it with no activation (or identity activation f(x) = x). As shown in Figure 4, our model with ReLU activation generally outperforms its variant without ReLU. The observation is consistent with previous work on convolutional neural networks and other neural network models.\nNone ReLU 0.4696 0.4724 0.4714 0.4769 0.4714 0.4828 0.4723 0.4665 0.4723 0.481 0.4723 0.4873 0.4723 0.4674 0.4723 0.4828 0.4732 0.4593 0.4732 0.4697 0.4741 0.4783\n0.475 0.4638 0.475 0.4778\n0.4759 0.4769 0.4759 0.4887 0.4759 0.4783 0.4759 0.4692 0.4759 0.4778 0.4768 0.4701 0.4768 0.4896 0.4777 0.4824 0.4787 0.476 0.4787 0.4851 0.4796 0.4747 0.4796 0.4801 0.4805 0.4869 0.4814 0.4742 0.4823 0.4729 0.4823 0.4661 0.4823 0.4652 0.4823 0.4819 0.4832 0.4719 0.4832 0.4787 0.4832 0.4891 0.4832 0.4769\n0.485 0.4719\n0.4859 0.476 0.4877 0.4873 0.4896 0.4602 0.4914 0.4765 0.4632 0.481 0.4668 0.4932 0.4696 0.4919 0.4705 0.4905 0.4714 0.4842 0.4714 0.49 0.4714 0.4869 0.4732 0.4814 0.4732 0.4905 0.4759 0.4887 0.4768 0.49 0.4768 0.4842 0.4796 0.4982 0.4796 0.486 0.4805 0.5081 0.4805 0.4964 0.4805 0.4882 0.4814 0.4959 0.4814 0.4932 0.4814 0.4955 0.4832 0.4986 0.4841 0.4801\n0.485 0.4964\n0.4859 0.4986 0.4859 0.4864 0.4868 0.486 0.4877 0.4959 0.4877 0.5009 0.4886 0.4878 0.4896 0.4937 0.4923 0.4986\n0.495 0.5063\n44.5%\n46.1%\n47.8%\n49.4%\n51.0%\n46.0% 47.0% 48.0% 49.0% 50.0%\nNone ReLU"}, {"heading": "7.3 Example Predictions", "text": "Figure 5 gives examples of input sentences and the corresponding predictions of our model in fine-grained sentiment classification. To see how our model captures the sentiment at different local context, we apply the learned softmax activation to the extracted features at each position without taking the average. That is, for each index i, we obtain the local sentiment p = softmax ( W> ( z(1)[i]\u2295 z(2)[i]\u2295 z(3)[i] )) . We\nplot the expected sentiment scores \u22112\ns=\u22122 s \u00b7p(s), where a score of 2 means \u201cvery positive\u201d, 0 means \u201cneutral\u201d and -2 means \u201cvery negative\u201d. As shown in the figure, our model successfully learns negation and double negation. The model also identifies positive and negative segments appearing in the sentence."}, {"heading": "8 Conclusion", "text": "We proposed a feature mapping operator for convolutional neural networks by modeling n-gram interactions based on tensor product and evaluating all non-consecutive n-gram vectors. The associated parameters are maintained as a low-rank tensor, which leads to efficient feature extraction via dynamic programming. The model achieves top performance on standard sentiment classification and document categorization tasks."}, {"heading": "Acknowledgments", "text": "We thank Kai Sheng Tai, Mohit Iyyer and Jordan Boyd-Graber for answering questions about their paper. We also thank Yoon Kim, the MIT NLP group and the reviewers for their comments. We\nacknowledge the support of the U.S. Army Research Office under grant number W911NF-10-10533. The work is developed in collaboration with the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI). Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston."], "venue": "International Conference on Machine Learning, ICML.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "52nd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Modeling interestingness with deep neural networks", "author": ["Jianfeng Gao", "Patrick Pantel", "Michael Gamon", "Xiaodong He", "Li Deng", "Yelong Shen."], "venue": "In", "citeRegEx": "Gao et al\\.,? 2014", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daume III."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Iyyer et al\\.,? 2015", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A unified sentence space for categorical distributional-compositional semantics: Theory and experiments", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman."], "venue": "In Proceedings of COLING: Posters.", "citeRegEx": "Kartsaklis et al\\.,? 2012", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014).", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Inductive learning in symbolic domains using structuredriven recurrent neural networks", "author": ["Andreas K\u00fcchler", "Christoph Goller."], "venue": "KI-96: Advances in Artificial Intelligence, pages 183\u2013197.", "citeRegEx": "K\u00fcchler and Goller.,? 1996", "shortCiteRegEx": "K\u00fcchler and Goller.", "year": 1996}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of Joint Conference on Lexical and Computational Semantics (*SEM).", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, November.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "ACL, pages 236\u2013244.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "volume 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive distributed representations", "author": ["Jordan B Pollack."], "venue": "Artificial Intelligence, 46:77\u2013105.", "citeRegEx": "Pollack.,? 1990", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil."], "venue": "Proceedings of the companion publication of the 23rd international conference on World", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML).", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Socher et al\\.,? 2011b", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910. Association for Com-", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Semantic parsing for single-relation question answering", "author": ["Wen-tau Yih", "Xiaodong He", "Christopher Meek."], "venue": "Proceedings of ACL.", "citeRegEx": "Yih et al\\.,? 2014", "shortCiteRegEx": "Yih et al\\.", "year": 2014}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun."], "venue": "arXiv preprint arXiv:1502.01710.", "citeRegEx": "Zhang and LeCun.,? 2015", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Non-linear interactions within a local context have been shown to improve empirical performance in various tasks (Mitchell and Lapata, 2008; Kartsaklis et al., 2012; Socher et al., 2013).", "startOffset": 113, "endOffset": 186}, {"referenceID": 14, "context": "Non-linear interactions within a local context have been shown to improve empirical performance in various tasks (Mitchell and Lapata, 2008; Kartsaklis et al., 2012; Socher et al., 2013).", "startOffset": 113, "endOffset": 186}, {"referenceID": 29, "context": "Non-linear interactions within a local context have been shown to improve empirical performance in various tasks (Mitchell and Lapata, 2008; Kartsaklis et al., 2012; Socher et al., 2013).", "startOffset": 113, "endOffset": 186}, {"referenceID": 29, "context": "This enables us to directly tap into non-linear interactions between adjacent word feature vectors (Socher et al., 2013; Lei et al., 2014).", "startOffset": 99, "endOffset": 138}, {"referenceID": 20, "context": "This enables us to directly tap into non-linear interactions between adjacent word feature vectors (Socher et al., 2013; Lei et al., 2014).", "startOffset": 99, "endOffset": 138}, {"referenceID": 31, "context": "6% on binary classification, outperforming the best published numbers obtained by a deep recursive model (Tai et al., 2015) and a convolutional model (Kim, 2014).", "startOffset": 105, "endOffset": 123}, {"referenceID": 15, "context": ", 2015) and a convolutional model (Kim, 2014).", "startOffset": 34, "endOffset": 45}, {"referenceID": 1, "context": "Deep neural networks have recently brought about significant advancements in various natural language processing tasks, such as language modeling (Bengio et al., 2003; Mikolov et al., 2010), sentiment analysis (Socher et al.", "startOffset": 146, "endOffset": 189}, {"referenceID": 21, "context": "Deep neural networks have recently brought about significant advancements in various natural language processing tasks, such as language modeling (Bengio et al., 2003; Mikolov et al., 2010), sentiment analysis (Socher et al.", "startOffset": 146, "endOffset": 189}, {"referenceID": 29, "context": ", 2010), sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al.", "startOffset": 28, "endOffset": 91}, {"referenceID": 11, "context": ", 2010), sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al.", "startOffset": 28, "endOffset": 91}, {"referenceID": 18, "context": ", 2010), sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al.", "startOffset": 28, "endOffset": 91}, {"referenceID": 4, "context": ", 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al.", "startOffset": 49, "endOffset": 123}, {"referenceID": 27, "context": ", 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al.", "startOffset": 49, "endOffset": 123}, {"referenceID": 3, "context": ", 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al.", "startOffset": 49, "endOffset": 123}, {"referenceID": 0, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 125}, {"referenceID": 6, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 125}, {"referenceID": 30, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014).", "startOffset": 57, "endOffset": 125}, {"referenceID": 21, "context": "Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al.", "startOffset": 116, "endOffset": 170}, {"referenceID": 12, "context": "Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al.", "startOffset": 116, "endOffset": 170}, {"referenceID": 25, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al.", "startOffset": 60, "endOffset": 101}, {"referenceID": 16, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al.", "startOffset": 60, "endOffset": 101}, {"referenceID": 4, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 5, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 33, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 26, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 13, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 34, "context": ", 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015).", "startOffset": 143, "endOffset": 282}, {"referenceID": 19, "context": "Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications.", "startOffset": 70, "endOffset": 90}, {"referenceID": 33, "context": "This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al.", "startOffset": 63, "endOffset": 81}, {"referenceID": 26, "context": ", 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014).", "startOffset": 34, "endOffset": 71}, {"referenceID": 8, "context": ", 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014).", "startOffset": 34, "endOffset": 71}, {"referenceID": 0, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence.", "startOffset": 58, "endOffset": 787}, {"referenceID": 0, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014). Kalchbrenner et al. (2014) instead propose (dynamic) k-max pooling operation for modeling sentences.", "startOffset": 58, "endOffset": 1052}, {"referenceID": 0, "context": ", 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K\u00fcchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014). Kalchbrenner et al. (2014) instead propose (dynamic) k-max pooling operation for modeling sentences. In addition, Kim (2014) combines CNNs of different filter widths and either static or fine-tuned word vectors.", "startOffset": 58, "endOffset": 1150}, {"referenceID": 7, "context": "descent using AdaGrad algorithm (Duchi et al., 2011).", "startOffset": 32, "endOffset": 52}, {"referenceID": 9, "context": "In addition, we randomly dropout (Hinton et al., 2012) units on the output feature representations z(i) at each level.", "startOffset": 33, "endOffset": 54}, {"referenceID": 29, "context": "For sentiment classification, we use the Stanford Sentiment Treebank benchmark (Socher et al., 2013).", "startOffset": 79, "endOffset": 100}, {"referenceID": 17, "context": "The top block lists recursive neural network models, the second block are convolutional network models and the third block contains other baseline methods, including the paragraph-vector model (Le and Mikolov, 2014), the deep averaging network model (Iyyer et al.", "startOffset": 193, "endOffset": 215}, {"referenceID": 11, "context": "The top block lists recursive neural network models, the second block are convolutional network models and the third block contains other baseline methods, including the paragraph-vector model (Le and Mikolov, 2014), the deep averaging network model (Iyyer et al., 2015) and our implementation of neural bag-of-words.", "startOffset": 250, "endOffset": 270}, {"referenceID": 11, "context": "The training time of baseline methods is taken from (Iyyer et al., 2015) or directly from the authors.", "startOffset": 52, "endOffset": 72}, {"referenceID": 28, "context": "network baselines include standard RNN (Socher et al., 2011b), RNTN with a small core tensor in the composition function (Socher et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 29, "context": ", 2011b), RNTN with a small core tensor in the composition function (Socher et al., 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 10, "context": ", 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al.", "startOffset": 39, "endOffset": 63}, {"referenceID": 31, "context": ", 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al., 2015).", "startOffset": 141, "endOffset": 159}, {"referenceID": 13, "context": "Such convolutional baselines include the dynamic CNN with k-max pooling DCNN (Kalchbrenner et al., 2014) and the convolutional model with multi-channel CNN-MC by Kim (2014).", "startOffset": 77, "endOffset": 104}, {"referenceID": 10, "context": ", 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al., 2015). These recursive models assume the input sentences are represented as parse trees. As a benefit, they can readily utilize annotations at the phrase level. In contrast, convolutional neural networks are trained on sequence-level, taking the original sequence and its label as training input. Such convolutional baselines include the dynamic CNN with k-max pooling DCNN (Kalchbrenner et al., 2014) and the convolutional model with multi-channel CNN-MC by Kim (2014). To leverage the phrase-level annotations in the Stanford Sentiment Treebank, all phrases and the corresponding labels are added as separate instances when training the sequence models.", "startOffset": 40, "endOffset": 624}, {"referenceID": 32, "context": "Word vectors The word vectors are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data (Turian et al., 2010).", "startOffset": 152, "endOffset": 173}, {"referenceID": 24, "context": "In particular, for the English sentiment classification task, we use the publicly available 300-dimensional GloVe word vectors trained on the Common Crawl with 840B tokens (Pennington et al., 2014).", "startOffset": 172, "endOffset": 197}, {"referenceID": 11, "context": "This choice of word vectors follows most recent work, such as DAN (Iyyer et al., 2015) and RLSTM (Tai et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 31, "context": ", 2015) and RLSTM (Tai et al., 2015).", "startOffset": 18, "endOffset": 36}, {"referenceID": 22, "context": "Therefore, we run word2vec (Mikolov et al., 2013) to train 200-dimensional word vectors on the 1.", "startOffset": 27, "endOffset": 49}, {"referenceID": 2, "context": "Implementation details The source code is implemented in Python using the Theano library (Bergstra et al., 2010), a flexible linear algebra compiler that can optimize userspecified computations (models) with efficient automatic low-level implementations, including (back-propagated) gradient calculation.", "startOffset": 89, "endOffset": 112}], "year": 2015, "abstractText": "The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of lowrank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the fine-grained sentiment classification task.1", "creator": "TeX"}}}