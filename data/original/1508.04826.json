{"id": "1508.04826", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2015", "title": "Dither is Better than Dropout for Regularising Deep Neural Networks", "abstract": "Regularisation of deep neural networks (DNN) during training is critical to performance. By far the most popular method is known as dropout. Here, cast through the prism of signal processing theory, we compare and contrast the regularisation effects of dropout with those of dither. We illustrate some serious inherent limitations of dropout and demonstrate that dither provides a far more effective regulariser which does not suffer from the same limitations.", "histories": [["v1", "Wed, 19 Aug 2015 23:02:37 GMT  (202kb)", "http://arxiv.org/abs/1508.04826v1", null], ["v2", "Wed, 26 Aug 2015 12:59:38 GMT  (206kb)", "http://arxiv.org/abs/1508.04826v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew j r simpson"], "accepted": false, "id": "1508.04826"}, "pdf": {"name": "1508.04826.pdf", "metadata": {"source": "META", "title": "Dither is Better than Dropout for Regularising Deep Neural Networks", "authors": ["Andrew J.R. Simpson"], "emails": ["Andrew.Simpson@Surrey.ac.uk"], "sections": [{"heading": null, "text": "during training is critical to performance. By far the most popular method is known as dropout. Here, cast through the prism of signal processing theory, we compare and contrast the regularisation effects of dropout with those of dither. We illustrate some serious inherent limitations of dropout and demonstrate that dither provides a far more effective regulariser which does not suffer from the same limitations.\nIndex terms\u2014Deep learning, regularisation, dropout, dither.\nI. INTRODUCTION\nIn nonlinear signal processing, the use of additive noise prior to nonlinear processing (such as quantization or truncation) acts to decorrelate (or suppress) nonlinear distortion products. This process is known as dithering and can also be used in discrete signal processing to mitigate aliasing issues resulting from nonlinear distortion products which fall beyond the Nyquist limit.\nDeep neural networks [1] may be interpreted as discrete (sampled) systems consisting of linear filters and nonlinear demodulation stages [2] and it has been suggested [3] that the inherent nonlinear distortion and aliasing contribute to problems of overfitting. Thus, in principle, if dither acts to suppress nonlinear distortion and aliasing it should also act to regularise a DNN.\nAt face value, dropout [4] appears somewhat compatible with dither and is known to be a useful regulariser. However, despite the cited motivation of \u2018preventing co-adaptation\u2019 [4], a coherent signal-processing-based rationale for dropout as regulariser has not emerged. Furthermore, the empirical results of dropout are typically conflated with those of stochastic gradient descent (SGD) and/or batch-averaged SGD and very little is known of possible dependencies or interactions between the two.\nFrom a signal processing point of view, the gradients necessary for SGD constitute the output of a high-pass filter and the process of averaging these gradients over a batch constitutes a low-pass filter. In the ubiquitous batch-based SGD, this combination results in a band-pass filter. Therefore, batch size directly affects the bandwidth of the band-pass filter. This, in turn, constrains the process of SGD by\npreventing, to some degree, high-frequency (i.e., fine or abrupt) SGD steps from occuring.\nIn terms of sampling theory, although dropout acts similarly to dither in decorrelating nonlinear distortion products by perturbing the nonlinearity, it is not additive. A further critical difference between dropout and dither is that dropout discards a number of samples \u2013 a process that may be interpreted as stochastic decimation. One consequence of this is that dropout introduces distortion. A second consideration is that, according to sampling theory, decimation may only safely be performed following suitable low-pass filtering. Hence, dropout must introduce aliasing and distortion. Thus, unless this aliasing and distortion can be suppressed, dropout is likely to cause overfitting rather than prevent it. In other words, it is predictable that dropout applied with small or no batch averaging will result in anti-regularisation.\nIn this paper, we illustrate that dither provides regularisation which is independent of batch size, whilst the effect of dropout ranges from regularisation to antiregularisation dependent upon the batch size."}, {"heading": "II. METHOD", "text": "Regularisation is critical in the so-called \u2018small-data regime\u2019 \u2013 where the balance between parameters and data is skewed towards the parameters. For case study, we chose the wellknown computer vision problem of hand-written digit classification using the MNIST dataset [5]. For the input layer we unpacked the images of 28x28 pixels into vectors of length 784. An example digit is given in Fig. 1. Pixel intensities were normalized to zero mean. Replicating Hinton\u2019s [6] architecture, but using the biased sigmoid activation function [2], we built a fully connected network of size 784x100x10\nunits, with a 10-unit softmax output layer, corresponding to the 10-way digit classification problem.\nIn order to place ourselves in the small-data regime, we used only the first 256 training examples of the MNIST dataset and tested on the full 10,000 test examples. We trained three versions of the model. The first version was trained without any regularisation. The second was trained with 50% dropout and the third version was trained with dither. For training with dither, uniform noise of unit scale and zero mean was added to the input (image only) data of each batch. The three classes of model were each independently instantiated and trained using SGD with batch sizes of 2, 4, 8, 16, 32, 64, 128 and 256 (i.e., 256 = full training set). Each separate model was trained for 100 full-sweep iterations of SGD (without momentum) and the test error computed (over the 10,000 test examples) at each iteration. For reliable comparison, each model was trained from the exact same random starting weights. A learning rate (SGD step size) of 1 was used for all training."}, {"heading": "III. RESULTS", "text": "Fig. 2a plots the test-error rates, as a function of full-sweep SGD iterations, for the un-regularised models of various batch sizes. Performance is dependent upon batch size; The model is essentially unable to learn anything useful when the batch size is less than 16 and peaks for batch size of 32. Fig. 2b plots the same for the models regularised using 50% dropout. As expected, performance is extremely dependent on batch size; Performance is substantially worse (than without dropout) for the batch sizes of 2 and 4, is improved (relative to no dropout) for the batch size of 8 and is similar (to performance without dropout) for the larger batch sizes. This tends to suggest that the regularisation provided by the data itself (at higher frequencies) is realised relatively well by simply averaging over larger batches (hence there is little evident advantage to dropout in this case).\nFig. 2c plots the test-error rate functions for the models trained with dither. As expected, relatively little dependence on batch size is in evidence and, in all cases, both learning rate and ultimate performance is starkly superior to dropout.\nAcross all the models, there is a general trend for the batchsize of 32 to perform best (more obviously in the nondithered cases). This tends to suggest that the data itself regularises best when averaged over batches of 32 and this probably relates to the nature of the data.\nIn summary, without dither the models at small batch sizes failed to learn anything useful and dropout made matters worse. Thus, the prediction (derived from signal processing theory) of dropout resulting in anti-regularisation for small batch sizes appears to have been confirmed. However, with dither, the same models trained (with the same batch sizes) were able to achieve an impressive nearly 80%-correct on the test set, despite only 256 training examples."}, {"heading": "IV. DISCUSSION AND CONCLUSION", "text": "In this paper, we have demonstrated that dither is a superior regulariser to dropout and that, unlike dropout, the regularisation provided by dither is more or less independent of batch size. We have argued that dither is superior to dropout as regulariser due to the fact that it is not dependent upon batch size and due to the fact that it is inherently wideband and additive. We have also documented, for the first time, paradoxical anti-regularisation effects of dropout at small batch sizes."}, {"heading": "ACKNOWLEDGMENT", "text": "AJRS did this work on the weekends and was supported by his wife and children."}], "references": [{"title": "Learning deep architectures for AI\u201d, Foundations and Trends in Machine Learning 2:1\u2013127", "author": ["Y Bengio"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Abstract Learning via Demodulation in a Deep Neural Network\u201d, arxiv.org", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Over-Sampling in a Deep Neural Network, arxiv.org", "author": ["AJR Simpson"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors\u201d, The Computing Research Repository (CoRR), abs/1207.0580", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "R Salakhutdinov"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P Haffner"], "venue": "Proc. IEEE", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "A fast learning algorithm for deep belief nets", "author": ["GE Hinton", "S Osindero", "Y Teh"], "venue": "Neural Computation", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks [1] may be interpreted as discrete (sampled) systems consisting of linear filters and nonlinear demodulation stages [2] and it has been suggested [3] that the inherent nonlinear distortion and aliasing contribute to problems of overfitting.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Deep neural networks [1] may be interpreted as discrete (sampled) systems consisting of linear filters and nonlinear demodulation stages [2] and it has been suggested [3] that the inherent nonlinear distortion and aliasing contribute to problems of overfitting.", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "Deep neural networks [1] may be interpreted as discrete (sampled) systems consisting of linear filters and nonlinear demodulation stages [2] and it has been suggested [3] that the inherent nonlinear distortion and aliasing contribute to problems of overfitting.", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "At face value, dropout [4] appears somewhat compatible with dither and is known to be a useful regulariser.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "However, despite the cited motivation of \u2018preventing co-adaptation\u2019 [4], a coherent signal-processing-based rationale for dropout as regulariser has not emerged.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "For case study, we chose the wellknown computer vision problem of hand-written digit classification using the MNIST dataset [5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "Replicating Hinton\u2019s [6] architecture, but using the biased sigmoid activation function [2], we built a fully connected network of size 784x100x10", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Replicating Hinton\u2019s [6] architecture, but using the biased sigmoid activation function [2], we built a fully connected network of size 784x100x10", "startOffset": 88, "endOffset": 91}], "year": 2015, "abstractText": "Regularisation of deep neural networks (DNN) during training is critical to performance. By far the most popular method is known as dropout. Here, cast through the prism of signal processing theory, we compare and contrast the regularisation effects of dropout with those of dither. We illustrate some serious inherent limitations of dropout and demonstrate that dither provides a far more effective regulariser which does not suffer from the same limitations.", "creator": "PDFCreator Version 1.7.1"}}}