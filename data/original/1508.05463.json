{"id": "1508.05463", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2015", "title": "StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity", "abstract": "Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that is ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet, where deep neural networks are formed via stochastic connectivity between neurons. Such stochastic synaptic formations in a deep neural network architecture can potentially allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using three image datasets. Experimental results show that a StochasticNet can be formed that provides comparable accuracy and reduced overfitting when compared to conventional deep neural networks with more than two times the number of neural connections.", "histories": [["v1", "Sat, 22 Aug 2015 03:36:43 GMT  (521kb)", "https://arxiv.org/abs/1508.05463v1", "8 pages"], ["v2", "Fri, 28 Aug 2015 19:05:03 GMT  (540kb)", "http://arxiv.org/abs/1508.05463v2", "8 pages"], ["v3", "Thu, 3 Sep 2015 01:34:17 GMT  (540kb)", "http://arxiv.org/abs/1508.05463v3", "8 pages"], ["v4", "Tue, 10 Nov 2015 20:30:05 GMT  (541kb)", "http://arxiv.org/abs/1508.05463v4", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["mohammad javad shafiee", "parthipan siva", "alexander wong"], "accepted": false, "id": "1508.05463"}, "pdf": {"name": "1508.05463.pdf", "metadata": {"source": "CRF", "title": "StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity", "authors": ["Mohammad Javad Shafiee"], "emails": ["mjshafiee@uwaterloo.ca", "parthipan.siva@aimetis.com", "a28wong@uwaterloo.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n05 46\n3v 4\n[ cs\n.C V"}, {"heading": "1 Introduction", "text": "Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8]. Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15]. However, the neural connectivity formation of deep neural networks has remained largely the same over the past decade and thus further exploration and investigation on alternative approaches to neural connectivity formation can hold considerable promise.\nTo explore alternate deep neural network connectivity formation, we take inspiration from nature by looking at the way brain develops synaptic connectivity between neurons. Recently, in a pivotal paper by Hill et al. [16], data of living brain tissue from Wistar rats was collected and used to construct a partial map of a rat brain. Based on this map, Hill et al.came to a very surprising conclusion. The synaptic formation, of specific functional connectivity in neocortical neural microcircuits, can be modelled and predicted as a random formation. In comparison, for the construction of deep neural networks, the neural connectivity formation is largely deterministic and pre-defined.\nMotivated by Hill et al.\u2019s finding of random neural connectivity formation, we aim to investigate the feasibility and efficacy of devising stochastic neural connectivity formation to construct deep neural networks. To achieve this goal, we introduce the concept of StochasticNet, where the key idea is to leverage random graph theory [17, 18] to form deep neural networks via\nstochastic connectivity between neurons. As such, we treat the formed deep neural networks as particular realizations of a random graph. Such stochastic synaptic formations in a deep neural network architecture can potentially allow for efficient utilization of neurons for performing specific tasks. Furthermore, since the focus is on neural connectivity, the StochasticNet architecture can be used directly like a conventional deep neural network and benefit from all of the same approaches used for conventional networks such as data augmentation, stochastic pooling, and Dropout [19], and DropConnect [20].\nWhile a number of stochastic strategies for improving deep neural network performance have been previously introduced [19\u201321], it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets\u2019 main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity [21], or removal/introduction of neural connectivity for regularization during training. More specifically, StochasticNets is a realization of a random graph formed prior to training and as such the connectivity in the network are inherently sparse, and are permanent and do not change during training. This is very different from Dropout [19] and DropConnect [20] where the activations and connections are temporarily removed during training and put back during test for regularization purposes only, and as such the resulting neural connectivity of the network remains dense. There is no notion of \u2019dropping\u2019 in StochasticNets as only a subset of possible neural connections are formed in the first place prior to training, and the resulting network connectivity of the network is sparse.\nStochasticNets are also very different from HashNets [21], where connection weights are randomly grouped into hash buckets, with each bucket sharing the same weights, to explicitly sparsifying into the network, since there is no notion of grouping/merging in StochasticNets; the formed StochasticNets are naturally sparse due to the formation process. In fact, stochastic strategies such as HashNets, Dropout, and DropConnect can be used in conjunction with StochasticNets.\nThe paper is organized as follows. First, a review of random graph theory is presented in Section 2. The theory and design considerations behind forming StochasticNet as a random graph realizations are discussed in Section 3. Experimental results using four image datasets (CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section 5. Finally, conclusions are drawn in Section 6."}, {"heading": "2 Review of Random Graph Theory", "text": "In this study, the goal is to leverage random graph theory [17, 18] to form the neural connectivity of deep neural networks in a stochastic manner. As such, it is important to first provide a general overview of random graph theory for context. In random graph theory, a random graph can be defined as the probability distribution over graphs [22]. A number of different random graph models have been proposed in literature.\nA commonly studied random graph model is that proposed by Gilbert [17], in which a random graph can be expressed by G(n, p), where all possible edge connectivity are said to occur independently with a probability of p, where 0 < p < 1. This random graph model was generalized by Kovalenko [23], in which the random graph can be expressed by G(V , pij), where V is a set of vertices and the edge connectivity between two vertices {i, j} in the graph is said to occur with a probability of pij , where 0 < pij < 1. An illustrative example of a random graph based on this model is shown in Figure 1. It can be seen that all possible edge connectivity between the nodes in the graph may occur independently with a probability of pij .\nTherefore, based on this generalized random graph model, realizations of random graphs can be obtained by starting with a set of n vertices V = {vq|1 \u2265 q \u2265 n} and randomly adding a set of edges between the vertices based on the set of possible edges E = {eij |1 \u2265 i \u2265 n, 1 \u2265 j \u2265 n} independently with a probability of pij . A number of realizations of the random\ngraph in Figure 1 are provided in Figure 2 for illustrative purposes. It is worth noting that because of the underlying probability distribution, the generated realizations of the random graph often exhibit differing edge connectivity.\nGiven that deep neural networks can be fundamentally expressed and represented as graphs G, where the neurons are vertices V and the neural connections are edges E , one intriguing idea for introducing stochastic connectivity for the formation of deep neural networks is to treat the formation of deep neural networks as particular realizations of random graphs, which we will describe in greater detail in the next section."}, {"heading": "3 StochasticNets: Deep Neural Networks as Random Graph Realizations", "text": "Let us represent the full network architecture of a deep neural network as a random graph G(V , p[i\u2192jk\u2192h]), where V is the the set of neurons V = {vi,k|1 \u2265 i \u2265 nl, 1 \u2265 k \u2265 mi}, with vi,k denoting the kth neuron at layer i, nl denoting the number of layers, mi denoting the number of neurons at layer i, and p[i\u2192jk\u2192h] is the probability that a neural connection occurs between neuron vj,h and vi,k .\nBased on the above random graph model for representing deep neural networks, one can then form a deep neural network as a realization of the random graph G(V , p[i\u2192jk\u2192h]) by starting with a set of neurons V , and randomly adding neural connections between the set of neurons independently with a probability of p[i\u2192jk\u2192h] as defined above.\nWhile one can form practically any type of deep neural network as a random graph realizations, an important design consideration for forming deep neural networks as random graph realizations is that different types of deep neural networks have fundamental properties in their network architecture that must be taken into account and preserved in the random graph realization. Therefore, to ensure that fundamental properties of the network architecture of a certain type of deep neural network is preserved, the probability p[i\u2192jk\u2192h] must be designed in such a way that these properties are enforced appropriately in the resultant random graph realization. Let us consider a general deep feed-forward neural network. First, in a deep feed-forward neural network, there can be no neural connections between non-adjacent layers. Second, in a deep feed-forward neural network, there can be no neural connections between neurons on the same layer. Therefore, to enforce these two properties, p[i\u2192jk\u2192h] = 0 when i = j || |i\u2212 j| > 2. An example random graph based on this random graph model for representing general deep feed-forward neural networks is shown in Figure 3, with an example realization of the random graph shown in Figure 4. It can be observed in Figure 4 that the neural connectivity for each neuron may be different due to the stochastic nature of neural connection formation.\nFurthermore, for specific types of deep feed-forward neural networks, additional considerations must be taken into account to preserve their properties in the resultant random graph realization. For example, in the case of deep convolutional neural networks, neural connectivity in the convolutional layers are arranged such that small spatially localized neural collections are connected to the same output neuron in the next layer. Furthermore, the weights of the neural connections are shared amongst different small neural collections. A significant benefit to this architecture is that it allows neural connectivity at the convolutional layers to be efficiently represented by a set of local receptive fields, thus greatly reducing memory requirements and computational complexity. To enforce these properties when forming deep convolutional neural networks as random graph realizations, one can further enforce the probability p[i\u2192jk\u2192h] such that the probability of neural connectivity is defined at a local receptive field level. As such, the neural connectivity for each randomly realized local receptive field is based on a probability distribution, with the\nneural connectivity configuration thus being shared amongst different small neural collections for a given randomly realized local receptive field.\nGiven this random graph model for representing deep convolutional neural networks, the resulting random graph realization is a deep convolutional neural network where each convolutional layer consists of a set of randomly realized local receptive fields K , with each randomly realized local receptive field Ki,k, which denotes the kth receptive field at layer i, consisting of neural connection weights of a set of random neurons within a small neural collection to the output neuron. An example of a realization of a deep convolutional neural network from a random graph is shown in Figure 5."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 Experimental Setup", "text": "To investigate the efficacy of StochasticNets, we construct StochasticNets with a deep convolutional neural network architecture and evaluate the constructed StochasticNets in a number of different ways. First, we investigate the effect of the number of neural connections formed in the constructed StochasticNets on its performance for the task of image object recognition. Second, we investigate the performance of StochasticNets when compared to baseline deep convolutional neural networks (which we will simply refer to as ConvNets) with standard neural connectivity for different image object recognition tasks based on different image datasets. Third, we investigate the relative speed of StochasticNets during classification with respect to the number of neural connections formed in the constructed StochasticNets. It is important to note that the main goal is to investigate the efficacy of forming deep neural networks via stochastic connectivity in the form of StochasticNets and the influence of stochastic connectivity parameters on network performance, and not to obtain maximum absolute performance; therefore, the performance\nof StochasticNets can be further optimized through additional techniques such as data augmentation and network regularization methods. For evaluation purposes, four benchmark image datasets are used: CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]. A description of each dataset and the StochasticNet configuration used are described below."}, {"heading": "4.1.1 Datasets", "text": "The CIFAR-10 image dataset [24] consists of 50,000 training images categorized into 10 different classes (5,000 images per class) of natural scenes. Each image is an RGB image that is 32\u00d732 in size. The MNIST image dataset [25] consists of 60,000 training images and 10,000 test images of handwritten digits. Each image is a binary image that is 28\u00d728 in size, with the handwritten digits are normalized with respect to size and centered in each image. The SVHN image dataset [26] consists of 604,388 training images and 26,032 test images of digits in natural scenes. Each image is an RGB image that is 32\u00d732 in size. The images in the MNIST dataset were resized to 32 \u00d7 32 by zero padding since the same StochasticNet network configuration is utilized for all mentioned image datasets. Finally, the STL-10 image dataset [27] consists of 5,000 labeled training images and 8,000 labeled test images categorized into 10 different classes (500 training images and 800 training images per class) of natural scenes. Each image is an RGB image that is 96\u00d796 in size. Note that the 100,000 unlabeled images in the STL-10 image dataset were not used in this study."}, {"heading": "4.1.2 StochasticNet Configuration", "text": "The StochasticNets used in this study for the all datasets are realized based on the LeNet-5 deep convolutional neural network [25] architecture, and consists of 3 convolutional layers with 32, 32, and 64 local receptive fields of size 5\u00d7 5 for the first, second, and third convolutional layers, respectively, and 1 hidden layer of 64 neurons, with all neural connections in the convolutional and hidden layers being randomly realized based on probability distributions. While it is possible to take advantage of any arbitrary distribution to construct StochasticNet realizations, for the purpose of this study the neural connection probability of the hidden layers follow a uniform distribution, while two different spatial distributions were explored for the convolutional layers: i) uniform distribution, and ii) a Gaussian distribution with the mean at the center of the receptive field and the standard deviation being a third of the receptive field size. All image datasets are with 10 class label outputs which is provided in the network setup."}, {"heading": "4.2 Number of Neural Connections", "text": "An experiment was conducted to illustrate the impact of the number of neural connections on the modeling accuracy of StochasticNets. Figure 6 demonstrates the training and test error versus the number of neural connections in the network for the CIFAR-10 dataset. A StochasticNet with the network configuration as described in Section 4.1.2 was provided to train the model. The neural connection probability is varied in both the convolutional layers and the hidden layer to achieve the desired number of neural connections for testing its effect on modeling accuracy.\nFigure 6 demonstrates the training and testing error vs. the neural connectivity percentage relative to the baseline ConvNet, for two different neural connection distributions: i) uniform distribution, and ii) a Gaussian distribution with the mean at the center of the receptive field and the standard deviation being a third of the receptive field size. It can be observed that StochasticNet is able to achieve the same test error as ConvNet when the number of neural connections in the StochasticNet is less than half that of the ConvNet. It can be also observed that, although increasing the number of neural connections resulted in lower training error, it does not not exhibit reductions in test error, which brings to light the issue of over-fitting. In other words, it can be observed that the proposed StochasticNets can improve the handling of over-fitting associated with deep neural networks while decreasing the number of neural connections, which in effect greatly reduces the number of computations and thus resulting in faster network training and usage. Finally, it is also observed that there is a noticeable difference in the training and test errors when using\nGaussian distributed connectivity when compared to uniform distributed connectivity, which indicates that the choice of neural connectivity probability distributions can have a noticeable impact on model accuracy."}, {"heading": "4.3 Comparisons with ConvNet", "text": "Motivated by the results shown in Figure 6, a comprehensive experiment were done to demonstrate the performance of the proposed StochasticNets on different benchmark image datasets. StochasticNet realizations were formed with 39% neural connectivity via Gaussian-distributed connectivity when compared to a conventional ConvNet. The StochasticNets and ConvNets were trained on four benchmark image datasets (i.e., CIFAR-10, MNIST, SVHN, and STL-10) and their training and test error performances are compared to each other. Since the neural connectivity of StochasticNets are realized stochastically, the performance of the StochasticNets was evaluated based on 25 trials (leading to 25 StochasticNet realizations) and the reported results are based on the average of the 25 trials. Figure 7 shows the training and test error results of the StochasticNets and ConvNets on the four different tested datasets. It can be observed that, despite the fact that there are less than half as many neural connections in the StochasticNet realizations, the test errors between ConvNets and the StochasticNet realizations can be considered to be the same for CIFAR-10, MNIST, and SVHN datasets. Interestingly, it was also observed that the test errors for the StochasticNet realizations is lower than that achieved using the ConvNet (relative improvement in test error rate of \u223c6% compared to ConvNet) for the STL-10 dataset, again despite the fact that there are less than half as many neural connections in the StochasticNet realizations. The results for the STL-10 dataset truly illustrates the particular effectiveness of StochasticNets, particularly when dealing with low number of training samples.\nFurthermore, the gap between the training and test errors of the StochasticNets is less than that of the ConvNets, which would indicate reduced overfitting in the StochasticNets. The standard deviation of the 25 trials for each error curve is shown as dashed lines around the error curve. It can be observed that the standard deviation of the 25 trials is very small and indicates that the proposed StochasticNet exhibited similar performance in all 25 trials."}, {"heading": "4.4 Relative Speed vs. Number of Neural Connections", "text": "Given that the experiments in the previous sections show that StochasticNets can achieve good performance relative to conventional ConvNets while having significantly fewer neural connections, we now further investigate the relative speed of StochasticNets during classification with respect to the number of neural connections formed in the constructed StochasticNets. Here, as with Section 4.2, the neural connection probability is varied in both the convolutional layers and the hidden layer to achieve the desired number of neural connections for testing its effect on the classification speed of the formed StochasticNets. Figure 8 demonstrates the relative classification time vs. the neural connectivity percentage relative to the baseline ConvNet. The relative time is defined as the time required during the classification process relative to that of the ConvNet. It can be observed that the relative time decreases as the number of neural connections decrease, which illustrates the potential for StochasticNets to enable more efficient classification."}, {"heading": "5 Conclusions", "text": "In this study, we introduced a new approach to deep neural network formation inspired by the stochastic connectivity exhibited in synaptic connectivity between neurons. The proposed StochasticNet is a deep neural network that is formed as a realization of a random graph, where the synaptic connectivity between neurons are formed stochastically based on a probability distribution. Using this approach, the neural connectivity within the deep neural network can be formed in a way that facilitates efficient neural utilization, resulting in deep neural networks with much fewer neural connections while achieving the same modeling accuracy. The effectiveness and efficiency of the proposed StochasticNet was evaluated using four popular benchmark image datasets and compared to a conventional convolutional neural network (ConvNet). Experimental results demonstrate that the proposed StochasticNet provides comparable accuracy as the conventional ConvNet with much less number of neural connections while reducing the overfitting issue associating with the conventional ConvNet for CIFAR-10, MNIST, and SVHN datasets. More interestingly, a StochasticNet with much less number of neural connections was found to achieve higher accuracy when compared to conventional deep neural networks for the STL-10 dataset. As such, the proposed StochasticNet holds great potential for enabling the formation of much more efficient deep neural networks that have fast operational speeds while still achieving strong accuracy."}, {"heading": "Acknowledgments", "text": "This work was supported by the Natural Sciences and Engineering Research Council of Canada, Canada Research Chairs Program, and the Ontario Ministry of Research and Innovation. The authors also thank Nvidia for the GPU hardware used in this study through the Nvidia Hardware Grant Program.\nAuthor contributions\nM.S. and A.W. conceived and designed the architecture. M.S., P.S., and A.W. worked on formulation and derivation of architecture. M.S. implemented the architecture and performed the experiments. M.S., P.S., and A.W. performed the data analysis. All authors contributed to writing the paper and to the editing of the paper."}], "references": [{"title": "Deep Speech: Scaling up end-to-end speech recognition", "author": ["A Hannun"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition", "author": ["G Dahl"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition", "author": ["K He"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F. Huang", "L. Bottou"], "venue": "Conference on Computer Vision and Pattern Recognition", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Visual Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks", "author": ["M. Zeller", "R. Fergus"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Regularization of Neural Networks using DropConnect", "author": ["L Wan"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K He"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Going Deeper with Convolutions", "author": ["C Szegedy"], "venue": "Conference on Computer Vision and Pattern Recognition", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Accelerating Very Deep Convolutional Networks for Classification and Detection", "author": ["X Zhang"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Statistical connectivity provides a sufficient foundation for specific functional connectivity in neocortical neural microcircuits", "author": ["S Hill"], "venue": "Proceedings of National Academy of Sciences of the United States of America", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "On random graphs I", "author": ["P. Erdos", "A. Renyi"], "venue": "Publ. Math. Debrecen", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1959}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["W Chen"], "venue": "Journal of Machine Learning Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Regularization of Neural Networks using DropConnect", "author": ["W Li"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Compressing Neural Networks with the Hashing Trick", "author": ["W Chen"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Probabilistic combinatorics and its applications", "author": ["B. Bollob\u00e1s", "F. Chung"], "venue": "American Mathematical Soc", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1991}, {"title": "The structure of random directed graph", "author": ["I. Kovalenko"], "venue": "Probab. Math. Statist", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1975}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun"], "venue": "Proceedings of the IEEE", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y Netzer"], "venue": "NIPS Workshop", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "An Analysis of Single Layer Networks in Unsupervised Feature Learning", "author": ["A Coates"], "venue": "AISTATS", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 121, "endOffset": 127}, {"referenceID": 1, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 121, "endOffset": 127}, {"referenceID": 2, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 148, "endOffset": 153}, {"referenceID": 3, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 148, "endOffset": 153}, {"referenceID": 4, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 148, "endOffset": 153}, {"referenceID": 5, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 148, "endOffset": 153}, {"referenceID": 6, "context": "Deep neural networks have shown considerable capabilities for handling specific complex tasks such as speech recognition [1, 2], object recognition [3\u20136], and natural language processing [7,8].", "startOffset": 187, "endOffset": 192}, {"referenceID": 7, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 122, "endOffset": 129}, {"referenceID": 8, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 122, "endOffset": 129}, {"referenceID": 9, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 152, "endOffset": 159}, {"referenceID": 10, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 152, "endOffset": 159}, {"referenceID": 11, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 152, "endOffset": 159}, {"referenceID": 5, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 186, "endOffset": 197}, {"referenceID": 12, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 186, "endOffset": 197}, {"referenceID": 13, "context": "Recent advances in improving the performance of deep neural networks have focused on areas such as network regularization [9, 10], activation functions [11\u201313], and deeper architectures [6, 14, 15].", "startOffset": 186, "endOffset": 197}, {"referenceID": 14, "context": "[16], data of living brain tissue from Wistar rats was collected and used to construct a partial map of a rat brain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "To achieve this goal, we introduce the concept of StochasticNet, where the key idea is to leverage random graph theory [17, 18] to form deep neural networks via", "startOffset": 119, "endOffset": 127}, {"referenceID": 16, "context": "Furthermore, since the focus is on neural connectivity, the StochasticNet architecture can be used directly like a conventional deep neural network and benefit from all of the same approaches used for conventional networks such as data augmentation, stochastic pooling, and Dropout [19], and DropConnect [20].", "startOffset": 282, "endOffset": 286}, {"referenceID": 17, "context": "Furthermore, since the focus is on neural connectivity, the StochasticNet architecture can be used directly like a conventional deep neural network and benefit from all of the same approaches used for conventional networks such as data augmentation, stochastic pooling, and Dropout [19], and DropConnect [20].", "startOffset": 304, "endOffset": 308}, {"referenceID": 16, "context": "While a number of stochastic strategies for improving deep neural network performance have been previously introduced [19\u201321], it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets\u2019 main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity [21], or removal/introduction of neural connectivity for regularization during training.", "startOffset": 118, "endOffset": 125}, {"referenceID": 17, "context": "While a number of stochastic strategies for improving deep neural network performance have been previously introduced [19\u201321], it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets\u2019 main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity [21], or removal/introduction of neural connectivity for regularization during training.", "startOffset": 118, "endOffset": 125}, {"referenceID": 18, "context": "While a number of stochastic strategies for improving deep neural network performance have been previously introduced [19\u201321], it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets\u2019 main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity [21], or removal/introduction of neural connectivity for regularization during training.", "startOffset": 118, "endOffset": 125}, {"referenceID": 18, "context": "While a number of stochastic strategies for improving deep neural network performance have been previously introduced [19\u201321], it is very important to note that the proposed StochasticNets is fundamentally different from these existing stochastic strategies in that StochasticNets\u2019 main significant contributions deals primarily with the formation of neural connectivity of individual neurons to construct efficient deep neural networks that are inherently sparse prior to training, while previous stochastic strategies deal with either the grouping of existing neural connections to explicitly enforce sparsity [21], or removal/introduction of neural connectivity for regularization during training.", "startOffset": 612, "endOffset": 616}, {"referenceID": 16, "context": "This is very different from Dropout [19] and DropConnect [20] where the activations and connections are temporarily removed during training and put back during test for regularization purposes only, and as such the resulting neural connectivity of the network remains dense.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "This is very different from Dropout [19] and DropConnect [20] where the activations and connections are temporarily removed during training and put back during test for regularization purposes only, and as such the resulting neural connectivity of the network remains dense.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "StochasticNets are also very different from HashNets [21], where connection weights are randomly grouped into hash buckets, with each bucket sharing the same weights, to explicitly sparsifying into the network, since there is no notion of grouping/merging in StochasticNets; the formed StochasticNets are naturally sparse due to the formation process.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Experimental results using four image datasets (CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section 5.", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "Experimental results using four image datasets (CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section 5.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "Experimental results using four image datasets (CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section 5.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "Experimental results using four image datasets (CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27]) to investigate the efficacy of StochasticNets with respect to different number of neural connections as well as different training set sizes is presented in Section 5.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "In this study, the goal is to leverage random graph theory [17, 18] to form the neural connectivity of deep neural networks in a stochastic manner.", "startOffset": 59, "endOffset": 67}, {"referenceID": 19, "context": "In random graph theory, a random graph can be defined as the probability distribution over graphs [22].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "This random graph model was generalized by Kovalenko [23], in which the random graph can be expressed by G(V , pij), where V is a set of vertices and the edge connectivity between two vertices {i, j} in the graph is said to occur with a probability of pij , where 0 < pij < 1.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "For evaluation purposes, four benchmark image datasets are used: CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27].", "startOffset": 74, "endOffset": 78}, {"referenceID": 22, "context": "For evaluation purposes, four benchmark image datasets are used: CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27].", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "For evaluation purposes, four benchmark image datasets are used: CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 24, "context": "For evaluation purposes, four benchmark image datasets are used: CIFAR-10 [24], MNIST [25], SVHN [26], and STL-10 [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 21, "context": "The CIFAR-10 image dataset [24] consists of 50,000 training images categorized into 10 different classes (5,000 images per class) of natural scenes.", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "The MNIST image dataset [25] consists of 60,000 training images and 10,000 test images of handwritten digits.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "The SVHN image dataset [26] consists of 604,388 training images and 26,032 test images of digits in natural scenes.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "Finally, the STL-10 image dataset [27] consists of 5,000 labeled training images and 8,000 labeled test images categorized into 10 different classes (500 training images and 800 training images per class) of natural scenes.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "The StochasticNets used in this study for the all datasets are realized based on the LeNet-5 deep convolutional neural network [25] architecture, and consists of 3 convolutional layers with 32, 32, and 64 local receptive fields of size 5\u00d7 5 for the first, second, and third convolutional layers, respectively, and 1 hidden layer of 64 neurons, with all neural connections in the convolutional and hidden layers being randomly realized based on probability distributions.", "startOffset": 127, "endOffset": 131}], "year": 2015, "abstractText": "Deep neural networks is a branch in machine learning that has seen a meteoric rise in popularity due to its powerful abilities to represent and model high-level abstractions in highly complex data. One area in deep neural networks that is ripe for exploration is neural connectivity formation. A pivotal study on the brain tissue of rats found that synaptic formation for specific functional connectivity in neocortical neural microcircuits can be surprisingly well modeled and predicted as a random formation. Motivated by this intriguing finding, we introduce the concept of StochasticNet, where deep neural networks are formed via stochastic connectivity between neurons. As a result, any type of deep neural networks can be formed as a StochasticNet by allowing the neuron connectivity to be stochastic. Stochastic synaptic formations, in a deep neural network architecture, can allow for efficient utilization of neurons for performing specific tasks. To evaluate the feasibility of such a deep neural network architecture, we train a StochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and STL-10). Experimental results show that a StochasticNet, using less than half the number of neural connections as a conventional deep neural network, achieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and SVHN dataset. Interestingly, StochasticNet with less than half the number of neural connections, achieved a higher accuracy (relative improvement in test error rate of \u223c6% compared to ConvNet) on the STL-10 dataset than a conventional deep neural network. Finally, StochasticNets have faster operational speeds while achieving better or similar accuracy performances.", "creator": "LaTeX with hyperref package"}}}