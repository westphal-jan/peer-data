{"id": "1508.06924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2015", "title": "Using Thought-Provoking Children's Questions to Drive Artificial Intelligence Research", "abstract": "We propose to use thought-provoking children's questions (TPCQs), namely Highlights BrainPlay questions, to drive artificial intelligence research. These questions are designed to stimulate thought and learning in children, and they can be used to do the same thing in AI systems. We introduce the TPCQ task, which consists of taking a TPCQ question as input and producing as output both (1) answers to the question and (2) learned generalizations. We discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay questions, and we report statistics on question type, question class, answer cardinality, answer class, types of knowledge needed, and types of reasoning needed. We find that BrainPlay questions span many aspects of intelligence. We envision an AI system based on the society of mind (Minsky 1986; 2006) consisting of a multilevel architecture with diverse resources that run in parallel to jointly answer and learn from questions. Because the answers to BrainPlay questions and the generalizations learned from them are often highly open-ended, we suggest using human judges for evaluation.", "histories": [["v1", "Thu, 27 Aug 2015 16:23:49 GMT  (23kb)", "https://arxiv.org/abs/1508.06924v1", null], ["v2", "Fri, 11 Sep 2015 13:01:00 GMT  (23kb)", "http://arxiv.org/abs/1508.06924v2", "update Institute name and URL; fix date of Myers and Myers interview; add reference to Turing++ questions; clarify that we developed the set of annotation tags"], ["v3", "Wed, 26 Jul 2017 00:34:24 GMT  (22kb)", "http://arxiv.org/abs/1508.06924v3", "update for EGPAI 2017"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["erik t mueller", "henry minsky"], "accepted": false, "id": "1508.06924"}, "pdf": {"name": "1508.06924.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n06 92\n4v 3\n[ cs\n.A I]\n2 6\nJu l 2"}, {"heading": "Introduction", "text": "As artificial intelligence tasks like fact-based question answering [Ferrucci et al., 2013] and face recognition [Taigman et al., 2014] become mostly solved, there is a need for harder tasks. Consider the following questions from the children\u2019s magazine Highlights:\nWhy doesn\u2019t every key open every lock?\nWhich is older, a tree or a leaf on the tree?\nWhy aren\u2019t pants pockets as big as backpacks?\nFlags wave, people wave, and the ocean has waves. How are these waves alike?\nWhat part of a fish is farthest from the head?\nIs an ice-cream cone wider at the bottom or at the top?\nCould you sing a song in a dark room? Could you put together a puzzle?\nWhy can\u2019t you move faster than your shadow?\nWhat might happen if you put a bee in your pocket?\nIf you could not remember today\u2019s date, what are five ways you could find out?\nAlthough these questions are short and designed to be answered by young children, they are very hard for computers. It is unlikely that the reader has heard these questions before, and yet correct answers can be produced by most children within seconds, as well as explanations of the reasoning behind these answers. The embarrassing fact is that answering and learning from these questions is way beyond the capabilities of existing AI systems. They are wide open. We propose answering and learning from thoughtprovoking children\u2019s questions (TPCQs), which are available in the BrainPlay1 column of Highlights, as a useful metric for driving research and evaluating general-purpose AI systems. TPCQs test a system\u2019s ability to make novel connections, which is necessary for intelligence. While this method does require that the system have a powerful language facility, this is a crucial capability for a large class of useful AI systems. Without the system having the capacity to understand and generate language, it is extremely difficult for researchers to communicate abstract goals and tasks to the system, to draw its attention to salient topics, to receive answers to questions, and for the system to explain its behavior.\nTask Definition 1 (TPCQ) Given a thought-provoking children\u2019s questionQ, produce\n\u2022 one or more answers to the question A1, A2, . . .\n\u2022 one or more learned generalizations L1, L2, . . .\nExample: Q: Name three animals that hatch from eggs. A1: birds A2: chickens A3: ducks A4: snakes L1: Animals with feathers hatch from eggs. Learning doesn\u2019t only happen through real experiences and doesn\u2019t always require the addition of new knowledge.A hallmark of human-level intelligence is the ability to combine existing knowledge through imagined situations, to answer questions which you have never before been asked. You may have pieces of knowledge whose connections are not apparent until someone pushes you to notice them. This is what BrainPlay questions are designed to do.\n1BrainPlay questions are published each month in Highlights, which is available from https://www.highlights.com/."}, {"heading": "Highlights BrainPlay", "text": "Highlights magazine was started in 1946 by Garry Cleveland Myers and Caroline Clark Myers. The magazine includes a BrainPlay column, which was called Headwork before November 2004. In this paper, we use the term BrainPlay for both Headwork and BrainPlay questions. The Highlights editors develop BrainPlay questions with great care. The questions are designed to \u201c[stimulate] children from five to twelve to think and reason by working over in their heads what is already there, arriving at new ideas not learned from books\u201d [Myers, 1968]. For example, consider the BrainPlay question \u201cIn a room with a staircase leading to the second floor, how can you figure out the height of the first-floor ceiling?\u201d This question suggests a novel technique: to measure the height of a ceiling when there is a staircase leading up to the next floor, multiply the rise of the steps by the number of steps. BrainPlay first appeared in the second issue of Highlights in September 1946 [Wood, 1986]. Each month, BrainPlay presents around 20 questions arranged by age level [Myers and Myers, 1964]. Correct answers to the questions aren\u2019t provided."}, {"heading": "Analysis of BrainPlay Questions", "text": "To get an idea of what we\u2019re up against, we performed an analysis of BrainPlay questions in the Highlights issues from January 2000 to December 2000. We started by segmenting each top-level question into one or more subquestions. For example, the top-level question\nWould you rather wear a hood or a hat? Why?\nis segmented into a first question and a second question. Table 1 shows the composition of subquestions. Table 2 gives statistics on the length of subquestions. The first question tends to be the longest. The second and following questions typically ask for explanations for the answer to the first question, ask variations on the first question (often involving coreference), or follow up in some other way. For the remainder of the analysis, we considered only first questions. We annotated each first question with exactly one question type, question class, answer cardinality, and answer class, and we annotated each first question with one or more types of knowledge needed and types of reasoning needed. We developed an initial set of annotation tags likeOpen-Ended and What-If and revised them as needed during the annotation process."}, {"heading": "Question Type", "text": "Statistics on the question type are shown in Table 3.\nOpen-Ended Answer choices aren\u2019t provided. What is your favorite way to travel? Name three uses for bells.\nMultiple Choice Answer choices are provided, and the question is not a Yes-No question. Would you rather receive a phone call or a letter? Is it harder to ride a bike or to run fast?\nYes-No The answer choices are yes and no. Do you know anyone else with your initials? Have you ever cried because you were very happy?"}, {"heading": "Question Class", "text": "Statistics on the question class are shown in Table 4.\nFacts Asks about facts (may require reasoning). Name three animals that hatch from eggs. How does a turtle protect itself?\nCaring Stimulates thought about caring and kindness. What could you do today to help someone else? If your family has company, what can you do to be a good host?\nWhat-If Asks about a hypothetical scenario. If you had a pet that could talk, what would the two of you talk about? If you could change your schedule at school, what would you change?\nComparative Involves a comparative. Is it easier to swallow a pill or a spoonful of medicine? Would it be easier to remember the date of a party or the date of a haircut appointment?\nPersonal Experience Asks about personal experiences. Have you ever been so busy that you forgot to eat a meal? What popular sayings did you first hear in a song or movie?\nPersonal Preference Asks about personal preferences. Describe your favorite place to go for a walk. If you could meet any person in the world, who would it be?\nTheory of Mind Evaluates theory of mind [Doherty, 2009]. Ryan looked at the sliced apple and said, \u201cThis must have been sliced a while ago.\u201d How might he have known? When Otis arrived at the pool, he quickly figured out which person was the new swim coach. How might he have guessed?\nPurpose Asks about the purpose or function of something. What tools do you need for drawing? Name three uses for bells.\nDifference Asks for the differences between two things. How is taking a music lesson different from playing music on your own? What\u2019s the difference between a riddle and a joke?\nReason Asks about the reason for something. Why do babies cry more often than adults? Why do we frame paintings and photos before hanging them up?\nMeaning Asks for the meaning of a word or phrase. What is meant by the saying \u201cMoney doesn\u2019t grow on trees\u201d? What does it mean to \u201cgo the extra mile\u201d?\nAction Asks for an action to be performed like singing or drawing. Draw a heart in the air with your finger. Make a hand signal that means \u201cgood job.\u201d\nPersonal Facts Asks about personal facts. Are you ticklish? How many teeth do you have?\nSimilarity Asks for the similarities between two things. How are socks and mittens alike? How is honey like maple syrup?\nSuperlative Involves a superlative. What is the best smell in spring? Where do you laugh the most: at school, at home, or with friends?\nDebugging Requires debugging of a problem or situation. When Erik looked at his plane tickets, it seemed as if his flight from Oregon to Rhode Island would take six hours longer than\nhis flight from Rhode Island to Oregon. Why was this? Jackson and his family were watching TV when suddenly they lost reception. What might have caused this?\nDescription Asks for a description. Describe some rocks you\u2019ve seen. Describe how a wheel works.\nCount Asks for a count. How many pets do you know by name?\nSort Asks for items to be sorted by some attribute. List these in order of size: moon, bird, star, airplane.\nA number of questions involve personal experiences, preferences, and facts. The answers to these questions are persondependent. How shall we deal with these? The first reaction might be simply to throw them out. But consider that an intelligent, autonomous AI system will have its own personal experiences and preferences. These are essential aspects of a general-purpose AI system. Therefore it would be a mistake to throw these questions out. Because there is no gold standard answer key for them, answers can be judged for plausibility by human judges, as in the Turing test [Turing, 1950]. Some questions request an action to be performed. Again, we could throw these out, but then we would be throwing out some of the most revealing questions. Instead, the system can perform the actions in a three-dimensional simulator (or in the world if the system has a body), and the results can be judged by humans. Human judging is more time-consuming, but it is currently the best way of evaluating novel, previously unseen answers to novel, previously unseen questions. A question like \u201cHave you ever been so busy that you forgot to eat a meal?\u201d makes sense for an AI system, because the question probes essential knowledge of goals, plans, and mental states. General-purpose AI systems must be able to recognize, remember, and apply concepts like \u201cbeing busy\u201d and \u201cforgetting to perform a task.\u201d"}, {"heading": "Answer Cardinality", "text": "Statistics on howmany answers are required by a question are shown in Table 5.\n1 One answer. Who is the tallest person you know? Is it easier to throw or to catch a ball?\n>1 More than one answer. How are a bird\u2019s wings different from a butterfly\u2019s wings? Why do people make New Year\u2019s resolutions?\n2 Two answers. What weather and location are ideal for stargazing? Think of a fruit and a vegetable that begin with the letter p.\n3 Three answers. Name three ways to have fun on a rainy day. Name three objects that are shaped like a triangle.\n5 Five answers. List the top five things that you like to do with your friends."}, {"heading": "Answer Class", "text": "Statistics on the answer class are shown in Table 6. A gold standard answer key can be developed for questions of class Exactly One and Several. Thus the answers to 103 (42.21%) of the 244 BrainPlay questions we analyzed can be evaluated automatically. What about the remaining questions? Human judging will be needed for the answers to questions of class Many, Personal, Open, Debatable, and Nontextual Answer. More points should be awarded for correct answers to harder questions.\nMany The question has many short, correct answers. When might it be useful to know some jokes? Where can you find spiders?\nExactly One The question has a single possible correct answer. During which season do you usually wear sunglasses? What does it mean to be \u201con cloud nine\u201d?\nSeveral The question has a few short, correct answers. What kinds of things do you write about in a diary? Name three animals that hatch from eggs.\nPersonal The question can only be answered relative to personal experience. Try to name all of the people you have talked with today. Would you rather receive a phone call or a letter?\nOpen The question has many possibly long answers. What might happen if televisions everywhere stopped working? If you had a pet that could talk, what would the two of you talk about?\nDebatable It is difficult to judge the correctness of the answer. Is it easier to swallow a pill or a spoonful of medicine? Is it harder to ride a bike or to run fast?\nNontextual Answer The question cannot be answered using text. Instead, it requires an action to be performed. Try to clap your hands behind your back. Sing part of a song you know."}, {"heading": "Types of Knowledge Needed", "text": "Statistics on the types of knowledge needed to answer questions are shown in Table 7. The percentages sum to more than 100 because each question is annotated with one or more types of knowledge.\nScripts Stereotypical situations and scripts [Schank and Abelson, 1977]. Name a game you can play alone. Would you rather receive a phone call or a letter?\nPlans/Goals Goals and plans [Schank and Abelson, 1977]. Why do people make New Year\u2019s resolutions? What are the benefits of working on a project with others?\nPhysics Physics. Is it easier to throw or to catch a ball? Try to clap your hands behind your back.\nProperties/Attributes Properties and attributes of people and things. What kinds of hats are casual? How are a snake and an eel similar?\nHuman Body The human body. Try to make your body into the shape of each letter in your name. What do elbows and knees have in common?\nRelations Database relations involving people or things. Whose phone numbers do you know by heart? Which is higher, clouds or the sun?\nInterpersonal Relations Interpersonal relations [Heider, 1958]. If a friend lied to you, how could he or she regain your trust? List the top five things that you like to do with your friends.\nEpisodic Memory Episodic memory [Tulving, 1983; Hasselmo, 2012]. Try to name all of the people you have talked with today. Tell about a time when you felt proud of someone.\nDevices/Appliances Devices. What tools do you need for drawing? What is let in or kept out by windows?\nMental States Mental states. Why do people make New Year\u2019s resolutions? If a friend lied to you, how could he or she regain your trust?\nAnimals Animals. Why might a bear with a cub be more dangerous than a bear by itself? Name an animal that can walk as soon as it is born.\nLexicon English lexicon or dictionary. What does it mean to be \u201con cloud nine\u201d? What does it mean to \u201cgo the extra mile\u201d?\nEmotions Human emotions. Describe how it feels to watch someone opening a gift that you gave. How can you tell when someone is nervous about something?\nShapes Shapes of objects. Name three objects that are shaped like a triangle. Draw polka dots.\nSounds Sounds. What noise would a dragon make? What kinds of shoes are noisy?\nLocation Locations and places. Describe your favorite place to go for a walk. Name three jobs that involve working outdoors.\nPlants Plants. During which season might you rake leaves? What makes a salad a salad?\nFood Food and cooking. Think of a fruit and a vegetable that begin with the letter p. Name three foods that are purple.\nWeather Weather. Name three ways to have fun on a rainy day. Where is the safest place to be during a thunderstorm?\nLetters The alphabet and letters. Try to make your body into the shape of each letter in your name. Which letters of the alphabet can you draw using only curved lines?\nTaste Taste. Name three foods that might cause you to make a face when you eat them.\nSmell Smell. What is the best smell in spring?"}, {"heading": "Types of Reasoning Needed", "text": "Statistics on the types of reasoning needed to answer questions are shown in Table 8. Again, the percentages sum to more than 100 because each question is annotated with one or more reasoning types.\nDatabase Retrieval Database retrieval. Who is the tallest person you know? Name three animals that hatch from eggs.\nSimulation Simulation of the course of events, not necessarily requiring physical or three-dimensional reasoning. Is it easier to swallow a pill or a spoonful of medicine? Why might a bear with a cub be more dangerous than a bear by itself?\nPlanning Planning or generating a sequence of actions to achieve a goal [Ghallab et al., 2004]."}, {"heading": "Reasoning Type % Questions # Questions", "text": "What might happen if televisions everywhere stopped working? Describe your favorite place to go for a walk.\nComparison Quantitative or qualitative comparison. Who is the tallest person you know? What do elbows and knees have in common?\nEpisodic Memory Retrieving or recalling personal experiences from episodic memory. Have you ever been so busy that you forgot to eat a meal? What mistakes have you made that you\u2019ve learned from?\nVisualization Visualization and imagery. How are a bird\u2019s wings different from a butterfly\u2019s wings? Of the stars, the moon, and the sun, which can be seen during the day?\n3D Simulation Physical or three-dimensional simulation. Try to clap your hands behind your back. Why don\u2019t we wear watches on our ankles?\nInvention Inventing or creating something. Describe a toy that you would like to invent. Make up a word that means \u201cso funny you can\u2019t stop laughing.\u201d\nArithmetic Arithmetic operations. How many inches have you grown in the past year? In what year will you be able to register to vote?"}, {"heading": "Correlation with Question Position", "text": "The correlation of various annotations with position in the BrainPlay column is given in Table 9. Only correlations with magnitude above 0.1 are shown. The Highlights editors present the BrainPlay questions in increasing order of difficulty [Myers and Myers, 1964], so these correlations give a rough idea of difficulty. High positive correlations correspond to high difficulty, whereas high negative correlations correspond to low difficulty."}, {"heading": "BrainPlay\u2019s Coverage of Intelligence", "text": "We can use the major sections of the fifth edition of The Cognitive Neurosciences [Gazzaniga and Mangun, 2014] as a guide to the many areas of human intelligence. A rough correspondence between these sections and BrainPlay is shown in Table 10. (\u201cVI Memory\u201d includes prediction and imagination.) We see that BrainPlay questions span many aspects of intelligence. By design and intent, many of the thought-provoking children\u2019s questions are designed to push the system into gener-\nating new knowledge, because many of the answers are openended and most often unlikely to have been seen before and stored explicitly. It is a hallmark of human-level intelligence that new knowledge can and often must be generated from existing knowledge when needed to accomplish a novel goal, and these questions are designed to exercise and expose those mechanisms."}, {"heading": "Related Work", "text": "In Aristo [Clark, 2015], a multiple choice elementary school science exam question is taken as input, and an answer is produced as output. Whereas Aristo probes science knowledge studied in school, the BrainPlay/TPCQ task explores knowledge any child acquires simply through experience. Elementary science exam questions evaluate understanding of connections learned in school, while TPCQs encourage creation of new connections. In the bAbI tasks [Weston et al., 2015], a simple story and question about the story are taken as input, and an answer is produced as output. The stories are generated using a simulator based on a simple world containing characters and objects. The questions are very simple and restricted compared to TPCQs. The MCTest dataset [Richardson et al., 2013] consists of short stories, multiple choice questions about the stories, and correct answers to the questions. The questions were designed such that answering them (1) requires information from two or more story sentences and (2) does not require a knowledge base. MCTest questions evaluate the ability to read, understand, and combine information provided in a text. TPCQs require knowledge and experience not provided in the question. In the recognizing textual entailment (RTE) task [Dagan et al., 2013], a text T and a hypothesis H are taken as input, and a label T entails H , H contradicts T , or unknown is produced as output. RTE is quite general, and resources that recognize entailment could be used as resources for performing the TPCQ task. The Winograd schema (WS) challenge [Levesque et al., 2012] is a variant of the RTE task more heavily focused on reasoning. In the VQA task [Antol et al., 2015], an image and a multiple choice or open-ended question about the image are taken as input, and an answer is produced as output. The VQA task often involves significant reasoning, like the TPCQ task. At the Center for Brains, Minds and Machines, the Turing++ questions on images [Poggio and Meyers, 2016] will be used to evaluate not only a system\u2019s responses to questions, but also how accurately the system matches human behavior and neural physiology. The system will be compared with fMRI and MEG recordings in humans and monkeys."}, {"heading": "Conclusion", "text": "Highlights BrainPlay questions can be answered by young children. If today\u2019s artificial intelligence systems can\u2019t even answer these questions, how can we really say that they are intelligent? We believe that building systems that can answer and learn from BrainPlay questions will increase progress in artificial intelligence."}, {"heading": "Acknowledgments", "text": "We thank Kent Johnson, CEO of Highlights for Children, Inc., for permission to use the BrainPlay questions. We also thank Patricia M. Mikelson and Sharon M. Umnik at Highlights for providing us with the BrainPlay material."}], "references": [{"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "CoRR, abs/1505.00468,", "citeRegEx": "Antol et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Elementary school science and math tests as a driver for AI: Take the Aristo Challenge! In Blai Bonet and Sven Koenig", "author": ["Peter Clark"], "venue": "editors, Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pages 4019\u20134021, Palo Alto, CA,", "citeRegEx": "Clark. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Recognizing textual entailment: Models and applications", "author": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto"], "venue": "Morgan & Claypool, San Rafael, CA,", "citeRegEx": "Dagan et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Theory of Mind: How Children Understand Others\u2019 Thoughts and Feelings", "author": ["Martin J. Doherty"], "venue": "Psychology Press, East Sussex,", "citeRegEx": "Doherty. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Watson: Beyond Jeopardy! Artificial Intelligence", "author": ["David Ferrucci", "Anthony Levas", "Sugato Bagchi", "David Gondek", "Erik T. Mueller"], "venue": "199\u2013200:93\u2013 105,", "citeRegEx": "Ferrucci et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "editors", "author": ["Michael S. Gazzaniga", "George R. Mangun"], "venue": "The Cognitive Neurosciences. MIT Press, Cambridge, MA, fifth edition,", "citeRegEx": "Gazzaniga and Mangun. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Automated Planning: Theory and Practice", "author": ["Malik Ghallab", "Dana Nau", "Paolo Traverso"], "venue": "Morgan Kaufmann, San Francisco,", "citeRegEx": "Ghallab et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "How We Remember: Brain Mechanisms of Episodic Memory", "author": ["Michael E. Hasselmo"], "venue": "MIT Press, Cambridge, MA,", "citeRegEx": "Hasselmo. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Hillsdale", "author": ["Fritz Heider. The Psychology of Interpersonal Relations. Lawrence Erlbaum"], "venue": "NJ,", "citeRegEx": "Heider. 1958", "shortCiteRegEx": null, "year": 1958}, {"title": "The Winograd schema challenge", "author": ["Levesque et al", "2012] Hector J. Levesque", "Ernest Davis", "Leora Morgenstern"], "venue": "Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Courtesy of Patricia M", "author": ["Garry Cleveland Myers", "Caroline Clark Myers. Unpublished interview with Garry Cleveland Myers", "Caroline Clark Myers"], "venue": "Mikelson,", "citeRegEx": "Myers and Myers. 1964", "shortCiteRegEx": null, "year": 1964}, {"title": "Columbus", "author": ["Garry Cleveland Myers. Headwork for elementary school children. Highlights for Children"], "venue": "Ohio,", "citeRegEx": "Myers. 1968", "shortCiteRegEx": null, "year": 1968}, {"title": "Turing++ questions: A test for the science of (human) intelligence", "author": ["Tomaso Poggio", "Ethan Meyers"], "venue": "AI Magazine, 37(1):73\u201377,", "citeRegEx": "Poggio and Meyers. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Richardson et al", "2013] Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "and Understanding: An Inquiry into Human Knowledge Structures", "author": ["Roger C. Schank", "Robert P. Abelson. Scripts", "Plans", "Goals"], "venue": "Lawrence Erlbaum, Hillsdale, NJ,", "citeRegEx": "Schank and Abelson. 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "DeepFace: Closing the gap to human-level performance in face verification", "author": ["Taigman et al", "2014] Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Oxford University Press", "author": ["Endel Tulving. Elements of episodic memory"], "venue": "New York,", "citeRegEx": "Tulving. 1983", "shortCiteRegEx": null, "year": 1983}, {"title": "Mind", "author": ["Alan M. Turing. Computing machinery", "intelligence"], "venue": "59(236):433\u2013460,", "citeRegEx": "Turing. 1950", "shortCiteRegEx": null, "year": 1950}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "CoRR, abs/1502.05698,", "citeRegEx": "Weston et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Headwork: Open ended questions except a few for the very young", "author": ["Jean Wood"], "venue": "Courtesy of Sharon M. Umnik,", "citeRegEx": "Wood. 1986", "shortCiteRegEx": null, "year": 1986}], "referenceMentions": [{"referenceID": 4, "context": "As artificial intelligence tasks like fact-based question answering [Ferrucci et al., 2013] and face recognition [Taigman et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 11, "context": "The questions are designed to \u201c[stimulate] children from five to twelve to think and reason by working over in their heads what is already there, arriving at new ideas not learned from books\u201d [Myers, 1968].", "startOffset": 192, "endOffset": 205}, {"referenceID": 19, "context": "BrainPlay first appeared in the second issue of Highlights in September 1946 [Wood, 1986].", "startOffset": 77, "endOffset": 89}, {"referenceID": 10, "context": "Each month, BrainPlay presents around 20 questions arranged by age level [Myers and Myers, 1964].", "startOffset": 73, "endOffset": 96}, {"referenceID": 3, "context": "Theory of Mind Evaluates theory of mind [Doherty, 2009].", "startOffset": 40, "endOffset": 55}, {"referenceID": 17, "context": "Because there is no gold standard answer key for them, answers can be judged for plausibility by human judges, as in the Turing test [Turing, 1950].", "startOffset": 133, "endOffset": 147}, {"referenceID": 14, "context": "Scripts Stereotypical situations and scripts [Schank and Abelson, 1977].", "startOffset": 45, "endOffset": 71}, {"referenceID": 14, "context": "Plans/Goals Goals and plans [Schank and Abelson, 1977].", "startOffset": 28, "endOffset": 54}, {"referenceID": 8, "context": "Interpersonal Relations Interpersonal relations [Heider, 1958].", "startOffset": 48, "endOffset": 62}, {"referenceID": 16, "context": "Episodic Memory Episodic memory [Tulving, 1983; Hasselmo, 2012].", "startOffset": 32, "endOffset": 63}, {"referenceID": 7, "context": "Episodic Memory Episodic memory [Tulving, 1983; Hasselmo, 2012].", "startOffset": 32, "endOffset": 63}, {"referenceID": 6, "context": "Planning Planning or generating a sequence of actions to achieve a goal [Ghallab et al., 2004].", "startOffset": 72, "endOffset": 94}, {"referenceID": 10, "context": "The Highlights editors present the BrainPlay questions in increasing order of difficulty [Myers and Myers, 1964], so these correlations give a rough idea of difficulty.", "startOffset": 89, "endOffset": 112}, {"referenceID": 5, "context": "We can use the major sections of the fifth edition of The Cognitive Neurosciences [Gazzaniga and Mangun, 2014] as a guide to the many areas of human intelligence.", "startOffset": 82, "endOffset": 110}, {"referenceID": 1, "context": "In Aristo [Clark, 2015], a multiple choice elementary school science exam question is taken as input, and an answer is produced as output.", "startOffset": 10, "endOffset": 23}, {"referenceID": 18, "context": "In the bAbI tasks [Weston et al., 2015], a simple story and question about the story are taken as input, and an answer is produced as output.", "startOffset": 18, "endOffset": 39}, {"referenceID": 2, "context": "In the recognizing textual entailment (RTE) task [Dagan et al., 2013], a text T and a hypothesis H are taken as input, and a label T entails H , H contradicts T , or unknown is produced as output.", "startOffset": 49, "endOffset": 69}, {"referenceID": 0, "context": "In the VQA task [Antol et al., 2015], an image and a multiple choice or open-ended question about the image are taken as input, and an answer is produced as output.", "startOffset": 16, "endOffset": 36}, {"referenceID": 12, "context": "At the Center for Brains, Minds and Machines, the Turing questions on images [Poggio and Meyers, 2016] will be used to evaluate not only a system\u2019s responses to questions, but also how accurately the system matches human behavior and neural physiology.", "startOffset": 77, "endOffset": 102}], "year": 2017, "abstractText": "We propose to use thought-provoking children\u2019s questions (TPCQs), namely Highlights BrainPlay questions, as a new method to drive artificial intelligence research and to evaluate the capabilities of general-purpose AI systems. These questions are designed to stimulate thought and learning in children, and they can be used to do the same thing in AI systems, while demonstrating the system\u2019s reasoning capabilities to the evaluator. We introduce the TPCQ task, which which takes a TPCQ question as input and produces as output (1) answers to the question and (2) learned generalizations. We discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay questions, and we report statistics on question type, question class, answer cardinality, answer class, types of knowledge needed, and types of reasoning needed. We find that BrainPlay questions span many aspects of intelligence. Because the answers to BrainPlay questions and the generalizations learned from them are often highly open-ended, we suggest using human judges for evaluation.", "creator": "LaTeX with hyperref package"}}}