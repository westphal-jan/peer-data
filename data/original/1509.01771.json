{"id": "1509.01771", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2015", "title": "Sampled Weighted Min-Hashing for Large-Scale Topic Mining", "abstract": "We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to automatically mine topics from large-scale corpora. SWMH generates multiple random partitions of the corpus vocabulary based on term co-occurrence and agglomerates highly overlapping inter-partition cells to produce the mined topics. While other approaches define a topic as a probabilistic distribution over a vocabulary, SWMH topics are ordered subsets of such vocabulary. Interestingly, the topics mined by SWMH underlie themes from the corpus at different levels of granularity. We extensively evaluate the meaningfulness of the mined topics both qualitatively and quantitatively on the NIPS (1.7 K documents), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora. Additionally, we compare the quality of SWMH with Online LDA topics for document representation in classification.", "histories": [["v1", "Sun, 6 Sep 2015 06:09:28 GMT  (295kb,D)", "http://arxiv.org/abs/1509.01771v1", "10 pages, Proceedings of the Mexican Conference on Pattern Recognition 2015"], ["v2", "Tue, 8 Sep 2015 03:14:12 GMT  (262kb,D)", "http://arxiv.org/abs/1509.01771v2", "10 pages, Proceedings of the Mexican Conference on Pattern Recognition 2015"]], "COMMENTS": "10 pages, Proceedings of the Mexican Conference on Pattern Recognition 2015", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["gibran fuentes-pineda", "ivan vladimir meza-ruiz"], "accepted": false, "id": "1509.01771"}, "pdf": {"name": "1509.01771.pdf", "metadata": {"source": "CRF", "title": "Sampled Weighted Min-Hashing for Large-Scale Topic Mining", "authors": ["Gibran Fuentes-Pineda", "Ivan Vladimir Meza-Ru\u0131\u0301z"], "emails": [], "sections": [{"heading": null, "text": "Keywords: large-scale topic mining, min-hashing, co-occurring terms"}, {"heading": "1 Introduction", "text": "The automatic extraction of topics has become very important in recent years since they provide a meaningful way to organize, browse and represent large-scale collections of documents. Among the most successful approaches to topic discovery are directed topic models such as Latent Dirichlet Allocation (LDA) [1] and Hierarchical Dirichlet Processes (HDP) [15] which are Directed Graphical Models with latent topic variables. More recently, undirected graphical models have been also applied to topic modeling, (e.g., Boltzmann Machines [13, 12] and Neural Autoregressive Distribution Estimators [9]). The topics generated by both directed and undirected models have been shown to underlie the thematic structure of a text corpus. These topics are defined as distributions over terms of a vocabulary and documents in turn as distributions over topics. Traditionally, inference in topic models has not scale well to large corpora, however, more efficient strategies have been proposed to overcome this problem (e.g., Online LDA [8] and stochastic variational inference [11]). Undirected Topic Models can be also trained efficiently using approximate strategies such as Contrastive Divergence [7].\nIn this work, we explore the mining of topics based on term co-occurrence. The underlying intuition is that terms consistently co-occurring in the same documents are likely to belong to the same topic. The resulting topics correspond to ordered subsets of the vocabulary rather than distributions over such a vocabulary. Since finding\nar X\niv :1\n50 9.\n01 77\n1v 1\n[ cs\n.L G\n] 6\nS ep\n2 01\n5\nco-occurring terms is a combinatorial problem that lies in a large search space, we propose Sampled Weighted Min-Hashing (SWMH), an extended version of Sampled MinHashing (SMH) [6]. SMH partitions the vocabulary into sets of highly co-occurring terms by applying Min-Hashing [2] to the inverted file entries of the corpus. The basic idea of Min-Hashing is to generate random partitions of the space so that sets with high Jaccard similarity are more likely to lie in the same partition cell.\nOne limitation of SMH is that the generated random partitions are drawn from uniform distributions. This setting is not ideal for information retrieval applications where weighting have a positive impact on the quality of the retrieved documents [14, 3]. For this reason, we extend SMH by allowing weights in the mining process which effectively extends the uniform distribution to a distribution based on weights. We demonstrate the validity and scalability of the proposed approach by mining topics in the NIPS, 20 Newsgroups, Reuters and Wikipedia corpora which range from small (a thousand of documents) to large scale (millions of documents). Table 1 presents some examples of mined topics and their sizes. Interestingly, SWMH can mine meaningful topics of different levels of granularity.\nThe remainder of the paper is organized as follows. Section 2 reviews the MinHashing scheme for pairwise set similarity search. The proposed approach for topic mining by SWMH is described in Sect. 3. Section 4 reports the experimental evaluation of SWMH as well as a comparison against Online LDA. Finally, Sect. 5 concludes the paper with some discussion and future work."}, {"heading": "2 Min-Hashing for Pairwise Similarity Search", "text": "Min-Hashing is a randomized algorithm for efficient pairwise set similarity search (see Algorithm 1). The basic idea is to define MinHash functions h with the property that the probability of any two sets A1, A2 having the same MinHash value is equal to their Jaccard Similarity, i.e.,\nP [h(A1) = h(A2)] = | A1 \u2229A2 | | A1 \u222aA2 | \u2208 [0, 1]. (1)\nEach MinHash function h is realized by generating a random permutation \u03c0 of all the elements and assigning the first element of a set on the permutation as its MinHash value. The rationale behind Min-Hashing is that similar sets will have a high probability of taking the same MinHash value whereas dissimilar sets will have a low probability. To cope with random fluctuations, multiple MinHash values are computed for each set from independent random permutations. Remarkably, it has been shown that the portion of identical MinHash values between two sets is an unbiased estimator of their Jaccard similarity [2].\nTaking into account the above properties, in Min-Hashing similar sets are retrieved by grouping l tuples g1, . . . , gl of r different MinHash values as follows\ng1(A1) = (h1(A1), h2(A1), . . . , hr(A1)) g2(A1) = (hr+1(A1), hr+2(A1), . . . , h2\u00b7r(A1)) \u00b7 \u00b7 \u00b7 gl(A1) = (h(l\u22121)\u00b7r+1(A1), h(l\u22121)\u00b7r+2(A1), . . . , hl\u00b7r(A1)) ,\nwhere hj(A1) is the j-th MinHash value. Thus, l different hash tables are constructed and two sets A1, A2 are stored in the same hash bucket on the k-th hash table if gk(A1) = gk(A2), k = 1, . . . , l. Because similar sets are expected to agree in several MinHash values, they will be stored in the same hash bucket with high probability. In contrast, dissimilar sets will seldom have the same MinHash value and therefore the probability that they have an identical tuple will be low. More precisely, the probability that two sets A1, A2 agree in the r MinHash values of a given tuple gk is P [gk(A1) = gk(A2)] = sim(A1, A2)\nr. Therefore, the probability that two sets A1, A2 have at least one identical tuple is Pcollision[A1, A2] = 1\u2212 (1\u2212 sim(A1, A2)r)l.\nThe original Min-Hashing scheme was extended by Chum et al. [5] to weighted set similarity, defined as\nsimhist(H1, H2) =\n\u2211 i wi min(H i 1, H i 2)\u2211\ni wi max(H i 1, H i 2) \u2208 [0, 1], (2)\nwhere Hi1, H i 2 are the frecuencies of the i-th element in the histograms H1 and H2 respectively and wi is the weight of the element. In this scheme, instead of generating random permutations drawn from a uniform distribution, the permutations are drawn from a distribution based on element weights. This extension allows the use of popular document representations based on weighting schemes such as tf-idf and has been applied to image retrieval [5] and clustering [4]."}, {"heading": "3 Sampled Min-Hashing for Topic Mining", "text": "Min-Hashing has been used in document and image retrieval and classification, where documents and images are represented as bags of words. Recently, it was also successfully applied to retrieving co-occurring terms by hashing the inverted file lists instead of the documents [5, 6]. In particular, Fuentes-Pineda et al. [6] proposed Sampled Min-Hashing (SMH), a simple strategy based on Min-Hashing to discover objects from large-scale image collections. In the following, we briefly describe SMH using the notation of terms, topics and documents, although it can be generalized to any type of\nAlgorithm 1: Pairwise Similarity Search by Min-Hashing Data: Database of sets A = A1, . . . , AN and query set q Result: Similar sets to q in A Indexing\n1. Compute l MinHash tuples gi(Aj), i = 1, . . . , l for each set Aj , j = 1, . . . , N in A. 2. Construct l hash tables and store each set Aj , j = 1, . . . , N in the buckets corresponding to gi(Aj), i = 1, . . . , l.\nQuerying\n1. Compute the l MinHash tuples gi(q), i = 1, . . . , l for the query set q. 2. Retrieve the sets stored in the buckets corresponding to gi(q), i = 1, . . . , l. 3. Compute the similarity between each retrieved set and q and return those with similarity\ngreater than a given threshold .\ndyadic data. The underlying idea of SMH is to mine groups of terms with high Jaccard Co-occurrence Coefficient (JCC), i.e.,\nJCC(T1, . . . , Tk) = |T1 \u2229 T2 \u2229 \u00b7 \u00b7 \u00b7 \u2229 Tk| |T1 \u222a T2 \u222a \u00b7 \u00b7 \u00b7 \u222a Tk| , (3)\nwhere the numerator correspond to the number of documents in which terms T1, . . . , Tk co-occur and the denominator is the number of documents with at least one of the k terms. Thus, Eq. 1 can be extended to multiple co-occurring terms as\nP [h(T1) = h(T2) . . . = h(Tk)] = JCC(T1, . . . , Tk). (4)\nFrom Eqs. 3 and 4, it is clear that the probability that all terms T1, . . . , Tk have the same MinHash value depends on how correlated their occurrences are: the more correlated the higher is the probability of taking the same MinHash value. This implies that terms consistently co-occurring in many documents will have a high probability of taking the same MinHash value.\nIn the same way as pairwise Min-Hashing, l tuples of r MinHash values are computed to find groups of terms with identical tuple, which become a co-occurring term set. By choosing r and l properly, the probability that a group of k terms has an identical tuple approximates a unit step function such that\nPcollision[T1, . . . , Tk] \u2248 { 1 if JCC(T1, . . . , Tk) \u2265 s\u2217 0 if JCC(T1, . . . , Tk) < s\u2217 ,\nHere, the selection of r and l is a trade-off between precision and recall. Given s\u2217 and r, we can determine l by setting Pcollision[T1, . . . , Tk] to 0.5, which gives\nl = log(0.5)\nlog(1\u2212 s\u2217r) .\nIn SMH, each hash table can be seen as a random partitioning of the vocabulary into disjoint groups of highly co-occurring terms, as illustrated in Fig. 1. Different partitions are generated and groups of discriminative and stable terms belonging to the same topic are expected to lie on overlapping inter-partition cells. Therefore, we cluster co-occurring term sets that share many terms in an agglomerative manner. We measure the proportion of terms shared between two co-occurring term sets C1 and C2 by their overlap coefficient, namely\novr(C1, C2) = | C1 \u2229 C2 |\nmin(| C1 |, | C2 |) \u2208 [0, 1].\nSince a pair of co-occurring term sets with high Jaccard similarity will also have a large overlap coefficient, finding pairs of co-occurring term sets can be speeded up by using Min-Hashing, thus avoiding the overhead of computing the overlap coefficient between all the pairs of co-occurring term sets.\nThe clustering stage merges chains of co-occurring term sets with high overlap coefficient into the same topic. As a result, co-occurring term sets associated with the same topic can belong to the same cluster even if they do not share terms with one another, as long as they are members of the same chain. In general, the generated clusters have the property that for any co-occurring term set, there exists at least one co-occurring term set in the same cluster with which it has an overlap coefficient greater than a given threshold .\nWe explore the use of SMH to mine topics from documents but we judge term cooccurrence by the Weighted Co-occurrence Coefficient (WCC), defined as\nWCC (T1, . . . , Tk) =\n\u2211 i wi min (T i 1, \u00b7 \u00b7 \u00b7 , T ik)\u2211\ni wi max (T i 1, \u00b7 \u00b7 \u00b7 , T ik)\n\u2208 [0, 1], (5)\nwhere T i1, \u00b7 \u00b7 \u00b7 , T ik are the frecuencies in which terms T1, . . . , Tk occur in the i-th document and the weight wi is given by the inverse of the size of the i-th document. We exploit the extended Min-Hashing scheme by Chum et al. [5] to efficiently find such co-occurring terms. We call this topic mining strategy Sampled Weighted Min-Hashing (SWMH) and summarize it in Algorithm 2.\nAlgorithm 2: Topic mining by SWMH Data: Inverted File Lists T = T1, . . . , TN Result: Mined Topics O = O1, . . . , OM Partitioning\n1. Compute l MinHash tuples gi(Tj), i = 1, . . . , l for each list Tj , j = 1, . . . , N in T . 2. Construct l hash tables and store each list Tj , j = 1, . . . , N in the bucket corresponding to gi(Tj), i = 1, . . . , l. 3. Mark each group of lists stored in the same bucket as a co-occurring term set.\nClustering\n1. Find pairs of co-occurring term sets with overlap coefficient greater than a given threshold . 2. Form a graph G with co-occurring term sets as vertices and edges defined between pairs with overlap coefficient greater than . 3. Mark each connected component of G as a topic."}, {"heading": "4 Experimental Results", "text": "In this section, we evaluate different aspects of the mined topics. First, we present a comparison between the topics mined by SWMH and SMH. Second, we evaluate the scalability of the proposed approach. Third, we use the mined topics to perform document classification. Finally, we compare SWMH topics with Online LDA topics.\nThe corpora used in our experiments were: NIPS, 20 Newsgroups, Reuters and Wikipedia1. NIPS is a small collection of articles (3, 649 documents), 20 Newsgroups is a larger collection of mail newsgroups (34, 891 documents), Reuters is a medium size collection of news (137, 589 documents) and Wikipedia is a large-scale collection of encyclopedia articles (1, 265, 756 documents) 2.\nAll the experiments presented in this work were performed on an Intel(R) Xeon(R) 2.66GHz workstation with 8GB of memory and with 8 processors. However, we would like to point out that the current version of the code is not parallelized, so we did not take advantage of the multiple processors."}, {"heading": "4.1 Comparison between SMH and SWMH", "text": "For these experiments, we used the NIPS and Reuters corpora and different values of the parameters s\u2217 and r, which define the number of MinHash tables. We set the parameters of similarity (s\u2217) to 0.15, 0.13 and 0.10 and the tuple size (r) to 3 and 4. These parameters rendered the following table sizes: 205, 315, 693, 1369, 2427, 6931. Figure 2 shows the effect of weighting on the amount of mined topics. First, notice the breaking point on both figures when passing from 1369 to 2427 tables. This effect corresponds to\n1 Wikipedia dump from 2013\u201309\u201304. 2 All corpora were preprocessed to cut off terms that appeared less than 6 times in the whole\ncorpus.\nresetting the s\u2217 to .10 when changing r from 3 to 4. Lower values in s\u2217 are more strict and therefore less topics are mined. Figure 2 also shows that the amount of mined topics is significantly reduced by SWMH, since the colliding terms not only need to appear on similar documents but now with similar proportions. The effect of using SWMH is also noticeable in the number of terms that compose a topic. The maximum reduction reached in NIPS was 73% while in Reuters was 45%."}, {"heading": "4.2 Scalability evaluation", "text": "To test the scalability of SWMH, we measured the time and memory required to mine topics in the Reuters corpus while increasing the number of documents to be analyzed. In particular, we perform 10 experiments with SWMH, each increasing the number of documents by 10% 3. Figure 3 illustrates the time taken to mine topics as we increase the number of documents and as we increase an index of complexity given by a combination of the size of the vocabulary and the average number of times a term appears in a document. As can be noticed, in both cases the time grows almost linearly and is in the thousand of seconds.\nThe mining times for the corpora were: NIPS, 43s; 20 Newsgroups, 70s; Reuters, 4, 446s and Wikipedia, 45, 834s. These times contrast with the required time by Online LDA to model 100 topics 4: NIPS, 60s; 20 Newsgroups, 154s and Reuters, 25, 997. Additionally, we set Online LDA to model 400 topics with the Reuters corpus and took 3 days. Memory figures follow a similar behavior to the time figures. Maximum memory: NIPS, 141MB; 20 Newsgroups, 164MB; Reuters, 530MB and Wikipedia, 1, 500MB."}, {"heading": "4.3 Document classification", "text": "In this evaluation we used the mined topics to create a document representation based on the similarity between topics and documents. This representation was used to train\n3 The parameters were fixed to s\u2217 = 0.1,r = 3, and overlap threshold of 0.7. 4 https://github.com/qpleple/online-lda-vb was adapted to use our file for-\nmats.\nan SVM classifier with the class of the document. In particular, we focused on the 20 Newsgroups corpus for this experiment. We used the typical setting of this corpus for document classification (60% training, 40% testing). Table 2 shows the performance for different variants of topics mined by SWMH and Online LDA topics. The results illustrate that the number of topics is relevant for the task: Online LDA with 400 topics is better than 100 topics. A similar behavior can be noticed for SWMH, however, the parameter r has an effect on the content of the topics and therefore on the performance."}, {"heading": "4.4 Comparison between mined and modeled topics", "text": "In this evaluation we compare the quality of the topics mined by SWMH against Online LDA topics for the 20 Newsgroups and Reuters corpora. For this we measure topic coherence, which is defined as\nC(t) = M\u2211 m=2 m\u22121\u2211 l=1 log D(vm, vl) D(vl) ,\nwhereD(vl) is the document frequency of the term vl, andD(vm, vl) is the co-document frequency of the terms vm and vl [10]. This metric depends on the first M elements of the topics. For our evaluations we fixed M to 10. However, we remark that the comparison is not direct since both the SWMH and Online LDA topics are different in nature: SWMH topics are subsets of the vocabulary with uniform distributions while Online LDA topics are distributions over the complete vocabulary. In addition, Online LDA\ngenerates a fixed number of topics which is in the hundreds while SWMH produces thousands of topics. For the comparison we chose the n-best mined topics by ranking them using an ad hoc metric involving the co-occurrence of the first element of the topic. For the purpose of the evaluation we limited the SWMH to the 500 best ranked topics. Figure 4 shows the coherence for each corpus. In general, we can see a difference in the shape and quality of the coherence box plots. However, we notice that SWMH produces a considerable amount of outliers, which calls for further research in the ranking of the mined topics and their relation with the coherence."}, {"heading": "5 Discussion and Future Work", "text": "In this work we presented a large-scale approach to automatically mine topics in a given corpus based on Sampled Weighted Min-Hashing. The mined topics consist of subsets of highly correlated terms from the vocabulary. The proposed approach is able to mine topics in corpora which go from the thousands of documents (1 min approx.) to the millions of documents (7 hrs. approx.), including topics similar to the ones produced by Online LDA. We found that the mined topics can be used to represent a document for classification. We also showed that the complexity of the proposed approach grows linearly with the amount of documents. Interestingly, some of the topics mined by SWMH are related to the structure of the documents (e.g., in NIPS the words in the first topic correspond to parts of an article) and others to specific groups (e.g., team sports in 20 Newsgroups and Reuters, or the Transformers universe in Wikipedia). These examples suggest that SWMH is able to generate topics at different levels of granularity.\nFurther work has to be done to make sense of overly specific topics or to filter them out. In this direction, we found that weighting the terms has the effect of discarding several irrelevant topics and producing more compact ones. Another alternative, it is to restrict the vocabulary to the top most frequent terms as done by other approaches. Other interesting future work include exploring other weighting schemes, finding a better representation of documents from the mined topics and parallelizing SWMH."}], "references": [{"title": "Latent Dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "On the resemblance and containment of documents", "author": ["Andrei Z. Broder"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "The importance of proper weighting methods", "author": ["Christopher Buckley"], "venue": "In Proceedings of the Workshop on Human Language Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "Large-scale discovery of spatially related images", "author": ["Ondrej Chum", "Jiri Matas"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Near duplicate image detection: min-hash and tf-idf weighting", "author": ["Ondrej Chum", "James Philbin", "Andrew Zisserman"], "venue": "In Proceedings of the British Machine Vision Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Scalable object discovery: A hash-based approach to clustering co-occurring visual words", "author": ["Gibran Fuentes Pineda", "Hisashi Koga", "Toshinori Watanabe"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2024}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Online learning for latent Dirichlet allocation", "author": ["Matthew D. Hoffman", "David M. Blei", "Francis Bach"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "A neural autoregressive topic model", "author": ["Hugo Larochelle", "Lauly Stanislas"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Optimizing semantic coherence in topic models", "author": ["David Mimno", "Hanna M. Wallach", "Edmund Talley", "Miriam Leenders", "Andrew McCallum"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Sparse stochastic inference for latent Dirichlet allocation", "author": ["David Mimno", "Matthew D. Hoffman", "David M. Blei"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Modeling documents with a deep Boltzmann machine", "author": ["Geoffrey Hinton"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Replicated softmax: An undirected topic model", "author": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley"], "venue": "Information Processing & Management,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1988}, {"title": "Hierarchical Dirichlet processes", "author": ["Yee Whye Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Among the most successful approaches to topic discovery are directed topic models such as Latent Dirichlet Allocation (LDA) [1] and Hierarchical Dirichlet Processes (HDP) [15] which are Directed Graphical Models with latent topic variables.", "startOffset": 124, "endOffset": 127}, {"referenceID": 14, "context": "Among the most successful approaches to topic discovery are directed topic models such as Latent Dirichlet Allocation (LDA) [1] and Hierarchical Dirichlet Processes (HDP) [15] which are Directed Graphical Models with latent topic variables.", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": ", Boltzmann Machines [13, 12] and Neural Autoregressive Distribution Estimators [9]).", "startOffset": 21, "endOffset": 29}, {"referenceID": 11, "context": ", Boltzmann Machines [13, 12] and Neural Autoregressive Distribution Estimators [9]).", "startOffset": 21, "endOffset": 29}, {"referenceID": 8, "context": ", Boltzmann Machines [13, 12] and Neural Autoregressive Distribution Estimators [9]).", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": ", Online LDA [8] and stochastic variational inference [11]).", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": ", Online LDA [8] and stochastic variational inference [11]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "Undirected Topic Models can be also trained efficiently using approximate strategies such as Contrastive Divergence [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "co-occurring terms is a combinatorial problem that lies in a large search space, we propose Sampled Weighted Min-Hashing (SWMH), an extended version of Sampled MinHashing (SMH) [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "SMH partitions the vocabulary into sets of highly co-occurring terms by applying Min-Hashing [2] to the inverted file entries of the corpus.", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "This setting is not ideal for information retrieval applications where weighting have a positive impact on the quality of the retrieved documents [14, 3].", "startOffset": 146, "endOffset": 153}, {"referenceID": 2, "context": "This setting is not ideal for information retrieval applications where weighting have a positive impact on the quality of the retrieved documents [14, 3].", "startOffset": 146, "endOffset": 153}, {"referenceID": 0, "context": "P [h(A1) = h(A2)] = | A1 \u2229A2 | | A1 \u222aA2 | \u2208 [0, 1].", "startOffset": 44, "endOffset": 50}, {"referenceID": 1, "context": "Remarkably, it has been shown that the portion of identical MinHash values between two sets is an unbiased estimator of their Jaccard similarity [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "[5] to weighted set similarity, defined as", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "simhist(H1, H2) = \u2211 i wi min(H i 1, H i 2) \u2211 i wi max(H i 1, H i 2) \u2208 [0, 1], (2)", "startOffset": 70, "endOffset": 76}, {"referenceID": 4, "context": "This extension allows the use of popular document representations based on weighting schemes such as tf-idf and has been applied to image retrieval [5] and clustering [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "This extension allows the use of popular document representations based on weighting schemes such as tf-idf and has been applied to image retrieval [5] and clustering [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 4, "context": "Recently, it was also successfully applied to retrieving co-occurring terms by hashing the inverted file lists instead of the documents [5, 6].", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "Recently, it was also successfully applied to retrieving co-occurring terms by hashing the inverted file lists instead of the documents [5, 6].", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "[6] proposed Sampled Min-Hashing (SMH), a simple strategy based on Min-Hashing to discover objects from large-scale image collections.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "ovr(C1, C2) = | C1 \u2229 C2 | min(| C1 |, | C2 |) \u2208 [0, 1].", "startOffset": 48, "endOffset": 54}, {"referenceID": 0, "context": ", Tk) = \u2211 i wi min (T i 1, \u00b7 \u00b7 \u00b7 , T i k) \u2211 i wi max (T i 1, \u00b7 \u00b7 \u00b7 , T i k) \u2208 [0, 1], (5)", "startOffset": 78, "endOffset": 84}, {"referenceID": 4, "context": "[5] to efficiently find such co-occurring terms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "whereD(vl) is the document frequency of the term vl, andD(vm, vl) is the co-document frequency of the terms vm and vl [10].", "startOffset": 118, "endOffset": 122}], "year": 2017, "abstractText": "We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to automatically mine topics from large-scale corpora. SWMH generates multiple random partitions of the corpus vocabulary based on term cooccurrence and agglomerates highly overlapping inter-partition cells to produce the mined topics. While other approaches define a topic as a probabilistic distribution over a vocabulary, SWMH topics are ordered subsets of such vocabulary. Interestingly, the topics mined by SWMH underlie themes from the corpus at different levels of granularity. We extensively evaluate the meaningfulness of the mined topics both qualitatively and quantitatively on the NIPS (1.7K documents), 20 Newsgroups (20K), Reuters (800K) and Wikipedia (4M) corpora. Additionally, we compare the quality of SWMH with Online LDA topics for document representation in classification.", "creator": "LaTeX with hyperref package"}}}