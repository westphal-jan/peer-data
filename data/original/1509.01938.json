{"id": "1509.01938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2015", "title": "Exploiting Out-of-Domain Data Sources for Dialectal Arabic Statistical Machine Translation", "abstract": "Statistical machine translation for dialectal Arabic is characterized by a lack of data since data acquisition involves the transcription and translation of spoken language. In this study we develop techniques for extracting parallel data for one particular dialect of Arabic (Iraqi Arabic) from out-of-domain corpora in different dialects of Arabic or in Modern Standard Arabic. We compare two different data selection strategies (cross-entropy based and submodular selection) and demonstrate that a very small but highly targeted amount of found data can improve the performance of a baseline machine translation system. We furthermore report on preliminary experiments on using automatically translated speech data as additional training data.", "histories": [["v1", "Mon, 7 Sep 2015 07:54:17 GMT  (28kb,D)", "http://arxiv.org/abs/1509.01938v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["katrin kirchhoff", "bing zhao", "wen wang"], "accepted": false, "id": "1509.01938"}, "pdf": {"name": "1509.01938.pdf", "metadata": {"source": "CRF", "title": "Exploiting Out-of-Domain Data Sources for Dialectal Arabic Statistical Machine Translation", "authors": ["Katrin Kirchhoff", "Bing Zhao", "Wen Wang"], "emails": ["kk2@u.washington.edu", "bingzhao@gmail.com", "wwang@speech.sri.com"], "sections": [{"heading": "1 Introduction", "text": "In the Arabic-speaking world, dialectal Arabic (DA) is used side-by-side with the standard form of the language, Modern Standard Arabic (MSA). Whereas the latter is used for written and formal oral communication (lectures, speeches), DA is used for everyday, casual communication. DA is almost never written; exceptions are transcriptions of spoken language, e.g., in novels, movie scripts, or in online blogs or forums. DA and MSA exhibit strong differences at the lexical, phonological, morphological, and syntactic levels; furthermore, the dialects themselves form a similarity continuum that ranges from closely related to mutual unintelligible. An overview of the main characteristics of DA can be found in (Habash, 2010). \u2217Work was done while the author was at SRI International.\nMost natural language processing (NLP) tools that have been developed for Arabic have been targeted towards MSA, for which large amounts of written data exist. NLP for DA suffers from a sparsity of tools as well as data. Work on DA annotation tools includes the development of morphological analyzers for Arabic dialects (Habash et al., 2005; Habash et al., 2012; Habash et al., 2013), treebanks (Maamouri et al., 2006) and parsers (Chiang et al., 2006), unsupervised (Duh and Kirchhoff, 2005) or supervised (Al-Sabbagh and Girju, 2012) training of POS taggers for DA, and lexicon acquisition (Duh and Kirchhoff, 2006). However, most of these have been targeted to the Egyptian or Levantine dialects and do not easily generalize to other dialects. There are a small number of speech and parallel text corpora for Egyptian, Levantine, and Iraqi DA, primarily available from the Linguistic Data Consortium (LDC) and the European Language Resources Association (ELRA). In general, however, spoken language needs to be recorded and transcribed to produce text data, which constitutes a bottleneck for the rapid acquisition of new data.\nThe lack of training data for DA in statistical machine translation (SMT) has only been addressed in a few previous studies; the standard approach has been to simply collect more training data by transcribing and translating DA speech. (Zbib et al., 2012) compare utilizing large amounts of MSA data for training and creating a small corpus of DA training data. They conclude that simply adding large amounts of mismatched (MSA) training data does not help, whereas even a small amount of dialectal data is very useful. Salloum and Habash (Salloum and Habash, 2011; Salloum and Habash, 2013) propose to transform DA to MSA by means of a combination of statistical processing and handcoded transformation rules, and to then apply MT ar X\niv :1\n50 9.\n01 93\n8v 1\n[ cs\n.C L\n] 7\nS ep\n2 01\n5\nsystems for MSA-to-English. Their work was on Egyptian Arabic, and porting this approach to a different dialect involves a fair amount of manual effort and dialect expertise. In (Aminian et al., 2014) the specific problem of out-of-vocabulary words in MT for DA is addressed by replacing DA words with their MSA equivalents.\nIn this paper we attempt to enrich available training data for Iraqi Arabic by automatically identifying IA-English parallel data in out-of-domain corpora of MSA and other dialects of Arabic. This procedure is based on the assumption that at least some dialects will exhibit similarities with IA. Corpora formally described as MSA may also contain dialectal data at the subsentential level due to codeswitching (mixed use of MSA and DA), which is common among Arabic speakers. In principle, automatic dialect identification methods (Alorifi, 2008; Sadat et al., 2014; Zaidan and Callison-Burch, 2014) might be used for this purpose; however, these methods are themselves error-prone and have not been developed for all dialects of Arabic. Our approach is to directly select data that is matched to features (n-grams) extracted from a sample corpus of the dialect of interest. In addition to finding dialect-matched data, the selected data is also likely to be matched with respect to topic and style. Two different data selection methods are investigated, the widely-used cross entropy method of (Moore and Lewis, 2010), and a more recent submodular data selection method (Wei et al., 2013). We demonstrate that the performance of SMT systems for IA can be improved by selecting a very small amount of highly targeted out-of-domain data. In addition, we conduct a preliminary investigation of the possibility of using automatically translated speech data as SMT training data.\nThe paper is structured as follows: we first report on previous work on data selection for SMT (Section 2). We then describe the submodular technique used in this paper in detail (Section 3). The data is described in Section 4; experiments are results are presented in Section 5. We provide conclusions in Section 6."}, {"heading": "2 Data Selection: Previous Work", "text": "A currently widely-used data selection method in SMT (which we also use as a baseline in Section 5) uses the cross-entropy between two language models (Moore and Lewis, 2010), one trained on the test set of interest, and another trained on generic or\nout-of-domain training data. We call this the crossentropy method. This method trains a test-set specific (or in-domain) language model, LMin, and a generic (out-of- or mixed-domain) language model, LMout. Each sentence x \u2208V in the training data is scored by both language models and is assigned the log ratio of the language model probabilities as a score:\nmce(x) = 1\n`(x) log[Pr(x|LMin)/Pr(x|LMout)] (1)\nwhere `(x) is the length of sentence x. Sentences are then ranked in descending order based on their scores and the top N sentences are chosen. Various extensions to this method have been proposed. In (Axelrod et al., 2011) the monolingual selection method is extended to bilingual corpora. In (Duh et al., 2013), neural language models are used instead of backoff language models. Finally, (Mediani et al., 2014) propose a different method for drawing the out-of-domain sample and the use of word-association models to improve the data for training the out-of-domain language model.\nThe cross-entropy approach ranks each sentence individually, without reference to other sentences. Thus, no sentence interactions can be modelled, such as redundancy at the sentential or subsentential level. Moreover, the method does not have a theoretical performance guarantee."}, {"heading": "3 Submodular Data Selection", "text": "Submodular functions (Edmonds, 1970; Fujishige, 2005) were first developed in mathematics, operations research and economics; more recently, they have been used for a variety of optimization problems in machine learning as well. For example, they have been applied to the problems of clustering (Narasimhan and Bilmes, 2007), observation selection (Krause et al., 2008), sensor placement (Krause and Guestrin, 2011), or image segmentation (Jegelka and Bilmes, 2011). Within natural language processing (NLP) submodular functions have been used for extractive text summarization (Lin and Bilmes, 2012).\nTo explain submodular functions, we introduce the following notation: assume a finite set of data elements V , the ground set. A valuation function f : 2V \u2192 R+ is then defined that returns a non-negative real value for any subset X \u2286V . The function f is called submodular if it satisfies the property of diminishing returns: for all X \u2286 Y and\nv /\u2208 Y , the following is true:\nf (X \u222a{v})\u2212 f (X)\u2265 f (Y \u222a{v})\u2212 f (Y ). (2)\nThis means that the incremental value (or gain) of element v decreases when the context in which v is considered grows from X to Y . The \u201cgain\u201d is defined as f (v|X), f (X \u222a{v})\u2212 f (X). Thus, f is submodular if f (v|X)\u2265 f (v|Y ). Submodularity is a natural model for data selection in SMT and other NLP tasks. The ground set V is the set of training data elements, and elements are selected from this set according to a submodular valuation function for any given subset of V . The value of this function diminishes for items that are (partially) redundant with other items in the already-selected subset, which is precisely the submodularity property. The specific function we utilize for the purpose of MT data selection is as follows:\nf (X) = \u2211 u\u2208U wu\u03c6u(\u2211 x\u2208X mu(x)) (3)\nHere, U is a set of features (such as words, n-grams, etc.), X is a subset of V , w is a non-negative weight, \u03c6 is a non-negative, non-decreasing concave function, and mu(x) is a score indicating how relevant u is in sample x. Thanks to the concave function, the contribution of each feature u in the context of an existing subset X diminishes as X grows.\nIn our work the feature set U consists of all ngrams up to a pre-specified length drawn from a representative in-domain data set. The feature relevance scores mu(x) are the tf-idf weighted counts of the the features (n-grams). The tf-idf (term frequency, inverse document frequency) values are computed by treating each sentence as a \u201cdocument\u201d. That is, the weighting term is\nt f \u2212 id f (u) = c(u,x)\u2217 log |V | c(u,V )\n(4)\nwhere c(u,x) is the count of u in x (term frequency), and c(u,V ) is the number of sentences out of V that u occurs in.\nThe above function can be optimized efficiently even for large data sets. Formally, we have the following optimization problem:\nX\u2217 \u2208 argmax X\u2286V,m(X)\u2264b f (X), (5)\nwhere b is a known budget \u2013 in the present context, the budget can be, e.g., the number of words\nAlgorithm 1: The Greedy Algorithm 1 Input: Submodular function f : 2V \u2192 R+,\ncost vector m, budget b, finite set V . 2 Output: Xk where k is the number of\nor parallel sentences to select. Solving this problem exactly is NP-complete (Feige, 1998), and expressing it as an ILP procedure renders it impractical for large data sizes. When f is submodular and the cost is just size (m(X) = |X |), then the simple greedy algorithm (detailed in Algorithm 1) will have a worst-case guarantee of f (X\u0303\u2217) \u2265 (1\u2212 1/e) f (Xopt) \u2248 0.63 f (Xopt) where Xopt is the optimal and X\u0303\u2217 is the greedy solution (Nemhauser et al., 1978). This constant factor guarantee stays the same as n grows; thus, it scales well to large data sets. The application of this procedure to the selection of training data for large-scale SMT tasks was described in (Kirchhoff and Bilmes, 2014). Here, we apply it in the same way to the selection of out-of-domain data for a small-scale task."}, {"heading": "4 Data", "text": "The in-domain data available for the present study is the Transtac corpus of Iraqi Arabic; the sizes of the training, tuning and development test sets are shown in Table 1.\nThe out-of-domain data sources used for the selection experiments are listed in Table 2. We utilize 22 LDC corpora that include MSA and other dialects of Arabic, notably Egyptian and Levantine. For example, training corpora developed for the GALE, TIDES, and BOLT projects were included, as were the Levantine Arabic Treebank, an Egyptian Arabic word alignment corpus, and a corpus of dialectal Arabic web data (75% Levantine, 25% Egyptian) that was translated through crowdsourc-\ning (thus, translations are noisy). Note that even though a corpus may be officially listed as MSA, it may contain segments of DA, especially when broadcast conversations (e.g., talkshows) are included."}, {"heading": "5 Experiments and Results", "text": "We use two different MT systems for translation from IA to English, an in-house system based on Moses and the SRI MT system developed for the DARPA BOLT (Broad Operational Language Translation) spoken dialog translation project (see (Ayan et al., 2013; Kirchhoff et al., 2015) for more details). The former is a flat phrase-based statistical MT system with a hierarchical lexicalized reordering model and a 6-gram language model trained on the target side of the Transtac training data. For preprocessing we use a statistical morphological segmenter developed in the BOLT project. The second system is similar in nature but has a hierarchical phrase-based translation model and utilizes sparse features (see (Zhao et al., 2014) for more\ninformation)."}, {"heading": "5.1 Initial evaluation of selection techniques", "text": "In an initial set of experiments we attempted to gauge the performance of the cross-entropy vs. the submodular selection technique by subselecting the Transtac training data. We chose 10-40% of the Transtac training set; the feature set U was the set of all n-grams up to length 7 of the tune and dev sets. We investigated both translation directions, IA \u2192 English and English\u2192 IA. Table 3 shows the BLEU scores.\nCompared to using 100% of the training data, the same or even better performance can be obtained by using a subset of the data when the submodular subselection technique is used, even at small percentages of the training data. The cross-entropy method falls short of this performance, presumably due to the failure of this method to control for redundancy in the selected set."}, {"heading": "5.2 Selection of out-of-domain training data", "text": "In order to integrate additional out-of-domain training data, we set a budget constraint of 100k words on the source side. The LDC corpora were preprocessed in the same manner as the Transtac data, i.e. , they were preprocessed and morphologically segmented. The greedy algorithm was used in combination with Equation 3 to select parallel sentences from the corpora listed in Table 2 such that the resulting corpus contains at most 100k words on the source side. The selected data was then added to both the MT and LM training data. Table 4 shows the BLEU scores and position-independent word error rate (PER) for the in-house MT system that was used for development purposes (note that baseline results are different from those in Table 3 because the baseline MT system changed in between experiments and was trained on different data set definitions and tokenization schemes). We again compared the cross-entropy against the submodular selection method. Improvements in the system are small; however, the submodular technique again shows slightly better results.\nWe subsequently used the selected data with\nthe submodular method in the second MT system, viz. the evaluation system developed for a bilingual dialog system, and tested the system on additional in-domain data sets. BLEU scores (shown in Table 5) show slight improvements of up to 0.5 absolute. Note that the selected data set was very small, containing only 7k sentences. Larger sets (up to 20k) were tried but were not found to be useful.\nWe analyzed the selected data as to its origin and found that the top three data sources were broadcast conversations from various GALE corpora (47%), the dialectal web corpus (35.7%), and the BOLT MT training data (9.9%)."}, {"heading": "5.3 Using translated speech data", "text": "In addition to the various parallel text corpora listed in Table 2 we also had access to an Iraqi Arabic Conversational Telephone Speech (CTS) corpus (LDC2006T16). This corpus includes with speech transcriptions but no translations. Although the data matches the dialect of interest is is not necessarily matched in topic or style. To obtain parallel data we translated the transcriptions of this corpus with our baseline IA\u2192 EN translation system. Those segments that were translated contiguously (i.e., without intervening out-of-vocabulary words) were extracted and added to the data from the corpora in Table 2. Data selection was then re-run. We found that in this experiment 80% of the selected data came from the CTS corpus; however, the translation performance did not improve (see Table 6). The likely reason is that translations were too noisy to be used as parallel data and introduced more confusability and irrelevant variation rather than contributing useful translations. The use of automatically translated speech data might be improved by selecting only the most confident translations according to the translation model scores."}, {"heading": "6 Conclusion", "text": "We have described data selection procedures for identifying Iraqi Arabic data resources in unrelated dialectal and/or MSA corpora. We have demon-\nstrated that judiciously selected data can improve MT performance even when the overall amount is very small. Furthermore, we have compared two different data selection techniques, the widely-used cross-entropy selection method, and a more recently developed method that relies on submodular function optimization. The latter performed slightly better than the former. Finally, we have conducted initial experiments on utilizing automatically translated conversational speech as additional training data. Whereas the data was strongly matched to the in-domain data on the source side, the translations were too noisy to yield any further improvement in machine translation performance."}, {"heading": "Acknowledgments", "text": "This study was funded by the Defense Advanced Research\nProjects Agency (DARPA) under contract HR0011-12-C-0016\n- subcontract 19-000234 and by the Intelligence Advanced\nResearch Projects Activity (IARPA) under agreement number\nFA8650-12-2-7263. The U.S. Government is authorized to\nreproduce and distribute reprints for Governmental purposes\nnotwithstanding any copyright notation thereon. The views\nand conclusions contained herein are those of the authors and\nshould not be interpreted as necessarily representing the offi-\ncial policies or endorsements, either expressed or implied, of\nIntelligence Advanced Research Projects Activity (IARPA) or\nthe U.S.Government."}], "references": [{"title": "A supervised POS tagger for written Arabic social networking corpora", "author": ["Al-Sabbagh", "R. Girju2012] Al-Sabbagh", "R. Girju"], "venue": "In Proceedings of KONVENS", "citeRegEx": "Al.Sabbagh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Al.Sabbagh et al\\.", "year": 2012}, {"title": "Automatic Identification of Arabic Dialects", "author": ["F.S. Alorifi"], "venue": null, "citeRegEx": "Alorifi,? \\Q2008\\E", "shortCiteRegEx": "Alorifi", "year": 2008}, {"title": "Handling OOV words in dialectal Arabic to English machine translation", "author": ["M. Aminian et al.2014] Aminian", "M. Ghoneim", "M. Diab"], "venue": "In EMNLP Workshop on Language Technology for Closely Related Languages and Language Variants", "citeRegEx": "Aminian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Aminian et al\\.", "year": 2014}, {"title": "Domain adaptation via pseudo in-domain data selection", "author": ["A. Axelrod et al.2011] Axelrod", "X. He", "J. Gao"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Axelrod et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Can you give me another word for hyperbaric? - improving speech translation using targeted clarification questions", "author": ["N.F. Ayan et al.2013] Ayan et al"], "venue": "Proceedings of ICASSP", "citeRegEx": "al.,? \\Q2013\\E", "shortCiteRegEx": "al.", "year": 2013}, {"title": "Parsing Arabic dialects", "author": ["Chiang et al.2006] Chiang", "David", "M.Diab", "N. Habash", "O. Rambow", "S. Shareef"], "venue": "In Proceedings of EACL", "citeRegEx": "Chiang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2006}, {"title": "Pos tagging of dialectal Arabic: a minimally supervised approach", "author": ["Duh", "K. Kirchhoff2005] Duh", "K. Kirchhoff"], "venue": "Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages,", "citeRegEx": "Duh et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Duh et al\\.", "year": 2005}, {"title": "Lexicon acquisition for dialectal Arabic using transductive learning", "author": ["Duh", "K. Kirchhoff2006] Duh", "K. Kirchhoff"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Duh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Duh et al\\.", "year": 2006}, {"title": "Adaptation data selection using neural language models: Experiments in machine translation", "author": ["K. Duh et al.2013] Duh", "G. Neubig", "K. Sudoh", "H. Tsukada"], "venue": "Proceedings of ACL", "citeRegEx": "Duh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duh et al\\.", "year": 2013}, {"title": "Combinatorial Structures and their Applications, chapter Submodular functions, matroids and certain polyhedra, pages 69\u201387", "author": ["J. Edmonds"], "venue": null, "citeRegEx": "Edmonds,? \\Q1970\\E", "shortCiteRegEx": "Edmonds", "year": 1970}, {"title": "A threshold of ln n for approximating set cover", "author": ["U. Feige"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Feige,? \\Q1998\\E", "shortCiteRegEx": "Feige", "year": 1998}, {"title": "Morphological analysis and generation for Arabic dialects", "author": ["N. Habash et al.2005] Habash", "O. Rambow", "G. Kiraz"], "venue": "In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages", "citeRegEx": "Habash et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2005}, {"title": "A morphological analyzer for Egyptian Arabic", "author": ["N. Habash et al.2012] Habash", "R. Eskander", "A. Hawwari"], "venue": "In Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMOR-", "citeRegEx": "Habash et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2012}, {"title": "Morphological analysis and disambiguation for dialectal Arabic", "author": ["N. Habash et al.2013] Habash", "R. Roth", "O. Rambow", "R. Eskander", "N. Tomeh"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Habash et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2013}, {"title": "Introduction to Arabic natural language processing", "author": ["N. Habash"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "Habash,? \\Q2010\\E", "shortCiteRegEx": "Habash", "year": 2010}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts. In Computer Vision and Pattern Recognition (CVPR), Colorado Springs, CO, June", "author": ["Jegelka", "S. Bilmes2011] Jegelka", "J.A. Bilmes"], "venue": null, "citeRegEx": "Jegelka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jegelka et al\\.", "year": 2011}, {"title": "Submodularity for data selection in statistical machine translation", "author": ["Kirchhoff", "K. Bilmes2014] Kirchhoff", "J. Bilmes"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kirchhoff et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kirchhoff et al\\.", "year": 2014}, {"title": "Morphological modeling for machine translation of English-iraqi Arabic spoken dialogs", "author": ["Y.C. Tam dn C. Richey", "W. Wang"], "venue": "Proceedings of NAACL", "citeRegEx": "Kirchhoff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kirchhoff et al\\.", "year": 2015}, {"title": "Submodularity and its applications in optimized information gathering", "author": ["Krause", "A. Guestrin2011] Krause", "C. Guestrin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Krause et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2011}, {"title": "Robust submodular observation selection", "author": ["A. Krause et al.2008] Krause", "H.B. McMahan", "C. Guestrin", "A. Gupta"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["Lin", "H. Bilmes2012] Lin", "J. Bilmes"], "venue": "In Uncertainty in Artifical Intelligence (UAI), Catalina Island,", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Developing and using a pilot dialectal Arabic treebank", "author": ["M. Maamouri et al.2006] Maamouri", "A. Bies", "T. Buckwalter", "M. Diab", "N. Habash", "O. Rambow", "D. Tabessi"], "venue": "In Proceedings of the Fifth International Conference on Language Resources", "citeRegEx": "Maamouri et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Maamouri et al\\.", "year": 2006}, {"title": "Improving in-domain data selection for small in-domain sets", "author": ["M. Mediani et al.2014] Mediani", "J. Winebarger", "A. Waibel"], "venue": "In Proceedings of IWSLT", "citeRegEx": "Mediani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mediani et al\\.", "year": 2014}, {"title": "Intelligent selection of language model training data", "author": ["Moore", "R. Lewis2010] Moore", "W. Lewis"], "venue": "In Proceedings EMNLP", "citeRegEx": "Moore et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2010}, {"title": "Local search for balanced submodular clustering", "author": ["Narasimhan", "M. Bilmes2007] Narasimhan", "J. Bilmes"], "venue": "In Proceedings of IJCAI", "citeRegEx": "Narasimhan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2007}, {"title": "An analysis of approximations for maximizing submodular set functions i", "author": ["L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "Nemhauser et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "Automatic identification of Arabic language varieties and dialects in social media", "author": ["F. Sadat et al.2014] Sadat", "F. Kazemi", "A. Farzindar"], "venue": "In Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP),", "citeRegEx": "Sadat et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sadat et al\\.", "year": 2014}, {"title": "Dialectal to standard Arabic paraphrasing to improve Arabic-English statistical machine translation", "author": ["Salloum", "W. Habash2011] Salloum", "N. Habash"], "venue": "In Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects", "citeRegEx": "Salloum et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Salloum et al\\.", "year": 2011}, {"title": "Dialectal Arabic to English machine translation: Pivoting through modern standard Arabic", "author": ["Salloum", "W. Habash2013] Salloum", "N. Habash"], "venue": "In Proceedings of NAACL", "citeRegEx": "Salloum et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Salloum et al\\.", "year": 2013}, {"title": "Using document summarization techniques for speech data subset selection", "author": ["K. Wei et al.2013] Wei", "Y. Liu", "K. Kirchhoff", "J. Bilmes"], "venue": "In Proceedings of NAACL", "citeRegEx": "Wei et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2013}, {"title": "Arabic dialect identification", "author": ["Zaidan", "O. Callison-Burch2014] Zaidan", "C. Callison-Burch"], "venue": "Computational Linguistics,", "citeRegEx": "Zaidan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaidan et al\\.", "year": 2014}, {"title": "Machine translation of Arabic dialects", "author": ["Zbib et al.2012] Zbib", "Rabih", "Erika Malchiodi", "Jacob Devlin", "David Stallard", "Spyros Matsoukas", "Richard Schwartz", "John Makhoul", "Omar F. Zaidan", "Chris Callison-Burch"], "venue": "In Proceedings of the 2012 Conference", "citeRegEx": "Zbib et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zbib et al\\.", "year": 2012}, {"title": "An autoencoder with bilingual sparse features for improved statistical machine translation", "author": ["B. Zhao et al.2014] Zhao", "Y.-C. Tam", "J. Zheng"], "venue": "In Proceedings of ICASSP", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "An overview of the main characteristics of DA can be found in (Habash, 2010).", "startOffset": 62, "endOffset": 76}, {"referenceID": 11, "context": "Work on DA annotation tools includes the development of morphological analyzers for Arabic dialects (Habash et al., 2005; Habash et al., 2012; Habash et al., 2013), treebanks (Maamouri et al.", "startOffset": 100, "endOffset": 163}, {"referenceID": 12, "context": "Work on DA annotation tools includes the development of morphological analyzers for Arabic dialects (Habash et al., 2005; Habash et al., 2012; Habash et al., 2013), treebanks (Maamouri et al.", "startOffset": 100, "endOffset": 163}, {"referenceID": 13, "context": "Work on DA annotation tools includes the development of morphological analyzers for Arabic dialects (Habash et al., 2005; Habash et al., 2012; Habash et al., 2013), treebanks (Maamouri et al.", "startOffset": 100, "endOffset": 163}, {"referenceID": 21, "context": ", 2013), treebanks (Maamouri et al., 2006) and parsers (Chiang et al.", "startOffset": 19, "endOffset": 42}, {"referenceID": 5, "context": ", 2006) and parsers (Chiang et al., 2006), unsupervised (Duh and Kirchhoff, 2005) or supervised (Al-Sabbagh and Girju, 2012) training of POS taggers for DA, and lexicon acquisition (Duh and Kirchhoff, 2006).", "startOffset": 20, "endOffset": 41}, {"referenceID": 31, "context": "(Zbib et al., 2012) compare utilizing large amounts of MSA data for training and creating a small corpus of DA training data.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "In (Aminian et al., 2014) the specific problem of out-of-vocabulary words in MT for DA is addressed by replacing DA words with their MSA equivalents.", "startOffset": 3, "endOffset": 25}, {"referenceID": 1, "context": "In principle, automatic dialect identification methods (Alorifi, 2008; Sadat et al., 2014; Zaidan and Callison-Burch, 2014) might be used for this purpose; however, these methods are themselves error-prone and have not been developed for all dialects of Arabic.", "startOffset": 55, "endOffset": 123}, {"referenceID": 26, "context": "In principle, automatic dialect identification methods (Alorifi, 2008; Sadat et al., 2014; Zaidan and Callison-Burch, 2014) might be used for this purpose; however, these methods are themselves error-prone and have not been developed for all dialects of Arabic.", "startOffset": 55, "endOffset": 123}, {"referenceID": 29, "context": "Two different data selection methods are investigated, the widely-used cross entropy method of (Moore and Lewis, 2010), and a more recent submodular data selection method (Wei et al., 2013).", "startOffset": 171, "endOffset": 189}, {"referenceID": 3, "context": "In (Axelrod et al., 2011) the monolingual selection method is extended to bilingual corpora.", "startOffset": 3, "endOffset": 25}, {"referenceID": 8, "context": "In (Duh et al., 2013), neural language models are used instead of backoff language models.", "startOffset": 3, "endOffset": 21}, {"referenceID": 22, "context": "Finally, (Mediani et al., 2014) propose a different method for drawing the out-of-domain sample and the use of word-association models to improve the data for training the out-of-domain language model.", "startOffset": 9, "endOffset": 31}, {"referenceID": 9, "context": "Submodular functions (Edmonds, 1970; Fujishige, 2005) were first developed in mathematics, operations research and economics; more recently, they have been used for a variety of optimization problems in machine learning as well.", "startOffset": 21, "endOffset": 53}, {"referenceID": 19, "context": "For example, they have been applied to the problems of clustering (Narasimhan and Bilmes, 2007), observation selection (Krause et al., 2008), sensor placement (Krause and Guestrin, 2011), or image segmentation (Jegelka and Bilmes, 2011).", "startOffset": 119, "endOffset": 140}, {"referenceID": 10, "context": "Solving this problem exactly is NP-complete (Feige, 1998), and expressing it as an ILP procedure renders it impractical for large data sizes.", "startOffset": 44, "endOffset": 57}, {"referenceID": 25, "context": "63 f (Xopt) where Xopt is the optimal and X\u0303\u2217 is the greedy solution (Nemhauser et al., 1978).", "startOffset": 69, "endOffset": 93}, {"referenceID": 17, "context": "We use two different MT systems for translation from IA to English, an in-house system based on Moses and the SRI MT system developed for the DARPA BOLT (Broad Operational Language Translation) spoken dialog translation project (see (Ayan et al., 2013; Kirchhoff et al., 2015) for more details).", "startOffset": 233, "endOffset": 276}, {"referenceID": 32, "context": "The second system is similar in nature but has a hierarchical phrase-based translation model and utilizes sparse features (see (Zhao et al., 2014) for more information).", "startOffset": 127, "endOffset": 146}], "year": 2015, "abstractText": "Statistical machine translation for dialectal Arabic is characterized by a lack of data since data acquisition involves the transcription and translation of spoken language. In this study we develop techniques for extracting parallel data for one particular dialect of Arabic (Iraqi Arabic) from out-ofdomain corpora in different dialects of Arabic or in Modern Standard Arabic. We compare two different data selection strategies (cross-entropy based and submodular selection) and demonstrate that a very small but highly targeted amount of found data can improve the performance of a baseline machine translation system. We furthermore report on preliminary experiments on using automatically translated speech data as additional training data.", "creator": "LaTeX with hyperref package"}}}