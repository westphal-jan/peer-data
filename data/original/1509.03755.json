{"id": "1509.03755", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2015", "title": "Toward better feature weighting algorithms: a focus on Relief", "abstract": "Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem's domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task. Some other feature weighting methods are reviewed in order to give some context and then the different existing extensions to the original algorithm are explained.", "histories": [["v1", "Sat, 12 Sep 2015 15:10:15 GMT  (46kb)", "http://arxiv.org/abs/1509.03755v1", null], ["v2", "Wed, 16 Sep 2015 11:58:32 GMT  (47kb)", "http://arxiv.org/abs/1509.03755v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gabriel prat masramon", "llu\\'is a belanche mu\\~noz"], "accepted": false, "id": "1509.03755"}, "pdf": {"name": "1509.03755.pdf", "metadata": {"source": "CRF", "title": "Toward better feature weighting algorithms: a focus on Relief", "authors": ["Gabriel Prat Masramon"], "emails": ["gprat@lsi.upc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n03 75\n5v 1\n[ cs\n.L G\nFeature weighting algorithms try to solve a problem of great impor-\nfor the features of a given domain. This relevance is primarily used for\nfeature selection as feature weighting can be seen as a generalization of it,\nbut it is also useful to better understand a problem\u2019s domain or to guide\nan inductor in its learning process. Relief family of algorithms are proven\nto be very effective in this task. Some other feature weighting methods\nare reviewed in order to give some context and then the different existing\nextensions to the original algorithm are explained.\nOne of Relief\u2019s known issues is the performance degradation of its\nestimates when redundant features are present. A novel theoretical def-\ninition of redundancy level is given in order to guide the work towards\nan extension of the algorithm that is more robust against redundancy. A\nnew extension is presented that aims for improving the algorithms perfor-\nmance. Some experiments were driven to test this new extension against\nthe existing ones with a set of artificial and real datasets and denoted that\nin certain cases it improves the weight\u2019s estimation accuracy."}, {"heading": "1 Overview", "text": "Feature selection is undoubtedly one of the most important problems in machine learning, pattern recognition and information retrieval, among others. A feature selection algorithm is a computational solution that is motivated by a certain definition of relevance. However, the relevance of a feature may have several definitions depending on the objective that is looked after.\nThe generic purpose pursued is the improvement of the inductive learner, either in terms of learning speed, generalization capacity or simplicity of the representation. It is then possible to understand better the obtained results, diminish the volume of storage, reduce noise generated by irrelevant or redundant features and eliminate useless knowledge.\nOn the other hand, feature weighting algorithms try to estimate relevance (in the form of weights to the features) rather than binarily deciding whether a feature is either relevant or not. This is a much harder problem, but also a more flexible framework from an inductive learning perspective. This kind of algorithms are confronted with the down-weighting of irrelevant features, the up-weighting of relevant ones and the problem of relevance assignment when redundancy is an issue.\nIn this work we review Relief, one of the most popular feature weighting algorithms. After a state-of-the-art in section 2 focused on feature weighting methods in general, in section we describe the algorithm and its more important extensions. We are primarily interested in coping with redundancy, and studying to what extent can the Relief algorithm be modified in order to better its treatment of redundancy, which is one of its known weaknesses. In this vein, section 3 points out a novel and general (though computationally infeasible) definition of redundancy level and try to relate it to the actual Relief performance. Next, we develop a \"double\" or feedback extension of the algorithm that takes its own estimations into account in order to improve general performance. We also complement this matter with a set of experiments in section 4. The work concludes with some open questions and clear avenues of continuation of the material herein presented."}, {"heading": "2 State of the art", "text": ""}, {"heading": "2.1 Introduction", "text": "In the last few years feature selection has become a more and more common topic of research. This popularity increase is probably due to the growth of the problem domains\u2019 number of features. No more than ten years ago few problems treated domains with more than 50 features. Nowadays most papers deal with domains with hundreds and even tens of thousands of features. New techniques have to be developed to address this kind of problems with many irrelevant and redundant features and comparatively few instances to learn from. One example of these new domains is web page categorization, a domain currently of much interest for internet search engines where thousands of terms can be found in a document. Another example can be appearance-based image classification methods which may use every pixel in the image. Classification problems with thousands of features are very common in medicine and biology; e.g. molecule classification, gene selection or medical diagnostics. In medical problems we typically have less than a hundred patients and for each patient we can have thousands of features evaluated.\nFeature selection can help us solving a classification problem with these characteristics for many reasons. Firstly it may make the task of data visualization and understanding easier by eliminating irrelevant features which can mislead the interpretation of the data. It can also reduce the cost of the measurements as we can avoid measuring irrelevant features; this is especially important in\ndomains where some features are very expensive to obtain, e.g., require a special medical test. In addition, a big benefit of feature selection is defying the curse of dimensionality to help the induction of good classifiers from the data. When many unuseful, i.e. irrelevant or redundant, features are present in training data, classifiers may find false regularities in the input features and learn from that instead of learning from the features that really determine the instance class (also valid when predicting the instance target value in the case of regression).\nThere are two main approaches to feature selection: filter methods and wrapper methods. Both methods can be included in the framework shown on Fig. 1. The main difference between them is the use of a classifier for the estimation of a feature usefulness. The two families of methods only differ in\nthe way they evaluate the candidate sets of features. While the former methods use a problem independent criterion, the latter use the performance of the final classifier to evaluate the quality of a feature subset. The basic idea of the filter methods is to select the features according to some prior knowledge of the data. For example, to select the features based on the conditional probability that a given instance is a member of a certain class given the value of its features. Another criterion commonly used by filter methods is the correlation of a feature with the class, i.e. selecting features with high correlation. More detailed criteria is given in section 2.2, where also more criteria are described. In contrast, wrapper methods suggest a set of features that are given to a classifier which uses them to classify some training data and returns the performance of the classification which is the acceptance criterion of the feature set.\nNow we have explained two approaches of feature subset evaluation, but is clear to see that if we had to test all possible subsets, using either of the methods, of features we would have a combinatorial explosion. If our initial set of features is F and |F| = n, the number of evaluations we would have to do would be equal to the cardinality of the power set of F : |P(F)| = 2n. For this reason diverse techniques have been developed to reduce the computational complexity of this problem.\nA different technique of determining feature usefulness apart from feature selection is a technique called feature weighting (or feature ranking). It consists of assigning a numeric value to each feature so as to indicate the feature\u2019s usefulness. Feature weighting can help solving the problem of feature selection. One possible approach to feature selection using feature weighting could be to first assign weights to features and then choose features according to their weights. This can be done either by having a rule to binarize the weights, e.g. select all the features with weight greater than zero, or by means of a weight guided feature subset evaluation, e.g. evaluating the subsets containing the features with greatest weight values. In fact, feature weighting could be seen as a generalization of feature selection, i.e. feature selection would be a specific kind of feature weighting where the weights assigned to features are binary.\nIn following sections we will explore various methods of existing feature weighting algorithms than and will discuss their properties to later have some starting point to describe and analyze the algorithm in the focus of this paper: Relief."}, {"heading": "2.2 Feature weighting", "text": "This section will review some of the most used feature weighting algorithms. Although the section is focused on feature weighting, most of the methods described below can also be used for feature selection.\nOn following subsections I and F represent the sets of instances and features respectively. I, I1 or Ii represent instances from I. X , Xi or Y are sets of possible feature values from a feature in F . C represents the set of possible class values. And their lower case versions represent single value in its correspondent upper case set, e.g. we will use c \u2208 C and x \u2208 X . We also will use a short notation to express probabilities, e.g. will write p(x) to represent the probability for feature X to have value x or p(c|x) to express the conditional probability of the class to have value c knowing that the feature X has value x.\nConditional Probabilities based methods\nThe first group of methods we will look at are the ones based on conditional probabilities of class given a feature value. Two simple methods using this idea were introduced in [Creecy et al., 1992]: per-category feature importance and cross-category feature importance (or, in short, PCF and CCF). One important limitation is that they can only deal with binary features, so numerical features must be discretized and symbolic features converted to a group of binary features. The weights assigned to features in the case of PCF depends on the class of the feature as seen in Eq. 2.1\nwPCF (X, c) = P (c|x), where x would be the positive feature value (2.1)\nso we have a weight for each feature and class. CCF relies on the same idea but instead of having one weight for each feature and class it have only a weight per\nfeature. It does so by averaging the weights across classes. In fact, as it shows Eq. 2.2, it uses the summation of squares of conditional probabilities.\nwCCF (X) = \u2211\nc\u2208C\nP (c|x)2, where x would be the positive feature value (2.2)\nLater on [Mohri and Tanaka, 1994] showed that PCF is too sensitive to class proportions and tends to answer the most frequent class when using it for classifying.\nA more sophisticated approach that also makes use of conditional probabilities is the one used by the value difference method (VDM) introduced by [Stanfill and Waltz, 1986]. This time no binarization of features is required, although numeric features still have to be discretized in order to calculate conditional probabilities as shown in Eq. 2.3. In addition this method does not assign weights to each feature but to each value of each feature.\nwV DM (X, x) =\n\u221a \u221a \u221a \u221a \u2211\nc\u2208C\n(\nP (x|c)\np(x)\n)2\n(2.3)\nThis weighting scheme was originally used to calculate distances between features.\nFinally we have Gini-index gain [Breiman et al., 1984] in Eq. 2.4 which can be interpreted as the expected error rate\nGG(X) = \u2211\nx\u2208X\nP (x) \u2211\nc\u2208C\nP (c|x)2 \u2212 \u2211\nc\u2208C\nP (c)2 (2.4)\nand is proven to be biased towards multiple valued features. In further sections we will see that this particular measure has some relation with the Relief algorithm.\nInformation theory based methods\nNot all the feature weighting methods are based on conditional probabilities, though. Now we will describe some methods based on information theory [Shannon, 1948, Shannon and Weaver, 1949].\nThe first one is just using Shannon\u2019s mutual information (MI) between two features X and Y in Eq. 2.5,\nMI(X,Y ) = H(X)\u2212H(X |Y ) = \u2211\nx\u2208X,y\u2208Y\np(x, y)log2 p(x, y)\np(x)p(y) (2.5)\nwhich is defined using entropies and conditional entropies (see Eq. 2.7),\nEntropy: H(X) = \u2212 \u2211\nx\u2208X\nP (x) log2 P (x) (2.6)\nConditional entropy: H(X |Y ) = H(X,Y )\u2212H(Y ) (2.7)\nJoint entropy: H(X,Y ) = \u2212 \u2211\nx\u2208X,y\u2208Y\nP (x, y) log2 P (x, y) (2.8)\nto weight features. A more informal but maybe more intuitive definition of mutual information is that MI measures the information of X that is also in Y . If the features are independent no information is shared so mutual information is zero. In the other end we have that one feature is an exact copy of the other, all the information it contains is also shared by the other so the mutual information is the same as the information conveyed by one of them, namely its entropy. A very popular feature weighting method uses the idea of mutual information. It was proposed by [Hunt et al., 1966] and it is used in [Quinlan, 1986] when splitting nodes in top down indutcion of decision trees (TDIDT) best known as ID3. The term information gain (IG) in Eq. 2.9 is used there. Its intuitive interpretation would be: The more an feature reduces class entropy when knowing its value, the more its weight. This is just another way to say: The more information is shared between an feature and the class, the more its weight. So if we have a set of classes C we can define IG for the class knowing the value of a feature X as shown in Eq. 2.9\nIG(C|X) = MI(C,X). (2.9)\nLater on, similar methods were introduced to reduce the bias of IG towards features with large number of values. The extreme case is using an feature with an ID code. It is clear to see that knowing the ID code we can precisely know the class of any instance in our training set. The problem is that we can say nothing about a new instance which will have another unknown ID code. One of these methods is gain ratio (GR) in Eq. 2.10 used by C4.5 decision tree induction algorithm [Quinlan, 1993] which normalizes IG by the amount of information needed to predict an features value (the entropy of the feature). But there are also various other proposals, among them there are entropy distance [MacKay, 2003] in Eq. 2.11 and the M\u00c3\u00a1ntaras distance between the class and the feature in Eq. 2.12 which was proved to be unbiased towards multiple-valued features.\nGR(C|X) = IG(C|X)\nH(X) (2.10)\nDH(C,X) = H(C,X)\u2212MI(C,X) (2.11)\nDM (C,X) = H(X |C) +H(C|X)\nH(C,X) = 2\u2212\nH(X) +H(C)\nH(C,X) (2.12)\nDistribution distance based methods\nAnother way to find dependencies between a feature and the class is to measure differences between their distributions. Perhaps the simplest way to do so is to compute the difference between the joint and the product distributions as shown in Eq. 2.13\nDiff(C,X) = \u2211\nc\u2208C,x\u2208X\n|P (c, x)\u2212 P (x)P (c)| (2.13)\nand this distance can be directly used as the features weight. Large differences between the joint and the product distributions indicate large dependency of the class on the feature, so the feature should be given a large weight. This can easily be applied to continuous features changing the sum for an integration. It can also easily be rescaled to the [0,1] interval as it has an upper bound of 1\u2212 \u2211\nx\u2208X P (x) 2.\nMore distance functions can be used here. An interesting one is the KullbackLeibler divergence which is not a distance in fact as it is not symmetric (i.e., DKL(X ||Y ) 6= DKL(Y ||X)). The application on feature weighting is to have the weigh be equal to the distance between the joint and the product distributions, see Eq. 2.14.\nDKL(P (X,C)||P (X)P (C)) = \u2211\nc\u2208C,x\u2208X\nP (c, x) log P (c, x)\nP (x)P (c) (2.14)\nNote that this is exactly the same as the mutual information between the feature and the class (see Eq. 2.5) so we have DKL(P (X,C)||P (X)P (C)) = MI(X,C).\nCorrelation based methods\nEven though this approach to feature weighting is treated last, maybe is one of the simplest as it does not care about continuous feature discretization or probability density estimations. It is usual in statistics to construct contingency tables for pairs of discrete variables to analyze their correlation. In our case (see Table 1) we will define a contingency table between the set of classes ci \u2208 C and the values of a feature xj \u2208 X . The inner cells in row i and column j of the table contain the number of instances of class ci that have feature X = xj . The row marginal totals will tell the number of instances for the corresponding class and the column marginal totals the number of instances with the corresponding value on feature X . Finally the sum of either marginal totals should be the total number of instances m. Looking at this table we can define chi-squared\nweight for feature X as shown on Eq. 2.15\nX2(X) = \u2211\nx\u2208X,c\u2208C\n(Ncx \u2212 Ecx) 2/Ecx (2.15)\nwhere Ecx is the expected number of instances of class c with value x on feature X calculated as Nc\u00b7N\u00b7x/m. X2 is distributed approximately as a \u03c72 with (v \u2212 1)(w \u2212 1) degrees of freedom. We should avoid terms with Ecx = 0 or replace them with a small positive number. We can see that in the extreme case that X and C are completely independent Ncx = Ecx is expected so large values of X2(X) indicate strong dependence between the feature and the class. Note that the result ofX2 depends not only on the joint probabilities P (c, x) = Ncx/m but also depends on the number of instances m. This dependency on the number of instances seems to make sense with the intuition that correlations calculated with small number of instances shall be less accurate."}, {"heading": "2.3 Relief", "text": "One common characteristic of the previously cited methods is that they treat features individually assuming conditional independence of features upon the class. In the other hand, Relief takes all other features in care when evaluating a specific feature. Another interesting characteristic of Relief is that it is aware of contextual information being able to detect local correlations of feature values and their ability to discriminate from an instance of a different class.\nThe main idea behind Relief is to assign large weights to features that contribute in separating near instances of different class and joining near instances belonging to the same class. The word \"near\" in the previous sentence is of crucial importance since we mentioned that one of the main differences between Relief and the other cited methods is the ability to take local context into account. Relief does not reward features that separate (join) instances of different (same) classes in general but features that do so for near instances.\nIn Fig. 2 we can see the original algorithm presented by Kira and Rendell in [Kira and Rendell, 1992]. We maintained the original notation that slightly differs from the used above as now features (attributes) are labeled A. There we can see that in the aim of detecting whether the feature is useful to discriminate near instances it selects two nearest neighbors of the current instance Ri. One from the same class H called the nearest hit and one from the different class M (the original Relief algorithm only dealt with two class problems) called the nearest miss. With these two nearest neighbors it increases the weight of the feature if it has the same value for both Ri and H and decreases it otherwise. The opposite occurs with the nearest miss, Relief increases the weight of a feature if it has opposite values for Ri and M and decreases it otherwise.\nOne of the central parts of Relief is the difference function diff which is also used to compute the distance between instances as shown in Eq. 2.16.\n\u03b4(I1, I2) = \u2211\ni\ndiff(Ai, I1, I2) (2.16)\nInput: for each training instance a vector of feature values and the class value Output: the vector W of estimations of the qualities of features\nThe original definition of diff was an heterogeneous distance metric composed of the overlap metric in Eq. 2.17 for nominal features and the normalized Euclidean distance in Eq. 2.18 for linear features, which [Wilson and Martinez, 1997] called HEOM.\ndiff(A, I1, I2) =\n{\n0 if value(A, I1) = value(A, I2) 1 otherwise (2.17)\ndiff(A, I1, I2) = |value(A, I1)\u2212 value(A, I2)|\nmax(A)\u2212min(A) (2.18)\nThe difference normalization with m guarantees that the weight range is [-1,1]. In fact the algorithm tries to approximate a probability difference in Eq. 2.20.\nW [A] \u2248P (different value of A|nearest instance from different class)\u2212 (2.19)\nP (different value of A|nearest instance from same class) (2.20)\nWe can see that for a set of instances I having a set of features F this algorithm has cost O(m\u00d7|I|\u00d7|F|) as it has to loop over m instances. For each instance in the main loop it has to compute its distance from all other instances so we have O(m \u00d7 |I|) times the complexity of calculating DRelief and we can easily see from Eq. 2.16 that its complexity is O(|F|), so we have our complexity: O(m\u00d7 |I| \u00d7 |F|). As m is a user defined parameter we can in some measure control the cost of Relief algorithm having a tradeoff between accuracy of estimation (for large m) and low complexity of the algorithm (for small m). However m can never be greater than |I|."}, {"heading": "2.4 Extensions of Relief", "text": "The first modification proposed to the algorithm is to make it deterministic by changing the outer loop through m randomly chosen instances for a loop\nover all instances. This obviously increases the algorithms computation cost which becomes O(|I|2 \u00d7 |F|) but makes experiments with small datasets more reproducible. Kononenko uses this simplified version of the algorithm in its paper [Kononenko, 1994] to test his new extensions to the original Relief. This version is also used by other authors [Kohavi and John, 1997] and its given the name Relieved with the final d for \"deterministic\".\nWe can find some extensions to the original Relief algorithm proposed in [Kononenko, 1994] in order to overcome some of its limitations: It couldn\u2019t deal with incomplete datasets, it was very sensible to noisy data and it could only deal with multi-class problems by splitting the problem into series of 2-class problems.\nTo able Relief to deal with incomplete datasets, i.e. that contained missing values, a modification of the diff function is needed. The new function must be capable of calculating the difference between a value of a feature and a missing value and between two missing values in addition to the calculation of difference between two known values. Kononenko proposed various modifications of this function in its paper and found one that performed better than the others it was the one in a version of Relief he called RELIEF-D (not to be confused with Releaved mentioned above). The difference function used by RELIEF-D can be seen in Eq. 2.21.\ndiff(A, I1, I2) =\n\n\n\n1\u2212 P (value(A, I2)|class(I1)) if I1 is missing 1\u2212 \u2211\na\u2208A\n[P (a|class(I1))\u00d7 P (a|class(I2))] if both missing\n(2.21) Now we will focus on giving Relief greater robustness against noise. This robustness can be achieved by increasing the number of nearest hits and misses to look at. This mitigates the effect of choosing a neighbor that would not have been the nearest without the effect of noise. The new algorithm has a new user defined parameter k that controls the number of nearest neighbors to use. In choosing k there is a tradeoff between locality and noise robustness. [Kononenko, 1994] states that 10 is a good choice for most purposes.\nThe last limitation was that the algorithm was only designed for 2-class problems. The straightforward extension to multi-class problems would be to take as the near miss the nearest neighbor belonging to a different class. This variant of Relief is the so-called Relief-E by Kononenko. But later on he proposes another variant which gave better results: This was to take the nearest neighbor (or the k nearest) from each class and average their contribution so as to keep the contributions of hits and misses symmetric and between the interval [0,1]. That gives the Relief-F (ReliefF from now on) algorithm seen in Fig. 3.\nThe above mentioned relation to impurity functions, in specific with Giniindex gain in Eq. 2.4, can be seen in [Robnik-\u0160ikonja and Kononenko, 2003] when developing the probability difference in Eq. 2.20 in the case that the algorithm uses a large number of nearest neighbors (i.e., when the selected instance could be anyone from the set of instances). This version of the algorithm is called myopic ReliefF as it loses its context of locality property. Rewriting Eq.\nInput: for each training instance a vector of feature values and the class value Output: the vector W of estimations of the qualities of features\n2.20 by removing the neighboring condition and by applying Bayes\u2019 rule, we obtain Eq. 2.22.\nW \u2032[A] = Psamecl|eqvalPeqval\nPsamecl \u2212\n(1\u2212 Psamecl|eqval)Peqval\n1\u2212 Psamecl (2.22)\nFor sampling with replacement we obtain we have:\nPeqval = \u2211\nc\u2208C\nP (c)2\nPsamecl|eqval = \u2211\nx\u2208X\n(\nP (x)2 \u2211\nx\u2208X P (x) 2 \u00d7\n\u2211\nc\u2208C\nP (c|x)2\n)\nNow we can rewrite Eq. 2.22 to obtain the myopic Relief weight estimation:\nW \u2032[A] = Peqval \u00d7GG\u2032(X)\nPsamecl1\u2212 Psamecl (2.23)\nWhere GG\u2032(A) is a modified Gini-index gain of attribute A as seen in Eq. 2.24.\nGG\u2032(X) = \u2211\nx\u2208X\n(\nP (x)2 \u2211\nx\u2208X P (x) 2 \u00d7\n\u2211\nc\u2208C\nP (c|x)2\n)\n\u2212 \u2211\nc\u2208C\nP (c)2 (2.24)\nAs we can see the difference in this modified version from its original Gini-index gain described above in Eq. 2.4 is that Gini-index gain used a factor:\nP (x) \u2211\nx\u2208X P (x) = P (x)\nwhile myopic ReliefF uses: P (x)2\n\u2211\nx\u2208X P (x) 2\nSo we can see how this myopic ReliefF in Eq. 2.23 holds some kind of normalization for multi-valued attributes when using the factor Peqval . This solves the bias of impurity functions towards attributes with multiple values. Anther improvement compared with Gini-index is that Gini-index gain values decrease when the number of classes increase. The denominator of Eq. 2.23 avoids this strange behavior."}, {"heading": "3 New apportations", "text": ""}, {"heading": "3.1 Redundancy analysis", "text": "To begin with the redundancy analysis of Relief, we first of all have to define exactly the meaning of redundancy. In general the definitions of redundancy we find in the literature are based on feature correlation, i.e. two features are\nredundant if their values are correlated. One interesting particular case is when one feature is an exact copy of another so their values are completely correlated, one feature is obviously redundant. But in reality a feature may not be completely correlated with another feature but may be (partially) correlated with a set of features. In such case it\u2019s not straightforward to determine redundancy. We can take as an example the features shown in Table 2. The feature fr is intuitively redundant with the set {f1, f2} but is not correlated with any of them, so it would not be redundant according to the correlation based definition of redundancy. So we have to find a better definition for feature redundancy\nthat enables us to identify not only pairs of redundant features but features redundant with any set of other features. Before giving the formal definition of redundancy let\u2019s introduce some previous definitions:\nDefinition 3.1 Let U = {\u03b1, \u03b2, . . .} be a set of discrete variables in a problem domain. Each variable is associated with a set of possible values. A configuration or a tuple u\u2032 of U\u2032 \u2286 U is an assignment of values to every variable in U\u2032.\nDefinition 3.2 A probabilistic domain model (PDM) P over U determines the probability P (u\u2032) of every tuple u\u2032 of U\u2032 for each U\u2032 \u2286 U.\nDefinition 3.3 For three disjoint subsets X, Y and Z \u2286 U, X and Y are said to be conditionally independent given Z under P , noted I(X,Z,Y)P or simply I(X,Z,Y) from now on, if (see [Pearl, 1988, pp 83\u201397])\nI(X,Z,Y) \u2261 P (x|y, z) = P (x|z) whenever P (y, z) > 0 (3.1)\nUsing this notation we can express unconditional independence as I(X, \u2205,Y), i.e.,\nI(X, \u2205,Y) \u2261 P (x|y) = P (x) whenever P (y) > 0\nNote that I(X,Z,Y) implies the conditional independence of all pairs of variables \u03b1 \u2208 X and \u03b2 \u2208 Y, but the converse is not necessarily true.\nDefinition 3.4 A Markov Blanket BLI(\u03b1) of an element \u03b1 \u2208 U is any subset S \u2282 U for which (see [Pearl, 1988])\nI(\u03b1,S,U\u2212 S\u2212 \u03b1) and \u03b1 /\u2208 S. (3.2)\nAn intuitive interpretation of Def. 3.3 would be: Once Z is given, the probability of X will not be affected by the discovery of Y. Or Y is irrelevant to X once we know Z. Note that the Markov blanket condition in Def. 3.4 is stronger than conditional independence. It is saying that not only that knowing \u03b1 is irrelevant to the class, but also to the rest of the features, so S has all the information that \u03b1 has about C and all the information \u03b1 has about U\u2212 S\u2212\u03b1. This takes us to our definition of redundancy:\nDefinition 3.5 Given a set of features F and a class feature C, a redundant feature \u03b1 \u2208 F is a feature for which exists a Markov blanket S = BLI(\u03b1) within {F, C} such that S \u2282 F.\nAn interesting property of Markov blankets is that if we removed a feature \u03b1 such that existed BLI(\u03b1) \u2282 U and now we are eliminating another feature \u03b2 such that exists BLI(\u03b2) \u2282 U\u2212 \u03b1 then we can prove that also exists BLI(\u03b1) \u2282 U\u2212 \u03b2, we can see the proof in [Koller and Sahami, 1996]. That is, a redundant feature remains redundant when other redundant features are removed. So if we proceed to remove features using this criterion, we will never have to reconsider our decisions.\nUnfortunately, there we rarely find a fully redundant feature, but rather one that its information is nearly subsumed by other features. So we would like to know not only whether a feature is redundant or not but its redundancy grade. We would like a function R\u2032 which given an feature \u03b1 \u2208 U and a set of features U \u2208 U gives us a degree of redundancy of this feature to the set. Ideally we would like a function R\u2032 : U\u00d7 U \u2192 [0, 1] than satisfies the following propositions:\nR\u2032(\u03b1,BLI(\u03b1)) = 1\nR\u2032(\u03b1,U\u2212 \u03b1i) \u2264 R \u2032(\u03b1,U), \u2200\u03b1i \u2208 U\nTo achieve this we should change the boolean definition of conditional independence to a some function of P (x|y, z) and P (x|z).\nDefinition 3.6 If we have that: U is our set of features, \u03b1 is the feature we are evaluating, and S is some subset of U not containing \u03b1. We defined u as a configuration of U. We will write su, s \u22121 u\nand \u03b1u for the configuration of S, the configuration of U\u2212S\u2212\u03b1 and the value of \u03b1 respectively when the configuration of U is u. Now we can define U as the set of all possible configurations of U for which P (u\u2212 su \u2212 \u03b1u, su) > 0.\nWith all that, we define Redundancy level R\u2032 as:\nR\u2032(\u03b1,U) = 1\u2212 max S\u2282U\u2212\u03b1\n(\n\u2211\nu\u2208U\n\u2223 \u2223P (\u03b1u|su)\u2212 P (\u03b1u|s\u22121u , su) \u2223 \u2223\n|U|\n)\nNote that the calculation of this redundancy level is exponential in the number of features in our set, as it compares the conditional probabilities of all\npossible subsets of U, so the max function will have to compare |P(U)| = 2|U| terms. And for each subset we also have an exponential cost in the number of values of the features because the sum is over each configuration u of U.\nIt is clear to see that, although Eq. 3.6 gives an intuitively consistent definition of redundancy level, its computational cost might be too large for R\u2032 to be directly applied in a feature weighting (or feature selection) algorithm. We should use an estimation of R\u2032 that maximized the tradeoff between accuracy and complexity. But in fact the aim of the definition of R\u2032 was not to have an efficient algorithm to calculate the redundancy level of a feature. The definition had three basic (related) objectives: first of all to provide a suitable formal definition of redundancy in order to study the effect of feature redundancy in the different existing algorithms, for instance ReliefF. And second to serve as some starting point for new extensions to methods which performance decreases in the presence of redundant features, again Relief is an example. And finally, to direct the developing of new algorithms that effectively and efficiently estimate redundancy."}, {"heading": "3.2 Double Relief", "text": "When more and more irrelevant features are added to a dataset the distance calculation of Relief degrades its performance as instances may be considered neighbors when in fact they are far from each other if we compute its distance only with the relevant features. In such cases the algorithm may lose its context of locality and in the end it may fail to recognize relevant features.\nThe diff(Ai, I1, I2) function calculates the difference between the values of the feature Ai for two instances I1 and I2. Sum of differences over all features is used to determine the distance between two instances in the nearest hit and miss calculation (see equation 2.16).\nAs seen in the k-nearest neighbors classification algorithm (kNN) many weighting schemes which assign different weights to the features in the calculation of the distance between instances (see equation 3.3).\n\u03b4\u2032(I1, I2) =\na \u2211\ni=1\nw(Ai) diff(Ai, I1, I2) (3.3)\nIn the same way that in [Wettschereck et al., 1997] Relief\u2019s estimates of features\u2019 quality have been used successfully as weights for the distance calculation of kNN we could use their estimation in the previous iteration to compute the distance between instances while searching the nearest hits and misses. We will refer to this version of ReliefF as double ReliefF or in short dReliefF. The problem using the weights estimates could be that in early iterations these estimations could be too biased to the first instances and could be far from the optimal weights. So, for small t, W [Ai] is very different from W [Ai]t.\nWhat we want is to begin the distance calculation without using the weight estimates and then, as Relief\u2019s weight estimates become more accurate (because more instances have been taken into account), increase the importance of these\nweights in the distance calculation. Lets have a distance calculation like the one in equation 3.4.\n\u03b4(I1, I2) =\na \u2211\ni=1\nf(W (Ai)t, t) diff(Ai, I1, I2) (3.4)\nWe would like a function f : R\u00d7 (0,\u221e) \u2192 R such that:\n\u2022 f(w, t) is increasing with respect to t\n\u2022 is continuous\n\u2022 f(w, 0) = 1\n\u2022 f(w,\u221e) = w\nOne such function could be the one in equation 3.5. And we will refer to the version of ReliefF using this distance equation as progressively weighted double relief or in short pdReliefF.\nf(w, t) = \u2212w + 1\ntT + w (3.5)\nWhere T is a control parameter that determines the steepness of the curve described by f (see figure 4). Another desirable property for our function would\nbe that it always gives the same results regardless of the number of iterations. In other words, if m is the total number of iterations, we would like f(w,m) to\nbe the same value whatever the value of m. To achieve that we must vary the value of T according to the total number of iterations so as to decrement the steepness of the function as the number of total iterations increases. The value of T for f(w,m) to be the same is T = 2/ log(m). In figure 5 we can see how f varies the influence of different weights (even a non realistic one that is greater than 1) as iterations go on. We can see that with this value for T the function converges in the first few iterations and then it stabilizes its value near w. For problems with many iterations a softer function may be tried if values converge prematurely."}, {"heading": "4 Empirical results", "text": "To begin with the empirical results we have to define a measure of success for the weights estimations. First of all we need to have a success criterion. For problems where we know which of the features are important (e.g., artificial datasets) some we can use this knowledge to evaluate estimates. In [Kononenko, 1994] separability and usability, two more indicators may be useful in case of negative separability - minimality and completeness - which can help in determining the quality of the given solution. See more precise definitions below.\nseparability Shows the ability of the weight estimates to distinguish between important and unimportant features. Positive separability (s > 0) means that important features are correctly separated from unimportant ones.\ns = WIworst \u2212WRbest \u2208 [\u22122, 2]\nusability Shows the ability of the weight estimates to distinguish on of the important feature from the unimportant ones. Positive usability (u > 0) means that almost one of the important features is correctly separated from unimportant ones.\nu = WIbest \u2212WRbest \u2208 [\u22122, 2]\nminimality Shows the ratio of important features in the minimum set of features that contains all the important features if we select features in decreasing weight order. Note that s > 0 \u21d2 m = 1.\nm = |I|/|M| \u2208 (0, 1] where M = {F |WF \u2265 WIworst}\ncompleteness Shows the ratio of important features that we would take if selecting features in decreasing weight order we stopped before selecting the first unimportant feature. Note that again s > 0 \u21d2 m = 1.\nc = |C|/|I| \u2208 (0, 1] where C = {F |WF > WRbest}\nThe first set of artificial problems to use is the so-called Modulo-p-I. In these datasets we will find I important features and R random ones. All of them integers in the range [0,p). The class value C is also an integer in the same range and can be calculated for an instance X having values X1, X2, . . . , XI in its important features as seen on Eq. 4.1. We will test our criteria for various parameters of ReliefF on two different problems (Modulo-2-2 and Modulo-4-3) incrementally adding random features.\nC(X) =\n(\nI \u2211\ni=1\nXi\n)\nmod p (4.1)\nIn Fig. 6 and 7 we can see the different behaviors of the three algorithms when more and more random features are added. While ReliefF seems to gradually degrade its performance, dReliefF is more erratic and pdReliefF obtains the best results. This supports our theory that although it seems a good idea to use ReliefF\u2019s own estimates as weights for its distance function, a bad start can make dReliefF\u2019s estimates even poor than ReliefF\u2019s.\nAnother good test is CorrAl dataset introduced in [Kohavi and John, 1997]. This dataset is composed of 6 features (A0, A1, B0, B1, C, I). C is 75% correlated with the class and four other features that can fully determine the class of the instance when used together. The class can be expressed as: (A0 \u2227 A1) \u2228 (B0 \u2227B1). And the last one, I, is completely random. Results are shown in table 3. So for CorrAl dataset the three algorithms correctly identify the irrelevant feature and rank it last, but the normal version of ReliefF give a larger weight to the correlated feature than it should be given. The double versions of the algorithm in the other hand correctly identify the four features that completely determine the class and give them larger weights, followed by the correlated one and leaving the random one last. We can see that the behavior of the two double versions is very similar, although the progressive weighted estimation is a little more usable, it\u2019s a little less separable.\nThe next dataset (led24) is one of the LED display domain datasets from [S. Hettich and Merz, 1998]. In fact it is an extension of the led7 dataset. The led7 dataset consists of 7 boolean valued features (I1, . . . , I7) each of them representing one of the light-emitting diodes contained on a LED display. They indicate whether the corresponding segment is on or off (see Fig. 8). And the class feature has range [0,9] and coincides with the digit represented by the display. This dataset has another added difficulty as it has a 10% of noise in its features, i.e., each instance\u2019s feature has a 10% chance of having its value negated. This is a quite difficult problem for classifiers and the version with 17 unimportant features is especially difficult, e.g. a nearest neighbor classification algorithm falls from a 71% of classification success with the 7 feature version to a poor 41% with the other one. So it would be desirable for ReliefF to separate the important features from the rest. Table 4 shows separability and usability\nfor this dataset. There it can be seen that the behavior for the three algorithms is extremely similar for this domain. All of them are able to separate the seven important features from the rest and even the values for s and u are almost the same for the three algorithms.\nFinally, the last artificial datasets to be tested are Monks datasets. They are interesting because even though they do not consist of lots of features, they are\nwell known datasets, have interesting feature interactions and can serve us to compare the algorithms order of each feature with its intended ordering. There are three Monks datasets but we will only use Monk-1 and Monk-3 because Monk-2 does not contain unimportant features. They consist of six numerical features A1, . . . , A6 with ranges varying from [1,2] to [1,4] and a boolean class value. For Monk-1 the class CM1 can be calculated as CM1 = (A1 = A2)\u2228(A5 = 1) and the class CM3 for Monk-3 as CM3 = (A5 = 3\u2227A4 = 1)\u2228(A5 6= 4\u2227A2 6= 3). So for the first problem, A3, A4 and A6 are unimportant and among the other three, A1 and A2 would help us better determine the class value than A5 as only one of the four possible values of A5 is important. For Monk-3 the important features will only be A5, A4 and A2 and the rest do not influence the instance\u2019s class. Among these three features, A5 and A2 should be preferred over A4 as using only the second term of the disjunct we can achieve a 97% performance. It is important to say that Monk-3 has a 5% of additional noise (misclassifications). Table 5 shows the results for the three variants of ReliefF when applied to\nthe Monk-1 dataset. We observe that although the three algorithms correctly separate the important features from the unimportant ones, only ReliefF gives the expected ordering for the important features. The same results for Monk3 dataset are shown in table 6. For this dataset we can see how separability, even though positive, is very small for the three algorithms. In addition, all of them rank A4 as the lowest of the important features which agrees with what we thought they should do. Here the double versions of the algorithms seem\nto help discriminating the important from the unimportant features, in the two cases they improve separability although the important feature order is worse. The double versions seem to increase the weight difference between important and unimportant features but decrease the weight difference of features in the same group.\nThe second group of experiments is with some well known datasets from UCI [S. Hettich and Merz, 1998]. These are datasets of real data, so we don\u2019t know which of the features may be important and which + may be not. For this reason we will not be able to compute the above criteria for these datasets. So to evaluate the quality of the algorithms\u2019 estimates we will use the performance obtained with a classifier. We will make various tests with the classifier. We will first of all try a classification with the feature with the greatest weight, then will use the two most weighted variables, end so on until all variables are used. When all tests are completed we will compare the performance of the classifier when using all features with the performance when using the best subset found using Relief\u2019s estimates. We will use the 1NN classifier because of its simplicity and sensibility to a bad choice of features.\nThe first chosen dataset is the E. coli promoter gene sequences. This dataset contains a set of 57 nominal variables representing a DNA sequence of nucleotides. A promoter is a DNA sequence that enables a gene to be transcribed. The promoter is recognized by RNA polymerase, which then initiates transcription. For the RNA polymerase to make contact, the DNA sequence must have a valid conformation so that the two pieces of the contact region spatially align. But shape of the DNA molecule is a very complex function of the nucleotide sequence due to the so complex interactions between them, so strong interactions among features are expected. In Fig. 9 we can see the results of applying feature selection in the way described above for the 1-NN classifier. As can be seen the results for the classification task are in general not very good, but we can see that for all the three versions of the algorithm the maximum performance is achieved when the number of used features is 2, much less than the initial 57 features. The three versions of the algorithm agree in the first two features to be add (15 and 16), although the ordering in the case of normal Relief is inverted it selects 16 first and then 15.\nAnother problem that can serve us to determine whether the weighted distance calculation makes sense is the lung cancer dataset also from UCI. It consists of data from 32 patients suffering three different types of pathological lung cancers. The objective is to distinguish among the three types of cancer given a set of 56 nominal features with ranges [0,3]. Authors of the dataset gave no information on the meaning of individual features. But probably data may be from different types of tests performed on patients and as there are many features one can venture the hypothesis that many of them may be standard tests that can not help in determining the patient\u2019s type of disease. So these unimportant features may affect the way that ReliefF chooses the nearest neighbors. Moreover, it is especially important to reduce the number of features in this problem because the number of instances is very low compared to it so classifiers may be fooled by unimportant features. We can see in Fig. 10 that\nin this case performance of the 1-NN classifier is significantly improved when we apply feature selection for this problem. While a classification using all of the features gives us a correct classification percent of 43.75, the bests results obtained with a subset of the features is above 90% with all of the versions of ReliefF. Although the same performance is achieved with the best subset given by dReliefF and the one given by pdReliefF, the results of the latter are better as the same performance is achieved with 4 less features."}, {"heading": "5 Conclusions and future work", "text": "In our experiments we have seen how the double versions of the algorithm helped in the correct feature weighting of some problems while in other cases performance is not improved and even it is diminished. An interesting property of these new versions of the algorithm is than they seem to help in problems where many irrelevant features exist, which was the initial objective. The performance of the algorithm improved in the modulo-p-I problems as more and more random features were added. We saw in the experiments that although ReliefF\u2019s performance with few attributes was better, as the number of random features increased it began to decrease and for a relatively small number of random features dReliefF and pdReliefF overperformed the original algorithm. Furthermore the performance of the latter methods did not vary with the addition of random features. In contrast, the results obtained with the LED dataset were not that encouraging. Although the dataset had more than twice random features than relevant ones the results for the three algorithms were very similar. This might be because the separability for this problem was so low (though positive) due to the difficulty of the problem (even without random features) for the presence of noise. The try with datasets having fewer irrelevant features, i.e. the Monks problems, gave very similar results for all the versions. This has a logical explanation: the behavior of the double version if all the attributes are relevant is not very different from the original one.\nThe experiments with real data from the UCI Machine Learning Repository [Hunt et al., 1966] that consisted in running a 1-NN classifier using successive subsets of features proposed by the three versions of ReliefF showed interesting results. The success evaluation criteria was the percentage of instances classified correctly using 5-fold crossvalidation. Two datasets were chosen for this experiments because of their large number of features and the intuition that they might contain large number of irrelevant features. In both of them the\ndouble versions of the algorithm chose a subset of features that helped the 1-NN best in classifying the instances. In the case of the DNA promoters dataset the performance increase was not significant but this may be due to the fact that 1-NN do not seem to be capable of solving this problem as it gave poor results in all cases. On the other hand, for the lung cancer dataset, we obtained significantly better classifying performance with the subset from the double versions and, in addition, the subset found by the pdReliefF had less variables than the one found by dReliefF.\nWe experienced almost no difference between the two double versions. This may be because of the progressive weighting function used. The function attenuates the weight estimates influence at first iterations but rapidly increases their influence and after the first few iterations the algorithm behaves exactly as dReliefF. So as a future work some other softer functions may be tested.\nAnother clear line of future work is the formal study of the influence that having redundant features has to ReliefF, dReliefF and pdReliefF. Robnik-\u0160ikonja and Kononenko started this study in [Robnik-\u0160ikonja and Kononenko, 2003] where they proved that the addition of successive copies of one feature divided the initial weight ReliefF assigned to the feature among all the copies. And they received the same weight. But still some crucial questions have to be answered: Do equal weights for two features mean that features are redundant to each other? Does an equal sequence of weight actualizations for two features mean that they are redundant to each other? How can Relief be extended to diminish or eliminate the negative effect of redundant features? Does ReliefF compute some kind of approximation to R\u2032?"}], "references": [{"title": "Classification and Regression Trees", "author": ["Breiman et al", "L. 1984] Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1984\\E", "shortCiteRegEx": "al. et al\\.", "year": 1984}, {"title": "Trading mips and memory for knowledge engineering", "author": ["Creecy et al", "R.H. 1992] Creecy", "B.M. Masand", "S.J. Smith", "D.L. Waltz"], "venue": "Commun. ACM,", "citeRegEx": "al. et al\\.,? \\Q1992\\E", "shortCiteRegEx": "al. et al\\.", "year": 1992}, {"title": "Experiments in Induction", "author": ["Hunt et al", "E.B. 1966] Hunt", "J. Marin", "P.J. Stone"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1966\\E", "shortCiteRegEx": "al. et al\\.", "year": 1966}, {"title": "The feature selection problem: Traditional methods and a new algorithm", "author": ["Kira", "Rendell", "K. 1992] Kira", "L.A. Rendell"], "venue": "In AAAI,", "citeRegEx": "Kira et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Kira et al\\.", "year": 1992}, {"title": "Wrappers for feature subset selection", "author": ["Kohavi", "John", "R. 1997] Kohavi", "G.H. John"], "venue": "Artif. Intell.,", "citeRegEx": "Kohavi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kohavi et al\\.", "year": 1997}, {"title": "Toward optimal feature selection", "author": ["Koller", "Sahami", "D. 1996] Koller", "M. Sahami"], "venue": null, "citeRegEx": "Koller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Koller et al\\.", "year": 1996}, {"title": "Information Theory, Inference, and Learning Algorithms", "author": ["MacKay", "D.J.C. 2003] MacKay"], "venue": null, "citeRegEx": "MacKay and MacKay,? \\Q2003\\E", "shortCiteRegEx": "MacKay and MacKay", "year": 2003}, {"title": "An optimal weighting criterion of case indexing for both numeric and symbolic attributes", "author": ["Mohri", "Tanaka", "T. 1994] Mohri", "H. Tanaka"], "venue": null, "citeRegEx": "Mohri et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 1994}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["Robnik-\u0160ikonja", "Kononenko", "M. 2003] Robnik-\u0160ikonja", "I. Kononenko"], "venue": "Machine Learning,", "citeRegEx": "Robnik.\u0160ikonja et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Robnik.\u0160ikonja et al\\.", "year": 2003}, {"title": "A mathematical theory of communication", "author": ["Shannon", "C.E. 1948] Shannon"], "venue": "Bell System Tech. J.,", "citeRegEx": "Shannon and Shannon,? \\Q1948\\E", "shortCiteRegEx": "Shannon and Shannon", "year": 1948}, {"title": "A review and empirical evaluation of feature weighting methods for a class of lazy learning", "author": ["Wettschereck et al", "D. 1997] Wettschereck", "D.W. Aha", "T. Mohri"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1997\\E", "shortCiteRegEx": "al. et al\\.", "year": 1997}, {"title": "Improved heterogeneous distance functions", "author": ["Wilson", "Martinez", "D.R. 1997] Wilson", "T.R. Martinez"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Wilson et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 1997}], "referenceMentions": [], "year": 2017, "abstractText": "Feature weighting algorithms try to solve a problem of great importance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem\u2019s domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task. Some other feature weighting methods are reviewed in order to give some context and then the different existing extensions to the original algorithm are explained. One of Relief\u2019s known issues is the performance degradation of its estimates when redundant features are present. A novel theoretical definition of redundancy level is given in order to guide the work towards an extension of the algorithm that is more robust against redundancy. A new extension is presented that aims for improving the algorithms performance. Some experiments were driven to test this new extension against the existing ones with a set of artificial and real datasets and denoted that in certain cases it improves the weight\u2019s estimation accuracy.", "creator": "LaTeX with hyperref package"}}}