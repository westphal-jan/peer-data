{"id": "1509.06095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2015", "title": "Multilayer bootstrap network for unsupervised speaker recognition", "abstract": "We apply multilayer bootstrap network (MBN), a recent proposed unsupervised learning method, to unsupervised speaker recognition. The proposed method first extracts supervectors from an unsupervised universal background model, then reduces the dimension of the high-dimensional supervectors by multilayer bootstrap network, and finally conducts unsupervised speaker recognition by clustering the low-dimensional data. The comparison results with 2 unsupervised and 1 supervised speaker recognition techniques demonstrate the effectiveness and robustness of the proposed method.", "histories": [["v1", "Mon, 21 Sep 2015 02:28:44 GMT  (185kb,D)", "http://arxiv.org/abs/1509.06095v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SD", "authors": ["xiao-lei zhang"], "accepted": false, "id": "1509.06095"}, "pdf": {"name": "1509.06095.pdf", "metadata": {"source": "CRF", "title": "MULTILAYER BOOTSTRAP NETWORK FOR UNSUPERVISED SPEAKER RECOGNITION", "authors": ["Xiao-Lei Zhang"], "emails": ["xiaolei.zhang9@gmail.com"], "sections": [{"heading": null, "text": "Index Terms\u2014 multilayer bootstrap network, speaker recognition, unsupervised learning."}, {"heading": "1. INTRODUCTION", "text": "Speaker recognition aims to identify speakers from their voices. It is important in many speech systems, such as speaker diarization, language recognition, and speech recognition. Supervised methods include maximum a posteriori estimation [1, 2], linear discriminative analysis (LDA) [3, 4], support vector machines [2], deep neural networks [5, 6], etc.\nBecause constructing a manually-labeled corpus is laboring intensive and time-consuming, it is strongly needed to develop unsupervised speaker recognition methods. Existing methods mainly include principle component analysis (PCA), k-means clustering, Gaussian mixture model (GMM), agglomerative hierarchical clustering, and joint factor analysis. For example, Wooters and Huijbregts [7] used agglomerative clustering to merge speaker segments by Bayesian information criterion. Iso [8] used vector quantization to encode speech segments and used spectral clustering, which is a kmeans clustering applied to a low-dimensional subspace of data, for speaker recognition. Nwe et al. [9] used a group of GMM clusterings to improve individual base GMM clusterings. Some methods apply clustering techniques, e.g. variational Bayesian expectation-maximization (EM) GMM [10] and spectral clustering [11], to a low-dimensional total variability subspace [4] that is learned from high-dimensional supervectors by joint factor analysis [4]. Some methods compensate the total variability space with new items, e.g. [12].\nBecause little prior knowledge of data is known beforehand, an unsupervised method should satisfy the following conditions: (i) no need for manually-labeled training data; (ii) no hyperparameter tunning for a satisfied performance; and (iii) robustness to different data or modeling conditions. Due to these strict requirements, unsupervised speaker recognition is a very difficult task. In this paper, we present a multilayer bootstrap network (MBN) [13] based algorithm. MBN is a recent proposed unsupervised nonlinear dimensionality reduction algorithm. Experimental results show that the proposed method satisfies these requirements.\nThis paper is organized as follows. In Section 2, we present the MBN-based system. In Section 3, we present the MBN algorithm and its typical hyperparameter setting. In Section 4, we present the relationship between MBN and deep learning. In Section 5, we report comparison results. In Section 6, we conclude this paper."}, {"heading": "2. SYSTEM", "text": "Given an unlabeled speaker recognition corpus, we propose the following unsupervised algorithm:1\n\u2022 The first step trains a speaker- and session-independent unsupervised universal background model (UBM) [1] from an acoustic feature, which produces a ddimensional supervector for each utterance, denoted as x = [nT , fT ]T where n is the accumulation of the mixture occupation over all frames of the utterance and f is the vector form of the centered first order statistics.\n\u2022 The second step reduces the dimension of x from d to d\u0304 (d\u0304 d) by multilayer bootstrap network (MBN) which is introduced in Section 3.\n\u2022 The third step conducts k-means clustering on the lowdimensional data if the number of the underlying speakers is known, or agglomerative clustering if the number of the speakers is unknown.\n1The source code is downloadable from http://sites.google.com/site/ zhangxiaolei321/speaker_recognition\nar X\niv :1\n50 9.\n06 09\n5v 1\n[ cs\n.L G\n] 2\n1 Se\np 20\n15"}, {"heading": "3. MULTILAYER BOOTSTRAP NETWORK", "text": "The structure of MBN [13] is shown in Fig. 1. MBN is a multilayer localized PCA algorithm that gradually enlarges the area of a local region implicitly from the bottom hidden layer to the top hidden layer by high-dimensional sparse coding, and gets a low-dimensional feature explicitly by PCA at the output layer.\nEach hidden layer of MBN consists of a group of mutually independent k-centers clusterings. Each k-centers clustering has k output units, each of which indicates one cluster. The output units of all clusterings are concatenated as the input of their upper layer [13].\nMBN is trained layer-by-layer from bottom up. For training a hidden layer given a d-dimensional input X = {x1, . . . ,xn}, MBN trains each clustering independently [13]:\n\u2022 Random feature selection. The first step randomly selects d\u0302 dimensions of X (d\u0302 \u2264 d) to form a new set X\u0302 = {x\u03021, . . . , x\u0302n}. This step is controlled by a hyperparameter a = d\u0302/d.\n\u2022 Random sampling. The second step randomly selects k data points from X\u0302 as the k centers of the clustering, denoted as {w1, . . . ,wk}. This step is controlled by a hyperparameter k.\n\u2022 Random reconstruction. The third step randomly selects d\u2032 dimensions of the k centers (d\u2032 \u2264 d\u0302/2) and does a one-step cyclic-shift as shown in Fig. 2. This step is controlled by a hyperparameter r = d\u2032/d\u0302.\n\u2022 Sparse representation learning. The fourth step assigns the input x\u0302 to one of the k clusters and outputs a k-dimensional indicator vector h = [h1, . . . , hk]T . For example, if x\u0302 is assigned to the second cluster, then h = [0, 1, 0, . . . , 0]T . The assignment is calculated according to the similarities between x\u0302 and the k centers, in terms of some predefined similarity measurement at the bottom layer, such as the minimum squared loss arg minki=1 \u2016wi\u2212 x\u0302\u20162, or in terms of arg maxki=1 wTi x\u0302 at all other hidden layers [13]."}, {"heading": "3.1. A typical hyperparameter setting", "text": "MBN has five hyperparameters { V,L, {kl}Ll=1, a, r } where V is the number of k-centers clusterings per layer, L is the number of hidden layers, and kl is the hyperparameter k at the lth hidden layer. As shown in [13], MBN is robust to hyperparameter selection. Here we introduce a typical setting:\n\u2022 Setting hyperparameter k. (i) k1 should be as large as possible, i.e. k1 \u2192 n. Suppose the largest k supported by hardware is kmax, then k1 = min(0.9n, kmax). (ii) kl decays with a factor of, e.g. 0.5, with the increase of hidden layers. That is to say, kl = 0.5kl\u22121. (iii) kL should be larger than the number of speakers c. Typically, kL \u2248 1.5c. If c is unknown, we simply set kL to a relatively large number, e.g. 30, since c is unlikely larger than 30 in a practical dialog.\n\u2022 Setting hyperparameter r. When a problem is smallscale, e.g. k1 > 0.8n, then r = 0.5; otherwise, r = 0.\n\u2022 Setting other hyperparameters. Hyperparameter V should be at least larger than 100, typically V = 400. Hyperparameter a is fixed to 0.5. Hyperparameter L is determined by k."}, {"heading": "4. RELATED WORK", "text": "The proposed method learns multilayer nonlinear transforms, which is related to deep learning (a.k.a., multilayer neural networks)\u2014a recent advanced topic in many speech processing fields, e.g. speaker recognition [5, 6], speech recognition [14], speech separation and enhancement [15\u201318], speech synthesis [19], and voice activity detection [20, 21]. The aforementioned deep learning methods are all supervised ones and limited to neural networks, while the proposed method is an unsupervised one and different from neural networks."}, {"heading": "5. EXPERIMENTS", "text": ""}, {"heading": "5.1. Experimental setup", "text": "We used the training corpus of speech separation challenge (SSC) [22]. The training corpus contains 34 speakers, each\nof which has 500 clean utterances. We selected the first 100 utterances (a.k.a, sessions) of each speaker for evaluation, which amounts to 3400 utterances. We set the frame length to 25 milliseconds and frame shift to 10 milliseconds, and extract a 25-dimensional MFCC feature.\nFor the proposed MBN-based speaker recognition, we adopted the typical parameter setting of MBN. Specifically, V = 400, a = 0.5, r = 0.5, and k were set to 3060-1530-765-382-191-95. The output of PCA was set to {2, 3, 5, 10, 30, 50} dimensions respectively. We assumed that the number of speakers was known, and used k-means clustering for clustering the low-dimensional data.\nWe compared with PCA, k-means clustering, and an LDA-based system, where the first two methods are unsupervised and the third one is supervised. For the PCA-based method, we first used the same UBM as the MBN-based method to extract high-dimensional supervectors, then reduced the dimension of the supervectors to {2, 3, 5, 10, 30, 50} respectively, and finally evaluated the low-dimensional output of PCA by k-means clustering. For the k-means-clusteringbased method, we apply k-means clustering to the highdimensional supervectors directly.\nThe LDA-based system2 uses UBM to extract a highdimensional feature, then uses joint factor analysis to reduce the high-dimensional feature to an intermediately low dimensional representation in an unsupervised way, and finally uses LDA, a supervised dimensionality reduction method, to reduce the intermediate representation to a low-dimensional subspace where classification is conducted by a probabilistic LDA algorithm. Since factor analysis is an unsupervised dimensionality reduction method, we set its output to {2, 3, 5, 10, 30, 50} dimensions respectively for comparison. We constructed a training set from the SSC corpus for this supervised method: each speaker consists of 100 training utterances, which are selected from the 400 remaining utterances of the speaker.\nThe performance was measured by normalized mutual information (NMI) [23]. MNI was proposed to overcome the label indexing problem between the ground-truth labels and the predicted labels. It is one of the standard evaluation metrics of unsupervised learning. The higher the NMI is, the better the performance is. We also report the classification accuracy of the LDA-based system in the Supplementary Material3 where we can see that NMI is consistent with classification accuracy."}, {"heading": "5.2. Results", "text": "Because all comparison methods use UBM to extract speakerand session-independent supervectors, we need to study how they behave in different UBM settings, in terms of mixture number and expectation-maximization (EM) iterations. (i)\n2The source code is downloadable from http://research.microsoft.com/enus/downloads/a6262fec-03a7-4060-a08c-0b0d037a3f5b/\n3http://sites.google.com/site/ zhangxiaolei321/speaker_recognition\nThe mixture number of UBM reflects the capacity of UBM for modelling an underlying data distribution: if the mixture number of UBM is smaller than the number of speakers, UBM is likely underfitting, i.e. it cannot grasp the data distribution well. To study this effect, we set the mixture number of UBM to {1, 2, 4, 8, 16, 32, 64} respectively. (ii) The number of EM iterations of UBM reflects the quality of the acoustic feature produced by UBM: if the EM optimization is not sufficient, the acoustic feature is noisy. To study this effect, we set the number of EM iterations of UBM to {0, 20} respectively, where setting the number of iterations to 0 means that UBM is initialized with randomly sampled means without EM optimization, which is the worst case.\nFig. 3 and Supplementary-Fig. 1 give a comparison example between PCA and MBN in visualizing the first 10 speakers, where a 16-mixtures UBM with 20 and 0 EM iteration are used to generate their inputs respectively. From the figures, we can see that MBN produces ideal visualizations.\nFig. 4 reports results with respect to the mixture number of UBM. Fig. 5 reports results with respect to the number of output dimensions. Supplementary-Tables 1 and 3 report the detailed results of the two figures. From the figures and tables, we observe the following phenomena: (i) the MBNbased method outperforms the PCA- and k-means-clusteringbased methods and approaches to the supervised LDA system in all cases; (ii) the MBN-based method is less sensitive to different parameter settings of both UBM and MBN itself; (iii) the LDA-based system is less sensitive to the mixture number of UBM, but sensitive to the number of output dimensions; (iv) the PCA-based method is sensitive to both the mixture number of UBM and the number of output dimensions, and strongly relies on the effectiveness of UBM; (v) the performance of the k-means-clustering-based method is consistent with that of the PCA-based method.\nFig. 6 reports results of the MBN-based method with respect to the number of hidden layers. From the figure, we ob-\nserve that the accuracy improves gradually with the increase of the number of hidden layers."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we have proposed a multilayer bootstrap network based unsupervised speaker recognition algorithm. The method first uses UBM to extract a high-dimensional feature from the original MFCC acoustic feature, then uses MBN to reduce the high-dimensional feature to a low-dimensional space, and finally clustering the low-dimensional data. We have compared it with the PCA-, k-means-clustering-, and LDA-based methods, where the first two methods are unsupervised and the third method is supervised. Experimental results have shown that the proposed method outperforms\nthe unsupervised methods and approaches to the supervised method. Moreover, it is insensitive to different parameter settings of UBM and MBN, which facilitates its practical use."}, {"heading": "7. ACKNOWLEDGEMENT", "text": "The author thanks Prof DeLiang Wang for providing the Ohio Computing Center and Dr Ke Hu for helping with the SSC corpus."}, {"heading": "8. REFERENCES", "text": "[1] Douglas A Reynolds, Thomas F Quatieri, and Robert B Dunn, \u201cSpeaker verification using adapted gaussian mixture models,\u201d Digital Signal Process., vol. 10, no. 1, pp. 19\u201341, 2000.\n[2] William M Campbell, Douglas E Sturim, Douglas A Reynolds, and Alex Solomonoff, \u201cSVM based speaker verification using a GMM supervector kernel and NAP variability compensation,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2006, vol. 1, pp. 97\u2013 100.\n[3] Patrick Kenny, Gilles Boulianne, Pierre Ouellet, and Pierre Dumouchel, \u201cJoint factor analysis versus eigenchannels in speaker recognition,\u201d IEEE Trans. Audio, Speech, Lang. Process., vol. 15, no. 4, pp. 1435\u20131447, 2007.\n[4] Najim Dehak, Patrick Kenny, R\u00e9da Dehak, Pierre Dumouchel, and Pierre Ouellet, \u201cFront-end factor analysis for speaker verification,\u201d IEEE Trans. Audio, Speech, Lang. Process., vol. 19, no. 4, pp. 788\u2013798, 2011.\n[5] Ke Chen and Ahmad Salman, \u201cLearning speakerspecific characteristics with a deep neural architecture,\u201d IEEE Trans. Neural Netw., vol. 22, no. 11, pp. 1744\u2013 1756, 2011.\n[6] Xiaojia Zhao, Yuxuan Wang, and DeLiang Wang, \u201cCochannel speaker identification in anechoic and reverberant conditions,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 22, no. 11, pp. 1727\u20131736, 2015.\n[7] Chuck Wooters and Marijn Huijbregts, \u201cThe ICSI RT07s speaker diarization system,\u201d in Multimodal Technologies for Perception of Humans, pp. 509\u2013519. Springer, 2008.\n[8] Ken-ichi Iso, \u201cSpeaker clustering using vector quantization and spectral clustering,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2010, pp. 4986\u20134989.\n[9] Tin Lay Nwe, Hanwu Sun, Bin Ma, and Haizhou Li, \u201cSpeaker clustering and cluster purification methods for rt07 and rt09 evaluation meeting data,\u201d IEEE Trans. Audio, Speech, Lang. Process., vol. 20, no. 2, pp. 461\u2013473, 2012.\n[10] Stephen H Shum, Najim Dehak, R\u00e9da Dehak, and James R Glass, \u201cUnsupervised methods for speaker diarization: An integrated and iterative approach,\u201d IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 10, pp. 2015\u20132028, 2013.\n[11] Naohiro Tawara, Tetsuji Ogawa, and Tetsunori Kobayashi, \u201cA comparative study of spectral clustering for i-vector-based speaker clustering under noisy conditions,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2015, pp. 2041\u20132045.\n[12] Kui Wu, Yan Song, Wu Guo, and Lirong Dai, \u201cIntraconversation intra-speaker variability compensation for speaker clustering,\u201d in Proc. Int. Sym. Chinese Spoken Lang. Process., 2012, pp. 330\u2013334.\n[13] Xiao-Lei Zhang, \u201cNonlinear dimensionality reduction of data by multilayer bootstrap networks,\u201d arXiv preprint arXiv:1408.0848, pp. 1\u201320, 2014.\n[14] George E Dahl, Dong Yu, Li Deng, and Alex Acero, \u201cContext-dependent pre-trained deep neural networks for large-vocabulary speech recognition,\u201d IEEE Trans. Audio, Speech, Lang. Process., vol. 20, no. 1, pp. 30\u201342, 2012.\n[15] Yuxuan Wang and DeLiang Wang, \u201cTowards scaling up classification-based speech separation,\u201d IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 7, pp. 1381\u2013 1390, 2013.\n[16] Yong Xu, Jun Du, Li-Rong Dai, and Chin-Hui Lee, \u201cA regression approach to speech enhancement based on deep neural networks,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 23, no. 1, pp. 7\u201319, 2015.\n[17] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, and Paris Smaragdis, \u201cJoint optimization of masks and deep recurrent neural networks for monaural source separation,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 23, no. 12, pp. 2136\u20132147, 2015.\n[18] Xiao-Lei Zhang and DeLiang Wang, \u201cDeep ensemble learning for monaural speech separation,\u201d Tech. Rep. OSU-CISRC-8/15-TR13, Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA, 2015.\n[19] Zhen-Hua Ling, Li Deng, and Dong Yu, \u201cModeling spectral envelopes using restricted boltzmann machines and deep belief networks for statistical parametric speech synthesis,\u201d IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 10, pp. 2129\u20132139, 2013.\n[20] Xiao-Lei Zhang and Ji Wu, \u201cDeep belief networks based voice activity detection,\u201d IEEE Trans. Audio, Speech, Lang. Process., vol. 21, no. 4, pp. 697\u2013710, 2013.\n[21] Xiao-Lei Zhang and DeLiang Wang, \u201cBoosting contextual information for deep neural network based voice activity detection,\u201d Tech. Rep. OSU-CISRC-5/15-TR06, Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA, 2015.\n[22] Martin Cooke and Te-Won Lee, \u201cSpeech separation challenge,\u201d http://staffwww. dcs.shef.ac.uk/people/M.Cooke/ SpeechSeparationChallenge.htm, 2006.\n[23] Alexander Strehl and Joydeep Ghosh, \u201cCluster ensembles\u2014a knowledge reuse framework for combining multiple partitions,\u201d J. Mach. Learn. Res., vol. 3, pp. 583\u2013617, 2003."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We apply multilayer bootstrap network (MBN), a recent pro-<lb>posed unsupervised learning method, to unsupervised speaker<lb>recognition. The proposed method first extracts supervectors<lb>from an unsupervised universal background model, then re-<lb>duces the dimension of the high-dimensional supervectors by<lb>multilayer bootstrap network, and finally conducts unsuper-<lb>vised speaker recognition by clustering the low-dimensional<lb>data. The comparison results with 2 unsupervised and 1 su-<lb>pervised speaker recognition techniques demonstrate the ef-<lb>fectiveness and robustness of the proposed method.", "creator": "LaTeX with hyperref package"}}}