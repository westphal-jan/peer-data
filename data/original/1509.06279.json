{"id": "1509.06279", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2015", "title": "Sports highlights generation based on acoustic events detection: A rugby case study", "abstract": "We approach the challenging problem of generating highlights from sports broadcasts utilizing audio information only. A language-independent, multi-stage classification approach is employed for detection of key acoustic events which then act as a platform for summarization of highlight scenes. Objective results and human experience indicate that our system is highly efficient.", "histories": [["v1", "Fri, 18 Sep 2015 12:47:09 GMT  (536kb)", "http://arxiv.org/abs/1509.06279v1", "IEEE International Conference on Consumer Electronics (IEEE ICCE 2015)"]], "COMMENTS": "IEEE International Conference on Consumer Electronics (IEEE ICCE 2015)", "reviews": [], "SUBJECTS": "cs.SD cs.AI cs.LG", "authors": ["anant baijal", "jaeyoun cho", "woojung lee", "byeong-seob ko"], "accepted": false, "id": "1509.06279"}, "pdf": {"name": "1509.06279.pdf", "metadata": {"source": "CRF", "title": "Sports highlights generation based on acoustic events detection: A rugby case study", "authors": ["Anant Baijal", "Jaeyoun Cho"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION The field of sports highlights detection or sports video summarization is extensively researched. Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10]. Studies have also used textual cues from social media for generating sports highlights [11]. For a task like automatic highlights generation using live TV broadcasts (or stored sports broadcasts), audio cues are a natural choice owing to their light computational needs and their feasibility in confirming to real-time requirements.\nLike sports themselves, the audio contents of each sport have their own unique characteristics: the game of rugby can be characterized by players\u2019 shouts, applause, whistles etc.; a basketball game can have dribbling sounds, sounds of ball hitting the board or sounds of players screeching the wooden court. Crowd noise and commentators\u2019 speech are common sounds found in many sports broadcasts. By analyzing certain audio characteristics, one can automatically generate highlights for sports.\nSome studies in the past have applied their approach or features for detecting highlights in more than one sport [5], but it may not be efficient to do so, given that some of the sounds found in one sport may be absent in another, and furthermore, the same sounds in different kind of sports can have different characteristics.\nNot much research has targeted the extremely popular sport of rugby. Based on our analysis of the audio contents of a number of rugby matches, we design the training mechanism and highlights generator engine, which are presented in Sec. II of this paper. Our approach can be utilized to mine highlight scenes from both live broadcasts and stored sports content.\nAudio based content analysis of rugby offers the following challenges: i) scoring events like \u2018try\u2019 do not produce distinct sounds (unlike ball-hit in baseball or kick in soccer); ii) the broadcast content is corrupted by random noises like music, shouts etc.; iii) the crowd is in \u2018cheer\u2019 mode for most part of the sport; iv) the broadcasts of matches differ in acoustic\nsettings and properties. The focus of our work is twofold: i) acoustic events detection with high recall rate and negligible precision error; ii) generation of highlight scenes using detected events.\nThe organization of the paper is as follows: our system comprising of training stage and highlights generation engine is presented in Section II. The dataset and both objective and subjective performance results are highlighted in Section III. Discussion and conclusions are presented in Section IV."}, {"heading": "II. SYSTEM", "text": ""}, {"heading": "A. Training stage", "text": "The training mechanism has three modules as shown in Fig.1, and explained in the following sub-sub sections:\n1) Data Labeling Based on audio contents of rugby matches, we observe the following acoustic events as representative candidates to generate highlights:\nReferee\u2019s whistle A referee\u2019s whistle is one the most important indicators of\nwhen a scene could be classified as highlight as this acoustic event happens in case of try, severe fouls, conversion kicks and other important events. Unlike many sports such as soccer, basketball or ice hockey, the sound of whistle in rugby broadcasts is very prominent (see Fig.2) and clearly audible in the audio stream of the broadcast. The audio contents of rugby matches have a whistle sound that is distinctive- it has a shrill tone and a pitch high enough for the whistle to be heard despite high crowd noise or commentators\u2019 speech.\nCommentators\u2019 excited speech We observe that the commentators\u2019 speech turns excited during a highlight scene while in other scenes it is unexcited and hence there is a strong correlation between an exciting scene and a commentators\u2019 speech. It is to be noted that the commentators\u2019 speech is corrupted with other sounds during the match including crowd noise.\nCrowd Noise A number of previously cited studies have used crowd noise as a cue for various sports. In rugby, however, we observe that the crowd, for most part, is in a high cheering - highly excitedhighly ebullient state, and so is the crowd noise. Acoustically, there is not much variation in the crowd noise in presence or absence of a highlight scene, and thus although a representative candidate, this category of sound is not used as a deciding event for highlights generation.\nThus only commentators\u2019 excited speech and referee\u2019s whistle sounds are considered as deciding events for audio based highlights in rugby. Additionally, we also label events like unexcited speech and music.\n2) Pre-processing and feature extraction\nThe input audio signal is divided into frames and converted into spectral domain. We then employ Mel Frequency Cepstral Coefficients (MFCC) [12] and their first order differential coefficients, commonly known as delta-MFCC, to extract requisite features.\nThe combination of MFCC and delta-MFCC works out very well for representing both the whistle sounds and the change in excitement level of speech: while MFCCs can capture the power spectral envelope of a signal and have proven to be very efficient for speech analysis, delta-MFCCs capture the dynamics, i.e., the variation of MFCC coefficients with time.\nTo tackle the problem of amplitude variation, we ignore the zeroth order coefficient of MFCC vector. In order to increase the detection rate of acoustic events, the MFCC and deltaMFCC features are extracted from the incoming audio stream but at different frame intervals. The feature vector used is 60- dimensional in total.\nSome spectrograms of typical audio events taken from actual rugby broadcasts are shown in Fig.2. It can be seen that the spectral contents of whistle, excited speech, and overlap of the two, clearly differ from each other.\n3) Learning\nWe make use of Gaussian Mixture Models (GMM) to learn the extracted features. GMMs are composed of probabilistic models, where each model has a mean and a co-variance matrix associated with it.\n( )\u2211 \u2211 =\n= M\ni iii cgwcp 1 ,|.)|( \u03bc\u03bb (1)\nHere, )|( \u03bbcp represents a mixture model, c represents the feature vector,\niw ; Mi ......3,2,1= denotes the mixture weights \u2211 ),|( iicg \u03bc represents the conditional Gaussian densities\nwith mean i\u03bc and covariance matrix \u2211 i ."}, {"heading": "B. Highlights Generator Engine", "text": "Based on our observations and analysis presented in the above sub-section, we present an algorithm that uses computationally inexpensive features combined with a simple yet efficient approach, to generate highlight scenes from rugby broadcasts. The algorithm can be seen in Fig. 3 and individual modules are explained in the following sub- subsections.\n1) Pre-processing and feature extraction The features are extracted in the same manner as explained earlier. It is to be noted that high noise content (crowd or music or other sounds) is present in the background in addition to the commentators\u2019 speech and referees\u2019 whistle sounds.\n2) Multi-stage classification It can be observed from Fig.2 that there is an overlap of other sounds (unexcited speech, crowd etc.) with key acoustic sounds, hence it becomes challenging for the classifier to categorize the events correctly as the learning models have only a limited discerning ability in case of a single-stage classification. For e.g., a segment having dominating \u2018crowd noise\u2019 overlapping with \u2018excited speech\u2019 may be classified under \u2018crowd\u2019 category, implying that we missed detecting a key event (excited speech). Multi-stage GMM classification merits in providing preferential classifying power (ability to choose the preferred audio event category in case of an overlap), thereby directly increasing the recall rate of key acoustic events.\nIn our approach, the trained GMM models first classify the incoming audio frames as either \u2018speech\u2019 or \u2018non-speech\u2019 acoustic events as the feature vector used is extremely efficient in detecting speech segments. This stage results in a high detection rate for \u2018excited speech\u2019 segments even in\npresence of overlapping non-speech sounds. The audio segments classified as \u2018speech\u2019 in the first stage are then passed through a second stage of classification after which they are classified by the GMM as \u2018excited speech\u2019 or \u2018unexcited speech\u2019 while the non-speech segments are categorized as either \u2018whistle\u2019 or \u2018others\u2019. We thus train five GMM models- a speech model, an excited speech model, an unexcited speech model, a whistle model and a model to classify all other acoustic events. The occurrences of \u2018excited speech\u2019 and \u2018whistle\u2019 are continuously stored in a buffer along with their timestamp information.\n1) Pre-processing and feature extraction 2) Multi-stage classification 3) Post-processing 3) Post-processing\nA decision window of a certain duration is glided over the classified frames that are continuously buffered. Now if the percentage of events of interest (\u2018whistle\u2019 and \u2018excited speech\u2019) in the window exceeds a set threshold, the starting point of that decision window (the timestamp of the first frame in the window) becomes the onset of the highlight scene. To determine the endpoint of the highlight scene, the decision window\u2019s starting (and thus, the ending point) are shifted by one until the shifted window also meets the set percentage threshold, and when it does not, the endpoint of the highlight is fixed as the endpoint of the previous window (timestamp of\nthe last frame in the previous window). This process is then repeated, the starting point being the immediate next frame to the endpoint of the highlight scene. The reasons for not generating highlight as and when each window meets the threshold condition are twofold: i) the presence of commentators\u2019 excited speech and whistle is strongly correlated with a scene being a highlight but these acoustic events may occur anywhere during the scene, even towards the ending of the scene; ii) this would imply that each highlight is of the same duration (which is not the case in actual sports)."}, {"heading": "III. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Dataset", "text": "We use actual broadcasts of a wide variety of rugby matches. The audio stream from rugby matches is converted to 16 kHz, 16-bit, mono-channel format. The training data are manually annotated for 11 different games to determine the ground truth for various audio event categories. For evaluation purposes, our test dataset consists of 5 games totaling 10 hours and 30 minutes in duration."}, {"heading": "B. Objective evaluation", "text": "We first calculate the recall and precision rates for detecting key acoustic events in both the classification stages (see Fig.3) as the generation of highlights is directly dependent on them. For classification stage 1, we pick 2808 \u2018speech\u2019 segments, while for classification stage 2, we pick 217 \u2018excited speech\u2019 segments and 112 \u2018whistle\u2019 segments (other than training data) and test our approach; the segments are of different duration. The results in Table I show that our approach yields extremely high recall-precision rates in both the stages.\nNext, we evaluate the overall performance of our approach for detecting highlights. The objective results in Table II indicate that our approach to detecting highlights in rugby yields promising results, with an overall try recall rate of 97.06% with a 93.41 % precision for highlight scenes. A consumer can skip through any unexciting or uninteresting scenes but he or she would be dissatisfied if the rugby highlights were to miss any try events, hence we evaluate the recall rate for try events while also checking on the precision of highlight scenes for overall viewing experience. It is worth emphasizing that \u2018scoring events\u2019 are not the only highlights of a match; consumers may also want to see other non-scoring\nevents which amount to human excitability. The duration of each highlight scene varies from about 10 to 20 seconds."}, {"heading": "C. User experience", "text": "To gauge the user experience, we asked 11 subjects (9 males, 2 females) to evaluate our highlights generator engine by giving a Mean Opinion Score (MOS), considering the following: i) the overall highlights viewing experience is enjoyable, entertaining and pleasant, and not marred by unexciting scenes; ii) the generated scenes do not begin or end abruptly; iii) the scenes are acoustically and/ or visually\nexciting. Each user watched at least ten scenes per match, picked at random. The overall MOS from Table III is calculated as 4.23 based on which we can state that the user experience is extremely positive."}, {"heading": "IV. DISCUSSION AND CONCLUSIONS", "text": "The precision error for \u2018whistle\u2019 category (see Table I) is a result of overlapping segments in the test data. Our experiments show that these segments under the whistle event category were sometimes classified as speech after the first classification. But using multi-stage GMM classification approach proved to be advantageous as all those segments were then recovered as \u2018excited speech\u2019. The overlapping segments were rightly classified as such: a commentators\u2019 voice is in an excited state when a whistle is sounded. Further, we find that our engine failed to detect one \u2018try\u2019 in Match 4 (see Table II), and our analysis showed us that the excited speech was completely overshadowed by crowd sound for the most part of the decision window.\nWe also note that few studies in the literature have reported on actually generating sports highlights once key acoustic events are detected; our highlight generator engine effectively mines the exciting scenes. We add margins of a few seconds\non either side of the generated highlight scenes to avoid abrupt beginning and ending of the highlight scenes, and doing so results in a positive experience for the users. Often live broadcasts are geographically targeted and hence the language of commentary may differ. Our language independent approach (using only excited speech and whistle sounds) easily addresses this concern.\nIn conclusion, we address the problem of generating highlights from rugby broadcasts using acoustic event detection. Our uni-modal approach makes use of a multi-stage classifier to detect key acoustic events which are fed to the highlight generator engine. The onset point and end-point selecting mechanism helps in setting the event boundaries and provides for smoothness and completeness of highlight viewing experience for the consumers. Given the simplicity and effectiveness of our highlights generator engine, it can be embedded in consumer electronics, and can be used for generating highlights in online settings (live TV broadcasts) as well as in offline scenarios (stored sports multimedia)."}], "references": [{"title": "Hierarchical language modeling for audio events detection in a sports game", "author": ["Qiang Huang", "Cox, S."], "venue": "IEEE ICASSP, pp.2286-2289, 2010", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatically extracting highlights for TV baseball programs", "author": ["Y. Rui", "A. Gupta", "A. Acero"], "venue": "Proc. ACM Multimedia, pp.105-115, 2000", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Audio based soccer game summarization", "author": ["H. Duxans", "X. Anguera", "D. Conejero"], "venue": "IEEE International Symposium on Broadband Multimedia Systems and Broadcasting, pp.1-6, 2009", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "A highlight scene detection and video summarization system using audio feature for a personal video recorder", "author": ["I. Otsuka", "K. Nakane", "A. Divakaran", "K. Hatanaka", "M. Ogawa"], "venue": "IEEE Trans. on Consumer Electronics , vol.51 (1), pp.112-116, 2005", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Audio events detection based highlights extraction from baseball, golf and soccer games in a unified framework", "author": ["Ziyou Xiong", "R. Radhakrishnan", "A. Divakaran", "T.S. Huang"], "venue": "IEEE ICASSP, pp.V-632-5, 2003", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "An Autonomous Framework to Produce and Distribute Personalized Team-Sport Video Summaries: A Basketball Case Study", "author": ["F. Chen", "D. Delannay", "C. De Vleeschouwer"], "venue": "IEEE Trans. on Multimedia, , vol.13 (6), pp.1381-1394, 2011", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-modal highlight generation for sports videos using an information-theoretic excitability measure", "author": ["T. Hasan", "H. Bo\u0159il", "A. Sangwan", "J.H. Hansen"], "venue": "EURASIP Journal on Advances in Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Event detection in field sports video using audio-visual features and a support vector Machine", "author": ["D.A. Sadlier", "N.E. O'Connor"], "venue": "IEEE Trans. on Circuits and Systems for Video Technology, vol.15 (10) pp.1225- 1233, 2005", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Integrating highlights for more complete sports video summarization", "author": ["D. Tjondronegoro", "Y.P.P. Chen", "B. Pham"], "venue": "Proc. of the IEEE MultiMedia,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Video abstraction: A systematic review and classification", "author": ["B.T. Truong", "S. Venkatesh"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "User generated highlight system for baseball games with social media activities", "author": ["Dongmahn Seo", "Suhyun Kim", "Hogun Park", "Heedong Ko"], "venue": "IEEE International Conference on Consumer Electronics (ICCE), pp.349-350, 2014", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences.", "author": ["S. Davis", "P. Mermelstein"], "venue": "IEEE Trans. on Acoustics, Speech, and Signal Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1980}], "referenceMentions": [{"referenceID": 0, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 1, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 2, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 3, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 4, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 35, "endOffset": 40}, {"referenceID": 5, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 88, "endOffset": 93}, {"referenceID": 7, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 88, "endOffset": 93}, {"referenceID": 8, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "Prior studies have used audio cues [1-5], visual cues [6], mixture of audio-visual cues [7-8], audio-textual cues [9], and a variety of features and classifiers to generate highlights [10].", "startOffset": 184, "endOffset": 188}, {"referenceID": 10, "context": "Studies have also used textual cues from social media for generating sports highlights [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Some studies in the past have applied their approach or features for detecting highlights in more than one sport [5], but it may not be efficient to do so, given that some of the sounds found in one sport may be absent in another, and furthermore, the same sounds in different kind of sports can have different characteristics.", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "We then employ Mel Frequency Cepstral Coefficients (MFCC) [12] and their first order differential coefficients, commonly known as delta-MFCC, to extract requisite features.", "startOffset": 58, "endOffset": 62}], "year": 2015, "abstractText": "We approach the challenging problem of generating highlights from sports broadcasts utilizing audio information only. A language-independent, multi-stage classification approach is employed for detection of key acoustic events which then act as a platform for summarization of highlight scenes. Objective results and human experience indicate that our system is highly efficient.", "creator": "PDFMerge! (http://www.pdfmerge.com)"}}}