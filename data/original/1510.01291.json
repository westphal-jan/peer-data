{"id": "1510.01291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2015", "title": "A Common-Factor Approach for Multivariate Data Cleaning with an Application to Mars Phoenix Mission Data", "abstract": "Data quality is fundamentally important to ensure the reliability of data for stakeholders to make decisions. In real world applications, such as scientific exploration of extreme environments, it is unrealistic to require raw data collected to be perfect. As data miners, when it is infeasible to physically know the why and the how in order to clean up the data, we propose to seek the intrinsic structure of the signal to identify the common factors of multivariate data. Using our new data driven learning method, the common-factor data cleaning approach, we address an interdisciplinary challenge on multivariate data cleaning when complex external impacts appear to interfere with multiple data measurements. Existing data analyses typically process one signal measurement at a time without considering the associations among all signals. We analyze all signal measurements simultaneously to find the hidden common factors that drive all measurements to vary together, but not as a result of the true data measurements. We use common factors to reduce the variations in the data without changing the base mean level of the data to avoid altering the physical meaning.", "histories": [["v1", "Mon, 5 Oct 2015 19:21:22 GMT  (1103kb)", "http://arxiv.org/abs/1510.01291v1", "12 pages, 10 figures, 1 table"], ["v2", "Wed, 7 Oct 2015 16:47:30 GMT  (1599kb)", "http://arxiv.org/abs/1510.01291v2", "12 pages, 10 figures, 1 table"]], "COMMENTS": "12 pages, 10 figures, 1 table", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["dongping fang", "elizabeth oberlin", "wei ding", "samuel p kounaves"], "accepted": false, "id": "1510.01291"}, "pdf": {"name": "1510.01291.pdf", "metadata": {"source": "CRF", "title": "A Common-Factor Approach for Multivariate Data Cleaning with an Application to Mars Phoenix Mission Data", "authors": ["Dongping Fang", "Elizabeth Oberlin", "Wei Ding", "Samuel P. Kounaves"], "emails": ["dongping.fang@zurichna.com", "Elizabeth.Oberlin@tufts.edu", "wei.ding@umb.edu", "Samuel.Kounave@tufts.edu"], "sections": [{"heading": null, "text": "the reliability of data for stakeholders to make decisions. In real world applications, such as scientific exploration of extreme environments, it is unrealistic to require raw data collected to be perfect. As data miners, when it is infeasible to physically know the why and the how in order to clean up the data, we propose to seek the intrinsic structure of the signal to identify the common factors of multivariate data. Using our new data-driven learning method\u2014the common-factor data cleaning approach, we address an interdisciplinary challenge on multivariate data cleaning when complex external impacts appear to interfere with multiple data measurements. Existing data analyses typically process one signal measurement at a time without considering the associations among all signals. We analyze all signal measurements simultaneously to find the hidden common factors that drive all measurements to vary together, but not as a result of the true data measurements. We use common factors to reduce the variations in the data without changing the base mean level of the data to avoid altering the physical meaning.\nWe have reanalyzed the NASA Mars Phoenix mission data used in the leading effort by Kounaves\u2019s team (lead scientist for the wet chemistry experiment on the Phoenix) [1, 2] with our proposed method to\nshow the resulting differences. We demonstrate that this new common-factor method successfully helps reducing systematic noises without definitive understanding of the source and without degrading the physical meaning of the signal.\nKeywords Data Cleaning, Factor Analysis, Statistical Learning"}, {"heading": "1. INTRODUCTION", "text": "Data quality is fundamentally important for domain scientists. In real world applications, such as scientific exploration of extreme environments, it is unrealistic to require data collection to be perfect. Our motivating application is to recover the true chemical analysis data from the Wet Chemistry Laboratory (WCL) on the 2008 Phoenix Mars Lander (Fig. 1 & [1]). The WCL collected over three-million data points and performed the first comprehensive wet chemical analysis of the soil on Mars. The initial data has provided new scientific insights into the history of Mars, its potential for supporting microbial life, and even its atmospheric chemistry, with resulting publications in Science [4]. Six years later less than 1% of the WCL data has been manually studied [2]. The main reason for this is the noise introduced by unexpected instrumental and environmental factors on Mars make data interpretation an ill-posed problem if data cleaning solely relies on the chemical and physical models understood on Earth.\nAs data miners, when it is infeasible to physically know the why and the how in order to clean up the data, we propose to seek the intrinsic structure of the signal to identify the common factors of multivariate data. The existing state-of-the-art denoising methods, including Fourier filtering [8], Kalman smoothing [9] [12], Gaussian smoothing [10], and Hidden Markov Model denoising [11], do not work well for this type of problem because they are designed to cope with a\nsingle data measurement, ignoring the associated shared behavior among multiple datasets. In [13-16], the principles of identifying common-mode regularities were discussed to identify a simplifying structure, shared trends in financial data and covariance selection in biological data, but not for the data cleaning using common external factors which is a quite different problem.\nWe study the intrinsic characteristics of the data and propose a new common-factor removal method that utilizes multiple sensor measurements simultaneously to find the hidden shared factors which drive all measurements to vary simultaneously. These common factors represent the errors and variations caused by the combined and complicated influence of common varying external factors. We iteratively estimate the common factors by minimizing the sum of squared errors of all the sensor data. We then clean the data by removing the effects of these common factors (details in Section 2). We compare our proposed method with state-of-the-art data denoising methods (details in Section 3). We reanalyze the WCL data used in the leading effort by Kounaves et al. [2] with our proposed method to show the data quality improvement (details in Section 4).\nThe contribution of this paper is that we address an interdisciplinary challenge to provide a new and physically meaningful data cleaning method to improve data quality in scientific data. The proposed common-factor data cleaning approach is designed for a scenario when multiple data measurements are impacted together by an unknown and hard-toreproduce real-world environment. In the Martian data analysis, we demonstrate that this new common-factor method can help reduce systematic noise without definitive understanding of the source and without degrading the physical meaning of the signal. Our results successfully lead to a more accurate measurement of the soil chemistry on. Though the idea of common factors is used in many fields [5, 13 \u2013 16], to the best of our knowledge, we are the first research team to use the idea of common hidden factors on data cleaning."}, {"heading": "2. METHODOLOGY", "text": "Let us assume that there are I signal sensors to collect data simultaneously. In the WCL data, these are I Ion Selective Electrode (ISE) sensors in the same beaker measuring various ions of interest. Let \ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61\n(\ud835\udc56\ud835\udc56) denote the measured value for signal \ud835\udc56\ud835\udc56 \u2208 {1, \u2026 , \ud835\udc3c\ud835\udc3c} at time \ud835\udc61\ud835\udc61 \u2208 {\ud835\udc61\ud835\udc611, \u2026 , \ud835\udc61\ud835\udc61\ud835\udc5b\ud835\udc5b}. So the observed data are \ufffd\ud835\udc61\ud835\udc61,\ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61 (1), \u2026 ,\ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61\n(\ud835\udc3c\ud835\udc3c)\ufffd \ud835\udc61\ud835\udc61=\ud835\udc61\ud835\udc611\n\ud835\udc61\ud835\udc61\ud835\udc5b\ud835\udc5b . In an ideal world of data\ncollection, each signal data measurements over time\nshould be a constant plus a random measurement error, i.e.\n\ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61 (\ud835\udc56\ud835\udc56) = \ud835\udf07\ud835\udf07(\ud835\udc56\ud835\udc56) + \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61 (\ud835\udc56\ud835\udc56), (1)\nwhere \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61 (\ud835\udc56\ud835\udc56) denotes the measurement error for signal i at time t, \ud835\udf07\ud835\udf07(\ud835\udc56\ud835\udc56) denotes the constant representing real potential for signal i. The \ud835\udf07\ud835\udf07(\ud835\udc56\ud835\udc56) does not change with time, and \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61\n(\ud835\udc56\ud835\udc56) is a random white noise so its value at different times or for different signals are independent, it follows\ncorr \ufffd\ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc611 (\ud835\udc56\ud835\udc561), \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc612 (\ud835\udc56\ud835\udc562)\ufffd = 0 for \ud835\udc56\ud835\udc561 \u2260 \ud835\udc56\ud835\udc562 or \ud835\udc61\ud835\udc611 \u2260 \ud835\udc61\ud835\udc612\n\ud835\udc49\ud835\udc49\ud835\udc49\ud835\udc49\ud835\udc49\ud835\udc49 \ufffd\ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61 (\ud835\udc56\ud835\udc56)\ufffd = \ud835\udf0e\ud835\udf0e\ud835\udc56\ud835\udc562 > 0\n(2)\nIf this is the case, the estimated measurement and associated error would simply be data mean \ud835\u0302\udf07\ud835\udf07(\ud835\udc56\ud835\udc56) = \ud835\udc38\ud835\udc38(\ud835\udea4\ud835\udea4)\ufffd\ufffd\ufffd\ufffd\ufffd = 1\n\ud835\udc5b\ud835\udc5b \u2211 \ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61\ud835\udc58\ud835\udc58\n(\ud835\udc56\ud835\udc56)\ud835\udc5b\ud835\udc5b \ud835\udc58\ud835\udc58=1 and standard deviation of mean\n\ud835\udf0e\ud835\udf0e\ufffd\ud835\udc56\ud835\udc56\ufffd 1 \ud835\udc5b\ud835\udc5b where \ud835\udf0e\ud835\udf0e\ufffd\ud835\udc56\ud835\udc562 = 1 \ud835\udc5b\ud835\udc5b\u22121 \u2211 (\ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61\ud835\udc58\ud835\udc58 (\ud835\udc56\ud835\udc56) \u2212 \ud835\udc38\ud835\udc38(\ud835\udea4\ud835\udea4)\ufffd\ufffd\ufffd\ufffd\ufffd)2\ud835\udc5b\ud835\udc5b\ud835\udc58\ud835\udc58=1 .\nBut in reality when complex external impacts appear to interfere with multiple data measurement, we will observe deviations from the ideal case. For example, as illustrated in Fig.2 of Martian soil data, different signal measurements were correlated, exhibiting systematic fluctuations.\nOur data-cleaning goal, formally speaking, is to remove the deviations to regain the forms of Eqs. (1) and (2). Formulation and Algorithm. Let K denotes the number of common factors, \ud835\udc6d\ud835\udc6d\ud835\udc8c\ud835\udc8c\ud835\udc8c\ud835\udc8c the kth common factor at time t. The observed data can be modeled as\n\ud835\udc6c\ud835\udc6c\ud835\udc8c\ud835\udc8c (\ud835\udc8a\ud835\udc8a) = \ud835\udf41\ud835\udf41(\ud835\udc8a\ud835\udc8a) + \ud835\udf37\ud835\udf37\ud835\udfcf\ud835\udfcf (\ud835\udc8a\ud835\udc8a)\ud835\udc6d\ud835\udc6d\ud835\udfcf\ud835\udfcf\ud835\udc8c\ud835\udc8c + \u22ef+ \ud835\udf37\ud835\udf37\ud835\udc72\ud835\udc72 (\ud835\udc8a\ud835\udc8a)\ud835\udc6d\ud835\udc6d\ud835\udc72\ud835\udc72\ud835\udc8c\ud835\udc8c + \ud835\udf3a\ud835\udf3a\ud835\udc8c\ud835\udc8c (\ud835\udc8a\ud835\udc8a) (3)\nwhere \ud835\udefd\ud835\udefd1 (\ud835\udc56\ud835\udc56), \u2026 ,\ud835\udefd\ud835\udefd\ud835\udc3e\ud835\udc3e (\ud835\udc56\ud835\udc56) are the coefficients of the K common factors for signal i, and \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61\n(\ud835\udc56\ud835\udc56) are random noise as in Eq. (2). Notice that the common factors are the same for all multivariate data, but their influences on each signal may be different due to its different physical properties which is reflected in the coefficients \ud835\udefd\ud835\udefd(\ud835\udc56\ud835\udc56)\ud835\udc60\ud835\udc60 for that signal. We want to use common factors to help us reduce the variations in the data without changing the base mean level of the data. So we require the base mean of factors to be zero.\nThe cleaned data to be calculated are\n\ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61 \u2217(\ud835\udc56\ud835\udc56) = \ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61 (\ud835\udc56\ud835\udc56) \u2212 \ud835\udefd\ud835\udefd1 (\ud835\udc56\ud835\udc56)\ud835\udc39\ud835\udc391\ud835\udc61\ud835\udc61 \u2212 \u22ef\u2212 \ud835\udefd\ud835\udefd\ud835\udc3e\ud835\udc3e (\ud835\udc56\ud835\udc56)\ud835\udc39\ud835\udc39\ud835\udc3e\ud835\udc3e\ud835\udc61\ud835\udc61 = \ud835\udf07\ud835\udf07(\ud835\udc56\ud835\udc56) + \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61\n(\ud835\udc56\ud835\udc56) (4) Parameter Estimation. Let observed data matrix of ISE sensor measurements be\n\ud835\udd3c\ud835\udd3c\ud835\udc5b\ud835\udc5b\u00d7\ud835\udc3c\ud835\udc3c = \ufffd \ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc611\n(1) \u2026 \ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc611 (\ud835\udc3c\ud835\udc3c)\n\u22ee \u22f1 \u22ee \ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61\ud835\udc5b\ud835\udc5b (1) \u2026 \ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61\ud835\udc5b\ud835\udc5b (\ud835\udc3c\ud835\udc3c) \ufffd = \ufffd\n\ud835\udc6c\ud835\udc6c\u2032\ud835\udc61\ud835\udc611 \u22ee\n\ud835\udc6c\ud835\udc6c\u2032\ud835\udc61\ud835\udc61\ud835\udc5b\ud835\udc5b \ufffd =\n\ufffd\ud835\udc6c\ud835\udc6c(1), \u2026 ,\ud835\udc6c\ud835\udc6c(\ud835\udc3c\ud835\udc3c)\ufffd,\nwhere \ud835\udc6c\ud835\udc6c\u2032\ud835\udc61\ud835\udc61\ud835\udc59\ud835\udc59 , \ud835\udc6c\ud835\udc6c (\ud835\udc56\ud835\udc56) are the lth row and ith column vectors of \ud835\udd3c\ud835\udd3c. The model parameters are\n\ud835\udd39\ud835\udd39\ud835\udc3c\ud835\udc3c\u00d7\ud835\udc3e\ud835\udc3e = \ufffd \ud835\udefd\ud835\udefd1\n(1) \u2026 \ud835\udefd\ud835\udefd\ud835\udc3e\ud835\udc3e (1)\n\u22ee \u22f1 \u22ee \ud835\udefd\ud835\udefd1 (\ud835\udc3c\ud835\udc3c) \u2026 \ud835\udefd\ud835\udefd\ud835\udc3e\ud835\udc3e (\ud835\udc3c\ud835\udc3c) \ufffd = (\ud835\udf37\ud835\udf371, \u2026 ,\ud835\udf37\ud835\udf37\ud835\udc3e\ud835\udc3e),\n\ud835\udd3d\ud835\udd3d\ud835\udc5b\ud835\udc5b\u00d7\ud835\udc3e\ud835\udc3e = \ufffd \ud835\udc39\ud835\udc391\ud835\udc61\ud835\udc611 \u22ef \ud835\udc39\ud835\udc39\ud835\udc3e\ud835\udc3e\ud835\udc61\ud835\udc611 \u22ee \u22f1 \u22ee\n\ud835\udc39\ud835\udc391\ud835\udc61\ud835\udc61\ud835\udc5b\ud835\udc5b \u22ef \ud835\udc39\ud835\udc39\ud835\udc3e\ud835\udc3e\ud835\udc61\ud835\udc61\ud835\udc5b\ud835\udc5b \ufffd = (\ud835\udc6d\ud835\udc6d1, \u2026 ,\ud835\udc6d\ud835\udc6d\ud835\udc3e\ud835\udc3e),\n\ud835\udf41\ud835\udf41 = (\ud835\udf07\ud835\udf07(1), \u2026 , \ud835\udf07\ud835\udf07(\ud835\udc3c\ud835\udc3c))\u2032, \u03a3 = diag(\ud835\udf0e\ud835\udf0e12, \u2026 ,\ud835\udf0e\ud835\udf0e\ud835\udc3c\ud835\udc3c2), where \ud835\udf37\ud835\udf37\ud835\udc58\ud835\udc58 and \ud835\udc6d\ud835\udc6d\ud835\udc58\ud835\udc58 are the kth column vector of coefficient matrix \ud835\udd39\ud835\udd39 and factor matrix \ud835\udd3d\ud835\udd3d respectively and \u03a3 is the diagonal variance matrix.\nGiven observed data \ud835\udd3c\ud835\udd3c, our goal here is estimate \ud835\udd39\ud835\udd39, \ud835\udd3d\ud835\udd3d, \ud835\udf41\ud835\udf41 and \u03a3 by minimizing sum of squared errors in Eq. (5)\n\ud835\udc46\ud835\udc46 = \u2211 \u2211 \ufffd\ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61 (\ud835\udc56\ud835\udc56) \u2212 \ud835\udc38\ud835\udc38\ufffd\ud835\udc61\ud835\udc61\n(\ud835\udc56\ud835\udc56)\ufffd 2\n\ud835\udc61\ud835\udc61 \ud835\udc3c\ud835\udc3c \ud835\udc56\ud835\udc56=1 = \u2211 \ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61 = \u2211 \ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56)\ud835\udc3c\ud835\udc3c\ud835\udc56\ud835\udc56=1\n(5)\nwhere \ud835\udc38\ud835\udc38\ufffd\ud835\udc61\ud835\udc61 (\ud835\udc56\ud835\udc56) = \ud835\udf07\ud835\udf07(\ud835\udc56\ud835\udc56) + \ud835\udefd\ud835\udefd1 (\ud835\udc56\ud835\udc56)\ud835\udc39\ud835\udc391\ud835\udc61\ud835\udc61 + \u22ef+ \ud835\udefd\ud835\udefd\ud835\udc3e\ud835\udc3e (\ud835\udc56\ud835\udc56)\ud835\udc39\ud835\udc39\ud835\udc3e\ud835\udc3e\ud835\udc61\ud835\udc61, and\n\ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61 = \u2211 \ufffd\ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61 (\ud835\udc56\ud835\udc56) \u2212 \ud835\udc38\ud835\udc38\ufffd\ud835\udc61\ud835\udc61\n(\ud835\udc56\ud835\udc56)\ufffd 2\ud835\udc3c\ud835\udc3c\n\ud835\udc56\ud835\udc56=1 (6)\n\ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56) = \u2211 \ufffd\ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61 (\ud835\udc56\ud835\udc56) \u2212 \ud835\udc38\ud835\udc38\ufffd\ud835\udc61\ud835\udc61\n(\ud835\udc56\ud835\udc56)\ufffd 2\n\ud835\udc61\ud835\udc61 (7)\nStarting from an initial value of \ud835\udf3d\ud835\udf3d = (\ud835\udd39\ud835\udd39,\ud835\udf41\ud835\udf41, \u03a3), we will do the minimization by alternately performing estimate factors \ud835\udd3d\ud835\udd3d given model parameters \ud835\udf3d\ud835\udf3d, and estimate \ud835\udf3d\ud835\udf3d given \ud835\udd3d\ud835\udd3d. Algorithm 1 describes the common factor approach.\nAlgorithm 1 Common-factor Learning with Least Square Regression\nStep I. Initialization. Apply statistical factor analysis to get initial estimates for \ud835\udd39\ud835\udd39, \ud835\udf41\ud835\udf41, and diagonal variance matrix \u03a3.\nStep II. Iteration: repeat 1 and 2 until converge. 1. Estimate \ud835\udd3d\ud835\udd3d for a given \ud835\udf3d\ud835\udf3d = (\ud835\udd39\ud835\udd39,\ud835\udf41\ud835\udf41, \u03a3).\na) For each t, perform weighted least square regression of (\ud835\udc6c\ud835\udc6c\ud835\udc61\ud835\udc61 \u2212 \ud835\udf41\ud835\udf41) on \ud835\udf37\ud835\udf371, \u2026 ,\ud835\udf37\ud835\udf37\ud835\udc3e\ud835\udc3e to get new estimates of common factors. This gives \ud835\udd3d\ud835\udd3d\u2032 = (\ud835\udd39\ud835\udd39\u2032\u03a3\u22121\ud835\udd39\ud835\udd39)\u22121\ud835\udd39\ud835\udd39\u2032\u03a3\u22121(\ud835\udd3c\ud835\udd3c \u2212 \ud835\udd4c\ud835\udd4c)\u2032,\nwhere \ud835\udd4c\ud835\udd4c = \ufffd \ud835\udf07\ud835\udf07(1) \u22ef \ud835\udf07\ud835\udf07(\ud835\udc3c\ud835\udc3c) \u22ee \u22f1 \u22ee\n\ud835\udf07\ud835\udf07(1) \u22ef \ud835\udf07\ud835\udf07(\ud835\udc3c\ud835\udc3c) \ufffd.\nb) Set trimmed mean of each factor to zero. For each factor, let\n\ud835\udc6d\ud835\udc6d\ud835\udc58\ud835\udc58 \u2190 \ud835\udc6d\ud835\udc6d\ud835\udc58\ud835\udc58 \u2212\ud835\udc5a\ud835\udc5a\ud835\udc58\ud835\udc58, where \ud835\udc5a\ud835\udc5a\ud835\udc58\ud835\udc58 is the trimmed mean of kth factor calculated by \ud835\udc5a\ud835\udc5a\ud835\udc58\ud835\udc58 = mean{\ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61\ud835\udc58\ud835\udc58: |\ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61\ud835\udc58\ud835\udc58 \u2212 mean(\ud835\udc6d\ud835\udc6d\ud835\udc58\ud835\udc58)| \u2264 \ud835\udc50\ud835\udc50 \u2219 \ud835\udf0e\ud835\udf0e(\ud835\udc6d\ud835\udc6d\ud835\udc58\ud835\udc58)} Therein \ud835\udf0e\ud835\udf0e(\ud835\udc6d\ud835\udc6d\ud835\udc58\ud835\udc58) is the standard deviation of \ud835\udc6d\ud835\udc6d\ud835\udc58\ud835\udc58, and c is a constant.\n2. Estimate \ud835\udf3d\ud835\udf3d for given \ud835\udd3d\ud835\udd3d. For given \ud835\udd3d\ud835\udd3d, fit the model in Eq. (3) for each ion i by least square linear regression, i.e. regress \ud835\udc6c\ud835\udc6c(\ud835\udc56\ud835\udc56) on \ud835\udc6d\ud835\udc6d1, \u2026 ,\ud835\udc6d\ud835\udc6d\ud835\udc3e\ud835\udc3e. This step gives the new estimates for \ud835\udf3d\ud835\udf3d = (\ud835\udd39\ud835\udd39,\ud835\udf41\ud835\udf41, \u03a3).\nStep I initializes the algorithm by the statistical factor analysis [5]. It produces reasonably good initial values for \ud835\udd39\ud835\udd39 before our search starts, but it doesn\u2019t minimize the sum of squared errors for the ISE signals and thus cannot fulfill our goal. We need step II to iteratively perform the minimization.\nStep II.1 uses estimated coefficients as known to estimate factor scores by minimizing \ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61 in Eq. (6) for each t, which results in the ordinary least square regression. Since each signal may have different variance we modify this step using weighted least square regression instead of the ordinary least square regression. Step II.2 uses estimated factors as knowns to get a new estimate of coefficients by minimizing \ud835\udc46\ud835\udc46(\ud835\udc56\ud835\udc56)\nin Eq. (7) for each signal i. We alternately use Step II.1 and Step II.2 until the sum of squared error stops decreasing.\nStep II.1.b) makes sure the trimmed mean of each factor is zero to serve the goal of cleaning up the variation part but not the base mean level part of the data. Without a good reason, the data base mean level should not be altered by any data cleaning method because it would change the physical meaning of the data. Common factors are capable of finding spikes (see factor plots in Fig. 7). We set the trimmed mean, instead of the regular mean, to be zero to reduce the influence of large outliers (spikes) on the base mean level of factors and in turn on the base mean of the data. In our calculation of the trimmed mean we suggest to statistically choose c = 2.326 which corresponds to 99%-percentile of standard normal distribution.\nDetermination of Number of Common Factors. When the number of factors increases, the total sum squared error in Eq. (5) will decrease. In the extreme case, the errors would decrease to zero if the number of factors is greater than or equal to the number of variables in the observed data. Our purpose is to use common factors to clean the data by removing influences that are believed common to variations of all ISE measurements, not the random intrinsic measurement error, \ud835\udf3a\ud835\udf3a\ud835\udc8c\ud835\udc8c\n(\ud835\udc8a\ud835\udc8a) in Eq. (1), which every device independently has. The key word here is common. So if adding a factor only decreases error of a single ISE measurement, this is not considered a common factor and is not used in the method. Our strategy is to try a range of number of factors starting from 0 factors, and stop when no significant multiple error decreases are observed. For example, the search will stop if the decrease of \ud835\udf48\ud835\udf48\ud835\udc8a\ud835\udc8a\ud835\udfd0\ud835\udfd0 is bigger than some critical value only in one ion."}, {"heading": "3. COMPARATIVE STUDIES USING SYNTHETIC DATA", "text": "Our goal of multivariate data cleaning is to identify the common variation part in the multiple series and regain the original data distribution in the forms of Eqs. (1) and (2). It leaves the measurement errors (white noise) alone while providing good estimates of the true mean and standard deviation of the estimates. In order to evaluate the performance the common-factor cleaning method, we compare it with commonly used methods of data cleaning for scientific applications including the Fourier filtering approach and Kalman smoother\napproach. The Fourier filtering method gives the estimate of the underlying mean but no associated errors. The Kalman smoother gives both mean and associated errors but needs a model, if the model is Eq. (1), the Kalman smoother just estimates the mean and prediction error by the data mean and standard deviation of the mean.\nIn the existing Marian soil data analysis, Kounaves et al. [2] used Fourier filtering method to get rid of the high frequency variations in WCL data. Toner et al. [3] used Kalman smoothing method with random walk plus noise model: \ud835\udc38\ud835\udc38\ud835\udc61\ud835\udc61 = \ud835\udf07\ud835\udf07\ud835\udc61\ud835\udc61 + \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61 & \ud835\udf07\ud835\udf07\ud835\udc61\ud835\udc61 = \ud835\udf07\ud835\udf07\ud835\udc61\ud835\udc61\u22121 + \ud835\udf02\ud835\udf02\ud835\udc61\ud835\udc61 . In our comparative studies, we keep the same settings for both methods.\nData Generation and Distribution. In our experiments, we simulate three independent series from model \ud835\udf07\ud835\udf07 + \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61 with \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61 following a standard normal distribution \ud835\udf00\ud835\udf00\ud835\udc61\ud835\udc61~\ud835\udc41\ud835\udc41(0,1) and \ud835\udf07\ud835\udf07 = 1, 5 & 10 respectively for the three series, each with 100 data points (Fig. 3a). We then add 1 common factor \ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61 (Fig. 3b) to contaminate series 1 to 3 in increasing degree (\ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61, 1.5\ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61 & 2\ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61) to simulate the observed series (Fig. 3c). We then apply on the contaminated series the Fourier filtering (Fig. 3d), Kalman smoother (Fig. 3e) and common-factor method (Fig. 3f) to clean the data. Fig. 3 clearly shows the difference of the common-factor cleaning method from the other methods, the white noises are still in after common-factor cleaning. Figs. 3a and 3f illustrate the strong similarity between the common-factor cleaned data and the original uncontaminated data, while Fourier filter and Kalman smoother (Figs. 3d and 3e) fail to recover the true data distribution.\nMean and Associated Error. Kalman smoother gives both mean and associated prediction error, we compare the common-factor cleaning method and the Kalman smoother method in terms of finding the mean and associated error. To do this, we perform the above mentioned simulation 1,000 times and apply the cleaning methods and then calculate the mean and associated error for each method. Hence we have 1,000 estimated means and standard errors (distribution of these 1000 estimated means and standard errors are in Figs 4 & 5). For simplicity, the associated error for Kalman smoother here is taken as the square root of the mean Kalman variance which is smaller than that used in Toner et al. [3] because they also included the variation in mean estimates at different time points.\nEstimation of Mean. For the three uncontaminated series, their true means are 1, 5 &10 respectively. Figure 4 shows that the Kalman smoother cleaned series (Fig. 4c) is very similar to that of the contaminated series (Fig. 4b) instead of the uncontaminated series (Fig. 4a). On the other hand, the common-factor cleaned series is more like the uncontaminated series: both centered on the same true mean (Figs. 4a & 4d). This shows the common-factor cleaning method\u2019s ability to remove systematic deviations while leaving the true mean unaffected.\nEstimation of Standard Deviation. For the three uncontaminated series, the true standard deviations of 100 data point means are 0.1. Figure 5 shows the distribution of the standard deviation of the mean. The contaminated series (Fig. 5b) and the Kalman smoother cleaned series (Fig. 5c) are both centered on similar values which are much larger than the true value. The common-factor cleaned data (Fig. 5d) have a standard error much closer to the true value (Fig. 5a), but it underestimates the error in series with larger variations."}, {"heading": "4. ANALYSIS OF MARTIAN SOIL DATA", "text": "Our evaluation using the simulated data are consistent with the assumption that the common variations between the individual data sets can be isolated and removed by analyzing the behavior of the data sets as a unit. In this section, we apply the common-factor cleaning method to the Wet Chemistry Laboratory (WCL) data from Phoenix lander mission.\n4.1 WCL data from Phoenix lander\nThe Wet Chemistry experiments used four identical WCL cells, Cells 0, 1, 2 and 3, to analyze the soluble contents of the Martian regolith. The analyses sampled Martian regolith from four separate locations on four separate Martian solar days (sols). Cell 0 analyzed the sample \u201cRosy Red\u201d taken from the surface of the Burn Alive trench; and Cells 1 and 2 analyzed samples \u201cSorceress 1\u201d and \u201cSorceress 2\u201d, respectively, both taken from adjacent locations at a depth of ~5cm, in contact with the ice table of the Snow White trench. Sample delivery to Cell 3 failed, and the data returned was used as an in situ blank against which the remaining three analyses could be compared. Figure 1 shows a schematic diagram of the WCL cell and how the four cells looked outside after the first-day analysis on Mars. Each cell consisted of (1) an upper actuator assembly with a drawer for adding soil, \u2018\u2018leaching solution\u2019\u2019, five crucibles with reagents used for the WCL calibration, and a stirrer; and (2) a lower beaker lined with an array of sensors including ion selective electrodes (ISE) for measuring K+, Na+, Mg2+, Ca2+, NH4+, Ba2+ (for SO42-), Cl-, Br-, I-, NO3-/ClO4-, H+(pH), Li+; and electrodes for measuring conductivity, redox potential, cyclic voltammetry (CV),\nchronopotentiometry (CP), and an IrO2 pH electrode. These data are publicly available at the NASA Planetary Data System [6, 7]. In this paper we only use the potential readings from the ISEs for Na+, K+, Ca2+, Mg2+, Cl-, ClO4-, and Li+ obtained from the analyses performed in Cell 0 on sol 30 (the 30th Martian solar day of the 152-sol Phoenix surface mission), Cell 1 on sol 41, and Cell 2 on sol 107. The data, prior to application of our commonfactor algorithm, from these cells is displayed in Fig. 2. The time intervals chosen for analysis correspond to the originally analyzed time-series as these represent the most stable and reliable portions of the data, and provide us the ability to compare our results with the originally published values. For each Cell two regions, confined between each set of vertical dashed lines in Fig. 2, were treated with our common-factor algorithm. The first region represents the calibration interval during which the ISEs were calibrated using a solution of known concentration, described in more detail elsewhere [2]. The second interval represents the sample interval and was taken after the addition of the ~1cm3 of Martian regolith to the WCL cells. Previous analysis of the WCL data has employed Fourier filtering [2] and Kalman smoothing [3] techniques to reduce the noise associated with the data sets, under the assumption that the associated noise is mostly random in nature. However, through inspection of the data, we observe that for much of the data, the potential readings of the different ISEs vary simultaneously and in a similar manner, although to varying degrees. This apparently systematic variation among sensors within the same beaker, and therefore subject to the same environmental conditions, lead us to believe that these deviations could be isolated and removed from the true signal in order to reduce the uncertainty in the measured concentration of each ion and provide meaningful quantitative results from the WCL ISE analyses. 4.2 Number of Common Factors The appropriate number of common factors to use in our algorithm is determined by considering the reduction in the standard error associated with the introduction of an additional common factor (strategy is described in Section 2). Fig. 6 displays the standard error for each ISE on each sol as a function of the number of common factors applied.\nAn overall reduction in standard error occurs for the majority of ISEs across all three sols with the addition of the first two common factors. Yet, upon the addition\nof the third common factor only one ISE error reduction is observed, suggesting the use of two common factors in our method.\nThe use of two common factors also make intuitive sense if we consider that the common variations are likely produced through two primary sources: electronic factors due to instrument malfunction, and physical factors relating to the combined effects of the physical environment inside the beaker. Therefore, we employ a two-common-factor algorithm to the WCL ISE data.\n4.3 Common-factor Data Cleaning\nThe two-common-factor algorithm was applied to the calibration and sample intervals of the WCL ISE data from cells 0, 1, and 2. The unprocessed data, the common-factor cleaned data, and the extracted common factors for each cell are displayed in Figures 7-9. The two-common-factor algorithm application to the WCL ISE measurements resulted in:\na) A reduction in variation compared to the original data.\nb) Automated removal of spikes in the signal that occur simultaneously in multiple sensors.\nc) Minimal deviation in mean potential from the original analysis, except for measurements in which large common variations significantly impact the mean.\nUsing the common-factor cleaned data, the total ion concentrations and associated uncertainty are calculated (Table 1 and Figure 10) and compared with concentration estimates from previous studies using Fourier filtering by Kounaves et al [2] and Kalman smoothing by Toner et al [3]. The new ion concentrations are determined using the mean of the common-factor cleaned data and the uncertainty was generated from the standard deviations for the newly cleaned data and the error values given by the original interpretation using the standard error propagation equation\n\u03c32(f(x, y, \u2026 )) = \ufffd\u2202f \u2202x \u03c3x\ufffd\n2 + \ufffd\u2202f\n\u2202y \u03c3y\ufffd\n2 + \u22ef\n(8). The highlighted values are the estimated total concentrations that are outside of the previously calculated concentration ranges. Highlights in red are ion concentrations outside Kounaves et al.\u2019s estimated range [2], and blue highlighted values are outside Toner et al.\u2019s estimated range [3]. The original analysis of the WCL data by Kounaves et al. used asymmetric errors to address the apparent bias in the signal noise. We report a symmetric error for our analysis, as our common-factor algorithm automatically accounts for this bias allowing the use of the standard error as an estimate of the uncertainty.\nThe concentration ranges estimated by our common-factor method overlap with the ranges estimated by the original Fourier filtering method for most ions in Cell 0 and Cell 1. The notable exception is the case of the Ca2+ ISE which is reported by our common-factor method to be significantly less than originally determined for all analyses. This extreme deviation is due to the effect of the presence of ClO4- on the Ca2+ sensor. As described in [2], the potential used to determine the concentration of Ca2+ is calculated based not only on the potential measured by the Ca2+ sensor, but also the concentration of ClO4- and the resulting changes in the reported Ca2+ concentration is affected by altered values for both measurements. For Cell 0, all three estimates agree primarily except Ca2+. For Cell 1, while our results are primarily in agreement with those reported by Kounaves et al., they differ dramatically from those reported by Toner et al. This is likely due to the handling of complications that arose during the initial calibration period where it is believe that the calibrant crucible intended to deliver a known concentration of ions to the leaching solution did not fully dissolve [2].\nThe analysis conducted in Cell 2 (Sorceress 2), shows the opposite trend, wherein our new concentration estimates vary significantly from Kounaves et al, yet agree with Toner et al. Deviation from the originally reported values in this case is not unexpected as the data returned from the Sorceress 2 analysis exhibited the largest degree of noise. That our newly reported values agree with the results published by Toner, suggests that these values are reasonable recalculations based on the denoised data set."}, {"heading": "5. CONCLUSION AND FUTURE DEVELOPMENT", "text": "In this paper, we present a new common-factor method for reducing unwanted variations from\ncommon interferences in data signals. The method is easy to use, intuitive and effective as a more unified approach for cleaning data. Our method eliminates the need for special handling of data in complicated scenarios where the origin of the noise is difficult to understand.\nTo date this common-factor algorithm has been successfully applied to the WCL experiments, as demonstrated, and would likely prove successful in other cases of data interpretation where the results are linked together in a sensor array and subject to extensive but unknown systematic noise. Sensor arrays are commonly employed in the field of environmental monitoring, wherein several different measurements are obtained simultaneously and the combination of data is used to obtain otherwise inaccessible information about the system.\nWhen these sensor arrays are employed in extreme and remote environments, the data obtained may exhibit extensive noise that, while unknown in source, affects all individual sensors to varying degrees. This new common-factor method can aid in reducing this systematic noise without a definitive understanding of the source and without degrading the physical meaning of the signal.\nThis work is important to scientific discoveries because of the following.\n\u2022 A method of removing common systematic error of unknown source can be implemented in data analysis for similar missions. This is paramount for analyses performed in extreme and extraterrestrial environments as unanticipated and unknown factors affecting data measurements are common.\n\u2022 The ability to analyze the data output from a sensor array for common variations that are independent of the chemistry provides the opportunity for gathering future data in complex samples where many unknown contributions to the signal exist.\n\u2022 A cleaner data set for the WCL analysis provides reduced uncertainty in the soluble chemistry of the Martian regolith. This will allow for more accurate geochemical models to be constructed and lead to a greater degree of certainty in the interpretations of the data."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "This work was supported by NASA under Grant NNX13AJ69G."}, {"heading": "7. REFERENCES", "text": "[1] Kounaves, S. P., Hecht, M. H., West, S. J,\nMorookian, J. M., Young, S. M. M., Quinn, R. C., Grunthaner, P., Wen, X., Weilert, M., Cable, C. A., Fisher, A., Gospodinova, K., Kapit, J., Stroble, S., Hsu, P. C., Clark, B. C., and Ming, D. W. 2009. The MECA Wet Chemistry Laboratory on the 2007 Phoenix Mars Scout Lander, Journal of geophysical research. 114, E00A19\n[2] Kounaves, S. P., Hecht, M. H., Kapit, J., Gospodinova, K., DeFlores, L., Quinn, R. C., Boynton, W. V., Clark, B. C., Catling, D. C., Hredzak, P., Ming, D. W., Moore, Q., Shusterman, J., Stroble, S., West, S. J, and Young, S. M. M.. 2010. Wet Chemistry experiments on the 2007 Phoenix Mars Scout Lander mission: Data analysis and results, Journal of geophysical research. 115, E00E10\n[3] Toner, J. D., Catling, D. C. and Light, B. 2013. Soluble salts at the Phoenix Lander site, Mars: A reanalysis of the Wet Chemistry Laboratory data. Geochimica et Cosmochimica Acta 136 (2014) 142\u2013168\n[4] Hecht, M. H., Kounaves, S. P., Quinn, R. C., West, S. J, Young, S. M. M., Ming, D. W., Catling, D. C., Clark, B. C., Boynton, W. V., DeFlores, L., Gospodinova, K., Kapit, J., and Smith, P. H.. 2009. \"Detection of Perchlorate and the Soluble Chemistry of Martian Soil at the Phoenix Lander Site\", Science, 325, 64-67\n[5] Anderson, T. W. An Introduction to Multivariate Statistical Analysis, 3rd Edition. New York: John Wiley and Sons, Inc.\n[6] Hecht, M.H., Phoenix MECA Non-Imaging Reduced Data V1.0, PHX-M-MECA-4-NIRDRV1.0, NASA Planetary Data System, 2008.\n[7] Hecht, Michael, Phoenix Mars MECA NonImaging EDR V1.0, NASA Planetary Data System, PHX-M-MECA-2-NIEDR-V1.0, 2008.\n[8] Walker, James S. A primer on wavelets and their scientific applications. CRC press, 1999.\n[9] Kalman, R.E. (1960). \"A new approach to linear filtering and prediction problems\". Journal of Basic Engineering 82 (1): pp. 35\u201345\n[10] Davies, E., Machine Vision: Theory, Algorithms and Practicalities, Academic Press, 1990, pp 42 - 44.\n[11] Ghabeli, Leila and Amindavar , Hamidreza, Image Denoising Using Hidden Markov Models, urAsiaICT 2002: Information and Communication Technology, Lecture Notes in Computer Science Volume 2510, 2002, pp 402-409\n[12] Andrews, D. and Mallows, C., \u201cScale mixtures of normal distributions,\u201d J.R. Statist. Doc, vol. 36, pp. 99, 1974.\n[13] PE\u00d1A, D. and BOX, G. E. P. (1987): Identifying a Simplifying Structure in Time Series. Journal of American Statistical Association, Vol. 82, 836\u2013 843.\n[14] Baillie, R.T., Bollerslev, T (1989): Common stochastic trends in a system of exchange rates. Journal of Finance 44(1), 167-181.\n[15] Zhang, B. et al. (2009): Differential dependency network analysis to identify condition-specific topological changes in biological networks, Bioinformatic, 25(4), 526-532.\n[16] Varoquaux, G. et al. (2010): Brain covariance selection: better individual functional connectivity models using population prior, Advances in Neural Information Processing Systems."}], "references": [{"title": "Soluble salts at the Phoenix Lander site, Mars: A reanalysis of the Wet Chemistry Laboratory data", "author": ["J.D. Toner", "D.C. Catling", "B. Light"], "venue": "Geochimica et Cosmochimica Acta", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Phoenix MECA Non-Imaging Reduced Data V1.0, PHX-M-MECA-4-NIRDR- V1.0", "author": ["M.H. Hecht"], "venue": "NASA Planetary Data System,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Phoenix Mars MECA Non- Imaging EDR V1.0", "author": ["Hecht", "Michael"], "venue": "NASA Planetary Data System,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "A primer on wavelets and their scientific applications", "author": ["Walker", "James S"], "venue": "CRC press,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "A new approach to linear filtering and prediction problems", "author": ["R.E. Kalman"], "venue": "Journal of Basic Engineering", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1960}, {"title": "Machine Vision: Theory, Algorithms and Practicalities", "author": ["E. Davies"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Image Denoising Using Hidden Markov Models, urAsia- ICT 2002: Information and Communication Technology, Lecture Notes in Computer", "author": ["Ghabeli", "Leila", "Amindavar", "Hamidreza"], "venue": "Science Volume", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Scale mixtures of normal distributions", "author": ["D. Andrews", "C. Mallows"], "venue": "J.R. Statist. Doc, vol. 36, pp. 99, 1974.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1974}, {"title": "Identifying a Simplifying Structure in Time Series", "author": ["D. PE\u00d1A", "G.E.P. BOX"], "venue": "Journal of American Statistical Association,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1987}, {"title": "Common stochastic trends in a system of exchange rates", "author": ["R.T. Baillie", "T Bollerslev"], "venue": "Journal of Finance", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Differential dependency network analysis to identify condition-specific topological changes in biological networks, Bioinformatic", "author": ["B Zhang"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Brain covariance selection: better individual functional connectivity models using population prior, Advances in Neural Information Processing Systems", "author": ["G Varoquaux"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "The existing state-of-the-art denoising methods, including Fourier filtering [8], Kalman smoothing [9] [12], Gaussian smoothing [10], and Hidden Markov Model denoising [11], do not work well for this type of problem because they are designed to cope with a single data measurement, ignoring the associated shared behavior among multiple datasets.", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "The existing state-of-the-art denoising methods, including Fourier filtering [8], Kalman smoothing [9] [12], Gaussian smoothing [10], and Hidden Markov Model denoising [11], do not work well for this type of problem because they are designed to cope with a single data measurement, ignoring the associated shared behavior among multiple datasets.", "startOffset": 99, "endOffset": 102}, {"referenceID": 7, "context": "The existing state-of-the-art denoising methods, including Fourier filtering [8], Kalman smoothing [9] [12], Gaussian smoothing [10], and Hidden Markov Model denoising [11], do not work well for this type of problem because they are designed to cope with a single data measurement, ignoring the associated shared behavior among multiple datasets.", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "The existing state-of-the-art denoising methods, including Fourier filtering [8], Kalman smoothing [9] [12], Gaussian smoothing [10], and Hidden Markov Model denoising [11], do not work well for this type of problem because they are designed to cope with a single data measurement, ignoring the associated shared behavior among multiple datasets.", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "The existing state-of-the-art denoising methods, including Fourier filtering [8], Kalman smoothing [9] [12], Gaussian smoothing [10], and Hidden Markov Model denoising [11], do not work well for this type of problem because they are designed to cope with a single data measurement, ignoring the associated shared behavior among multiple datasets.", "startOffset": 168, "endOffset": 172}, {"referenceID": 8, "context": "In [13-16], the principles of identifying common-mode regularities were discussed to identify a simplifying structure, shared trends in financial data and covariance selection in biological data, but not for the data cleaning using common external factors which is a quite different problem.", "startOffset": 3, "endOffset": 10}, {"referenceID": 9, "context": "In [13-16], the principles of identifying common-mode regularities were discussed to identify a simplifying structure, shared trends in financial data and covariance selection in biological data, but not for the data cleaning using common external factors which is a quite different problem.", "startOffset": 3, "endOffset": 10}, {"referenceID": 10, "context": "In [13-16], the principles of identifying common-mode regularities were discussed to identify a simplifying structure, shared trends in financial data and covariance selection in biological data, but not for the data cleaning using common external factors which is a quite different problem.", "startOffset": 3, "endOffset": 10}, {"referenceID": 11, "context": "In [13-16], the principles of identifying common-mode regularities were discussed to identify a simplifying structure, shared trends in financial data and covariance selection in biological data, but not for the data cleaning using common external factors which is a quite different problem.", "startOffset": 3, "endOffset": 10}, {"referenceID": 8, "context": "Though the idea of common factors is used in many fields [5, 13 \u2013 16], to the best of our knowledge, we are the first research team to use the idea of common hidden factors on data cleaning.", "startOffset": 57, "endOffset": 69}, {"referenceID": 11, "context": "Though the idea of common factors is used in many fields [5, 13 \u2013 16], to the best of our knowledge, we are the first research team to use the idea of common hidden factors on data cleaning.", "startOffset": 57, "endOffset": 69}, {"referenceID": 0, "context": "[3] used Kalman smoothing method with random walk plus noise model: EEtt = \u03bc\u03bctt + \u03b5\u03b5tt & \u03bc\u03bctt = \u03bc\u03bctt\u22121 + \u03b7\u03b7tt .", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[3] because they also included the variation in mean estimates at different time points.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "These data are publicly available at the NASA Planetary Data System [6, 7].", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "These data are publicly available at the NASA Planetary Data System [6, 7].", "startOffset": 68, "endOffset": 74}, {"referenceID": 0, "context": "Previous analysis of the WCL data has employed Fourier filtering [2] and Kalman smoothing [3] techniques to reduce the noise associated with the data sets, under the assumption that the associated noise is mostly random in nature.", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "Using the common-factor cleaned data, the total ion concentrations and associated uncertainty are calculated (Table 1 and Figure 10) and compared with concentration estimates from previous studies using Fourier filtering by Kounaves et al [2] and Kalman smoothing by Toner et al [3].", "startOffset": 279, "endOffset": 282}, {"referenceID": 0, "context": "\u2019s estimated range [3].", "startOffset": 19, "endOffset": 22}], "year": 2015, "abstractText": "Data quality is fundamentally important to ensure the reliability of data for stakeholders to make decisions. In real world applications, such as scientific exploration of extreme environments, it is unrealistic to require raw data collected to be perfect. As data miners, when it is infeasible to physically know the why and the how in order to clean up the data, we propose to seek the intrinsic structure of the signal to identify the common factors of multivariate data. Using our new data-driven learning method\u2014the common-factor data cleaning approach, we address an interdisciplinary challenge on multivariate data cleaning when complex external impacts appear to interfere with multiple data measurements. Existing data analyses typically process one signal measurement at a time without considering the associations among all signals. We analyze all signal measurements simultaneously to find the hidden common factors that drive all measurements to vary together, but not as a result of the true data measurements. We use common factors to reduce the variations in the data without changing the base mean level of the data to avoid altering the physical meaning. We have reanalyzed the NASA Mars Phoenix mission data used in the leading effort by Kounaves\u2019s team (lead scientist for the wet chemistry experiment on the Phoenix) [1, 2] with our proposed method to show the resulting differences. We demonstrate that this new common-factor method successfully helps reducing systematic noises without definitive understanding of the source and without degrading the physical meaning of the signal.", "creator": "Acrobat PDFMaker 11 for Word"}}}