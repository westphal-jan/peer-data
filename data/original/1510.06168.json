{"id": "1510.06168", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2015", "title": "Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network", "abstract": "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger.", "histories": [["v1", "Wed, 21 Oct 2015 08:57:30 GMT  (55kb)", "http://arxiv.org/abs/1510.06168v1", "rejected by ACL 2015 short, score: 4,3,2 (full is 5)"]], "COMMENTS": "rejected by ACL 2015 short, score: 4,3,2 (full is 5)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["peilu wang", "yao qian", "frank k soong", "lei he", "hai zhao"], "accepted": false, "id": "1510.06168"}, "pdf": {"name": "1510.06168.pdf", "metadata": {"source": "CRF", "title": "Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network", "authors": ["Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao"], "emails": ["helei}@microsoft.com,", "zhaohai@cs.sjtu.edu.cn,", "yqian@ets.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n06 16\n8v 1\n[ cs\n.C L\n] 2\n1 O\nct 2\n01 5"}, {"heading": "1 Introduction", "text": "Bidirectional long short-term memory (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) (BLSTM) is a type of recurrent neural network (RNN) that can incorporate contextual information from long period of fore-and-aft inputs. It has been proven a powerful model for sequential labeling tasks. For applications in natural language processing (NLP), it has helped achieve superior performance in language modeling (Sundermeyer et al., 2012; Sundermeyer et al., 2015), language understanding (Yao et al., 2013), and machine translation (Sundermeyer et al., 2014). Since part-of-speech (POS) tagging is a typical sequential labeling task, it seems natural to expect BLSTM RNN can also be effective for this task.\nAs a neural network model, it is awkward for BLSTM RNN to make use of conventional NLP features, such as morphological features. Since these features are discrete and has to be\nrepresented as one-hot vector to be used, using rich this type of features leads to too large input layer to maintain and update. Therefore, we avoid using such features except word form and simple capital features, instead we involve word embedding. Word embedding is a low dimensional real-valued vector used to represent word. It is considered containing part of syntactic and semantic information and has shown a very attractive feature for various of language processing tasks (Collobert and Weston, 2008; Turian et al., 2010a; Collobert et al., 2011). Word embedding can be obtained by training a neural network model, especially, a neural network language model (Bengio et al., 2006; Mikolov et al., 2010) or a neural network designed for a specific task (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014a). Currently many word embeddings trained on quite large corpora are available on line. However, these embeddings are trained by neural networks that are very different from BLSTM RNN. This inconsistency is supposed as an shortcoming to make the most of these trained word embeddings. To conquer this shortcoming, we also propose a novel method to train word embedding on unlabeled data with BLSTM RNN.\nThe main contributions of this work include: First, it shows an effective way to use BLSTM RNN for POS tagging task and achieves a state-ofthe-art tagging accuracy. Second, a novel method for training word embedding is proposed. Finally, we demonstrate that competitive tagging accuracy can be obtained without using morphological features, which makes this approach more practical to tag a language that lacks of necessary morphological knowledge."}, {"heading": "2 Methods", "text": ""}, {"heading": "2.1 BLSTM RNN for POS Tagging", "text": "Given a sentence w1, w2, ..., wn with tags y1, y2, ..., yn, BLSTM RNN is used to predict the tag probability distribution of each word. The usage is illustrated in Figure 1. Here wi is\nthe one hot representation of the current word. It is a binary vector of dimension |V | where V is the vocabulary. To reduce |V |, each letter in input word is transferred into lower case. To still keep the upper case information, a function f(wi) is introduced to indicate the original case information of word wi. More specifically, f(wi) returns a three-dimensional binary vector to tell if wi is full lowercase, full uppercase or leading with a capital letter. The input vector Ii of the neural network is computed as:\nIi = W1wi +W2f(wi)\nwhere W1 and W2 are weight matrixes connecting two layers. W1wi is the word embedding of wi which has a much smaller dimension than wi. In practice, W1 is implemented as a lookup table, W1wi is returned by referring to the word embedding of wi stored in this table. To use word embeddings trained by other task or method, we just need to initialize this lookup table with those external embeddings. For words without corresponding external embeddings, their word embeddings are initialized with uniformly distributed random values, ranging from -0.1 to 0.1. The implementation of BLSTM layer is detailed descripted in (Graves, 2012) and therefore is skipped in this paper. This layer incorporates information from the past and future histories when making prediction for current word and is updated as a function of the entire input sentence. The output layer is a softmax layer whose dimension is the number of tag types. It outputs the tag probability distribution of input word wi. All weights are trained using\nbackpropagation and gradient descent algorithm to maximize the likelihood on training data:\n\u220f\ni\u22081,...,n\nPi(yi|w1, w2, ..., wn)\nThe obtained probability distribution of each step is supposed independent with each other. The utilization of contextual information strictly comes from the BLSTM layer. Thus, in inference phase, the likeliest tag y\u2032i of input word wi can just be chose as:\ny\u2032i = arg max t\u22081,...,m Pi(t|w1, w2, ..., wn)\nwhere m is the number of tag types."}, {"heading": "2.2 Word Embedding", "text": "In this section, we propose a novel method to train word embedding on unlabeled data with BLSTM RNN. In this approach, BLSTM RNN is also used to do a tagging task, but only has two types of tags to predict: incorrect/correct. The input is a sequence of words which is a normal sentence with some words replaced by randomly chosen words. For those replaced words, their tags are 0 (incorrect) and for those that are not replaced, their tags are 1 (correct). Although it is possible that some replaced words are also reasonable in the sentence, they are still considered \u201cincorrect\u201d. Then BLSTM RNN is trained to minimize the binary classification error on the training corpus. The neural network structure is the same as that in Figure 1. When the neural network is trained, W1 contains all trained word embeddings."}, {"heading": "3 Experiments", "text": "BLSTM RNN systems in our experiments are implemented with CURRENT (Weninger et al., 2014), a machine learning library for RNN which adopts GPU acceleration. The activation function of input layer is identity function, hidden layer is logistic function, while the output layer uses softmax function for multiclass classification. Neural network is trained using statistical gradient descent algorithm with constant learning rate."}, {"heading": "3.1 Corpora", "text": "The part-of-speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III (Marcus et al., 1993). Training, development and test sets are split following setup in\n(Collins, 2002). Table 1 lists the detailed information of the three data sets.\nTo train word embedding, we uses North American news (Graff, 2008) as the unlabeled data. This corpus contains about 536 million words. It is tokenized using the Penn Treebank tokenizer script 1. All consecutive digits occurring within a word are replaced with the symbol \u201c#\u201d. For example, both words \u201cTel192\u201d and \u201cTel6\u201d are transferred to the same word \u201cTel#\u201d."}, {"heading": "3.2 Hidden Layer Size", "text": "We evaluate different sizes of hidden layer in BLSTM RNN to pick up the best structure for later experiments. The input layer size is set to 100 and output layer size is fixed as 45 in all experiments. The accuracies on WSJ test set are shown in Figure 2. It shows that hidden layer size has a\nlimited impact on performance when it becomes large enough. To keep a good trade-off of accuracy, model size and running time, we choose 100 which is the smallest layer size to get \u201creasonable\u201d performance as the hidden layer size in all the following experiments."}, {"heading": "3.3 POS Tagging Accuracies", "text": "Table 2 compares the performance of our systems with other baseline systems.\nBaseline systems. Four typical systems are chosen as baseline systems. (Toutanova et al., 2003) is one of the most commonly used approaches which is also known as Stanford tagger. (Huang et al., 2012) is the system reports best accuracy on WSJ test set\n1https://www.cis.upenn.edu/\u02dctreebank/tokenization.html\n(97.35%). In fact, (Spoustova\u0301 et al., 2009) reports a higher accuracy (97.44%), but this work relies on multiple trained taggers and combines their tagging results. Here we focus on single model tagging algorithm and therefore do not include this work as baseline. Besides, (Moore, 2014) (97.34%) and (Shen et al., 2007) (97.33%) also reach accuracy above 97.3%. These two systems plus (Huang et al., 2012) are considered as current state-of-the-art systems. All these systems rely on rich morphological features. In contrast, (Collobert et al., 2011) NN only uses word form and capital features. (Collobert et al., 2011) NN+WE also incorporates word embeddings trained on unlabeled data like our approach. The main difference is that (Collobert et al., 2011) uses feedforward neural network instead of BLSTM RNN.\nBLSTM-RNN is the system described in Section 2.1 which only uses word form and capital features. The vocabulary we used in this experiment is all words appearing in WSJ Penn Treebank training set, merging with the most common 100,000 words in North American news corpus, plus one single \u201cUNK\u201d symbol for replacing all out of vocabulary words.\nWithout the help of morphological features, it is not surprising that BLSTM-RNN falls behind the state-of-the-art system. However, BLSTM-RNN surpasses (Collobert et al., 2011) NN which is also neural network based method and uses the same input features. It is consistent with (Fernandez et al., 2014; Fan et al., 2014), in which BLSTM RNN outperforms feedforward neural network.\nBLSTM-RNN+WE. To construct corpus for training word embeddings, about 20% words in normal sentences of North American news corpus are replaced with randomly selected words. Then BLSTM RNN is trained to judge which word has been replaced as described in Section 2.2. The vo-\ncabulary for this task contains the 100,000 most common words in North American news corpus and one special \u201cUNK\u201d symbol. When training is finished, word embedding lookup table (W1) in BLSTM RNN for POS tagging is initialized with the trained word embeddings. The following training and testing are the same as previous experiment.\nTable 2 shows the results of using word embeddings trained on the first 10 million words (WE(10m)), first 100 million words (WE(100m)) and all 530 million words (WE(all)) of North American news corpus. While WE(10m) does not show much help for the improvement, WE(100m) and WE(all) significantly boosts the performance. It shows that BLSTM RNN can benefit from word embeddings trained on large unlabeled corpus and larger training corpus leads to a better performance. This suggests that the result may be further improved by using even bigger unlabeled data set. With the help of GPU, WE(all) can be trained in about one day (23 hrs). The training time increases linearly with the training corpus size.\nWE(all) reduces over 20% error rate of BLSTM-RNN and lets the result comparable with (Toutanova et al., 2003). Note that this result is obtained without using any morphological features. Current state-of-theart systems (Moore, 2014; Shen et al., 2007; Huang et al., 2012) all utilize morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4). Moreover, (Shen et al., 2007) also involves prefix and suffix of length from 5 to 9. (Moore, 2014) adds extra elaborately designed features, including flags indicating if word ends with \u2212ed or \u2212ing, etc. In practice, many languages with rich morphological forms lack of necessary or effective morphological processing tools. In these cases, a POS tagger that does not rely on morphological features is more realistic for use.\nBLSTM-RNN+WE(all)+suffix2. In this experiment, we add bigram suffix of each word as\nextra feature. These last 2 characters are represented as one-hot vector and appended to the original extra feature vector (f(wi)). The other configuration follows BLSTM-RNN+WE(all). The additional feature furthermore pushes up the accuracy and lets the approach get the state-of-theart performance (97.40%). However, adding more morphological features such as trigram suffix does not further improve the performance. One possible reason is that adding such feature brings a much longer extra feature vector which needs retuning parameters such as learning rate and hidden layer size to get the optimum performance."}, {"heading": "3.4 Different Word Embeddings", "text": "In this experiment, six types of published welltrained word embeddings are evaluated. The basic information of involved word embeddings and results are listed in Table 3 where RCV1 represents the Reuters Corpus Volume 1 news set. The OOV (out of vocabulary) column indicates the rate of words in vocabulary of BLSTM RNN for POS tagging that are not covered by external word embedding vocabulary. The usage of word embeddings is the same as in BLSTM-RNN+WE experiment except that input layer size here is equal to the dimension of external word embedding.\nAll word embeddings bring about higher accuracy. However, none of them can enhance BLSTM RNN tagging to get a competitive accuracy, despite of larger corpora that they are trained on and lower OOV rate. (Pennington et al., 2014b)1 (97.12%) has the highest accuracy among them but it is still lower than (Toutanova et al., 2003) (97.24%). Although more experiments are needed to judge which word embeddings are better, this experiment at least shows word embeddings trained by BLSTM RNN are essential in our POS tagging approach to achieve a superior performance."}, {"heading": "4 Conclusions", "text": "In this paper, BLSTM RNN is proposed for POS tagging and training word embedding. Combined with word embedding trained on big unlabeled data, this approach gets state-of-the-art accuracy on WSJ test set without using rich morphological features. BLSTM RNN with word embedding is expected as an effective solution for tagging tasks and worth further exploration."}], "references": [{"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "JeanLuc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Michael Collins"], "venue": "In EMNLP,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural Language Processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Fan et al.2014] Yuchen Fan", "Yao Qian", "Fenglong Xie", "Frank K. Soong"], "venue": "In INTERSPEECH,", "citeRegEx": "Fan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "Prosody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks", "author": ["Asaf Rendel", "Bhuvana Ramabhadran", "Ron Hoory"], "venue": null, "citeRegEx": "Fernandez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fernandez et al\\.", "year": 2014}, {"title": "Supervised sequence labelling with recurrent neural networks, volume", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Structured Perceptron with Inexact Search", "author": ["Huang et al.2012] Liang Huang", "Suphan Fayong", "Yang Guo"], "venue": "In HLT-NAACL,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Fast high-accuracy part-of-speech tagging by independent classifiers", "author": ["Robert Moore"], "venue": "In Coling,", "citeRegEx": "Moore.,? \\Q2014\\E", "shortCiteRegEx": "Moore.", "year": 2014}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "GloVe", "author": ["Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "http://nlp.stanford.edu/projects/glove/.", "citeRegEx": "Pennington et al\\.,? 2014b", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A Maximum Entropy Model for Part-Of-Speech Tagging", "author": ["Adwait Ratnaparkhi"], "venue": "In EMNLP,", "citeRegEx": "Ratnaparkhi.,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi.", "year": 1996}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Guided Learning for Bidirectional Sequence Classification", "author": ["Shen et al.2007] Libin Shen", "Giorgio Satta", "Aravind Joshi"], "venue": "In ACL,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Semi-Supervised Training for the Averaged Perceptron POS Tagger", "author": ["Jan Haji\u010d", "Jan Raab", "Miroslav Spousta"], "venue": "In EACL,", "citeRegEx": "Spoustov\u00e1 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Spoustov\u00e1 et al\\.", "year": 2009}, {"title": "LSTM Neural Networks for Language Modeling", "author": ["Ralf Schl\u00fcter", "Hermann Ney"], "venue": null, "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Translation Modeling with Bidirectional Recurrent Neural Networks", "author": ["Tamer Alkhouli", "Joern Wuebker", "Hermann Ney"], "venue": "In EMNLP,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "From feedforward to recurrent lstm neural networks for language modeling", "author": ["Hermann Ney", "Ralf Schluter"], "venue": null, "citeRegEx": "Sundermeyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network", "author": ["Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In HLT-NAACL", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Word Representations: A Simple and General Method for Semi-supervised Learning", "author": ["Lev Ratinov", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Word representations for NLP", "author": ["Lev Ratinov", "Yoshua Bengio."], "venue": "http://metaoptimize.com/projects/wordreprs/.", "citeRegEx": "Ratinov and Bengio.,? 2010b", "shortCiteRegEx": "Ratinov and Bengio.", "year": 2010}, {"title": "Introducing CURRENNT\u2013the Munich open-source CUDA RecurREnt Neural Network Toolkit", "author": ["Johannes Bergmann", "Bj\u00f6rn Schuller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weninger et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weninger et al\\.", "year": 2014}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "MeiYuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In INTERSPEECH,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "For applications in natural language processing (NLP), it has helped achieve superior performance in language modeling (Sundermeyer et al., 2012; Sundermeyer et al., 2015), language understanding (Yao et al.", "startOffset": 119, "endOffset": 171}, {"referenceID": 21, "context": "For applications in natural language processing (NLP), it has helped achieve superior performance in language modeling (Sundermeyer et al., 2012; Sundermeyer et al., 2015), language understanding (Yao et al.", "startOffset": 119, "endOffset": 171}, {"referenceID": 26, "context": ", 2015), language understanding (Yao et al., 2013), and machine translation (Sundermeyer et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 20, "context": ", 2013), and machine translation (Sundermeyer et al., 2014).", "startOffset": 33, "endOffset": 59}, {"referenceID": 3, "context": "It is considered containing part of syntactic and semantic information and has shown a very attractive feature for various of language processing tasks (Collobert and Weston, 2008; Turian et al., 2010a; Collobert et al., 2011).", "startOffset": 152, "endOffset": 226}, {"referenceID": 0, "context": "Word embedding can be obtained by training a neural network model, especially, a neural network language model (Bengio et al., 2006; Mikolov et al., 2010) or a neural network designed for a specific task (Collobert et al.", "startOffset": 111, "endOffset": 154}, {"referenceID": 10, "context": "Word embedding can be obtained by training a neural network model, especially, a neural network language model (Bengio et al., 2006; Mikolov et al., 2010) or a neural network designed for a specific task (Collobert et al.", "startOffset": 111, "endOffset": 154}, {"referenceID": 3, "context": ", 2010) or a neural network designed for a specific task (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014a).", "startOffset": 57, "endOffset": 130}, {"referenceID": 6, "context": "(Graves, 2012) and therefore is skipped in this paper.", "startOffset": 0, "endOffset": 14}, {"referenceID": 25, "context": "BLSTM RNN systems in our experiments are implemented with CURRENT (Weninger et al., 2014), a machine learning library for RNN which adopts GPU acceleration.", "startOffset": 66, "endOffset": 89}, {"referenceID": 9, "context": "The part-of-speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III (Marcus et al., 1993).", "startOffset": 110, "endOffset": 131}, {"referenceID": 1, "context": "(Collins, 2002).", "startOffset": 0, "endOffset": 15}, {"referenceID": 22, "context": "(Toutanova et al., 2003) is one of the most commonly used approaches which is also known as Stanford tagger.", "startOffset": 0, "endOffset": 24}, {"referenceID": 8, "context": "(Huang et al., 2012) is the system reports best accuracy on WSJ test set", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "html Sys Acc (%) (Toutanova et al., 2003) 97.", "startOffset": 17, "endOffset": 41}, {"referenceID": 8, "context": "24 (Huang et al., 2012) 97.", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "35 (Collobert et al., 2011) NN 96.", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "36 (Collobert et al., 2011) NN+WE 97.", "startOffset": 3, "endOffset": 27}, {"referenceID": 18, "context": "In fact, (Spoustov\u00e1 et al., 2009) reports a higher accuracy (97.", "startOffset": 9, "endOffset": 33}, {"referenceID": 12, "context": "Besides, (Moore, 2014) (97.", "startOffset": 9, "endOffset": 22}, {"referenceID": 17, "context": "34%) and (Shen et al., 2007) (97.", "startOffset": 9, "endOffset": 28}, {"referenceID": 8, "context": "These two systems plus (Huang et al., 2012) are considered as current state-of-the-art systems.", "startOffset": 23, "endOffset": 43}, {"referenceID": 3, "context": "In contrast, (Collobert et al., 2011) NN only uses word form and capital features.", "startOffset": 13, "endOffset": 37}, {"referenceID": 3, "context": "(Collobert et al., 2011) NN+WE also incorporates word embeddings trained on unlabeled data like our approach.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "The main difference is that (Collobert et al., 2011) uses feedforward neural network instead of BLSTM RNN.", "startOffset": 28, "endOffset": 52}, {"referenceID": 3, "context": "However, BLSTM-RNN surpasses (Collobert et al., 2011) NN which is also neural network based method and uses the same input features.", "startOffset": 29, "endOffset": 53}, {"referenceID": 5, "context": "with (Fernandez et al., 2014; Fan et al., 2014), in which BLSTM RNN outperforms feedforward neural network.", "startOffset": 5, "endOffset": 47}, {"referenceID": 4, "context": "with (Fernandez et al., 2014; Fan et al., 2014), in which BLSTM RNN outperforms feedforward neural network.", "startOffset": 5, "endOffset": 47}, {"referenceID": 14, "context": "86 (Pennington et al., 2014b)1 100 400K Wiki (6B) 0.", "startOffset": 3, "endOffset": 29}, {"referenceID": 14, "context": "12 (Pennington et al., 2014b)2 100 1193K Twitter (27B) 0.", "startOffset": 3, "endOffset": 29}, {"referenceID": 22, "context": "WE(all) reduces over 20% error rate of BLSTM-RNN and lets the result comparable with (Toutanova et al., 2003).", "startOffset": 85, "endOffset": 109}, {"referenceID": 12, "context": "art systems (Moore, 2014; Shen et al., 2007; Huang et al., 2012) all utilize morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4).", "startOffset": 12, "endOffset": 64}, {"referenceID": 17, "context": "art systems (Moore, 2014; Shen et al., 2007; Huang et al., 2012) all utilize morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4).", "startOffset": 12, "endOffset": 64}, {"referenceID": 8, "context": "art systems (Moore, 2014; Shen et al., 2007; Huang et al., 2012) all utilize morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4).", "startOffset": 12, "endOffset": 64}, {"referenceID": 15, "context": ", 2012) all utilize morphological features proposed in (Ratnaparkhi, 1996) which involves n-gram prefix and suffix (n = 1 to 4).", "startOffset": 55, "endOffset": 74}, {"referenceID": 17, "context": "Moreover, (Shen et al., 2007) also involves prefix and suffix of length from 5 to 9.", "startOffset": 10, "endOffset": 29}, {"referenceID": 12, "context": "(Moore, 2014) adds extra elaborately designed features, including flags indicating if word ends with \u2212ed or \u2212ing, etc.", "startOffset": 0, "endOffset": 13}, {"referenceID": 14, "context": "(Pennington et al., 2014b)1", "startOffset": 0, "endOffset": 26}, {"referenceID": 22, "context": "12%) has the highest accuracy among them but it is still lower than (Toutanova et al., 2003) (97.", "startOffset": 68, "endOffset": 92}], "year": 2015, "abstractText": "Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTMRNN) has been shown to be very effective for tagging sequential data, e.g. speech utterances or handwritten documents. While word embedding has been demoed as a powerful representation for characterizing the statistical properties of natural language. In this study, we propose to use BLSTM-RNN with word embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is achieved. Without using morphological features, this approach can also achieve a good performance comparable with the Stanford POS tagger.", "creator": "LaTeX with hyperref package"}}}