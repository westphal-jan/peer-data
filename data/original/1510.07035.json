{"id": "1510.07035", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Oct-2015", "title": "Fast Latent Variable Models for Inference and Visualization on Mobile Devices", "abstract": "In this project we outline Vedalia, a high performance distributed network for performing inference on latent variable models in the context of Amazon review visualization. We introduce a new model, RLDA, which extends Latent Dirichlet Allocation (LDA) [Blei et al., 2003] for the review space by incorporating auxiliary data available in online reviews to improve modeling while simultaneously remaining compatible with pre-existing fast sampling techniques such as [Yao et al., 2009; Li et al., 2014a] to achieve high performance. The network is designed such that computation is efficiently offloaded to the client devices using the Chital system [Robinson &amp; Li, 2015], improving response times and reducing server costs. The resulting system is able to rapidly compute a large number of specialized latent variable models while requiring minimal server resources.", "histories": [["v1", "Fri, 23 Oct 2015 05:26:09 GMT  (694kb,D)", "http://arxiv.org/abs/1510.07035v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.DC cs.IR", "authors": ["joseph w robinson", "aaron q li"], "accepted": false, "id": "1510.07035"}, "pdf": {"name": "1510.07035.pdf", "metadata": {"source": "META", "title": "Fast Latent Variable Models for Inference and Visualization on Mobile Devices", "authors": ["Joseph W Robinson", "Aaron Q Li"], "emails": ["JWROBINS@ANDREW.CMU.EDU", "AARON@POTATOS.IO"], "sections": [{"heading": "1. Introduction", "text": "Recent advances in machine learning have given rise to several classes of latent variable models, allowing one to embed additional unobserved structure into a problem in order to improve results. One such method is Latent Dirichlet Allocation (LDA) (Blei et al., 2003), a generative model in which documents are assumed to contain a mixture of topics; each topic is then represented as a probability distribution over words in the corpus. A recent application of this is in Amazon review modeling (Li et al., 2014c). In this approach, each review text is treated as a document, and products are displayed via a word cloud containing the top n words per topic. However, while work such as the Alias method (Li et al., 2014a), GPU-sampling (Li, 2012),\nCopyright 2015 by the author(s).\nand Parameter Server (Li et al., 2013; 2015) has resulted in substantial speed improvements for large scale systems, much less has been done for a more hardware-constrained setting such as smartphones running Android or iOS. Performance aside, even less has been explored with respect to obtaining interpretable and visualizable results from these models."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Current Amazon System", "text": "The current Amazon system for displaying reviews is divided into three sections. Quotes give a user a set of three one-line excerpts from reviews as a high level overview of the product. A list of \u201cmost helpful reviews\u201d give the user more detailed information via the eyes of a select few reviewers. The full review list is then available as a backup for the ambitious buyer who wishes to sift through thousands of reviews to ensure their purchase will be worthwhile. It is easy to spot a couple potential flaws in this system \u2013 a user is either able to view the experience of a small choice set of individuals, or they must look through the full uncurated review set. Thus, their is no aggregation of knowledge despite the availability of millions of reviews throughout the site. In addition, the system does not quickly convey a multi-faceted view of the product. For example, a user buying a smartphone might be interested in knowing the general sentiment about the phone\u2019s camera, battery life, performance (does it lag?), reliability of connection, etc."}, {"heading": "2.2. Previous Topic Modeling Approach", "text": "The previous approach uses LDA to overcome this issue by representing each product by its topic distribution. Each topic distribution is then displayed to the user via a word cloud in order to provide multifaceted information about a product in a way that is intuitive, compact, and aggregate. The system retains completeness in the sense that the full review set is still available to the user via an interactive\nar X\niv :1\n51 0.\n07 03\n5v 1\n[ cs\n.L G\n] 2\n3 O\nct 2\n01 5\ntopic-based search feature. However, this system still falls short in several areas. Namely, the system:\n\u2022 Requires that models are built on the cloud and returned for the user for display\n\u2022 Is far too slow to be run on mobile operating systems\n\u2022 Does not take advantage of auxiliary data such as ratings, helpfulness votes, and users when estimating topics\n\u2022 Utilizes a fixed number of topics (16) regardless of the product\n\u2022 Does not give rise to results that are easily displayable on a small-screen mobile device\n\u2022 Uses a fixed dataset (Leskovec & Krevl, 2014) rather than an adaptive one\nThe first point gives rise to several issues when scaling the service. Previously, computing topics was done with MALLET (McCallum, 2002), a machine learning toolkit written in Java. Aside from the toolkit\u2019s inability to efficient parallelize LDA training, the use of MALLET server-side results in large computation time and an unscalable system in terms of cost. We plan to address this issue by building a system that can run efficiently in a mobile setting, thus offloading the vast majority of computation to a distributed system of clients connected by a central model cache and updating server. To do this, the proposed learning algorithm must efficiently run in a locally parallel system (2-4 cores) in an amount of time that is acceptable for a typical user. Sampling for the algorithm must be done from scratch; as an example of MALLET\u2019s poor performance on Android, it was found that a small topic modeling job ( 350 reviews) caused the app to crash after several minutes largely spent in garbage collection when using MALLET due to high memory consumption.\nAnother weakness of the previous system is omission of auxiliary data from the underlying probability model. By using standard LDA, ratings, helpfulness votes, and the user network cannot be used for directly improving results. While post-processing can help with some of these issues, it cannot effectively improve the quality of topics themselves. Thus, we present a new latent variable model, RLDA, that incorporates this information in order to improve topic quality and reduce noise. In addition, RLDA allows for a variable number of topics in order to help avoid the display of information-void topics and improve user experience.\nAlong these same lines, the final result must be easily visualizable on a mobile device. While the previous system was\neasily viewable on a desktop web browser, the size constraints of a mobile device coupled with the lack of mouse prohibit the use of many previous features. Namely, the number of topics displayed and the size of the visualization circle are too large to display on a mobile device while maintaining visible text, and the lack of a mouse invalidates the hover-based review selection system. In later sections, we demonstrate how our revised approach overcomes these issues to allow RLDA results to effectively be visualized on small-screen devices.\nAs a final note, the previous system utilized a fixed dataset (Leskovec & Krevl, 2014) which did not include recent products or allow for model updating as new reviews appear. For a potential buyer, it is often useful to know if a product has a tendency to fail a few months after purchasing it. We address this issue by creating a system that dynamically updates models as new data becomes available using efficient sampling techniques."}, {"heading": "2.3. Pre-existing work on modeling reviews", "text": "Pre-existing latent variable models designed for analyzing reviews, such as (Brody & Elhadad, 2010; Jo & Oh, 2011; Titov & McDonald, 2008), generally fall short in scalability and generality. They generally make improvement over LDA by using word associations and sentence context to form more representative words. In addition to that, they also focus specifically on a fixed number of known aspects. This severely limits the potential application of these models, and as a result they cannot capture the unknown topics/aspects at fine-grained level (e.g a third-party charging adapter does not work with some Apple computers). These models also ignore a large quantity of auxiliary data such as user ratings, helpfulness, and unhelpfulness."}, {"heading": "2.4. Latent Dirichlet Allocation", "text": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a widely used topic model in which documents are assumed to be generated from mixture distributions of language models associated with individual topics. That is, the documents are generated by the latent variable model below:\n\u03b1 \u03b8d zdi wdi \u03c6k \u03b2\nfor all i\nfor all d\nfor all k\nThe generative process is as follows:\nFor each document d draw a topic distribution \u03b8d\nfrom a Dirichlet distribution with parameter \u03b1\n\u03b8d \u223c Dir(\u03b1) (1)\nFor each topic t draw a word distribution from a Dirichlet distribution with parameter \u03b2\n\u03c8t \u223c Dir(\u03b2) (2)\nFor each word i \u2208 {1...nd} in document d draw a topic from the multinomial \u03b8d via\nzdi \u223cMult(\u03b8d) (3)\nDraw a word from the multinomial \u03c8zdi via\nwdi \u223cMult(\u03c8zdi) (4)\nThe Dirichlet-multinomial design in this model makes it simple to do inference due to distribution conjugacy \u2013 we can integrate out the multinomial parameters \u03b8d and \u03c8k, thus allowing one to express p(w, z|\u03b1, \u03b2, nd) in a closedform (Yao et al., 2009). This yields a Gibbs sampler for drawing p(zdi|rest) efficiently. The conditional probability is given by\np(zdi|rest) \u221d (n\u2212ditd + \u03b1t)(n \u2212di tw + \u03b2w)\nn\u2212dit + \u03b2\u0304 (5)\nHere the count variables ntd, ntw and nt denote the number of occurrences of a particular (topic,document) and (topic,word) pair, or of a particular topic, respectively. Moreover, the superscript .\u2212di denotes count when ignoring the pair (zdi, wdi). For instance, n\u2212ditw is obtained when ignoring the (topic,word) combination at position (d, i). Finally, \u03b2\u0304 := \u2211 w \u03b2w denotes the joint normalization.\nSampling from (5) requires O(k) time since we have k nonzero terms in a sum that need to be normalized. In large datasets where the number of topics may be large, this is computationally costly. However, there are many approaches for substantially accelerating sampling speed by exploiting the topic sparsity to reduce time complexity to O(kd + kw) (Yao et al., 2009) and further to O(kd) (Li et al., 2014a), where O(kd) denotes the number of topics instantiated in a document and O(kw) denotes the number of topics instantiated for a word across all documents."}, {"heading": "2.5. Chital Computation Marketplace", "text": "Chital is a scalable, distributed computation marketplace designed for the efficient allocation of high CPU, low network bandwidth tasks among a network of mobile devices. The five key aspects of Chital are 1) task distribution via the marketplace, 2) a credit score system for monitoring user behavior 3) real-time matching mechanisms for maximizing user gain 4) an optional lottery system for further incentivizing participation 5) an evaluation system for verifying the submitted models. Each of these is discussed in detail below."}, {"heading": "2.5.1. MARKETPLACE", "text": "The marketplace is the major underlying component in Chital for task allocation. In the marketplace, each user has the option of opting-in to background computation; once opted in, this user is then listed as a computational seller and can be assigned modeling tasks to be run in the background. When a Vedalia user enters a query, a matching request is sent to a centralized server system \u2013 this user is now a buyer. Assuming the buyer has sufficient computational power on his phone, the buyer is also automatically listed as a seller for the duration of his model computation. The marketplace then matches the buyer with a pair of sellers and requests that both sellers generate a model from the supplied data. This data is then returned to the central servers, where the system determines whether model verification is necessary. Let c1 and c2 denote the credit of the two sellers, and p1 and p2 denote the perplexity of the sellers\u2019 results. Then the probability of secondary verification is defined as:\n1\u2212 1 3\n[ 1\n1 + e\u2212(c1+c2) + 2\nmin(p1, p2)\nmax(p1, p2)\n] (6)\nThus, high seller credit scores and high perplexity match reduce the probability of verification, and vice versa. The best model (measured by perplexity) that passes verification is then returned to the original buyer."}, {"heading": "2.5.2. CREDIT SYSTEM", "text": "A 0-sum credit system is established that begins with two 0-credit sellers for computation. Then, each user that joins the system as a seller begins with 0 credit. When building a model, the perplexities of each of the two models returned by the sellers are compared; a credit from the worst model\u2019s seller is then transferred to the best model\u2019s seller. The best model\u2019s seller is additionally awarded a t \u00b7 i\u2217 lottery tickets, where t is the number of tokens processed and i\u2217 is the number of sampling iterations performed by the best model. Assuming every seller is honest, each seller has expectation 0 credit over time. However, in the event that a malicious seller attempts to provide phony results in order to acquire lottery tickets, the credit distribution shifts from the bad to good users. As a result, the system becomes less likely to need to verify results of good users, and becomes increasingly likely to perform verification on bad users."}, {"heading": "2.5.3. MATCHING MECHANISMS", "text": "The core of Chital is a real-time matching system that pairs each query with two sellers. The matching problem can be formulated as a bipartite matching problem with both sides of the vertices arriving online. Each buyer vertex is required to match with two seller vertices. In addition to that, after a matching is established, the matched vertices\nonly become temporarily unavailable for a period of time based on the performance of seller nodes and the task size of buyer node, before the matching is removed and the vertices become available again.\nAlthough online graph matching especially online bipartite matching is a well-studied major research area (Karp et al., 1990; Mehta, 2013), our problem setup makes it difficult to apply any existing algorithm for two reasons: our problem introduces an extra \u201dtime dimension\u201d, and our objective is to maximize overall user gain thereby to convince them joining the system voluntarily. Based on these, we developed the concept of strategyproofness and Nash equilibrium in another recent work of ours (Robinson & Li, 2015) and, studied and created a suite of new real-time matching algorithms to achieve our goal."}, {"heading": "2.5.4. LOTTERY SYSTEM", "text": "To further incentivize seller participation, a lottery system can be constructed in which a portion of app advertising revenue is allocated for a lottery system. At the end of each lottery period, a user a sampled at random with probability of winning proportionate to the user\u2019s number of lottery tickets. The full lottery amount is then awarded to this winning user.\nNote, however, the existence of the lottery system is entirely optional, since a rational user would voluntarily participate the system if a good matching mechanism with strategyproofness and empirical Nash equilibrium were used. In our empirical studies (Robinson & Li, 2015), we found under appropriate parameters, users always save overall computation time by a large margin within our simulation."}, {"heading": "2.5.5. EVALUATION SYSTEM", "text": "Evaluation is a multi-stage system consisting of model validation, selection, and verification.\nIn validation, basic properties of the submitted distributions are verified (e.g. sum to 1). Any model that fails validation is immediately rejected.\nIn selection, the perplexity of each submitted model is computed. The model with the lower perplexity is the selected to be returned to the end user, pending verification.\nIn verification, the marketplace first computes the probability pv of secondary verification as described in 2.5.1. A value s is then sampled uniformly from [0,1]; if s > pv , verification occurs. In verification, a few additional iterations of Gibbs sampling are run on the selected model on Chital servers, and final model perplexity is computed. If the final perplexity deviates substantially from that of the submitted model, the submitted model has not converged\nand is thus rejected by the system."}, {"heading": "3. Proposed System", "text": "We propose a system that incorporates a new latent variable model, RLDA, that is well-suited for mobile devices and review analysis. It naturally extends LDA while simultaneously maintains a structure that allows the use techniques introduced in SparseLDA and AliasLDA to achieve high-performance at any scale. Furthermore, the system is integrated with Chital for scalability, and is accessible to the end user in the form of a mobile application."}, {"heading": "3.1. RLDA", "text": "Review-augmented Latent Dirichlet Allocation (RLDA) is an adaptation of LDA that is well-suited for modeling reviews in a mobile setting due to its high sampling performance and increased structure with respect to standard LDA. In Figure 3.1 we present the RLDA model in plate notation. The notations are described in Figure 3.1\nNote that despite the introduction of latent variables r\u0303d, cd, \u03c8d, and our per-document observed variables rd, \u03c3d, hd, ud, \u03bdd, the basic structure of LDA is maintained. How-\never, we can see that the topic distribution of each review is now dependent on the review\u2019s bias-corrected rating. This makes sense intuitively in that we expect more negative reviews to talk about different topics that wholly positive ones; as an example, negative reviews might tend to focus on poor product quality and customer service, with positive reviews focusing on product satisfaction and example use cases. The model also incorporates a Bernoulli review quality rating \u03c8d, which takes into account review helpfulness votes, unhelpfulness votes, and writing quality (out-of-vocabulary rate, punctuational correctness, average word length, etc.).\nCreating a model while simultaneously maintaining high sampling performance can be very challenging. To our knowledge, many pre-existing latent variable models overlook this issue albeit providing interesting results in accuracy in a laboratory setting for certain categories of reviews. In Section 4.3, we describe efficient sampling techniques for the RLDA model that build on existing LDA sampling methods."}, {"heading": "3.2. Model Updating", "text": "Using the Chital system, model updating follows naturally by performing sampling using the existing model with the new reviews added to the review set. In this way, if a lottery system is used, the number of lottery tickets awarded to the seller is fairly determined by the amount of computation required to update the model. To avoid convergence to poor optima, we recompute a product model after every few updates. This methodology allows for products to be quickly updated when new reviews become available while maintaining model quality via occasional full recomputes."}, {"heading": "3.3. Core Set Selection", "text": "To accommodate a variable number of topics, we first perform RLDA sampling with a fixed number of topics k. The number of topics can then be reduced to a smaller core\nset post-sampling by using techniques in (Feldman et al., 2011) combined with estimating the informativeness of the top words in each topic."}, {"heading": "3.4. Visualization", "text": "Given the limited screen space available on mobile devices, the interface is designed with simplicity in mind from the ground up. The initial screen is a simple search, with a singular entry box in which the user can query products. After submitting a product query, the user is provided with a list of products to select from in order to build a topic model.\nIn contrast to (Li et al., 2014c), the display of topics is returned to its core. We display each topic using its review in topic-document sorted order, however in contrast to Amazon\u2019s system we partition the visualization in a set of tabs, one for each topic. The user can the use an intuitive SeekBar to select topics. Once selected, a topic summary is displayed including topic weighted rating, topic weight, and the top k tokens of the topic listed as keywords. Above this topic summary is a a review ViewPager, which can be using to quickly pan through reviews in sorted order according to topic probability of the selected topic. Within the review text, each region of text corresponding to a keyword lemma is bolded in order to bring attention to regions of the review that are pertinent to the selected topic. In this way, I used can quickly glance at select regions of a review when learning about features of a product that they deem important. We defer more discussion of visualization to the case studies in which we provide examples of Quokka visualizations."}, {"heading": "4. Implementation Details", "text": "Below we briefly describe the implementation of Vedalia, with emphasis on performance-critical details and modeling."}, {"heading": "4.1. Architecture, Preprocessing, and Database", "text": "To accommodate the new system design we made substantial architecture change compared to the previous system (Li et al., 2014b). We dropped the integration with parameter-server-like computing clusters, but substantially increased the power of pre-processing clusters and databases. Furthermore, the model result selection system and intermediate-result push-update mechanism are no longer required, hence entirely replaced by Chital system.\nWe schedule large-scale batch review preprocessing task using Apache Spark (Zaharia et al., 2010) combined with Stanford CoreNLP (Manning et al., 2014) as soon as enough reviews are committed into the database, so to reduce waiting time and eliminate overhead during model\ncomputation. We deployed a 1-rack, multi-node Cassandra (Lakshman & Malik, 2010) database for storing and streaming product information, raw reviews, and analyzed reviews, so to achieve best performance in fault tolerance, consistency, and read-and-write at scale. An analyzed review is a review attached with pre-processing result in compressed binary format.\nTo facilitate searching we deployed a multi-node scalable search engine, ElasticSearch (elasticsearch, 2015), that operates alongside Cassandra, simultaneously indexes every product and every review inserted to the database. ElasticSearch complements Cassandra with its higher insertion performance and much more flexible query structure, while simultaneously taking benefit from Cassandra\u2019s high consistency and fault tolerance. Web servers are deployed in the front end which exposes APIs that process product and review queries and stream the result back to query initiators, while simultaneously provides network-level isolation and security guarantee.\nThis architecture effectively establishes a scalable system which is not sensitive to the number of users making queries, due to Chital\u2019s ability to offload computation to users themselves. Since reviews are preprocessed in batch at insertion as Spark tasks, large amount of workers are only allocated temporarily for a short period of time. Modern cloud computing platforms such as Google Compute Platform allow this to be done in an automated fashion, and charge for only the amount of time allocated on a perminute basis. To process the entire collection of 23 million reviews in SNAP dataset (Leskovec & Krevl, 2014), we used only up to 100 cores with 500GB of memory resources for 2 days."}, {"heading": "4.2. Model Views", "text": "To reduce bandwidth and protect models from outside use, we avoid sending the entire model to the end user. The initial model view is streamed to the user as a list of topic descriptions (id, probability, expected rating, expected helpfulness, expected unhelpfulness) and their associated top n words. As with the previous system, we defer sending review text to the end user until it is requested. This is of particular importance in a mobile setting, where many users will be using the app on a bandwidth-limited data plan. To improve user experience, reviews can be cached for offline viewing."}, {"heading": "4.3. Sampling", "text": "Sampling can be performed by following a procedure which transforms the auxiliary information along with other latent variables into word observation, then sample the transformed data in an LDA-like fashion, where an adaptation of SparseLDA (Yao et al., 2009) sampling is\nperformed in order to estimate model parameters. We define review score tier cd,t as:\ncd,1 := p(r\u0303d \u2264 1.5), cd,2 := p(r\u0303d \u2208 (1.5, 2.5]), cd,3 := p(r\u0303d \u2208 (2.5, 3.5]), cd,4 := p(r\u0303d \u2208 (3.5, 4.5]), cd,5 := p(r\u0303d > 4.5),\nAdditionally, we need to characterize the distribution of \u03c8d. We train a logistic regression model mapping {\u03bdd, ud, hd} \u2192 is relevant, where is relevant = 1 if the review is relevant to the product being reviewed, and 0 otherwise. As an example, one Amazon review for a Macbook Pro says \u201dThe product is good but I find that my neck is getting sore from using it.\u201d; the goal of the logistic model, then, is to label with review as not relevant. While the original intent was to train a model using data collected from Amazon Mechanical Turk, we later chose to hand-label a set of reviews in order to train our classifier as a means of cutting our implementation costs.\nSince the vast majority of Amazon reviews come from users who have only reviewed a single product, estimating the distribution of a general user\u2019s bias-corrected rating is often impossible. In order to reduce within-topic rating variability, for a general user we assume low rating variance and approximate the rating distribution by adding the review only for the given rating. This is achieved by appending \u201c rating\u201d to each token within a review, then stripping out the rating suffix when displaying keywords to the user. Notice that in making this approximation we simultaneously utilize the imposed independence assumption that \u03c8d \u22a5 cd|wd\u2217, as shown in the RLDA graphical model.\nApproximate weighting is performed by allocating the bottom wbits bits of review-topic and word-topic counts for fractional counts. What previously would correspond to a count increment of 1 is mapped to an increment of 2wbits+1. Fractional counts can then be approximated as an integerrounded fraction of 2wbits+1, providing us with 1\n2wbits+1\nprecision. Count sparsity can be imposed by reducing the value of wbits \u2013 all fractional counts below 12wbits+2 will be treated as a 0-count."}, {"heading": "5. Case Study", "text": "In order to evaluate the effectiveness of the system, we compare the new Vedalia system to the current Amazon system.\nFor our case study, we examine the use of the Quokka system for the iHome iH5 Clock Radio and Speaker System for iPod (ASIN B00080FO4O). At the time of modeling,\nthe product had 487 reviews with an average rating of approximate 3.5 stars. Upon submitting a query for this product, the user is presented with the output given in Figures 3 and 4. In Figure 3, we see that the topic keywords are\ngenerally positive, with the highlighting bringing attention to the iHome\u2019s ability to charge your phone\u2019s battery and the mild (but not substantial) disappointment in the brightness of the screen when trying to sleep. In Figure 4, we see more negative highlighted keywords, with emphasis on the product\u2019s sub-par build quality and unjustifiably high price.\nIn Figure 5, we see the review representation in the official Amazon app for Android. Aside from the increased effort required to simply navigate through individual reviews, the system has no way of drawing the user\u2019s attention to any specific region of reviews, leaving the user to dig through mounds of text in order to find the specific information he is looking for.\nFor the iHome product modeled above, the time until initial results appeared was approximately 5 seconds, with final results appearing in 15 seconds."}, {"heading": "6. Future Work", "text": "In the immediate future, we will be submitting a patch to MALLET (McCallum, 2002) in order to fix the broken ParallelTopicModel parallel implementation. Many users of MALLET will note that the library runs substantially slower when using the multithreaded implementation. This slowdown is due to the \u201cThread.sleep()\u201d calls in the inner loop of parameter estimation when using more than one thread, which appears to be a temporary hack used to correct for a concurrency bug (non-volatile thread-cached\nboolean value accessed from another thread). In designing our system, we corrected this bug and refactored the ParallelTopicModel and WorkerRunnable code in order to achieve substantially higher performance and fully utilize all cores in a multithreaded environment.\nRegarding future work on RLDA, we wish to continually improve the model\u2019s performance on products with a limited number of reviews. The availability of a hierarchical structure of products allows for more advanced models that utilize product categories and reviews of similar products in order to better estimate topics in low-review situations. We also will be pursuing the idea of computing a single model per group of related products in order to leverage similarities in topics and improve topic estimation.\nWe would like to further investigate the performance of RLDA under some classical metrics to validate its superior performance compared to standard LDA in the context of product review modeling. We also plan to implement and test Chital at a larger scale and refine our user interface over the next few months so as to begin field testing with real users. As previously discussed, we will be releasing the app to the Google Play Store shortly \u2013 as such, modeling performance, usability, and robustness in low-review situations are critical areas that require further optimization."}], "references": [{"title": "An unsupervised aspect-sentiment model for online reviews", "author": ["Brody", "Samuel", "Elhadad", "Noemie"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Brody et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brody et al\\.", "year": 2010}, {"title": "Scalable training of mixture models via coresets", "author": ["Feldman", "Dan", "Faulkner", "Matthew", "Krause", "Andreas"], "venue": "In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Feldman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2011}, {"title": "Aspect and sentiment unification model for online review analysis", "author": ["Jo", "Yohan", "Oh", "Alice H"], "venue": "In Proceedings of the Forth International Conference on Web Search and Web Data", "citeRegEx": "Jo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jo et al\\.", "year": 2011}, {"title": "An optimal algorithm for online bipartite matching", "author": ["Karp", "Vazirani"], "venue": "In STOC: ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Karp et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Karp et al\\.", "year": 1990}, {"title": "Cassandra: A decentralized structured storage system", "author": ["Lakshman", "Avinash", "Malik", "Prashant"], "venue": "SIGOPS Oper. Syst. Rev.,", "citeRegEx": "Lakshman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lakshman et al\\.", "year": 2010}, {"title": "Snap large network dataset collection", "author": ["Leskovec", "Jure", "Krevl", "Andrej"], "venue": null, "citeRegEx": "Leskovec et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2014}, {"title": "Multi-gpu distributed parallel bayesian differential topic", "author": ["Li", "Aaron Q"], "venue": null, "citeRegEx": "Li and Q.,? \\Q2012\\E", "shortCiteRegEx": "Li and Q.", "year": 2012}, {"title": "Reducing sampling complexity of topic models, 2014a", "author": ["Li", "Aaron Q", "Ahmed", "Amr", "Ravi", "Sujith", "Smola", "Alexander J"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Creating scalable and interactive web applications using high performance latent variable models", "author": ["Li", "Aaron Q", "Deng", "Yuntian", "Jing", "Kublai", "Robinson", "Joseph W"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Creating scalable and interactive web applications using high performance latent variable models, 2014c", "author": ["Li", "Aaron Q", "Robinson", "Joseph W", "Deng", "Yuntian", "Jing", "Kublai"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Parameter server for distributed machine", "author": ["Li", "Mu", "Zhou", "Yang", "Zichao", "Aaron Q", "Xia", "Fei", "Anderson", "David G", "Smola", "Alexander J"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Kachites. Mallet: A machine learning for language toolkit", "author": ["McCallum", "Andrew"], "venue": null, "citeRegEx": "McCallum and Andrew,? \\Q2002\\E", "shortCiteRegEx": "McCallum and Andrew", "year": 2002}, {"title": "Online matching and ad allocation", "author": ["Mehta", "Aranyak"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Mehta and Aranyak.,? \\Q2013\\E", "shortCiteRegEx": "Mehta and Aranyak.", "year": 2013}, {"title": "Scalable computation marketplace for latent variable modeling on mobile", "author": ["Robinson", "Joseph W", "Li", "Aaron Q"], "venue": null, "citeRegEx": "Robinson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Robinson et al\\.", "year": 2015}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["Titov", "Ivan", "McDonald", "Ryan T"], "venue": "In Proceedings of the 17th International Conference on World Wide Web,", "citeRegEx": "Titov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2008}, {"title": "Efficient methods for topic model inference on streaming document collections", "author": ["Yao", "Limin", "Mimno", "David", "McCallum", "Andrew"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2009}, {"title": "Spark:cluster computing working", "author": ["Zaharia", "Matei", "Chowdhury", "Mosharaf", "Franklin", "Michael J", "Shenker", "Scott", "Stoica", "Ion"], "venue": null, "citeRegEx": "Zaharia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zaharia et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": ", 2003) for the review space by incorporating auxiliary data available in online reviews to improve modeling while simultaneously remaining compatible with preexisting fast sampling techniques such as (Yao et al., 2009; Li et al., 2014a) to achieve high performance.", "startOffset": 201, "endOffset": 237}, {"referenceID": 10, "context": "and Parameter Server (Li et al., 2013; 2015) has resulted in substantial speed improvements for large scale systems, much less has been done for a more hardware-constrained setting such as smartphones running Android or iOS.", "startOffset": 21, "endOffset": 44}, {"referenceID": 15, "context": "The Dirichlet-multinomial design in this model makes it simple to do inference due to distribution conjugacy \u2013 we can integrate out the multinomial parameters \u03b8d and \u03c8k, thus allowing one to express p(w, z|\u03b1, \u03b2, nd) in a closedform (Yao et al., 2009).", "startOffset": 232, "endOffset": 250}, {"referenceID": 15, "context": "However, there are many approaches for substantially accelerating sampling speed by exploiting the topic sparsity to reduce time complexity to O(kd + kw) (Yao et al., 2009) and further to O(kd) (Li et al.", "startOffset": 154, "endOffset": 172}, {"referenceID": 3, "context": "Although online graph matching especially online bipartite matching is a well-studied major research area (Karp et al., 1990; Mehta, 2013), our problem setup makes it difficult to apply any existing algorithm for two reasons: our problem introduces an extra \u201dtime dimension\u201d, and our objective is to maximize overall user gain thereby to convince them joining the system voluntarily.", "startOffset": 106, "endOffset": 138}, {"referenceID": 1, "context": "The number of topics can then be reduced to a smaller core set post-sampling by using techniques in (Feldman et al., 2011) combined with estimating the informativeness of the top words in each topic.", "startOffset": 100, "endOffset": 122}, {"referenceID": 16, "context": "We schedule large-scale batch review preprocessing task using Apache Spark (Zaharia et al., 2010) combined with Stanford CoreNLP (Manning et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 15, "context": "Sampling can be performed by following a procedure which transforms the auxiliary information along with other latent variables into word observation, then sample the transformed data in an LDA-like fashion, where an adaptation of SparseLDA (Yao et al., 2009) sampling is performed in order to estimate model parameters.", "startOffset": 241, "endOffset": 259}], "year": 2015, "abstractText": "In this project we outline Vedalia, a high performance distributed network for performing inference on latent variable models in the context of Amazon review visualization. We introduce a new model, RLDA, which extends Latent Dirichlet Allocation (LDA) (Blei et al., 2003) for the review space by incorporating auxiliary data available in online reviews to improve modeling while simultaneously remaining compatible with preexisting fast sampling techniques such as (Yao et al., 2009; Li et al., 2014a) to achieve high performance. The network is designed such that computation is efficiently offloaded to the client devices using the Chital system (Robinson & Li, 2015), improving response times and reducing server costs. The resulting system is able to rapidly compute a large number of specialized latent variable models while requiring minimal server resources.", "creator": "LaTeX with hyperref package"}}}