{"id": "1510.07526", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2015", "title": "Empirical Study on Deep Learning Models for Question Answering", "abstract": "In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.", "histories": [["v1", "Mon, 26 Oct 2015 16:03:27 GMT  (215kb,D)", "http://arxiv.org/abs/1510.07526v1", null], ["v2", "Tue, 27 Oct 2015 16:56:48 GMT  (215kb,D)", "http://arxiv.org/abs/1510.07526v2", null], ["v3", "Fri, 20 Nov 2015 15:36:56 GMT  (231kb,D)", "http://arxiv.org/abs/1510.07526v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["yang yu", "wei zhang", "chung-wei hang", "bing xiang", "bowen zhou"], "accepted": false, "id": "1510.07526"}, "pdf": {"name": "1510.07526.pdf", "metadata": {"source": "CRF", "title": "Empirical Study on Deep Learning Models for QA", "authors": ["Yang Yu", "Wei Zhang", "Chung-Wei Hang", "Bowen Zhou"], "emails": ["zhou}@us.ibm.com"], "sections": [{"heading": null, "text": "In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14]. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem."}, {"heading": "1 Introduction", "text": "Question Answering (QA) is a natural language processing (NLP) task that requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question [6]. There are many different approaches to QA: constructing NLP pipeline where each component is separately trained and then assembled [4], building large knowledge bases (KBs) [2] and reasoning with facts therein, and machine \u201creading\u201d approach to comprehend question and documents [6] where answers are contained. Recently, various deep learning (DL) models are proposed for different learning problems. DL models are usually differentiable from end to end through gradient descent. They require neither any hand craft features nor separately tuned components. Thus we think it is important to study these models on addressing QA problem.\nImplicitly or explicitly, solving QA problem can be divided into two steps. The first step locates the information that is relevant to the question, e.g. sentences in a text document or facts in a knowledge graph. We call it the \u201csearch step.\u201d The second step which we call \u201cgeneration step\u201d, extracts or generates answer from the relevant pieces of information detected in the search step. This paper focuses on reading comprehension type QA, where search and generation sometimes are coupled.\nWe focus on Neural Machine Translation (NMT), Memory Network (MemNN), and Neural Turing Machine (NTM) models, as they have representative state-of-the-art DL model architectures in categories they fall in. We conducted empirical studies in this work to better understand the strength and places to improve in each model on solving QA and experiment settings follow the 2-step QA framework. We will briefly describe the NMT, NTM and MemNN in the next section."}, {"heading": "2 Deep Learning Models for Question Answering", "text": "MemNN MemNNs have been applied to QA [15, 11] and have shown promising results with different input transformation or model changes. Its strength mainly lies in reasoning with inference components combined with a long-term memory component and learning how to use these jointly. A general MemNN has four modules: input which converts the incoming input to the internal feature representation, output which produces a new output, generalize which updates old memories given the new input, and response which converts the output into the response format desired. The memory network described in [15] memorizes each fact in a memory slot and uses supporting facts for\nar X\niv :1\n51 0.\n07 52\n6v 1\n[ cs\n.C L\n] 2\n6 O\nct 2\n01 5\na given question labeled in the training data to learn to search facts. Sainbayar et al. [11] described another version of the MemNN that can be trained end-to-end without knowing the supporting facts.\nNMT Using Machine Translation (MT) technique for QA is not new, as generating answer by given question can be regarded as generating target text by given source text, or in other words, the answer is a translation of the question. Several previous works have used translation models to determine answers [3, 12]. NMT brings new approaches to machine translation, for example two recurrent neural network (RNN) models are proposed [7, 13]. Following previous success of applying MT for QA, we think it is important to study if NMT could further help QA. As to the best of our knowledge, no one has done this study yet. Traditional translation systems usually are phrase-based where small sub-components are tuned separately (e.g., Koehn et al. [8]). NMT improves over phrase-based systems by using a neural network to encode a sentence in source language and decode it into a sentence in target language, which is a end-to-end system. However a main constraint of this encoder-decoder approach is that the model needs to compress all information of a source sentence into a fixed-length vector, which is difficult for long sentences or passages for reading comprehension style QA. In order to address this issue, Bahdanau et al. [1] introduced an extension to the encoder-decoder model which uses bi-directional RNN and learns to align and translate jointly. The NMT model we use is shown in the Figure 1a. The input includes passage and question and they are delimited by a marker. From the figure, we see the two RNNs read word by word in the input of different directions. Each time when the model generates an answer word, it searches for multiple positions in the passage where the most relevant information is concentrated. The model then predicts an answer word based on the context vectors associated with these positions and all the previous generated answer words. Formally, the i-th answer word yi (equation 1) is conditioned on the words in answer before word yi and the passage x. In RNN, the conditional probability is modeled as a nonlinear function g() which depends on previous answer word yi\u22121, the RNN hidden state si for time i and the context ci. The context vector ci is a weighted sum of a sequence of annotations. Each annotation has information about the complete passage with a focus on the parts around the i-th word.\np(yi|y1, ..., yi\u22121,x) = g(yi\u22121, si, ci) = g(yi\u22121, f(si\u22121, yi\u22121, ci), ci) (1)\nNTM NTM [5] resembles Turing machines in that it could learn arbitrary procedure in theory. As we believe that QA problem can be solved with (probably sophisticated) programs, in this paper we would examine how well NTM performs on reading comprehension tasks. NTMs are essentially RNNs, which in turn are Turing-complete [10] and capable of encoding any computer program in theory, yet not always practical. NTM is end-to-end trainable with gradient descent, due to the fact that every component is differentiable that is enabled by converting the hard read and write into \u2018blurry\u2019 operations that interact to greater or lesser degree. The read and write heads, as well as the memory component are recurrently updated through time, no matter if the controller is recurrent or not. This paper is the first to examine Neural Turing Machines on QA problems. Our implementation of NTM (see Figure 1b) internally uses a single-layer LSTM network as controller. NTM inputs are word distributed representations [9]. Word embedding is directed to LSTM controller within NTM, and the output of NTM is generated from softmax layer where each bit of output corresponds to an answer. In doing so we regard the QA problem as a multi-class (multi-word) classification problem\ngiven that the number of answers is finite and small. When multiple answers are needed, top n words with top n probabilities will be selected."}, {"heading": "3 Experiments", "text": "The AI-Complete data [14] is a synthetic dataset designed for a set of AI tasks. For each task, small and large training sets have 1k and 10k questions, respectively, and test set has 1k questions. The supervision in the training set is given by the true answers to questions, and the set of supporting facts for answering a given question. All tasks are so clean that a human could achieve 100% accuracy, as they are generated with a simulation. In this data, every statement in a passage has a number ID and each question is associated with the true answer and the IDs of supporting facts.\nMemNN [15] has shown promising results on AI-Complete data. So first we want to learn how different MemNN components contribute to it. We choose the MemNN with adaptive memory [14] as baseline (column a). We divide it into two virtual steps as the 2-step QA framework. We believe searching supporting facts (MemNN-S) is very important to MemNN while the MemNN response module (MemNN-R) has more room to improve, because it does not have any attention mechanism to focus on words in retrieved facts. We train MemNN-S by using complete passage as input and supporting facts as prediction and train MemNN-R by using true supporting facts as input and final answer as prediction. Column b,c in Table 1 confirm our hypothesis that the accuracy of searching supporting facts from MemNN-S is much better than the accuracy of predicting final answer words from MemNN-R.\nFollowing our findings in the first experiment, we want to see if NMT or NTM could do better than MemNN-R. Using the same setting as testing MemNN-R, both NMT and NTM show almost perfect results (column d,e). As we analyzed, we think the main reason is that NMT\u2019s attention machanism and NTM\u2019s memory component help. Then we wonder if the supporting facts input is not perfect, for example if the search step can only mark some facts as supporting facts with high probability. Therefore in this experiment, we input the complete passage including non-supporting facts, and use markers to annotate the begin and end of the supporting facts in the passage. Including nonsupporting facts in input brings noise and it requires the model to memorize relevant information and ignore non-relevant ones. From the results in group (iii) we see that NMT and NTM both perform very good with just a little expected drop from using supporting facts only. NTM is shown to be better in that NTM\u2019s explicit memory component and content addressing mechanism could directly target the relevant information stored.\nAlthough NMT and NTM showed good capability of solving the generation step in experiments above. supporting facts still need to be identified before the models could be applied. As we analyzed above, MemNN-S is good at searching supporting facts. Thus, we use MemNN-S to generate facts for those models and then apply NMT based on its fact-searching results. We see this combination (column h) improves the average performance over baseline (column a). It proves that this novel\narchitectural combination fusees the advantage of each, i.e. the memory component and attention mechanism.\nOne major advantage of applying DL models to solve QA is that they can be learned/tuned end-toend. Therefore the combination of two models through separate training may be less advantageous than single model that has both key elements. As we analyzed, NMT\u2019s architecture is essentially an attention mechanism over bidirectional RNN which has some memory functionality. Thus we wonder how NMT single model would perform compared to architectural combination shown above. In this group of experiments, we use NMT model for end-to-end QA without the medium step of finding supporting facts to compare with previous architectural combination. That being said, supporting facts are not marked up in both training and testing which reduced a lot information that model can use to learn. So it requires that attention on supporting facts need to be learned as an implicit byproduct by the NMT attention mechanism. We run NMT on both small and large training sets. The experiment results (column i,j) show that it got 72% without any tuning or specialization on QA problem. This result on small training set is within a reasonable gap to previous architectural combination, considering the model does not use supporting facts at all. Furthermore, when training data is sufficient, the accuracy is even comparable to the state-of-the-art accuracy (92%) from [11] where specialized features are added and tuned specifically for this data. In sum, we think the NMT has potential to address certain QA problem, as it has both memory supported in RNN and the attention over the memory."}, {"heading": "4 Conclusions", "text": "We studied several state-of-the-art DL models proposed for different learning tasks on solving QA problem. Experiment results suggest that a good agent need to remember and forget some facts when necessary and external memory may be a choice. We are also convinced that to generate answer, appropriate attention mechanism can do well. Therefore, we believe a DL model combining memory and attention mechanism great potential on handling QA problem."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1506.02075", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Question answering passage retrieval using dependency relations", "author": ["H. Cui", "R. Sun", "K. Li", "M.-Y. Kan", "T.-S. Chua"], "venue": "Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 400\u2013407, New York, NY, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Building watson: An overview of the deepqa project", "author": ["D.A. Ferrucci", "E.W. Brown", "J. Chu-Carroll", "J. Fan", "D. Gondek", "A. Kalyanpur", "A. Lally", "J.W. Murdock", "E. Nyberg", "J.M. Prager", "N. Schlaefer", "C.A. Welty"], "venue": "AI Magazine, 31(3):59\u201379", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "CoRR, abs/1506.03340", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "EMNLP", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 48\u201354, Stroudsburg, PA, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "On the computational power of neural nets", "author": ["H.T. Siegelmann", "E.D. Sontag"], "venue": "Journal of Computer and System Sciences, 50(1):132\u2013150", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "arXiv preprint arXiv:1503.08895", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to rank answers to non-factoid questions from web collections", "author": ["M. Surdeanu", "M. Ciaramita", "H. Zaragoza"], "venue": "Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V.V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint arXiv:1502.05698", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14].", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14].", "startOffset": 91, "endOffset": 94}, {"referenceID": 14, "context": "We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14].", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "Question Answering (QA) is a natural language processing (NLP) task that requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "There are many different approaches to QA: constructing NLP pipeline where each component is separately trained and then assembled [4], building large knowledge bases (KBs) [2] and reasoning with facts therein, and machine \u201creading\u201d approach to comprehend question and documents [6] where answers are contained.", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "There are many different approaches to QA: constructing NLP pipeline where each component is separately trained and then assembled [4], building large knowledge bases (KBs) [2] and reasoning with facts therein, and machine \u201creading\u201d approach to comprehend question and documents [6] where answers are contained.", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "There are many different approaches to QA: constructing NLP pipeline where each component is separately trained and then assembled [4], building large knowledge bases (KBs) [2] and reasoning with facts therein, and machine \u201creading\u201d approach to comprehend question and documents [6] where answers are contained.", "startOffset": 279, "endOffset": 282}, {"referenceID": 14, "context": "MemNN MemNNs have been applied to QA [15, 11] and have shown promising results with different input transformation or model changes.", "startOffset": 37, "endOffset": 45}, {"referenceID": 10, "context": "MemNN MemNNs have been applied to QA [15, 11] and have shown promising results with different input transformation or model changes.", "startOffset": 37, "endOffset": 45}, {"referenceID": 14, "context": "The memory network described in [15] memorizes each fact in a memory slot and uses supporting facts for", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "[11] described another version of the MemNN that can be trained end-to-end without knowing the supporting facts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Several previous works have used translation models to determine answers [3, 12].", "startOffset": 73, "endOffset": 80}, {"referenceID": 11, "context": "Several previous works have used translation models to determine answers [3, 12].", "startOffset": 73, "endOffset": 80}, {"referenceID": 6, "context": "NMT brings new approaches to machine translation, for example two recurrent neural network (RNN) models are proposed [7, 13].", "startOffset": 117, "endOffset": 124}, {"referenceID": 12, "context": "NMT brings new approaches to machine translation, for example two recurrent neural network (RNN) models are proposed [7, 13].", "startOffset": 117, "endOffset": 124}, {"referenceID": 7, "context": "[8]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] introduced an extension to the encoder-decoder model which uses bi-directional RNN and learns to align and translate jointly.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "NTM NTM [5] resembles Turing machines in that it could learn arbitrary procedure in theory.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "NTMs are essentially RNNs, which in turn are Turing-complete [10] and capable of encoding any computer program in theory, yet not always practical.", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "NTM inputs are word distributed representations [9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 13, "context": "The AI-Complete data [14] is a synthetic dataset designed for a set of AI tasks.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "MemNN [15] has shown promising results on AI-Complete data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "We choose the MemNN with adaptive memory [14] as baseline (column a).", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "Furthermore, when training data is sufficient, the accuracy is even comparable to the state-of-the-art accuracy (92%) from [11] where specialized features are added and tuned specifically for this data.", "startOffset": 123, "endOffset": 127}], "year": 2015, "abstractText": "In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation [1], Neural Turing Machine [5], and Memory Networks [15] for a simulated QA data set [14]. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.", "creator": "LaTeX with hyperref package"}}}