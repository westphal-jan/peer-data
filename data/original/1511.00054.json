{"id": "1511.00054", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2015", "title": "Gaussian Process Random Fields", "abstract": "Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.", "histories": [["v1", "Sat, 31 Oct 2015 01:02:14 GMT  (3253kb,D)", "http://arxiv.org/abs/1511.00054v1", "Advances in Neural Information Processing Systems (NIPS), 2015"]], "COMMENTS": "Advances in Neural Information Processing Systems (NIPS), 2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["david a moore", "stuart j russell"], "accepted": true, "id": "1511.00054"}, "pdf": {"name": "1511.00054.pdf", "metadata": {"source": "CRF", "title": "Gaussian Process Random Fields", "authors": ["David A. Moore"], "emails": ["russell}@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location."}, {"heading": "1 Introduction", "text": "Many machine learning tasks can be framed as learning a function given noisy information about its inputs and outputs. In regression and classification, we are given inputs and asked to predict the outputs; by contrast, in latent variable modeling we are given a set of outputs and asked to reconstruct the inputs that could have produced them. Gaussian processes (GPs) are a flexible class of probability distributions on functions that allow us to approach function-learning problems from an appealingly principled and clean Bayesian perspective. Unfortunately, the time complexity of exact GP inference is O(n3), where n is the number of data points. This makes exact GP calculations infeasible for real-world data sets with n > 10000.\nMany approximations have been proposed to escape this limitation. One particularly simple approximation is to partition the input space into smaller blocks, replacing a single large GP with a multitude of local ones. This gains tractability at the price of a potentially severe independence assumption.\nIn this paper we relax the strong independence assumptions of independent local GPs, proposing instead a Markov random field (MRF) of local GPs, which we call a Gaussian Process Random Field (GPRF). A GPRF couples local models via pairwise potentials that incorporate covariance information. This yields a surrogate for the full GP marginal likelihood that is simple to implement and can be tractably evaluated and optimized on large datasets, while still enforcing a smooth covariance structure. The task of approximating the marginal likelihood is motivated by unsupervised applications such as the GP latent variable model [1], but examining the predictions made by our model also yields a novel interpretation of the Bayesian Committee Machine [2].\nWe begin by reviewing GPs and MRFs, and some existing approximation methods for large-scale GPs. In Section 3 we present the GPRF objective and examine its properties as an approximation to the full GP marginal likelihood. We then evaluate it on synthetic data as well as an application to seismic event location.\nar X\niv :1\n51 1.\n00 05\n4v 1\n[ cs\n.L G\n] 3\n1 O"}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Gaussian processes", "text": "Gaussian processes [3] are distributions on real-valued functions. GPs are parameterized by a mean function \u00b5\u03b8(x), typically assumed without loss of generality to be \u00b5(x) = 0, and a covariance function (sometimes called a kernel) k\u03b8(x,x\u2032), with hyperparameters \u03b8. A common choice is the squared exponential covariance, kSE(x,x\u2032) = \u03c32f exp ( \u2212 12\u2016x\u2212 x \u2032\u20162/`2 ) , with hyperparameters \u03c32f and ` specifying respectively a prior variance and correlation lengthscale.\nWe say that a random function f(x) is Gaussian process distributed if, for any n input points X , the vector of function values f = f(X) is multivariate Gaussian, f \u223c N (0, k\u03b8(X,X)). In many applications we have access only to noisy observations y = f + \u03b5 for some noise process \u03b5. If the noise is iid Gaussian, i.e., \u03b5 \u223c N (0, \u03c32nI), then the observations are themselves Gaussian, y \u223c N (0,Ky), where Ky = k\u03b8(X,X) + \u03c32nI. The most common application of GPs is to Bayesian regression [3], in which we attempt to predict the function values f\u2217 at test points X\u2217 via the conditional distribution given the training data, p(f\u2217|y;X,X\u2217, \u03b8). Sometimes, however, we do not observe the training inputsX , or we observe them only partially or noisily. This setting is known as the Gaussian Process Latent Variable Model (GPLVM) [1]; it uses GPs as a model for unsupervised learning and nonlinear dimensionality reduction. The GP-LVM setting typically involves multi-dimensional observations, Y = (y(1), . . . ,y(D)), with each output dimension y(d) modeled as an independent Gaussian process. The input locations and/or hyperparameters are typically sought via maximization of the marginal likelihood\nL(X, \u03b8) = log p(Y ;X, \u03b8) = D\u2211 i=1 \u22121 2 log |Ky| \u2212 1 2 yTi K \u22121 y yi + C\n= \u2212D 2 log |Ky| \u2212 1 2 tr(K\u22121y Y Y T ) + C, (1)\nthough some recent work [4, 5] attempts to recover an approximate posterior on X by maximizing a variational bound. Given a differentiable covariance function, this maximization is typically performed by gradient-based methods, although local maxima can be a significant concern as the marginal likelihood is generally non-convex."}, {"heading": "2.2 Scalability and approximate inference", "text": "The main computational difficulty in GP methods is the need to invert or factor the kernel matrix Ky , which requires time cubic in n. In GP-LVM inference this must be done at every optimization step to evaluate (1) and its derivatives.\nThis complexity has inspired a number of approximations. The most commonly studied are inducingpoint methods, in which the unknown function is represented by its values at a set of m inducing points, where m n. These points can be chosen by maximizing the marginal likelihood in a surrogate model [6, 7] or by minimizing the KL divergence between the approximate and exact GP posteriors [8]. Inference in such models can typically be done in O(nm2) time, but this comes at the price of reduced representational capacity: while smooth functions with long lengthscales may be compactly represented by a small number of inducing points, for quickly-varying functions with\nsignificant local structure it may be difficult to find any faithful representation more compact than the complete set of training observations.\nA separate class of approximations, so-called \u201clocal\u201d GP methods [3, 9, 10], involves partitioning the inputs into blocks of m points each, then modeling each block with an independent Gaussian process. If the partition is spatially local, this corresponds to a covariance function that imposes independence between function values in different regions of the input space. Computationally, each block requires only O(m3) time; the total time is linear in the number of blocks. Local approximations preserve short-lengthscale structure within each block, but their harsh independence assumptions can lead to predictive discontinuities and inaccurate uncertainties (Figure 1b). These assumptions are problematic for GP-LVM inference because the marginal likelihood becomes discontinuous at block boundaries. Nonetheless, local GPs sometimes work very well in practice, achieving results comparable to more sophisticated methods in a fraction of the time [11].\nThe Bayesian Committee Machine (BCM) [2] attempts to improve on independent local GPs by averaging the predictions of multiple GP experts. The model is formally equivalent to an inducingpoint model in which the test points are the inducing points, i.e., it assumes that the training blocks are conditionally independent given the test data. The BCM can yield high-quality predictions that avoid the pitfalls of local GPs (Figure 1c), while maintaining scalability to very large datasets [12]. However, as a purely predictive approximation, it is unhelpful in the GP-LVM setting, where we are interested in the likelihood of our training set irrespective of any particular test data. The desire for a BCM-style approximation to the marginal likelihood was part of the motivation for this current work; in Section 3.2 we show that the GPRF proposed in this paper can be viewed as such a model.\nMixture-of-experts models [13, 14] extend the local GP concept in a different direction: instead of deterministically assigning points to GP models based on their spatial locations, they treat the assignments as unobserved random variables and do inference over them. This allows the model to adapt to different functional characteristics in different regions of the space, at the price of a more difficult inference task. We are not aware of mixture-of-experts models being applied in the GP-LVM setting, though this should in principle be possible.\nSimple building blocks are often combined to create more complex approximations. The PIC approximation [15] blends a global inducing-point model with local block-diagonal covariances, thus capturing a mix of global and local structure, though with the same boundary discontinuities as in \u201cvanilla\u201d local GPs. A related approach is the use of covariance functions with compact support [16] to capture local variation in concert with global inducing points. [11] surveys and compares several approximate GP regression methods on synthetic and real-world datasets.\nFinally, we note here the similar title of [17], which is in fact orthogonal to the present work: they use a random field as a prior on input locations, whereas this paper defines a random field decomposition of the GP model itself, which may be combined with any prior on X ."}, {"heading": "2.3 Markov Random Fields", "text": "We recall some basic theory regarding Markov random fields (MRFs), also known as undirected graphical models [18]. A pairwise MRF consists of an undirected graph (V,E), along with node potentials \u03c8i and edge potentials \u03c8ij , which define an energy function on a random vector y,\nE(y) = \u2211 i\u2208V \u03c8i(yi) + \u2211 (i,j)\u2208E \u03c8ij(yi,yj), (2)\nwhere y is partitioned into components yi identified with nodes in the graph. This energy in turn defines a probability density, the \u201cGibbs distribution\u201d, given by p(y) = 1Z exp(\u2212E(y)) where Z = \u222b exp(\u2212E(z))dz is a normalizing constant.\nGaussian random fields are the special case of pairwise MRFs in which the Gibbs distribution is multivariate Gaussian. Given a partition of y into sub-vectors y1,y2, . . . ,yM , a zero-mean Gaussian distribution with covariance K and precision matrix J = K\u22121 can be expressed by potentials\n\u03c8i(yi) = \u2212 1\n2 yTi Jiiyi, \u03c8ij(yi,yj) = \u2212yTi Jijyj (3)\nwhere Jij is the submatrix of J corresponding to the sub-vectors yi, yj . The normalizing constant Z = (2\u03c0)n/2|K|1/2 involves the determinant of the covariance matrix. Since edges whose potentials\nare zero can be dropped without effect, the nonzero entries of the precision matrix can be seen as specifying the edges present in the graph."}, {"heading": "3 Gaussian Process Random Fields", "text": "We consider a vector of n real-valued1 observations y \u223c N (0,Ky) modeled by a GP, where Ky is implicitly a function of input locations X and hyperparameters \u03b8. Unless otherwise specified, all probabilities p(yi), p(yi,yj), etc., refer to marginals of this full GP. We would like to perform gradient-based optimization on the marginal likelihood (1) with respect to X and/or \u03b8, but suppose that the cost of doing so directly is prohibitive.\nIn order to proceed, we assume a partition y = (y1,y2, . . . ,yM ) of the observations into M blocks of size at most m, with an implied corresponding partition X = (X1, X2, . . . , XM ) of the (perhaps unobserved) inputs. The source of this partition is not a focus of the current work; we might imagine that the blocks correspond to spatially local clusters of input points, assuming that we have noisy observations of the X values or at least a reasonable guess at an initialization. We let Kij = cov\u03b8(yi,yj) denote the appropriate submatrix of Ky, and Jij denote the corresponding submatrix of the precision matrix Jy = K\u22121y ; note that Jij 6= (Kij)\u22121 in general."}, {"heading": "3.1 The GPRF Objective", "text": "Given the precision matrix Jy, we could use (3) to represent the full GP distribution in factored form as an MRF. This is not directly useful, since computing Jy requires cubic time. Instead we propose approximating the marginal likelihood via a random field in which local GPs are connected by pairwise potentials. Given an edge set which we will initially take to be the complete graph, E = {(i, j)|1 \u2264 i < j \u2264M}, our approximate objective is\nqGPRF (y;X, \u03b8) = M\u220f i=1 p(yi) \u220f (i,j)\u2208E p(yi,yj) p(yi)p(yj) , (4)\n= M\u220f i=1 p(yi) 1\u2212|Ei| \u220f (i,j)\u2208E p(yi,yj)\nwhere Ei denotes the neighbors of i in the graph, and p(yi) and p(yi,yj) are marginal probabilities under the full GP; equivalently they are the likelihoods of local GPs defined on the points Xi and Xi \u222aXj respectively. Note that these local likelihoods depend implicitly on X and \u03b8. Taking the log, we obtain the energy function of an unnormalized MRF\nlog qGPRF (y;X, \u03b8) = M\u2211 i=1 (1\u2212 |Ei|) log p(yi) + \u2211 (i,j)\u2208E log p(yi,yj) (5)\nwith potentials\n\u03c8GPRFi (yi) = (1\u2212 |Ei|) log p(yi), \u03c8GPRFij (yi,yj) = log p(yi,yj). (6)\nWe refer to the approximate objective (5) as qGPRF rather than pGPRF to emphasize that it is not in general a normalized probability density. It can be interpreted as a \u201cBethe-type\u201d approximation [19], in which a joint density is approximated via overlapping pairwise marginals. In the special case that the full precision matrix Jy induces a tree structure on the blocks of our partition, qGPRF recovers the exact marginal likelihood. (This is shown in the supplementary material.) In general this will not be the case, but in the spirit of loopy belief propagation [20], we consider the tree-structured case as an approximation for the general setting.\nBefore further analyzing the nature of the approximation, we first observe that as a sum of local Gaussian log-densities, the objective (5) is straightforward to implement and fast to evaluate. Each of the O(M2) pairwise densities requires O((2m)3) = O(m3) time, for an overall complexity of\n1The extension to multiple independent outputs is straightforward.\nO(M2m3) = O(n2m) when M = n/m. The quadratic dependence on n cannot be avoided by any algorithm that computes similarities between all pairs of training points; however, in practice we will consider \u201clocal\u201d modifications in which E is something smaller than all pairs of blocks. For example, if each block is connected only to a fixed number of spatial neighbors, the complexity reduces to O(nm2), i.e., linear in n. In the special case where E is the empty set, we recover the exact likelihood of independent local GPs.\nIt is also straightforward to obtain the gradient of (5) with respect to hyperparameters \u03b8 and inputs X , by summing the gradients of the local densities. The likelihood and gradient for each term in the sum can be evaluated independently using only local subsets of the training data, enabling a simple parallel implementation.\nHaving seen that qGPRF can be optimized efficiently, it remains for us to argue its validity as a proxy for the full GP marginal likelihood. Due to space constraints we defer proofs to the supplementary material, though our results are not difficult. We first show that, like the full marginal likelihood (1), qGPRF has the form of a Gaussian distribution, but with a different precision matrix.\nTheorem 1. The objective qGPRF has the form of an unnormalized Gaussian density with precision matrix J\u0303 , with blocks J\u0303ij given by\nJ\u0303ii = K \u22121 ii + \u2211 j\u2208Ei ( Q (ij) 11 \u2212K \u22121 ii ) , J\u0303ij = { Q (ij) 12 if (i, j) \u2208 E 0 otherwise. ) , (7)\nwhere Q(ij) is the local precision matrix Q(ij) defined as the inverse of the marginal covariance,\nQ(ij) =\n( Q\n(ij) 11 Q (ij) 12\nQ (ij) 21 Q (ij) 22\n) = ( Kii Kij Kji Kjj )\u22121 .\nAlthough the Gaussian density represented by qGPRF is not in general normalized, we show that it is approximately normalized in a certain sense.\nTheorem 2. The objective qGPRF is approximately normalized in the sense that the optimal value of the Bethe free energy [19], FB(b) = \u2211 i\u2208V (\u222b yi bi(yi) (1\u2212 |Ei|) ln bi(yi) ln\u03c8i(yi) ) + \u2211 (i,j)\u2208E (\u222b yi,yj bij(yi,yj) ln bij(yi,yj) \u03c8ij(yi,yj)) ) \u2248 logZ, (8) the approximation to the normalizing constant found by loopy belief propagation, is precisely zero. Furthermore, this optimum is obtained when the pseudomarginals bi, bij are taken to be the true GP marginals pi, pij .\nThis implies that loopy belief propagation run on a GPRF would recover the marginals of the true GP."}, {"heading": "3.2 Predictive equivalence to the BCM", "text": "We have introduced qGPRF as a surrogate model for the training set (X,y); however, it is natural to extend the GPRF to make predictions at a set of test points X\u2217, by including the function values f\u2217 = f(X\u2217) as anM+1st block, with an edge to each of the training blocks. The resulting predictive distribution,\npGPRF (f \u2217|y) \u221d qGPRF (f\u2217,y) = p(f\u2217) M\u220f i=1 p(yi, f \u2217) p(yi)p(f\u2217)  M\u220f i=1 p(yi) \u220f (i,j)\u2208E p(yi,yj) p(yi)p(yj)  \u221d p(f\u2217)1\u2212M\nM\u220f i=1 p(f\u2217|yi), (9)\ncorresponds exactly to the prediction of the Bayesian Committee Machine (BCM) [2]. This motivates the GPRF as a natural extension of the BCM as a model for the training set, providing an alternative to\nthe standard transductive interpretation of the BCM.2 A similar derivation shows that the conditional distribution of any block yi given all other blocks yj 6=i also takes the form of a BCM prediction, suggesting the possibility of pseudolikelihood training [21], i.e., directly optimizing the quality of BCM predictions on held-out blocks (not explored in this paper)."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Uniform Input Distribution", "text": "We first consider a 2D synthetic dataset intended to simulate spatial location tasks such as WiFiSLAM [22] or seismic event location (below), in which we observe high-dimensional measurements but have only noisy information regarding the locations at which those measurements were taken. We sample n points uniformly from the square of side length \u221a n to generate the true inputs X , then sample 50-dimensional output Y from independent GPs with SE kernel k(r) = exp(\u2212(r/`)2) for ` = 6.0 and noise standard deviation \u03c3n = 0.1. The observed points Xobs \u223c N(X,\u03c32obsI) arise by corrupting X with isotropic Gaussian noise of standard deviation \u03c3obs = 2. The parameters `, \u03c3n, and \u03c3obs were chosen to generate problems with interesting short-lengthscale structure for which GP-LVM optimization could nontrivially improve the initial noisy locations. Figure 2a shows a typical sample from this model.\nFor local GPs and GPRFs, we take the spatial partition to be a grid with n/m cells, where m is the desired number of points per cell. The GPRF edge set E connects each cell to its eight neighbors (Figure 2c), yielding linear time complexity O(nm2). During optimization, a practical choice is necessary: do we use a fixed partition of the points, or re-assign points to cells as they cross spatial boundaries? The latter corresponds to a coherent (block-diagonal) spatial covariance function, but introduces discontinuities to the marginal likelihood. In our experiments the GPRF was not sensitive to this choice, but local GPs performed more reliably with fixed spatial boundaries (in spite of the discontinuities), so we used this approach for all experiments.\nFor comparison, we also evaluate the Sparse GP-LVM, implemented in GPy [23], which uses the FITC approximation to the marginal likelihood [7]. (We also considered the Bayesian GP-LVM [4], but found it to be more resource-intensive with no meaningful difference in results on this problem.) Here the approximation parameter m is the number of inducing points.\nWe ran L-BFGS optimization to recover maximum a posteriori (MAP) locations, or local optima thereof. Figure 3a shows mean location error (Euclidean distance) for n = 10000 points; at this size it is tractable to compare directly to the full GP-LVM. The GPRF with a large block size (m=1111, corresponding to a 3x3 grid) nearly matches the solution quality of the full GP while requiring less time, while the local methods are quite fast to converge but become stuck at inferior optima. The FITC optimization exhibits an interesting pathology: it initially moves towards a good solution but then diverges towards what turns out to correspond to a contraction of the space (Figure 2d); we conjecture this is because there are not enough inducing points to faithfully represent the full GP\n2The GPRF is still transductive, in the sense that adding a test block f\u2217 will change the marginal distribution on the training observations y, as can be seen explicitly in the precision matrix (10). The contribution of the GPRF is that it provides a reasonable model for the training-set likelihood even in the absence of test points.\n(a) Mean location error over time for n = 10000, including comparison to full GP. (b) Mean error at convergence as a function of n, with learned lengthscale. (c) Mean location error over time for n = 80000.\nFigure 3: Results on synthetic data.\ndistribution over the entire space. A partial fix is to allow FITC to jointly optimize over locations and the correlation lengthscale `; this yielded a biased lengthscale estimate \u02c6\u0300\u2248 7.6 but more accurate locations (FITC-500-` in Figure 3a).\nTo evaluate scaling behavior, we next considered problems of increasing size up to n = 80000.3 Out of generosity to FITC we allowed each method to learn its own preferred lengthscale. Figure 3b reports the solution quality at convergence, showing that even with an adaptive lengthscale, FITC requires increasingly many inducing points to compete in large spatial domains. This is intractable for larger problems due to O(m3) scaling; indeed, attempts to run at n > 55000 with 2000 inducing points exceeded 32GB of available memory. Recently, more sophisticated inducing-point methods have claimed scalability to very large datasets [24, 25], but they do so with m \u2264 1000; we expect that they would hit the same fundamental scaling constraints for problems that inherently require many inducing points.\nOn our largest synthetic problem, n = 80000, inducing-point approximations are intractable, as is the full GP-LVM. Local GPs converge more quickly than GPRFs of equal block size, but the GPRFs find higher-quality solutions (Figure 3c). After a short initial period, the best performance always belongs to a GPRF, and at the conclusion of 24 hours the best GPRF solution achieves mean error 42% lower than the best local solution (0.18 vs 0.31)."}, {"heading": "4.2 Seismic event location", "text": "We next consider an application to seismic event location, which formed the motivation for this work. Seismic waves can be viewed as high-dimensional vectors generated from an underlying threedimension manifold, namely the Earth\u2019s crust. Nearby events tend to generate similar waveforms; we can model this spatial correlation as a Gaussian process. Prior information regarding the event locations is available from traditional travel-time-based location systems [26], which produce an independent Gaussian uncertainty ellipse for each event.\nA full probability model of seismic waveforms, accounting for background noise and performing joint alignment of arrival times, is beyond the scope of this paper. To focus specifically on the ability to approximate GP-LVM inference, we used real event locations but generated synthetic waveforms by sampling from a 50-output GP using a Mate\u0301rn kernel [3] with \u03bd = 3/2 and a lengthscale of 40km. We also generated observed location estimates Xobs, by corrupting the true locations with\n3The astute reader will wonder how we generated synthetic data on problems that are clearly too large for an exact GP. For these synthetic problems as well as the seismic example below, the covariance matrix is relatively sparse, with only ~2% of entries corresponding to points within six kernel lengthscales of each other. By considering only these entries, we were able to draw samples using a sparse Cholesky factorization, although this required approximately 30GB of RAM. Unfortunately, this approach does not straightforwardly extend to GP-LVM inference under the exact GP, as the standard expression for the marginal likelihood derivatives\n\u2202\n\u2202xi log p(y) =\n1 2 tr (( (K\u22121y y)(K \u22121 y y) T \u2212K\u22121y ) \u2202Ky \u2202xi ) involves the full precision matrix K\u22121y which is not sparse in general. Bypassing this expression via automatic differentiation through the sparse Cholesky decomposition could perhaps allow exact GP-LVM inference to scale to somewhat larger problems.\nGaussian noise of standard deviation 20km in each dimension. Given the observed waveforms and noisy locations, we are interested in recovering the latitude, longitude, and depth of each event.\nOur dataset consists of 107556 events detected at the Mankachi array station in Kazakstan between 2004 and 2012. Figure 4a shows the event locations, colored to reflect a principle axis tree partition [27] into blocks of 400 points (tree construction time was negligible). The GPRF edge set contains all pairs of blocks for which any two points had initial locations within one kernel lengthscale (40km) of each other. We also evaluated longer-distance connections, but found that this relatively local edge set had the best performance/time tradeoffs: eliminating edges not only speeds up each optimization step, but in some cases actually yielded faster per-step convergence (perhaps because denser edge sets tended to create large cliques for which the pairwise GPRF objective is a poor approximation).\nFigure 4b shows the quality of recovered locations as a function of computation time; we jointly optimized over event locations as well as two lengthscale parameters (surface distance and depth) and the noise variance \u03c32n. Local GPs perform quite well on this task, but the best GPRF achieves 7% lower mean error than the best local GP model (12.8km vs 13.7km, respectively) given equal time. An even better result can be obtained by using the results of a local GP optimization to initialize a GPRF. Using the same partition (m = 800) for both local GPs and the GPRF, this \u201chybrid\u201d method gives the lowest final error (12.2km), and is dominant across a wide range of wall clock times, suggesting it as a promising practical approach for large GP-LVM optimizations."}, {"heading": "5 Conclusions and Future Work", "text": "The Gaussian process random field is a tractable and effective surrogate for the GP marginal likelihood. It has the flavor of approximate inference methods such as loopy belief propagation, but can be analyzed precisely in terms of a deterministic approximation to the inverse covariance, and provides a new training-time interpretation of the Bayesian Committee Machine. It is easy to implement and can be straightforwardly parallelized.\nOne direction for future work involves finding partitions for which a GPRF performs well, e.g., partitions that induce a block near-tree structure. A perhaps related question is identifying when the GPRF objective defines a normalizable probability distribution (beyond the case of an exact tree structure) and under what circumstances it is a good approximation to the exact GP likelihood.\nThis evaluation in this paper focuses on spatial data; however, both local GPs and the BCM have been successfully applied to high-dimensional regression problems [11, 12], so exploring the effectiveness of the GPRF for dimensionality reduction tasks would also be interesting. Another useful avenue is to integrate the GPRF framework with other approximations: since the GPRF and inducing-point methods have complementary strengths \u2013 the GPRF is useful for modeling a function over a large space, while inducing points are useful when the density of available data in some region of the space exceeds what is necessary to represent the function \u2013 an integrated method might enable new applications for which neither approach alone would be sufficient."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their helpful suggestions. This work was supported by DTRA grant #HDTRA-11110026, and by computing resources donated by Microsoft Research under an Azure for Research grant."}, {"heading": "A Block tree structure", "text": "It is straightforward to see that the GPRF objective is exact when the MRF induced by the true precision matrix J , with respect to our chosen partition of y, is a tree. For any choice of root node yroot, the tree structure implies that we can write the true GP distribution as a product of parent-conditional distributions,\np(y) = p(yroot) \u220f i 6=root p(yi|y\u03c0(i))\nwhere \u03c0(i) is the (unique) parent of node i with respect to our chosen root. Then expanding the conditional distribution\np(y) = p(yroot) \u220f i6=root p(yi,y\u03c0(i)) p(y\u03c0(i))\n= p(yroot) \u220f i6=root p(yi) p(yi,y\u03c0(i)) p(yi)p(y\u03c0(i))\n= (\u220f i p(yi) ) \u220f i6=root p(yi) p(yi,y\u03c0(i)) p(yi)p(y\u03c0(i))  yields exactly the GPRF objective for the edge set E = (i, \u03c0(i)), i.e., the edges that define the tree.\nNote that the structure of the MRF induced by the true GP will depend on the partition we choose: a given precision matrix may induce a tree structure for some choices of partition but not for others (e.g., even a fully dense matrix can be viewed as a tree for trivial partitions that split the dataset into only one or two blocks).\nIn many cases it is easier to reason about the structure of the covariance matrix than that of the precision matrix. Assuming a stationary kernel, nonzero (or non-negligible) entries of the covariance matrix correspond to data points that are nearby to each other, meaning that the sparsity pattern of the covariance matrix reflects the geometry of the data itself. If the data can be viewed as lying on a treelike manifold \u2013 for example, seismic fault lines, or even trivial special cases such as time series data which lies on the real line \u2013 then for reasonable choices of partition, a graph connecting nearby blocks of data points will have a tree structure. Of course, there is no formal guarantee that this structure will fully carry over to the precision matrix, though intuitively we\u2019d expect that points very distant from each other are also unlikely to interact strongly in the precision matrix."}, {"heading": "B Approximation to the true Gaussian", "text": "In this section we prove Theorem 1 from the main text, showing that qGPRF is an unnormalized Gaussian density with a particular precision matrix.\nFor any pair of blocks (i, j), define the local precision matrix Q(ij) to be the inverse of the marginal covariance,\nQ(ij) =\n( Q\n(ij) 11 Q (ij) 12\nQ (ij) 21 Q (ij) 22\n) = ( Kii Kij Kji Kjj )\u22121 ,\nThe notation Q(ij) is used to distinguish these local precision matrices from the blocks Jij of the global precision matrix. Writing qGPRF in terms of unnormalized Gaussian densities,\nlog qGPRF (y) = \u2212 1\n2 M\u2211 i=1 (1\u2212 |Ei|)yTi K\u22121ii yi \u2212 1 2 \u2211 (i,j)\u2208E ( yi yj )T Q(ij) ( yi yj ) + C\n= \u22121 2 M\u2211 i=1 yTi ( K\u22121ii \u2212 |Ei|K \u22121 ii ) yi \u2212 1 2  \u2211 (i,j)\u2208E yTi Q (ij) 11 yi + 2y T i Q (ij) 12 yj + y T j Q (ij) 22 yj + C = \u22121\n2 M\u2211 i=1 yTi K\u22121ii + \u2211 j\u2208Ei ( Q (ij) 11 \u2212K \u22121 ii )yi \u2212 \u2211 (i,j)\u2208E yTi Q (ij) 12 yj + C\nwe obtain the standard form of a Gaussian MRF (expression (3) from the main text) showing that qGPRF does in fact induce a Gaussian density on y. Note that in passing from the second to the third line we used the fact that Qij11 = Q ji 22, by definition. This Gaussian representation allows us to read off the implicit precision matrix J\u0303 in block wise form\nJ\u0303ii = K \u22121 ii + \u2211 j\u2208Ei ( Q (ij) 11 \u2212K \u22121 ii ) , J\u0303ij = { Q (ij) 12 if (i, j) \u2208 E 0 otherwise. (10)\nWe see that the off-diagonal blocks of the precision matrix are simply the corresponding blocks of the pairwise local precisions. Each diagonal block, by contrast, combines the inverse of the local covariance matrix with corrections from the pairwise precisions. Note that the approximate precision J\u0303 may not be positive definite. In this case qGPRF is not a normalizable density, although it is still \u201capproximately normalized\u201d in the sense of the next section."}, {"heading": "C Approximate normalization", "text": "In this section we prove Theorem 2 from the main text: Theorem 2. The objective qGPRF is approximately normalized in the sense that the optimal value of the Bethe free energy [19], FB(b) = \u2211 i\u2208V (\u222b yi bi(yi) (1\u2212 |Ei|) ln b(yi) ln\u03c8i(yi) ) + \u2211 (i,j)\u2208E (\u222b yi,yj bij(yi,yj) ln bij(yi,yj) \u03c8ij(yi,yj)) ) \u2248 logZ, (11) the approximation to the normalizing constant found by loopy belief propagation, is precisely zero. Furthermore, this optimum is obtained when the pseudomarginals bi, bij are taken to be the true GP marginals pi, pij .\nProof. These claims are established rather directly by substituting the GPRF potentials \u03c8GPRFi , \u03c8 GPRF ij for the log pseudomarginals log\u03c8i, log\u03c8ij in (11), yielding\nFB(b) = \u2211 i\u2208V (\u222b yi bi(yi) (1\u2212 |Ei|) ln bi(yi) (1\u2212 |Ei|) ln p(yi) ) + \u2211 (i,j)\u2208E (\u222b yi,yj bij(yi,yj) ln bij(yi,yj) p(yi,yj)) )\n= \u2211 i\u2208V KL[bi\u2016pi] + \u2211 (i,j)\u2208E KL[bij\u2016pij ], (12)\nwhere KL[b\u2016p] = \u222b b(x) ln b(x)p(x)dx is the Kullback-Liebler divergence between distributions b and p. This is minimized when the distributions are equal, at which point the divergence is zero. Thus, taking bi = piand bij = pij yields the optimal value FB = 0.\nWe might have hoped that, as local GPs match the marginal distributions of the full GP on individual blocks, perhaps a higher-order approximation could match the exact marginals on pairs of blocks. This is not possible, since any Gaussian distribution whose pairwise marginals match the full GP must in fact be the full GP (Gaussians are entirely characterized by their covariances). Instead we can view qGPRF as approximately matching the pairwise marginals of the full GP, in the sense that the pseudomarginals found by running loopy belief propagation on qGPRF are in fact the true marginals of the full GP. This is a consequence of the fact that loopy BP converges to stationary points of the Bethe energy [19]."}], "references": [{"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["Neil D Lawrence"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A Bayesian committee machine", "author": ["Volker Tresp"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Gaussian Processes for Machine Learning", "author": ["Carl Rasmussen", "Chris Williams"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Bayesian Gaussian process latent variable model", "author": ["Michalis K Titsias", "Neil D Lawrence"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes", "author": ["Andreas C. Damianou", "Michalis K. Titsias", "Neil D. Lawrence"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "A unifying view of sparse approximate Gaussian process regression", "author": ["Joaquin Qui\u00f1onero-Candela", "Carl Edward Rasmussen"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Learning for larger datasets with the Gaussian process latent variable model", "author": ["Neil D Lawrence"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["Michalis K Titsias"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Model learning with local Gaussian process regression", "author": ["Duy Nguyen-Tuong", "Matthias Seeger", "Jan Peters"], "venue": "Advanced Robotics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Domain decomposition approach for fast Gaussian process regression of large spatial data sets", "author": ["Chiwoo Park", "Jianhua Z Huang", "Yu Ding"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "A framework for evaluating approximation methods for Gaussian process regression", "author": ["Krzysztof Chalupka", "Christopher KI Williams", "Iain Murray"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Distributed Gaussian Processes", "author": ["Marc Peter Deisenroth", "Jun Wei Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Infinite mixtures of Gaussian process experts", "author": ["Carl Edward Rasmussen", "Zoubin Ghahramani"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Fast allocation of Gaussian process experts", "author": ["Trung Nguyen", "Edwin Bonilla"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Local and global sparse Gaussian process approximations", "author": ["Edward Snelson", "Zoubin Ghahramani"], "venue": "In Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Modelling local and global phenomena with sparse Gaussian processes", "author": ["Jarno Vanhatalo", "Aki Vehtari"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Gaussian process latent random field", "author": ["Guoqiang Zhong", "Wu-Jun Li", "Dit-Yan Yeung", "Xinwen Hou", "Cheng-Lin Liu"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Probabilistic graphical models: Principles and techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Bethe free energy, Kikuchi approximations, and belief propagation algorithms", "author": ["Jonathan S Yedidia", "William T Freeman", "Yair Weiss"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["Kevin P Murphy", "Yair Weiss", "Michael I Jordan"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Statistical analysis of non-lattice data", "author": ["Julian Besag"], "venue": "The Statistician,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1975}, {"title": "WiFi-SLAM using Gaussian process latent variable models", "author": ["Brian Ferris", "Dieter Fox", "Neil D Lawrence"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Gaussian processes for big data", "author": ["James Hensman", "Nicolo Fusi", "Neil D Lawrence"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Distributed variational inference in sparse Gaussian process regression and latent variable models", "author": ["Yarin Gal", "Mark van der Wilk", "Carl Rasmussen"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The task of approximating the marginal likelihood is motivated by unsupervised applications such as the GP latent variable model [1], but examining the predictions made by our model also yields a novel interpretation of the Bayesian Committee Machine [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "The task of approximating the marginal likelihood is motivated by unsupervised applications such as the GP latent variable model [1], but examining the predictions made by our model also yields a novel interpretation of the Bayesian Committee Machine [2].", "startOffset": 251, "endOffset": 254}, {"referenceID": 2, "context": "1 Gaussian processes Gaussian processes [3] are distributions on real-valued functions.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "The most common application of GPs is to Bayesian regression [3], in which we attempt to predict the function values f\u2217 at test points X\u2217 via the conditional distribution given the training data, p(f\u2217|y;X,X\u2217, \u03b8).", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "This setting is known as the Gaussian Process Latent Variable Model (GPLVM) [1]; it uses GPs as a model for unsupervised learning and nonlinear dimensionality reduction.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "though some recent work [4, 5] attempts to recover an approximate posterior on X by maximizing a variational bound.", "startOffset": 24, "endOffset": 30}, {"referenceID": 4, "context": "though some recent work [4, 5] attempts to recover an approximate posterior on X by maximizing a variational bound.", "startOffset": 24, "endOffset": 30}, {"referenceID": 5, "context": "These points can be chosen by maximizing the marginal likelihood in a surrogate model [6, 7] or by minimizing the KL divergence between the approximate and exact GP posteriors [8].", "startOffset": 86, "endOffset": 92}, {"referenceID": 6, "context": "These points can be chosen by maximizing the marginal likelihood in a surrogate model [6, 7] or by minimizing the KL divergence between the approximate and exact GP posteriors [8].", "startOffset": 86, "endOffset": 92}, {"referenceID": 7, "context": "These points can be chosen by maximizing the marginal likelihood in a surrogate model [6, 7] or by minimizing the KL divergence between the approximate and exact GP posteriors [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 2, "context": "A separate class of approximations, so-called \u201clocal\u201d GP methods [3, 9, 10], involves partitioning the inputs into blocks of m points each, then modeling each block with an independent Gaussian process.", "startOffset": 65, "endOffset": 75}, {"referenceID": 8, "context": "A separate class of approximations, so-called \u201clocal\u201d GP methods [3, 9, 10], involves partitioning the inputs into blocks of m points each, then modeling each block with an independent Gaussian process.", "startOffset": 65, "endOffset": 75}, {"referenceID": 9, "context": "A separate class of approximations, so-called \u201clocal\u201d GP methods [3, 9, 10], involves partitioning the inputs into blocks of m points each, then modeling each block with an independent Gaussian process.", "startOffset": 65, "endOffset": 75}, {"referenceID": 10, "context": "Nonetheless, local GPs sometimes work very well in practice, achieving results comparable to more sophisticated methods in a fraction of the time [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "The Bayesian Committee Machine (BCM) [2] attempts to improve on independent local GPs by averaging the predictions of multiple GP experts.", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "The BCM can yield high-quality predictions that avoid the pitfalls of local GPs (Figure 1c), while maintaining scalability to very large datasets [12].", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "Mixture-of-experts models [13, 14] extend the local GP concept in a different direction: instead of deterministically assigning points to GP models based on their spatial locations, they treat the assignments as unobserved random variables and do inference over them.", "startOffset": 26, "endOffset": 34}, {"referenceID": 13, "context": "Mixture-of-experts models [13, 14] extend the local GP concept in a different direction: instead of deterministically assigning points to GP models based on their spatial locations, they treat the assignments as unobserved random variables and do inference over them.", "startOffset": 26, "endOffset": 34}, {"referenceID": 14, "context": "The PIC approximation [15] blends a global inducing-point model with local block-diagonal covariances, thus capturing a mix of global and local structure, though with the same boundary discontinuities as in \u201cvanilla\u201d local GPs.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "A related approach is the use of covariance functions with compact support [16] to capture local variation in concert with global inducing points.", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "[11] surveys and compares several approximate GP regression methods on synthetic and real-world datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Finally, we note here the similar title of [17], which is in fact orthogonal to the present work: they use a random field as a prior on input locations, whereas this paper defines a random field decomposition of the GP model itself, which may be combined with any prior on X .", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "We recall some basic theory regarding Markov random fields (MRFs), also known as undirected graphical models [18].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "It can be interpreted as a \u201cBethe-type\u201d approximation [19], in which a joint density is approximated via overlapping pairwise marginals.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": ") In general this will not be the case, but in the spirit of loopy belief propagation [20], we consider the tree-structured case as an approximation for the general setting.", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "The objective qGPRF is approximately normalized in the sense that the optimal value of the Bethe free energy [19],", "startOffset": 109, "endOffset": 113}, {"referenceID": 1, "context": "corresponds exactly to the prediction of the Bayesian Committee Machine (BCM) [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 20, "context": "2 A similar derivation shows that the conditional distribution of any block yi given all other blocks yj 6=i also takes the form of a BCM prediction, suggesting the possibility of pseudolikelihood training [21], i.", "startOffset": 206, "endOffset": 210}, {"referenceID": 21, "context": "1 Uniform Input Distribution We first consider a 2D synthetic dataset intended to simulate spatial location tasks such as WiFiSLAM [22] or seismic event location (below), in which we observe high-dimensional measurements but have only noisy information regarding the locations at which those measurements were taken.", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "For comparison, we also evaluate the Sparse GP-LVM, implemented in GPy [23], which uses the FITC approximation to the marginal likelihood [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "(We also considered the Bayesian GP-LVM [4], but found it to be more resource-intensive with no meaningful difference in results on this problem.", "startOffset": 40, "endOffset": 43}, {"referenceID": 22, "context": "Recently, more sophisticated inducing-point methods have claimed scalability to very large datasets [24, 25], but they do so with m \u2264 1000; we expect that they would hit the same fundamental scaling constraints for problems that inherently require many inducing points.", "startOffset": 100, "endOffset": 108}, {"referenceID": 23, "context": "Recently, more sophisticated inducing-point methods have claimed scalability to very large datasets [24, 25], but they do so with m \u2264 1000; we expect that they would hit the same fundamental scaling constraints for problems that inherently require many inducing points.", "startOffset": 100, "endOffset": 108}, {"referenceID": 2, "context": "To focus specifically on the ability to approximate GP-LVM inference, we used real event locations but generated synthetic waveforms by sampling from a 50-output GP using a Mat\u00e9rn kernel [3] with \u03bd = 3/2 and a lengthscale of 40km.", "startOffset": 187, "endOffset": 190}, {"referenceID": 10, "context": "This evaluation in this paper focuses on spatial data; however, both local GPs and the BCM have been successfully applied to high-dimensional regression problems [11, 12], so exploring the effectiveness of the GPRF for dimensionality reduction tasks would also be interesting.", "startOffset": 162, "endOffset": 170}, {"referenceID": 11, "context": "This evaluation in this paper focuses on spatial data; however, both local GPs and the BCM have been successfully applied to high-dimensional regression problems [11, 12], so exploring the effectiveness of the GPRF for dimensionality reduction tasks would also be interesting.", "startOffset": 162, "endOffset": 170}], "year": 2015, "abstractText": "Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.", "creator": "LaTeX with hyperref package"}}}