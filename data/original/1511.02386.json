{"id": "1511.02386", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2015", "title": "Hierarchical Variational Models", "abstract": "Black box inference allows researchers to easily prototype and evaluate an array of models. Recent advances in variational inference allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution which maintains efficient computation? To address this, we develop hierarchical variational models. In a hierarchical variational model, the variational approximation is augmented with a prior on its parameters, such that the latent variables are conditionally independent given this shared structure. This preserves the computational efficiency of the original approximation, while admitting hierarchically complex distributions for both discrete and continuous latent variables. We study hierarchical variational models on a variety of deep discrete latent variable models. Hierarchical variational models generalize other expressive variational distributions and maintains higher fidelity to the posterior.", "histories": [["v1", "Sat, 7 Nov 2015 19:01:48 GMT  (55kb,D)", "http://arxiv.org/abs/1511.02386v1", null], ["v2", "Mon, 30 May 2016 21:16:38 GMT  (213kb,D)", "http://arxiv.org/abs/1511.02386v2", "Appears in International Conference on Machine Learning, 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO stat.ME", "authors": ["rajesh ranganath", "dustin tran", "david m blei"], "accepted": true, "id": "1511.02386"}, "pdf": {"name": "1511.02386.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Variational Models", "authors": ["Rajesh Ranganath", "Dustin Tran", "David M. Blei"], "emails": ["rajeshr@cs.princeton.edu", "dtran@g.harvard.edu", "david.blei@columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "Black box variational inference (BBVI) is important to realizing the potential of modern applied Bayesian statistics. The promise of BBVI is that an investigator can specify any probabilistic model of hidden and observed variables, and then efficiently approximate the corresponding posterior without additional effort (Ranganath et al., 2014).\nBBVI is a form of variational inference (Jordan et al., 1999; Wainwright and Jordan, 2008). It sets up a parameterized distribution over the latent variables and then optimizes the parameters to be close to the posterior distribution of interest. Typically this is done with the mean-field family, where each variable is independent and governed by its own variational parameters. Mean-field inference enables efficient BBVI algorithms, but it is limited by the strong factorization. It cannot capture dependencies between latent variables\u2014this may be intrinsically important and also help improve the accuracy of the marginals.\nIn this paper we develop a black box variational method\nthat goes beyond the mean-field and, indeed, beyond directly parameterized variational families in general. Our method remains tractable but uses a richer family of variational distributions and finds better approximations to the posterior.\nThe key idea is this: we treat the original variational family as a \u201cmodel\u201d of the latent variables, and then expand it hierarchically as is commonly done in Bayesian statistics. The resulting variational model is a two-stage distribution that first draws variational parameters from a \u201cprior\u201d and then draws latent variables from the resulting variational distribution (\u201clikelihood\u201d). The \u201cposterior\u201d will play a role during inference, i.e., the conditional distribution of the variational parameters given a realization of the hidden variables. With a hierarchical variational model, rather than fit the mean-field parameters directly, one fits the hyperparameters of the prior. This construction maintains the computational efficiency\nSeen as a traditional variational method, hierarchical variational models transform the variational family to the distribution obtained by marginalizing out the variational parameters under a prior. This expands the family from those that are directly parameterized, and thus can better approximate the posterior. Note the prior is a choice\u2014for example, we will show how to use the recently-proposed normalizing flows (Rezende and Mohamed, 2015) as a prior to a mean-field distribution.\nIn this paper we define hierarchical variational models and develop a general algorithm for fitting them in the context of black box inference. Our algorithm maintains the computational efficiency of the original variational family. We demonstrate our methods with a study of approximate posteriors for several variants of deep exponential families (with Poisson layers) (Ranganath et al., 2015). In our study, hierarchical variational models always found better approximations to the exact posterior.\nRelated Work. Variational models have a rich history. Inspired by kernel density estimation, this was classically studied by Jaakkola and Jordan (1998), and later revisited by Gershman et al. (2012), in which one speci-\nar X\niv :1\n51 1.\n02 38\n6v 1\n[ st\nat .M\nL ]\n7 N\nov 2\n01 5\nfies a mixture of Gaussians (MIXTURE) as the variational distribution. Lawrence (2000) explores a rich class of variational approximations formed by both mixtures and Markov chains. Recently, the idea of latent variables in the variational approximation has received some recent interest by Salimans et al. (2013, 2015), but they are limited to cases where the posterior of the variational latent variables is known or differentiable. These posterior models capture dependencies that are lost in simpler approximations.\nThere has been a line of work for variational approximations that capture dependencies in differentiable probability models (Titsias and L\u00e1zaro-Gredilla, 2014; Rezende and Mohamed, 2015; Salimans et al., 2015; Kucukelbir et al., 2015), but there has been limited work in variational methods that capture dependencies with discrete latent variables. Such methods typically apply the score function estimator (Ranganath et al., 2014; Mnih and Gregor, 2014). These methods can be used to posit rich approximations, but are either limited by the noise in stochastic gradients, or are quadratic in the number of latent variables. For example Mnih and Gregor (2014) apply such techniques for variational approximations in sigmoid belief networks, but this approach is limited to stochastic feed forward networks. Additionally, the variance increases as the number of layers increase. Tran et al. (2015) propose copulas (COPULA VI) as a way of learning dependencies in factorized approximations. Copulas can be extended to the framework of hierarchical variational models, whereas the direct approach taken in Tran et al. (2015) requires computation quadratic in the number of latent variables."}, {"heading": "2 Black Box Variational Inference", "text": "Black box variational methods provide a mechanism to approximate posterior distributions with minimal analytic work. Let p(z |x) denote a posterior distribution, which is a distribution on d latent variables z1, . . . ,zd conditioned on a set of observations x. In variational inference, one posits a family of distributions q(z;\u03bb), parameterized by \u03bb, and minimizes the KL divergence to the posterior distribution (Jordan et al., 1999; Bishop, 2006; Wainwright and Jordan, 2008).\nThis is equivalent to maximizing the Evidence Lower BOund (ELBO),\nL (\u03bb) = Eq(z;\u03bb)[log p(x,z)\u2212 log q(z;\u03bb)]. (1)\nBlack box methods maximize the ELBO by constructing noisy gradients via samples from the variational approximation. This avoids model-specific computations, requiring only that the practitioner write a function to evaluate the model log-likelihood. Following the setting of black\nbox inference, we now develop a framework for richer variational families."}, {"heading": "3 Variational Models", "text": "While black box variational methods expose variational inference algorithms to all probabilistic models, it remains an open problem to specify a variational distribution which both maintains high fidelity to arbitrary posteriors and is computationally tractable.\nFundamentally, black box methods also expose the loose requirements necessary for specifying an approximating distribution to the posterior. Such a model must satisfy three requirements: 1. a generating process for obtaining posterior samples z, 2. computation of its log density function log q(z), and 3. proper support within the posterior.\nMean-field Variational Models The most common choice satisfying these properties is the mean-field approximation\nqMF(z;\u03bb) = d \u220f\ni=1\nq(zi;\u03bbi),\nwhere \u03bbi denotes the parameters of the i th latent variable.\nMean-field approximations turn the intractable problem of computing the posterior into an computationally feasible optimization problem. However, its marginal factorization compromises the expressivity of the variational model: it abandons recovery of any dependence structure of the posterior, and it cannot in general capture all marginal information unless correctly specified. This predicates the need for more expressive variational models, which apply to both discrete and continuous variables yet remain computationally tractable. We develop hierarchical variational models to this extent.\nHierarchical Variational Models Viewing the meanfield distribution plainly as a model of the posterior, a natural way to introduce more complexity is to construct it hierarchically. That is, we place a prior distribution q(\u03bb;\u03b8 ) with parameters \u03b8 on the mean-field parameters and proceed to marginalize it out. Adding a one layer hierarchical prior leads to the variational model\nqHVM(z;\u03b8 ) =\n\u222b\n\n\nd \u220f\ni=1\nq(zi |\u03bbi)\n\nq(\u03bb;\u03b8 )d\u03bb. (2)\nThe variational prior q(\u03bb;\u03b8 ) acts as a distribution over variational distributions such that given its structure, each posterior variable zi is conditionally independent. Thus we term qHVM as the hierarchical variational model, which\ncan either be a discrete or continuous distribution. Figure 1 displays this graphically. For simplicity, we focus on one level hierarchies.\nThe augmentation with a variational prior has strong ties to empirical Bayesian methods, which use data to estimate hyperparameters of a prior distribution (Robbins, 1964; Efron and Morris, 1973). In general, empirical Bayes considers the fully Bayesian treatment of a hyperprior on the original prior\u2014here, the variational prior on the original mean-field\u2014and proceeds to integrate it out. As this is analytically intractable, much work has been on parametric estimation, which seek point estimates rather than the whole distribution encoded by the hyperprior. We avoid this at the level of the hyperprior (variational prior) based on techniques discussed later; however, our procedure can still be viewed in this framework at one higher level. That is, we seek a point estimate of the \"variational hyperprior\" which governs the parameters on the variational prior.\nA similar methodology also arises in the policy search literature (R\u00fcckstie\u00df et al., 2008; Sehnke et al., 2008). Policy search methods aim to maximize the expected reward for a sequential decision-making task, by positing a distribution over trajectories and proceeding to learn its parameters. This distribution is known as the policy, and an upper-level policy considers a distribution over the original policy. This encourages exploration in the latent variable space and can be seen as a form of annealing.\nThe variational prior q(\u03bb;\u03b8 ) is parameterized by a vector \u03b8 . These are the parameters we optimize over to find the optimal variational distribution within the class of hierarchical variational models. The ELBO using the hierarchical\nvariational model is\nL (\u03b8 ) = EqHVM(z;\u03b8 )[log p(x,z)\u2212 log qHVM(z;\u03b8 )]. (3)\nWe can lay out the properties required of variational models to ensure that the objective remains analytically tractable. The first term in the objective is tractable as long as we can sample from q and q has proper support. The second term with log qHVM(z;\u03b8 ), the entropy, contains an integral (Eq.2) that is in general analytically intractable.\nWe construct a bound on the entropy term by introducing a distribution r(\u03bb |z;\u03c6) with parameters \u03c6, and applying the variational principle:\n\u2212EqHVM[log qHVM(z)] (4) \u2265\u2212Eq(z,\u03bb)[log q(\u03bb) + log q(z |\u03bb)\u2212 log r(\u03bb |z;\u03c6)].\nIt can be shown that the bound is exact when r(\u03bb |z;\u03c6) matches the variational posterior q(\u03bb |z;\u03b8 ). From this perspective, we can view r as a recursive variational approximation. That is, it is a model for the posterior q of the mean-field parameters \u03bb given a realization of the latent variables z.\nThe bound is derived by introducing a term KL(q\u2016r). Due to the asymmetry of KL-divergence, r could also be substituted into the first rather than the second argument of the KL divergence; this produces an alternative bound to Eq.4. The bound can also be extended to multi-level hierarchical variational models, where now we model the posterior distribution q of the higher levels using higher levels in r. Derivations of the bound and more details are available in the appendix.\nWe note that Eq.4 is tighter than the trivial conditional entropy bound of H[qHVM]\u2265H[q |\u03bb] (Cover and Thomas, 2012). This bound is attained when specifying the recursive approximation to be the prior, i.e., it is the special case when r(\u03bb |z;\u03c6) = q(\u03bb;\u03b8 ).\nSubstituting the entropy bound (Eq.4) into the ELBO in Eq.3 gives a tractable lower bound which we call the hierarchical ELBO, denoted with fL :\nfL (\u03b8 ,\u03c6) = Eq(z,\u03bb;\u03b8 ) h log p(x,z) + log r(\u03bb |z;\u03c6)\n\u2212 d \u2211\ni=1\nlog q(zi |\u03bbi)\u2212 log q(\u03bb;\u03b8 ) i . (5)\nAs all of the terms are known, this objective is tractable. We can fit q and r simultaneously by maximizing Eq.5 with respect to \u03b8 and \u03c6. Maximizing this bound is equivalent to minimizing an upper bound on the KL-divergence of qHVM to the black box posterior p(z | x). Similar to the EM-algorithm (Dempster et al., 1977), optimizing \u03b8 improves the posterior approximation, while optimizing \u03c6\ntightens the upper bound on the KL divergence (improving the recursive variational approximation).\nWe can analyze Eq.5 by rewriting it in terms of the meanfield lower bound LMF(\u03bb),\nfL (\u03b8 ,\u03c6) = Eq[LMF(\u03bb)] +Eq[log r(\u03bb |z;\u03c6)\u2212 log q(\u03bb;\u03b8 )].\nThis shows that Eq.5 is a sum of two terms: a Bayesian model average of mean-field objectives, with weights given by the variational prior q(\u03bb;\u03b8 ); and a correction term that\u2019s a function of both the auxiliary distribution r and the variational prior. Since mixtures (i.e. convex combinations) cannot be sharper than their components, r must not be conditionally independent of z, in order for this bound to be better than the mean-field\u2019s."}, {"heading": "4 Specifying the Hierarchical Variational Model", "text": "Specifying a hierarchical variational model requires two components: the variational likelihood q(z | \u03bb) and the prior q(\u03bb;\u03b8 ). The likelihood factors q(zi |\u03bbi) can be chosen in the same way that mean-field factors are typically chosen. This leaves the specification of the prior distribution q(\u03bb;\u03b8 ). The prior distribution should satisfy two criteria. First, the components of the vector \u03bb should not be independent as this reduces to the mean-field. Additionally, we want our choice of q(\u03bb;\u03b8 ) to allow for black box inference over models with both discrete and continuous latent variables. This criteria is discussed more directly in the next section when we discuss optimization with stochastic gradients.\nMost variational models which move beyond mean-field can be extended to this framework. In general, one can take any tractable variational family typically used in variational inference, and specify it as the variational prior q(\u03bb;\u03b8 ). We outline several examples.\nMixture of Gaussians. A mixture of Gaussians can approximate arbitrary distributions given enough components and has been considered in traditional literature (Jaakkola and Jordan, 1998; Lawrence, 2000; Gershman and Blei, 2012). Thus they form an interesting class of prior distributions q(\u03bb;\u03b8 ). Formally, let K be the number of components, \u03c0 be a probability vector, \u00b5k, and \u03c3k be the parameters of a d-dimensional multivariate Gaussian with diagonal covariance. The variational prior is\nq(\u03bb;\u03b8 ) = K \u2211\ni=1\n\u03c0kN(\u00b5k,\u03c3k).\nThe parameters \u03b8 consist of the probability vector \u03c0 as well as the component means \u00b5k and variances \u03c3k. Relationships between different latent variables are captured\nby the mixture locations \u00b5k. For example, a two component mixture with two latent variables can capture that the latent variables are either very positive or very negative.\nNormalizing Flows. Normalizing flows, which was previously introduced as a variational approximation for differentiable probability models in Rezende and Mohamed (2015), work by transforming a random variable \u03bb0 with a known simple distribution such as the standard normal through a sequence of invertible functions f1 to fK . As each function is applied, the distribution of the output is a contorted version of the original distribution. This leads to very complicated variational families.\nConsider normalizing flows for the variational prior. Formally, let q0 be the distribution for \u03bb0 and \u03bb be the result after k transformations. Then the log density of \u03bb is\nlog q(\u03bb) = log q(\u03bb0)\u2212 K \u2211\nk=1\nlog det( \u2202 fk \u2202 zk )\n(6)\nThe sequence of Jacobians describe the transport of probability mass through the flow. Functions that lead to complicated densities with efficient linear-time Jacobians can be created (Rezende and Mohamed, 2015). In this paper we focus on the planar flows f (\u03bb) = \u03bb + uh(w>\u03bb + b), where u, w, and b are (variational) parameters and h is a non-linearity such as the hyperbolic tangent function. Let \u03c8(\u03bb) =\u2207\u03bbh(\u03bb); then the flow density is\nlog q(\u03bb) = log q(\u03bb0)\u2212 K \u2211\nk=1\nlog 1+ u>k\u03c8(\u03bbk) .\nWe focus on this variational model in our experiments, so we describe the approximation r when the variational prior q(\u03bb) is a normalizing flow. The optimal r is the variational posterior q(\u03bb | z). It is a continuous distribution, so we could again define r using a normalizing flow. The problem with this approach is that the intermediary \u03bb\u2019s to \u03bb0 required for the flow are unknown and generally require numeric inversions. Instead we define a normalizing flow where the inverse flow has a known parametric form. That is, let the distribution of \u03bb in r be given by a sequence of invertible transforms g1 to gL of a simple distribution r0. Now let g\n\u22121(\u03bb) have a known form. Then the density of \u03bb in r is\nlog r(\u03bb | z) = log r(\u03bb0 |z) + K \u2211\nk=1\nlog det( \u2202 g\u22121k \u2202 \u03bbk )\n!\n. (7)\nThe sequence of intermediary \u03bb can be computed quickly by applying the known inverse functions. This yields a flexible parameterization of r, which admits easy computation of both the gradients of \u03c6 and \u03b8 . We specify\nr(\u03bb0 |z) to be a factorized regression:\nr(\u03bb0 |z) = d \u220f\ni=1\nr(\u03bb0 i |zi). (8)\nIn words, the distribution r parameterizes a transformation under which the initial distribution factorizes.\nOther Variational Models. Variational models can be constructed from many other classes of distributions. For example, copulas explicitly introduce dependence in a collection of d random variables with parameterized marginals, using joint distributions on the d-dimensional hypercubes (Nelsen, 2006). These can used as priors on either point mass or traditional mean-field likelihoods. A class of variational models can also be constructed by replacing the mixture model with a factorial mixture (Ghahramani, 1995). This leads to a much richer posterior approximation with the same number of parameters and latent variables z."}, {"heading": "5 Optimizing the Hierarchical ELBO", "text": "We maximize the hierarchical ELBO with stochastic optimization (Robbins and Monro, 1951), which follows noisy, yet unbiased, gradients of the objective.\nStochastic Gradient of the ELBO. The score function estimator for the gradient of the ELBO applies to both discrete and continuous latent variable models. It has strong roots in the policy search literature and evolutionary algorithms, where it is more commonly known as the REINFORCE gradient (Williams, 1992). It is given by\n\u2207score\u03bb L = (9) Eq(z |\u03bb)[\u2207\u03bb log q(z |\u03bb)(log p(x,z)\u2212 log q(z |\u03bb))].\nSee Ranganath et al. (2014) for a full derivation. We can construct noisy gradients from Eq.9 by Monte Carlo approximating the expectation. Formally, let S be the number of samples, the Monte Carlo estimate is\n1\nS\nS \u2211\ns=1\n\u2207\u03bb log q(zs |\u03bb)(log p(x,zs)\u2212 log q(zs |\u03bb)),\nwhere zs \u223c q(z |\u03bb).\nIn general, the score function estimator exhibits high variance. This is not surprising given that the score function estimator makes very few restrictions on the class of models, and requires access only to zero-order information. Roughly, the variance of this estimator scales with the number of random variables (Ranganath et al., 2014; Mnih and Gregor, 2014).\nIn mean-field models, the gradient of the ELBO with respect to \u03bbi can be written as\n\u2207\u03bbiLMF = Eq(zi ;\u03bbi)[\u2207\u03bbi log q(zi;\u03bbi)\n(log pi(x,z)\u2212 log q(zi;\u03bbi))], (10)\nwhere log pi(x,z) are the components in the joint distribution that contain zi . This update is not only local, but it drastically reduces the variance of Eq.9 to make it computationally efficient.\nIn the case of differentiable latent variables, one would like to take advantage of model gradients. One such estimator does this using reparameterization: the ELBO is written in terms of a random variable \u03b5, whose distribution s is free of the variational parameters and such that the original z can be written as a deterministic function z= z(\u03b5;\u03bb). This allows gradients with respect to the variational parameters to directly propagate inside the expectation:\n\u2207rep\u03bb L = Es(\u03b5)[(\u2207z log p(x,z)\u2212\u2207z log q(z))\u2207\u03bbz(\u03b5;\u03bb)].\nSimilar to the score gradient, we can construct noisy gradients from this expression via Monte Carlo. Empirically reparameterization gradients have been shown to have much lower variance than the score function gradient (Titsias, 2015). In the appendix, we explore an analytic equality for both gradients to explain the empirically observed difference in variance.\nStochastic Gradient of the Hierarchical ELBO. As discussed in Section 4, the variational prior q(\u03bb;\u03b8 ) can be constructed from both discrete and continuous distributions. However, due to the efficiency of Monte Carlo estimates for differentiable probability models using the reparameterization gradient, we focus on differentiable priors such as the normalizing flow.\nTo optimize Eq.5 we need to compute the stochastic gradient with respect to \u03c6 and \u03b8 . Due to the the choice of differentiable prior, we can use the reparameterization gradient on q(\u03bb). Let \u03b5 be a distribution drawn from a standard distribution s such as the standard Gaussian. Then let \u03bb be written as a function of \u03b5 and \u03b8 denoted \u03bb(\u03b5;\u03b8 ). Next we define V to be the score function\nV =\u2207\u03bb log q(z |\u03bb).\nThen the gradient of the hierarchical ELBO with respect to \u03b8 is\n\u2207\u03b8eL(\u03b8 ,\u03c6) = Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)\u2207\u03bbLMF(\u03bb)] +Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)\u2207\u03bb[log r(\u03bb |z;\u03c6)\u2212 log q(\u03bb;\u03b8 )]] +Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)Eq(z |\u03bb)[V log r(\u03bb |z;\u03c6)]]. (11)\nThe first term is the gradient of the mean-field variational approximation scaled by the chain rule gradient from\nreparameterization. Thus hierarchical variational models inherit the variance reduced gradient (Eq.10) from the mean-field factorization. The second and third terms try to match r and q. The second term is strictly based on reparameterization, and thus exhibits low variance. The third term involves potentially a high variance gradient due to the appearance of all the latent variables. Since the distribution q(z |\u03bb(\u03b5;\u03b8 )) factorizes by definition, we can apply the same variance reduction for r as for done in the mean-field with p. We examine this below.\nLocal Learning with r. Let ri be the terms log r(\u03bb |zi) containing zi , and define Vi to be the local score\nVi =\u2207\u03bb log q(zi |\u03bbi).\nThen the last term in Eq.11 can be transformed as\nEs(\u03b5)[\u2207\u03b8\u03bb(\u03b5;\u03b8 )Eq(z |\u03bb)[V log r(\u03bb |z;\u03c6)]]\n= Es(\u03b5)\n\n\u2207\u03b8\u03bb(\u03b5;\u03b8 )Eq(z |\u03bb)\n\n\nd \u2211\ni=1\nVi log ri(\u03bb |z;\u03c6)\n\n\n\n .\nWe derive this expression along with the full gradient in the appendix. When ri does not depend on too many variables, this gradient effectively combines both the computational efficiency of the mean-field and reparameterizations for a hierarchical variational model for both discrete and continuous latent variables. For example, in the variational model based on normalizing flows, the term ri only depends on zi as it is factorized (Eq.8).\nStochastic Gradient with respect to \u03c6. Finally, as the expectation in the hierarchical ELBO (Eq.5) does not depend on \u03c6, we can simply pass the gradient operator inside the expectation to obtain\n\u2207\u03c6 fL = Eq(z,\u03bb)[\u2207\u03c6 log r(\u03bb |z,\u03c6)]. (12)\nAlgorithm. The inference procedure is outlined in Algorithm 1, where we evaluate noisy estimates of both gradients by sampling from the joint q(z,\u03bb). In general, these gradients can be computed via automatic differentiation systems such as those available in Stan and Theano (Stan Development Team, 2015; Bergstra et al., 2010); this removes the need for model-specific computations, and moreover we note that no assumption has been made on log p(x,z) other than the ability to calculate it.\nTable 1 outlines black box methods and their complexity requirements. Hierarchical variational models equipped with a normalizing flow prior has complexity linear in the number of latent variables scaled by the length of the flow used to represent r and q.\nAlgorithm 1: Black box inference with HIERARCHICAL VM Input : Model log p(x,z),\nVariational model q(z |\u03bb)q(\u03bb;\u03b8 ), Auxiliary Distribution r(\u03bb |x,z;\u03c6)\nOutput: Variational Parameters: \u03b8 Auxiliary Parameters: \u03c6 Initialize \u03c6 and \u03bb randomly; while change in ELBO is above some threshold do\nCompute unbiased estimate of \u2207\u03b8L using Eq.11 ; Compute unbiased estimate of \u2207\u03c6L using Eq.12 ; Update \u03c6 and \u03bb using stochastic gradient ascent ;\nend\nMulti-level q(\u03bb;\u03b8 ). Multi-level hierarchical variational models can contain both discrete and differentiable latent variables. Higher level differentiable variables can be addressed by repeated use of the reparameterization trick. Discrete variables in the prior pose a difficulty as the learning signal used to estimate them has high variance. Local expectation gradients (Titsias, 2015) provide an efficient gradient estimator for variational approximations over discrete variables with small support, done by analytically marginalizing over each discrete variable individually. This approach can be combined with the gradient in Eq.11 to form an efficient gradient estimator. We detail this in the appendix.\nInference Networks. Classically, variational inference on models with random variables associated with each data point requires finding the optimal variational parameters for each data point\u2019s variational factor. This process can be computationally prohibitive especially at test time. The use of inference networks (Dayan, 2000; Stuhlm\u00fcller et al., 2013; Kingma and Welling, 2014; Rezende et al., 2014) amortizes the cost of estimating these local variational parameters by tying them together through a neural network. Specifically, the data point specific variational parameters are outputs to a neural network with the data point as input. The parameters of the neural network \u03b6 then become the variational parameters; this reduces the cost of estimating the parameters of all the data points to estimating parameters of the inference network. Inference networks can be applied to hierarchical variational models by making both the parameters of the variational model and auxiliary distribution functions of their conditioning sets."}, {"heading": "6 Experimental Results", "text": "We have introduced a new class of variational models and developed efficient black box algorithms for their computation. We focus on hierarchical variational models defined by normalizing flows. We first consider a toy ex-\nample. Then we compare our proposed variational approximations to more standard ones on deep exponential families (Ranganath et al., 2015), a class of hierarchical models where each observation is represented by multiple layers of exponential family random variables."}, {"heading": "6.1 Toy Example: Correlated Bernoullis", "text": "Consider a toy model with no observations and two binary variables, defined by the following probability distribution:\nz1 = 0 z1 = 1\nz0 = 0 0.1 0.4 z1 = 1 0.4 0.1\nThe mean-field approximation has a hard time capturing this distribution due to both the presence of negative correlation and that the negative correlation is not strong enough to bifurcate the optimization problem into two modes. Thus the optimal mean-field approximation is uniform.\nFigure 2 plots the ELBO and KL-divergence for the hierarchical variational model and mean-field, where we specify\na hierarchical variational model with prior and auxiliary distribution given by a normalizing flow. The KL is improved by over an order of magnitude, and a flow length of 8 exactly recovers the probability table up to numerical precision."}, {"heading": "6.2 Deep Exponential Families", "text": "The deep exponential family (DEF) (Ranganath et al., 2015) forms a class of probabilistic models built from exponential families (Brown, 1986) whose latent structure parallels the architectures used in deep neural networks.\nModel. Exponential families are parameterized by a set of natural parameters. We denote a draw from an unspecified exponential family with natural parameter \u03b7 as EXPFAM(\u03b7). The natural parameter in deep exponential families are constructed from an inner product of the previous layer with shared weights passed through a link function g.\nLet xi be an observation, zi,` be a vector of latent variables with zi,`,k as an element of that vector, L be the total number of layers and W`,k be a shared vector of random variables across observations, then deep exponential families are\nW`,k \u223c EXPFAMW (\u03be) zi,L,k \u223c EXPFAML(\u03b7) zi,`,k \u223c EXPFAM`(g`(z>i,l+1Wl,k)\nxi,n \u223c Poisson(W>0,nzi,1).\nHere, we specialized the likelihood on x for count data which we focus on in our experiments.\nWe focus on Poisson DEFs in particular. In a sigmoid belief network introduced by Neal (1990), each observation either turns a feature on or off, while in a Poisson DEF each observation expresses each feature a positive integer number of times. This means Poisson DEFs are a multifeature generalization of the classic sigmoid belief network.\nThe Poisson DEF has Poisson distributed latent variables and normal weights with the log-softplus link function. This means the conditional distribution is\np(zi,`,k |zi,l+1,Wl,k)\u223c Poisson(log(1+ exp(z>i,l+1Wl,k))).\nVariational Models We consider the variational approximation that adds dependence to the z\u2032s. We parameterize each variational prior q(\u03bbzi ) with a normalizing planar flow of length 2, and use the inverse flow of length 10 for r(\u03bbzi ). In a pilot study, we found little improvement with longer flow lengths. We compare to the mean-field approximation from Ranganath et al. (2015).\nData. We consider two text corpora of news and scientific articles: The New York Times and Science. Both datasets have 11K documents. The New York Times consists of 8K terms and Science consists of 5.9K terms.\nEvaluation. We examine both held out perplexity and the bound on the evidence for held out data.\nTo compute held-out perplexity, we use ten percent of each held-out observation to compute the variational parameters associated with that observation, then compute the perplexity on the remaining 90 percent. Held out perplexity is\nexp\n\u2212 \u2211\nd\n\u2211\nw\u2208d log p(w |# held out in d) Nheld out\nThis is a document complete evaluation metric (Wallach et al., 2009) where the words are tested independently.\nThe bound on the held out evidence is the ELBO for the mean-field approximation, and the hierarchical ELBO for the hierarchical variational model. Note that the held out evidence directly measures how well the posterior is approximated by the variational model. On the other hand, held out perplexity may be affected by the fact that good posterior estimates might overfit on test data.\nHyperparameters. We study one, two, and three layer DEFs with 100, 30, and 15 units respectively and set prior hyperparameters following Ranganath et al. (2015). We use Nesterov\u2019s accelerated gradient with momentum parameter of 0.9, combined with RMSProp with a scaling factor of 10\u22122, to maximize the lower bound.\nResults. HIERARCHICAL VM achieves better performance on both perplexity and held-out likelihood across all data sets and models. The two-layer Poisson DEF achieves the lowest perplexities for both data sets, which we conjecture is due to the need for a wider number of latent variables available in the third layer (more than 15). The meanfield approximation appears to be more sensitive to the\narchitecture used in the DEF as evidenced by the poor performance of the two layer Poisson DEF on The New York Times. We also observe similar performance when training sigmoid belief networks with varying layers on both data sets."}, {"heading": "7 Discussion", "text": "We present hierarchical variational models: a Bayesian framework for introducing a richer class of posterior approximations, constructed by placing priors on existing tractable variational families. We achieve a tractable objective by estimating a model of the \u201cposterior\u201d of the variational parameters in the hierarchical variational model. We show recovery on a toy example and better posterior approximations on deep discrete latent variable models. There are several avenues for future work such as exploring alternative bounds and analyzing our approach in the empirical Bayes framework. Lastly, new kinds of hierarchical variational model can be explored by placing priors on other tractable variational families and iteratively expanding our recursive posterior approximation to more than simply q and r; this yields a form of annealed variational approximation."}, {"heading": "A Appendix", "text": "Tractable bound on the entropy. Deriving an analytic expression for the entropy of qHVM is generally intractable due to the integral in the definition of qHVM. However, it is tractable when we know the distribution q(\u03bb |z). This can be seen by noting from standard Bayes\u2019 rule that\nq(z)q(\u03bb |z) = q(\u03bb)q(z |\u03bb). (13)\nThe right hand side is specified by the construction of the hierarchical variational model. Note also that q(\u03bb |z) can be interpreted as the posterior distribution of the original variational parameters \u03bb given the model, thus we will denote it as qPOST(\u03bb |z).\nIn general, computing qPOST(\u03bb |z) from the specification of the hierarchical variational model is as hard as the integral needed to compute the entropy from Eq.3. Instead, we approximate qPOST with an auxiliary distribution r(\u03bb |z;\u03c6) parameterized by \u03c6. This yields a bound on the entropy in terms of the analytically known distributions r(\u03bb |z), q(z |\u03bb), and q(\u03bb).\nFirst note that the KL-divergence between two distributions is greater than zero, and is precisely zero only when the two distributions are equal. This means the entropy can be bounded as follows:\n\u2212EqHVM[log qHVM(z)] =\u2212EqHVM[log qHVM(z)\u2212 KL(qPOST(\u03bb |z)||qPOST(\u03bb |z))] \u2265\u2212EqHVM[log qHVM(z) + KL(qPOST(\u03bb |z)||r(\u03bb |z;\u03c6))]] =\u2212EqHVM[EqPOST[log qHVM(z) + log qPOST(\u03bb |z)\n\u2212 log r(\u03bb |z;\u03c6)]] =\u2212Eq(z,\u03bb)[log qHVM(z) + log qPOST(\u03bb |z)\u2212 log r(\u03bb |z;\u03c6)].\nThen by Eq.13, the bound simplifies to\n\u2212EqHVM[log qHVM(z)] \u2265\u2212Eq(z,\u03bb)[log q(\u03bb) + log q(z |\u03bb)\u2212 log r(\u03bb |z;\u03c6)].\nA similar bound in derived by Salimans et al. (2015) directly for log p(x).\nIn the above derivation, the approximation r to the variational posterior qPOST(\u03bb |z) is placed as the second argument of a KL-divergence term. Replacing the first argument instead yields a different tractable upper bound as\nwell.\n\u2212EqHVM[log q(z)] = EqHVM[\u2212 log q(z) + KL(qPOST(\u03bb |z)||qPOST(\u03bb |z))] \u2264 EqHVM[\u2212 log q(z) + KL(r(\u03bb |z;\u03c6)||qPOST(\u03bb |z))]] = EqHVM[Er[\u2212 log q(z)\u2212 log qPOST(\u03bb |z) + log r(\u03bb |z;\u03c6)]]\n= EqHVM[Er[\u2212 log q(z)\u2212 log q(z |\u03bb)q(\u03bb)\nq(z) + log r(\u03bb |z;\u03c6)]]\n= EqHVM[Er[\u2212 log q(\u03bb)\u2212 log q(z |\u03bb) + log r(\u03bb |z;\u03c6)]].\nThe bound is also tractable when r and qHVM can be sampled and all distributions are analytic. The derivation of these two bounds parallels the development of expectation propagation (Minka, 2001) and variational Bayes (Jordan, 1999) which are based on alternative forms of the KL-divergence1. Exploring the role and relative merits of both bounds we derive in the context of variational models will be an important direction in the study of variational models with latent variables.\nGradient Derivation. We derive the gradient of the hierarchical ELBO using its mean-field representation:\nfL (\u03b8 ,\u03c6) = Eq[L (\u03bb)] +Eq[(log r(\u03bb |z;\u03c6)\u2212 log q(\u03bb;\u03b8 ))].\nUsing the reparameterization \u03bb(\u03b5;\u03b8 ), where \u03b5 \u223c s, this is\nfL (\u03b8 ,\u03c6) = Es(\u03b5)[L (\u03bb(\u03b5;\u03b8 ))] +Es(\u03b5)[Eq(z |\u03bb)[(log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)]\u2212 log q(\u03bb(\u03b5;\u03b8 );\u03b8 ))].\nWe now differentiate the three additive terms with respect to \u03b8 . As in the main text, we suppress \u03b8 in the definition of \u03bb when clear and define the score function\nV =\u2207\u03bb log q(z |\u03bb).\nBy the chain rule the derivative of the first term is\n\u2207\u03b8Es(\u03b5)[L (\u03bb(\u03b5;\u03b8 ))] = Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)\u2207\u03bbL (\u03bb)].\nWe now differentiate the second term:\n\u2207\u03b8Es(\u03b5)[Eq(z |\u03bb)[log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6]]\n=\u2207\u03b8Es(\u03b5)\n\u222b\nq(z |\u03bb) log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)dz\n= Es(\u03b5) \u2207\u03b8\n\u222b\nq(z |\u03bb) log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)dz\n= Es(\u03b5) \u2207\u03b8\u03bb(\u03b5)\u2207\u03bb\n\u222b\nq(z |\u03bb) log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)dz\n.\n1Note that the first bound, which corresponds to the objective in expectation propagation (EP), directly minimizes KL(q\u2016r) whereas EP only minimizes this locally.\nApplying the product rule to the inner derivative gives\n\u2207\u03bb\n\u222b\nq(z |\u03bb) log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)dz\n=\n\u222b\n\u2207\u03bbq(z |\u03bb) log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)dz\n+\n\u222b\nq(z |\u03bb)\u2207\u03bb log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)dz\n=\n\u222b\n\u2207\u03bb log q(z |\u03bb)q(z |\u03bb) log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)dz\n+\n\u222b\nq(z |\u03bb)\u2207\u03bb log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)dz\n= Eq(z |\u03bb)[V log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)] +Eq(z |\u03bb)[\u2207\u03bb log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)].\nSubstituting this back into the previous expression gives the gradient of the second term\nEs(\u03b5)[\u2207\u03b8\u03bb(\u03b5)Eq(z |\u03bb)[V log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)]] +Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)Eq(z |\u03bb)[\u2207\u03bb log r(\u03bb(\u03b5;\u03b8 ) |z;\u03c6)]]\nThe third term also follows by the chain rule\n\u2207\u03b8Es(\u03b5)[log q(\u03bb(\u03b5;\u03b8 );\u03b8 )] = Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)\u2207\u03bb log q(\u03bb;\u03b8 ) +\u2207\u03b8 log q(\u03bb;\u03b8 )] = Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)\u2207\u03bb log q(\u03bb;\u03b8 )]\nwhere the last equality follows by\nEs(\u03b5)[\u2207\u03b8 log q(\u03bb;\u03b8 )] = Eq(\u03bb;\u03b8 )[\u2207\u03b8 log q(\u03bb;\u03b8 )] = 0.\nCombining these together gives the total expression for the gradient\n\u2207\u03b8eL(\u03b8 ,\u03c6) = Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)\u2207\u03bbLMF(\u03bb)] +Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)\u2207\u03bb[log r(\u03bb |z;\u03c6)\u2212 log q(\u03bb;\u03b8 )]] +Es(\u03b5)[\u2207\u03b8\u03bb(\u03b5)Eq(z |\u03bb)[V log r(\u03bb |z;\u03c6)]].\nIntroducing ri to the gradient. One term of the gradient involves the product of the score function with all of r,\nEs(\u03b5)[\u2207\u03b8\u03bb(\u03b5)Eq(z |\u03bb)[V log r(\u03bb |z;\u03c6)]].\nLocalizing (Rao-Blackwellizing) the inner expectation as in Ranganath et al. (2014); Mnih and Gregor (2014) can drastically reduce the variance. Recall that\nq(z |\u03bb) = d \u220f\ni=1\nq(zi |\u03bbi).\nNext, we define Vi to be the score functions of the factor. That is\nVi =\u2207\u03bb log q(zi |\u03bbi).\nThis is a vector with nonzero entries corresponding to \u03bbi . Substituting the factorization into the gradient term yields\nEs(\u03b5)\n \u2207\u03b8\u03bb(\u03b5) d \u2211\ni=1\nEq(z |\u03bb)[Vi log r(\u03bb |z;\u03c6)]\n\n . (14)\nNow we define ri to be the terms in log r containing zi and r\u2212i to be the remaining terms. Then the inner expectation in the gradient term is\nd \u2211\ni=1\nEq(z |\u03bb)[Vi(log ri(\u03bb |z;\u03c6) + log r\u2212i(\u03bb |z;\u03c6))]\n= d \u2211\ni=1\nEq(zi |\u03bb)[ViEq(z\u2212i |\u03bb)[log ri(\u03bb |z;\u03c6) + log r\u2212i(\u03bb |z;\u03c6)]],\n= d \u2211\ni=1\nEq(z |\u03bb)[Vi log ri(\u03bb |z;\u03c6)],\nwhere the last equality follows from the expectation of the score function of a distribution is zero. Substituting this back into Eq.14 yields the desired result\nEs(\u03b5)[\u2207\u03b8\u03bb(\u03b5;\u03b8 )Eq(z |\u03bb)[V log r(\u03bb |z;\u03c6)]]\n= Es(\u03b5)\n\n\u2207\u03b8\u03bb(\u03b5;\u03b8 )Eq(z |\u03bb)\n\n\nd \u2211\ni=1\nVi log ri(\u03bb |z;\u03c6)\n\n\n\n .\nEquality of Two Gradients. We now provide a direct connection between the score gradient and the reparameterization gradient. We carry this out in one-dimension for clarity, but the same principle holds in higher dimensions. Let Q be the cumulative distribution function (CDF) of q and let z = T (z0;\u03bb) be reparameterizable in terms of a uniform random variable z0 (inverse-CDF sampling). We focus on the one dimensional case for simplicity. Recall integration by parts computes a definite integral as\n\u222b\nsupp(z)\nw(z)dv(z) = |w(z)v(z)|supp(z) \u2212 \u222b\nsupp(z)\nv(z)dw(z),\nwhere the | \u00b7 | notation indicates evaluation of a portion of the integral. In the subsequent derivation, we let w(z) = log p(x,z) \u2212 log q(z), and let dv(z) = \u2207\u03bb log q(z)q(z) = \u2207\u03bbq(z).\nRecall that we assume that we can CDF-transform z and that the transformation is differentiable. That is, when u is a standard uniform random variable, z = Q\u22121(u,\u03bb).\nThen\n\u2207score\u03bb L = Eq(z |\u03bb)[\u2207\u03bb log q(z |\u03bb)(log p(x,z)\u2212 log q(z |\u03bb))]\n=\n\u222b\nsupp(z)\n\u2207\u03bbq(z |\u03bb)(log p(x,z)\u2212 log q(z |\u03bb))]dz\n=\n\u222b\nz\n\u2207\u03bbq(z |\u03bb)dz (log p(x,z)\u2212 log q(z |\u03bb))\nsupp(z)\n\u2212 \u222b \u222b\nz\n\u2207\u03bbq(z |\u03bb)dz \u2207z[log p(x,z)\u2212 log q(z |\u03bb)]dz\n= \u2207\u03bbQ(z |\u03bb)(log p(x,z)\u2212 log q(z |\u03bb))\nsupp(z)\n\u2212 \u222b\n\u2207\u03bb [Q(z |\u03bb)]\u2207z[log p(x,z)\u2212 log q(z |\u03bb)]dz\n= \u2207\u03bbQ(z |\u03bb)(log p(x,z)\u2212 log q(z |\u03bb))\nsupp(z)\n+\n\u222b\nq(z |\u03bb)\u2207\u03bb [z]\u2207z[log p(x,z)\u2212 log q(z |\u03bb)]dz\n= \u2207\u03bbQ(z |\u03bb)(log p(x,z)\u2212 log q(z |\u03bb))\nsupp(z)\n+\u2207rep\u03bb L ,\nwhere the second to last equality follows by the derivative of the CDF function (Hoffman and Blei, 2015). By looking at the Monte Carlo expression of both sides, we can see the reduction in variance that the reparameterization gradient has over the score gradient comes from the analytic computation of the gradient of the definite integral (which has value 0).\nOptimizing with Discrete Variables in the Variational Prior. In the setting where the prior has discrete variables, optimization requires a little more work. First we note that in a non-degenerate mean-field setup that the \u03bb\u2019s are differentiable parameters of probability distributions. This means they will always, conditional on the discrete variables, be differentiable in the variational prior. This means that we can both compute the gradients for these parameters using the technique from above and that the discrete variables exist at a higher level of the hierarchical variational model. The gradients of discrete variables can be computed using the score gradient, but Monte Carlo estimates of this will have high variance due to no simplification of the learning signal (like in the mean-field). We can step around this issue by using local expectation gradients (Titsias, 2015) which marginalize out one variable at a time to get low variance stochastic gradients. This is generally tractable when the discrete variables have small support such as in binary variables."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy)", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Fundamentals of Statistical Exponential Families", "author": ["L. Brown"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "Brown,? \\Q1986\\E", "shortCiteRegEx": "Brown", "year": 1986}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": null, "citeRegEx": "Cover and Thomas,? \\Q2012\\E", "shortCiteRegEx": "Cover and Thomas", "year": 2012}, {"title": "Helmholtz machines and wake-sleep learning. Handbook of Brain Theory and Neural Network", "author": ["P. Dayan"], "venue": null, "citeRegEx": "Dayan,? \\Q2000\\E", "shortCiteRegEx": "Dayan", "year": 2000}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Combining possibly related estimation problems", "author": ["B. Efron", "C. Morris"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Efron and Morris,? \\Q1973\\E", "shortCiteRegEx": "Efron and Morris", "year": 1973}, {"title": "Nonparametric variational inference", "author": ["S. Gershman", "M. Hoffman", "D. Blei"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Gershman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2012}, {"title": "A tutorial on Bayesian nonparametric models", "author": ["S.J. Gershman", "D.M. Blei"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "Gershman and Blei,? \\Q2012\\E", "shortCiteRegEx": "Gershman and Blei", "year": 2012}, {"title": "Factorial learning and the em algorithm", "author": ["Z. Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ghahramani,? \\Q1995\\E", "shortCiteRegEx": "Ghahramani", "year": 1995}, {"title": "Structured Stochastic Variational Inference", "author": ["M.D. Hoffman", "D.M. Blei"], "venue": "In Artificial Intelligence and Statistics", "citeRegEx": "Hoffman and Blei,? \\Q2015\\E", "shortCiteRegEx": "Hoffman and Blei", "year": 2015}, {"title": "Improving the Mean Field Approximation Via the Use of Mixture Distributions", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "In Learning in Graphical Models,", "citeRegEx": "Jaakkola and Jordan,? \\Q1998\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 1998}, {"title": "Learning in Graphical Models", "author": ["M. Jordan"], "venue": null, "citeRegEx": "Jordan,? \\Q1999\\E", "shortCiteRegEx": "Jordan", "year": 1999}, {"title": "Introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakkola", "L. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": "In International Conference on Learning Representations (ICLR-14)", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Automatic Variational Inference in Stan", "author": ["A. Kucukelbir", "R. Ranganath", "A. Gelman", "D.M. Blei"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Kucukelbir et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kucukelbir et al\\.", "year": 2015}, {"title": "Variational Inference in Probabilistic Models", "author": ["N. Lawrence"], "venue": "PhD thesis", "citeRegEx": "Lawrence,? \\Q2000\\E", "shortCiteRegEx": "Lawrence", "year": 2000}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["T. Minka"], "venue": "PhD thesis,", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Learning stochastic feedforward networks", "author": ["R. Neal"], "venue": "Tech. Rep. CRG-TR-90-7: Department of Computer Science,", "citeRegEx": "Neal,? \\Q1990\\E", "shortCiteRegEx": "Neal", "year": 1990}, {"title": "An Introduction to Copulas (Springer Series in Statistics)", "author": ["R.B. Nelsen"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Nelsen,? \\Q2006\\E", "shortCiteRegEx": "Nelsen", "year": 2006}, {"title": "Deep Exponential Families", "author": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei"], "venue": "In Artificial Intelligence and Statistics", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Stochastic back-propagation and variational infernece in deep latent gaussian models. ArXiv e-prints", "author": ["D. Rezende", "S. Mohamed", "D. Wierstra"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["D.J. Rezende", "S. Mohamed"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML-15)", "citeRegEx": "Rezende and Mohamed,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed", "year": 2015}, {"title": "The empirical bayes approach to statistical decision problems", "author": ["H. Robbins"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Robbins,? \\Q1964\\E", "shortCiteRegEx": "Robbins", "year": 1964}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Robbins and Monro,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Statedependent exploration for policy gradient methods", "author": ["T. R\u00fcckstie\u00df", "M. Felder", "J. Schmidhuber"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "R\u00fcckstie\u00df et al\\.,? \\Q2008\\E", "shortCiteRegEx": "R\u00fcckstie\u00df et al\\.", "year": 2008}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["T. Salimans", "D. Kingma", "M. Welling"], "venue": "In International Conference on Artifical Intelligence and Statistics", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Fixed-form variational posterior approximation through stochastic linear regression", "author": ["T. Salimans", "Knowles", "D. A"], "venue": "Bayesian Analysis,", "citeRegEx": "Salimans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2013}, {"title": "Policy gradients with parameterbased exploration for control", "author": ["F. Sehnke", "C. Osendorfer", "T. R\u00fcckstie\u00df", "A. Graves", "J. Peters", "J. Schmidhuber"], "venue": "In Artificial Neural NetworksICANN", "citeRegEx": "Sehnke et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sehnke et al\\.", "year": 2008}, {"title": "Learning stochastic inverses", "author": ["A. Stuhlm\u00fcller", "J. Taylor", "N. Goodman"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Stuhlm\u00fcller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Stuhlm\u00fcller et al\\.", "year": 2013}, {"title": "Doubly stochastic variational bayes for non-conjugate inference", "author": ["M. Titsias", "M. L\u00e1zaro-Gredilla"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Titsias and L\u00e1zaro.Gredilla,? \\Q2014\\E", "shortCiteRegEx": "Titsias and L\u00e1zaro.Gredilla", "year": 2014}, {"title": "Local Expectation Gradients for Doubly Stochastic Variational Inference", "author": ["M.K. Titsias"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Titsias,? \\Q2015\\E", "shortCiteRegEx": "Titsias", "year": 2015}, {"title": "Copula variational inference", "author": ["D. Tran", "D.M. Blei", "E.M. Airoldi"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Tran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2015}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M. Wainwright", "M. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Evaluation methods for topic models", "author": ["H. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "In Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}], "referenceMentions": [{"referenceID": 13, "context": "BBVI is a form of variational inference (Jordan et al., 1999; Wainwright and Jordan, 2008).", "startOffset": 40, "endOffset": 90}, {"referenceID": 34, "context": "BBVI is a form of variational inference (Jordan et al., 1999; Wainwright and Jordan, 2008).", "startOffset": 40, "endOffset": 90}, {"referenceID": 23, "context": "Note the prior is a choice\u2014for example, we will show how to use the recently-proposed normalizing flows (Rezende and Mohamed, 2015) as a prior to a mean-field distribution.", "startOffset": 104, "endOffset": 131}, {"referenceID": 21, "context": "We demonstrate our methods with a study of approximate posteriors for several variants of deep exponential families (with Poisson layers) (Ranganath et al., 2015).", "startOffset": 138, "endOffset": 162}, {"referenceID": 10, "context": "Inspired by kernel density estimation, this was classically studied by Jaakkola and Jordan (1998), and later revisited by Gershman et al.", "startOffset": 71, "endOffset": 98}, {"referenceID": 7, "context": "Inspired by kernel density estimation, this was classically studied by Jaakkola and Jordan (1998), and later revisited by Gershman et al. (2012), in which one speciar X iv :1 51 1.", "startOffset": 122, "endOffset": 145}, {"referenceID": 31, "context": "There has been a line of work for variational approximations that capture dependencies in differentiable probability models (Titsias and L\u00e1zaro-Gredilla, 2014; Rezende and Mohamed, 2015; Salimans et al., 2015; Kucukelbir et al., 2015), but there has been limited work in variational methods that capture dependencies with discrete latent variables.", "startOffset": 124, "endOffset": 234}, {"referenceID": 23, "context": "There has been a line of work for variational approximations that capture dependencies in differentiable probability models (Titsias and L\u00e1zaro-Gredilla, 2014; Rezende and Mohamed, 2015; Salimans et al., 2015; Kucukelbir et al., 2015), but there has been limited work in variational methods that capture dependencies with discrete latent variables.", "startOffset": 124, "endOffset": 234}, {"referenceID": 27, "context": "There has been a line of work for variational approximations that capture dependencies in differentiable probability models (Titsias and L\u00e1zaro-Gredilla, 2014; Rezende and Mohamed, 2015; Salimans et al., 2015; Kucukelbir et al., 2015), but there has been limited work in variational methods that capture dependencies with discrete latent variables.", "startOffset": 124, "endOffset": 234}, {"referenceID": 15, "context": "There has been a line of work for variational approximations that capture dependencies in differentiable probability models (Titsias and L\u00e1zaro-Gredilla, 2014; Rezende and Mohamed, 2015; Salimans et al., 2015; Kucukelbir et al., 2015), but there has been limited work in variational methods that capture dependencies with discrete latent variables.", "startOffset": 124, "endOffset": 234}, {"referenceID": 18, "context": "Such methods typically apply the score function estimator (Ranganath et al., 2014; Mnih and Gregor, 2014).", "startOffset": 58, "endOffset": 105}, {"referenceID": 15, "context": "Lawrence (2000) explores a rich class of variational approximations formed by both mixtures and Markov chains.", "startOffset": 0, "endOffset": 16}, {"referenceID": 15, "context": ", 2015; Kucukelbir et al., 2015), but there has been limited work in variational methods that capture dependencies with discrete latent variables. Such methods typically apply the score function estimator (Ranganath et al., 2014; Mnih and Gregor, 2014). These methods can be used to posit rich approximations, but are either limited by the noise in stochastic gradients, or are quadratic in the number of latent variables. For example Mnih and Gregor (2014) apply such techniques for variational approximations in sigmoid belief networks, but this approach is limited to stochastic feed forward networks.", "startOffset": 8, "endOffset": 458}, {"referenceID": 15, "context": ", 2015; Kucukelbir et al., 2015), but there has been limited work in variational methods that capture dependencies with discrete latent variables. Such methods typically apply the score function estimator (Ranganath et al., 2014; Mnih and Gregor, 2014). These methods can be used to posit rich approximations, but are either limited by the noise in stochastic gradients, or are quadratic in the number of latent variables. For example Mnih and Gregor (2014) apply such techniques for variational approximations in sigmoid belief networks, but this approach is limited to stochastic feed forward networks. Additionally, the variance increases as the number of layers increase. Tran et al. (2015) propose copulas (COPULA VI) as a way of learning dependencies in factorized approximations.", "startOffset": 8, "endOffset": 695}, {"referenceID": 15, "context": ", 2015; Kucukelbir et al., 2015), but there has been limited work in variational methods that capture dependencies with discrete latent variables. Such methods typically apply the score function estimator (Ranganath et al., 2014; Mnih and Gregor, 2014). These methods can be used to posit rich approximations, but are either limited by the noise in stochastic gradients, or are quadratic in the number of latent variables. For example Mnih and Gregor (2014) apply such techniques for variational approximations in sigmoid belief networks, but this approach is limited to stochastic feed forward networks. Additionally, the variance increases as the number of layers increase. Tran et al. (2015) propose copulas (COPULA VI) as a way of learning dependencies in factorized approximations. Copulas can be extended to the framework of hierarchical variational models, whereas the direct approach taken in Tran et al. (2015) requires computation quadratic in the number of latent variables.", "startOffset": 8, "endOffset": 920}, {"referenceID": 13, "context": "In variational inference, one posits a family of distributions q(z;\u03bb), parameterized by \u03bb, and minimizes the KL divergence to the posterior distribution (Jordan et al., 1999; Bishop, 2006; Wainwright and Jordan, 2008).", "startOffset": 153, "endOffset": 217}, {"referenceID": 1, "context": "In variational inference, one posits a family of distributions q(z;\u03bb), parameterized by \u03bb, and minimizes the KL divergence to the posterior distribution (Jordan et al., 1999; Bishop, 2006; Wainwright and Jordan, 2008).", "startOffset": 153, "endOffset": 217}, {"referenceID": 34, "context": "In variational inference, one posits a family of distributions q(z;\u03bb), parameterized by \u03bb, and minimizes the KL divergence to the posterior distribution (Jordan et al., 1999; Bishop, 2006; Wainwright and Jordan, 2008).", "startOffset": 153, "endOffset": 217}, {"referenceID": 24, "context": "The augmentation with a variational prior has strong ties to empirical Bayesian methods, which use data to estimate hyperparameters of a prior distribution (Robbins, 1964; Efron and Morris, 1973).", "startOffset": 156, "endOffset": 195}, {"referenceID": 6, "context": "The augmentation with a variational prior has strong ties to empirical Bayesian methods, which use data to estimate hyperparameters of a prior distribution (Robbins, 1964; Efron and Morris, 1973).", "startOffset": 156, "endOffset": 195}, {"referenceID": 26, "context": "A similar methodology also arises in the policy search literature (R\u00fcckstie\u00df et al., 2008; Sehnke et al., 2008).", "startOffset": 66, "endOffset": 111}, {"referenceID": 29, "context": "A similar methodology also arises in the policy search literature (R\u00fcckstie\u00df et al., 2008; Sehnke et al., 2008).", "startOffset": 66, "endOffset": 111}, {"referenceID": 3, "context": "4 is tighter than the trivial conditional entropy bound of H[qHVM]\u2265H[q |\u03bb] (Cover and Thomas, 2012).", "startOffset": 75, "endOffset": 99}, {"referenceID": 5, "context": "Similar to the EM-algorithm (Dempster et al., 1977), optimizing \u03b8 improves the posterior approximation, while optimizing \u03c6", "startOffset": 28, "endOffset": 51}, {"referenceID": 11, "context": "A mixture of Gaussians can approximate arbitrary distributions given enough components and has been considered in traditional literature (Jaakkola and Jordan, 1998; Lawrence, 2000; Gershman and Blei, 2012).", "startOffset": 137, "endOffset": 205}, {"referenceID": 16, "context": "A mixture of Gaussians can approximate arbitrary distributions given enough components and has been considered in traditional literature (Jaakkola and Jordan, 1998; Lawrence, 2000; Gershman and Blei, 2012).", "startOffset": 137, "endOffset": 205}, {"referenceID": 8, "context": "A mixture of Gaussians can approximate arbitrary distributions given enough components and has been considered in traditional literature (Jaakkola and Jordan, 1998; Lawrence, 2000; Gershman and Blei, 2012).", "startOffset": 137, "endOffset": 205}, {"referenceID": 23, "context": "Normalizing flows, which was previously introduced as a variational approximation for differentiable probability models in Rezende and Mohamed (2015), work by transforming a random variable \u03bb0 with a known simple distribution such as the standard normal through a sequence of invertible functions f1 to fK .", "startOffset": 123, "endOffset": 150}, {"referenceID": 23, "context": "Functions that lead to complicated densities with efficient linear-time Jacobians can be created (Rezende and Mohamed, 2015).", "startOffset": 97, "endOffset": 124}, {"referenceID": 20, "context": "For example, copulas explicitly introduce dependence in a collection of d random variables with parameterized marginals, using joint distributions on the d-dimensional hypercubes (Nelsen, 2006).", "startOffset": 179, "endOffset": 193}, {"referenceID": 9, "context": "A class of variational models can also be constructed by replacing the mixture model with a factorial mixture (Ghahramani, 1995).", "startOffset": 110, "endOffset": 128}, {"referenceID": 25, "context": "We maximize the hierarchical ELBO with stochastic optimization (Robbins and Monro, 1951), which follows noisy, yet unbiased, gradients of the objective.", "startOffset": 63, "endOffset": 88}, {"referenceID": 36, "context": "It has strong roots in the policy search literature and evolutionary algorithms, where it is more commonly known as the REINFORCE gradient (Williams, 1992).", "startOffset": 139, "endOffset": 155}, {"referenceID": 21, "context": "See Ranganath et al. (2014) for a full derivation.", "startOffset": 4, "endOffset": 28}, {"referenceID": 18, "context": "Roughly, the variance of this estimator scales with the number of random variables (Ranganath et al., 2014; Mnih and Gregor, 2014).", "startOffset": 83, "endOffset": 130}, {"referenceID": 32, "context": "Empirically reparameterization gradients have been shown to have much lower variance than the score function gradient (Titsias, 2015).", "startOffset": 118, "endOffset": 133}, {"referenceID": 0, "context": "In general, these gradients can be computed via automatic differentiation systems such as those available in Stan and Theano (Stan Development Team, 2015; Bergstra et al., 2010); this removes the need for model-specific computations, and moreover we note that no assumption has been made on log p(x,z) other than the ability to calculate it.", "startOffset": 125, "endOffset": 177}, {"referenceID": 32, "context": "Local expectation gradients (Titsias, 2015) provide an efficient gradient estimator for variational approximations over discrete variables with small support, done by analytically marginalizing over each discrete variable individually.", "startOffset": 28, "endOffset": 43}, {"referenceID": 4, "context": "The use of inference networks (Dayan, 2000; Stuhlm\u00fcller et al., 2013; Kingma and Welling, 2014; Rezende et al., 2014) amortizes the cost of estimating these local variational parameters by tying them together through a neural network.", "startOffset": 30, "endOffset": 117}, {"referenceID": 30, "context": "The use of inference networks (Dayan, 2000; Stuhlm\u00fcller et al., 2013; Kingma and Welling, 2014; Rezende et al., 2014) amortizes the cost of estimating these local variational parameters by tying them together through a neural network.", "startOffset": 30, "endOffset": 117}, {"referenceID": 14, "context": "The use of inference networks (Dayan, 2000; Stuhlm\u00fcller et al., 2013; Kingma and Welling, 2014; Rezende et al., 2014) amortizes the cost of estimating these local variational parameters by tying them together through a neural network.", "startOffset": 30, "endOffset": 117}, {"referenceID": 22, "context": "The use of inference networks (Dayan, 2000; Stuhlm\u00fcller et al., 2013; Kingma and Welling, 2014; Rezende et al., 2014) amortizes the cost of estimating these local variational parameters by tying them together through a neural network.", "startOffset": 30, "endOffset": 117}, {"referenceID": 31, "context": ", 2014) O (d) O (d) 7 discrete/continuous DSVI (Titsias and L\u00e1zaro-Gredilla, 2014) O (d2) O (d2) 3 differentiable COPULA VI (Tran et al.", "startOffset": 47, "endOffset": 82}, {"referenceID": 33, "context": ", 2014) O (d) O (d) 7 discrete/continuous DSVI (Titsias and L\u00e1zaro-Gredilla, 2014) O (d2) O (d2) 3 differentiable COPULA VI (Tran et al., 2015) O (d2) O (d2) 3 discrete/continuous MIXTURE (Jaakkola and Jordan, 1998; Lawrence, 2000) O (Kd) O (Kd) 3 discrete/continuous NF (Rezende and Mohamed, 2015) O (Kd) O (Kd) 3 differentiable HIERARCHICAL VM w/ NF O (Kd) O (Kd) 3 discrete/continuous", "startOffset": 124, "endOffset": 143}, {"referenceID": 11, "context": ", 2015) O (d2) O (d2) 3 discrete/continuous MIXTURE (Jaakkola and Jordan, 1998; Lawrence, 2000) O (Kd) O (Kd) 3 discrete/continuous NF (Rezende and Mohamed, 2015) O (Kd) O (Kd) 3 differentiable HIERARCHICAL VM w/ NF O (Kd) O (Kd) 3 discrete/continuous", "startOffset": 52, "endOffset": 95}, {"referenceID": 16, "context": ", 2015) O (d2) O (d2) 3 discrete/continuous MIXTURE (Jaakkola and Jordan, 1998; Lawrence, 2000) O (Kd) O (Kd) 3 discrete/continuous NF (Rezende and Mohamed, 2015) O (Kd) O (Kd) 3 differentiable HIERARCHICAL VM w/ NF O (Kd) O (Kd) 3 discrete/continuous", "startOffset": 52, "endOffset": 95}, {"referenceID": 23, "context": ", 2015) O (d2) O (d2) 3 discrete/continuous MIXTURE (Jaakkola and Jordan, 1998; Lawrence, 2000) O (Kd) O (Kd) 3 discrete/continuous NF (Rezende and Mohamed, 2015) O (Kd) O (Kd) 3 differentiable HIERARCHICAL VM w/ NF O (Kd) O (Kd) 3 discrete/continuous", "startOffset": 135, "endOffset": 162}, {"referenceID": 21, "context": "Then we compare our proposed variational approximations to more standard ones on deep exponential families (Ranganath et al., 2015), a class of hierarchical models where each observation is represented by multiple layers of exponential family random variables.", "startOffset": 107, "endOffset": 131}, {"referenceID": 21, "context": "The deep exponential family (DEF) (Ranganath et al., 2015) forms a class of probabilistic models built from exponential families (Brown, 1986) whose latent structure parallels the architectures used in deep neural networks.", "startOffset": 34, "endOffset": 58}, {"referenceID": 2, "context": ", 2015) forms a class of probabilistic models built from exponential families (Brown, 1986) whose latent structure parallels the architectures used in deep neural networks.", "startOffset": 78, "endOffset": 91}, {"referenceID": 19, "context": "In a sigmoid belief network introduced by Neal (1990), each observation either turns a feature on or off, while in a Poisson DEF each observation expresses each feature a positive integer number of times.", "startOffset": 42, "endOffset": 54}, {"referenceID": 21, "context": "We compare to the mean-field approximation from Ranganath et al. (2015).", "startOffset": 48, "endOffset": 72}, {"referenceID": 35, "context": "This is a document complete evaluation metric (Wallach et al., 2009) where the words are tested independently.", "startOffset": 46, "endOffset": 68}, {"referenceID": 21, "context": "We study one, two, and three layer DEFs with 100, 30, and 15 units respectively and set prior hyperparameters following Ranganath et al. (2015). We use Nesterov\u2019s accelerated gradient with momentum parameter of 0.", "startOffset": 120, "endOffset": 144}], "year": 2017, "abstractText": "Black box inference allows researchers to easily prototype and evaluate an array of models. Recent advances in variational inference allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution which maintains efficient computation? To address this, we develop hierarchical variational models. In a HIERARCHICAL VM, the variational approximation is augmented with a prior on its parameters, such that the latent variables are conditionally independent given this shared structure. This preserves the computational efficiency of the original approximation, while admitting hierarchically complex distributions for both discrete and continuous latent variables. We study HIERARCHICAL VM on a variety of deep discrete latent variable models. HIERARCHICAL VM generalizes other expressive variational distributions and maintains higher fidelity to the posterior.", "creator": "LaTeX with hyperref package"}}}