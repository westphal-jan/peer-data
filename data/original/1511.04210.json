{"id": "1511.04210", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2015", "title": "On the Quality of the Initial Basin in Overspecified Neural Networks", "abstract": "Over the past few years, artificial neural networks have seen a dramatic resurgence in popularity as a tool for solving hard learning problems in AI applications. While it is widely known that neural networks are computationally hard to train in the worst case, in practice, neural networks are trained efficiently using SGD methods and a variety of techniques which accelerate the learning process. One mechanism which has been suggested to explain this is overspecification, which is the training of a network larger than what would be needed with unbounded computational power. Empirically, despite worst-case NP-hardness results, large networks tend to achieve a smaller error over the training set.", "histories": [["v1", "Fri, 13 Nov 2015 09:35:34 GMT  (313kb,D)", "http://arxiv.org/abs/1511.04210v1", null], ["v2", "Tue, 9 Feb 2016 16:22:46 GMT  (164kb,D)", "http://arxiv.org/abs/1511.04210v2", "Significantly different version, with more general results"], ["v3", "Tue, 14 Jun 2016 05:39:27 GMT  (164kb,D)", "http://arxiv.org/abs/1511.04210v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["itay safran", "ohad shamir"], "accepted": true, "id": "1511.04210"}, "pdf": {"name": "1511.04210.pdf", "metadata": {"source": "CRF", "title": "On the Quality of the Initial Basin in Overspecified Neural Networks", "authors": ["Itay Safran", "Ohad Shamir"], "emails": ["itay.safran@weizmann.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "In this work, we aspire to understand this phenomenon. In particular, we wish to better understand the behavior of the error over the sample as a function of the weights of the network, where we focus mostly on neural nets comprised of 2 layers, although we will also consider single neuron nets and nets of arbitrary depth, investigating properties such as the number of local minima the function has, and the probability of initializing from a basin with a given minimal value, with the goal of finding reasonable conditions under which efficient learning of the network is possible."}, {"heading": "1 Introduction", "text": "Inspired by current understanding of humans\u2019 central nervous system, artificial feedforward neural networks are a class of predictors characterized by a directed graph, where each node computes a weighted linear combination of its inputs, followed by a non-linear scalar activation function. Propagating the input forward through the graph, the net returns its value via the output of the final layer in the graph. Artificial neural nets are a known to have broad expressive power [17, 18, 20]. A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].\nThe biggest hurdle in training neural networks is the training time, and indeed most theoretical results in the field are negative results. By reduction to k-coloring, it has been shown that finding the weights that best fit the training set is NP-hard [6]. This theoretical hardness result is contrasted by their good practical performance, hence motivating this research.\nWhen dealing with optimization problems such as minimizing the loss over a sample as a function of the weights of the network, a pivotal property of the objective function is the number and distribution of its local minima. This property can indicate the difficulty of the minimization problem; If a function has exponentially many poor local minima (as a function of the input dimension), then without any additional knowledge regarding the function, stochastic gradient descent methods (which are the methods of choice for training such networks) are prone to failure, and will likely converge to such a poor local minima depending\nar X\niv :1\n51 1.\n04 21\n0v 1\n[ cs\n.L G\n] 1\n3 N\nov 2\non the starting point of the algorithm. It is also known that even neural networks comprised of a single neuron might have exponentially many local minima [2].\nAt the other extreme, if a function has a single local minimum, then stochastic gradient descent methods will generally be quite efficient under \u201cmost\u201d random starting points. Consider a huge neural network where the size of the layer before the output layer (henceforth, \u201cthe last hidden layer\u201d) is at least the size of the training sample. It was recently shown that for such networks, under mild assumptions, global optima are ubiquitous, and \u201cmost\u201d starting points will lead to the global optima upon optimizing the weights of the last layer [17]. Unfortunately, due to the huge size of such a network, it is very prone to overfitting. However, it does suggest that large networks are indeed easier to train, and perhaps in such cases the set of points from which SGD methods converge to a satisfactory configuration is relatively large. This naturally motivates the idea of overspecification: Making the network somewhat larger than what would be needed (in terms of reaching a given training error, given unbounded computational power), but not so large as to cause overfitting, in the hope that it will make the training problem easier. There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited. A main goal of our work is to further study the effectiveness of overspecification, and identify cases where it is possible to provide rigorous guarantees.\nWhat we do\nOne way to study the hardness of the training problem has been to consider the number of non-global local minima the objective function has [2]. However, it is possible that good local minima (in terms of objective value) are nevertheless easy to find, either by consisting of a large portion of the local minima, or by being significantly more attractive to first order optimization methods, such as stochastic gradient descent. On the other hand, we do not want our results to depend on the exact details of the optimization method used. Therefore, informally speaking, we mostly focus on the following question: For a given network, assuming we initialize the weights at random, what is the probability that we begin from a basin with a good local minimum?\nWe study this question starting with a single neuron (section 3), and then move to fully connected neural nets of higher depth, with a single output neuron and using a rectified linear unit (ReLU) as the activation function (sections 4-6). More specifically, we provide the following results:\n\u2022 We begin by extending the result of [2] on exponentially many local minimum basins for single neurons, by showing that it holds even if there exists a hypothesis which achieves an arbitrarily small positive training error. We then show that this result is in fact surprisingly brittle to overspecification: Using the same dataset, and replacing a single neuron by a small two-layer network, the probability of reaching a global minimum basin converges exponentially to 1 with the network size.\n\u2022 We identify and formally study geometric properties of the objective function, when training neural networks with ReLU activation function and the squared loss, which may contribute to its amenability to first-order optimization methods. In particular, we show that for ReLU networks of arbitrary depth, with high probability, there exists a strictly decreasing path from a random initialization point to a global minimum. Although this does not ensure that such a global minimum will be reached, it does mean that \u201ccrossing valleys\u201d across the non-convex loss surface is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11]. We also upper bound the objective values of local minima for such general networks. Finally, we prove other useful properties more specific for two-layer networks, such as establishing the piece-wise convex structure of the objective function, with respect to the optimization over the weights of the first layer.\n\u2022 Focusing on two-layer ReLU nets and the squared loss, we study conditions under which overspecification and random initialization will land us in a good basin with high probability. We first show this under a realizability assumption (i.e. that there exist a small two-layer net attaining zero loss on the training set). However, the required amount of overspecification can scale exponentially with the dimension (although it can also be the data\u2019s intrinsic dimension). Next, using a different technique, and without requiring realizability nor a harsh dependence on the dimension, we also show a similar result for high-dimensional data, as long as the space dimension d exceeds the data size. In general, since we do not incorporate regularization, this is a regime prone to overfitting. However, we strengthen this result, by showing a similar result which holds even for data whose size exceeds d, as long as it is clustered in k \u2264 d clusters in general position (which is often a reasonable assumption).\nWe emphasize that when using first order optimization techniques, initializing from an optimal basin does not guarantee we converge to the global minimum, nor does initializing from a bad basin guarantees converging to a bad minimum, as the gradient step taken in gradient descent might throw our algorithm to a different basin, while in stochastic gradient descent further uncertainty is introduced via the stochastic nature of the algorithm. For this reason, we keep our focus on the geometry of the objective function rather than the probability of a specific method producing a good result.\nFinally, we remark that unlike some other works in the field, we focus on the training error and do not explicitly consider the generalization capabilities of neural nets or their expressive power.\nRelated work\nWe begin by discussing the work done in [17], where the authors show that for any non-linear activation function and architecture large enough such that the last hidden layer is at least as large as the size of the sample, then global minima are ubiquitous. The key observation behind this result is that if we assume that our network is large enough, specifically the size of the last hidden layer is larger than the data size, then we can ignore the weights of the previous layers by fixing them arbitrarily and then solve the corresponding optimization problem w.r.t. the weights of the last layer, which will become tractable due to the non-linear nature of the activation function. They also show empirically how overspecification can aid training using stochastic gradient descent. While we shall also highlight the effect overspecification has on the surface of the objective function, our theoretical result focus on networks with mild overspecification, and of size smaller than the data size. Also, the authors investigate regimes in which efficient learning of nets with polynomial activation functions are possible, whereas we shall only focus on the more common activation functions, such as ReLU or sigmoid.\nOther recent efforts in the field include [1, 3, 12]. In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19]. In the above framework, the author demonstrates how the problem of training a net of the aforementioned architecture can be reduced to another NP-Hard problem which allows for a geometric interpretation of the problem with potential approximation algorithms. In [1], the authors show that two-layer neural nets of sufficient size can learn low degree polynomials using gradient descent. In [12], the authors present an algorithm with provable guarantees for training neural nets via tensor decompositions. However, strong assumptions need to be made, such as knowing the underlying distribution over the training instances, as well as derivatives of it. Unlike our work, which mainly investigates the geometry of the objective function without highlighting any specific learning algorithm, the authors of the three aforementioned papers are mainly concerned with the algorithmic aspect of learning and approximating the net properly.\nAnother relevant work is [7], in which the authors investigate the surface area of the loss function of neural nets using ReLUs as activation functions, and make somewhat strong distributional assumptions on the data, such as assuming that the coordinates of the observations in the data are standard normally distributed, and in particular the coordinates are independent of one another, and modeling the connection between neurons as a Bernoulli random variable with constant success probability. In addition to these assumptions, the authors also restrict the set of weights of the class of nets to a sphere. The aforementioned assumptions allow the authors to translate the loss over the data as a function of the weights of a fixed depth H and asymptotically increasing network size to the Hamiltonian of the H-spin spherical spin-glass model, which has been studied extensively in the last few decades. Results on the H-spin glass model imply that for asymptotically large networks all critical values of the loss function whose Hessian has a non-diverging number of negative eigenvalues (which indicates a \u201csmall\u201d number of descending directions) must lie in a finite interval, under which critical points are unlikely to form, and above it critical points have a diverging number of negative eigenvalues in the size of the network, and represent saddle points. See also [10] and references therein for related works along similar lines. While this work and ours are similar, in that both investigate the geometry of the objective function and its critical points, we pose milder assumptions on the distribution, and provide non-asymptotic guarantees."}, {"heading": "2 Preliminaries and notation", "text": ""}, {"heading": "2.1 Neural nets and Machine Learning conventions", "text": "We begin by giving a formal definition of the type of artificial neural network studied in this work. A fully connected feedforward artificial neural network is a directed acyclic graphN = (V,E). The neurons can be decomposed into layers where each neuron is connected to all neurons in the succeeding layer, so no cycles are possible. Each neuron computes a function of the form x 7\u2192 \u03c6 ( w>x + b ) where w is a weight vector and b is a bias term specific to that neuron, and \u03c6 : R \u2192 R is a fixed non-linear activation function, such as a rectified linear unit (ReLU, defined as \u03c6 (z) = max {0, z}) or a sigmoid (\u03c6 (z) = 1\n1+e\u2212z )). For a vector b = (b1, . . . , bn) and a matrix\nW = . . . w1 . . .... . . . wn . . .  letting \u03c6 (Wx + b) := ( \u03c6 ( w>1 x + b1 ) , . . . , \u03c6 ( w>n x + bn )) , we can define a layer of n neurons as\nx 7\u2192 \u03c6 (Wx + b)\nFinally, by denoting the output of the ith layer as Oi for , we can define a network of arbitrary depth recursively by\nOi+1 = \u03c6 (Wi+1Oi + bi+1)\nwhere Wi,bi represents the matrix of weights and bias of the ith layer, respectively. We assume for simplicity that for multi-layer networks, the final layer h computes a scalar value, and is a purely linear function with no bias, i.e. Oh = \u3008Wh, Oh\u22121\u3009. Define the depth of the net as the number of layers h, and denote the number of neurons ni in the ith layer as the size of the layer. We define the width of a net as maxi\u2208[h] ni.\nWe point out that for simplicity, we do not impose any constraints on the weights of each neuron (i.e. regularization, or having convolutional layers).\nMoreover, to simplify the analysis when discussing networks of depth 1 or 2, we will work with nets without bias (where b = 0 for all neurons, not just the output neuron). This is justified, since one could simulate the additional bias term by incrementing the dimension of the data and mapping an instance in the dataset using x 7\u2192 (x, 1) \u2208 Rd+1, in this way the last coordinate of the weight of a neuron will function as a bias term. Having such a fixed coordinate does not affect the validity of our results for 1 and 2 layer nets. The only exception is theorem 2, where it is assumed that each training instance is a singleton with a non-zero value on a different coordinate. However, this result serves merely as an example of how the construction from theorem 1 breaks down for two-layer nets (even without bias). With this sole exception, whenever we do not introduce a bias term in our analysis, then one can increment the dimension of the dataset by 1, and apply the result to an architecture with bias. This is done in order to simplify our analysis. We use S = (xt, yt) m t=1 to denote the training data, where xt \u2208 Rd represents the tth training instance of dimension d and yt \u2208 R represents the corresponding target value, and where m is used to denote the number of instances in the sample.\nWhen referring to a set of predictors H, we identify each predictor in the class using its set of weights, i.e. when our class is the class of single neuron nets (section 3), each predictor is identified using a set of weights w \u2208 Rd, where we use the notation wj to denote its jth coordinate, and when our class is the class of two-layer nets (sections 4-6), each predictor is identified using a set of nd+n weights as explained in the next definition.\nDefinition 1. (Two-layer neural network) To represent the output of a fully connected neural net with 2- layers without bias on the input x \u2208 Rd, where the first layer is of size n and W = (w1, . . . ,wn) are its weights, and v \u2208 Rn are the weights of the output neuron, we define\nNn (W,v) (x) := n\u2211 i=1 vi \u00b7 \u03c6 (\u3008wi,x\u3009)\nwhere \u3008a,b\u3009 := \u2211d\ni=1 aibi represents the dot product of two vectors a,b \u2208 Rd, and \u03c6 : R \u2192 R is a non-linear activation function.\nDefinition 2. (Objective function) When discussing a single neuron and various loss functions, we define the objective function as\nES (w) := 1\nm m\u2211 t=1 L (\u03c6 (\u3008w,xt\u3009) , yt)\nwhere L is the loss function. When discussing two-layer networks and the squared loss, we use the following notation\nES (Nn (W,v)) := m\u2211 t=1 (Nn (W,v) (x)\u2212 yt)2\nDefinition 3. (Positive-homogeneity) An activation function \u03c6 is said to be positive-homogeneous if \u2200c \u2208 (0,\u221e) \u03c6 (x) = c\u03c6 ( x c ) . A ReLU is one such example.\nDefinition 4. ( -realizability) A sample S is said to be -realizable for some \u2265 0 with respect to a class of predictors H if there exists some h \u2208 H s.t. ES (h) \u2264 , where ES (h) is the loss of h over the sample S. Since the relevant predictor will always be clear from context, we do not state it explicitly when assuming -realizability."}, {"heading": "2.2 Basin Value Distribution", "text": "Definition 5. (Basin) We define a basin of the target function f to optimize over, as a connected set C for which \u2200\u03b1 \u2208 R the set {x \u2208 C : f (x) \u2264 \u03b1} is also connected.\nBasins are of utmost importance in optimization as a basin cannot contain more than one minimal surface. Throughout our work, we analyze the surface of the objective function by analyzing the minimal value attained in the basin of initialization. Following standard practice, we generally assume that the weights are initialized randomly, where in each subsection we explicitly specify the distribution of the weights.\nDefinition 6. (Basin Value Distribution) For some > 0, in section 3, when discussing single neurons, we let F ( ) denote the probability of initializing the weights from a basin achieving a minimal value of at most . In sections 4-6, we use the notation Fn ( ) to denote the same probability, but with respect to a two-layer neural net of width n.\nWe note that the precise objective function and the initialization distribution depend on the specific result, and will be specified later."}, {"heading": "2.3 Region Partitions", "text": "In sections 4-6, we investigate the surface area of the objective function (in the context of ReLU activation functions) via a partition by hyperplanes that the sample induces on the weight space. The following are the definitions used in that context. In a neural network, each neuron receives its input and then computes a dot product with its weights before computing the output of the ReLU and propagating the result forward to the next layer. Since the ReLU is the zero function on one halfspace and the identity function on the other, we seek to understand the behavior of the objective function on each halfspace separately.\nDefinition 7. (Region) For each sample instance xt \u2208 S, t \u2208 [m] and b \u2208 {\u22121, 1}, we denote the open halfspace Hbt = { w \u2208 Rd : \u3008w,xt\u3009 \u00b7 b > 0 } . Let b = (b1, . . . , bm) \u2208 {\u22121, 1}m, an open region R is defined as the non-empty intersection of m halfspaces\nRbS = m\u22c2 t=1 Hbtt 6= \u2205\nand a closed region is defined as the topological closure of an open region.\nDefinition 8. (Region Partition) The set of all regions RS = { RbS 6= \u2205 : b \u2208 {\u22121, 1} m }\nis referred to as the region partition induced by S. We point out that RS is essentially a partition of Rd viewed as the space of weight vectors, and not of the space from which the training instances come from.\nWhen introducing two-layer nets with first layer of size n, and initializing the weights randomly, the weights of each neuron might be initialized from different regions in the region partition. In this scenario when we wish to analyze the initialization point, we seek to investigate the behavior of the objective function over the combination of n regions initialized from.\nDefinition 9. (Region combination) A region combination is defined by\n(R1, . . . , Rn)\nwhere \u2200i \u2208 [n] wi, the weights initialized for the ith neuron satisfy wi \u2208 Ri, where \u2200i \u2208 [n] Ri \u2208 RS .\nWe note that even though there exist points which belong to no open region (the origin is one such example), these points are of Lebesgue measure 0 with respect to the initialization distribution, and therefore we assume that there will always be a region containing the initialization point when initializing from a continuous distribution."}, {"heading": "2.4 Analysis on the sphere", "text": "In sections 5-6, we resort to some analysis on the sphere to obtain results on Fn ( ). Throughout those sections, the following definitions are used.\nDefinition 10. (Unit sphere) Let Sd\u22121 = { x \u2208 Rd : \u2016x\u20162 = 1 } denote the d\u2212 1 dimensional unit sphere.\nDefinition 11. (Hyper-spherical cap) Let Sd\u22121 (a, \u03b8) = { b \u2208 Sd\u22121 : \u3008a,b\u3009 \u2264 cos \u03b8 } denote the d\u2212 1 dimensional hyper-spherical cap of spherical radius \u03b8, centered at a.\nDefinition 12. (Spherical distance) Let s (a,b) := arccos (\u3008a,b\u3009) denote the spherical distance (the angle between the two vectors) of two points a,b \u2208 Sd\u22121. When the spherical radius between a,b is taken as the smaller angle \u03b8 < \u03c02 , s forms a metric on S d\u22121.\nDefinition 13. (Unit sphere surface area) Let \u03c9d\u22121 = \u03c3d\u22121 ( Sd\u22121 ) denote the surface area of the d \u2212 1 dimensional unit sphere, where \u03c3d\u22121 is the d\u2212 1 dimensional Lebesgue measure.\nDefinition 14. (hyper-spherical cap surface area) Let \u03bdd (\u03b8) = \u03c3d\u22121 ( sd\u22121 (a, \u03b8) ) denote the surface area of an angle \u03b8 hyper-spherical cap (independent of a).\nDefinition 15. (Closed ball) Let B\u0304\u03b4 (c) = {x : \u2016c\u2212 x\u20162 \u2264 \u03b4} denote the closed ball of radius \u03b4 centered at c."}, {"heading": "3 Hardness result for single neuron nets", "text": "Considering the complicated nature the surface of the objective function, the most natural starting point is the simplest possible architecture, namely a single neuron without bias: x 7\u2192 \u03c6 (\u3008w,x\u3009). Although simple to analyze and program, nets comprised of a single neuron prove to be very hard to train in the worst case, as the objective function might contain exponentially many poor local minima as a function of the dimension d [2]. However, the authors in [2] also demonstrate that for the 0-realizable case, under some mild conditions on the loss and activation functions, there exists a single minimal surface of the objective function. Bridging the gap between the two aforementioned results, we first provide a construction demonstrating that even if the realizability assumption is only slightly broken, then an exponential number of local minima could still arise. Furthermore, when assuming that the weight vector of the neuron is initialized uniformly at random from the unit sphere, then the distribution of the minimal value in the basin we initialize from is strongly concentrated around a sub-optimal value as the dimension increases. We point out that in this regime where we consider a single neuron, a region in fact serves as a basin - it partitions our optimization space into sections where the objective function is convex in (see lemma 3 for further detail, and note that in the case of a single neuron, a region combination is merely a single region)."}, {"heading": "3.1 Exponentially many local minima for -realizable nets", "text": "Theorem 1. Consider a single neuron neural net with transfer function \u03c6 : R \u2192 R s.t. \u03c6 is strictly monotonically increasing and piecewise differentiable on the positive part of the x axis, \u2203\u03b4 > 0 s.t. \u03c6 is constant on the interval [\u2212\u03b4, 0] 1, a symmetric loss function L : R2 \u2192 R+ s.t. L (a, b) = 0 \u21d0\u21d2 a = b, where L is strictly monotonically increasing in |a\u2212 b|. Also, we assume that for any y \u2208 Im (\u03c6) the loss becomes saturated on the negative part of the x axis, i.e. lim\nz\u2192\u2212\u221e L (\u03c6 (z) , y) = Ly for some Ly \u2208 R+. Then\nthere exists a constant c \u2208 R which depends only on \u03c6, L and an -realizable sample S for sufficiently small > 0, such that the error over the sample ES contains 2d strict local minima, and F ( 1 4c ) \u2264 e\u2212 1 16 d.\nIn other words, we have exponentially many local minima, where the probability of initializing from a sub-optimal basin converges exponentially fast to 1, yet there exists a solution which obtains an error of .\nProof. Let > 0 be sufficiently small (smaller than a loss and activation - dependent constant which shall become evident later). Denote L\u2032 := L (\u03c6 (0) , \u03c6 (\u03b4)). Since both \u03c6 and L are strictly monotonic, we have L\u2032 > 0 and there exists some \u03b41 \u2208 (0, \u03b4) s.t. L (\u03c6 (0) , \u03c6 (\u03b41)) < 2 . Consider the sample\nS = {( x1 , \u03b4, y1 , \u03c6 (\u03b41) ) , ( x2 , \u2212\u03b4, y2 , \u03c6 (\u03b4) )} We compute\nL (\u03c6 (wx1) , y1) =  \u2208 (0, 2 ) , w = \u22121 \u2208 (0, 2 ) , w = 0 0, w = \u03b41\u03b4\nL (\u03c6 (wx2) , y2) =  0, w = \u22121 L\u2032, w = 0\nL\u2032, w = \u03b41\u03b4\nThus the total error over the sample is\nES (w) , 1\n2 2\u2211 i=1 L (\u03c6 (wxi) , yi) =  < , w = \u22121 > L \u2032 2 , w = 0 L\u2032\n2 , w = \u03b41 \u03b4\nBy the monotonicity of L, we have that \u2200w \u2265 0 L (\u03c6 (wx2) , y2) \u2265 L\u2032, so the local minimum ES has in (0,\u221e) is of value \u2265 L\u20322 , and one must necessarily exist as ES (0) > L\u2032\n2 . On the other hand, S is -realizable as ES (\u22121) < , so by the monotonicity of \u03c6, L there exists some local minimum of value < . We note that since \u03c6 is strictly monotonically increasing on the positive part of the x axis, we have that either the loss on x1 or on x2 is strictly monotonic at any point w \u2208 R, so ES has exactly 2 local minima. Furthermore, since the loss on x1 is constant \u2200w \u2208 [\u22121, 0] and the loss on x2 is constant \u2200w \u2208 [0, 1], we have that the objective function contains two basins meeting at zero, so the sign of w determines which of the basins we fall into. To conclude the derivation so far, we showed that there is a one-dimensional sample for which ES has exactly two basins, one \u201cgood\u201d (with minimal value at most ) and one \u201cbad\u201d (with minimal value at least L\u2032\n2 ).\n1\u03c6 (x) = max (0, x) - ReLU is one example for such a transfer function.\nWe now extend our sample to be d-dimensional in a similar manner as did the authors in [2] as follows: For i = 1, 2 and j \u2208 [d], we use the mapping xi,j 7\u2192 (0, . . . , 0, xi, 0, . . . , 0) where the non-zero coordinate is the jth coordinate. It is straightforward to show that the partial derivative \u2202\u2202wj is 0 for xi,k with j 6= k, so the geometry of the surface of the objective function ES is independent for each coordinate separately. Now, every Cartesian product of local minima in the one-dimensional setting form a d-dimensional local minimum. Since we have at least two local minima, a good and another bad one in each coordinate, this combines into 2d local minima, where each minimum\u2019s value would be the average of the one-dimensional minima forming it. Note that the combination of all good minima forms the global minimum with value < , thus the data is -realizable. We stress that an important property of this initialization scheme is that the signs of the coordinates of the initialization point is uniformly distributed on the Boolean cube, as it implies that on each coordinate, independently, we have a probability 0.5 of reaching a bad basin, hence the number of bad basins we initialize from is distributed B (d, 0.5). Letting c := L \u2032\n2 , we have from Chernoff\u2018s bound that\nF\n( 1\n4 c\n) \u2264 e\u2212 1 16 d\nIn the appendix, we provide a detailed example of this theorem and the construction for the squared loss, and another example for sigmoid activation which demonstrates how a similar construction can be made for activation functions which are not constant over some interval [\u2212\u03b4, 0]. A potential drawback of the general theorem is that the norm of the training instances is not necessarily bounded as a function of 1 . However, in one of the examples we show that the same issues can occur even if the data norm scales only logarithmically with 1 . We also remark that the overwhelming majority of the local minima formed above have an objective value sub-optimal by \u2126(c)."}, {"heading": "3.2 Learning singleton data sets using ReLUs", "text": "In subsection 3.1, we\u2019ve seen that even when achieving an arbitrarily small error using a single neuron is possible, learning might still be a difficult task. The hardness results above rely on analysis of training sets S of the form\n{((xt, 0, . . . , 0) , yt) , ((0,xt, 0, . . . , 0) , yt) , . . . , ((0, . . . , 0,xt) , yt)}\nAlong each coordinate there are two local minima, only one of which is good. Under the initialization distribution considered, the probabilities of hitting the good basin along each coordinate are independent and strictly less than 1, hence the probability of \u201chitting\u201d the right basin across all coordinates is exponentially small.\nAs discussed in the introduction, it is natural to study what happens to such a hardness construction under overspecification, which here means replacing a single neuron by a two-layer network of some width n > 1. Surprisingly, it turns out that in this case, the probability of reaching a sub-optimal region decays exponentially in n. Intuitively, this is because for such constructions, for each coordinate it is enough that one of the n neurons in the first layer will initialize at a correct basin. This will happen with overwhelming probability if n is moderately large.\nWe are going to look at a simple situation, where the output neuron is fixed, and we only optimize over the neurons in the first layer. Specifically, we will consider ReLU activation paired with the squared loss, with an initialization distribution where the weight vector of the output neuron is initialized to be in {\u22121, 1}n\nuniformly at random, and the weights of each neuron in the first layer is drawn uniformly at random from the unit sphere (we note that our results would still hold using other scales \u2013 see lemma 1 and lemma 2 below for further details). We then consider optimizing the weights of the first layer only, i.e.\nmin w1,...,wn\n1\nm m\u2211 t=1 ( n\u2211 i=1 vi max {0, \u3008wi,xt\u3009} \u2212 yt )2 (1)\nNote that for every wi, this function is convex and quadratic in wi inside a region (as defined in definition 7, i.e. the region where the sign of \u3008wi,xt\u3009 remains the same for all t). Moreover, over any such combination of regions (one for each wi), the function above is convex and quadratic, and therefore corresponds to a basin over (w1, . . . ,wn) (we will derive this fact more rigorously in subsection 4.2). As a result, to get a lower bound on the probability of starting at a good basin for Equation 1, it is enough to lower bound the probability of initializing at a region combination, in which there is a low minimal value. Based on this observation, we have the following theorem:\nTheorem 2. Let S \u2286 Rd be a finite sample comprised of singletons (i.e. \u2200x \u2208 S \u2203j \u2208 [d] s.t. xj 6= 0 and xi = 0 for any i 6= j). Let \u03b1 denote the optimal value that can be achieved on S using a single neuron with the ReLU activation function. Then\nFn (\u03b1) \u2265 1\u2212 4d ( 3\n4 )n Where Fn is defined with respect to the initialization distribution discussed above, and the objective function in Equation 1.\nProof. Denote Sj = { x \u2208 S : xj 6= 0 } , S+j = { x \u2208 S : xj > 0 } , S\u2212j = { x \u2208 S : xj < 0 } Let wji , i \u2208 [n] , j \u2208 [d] denote the jth coordinate of the ith neuron in the first layer. Since the order of the neurons in the first layer does not matter, we may assume w.l.o.g. that v, the weights of the second layer, is of the form\nv = 1, . . . , 1\ufe38 \ufe37\ufe37 \ufe38 \u00d7k ,\u22121, . . . ,\u22121\ufe38 \ufe37\ufe37 \ufe38 \u00d7n\u2212k  for some k \u2208 {0, . . . , n}. For fixed j we have\nESj (Nn (W,v)) = ES+j (Nn (W,v)) + ES\u2212j (Nn (W,v))\n= \u2211 x\u2208Sj ( k\u2211 `=1 \u03c6 ( xjwj` ) \u2212 n\u2211 `=k+1 \u03c6 ( xjwj` ) \u2212 y (x) )2\nso for j1 6= j2, ESj1 (Nn (W,v)) and ESj2 (Nn (W,v)) are independent. Thus, in order to minimize ES (Nn (W,v)) = \u2211d j=1ESj (Nn (W,v)) it is sufficient to minimize ESj (Nn (W,v)) for each j \u2208 [d] separately. Clearly, minimizing ESj is equivalent to minimizing ES+j and ES\u2212j separately, as once more these two are\nindependent of one another due to the nature of the ReLU activation function. Observing that the prediction of Nn (W,v) on any x \u2208 S+j is simply a linear combination of the weights satisfying wj` > 0, ` \u2208 [n], we have that composing this linear combination with the convex squared loss yields a convex function. We note that the regions of the region partition induced by S+j are just the two halfspaces induced by any x \u2208 S+j , so ES+j vanishes on one halfspace, and is convex on the other halfspace, therefore we have thatES+j has a single global minimum on the halfspace it does not vanish in, and the same goes for S\u2212j on the other halfspace. Denote the optimal points minimizing ES+j , ES\u2212j as h + j , h \u2212 j respectively, we have that a sufficient condition for an orthant to contain a predictor minimizing ES+j is having w j i1 ,wji2 > 0 for i1, i2 \u2208 {1, . . . , k} \u00d7 {k + 1, . . . , n} since: If h+j > 0 then\nwji =\n{ h+j , i = i1\n0, i 6= i1 achieves the minimum. If h+j < 0 then\nwji = { \u2212h+j , i = i2 0, i 6= i2\nachieves the minimum. Similarly, an orthant containing a predictor with wji1 ,w j i2 < 0 for i1, i2 \u2208 {1, . . . , k}\u00d7 {k + 1, . . . , n} will minimize ES\u2212j . If we initialize a point on the unit sphere, then the probability of having exactly k positive weights on the output neuron is ( n k ) 2\u2212n. The probability of having weights of different sign in the first layer connected to the k positive weights is 1 \u2212 1 2k\u22121\n, and the probability of having weights of different sign in the first layer connected to the n\u2212 k negative weights is 1\u2212 1\n2n\u2212k\u22121 .\nOverall, the probability of initializing from an orthant which contains a global minimum is\nn\u22121\u2211 k=1 ( n k ) 2\u2212n ( 1\u2212 1 2k\u22121 )( 1\u2212 1 2n\u2212k\u22121 ) = 1\u2212 4 ( 3\n4\n)n + 6 ( 2\n4\n)n \u2212 4 ( 1\n4 )n \u2265 1\u2212 4 ( 3\n4 )n Assuming n \u2265 5 and using Bernoulli\u2019s inequality, the probability of this happening for all j \u2208 [d] is\n\u2265 ( 1\u2212 4 ( 3\n4 )n)d \u2265 1\u2212 4d ( 3\n4\n)n\nThis result is an interesting example of the power of overspecification, as more neurons in the first layer of the chosen architecture lead to a rapid exponential decay in the values of the local minima of the objective function, and motivates us in further studying overspecification in the following sections."}, {"heading": "4 Structural properties of the objective function", "text": "In this section, we state and prove several results regarding the geometry of the objective function, which allow us to gain more insight on its structure, as well as lay the foundations for the results to come in sections 5, 6. From this section and onwards, we always consider ReLU activation functions and the squared loss."}, {"heading": "4.1 Existence of a path to the global minimum", "text": "Moving to a general regime where our net is of arbitrary depth and with a single output neuron, theorem 3 below demonstrates that under mild conditions, most of the surface of the objective function overlooks the global minimum. i.e., there exists a monotonically decreasing path from \u201chigh\u201d starting points to the global minimum. Moreover, in theorem 4, we demonstrate that with a suitable random initialization, we are overwhelmingly likely to initialize at such a starting point.\nIt should be noted that this monotonic path could be difficult to find using first order optimization techniques. However, this result nevertheless sheds light on the nature of the surface function, demonstrating that it is not completely sporadic in the sense that \u201ccrossing valleys\u201d is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11].\nTheorem 3. Let N (~w) \u2208 Rm denote the vector of outputs of a ReLU net over a sample S, where we view the set of weights and bias terms of the net as a single vector ~w. And consider the objective function with respect to the squared loss ES (N (~w)) = \u2211m t=1 (N (~w) (xt)\u2212 yt)\n2. Let winit denote an initialization point satisfying \u2016N (winit)\u2212 y\u20162 > \u2016y\u20162 and let wopt denote a global minimum. Assume that there exists a continuous path \u03b3 (t), t \u2208 [0, 1] s.t. \u03b3 (0) = winit, \u03b3 (1) = wopt and \u2200t \u2208 [0, 1], there exist an instance x in the training set such that N (\u03b3(t)) (x) 6= 0. Then there exists a strictly decreasing path from winit to wopt.\nIn theorem 4, we show that the requirement of \u2016N (winit)\u2212 y\u20162 > \u2016y\u20162 indeed holds with high probability under a suitable random initialization. The requirement that N(\u03b3(t)) is not always zero also appears reasonable, as for most neural nets and training sets it is generally possible to continuously change weights from one configuration to another, without making the prediction zero at all data points.\nProof. Noting that the squared loss over the sample for weights w can be represented as 1m \u2016N (w)\u2212 y\u2016 2 2, we begin by establishing the following inequality. We have\n\u2016N (winit)\u2212 y\u20162 > \u2016y\u20162\n\u21d2 \u2016N (winit)\u20162 \u2212 2 \u3008N (winit) ,y\u3009 > 0 (2)\nDefine Nt := N (\u03b3 (t)), we once again use the positively-homogeneous nature of the ReLU to scale the weights of the net by a positive factor at > 0, while stressing that the addition of a bias term does not affect this trick and nevertheless allows us to scale the weights by a constant positive factor, effectively forcing \u2200t \u2208 [0, 1] the equality\n\u2016atNt \u2212 y\u20162 = \u2016N0 \u2212 y\u20162 (1\u2212 t) + \u2016y\u20162 t\nwhich will yield a path from winit to a1wopt with an error over the sample monotonically decreasing to 1 m \u2016y\u2016 2. Solving the above equality for at\na2t \u2016Nt\u2016 2 \u2212 2at \u3008Nt,y\u3009+ \u2016y\u20162 = ( \u2016N0\u20162 \u2212 2 \u3008N0,y\u3009 ) (1\u2212 t) + \u2016y\u20162\n\u21d2 a2t \u2016Nt\u2016 2 \u2212 2at \u3008Nt,y\u3009+ ( 2 \u3008N0,y\u3009 \u2212 \u2016N0\u20162 ) (1\u2212 t) = 0\ntaking the positive root yields\nat =\n2 \u3008Nt,y\u3009+ \u221a 4 \u3008Nt,y\u30092 \u2212 4 \u2016Nt\u20162 ( 2 \u3008N0,y\u3009 \u2212 \u2016N0\u20162 ) (1\u2212 t)\n2 \u2016Nt\u20162\n=\n\u3008Nt,y\u3009+ \u221a \u3008Nt,y\u30092 \u2212 \u2016Nt\u20162 ( 2 \u3008N0,y\u3009 \u2212 \u2016N0\u20162 ) (1\u2212 t)\n\u2016Nt\u20162\nwe note that from Equation 2 we have that \u2016Nt\u20162 (1\u2212 t) ( 2 \u3008N0,y\u3009 \u2212 \u2016N0\u20162 ) < 0, thus the discriminant is > |\u3008Nt,y\u3009|, so at is well defined and positive. If a1 = 1 then \u03b3\u0303 (t) = at\u03b3 (t) is a strictly decreasing path from winit to wopt. Otherwise, since \u2016aN1 \u2212 y\u20162 is convex in a, we have by the optimality assumption of wopt that a = 1 is the global minimum, thus \u03b3\u2032 (t) = (a1 (1\u2212 t) + t)wopt results in a strictly decreasing path from a1\u03b3 (1) to wopt. Finally, concatenating both \u03b3, \u03b3\u2032 yields a strictly decreasing path from winit to wopt.\nTheorem 3 required that we initialize from a point achieving an error of more than 1m \u2016y\u2016 2. We now show that this requirement is fulfilled with probability \u2265 14 (1\u2212 2 \u2212nh\u22121), where nh\u22121 is the size of the last hidden layer. In the following theorem, we assume that all neurons are initialized uniformly from the unit sphere. Moreover, as in the case of a single neuron, the theorem would work as well with initializing at any other origin-centered sphere (see proof for details).\nTheorem 4. Let N ( W h1 ,b h 1 ) (x) : Rd \u2192 R be a fully connected ReLU neural network of depth h with the ReLU activation function, where Wi,bi, i \u2208 [h] are the weight matrix and bias vector of the ith layer respectively, and let ni be the number of neurons in the ith layer. Let w denote the weights of the entire network (including bias), and suppose that w is distributed uniformly over the unit sphere, i.e. assume that \u2016w\u2016 = 1 w.p. 1. Then\nPr [ m\u2211 t=1 ( N ( W h1 ,b h 1 ) (xt)\u2212 yt )2 > \u2016y\u20162 ] \u2265 1 4 ( 1\u2212 2\u2212nh\u22121 ) Proof. Observe that following our distribution assumption, the signs of the coordinate of each weight are distributed uniformly and independently (in fact, our result would apply for any distribution which induces such a distribution on the signs of the coordinates, and in particular initializing uniformly from any origincentered sphere) which implies that \u2200x \u2208 Rd, Pr [\u3008w,x\u3009+ b > 0] = Pr [\u3008w,x\u3009 \u2212 b < 0] = 12 , so we have that Pr [\u03c6 (\u3008w,x\u3009+ b) 6= 0] = 12 . Thus any neuron except the output neuron outputs a non-zero value w.p. 1 2 , regardless of its input x. Thus for the output neuron to receive input 6= 0 we need at least one neuron in the (h\u2212 1)th layer to output a non-zero value, which happens w.p. at least 1\u22122\u2212nh\u22121 . Given that the output neuron does receive an input 6= 0, then w.p. 12 its output will be non-zero.\nNow, let p \u2208 Rm denote the output vector of the net on S, so the error on the sample is given by 1m \u2016p\u2212 y\u2016 2. Given that p 6= 0, we have \u2016p\u20162 > 0, and since either one of \u3008p,y\u3009 , \u3008\u2212p,y\u3009 is non-negative we get that\nmax { \u2016p\u2212 y\u20162 , \u2016\u2212p\u2212 y\u20162 } > \u2016y\u20162\nBy the symmetry of the distribution of the weights, we have w.p. 12 that indeed the error is greater than \u2016y\u20162, and we have\nPr [ m\u2211 t=1 ( N ( W h1 ,b h 1 ) (xt)\u2212 yt )2 > \u2016y\u20162 ] \u2265 ( 1\u2212 2\u2212nh\u22121 ) \u00b7 1 2 \u00b7 1 2\n= 1\n4\n( 1\u2212 2\u2212nh\u22121 ) concluding the proof of the theorem.\nAnother interesting corollary of theorem 3 is that the objective value of any local minimum cannot be too large:\nCorollary 1. Let N as in 3, and suppose that \u2200w s.t. \u2016N (w)\u2212 y\u20162 > \u2016y\u20162 there exists a path \u03b3 as in 3, then there are no local minima with value > 1m \u2016y\u2016 2.\nProof. Suppose by contradiction that there exists a local minimum w with\n\u2016N (w)\u2212 y\u20162 > \u2016y\u20162\nThen for = \u2016N (w)\u2212 y\u20162 \u2212 \u2016y\u20162 > 0 we have by the continuity of N a \u03b4 > 0 and an environment B\u03b4 (w) s.t.\n\u2200w\u2032 \u2208 B\u03b4 (w) \u2223\u2223\u2223\u2225\u2225N (w\u2032)\u2212 y\u2225\u22252 \u2212 \u2016N (w)\u2212 y\u20162\u2223\u2223\u2223 <\n\u21d2 \u2200w\u2032 \u2208 B\u03b4 (w) \u2225\u2225N (w\u2032)\u2212 y\u2225\u22252 > \u2016y\u20162\nbut since \u2016N (w)\u2212 y\u20162 > \u2016y\u20162 we have from theorem 3 a monotonically decreasing path to wopt, which is a contradiction."}, {"heading": "4.2 Partitioning the weight space of the objective function", "text": "We now move to investigating 2-layer ReLU neural nets with respect to the squared loss, where the objective function has the form\n1\nm m\u2211 t=1 ( n\u2211 i=1 vi max {0, \u3008wi,xt\u3009} \u2212 yt )2 . (3)\nAlthough still relatively simple, they already possess universal approximation properties [8], and encapsulate the challenge of handling a highly non-convex objective function.\nThe goal of this subsection is to capture some important structural properties of such 2-layer networks, which will be used in deriving the results of the following sections. Intuitively, our formal lemmas capture the following two simple but important observations:\n\u2022 First, due to the positive homogeneity of the ReLU function, the neural network is invariant to multiplying each wi and dividing the corresponding vi by the same positive factor c. Therefore, in terms of expressiveness, there is no loss of generality by assuming that each vi is in {\u22121,+1}. In fact, for the results we will show later on (and similar to the results of subsection 3.2), it is enough to just randomly choose and fix each vi \u2208 {\u22121,+1} uniformly at random, and optimize over the weights of the first layer only.\n\u2022 Second, the ReLU function is piecewise linear, depending on the sign of \u3008w,x\u3009 so equation 3 is in fact a convex quadratic function of w1, . . . ,wn inside each region where the signs of \u3008wi,xt\u3009 remain the same for all i, t (see the formal definition of a region in section 2). Therefore, equation 3 is piecewise quadratic as a function of w1, . . . ,wn. Moreover, by definition, each such convex quadratic region is a basin. As a result, in a scenario where we optimize over the weights of the first layer only, in order to lower bound the probability of initializing at a basin with a local minima at most , it is enough to lower bound the probability of initializing at a region combination, inside which the minimal value is at most .\nWe begin with the following lemma, which implies that regions (as defined in section 2) are in fact cones. Therefore, the region combination we are in is not affected by rescaling the first layer weights by constant positive factors. As a result, in terms of analyzing which region combination we land on, we can assume w.l.o.g. uniform random initialization over the unit sphere, rather than a sphere of some other radius.\nLemma 1. Let S denote the dataset, let (R1, . . . , Rn), \u2200i \u2208 [n] Ri \u2208 RS be a region combination, wi \u2208 Ri, a > 0. Then awi \u2208 Ri.\nProof. \u2200i \u2208 [n], let bi = ( bi1, . . . , b i m ) \u2208 {\u22121, 1}m s.t. Ri = Rb i S , then\nwi \u2208 Ri \u21d2 \u2200t \u2208 [m] \u3008wi,xt\u3009 \u00b7 bit > 0 \u21d2 \u2200t \u2208 [m] a \u3008wi,xt\u3009 \u00b7 bit > 0 \u21d2 \u2200t \u2208 [m] \u3008awi,xt\u3009 \u00b7 bit > 0 \u21d2 awi \u2208 R\nWe continue by reminding the reader a few notational conventions (formally defined in section 2): Let Nn (W, v) denote a net with architecture x 7\u2192 v>\u03c6 (Wx), with a first layer of size n where v \u2208 Rn,W = (w1, . . . .wn) \u2208 Rn\u00d7d are the weights. We note that as was previously explained in 2.1, we do not introduce a bias term in the analysis whenever possible. Denote by ES (Nn (W,v)) the average squared loss of Nn on a given sample S = {(xt, yt)}mt=1 with weights W,v. Finally, let \u03c6 (x) = max {0, x} be the ReLU function, and recall that it is positive homogeneous in the sense that for any c > 0, \u03c6 (x) = c\u03c6 (x/c).\nLemma 2. (Reduction of the optimization problem to nets of the form x 7\u2192 a>\u03c6 (Wx) where a \u2208 {\u22121, 1}n): Let S = {(xt, yt)}mt=1 be a finite sample, let Nn (W,v) as above. Let Nn (W,a) denote the net x 7\u2192 a>\u03c6 (Wx) where a \u2208 {\u22121, 1}n. Denote by ES (Nn (W,v)) the squared loss of Nn on the sample S with weights W,v and let R = (R1, . . . , Rn) be some closed region combination. Then for all W = (w1, . . . ,wn) \u2208 R and for all v = (v1, . . . , vn) \u2208 Rn there exists W \u2032 \u2208 R,a = (a1, . . . , an) \u2208 {\u22121, 1}n s.t. \u2200x \u2208 Rd Nn (W,v) (x) = Nn (W,a) (x) and \u2200i \u2208 [n] vi \u00b7 ai \u2265 0.\nProof. Clearly, the left-hand side of the equality is not greater than the right-hand side as {\u22121, 1}n \u2286 Rn. Let R = (R1, . . . , Rn) be some region combination, let(W,v) \u2208 R \u00d7 Rn be some arbitrary point. If vi = 0 for some i \u2208 [n], then the output of the i-th neuron is always canceled. So by modifying wi := 0 and vi = 1 we have an equivalent net, so we may assume w.l.o.g \u2200i vi 6= 0. Now, by using the positivehomogeneity of the ReLU and taking ci = |vi|, we have an equivalent net with weights (W,a) for all k \u2208 N s.t. a \u2208 {\u22121, 1}n. We note that from lemma 1, any rescaling done does not change the region combination W belongs to, so W \u2032 \u2208 R, and since vi is scaled by a positive constant then vi \u00b7 ai \u2265 0, which concludes the proof of the lemma.\nWhat lemma 2 in fact tells us, is that in terms of the function computed by a two-layer network, the two architectures are equivalent. For this reason, from this point onwards we consider the optimization problem over W and a \u2208 {\u22121, 1}n. Rather than optimizing over the discrete set a \u2208 {\u22121, 1}n, we will just pick those uniformly at random and fix them. Namely, in the context of two-layer networks, we will consider an optimization scenario where we only optimize over W (or equivalently, w1, . . . ,wn in equation 3). While this changes somewhat the optimization problem it would turn out that for the scenarios considered later in the work, it is enough for attaining good basins with high probability, and the analysis is conceptually much simpler.\nWe now turn to characterize the piecewise-quadratic nature of ES , as a function of W :\nLemma 3. (Piecewise quadratic structure of the objective function): Let Nn (W,v) as above. Then ES (Nn (W,v)) where we restrict W to any region combination induced by the sample is quadratic and convex in W .\nProof. We note that Nn (W, v) where W is restricted to some region combination is a linear combination of affine functions, and since the squared loss is convex, their composition is convex and quadratic. Summing over all samples, we obtain a convex and quadratic function.\nA simple implementation of lemma 3 allows us to bound the value of any local minimum which the objective function obtains, as follows:\nCorollary 2. \u2200n \u2208 N, Fn ( 1 m \u2211m t=1 y 2 t ) = 1. i.e. there are no local minima achieving an error of more than 1 m \u2211m t=1 y 2 t .\nProof. Let (W, v) \u2208 Rn\u00d7d \u00d7 {\u22121, 1}n be a local minimum point, then W is contained in some region combination. Since the origin ~0 \u2208 Rn\u00d7d is contained inside the closure of all region combinations, and since the objective function is convex in W in each region combination, we have that ES (Nn (W, v)) \u2264 ES (Nn (0, v)). But since\nES (Nn (0, v)) = 1\nm m\u2211 t=1 y2t\nthe theorem follows.\nWe point out that corollary 2 is a special case of corollary 1, applied to two-layer nets, but nevertheless shows corollary 1 still holds even without the assumption of the existence of a path \u03b3."}, {"heading": "5 Successful random initialization paired with overspecification", "text": "In this section we provide results demonstrating that for data which is realizable using two-layer networks, that the probability of initializing from a basin containing a global minimum increases as we add more neurons to the first layer, corresponding to the idea of overspecification. We note that these results hold without significant additional assumptions, but on the flip side, the number of neurons required to guarantee a constant success probability increases exponentially with the dimension d. Thus, the result is only meaningful when the dimension is modest. In the next section, we show results with a much weaker dependence on d, but under additional assumptions. Following the discussion in the previous section, we consider an initialization scheme where the weights of the output neuron is chosen from {\u22121,+1}n and then fixed, the weights of each neuron in the first layer is chosen uniformly at random from the unit sphere, and Fn( ) (the probability of initializing at a basin with minimal value at most ) is defined with respect to that initialization scheme. Also, we assume that the dataset satisfies \u2200t \u2016xt\u2016 \u2264 1. We stress that this assumption is without much loss of generality, as a similar result can be shown to hold for any other bound on the data.\nTheorem 5. Let > 0, let S be a 0-realizable data set w.r.t. two-layer nets of width n, satisfying \u2200t \u2208 [m] \u2016xt\u20162 \u2264 1. Let (W \u2217,v\u2217), W \u2217 = (w\u22171, . . . ,w\u2217n), w\u2217i \u2208 Rd denote a global minimum achieving ES (Nn (W \u2217,v\u2217)) = 0, let B s.t. \u2200i \u2208 [n] \u2016w\u2217i \u2016 \u2264 B, and define for any c > 0\npj ( ) := 1\n2\u03c0 (d\u2212 1)\n(\u221a nB \u00b7 \u221a 1\u2212 4n2B2 )j Nc := (1 + c) \u2308 n\npd\u22121 ( ) \u2309 Then\nFN ( ) \u2265 max ( 1\n2 , 1\u2212 exp\n( \u2212 1\n4pd\u22121 ( )\nc2\n(1 + c) n )) To proceed, we will need the following two technical lemmas, whose proof may be found in appendix\n8.3.1, 8.3.2.\nLemma 4. Let \u03b4 > 0 and let a \u2208 Sd\u22121 \u2286 Rd be a point on the d \u2212 1-dimensional unit sphere. Let b be a point chosen uniformly at random on Sd\u22121. Then\nPr [\u2016a\u2212 b\u20162 \u2264 \u03b4] \u2265 1\n\u03c0 (d\u2212 1)\n( \u03b4 \u221a 1\u2212 \u03b4 2\n4\n)d\u22121\nLemma 5. Let x = ( x1, . . . , xd ) \u2208 Rd, then Nn (w1, . . . ,wn,v) (x) is \u2016x\u2016-Lipschitz in each wi.\nProof. Initializing some W (N) \u2208 RN\u00d7d uniformly on the unit sphere and some v(N) \u2208 {\u22121, 1}N uniformly, let A denote the event where there exists a subset of n neurons with weights denoted ( W (n),v(n) ) such that\nES ( Nn ( W (n),v(n) )) \u2264\nWe now show that the occurrence of A is sufficient for having\nES ( NN ( W (N),v(N) )) \u2264\nSuppose A occurs, and assume w.l.o.g. that the n neurons achieving an error of less than are the first n neurons. Then we have that\nES ( NN ( W (n), 0, . . . , 0,v(N) )) = ES ( Nn ( W (n),v(n) )) \u2264\nOn the other hand, ( W (N),v(N) ) belongs to some region combination. In particular, we have that(\nW (n), 0, . . . , 0,v(N) )\nbelongs to the closure of the same region combination, as the closure of all regions contains the origin. By lemma 3, we have that ES is convex in W (N) on the given region combination, so the minimum of the region combination satisfies\nES ( NN ( W \u2032,v\u2032 )) \u2264 ES ( NN ( W (n), 0, . . . , 0,v(N) )) where (W \u2032,v\u2032) denotes the point in the region combination achieving its local minimum. Thus we have that the occurrence of A implies achieving an error \u2264 . To finish the proof, we now need to lower bound Pr [A]. For each neuron with weights wi \u2208 Rd s.t. \u2016wi\u2016 = 1, we have\nPr [ \u2016\u2016w\u2217i \u2016 \u00b7wi \u2212w\u2217i \u2016 \u2264 \u221a\nn\n] = Pr [\u2225\u2225\u2225\u2225wi \u2212 w\u2217i\u2016w\u2217i \u2016 \u2225\u2225\u2225\u2225 \u2264 \u221a n \u2016w\u2217i \u2016 ] lemma 4 \u2265 1\n\u03c0 (d\u2212 1)\n( \u221a n \u2016w\u2217i \u2016 \u00b7 \u221a 1\u2212 4n2 \u2016w\u2217i \u2016 2 )d\u22121\n\u2265 1 \u03c0 (d\u2212 1)\n(\u221a nB \u00b7 \u221a 1\u2212 4n2B2 )d\u22121 = 2pd\u22121 ( )\nand also Pr [vi = v\u2217i ] = 1\n2\nand since the two events are independent we have that both occur w.p. pd\u22121 ( ). Also, this event is independent for each neuron, so we have w.p. at least pd\u22121 ( ) for each neuron to initialize \u2018close\u2018 enough to (w\u2217i , v \u2217 i ). In this sense, we can lower bound the number of good initializations from below using X \u223c B (N, pd\u22121 ( )), where B (N, p) is the binomial distribution. The median of a binomially distributed random variable is dNpd\u22121 ( )e or bNpd\u22121 ( )c (or both), so either case we have Pr [X \u2265 dNpd\u22121 ( )e] \u2265 12 or Pr [X \u2265 dNpd\u22121 ( )e] \u2265 Pr [X \u2265 bNpd\u22121 ( )c] \u2265 12 , therefore we compute\n1 2 \u2264 Pr [X \u2265 dNpd\u22121 ( )e]\n= Pr [ X \u2265 \u2308\u2308 n\npd\u22121 ( )\n\u2309 pd\u22121 ( ) \u2309] \u2264 Pr [ X \u2265 \u2308 n\npd\u22121 ( ) pd\u22121 ( ) \u2309] = Pr [X \u2265 n]\nAlso, by using Chernoff\u2018s bound we can bound the tail of X as follows\nF ( n; (1 + c) \u2308 n\npd\u22121 ( )\n\u2309 , pd\u22121 ( ) )\n\u2264 exp \u2212 1 2pd\u22121 ( ) ( (1 + c) \u2308 n pd\u22121( ) \u2309 pd\u22121 ( )\u2212 n )2 (1 + c) \u2308 n\npd\u22121( )\n\u2309 \n\u2264 exp ( \u22121\n4\nc2\n(1 + c) n ) thus with probability \u2265 max ( 1 2 , 1\u2212 exp ( \u221214 c2 (1+c)n )) we have n neurons s.t. \u2016\u2016w\u2217i \u2016 \u00b7wi \u2212w\u2217i \u2016 \u2264 \u221a n and vi = v \u2217 i . Assume w.l.o.g. that these neurons are numbered (wi, vi) for i = 1, . . . , n and let\nWi = ( w\u22171, . . . ,w \u2217 i , \u2225\u2225w\u2217i+1\u2225\u2225wi+1, . . . , \u2016w\u2217n\u2016wn)\nfor i = 0, . . . , n. Observing thatWn = W \u2217, and thatW0 andW both belong to the same region combination as a result of lemma 1, we can substitute Nn (W,v\u2217) with Nn (W0,v\u2217) w.l.o.g., as initializing from either is equivalent. We have\nES (Nn (W0,v \u2217)) =\n1\nm m\u2211 t=1 (Nn (W0,v \u2217) (xt)\u2212 yt)2\n= 1\nm m\u2211 t=1 \u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 (Nn (Wi\u22121,v \u2217) (xt)\u2212Nn (Wi,v\u2217) (xt)) \u2223\u2223\u2223\u2223\u2223 2\ntriangle inequality \u2264 1\nm m\u2211 t=1 ( n\u2211 i=1 |Nn (Wi\u22121,v\u2217) (xt)\u2212Nn (Wi,v\u2217) (xt)| )2 lemma 5 \u2264 1\nm m\u2211 t=1 \u2223\u2223\u2223\u2223\u2223\u2016xt\u2016 ( n\u2211 i=1 \u2016Wi\u22121 \u2212Wi\u2016 )\u2223\u2223\u2223\u2223\u2223 2\n\u2264 ( n\u2211 i=1 \u2016Wi\u22121 \u2212Wi\u2016 )2\n= ( n\u2211 i=1 \u2016\u2016w\u2217i \u2016 \u00b7wi \u2212w\u2217i \u2016 )2\n\u2264 ( n\u2211 i=1 \u221a n )2 =\nthus concluding the proof of theorem 5.\nTheorem 5 shows that the more neurons we use, the more likely we are to capture a good initialization on a subset of the neurons which will lead to a good minimum, albeit the number of neurons needed to have\na non-negligible success rate is exponential in the dimension. We do note, however, that the result remains essentially the same if the dimension d is replaced by the intrinsic data dimension, or rank of the matrix X which columns are x1, . . . ,xm. Roughly speaking, this is because the region partition corresponds to an embedding in Rd of a partition in the column space ofX , so its structure remains similar. Formally, we have the following theorem:\nTheorem 6. Let > 0, S, (W \u2217,v\u2217) and B as above, and further assume that Rank (X) = k < d where X is the matrix which columns are x1, . . . ,xm , and define for any c > 0\nNc := (1 + c)\n\u2308 n\npk\u22121 ( ) \u2309 Then\nFN ( ) \u2265 max ( 1\n2 , 1\u2212 exp\n( \u2212 1\n4pk\u22121 ( )\nc2\n(1 + c) n )) Proof. Let V = span (x1, . . . ,xm), and define T (x) := v\u2016v\u20162 where x = v + v\n\u22a5 for v \u2208 V,v\u22a5 \u2208 V \u22a5. First, we observe that for any initialization W = (w1, . . . ,wn), W and T (W ) := (T (w1) , . . . , T (wn)) both belong to the same region combination as \u2200i \u2208 [n] , \u2200t \u2208 [m]\n\u3008xt,wi\u3009 = \u2329 xt, \u2016w\u20162 \u00b7 T (wi) + w \u22a5 i \u232a = \u3008xt, \u2016w\u20162 \u00b7 T (wi)\u3009+ \u2329 xt,w \u22a5 i\n\u232a = \u3008xt, \u2016w\u20162 \u00b7 T (wi)\u3009 = \u2016w\u20162 \u00b7 \u3008xt, T (wi)\u3009\n\u21d2 sign (\u3008xt,wi\u3009) = sign (\u3008xt, T (wi)\u3009)\nthus bothW,T (W ) belong to the same basin achieving the same minimal value. Since any rotation \u0398 under which V \u22a5 is invariant commutes with T , we have for any measurable set A \u2286 V\n\u03c3k\u22121 (A) = \u03c3d\u22121 ( \u0398T\u22121 (A) ) = \u03c3d\u22121 ( T\u22121 (\u0398A) ) So initializing uniformly on Sd\u22121 is equivalent to initializing uniformly on Sk\u22121 in the sense of the region combination we initialize from, and we are reduced to the conditions of theorem 5.\n6 Learning m-rank or clustered data\nIn this section, we will show that when learning data of high dimension (specifically, when the dimension satisfiesm \u2264 d, wherem is the data size), local minima are not an issue. Building on the previous result, we then show that when our data is clustered into k \u2264 d small enough clusters, then once again local minima will not pose a problem. We stress that we follow the same initialization procedure as in section 5, and optimize only over the weights of the first layer."}, {"heading": "6.1 Learning Rank m Data", "text": "We now assume that our data matrix X satisfies RankX = m, where m is the number of data points. We note that this immediately implies m \u2264 d. Namely, that the number of training examples is not larger than\nthe dimension. Even though this regime might be strongly prone to overfitting, this allows us to investigate the surface area of the objective function effectively, while also serving as a base for the clustered data scenario that we\u2019ll be studying in theorem 8. As in the previous section, Fn is defined with respect to an initialization distribution where the output neuron is fixed at random, and where the optimization is done with respect to the first layer. Our main result in this section is the following theorem, which implies that under the rank assumption, a two-layer network of size O (log (m)) is sufficient to initialize in a basin with 0 error with overwhelming probability.\nTheorem 7. Assume RankX = m, and let Y = (y1, . . . , ym) be arbitrary. Then\nFn (0) \u2265 1\u2212m ( 3\n4 )n Proof. Denote the initialization point as W = (w1, . . . ,wn), and define w\u2032i = \u2211m t\u2032=1 ai,t\u2032xt\u2032 where ai,t\u2032 \u2208 R are to be determined later. We want to show that for well chosen values of ai,t\u2032 , w\u2032i belongs to the same region as wi and achieves a perfect prediction over a certain subset of S while achieving a prediction of 0 over the rest of the sample, effectively predicting the subset without affecting the prediction over the rest of the sample. By combining enough neurons in this manner, we are able to obtain a perfect prediction over the data, i.e., an error of 0:\nDefine the vector y\u2032i =  y \u2032 i,1 ...\ny\u2032i,m\n where\ny\u2032i,t = { |yt| \u3008wi,xt\u3009 > 0, vi \u00b7 yt \u2265 0, \u2200j < i \u3008wj ,xt\u3009 \u2264 0 \u2227 vj \u00b7 yt < 0 0 o.w.\nand choose ai = ai,1... ai,m  s.t. the equality XX>ai = y \u2032 i\nholds. We first stress that by our assumption,\nXX> = \u3008x1,x1\u3009 . . . \u3008x1,xt\u3009... . . . ... \u3008xt,x1\u3009 . . . \u3008xt,xt\u3009  \u2208M(m\u00d7m) is of rank m, and therefore ai exists and is well-defined. Assuming that for any t \u2208 [m] there exists some neuron i s.t. \u3008wi,xt\u3009 > 0, vi \u00b7 yt \u2265 0 (We will later analyze the probability of this actually happening), we compute the prediction of our net with weights\nW \u2032 = (w\u20321, . . . ,w \u2032 n) on xt:\nNn ( W \u2032,v ) (xt) = n\u2211 i=1 vi\u03c6 (\u2329 w\u2032i,xt \u232a) =\nn\u2211 i=1 vi\u03c6 (\u2329 m\u2211 t\u2032=1 ai,t\u2032xt\u2032 ,xt \u232a)\n= n\u2211 i=1 vi\u03c6 ( m\u2211 t\u2032=1 ai,t\u2032 \u3008xt\u2032 ,xt\u3009 )\n= n\u2211 i=1 vi\u03c6 ( y\u2032i,t )\n= n\u2211 i=1 vi\u03c6 ( |yt| \u00b7 1\u3008wi,xt\u3009>0, vi\u00b7yt\u22650, \u2200j<i \u3008wj ,xt\u3009\u22640\u2227vj \u00b7yt<0 ) =\nn\u2211 i=1 yt \u00b7 1\u3008wi,xt\u3009>0, vi\u00b7yt\u22650, \u2200j<i \u3008wj ,xt\u3009\u22640\u2227vj \u00b7yt<0\n= yt\nwhere the last equality comes from our assumption that there exists some neuron i s.t. \u3008wi,xt\u3009 > 0, vi \u00b7yt \u2265 0, and from the definition of y\u2032i,t which asserts that at most a single neuron will predict xt. Thus we have\n\u2200t \u2208 [m] Nn ( W \u2032,v ) (xt) = yt\n\u21d2 ES ( W \u2032,v ) = 0\nTo put this result in different words, if xt is positive on the region combination of wi and if vi has the same sign as yt, then w\u2032i predicts xt correctly, given that xt was not previously predicted by a neuron w \u2032 j where j < i. We now assert that w\u2032i and wi indeed belong to the same region with respect to the region partition induced by S: Note that wi,w\u2032i belong to the same region induced by {xt} \u21d0\u21d2 sign (\u3008wi,xt\u3009) \u00b7 sign (\u3008w\u2032i,xt\u3009) \u2265 0. Thus we compute: If \u3008wi,xt\u3009 > 0, vi \u00b7 yt \u2265 0, \u2200j < i \u3008wj ,xt\u3009 \u2264 0 \u2227 vj \u00b7 yt < 0 all hold, then we have\nsign (\u3008wi,xt\u3009) = 1\nand\nsign (\u2329 w\u2032i,xt \u232a) = sign (\u2329 w\u2032i,xt \u232a) = sign\n(\u2329 m\u2211 t\u2032=1 ai,t\u2032xt\u2032 ,xt \u232a)\n= sign ( m\u2211 t\u2032=1 ai,t\u2032 \u3008xt\u2032 ,xt\u3009 ) = sign ( y\u2032i,t ) = sign (|yt|) \u2265 0\notherwise, we have sign (\u3008wi,xt\u3009) \u2264 0\nand\nsign (\u2329 w\u2032i,xt \u232a) = sign ( y\u2032i,t )\n= 0\nTherefore we have that wi,w\u2032i belong to the same region w.r.t. {xt} for any t and thus both belong to the same region w.r.t. the region partition induced by S. Finally, we define the event Ati := \u3008wi,xt\u3009 > 0, vi \u00b7yt \u2265 0, i.e. the ith neuron is able to predict xt correctly. Since vi,wi are independent we have\nPr [ Ati ] = Pr [\u3008wi,xt\u3009 > 0] \u00b7 Pr [vi \u00b7 yt \u2265 0]\n\u2265 1 4\n\u21d2 Pr [ Ati ] \u2264 3\n4\nsince (wi,wj) , (vi, vj) are independent for i 6= j, we have that Ati, Atj are mutually independent, so\nPr [ n\u22c2 i=1 Ati ] \u2264 ( 3 4 )n using the union bound on \u22c2n i=1A t i for t = 1, . . .m we get\nPr [\u2203t s.t. no neuron predicts xt] \u2264 m ( 3\n4 )n Thus, the probability of initializing from a basin achieving a global minimum with error 0 is at least\n1\u2212m ( 3\n4\n)n"}, {"heading": "6.2 Learning Clustered Data", "text": "In the previous section, we showed that when training on m \u2264 d data points, local minima are not an issue. Typically, when training neural networks, we expect the converse to be true - the dimension of the data is smaller than the size of the sample. To say something meaningful in that regime, we will consider an extension of the previous result, where instead of having fewer data points than dimensions d, we assume that the data is composed of k \u2264 d relatively small clusters. Intuitively, if the clusters are sufficiently small, the surface of the objective function will resemble that of having k \u2264 d data points, and therefore local minima might still not pose a problem.\nThe difficulty in analyzing the surface of the objective function when m d arises from the somewhat complicated region partition associated with the data. In the clustered case however, we would expect the\nregion partition to look the same as the region partition induced by the centers of the clusters, up to some noise which is created around each cluster center, which stems from the multiple points in each cluster.\nMore specifically, the hyperplane arrangement will be composed of several large regions which resemble the regions if we just had the cluster centers as the data; and many small \u201cnoisy\u201d regions close to their boundary, which reflect the fact that the data points don\u2018t lie exactly in the cluster center (see Figure 2).\nWhile it is unclear as to how does the surface of the objective function looks like in the new regions introduced by the cluster, henceforth the \u201cnoisy regions\u201d, we focus on the surface of the objective function in the regions induced by the centers of the clusters, that vary slightly due to the clusterability of the data.\nWe begin by bounding the radii of the clusters as to guarantee that the probability of initializing from noisy regions is negligible. Recall that we assume the weights of each neuron are initialized uniformly at random from the sphere, consider for arbitrary j the closed ball B\u0304\u03b4j (cj) which contains all the points of the jth cluster. The set of points residing in noisy regions is given by\nAj = { x : \u2016x\u2016 = 1, \u2203y \u2208 B\u03b4j (cj) s.t. \u3008x, y\u3009 = 0 } i.e. the set of points which are perpendicular to some point in the cluster. Let Acj = Sd\\Aj denote the complement of Aj with respect to the unit sphere, we want to lower bound the probability of initializing from Acj , i.e. we want to lower bound the ratio\n\u03c3d\u22121 ( Acj ) \u03c9d\u22121\nwhere \u03c3d\u22121 denotes the d\u2212 1 dimensional Lebesgue measure.\nLemma 6. Let S (x, \u03b8) denote the open spherical cap of spherical radius \u03b8 and center x. Then\nS ( cj , \u03c0\n2 \u2212 2 arcsin \u03b4j 2 \u2016cj\u2016\n) \u222a\u0307S ( \u2212cj , \u03c0\n2 \u2212 2 arcsin \u03b4j 2 \u2016cj\u2016\n) \u2286 Acj\nProof. Clearly the two spherical caps are disjoint as they are of spherical radius\u2264 \u03c02 and the two originate in two diametrically opposite points. Assume x \u2208 S ( cj , \u03c0 2 \u2212 2 arcsin \u03b4j 2\u2016cj\u2016 ) , then the projection of B\u0304\u03b4j (cj)\nonto Sd\u22121, denoted Pj , is a spherical cap of spherical radius \u03b8 := 2 arcsin \u03b4j\n2\u2016cj\u2016 . Since the dot product is a\nbi-linear operation, it suffices to show that \u2200y \u2208 Pj \u3008x\u0303,y\u3009 6= 0, where x\u0303 \u2208 Sd\u22121 is the projection of x onto Sd\u22121. Let y \u2208 Pj , using the fact that s, the spherical distance function, satisfies the triangle inequality we have\ns (x\u0303,y) \u2264 s (x\u0303, cj) + s (cj ,y)\n< \u03c0\n2 \u2212 \u03b8 + \u03b8\n= \u03c0\n2\n\u21d2 \u3008x\u0303,y\u3009 6= 0\nwhere the same argument works for x \u2208 S ( \u2212cj , \u03c02 \u2212 2 arcsin \u03b4j 2\u2016cj\u2016 ) and \u2212Pj .\nLemma 7. \u2200\u03b8 \u2265 0 we have \u03c0 2 \u2212\u03b8\u222b\n0\nsind\u22122 \u03bed\u03be \u2265 \u03c9d\u22121 2\u03c9d\u22122 \u2212 \u03b8\nProof. Consider the function f (\u03b8) = (\u222b \u03c0 2 \u2212\u03b8\n0 sin d\u22122 \u03bed\u03be ) \u2212 ( \u03c9d\u22121 2\u03c9d\u22122 \u2212 \u03b8 )\n, it is monotonically increasing in [0,\u221e) since\nf \u2032 (\u03b8) = \u2202\n\u2202\u03b8\n  \u03c0 2 \u2212\u03b8\u222b\n0\nsind\u22122 \u03bed\u03be \u2212 ( \u03c9d\u22121 2\u03c9d\u22122 \u2212 \u03b8 )\n= \u2212 sind\u22122 (\u03c0 2 \u2212 \u03b8 ) + 1 \u2265 0\nand since f (0) = 0 we have \u2200\u03b8 \u2208 [0,\u221e) that \u222b \u03c0 2 \u2212\u03b8\n0 sin d\u22122 \u03bed\u03be \u2265 \u03c9d\u221212\u03c9d\u22122 \u2212 \u03b8.\nEquipped with the above lemmas, we now formalize our result for clustered data:\nTheorem 8. Suppose our data is clustered into k \u2264 d clusters. Specifically, that the following assumptions hold:\n\u2022 \u2203c1, . . . , ck and \u03b41, . . . , \u03b4k s.t. \u2200x \u2208 S \u2203j \u2208 [k] s.t. x \u2208 B\u0304\u03b4j (cj) where B\u0304\u03b4 (c) denotes the closed ball of radius \u03b4 and center c.\n\u2022 \u2200j \u2208 [k] \u03b4j \u2264 \u03b4 for some \u03b4 \u2208 R+. \u2022 \u2200j \u2208 [k] \u03b4j\u2016cj\u2016 \u2264 2 sin ( \u221a 2\u03c0 16d \u221a d ) and \u2200j \u2208 [k] \u2016cj\u2016 \u2265 c for some c > 0.\n\u2022 \u2200t \u2208 [m] \u2016xt\u2016 \u2264 B for some B \u2208 R.\n\u2022 Assume that the corresponding values of the x\u2018s in each cluster is L-Lipschitz, i.e. if x1,x2 belong to the same cluster then their corresponding target values y (x1) , y (x2) satisfy \u2016y (x1)\u2212 y (x2)\u2016 \u2264 L \u2016x1 \u2212 x2\u2016.\nDenote asC the matrix which rows are c1, . . . , ck, and let \u03ba (C) = \u03c3max(C) \u03c3min(C) denote the condition number of C (or the ratio of the largest and smallest singular value of C). Let c (xt) : Rd \u2192 Rd, xt 7\u2192 cj where j is the index of the cluster which xt belongs to, and finally, let y\u0302 = (y\u03021, . . . , y\u0302m) \u2208 Rk denote the target values of the centers c1, . . . ck, which we define as y\u0302j = mint:c(xt)=cj |yt|+ 2L\u03b4j . Then\nFn\n( \u03b42 (( 1 + B\nc\n) n \u03ba (C)\n\u03c3min (C) \u2016y\u0302\u20162 + L\n)2) \u2265 1\u2212 d ( 7\n8 )n Note that \u03b4 measures how tight the clusters are, whereas \u03ba (C) and \u03c3min (C) can be thought of as constants assuming the cluster centers are in general position. So, the theorem implies that for sufficiently tight clusters, then with overwhelming probability, we will initialize from a basin containing a low-error minimum, as long as the network size is O (log (d)).\nProof. We now compute using 6 and 7, and the fact that \u2200d \u2265 2 \u03c9d\u22121\u03c9d \u2264 \u221a d 2\u03c0 ([15, Lemma 2.3.20])\n\u03c3d\u22121 ( Acj ) \u03c9d\u22121 \u2265 \u03c3d\u22121 ( S ( cj , \u03c0 2 \u2212 \u03b8 )) + \u03c3d\u22121 ( S ( \u2212cj , \u03c02 \u2212 \u03b8 )) \u03c9d\u22121\n= 2\u03bdd\u22121\n( \u03c0 2 \u2212 \u03b8 ) \u03c9d\u22121\n= 2 \u03c9d\u22122 \u03c9d\u22121\n\u03c0 2 \u2212\u03b8\u222b\n0\nsind\u22122 \u03bed\u03be\nLemma 7 \u2265 2\u03c9d\u22122 \u03c9d\u22121 ( \u03c9d\u22121 2\u03c9d\u22122 \u2212 \u03b8 )\n= 1\u2212 2\u03c9d\u22122\u03b8 \u03c9d\u22121\n\u2265 1\u2212 4 \u221a d\n2\u03c0 \u00b7 arcsin \u03b4j 2 \u2016cj\u2016\n\u2265 1\u2212 4 \u221a d\n2\u03c0 \u00b7 arcsin\n( sin ( \u221a 2\u03c0\n16d \u221a d )) = 1\u2212 1\n4d\nApplying the union bound to the k \u2264 d events where we initialize from Aj , we have that we don\u2019t initialize from a noisy region w.p. at least 34 . For a given t \u2208 [m], using the union bound again, the probability of initializing from a region in which any internal point w \u2208 Rd satisfies \u3008xt,w\u3009 > 0 is at least 14 , and finally, since vi has the correct sign w.p. 1 2 and is independent of where we initialize wi from, we are able to predict xt w.p. at least 78 . Using the union bound once more in the same manner as we did in the previous section gives that we initialize \u201cproperly\u201d w.p. at least\n1\u2212 k ( 7\n8\n)n \u2265 1\u2212 d ( 7\n8\n)n\nWe now turn to showing that when we initialize properly, then we can bound the error obtained in the basin we initialized from. We first stress that initializations made from noisy regions can be ignored using convexity considerations as in the previous sections (Theorem 5). Note thatNn (w1 . . . ,wn,v) (x) is \u2016x\u20162-Lipschitz in each wi (see the proof of lemma 5 in appendix 8.3.2), it is straightforward showing that\nNn (w1, . . . ,wn,v) (x) is \u2211n\ni=1 \u2016wi\u2016-Lipschitz in x using the same technique. Since \u03ba (C) is defined, in particular C is of full-rank and since we initialized properly, by theorem 7 there exists a set of weight W \u2032 = (w\u20321, . . . ,w \u2032 n) s.t. ES\u2032 (W \u2032,v) = 0 where S\u2032 = (cj , y\u0302j) k j=1 is the sample comprised of the clusters\u2019 centers. So for the purpose of training a net on the centers, we have that local minima are not an issue. However, even when initializing from non-noisy regions we have that the region combination of the data is affected when adding multiple observations in each cluster. We now show the existence of a set of weights W = (w1, . . . ,wn) in any non-noisy region that performs on the data S\u2032 well, and we then continue to argue that as a result of this W also performs well on the clustered data S, using Lipschitz considerations. For a randomly initialized w\u0302i \u2208 Rd which doesn\u2019t belong to a noisy region, if w\u2032i is inside the region then take w\u2032i = wi. If w\u2032i falls on a noisy region, we take wi closest to w \u2032 i in the \u2016\u00b7\u20162 norm sense. Denote the origin as O, and denote the point at which the line connecting O and w\u2032i is tangent to B\u03b4i (ci) by Hi, then the vertices O,wi,w\n\u2032 i and O, ci, Hi form similar triangles, and we have\u2225\u2225wi \u2212w\u2032i\u2225\u2225 = \u03b4ici \u2225\u2225w\u2032i\u2225\u22252\n\u2264 \u03b4 c \u2225\u2225w\u2032i\u2225\u22252 Recall that w\u2032i is defined as w \u2032 i := \u2211k j=1 ai,jcj = C >ai where ai is the solution to the equation\nCC>ai = y \u2032 i\nWe derive a bound on \u2016w\u2032i\u20162 as follows:\nCC>ai = y \u2032 i \u21d2 ai = ( CC> )\u22121 y\u2032i\n\u21d2 C>ai = C> ( CC> )\u22121 y\u2032i\n\u21d2 \u2225\u2225w\u2032i\u2225\u22252 = \u2225\u2225\u2225\u2225C> (CC>)\u22121 y\u2032i\u2225\u2225\u2225\u2225\n2 \u2264 \u2225\u2225\u2225C>\u2225\u2225\u2225\nop \u2225\u2225\u2225\u2225(CC>)\u22121\u2225\u2225\u2225\u2225 op \u2225\u2225y\u2032i\u2225\u22252 = \u03c3max (C) \u00b7 1\n\u03c32min (C) \u2225\u2225y\u2032i\u2225\u22252 \u2264 \u03ba (C)\n\u03c3min (C) \u2016y\u0302\u20162\nwhere \u2016\u00b7\u2016op denotes the operator norm. We now demonstrate how W \u2032 predicts S well:\u2223\u2223Nn (W \u2032,v) (xt)\u2212 yt\u2223\u2223\n= \u2223\u2223Nn (W \u2032,v) (xt)\u2212Nn (W \u2032,v) (c (xt)) +Nn (W \u2032,v) (c (xt))\u2212 yt\u2223\u2223\n\u2264 \u2223\u2223Nn (W \u2032,v) (xt)\u2212Nn (W \u2032,v) (c (xt))\u2223\u2223+ \u2223\u2223Nn (W \u2032,v) (c (xt))\u2212 yt\u2223\u2223\n\u2264 \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 w\u2032i \u2225\u2225\u2225\u2225\u2225 \u00b7 \u2016xt \u2212 c (xt)\u2016+ |y\u0302t \u2212 yt| \u2264 \u03b4n \u03ba (C)\n\u03c3min (C) \u2016y\u0302\u20162 + 2L\u03b4\nwhere the last inequality comes from yt, y\u0302t belonging to a ball of diameter at most 2\u03b4 and the target values being L-Lipschitz. In a similar manner, we show how W serves as a surrogate for W \u2032:\u2223\u2223Nn (W,v) (xt)\u2212Nn (W \u2032,v) (xt)\u2223\u2223 = n\u2211\ni=1\n\u2016xt\u2016 \u00b7 \u2225\u2225wi \u2212w\u2032i\u2225\u2225\n\u2264 n\u2211 i=1 B \u00b7 \u03b4 c \u2225\u2225w\u2032i\u2225\u22252 \u2264\nn\u2211 i=1 B \u00b7 \u03b4 c \u03ba (C) \u03c3min (C) \u2016y\u0302\u20162\n= nB \u00b7 \u03b4 c \u03ba (C) \u03c3min (C) \u2016y\u0302\u20162\nFinally, combining these two inequalities yields\nES (W, v) = 1\nm m\u2211 t=1 (Nn (W,v) (xt)\u2212 y\u0302t)2\n= 1\nm m\u2211 t=1 \u2223\u2223Nn (W,v) (xt)\u2212Nn (W \u2032,v) (xt) +Nn (W \u2032,v) (xt)\u2212 y\u0302t\u2223\u22232 \u2264 1\nm m\u2211 t=1 (\u2223\u2223Nn (W,v) (xt)\u2212Nn (W \u2032,v) (xt)\u2223\u2223+ \u2223\u2223Nn (W \u2032,v) (xt)\u2212 y\u0302t\u2223\u2223)2 \u2264 1\nm m\u2211 t=1 ( nB \u00b7 \u03b4 c \u03ba (C) \u03c3min (C) \u2016y\u0302\u20162 + \u03b4n \u03ba (C) \u03c3min (C) \u2016y\u0302\u20162 + 2L\u03b4 )2 = \u03b42 (( 1 + B\nc\n) n \u03ba (C)\n\u03c3min (C) \u2016y\u0302\u20162 + 2L )2 Thus concluding the proof of the theorem."}, {"heading": "7 Conclusions and future work", "text": "In this work, we studied the geometry of the surface of the objective function of artificial neural networks, via investigating the distribution of the minimal value of the basin we initialize from. We demonstrated a\nhardness example for single neuron nets which breaks down under overspecification with two-layer nets. We then moved on to characterize various properties of the objective function, which allowed us to further investigate the effects overspecification has on the geometry of the objective function. In particular, we showed that for ReLU networks of arbitrary depth, then with constant probability there exists a strictly monotonic path from the initial weights to a global optimum, hence \u201cvalley crossing\u201d is not strictly necessary. In the more particular context of two-layer ReLU networks, where we randomly choose the weights of the output neuron and optimize over the weights of the first layer, we showed that as the network size increases, we will initialize from a good basin with overwhelming probability, under various combinations of assumptions which include low rank, realizability and clusterability.\nWhile the results provided in this work demonstrate how overspecification can help us in the task of training neural nets, they still rely on assumptions which are non-trivial in practice (such as a strong cluster structure, small dimension or more dimensions than data points). Therefore, it would be interesting to provide similar results under weaker assumptions. Furthermore, although quite similar to the architecture used in practice as explained by 2, it is still of great interest to apply similar results without fixing the weights of the output layer. Finally, although possessing universal approximation capabilities, two-layer nets are seldom used in complicated learning problems, as deeper architectures are believed to be crucial for the compact representation of elaborate functions. For this reason, it is of great interest to provide analogous results for deeper architectures, such as the one provided in theorem 3."}, {"heading": "8 Appendix", "text": "In this appendix, we illustrate two specific constructions of theorem 1, one for ReLU and one for a sigmoid, both paired with the squared loss. We also prove two technical lemmas used in the proof of theorem 5."}, {"heading": "8.1 Exponentially many local minima for -realizable nets with ReLU activation.", "text": "Define \u03c6 (x) = max {0, x} , L ( y, y\u2032 ) = ( y \u2212 y\u2032 )2\nGiven > 0, consider the following sample:\nS =\n{( 1\n2 , \u221a 2\n) , (\u22121, 1) } Define for i = 1, 2\nLi (w) = (\u03c6 (wxi)\u2212 yi)2\nAnd denote ES (w) = 1\n2 (L1 (w) + L2 (w))\nNote that ES (\u22121) =\nES\n( 2 \u221a 2 ) = 1\n2\nare both local minima, and thus S is -realizable. It is straightforward showing thatES is convex in (\u2212\u221e, 0) and (0,\u221e), which is also evident in Figure 3, thus if we initialize uniformly on the unit ball then we have a 50% chance to initialize from the bad basin. Extending the sample into a d-dimensional one as we did in theorem 1, we have an -realizable dataset S with 2d local minima. Furthermore, we have that\nF\n( 1\n8\n) \u2264 e\u2212 d 16"}, {"heading": "8.2 Exponentially many local minima for -realizable nets with sigmoid activation.", "text": "The proof of theorem 1 required that the activation function will satisfy \u2200z \u2208 (\u2212\u03b4, 0) \u03c6 (z) = c for some c \u2208 R and \u03b4 > 0. While this demand includes the commonly used ReLU activation, there are other popular choices for activations, namely sigmoid or tanh, which do not fall under this category. We now provide an explicit construction for the sigmoid activation paired with the squared loss, which results in 2d local\nminima, where the fraction of sub-optimal minima converges exponentially fast to 1. We stress that one can construct such an example for the tanh activation similarly. Define\n\u03c6 (x) = 1\n1 + e\u2212x , L\n( y, y\u2032 ) = ( y \u2212 y\u2032 )2 Given > 0, consider the following sample:\nS = {(\u2212x1, y1) , (x2, y2)}\nwhere\nx1 , 4 ln\n( 2 ) y1 , 1\n2\nx2 , ln\n( 1 + 2 \u221a 2\n1\u2212 2 \u221a 2 ) y2 , \u03c6 (\u2212x2) = 1\n2 \u2212 \u221a 2\nDefine for i = 1, 2 Li (w) = (\u03c6 (wxi)\u2212 yi)2\nThen the loss over the second sample L2 (w) is given by\nL2 (w) =\n( \u03c6 ( w ln ( 1 + 2 \u221a 2\n1\u2212 2 \u221a 2\n)) \u2212 ( 1\n2 \u2212 \u221a 2\n))2\nLemma 8. For any w \u2208 (\u22121, 0]\nlim \u21920+\nL2 (w)\n2 (1 + w)2 = 1\nProof. Substituting \u03c6 (wx2) with its Taylor expansion at = 0, 12 + w \u221a 2 +O ( 1.5 ) yields:\nlim \u21920+\n( 1 2 + w \u221a 2 +O ( 1.5 ) \u2212 ( 1 2 \u2212 \u221a 2 ))2\n2 (1 + w)2\n= lim \u21920+\n(\u221a 2 (1 + w) +O ( 1.5 ))2\n2 (1 + w)2\n= lim \u21920+\n2 (1 + w)2 +O ( 2 )\n2 (1 + w)2\n= lim \u21920+\n1 + O ( 2 )\n= 1\nNow, by lemma 8, and for the purpose of determining the asymptotic behavior of the sample when \u2192 0+, we may w.l.o.g. use L\u03032 (w) , 2 (1 + w)2 as a surrogate for L2 (w), since for small enough > 0 the aforementioned surrogate behaves the same up to some constant multiplicative factor, which in turn becomes additive under the effect of the ln function (see Figure 5).\nWe compute\nx1 = 4 ln\n( 2 ) > 2 ln ( 2\n) as \u21920+ \u223c 2 ln ( 2\n1\u2212 \u221a 1\u2212 2 ) \u21d2 \u22121\n2 x1 < ln\n( 1\u2212 \u221a\n1\u2212 2 2 ) \u21d2 e\u2212 1 2 x1 < 1\u2212 \u221a\n1\u2212 2 2\n\u21d2 \u221a 1\u2212 2 < 1\u2212 2e\u2212 1 2 x1\n\u2264 1\u2212 e \u2212 1 2 x1\n1 + e\u2212 1 2 x1\n\u21d2 1 4 \u2212 2 < 1 4 \u00b7\n( 1\u2212 e\u2212 1 2 x1\n1 + e\u2212 1 2 x1 )2 = L1 ( \u22121\n2 ) and since L1 (w) in monotone decreasing in (\u2212\u221e, 0] we have \u2200w \u2208 ( \u2212\u221e,\u221212 ] L1 (w) > 1\u22122 4 . Thus\nL1 (w) =  0, w = 0 > 1\u22122 4 , w = \u2212 1 2\n< 14 , w = \u22121 and therefore the total error over the sample ES (w) = 12 ( L1 (w) + L\u03032 (w) ) satisfies\nES (w) =  , w = 0 > 18 , w = \u2212 1 2\n< 18 , w = \u22121\nso there exists some bad local minimum of ES (w) with value > 18 in the interval ( \u2212\u221e,\u221212 ] , whereas S is\n-realizable due to ES (0) = (see Figure 6). Extending the sample into a d-dimensional one as we previously did, we have an -realizable dataset S where ES contains 2d local minima. Defining a uniform distribution over all local minima, we have that the number of bad local minima is distributed B (d, 0.5), so from Chernoff\u2018s bound we have that the portion of minima attaining value at most 132 is bounded by e \u2212 1 16 d. Finally, we point out that even though our construction required the data to be unbounded, its growth rate is merely logarithmic in 1 ."}, {"heading": "8.3 Technical results from section 5", "text": ""}, {"heading": "8.3.1 Proof of lemma 4.", "text": "Proof. For a point a \u2208 Sd\u22121, let Sd\u22121 (a, \u03b8) := { b \u2208 Sd : \u3008a,b\u3009 \u2264 cos \u03b8 } be the hyper-spherical cap of angle \u03b8 \u2208 [0, \u03c0]. Note that if a,b \u2208 Sd form an angle of \u03b8\u2032 \u2208 [0, \u03b8] (i.e. b \u2208 Sd\u22121 (a, \u03b8)) then they form an isosceles triangle with apex angle \u03b8\u2032 and equal sides of length 1, so the distance between a and b satisfies\n\u2016a\u2212 b\u2016 = 2 sin ( \u03b8\u2032\n2 ) \u2264 2 sin ( \u03b8\n2 ) taking \u03b4 := 2 sin ( \u03b8 2 ) we have that \u03b8 = 2 arcsin ( \u03b4 2 ) , so in order for us to lower bound Pr [\u2016a\u2212 b\u20162 \u2264 \u03b4], we need to compute the surface area \u03bdd\u22121 (\u03b8) of the hyper-spherical cap of angle \u03b8 at point a, and normalize this quantity by the area of the entire hyper-sphere \u03c9d\u22121. The surface area of a hyper-spherical cap of radius \u03b8 is given by the formula: ([16])\n\u03bdd\u22121 (\u03b8) = \u03c9d\u22122 \u03b8\u222b 0 ( sind\u22122 \u03bed\u03be )\nwhere \u03c9d\u22121 denotes the surface area of Sd\u22121. Consider the function f (\u03b8) = \u222b \u03b8 0 ( sind\u22122 \u03bed\u03be ) \u2212 1d\u22121 sin d\u22121 \u03b8. It is monotonically increasing in [0, \u03c0] since\nf \u2032 (\u03b8) = \u2202\n\u2202\u03b8  \u03b8\u222b 0 ( sind\u22122 \u03bed\u03be ) \u2212 1 d\u2212 1 sind\u22121 \u03b8  = sind\u22122 \u03b8 \u2212 sind\u22122 \u03b8 \u00b7 cos \u03b8 = sind\u22122 \u03b8 \u00b7 (1\u2212 cos \u03b8)\n\u2200\u03b8\u2208[0,\u03c0]\n\u2265 0\nand since f (0) = 0 we have \u2200\u03b8 \u2208 [0, \u03c0] that \u222b \u03b8 0 ( sind\u22122 \u03bed\u03be ) \u2265 1d\u22121 sin\nd\u22121 \u03b8. We compute\nPr [Ad] = \u03c9d\u22122 \u03c9d\u22121 \u03b8\u222b 0 ( sind\u22122 \u03bed\u03be ) \u2265 \u03c9d\u22122\n\u03c9d\u22121 \u00b7 sin\nd\u22121 \u03b8\nd\u2212 1\n= \u03c9d\u22122 \u03c9d\u22121\n\u00b7 sind\u22121\n( 2 arcsin ( \u03b4 2 )) d\u2212 1\nusing the formulas sin (arcsinx) = x, cos (arcsinx) = \u221a 1\u2212 x2 and sin 2x = 2 sinx \u00b7 cosx, we have\nsind\u22121 ( 2 arcsin ( \u03b4\n2\n)) = ( \u03b4 \u221a 1\u2212 \u03b4 2\n4 )d\u22121 finally, \u03c9d\u22122\u03c9d\u22121 can be shown to be monotonically increasing for all d \u2265 2, so \u03c9d\u22122 \u03c9d\u22121 \u2265 \u03c90\u03c91 = 1 \u03c0 , thus yielding\nPr [Ad] \u2265 1\n\u03c0 (d\u2212 1)\n( \u03b4 \u221a 1\u2212 \u03b4 2\n4 )d\u22121 which concludes the proof of the lemma.\nBefore moving on to the next lemma, we stress that for a moderately sized dimension d, and in particular for smaller values of \u03b4, we have that the ( 1\u2212 \u03b424 ) d\u22121 2 term is approximately 1, thus Pr [Ad] \u2248 \u03b4 d\u22121\n\u03c0(d\u22121) . Specifically, it can be readily seen that a more precise computation yields the following bounds for small d\nd Pr [Ad] \u2265 2 \u03b4\u03c0 3 \u03b4 2\n2"}, {"heading": "8.3.2 Proof of lemma 5.", "text": "Proof. Fix i \u2208 [n] and Compute\u2223\u2223\u2223\u2223\u2223 \u2202\u2202wji Nn (W,v) (x) \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223g (\u3008wi,xt\u3009)xj \u00b7 vi\u2223\u2223 \u2264\n\u2223\u2223xj\u2223\u2223 thus\n\u2016\u2207Nn (W,v) (x)\u2016 = \u2225\u2225\u2225\u2225\u2225 ( \u2202\n\u2202wj1 Nn (W,v) (x) , . . . ,\n\u2202\n\u2202wjn Nn (W,v) (x) )\u2225\u2225\u2225\u2225\u2225 \u2264\n\u2225\u2225(x1, . . . , xn)\u2225\u2225 = \u2016x\u2016\nsince Nn is differential almost everywhere and has a bounded sub-gradient we have that it is \u2016x\u2016-Lipschitz in wi."}], "references": [{"title": "Learning polynomials with neural networks", "author": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1908\u20131916", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Exponentially many local minima for single neurons", "author": ["P. Auer", "M. Herbster", "M.K. Warmuth"], "venue": "NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["F. Bach"], "venue": "arXiv preprint arXiv:1412.8690", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798\u20131828", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex neural networks", "author": ["Y. Bengio", "N.L. Roux", "P. Vincent", "O. Delalleau", "P. Marcotte"], "venue": "Advances in neural information processing systems, pages 123\u2013130", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Training a 3-node neural network is np-complete", "author": ["A.L. Blum", "R.L. Rivest"], "venue": "Neural Networks, 5(1):117\u2013127", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "The loss surface of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": "arXiv preprint arXiv:1412.0233", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems, 2(4):303\u2013314", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1989}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8609\u20138613. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Qualitatively characterizing neural network optimization problems", "author": ["I.J. Goodfellow", "O. Vinyals"], "venue": "arXiv preprint arXiv:1412.6544", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": "CoRR abs/1506.08473", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8595\u20138598. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributing points on the sphere: partitions", "author": ["P. Leopardi"], "venue": "separation, quadrature and energy. PhD thesis, University of New South Wales", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Concise formulas for the area and volume of a hyperspherical cap", "author": ["S. Li"], "venue": "Asian Journal of Mathematics and Statistics, 4(1):66\u201370", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "On the computational efficiency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "NIPS, pages 855\u2013863", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Relations among complexity measures", "author": ["N. Pippenger", "M.J. Fischer"], "venue": "Journal of the ACM (JACM), 26(2):361\u2013381", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1979}, {"title": "l1 regularization in infinite dimensional feature spaces", "author": ["S. Rosset", "G. Swirszcz", "N. Srebro", "J. Zhu"], "venue": "Learning theory, pages 544\u2013558. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning kernel-based halfspaces with the 0-1 loss", "author": ["S. Shalev-Shwartz", "O. Shamir", "K. Sridharan"], "venue": "SIAM Journal on Computing, 40(6):1623\u20131646", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer Vision\u2013ECCV 2014, pages 818\u2013833. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Artificial neural nets are a known to have broad expressive power [17, 18, 20].", "startOffset": 66, "endOffset": 78}, {"referenceID": 17, "context": "Artificial neural nets are a known to have broad expressive power [17, 18, 20].", "startOffset": 66, "endOffset": 78}, {"referenceID": 19, "context": "Artificial neural nets are a known to have broad expressive power [17, 18, 20].", "startOffset": 66, "endOffset": 78}, {"referenceID": 3, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 8, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 12, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 13, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 20, "context": "A recent breakthrough in the effectiveness of neural networks has led to very impressive practical performance on a variety of domains (a few recent examples include [4, 9, 13, 14, 21].", "startOffset": 166, "endOffset": 184}, {"referenceID": 5, "context": "By reduction to k-coloring, it has been shown that finding the weights that best fit the training set is NP-hard [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "It is also known that even neural networks comprised of a single neuron might have exponentially many local minima [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 16, "context": "It was recently shown that for such networks, under mild assumptions, global optima are ubiquitous, and \u201cmost\u201d starting points will lead to the global optima upon optimizing the weights of the last layer [17].", "startOffset": 204, "endOffset": 208}, {"referenceID": 2, "context": "There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited.", "startOffset": 78, "endOffset": 88}, {"referenceID": 6, "context": "There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited.", "startOffset": 78, "endOffset": 88}, {"referenceID": 16, "context": "There is some theoretical and empirical evidence that this is indeed the case [3, 7, 17], but our theoretical understanding of this phenomenon is still quite limited.", "startOffset": 78, "endOffset": 88}, {"referenceID": 1, "context": "What we do One way to study the hardness of the training problem has been to consider the number of non-global local minima the objective function has [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 1, "context": "More specifically, we provide the following results: \u2022 We begin by extending the result of [2] on exponentially many local minimum basins for single neurons, by showing that it holds even if there exists a hypothesis which achieves an arbitrarily small positive training error.", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "Although this does not ensure that such a global minimum will be reached, it does mean that \u201ccrossing valleys\u201d across the non-convex loss surface is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11].", "startOffset": 244, "endOffset": 248}, {"referenceID": 16, "context": "Related work We begin by discussing the work done in [17], where the authors show that for any non-linear activation function and architecture large enough such that the last hidden layer is at least as large as the size of the sample, then global minima are ubiquitous.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "Other recent efforts in the field include [1, 3, 12].", "startOffset": 42, "endOffset": 52}, {"referenceID": 2, "context": "Other recent efforts in the field include [1, 3, 12].", "startOffset": 42, "endOffset": 52}, {"referenceID": 11, "context": "Other recent efforts in the field include [1, 3, 12].", "startOffset": 42, "endOffset": 52}, {"referenceID": 2, "context": "In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19].", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19].", "startOffset": 253, "endOffset": 260}, {"referenceID": 18, "context": "In [3], the author studies an approach reducing the finite dimensional problem of training a 2-layer neural net paired with ReLUs with a fixed architecture into a problem of finding a sparse measure on an infinite dimensional space (building on work in [5, 19].", "startOffset": 253, "endOffset": 260}, {"referenceID": 0, "context": "In [1], the authors show that two-layer neural nets of sufficient size can learn low degree polynomials using gradient descent.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "In [12], the authors present an algorithm with provable guarantees for training neural nets via tensor decompositions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "Another relevant work is [7], in which the authors investigate the surface area of the loss function of neural nets using ReLUs as activation functions, and make somewhat strong distributional assumptions on the data, such as assuming that the coordinates of the observations in the data are standard normally distributed, and in particular the coordinates are independent of one another, and modeling the connection between neurons as a Bernoulli random variable with constant success probability.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "See also [10] and references therein for related works along similar lines.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "Although simple to analyze and program, nets comprised of a single neuron prove to be very hard to train in the worst case, as the objective function might contain exponentially many poor local minima as a function of the dimension d [2].", "startOffset": 234, "endOffset": 237}, {"referenceID": 1, "context": "However, the authors in [2] also demonstrate that for the 0-realizable case, under some mild conditions on the loss and activation functions, there exists a single minimal surface of the objective function.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "Furthermore, since the loss on x1 is constant \u2200w \u2208 [\u22121, 0] and the loss on x2 is constant \u2200w \u2208 [0, 1], we have that the objective function contains two basins meeting at zero, so the sign of w determines which of the basins we fall into.", "startOffset": 95, "endOffset": 101}, {"referenceID": 1, "context": "We now extend our sample to be d-dimensional in a similar manner as did the authors in [2] as follows: For i = 1, 2 and j \u2208 [d], we use the mapping xi,j 7\u2192 (0, .", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "However, this result nevertheless sheds light on the nature of the surface function, demonstrating that it is not completely sporadic in the sense that \u201ccrossing valleys\u201d is not a must to reach a good solution, and accords with recent empirical evidence to this effect [11].", "startOffset": 269, "endOffset": 273}, {"referenceID": 0, "context": "Assume that there exists a continuous path \u03b3 (t), t \u2208 [0, 1] s.", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "\u03b3 (0) = winit, \u03b3 (1) = wopt and \u2200t \u2208 [0, 1], there exist an instance x in the training set such that N (\u03b3(t)) (x) 6= 0.", "startOffset": 37, "endOffset": 43}, {"referenceID": 0, "context": "We have \u2016N (winit)\u2212 y\u2016 > \u2016y\u2016 \u21d2 \u2016N (winit)\u2016 \u2212 2 \u3008N (winit) ,y\u3009 > 0 (2) Define Nt := N (\u03b3 (t)), we once again use the positively-homogeneous nature of the ReLU to scale the weights of the net by a positive factor at > 0, while stressing that the addition of a bias term does not affect this trick and nevertheless allows us to scale the weights by a constant positive factor, effectively forcing \u2200t \u2208 [0, 1] the equality \u2016atNt \u2212 y\u2016 = \u2016N0 \u2212 y\u2016 (1\u2212 t) + \u2016y\u2016 t", "startOffset": 399, "endOffset": 405}, {"referenceID": 7, "context": "Although still relatively simple, they already possess universal approximation properties [8], and encapsulate the challenge of handling a highly non-convex objective function.", "startOffset": 90, "endOffset": 93}], "year": 2017, "abstractText": "Over the past few years, artificial neural networks have seen a dramatic resurgence in popularity as a tool for solving hard learning problems in AI applications. While it is widely known that neural networks are computationally hard to train in the worst case, in practice, neural networks are trained efficiently using SGD methods and a variety of techniques which accelerate the learning process. One mechanism which has been suggested to explain this is overspecification, which is the training of a network larger than what would be needed with unbounded computational power. Empirically, despite worst-case NP-hardness results, large networks tend to achieve a smaller error over the training set. In this work, we aspire to understand this phenomenon. In particular, we wish to better understand the behavior of the error over the sample as a function of the weights of the network, where we focus mostly on neural nets comprised of 2 layers, although we will also consider single neuron nets and nets of arbitrary depth, investigating properties such as the number of local minima the function has, and the probability of initializing from a basin with a given minimal value, with the goal of finding reasonable conditions under which efficient learning of the network is possible.", "creator": "LaTeX with hyperref package"}}}