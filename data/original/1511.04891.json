{"id": "1511.04891", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Sherlock: Scalable Fact Learning in Images", "abstract": "How to build a machine learning method that can continuously gain structured visual knowledge by learning structured facts? Our goal in this paper is to address this question by proposing a problem setting, where training data comes as structured facts in images with different types including (1) objects(e.g., &lt; boy &gt;), (2) attributes (e.g., &lt; boy,tall &gt;), (3) actions (e.g., &lt; boy, playing &gt;), (4) interactions (e.g., &lt; boy, riding, a horse &gt;). Each structured fact has a semantic language view (e.g., &lt; boy, playing &gt;) and a visual view (an image with this fact). A human is able to efficiently gain visual knowledge by learning facts in a never ending process, and as we believe in a structured way (e.g., understanding \"playing\" is the action part of &lt; boy, playing &gt;, and hence can generalize to recognize &lt; girl, playing &gt; if just learn &lt; girl &gt; additionally). Inspired by human visual perception, we propose a model that is (1) able to learn a representation, we name as wild-card, which covers different types of structured facts, (2) could flexibly get fed with structured fact language-visual view pairs in a never ending way to gain more structured knowledge, (3) could generalize to unseen facts, and (4) allows retrieval of both the fact language view given the visual view (i.e., image) and vice versa. We also propose a novel method to generate hundreds of thousands of structured fact pairs from image caption data, which are necessary to train our model and can be useful for other applications.", "histories": [["v1", "Mon, 16 Nov 2015 09:56:04 GMT  (5197kb,D)", "http://arxiv.org/abs/1511.04891v1", "Under Review for ICLR2016 (initial submission, Nov 15-2016)"], ["v2", "Thu, 19 Nov 2015 22:36:55 GMT  (7322kb,D)", "http://arxiv.org/abs/1511.04891v2", "Under Review for ICLR2016 (Update submission, Nov 19-2016)"], ["v3", "Fri, 8 Jan 2016 02:56:24 GMT  (8880kb,D)", "http://arxiv.org/abs/1511.04891v3", "Jan 7 Update"], ["v4", "Sat, 2 Apr 2016 05:26:39 GMT  (8879kb,D)", "http://arxiv.org/abs/1511.04891v4", "Jan 7 Update"]], "COMMENTS": "Under Review for ICLR2016 (initial submission, Nov 15-2016)", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["mohamed elhoseiny", "scott cohen", "walter chang", "brian l price", "ahmed m elgammal"], "accepted": true, "id": "1511.04891"}, "pdf": {"name": "1511.04891.pdf", "metadata": {"source": "CRF", "title": "SHERLOCK: MODELING STRUCTURED KNOWLEDGE IN IMAGES", "authors": ["Mohamed Elhoseiny", "Scott Cohen", "Walter Chang", "Brian Price", "Ahmed Elgammal"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "It is a capital mistake to theorize in advance of the facts. -Sherlock Holmes (A Scandal in Bohemia)\nLet us imagine a scene with the following facts: <man>, <baby>, <toy>, <man, smiling>, <baby, smiling>, <baby, sitting on, chair>, <man, sitting on, chair>, <baby, sitting on, chair>, <baby, holding, toy>, <man, feeding, baby>. We might expect that the imagined scene will be very close to the image in Fig. 1 due to the precise structured description. On the other hand, if we were given the same image and asked to describe it, we might expect only a short title \u201cman feeding a baby\u201d. Providing the given structured facts from this image assumes a detective\u2019s eye that look for structured details that we aim to model. State-of-the-art captioning methods (e.g., Karpathy & Fei-Fei (2015); Vinyals et al. (2015); Xu et al. (2015); Mao et al. (2015)) rely on the idea of generating a sequence of words given an image of a scene, inspired by the success of sequence to sequence training of neural nets in machine translation systems (e.g., Cho et al. (2014)). While it is an impressive step, the mechanism of these captioning systems makes them incapable of conveying structured information in an image and providing a confidence of the generated caption given the facts in the image. It might also provide a limited description like \u201cman feeding a baby\u201d, which makes the image search difficult on the other direction due to lack of representation. Captions and unstructured tags are mainly a vehicle to communicate facts with humans. However, they may not be the best way to represent that knowledge in a way that is searchable for the machine. There are advantages to having explicit, structured knowledge for image search. If one searches for images of a \u201cred flower\u201d, a bag-of-words approach that considers \u201dred\u201d and \u201cflower\u201d separately may return images of flowers that are not red but have red elsewhere in the image. It is important to know that a user is looking for the fact <flower,red>. Modeling the connection between the provided structured facts in its language form and its visual view (i.e., an image containing it) facilitates gaining richer visual knowledge, which is our focus in this paper. Several applications can make use of modeling\nar X\niv :1\n51 1.\n04 89\n1v 1\n[ cs\n.C V\n] 1\n6 N\nov 2\n01 5\nthat connection, such as structured fact tagging, high precision image search from text, generating comprehensive descriptions of complicated scenes, and making higher level reasoning about a scene.\nHow to model structured facts to enable machines to gain structured visual knowledge?\nAnother motivating perspective of modeling structured facts is to look at gaining visual knowledge as learning more and more structured facts since it is both richer and allows better generalization to unseen facts. For example if a model learned during training about <girl, smiling>,\n<boy, petting, a dog>, <girl, riding, a horse>, and <boy>, it should be able to recognize the fact <boy, petting, a horse> even though it did not see that fact before. In our work, we aim to cover three types of facts by the same model, which we define as follows. First order facts in images are defined by the set of objects and the scene in an image (e.g,. <baby>, <toy>, <man>). Second order facts are defined by attributes and single-frame action that might be performed by the objects (e.g., < baby, smiling>, <man, dancing>). An object interacting with another object defines a third order fact in the image (e.g., <man, feeding, baby >, <baby, sitting on, chair>). We denote the first, second, and third order facts by <Subject>, <Subject, Predicate>, and <Subject, Predicate, Object> respectively, abbreviated as <S>, <S,P>, and <S,P,O>; see Fig. 1. Inspired by the modifier concept in language grammar, we model higher order facts as visual modifiers of the low order facts. For example, <baby, smiling> is applying <smiling> visual modifier of <baby>. Based on this notion, we propose a model for learning representation of structured fact covering its different orders, and can continuously be fed with these facts to gain structured knowledge. Specifically, both language and visual views of a structured fact inhabit in a continuous space. Modeling structured facts in a continuous space allows us to extend the gained knowledge from the training facts to unseen facts. Since the proposed setting is aiming for a model that is an eye for details and potentially allows higher order reasoning (Fig. 1), we denote this problem by the Sherlock Problem.\nThere are indeed other research problems that aim to explicity understand facts about an image. The main paradigm in object (e.g., Simonyan & Zisserman (2015)), scene (e.g., Zhou et al. (2014)), and activity categorization (e.g., Gkioxari & Malik (2015)) methods is to have a set of discrete categories that the system can recognize. This faces a scalability problem since adding a new category means changing the architecture of the model by adding thousands or parameters and re-training the model (e.g., needed for adding a new output node). This is also shared with recent attribute based methods(e.g., Chen & Grauman (2014)) that realizes the idea that attribute appearance is dependent on the class as opposed to earlier models (e.g., Lampert et al. (2009)), which motivates them to jointly learn a classifier for every class attribute pair suffering from scalability problems as the number of the classes and attributes grows. Attributed classes is a subset of the second order facts that we aim to cover together with other types of facts. Our goal is to model structured facts in a way that is scalable, i.e., to avoid the need of changing the model while adding more facts. Furthermore, we aim that the model can gain structured visual knowledge by being continuously fed with instances of structured facts, that could be of different types (e.g., <woman, smiling>, or <person, playing, soccer>, < man, tall>, <kid>) with example image(s). At any point the machine does not have a fixed dictionary of trained object, scene, activity categories. From this point, we might refer to a structured fact as simply a fact for simplicity.\nRecent works in language and vision involves using unannotated text to improve object recognition and/or achieve zero-shot learning. In Frome et al. (2013); Norouzi et al. (2014) and Socher et al. (2013), word embedding language models (e.g., Mikolov et al. (2013)) was adopted to represent class names as vectors. Their framework is based on mapping images into the learned language model then perform classification in that space. Other works models the opposite direction by mapping unstructured text descriptions for classes into a visual classifier Elhoseiny et al. (2013); Ba et al. (2015). In contrast to these works, our goal is to gain visual knowledge for not only first order facts (i.e., objects<S>) but also for second<S,P>, and third order facts<S,P,O>, and also maintain the structure of the facts. Let\u2019s assume that |S|, |P|, and |O| denotes the number of subjects, predicates,\nobjects respectively. The scale of unique second and third order facts is bounded by |S| \u00d7 |P|, and |S| \u00d7 |P| \u00d7 |O| possibilities respectively that could easily reach hundreds of thousands or millions of unique facts and needs careful attention while designing a model maintaining the structure we aim at. Some of the earlier captioning systems involves learning an intermediate object, verb, subject representation of images prior to generating captions, e.g. Kulkarni et al. (2013); Yu & Siskind (2013) however, they fall into the aforementioned scalability limitation.\nVery recently, Johnson et al. (2015) proposed an interesting work to retrieve images using scene graphs, which captures similar object attribute and relationship information to our representation. But the Scene Graph work does not provide a method to automatically extract a scene graph for an image, but rather manually obtained a dataset for 5000 scenes. It can only solve for the best grounding of a given a manually annotated scene graph to an image, which is key difference to our work together with our learning representation. In addition, our work is on a much larger scale and models two-way retrieval.\nHow to collect structured fact annotations needed to train a Sherlock Model?: In order to train a model for our setting, we needed to collect structured fact annotations in the form of language view, visual view pairs(i.e., <baby, sitting on, chair> as the language a view and an image with this fact as a visual view), which is a challenging task. We started by manually annotating and mining several existing datasets to extract structured fact annotations, which we found limiting for both covering different types of facts and number of image as detailed later in Sec. 3. One of the most interesting relevant works is Never Ending Image Learner (NEIL) Chen et al. (2013), where they showed that visual concepts predefined in an ontology can be learnt by collecting its training data from the web. In a follow-up work, Divvala et al. (2014) similarly web-collected images for concepts related to a predefined object using Google-N-gram data. This opens the question of whether we can collect structured fact annotations from the web. There are two issue that we face for our setting. First, it is hard to define the space of structured visual knowledge and then search for it. Second, using Google image search is not reliable to collect data for concepts with less images in the web. The main assumption for this method depends on both the likelihood that the top retrieved image belongs to the searched concept, and the availability of image annotated with the searched concept. These problems motivated us to propose a novel method to automatically annotate structured facts by processing images-caption data. Our Sherlock Automatic Fact Annotation (SAFA) started by processing challenging unconstrained noisy captions to extract fact language view by multiple methods. Then, we grounded the facts extracted from captions to image regions as fact visual view. There are two advantages of collecting structured fact annotations by the proposed method. First, it assumes that structured facts in the captions is likely to be located in the given image, which is expected to occur in image-caption data with a very high probability compared to the assumptions in Chen et al. (2013); Divvala et al. (2014) and makes it likely to produce accurate annotations as we show in our results. Second, it could be used to collect knowledge very cheaply for several tasks. Our automatic annotations resulted in hundreds of thousands of images and tens of thousands of unique knowledge annotations, needed to feed our model, in just several hours with our current implementation whose speed is far from optimal The contributions of this paper are as follows: \u2022 We introduce the Sherlock problem of structured knowledge modeling in an image and propose\none model that can learn structured facts of different types and perform both-view retrieval ( retrieve structured fact language view (i.e. <S:people, P:walking , >) given the visual view (i.e. image) and vice versa). \u2022 We propose an automatic stuctured fact annotation method based on sophisticated Natural Language Processing methods for acquiring high quality structured fact annotation pairs at large scale from free-form image descriptions. We applied the pipeline to MS COCO Lin et al. (2014) and Flickr30K Entities Plummer et al. (2015); Young et al. (2014) image caption datasets. In total, we build a structured fact dataset of more than 816, 000 language&image-view fact pairs covering more than 202, 000 unique facts in the language view. \u2022 We develop a novel learning representation network architecture to model the jointly model structured fact language and visual views by mapping both views into a common space and using a wild card loss to uniformly represent first, second, and third order facts.\nOur modeling approach is scalable to new facts without any change to the network architecture.\n2 PROBLEM DEFINITION\nWe aim at modeling structured knowledge in images as a problem that comes with views, one in the visual domain V and one in the language domain L. We start by introducing some notation. Let f be a structured fact, fv \u2208 V denotes the view of f in the visual domain, and fl \u2208 L denotes the view of f in the language domain. For instance, an annotated fact, with language view fl =<S:girl, P:riding, O:bike> would have a corresponding visual view fv as an image where this fact occurs; see example in Fig. 2.\nOur goal is to learn a representation that covers first-order facts <S> (objects), second-order facts <S,P> (actions and attributes), and third-order facts <S,P,O> (interaction and positional facts). We represent all types of facts as an embedding problem into what we call \u201cstructured fact space\u201d. We define \u201cstructured fact space\u201d as a learning representation of three hyper-dimensions that we denote as \u03c6S \u2208 RdS , \u03c6P \u2208 RdP , and \u03c6O \u2208 RdO (Fig. 2). We denote the embedding functions from a visual view of a fact fv to \u03c6S , \u03c6P , and \u03c6O as \u03c6VS (fv), \u03c6 V P (fv), and \u03c6 V O(fv), respectively. Similarly, we denote the embedding functions of from a language view of a fact fl to \u03c6S , \u03c6P , and \u03c6O as \u03c6LS(fl), \u03c6 L P (fl), and \u03c6 L O(fl), respectively. We denote the concatenation of the visual view hyper-dimensions\u2019 embedding as \u03c6V(fv), and the language view hyper-dimensions\u2019 embedding as \u03c6L(fl), where \u03c6V(fv) and \u03c6L(fl) are visual embedding and language embedding of f , respectively:\n\u03c6V(fv) = [\u03c6 V S (fv), \u03c6 V P (fv), \u03c6 V O(fv)], \u03c6 L(fl) = [\u03c6 L S(fl), \u03c6 L P (fl), \u03c6 L O(fl)] (1)\nIt is not hard to see that third-order facts <S,P,O> can be directly embedded to the structured fact space by \u03c6V(fv) for the image view and \u03c6L(fl) for the language view. A question remains how firstand second-order facts be represented in Eq. 1, so that we have a unified fact learning model that covers facts of all orders. In the rest of this section, we present the notion of fact modifiers and how they are reflected in our proposed learning representation."}, {"heading": "2.1 HIGH ORDER FACTS AS MODIFIERS", "text": "First-order facts are facts that indicate an object like <S: person>. Second-order facts gets more specific about the subject (e.g. <S: person, P: playing>). Third-order facts get even more specific (<S: person, P: playing, O: piano>). Inspired by the concept of modifiers in language grammar, we propose to define higher order facts as lower order facts with an additional modifier applied to it. For example, adding the modifier P: eating to the fact <S: kid>, constructs the fact <S: kid, P: eating>. Further applying the modifier O: ice cream to the fact <S: kid, P: eating>, construct the fact <S: kid, P: eating, O: ice cream>. Similarly, attributes could be seen as modifiers to a subject (e.g., applying P: smiling to fact <S: baby> constructs the fact <S: baby, P: smiling>)."}, {"heading": "2.2 WILD-CARD REPRESENTATION", "text": "Based on our \u201cfact modifier\u201d observation, we propose to represent both first- and second-order facts as wild cards, as illustrated in Eq. 2 and 3 for first-order and second-order facts, respectively. We denote the wild-card modifier by \u201c\u2217\u201d. First-Order Facts wild-card representation <S>\n\u03c6V(fv) = [\u03c6 V S (fv), \u03c6 V P (fv) = \u2217, \u03c6VO(fv) = \u2217], \u03c6L(fl) = [\u03c6LS(fl), \u03c6LP (fl) = \u2217, \u03c6LO(fl) = \u2217] (2)\nSecond-Order Facts wild-card representation <S, P>\n\u03c6V(fv) = [\u03c6 V S (fv), \u03c6 V P (fv), \u03c6 V O(fv) = \u2217], \u03c6L(fl) = [\u03c6LS(fl), \u03c6LP (fl), \u03c6LO(fl) = \u2217] (3)\nSetting \u03c6P and \u03c6O to \u2217 for first-order facts is interpreted to mean that the P and O modifiers are not of interest for first-order facts, which is intuitive. Similarly, setting \u03c6O to \u2217 for second-order facts indicates that the O modifier is not of interest for single-frame actions and attributes.\nLower order facts do not necessarily mean that a higher order fact does not exist in it. For example, <person> fact in an image does not mean that he is not performing an action or has a particular attribute like tall. It rather means that we don\u2019t know. Hence, the wild cards (i.e. \u2217) of the lower order facts are not penalized during training in our loss, as we illustrate later at the end of Sec. 4. We name both first and second-order facts as wild-card fact."}, {"heading": "3 DATA COLLECTION OF STRUCTURED FACTS", "text": "In order to train a machine learning model that connects the structured fact language view in L with its visual view in V , we need to collect large scale data in the form of (fv , fl) pairs. Data collection especially for large scale problems has become an increasingly challenging task. It is further challenging in our setting since our knowledge model relies on the localized association of a structured language fact fl with an image fv when such facts occur. In particular, it is a complex task to collect annotations especially for second-order facts < S,P > and third-order facts <S, P, O>. Also, multiple structured language facts could be assigned to the same image (e.g., <S: man, P: smiling>, and < S :man, P: wearing, O: glass>. If these facts refer to the same man, the same image example could be used to learn about both facts.\nWe began our data collection by augmenting existing datasets with fact language view labels (i.e., fl):PPMI Yao & Fei-Fei (2010), Stanford40 Yao et al. (2011), Pascal Actions Everingham et al.,\nSports Gupta (2009), Visual Phrases Sadeghi & Farhadi (2011), INTERACT Antol et al. (2014) datasets. The union of these 6 datasets resulted in 186 facts with 28,624 images as broken out in Table 1.\nWe also extracted structured facts from the Scene Graph dataset Johnson et al. (2015) which has manually annotated 5000 images by MTurkers in a graph structure from which first-, second-, and third-order relationship can be easily extracted. We extracted 110,000 second-order facts and 112,000 third-order facts. The majority of these are positional relationships, and covers only 5000 scenes.\nWe further propose a method to automatically collect structured fact annotations from datasets that come in the form of image-caption pairs, which can more quickly provide useful facts (Sec. 3.1). Our proposed method opens the door for easy continual fact collection in the future from caption datasets and even the web or in general any naturally occurring documents with captioned images."}, {"heading": "3.1 SHERLOCK AUTOMATIC FACT ANNOTATION (SAFA) FROM IMAGES WITH CAPTIONS", "text": "While crowd sourcing can be used to provide additional data, our task of labelling second- and third-order facts is complex, and acquiring enough data would be expensive.\nInstead, we automatically obtain a large quantity of high quality facts from caption datasets using natural language processing methods. Since caption writing is free-form and an easy task for crowdsourcing workers, such free-form descriptions are readily available in existing image caption datasets (e.g. MS COCO Lin et al. (2014)) and more data can be efficiently collected in the future for\nimproved learning. We can also mine existing tagged images from sources such as Flickr that typically have single word, short phrase, or single sentence image descriptions.\nIn our work, we focused on the MS COCO image caption dataset Lin et al. (2014) and the newly collected Flickr30K entities Plummer et al. (2015) to both collect and automatically generate new structured fact annotations. In contrast to having annotations for only 5000 scenes (e.g., in the manually annotated Scene graph dataset), we automatically extracted facts from more than 600,000 captions associated with 120,000 MS COCO images, and facts from 150,000 captions associated with 30,000 scenes for the Flickr30K dataset; this provided us with 30 times more additional coverage over the initial structured facts that were associated with the provided scene data.\nWe propose SAFA as a two step automatic annotation process: (i) fact extraction from captions, (ii) fact localization in images.\nFirst, the captions associated with the given image are analyzed to extract sets of clauses that are considered as candidate < S,P >, and < S,P,O > facts in the image. We extract clauses using two state-of-the-art methods: Sedona sed (2015) and Clausie Del Corro & Gemulla (2013). Clauses form facts but are not necessarily facts by themselves.\nCaptions can provide a tremendous amount of information to image understanding systems. However, developing NLP systems to accurately and completely extract structured knowledge from freeform text is an open research problem. We addressed several challenging linguistic issues by evolving our NLP pipeline to: 1) correct many common spelling and punctuation mistakes, 2) resolve word sense ambiguity within clauses, and 3) learn both a common spatial preposition lexicon (e.g., \u201dnext to\u201d, \u201don top of\u201d, \u201din front of\u201d) that consists of over 110 such terms, as well as a lexicon of over two dozen collection phrase adjectives (e.g., \u201dgroup of\u201d, \u201dbunch of\u201d, \u201dcrowd of\u201d, \u201dherd of\u201d). These strategies allowed us to extract more and more interesting structured knowledge for learning to understand our images.\nSecond, we try to localize these clauses within the image (see Fig. 3). The subset of clauses that are successfully located in the image are saved as additional fact annotations for training our model. We collected 146,515, 157,122, and 76,772 annotations from Flickr30K Entities, MS COCO training, and validation sets, respectively. While we ignored some types of clauses that were likely to produce incorrect annotations, we achieved a total of 380,409 second- and third-order fact annotations. With future improvement and by carefully considering more clauses, SAFA can potentially collect many more such annotations.\nWe note that the process of localizing facts in an image is constrained by information in the dataset. For MS COCO, the dataset contains object annotations for about 80 different objects as provided by the training and validation sets. This allowed us to localize first-order facts for objects using bounding box information. In order to locate higher-order facts in images, we started by defining visual entities. In case of the MS COCO dataset Lin et al. (2014), we define a visual entity as any noun that is either (1) one of Microsoft COCO dataset objects, (2) a noun in the WordNet ontology Miller (1995); Leacock & Chodorow (1998) that is an immediate or indirect hyponym of one of the MS COCO objects. Since, wordNet is searchable by a sense not a word, we performed word sense disambiguation on the sentences using a State-of-the-art method Zhong & Ng (2010), or (3) one of the SUN dataset scenes Xiao et al. (2010). We expect visual entities to be appear either in the S or the O part (if it exists) of a candidate fact fl. This allows us to then localize facts for images in the MS COCO dataset. Given a candidate third-order fact, we first try to assign each S and O to one of the visual entities. If S and O elements are not visual entities, then the clause is ignored. Otherwise, the clauses are processed by several heuristics, detailed in the supplementary materials. Our heuristics take into account whether the subject or the object is singular or plural, or a scene. E.g.,instance, for clauses in the fact< S :men, P : chasing, O : soccer ball>, our method takes into account that \u201cmen\u201d may require the union of multiple candidate bounding boxes, while for \u201csoccer ball\u201d, it is expected that there is a single bounding box.\nIn Flickr30K Entities dataset Plummer et al. (2015), the bounding box annotations are presented as phrase labels for sentences (for each phrase in a caption that refers to an entity in the scene). A difference in processing occurs due to the definition of a visual entity in Flickr30K. In this dataset a visual entity is considered to be a phrase with a bounding box annotation or one of the SUN scenes. Several heuristics were developed and applied to collect these fact annotations; see our supplementary materials for details."}, {"heading": "4 SHERLOCK MODELS", "text": "Our goal is to propose a two-view structured fact embedding model with four properties: (1) can be continuously fed with new facts without changing the architecture, (2) is able learn with wild card to support all types of facts, (3) could generalize to unseen facts, (4) allows two way retrieval (i.e., retrieve relevant facts in language view given an image, and retrieve relevant images given a fact in a language view). Satisfying these properties can be achieved by using a generative model p(fv, fl) that connects the visual and the language views of f , where more importantly fv and fv inhabit in a continuous space. Our method is to model p(fv, fl) \u221d s(\u03c6V(fv), \u03c6L(fl)), where s(\u00b7, \u00b7) is a similarity function defined over the structured fact space denoted by S, where S is a discriminative space of facts. Our objective is that two views of the same fact should be embedded so that they are close to each other. The question now is how to model and train \u03c6V(fv), and \u03c6L(fl). We choose to model \u03c6V(fv) as a CNN encoder (e.g., Krizhevsky et al. (2012); Simonyan & Zisserman (2015)), and \u03c6L(fl) as RNN encoder (e.g., Mikolov et al. (2013); Pennington et al. (2014)), due to their recent success as encoders for images and words, respectively. We propose two models for learning facts, denoted by Model 1 and Model 2. Model 1 and 2 shares the same structured fact language embedding/encoder but differ in the structured fact image encoder.\nWe start by defining an activation operator \u03c8(\u03b8, a), where a is an input, and \u03b8 is a series of one or more neural network layers (may include different layer types, e.g., convolution, one pooling, then another convolution and pooling). The operator \u03c8(\u03b8, a) applies \u03b8 parameters layer by layer to finally compute the activation of \u03b8 subnetwork given a. We will use the operator \u03c8(\u00b7, \u00b7) to define Model 1 and Model 2 structured fact image encoders.\nModel 1 (structured fact CNN image encoder): In Model 1, a structured fact is visually encoded by sharing Convolutional layer parameters (denoted by \u03b8vc ), and fully connected layer parameters (denoted by \u03b8uv ); see Fig. 4(a). Then W v S , W v P , and W v O transformation matrices are applied to produce \u03c6VS (fv),\u03c6 V P (fv) , and \u03c6 V O(fv):\n\u03c6VS (fv) = W S v \u03c8(\u03b8 u v , \u03c8(\u03b8 c v, fv)), \u03c6 V P (fv) = W P v \u03c8(\u03b8 u v , \u03c8(\u03b8 c v, fv)), \u03c6VO(fv) = W O v \u03c8(\u03b8 u v , \u03c8(\u03b8 c v, fv))\n(4)\nModel 2 (structured fact CNN image encoder): In contrast to Model 1, we use different convolutional layers for S that for P and O, inspired by the idea that P and O are modifiers to S (Fig. 4(b)). Starting from fv , there is a common set of convolutional layers, denoted by \u03b8c0v , then the network splits into two branches, producing two sets of convolutional layers \u03b8cSv and \u03b8 cPO v , followed by two sets of fully connected layers \u03b8uSv and \u03b8 uPO v . Finally \u03c6 V S (fv),\u03c6 V P (fv) , and \u03c6 V O(fv) are computed by applying W vS , W v P , and W v O transformation matrices:\n\u03c6VS (fv) = W S v \u03c8(\u03b8 uS v , \u03c8(\u03b8 cS v , \u03c8(\u03b8 v c0 , fv))), \u03c6 V P (fv) = W P v \u03c8(\u03b8 uPO v , \u03c8(\u03b8 cPO v , \u03c8(\u03b8 v c0 , fv))), \u03c6VO(fv) = W O v \u03c8(\u03b8 uPO v , \u03c8(\u03b8 cPO v , \u03c8(\u03b8 v c0 , fv)))\n(5)\nStructured fact RNN language encoder: Structured fact language view is encoded using RNN word embedding vectors for S, P and, O. Hence, in our case \u03c6LS(fl) = RNN\u03b8l(f S l ), \u03c6 L P (fl) = RNN\u03b8l(f P l ), \u03c6 L O(fl) = RNN\u03b8l(f O l ), where f S l , f P l , and f O l are the Subject, Predicate, and Object\nparts of fl \u2208 L. For each of them, the literals are dropped and if any of fSl , fPl , or fOl contain multiple words, we compute the average vector as the representation of that part. We denote the RNN language encoder parameters by \u03b8L. In our experiments, \u03b8l is fixed to a pretrained word vector embedding model (e.g. Mikolov et al. (2013); Pennington et al. (2014)) for fSl , f P l , and f O l ; see Fig 4(c).\nLoss function: One way to model p(fv, fl) for Model 1 and Model 2 is to assume that p(fv, fl) \u221d= exp(\u2212lossw(fv, fl)), and minimize lossw(fv, fl) distance loss, which we define as follows\nlossw(fv, fl) = wfS \u00b7 \u2016\u03c6VS (fv)\u2212 \u03c6LS(fl)\u20162 + wfP \u00b7 \u2016\u03c6VP (fv)\u2212 \u03c6LP (fl)\u20162 + wfO \u00b7 \u2016\u03c6VO(fv)\u2212 \u03c6LO(fl)\u20162 (6)\nwhich minimizes the distances between the embedding of the visual view and the language view. Our solution to penalize wild-card facts is to ignore the wild-card modifiers in the loss. HerewfS = 1, wfP = 1, w f O = 1 for <S,P,O> facts , w f S = 1, w f P = 1, w f O = 0 for <S,P> facts, and w f S = 1, wfP = 0, w f O = 0 for <S> facts. Hence lossw does not penalize the O modifier for second-order facts or the P andO modifiers for first-order facts, which follows our definition a wild-card modifier.\nTesting (Two-view retrieval): After a Sherlock Model is trained (either Model 1 or 2), we embed all the test fvs by \u03c6V(fv), and all the testing fls with \u03c6L(fl). For language view retrieval given an image, we compute the cosine similarity between a given \u03c6V(fv) in the test set and all the \u03c6L(fl), which indicates relevance for each fl for the given fv . For image retrieval given an fl, we compute the cosine similarity between the given \u03c6L(fl) with all \u03c6V(fv) in the test set, which indicates relevance for each fv for the given fl. For wild card facts, the wild-card part is ignored in the similarity."}, {"heading": "5 EXPERIMENTS", "text": ""}, {"heading": "5.1 SAFA EVALUATION", "text": "We start with our evaluation of the automatically collected annotation by the two-step SAFA process, where facts\u2019 language view are first extracted from the caption, then the facts are located in the image. We propose three questions to evaluate each annotation:\n(Q1) Is the extracted fact correct (Yes/No)? The purpose of this question is to evaluate errors captured by the first step, which extract facts by Sedona or Clausie.\n(Q2) Is the fact located in the image (Yes/No)? In some cases, there might be a fact mentioned in the caption that does not exist in the image and mistakenly considered as an annotation.\n(Q3) How accurate is the box assigned to a given fact (a to g)? a (about right), b (a bit big), c (a bit small), d (too small), e (too big), f (totally wrong box), g (fact does not exist or other). Instructions of these questions as we illustrated to the participants could be found in this url1\nWe evaluate these three question for the facts that were successfully assigned a box (i.e. grounded) in the image, since the main purpose of this evaluation is to measure the usability of the collected annotations as training data for our model. We created an Amazon Mechanical Turk form to ask these three questions to MTurk workers. So far, we collected a total of 8,535 evaluation responses, which are an evaluation of 2845 (fv, fl) pairs, each pair is evaluated by three workers each (3654, 2242, and 2639 responses for COCO train, COCO validation, and Flickr30K Entities respectively).\nTable 2 shows the evaluation results collected by MTurk workers so far. The results indicate that the collected data is useful for training, since 79.0% of the collected data are correct facts with bounding boxes that are either about right, a bit small or a bit big."}, {"heading": "5.2 KNOWLEDGE MODELING EXPERIMENTS", "text": "Since our Sherlock models is continuous on both language and visual views of facts, we can perform two way retrieval from the visual view given the language view and vice versa. We start by presenting evaluation metrics used in our small, medium scale, and large scale experiments for both view retrieval\nMetrics for visual view retrieval (retrieving fv given fl ): To retrieve an image ( visual view) given a language view like (e.g. <S: person, P: riding, O: horse>), we measure the performance by mAP (Mean Average Precision) and ROC AUC performance on the test set of each designated dataset in this section. An image fv is considered positive only if there is a pair (fl, fv in the annotations. Even if the retrieved image is relevant but such pair does not exist, it is considered not correct. We also use mAP10, mAP100 variants of the mAP metric that computes the mAP the evaluation based on only the top 10 or 100 retrieved images, which is useful for evaluating large scale experiments.\nMetrics for language view retrieval (retrieving fl given fv ): To retrieve fact language view given an image. we use top 1, top 5, top 10 accuracy for evaluation. We also used MRR (mean reciprocal ranking) metric which is basically 1/r where r is the rank of the correct class. An important issue with our setting is that there might be multiple facts in the same images. Given that there are L correct facts in the given image to achieve top 1 performance these L facts must all be in the top L retrieved facts. Accordingly, top K retrieved facts means the L facts are in the top L + K \u2212 1 retrieved facts. Similar to visual-view retrieval, fact language view fl is considered correct only if there is a pair (fl, fv in the annotations.\nEvaluation metrics for the Sherlock Problem and especially for scale of several tens of thousands unique facts and near a million image, which is the scale of our largest experiment. It is not hard to see that the aforementioned metrics are very harsh especially in the large scale setting. For instance, if the correct fact <S:man,P: jumping> in an image, and our model return <S:person, P:jumping>, these metrics gives a Sherlock model zero credit for this result. Also, the evaluation is limited to the ground truth fact annotations. There might be several facts in an image but the annotation is only provided one and miss the several others(e.g., an image with <S:man,P: walking>, and <S:man,P: wearing, O: hat>). While these these metrics were used for our quantitative evaluation, we qualitatively found it is harsh for our large scale experiment and we think defining metrics for the Sherlock problem setting is an interesting area to explore.\nStructured fact language Encoder: In all the following experiments, \u03b8l is defined as by GloVE840B RNN model Pennington et al. (2014), which is used for encoding structured fact in the language view for both Model 1 and Model 2 (i.e., \u03c6LS(fl), \u03c6 L P (fl), and \u03c6 L O(fl)). We used Caffe framework Jia et al. (2014) to implement our models.\nExperiments Overview: We performed small& medium experiments in Sec 5.2.1 an 5.2.2, and large scale experiments in Sec 5.2.3 to evaluate our work. In the small and the medium scale experiments, each fact language view fl has corresponding tens of visual views fv (i.e., images) where a subset is used for training and the other set is used for testing. So, each image we test on belong to a fact that was seen by other images in the training set. The purpose of these experiments is mainly to contrast against some existing methods with fixed dictionary of facts, and also to compare to with a recent version of the well-known CCA multiview method Gong et al. (2014) that could be also applied in our setting. In the large scale experiment, the collected data form is more than 816,000 (fv, fl) pairs, covering more than 202,000 unique facts in the language view fl. The training testing split is performed by randomly splitting all the pairs into 80% training pairs and 20% testing pairs. This results in 168,691 testing (fv, fl) pairs with 58,417 unique fl and at test time, where 31,677 out of them are unseen during training. We contrast Model 1 and 2 on small,medium and large scale experiments and show that Model 2 is better, as detailed later."}, {"heading": "5.2.1 EXPERIMENTS ON PASCAL ACTIONS EVERINGHAM ET AL.", "text": "The purpose of this experiment is to show that generative models like our proposed model can discriminate between classes without having a discrete label assigned. Also, we show how Model 1 compares to a recent version of CCA as a multiview method Gong et al. (2014). In particular, we applied four experiments on Pascal Actions dataset using CCA with two different features, CNN classification by fine-tuning AlexNet Krizhevsky et al. (2012) on Pascal Ac-\ntions, and our proposed Model 1. To explore the behavior of CCA on different visual view features for fv , we performed two CCA experiments using pool5 layer activation features from AlexNet Krizhevsky et al. (2012) for the first experiment and fc6 activation features for the second experiment from the same AlexNet. As language view features for fl, we applied \u03a6L(fl) using GloVE as language view features. Since CCA Gong et al. (2014), does not support wildcards, we fill the wild-card parts of \u03a6L(fl) with zeros. For Model 1 in this experiment, we constructed\nTable 3: Pascal Actions Experiments\nLanguage View retrieval% Visual View Retrieval\nTop 1 Acc Top 5 Acc MRR AP% AP10% AUC CCA- pool5 Gong et al. (2014) 31.34 68.4 48.84 11 24.5 0.53\nCCA-fc6 Gong et al. (2014) 20.99 70.2 41.12 13.0 24.1% 0.5619 Model 1 (AlexNet as image encoder) 66.52 93.38 78.26 69.47 94.2 0.9077\nTop 1 Acc Top 2 Acc Top 3 Acc CNN Classification ( Krizhevsky et al. (2012)) 68.7 83.26 89.28 - - -\nChance 10 - -\nin \u03b8cv as the first five convolutional layers in AlexNet Krizhevsky et al. (2012) and \u03b8uv as the two following fully connected layers fc6 and fc7. \u03b8cv and \u03b8 v u are initialized with an AlexNet model pretrained on the ImageNet dataset Deng et al. (2009). WSv , W P v , and W O v are initialized randomly.\nTable 3 shows the performance of CCA-pool5, CCA-fc6, and Model 1 for retrieval from both the language and the visual views. It is not hard to see that CCA-pool5 outperforms CCA-fc6 for language view retrieval by a margin and CCA-fc6 is slightly worse than CCA-pool5 for visualview retrieval. Both are clearly better than Chance perfomance for the 10 classes on Pascal12 which is 10%. We think this behavior is due that pool5 contains more spatial information compared to fc6 where spatial information is collapsed, and spatial information is important in recognizing activities. Comparing Model 1 to CCA results, Model 1 results significantly outperforms both CCA pool5 and fc6. Our intuition for this result is that Model 1 learns spatial convolution filters which is not available in CCA, and adapt it to discriminate between actions. Finally, Table 3 also shows that CNN classification(e.g., Krizhevsky et al. (2012)) performs only slightly better than Model 1. However, CNN classification is not applicable for the setting where facts are unseen, and does not support two-view retrieval. We also think that using a more discriminative loss (e.g., ranking loss) compared to the Euclidean loss, used in our experiments, would fill this small gap. Since there are\u2248 100 billion pairs in our large scale experiments (our goal), minimizing the ranking loss is not trivial for our setting. However, it is an interesting future work to further improve discriminative power of Sherlock Models."}, {"heading": "5.2.2 SMALL AND MID-SCALE EXPERIMENTS", "text": "We performed several experiments to compare between Model 1 and Model 2 on several datasets, which are Stanford40 Yao et al. (2011), Pascal Actions Yao & Fei-Fei (2010), Visual Phrases Sadeghi & Farhadi (2011), and the union of six datasets described earlier in Table 1 in Sec. 3. We used the training and test splits defined on the annotations that came of those datasets. For the union of six datasets, we unioned the training and testing annotations to get the final split.\nModel 1 and Model 2 setup: For Model 2, \u03b8c0v is constructed by the convolutional layers and pooling layer in VGG-16 named conv_1_1 until pool3 layer, which has seven convolution layers. \u03b8cSv and \u03b8 cPO v are two branches of convolution and pooling layers that have the same architectures as VGG-16 layers named conv_4_1 until pool5 layer which makes six convolution-pooling layers in each branch. Finally, \u03b8uSv and \u03b8 uPO v are constructed as two instances of fc6 and fc7 layers in VGG-16 network. While, the two branches of layers share same construction but they are optimized over different losses as detailed in Sec 4 and will be different when the model gets trained. In contrast to the experiment in the previous section, Model 1 is constructed here from VGG-16 by constructing \u03b8cv as the layer conv_1_1 to pool5, and \u03b8 u v as the two following fully connected layers fc6 and fc7 in VGG-16. WSv , W P v , and W O v in both Model 1 and 2 are initialized randomly, and the rest are initialized from VGG-16 trained on ImageNet dataset.\nTable 4 show the performance of both Model 1 and Model 2 on these four datasets for both-view retrieval tasks. We may notice that Model 2 works relatively better as the dataset size increases. The performances of Model 1 and 2 are very similar in small datasets like Pascal Actions. In the\n6DS experiment, we also performed the CNN classification but using VGG-Net for this experiment, which leads to the same conclusion we discussed in the previous experiment in Sec. 5.2.1.\nWhy Model 2 works better than Model 1? Our intuition behind this result is that Model 2 learns a different set of convolutional filters \u03b8cPOv to understand the PO branch as visual modifiers. This makes a separate bank of filters \u03b8cPOv to learn action/attributes and interaction related concepts, whch is different from the filter bank learnt to discriminate between different subjects for the S branch \u03b8cSv . In contrast, Model 1 is trained by optimizing the same bank of filters \u03b8cv for SPO altogether, which might contradicting to optimize for both S and PO together; see Fig 4."}, {"heading": "5.2.3 LARGE SCALE EXPERIMENT", "text": "In this experiment, we used all the data described in Sec. 3 including the automatically annotated data. This data consists mainly of second- and third-order facts. We further augmented this data with 2000 images for each MSCOCO object (80 classes) as first-order facts. We also used object annotations in sceneGraph dataset as first-order fact annotations with a maximum of 2000 images per object. We ignored the facts with spelling mistakes. Finally, we randomly split all the annotation into\n80%-20% split, constructing sets of 647,746 (fv, fl) training pairs (with 171,269 unique fact language views fl) and 168,691 (fv, fl) testing pairs (with 58,417 unique fl), for a total of (fv, fl) 816,436 pairs, 202,946 unique fl. Table 5 shows the coverage of different types of facts in the training and the test split and the intersection between them that there is a total of\n31,677 unique unseen facts out of the 58,417 testing facts in the language view. The majority of the facts have only one example; see Fig 5 and 6. Model 1 and Model 2 setup is the exactly the same as defined in Sec. 5.2.2.\nSince we perform retrieval in both directions, we computed all pairs of similarity between facts and images. In this setting, it is computationally expensive to compute the similarity between all pairs. We used KD-Trees databases for Approximate Nearest Neighbor (ANN) search. We used FLANN library to create the ANN databases Muja & Lowe (2009), and we restrict to compute the 100 nearest neighbors for fl given fv , and vice versa.\nTable 6 show the performance of Model 1 and Model 2. The results indicate that Model 2 is better than Model 1 for retrieval from both-view, which is consistent with our medium scale results and our intuition. Model 2 is also multiple orders of magnitude better than chance. Figure 7 and Figure 8 show two qualitative examples for both language view and image view retrieval; red facts in language view means it is not seen in the training data. Figure 7 (right) show <S:airplane, P: flying> example, where the retrieval results works well but the Average Precision performance metric give us zero score since none of these examples and annotated as <S: airplane, P:flying>, which indicates the need to design better metrics for Sherlock problem. One severe problem in the visual view\nretrieval metrics is that the majority of the testing fact language views have only one positive annotation. Figure 7 (left) shows that <S:dog,P:riding, O:wave> has the closest distance to the given image. However, <S:dog,P:riding, O:wave> is never seen in the training data. Figure 8 (right) shows several examples that shows hoe the trained model understand the difference between <man, eats, slice >, and <girl, eating, slice > (i.e., gender). It also understands what \u201cgroup\u201d means in the < S:group, P: covered, O: mountain> example. Our model is also able to retrieve facts like <girl, using, racket >, which is not seen in the training data."}, {"heading": "6 CONCLUSION", "text": "We introduce the Sherlock problem, the problem of associating high-order visual and language facts. We present a novel neural network approach for mapping visual facts and language facts into a common, continuous structured fact space that allows us to associate natural language facts with images and images with natural language structured descriptions. In future work, we plan to improve upon this model, and well as explore its applications toward high-precision image tagging and search, caption generation, and image knowledge abstraction."}], "references": [{"title": "Zero-shot learning via visual abstraction", "author": ["Antol", "Stanislaw", "Zitnick", "C Lawrence", "Parikh", "Devi"], "venue": "In ECCV", "citeRegEx": "Antol et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2014}, {"title": "Predicting deep zero-shot convolutional neural networks using textual descriptions", "author": ["Ba", "Jimmy", "Swersky", "Kevin", "Fidler", "Sanja", "Salakhutdinov", "Ruslan"], "venue": "In ICCV,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Inferring analogous attributes", "author": ["Chen", "Chao-Yeh", "Grauman", "Kristen"], "venue": "In CVPR,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Extracting visual knowledge from web data", "author": ["Chen", "Xinlei", "Shrivastava", "Ashish", "Gupta", "Arpan. Neil"], "venue": "In ICCV,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Clausie: clause-based open information extraction", "author": ["Del Corro", "Luciano", "Gemulla", "Rainer"], "venue": "In WWW,", "citeRegEx": "Corro et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Corro et al\\.", "year": 2013}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR. IEEE,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Learning everything about anything: Webly-supervised visual concept learning", "author": ["Divvala", "Santosh K", "Farhadi", "Alireza", "Guestrin", "Carlos"], "venue": "In CVPR,", "citeRegEx": "Divvala et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Divvala et al\\.", "year": 2014}, {"title": "Write a classifier: Zero-shot learning using purely textual descriptions", "author": ["Elhoseiny", "Mohamed", "Saleh", "Burhan", "Elgammal", "Ahmed"], "venue": "In ICCV,", "citeRegEx": "Elhoseiny et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elhoseiny et al\\.", "year": 2013}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Mikolov", "Tomas"], "venue": "In NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Finding action tubes", "author": ["Gkioxari", "Georgia", "Malik", "Jitendra"], "venue": "In CVPR,", "citeRegEx": "Gkioxari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gkioxari et al\\.", "year": 2015}, {"title": "A multi-view embedding space for modeling internet images, tags, and their semantics", "author": ["Gong", "Yunchao", "Ke", "Qifa", "Isard", "Michael", "Lazebnik", "Svetlana"], "venue": "International journal of computer vision,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Online; accessed 15-July-2015", "author": ["Gupta", "Abhinav"], "venue": "Sports Dataset. http://www.cs.cmu.edu/ \u0303abhinavg/Downloads. html,", "citeRegEx": "Gupta and Abhinav.,? \\Q2009\\E", "shortCiteRegEx": "Gupta and Abhinav.", "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "In ACM Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Image retrieval using scene graphs", "author": ["Johnson", "Justin", "Krishna", "Ranjay", "Stark", "Michael", "Li", "Li-Jia", "Shamma", "David", "Bernstein", "Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "In CVPR", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["Kulkarni", "Gaurav", "Premraj", "Visruth", "Ordonez", "Vicente", "Dhar", "Sudipta", "Li", "Siming", "Choi", "Yejin", "Berg", "Alexander C", "Tamara"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2013}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Lampert", "Christoph H", "Nickisch", "Hannes", "Harmeling", "Stefan"], "venue": "In CVPR,", "citeRegEx": "Lampert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Combining local context and wordnet similarity for word sense identification", "author": ["Leacock", "Claudia", "Chodorow", "Martin"], "venue": "WordNet: An electronic lexical database,", "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In ECCV. Springer,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Mao", "Junhua", "Xu", "Wei", "Yang", "Yi", "Wang", "Jiang", "Yuille", "Alan"], "venue": null, "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Flann-fast library for approximate nearest neighbors user manual", "author": ["Muja", "Marius", "Lowe", "David"], "venue": "Computer Science Department,", "citeRegEx": "Muja et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Muja et al\\.", "year": 2009}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Norouzi", "Mohammad", "Mikolov", "Tomas", "Bengio", "Samy", "Singer", "Yoram", "Shlens", "Jonathon", "Frome", "Andrea", "Corrado", "Greg S", "Dean", "Jeffrey"], "venue": "In ICLR,", "citeRegEx": "Norouzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models", "author": ["Plummer", "Bryan", "Wang", "Liwei", "Cervantes", "Chris", "Caicedo", "Juan", "Hockenmaier", "Julia", "Lazebnik", "Svetlana"], "venue": "In ICCV,", "citeRegEx": "Plummer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Recognition using visual phrases", "author": ["Sadeghi", "Mohammad Amin", "Farhadi", "Ali"], "venue": "In CVPR,", "citeRegEx": "Sadeghi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sadeghi et al\\.", "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Zero shot learning through cross-modal transfer", "author": ["Socher", "Richard", "Ganjoo", "Milind", "Sridhar", "Hamsa", "Bastani", "Osbert", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["Xiao", "Jianxiong", "Hays", "James", "Ehinger", "Krista", "Oliva", "Aude", "Torralba", "Antonio"], "venue": "In CVPR,", "citeRegEx": "Xiao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "In ICML", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Grouplet: A structured image representation for recognizing human and object interactions", "author": ["Yao", "Bangpeng", "Fei-Fei", "Li"], "venue": "In CVPR,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["Yao", "Bangpeng", "Jiang", "Xiaoye", "Khosla", "Aditya", "Lin", "Andy Lai", "Guibas", "Leonidas", "Fei-Fei", "Li"], "venue": "In ICCV,", "citeRegEx": "Yao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2011}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Young", "Peter", "Lai", "Alice", "Hodosh", "Micah", "Hockenmaier", "Julia"], "venue": "TACL, pp", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}, {"title": "Grounded language learning from video described with sentences", "author": ["Yu", "Haonan", "Siskind", "Jeffrey Mark"], "venue": "In ACL,", "citeRegEx": "Yu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2013}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhong", "Zhi", "Ng", "Hwee Tou"], "venue": "In ACL,", "citeRegEx": "Zhong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}, {"title": "Learning deep features for scene recognition using places database", "author": ["Zhou", "Bolei", "Lapedriza", "Agata", "Xiao", "Jianxiong", "Torralba", "Antonio", "Oliva", "Aude"], "venue": "In NIPS,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": ", Karpathy & Fei-Fei (2015); Vinyals et al. (2015); Xu et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 29, "context": ", Karpathy & Fei-Fei (2015); Vinyals et al. (2015); Xu et al. (2015); Mao et al.", "startOffset": 29, "endOffset": 69}, {"referenceID": 20, "context": "(2015); Mao et al. (2015)) rely on the idea of generating a sequence of words given an image of a scene, inspired by the success of sequence to sequence training of neural nets in machine translation systems (e.", "startOffset": 8, "endOffset": 26}, {"referenceID": 4, "context": ", Cho et al. (2014)).", "startOffset": 2, "endOffset": 20}, {"referenceID": 38, "context": ", Zhou et al. (2014)), and activity categorization (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 38, "context": ", Zhou et al. (2014)), and activity categorization (e.g., Gkioxari & Malik (2015)) methods is to have a set of discrete categories that the system can recognize.", "startOffset": 2, "endOffset": 82}, {"referenceID": 38, "context": ", Zhou et al. (2014)), and activity categorization (e.g., Gkioxari & Malik (2015)) methods is to have a set of discrete categories that the system can recognize. This faces a scalability problem since adding a new category means changing the architecture of the model by adding thousands or parameters and re-training the model (e.g., needed for adding a new output node). This is also shared with recent attribute based methods(e.g., Chen & Grauman (2014)) that realizes the idea that attribute appearance is dependent on the class as opposed to earlier models (e.", "startOffset": 2, "endOffset": 457}, {"referenceID": 18, "context": ", Lampert et al. (2009)), which motivates them to jointly learn a classifier for every class attribute pair suffering from scalability problems as the number of the classes and attributes grows.", "startOffset": 2, "endOffset": 24}, {"referenceID": 7, "context": "In Frome et al. (2013); Norouzi et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 7, "context": "In Frome et al. (2013); Norouzi et al. (2014) and Socher et al.", "startOffset": 3, "endOffset": 46}, {"referenceID": 7, "context": "In Frome et al. (2013); Norouzi et al. (2014) and Socher et al. (2013), word embedding language models (e.", "startOffset": 3, "endOffset": 71}, {"referenceID": 7, "context": "In Frome et al. (2013); Norouzi et al. (2014) and Socher et al. (2013), word embedding language models (e.g., Mikolov et al. (2013)) was adopted to represent class names as vectors.", "startOffset": 3, "endOffset": 132}, {"referenceID": 7, "context": "Other works models the opposite direction by mapping unstructured text descriptions for classes into a visual classifier Elhoseiny et al. (2013); Ba et al.", "startOffset": 121, "endOffset": 145}, {"referenceID": 1, "context": "(2013); Ba et al. (2015). In contrast to these works, our goal is to gain visual knowledge for not only first order facts (i.", "startOffset": 8, "endOffset": 25}, {"referenceID": 17, "context": "Kulkarni et al. (2013); Yu & Siskind (2013) however, they fall into the aforementioned scalability limitation.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "Kulkarni et al. (2013); Yu & Siskind (2013) however, they fall into the aforementioned scalability limitation.", "startOffset": 0, "endOffset": 44}, {"referenceID": 14, "context": "Very recently, Johnson et al. (2015) proposed an interesting work to retrieve images using scene graphs, which captures similar object attribute and relationship information to our representation.", "startOffset": 15, "endOffset": 37}, {"referenceID": 2, "context": "One of the most interesting relevant works is Never Ending Image Learner (NEIL) Chen et al. (2013), where they showed that visual concepts predefined in an ontology can be learnt by collecting its training data from the web.", "startOffset": 80, "endOffset": 99}, {"referenceID": 2, "context": "One of the most interesting relevant works is Never Ending Image Learner (NEIL) Chen et al. (2013), where they showed that visual concepts predefined in an ontology can be learnt by collecting its training data from the web. In a follow-up work, Divvala et al. (2014) similarly web-collected images for concepts related to a predefined object using Google-N-gram data.", "startOffset": 80, "endOffset": 268}, {"referenceID": 2, "context": "One of the most interesting relevant works is Never Ending Image Learner (NEIL) Chen et al. (2013), where they showed that visual concepts predefined in an ontology can be learnt by collecting its training data from the web. In a follow-up work, Divvala et al. (2014) similarly web-collected images for concepts related to a predefined object using Google-N-gram data. This opens the question of whether we can collect structured fact annotations from the web. There are two issue that we face for our setting. First, it is hard to define the space of structured visual knowledge and then search for it. Second, using Google image search is not reliable to collect data for concepts with less images in the web. The main assumption for this method depends on both the likelihood that the top retrieved image belongs to the searched concept, and the availability of image annotated with the searched concept. These problems motivated us to propose a novel method to automatically annotate structured facts by processing images-caption data. Our Sherlock Automatic Fact Annotation (SAFA) started by processing challenging unconstrained noisy captions to extract fact language view by multiple methods. Then, we grounded the facts extracted from captions to image regions as fact visual view. There are two advantages of collecting structured fact annotations by the proposed method. First, it assumes that structured facts in the captions is likely to be located in the given image, which is expected to occur in image-caption data with a very high probability compared to the assumptions in Chen et al. (2013); Divvala et al.", "startOffset": 80, "endOffset": 1609}, {"referenceID": 2, "context": "One of the most interesting relevant works is Never Ending Image Learner (NEIL) Chen et al. (2013), where they showed that visual concepts predefined in an ontology can be learnt by collecting its training data from the web. In a follow-up work, Divvala et al. (2014) similarly web-collected images for concepts related to a predefined object using Google-N-gram data. This opens the question of whether we can collect structured fact annotations from the web. There are two issue that we face for our setting. First, it is hard to define the space of structured visual knowledge and then search for it. Second, using Google image search is not reliable to collect data for concepts with less images in the web. The main assumption for this method depends on both the likelihood that the top retrieved image belongs to the searched concept, and the availability of image annotated with the searched concept. These problems motivated us to propose a novel method to automatically annotate structured facts by processing images-caption data. Our Sherlock Automatic Fact Annotation (SAFA) started by processing challenging unconstrained noisy captions to extract fact language view by multiple methods. Then, we grounded the facts extracted from captions to image regions as fact visual view. There are two advantages of collecting structured fact annotations by the proposed method. First, it assumes that structured facts in the captions is likely to be located in the given image, which is expected to occur in image-caption data with a very high probability compared to the assumptions in Chen et al. (2013); Divvala et al. (2014) and makes it likely to produce accurate annotations as we show in our results.", "startOffset": 80, "endOffset": 1632}, {"referenceID": 2, "context": "One of the most interesting relevant works is Never Ending Image Learner (NEIL) Chen et al. (2013), where they showed that visual concepts predefined in an ontology can be learnt by collecting its training data from the web. In a follow-up work, Divvala et al. (2014) similarly web-collected images for concepts related to a predefined object using Google-N-gram data. This opens the question of whether we can collect structured fact annotations from the web. There are two issue that we face for our setting. First, it is hard to define the space of structured visual knowledge and then search for it. Second, using Google image search is not reliable to collect data for concepts with less images in the web. The main assumption for this method depends on both the likelihood that the top retrieved image belongs to the searched concept, and the availability of image annotated with the searched concept. These problems motivated us to propose a novel method to automatically annotate structured facts by processing images-caption data. Our Sherlock Automatic Fact Annotation (SAFA) started by processing challenging unconstrained noisy captions to extract fact language view by multiple methods. Then, we grounded the facts extracted from captions to image regions as fact visual view. There are two advantages of collecting structured fact annotations by the proposed method. First, it assumes that structured facts in the captions is likely to be located in the given image, which is expected to occur in image-caption data with a very high probability compared to the assumptions in Chen et al. (2013); Divvala et al. (2014) and makes it likely to produce accurate annotations as we show in our results. Second, it could be used to collect knowledge very cheaply for several tasks. Our automatic annotations resulted in hundreds of thousands of images and tens of thousands of unique knowledge annotations, needed to feed our model, in just several hours with our current implementation whose speed is far from optimal The contributions of this paper are as follows: \u2022 We introduce the Sherlock problem of structured knowledge modeling in an image and propose one model that can learn structured facts of different types and perform both-view retrieval ( retrieve structured fact language view (i.e. <S:people, P:walking , >) given the visual view (i.e. image) and vice versa). \u2022 We propose an automatic stuctured fact annotation method based on sophisticated Natural Language Processing methods for acquiring high quality structured fact annotation pairs at large scale from free-form image descriptions. We applied the pipeline to MS COCO Lin et al. (2014) and Flickr30K Entities Plummer et al.", "startOffset": 80, "endOffset": 2666}, {"referenceID": 2, "context": "One of the most interesting relevant works is Never Ending Image Learner (NEIL) Chen et al. (2013), where they showed that visual concepts predefined in an ontology can be learnt by collecting its training data from the web. In a follow-up work, Divvala et al. (2014) similarly web-collected images for concepts related to a predefined object using Google-N-gram data. This opens the question of whether we can collect structured fact annotations from the web. There are two issue that we face for our setting. First, it is hard to define the space of structured visual knowledge and then search for it. Second, using Google image search is not reliable to collect data for concepts with less images in the web. The main assumption for this method depends on both the likelihood that the top retrieved image belongs to the searched concept, and the availability of image annotated with the searched concept. These problems motivated us to propose a novel method to automatically annotate structured facts by processing images-caption data. Our Sherlock Automatic Fact Annotation (SAFA) started by processing challenging unconstrained noisy captions to extract fact language view by multiple methods. Then, we grounded the facts extracted from captions to image regions as fact visual view. There are two advantages of collecting structured fact annotations by the proposed method. First, it assumes that structured facts in the captions is likely to be located in the given image, which is expected to occur in image-caption data with a very high probability compared to the assumptions in Chen et al. (2013); Divvala et al. (2014) and makes it likely to produce accurate annotations as we show in our results. Second, it could be used to collect knowledge very cheaply for several tasks. Our automatic annotations resulted in hundreds of thousands of images and tens of thousands of unique knowledge annotations, needed to feed our model, in just several hours with our current implementation whose speed is far from optimal The contributions of this paper are as follows: \u2022 We introduce the Sherlock problem of structured knowledge modeling in an image and propose one model that can learn structured facts of different types and perform both-view retrieval ( retrieve structured fact language view (i.e. <S:people, P:walking , >) given the visual view (i.e. image) and vice versa). \u2022 We propose an automatic stuctured fact annotation method based on sophisticated Natural Language Processing methods for acquiring high quality structured fact annotation pairs at large scale from free-form image descriptions. We applied the pipeline to MS COCO Lin et al. (2014) and Flickr30K Entities Plummer et al. (2015); Young et al.", "startOffset": 80, "endOffset": 2711}, {"referenceID": 2, "context": "One of the most interesting relevant works is Never Ending Image Learner (NEIL) Chen et al. (2013), where they showed that visual concepts predefined in an ontology can be learnt by collecting its training data from the web. In a follow-up work, Divvala et al. (2014) similarly web-collected images for concepts related to a predefined object using Google-N-gram data. This opens the question of whether we can collect structured fact annotations from the web. There are two issue that we face for our setting. First, it is hard to define the space of structured visual knowledge and then search for it. Second, using Google image search is not reliable to collect data for concepts with less images in the web. The main assumption for this method depends on both the likelihood that the top retrieved image belongs to the searched concept, and the availability of image annotated with the searched concept. These problems motivated us to propose a novel method to automatically annotate structured facts by processing images-caption data. Our Sherlock Automatic Fact Annotation (SAFA) started by processing challenging unconstrained noisy captions to extract fact language view by multiple methods. Then, we grounded the facts extracted from captions to image regions as fact visual view. There are two advantages of collecting structured fact annotations by the proposed method. First, it assumes that structured facts in the captions is likely to be located in the given image, which is expected to occur in image-caption data with a very high probability compared to the assumptions in Chen et al. (2013); Divvala et al. (2014) and makes it likely to produce accurate annotations as we show in our results. Second, it could be used to collect knowledge very cheaply for several tasks. Our automatic annotations resulted in hundreds of thousands of images and tens of thousands of unique knowledge annotations, needed to feed our model, in just several hours with our current implementation whose speed is far from optimal The contributions of this paper are as follows: \u2022 We introduce the Sherlock problem of structured knowledge modeling in an image and propose one model that can learn structured facts of different types and perform both-view retrieval ( retrieve structured fact language view (i.e. <S:people, P:walking , >) given the visual view (i.e. image) and vice versa). \u2022 We propose an automatic stuctured fact annotation method based on sophisticated Natural Language Processing methods for acquiring high quality structured fact annotation pairs at large scale from free-form image descriptions. We applied the pipeline to MS COCO Lin et al. (2014) and Flickr30K Entities Plummer et al. (2015); Young et al. (2014) image caption datasets.", "startOffset": 80, "endOffset": 2732}, {"referenceID": 33, "context": ", fl):PPMI Yao & Fei-Fei (2010), Stanford40 Yao et al. (2011), Pascal Actions Everingham et al.", "startOffset": 44, "endOffset": 62}, {"referenceID": 33, "context": ", fl):PPMI Yao & Fei-Fei (2010), Stanford40 Yao et al. (2011), Pascal Actions Everingham et al., Sports Gupta (2009), Visual Phrases Sadeghi & Farhadi (2011), INTERACT Antol et al.", "startOffset": 44, "endOffset": 117}, {"referenceID": 33, "context": ", fl):PPMI Yao & Fei-Fei (2010), Stanford40 Yao et al. (2011), Pascal Actions Everingham et al., Sports Gupta (2009), Visual Phrases Sadeghi & Farhadi (2011), INTERACT Antol et al.", "startOffset": 44, "endOffset": 158}, {"referenceID": 0, "context": ", Sports Gupta (2009), Visual Phrases Sadeghi & Farhadi (2011), INTERACT Antol et al. (2014) datasets.", "startOffset": 73, "endOffset": 93}, {"referenceID": 14, "context": "We also extracted structured facts from the Scene Graph dataset Johnson et al. (2015) which has manually annotated 5000 images by MTurkers in a graph structure from which first-, second-, and third-order relationship can be easily extracted.", "startOffset": 64, "endOffset": 86}, {"referenceID": 20, "context": "MS COCO Lin et al. (2014)) and more data can be efficiently collected in the future for", "startOffset": 8, "endOffset": 26}, {"referenceID": 20, "context": "In our work, we focused on the MS COCO image caption dataset Lin et al. (2014) and the newly collected Flickr30K entities Plummer et al.", "startOffset": 61, "endOffset": 79}, {"referenceID": 20, "context": "In our work, we focused on the MS COCO image caption dataset Lin et al. (2014) and the newly collected Flickr30K entities Plummer et al. (2015) to both collect and automatically generate new structured fact annotations.", "startOffset": 61, "endOffset": 144}, {"referenceID": 20, "context": "In case of the MS COCO dataset Lin et al. (2014), we define a visual entity as any noun that is either (1) one of Microsoft COCO dataset objects, (2) a noun in the WordNet ontology Miller (1995); Leacock & Chodorow (1998) that is an immediate or indirect hyponym of one of the MS COCO objects.", "startOffset": 31, "endOffset": 49}, {"referenceID": 20, "context": "In case of the MS COCO dataset Lin et al. (2014), we define a visual entity as any noun that is either (1) one of Microsoft COCO dataset objects, (2) a noun in the WordNet ontology Miller (1995); Leacock & Chodorow (1998) that is an immediate or indirect hyponym of one of the MS COCO objects.", "startOffset": 31, "endOffset": 195}, {"referenceID": 20, "context": "In case of the MS COCO dataset Lin et al. (2014), we define a visual entity as any noun that is either (1) one of Microsoft COCO dataset objects, (2) a noun in the WordNet ontology Miller (1995); Leacock & Chodorow (1998) that is an immediate or indirect hyponym of one of the MS COCO objects.", "startOffset": 31, "endOffset": 222}, {"referenceID": 20, "context": "In case of the MS COCO dataset Lin et al. (2014), we define a visual entity as any noun that is either (1) one of Microsoft COCO dataset objects, (2) a noun in the WordNet ontology Miller (1995); Leacock & Chodorow (1998) that is an immediate or indirect hyponym of one of the MS COCO objects. Since, wordNet is searchable by a sense not a word, we performed word sense disambiguation on the sentences using a State-of-the-art method Zhong & Ng (2010), or (3) one of the SUN dataset scenes Xiao et al.", "startOffset": 31, "endOffset": 452}, {"referenceID": 20, "context": "In case of the MS COCO dataset Lin et al. (2014), we define a visual entity as any noun that is either (1) one of Microsoft COCO dataset objects, (2) a noun in the WordNet ontology Miller (1995); Leacock & Chodorow (1998) that is an immediate or indirect hyponym of one of the MS COCO objects. Since, wordNet is searchable by a sense not a word, we performed word sense disambiguation on the sentences using a State-of-the-art method Zhong & Ng (2010), or (3) one of the SUN dataset scenes Xiao et al. (2010). We expect visual entities to be appear either in the S or the O part (if it exists) of a candidate fact fl.", "startOffset": 31, "endOffset": 509}, {"referenceID": 27, "context": "In Flickr30K Entities dataset Plummer et al. (2015), the bounding box annotations are presented as phrase labels for sentences (for each phrase in a caption that refers to an entity in the scene).", "startOffset": 30, "endOffset": 52}, {"referenceID": 16, "context": ", Krizhevsky et al. (2012); Simonyan & Zisserman (2015)), and \u03c6(fl) as RNN encoder (e.", "startOffset": 2, "endOffset": 27}, {"referenceID": 16, "context": ", Krizhevsky et al. (2012); Simonyan & Zisserman (2015)), and \u03c6(fl) as RNN encoder (e.", "startOffset": 2, "endOffset": 56}, {"referenceID": 16, "context": ", Krizhevsky et al. (2012); Simonyan & Zisserman (2015)), and \u03c6(fl) as RNN encoder (e.g., Mikolov et al. (2013); Pennington et al.", "startOffset": 2, "endOffset": 112}, {"referenceID": 16, "context": ", Krizhevsky et al. (2012); Simonyan & Zisserman (2015)), and \u03c6(fl) as RNN encoder (e.g., Mikolov et al. (2013); Pennington et al. (2014)), due to their recent success as encoders for images and words, respectively.", "startOffset": 2, "endOffset": 138}, {"referenceID": 22, "context": "Mikolov et al. (2013); Pennington et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "Mikolov et al. (2013); Pennington et al. (2014)) for f l , f P l , and f O l ; see Fig 4(c).", "startOffset": 0, "endOffset": 48}, {"referenceID": 25, "context": "Structured fact language Encoder: In all the following experiments, \u03b8l is defined as by GloVE840B RNN model Pennington et al. (2014), which is used for encoding structured fact in the language view for both Model 1 and Model 2 (i.", "startOffset": 108, "endOffset": 133}, {"referenceID": 13, "context": "We used Caffe framework Jia et al. (2014) to implement our models.", "startOffset": 24, "endOffset": 42}, {"referenceID": 11, "context": "The purpose of these experiments is mainly to contrast against some existing methods with fixed dictionary of facts, and also to compare to with a recent version of the well-known CCA multiview method Gong et al. (2014) that could be also applied in our setting.", "startOffset": 201, "endOffset": 220}, {"referenceID": 11, "context": "Also, we show how Model 1 compares to a recent version of CCA as a multiview method Gong et al. (2014). In particular, we applied four experiments on Pascal Actions dataset using CCA with two different features, CNN classification by fine-tuning AlexNet Krizhevsky et al.", "startOffset": 84, "endOffset": 103}, {"referenceID": 11, "context": "Also, we show how Model 1 compares to a recent version of CCA as a multiview method Gong et al. (2014). In particular, we applied four experiments on Pascal Actions dataset using CCA with two different features, CNN classification by fine-tuning AlexNet Krizhevsky et al. (2012) on Pascal Ac-", "startOffset": 84, "endOffset": 279}, {"referenceID": 15, "context": "To explore the behavior of CCA on different visual view features for fv , we performed two CCA experiments using pool5 layer activation features from AlexNet Krizhevsky et al. (2012) for the first experiment and fc6 activation features for the second experiment from the same AlexNet.", "startOffset": 158, "endOffset": 183}, {"referenceID": 11, "context": "Since CCA Gong et al. (2014), does not support wildcards, we fill the wild-card parts of \u03a6(fl) with zeros.", "startOffset": 10, "endOffset": 29}, {"referenceID": 10, "context": "Language View retrieval% Visual View Retrieval Top 1 Acc Top 5 Acc MRR AP% AP10% AUC CCA- pool5 Gong et al. (2014) 31.", "startOffset": 96, "endOffset": 115}, {"referenceID": 10, "context": "Language View retrieval% Visual View Retrieval Top 1 Acc Top 5 Acc MRR AP% AP10% AUC CCA- pool5 Gong et al. (2014) 31.34 68.4 48.84 11 24.5 0.53 CCA-fc6 Gong et al. (2014) 20.", "startOffset": 96, "endOffset": 172}, {"referenceID": 10, "context": "Language View retrieval% Visual View Retrieval Top 1 Acc Top 5 Acc MRR AP% AP10% AUC CCA- pool5 Gong et al. (2014) 31.34 68.4 48.84 11 24.5 0.53 CCA-fc6 Gong et al. (2014) 20.99 70.2 41.12 13.0 24.1% 0.5619 Model 1 (AlexNet as image encoder) 66.52 93.38 78.26 69.47 94.2 0.9077 Top 1 Acc Top 2 Acc Top 3 Acc CNN Classification ( Krizhevsky et al. (2012)) 68.", "startOffset": 96, "endOffset": 354}, {"referenceID": 10, "context": "Language View retrieval% Visual View Retrieval Top 1 Acc Top 5 Acc MRR AP% AP10% AUC CCA- pool5 Gong et al. (2014) 31.34 68.4 48.84 11 24.5 0.53 CCA-fc6 Gong et al. (2014) 20.99 70.2 41.12 13.0 24.1% 0.5619 Model 1 (AlexNet as image encoder) 66.52 93.38 78.26 69.47 94.2 0.9077 Top 1 Acc Top 2 Acc Top 3 Acc CNN Classification ( Krizhevsky et al. (2012)) 68.7 83.26 89.28 - - Chance 10 - in \u03b8 v as the first five convolutional layers in AlexNet Krizhevsky et al. (2012) and \u03b8 v as the two following fully connected layers fc6 and fc7.", "startOffset": 96, "endOffset": 470}, {"referenceID": 6, "context": "\u03b8 v and \u03b8 v u are initialized with an AlexNet model pretrained on the ImageNet dataset Deng et al. (2009). W v , W P v , and W O v are initialized randomly.", "startOffset": 87, "endOffset": 106}, {"referenceID": 6, "context": "\u03b8 v and \u03b8 v u are initialized with an AlexNet model pretrained on the ImageNet dataset Deng et al. (2009). W v , W P v , and W O v are initialized randomly. Table 3 shows the performance of CCA-pool5, CCA-fc6, and Model 1 for retrieval from both the language and the visual views. It is not hard to see that CCA-pool5 outperforms CCA-fc6 for language view retrieval by a margin and CCA-fc6 is slightly worse than CCA-pool5 for visualview retrieval. Both are clearly better than Chance perfomance for the 10 classes on Pascal12 which is 10%. We think this behavior is due that pool5 contains more spatial information compared to fc6 where spatial information is collapsed, and spatial information is important in recognizing activities. Comparing Model 1 to CCA results, Model 1 results significantly outperforms both CCA pool5 and fc6. Our intuition for this result is that Model 1 learns spatial convolution filters which is not available in CCA, and adapt it to discriminate between actions. Finally, Table 3 also shows that CNN classification(e.g., Krizhevsky et al. (2012)) performs only slightly better than Model 1.", "startOffset": 87, "endOffset": 1077}, {"referenceID": 34, "context": "We performed several experiments to compare between Model 1 and Model 2 on several datasets, which are Stanford40 Yao et al. (2011), Pascal Actions Yao & Fei-Fei (2010), Visual Phrases Sadeghi & Farhadi (2011), and the union of six datasets described earlier in Table 1 in Sec.", "startOffset": 114, "endOffset": 132}, {"referenceID": 34, "context": "We performed several experiments to compare between Model 1 and Model 2 on several datasets, which are Stanford40 Yao et al. (2011), Pascal Actions Yao & Fei-Fei (2010), Visual Phrases Sadeghi & Farhadi (2011), and the union of six datasets described earlier in Table 1 in Sec.", "startOffset": 114, "endOffset": 169}, {"referenceID": 34, "context": "We performed several experiments to compare between Model 1 and Model 2 on several datasets, which are Stanford40 Yao et al. (2011), Pascal Actions Yao & Fei-Fei (2010), Visual Phrases Sadeghi & Farhadi (2011), and the union of six datasets described earlier in Table 1 in Sec.", "startOffset": 114, "endOffset": 210}], "year": 2017, "abstractText": "How to build a machine learning method that can continuously gain structured visual knowledge by learning structured facts? Our goal in this paper is to address this question by proposing a problem setting, where training data comes as structured facts in images with different types including (1) objects(e.g., <boy>), (2) attributes (e.g., <boy,tall>), (3) actions (e.g., <boy, playing>), (4) interactions (e.g., <boy, riding, a horse >). Each structured fact has a semantic language view (e.g., < boy, playing>) and a visual view (an image with this fact). A human is able to efficiently gain visual knowledge by learning facts in a never ending process, and as we believe in a structured way (e.g., understanding \u201cplaying\u201d is the action part of < boy, playing>, and hence can generalize to recognize <girl, playing > if just learn <girl> additionally). Inspired by human visual perception, we propose a model that is (1) able to learn a representation, we name as wild-card, which covers different types of structured facts, (2) could flexibly get fed with structured fact language-visual view pairs in a never ending way to gain more structured knowledge, (3) could generalize to unseen facts, and (4) allows retrieval of both the fact language view given the visual view (i.e., image) and vice versa. We also propose a novel method to generate hundreds of thousands of structured fact pairs from image caption data, which are necessary to train our model and can be useful for other applications.", "creator": "LaTeX with hyperref package"}}}