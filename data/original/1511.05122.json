{"id": "1511.05122", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Adversarial Manipulation of Deep Representations", "abstract": "We show that the internal representations of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image manipulation designed to produce erroneous class labels, while here we look at internal layers of the representation. Despite the similarities in the generation process, our class of adversarial images differs qualitatively from previous ones. Specifically, while the input image is perceptually similar to an image of one class, its internal representation appear remarkably like that of natural images in a different class. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves.", "histories": [["v1", "Mon, 16 Nov 2015 20:48:20 GMT  (4769kb,D)", "http://arxiv.org/abs/1511.05122v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Thu, 19 Nov 2015 21:00:44 GMT  (5272kb,D)", "http://arxiv.org/abs/1511.05122v2", "Under review as a conference paper at ICLR 2016"], ["v3", "Mon, 23 Nov 2015 20:56:44 GMT  (5273kb,D)", "http://arxiv.org/abs/1511.05122v3", "Under review as a conference paper at ICLR 2016"], ["v4", "Fri, 11 Dec 2015 21:03:14 GMT  (8849kb,D)", "http://arxiv.org/abs/1511.05122v4", "Under review as a conference paper at ICLR 2016"], ["v5", "Thu, 7 Jan 2016 20:59:55 GMT  (9280kb,D)", "http://arxiv.org/abs/1511.05122v5", "Under review as a conference paper at ICLR 2016"], ["v6", "Tue, 12 Jan 2016 20:51:51 GMT  (9213kb,D)", "http://arxiv.org/abs/1511.05122v6", "Under review as a conference paper at ICLR 2016"], ["v7", "Wed, 13 Jan 2016 20:57:33 GMT  (9463kb,D)", "http://arxiv.org/abs/1511.05122v7", "Under review as a conference paper at ICLR 2016"], ["v8", "Tue, 1 Mar 2016 20:51:06 GMT  (9461kb,D)", "http://arxiv.org/abs/1511.05122v8", "Accepted as a conference paper at ICLR 2016"], ["v9", "Fri, 4 Mar 2016 20:21:24 GMT  (9461kb,D)", "http://arxiv.org/abs/1511.05122v9", "Accepted as a conference paper at ICLR 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["sara sabour", "yanshuai cao", "fartash faghri", "david j fleet"], "accepted": true, "id": "1511.05122"}, "pdf": {"name": "1511.05122.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J. Fleet"], "emails": ["saaraa@cs.toronto.edu", "fleet@cs.toronto.edu", "ycao@architech.ca", "ffaghri@architech.ca"], "sections": [{"heading": null, "text": "We show that the internal representations of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image manipulation designed to produce erroneous class labels, while here we look at internal layers of the representation. Despite the similarities in the generation process, our class of adversarial images differs qualitatively from previous ones. Specifically, while the input image is perceptually similar to an image of one class, its internal representation appear remarkably like that of natural images in a different class. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves."}, {"heading": "1 INTRODUCTION", "text": "Recent papers have shown that deep neural networks (DNNs) for image classification can be circumvented, often in relatively simple ways (Fawzi et al., 2015; Goodfellow et al., 2014; Gu & Rigazio, 2014; Nguyen et al., 2015; Szegedy et al., 2014b; Tabacof & Valle, 2015). One category of such adversarial images disrupts DNN classification of given images (sources) even though the adversarials differ almost imperceptibly from those sources (Goodfellow et al., 2014; Szegedy et al., 2014b). Moreover they do so with high confidence, both with DNNs and other models. Such phenomena are important, not just because they reveal weaknesses in learned representations and classifiers, but because 1) they provide opportunities to explore fundamental questions about the nature of learned representations, e.g., whether they are inherent in the problem or the underlying model, and 2) such adversarial images might be harnessed to guide the development of new learning algorithms that exhibit improved robustness and better generalization (Goodfellow et al., 2014; Gu & Rigazio, 2014).\nThe research on adversarial images to date has focused on disrupting classification, i.e., images classified with labels that are patently inconsistent with human perception. In this paper we rather focus directly on properties of internal representations learned by deep networks. We introduce a somewhat more insidious category of adversarial images that are confused with other images not just in the DNN class label, but in the internal representation.\nSpecifically, given a source image, a target image, and a trained DNN, we find small perturbations to the source image that cause the internal representation on a specified layer (or above) to be remarkably similar to that of the target (guide) image, and hence far from that of the source. We further show that the internal representation of such an adversarial image is not an outlier per se. Unlike previous categories of adversarial images, which we show to have properties to be considered as outliers.\n\u2217First two authors contributed equally.\nar X\niv :1\n51 1.\n05 12\n2v 1\n[ cs\n.C V\n] 1\n6 N\nov 2\n01 5"}, {"heading": "2 RELATED WORK", "text": "There are two main approaches to construct adversarial images that disrupt DNN classification. These approaches differ fundamentally in whether they reveal the behavior of DNNs in the neighborhood of natural images, or the adversarial images themselves are overtly unnatural.\nIn the latter case, Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are classified by DNNs as common objects with high confidence (often 99%). While interesting, such adversarial images are totally different from the training data, or natural images per se. Because natural images only occupy a remarkably small volume of the space of all possible images, it is perhaps not surprising that discriminative DNNs trained on natural images have trouble coping with such out-of-sample data. In our approach, similar to Szegedy et al. (2014b), we focus on adversarial images that appear natural.\nTo construct an adversarial image, Szegedy et al. (2014b) use gradient-based optimization on the classification loss with respect to the image perturbation, , with a penalty on the magnitude of . Given an image I , a DNN classifier f , and an erroneous label l, they find the perturbation that minimizes loss(f(I + ), l) + c\u2016 \u20162. Here, c is chosen by line-search to find the smallest that achieves f(I + ) = l. The authors argue that the resulting adversarial images occupy low probability \u201cpockets\u201d in the manifold, which act like \u201cblind spots\u201d to the DNN. The adversarial construction in our paper extends the approach of Szegedy et al. (2014b). In Sec. 3, we use gradientbased optimization to find small image perturbations. But instead of inducing misclassification, we induce dramatic changes in the internal DNN representation.\nLater work by Goodfellow et al. (2014) shows that adversarial images are more common, and can be found by taking steps in the direction of the gradient of loss(f(I + ), l). Goodfellow et al. (2014) also show that adversarial examples exist for many other models, including linear classifiers. They argue that the problem arises when models are \u201ctoo linear\u201d rather than \u201ctoo nonlinear\u201d. Fawzi et al. (2015) later propose a more general framework to explain adversarial images, formalizing the intuition that the problem occurs when DNNs and other models are not sufficiently \u201cflexible\u201d for the given classification task.\nIn Sec. 4, we show that our new adversarial images exhibit qualitatively different properties from other categories of adversarial examples. In particular, the internal DNN representation of our adversarial images is close to specified neighborhoods of natural images and does not look unnatural with respect to various measures."}, {"heading": "3 ADVERSARIAL IMAGE GENERATION", "text": "Let Is and Ig denote two images called the source and guide. Let \u03c6k be the mapping from an image to the internal representation at layer k of a DNN. Our goal is to find a new image, I\u03b1, such that the Euclidian Distance between \u03c6k(I\u03b1) and \u03c6k(Ig) be as small as possible, and I\u03b1 remains close to the source Is. More precisely, I\u03b1 is defined to be the solution to the following constrained optimization problem:\nI\u03b1 = arg min I \u2016\u03c6k(I)\u2212 \u03c6k(Ig) \u201622 (1)\nsubject to \u2016I \u2212 Is\u2016\u221e < \u03b4 (2)\nThe constraint on the distance between I\u03b1 and Is is formulated in terms of the L\u221e norm to limit the maximum deviation of any pixel between I\u03b1 and Is to be less than \u03b4. The goal is to constrain the degree to which the perturbation is perceptible. While the L\u221e norm is not the best available measure of human visual discriminability (e.g., see Wang et al. (2004)), it is superior to the L2 norm.\nRather than searching over choices of \u03b4 for each image, we find that a fixed value of \u03b4 = 10 intensity levels (out of 255) usually produces negligible perceptual distortion on different layers, networks and datasets. This simplifies adversarial generation and simplifies analysis. We only set \u03b4 larger when optimizing lower network layers close to the input. Finally, we implement the optimization using l-BFGS-b, with the inequality (2) expressed as a box constraint around Is.\nWe use \u03b1kij to denote the DNN representation at layer k, from the adversarial image built from source i and guide j. For notational convenience, whenever clear, we drop the indices to use \u03b1 to denote\nadversarial image representation. Similarly, we use s and g to denote source, guide representations respectively.\nFigure 1 shows some adversarial images from this generation process, using the the well-known BVLC Caffe Reference model (Caffenet) (Jia et al., 2014). Each row shows the source, the guide, and adversarial images, along with the differences between them and the source. The adversarial cases shown used different perturbation bounds (\u03b4), and optimization at different layers of Caffenet, namely FC7 (fully connected level 7), P5 (pooling layer 5), and C3 (convolution layer 3). Inspecting the adversarial images one can see that larger values of \u03b4 allow more noticeable perturbations. That said, we have found no natural images in which the guide image perceptible in the adversarial image. Nor do we see a significant amount of salient structure in difference images.\nWhile the class label was not an explicit factor in the optimization, we find that the class labels assigned to the adversarial images are almost always that of the guide. We take 100 random pair of images from Imagenet (Deng et al., 2009) used to train Caffenet, apply optimization at FC7 with delta = 10, and we find that the class label assigned to the adversarial image is never equal that of the source image, and in 95% of cases matches the guide class. This is true whether the source images are from the training, validation, or testing sets of images.\nWe also find similar results on different networks and datasets. In addition to Caffenet, we apply our approach to AlexNet (Krizhevsky et al., 2012), GoogleNet (Szegedy et al., 2014a; Wu et al.), and VGG CNN-S (Chatfield et al., 2014), all trained on the the Imagenet ILSVRC dataset (Deng et al., 2009). We also used AlexNet trained on the Places205 dataset, and on a hybrid dataset comprising 205 scene classes and 977 classes from ImageNet (Zhou et al., 2014). In all these cases, using 100 random source-guide pairs we observe similar behaviour. Class labels assigned to adversarial images do not match the source. Rather, in 97% to 100% of all cases the predicted class label is that of the guide.\nSimilar to other approaches for generating adversarial images, we find that adversarials generated on one network are similarly misclassified in other networks. Using the same 100 source-guide pairs on each of the above models we find that, on average, 54% of adversarial images obtained from one network are misclassified by the other networks as well. However, they are not usually classified with the label of the guide when tested on one of the other networks.\nWe next turn to consider internal representations \u2013 do they resemble those of the source, the guide, or some combination of the two? One way to probe the internal representations, following Mahendran & Vedaldi (2014), is to invert the mapping, allowing us to display the image reconstructed from the internal representation. The two panels in Fig. 2 show the results with two source-guide pairs. Each Input row consists of the source (left), guide (right) and adervarisal images constructed from layers FC7, P5 and C3 of Caffenet (middle). The subsequent rows show images reconstructed from the internal representations of these five images, again from layers C2, P5 and FC7. Fig. 2 shows that while the lower layer representations bear similarity to the source, the upper layers are remarkably similar to the guide. When optimized using C3, the reconstructed image from C3 shows a mixture of source and guide. Generally we find that the internal representations begin to mimic the guide at\nthe layer that was targeted in the optimization. Not only human and DNN perception of an image could be totally in odd, but also the adversed representations are not out of sample and could be naturally similar to a specific targeted representation."}, {"heading": "4 EXPERIMENTAL EVALUATION", "text": "We investigate the quality of an adversarial image by asking two questions. First, does the representation of the adversarial look like its guide image? Second, does the representation of the adversarial look like a natural image? To answer these questions quantitatively, we use several measures of similarity described below in order to show that the adversarial images we obtain have tested properties of natural image representations. They do not appear to be outliers from the training corpus in any significant way, as judged by the internal representations.\nTo this end we focus on the Caffenet benchmark , and random sets of source and guide images. Both the source and guide sets consist of images from training, testing and validation sets of Imagenet. To facilitate quantitive analysis, we only selected those images as guides whose labels are correctly predicted by caffenet from training and validation. In our experimental setup, we generate an advesarial image from every combination of seed and guide images.\nAlthough the main focus of the experiments is on Caffenet, further experiments in Sec. 4.1 report results on other well-known convolutional neural networks as well."}, {"heading": "4.1 SIMILARITY TO THE GUIDE REPRESENTATION", "text": "Given any image as a guide we show that the generated adversarial representation looks similar to the guide. We demonstrate that the representation is not only far from the source and close to the guide\u2019s representation but also the neighborhood around the adversarial is almost exactly the same as the guide. First, we evaluate the distance of the adversarial to the source and the guide relative to various prototypical and interpretable distances in the feature space. Then, by evaluating average distnances to top near neighbors as well as intersection of top near neighbor sets, we provide additional proof that adversarials are close to their guides.\nFirst, we examine that a small bounded perturbation in the image space leads to a relatively large change directed to a guide\u2019s representation. We report results of 20 source images. In this set 5 images were selected at random from each of the training, testing and validation sets of Imagenet. Five more images were selected manually from Wikipedia and the Imagenet validation set for greater diversity.\nGuides for all experiments on layer FC7 were done using 3 random images from all 1000 classes, along with 30 from the test and validation set each. For the remaining experiments we used images from random 100 classes for expedience.\nResults in Fig. 3(b) show that small perturbation (\u03b4 = 5) of source image can move the representation on average 79% closer to the guide FC7 representation. The same phenomena for Pool5 is shown in Fig. 3(a) and is observed to be 60% with \u03b4 = 10 which is still significant.\nLater, we evaluate whether the adversarial distance to guide is close enough to be a typical of the training images of the guide class. To that end, Fig. 3(c) show histogram of FC7 distances between adversarial images and their guides, relative to the average FC7 distance between all other images in the guide class and their respective NNs. The adversarial images are on average 78% closer to their guides than are other images to their NNs from the guide class. Also, Figure 3(d) shows that the distance to source is not a typical pairwise distance of its class anymore and only 8% of the adversarials with \u03b4 = 10 have ratio smaller than 1 which could be due to the closeness between the class clusters themselves.\nBeside the Euclidean metric, we use two scores capturing neighbourhood properties to investigate how close the adversarials are to their guides. The first is neighbourhood intersection: If two points share most of all their topK nearest neighbors (NN), then it is an indication that the two points could be close. Along with the NN intersections, we compute the average distance to top K NN. Taking\nthis average distance as scalar score of a point itself, we rank the points in their class so that the unit of distance is removed and comparison made easy. We denote this rank of a point x as rK(x).\nIn our experiments, we use K = 3. Since our adversarials are by construction close to the guide, we exclude the guide when measuring the two above-mentioned scores for an adversarial. Beside networks trained on ImageNet and Places205 dataset, we also test a network trained on Flickr Style dataset (Karayev et al., 2013) which has 80, 000 images categorized into 20 categories of photography styles. Table 1 shows 3NN intersection as well as the difference in rank between adversarial and guide, \u2206r3(\u03b1,g) = r3(\u03b1)\u2212 r3(g). When \u03b1 is close enough to g, we expect the intersection to be high, and rank difference to be small in magnitude. As shown in Table 1, in most cases they share exactly the same 3NN; and for at least 50% of them their rank is more similar than approximately 90% of data points in that class. These results are for sources and guides taken from the training set and the same statistics are observed for data from test or validation sets."}, {"heading": "4.2 SIMILARITY TO NATURAL REPRESETNATIONS", "text": "Having established that the representation of adversarial image (\u03b1) is close to that of the guide (g) in Euclidean distance, the immediate question is whether \u03b1 looks like a natural represenation near g. In other words, in the vicinity of g, is \u03b1 an inlier endowed with the same characteristics of other points in that neighbourhood?\nWe answer this question by studying two measurements of points in the neighbourhood: first, a probabilistic parametric measure giving the log likelihood of a point in the manifold tangent plane at g; second, a geometric nonparametric measure describing relationships of points. On both measurements, our study involves the following set of points: the guide g; the adversarial \u03b1; a set of reference points Nref consisting of 15 random points fromN20(g); a set of \u201cclose\u201d NN\u2019s not in the reference set, Nc = N20(g) \\Nref ; and a set of \u201cfar\u201d NN\u2019s Nf = N50(g) \\N40(g). The reference set Nref is used by the measurement construction, and \u03b1 as well as points in Nc and Nf are scored relative to g by the two measurements. Because we use up to the 50-th NN in this set of experiments, for which Euclidean distance might not be a reliable measure of representation similarity at long range in a high dimensional space like feature space P5, we use the cosine distance for all NNs mentioned above.\nSource image set used in this experiments is the same 20 images of Sec. 4.1. Also, we experimented on the reduced version of the same guide set as in Sec. 4.1 which consists 30 random classes for experimental expedience."}, {"heading": "4.2.1 MANIFOLD TANGENT SPACE", "text": "We build a probabilistic model of neighbours of g on the manifold tangent plane at g, and compare the likelihood of \u03b1 to other points. To this end, we make use of a probabilistic PCA (PPCA) constructed onNref , whose principal space is a secant plane that has approximately the same normal direction as the tangent plane, but generally does not pass through g because of the curvature of manifold. Therefore, we correct the small offset by shifting the plane to pass through g. With the PPCA model, this can be achieved by moving the mean of the high dimensional Gaussian to g. We then evaluate the log likelihood of a point normalized by the log likelihood of g under the Gaussian,\nwhich we denote \u2206L(\u00b7,g) = L(\u00b7)\u2212 L(g). For a large number of guide and source pairs, we repeat this measurement, and compare the distribution of \u2206L for \u03b1, and for points from Nc and Nf .\nResults for FC7 and P5 as well as for guide images sampled from training set and validation set respectively are shown in the first two columns of Fig. 4. Since the Gaussian is centred at g, \u2206L is upper bounded by zero in all sub-figures. The plots show that that \u03b1 is well explained locally by the manifold tangent plane. Furthermore, the fact that statistics of \u03b1 behave the same for guide images from training versus validation set shows that the phenomenon of adversarial perturbation for representation is a property of representation itself rather than the generalization behaviour of the model."}, {"heading": "4.2.2 ANGULAR CONSISTENCY MEASURE", "text": "If NN\u2019s of g are too sparse in the high dimensional feature space or if the manifold has high curvature, the resulting linear Gaussian model might be of low fidelity. Therefore, we present another way to find out whether \u03b1 is an inlier in the vicinity of g without relying on the manifold assumption. We take a set of reference points near g, and measure directions from guide to the reference points, and compare the corresponding directions measured for another point, say \u03b1. We refer to this measure as the angular consistency. Specifically, let {xi}ki=1 be a set of reference points, and let z be another point near g, which can be \u03b1 or another NN not in the reference set. Define vi(z) = xi\u2212 z to be the vector from z to xi; and similarly for vi(g). Then the angular consistency \u2126(z,g) between z and g with respect to the reference set is defined as:\n\u2126(z,g) = 1\nk k\u2211 i \u3008vi(z), vi(g)\u3009 \u2016vi(z)\u2016\u2016vi(g)\u2016\n(3)\nSimilar to Sec. 4.2.1, we compute \u2126(z,g) where z is taken to be \u03b1 as well as from Nc and Nf , then repeat for different guide and adversarial images, and plot the distribution of \u2126 in Fig. 4(c) and 4(f). The maximum angular consistency is 1, in which case the point coincides with g. Aside from the difference in scaling and upper bound, the angular consistency plots 4(c) and 4(f) show strikingly the same pattern as the likelihood comparison in the first two columns of Fig. 4, confirming that in general \u03b1 is inlier near corresponding g."}, {"heading": "4.3 COMPARISONS TO OTHER CATEGORIES OF ADVERSARIALS", "text": "We now compare our adversarial examples to other ones created for triggering mis-classification by the optimization of Szegedy et al. (2014b), and by the fast gradient perturbation method of Goodfellow et al. (2014). We also apply the fast gradient perturbation idea on internal representations, and compare to our constrained optimization in order to demonstrate that the phenomenon reported in this work cannot be explained by linear perturbation.\nWe hereby refer to our results as feature adversarials via optimization (feat-opt). The adversarial images generated for triggering mis-classification via the optimization of Szegedy et al. (2014b) is briefly described in Sec.2, and referred to as label adversarials via optimization (label-opt) from here on. Goodfellow et al. (2014) also proposed a way to construct adversarial examples that confuse classifiers by taking a small step consistent with the gradient, i.e. label adversarials via fast gradient (label-fgrad). Specifically, it takes the perturbation defined by \u03b4sign(\u2207I loss(f(I), l)), where f is the classifier, and l an erroneous label for input image I . Finally, we apply the fast gradient method to the representation, i.e. taking the perturbation defined by \u03b4sign(\u2207I\u2016\u03c6(I)\u2212\u03c6(Ig)\u20162). We call this last type feature adversarial via fast gradient (feat-fgrad).\nTo verify that the other types of adversarials listed above are qualitatively different from feat-opt, we show three sets of empirical results.\nFirst, for \u03b1, g as well as other points predicted to have the same class label as g, we compute the average distance to three nearest neighbours. Taking this average distance as scalar score of a point itself, we rank \u03b1 and NN1 (n1) of the \u03b1 (excluding g when present) among all other points. Fig. 5 shows rank of \u03b1 versus rank of n1(\u03b1) for different types of adversarials. For all except feat-opt, the rank of \u03b1 does not correlate well with the rank of n1(\u03b1), meaning that \u03b1 is not close to n1(\u03b1) for the other adversarial types.\nSecond, we use the manifold PPCA approach in Sec. 4.2.1. Comparing to peaked histogram of standardized likelihood of feat-opt saw in Fig. 4, Fig. 6 shows that label adversarial examples (labelopt and label-fgrad) are not represented well by the Gaussian around the first NN of the adversarial. And for the case of feat-fgrad, it is not well explained by the Gaussian around g.\nLast, we analyze the sparsity patterns on different layers for different adversarial construction methods. It is well known that DNNs with ReLU activation units have sparse activations (Glorot et al. (2011)). Therefore, if the degree of sparsity increases after the adversarial perturbation, the adversarial example is using extra active paths to manipulate the resulting represenation. We also investigate how much activate units are shared between source and adversarial, as well as guide and adversarial, by computing the intersection over union I/U of active units. If the I/U is high on all layers, then two represenations share most active paths. On the other hand, if I/U is low, while the degree of sparsity remains the same, then the adversarial must have closed some activation paths and opened new ones. In Table. 2, \u2206S is the difference between the proportion of nonzero activations on a few selected layers between the source image represenation and different types of adversarials. One can see that for all except FC7 of label-adv, the difference is significant. The column \u201cI/U with s\u201d also shows that feat-adv uses very different activation paths from s when compared to other adversarials. In the last column \u201cI/U with g\u201d, feat-adv shows much higher I/U with the guide representation on the targeted layer FC7 comparing to the one obtained by fgrad-feat.\nOne implication of observations on the sparsity patterns is that the linear perturbation explanation of label adversarial examples in Goodfellow et al. (2014) does not seem to apply to our class of represenation adversarial examples. Because Table 2 shows that fast gradient methods produce adversarials that share almost the same active paths as the source image, regardless whether applied on labels or representatation. The fast gradient method is merely making exisiting active units change values, hence looks a lot more different from the guide represenation comparing to our results."}, {"heading": "5 DISCUSSION", "text": "The approach for generating adversarial images with internal representations that mimic those of other images has worked well with a remarkably broad class of images, including images from training and test sets, and images of all classes in the Imagenet and Places datasets. Nevertheless, there are cases in which our optimization was not successful in generating adversarial images. For MNIST images and LeNet we found it necessary to relax the magnitude bound on the perturbations to the point that the guide became perceptable in the adversarial image. With LeNet on CIFAR10 inputs the optimization was only successful on some images . But both cases are somewhat atypical since the network is not particularly deep, and the data are not typical of natural images in appearance and resolution.\nWith Caffenet pre-trained on ImageNet and then fine-tuned on the Flickr Style dataset Karayev et al. (2013), we could readily generate adversarial image using FC8 (the unnormalized class scores), to be similar to the guide with \u2206r3 = 0. However, in some cases on FC7 the optimization quickly terminates, failing to obtain a representation close to the guide. One possible cause may be that the fine-tuning distorts the original natural image representation to benefit style classification. As a consequence, the FC7 layer no longer gives a good generic image represenation, and Euclidean distance on FC7 no longer a useful loss function.\nThese failures suggest that the adversarial phenomena reported here depend both on having deep networks and a broad class of natural image inputs. Nevertheless, further work is needed to understand the nature of these adversarial images and their existence in deep networks.\nACKNOWLEDGMENTS This research was financially supported in part by Architech Solutions Consulting Services Inc, MITACS, NSERC Canada, and the Canadian Institute for Advanced Research (CIFAR). Also, we would like to thank Foteini Agrafioti for her support."}], "references": [{"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K REFERENCES Chatfield", "K Simonyan", "A Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3531,", "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J Deng", "W Dong", "R Socher", "LJ Li", "K Li", "L. Fei-Fei"], "venue": "In IEEE CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Fundamental limits on adversarial robustness", "author": ["A Fawzi", "O Fawzi", "P. Frossard"], "venue": "In ICML,", "citeRegEx": "Fawzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fawzi et al\\.", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Explaining and harnessing adversarial examples", "author": ["IJ Goodfellow", "J Shlens", "C. Szegedy"], "venue": "In ICLR (arXiv:1412.6572),", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S Gu", "L. Rigazio"], "venue": "In Deep Learning and Representation Learning Workshop (arXiv:1412.5068),", "citeRegEx": "Gu and Rigazio,? \\Q2014\\E", "shortCiteRegEx": "Gu and Rigazio", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y Jia", "E Shelhamer", "J Donahue", "S Karayev", "J Long", "R Girshick", "S Guadarrama", "T. Darrell"], "venue": "In ACM Int. Conf. Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "Hinton", "GE"], "venue": "In NIPS, pp. 1097\u20131105,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Understanding deep image representations by inverting them", "author": ["A Mahendran", "A. Vedaldi"], "venue": "In IEEE CVPR (arXiv:1412.0035),", "citeRegEx": "Mahendran and Vedaldi,? \\Q2014\\E", "shortCiteRegEx": "Mahendran and Vedaldi", "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A Nguyen", "J Yosinski", "J. Clune"], "venue": "In IEEE CVPR (arXiv:1412.1897),", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C Szegedy", "W Zaremba", "I Sutskever", "J Bruna", "D Erhan", "I Goodfellow", "R. Fergus"], "venue": "In ICLR (arXiv:1312.6199),", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Exploring the space of adversarial images", "author": ["Tabacof", "Pedro", "Valle", "Eduardo"], "venue": "arXiv preprint arXiv:1510.05328,", "citeRegEx": "Tabacof et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tabacof et al\\.", "year": 2015}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Z Wang", "AC Bovik", "HR Sheikh", "Simoncelli", "EP"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Wang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2004}, {"title": "Learning deep features for scene recognition using places database", "author": ["B Zhou", "A Lapedriza", "J Xiao", "A Torralba", "A. Oliva"], "venue": "In NIPS, pp. 487\u2013495,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "Recent papers have shown that deep neural networks (DNNs) for image classification can be circumvented, often in relatively simple ways (Fawzi et al., 2015; Goodfellow et al., 2014; Gu & Rigazio, 2014; Nguyen et al., 2015; Szegedy et al., 2014b; Tabacof & Valle, 2015).", "startOffset": 136, "endOffset": 268}, {"referenceID": 4, "context": "Recent papers have shown that deep neural networks (DNNs) for image classification can be circumvented, often in relatively simple ways (Fawzi et al., 2015; Goodfellow et al., 2014; Gu & Rigazio, 2014; Nguyen et al., 2015; Szegedy et al., 2014b; Tabacof & Valle, 2015).", "startOffset": 136, "endOffset": 268}, {"referenceID": 9, "context": "Recent papers have shown that deep neural networks (DNNs) for image classification can be circumvented, often in relatively simple ways (Fawzi et al., 2015; Goodfellow et al., 2014; Gu & Rigazio, 2014; Nguyen et al., 2015; Szegedy et al., 2014b; Tabacof & Valle, 2015).", "startOffset": 136, "endOffset": 268}, {"referenceID": 4, "context": "One category of such adversarial images disrupts DNN classification of given images (sources) even though the adversarials differ almost imperceptibly from those sources (Goodfellow et al., 2014; Szegedy et al., 2014b).", "startOffset": 170, "endOffset": 218}, {"referenceID": 4, "context": ", whether they are inherent in the problem or the underlying model, and 2) such adversarial images might be harnessed to guide the development of new learning algorithms that exhibit improved robustness and better generalization (Goodfellow et al., 2014; Gu & Rigazio, 2014).", "startOffset": 229, "endOffset": 274}, {"referenceID": 7, "context": "In the latter case, Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are classified by DNNs as common objects with high confidence (often 99%).", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": "In the latter case, Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are classified by DNNs as common objects with high confidence (often 99%). While interesting, such adversarial images are totally different from the training data, or natural images per se. Because natural images only occupy a remarkably small volume of the space of all possible images, it is perhaps not surprising that discriminative DNNs trained on natural images have trouble coping with such out-of-sample data. In our approach, similar to Szegedy et al. (2014b), we focus on adversarial images that appear natural.", "startOffset": 20, "endOffset": 592}, {"referenceID": 7, "context": "In the latter case, Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are classified by DNNs as common objects with high confidence (often 99%). While interesting, such adversarial images are totally different from the training data, or natural images per se. Because natural images only occupy a remarkably small volume of the space of all possible images, it is perhaps not surprising that discriminative DNNs trained on natural images have trouble coping with such out-of-sample data. In our approach, similar to Szegedy et al. (2014b), we focus on adversarial images that appear natural. To construct an adversarial image, Szegedy et al. (2014b) use gradient-based optimization on the classification loss with respect to the image perturbation, , with a penalty on the magnitude of .", "startOffset": 20, "endOffset": 703}, {"referenceID": 7, "context": "In the latter case, Nguyen et al. (2015) describe an evolutionary algorithm to generate images comprising 2D patterns that are classified by DNNs as common objects with high confidence (often 99%). While interesting, such adversarial images are totally different from the training data, or natural images per se. Because natural images only occupy a remarkably small volume of the space of all possible images, it is perhaps not surprising that discriminative DNNs trained on natural images have trouble coping with such out-of-sample data. In our approach, similar to Szegedy et al. (2014b), we focus on adversarial images that appear natural. To construct an adversarial image, Szegedy et al. (2014b) use gradient-based optimization on the classification loss with respect to the image perturbation, , with a penalty on the magnitude of . Given an image I , a DNN classifier f , and an erroneous label l, they find the perturbation that minimizes loss(f(I + ), l) + c\u2016 \u2016. Here, c is chosen by line-search to find the smallest that achieves f(I + ) = l. The authors argue that the resulting adversarial images occupy low probability \u201cpockets\u201d in the manifold, which act like \u201cblind spots\u201d to the DNN. The adversarial construction in our paper extends the approach of Szegedy et al. (2014b). In Sec.", "startOffset": 20, "endOffset": 1291}, {"referenceID": 3, "context": "Later work by Goodfellow et al. (2014) shows that adversarial images are more common, and can be found by taking steps in the direction of the gradient of loss(f(I + ), l).", "startOffset": 14, "endOffset": 39}, {"referenceID": 3, "context": "Later work by Goodfellow et al. (2014) shows that adversarial images are more common, and can be found by taking steps in the direction of the gradient of loss(f(I + ), l). Goodfellow et al. (2014) also show that adversarial examples exist for many other models, including linear classifiers.", "startOffset": 14, "endOffset": 198}, {"referenceID": 2, "context": "Fawzi et al. (2015) later propose a more general framework to explain adversarial images, formalizing the intuition that the problem occurs when DNNs and other models are not sufficiently \u201cflexible\u201d for the given classification task.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": ", see Wang et al. (2004)), it is superior to the L2 norm.", "startOffset": 6, "endOffset": 25}, {"referenceID": 6, "context": "Figure 1 shows some adversarial images from this generation process, using the the well-known BVLC Caffe Reference model (Caffenet) (Jia et al., 2014).", "startOffset": 132, "endOffset": 150}, {"referenceID": 1, "context": "We take 100 random pair of images from Imagenet (Deng et al., 2009) used to train Caffenet, apply optimization at FC7 with delta = 10, and we find that the class label assigned to the adversarial image is never equal that of the source image, and in 95% of cases matches the guide class.", "startOffset": 48, "endOffset": 67}, {"referenceID": 7, "context": "In addition to Caffenet, we apply our approach to AlexNet (Krizhevsky et al., 2012), GoogleNet (Szegedy et al.", "startOffset": 58, "endOffset": 83}, {"referenceID": 0, "context": "), and VGG CNN-S (Chatfield et al., 2014), all trained on the the Imagenet ILSVRC dataset (Deng et al.", "startOffset": 17, "endOffset": 41}, {"referenceID": 1, "context": ", 2014), all trained on the the Imagenet ILSVRC dataset (Deng et al., 2009).", "startOffset": 56, "endOffset": 75}, {"referenceID": 13, "context": "We also used AlexNet trained on the Places205 dataset, and on a hybrid dataset comprising 205 scene classes and 977 classes from ImageNet (Zhou et al., 2014).", "startOffset": 138, "endOffset": 157}, {"referenceID": 0, "context": "), and VGG CNN-S (Chatfield et al., 2014), all trained on the the Imagenet ILSVRC dataset (Deng et al., 2009). We also used AlexNet trained on the Places205 dataset, and on a hybrid dataset comprising 205 scene classes and 977 classes from ImageNet (Zhou et al., 2014). In all these cases, using 100 random source-guide pairs we observe similar behaviour. Class labels assigned to adversarial images do not match the source. Rather, in 97% to 100% of all cases the predicted class label is that of the guide. Similar to other approaches for generating adversarial images, we find that adversarials generated on one network are similarly misclassified in other networks. Using the same 100 source-guide pairs on each of the above models we find that, on average, 54% of adversarial images obtained from one network are misclassified by the other networks as well. However, they are not usually classified with the label of the guide when tested on one of the other networks. We next turn to consider internal representations \u2013 do they resemble those of the source, the guide, or some combination of the two? One way to probe the internal representations, following Mahendran & Vedaldi (2014), is to invert the mapping, allowing us to display the image reconstructed from the internal representation.", "startOffset": 18, "endOffset": 1191}, {"referenceID": 6, "context": "Model Layer \u22293NN = 3 \u22293NN \u2265 2 \u2206r3 median, [min, max] (%) CaffeNet (Jia et al., 2014) FC7 71 95 \u22125.", "startOffset": 66, "endOffset": 84}, {"referenceID": 7, "context": "00] AlexNet (Krizhevsky et al., 2012) FC7 72 97 \u22125.", "startOffset": 12, "endOffset": 37}, {"referenceID": 0, "context": "10] VGG CNN S (Chatfield et al., 2014) FC7 84 100 \u22123.", "startOffset": 14, "endOffset": 38}, {"referenceID": 13, "context": "00] Places205 AlexNet (Zhou et al., 2014) FC7 91 100 \u22121.", "startOffset": 22, "endOffset": 41}, {"referenceID": 13, "context": "04] Places205 Hybrid (Zhou et al., 2014) FC7 85 100 \u22121.", "startOffset": 21, "endOffset": 40}, {"referenceID": 8, "context": "We now compare our adversarial examples to other ones created for triggering mis-classification by the optimization of Szegedy et al. (2014b), and by the fast gradient perturbation method of Goodfellow et al.", "startOffset": 119, "endOffset": 142}, {"referenceID": 3, "context": "(2014b), and by the fast gradient perturbation method of Goodfellow et al. (2014). We also apply the fast gradient perturbation idea on internal representations, and compare to our constrained optimization in order to demonstrate that the phenomenon reported in this work cannot be explained by linear perturbation.", "startOffset": 57, "endOffset": 82}, {"referenceID": 3, "context": "(2014b), and by the fast gradient perturbation method of Goodfellow et al. (2014). We also apply the fast gradient perturbation idea on internal representations, and compare to our constrained optimization in order to demonstrate that the phenomenon reported in this work cannot be explained by linear perturbation. We hereby refer to our results as feature adversarials via optimization (feat-opt). The adversarial images generated for triggering mis-classification via the optimization of Szegedy et al. (2014b) is briefly described in Sec.", "startOffset": 57, "endOffset": 514}, {"referenceID": 3, "context": "(2014b), and by the fast gradient perturbation method of Goodfellow et al. (2014). We also apply the fast gradient perturbation idea on internal representations, and compare to our constrained optimization in order to demonstrate that the phenomenon reported in this work cannot be explained by linear perturbation. We hereby refer to our results as feature adversarials via optimization (feat-opt). The adversarial images generated for triggering mis-classification via the optimization of Szegedy et al. (2014b) is briefly described in Sec.2, and referred to as label adversarials via optimization (label-opt) from here on. Goodfellow et al. (2014) also proposed a way to construct adversarial examples that confuse classifiers by taking a small step consistent with the gradient, i.", "startOffset": 57, "endOffset": 651}, {"referenceID": 3, "context": "It is well known that DNNs with ReLU activation units have sparse activations (Glorot et al. (2011)).", "startOffset": 79, "endOffset": 100}, {"referenceID": 4, "context": "One implication of observations on the sparsity patterns is that the linear perturbation explanation of label adversarial examples in Goodfellow et al. (2014) does not seem to apply to our class of represenation adversarial examples.", "startOffset": 134, "endOffset": 159}], "year": 2017, "abstractText": "We show that the internal representations of an image in a deep neural network (DNN) can be manipulated to mimic those of other natural images with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image manipulation designed to produce erroneous class labels, while here we look at internal layers of the representation. Despite the similarities in the generation process, our class of adversarial images differs qualitatively from previous ones. Specifically, while the input image is perceptually similar to an image of one class, its internal representation appear remarkably like that of natural images in a different class. This phenomenon raises questions about DNN representations, as well as the properties of natural images themselves.", "creator": "LaTeX with hyperref package"}}}