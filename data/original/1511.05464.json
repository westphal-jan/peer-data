{"id": "1511.05464", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Extending Gossip Algorithms to Distributed Estimation of U-statistics", "abstract": "Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of $U$-statistics, relying on more expensive averaging over pairs of observations, is a less investigated area. Yet, such data functionals are essential to describe global properties of a statistical population, with important examples including Area Under the Curve, empirical variance, Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the $U$-statistic of interest. We establish convergence rate bounds of $O(1/t)$ and $O(\\log t / t)$ for the synchronous and asynchronous cases respectively, where $t$ is the number of iterations, with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.", "histories": [["v1", "Tue, 17 Nov 2015 16:49:52 GMT  (341kb,D)", "http://arxiv.org/abs/1511.05464v1", "to be presented at NIPS 2015"]], "COMMENTS": "to be presented at NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.DC cs.LG cs.SY", "authors": ["igor colin", "aur\u00e9lien bellet", "joseph salmon", "st\u00e9phan cl\u00e9men\u00e7on"], "accepted": true, "id": "1511.05464"}, "pdf": {"name": "1511.05464.pdf", "metadata": {"source": "CRF", "title": "Extending Gossip Algorithms to Distributed Estimation of U -Statistics", "authors": ["Igor Colin", "Aur\u00e9lien Bellet"], "emails": ["first.last@telecom-paristech.fr", "aurelien.bellet@inria.fr", "first.last@telecom-paristech.fr", "first.last@telecom-paristech.fr"], "sections": [{"heading": "1 Introduction", "text": "Decentralized computation and estimation have many applications in sensor and peer-to-peer networks as well as for extracting knowledge from massive information graphs such as interlinked Web documents and on-line social media. Algorithms running on such networks must often operate under tight constraints: the nodes forming the network cannot rely on a centralized entity for communication and synchronization, without being aware of the global network topology and/or have limited resources (computational power, memory, energy). Gossip algorithms [20, 19, 6], where each node exchanges information with at most one of its neighbors at a time, have emerged as a simple yet powerful technique for distributed computation in such settings. Given a data observation on each node, gossip algorithms can be used to compute averages or sums of functions of the data that are separable across observations (see for example [11, 2, 16, 12, 10] and references therein). Unfortunately, these algorithms cannot be used to efficiently compute quantities that take the form of an average over pairs of observations, also known as U -statistics [13]. Among classical U -statistics used in machine learning and data mining, one can mention, among others: the sample variance, the Area Under the Curve (AUC) of a classifier on distributed data, the Gini mean difference, the Kendall\nar X\niv :1\n51 1.\n05 46\n4v 1\n[ st\nat .M\ntau rank correlation coefficient, the within-cluster point scatter and several statistical hypothesis test statistics such as Wilcoxon Mann-Whitney [15].\nIn this paper, we propose randomized synchronous and asynchronous gossip algorithms to efficiently compute a U -statistic, in which each node maintains a local estimate of the quantity of interest throughout the execution of the algorithm. Our methods rely on two types of iterative information exchange in the network: propagation of local observations across the network, and averaging of local estimates. We show that the local estimates generated by our approach converge in expectation to the value of the U -statistic at rates of O(1/t) and O(log t/t) for the synchronous and asynchronous versions respectively, where t is the number of iterations. These convergence bounds feature datadependent terms that reflect the hardness of the estimation problem, and network-dependent terms related to the spectral gap of the network graph [4], showing that our algorithms are faster on wellconnected networks. The proofs rely on an original reformulation of the problem using \u201cphantom nodes\u201d, i.e., on additional nodes that account for data propagation in the network. Our results largely improve upon those presented in [18]: in particular, we achieve faster convergence together with lower memory and communication costs. Experiments conducted on AUC and within-cluster point scatter estimation using real data confirm the superiority of our approach.\nThe rest of this paper is organized as follows. Section 2 introduces the problem of interest as well as relevant notation. Section 3 provides a brief review of the related work in gossip algorithms. We then describe our approach along with the convergence analysis in Section 4, both in the synchronous and asynchronous settings. Section 5 presents our numerical results."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Definitions and Notations", "text": "For any integer p > 0, we denote by [p] the set {1, . . . , p} and by |F | the cardinality of any finite set F . We represent a network of size n > 0 as an undirected graph G = (V,E), where V = [n] is the set of vertices and E \u2286 V \u00d7 V the set of edges. We denote by A(G) the adjacency matrix related to the graph G, that is for all (i, j) \u2208 V 2, [A(G)]ij = 1 if and only if (i, j) \u2208 E. For any node i \u2208 V , we denote its degree by di = |{j : (i, j) \u2208 E}|. We denote by L(G) the graph Laplacian of G, defined by L(G) = D(G)\u2212A(G) where D(G) = diag(d1, . . . , dn) is the matrix of degrees. A graphG = (V,E) is said to be connected if for all (i, j) \u2208 V 2 there exists a path connecting i and j; it is bipartite if there exist S, T \u2282 V such that S \u222a T = V , S \u2229 T = \u2205 and E \u2286 (S \u00d7 T )\u222a (T \u00d7 S). A matrix M \u2208 Rn\u00d7n is nonnegative (resp. positive) if and only if for all (i, j) \u2208 [n]2, [M ]ij \u2265 0, (resp. [M ]ij > 0). We write M \u2265 0 (resp. M > 0) when this holds. The transpose of M is denoted by M>. A matrix P \u2208 Rn\u00d7n is stochastic if and only if P \u2265 0 and P1n = 1n, where 1n = (1, . . . , 1)\n> \u2208 Rn. The matrix P \u2208 Rn\u00d7n is bi-stochastic if and only if P and P> are stochastic. We denote by In the identity matrix in Rn\u00d7n, (e1, . . . , en) the standard basis in Rn, I{E} the indicator function of an event E and \u2016 \u00b7 \u2016 the usual `2 norm."}, {"heading": "2.2 Problem Statement", "text": "LetX be an input space and (X1, . . . , Xn) \u2208 Xn a sample of n \u2265 2 points in that space. We assume X \u2286 Rd for some d > 0 throughout the paper, but our results straightforwardly extend to the more general setting. We denote as X = (X1, . . . , Xn)> the design matrix. Let H : X \u00d7 X \u2192 R be a measurable function, symmetric in its two arguments and with H(X,X) = 0, \u2200X \u2208 X . We consider the problem of estimating the following quantity, known as a degree two U -statistic [13]:1\nU\u0302n(H) = 1\nn2 n\u2211 i,j=1 H(Xi, Xj). (1)\nIn this paper, we illustrate the interest of U -statistics on two applications, among many others. The first one is the within-cluster point scatter [5], which measures the clustering quality of a partition\n1We point out that the usual definition of U -statistic differs slightly from (1) by a factor of n/(n\u2212 1).\nP of X as the average distance between points in each cell C \u2208 P . It is of the form (1) with\nHP(X,X \u2032) = \u2016X \u2212X \u2032\u2016 \u00b7 \u2211 C\u2208P I{(X,X\u2032)\u2208C2}. (2)\nWe also study the AUC measure [9]. For a given sample (X1, `1), . . . , (Xn, `n) on X \u00d7 {\u22121,+1}, the AUC measure of a linear classifier \u03b8 \u2208 Rd\u22121 is given by:\nAUC(\u03b8) =\n\u2211 1\u2264i,j\u2264n(1\u2212 `i`j)I{`i(\u03b8>Xi)>\u2212`j(\u03b8>Xj)}\n4 (\u2211 1\u2264i\u2264n I{`i=1} )(\u2211 1\u2264i\u2264n I{`i=\u22121} ) . (3)\nThis score is the probability for a classifier to rank a positive observation higher than a negative one.\nWe focus here on the decentralized setting, where the data sample is partitioned across a set of nodes in a network. For simplicity, we assume V = [n] and each node i \u2208 V only has access to a single data observation Xi.2 We are interested in estimating (1) efficiently using a gossip algorithm."}, {"heading": "3 Related Work", "text": "Gossip algorithms have been extensively studied in the context of decentralized averaging in networks, where the goal is to compute the average of n real numbers (X = R):\nX\u0304n = 1\nn n\u2211 i=1 Xi = 1 n X>1n. (4)\nOne of the earliest work on this canonical problem is due to [20], but more efficient algorithms have recently been proposed, see for instance [11, 2]. Of particular interest to us is the work of [2], which introduces a randomized gossip algorithm for computing the empirical mean (4) in a context where nodes wake up asynchronously and simply average their local estimate with that of a randomly chosen neighbor. The communication probabilities are given by a stochastic matrix P , where pij is the probability that a node i selects neighbor j at a given iteration. As long as the network graph is connected and non-bipartite, the local estimates converge to (4) at a rate O(e\u2212ct) where the constant c can be tied to the spectral gap of the network graph [4], showing faster convergence for well-connected networks.3 Such algorithms can be extended to compute other functions such as maxima and minima, or sums of the form \u2211n i=1 f(Xi) for some function f : X \u2192 R (as done for instance in [16]). Some work has also gone into developing faster gossip algorithms for poorly connected networks, assuming that nodes know their (partial) geographic location [7, 14]. For a detailed account of the literature on gossip algorithms, we refer the reader to [19, 6].\nHowever, existing gossip algorithms cannot be used to efficiently compute (1) as it depends on pairs of observations. To the best of our knowledge, this problem has only been investigated in [18]. Their algorithm, coined U2-gossip, achieves O(1/t) convergence rate but has several drawbacks. First, each node must store two auxiliary observations, and two pairs of nodes must exchange an observation at each iteration. For high-dimensional problems (large d), this leads to a significant memory and communication load. Second, the algorithm is not asynchronous as every node must update its estimate at each iteration. Consequently, nodes must have access to a global clock, which is often unrealistic in practice. In the next section, we introduce new synchronous and asynchronous algorithms with faster convergence as well as smaller memory and communication cost per iteration."}, {"heading": "4 GoSta Algorithms", "text": "In this section, we introduce gossip algorithms for computing (1). Our approach is based on the observation that U\u0302n(H) = 1/n \u2211n i=1 hi, with hi = 1/n \u2211n j=1H(Xi, Xj), and we write h = (h1, . . . , hn) >. The goal is thus similar to the usual distributed averaging problem (4), with the key difference that each local value hi is itself an average depending on the entire data sample. Consequently, our algorithms will combine two steps at each iteration: a data propagation step to\n2Our results generalize to the case where each node holds a subset of the observations (see Section 4). 3For the sake of completeness, we provide an analysis of this algorithm in the supplementary material.\nAlgorithm 1 GoSta-sync: a synchronous gossip algorithm for computing a U -statistic Require: Each node k holds observation Xk 1: Each node k initializes its auxiliary observation Yk = Xk and its estimate Zk = 0 2: for t = 1, 2, . . . do 3: for p = 1, . . . , n do 4: Set Zp \u2190 t\u22121t Zp + 1 t H(Xp, Yp)\nallow each node i to estimate hi, and an averaging step to ensure convergence to the desired value U\u0302n(H). We first present the algorithm and its analysis for the (simpler) synchronous setting in Section 4.1, before introducing an asynchronous version (Section 4.2)."}, {"heading": "4.1 Synchronous Setting", "text": "In the synchronous setting, we assume that the nodes have access to a global clock so that they can all update their estimate at each time instance. We stress that the nodes need not to be aware of the global network topology as they will only interact with their direct neighbors in the graph.\nLet us denote by Zk(t) the (local) estimate of U\u0302n(H) by node k at iteration t. In order to propagate data across the network, each node k maintains an auxiliary observation Yk, initialized to Xk. Our algorithm, coined GoSta, goes as follows. At each iteration, each node k updates its local estimate by taking the running average of Zk(t) and H(Xk, Yk). Then, an edge of the network is drawn uniformly at random, and the corresponding pair of nodes average their local estimates and swap their auxiliary observations. The observations are thus each performing a random walk (albeit coupled) on the network graph. The full procedure is described in Algorithm 1.\nIn order to prove the convergence of Algorithm 1, we consider an equivalent reformulation of the problem which allows us to model the data propagation and the averaging steps separately. Specifically, for each k \u2208 V , we define a phantom Gk = (Vk, Ek) of the original network G, with Vk = {vki ; 1 \u2264 i \u2264 n} and Ek = {(vki , vkj ); (i, j) \u2208 E}. We then create a new graph G\u0303 = (V\u0303 , E\u0303) where each node k \u2208 V is connected to its counterpart vkk \u2208 Vk:{\nV\u0303 = V \u222a (\u222ank=1Vk) E\u0303 = E \u222a (\u222ank=1Ek) \u222a {(k, vkk); k \u2208 V }\nThe construction of G\u0303 is illustrated in Figure 1. In this new graph, the nodes V from the original network will hold the estimates Z1(t), . . . , Zn(t) as described above. The role of each Gk is to simulate the data propagation in the original graph G. For i \u2208 [n], vki \u2208 V k initially holds the value H(Xk, Xi). At each iteration, we draw a random edge (i, j) of G and nodes vki and v k j swap their value for all k \u2208 [n]. To update its estimate, each node k will use the current value at vkk .\nWe can now represent the system state at iteration t by a vector S(t) = (S1(t)>,S2(t)>)> \u2208 Rn+n2 . The first n coefficients, S1(t), are associated with nodes in V and correspond to the estimate vector Z(t) = [Z1(t), . . . , Zn(t)]>. The last n2 coefficients, S2(t), are associated with nodes in (Vk)1\u2264k\u2264n and represent the data propagation in the network. Their initial value is set to S2(0) = (e>1 H, . . . , e > nH) so that for any (k, l) \u2208 [n]2, node vkl initially stores the value H(Xk, Xl).\nRemark 1. The \u201cphantom network\u201d G\u0303 is of size O(n2), but we stress the fact that it is used solely as a tool for the convergence analysis: Algorithm 1 operates on the original graph G.\nThe transition matrix of this system accounts for three events: the averaging step (the action of G on itself), the data propagation (the action of Gk on itself for all k \u2208 V ) and the estimate update (the action of Gk on node k for all k \u2208 V ). At a given step t > 0, we are interested in characterizing the transition matrix M(t) such that E[S(t + 1)] = M(t)E[S(t)]. For the sake of clarity, we write M(t) as an upper block-triangular (n+ n2)\u00d7 (n+ n2) matrix:\nM(t) =\n( M1(t) M2(t)\n0 M3(t)\n) , (5)\nwith M1(t) \u2208 Rn\u00d7n, M2(t) \u2208 Rn\u00d7n 2 and M3(t) \u2208 Rn 2\u00d7n2 . The bottom left part is necessarily 0, because G does not influence any Gk. The upper left M1(t) block corresponds to the averaging step; therefore, for any t > 0, we have:\nM1(t) = t\u2212 1 t \u00b7 1 |E| \u2211 (i,j)\u2208E ( In \u2212 1 2 (ei \u2212 ej)(ei \u2212 ej)> ) = t\u2212 1 t W2 (G) ,\nwhere for any \u03b1 > 1, W\u03b1(G) is defined by:\nW\u03b1(G) = 1 |E| \u2211\n(i,j)\u2208E\n( In \u2212 1\n\u03b1 (ei \u2212 ej)(ei \u2212 ej)>\n) = In \u2212 2\n\u03b1|E| L(G). (6)\nFurthermore, M2(t) and M3(t) are defined as follows:\nM2(t) = 1\nt  e>1 0 \u00b7 \u00b7 \u00b7 0 0 . . . ... ...\n. . . 0 0 \u00b7 \u00b7 \u00b7 0 e>n  \ufe38 \ufe37\ufe37 \ufe38\nB\nand M3(t) =  W1 (G) 0 \u00b7 \u00b7 \u00b7 0 0 . . . ... ... . . . ...\n0 \u00b7 \u00b7 \u00b7 0 W1 (G)  \ufe38 \ufe37\ufe37 \ufe38\nC\n,\nwhere M2(t) is a block diagonal matrix corresponding to the observations being propagated, and M3(t) represents the estimate update for each node k. Note that M3(t) = W1 (G)\u2297 In where \u2297 is the Kronecker product.\nWe can now describe the expected state evolution. At iteration t = 0, one has: E[S(1)] = M(1)E[S(0)] = M(1)S(0) = (\n0 B 0 C\n)( 0\nS2(0)\n) = ( BS2(0) CS2(0) ) . (7)\nUsing recursion, we can write: E[S(t)] = M(t)M(t\u2212 1) . . .M(1)S(0) = ( 1 t \u2211t s=1W2 (G) t\u2212s BCs\u22121S2(0)\nCtS2(0)\n) . (8)\nTherefore, in order to prove the convergence of Algorithm 1, one needs to show that limt\u2192+\u221e 1 t \u2211t s=1W2 (G) t\u2212s BCs\u22121S2(0) = U\u0302n(H)1n. We state this precisely in the next theorem. Theorem 1. Let G be a connected and non-bipartite graph with n nodes, X \u2208 Rn\u00d7d a design matrix and (Z(t)) the sequence of estimates generated by Algorithm 1. For all k \u2208 [n], we have:\nlim t\u2192+\u221e\nE[Zk(t)] = 1\nn2 \u2211 1\u2264i,j\u2264n H(Xi, Xj) = U\u0302n(H). (9)\nMoreover, for any t > 0,\u2225\u2225\u2225E[Z(t)]\u2212 U\u0302n(H)1n\u2225\u2225\u2225 \u2264 1 ct \u2225\u2225\u2225h\u2212 U\u0302n(H)1n\u2225\u2225\u2225+ ( 2 ct + e\u2212ct )\u2225\u2225H\u2212 h1>n \u2225\u2225 ,\nwhere c = c(G) := 1\u2212 \u03bb2(2) and \u03bb2(2) is the second largest eigenvalue of W2 (G).\nProof. See supplementary material.\nTheorem 1 shows that the local estimates generated by Algorithm 1 converge to U\u0302n(H) at a rate O(1/t). Furthermore, the constants reveal the rate dependency on the particular problem instance. Indeed, the two norm terms are data-dependent and quantify the difficulty of the estimation problem itself through a dispersion measure. In contrast, c(G) is a network-dependent term since 1\u2212\u03bb2(2) = \u03b2n\u22121/|E|, where \u03b2n\u22121 is the second smallest eigenvalue of the graph LaplacianL(G) (see Lemma 1 in the supplementary material). The value \u03b2n\u22121 is also known as the spectral gap of G and graphs with a larger spectral gap typically have better connectivity [4]. This will be illustrated in Section 5.\nComparison to U2-gossip. To estimate U\u0302n(H), U2-gossip [18] does not use averaging. Instead, each node k requires two auxiliary observations Y (1)k and Y (2) k which are both initialized to Xk. At each iteration, each node k updates its local estimate by taking the running average of Zk and H(Y\n(1) k , Y (2) k ). Then, two random edges are selected: the nodes connected by the first (resp. second) edge swap their first (resp. second) auxiliary observations. A precise statement of the algorithm is provided in the supplementary material. U2-gossip has several drawbacks compared to GoSta: it requires initiating communication between two pairs of nodes at each iteration, and the amount of communication and memory required is higher (especially when data is high-dimensional). Furthermore, applying our convergence analysis to U2-gossip, we obtain the following refined rate:4\u2225\u2225\u2225E[Z(t)]\u2212 U\u0302n(H)1n\u2225\u2225\u2225 \u2264 \u221an\nt\n( 2\n1\u2212 \u03bb2(1) \u2225\u2225\u2225h\u2212 U\u0302n(H)1n\u2225\u2225\u2225+ 1 1\u2212 \u03bb2(1)2 \u2225\u2225H\u2212 h1>n \u2225\u2225) , (10)\nwhere 1 \u2212 \u03bb2(1) = 2(1 \u2212 \u03bb2(2)) = 2c(G) and \u03bb2(1) is the second largest eigenvalue of W1(G). The advantage of propagating two observations in U2-gossip is seen in the 1/(1 \u2212 \u03bb2(1)2) term, however the absence of averaging leads to an overall \u221a n factor. Intuitively, this is because nodes do not benefit from each other\u2019s estimates. In practice, \u03bb2(2) and \u03bb2(1) are close to 1 for reasonablysized networks (for instance, \u03bb2(2) = 1 \u2212 1/n for the complete graph), so the square term does not provide much gain and the \u221a n factor dominates in (10). We thus expect U2-gossip to converge slower than GoSta, which is confirmed by the numerical results presented in Section 5."}, {"heading": "4.2 Asynchronous Setting", "text": "In practical settings, nodes may not have access to a global clock to synchronize the updates. In this section, we remove the global clock assumption and propose a fully asynchronous algorithm where each node has a local clock, ticking at a rate 1 Poisson process. Yet, local clocks are i.i.d. so one can use an equivalent model with a global clock ticking at a rate n Poisson process and a random edge draw at each iteration, as in synchronous setting (one may refer to [2] for more details on clock modeling). However, at a given iteration, the estimate update step now only involves the selected pair of nodes. Therefore, the nodes need to maintain an estimate of the current iteration number to ensure convergence to an unbiased estimate of U\u0302n(H). Hence for all k \u2208 [n], let pk \u2208 [0, 1] denote the probability of node k being picked at any iteration. With our assumption that nodes activate with a uniform distribution over E, pk = 2dk/|E|. Moreover, the number of times a node k has been selected at a given iteration t > 0 follows a binomial distribution with parameters t and pk. Let us define mk(t) such that mk(0) = 0 and for t > 0:\nmk(t) = { mk(t\u2212 1) + 1pk if k is picked at iteration t, mk(t\u2212 1) otherwise.\n(11)\nAlgorithm 2 GoSta-async: an asynchronous gossip algorithm for computing a U -statistic Require: Each node k holds observation Xk and pk = 2dk/|E| 1: Each node k initializes Yk = Xk, Zk = 0 and mk = 0 2: for t = 1, 2, . . . do 3: Draw (i, j) uniformly at random from E 4: Set mi \u2190 mi + 1/pi and mj \u2190 mj + 1/pj 5: Set Zi, Zj \u2190 12 (Zi + Zj) 6: Set Zi \u2190 (1\u2212 1pimi )Zi + 1 pimi H(Xi, Yi)\n7: Set Zj \u2190 (1\u2212 1pjmj )Zj + 1 pjmj H(Xj , Yj) 8: Swap auxiliary observations of nodes i and j: Yi \u2194 Yj 9: end for\nFor any k \u2208 [n] and any t > 0, one has E[mk(t)] = t \u00d7 pk \u00d7 1/pk = t. Therefore, given that every node knows its degree and the total number of edges in the network, the iteration estimates are unbiased. We can now give an asynchronous version of GoSta, as stated in Algorithm 2.\nTo show that local estimates converge to U\u0302n(H), we use a similar model as in the synchronous setting. The time dependency of the transition matrix is more complex ; so is the upper bound.\nTheorem 2. Let G be a connected and non bipartite graph with n nodes, X \u2208 Rn\u00d7d a design matrix and (Z(t)) the sequence of estimates generated by Algorithm 2. For all k \u2208 [n], we have:\nlim t\u2192+\u221e\nE[Zk(t)] = 1\nn2 \u2211 1\u2264i,j\u2264n H(Xi, Xj) = U\u0302n(H). (12)\nMoreover, there exists a constant c\u2032(G) > 0 such that, for any t > 1,\u2225\u2225\u2225E[Z(t)]\u2212 U\u0302n(H)1n\u2225\u2225\u2225 \u2264 c\u2032(G) \u00b7 log t t \u2016H\u2016. (13)\nProof. See supplementary material.\nRemark 2. Our methods can be extended to the situation where nodes contain multiple observations: when drawn, a node will pick a random auxiliary observation to swap. Similar convergence results are achieved by splitting each node into a set of nodes, each containing only one observation and new edges weighted judiciously."}, {"heading": "5 Experiments", "text": "In this section, we present two applications on real datasets: the decentralized estimation of the Area Under the ROC Curve (AUC) and of the within-cluster point scatter. We compare the performance of our algorithms to that of U2-gossip [18] \u2014 see supplementary material for additional comparisons to some baseline methods. We perform our simulations on the three types of network described below (corresponding values of 1\u2212 \u03bb2(2) are shown in Table 1). \u2022 Complete graph: This is the case where all nodes are connected to each other. It is the ideal situation in our framework, since any pair of nodes can communicate directly. For a complete graph G of size n > 0, 1\u2212 \u03bb2(2) = 1/n, see [1, Ch.9] or [4, Ch.1] for details. \u2022 Two-dimensional grid: Here, nodes are located on a 2D grid, and each node is connected to its four neighbors on the grid. This network offers a regular graph with isotropic communication, but its diameter ( \u221a n) is quite high, especially in comparison to usual scale-free networks.\n\u2022 Watts-Strogatz: This random network generation technique is introduced in [21] and allows us to create networks with various communication properties. It relies on two parameters: the average degree of the network k and a rewiring probability p. In expectation, the higher the rewiring probability, the better the connectivity of the network. Here, we use k = 5 and p = 0.3 to achieve a connectivity compromise between the complete graph and the two-dimensional grid.\n4The proof can be found in the supplementary material.\nAUC measure. We first focus on the AUC measure of a linear classifier \u03b8 as defined in (3). We use the SMVguide3 binary classification dataset which contains n = 1260 points in d = 23 dimensions.5 We set \u03b8 to the difference between the class means. For each generated network, we perform 50 runs of GoSta-sync (Algorithm 1) and U2-gossip. The top row of Figure 2 shows the evolution over time of the average relative error and the associated standard deviation across nodes for both algorithms on each type of network. On average, GoSta-sync outperforms U2-gossip on every network. The variance of the estimates across nodes is also lower due to the averaging step. Interestingly, the performance gap between the two algorithms is greatly increasing early on, presumably because the exponential term in the convergence bound of GoSta-sync is significant in the first steps.\nWithin-cluster point scatter. We then turn to the within-cluster point scatter defined in (2). We use the Wine Quality dataset which contains n = 1599 points in d = 12 dimensions, with a total ofK = 11 classes.6 We focus on the partition P associated to class centroids and run the aforementioned methods 50 times. The results are shown in the bottom row of Figure 2. As in the case of AUC, GoSta-sync achieves better perfomance on all types of networks, both in terms of average error and variance. In Figure 3a, we show the average time needed to reach a 0.2 relative error on a complete graph ranging from n = 50 to n = 1599. As predicted by our analysis, the performance gap widens in favor of GoSta as the size of the graph increases. Finally, we compare the performance of GoSta-sync and GoSta-async (Algorithm 2) in Figure 3b. Despite the slightly worse theoretical convergence rate for GoSta-async, both algorithms have comparable performance in practice."}, {"heading": "6 Conclusion", "text": "We have introduced new synchronous and asynchronous randomized gossip algorithms to compute statistics that depend on pairs of observations (U -statistics). We have proved the convergence rate in\n5This dataset is available at http://mldata.org/repository/data/viewslug/svmguide3/ 6This dataset is available at https://archive.ics.uci.edu/ml/datasets/Wine\nboth settings, and numerical experiments confirm the practical interest of the proposed algorithms. In future work, we plan to investigate whether adaptive communication schemes (such as those of [7, 14]) can be used to speed-up our algorithms. Our contribution could also be used as a building block for decentralized optimization of U -statistics, extending for instance the approaches of [8, 17].\nAcknowledgements This work was supported by the chair Machine Learning for Big Data of Te\u0301le\u0301com ParisTech, and was conducted when A. Bellet was affiliated with Te\u0301le\u0301com ParisTech."}, {"heading": "A Preliminary Results", "text": "Here, we state preliminary results on the matrices W\u03b1(G) that will be useful for deriving convergence proofs and compare the algorithms.\nFirst, we characterize the eigenvalues of W\u03b1(G) in terms of those of the graph Laplacian.\nLemma 1. Let G = (V,E) be an undirected graph and let (\u03b2i)1\u2264i\u2264n be the eigenvalues of L(G), sorted in decreasing order. For any \u03b1 \u2265 1, we denote as (\u03bbi(\u03b1))1\u2264i\u2264n the eigenvalues of W\u03b1(G), sorted in decreasing order. Then, for any 1 \u2264 i \u2264 n,\n\u03bbi(\u03b1) = 1\u2212 2\u03b2n\u2212i+1 \u03b1|E| . (14)\nProof. Let \u03b1 \u2265 1. The matrix W\u03b1(G) can be rewritten as follow:\nW\u03b1(G) = 1 |E| \u2211\n(i,j)\u2208E\n( In \u2212 1\n\u03b1 (ei \u2212 ej)(ei \u2212 ej)>\n) (15)\n= In \u2212 1 \u03b1|E| \u2211\n(i,j)\u2208E\n(ei \u2212 ej)(ei \u2212 ej)> = In \u2212 2\n\u03b1|E| L(G). (16)\nLet \u03c6i \u2208 Rn be an eigenvector of L(G) corresponding to an eigenvalue \u03b2i, then we have:\nW\u03b1(G)\u03c6i = ( In \u2212 2\n\u03b1|E| L(G)\n) \u03c6i = ( 1\u2212 2\n\u03b1|E| \u03b2i\n) \u03c6i.\nThus, \u03c6i is also an eigenvector of W\u03b1(G) for the eigenvalue 1\u2212 2\u03b1|E|\u03b2i and the result holds.\nThe following lemmata provide essential properties on W\u03b1(G) eigenvalues.\nLemma 2. Let n > 0 and let G = ([n], E) be an undirected graph. If G is connected and nonbipartite, then for any \u03b1 \u2265 1, W\u03b1(G) is primitive, i.e., there exists k > 0 such that W\u03b1(G)k > 0.\nProof. Let \u03b1 \u2265 1. For every (i, j) \u2208 E, In \u2212 1\u03b1 (ei \u2212 ej)(ei \u2212 ej) > is nonnegative. Therefore W\u03b1(G) is also nonnegative. For any 1 \u2264 k < l \u2264 n, by definition of W\u03b1(G), one has the following equivalence: ([A(G)]kl > 0)\u21d4 ([W\u03b1(G)]kl > 0) . By hypothesis, G is connected. Therefore, for any pair of nodes (k, l) \u2208 V 2 there exists an integer skl > 0 such that [A(G)skl ]kl > 0 so W\u03b1(G) is irreducible. Also, G is non bipartite so similar reasoning can be used to show that W\u03b1(G) is aperiodic.\nBy the Lattice Theorem (see [3, Th. 4.3, p.75]), for any 1 \u2264 k, l \u2264 n there exists an integer mkl such that, for any m \u2265 mkl:\n[W\u03b1(G) m]kl > 0.\nFinally, we can define m\u0304 = supk,lmkl and observe that W\u03b1(G) m\u0304 > 0.\nLemma 3. Let G = (V,E) be a connected and non bipartite graph. Then for any \u03b1 \u2265 1,\n1 = \u03bb1(\u03b1) > \u03bb2(\u03b1),\nwhere \u03bb1(\u03b1) and \u03bb2(\u03b1) are respectively the largest and the second largest eigenvalue of W\u03b1(G).\nProof. Let \u03b1 \u2265 1. The matrix W\u03b1(G) is bistochastic, so \u03bb1(\u03b1) = 1. By Lemma 2, W\u03b1(G) is primitive. Therefore, by the Perron-Frobenius Theorem (see [3, Th. 1.1, p.197]), we can conclude that \u03bb1(\u03b1) > \u03bb2(\u03b1).\nAlgorithm 3 Gossip algorithm proposed in [2] to compute the standard mean (4) Require: Each node i holds observation Xi\n1: Each node initializes its estimator Zi \u2190 Xi 2: for t = 1, 2, . . . do 3: Draw (i, j) uniformly at random from E 4: Set Zi, Zj \u2190 Zi+Zj2 5: end for"}, {"heading": "B Gossip Algorithm for Standard Averaging", "text": "In this part, we provide a description and analysis of the randomized gossip algorithm proposed in [2] for the standard averaging problem (4). The procedure is shown in Algorithm 3 and goes as follows. For each node k \u2208 [n], an estimator Zk(t) is initialized to the observation of the node Zk(0) = Xk. At each iteration t > 0, an edge (i, j) \u2208 E is picked uniformly at random over E. Then, the corresponding nodes average their current estimators, while the others remain unchanged:\nZi(t) = Zj(t) = Zi(t\u2212 1) + Zj(t\u2212 1)\n2 . (17)\nThe evolution of estimates can be characterized using transition matrices. If an edge (i, j) \u2208 E is picked at iteration t > 0, one can re-formulate (17) as:\nZ(t) = ( In \u2212 (ei \u2212 ej)(ei \u2212 ej)>\n2\n) Z(t\u2212 1),\nwhere Z(t) = (Z1(t), . . . , Zn(t)) \u2208 Rn is the (global) vector of mean estimates. The edges being drawn uniformly at random, the expected transition matrix is simply W2(G) with the notation introduced in (6). Then, for all t > 0, the expectation of the global estimate at iteration t is characterized recursively by:\nE[Z(t)] = W2(G)E[Z(t\u2212 1)] = W2(G)tE[Z(0)] = W2(G)tX,\nwhere E stands for the expectation with respect to the edge sampling process.\nWe can now state a convergence result for Algorithm 3, rephrasing slightly the results from [2]. Theorem 3. Let us assume that G is connected and non bipartite. Then, for Z(t) defined in Algorithm 3, we have that for all k \u2208 [n]:\nlim t\u2192+\u221e\nE[Zk(t)] = X\u0304n,\nMoreover, for any t > 0,\n\u2016E[Z(t)]\u2212 X\u0304n1n\u2016 \u2264 e\u2212ct\u2016X\u2212 X\u0304n1n\u2016.\nwhere c = (1\u2212 \u03bb2(2)) > 0, with \u03bb2(2) the second largest eigenvalue of W2(G).\nProof. In order to prove the convergence of the estimates in expectation, one has to prove that W2 (G)X converges to the objective X\u0304n1n.\nRemark that W2 (G) is bi-stochastic. Let us denote as (\u03bbk(2))1\u2264k\u2264n and (\u03c6k)1\u2264k\u2264n respectively the eigenvalues (sorted in decreasing order) and the corresponding eigenvectors of W2 (G). Lemma 3 indicates that 1 = \u03bb1(2) > \u03bb2(2). Therefore, we only need to prove the second assertion since the first one is a direct consequence. Since W2 (G) is symmetric, we can pick eigenvectors that are orthonormal and select \u03c61 such that:\n\u03c61 = 1\u221a n 1n.\nLet us also define D2 = diag(\u03bb1(2), . . . , \u03bbn(2)) and P = [\u03c61, . . . , \u03c6n]. Thus, we have:\nW2 (G) = PD2P >.\nLet us split D2 by defining Q2 \u2208 Rn\u00d7n and R2 \u2208 Rn\u00d7n such that:{ Q2 = diag(\u03bb1(2), 0, . . . , 0) R2 = diag(0, \u03bb2(2), . . . , \u03bbn(2))\nThen, for any t > 0, we can write: E[Z(t)] = W2 (G)tX = PDt2P>X = P ( Qt2 +R t 2 ) P>X = PQt2P >X + PRt2P >X.\nReminding \u03bb1(2) = 1, the first term can be rewritten:\nPQt2P >X = \u03c61\u03c6 > 1 X =\n1\nn\n( 1>nX ) 1n,\nwhich corresponds to the objective X\u0304n1n. We write |||\u00b7||| the operator norm of a matrix. Since R21n = 0, one has for any t > 0,\n\u2016E[Z(t)]\u2212 X\u0304n1n\u2016 = \u2016W2 (G)tX\u2212 X\u0304n1n\u2016 = \u2225\u2225\u2225\u2225W2 (G)tX\u2212 1n (1>nX)1n \u2225\u2225\u2225\u2225 = \u2225\u2225PRt2P>X\u2225\u2225 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223PRt2P>\u2223\u2223\u2223\u2223\u2223\u2223 \u2016X\u2212 X\u0304n1n\u2016\n\u2264 (\u03bb2(2))t\u2016X\u2212 X\u0304n1n\u2016, and the result holds since 0 \u2264 \u03bb2(2) < 1."}, {"heading": "C Convergence Proofs for GoSta", "text": "C.1 Proof of Theorem 1 (Synchronous Setting)\nOur main goal is to characterize the behavior of S1(t), as it corresponds to the estimates Z(t). As for the standard gossip averaging, our proof relies on the study of eigenvalues and eigenvectors of the transition matrix M(t).\nProof. By definition of M1(t), we have that\nM1(t) = t\u2212 1 t W2 (G) = t\u2212 1 t PD2P >.\nSimilarly, using the fact that C = W1 (G)\u2297 In, we have that\nC = PcDcP > c ,\nwhere Pc = P \u2297 In, D = D1\u2297 In and D1 = diag(\u03bb1(1), . . . , \u03bbn(1)). The expected value of S1(t) can then be rewritten:\nE[S1(t)] = 1\nt t\u2211 s=1 W2 (G) t\u2212s BCs\u22121S2(0) = 1 t P\n( t\u2211\ns=1\nDt\u2212s2 P >BPcD s\u22121 c ) P>c S2(0). (18)\nOur objective is to extract the value U\u0302n(H) from the expression (18) by separating \u03bb2(1) and \u03bb1(1) from other eigenvalues. Let Qc = Q1 \u2297 In and Rc = R1 \u2297 In. We can now write E[S1(t)] = L1(t) + L2(t) + L3(t) + L4(t), where: L1(t) = 1 t \u2211t s=1 PQ t\u2212s 2 P >BPcQcP > c S2(0), L2(t) = 1 t \u2211t s=1 PR t\u2212s 2 P >BPcQcP > c S2(0), L3(t) = 1 t \u2211t s=1 PQ t\u2212s 2 P >BPcR s\u22121 c P > c S2(0),\nL4(t) = 1 t \u2211t s=1 PR t\u2212s 2 P >BPcR s\u22121 c P > c S2(0).\nWe will now show that for any t > 0, L1(t) is actually U\u0302n(H). We have:\nPQ2P > =\n1 n 1n1 > n .\nSimilarly, we have:\nPcQcP > c = (PQ1P >)\u2297 In = 1\nn (1n1\n> n )\u2297 In.\nFinally, we can write:\nL1(t) = PQ2P >BPcQcP > c S2(0) =\n1\nn2 1n1\n> nB ( (1n1 > n )\u2297 In ) S2(0)\n= 1\nn2 1n1\n> n (1 > n \u2297 In)S2(0) =\n1\nn2 1n1\n> n2S2(0) = U\u0302n(H)1n.\nLet us now focus on the other terms. For t > 0, we have:\n\u2016L2(t)\u2016 \u2264 1\nt t\u2211 s=1 \u2225\u2225PRt\u2212s2 P>BPcQcP>c S2(0)\u2225\u2225 = 1\nt t\u2211 s=1 \u2225\u2225\u2225\u2225PRt\u2212s2 P>B( 1n(1n1>n )\u2297 In ) S2(0) \u2225\u2225\u2225\u2225 = 1\nt t\u2211 s=1 \u2225\u2225\u2225\u2225PRt\u2212s2 P>( 1n1>n \u2297 In ) S2(0) \u2225\u2225\u2225\u2225 . One has: \u2225\u2225\u2225\u2225( 1n1>n \u2297 In ) S2(0) \u2225\u2225\u2225\u22252 = n\u2211 i=1 ( 1 n 1>nHei )2 = \u2225\u2225\u2225\u2225 1nH1n \u2225\u2225\u2225\u22252 = \u2016h\u20162.\nTherefore, we obtain:\n\u2016L2(t)\u2016 \u2264 1\nt t\u2211 s=1 \u2225\u2225PRt\u2212s2 P>h\u2225\u2225 . By definition, for any t \u2265 s > 0, PRt\u2212s2 P>1n = 0. Therefore, one has:\n\u2016L2(t)\u2016 \u2264 1\nt t\u2211 s=1 \u2225\u2225PRt\u2212s2 P>h\u2225\u2225 \u2264 1t t\u2211 s=1 (\u03bb2(2)) t\u2212s\u2016h\u2212 U\u0302n(H)1n\u2016\n\u2264 1 t \u00b7 1 1\u2212 \u03bb2(2) \u2016h\u2212 U\u0302n(H)1n\u2016,\nsince 1nh1n = U\u0302n(H)1n. Similarly, one has:\n\u2016L3(t)\u2016 \u2264 1 t \u00b7 1 1\u2212 \u03bb2(1) \u2016H\u2212 1>nh\u2016,\nby definition of h and using PRcP>1n2 = 0. The final term can be upper bounded similarly to previous proofs:\n\u2016L4(t)\u2016 \u2264 1\nt t\u2211 s=1 \u2225\u2225PRt\u2212s2 P>BPcRscP>c S2(0)\u2225\u2225 \u2264 1 t t\u2211 s=1 \u2225\u2225\u2225\u2225PRt\u2212s2 P>BPcRscP>c (S2(0)\u2212 1n1>nS2(0) )\u2225\u2225\u2225\u2225\n\u2264 1 t\n( t\u2211\ns=1\n(\u03bb2(2)) t\u2212s\u03bb2(1) s ) \u2016H\u2212 1>nh\u2016.\nLemma 1 indicates that \u03bb2(2) > \u03bb2(1), so\nL4(t) \u2264 (\u03bb2(2))t\u2016H\u2212 1>nh\u2016. Using Lemma 1 and above inequalities, one can finally write:\u2225\u2225\u2225S1(t)\u2212 U\u0302n(H)1n\u2225\u2225\u2225 \u2264 \u2016L2(t)\u2016+ \u2016L3(t)\u2016+ \u2016L4(t)\u2016\n\u2264 c t \u2016h\u2212 U\u0302n(H)1n\u2016+\n( 2\nct + e\u2212ct\n) \u2016H\u2212 1>nh\u2016,\nwith c = 1\u2212 \u03bb2(2).\nC.2 Proof of Theorem 2 (Asynchronous Setting)\nFor t > 0, let us denote as M(t) the expected transition matrix at iteration t. With the notation introduced in the synchronous setting, it yields(\nM1(t) M2(t) 0 C\n) .\nThe propagation step is unaltered w.r.t. the synchronous case, thus the bottom right block is unmodified. On the other hand, both the transmission step and the averaging step differ: only the selected nodes update their estimators from their associated phantom graph. Therefore, we have:\nM2(t) = 1 |E| \u2211\n(i,j)\u2208E\nE  I{ 1\u2208(i,j) } 1 m1(t)p1 e>1 0 \u00b7 \u00b7 \u00b7 0 0 . . . ... ...\n. . . 0 0 \u00b7 \u00b7 \u00b7 0 I{ n\u2208(i,j) } 1 mn(t)pn e>n\n .\nFor any k \u2208 [n] and t > 0, mk(t) is an unbiased estimator of t. Moreover, \u2211 (i,j)\u2208E I{k\u2208(i,j)} = 2dk. Therefore, we can write:\nM2(t) = 1\nt|E|  2d1 p1 e>1 0 \u00b7 \u00b7 \u00b7 0 0 . . . ... ...\n. . . 0 0 \u00b7 \u00b7 \u00b7 0 2dnpn e > n\n = 1t  e>1 0 \u00b7 \u00b7 \u00b7 0 0 . . . ... ...\n. . . 0 0 \u00b7 \u00b7 \u00b7 0 e>n  = Bt . Similarly for M1(t):\nM1(t) = W2(G)\u2212 1 2t|E| \u2211\n(i,j)\u2208E\n( 1\npi ei(ei + ej)\n> + 1\npj ej(ei + ej)\n> ) .\nUsing the definition of (pk)k\u2208[n] yields:\nM1(t) = W2(G)\u2212 1\n2t\n( In +D(G) \u22121A(G) ) .\nWe can now write the expected value of the state vector S(t) similarly to the synchronous setting: E[S(t)] = ( S1(t) S2(t) ) = (\u2211t s=1 (M1(t) . . .M1(s+ 1)) B s C s\u22121S2(0) CtS2(0) ) .\nAs in the synchronous setting, our proof rely on the eigenvalues of M(t).\nProof. For t > 0, we have:\nM1(t) = W2 (G)\u2212 1\n2t\n( In +D \u22121(G)A(G) ) = W2 (G)\u2212 1\nt In +\n1 2t D(G)\u22121L(G).\nSince M1(t)1n = ( 1 \u2212 1t ) 1n, we have |||M1(t)||| \u2265 1 \u2212 1t . Let us denote Sp ( L(G) ) = { \u03b2 \u2208\nR, \u2203\u03c6 \u2208 Rn, L(G)\u03c6 = \u03b2\u03c6 } . Let \u03b2 \u2208 Sp ( L(G) ) and \u03c6 \u2208 Rn a corresponding eigenvector. One can write:\nM1(t)\u03c6 = ( W2 (G)\u2212 1\nt In +\n1 2t D(G)L(G)\u22121\n) \u03c6 = ( 1\u2212 \u03b2 |E| \u2212 1 t ) \u03c6+ \u03b2 2t D(G)\u22121\u03c6\n= (( 1\u2212 1\nt\n) In \u2212 \u03b2\n|E|\n( In \u2212 |E|D(G)\u22121\n2t\n)) \u03c6.\nThe above matrix is diagonal, therefore we can write:\n\u2016M1(t)\u03c6\u2016 \u2264 max i\n( 1\u2212 1\nt \u2212 \u03b2 |E|\n( 1\u2212 |E|\n2dit\n)) \u2016\u03c6\u2016 = ( 1\u2212 1\nt \u2212 \u03b2 |E|\n( 1\u2212 1\npt\n)) \u2016\u03c6\u2016,\nwhere p = mini 2di|E| is the minimum probability of a node being picked at any iteration. Thus, we can see that if \u03b2 > 0, \u2016M1(t)\u03c6\u2016 < (1 \u2212 1t )\u2016\u03c6\u2016 if t < tc = 1 p . Consequently, if t \u2265 tc then |||M1(t)||| = 1 \u2212 1t . Here, tc represents the minimum number of iteration needed for every node to have been picked at least once, in expectation.\nLet (\u03b21, . . . , \u03b2n) \u2208 R and P = (\u03c61, . . . , \u03c6n) \u2208 Rn\u00d7n be respectively the eigenvalues and eigenvectors of L(G) (sorted in decreasing order), such that P is the same matrix than the one introduced in Section B. We have:\nM1(t)P = PK(t) = P\n(( 1\u2212 1\nt\n) In \u2212 1\n|E|\n( In \u2212\n|E| 2t P>D(G)\u22121P\n) DL(G) ) ,\nwhere DL(G) = diag(\u03b2n, . . . , \u03b21). Let P1 = ( \u03c61, 0, . . . , 0 ) . The matrix K(t) can be rewritten as follows:\nK(t) = ( 1\u2212 1\nt\n) In \u2212 1\n|E|\n( In \u2212\n|E| 2t P>D(G)\u22121P\n) DL(G) = ( 1\u2212 1\nt\n) Q+ 1\nt U +R(t),\nwhere Q, U and R(t) are defined by: Q = diag(1, 0, . . . , 0), U = 12P > 1 D(G)\n\u22121PDL(G), R(t) = K(t)\u2212 ( 1\u2212 1t ) Q\u2212 1tU , for all t > 0.\nUsing the fact that \u03b2n = 0, one can show that U has the form 0 \u2217 \u00b7 \u00b7 \u00b7 \u2217 0\n0... 0  . Since M1(t)1n = ( 1\u2212 1t ) 1n, we can also show that, for t > 0, R(t)e1 = 0n and e>1 R(t) = 0 > n .\nLet t > 0. We can write:\nM1(t+ 1)M1(t) = PK(t+ 1)K(t)P >\n= P\n( t\nt+ 1 Q+\n1\nt+ 1 U +R(t+ 1) )( t\u2212 1 t Q+ 1 t U +R(t) ) P>\n= P ( t\u2212 1 t+ 1 Q+ 1 t+ 1 U ( In +R(t) ) +R(t+ 1)R(t) ) P>.\nRecursively, we obtain, for t > s > 0:\nM1(t : s) = M1(t) . . .M1(s+ 1) = P\n( s\nt Q+\n1 t U t\u22121\u2211 r=s R(r : s) +R(t : s)\n) P>,\nwhere we use the convention R(t\u2212 1 : t\u2212 1) = In. Let us now write the expected value of the estimates:\nE[S1(t)] = t\u2211\ns=1\nM1(t : s) B\ns Cs\u22121S2(0)\n= t\u2211 s=1 M1(t : s) B s PcQcP > c S2(0) + t\u2211 s=1 M1(t : s) B s PcR s\u22121 c P > c S2(0)\n= t\u2211 s=1 M1(t : s) h s + t\u2211 s=1 M1(t : s) B s PcR s\u22121 c P > c S2(0).\nThe first term can be rewritten as: t\u2211\ns=1\nM1(t : s) h\ns = t\u2211 s=1 P\n( s\nt Q+\nU\nt t\u22121\u2211 r=s R(r : s) +R(t : s)\n) P> h\ns\n= 1\nt t\u2211 s=1 PQP>h + 1 t t\u2211 s=1 PU t\u22121\u2211 r=s R(r : s)P> h s + t\u2211 s=1 PR(t : s)P> h s\n= U\u0302n(H)1n + 1\nt t\u2211 s=1 PU t\u22121\u2211 r=s R(r : s)P> h s + t\u2211 s=1 PR(t : s)P> h s\n= U\u0302n(H)1n + L1(t) + L2(t).\nThe second term of the expected estimates can be rewritten as: t\u2211 s=1 M1(t : s) B s PcR s\u22121 c P > c S2(0) = t\u2211 s=1 P ( s t Q+ U t t\u22121\u2211 r=s R(r : s) +R(t : s) ) P> B s PcR s\u22121 c P > c S2(0)\n= L3(t) + L4(t) + L5(t).\nNow, we need to upper bound \u2016Li(t)\u2016 for 1 \u2264 i \u2264 5. One has:\n\u2016L1(t)\u2016 = \u2225\u2225\u2225\u2225\u22251t t\u2211\ns=1\nPU t\u22121\u2211 r=s R(r : s)P> h s \u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225\u22251t t\u2211 s=1 PU t\u22121\u2211 r=s R(r : s)P> ( h\u2212 U\u0302n(H)1n ) s \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 1 t t\u2211 s=1 \u2225\u2225\u2225\u2225\u2225\u2225PU t\u22121\u2211 r=s R(r : s)P> ( h\u2212 U\u0302n(H)1n ) s\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 |||U |||\nt\n( t\u2211\ns=1\n1\ns t\u22121\u2211 r=s |||R(r : s)|||\n) \u2016h\u2212 U\u0302n(H)1n\u2016.\nThe norm of U can be developed:\n|||U ||| \u2264 1 2 \u2223\u2223\u2223\u2223\u2223\u2223D(G)\u22121\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223DL(G)\u2223\u2223\u2223\u2223\u2223\u2223 = \u03b21|E|p . Moreover, for 2 \u2264 i \u2264 n, one has:\n\u2016R(t)ei\u2016 = \u2225\u2225\u2225\u2225(1\u2212 1t ) ei \u2212 \u03b2n\u2212i+1 |E| ei + \u03b2n\u2212i+1 2t P>2:D(G) \u22121\u03c6i \u2225\u2225\u2225\u2225 \u2264 ((\n1\u2212 1 t ) \u2212 \u03b2n\u2212i+1 |E| ) \u2016ei\u2016+ \u2225\u2225\u2225\u2225\u03b2n\u2212i+12t P>2:D(G)\u22121\u03c6i \u2225\u2225\u2225\u2225\n\u2264 ((\n1\u2212 1 t ) \u2212 \u03b2n\u2212i+1 |E| ( 1\u2212 1 pt )) \u2016ei\u2016.\nFor t > 0, let us define \u00b5R(t) by:\n\u00b5R(t) =\n( 1\u2212 1\nt ) \u2212 \u03b2n\u22121 |E| ( 1\u2212 1 pt ) .\nWe then have, for any t > 0, |||R(t)||| < \u00b5R(t). Thus,\n\u2016L1(t)\u2016 \u2264 \u03b21 |E|pt\n( t\u2211\ns=1\n1\ns t\u22121\u2211 r=s \u00b5R(r : s)\n) \u2016h\u2212 U\u0302n(H)1n\u2016.\nAlso,\n\u2016L2(t)\u2016 = \u2225\u2225\u2225\u2225\u2225 t\u2211\ns=1\nPR(t : s)P> h\u2212 U\u0302n(H)1n\ns\n\u2225\u2225\u2225\u2225\u2225 \u2264 t\u2211\ns=1\n\u00b5R(t : s)\ns \u2016h\u2212 U\u0302n(H)1n\u2016\nA reasoning similar to the synchronous setting can be applied to L3(t):\n\u2016L3(t)\u2016 = 1\nt \u2225\u2225\u2225\u2225\u2225 t\u2211\ns=1\nPQP> B\ns PcR\ns\u22121 c P > c h \u2225\u2225\u2225\u2225\u2225 \u2264 1t \u00b7 11\u2212 \u03bb2(1)\u2016H\u2212 h1>n \u2016. Concerning L4(t), one can write:\n\u2016L4(t)\u2016 = 1\nt \u2225\u2225\u2225\u2225\u2225 t\u2211\ns=1\n( U\nt\u22121\u2211 r=s R(r : s)\n) B\ns PcR\ns\u22121 c P > c S2(0) \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b21 |E|pt t\u2211 s=1 1 s ( t\u22121\u2211 r=s \u00b5R(r : s) ) (\u03bb2(1)) s\u22121\u2016H\u2212 h1>n \u2016.\nSimilarly, one has:\n\u2016L5(t)\u2016 \u2264 \u2016H\u2212 h1>n \u2016 t\u2211\ns=1\n\u00b5R(t : s)\ns (\u03bb2(1))\ns\u22121.\nNow, for t > s > 1, one only need to find appropriate rates on \u2211t s=1\n1 s\u00b5R(t : s) and\u2211t\ns=1 1 s \u2211t\u22121 r=s \u00b5R(r : s) to conclude. Here, for t > 1, \u00b5R(t) can be rewritten as follow:\n\u00b5R(t) = ( t\u2212 1 t ) \u03bb2(1) ( 1 + (1\u2212 \u03bb2(1)) c t ) ,\nwith c = 1\u03bb2(1)p \u2212 1. If c < 1, one cas use a reasoning similar to the synchronous setting and conclude. However, c is often greater than 1. In this case, one has:\n\u00b5R(t) \u2264 ( t\u2212 1 t ) \u03bb2(1) ( 1 + c t ) .\nFor t > s > 0, the product \u00b5R(t : s) can then be bounded as follow:\n\u00b5R(t : s) \u2264 s\nt \u03bb2(1)\nt\u2212s (\n1 + c\nt\u2212 1\n) . . . ( 1 + c\ns\n) .\nUsing the definition of tc, it is clear that, for t \u2265 tc, one has \u03bb2(1)(1 + ct\u22121 ) < 1. We can use this result to upper bound \u2211t s=1 \u00b5R(t:s) s with a geometric series:\nt\u2211 s=1 1 s \u00b5R(t : s) \u2264 1 t t\u2211 s=1 \u03bb2(1) t\u2212s ( 1 + c t\u2212 1 ) . . . ( 1 + c s ) \u2264 1 t t\u2211 s=tc+1 \u03bb2(1) t\u2212s ( 1 + c tc )t\u2212s + 1 t tc\u2211 s=1 \u03bb2(1) t\u2212s ( 1 + c t\u2212 1 ) . . . ( 1 + c s\n) \u2264 1 t \u00b7 1 1\u2212 \u00b5c + tc t (1 + c)tce\u2212(1\u2212\u03bbc)(t\u2212tc),\nwhere \u00b5c = \u03bb2(1) (\n1 + ctc\n) . Therefore, we have that \u2211t s=1 1 s\u00b5R(t : s) = O(1/t). Let us now\nfocus on the second bound. For t > tc and 1 < s < t, one has: t\u2211\ns=1\n1\ns t\u22121\u2211 r=s \u00b5R(r : s) \u2264 tc\u2211 s=1 1 s tc\u2211 r=s \u00b5R(r : s) + tc\u2211 s=1 1 s t\u22121\u2211 r=tc+1 \u00b5R(r : s) + t\u2211 s=tc+1 1 s t\u22121\u2211 r=s \u00b5R(r : s)\n\u2264 tc\u2211 s=1 tc\u2211 r=s 1 r \u03bb2(2) r\u2212s(1 + c)r\u2212s + tc\u2211 s=1 t\u22121\u2211 r=tc+1 \u00b5r\u2212sc r + t\u2211 s=tc+1 t\u22121\u2211 r=s \u00b5r\u2212sc r\n\u2264 tc tc\u2211 r=1 1 r \u03bb2(2) r(1 + c)r + tc\u00b5 \u2212tc c t\u22121\u2211 r=tc+1 \u00b5rc r + t\u2211 s=1 1 s s\u22121\u2211 r=0 \u00b5rc\n\u2264 tc(1 + c)tc \u2212 tc\u00b5\u2212tcc log(1\u2212 \u00b5c) + 1\n1\u2212 \u00b5c t\u2211 s=1 1 s\n\u2264 tc(1 + c)tc + tc\u00b5\nt\u2212c c\n1\u2212 \u00b5c +\n1\n1\u2212 \u00b5c log(t+ 1).\nAlgorithm 4 U2-gossip [18] Require: Each node k holds observation Xk\n1: Each node initializes Y (1)k \u2190 Xk, Y (2) k \u2190 Xk, Zk \u2190 0 2: for t = 1, 2, . . . do 3: for p = 1, . . . , n do 4: Zp \u2190 t\u22121t Zp + 1 tH(Y (1) p , Y (2) p ) 5: end for 6: Draw (i, j) uniformly at random from E 7: Nodes i and j swap their first auxiliary observations: Y (1)i \u2194 Y (1) j 8: Draw (k, l) uniformly at random from E 9: Nodes k and l swap their second auxiliary observations: Y (2)k \u2194 Y (2) l\n10: end for\nThus, \u2211t s=1 1 s \u2211t\u22121 r=s \u00b5R(r : s) = O(log t).\nUsing these results and the previous expressions of L1(t), . . . , L5(t), one can conclude that, for t > 1, \u2016E[Z(t)]\u2212 U\u0302n(H)1n\u2016 = O(log t/t)."}, {"heading": "D U2-gossip Algorithm", "text": "U2-gossip [18] is an alternative approach for computing U -statistics. In this algorithm, each node stores two auxiliary observations that are propagated using independent random walks. These two auxiliary observations will be used for estimating the U -statistic \u2013 see Algorithm 4 for details. This algorithm has an O(1/t) convergence rate, as stated in Theorem 4.\nLet k \u2208 [n]. At iteration t = 1, the auxiliary observations have not been swapped yet, so the expected estimator E[Zk] is simply updated as follow:\nE[Zk(1)] = E[Zk(0)] + e>kHek.\nThen, at the end of the iteration, auxiliary observations are randomly swapped. Therefore, one has:\nE[Zk(2)] = 1\n2 E[Zk(1)] +\n1\n2\n( W1 (G) e > k ) HW1 (G) ek.\nUsing recursion, we can write, for any t > 0 and any k \u2208 [n]:\nE[Zk(t)] = t\u22121\u2211 s=0 e>kW1 (G) s HW1 (G) s ek. (19)\nWe can now state a convergence result for Algorithm 4. Theorem 4. Let us assume that G is connected and non bipartite. Then, for Z(t) defined in Algorithm 4, we have that for all k \u2208 [n]:\nlim t\u2192+\u221e\nE[Zk(t)] = 1\nn2 \u2211 1\u2264i,j\u2264n H(Xi, Xj) = U\u0302n(H)\nMoreover, for any t > 0,\u2225\u2225\u2225E[Z(t)]\u2212 U\u0302n(H)1n\u2225\u2225\u2225 \u2264 \u221an t ( 2 1\u2212 \u03bb2(1) \u2016h\u2212 U\u0302n(H)1n\u2016+\n1\n1\u2212 \u03bb2(1)2 \u2016H\u2212 h1>n \u2016\n) ,\nwhere \u03bb2(1) is the second largest eigenvalue of W1 (G).\nProof. Let k \u2208 [n] and t > 0. Using the expression of E[Zk(t)] established in (19), one has:\nE[Zk(t)] = 1\nt t\u2211 s=0 e>kW1 (G) s HW1 (G) s ek = 1 t t\u2211 s=0 e>k P >Ds1PHPD s 1P >ek,\nwhere P is the eigenvectors matrix introduced in Section B and D1 = diag(\u03bb1(1), . . . , \u03bbn(1)). Similarly to previous proofs, we split D1 = Q1 + P1 where Q1 = diag(1, 0, . . . , 0) and R1 = diag(0, \u03bb2(1), . . . , \u03bbn(1)). Now, we can write E[Zk(t)] = L1(t) + L2(t) + L3(t) + L4(t) with L1(t), L2(t), L3(t) and L4(t) defined as follows: L1(t) = 1 t \u2211t s=1 e > k P >Qs1PHPQ s 1P >ek L2(t) = 1 t \u2211t s=1 e > k P >Rs1PHPQ s 1P >ek L3(t) = 1 t \u2211t s=1 e > k P >Qs1PHPR s 1P >ek\nL4(t) = 1 t \u2211t s=1 e > k P >Rs1PHPR s 1P >ek .\nThe first term can be rewritten:\nL1(t) = e > k P >Q1PHPQ1P >ek = 1\nn2 1>nH1n = U\u0302n(H).\nThen, one has:\n|L2(t)| \u2264 1\nt t\u2211 s=0 \u2016e>k PRs1P>HPQ1P>ek\u2016 \u2264 1 t t\u2211 s=0 \u2016PRs1P>h\u2016\n\u2264 1 t t\u2211 s=0 (\u03bb2(1)) s\u2016h\u2212 U\u0302n(H)1n\u2016 \u2264 1 t \u00b7 1 1\u2212 \u03bb2(1) \u2016h\u2212 U\u0302n(H)1n\u2016,\nsince \u03bb2(1) < 1. Similarly, we have |L3(t)| \u2264 1t \u00b7 \u03bb2(1) 1\u2212\u03bb2(1)\u2016h \u2212 U\u0302n(H)1n\u2016. The final term L4(t) can be bounded as follow:\nL4(t) \u2264 1\nt t\u2211 s=0 \u2223\u2223e>k PRs1P>HPQ1P>ek\u2223\u2223 = 1t t\u2211 s=0 \u2223\u2223\u2223e>k PRs1P> (H\u2212 1nh>)PQ1P>ek\u2223\u2223\u2223 \u2264 1 t t\u2211 s=0 (\u03bb2(1)) 2s \u2225\u2225\u2225H\u2212 1nh>\u2225\u2225\u2225 \u2264 1 t \u00b7 1 1\u2212 (\u03bb2(1))2 \u2225\u2225\u2225H\u2212 1nh>\u2225\u2225\u2225 .\nWith above relations, the expected difference can be bounded as follow:\u2223\u2223\u2223E[Zk(t)]\u2212 U\u0302n(H)\u2223\u2223\u2223 \u2264 |L2(t)|+ |L3(t)|+ |L4(t)| \u2264 1 t \u00b7 2 1\u2212 \u03bb2(1) \u2225\u2225\u2225h\u2212 U\u0302n(H)1n\u2225\u2225\u2225+ 1 t \u00b7 1 1\u2212 (\u03bb2(1))2 \u2225\u2225\u2225H\u2212 1nh>\u2225\u2225\u2225 .\nFinally, we can conclude:\u2225\u2225\u2225E[Z(t)]\u2212 U\u0302n(H)\u2225\u2225\u2225 \u2264 \u221anmax k\u2208[n] \u2223\u2223\u2223E[Zk(t)]\u2212 U\u0302n(H)\u2223\u2223\u2223 \u2264 \u221a n\nt \u00b7 2 1\u2212 \u03bb2(1) \u2225\u2225\u2225h\u2212 U\u0302n(H)1n\u2225\u2225\u2225+ \u221an t \u00b7 1 1\u2212 (\u03bb2(1))2 \u2225\u2225\u2225H\u2212 1nh>\u2225\u2225\u2225 ."}, {"heading": "E Comparison to Baseline Methods", "text": "In this section, we use the within-cluster point scatter problem studied in Section 5 to compare our algorithms to two \u2014 more naive \u2014 baseline methods, described below.\nGossip-flooding baseline. This baseline uses the same communication scheme than GoSta-async (Algorithm 2) to flood observations across the network, but we assume that each node has enough memory to store all the observations it receives. At each iteration, each selected node picks a random observation among those it currently holds and send it to the other (tagged with the node which initially possessed it, to avoid storing duplicates). The local estimates are computed using the subset of observations available at each node (the averaging step is removed).\nFigure 4 shows the evolution over time of the average relative error and the associated standard deviation across nodes for this baseline and GoSta-async on the networks introduced in Section 5. On average, GoSta-async slightly outperforms Gossip-flooding, and this difference gets larger as the network connectivity decreases. The variance of the estimates across nodes is also lower for GoStaasync. This confirms the interest of averaging the estimates, and shows that assuming large memory at each node is not necessary to achieve good performance. Finally, note that updating the local estimate of a node is computationally much cheaper in GoSta-async (only one function evaluation) than in Gossip-flooding (as many function evaluations as there are observations on the node).\nMaster-node baseline. This baseline has access to a master nodeM which is connected to every other node in the network. Initially, at t = 0, each node i \u2208 [n] sends its observation Xi to M. Then, at each iteration t \u2208 [n],M sends observationXt to every node of the network. As in Gossipflooding, the estimates are computed using the subset of observations available at each node. The performance of this baseline does not depend on the original network, since communication goes through the master-nodeM. This allows us to compare our approach to the ideal scenario of a \u201cstar\u201d network, where a central node can efficiently broadcast information to the entire network.\nFor a fair comparison with GoSta-async, we evaluate the methods with respect to the communication cost instead of the number of iterations. Figure 5 shows the evolution of the average relative error for this baseline and GoSta-async. We can see that the Master-node baseline performs better early on, but GoSta-async quickly catches up (the better the connectivity, the sooner). This shows that our data propagation and averaging mechanisms compensate well for the lack of central node."}], "references": [{"title": "Modern Graph Theory, volume 184", "author": ["B\u00e9la Bollob\u00e1s"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Randomized gossip algorithms", "author": ["Stephen P. Boyd", "Arpita Ghosh", "Balaji Prabhakar", "Devavrat Shah"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Markov chains: Gibbs fields, Monte Carlo simulation, and queues, volume 31", "author": ["Pierre Bremaud"], "venue": "Springer Science & Business Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Spectral Graph Theory, volume 92", "author": ["Fan R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "On U-processes and clustering performance", "author": ["St\u00e9phan Cl\u00e9men\u00e7on"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Gossip Algorithms for Distributed Signal Processing", "author": ["Alexandros G. Dimakis", "Soummya Kar", "Jos\u00e9 M.F. Moura", "Michael G. Rabbat", "Anna Scaglione"], "venue": "Proceedings of the IEEE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Geographic Gossip: Efficient Averaging for Sensor Networks", "author": ["Alexandros G. Dimakis", "Anand D. Sarwate", "Martin J. Wainwright"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling", "author": ["John C. Duchi", "Alekh Agarwal", "Martin J. Wainwright"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "The meaning and use of the area under a receiver operating characteristic (ROC) curve", "author": ["James A. Hanley", "Barbara J. McNeil"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1982}, {"title": "Randomized rumor spreading", "author": ["Richard Karp", "Christian Schindelhauer", "Scott Shenker", "Berthold Vocking"], "venue": "In Symposium on Foundations of Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Gossip-Based Computation of Aggregate Information", "author": ["David Kempe", "Alin Dobra", "Johannes Gehrke"], "venue": "In Symposium on Foundations of Computer Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "U-Statistics: Theory and Practice", "author": ["Alan J. Lee"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Location-Aided Fast Distributed Consensus in Wireless Networks", "author": ["Wenjun Li", "Huaiyu Dai", "Yanbing Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other", "author": ["Henry B. Mann", "Donald R. Whitney"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1947}, {"title": "Fast distributed algorithms for computing separable functions", "author": ["Damon Mosk-Aoyama", "Devavrat Shah"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Distributed subgradient methods for multi-agent optimization", "author": ["Angelia Nedic", "Asuman Ozdaglar"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Gossip Algorithms for Computing U-Statistics", "author": ["Kristiaan Pelckmans", "Johan Suykens"], "venue": "In IFAC Workshop on Estimation and Control of Networked Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Problems in decentralized decision making and computation", "author": ["John N. Tsitsiklis"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1984}, {"title": "Collective dynamics of \u2018small-world\u2019networks", "author": ["Duncan J Watts", "Steven H Strogatz"], "venue": "Nature, 393(6684):440\u2013442,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}], "referenceMentions": [{"referenceID": 17, "context": "Gossip algorithms [20, 19, 6], where each node exchanges information with at most one of its neighbors at a time, have emerged as a simple yet powerful technique for distributed computation in such settings.", "startOffset": 18, "endOffset": 29}, {"referenceID": 5, "context": "Gossip algorithms [20, 19, 6], where each node exchanges information with at most one of its neighbors at a time, have emerged as a simple yet powerful technique for distributed computation in such settings.", "startOffset": 18, "endOffset": 29}, {"referenceID": 10, "context": "Given a data observation on each node, gossip algorithms can be used to compute averages or sums of functions of the data that are separable across observations (see for example [11, 2, 16, 12, 10] and references therein).", "startOffset": 178, "endOffset": 197}, {"referenceID": 1, "context": "Given a data observation on each node, gossip algorithms can be used to compute averages or sums of functions of the data that are separable across observations (see for example [11, 2, 16, 12, 10] and references therein).", "startOffset": 178, "endOffset": 197}, {"referenceID": 14, "context": "Given a data observation on each node, gossip algorithms can be used to compute averages or sums of functions of the data that are separable across observations (see for example [11, 2, 16, 12, 10] and references therein).", "startOffset": 178, "endOffset": 197}, {"referenceID": 9, "context": "Given a data observation on each node, gossip algorithms can be used to compute averages or sums of functions of the data that are separable across observations (see for example [11, 2, 16, 12, 10] and references therein).", "startOffset": 178, "endOffset": 197}, {"referenceID": 11, "context": "Unfortunately, these algorithms cannot be used to efficiently compute quantities that take the form of an average over pairs of observations, also known as U -statistics [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "tau rank correlation coefficient, the within-cluster point scatter and several statistical hypothesis test statistics such as Wilcoxon Mann-Whitney [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 3, "context": "These convergence bounds feature datadependent terms that reflect the hardness of the estimation problem, and network-dependent terms related to the spectral gap of the network graph [4], showing that our algorithms are faster on wellconnected networks.", "startOffset": 183, "endOffset": 186}, {"referenceID": 16, "context": "Our results largely improve upon those presented in [18]: in particular, we achieve faster convergence together with lower memory and communication costs.", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": "We consider the problem of estimating the following quantity, known as a degree two U -statistic [13]:1", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "The first one is the within-cluster point scatter [5], which measures the clustering quality of a partition We point out that the usual definition of U -statistic differs slightly from (1) by a factor of n/(n\u2212 1).", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "We also study the AUC measure [9].", "startOffset": 30, "endOffset": 33}, {"referenceID": 17, "context": "One of the earliest work on this canonical problem is due to [20], but more efficient algorithms have recently been proposed, see for instance [11, 2].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "One of the earliest work on this canonical problem is due to [20], but more efficient algorithms have recently been proposed, see for instance [11, 2].", "startOffset": 143, "endOffset": 150}, {"referenceID": 1, "context": "One of the earliest work on this canonical problem is due to [20], but more efficient algorithms have recently been proposed, see for instance [11, 2].", "startOffset": 143, "endOffset": 150}, {"referenceID": 1, "context": "Of particular interest to us is the work of [2], which introduces a randomized gossip algorithm for computing the empirical mean (4) in a context where nodes wake up asynchronously and simply average their local estimate with that of a randomly chosen neighbor.", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "As long as the network graph is connected and non-bipartite, the local estimates converge to (4) at a rate O(e\u2212ct) where the constant c can be tied to the spectral gap of the network graph [4], showing faster convergence for well-connected networks.", "startOffset": 189, "endOffset": 192}, {"referenceID": 14, "context": "3 Such algorithms can be extended to compute other functions such as maxima and minima, or sums of the form \u2211n i=1 f(Xi) for some function f : X \u2192 R (as done for instance in [16]).", "startOffset": 174, "endOffset": 178}, {"referenceID": 6, "context": "Some work has also gone into developing faster gossip algorithms for poorly connected networks, assuming that nodes know their (partial) geographic location [7, 14].", "startOffset": 157, "endOffset": 164}, {"referenceID": 12, "context": "Some work has also gone into developing faster gossip algorithms for poorly connected networks, assuming that nodes know their (partial) geographic location [7, 14].", "startOffset": 157, "endOffset": 164}, {"referenceID": 5, "context": "For a detailed account of the literature on gossip algorithms, we refer the reader to [19, 6].", "startOffset": 86, "endOffset": 93}, {"referenceID": 16, "context": "To the best of our knowledge, this problem has only been investigated in [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "The value \u03b2n\u22121 is also known as the spectral gap of G and graphs with a larger spectral gap typically have better connectivity [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 16, "context": "To estimate \u00dbn(H), U2-gossip [18] does not use averaging.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "so one can use an equivalent model with a global clock ticking at a rate n Poisson process and a random edge draw at each iteration, as in synchronous setting (one may refer to [2] for more details on clock modeling).", "startOffset": 177, "endOffset": 180}, {"referenceID": 0, "context": "Hence for all k \u2208 [n], let pk \u2208 [0, 1] denote the probability of node k being picked at any iteration.", "startOffset": 32, "endOffset": 38}, {"referenceID": 16, "context": "We compare the performance of our algorithms to that of U2-gossip [18] \u2014 see supplementary material for additional comparisons to some baseline methods.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "\u2022 Watts-Strogatz: This random network generation technique is introduced in [21] and allows us to create networks with various communication properties.", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "In future work, we plan to investigate whether adaptive communication schemes (such as those of [7, 14]) can be used to speed-up our algorithms.", "startOffset": 96, "endOffset": 103}, {"referenceID": 12, "context": "In future work, we plan to investigate whether adaptive communication schemes (such as those of [7, 14]) can be used to speed-up our algorithms.", "startOffset": 96, "endOffset": 103}, {"referenceID": 7, "context": "Our contribution could also be used as a building block for decentralized optimization of U -statistics, extending for instance the approaches of [8, 17].", "startOffset": 146, "endOffset": 153}, {"referenceID": 15, "context": "Our contribution could also be used as a building block for decentralized optimization of U -statistics, extending for instance the approaches of [8, 17].", "startOffset": 146, "endOffset": 153}, {"referenceID": 0, "context": "References [1] B\u00e9la Bollob\u00e1s.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Stephen P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Pierre Bremaud.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Fan R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] St\u00e9phan Cl\u00e9men\u00e7on.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Alexandros G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Alexandros G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] John C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] James A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Richard Karp, Christian Schindelhauer, Scott Shenker, and Berthold Vocking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] David Kempe, Alin Dobra, and Johannes Gehrke.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Alan J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Wenjun Li, Huaiyu Dai, and Yanbing Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Henry B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Damon Mosk-Aoyama and Devavrat Shah.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Angelia Nedic and Asuman Ozdaglar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Kristiaan Pelckmans and Johan Suykens.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] John N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Duncan J Watts and Steven H Strogatz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Algorithm 3 Gossip algorithm proposed in [2] to compute the standard mean (4) Require: Each node i holds observation Xi 1: Each node initializes its estimator Zi \u2190 Xi 2: for t = 1, 2, .", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "In this part, we provide a description and analysis of the randomized gossip algorithm proposed in [2] for the standard averaging problem (4).", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "We can now state a convergence result for Algorithm 3, rephrasing slightly the results from [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "Algorithm 4 U2-gossip [18] Require: Each node k holds observation Xk 1: Each node initializes Y (1) k \u2190 Xk, Y (2) k \u2190 Xk, Zk \u2190 0 2: for t = 1, 2, .", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "U2-gossip [18] is an alternative approach for computing U -statistics.", "startOffset": 10, "endOffset": 14}], "year": 2015, "abstractText": "Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of U statistics, relying on more expensive averaging over pairs of observations, is a less investigated area. Yet, such data functionals are essential to describe global properties of a statistical population, with important examples including Area Under the Curve, empirical variance, Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the U -statistic of interest. We establish convergence rate bounds of O(1/t) and O(log t/t) for the synchronous and asynchronous cases respectively, where t is the number of iterations, with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.", "creator": "LaTeX with hyperref package"}}}