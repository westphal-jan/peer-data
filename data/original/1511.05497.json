{"id": "1511.05497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Learning Neural Network Architectures using Backpropagation", "abstract": "Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.", "histories": [["v1", "Tue, 17 Nov 2015 18:26:11 GMT  (104kb,D)", "http://arxiv.org/abs/1511.05497v1", "ICLR 2016 submission"], ["v2", "Tue, 2 Aug 2016 11:46:48 GMT  (101kb,D)", "http://arxiv.org/abs/1511.05497v2", "BMVC 2016 ; Title modified from 'Learning the Architecture of Deep Neural Networks'"]], "COMMENTS": "ICLR 2016 submission", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["suraj srinivas", "r venkatesh babu"], "accepted": false, "id": "1511.05497"}, "pdf": {"name": "1511.05497.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Suraj Srinivas"], "emails": ["{surajsrinivas@ssl.,", "venky@}serc.iisc.in"], "sections": [{"heading": null, "text": "Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy."}, {"heading": "1 INTRODUCTION", "text": "Everything should be made as simple as possible, but not simpler - Albert Einstein\nFor large-scale tasks like image classification, the general practice has been to train large networks with many millions of parameters (see Krizhevsky et al. (2012); Simonyan & Zisserman (2015); Szegedy et al. (2015)). It is natural to ask - are all of these parameters really needed for good performance? In other words, are these models as simple as they can be? A smaller model has the advantage of being faster to evaluate and easier to store - both of which are crucial for real-time and embedded applications. In this work, we consider building smaller networks that achieve similar performance.\nOften, regularizers are used to encourage learning of simpler models. These usually restrict the magnitude (`2) or the sparsity (`1) of individual weights. However, a neural net designer typically views complexity as a quantity related to the width or depth of networks. Here, width of a layer refers to the number of neurons in that layer, while depth simply corresponds to the total number of layers. Generally speaking, the more the number of neurons, the greater the width and depth, the more complex is the model. Naturally, one would want to restrict the total number of neurons as a means of controlling model complexity. However, width and depth of networks are integers, making them difficult to optimize over.\nThe overall contributions of the paper are as follows.\n\u2022 We propose novel regularizers and trainable parameters which restrict the total number of neurons in a neural network model - thus effectively selecting width and depth (Section 2)\n\u2022 We identify some desirable properties for an architecture-learning algorithm and show that they hold for the proposed method (Section 3)\n\u2022 We experimentally validate our method and show that it can learn models with considerably small number of parameters (Section 4)\nar X\niv :1\n51 1.\n05 49\n7v 1\n[ cs\n.L G\n] 1\n7 N\nov 2\n01 5"}, {"heading": "2 COMPLEXITY AS A REGULARIZER", "text": "In general, the term \u2018architecture\u2019 of a neural network can refer to aspects of a network other than width and depth (like filter size, stride, etc). However, here we use that word to simply mean width and depth. Given that we want to reduce the complexity of the model, let us formally define our notion of complexity. Notation. Let \u03a6 = [n1, n2, ..., nm, 0, ...] be an infinite-dimensional vector whose first m components are positive numbers, while the rest are zeros. This represents an m-layer neural network architecture with ni neurons for the ith layer.\nFor these vectors, we define an associated norm which corresponds to our notion of architectural complexity of the neural network. Given that we want to restrict the width and depth of neural networks, our notion of complexity is simply the total number of neurons. Definition. The complexity of a m-layer neural network with architecture \u03a6 is given by the `1 norm\n\u2016\u03a6\u2016 = m\u2211 i=1 ni.\nOur overall objective can hence be stated as the following optimization problem.\n\u03b8\u0302, \u03a6\u0302 = arg min \u03b8,\u03a6\n`(y\u0302(\u03b8,\u03a6), y) + \u03bb\u2016\u03a6\u2016 (1)\nwhere \u03b8 denotes the weights of the neural network, and \u03a6 the architecture. `(y\u0302(\u03b8,\u03a6), y) denotes the loss function, which depends on the underlying task to be solved. For example, squared-error loss functions are generally used for regression problems and cross-entropy loss for classification. In this objective, there exists the classical trade-off between model complexity and loss, which is handled by the \u03bb parameter. Note that we learn both the weights (\u03b8) as well as the architecture (\u03a6) in this problem. We term any algorithm which solves the above problem as an Architecture-Learning (AL) algorithm.\nWe observe that the task defined above is very difficult to solve, primarily because \u2016\u03a6\u2016 is an integer. This makes it an integer programming problem. Hence, we cannot use gradient-based techniques to optimize for this. The main contribution of this work is the re-stating of this optimization problem so that Stochastic Gradient Descent (SGD) and back-propagation may be used."}, {"heading": "2.1 A STRATEGY FOR A TRAINABLE REGULARIZER", "text": "We require a strategy to automatically select a neural network\u2019s architecture, i.e; the width of each layer and depth of the network. One way to select for width of a layer is to introduce additional learnable parameters which multiply with every neuron\u2019s output, as shown on the left of Figure 1. If these new parameters are restricted to be binary, then those neurons with a zero-parameter can simply be removed. In the figure, the trainable parameters corresponding to neurons with values b and d are zero, nullifying their contribution. Thus, the sum of these binary trainable parameters will be equal to the effective width of the network.\nTo further reduce the complexity of network, we also strive to reduce the network\u2019s depth. It is well known that two neural network layers without any non-linearity between them is equivalent\nto a single layer, whose parameters are given by the matrix product of the weight matrices of the original two layers. This is shown on the right of Figure 1. We can therefore consider a trainable non-linearity, which prefers \u2018linearity\u2019 over \u2018non-linearity\u2019. Wherever linearity is selected, the corresponding layer can be combined with the next layer. Hence, the total complexity of the neural network would be the number of neurons in layers with a non-linearity.\nIn this work, we combine both these intuitive observations into one single framework. This is captured in our definition of the tri-state ReLU which follows."}, {"heading": "2.1.1 DEFINITION: TRI-STATE RELU", "text": "We define a new trainable non-linearity which we call the tri-state ReLU (tsReLU) as follows:\ntsReLU(x) = { wx, x \u2265 0 wdx, otherwise\n(2)\nThis reduces to the usual ReLU for w = 1 and d = 0. For a fixed w = 1 and a trainable d, this turns into parametric ReLU introduced by He et al. (2015). For us, both w and d are trainable. However, we restrict both these parameters to take only binary values. As a result, three possible states exist for this function. For w = 0, this function is always returns zero. For w = 1 and d = 0 it behaves similar to ReLU, while for d = 1 it reduces to the identity function.\nHere, parameter w selects for the width of the layer, while d decides depth. While the w parameter is different across channels of a layer, the d parameter is tied to the same value across all channels. If d = 1, we can combine that layer with the next to yield a single layer. If w = 0 for any channel, we can simply remove that neuron as well as the corresponding weights in the next layer.\nThus, our objective while using the tri-state ReLU is\nMinimize \u03b8,wij ,di:\u2200i,j `(y\u0302(\u03b8,w,d), y)\nsuch that wij , di \u2208 {0, 1} \u2200i, j\n(3)\nNote that for \u03bb = 0, it converts the objective in Equation 1 from an integer programming problem to that of binary programming."}, {"heading": "2.1.2 LEARNING BINARY PARAMETERS", "text": "Given the definition of tri-state RelU (tsReLU) above, we require a method to learn binary parameters for w and d. To this end, we use the regularizer given by w(1 \u2212 w) (or d(1 \u2212 d)), as used by Murray & Ng (2010). This is a smooth regularizer - which is helpful for gradient descent procedures. This regularizer encourages binary values for parameters, if they are constrained to lie in [0, 1].\n\u22120.5 0 0.5 1 0\n0.05\n0.1\n0.15\n0.2\n0.25\nx\ny\ny = x2 y = x(1\u2212x)\nWith this intuition, we now state our tsReLU optimization objective.\n\u03b8,w,d = arg min \u03b8,wij ,di:\u2200i,j `( y\u0302(\u03b8,w,d), y ) + \u03bb1 m\u2211 i=1 ni\u2211 j=1 wij(1\u2212 wij) + \u03bb2 m\u2211 i=1 di(1\u2212 di) (4)\nNote that \u03bb1 is the regularization constant for the width-limiting term, while \u03bb2 is for the depthlimiting term. This objective can be solved using the usual back-propagation algorithm. Let t(x) := tsReLU(x), and E be the objective function. The gradient for the width-limiting term is simply\n\u2202E \u2202wij = \u2211 x \u2202E \u2202t(x) x+ \u03bb1(1\u2212 2wij) (5)\nwhile that for the depth-limiting term is\n\u2202E \u2202di = \u2211 \u2200x<0 \u2202E \u2202t(x) x+ \u03bb2(1\u2212 2di) (6)\nThe only difference being that the second term is computed only for those terms for which the activation is negative. As indicated earlier, this binarizing regularizer works only if w\u2019s and d\u2019s are guaranteed to be in [0, 1]. To enforce the same, we perform clipping after parameter update.\nAfter optimization, even though the final parameters are expected to be close to binary, they are still real numbers close to 0 and 1. Let wij be the parameter obtained during the optimization. The tsReLU function uses a binarized version of this variable\nw\u2032ij = { 1, wij \u2265 0.5 0, otherwise\nduring the feedforward stage. Note that wij slowly changes during training, while w\u2032ij only reflects the changes made to wij . Note that a similar equation holds for d\u2032i."}, {"heading": "2.2 ADDING MODEL COMPLEXITY", "text": "In the sections above, we considered the problem of training binary tsReLU parameters without restricting the model complexity. As a result, the objective function described above does not nec-\nessarily select for smaller models. Let hi = ni\u2211 j=1 wij correspond to the complexity of a layer. The model complexity term is given by\n\u2016\u03a6\u2016 = m\u2211 i=1 hi 1(di=0)\nThis is formulated such that for di = 0, the complexity in a layer is just hi, while for di = 1 (nonlinearity absent), the complexity is 0. Overall, it counts the total number of non-zero neurons in the model at convergence.\nWe now add a regularizer analogous to model complexity (defined above) in our optimization objective in Equation 4 . Let us call the regularizer corresponding to model complexity as Rm(h,d), which is given by\nRm(h,d) = \u03bb3 m\u2211 i=1 hi1(di<0.5) \u2212 \u03bb4 m\u2211 i=1 di (7)\nThe first term in the above equation limits the complexity of each layer\u2019s width, while the second term limits the network\u2019s depth by encouraging linearity. Note that the first term becomes zero when a non-linearity is absent. Also note that the indicator function in the first term is non-differentiable. As a result, we simply treat that term as a constant with respect to di.\nPrevious to adding this regularizer, all parameters wij and di in equation 4 were considered independently of one another. In particular, the first term of this regularizer accomplishes two things:\n1. It makes a collective decision for all the hi = ni\u2211 j=1 wij in a layer, encouraging it to be low.\n2. It introduces a coupling between width (hi) and non-linearity (di) of a layer."}, {"heading": "2.3 DIFFERENT LEARNING SCHEMES", "text": "The regularizers described previously approximately minimized the complexity of the neural network. However, we used a rather simple definition of complexity, which corresponded to sum of all neurons in a neural network. What happens when we use different notions of complexity in the regularizer? Here we shall consider two such variations with interesting properties. In both cases,\nlet \u2016\u03a6\u2016 := m\u2211 i=1 n\u2032i be the model complexity, where n \u2032 i is defined as follows.\n\u2022 Sublinear complexity: When learning from scratch, we wish for our network to learn with a larger architecture initially and then slowly throw away neurons to get a smaller model. To encourage this behaviour, we may use n\u2032i := ni\u2212nxi , where x < 1. We may analyse the gradient of the defined complexity to identify it\u2019s behaviour. This makes the complexity sub-linear, as well as encourages the above mentioned behaviour.\n\u2022 Fixed Final Width: We may sometimes wish to have a tighter control over the final learnt architecture. In such case, we may use n\u2032i := ni \u2212 ai log(ni), where ai is the final desired width for the ith layer. This ensures that the final architecture is at least equal to a. Additionally, we may also use barrier methods to place lower bounds on the learnt architecture."}, {"heading": "3 PROPERTIES OF THE METHOD", "text": "Now that we have a method to determine the neural network architecture along with the weights, we expect certain properties to naturally hold. While this list is certainly not exhaustive, it represents some desirable properties.\n1. Non-redundancy of architecture: The learnt final architecture must not have any redundant neurons. Removing neurons should necessarily degrade performance.\n2. Local-optimality of weights: The performance of the learnt final architecture must at least be equal to a trained neural network initialized with this final architecture.\n3. Mirroring data-complexity: A \u2018harder\u2019 dataset should result in a larger model than an \u2018easier\u2019 dataset.\nWe intuitively observe that all these properties would automatically hold if a \u2018master\u2019 property which requires both the architecture and the weights be optimal holds. In such a case, any smaller architecture with any set of possible weights would necessarily degrade performance. Given that the optimization objective of neural networks is highly non-convex, such a property cannot be guaranteed. As a result, we restrict ourselves to studying the three properties listed.\nIn the text that follows, we provide statements that hold for our method. These are obtained by analysing widths of each layer of a neural network assuming that depth is never collapsed. In other words, these hold for neural networks with a single hidden layer. Proofs are provided in the Appendix.\nNON-REDUNDANCY OF ARCHITECTURE\nThis is an important property that forms the main motivation for doing architecture-learning. Such a procedure can replace the node-pruning techniques that are used to compress neural networks.\nProposition 1. At convergence, the loss value (`) of the proposed method satisfies \u2202`\n\u2202\u03a6 < 0\nThis statement implies that change in architecture is inversely proportional to change in loss. In other words, if the architecture grows smaller, the loss must go up. While there isn\u2019t a strict relationship between loss and accuracy, a high loss generally indicates worse accuracy.\nLOCAL OPTIMALITY OF WEIGHTS\nThe proposed method produces a neural network architecture along with learnt weights. What would happen if we learnt the neural network model from a fixed architecture (equal to the obtained final architecture in the first case)? This property ensures that in both cases we fall into a local minimum with architecture \u03a6. Proposition 2. Let `1 be the loss value at convergence obtained by training a neural network on data D with a fixed architecture \u03a6. Let `2 be the loss at convergence when the neural network is trained with the proposed method on data D such that it results in the same final architecture \u03a6. Then,\n\u2202`1 \u2202\u03b8 < and \u2202`2 \u2202\u03b8 < .\nMIRRORING DATA-COMPLEXITY\nCharacterizing data-complexity has traditionally been hard (see Ho & Basu (2002)). Here, we consider the following approach. Proposition 3. Let D1 and D2 be two datasets which produce losses `1 and `2 upon training with a fixed architecture \u03a6 such that `1 > `2. When trained with the proposed method, the final architectures \u03a6\u03021 and \u03a6\u03022 (corresponding to D1 and D2) satisfy the relation \u2016\u03a6\u03021\u2016 > \u2016\u03a6\u03022\u2016 at convergence.\nHere, D1 is the \u2018harder\u2019 dataset because it produces a higher loss on the same neural network architecture. As a result, the \u2018harder\u2019 dataset always produces a larger final architecture. We do not provide a proof for this statement. Instead, we experimentally verify this in Section 5.2."}, {"heading": "4 RELATED WORK", "text": "There have been many works which look at performing compression of a neural network. Weightpruning techniques were popularized by LeCun et al. (1989) and Hassibi et al. (1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively. Recently, Srinivas & Babu (2015) proposed a neuron pruning technique which scaled better than these weight pruning techniques. This work, on the other hand, learns a neural architecture that does not require neuron pruning (Property 1). The learning objective can thus be seen as performing pruning and learning together, unlike the work of Han et al. (2015), who perform both operations alternately.\nLearning neural network architecture has also been explored to some extent. The Cascadecorrelation algorithm by Fahlman & Lebiere (1989) proposed a novel learning rule to \u2018grow\u2019 the neural network. However, it was shown for only a single layer network and is hence not clear how to scale to large deep networks. Our work is inspired from the recent work of Kulkarni et al. (2015) who proposed to learn the width of neural networks in a way similar to ours. Specifically, they proposed to learn a diagonal matrix D along with neurons Wx, such that DWx represents that layer\u2019s neurons. However, instead of imposing a binary constraint (like ours), they learn realweights and impose an `1-based sparsity-inducing regularizer on D to encourage zeros. By imposing a binary constraint, we are able to directly regularize for the model complexity. Recently, Bayesian Optimization-based algorithms (see Snoek et al. (2012)) have also been proposed for automatically learning hyper-parameters of neural networks. However, these have been shown to work for real-valued quantities like learning rate, whereas width and depth are typically integers. Having\nconverted the integer problem to a real-valued one, one can now use Bayesian Optimization to now select these new hyper-parameters.\nAnother way to perform compression is to train a smaller model to mimic a larger model. Bucilua et al. (2006) proposed a way to achieve the same - and trained smaller models which had accuracies similar to larger networks. Ba & Caruana (2014) used the approach to show that shallower (but much wider) models can be trained to perform as well as deep models. Knowledge Distillation (KD) by Hinton et al. (2014) is a more general approach, of which Bucila et al.\u2019s is a special case. Our method of learning with a fixed final architecture is reminiscent of KD. However, the formulations seem to be very different. FitNets by Romero et al. (2014) use a KD-like method at several layers to learn networks which are deeper but thinner (in contrast to Ba and Caruna\u2019s shallow and wide), and achieve high levels of compression on trained models. In contrast, our method can only make networks shallower, not deeper. However, we note that it is possible to modify our method to enable such learning.\nMany methods have been proposed to train models that are deep, yet have a lower parameterisation than conventional networks. Collins & Kohli (2014) propose a sparsity inducing regulariser for backpropogation which promotes many weights to have zero magnitude. They achieve reduction in memory consumption when compared to traditionally trained models. In contrast, our method promotes neurons to have a zero-magnitude. As a result, our overall objective function is much simpler to solve. Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al. (2014) propose an Adaptive Fastfood transform, which is an efficient re-parametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation.\nSome recent works have also focussed on using approximations of weight matrices to perform compression. Jaderberg et al. (2014) and Denton et al. (2014) use SVD-based low rank approximations of the weight matrix. Gong et al. (2014) use a clustering-based product quantization approach to build an indexing scheme that reduces the space occupied by the matrix on disk."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we perform experiments to analyse the behaviour of our method. In the first set of experiments, we evaluate performance on the MNIST dataset. Later, we look at a case study on ILSVRC 2012 dataset. All our experiments are performed using the Caffe Deep learning framework (see Jia et al. (2014))."}, {"heading": "5.1 COMPRESSION PERFORMANCE", "text": "We evaluate our method on the MNIST dataset, using a LeNet-like (see LeCun et al. (1998)) architecture. The network consists of two 5\u00d7 5 convolutional layers with 20 and 50 filters, and two fully connected layers with 500 and 10 (output layer) neurons. While there is nothing particularly special about this architecture, we simply use it as a starting point to learn smaller architectures. Further details of the experimental setup is given in the Appendix.\nStarting from the baseline architecture, we learn smaller architectures with variations of our method. Note that there is max-pooling applied after each of the convolutional layers, which rules out depth selection for those two layers. We run our architecture-learning (AL) method with the linear complexity and the sub-linear complexity measures. We also compare against baselines of directly training a neural network (NN) on the final architecture, and our method of learning a fixed final width (FFW) for various layers. In Table 1, the Layers Learnt column has binary elements (w, d) which denotes whether width(w) or depth(d) are learnt for each layer in the baseline network. As an example, the second row shows a method where only the width is learnt in the first two layers, and depth also learnt in the third layer. This table shows that all considered methods perform more or less equally well. This empirically confirms Proposition 2.\nWe also compare the compression performance of our linear and sub-linear complexity AL methods against SVD-based compression of the weight matrix in Table 2. Here we only compress layer 3\n(which has 800 \u00d7 500 weights) using SVD. The results show that learning a smaller network is beneficial over learning a large network and then performing SVD-based compression."}, {"heading": "5.2 ANALYSIS", "text": "We now perform a few more experiments to further analyse the behaviour of our method. In all cases, we train \u2018AL-linear2\u2019-like models, and consider the third layer for evaluation. We start learning with the baseline architecture considered above.\nFirst, we look at the effects of using a different complexity measures. Specifically, we look at the learning dynamics for the AL-linear and AL-sublinear methods. From Figure 4a, we observe that the sublinear method explores intermediate architectures for an extended time, and hence looks much smoother. We also observe that in both methods, the convergence of architectures happen quite early on (\u223c 10k iterations) - leaving the usual weight learning for the remaining iterations. Using a smaller \u03bb1 (blue curve) is another way by which one can slow down architecture-learning by delaying convergence.\nSecond, we look at the learnt architectures for different amounts of data complexity, as in Property 3. A simple way to obtain data of differing complexity is to simply vary the number of classes in a multi-class problem like MNIST. We hence vary the number of classes from 2 \u2212 10, and run our method for each case without changing any hyper-parameters. As seen in Figure 4b, we see an almost monotonic increase in both architectural complexity and error rate, which confirms our hypothesis."}, {"heading": "5.3 CASE STUDY: ALEXNET", "text": "For the experiments that follow, we use an AlexNet-like (see Krizhevsky et al. (2012)) model, called CaffeNet, provided with the Caffe Deep Learning framework. It is very similar to AlexNet, except that the order of max-pooling and normalization have been interchanged. We use the ILSVRC 2012 (see Russakovsky et al. (2015)) validation set to compute accuracies in the Table 3. Unlike the experiments done previously, we start with a pre-trained model and then perform architecture learning (AL) on it. We see that our method does not perform as well as the state of the art compression methods. This points to the fact that fully-connected layers may indeed be over-parametrized and that clever approaches of weight re-parametrization (see Yang et al. (2014)) do better when it comes to compression performance.\nMany compression methods are formulated keeping only fully-connected layers in mind. For tasks like Semantic Segmentation, networks with only convolutional layers are used (see Long et al.\n(2015)). Further, state-of-the-art image classification networks (see Szegedy et al. (2015)) are increasingly deep with customized layer definitions (Eg: Inception module). Our method is applicable to both these cases. When applied to convolutional layers, we surprisingly get only small reductions in layers 1, 2, and none after that. This suggests that there is scope for increasing the width of these layers."}, {"heading": "6 CONCLUSIONS", "text": "We proposed a method to learn a neural network\u2019s architecture along with weights. Rather than directly selecting width and depth, we introduced real-valued hyper-parameters which selected width and depth for us. We also saw that we get smaller architectures for MNIST and ImageNet datasets that perform on par with the large architectures. Our method is very simple and straightforward, and can be suitably applied to any neural network. This can also be used as a tool to further explore the dependence of architecture on the optimization and convergence of neural networks."}, {"heading": "APPENDIX A", "text": "PROOFS OF PROPOSITIONS\nLetE = `+\u03bbbRb+\u03bbmRm be total objective function, whereRb is the binarizing regularizer,Rm = \u2016\u03c6\u2016 is the model complexity term. At convergence, we assume that Rb = 0 as the corresponding weights are all binary or close to binary.\nProof of proposition 1. At convergence, we assume \u2202E\n\u2202\u03c6 < , for some \u2192 0+.\n\u2202` \u2202\u03c6 < \u2212\u03bbm \u2202\u2016\u03c6\u2016 \u2202\u03c6 + =\u21d2 \u2202` \u2202\u03c6 < \u2212\u03bbm + =\u21d2 \u2202` \u2202\u03c6 < 0\nfor some sufficiently small.\nProof of proposition 2. Let q = \u2202`2 \u2202\u03b82 . Also let\n\u03bbb = ED(q) + k\u03c3 where \u03c3 = ED(q \u2212ED(q))2\nIf k \u2192\u221e then P(q > \u03bbb)\u2192 0\nLet Rb = 0 at tth iteration. Note that Rb = 1/4 is the least value that can trigger a change in architecture in our method.\nThen for large enough \u03bbb, Rb < 1/4 for all successive iterations with high probability. As a result, after the tth iteration, architecture learning effectively stops.\nAfter T >> t iterations, we have\n\u2202`1 \u2202\u03b81 < and \u2202`2 \u2202\u03b82 < (8)\nfor some \u2192 0+. However, if \u03b81 \u2208 Rd1 , then \u03b82 \u2208 Rd2 , such that d1 < d2. Without loss of generality, let us assume that neurons corresponding to first d1 weights are selected for, while the rest are inactive. As a result, \u2202`2\n\u2202\u03b82(d) = 0, for d \u2208 [d1, d2]. Hence, the following holds\n\u2202`2 \u2202\u03b81 < . This, along with equation 8, proves the assertion."}, {"heading": "APPENDIX B", "text": "EXPERIMENTAL DETAILS\nMNIST EXPERIMENTS\nFor training the network, we used the ADAM optimizer (see Kingma & Ba (2014)), with a learning rate of 0.001, momentum of 0.9 and momentum2 of 0.99, over a total of 30, 000 iterations. Dropout was not used while training these networks. For the baseline model, each layer is followed by a ReLU non-linearity. For the AL-sublinear experiments, we used \u2016\u03c6\u2016 = \u2016\u03c6\u2212 \u03c6x\u2016, where x = 0.7.\nALEXNET EXPERIMENTS\nWe used the vanilla SGD + momentum optimizer used by Krizhevsky et al. (2012). We used a fixed learning rate of 10\u22124 for weights and 10\u22125 for architecture, with a momentum of 0.9, over 50, 000 iterations. The resulting weights were fine-tuned for 10, 000 iterations, while fixing the architecture. Dropout was omitted during the entire process."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Ba", "Jimmy", "Caruana", "Rich"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Memory bounded deep convolutional networks", "author": ["Collins", "Maxwell D", "Kohli", "Pushmeet"], "venue": "CoRR, abs/1412.1442,", "citeRegEx": "Collins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "de Freitas", "Nando"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily L", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "The cascade-correlation learning architecture", "author": ["Fahlman", "Scott E", "Lebiere", "Christian"], "venue": null, "citeRegEx": "Fahlman et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Fahlman et al\\.", "year": 1989}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William J"], "venue": "arXiv preprint arXiv:1506.02626,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Hassibi", "Babak", "Stork", "David G"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey E", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "In NIPS 2014 Deep Learning Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Complexity measures of supervised classification problems", "author": ["Ho", "Tin Kamo", "Basu", "Mitra"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Ho et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2002}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In Proceedings of the British Machine Vision Conference. BMVA Press,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A", "Howard", "Richard E", "Jackel", "Lawrence D"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "An algorithm for nonlinear optimization problems with binary variables", "author": ["Murray", "Walter", "Ng", "Kien-Ming"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Murray et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2010}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael", "Berg", "Alexander C", "Fei-Fei", "Li"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Data-free parameter pruning for deep neural networks", "author": ["Srinivas", "Suraj", "Babu", "R. Venkatesh"], "venue": "Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Srinivas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Deep fried convnets", "author": ["Yang", "Zichao", "Moczulski", "Marcin", "Denil", "Misha", "de Freitas", "Nando", "Smola", "Alex", "Song", "Le", "Wang", "Ziyu"], "venue": "arXiv preprint arXiv:1412.7149,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "For large-scale tasks like image classification, the general practice has been to train large networks with many millions of parameters (see Krizhevsky et al. (2012); Simonyan & Zisserman (2015); Szegedy et al.", "startOffset": 141, "endOffset": 166}, {"referenceID": 14, "context": "For large-scale tasks like image classification, the general practice has been to train large networks with many millions of parameters (see Krizhevsky et al. (2012); Simonyan & Zisserman (2015); Szegedy et al.", "startOffset": 141, "endOffset": 195}, {"referenceID": 14, "context": "For large-scale tasks like image classification, the general practice has been to train large networks with many millions of parameters (see Krizhevsky et al. (2012); Simonyan & Zisserman (2015); Szegedy et al. (2015)).", "startOffset": 141, "endOffset": 218}, {"referenceID": 8, "context": "For a fixed w = 1 and a trainable d, this turns into parametric ReLU introduced by He et al. (2015). For us, both w and d are trainable.", "startOffset": 83, "endOffset": 100}, {"referenceID": 13, "context": "Weightpruning techniques were popularized by LeCun et al. (1989) and Hassibi et al.", "startOffset": 45, "endOffset": 65}, {"referenceID": 6, "context": "(1989) and Hassibi et al. (1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively.", "startOffset": 11, "endOffset": 33}, {"referenceID": 6, "context": "(1989) and Hassibi et al. (1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively. Recently, Srinivas & Babu (2015) proposed a neuron pruning technique which scaled better than these weight pruning techniques.", "startOffset": 11, "endOffset": 143}, {"referenceID": 6, "context": "The learning objective can thus be seen as performing pruning and learning together, unlike the work of Han et al. (2015), who perform both operations alternately.", "startOffset": 104, "endOffset": 122}, {"referenceID": 6, "context": "The learning objective can thus be seen as performing pruning and learning together, unlike the work of Han et al. (2015), who perform both operations alternately. Learning neural network architecture has also been explored to some extent. The Cascadecorrelation algorithm by Fahlman & Lebiere (1989) proposed a novel learning rule to \u2018grow\u2019 the neural network.", "startOffset": 104, "endOffset": 301}, {"referenceID": 6, "context": "The learning objective can thus be seen as performing pruning and learning together, unlike the work of Han et al. (2015), who perform both operations alternately. Learning neural network architecture has also been explored to some extent. The Cascadecorrelation algorithm by Fahlman & Lebiere (1989) proposed a novel learning rule to \u2018grow\u2019 the neural network. However, it was shown for only a single layer network and is hence not clear how to scale to large deep networks. Our work is inspired from the recent work of Kulkarni et al. (2015) who proposed to learn the width of neural networks in a way similar to ours.", "startOffset": 104, "endOffset": 544}, {"referenceID": 6, "context": "The learning objective can thus be seen as performing pruning and learning together, unlike the work of Han et al. (2015), who perform both operations alternately. Learning neural network architecture has also been explored to some extent. The Cascadecorrelation algorithm by Fahlman & Lebiere (1989) proposed a novel learning rule to \u2018grow\u2019 the neural network. However, it was shown for only a single layer network and is hence not clear how to scale to large deep networks. Our work is inspired from the recent work of Kulkarni et al. (2015) who proposed to learn the width of neural networks in a way similar to ours. Specifically, they proposed to learn a diagonal matrix D along with neurons Wx, such that DWx represents that layer\u2019s neurons. However, instead of imposing a binary constraint (like ours), they learn realweights and impose an `1-based sparsity-inducing regularizer on D to encourage zeros. By imposing a binary constraint, we are able to directly regularize for the model complexity. Recently, Bayesian Optimization-based algorithms (see Snoek et al. (2012)) have also been proposed for automatically learning hyper-parameters of neural networks.", "startOffset": 104, "endOffset": 1079}, {"referenceID": 6, "context": "Knowledge Distillation (KD) by Hinton et al. (2014) is a more general approach, of which Bucila et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 6, "context": "Knowledge Distillation (KD) by Hinton et al. (2014) is a more general approach, of which Bucila et al.\u2019s is a special case. Our method of learning with a fixed final architecture is reminiscent of KD. However, the formulations seem to be very different. FitNets by Romero et al. (2014) use a KD-like method at several layers to learn networks which are deeper but thinner (in contrast to Ba and Caruna\u2019s shallow and wide), and achieve high levels of compression on trained models.", "startOffset": 31, "endOffset": 286}, {"referenceID": 6, "context": "Knowledge Distillation (KD) by Hinton et al. (2014) is a more general approach, of which Bucila et al.\u2019s is a special case. Our method of learning with a fixed final architecture is reminiscent of KD. However, the formulations seem to be very different. FitNets by Romero et al. (2014) use a KD-like method at several layers to learn networks which are deeper but thinner (in contrast to Ba and Caruna\u2019s shallow and wide), and achieve high levels of compression on trained models. In contrast, our method can only make networks shallower, not deeper. However, we note that it is possible to modify our method to enable such learning. Many methods have been proposed to train models that are deep, yet have a lower parameterisation than conventional networks. Collins & Kohli (2014) propose a sparsity inducing regulariser for backpropogation which promotes many weights to have zero magnitude.", "startOffset": 31, "endOffset": 782}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al. (2014) propose an Adaptive Fastfood transform, which is an efficient re-parametrization of fully-connected layer weights.", "startOffset": 0, "endOffset": 209}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al. (2014) propose an Adaptive Fastfood transform, which is an efficient re-parametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation. Some recent works have also focussed on using approximations of weight matrices to perform compression. Jaderberg et al. (2014) and Denton et al.", "startOffset": 0, "endOffset": 530}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al. (2014) propose an Adaptive Fastfood transform, which is an efficient re-parametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation. Some recent works have also focussed on using approximations of weight matrices to perform compression. Jaderberg et al. (2014) and Denton et al. (2014) use SVD-based low rank approximations of the weight matrix.", "startOffset": 0, "endOffset": 555}, {"referenceID": 2, "context": "Denil et al. (2013) demonstrate that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al. (2014) propose an Adaptive Fastfood transform, which is an efficient re-parametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation. Some recent works have also focussed on using approximations of weight matrices to perform compression. Jaderberg et al. (2014) and Denton et al. (2014) use SVD-based low rank approximations of the weight matrix. Gong et al. (2014) use a clustering-based product quantization approach to build an indexing scheme that reduces the space occupied by the matrix on disk.", "startOffset": 0, "endOffset": 634}, {"referenceID": 12, "context": "All our experiments are performed using the Caffe Deep learning framework (see Jia et al. (2014)).", "startOffset": 79, "endOffset": 97}, {"referenceID": 15, "context": "We evaluate our method on the MNIST dataset, using a LeNet-like (see LeCun et al. (1998)) architecture.", "startOffset": 69, "endOffset": 89}, {"referenceID": 14, "context": "3 CASE STUDY: ALEXNET For the experiments that follow, we use an AlexNet-like (see Krizhevsky et al. (2012)) model, called CaffeNet, provided with the Caffe Deep Learning framework.", "startOffset": 83, "endOffset": 108}, {"referenceID": 14, "context": "3 CASE STUDY: ALEXNET For the experiments that follow, we use an AlexNet-like (see Krizhevsky et al. (2012)) model, called CaffeNet, provided with the Caffe Deep Learning framework. It is very similar to AlexNet, except that the order of max-pooling and normalization have been interchanged. We use the ILSVRC 2012 (see Russakovsky et al. (2015)) validation set to compute accuracies in the Table 3.", "startOffset": 83, "endOffset": 346}, {"referenceID": 14, "context": "3 CASE STUDY: ALEXNET For the experiments that follow, we use an AlexNet-like (see Krizhevsky et al. (2012)) model, called CaffeNet, provided with the Caffe Deep Learning framework. It is very similar to AlexNet, except that the order of max-pooling and normalization have been interchanged. We use the ILSVRC 2012 (see Russakovsky et al. (2015)) validation set to compute accuracies in the Table 3. Unlike the experiments done previously, we start with a pre-trained model and then perform architecture learning (AL) on it. We see that our method does not perform as well as the state of the art compression methods. This points to the fact that fully-connected layers may indeed be over-parametrized and that clever approaches of weight re-parametrization (see Yang et al. (2014)) do better when it comes to compression performance.", "startOffset": 83, "endOffset": 782}, {"referenceID": 24, "context": "Further, state-of-the-art image classification networks (see Szegedy et al. (2015)) are increasingly deep with customized layer definitions (Eg: Inception module).", "startOffset": 61, "endOffset": 83}, {"referenceID": 25, "context": "60 35 SVD-quarter-F (Yang et al. (2014)) 25.", "startOffset": 21, "endOffset": 40}, {"referenceID": 25, "context": "60 35 SVD-quarter-F (Yang et al. (2014)) 25.6M 56.19 58 Adaptive FastFood 32 (Yang et al. (2014)) 22.", "startOffset": 21, "endOffset": 97}], "year": 2015, "abstractText": "Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.", "creator": "LaTeX with hyperref package"}}}