{"id": "1511.06392", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Neural Random-Access Machines", "abstract": "In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation.", "histories": [["v1", "Thu, 19 Nov 2015 21:36:28 GMT  (53kb,D)", "http://arxiv.org/abs/1511.06392v1", "ICLR submission, 13 pages with bibliography and appendix, 3 figures, 2 tables"], ["v2", "Thu, 7 Jan 2016 10:27:06 GMT  (124kb,D)", "http://arxiv.org/abs/1511.06392v2", "ICLR submission, 17 pages, 9 figures, 6 tables (with bibliography and appendix)"], ["v3", "Tue, 9 Feb 2016 21:29:07 GMT  (124kb,D)", "http://arxiv.org/abs/1511.06392v3", "ICLR submission, 17 pages, 9 figures, 6 tables (with bibliography and appendix)"]], "COMMENTS": "ICLR submission, 13 pages with bibliography and appendix, 3 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["karol kurach", "marcin", "rychowicz", "ilya sutskever"], "accepted": true, "id": "1511.06392"}, "pdf": {"name": "1511.06392.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Karol Kurach"], "emails": ["kkurach@google.com", "marcina@google.com", "ilyasu@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep learning is successful for two reasons. First, deep neural networks are able to represent the \u201cright\u201d kind of functions; second, deep neural networks are trainable. Deep neural networks can be potentially improved if they get deeper and have fewer parameters, while maintaining trainability. By doing so, we move closer towards a practical implementation of Solomonoff induction (Solomonoff, 1964). The first model that we know of that attempted to train extremely deep networks with a large memory and few parameters is the Neural Turing Machine (NTM) (Graves et al., 2014) \u2014 a computationally universal deep neural network that is trainable with backpropagation. Other models with this property include variants of Stack-Augmented recurrent neural networks (Joulin & Mikolov, 2015; Grefenstette et al., 2015), and the Grid-LSTM (Kalchbrenner et al., 2015)\u2014of which the Grid-LSTM has achieved the greatest success on both synthetic and real tasks. The key characteristic of these models is that their depth, the size of their short term memory, and their number of parameters are no longer confounded and can be altered independently \u2014 which stands in contrast to models like the LSTM (Hochreiter & Schmidhuber, 1997), whose number of parameters grows quadratically with the size of their short term memory.\nA fundamental operation of modern computers is pointer manipulation and dereferencing. In this work, we investigate a model class that we name the Neural Random-Access Machine (NRAM), which is a neural network that has, as primitive operations, the ability to manipulate, store in memory, and dereference pointers into its working memory. By providing our model with dereferencing as a primitive, it becomes possible to train models on problems whose solutions require pointer manipulation and chasing. Although all computationally universal neural networks are equivalent, which means that the NRAM model does not have a representational advantage over other models if they are given a sufficient number of computational steps, in practice, the number of timesteps that a given model has is highly limited, as extremely deep models are very difficult to train. As a result, the model\u2019s core primitives have a strong effect on the set of functions that can be feasibly learned in practice, similarly to the way in which the choice of a programming language strongly affects the functions that can be implemented with an extremely small amount of code.\nFinally, the usefulness of computationally-universal neural networks depends entirely on the ability of backpropagation to find good settings of their parameters. Indeed, it is trivial to define the \u201coptimal\u201d hypothesis class (Solomonoff, 1964), but the problem of finding the best (or even a good)\n\u2217Equal contribution.\nar X\niv :1\n51 1.\n06 39\n2v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nfunction in that class is intractable. Our work puts the backpropagation algorithm to another test, where the model is extremely deep and intricate.\nIn our experiments, we evaluate our model on several algorithmic problems whose solutions required pointer manipulation and chasing. These problems include algorithms on a linked-list and a binary tree. While we were able to achieve encouraging results on these problems, we found that standard optimization algorithms struggle with these extremely deep and nonlinear models. We believe that advances in optimization methods will likely lead to better results."}, {"heading": "2 RELATED WORK", "text": "There has been a significant interest in the problem of learning algorithms in the past few years. The most relevant recent paper is Neural Turing Machines (NTMs) (Graves et al., 2014). It was the first paper to explicitly suggest the notion that it is worth training a computationally universal neural network, and achieved encouraging results.\nA follow-up model that had the goal of learning algorithms was the Stack-Augmented Recurrent Neural Network (Joulin & Mikolov, 2015) This work demonstrated that the Stack-Augmented RNN can generalize to long problem instances from short problem instances. A related model is the Reinforcement Learning Neural Turing Machine (Zaremba & Sutskever, 2015), which attempted to use reinforcement learning techniques to train a discrete-continuous hybrid model.\nThe memory network (Weston et al., 2014) is an early model that attempted to explicitly separate the memory from computation in a neural network model. The followup work of Sukhbaatar et al. (2015) combined the memory network with the soft attention mechanism, which allowed it to be trained with less supervision.\nThe Grid-LSTM (Kalchbrenner et al., 2015) is a highly interesting extension of LSTM, which allows to use LSTM cells for both deep and sequential computation. It achieves excellent results on both synthetic, algorithmic problems and on real tasks, such as language modelling, machine translation, and object recognition.\nThe Pointer Network (Vinyals et al., 2015) is somewhat different from the above models in that it does not have a writable memory \u2014 it is more similar to the attention model of Bahdanau et al. (2014) in this regard. Despite not having a memory, this model was able to solve a number of difficult algorithmic problems that include the convex hull and the approximate 2D travelling salesman problem (TSP).\nFinally, it is important to mention the attention model of Bahdanau et al. (2014). Although this work is not explicitly aimed at learning algorithms, it is by far the most practical model that has an \u201calgorithmic bent\u201d. Indeed, this model has proven to be highly versatile, and variants of this model have achieved state-of-the-art results on machine translation (Luong et al., 2015), speech recognition (Chan et al., 2015), and syntactic parsing (Vinyals et al., 2014), without the use of almost any domain-specific tuning."}, {"heading": "3 MODEL", "text": "In this section we describe the NRAM model. We start with a description of the simplified version of our model which does not use an external memory and then explain how to augment it with a variable-size random-access memory. The core part of the model is a neural controller, which acts as a \u201cprocessor\u201d. The controller can be a feedforward neural network or an LSTM, and it is the only trainable part of the model.\nThe model contains R registers, each of which holds an integer value. To make our model trainable with gradient descent, we made it fully differentiable. Hence, each register represents an integer value with a distribution over the set {0, 1, . . . ,M \u2212 1}, for some constant M . We do not assume that these distributions have any special form \u2014 they are simply stored as vectors p \u2208 RM satisfying pi \u2265 0 and \u2211 i pi = 1. The controller does not have direct access to the registers; it can interact with them using a number of prespecified modules (gates), such as integer addition or equality test.\nLet\u2019s denote the modules m1,m2, . . . ,mQ, where each module is a function:\nmi : {0, 1, . . . ,M \u2212 1} \u00d7 {0, 1, . . . ,M \u2212 1} \u2192 {0, 1, . . . ,M \u2212 1}.\nOn a high level, the model performs a sequence of timesteps, each of which consists of the following substeps:\n1. The controller gets some inputs depending on the values of the registers (the controller\u2019s inputs are described in Sec. 3.1).\n2. The controller updates its internal state (if the controller is an LSTM).\n3. The controller outputs the description of a \u201cfuzzy circuit\u201d with inputs r1, . . . , rR, gates m1, . . . ,mQ and R outputs.\n4. The values of the registers are overwritten with the outputs of the circuit.\nMore precisely, each circuit is created as follows. The inputs for the module mi are chosen by the controller from the set {r1, . . . , rR, o1, . . . , oi\u22121}, where:\n\u2022 rj is the value stored in the j-th register at the current timestep, and \u2022 oj is the output of the module mj at the current timestep.\nHence, for each 1 \u2264 i \u2264 Q the controller chooses weighted averages of the values {r1, . . . , rR, o1, . . . , oi\u22121} which are given as inputs to the module. Therefore,\noi = mi ( (r1, . . . , rR, o1, . . . , oi\u22121) T softmax(ai), (r1, . . . , rR, o1, . . . , oi\u22121) T softmax(bi) ) ,\n(1) where the vectors ai, bi \u2208 RR+i\u22121 are produced by the controller. Recall that the variables rj represent probability distributions and therefore the inputs to mi, being weighted averages of probability distributions, are also probability distributions. Thus, as the modules mi are originally defined for integer inputs and outputs, we must extend their domain to probability distributions as inputs, which can be done in a natural way (and make their output also be a probability distribution):\n\u22000\u2264c<M P (mi(A,B) = c) = \u2211\n0\u2264a,b<M\nP(A = a)P(B = b)[mi(a, b) = c]. (2)\nAfter the modules have produced their outputs, the controller decides which of the values {r1, . . . , rR, o1, . . . , oQ} should be stored in the registers. In detail, the controller outputs the vectors ci \u2208 RR+Q for 1 \u2264 i \u2264 R and the values of the registers are updated (simultaneously) using the formula:\nri := (r1, . . . , rR, o1, . . . , oQ) T softmax(ci). (3)"}, {"heading": "3.1 CONTROLLER\u2019S INPUTS", "text": "Recall that at the beginning of each timestep the controller receives some inputs, and it is an important design decision to decide where should these inputs come from. A naive approach is to use the values of the registers as inputs to the controller. However, the values of the registers are probability distributions and are stored as vectors p \u2208 RM . If the entire distributions were given as inputs to the controller then the number of the model\u2019s parameters would depend on M . This would be undesirable because, as will be explained in the next section, the value M is linked to the size of an external random-access memory tape and hence it would prevent the model from generalizing to different memory sizes.\nHence, for each 1 \u2264 i \u2264 R the controller receives, as input, only one scalar from each register, namely P(ri = 0) \u2014 the probability that the value in the register is equal 0. This solution has an additional advantage, namely it limits the amount of information available to the controller and forces it to rely on the modules instead of trying to solve the problem on its own. Notice that this information is sufficient to get the exact value of ri if ri \u2208 {0, 1}, which is the case whenever ri is an output of a ,,boolean\u201d module, e.g. the inequality test module mi(a, b) = [a < b]."}, {"heading": "3.2 MEMORY TAPE", "text": "One could use the model described so far for learning sequence-to-sequence transformations by initializing the registers with the input sequence, and training the model to produce the desired output sequence in its registers after a given number of timesteps. The disadvantage of such model is that it would be completely unable to generalize to longer sequences, because the length of the sequence that the model can process is equal to the number of its registers, which is constant.\nTherefore, we extend the model with a variable-size memory tape, which consists of M memory cells, each of which stores a distribution over the set {0, 1, . . . ,M\u22121}. Notice that each distribution stored in a memory cell or a register can be interpreted as a fuzzy address in the memory and used as a fuzzy pointer. We will hence identify integers in the set {0, 1, . . . ,M \u2212 1} with pointers to the memory. Therefore, the value in each memory cell may be interpreted as an integer or as a pointer. The exact state of the memory can be described by a matrixM \u2208 RMM , where the valueMi,j is the probability that the i-th cell holds the value j.\nThe model interacts with the memory tape solely using two special modules:\n\u2022 READ module: this module takes as the input a pointer1 and returns the value stored under the given address in the memory. This operation is extended to fuzzy pointers similarly to Eq. 2. More precisely, if p is a vector representing the probability distribution of the input (i.e. pi is the probability that the input pointer points to the i-th cell) then the module returns the valueMT p.\n\u2022 WRITEmodule: this module takes as the input a pointer p and a value a and stores the value a under the address p in the memory. The fuzzy form of the operation can be effectively expressed using matrix operations 2.\nThe full architecture of the NRAM model is presented on Fig. 1\n1Formally each module takes two arguments. In this case the second argument is simply ignored. 2The exact formula isM := (J \u2212 p)JT \u00b7 M+ paT , where J denotes a (column) vector consisting of M\nones and \u00b7 denotes coordinate-wise multiplication."}, {"heading": "3.3 INPUTS AND OUTPUTS HANDLING", "text": "The memory tape also serves as an input-output channel \u2014 the model\u2019s memory is initialized with the input sequence and the model is expected to produce the output in the memory. Moreover, we use a novel way of deciding how many timesteps should be executed. After each timestep we let the controller decide whether it would like to continue the execution or finish it, in which case the current state of the memory is treated as the output.\nMore precisely, after the timestep t the controller outputs a scalar ft \u2208 [0, 1]3, which denotes the willingness to finish the execution in the current timestep. Therefore, the probability that the execution has not been finished before the timestep t is equal \u220ft\u22121 i=1(1 \u2212 fi), and the probability that\nthe output is produced exactly at the timestep t is equal pt = ft \u00b7 \u220ft\u22121\ni=1(1 \u2212 fi). There is also some maximal allowed number of timesteps T , which is a hyperparameter. The model is forced to produce output in the last step if it has not done it yet, i.e. pT = 1\u2212 \u2211T\u22121 i=1 pi regardless of the value fT .\nLet M(t) \u2208 RMM denote the memory matrix after the timestep t, i.e. M (t) i,j is the probability that the i-th memory cell holds the value j after the timestep t. For an input-output pair (x, y), where x, y \u2208 {0, 1, . . . ,M \u2212 1}M we define the loss of the model as the expected negative log-likelihood of producing the correct output, i.e., \u2212 \u2211T t=1 ( pt \u00b7 \u2211M i=1 log(M (t) i,yi ) ) assuming that the memory was initialized with the sequence x4. Moreover, for all problems we consider the output sequence is shorter than the memory. Therefore, we compute the loss only over memory cells, which should contain the output."}, {"heading": "3.4 DISCRETIZATION", "text": "Computing the outputs of modules, represented as probability distributions, is a computationally costly operation. For example, computing the output of the READ module takes \u0398(M2) time as it requires the multiplication of the matrixM\u2208 RMM and the vector p \u2208 RM . One may however suspect (and we empirically verify this claim in Sec. 4) that the NRAM model naturally learns solutions in which the distributions of intermediate values have very low entropy. The argument for this hypothesis is that fuzziness in the intermediate values would probably propagate to the output and cause a higher value of the cost function. To test this hypothesis we trained the model and then used its discretized version during interference. In the discretized version every module gets as inputs the values from modules (or registers), which are the most probable to produce the given input accordingly to the distribution outputted by the controller. More precisely, it corresponds to replacing the function softmax in equations (1,3) with the function returning the vector containing 1 on the position of the maximum value in the input and zeros on all other positions.\nNotice that in the discretized NRAM model each register and memory cell stores an integer from the set {0, 1, . . . ,M \u2212 1} and therefore all modules may be executed efficiently (assuming that the functions represented by the modules can be efficiently computed). In case of a feedforward controller and a small (e.g. \u2264 20) number of registers the interference can be accelerated even further. Recall that the only inputs to the controller are binarized values of the register. Therefore, instead of executing the controller one may simple precompute the (discretized) controller\u2019s output for each configuration of the registers\u2019 binarized values. Such algorithm would enjoy an extremely efficient implementation in machine code.\n3In fact, the controller outputs a scalar xi and fi = sigmoid(xi). 4One could also use the negative log-likelihood of the expected output, i.e. \u2212 \u2211M i=1 log (\u2211T t=1 pt \u00b7 M (t) i,yi ) as the loss function."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 TRAINING PROCEDURE", "text": "The NRAM model is fully differentiable and we trained it using the Adam optimization algorithm (Kingma & Ba, 2014) with the negative log-likelihood cost function. Notice that we do not use any additional supervised data (such as memory access traces) beyond pure input-output examples.\nWe used multilayer perceptrons (MLPs) with two hidden layers or LSTMs with a hidden layer between input and LSTM cells as controllers. The number of hidden units in each layer was equal. The ReLu nonlinearity (Nair & Hinton, 2010) was used in all experiments.\nBelow are some important techniques that we used in the training:\nCurriculum learning As noticed in several papers (Bengio et al., 2009; Zaremba & Sutskever, 2014), curriculum learning is crucial for training deep networks on very complicated problems. We followed the curriculum learning schedule from Zaremba & Sutskever (2014) without any modifications. The details can be found in Appendix B.\nGradient clipping Notice that the depth of the unfolded execution is roughly a product of the number of timesteps and the number of modules. Even for moderately small experiments (e.g. 14 modules and 20 timesteps) this value easily exceeds a few hundreds. In networks of such depth, the gradients can often \u201cexplode\u201d (Bengio et al., 1994), what makes training by backpropagation much harder. We noticed that the gradients w.r.t. the intermediate values inside the backpropagation were so large, that they sometimes led to an overflow in single-precision floating-point arithmetic. Therefore, we clipped the gradients w.r.t. the activations, within the execution of the backpropagation algorithm. More precisely, each coordinate is separately cropped into the range [\u2212C1, C1] for some constant C1. Before updating parameters, we also globally rescale the whole gradient vector, so that its L2 norm is not bigger than some constant value C2.\nNoise We added random Gaussian noise to the computed gradients after the backpropagation step. The variance of this noise decays exponentially during the training.\nEnforcing Distribution Constraints For very deep networks, a small error in one place can propagate to a huge error in some other place. This was the case with our pointers: they are probability distributions over memory cells and they should sum up to 1. However, after a number of operations are applied, they can accumulate error as a result of inaccurate floating-point arithmetic.\nWe have a special layer which is responsible for rescaling all values (multiplying by the inverse of their sum), to make sure they always represent a probability distribution. We add this layer to our model in a few critical places (eg. after the softmax operation)5.\nEntropy While searching for a solution, the network can fix the pointer distribution on some particular value. This is advantageous at the end of training, because ideally we would like to be able to discretize the model. However, if this happens at the begin of the training, it could force the network to stay in a local minimum, with a small chance of moving the probability mass to some other value. To address this problem, we encourage the network to explore the space of solutions by adding an \u201dentropy bonus\u201d, that decreases over time. More precisely, for every distribution outputted by the controller, we subtract from the cost function the entropy of the distribution multiplied by some coefficient, which decreases exponentially during the training.\nLimiting the values of logarithms There are two places in our model where the logarithms are computed \u2014 in the cost function and in the entropy computation. Inputs to whose logarithms can be very small numbers, which may cause very big values of the cost function or even overflows in floating-point arithmetic. To prevent this phenomenon we use log(max(x, )) instead of log(x) for some small hyperparameter whenever a logarithm is computed.\n5We do not however backpropagate through these renormalizing operations, i.e. during the backward pass we simply assume that they are identities."}, {"heading": "4.2 TASKS", "text": "We now describe the tasks used in our experiments. For every task, the input is given to the network in the memory tape, and the network\u2019s goal is to modify the memory according to the task\u2019s specification. We allow the network to modify the original input. The final error for a test case is computed as cm , where c is the number of correctly written cells, and m represents the total number of cells that should be modified.\nDue to limited space, we describe the tasks only briefly here. The detailed memory layout of inputs and outputs can be found in the Appendix A.\n1. Access Given a value k and an array A, return A[k].\n2. Increment Given an array, increment all its elements by 1.\n3. Copy Given an array and a pointer to the destination, copy all elements from the array to the given location.\n4. Reverse Given an array and a pointer to the destination, copy all elements from the array in reversed order.\n5. Swap Given two pointers p, q and an array A, swap elements A[p] and A[q].\n6. Permutation Given two arrays of n elements: P (contains a permutation of numbers 1, . . . , n) and A (contains random elements), permutate A according to P .\n7. ListK Given a pointer to the head of a linked list and a number k, find the value of the k-th element on the list.\n8. ListSearch Given a pointer to the head of a linked list and a value v to find return a pointer to the first node on the list with the value v.\n9. Merge Given pointers to 2 sorted arrays A and B, merge them.\n10. WalkBST Given a pointer to the root of a Binary Search Tree, and a path to be traversed (sequence of left/right steps), return the element at the end of the path."}, {"heading": "4.3 MODULES", "text": "In all of our experiments we used the same sequence of 14 modules: READ (described in Sec. 3.2), ZERO(a, b) = 0, ONE(a, b) = 1, TWO(a, b) = 2, INC(a, b) = (a+1) mod M , ADD(a, b) = (a+b) mod M , SUB(a, b) = (a\u2212b) mod M , DEC(a, b) = (a\u22121) mod M , LESS-THAN(a, b) = [a < b], LESS-OR-EQUAL-THAN(a, b) = [a \u2264 b], EQUALITY-TEST(a, b) = [a = b], MIN(a, b) = min(a, b), MAX(a, b) = max(a, b), WRITE (described in Sec. 3.2).\nWe also considered settings in which the module sequence is repeated many times, e.g. there are 28 modules, where modules number 1. and 15. are READ, modules number 2. and 16. are ZERO and so on. The number of repetitions is a hyperparameter."}, {"heading": "4.4 RESULTS", "text": "Overall, we were able to find parameters that achieved an error 0 for all problems except Merge and WalkBST (where we got an error of \u2264 1%). As described in 4.2, our metric is an accuracy on the memory cells that should be modified. To compute it, we take the continuous memory state produced by our network, then discretize it (every cell will contain the value with the highest probability), and finally compare with the expected output. The results of the experiments are summarized in Table 1.\nBelow we describe our results on all 10 tasks in more detail. We divide them into 2 categories: \u201deasy\u201d and \u201dhard\u201d tasks. Easy tasks is a category of tasks that achieved low error scores for many sets of parameters and we did not have to spend much time trying to tune them. First 5 problems from our task list belong to this category. Hard tasks, on the other hand, are problems that often trained to low error rate only in a very small number of cases, eg. 1 out of 100."}, {"heading": "4.4.1 EASY TASKS", "text": "This category includes the following problems: Access, Increment, Copy, Reverse, Swap. For all of them we were able to find many sets of hyperparameters that achieved error 0, or close to it without much effort.\nWe also tested how those solutions generalize to longer input sequences. To do this, for every problem we selected a model that achieved error 0 during the training, and tested it on inputs with lengths up to 506. To perform these tests we also increased the memory size and the number of allowed timesteps.\nIn all cases the model solved the problem perfectly, what shows that it generalizes not only to longer input sequences, but also to different memory sizes and numbers of allowed timesteps. Moreover, the discretized version of the model (see Sec. 3.4 for details) also solves all the problems perfectly. These results show that the NRAM model naturally learns \u201calgorithmic\u201d solutions, which generalize well.\nWe were also interested if the found solutions generalize to sequences of arbitrary length. It is easiest to verify in the case of a discretized model with a feedforward controller. That is because then\n6Unfortunately we could not test for lengths longer than 50 due to the memory restrictions.\ncircuits outputed by the controller depend solely on the values of registers, which are integers. We manually analysed circuits for problems Copy and Increment and verified that found solutions generalize to inputs of arbitrary length, assuming that the number of allowed timesteps is appropriate.\n4.4.2 HARD TASKS\nThis category includes: Permutation, ListK, ListSearch, Merge and WalkBST. For all of them we had to perform an extensive random search to find a good set of hyperparameters. Usually, most of the parameter combinations were stuck on the starting curriculum level with a high error of 50%\u2212 70%. For the first 3 tasks we managed to train the network to achieve error 0. For WalkBST and Merge the training errors were 0.3% and 1% respectively. For training those problems we had to introduce additional techniques described in Sec. 4.1.\nFor Permutation, ListK and WalkBST our model generalizes very well and achieves low error rates on inputs at least twice longer than the ones seen during the training. The exact generalization errors are shown in Fig. 2.\nThe only hard problem on which our model discretizes well is Permutation \u2014 on this task the discretized version of the model produces exactly the same outputs as the original model on all cases tested. For the remaining four prob-\nlems the discretized version of the models perform very poorly (error rates \u2265 70%). We believe that better results may be obtained by using some techniques encouraging discretization during the training 7.\nWe noticed that the training procedure is very unstable and the error often raises from a few percents to e.g. 70% in just one epoch. Moreover, even if we use the best found set of hyperparameters, the percent of random seeds that converges to error 0 was usually equal about 11%. We observed that the percent of converging seeds is much lower if we do not add noise to the gradient \u2014 in this case only about 1% of seeds converge.\n7One could for example add at later stages of training a penalty proportional to the entropy of the intermediate values of registers/memory."}, {"heading": "4.5 COMPARISON TO EXISTING MODELS", "text": "A comparison to other models is challenging because we are the first to consider problems with pointers. The NTM can solve tasks like Copy or Reverse, but it suffers from the inability to naturally store a pointer to a fixed location in the memory. This makes it unlikely that it could solve tasks such as ListK, ListSearch or WalkBST since the pointers used in these tasks refer to absolute positions.\nWhat distinguishes our model from most of the previous attempts (including NTMs, Memory Networks, Pointer Networks) is the lack of content-based addressing. It was a deliberate design decision, since this kind of addressing inherently slows down the memory access. In contrast, our model \u2014 if discretized \u2014 can access the memory in a constant time.\nThe NRAM is also the first model that we are aware of employing a differentiable mechanism for deciding when to finish the computation."}, {"heading": "4.6 EXEMPLARY EXECUTION", "text": "We present one example execution of our model for the problem Copy. For the example, we use a very small model with 12 memory cells, 4 registers and the standard set of 14 modules. The controller for this model is a feedforward network, and we run it for 11 timesteps. Table 2 contains, for every timestep, the state of the memory and registers at the begin of the timestep.\nThe model can execute different circuits at different timesteps. In particular, we observed that the first circuit is slightly different from the rest, since it needs to handle the initialization. Starting from the second step all generated circuits are the same. We present this circuit in Fig. 3. The register r2 is constant and keeps the offset between the destination array and the source array (6 \u2212 1 = 5 in this case). The register r3 is responsible for incrementing the pointer in the source array. Its value is copied to r48, the register used by the READ module. For the WRITE module, it also uses r4 which is shifted by r2. The register r1 is unused. This solution generalizes to sequences of arbitrary length."}, {"heading": "5 CONCLUSIONS", "text": "In this paper we presented the Neural Random-Access Machine, which can learn to solve problems that require explicit manipulation and dereferencing of pointers.\nWe showed that this model can learn to solve a number of algorithmic problems and generalize well to inputs longer than ones seen during the training. In particular, for some problems it generalizes to inputs of arbitrary length.\nHowever, we noticed that the optimization problem resulting from the backpropagating through the execution trace of the program is very challenging for standard optimization techniques. It seems likely that a method that can search in an easier \u201cabstract\u201d space would be more effective at solving such problems.\n8In our case r3 < r2, so the MIN module always outputs the value r3 + 1. It is not satisfied in the last timestep, but then the array is already copied."}, {"heading": "A DETAILED TASKS DESCRIPTIONS", "text": "In this section we describe in details the memory layout of inputs and outputs for the tasks used in our experiments. In all descriptions below, big letters represent arrays and small letters represents pointers. NULL denotes the value 0 and is used to mark the end of an array or a missing next element in a list or a binary tree.\n1. Access Given a value k and an array A, return A[k]. Input is given as k,A[0], .., A[n \u2212 1], NULL and the network should replace the first memory cell with A[k].\n2. Increment Given an array A, increment all its elements by 1. Input is given as A[0], ..., A[n\u2212 1], NULL and the expected output is A[0] + 1, ..., A[n\u2212 1] + 1.\n3. Copy Given an array and a pointer to the destination, copy all elements from the array to the given location. Input is given as p,A[0], ..., A[n\u22121] where p points to one element after A[n\u22121]. The expected output isA[0], ..., A[n\u22121] at positions p, ..., p+n\u22121 respectively.\n4. Reverse Given an array and a pointer to the destination, copy all elements from the array in reversed order. Input is given as p,A[0], ..., A[n \u2212 1] where p points one element after A[n\u22121]. The expected output isA[n\u22121], ..., A[0] at positions p, ..., p+n\u22121 respectively.\n5. Swap Given two pointers p, q and an array A, swap elements A[p] and A[q]. Input is given as p, q, A[0], .., A[p], ..., A[q], ..., A[n \u2212 1], 0. The expected modified array A is: A[0], ..., A[q], ..., A[p], ..., A[n\u2212 1].\n6. Permutation Given two arrays of n elements: P (contains a permutation of numbers 0, . . . , n\u2212 1) and A (contains random elements), permutate A according to P . Input is given as a, P [0], ..., P [n \u2212 1], A[0], ..., A[n \u2212 1], where a is a pointer to the array A. The expected output is A[P [0]], ..., A[P [n\u2212 1]], which should override the array P .\n7. ListK Given a pointer to the head of a linked list and a number k, find the value of the k-th element on the list. List nodes are represented as two adjacent memory cells: a pointer to the next node and a value. Elements are in random locations in the memory, so that the network needs to follow the pointers to find the correct element. Input is given as: head, k, out, ... where head is a pointer to the first node on the list, k indicates how many hops are needed and out is a cell where the output should be put.\n8. ListSearch Given a pointer to the head of a linked list and a value v to find return a pointer to the first node on the list with the value v. The list is placed in memory in the same way as in the task ListK. We fill empty memory with \u201ctrash\u201d values to prevent the network from \u201ccheating\u201d and just iterating over the whole memory.\n9. Merge Given pointers to 2 sorted arrays A and B, and the pointer to the output o, merge the two arrays into one sorted array. The input is given as: a, b, o, A[0], .., A[n \u2212 1], G,B[0], ..., B[m\u2212 1], G, where G is a special guardian value, a and b point to the first elements of arrays A and B respectively, and o points to the address after the second G. The n+m element should be written in correct order starting from position o.\n10. WalkBST Given a pointer to the root of a Binary Search Tree, and a path to be traversed, return the element at the end of the path. The BST nodes are represented as tripes (v, l, r), where v is the value, and l, r are pointers to the left/right child. The triples are placed randomly in the memory. Input is given as root, out, d1, d2, ..., dk, NULL, ..., where root points to the root node and out is a slot for the output. The sequence d1...dk, di \u2208 {0, 1} represents the path to be traversed: di = 0 means that the network should go to the left child, di = 1 represents going to the right child."}, {"heading": "B DETAILS OF CURRICULUM TRAINING", "text": "As noticed in several papers (Bengio et al., 2009; Zaremba & Sutskever, 2014), curriculum learning is crucial for training deep networks on very complicated problems. We followed the curriculum learning schedule from Zaremba & Sutskever (2014) without any modifications.\nFor each of the tasks we have manually defined a sequence of subtasks with increasing difficulty, where the difficulty is usually measured by the length of the input sequence. During training the input-output examples are sampled from a distribution that is determined by the current difficulty level D. The level is increased (up to some maximal value) whenever the error rate of the model goes below some threshold. Moreover, we ensure that successive increases of D are separated by some number of batches.\nIn more detail, to generate an input-output example we first sample a difficulty d from a distribution determined by the current level D and then draw the example with the difficulty d. The procedure for sampling d is the following:\n\u2022 with probability 10%: pick d uniformly at random from the set of all possible difficulties; \u2022 with probability 25%: pick d uniformly from [1, D + e], where e is a sample from a geo-\nmetric distribution with a success probability 1/2; \u2022 with probability 65%: set d = D + e, where e is sampled as above.\nNotice that the above procedure guarantees that every difficulty d can be picked regardless of the current level D, which has been shown to increase performance Zaremba & Sutskever (2014)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Listen, attend and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Luong", "Minh-Thang", "Pham", "Hieu", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "A formal theory of inductive inference", "author": ["Solomonoff", "Ray J"], "venue": "i. Information and control,", "citeRegEx": "Solomonoff and J.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff and J.", "year": 1964}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "The first model that we know of that attempted to train extremely deep networks with a large memory and few parameters is the Neural Turing Machine (NTM) (Graves et al., 2014) \u2014 a computationally universal deep neural network that is trainable with backpropagation.", "startOffset": 154, "endOffset": 175}, {"referenceID": 5, "context": "Other models with this property include variants of Stack-Augmented recurrent neural networks (Joulin & Mikolov, 2015; Grefenstette et al., 2015), and the Grid-LSTM (Kalchbrenner et al.", "startOffset": 94, "endOffset": 145}, {"referenceID": 8, "context": ", 2015), and the Grid-LSTM (Kalchbrenner et al., 2015)\u2014of which the Grid-LSTM has achieved the greatest success on both synthetic and real tasks.", "startOffset": 27, "endOffset": 54}, {"referenceID": 4, "context": "The most relevant recent paper is Neural Turing Machines (NTMs) (Graves et al., 2014).", "startOffset": 64, "endOffset": 85}, {"referenceID": 8, "context": "The Grid-LSTM (Kalchbrenner et al., 2015) is a highly interesting extension of LSTM, which allows to use LSTM cells for both deep and sequential computation.", "startOffset": 14, "endOffset": 41}, {"referenceID": 10, "context": "Indeed, this model has proven to be highly versatile, and variants of this model have achieved state-of-the-art results on machine translation (Luong et al., 2015), speech recognition (Chan et al.", "startOffset": 143, "endOffset": 163}, {"referenceID": 3, "context": ", 2015), speech recognition (Chan et al., 2015), and syntactic parsing (Vinyals et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 14, "context": ", 2015), and syntactic parsing (Vinyals et al., 2014), without the use of almost any domain-specific tuning.", "startOffset": 31, "endOffset": 53}, {"referenceID": 2, "context": "The most relevant recent paper is Neural Turing Machines (NTMs) (Graves et al., 2014). It was the first paper to explicitly suggest the notion that it is worth training a computationally universal neural network, and achieved encouraging results. A follow-up model that had the goal of learning algorithms was the Stack-Augmented Recurrent Neural Network (Joulin & Mikolov, 2015) This work demonstrated that the Stack-Augmented RNN can generalize to long problem instances from short problem instances. A related model is the Reinforcement Learning Neural Turing Machine (Zaremba & Sutskever, 2015), which attempted to use reinforcement learning techniques to train a discrete-continuous hybrid model. The memory network (Weston et al., 2014) is an early model that attempted to explicitly separate the memory from computation in a neural network model. The followup work of Sukhbaatar et al. (2015) combined the memory network with the soft attention mechanism, which allowed it to be trained with less supervision.", "startOffset": 65, "endOffset": 900}, {"referenceID": 0, "context": ", 2015) is somewhat different from the above models in that it does not have a writable memory \u2014 it is more similar to the attention model of Bahdanau et al. (2014) in this regard.", "startOffset": 142, "endOffset": 165}, {"referenceID": 0, "context": ", 2015) is somewhat different from the above models in that it does not have a writable memory \u2014 it is more similar to the attention model of Bahdanau et al. (2014) in this regard. Despite not having a memory, this model was able to solve a number of difficult algorithmic problems that include the convex hull and the approximate 2D travelling salesman problem (TSP). Finally, it is important to mention the attention model of Bahdanau et al. (2014). Although this work is not explicitly aimed at learning algorithms, it is by far the most practical model that has an \u201calgorithmic bent\u201d.", "startOffset": 142, "endOffset": 451}, {"referenceID": 2, "context": "Curriculum learning As noticed in several papers (Bengio et al., 2009; Zaremba & Sutskever, 2014), curriculum learning is crucial for training deep networks on very complicated problems.", "startOffset": 49, "endOffset": 97}, {"referenceID": 1, "context": "Curriculum learning As noticed in several papers (Bengio et al., 2009; Zaremba & Sutskever, 2014), curriculum learning is crucial for training deep networks on very complicated problems. We followed the curriculum learning schedule from Zaremba & Sutskever (2014) without any modifications.", "startOffset": 50, "endOffset": 264}, {"referenceID": 1, "context": "In networks of such depth, the gradients can often \u201cexplode\u201d (Bengio et al., 1994), what makes training by backpropagation much harder.", "startOffset": 61, "endOffset": 82}], "year": 2015, "abstractText": "In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions.", "creator": "LaTeX with hyperref package"}}}