{"id": "1511.06396", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Multilingual Relation Extraction using Compositional Universal Schema", "abstract": "When building a knowledge base (KB) of entities and relations from multiple structured KBs and text, universal schema represents the union of all input schema, by jointly embedding all relation types from input KBs as well as textual patterns expressing relations. In previous work, textual patterns are parametrized as a single embedding, preventing generalization to unseen textual patterns. In this paper we employ an LSTM to compositionally capture the semantics of relational text. We dramatically demonstrate the flexibility of our approach by evaluating in a multilingual setting, in which the English training data entities overlap with the seed KB, but the Spanish text does not. Additional improvements are obtained by tying word embeddings across languages. In extensive experiments on the English and Spanish TAC KBP benchmark, our techniques provide substantial accuracy improvements. Furthermore we find that training with the additional non-overlapping Spanish also improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in low-resource domains and languages.", "histories": [["v1", "Thu, 19 Nov 2015 21:42:23 GMT  (36kb)", "https://arxiv.org/abs/1511.06396v1", "Under review as a conference paper at ICLR 2016"], ["v2", "Thu, 3 Mar 2016 20:28:36 GMT  (313kb,D)", "http://arxiv.org/abs/1511.06396v2", "Accepted to NAACL 2016"]], "COMMENTS": "Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["patrick verga", "david belanger", "emma strubell", "benjamin roth", "andrew mccallum"], "accepted": true, "id": "1511.06396"}, "pdf": {"name": "1511.06396.pdf", "metadata": {"source": "CRF", "title": "Multilingual Relation Extraction using Compositional Universal Schema", "authors": ["Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum"], "emails": ["mccallum}@cs.umass.edu"], "sections": [{"heading": null, "text": "Universal schema builds a knowledge base (KB) of entities and relations by jointly embedding all relation types from input KBs as well as textual patterns expressing relations from raw text. In most previous applications of universal schema, each textual pattern is represented as a single embedding, preventing generalization to unseen patterns. Recent work employs a neural network to capture patterns\u2019 compositional semantics, providing generalization to all possible input text. In response, this paper introduces significant further improvements to the coverage and flexibility of universal schema relation extraction: predictions for entities unseen in training and multilingual transfer learning to domains with no annotation. We evaluate our model through extensive experiments on the English and Spanish TAC KBP benchmark, outperforming the top system from TAC 2013 slot-filling using no handwritten patterns or additional annotation. We also consider a multilingual setting in which English training data entities overlap with the seed KB, but Spanish text does not. Despite having no annotation for Spanish data, we train an accurate predictor, with additional improvements obtained by tying word embeddings across languages. Furthermore, we find that multilingual training improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in a variety of languages and domains."}, {"heading": "1 Introduction", "text": "The goal of automatic knowledge base construction (AKBC) is to build a structured knowledge base (KB) of facts using a noisy corpus of raw text evidence, and perhaps an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008). AKBC supports downstream reasoning at a high level about extracted entities and their relations, and thus has broad-reaching applications to a variety of domains.\nOne challenge in AKBC is aligning knowledge from a structured KB with a text corpus in order to perform supervised learning through distant supervision. Universal schema (Riedel et al., 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns. This propagates information between KB annotation and corresponding textual evidence.\nThe above applications of universal schema express each text relation as a distinct item to be embedded. This harms its ability to generalize to inputs not precisely seen at training time. Recently, Toutanova et al. (2015) addressed this issue by embedding text patterns using a deep sentence encoder, which captures the compositional semantics of textual relations and allows for prediction on inputs never seen before.\nThis paper further expands the coverage abilities of universal schema relation extraction by introducing techniques for forming predictions for new entities unseen in training and even for new domains with no associated annotation. In the extreme example of domain adaptation to a completely new language, we may have limited linguistic resources or labeled data such as treebanks, and only rarely a KB with adequate coverage. Our method performs multilingual transfer learning, providing a predictive model for a language with no coverage in an existing KB, by leveraging common representations for shared entities across text corpora. As depicted in Figure 1, we simply require that one language have an available KB of seed facts. We can further improve our models by tying a small set of word embeddings across languages using only simple knowledge about word-level translations, learning to embed semantically similar textual patterns from different languages into the same latent space.\nIn extensive experiments on the TAC Knowledge Base Population (KBP) slot-filling benchmark we outperform the top 2013 system with an F1 score of 40.7 and perform relation extraction in Spanish with no labeled data or direct overlap between the Spanish training corpus and\nar X\niv :1\n51 1.\n06 39\n6v 2\n[ cs\n.C L\n] 3\nM ar\n2 01\nthe training KB, demonstrating that our approach is wellsuited for broad-coverage AKBC in low-resource languages and domains. Interestingly, joint training with Spanish improves English accuracy."}, {"heading": "2 Background", "text": "AKBC extracts unary attributes of the form (subject, attribute), typed binary relations of the form (subject, relation, object), or higher-order relations. We refer to subjects and objects as entities. This work focuses solely on extracting binary relations, though many of our techniques generalize naturally to unary prediction. Generally, for example in Freebase (Bollacker et al., 2008), higher-order relations are expressed in terms of collections of binary relations.\nWe now describe prior work on approaches to AKBC. They all aim to predict (s, r, o) triples, but differ in terms of: (1) input data leveraged, (2) types of annotation required, (3) definition of relation label schema, and (4) whether they are capable of predicting relations for entities unseen in the training data. Note that all of these methods require pre-processing to detect entities, which may result in additional KB construction errors."}, {"heading": "2.1 Relation Extraction as Link Prediction", "text": "A knowledge base is naturally described as a graph, in which entities are nodes and relations are labeled edges (Suchanek et al., 2007; Bollacker et al., 2008). In the case of knowledge graph completion, the task is akin to link prediction, assuming an initial set of (s, r, o) triples. See Nickel et al. (2015) for a review. No accompanying text data is necessary, since links can be predicted using properties of the graph, such as transitivity. In order to generalize well, prediction is often posed as low-rank matrix or tensor factorization. A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dura\u0301n et al., 2015; Yang\net al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al., 2013). Other approaches model the compositionality of multi-hop paths, typically for question answering (Bordes et al., 2014; Gu et al., 2015; Neelakantan et al., 2015)."}, {"heading": "2.2 Relation Extraction as Sentence Classification", "text": "Here, the training data consist of (1) a text corpus, and (2) a KB of seed facts with provenance, i.e. supporting evidence, in the corpus. Given individual an individual sentence, and pre-specified entities, a classifier predicts whether the sentence expresses a relation from a target schema. To train such a classifier, KB facts need to be aligned with supporting evidence in the text, but this is often challenging. For example, not all sentences containing Barack and Michelle Obama state that they are married. A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015). An additional degree of freedom in these approaches is whether they classify individual sentences or predicting at the corpus level by aggregating information from all sentences containing a given pair of entities before prediction. The former approach is often preferable in practice, due to the simplicity of independently classifying individual sentences and the ease of associating each prediction with a provenance. Prior work has applied deep learning to small-scale relation extraction problems, where functional relationships are detected between common nouns (Li et al., 2015; dos Santos et al., 2015). Xu et al. (2015) apply an LSTM to a parse path, while Zeng et al. (2015) use a CNN on the raw text, with a special temporal pooling operation to separately embed the text around each entity."}, {"heading": "2.3 Open-Domain Relation Extraction", "text": "In the previous two approaches, prediction is carried out with respect to a fixed schema R of possible relations r. This may overlook salient relations that are expressed in the text but do not occur in the schema. In response, open-domain information extraction (OpenIE) lets the text speak for itself: R contains all possible patterns of text occurring between entities s and o (Banko et al., 2007; Etzioni et al., 2008; Yates and Etzioni, 2007). These are obtained by filtering and normalizing the raw text. The approach offers impressive coverage, avoids issues of distant supervision, and provides a useful exploratory tool. On the other hand, OpenIE predictions are difficult to use in downstream tasks that expect information from a fixed schema.\nTable 1 provides examples of OpenIE patterns. The examples in row two and three illustrate relational contexts\nfor which similarity is difficult to be captured by an OpenIE approach because of their syntactically complex constructions. This motivates the technique in Section 3.2, which uses a deep architecture applied to raw tokens, instead of rigid rules for normalizing text to obtain patterns."}, {"heading": "2.4 Universal Schema", "text": "When applying Universal Schema (Riedel et al., 2013) (USchema) to relation extraction, we combine the OpenIE and link-prediction perspectives. By jointly modeling both OpenIE patterns and the elements of a target schema, the method captures broader relational structure than multi-class classification approaches that just model the target schema. Furthermore, the method avoids the distant supervision alignment difficulties of Section 2.2.\nRiedel et al. (2013) augment a knowledge graph from a seed KB with additional edges corresponding to OpenIE patterns observed in the corpus. Even if the user does not seek to predict these new edges, a joint model over all edges can exploit regularities of the OpenIE edges to improve modeling of the labels from the target schema.\nThe data still consist of (s, r, o) triples, which can be predicted using link-prediction techniques such as lowrank factorization. Riedel et al. (2013) explore a variety of approximations to the 3-mode (s, r, o) tensor. One such probabilistic model is:\nP ((s, r, o)) = \u03c3 ( u>s,ovr ) , (1)\nwhere \u03c3() is a sigmoid function, us,o is an embedding of the entity pair (s, o), and vr is an embedding of the relation r, which may be an OpenIE pattern or a relation from the target schema. All of the exposition and results in this paper use this factorization, though many of the techniques we present later could be applied easily to\nthe other factorizations described in Riedel et al. (2013). Note that learning unique embeddings for OpenIE relations does not guarantee that similar patterns, such as the final two in Table 1, will be embedded similarly.\nAs with most of the techniques in Section 2.1, the data only consist of positive examples of edges. The absence of an annotated edge does not imply that the edge is false. In fact, we seek to predict some of these missing edges as true. Riedel et al. (2013) employ the Bayesian Personalized Ranking (BPR) approach of Rendle et al. (2009), which does not explicitly model unobserved edges as negative, but instead seeks to rank the probability of observed triples above unobserved triples.\nRecently, Toutanova et al. (2015) extended USchema to not learn individual pattern embeddings vr, but instead to embed text patterns using a deep architecture applied to word tokens. This shares statistical strength between OpenIE patterns with similar words. We leverage this approach in Section 3.2. Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015)."}, {"heading": "2.5 Multilingual Embeddings", "text": "Much work has been done on multilingual word embeddings. Most of this work uses aligned sentences from the Europarl dataset (Koehn, 2005) to align word embeddings across languages (Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014). Others (Mikolov et al., 2013; Faruqui et al., 2014) align separate singlelanguage embedding models using a word-level dictionary. Mikolov et al. (2013) use translation pairs to learn a linear transform from one embedding space to another.\nHowever, very little work exists on multilingual relation extraction. Faruqui and Kumar (2015) perform multilingual OpenIE relation extraction by projecting all languages to English using Google translate. However, as explained in Section 2.3 the OpenIE paradigm is not amenable to prediction within a fixed schema. Further, their approach does not generalize to low-resource languages where translation is unavailable \u2013 while we use translation dictionaries to improve our results, our experiments demonstrate that our method is effective even without this resource."}, {"heading": "3 Methods", "text": ""}, {"heading": "3.1 Universal Schema as Sentence Classifier", "text": "Similar to many link prediction approaches, (Riedel et al., 2013) perform transductive learning, where a model is learned jointly over train and test data. Predictions are made by using the model to identify edges that were unobserved in the test data but likely to be true. The approach is vulnerable to the cold start problem in collab-\nFigure 2: Universal Schema jointly embeds KB and textual relations from Spanish and English, learning dense representations for entity pairs and relations using matrix factorization. Cells with a 1 indicate triples observed during training (left). The bold score represents a test-time prediction by the model (right). Using transitivity through KB/English overlap and English/Spanish overlap, our model can predict that a text pattern in Spanish evidences a KB relation despite no overlap between Spanish/KB entity pairs. At train time we use BPR loss to maximize the inner product of entity pairs with KB relations and text patterns encoded using a bidirectional LSTM. At test time we score compatibility between embedded KB relations and encoded textual patterns using cosine similarity. In our Spanish model we treat embeddings for a small set of English/Spanish translation pairs as a single word, e.g. casado and married.\npe r:s\npo us e ... pe r:b or\nn_ in\nar g1\n\u2018s w\nife ar g2 ... ar g1 w as b or n\nin ar\ng 2\n... ar g1\nes la\nes\npo sa\nd e a\nrg 2\n... ar g1\nn ac\ni\u00f3\nen ar\ng2 ...\nEnglish Spanish\nBarack Obama/ Michelle Obama\nMar\u00eda M\u00fanera/ Juan M Santos\nBarack Obama/ Hawaii Mar\u00eda M\u00fanera/ Colombia\nBernie Sanders/ Jane O'Meara\n...\n...\n1\n1\n1\n1\n.93\n1 1\n...\n1 bidirectional LSTM\narg1 est\u00e1 casado/married con arg2 max pool\nInput : [per:spouse] [Mar\u00eda M\u00fanera est\u00e1 casado con Juan M Santos]\nper:spouse\ncosine similarity\n.93\norative filtering (Schein et al., 2002): it is unclear how to form predictions for unseen entity pairs, without refactorizing the entire matrix or applying heuristics.\nIn response, this paper re-purposes USchema as a means to train a sentence-level relation classifier, like those in Section 2.2. This allows us to avoid errors from aligning distant supervision to the corpus, but is more deployable for real world applications. It also provides opportunities in Section 3.4 to improve multilingual AKBC.\nWe produce predictions using a very simple approach: (1) scan the corpus and extract a large quantity of triplets (s, rtext, o), where rtext is an OpenIE pattern. For each triplet, if the similarity between the embedding of rtext and the embedding of a target relation rschema is above some threshold, we predict the triplet (s, rschema, o), and its provenance is the input sentence containing (s, rtext, o). We refer to this technique as pattern scoring. In our experiments, we use the cosine distance between the vectors (Figure 2). In Section 7.3, we discuss details for how to make this distance welldefined."}, {"heading": "3.2 Using a Compositional Sentence Encoder to Predict Unseen Text Patterns", "text": "The pattern scoring approach is subject to an additional cold start problem: input data may contain patterns unseen in training. This section describes a method for us-\ning USchema to train a relation classifier that can take arbitrary context tokens (Section 2.3) as input.\nFortunately, the cold start problem for context tokens is more benign than that of entities since we can exploit statistical regularities of text: similar sequences of context tokens should be embedded similarly. Therefore, following Toutanova et al. (2015), we embed raw context tokens compositionally using a deep architecture. Unlike Riedel et al. (2013), this requires no manual rules to map text to OpenIE patterns and can embed any possible input string. The modified USchema likelihood is:\nP ((s, r, o)) = \u03c3 ( u>s,oEncoder(r) ) . (2)\nHere, if r is raw text, then Encoder(r) is parameterized by a deep architecture. If r is from the target schema, Encoder(r) is a produced by a lookup table (as in traditional USchema). Though such an encoder increases the computational cost of test-time prediction over straightforward pattern matching, evaluating a deep architecture can be done in large batches in parallel on a GPU.\nBoth convolutional networks (CNNs) and recurrent networks (RNNs) are reasonable encoder architectures, and we consider both in our experiments. CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Unlike Toutanova et al. (2015), we also consider RNNs, specifically Long-Short Term Memory Networks\n(LSTMs) (Hochreiter and Schmidhuber, 1997). LSTMs have proven successful in a variety of tasks requiring encoding sentences as vectors (Sutskever et al., 2014; Vinyals et al., 2014). In our experiments, LSTMs outperform CNNs.\nThere are two key differences between our sentence encoder and that of Toutanova et al. (2015). First, we use the encoder at test time, since we process the context tokens for held-out data. On the other hand, Toutanova et al. (2015) adopt the transductive approach where the encoder is only used to help train better representations for the relations in the target schema; it is ignored when forming predictions. Second, we apply the encoder to the raw text between entities, while Toutanova et al. (2015) first perform syntactic dependency parsing on the data and then apply an encoder to the path between the two entities in the parse tree. We avoid parsing, since we seek to perform multilingual AKBC, and many languages lack linguistic resources such as treebanks. Even parsing nonnewswire English text, such as tweets, is extremely challenging."}, {"heading": "3.3 Modeling Frequent Text Patterns", "text": "Despite the coverage advantages of using a deep sentence encoder, separately embedding each OpenIE pattern, as in Riedel et al. (2013), has key advantages. In practice, we have found that many high-precision patterns occur quite frequently. For these, there is sufficient data to model them with independent embeddings per pattern, which imposes minimal inductive bias on the relationship between patterns. Furthermore, some discriminative phrases are idiomatic, i.e.. their meaning is not constructed compositionally from their constituents. For these, a sentence encoder may be inappropriate.\nTherefore, pattern embeddings and deep token-based encoders have very different strengths and weaknesses. One values specificity, and models the head of the text distribution well, while the other has high coverage and captures the tail. In experimental results, we demonstrate that an ensemble of both models performs substantially better than either in isolation."}, {"heading": "3.4 Multilingual Relation Extraction with Zero Annotation", "text": "The models described in previous two sections provide broad-coverage relation extraction that can generalize to all possible input entities and text patterns, while avoiding error-prone alignment of distant supervision to a corpus. Next, we describe techniques for an even more challenging generalization task: relation classification for input sentences in completely different languages.\nTraining a sentence-level relation classifier, either using the alignment-based techniques of Section 2.2, or the alignment-free method of Section 3.1, requires an avail-\nable KB of seed facts that have supporting evidence in the corpus. Unfortunately, available KBs have low overlap with corpora in many languages, since KBs have cultural and geographical biases. In response, we perform multilingual relation extraction by jointly modeling a highresource language, such as English, and an alternative language with no KB annotation. This approach provides transfer learning of a predictive model to the alternative language, and generalizes naturally to modeling more languages.\nExtending the training technique of Section 3.1 to corpora in multiple languages can be achieved by factorizing a matrix that mixes data from a KB and from the two corpora. In Figure 1 we split the entities of a multilingual training corpus into sets depending on whether they have annotation in a KB and what corpora they appear in. We can perform transfer learning of a relation extractor to the low-resource language if there are entity pairs occurring in the two corpora, even if there is no KB annotation for these pairs. Note that we do not use the entity pair embeddings at test time: They are used only to bridge the languages during training. To form predictions in the low-resource language, we can simply apply the pattern scoring approach of Section 3.1.\nIn Section 5, we demonstrate that jointly learning models for English and Spanish, with no annotation for the Spanish data, provides fairly accurate Spanish AKBC, and even improves the performance of the English model. Note that we are not performing zero-shot learning of a Spanish model (Larochelle et al., 2008). The relations in the target schema are language-independent concepts, and we have supervision for these in English."}, {"heading": "3.5 Tied Sentence Encoders", "text": "The sentence encoder approach of Section 3.2 is complementary to our multilingual modeling technique: we simply use a separate encoder for each language. This approach is sub-optimal, however, because each sentence encoder will have a separate matrix of word embeddings for its vocabulary, despite the fact that there may be considerable shared structure between the languages. In response, we propose a straightforward method for tying the parameters of the sentence encoders across languages.\nDrawing on the dictionary-based techniques described in Section 2.5, we first obtain a list of word-word translation pairs between the languages using a translation dictionary. The first layer of our deep text encoder consists of a word embedding lookup table. For the aligned word types, we use a single cross-lingual embedding. Details of our approach are described in Appendix 7.5."}, {"heading": "4 Task and System Description", "text": "We focus on the TAC KBP slot-filling task. Much related work on embedding knowledge bases evaluates on\nthe FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015). Here, relation extraction is posed as link prediction on a subset of Freebase. This task does not capture the particular difficulties we address: (1) evaluation on entities and text unseen during training, and (2) zero-annotation learning of a predictor for a low-resource language.\nAlso, note both Toutanova et al. (2015) and Riedel et al. (2013) explore the pros and cons of learning embeddings for entity pairs vs. separate embeddings for each entity. As this is orthogonal to our contributions, we only consider entity pair embeddings, which performed best in both works when given sufficient data."}, {"heading": "4.1 TAC Slot-Filling Benchmark", "text": "The aim of the TAC benchmark is to improve both coverage and quality of relation extraction evaluation compared to just checking the extracted facts against a knowledge base, which can be incomplete and where the provenances are not verified. In the slot-filling task, each system is given a set of paired query entities and relations or \u2018slots\u2019 to fill, and the goal is to correctly fill as many slots as possible along with provenance from the corpus. For example, given the query entity/relation pair (Barack Obama, per:spouse), the system should return the entity Michelle Obama along with sentence(s) whose text expresses that relation. The answers returned by all participating teams, along with a human search (with timeout), are judged manually for correctness, i.e. whether the provenance specified by the system indeed expresses the relation in question.\nIn addition to verifying our models on the 2013 and 2014 English slot-filling task, we evaluate our Spanish models on the 2012 TAC Spanish slot-filling evaluation. Because this TAC track was never officially run, the coverage of facts in the available annotation is very small, resulting in many correct predictions being marked incorrectly as precision errors. In response, we manually annotated all results returned by the models considered in Table 4. Precision and recall are calculated with respect to the union of the TAC annotation and our new labeling1."}, {"heading": "4.2 Retrieval Pipeline", "text": "Our retrieval pipeline first generates all valid slot filler candidates for each query entity and slot, based on entities extracted from the corpus using FACTORIE (McCallum et al., 2009) to perform tokenization, segmentation, and entity extraction. We perform entity linking by heuristically linking all entity mentions from our text corpora to a Freebase entity using anchor text in Wikipedia. Making use of the fact that most Freebase entries contain a link to the corresponding Wikipedia page, we link all\n1Following Surdeanu et al. (2012) we remove facts about undiscovered entities to correct for recall.\nentity mentions from our text corpora to a Freebase entity by the following process: First, a set of candidate entities is obtained by following frequent link anchor text statistics. We then select that candidate entity for which the cosine similarity between the respective Wikipedia and the sentence context of the mention is highest, and link to that entity if a threshold is exceeded.\nAn entity pair qualifies as a candidate prediction if it meets the type criteria for the slot.2 The TAC 2013 English and Spanish newswire corpora each contain about 1 million newswire documents from 2009\u20132012. The document retrieval and entity matching components of our relation extraction pipeline are based on RelationFactory (Roth et al., 2014), the top-ranked system of the 2013 English slot-filling task. We also use the English distantly supervised training data from this system, which aligns the TAC 2012 corpus to Freebase. More details on alignment are described in Appendix 7.4.\nAs discussed in Section 3.3, models using a deep sentence encoder and using a pattern lookup table have complementary strengths and weaknesses. In response, we present results where we ensemble the outputs of the two models by simply taking the union of their individual outputs. Slightly higher results might be obtained through more sophisticated ensembling schemes."}, {"heading": "4.3 Model Details", "text": "All models are implemented in Torch (code publicly available3). Models are tuned to maximize F1 on the 2012 TAC KBP slot-filling evaluation. We additionally tune the thresholds of our pattern scorer on a per-relation basis to maximize F1 using 2012 TAC slot-filling for English and the 2012 Spanish slot-filling development set for Spanish. As in Riedel et al. (2013), we train using the BPR loss of Rendle et al. (2009). Our CNN is implemented as described in Toutanova et al. (2015), using width-3 convolutions, followed by tanh and max pool layers. The LSTM uses a bi-directional architecture where the forward and backward representations of each hidden state are averaged, followed by max pooling over time. See Section 7.2\nWe also report results including an alternate names (AN) heuristic, which uses automatically-extracted rules to detect the TAC \u2018alternate name\u2019 relation. To achieve this, we collect frequent Wikipedia link anchor texts for\n2Due to the difficulty of retrieval and entity detection, the maximum recall for predictions is limited. For this reason, Surdeanu et al. (2012) restrict the evaluation to answer candidates returned by their system and effectively rescaling recall. We do not perform such a re-scaling in our English results in order to compare to other reported results. Our Spanish numbers are rescaled. All scores reflect the \u2018anydoc\u2019 (relaxed) scoring to mitigate penalizing effects for systems not included in the evaluation pool.\n3https://github.com/patverga/ torch-relation-extraction\neach query entity. If a high probability anchor text cooccurs with the canonical name of the query in the same document, we return the anchor text as a slot filler."}, {"heading": "5 Experimental Results", "text": "In experiments on the English and Spanish TAC KBC slot-filling tasks, we find that both USchema and LSTM models outperform the CNN across languages, and that the LSTM tends to perform slightly better than USchema as the only model. Ensembling the LSTM and USchema models further increases final F1 scores in all experiments, suggesting that the two different types of model compliment each other well. Indeed, in Section 5.3 we present quantitative and qualitative analysis of our results which further confirms this hypothesis: the LSTM and USchema models each perform better on different pattern lengths and are characterized by different precision-recall tradeoffs."}, {"heading": "5.1 English TAC Slot-filling Results", "text": "Tables 2 and 3 present the performance of our models on the 2013 and 2014 English TAC slot-filling tasks. Ensembling the LSTM and USchema models improves F1 by 2.2 points for 2013 and 1.7 points for 2014 over the strongest single model on both evaluations, LSTM. Adding the alternative names (AN) heuristic described in Section 4.3 increases F1 by an additional 2 points on 2013, resulting in an F1 score that is competitive with the state-of-the-art. We also demonstrate the effect of jointly learning English and Spanish models on English slot-filling performance. Adding Spanish data improves our F1 scores by 1.5 points on 2013 and 1.1 on 2014 over using English alone. This places are system higher than the top performer at the 2013 TAC slot-filling task even though our system uses no hand-written rules.\nThe state of the art systems on this task all rely on matching handwritten patterns to find additional answers while our models use only automatically generated, indirect supervision; even our AN heuristics (Section 4.2) are automatically generated. The top two 2014 systems were Angeli et al. (2014) and RPI Blender (Surdeanu and Ji., 2014) who achieved F1 scores of 39.5 and 36.4 respectively. Both of these systems used additional active learning annotation. The third place team (Lin et al., 2014) relied on highly tuned patterns and rules and achieved an F1 score of 34.4.\nOur model performs substantially better on 2013 than 2014 for two reasons. First, our RelationFactory (Roth et al., 2014) retrieval pipeline was a top retrieval pipeline on the 2013 task, but was outperformed on the 2014 task which introduced new challenges such as confusable entities. Second, improved training using active learning gave the top 2014 systems a boost in performance. No 2013 systems, including ours, use active learning. Bentor et al. (2014), the 4th place team in the 2014 evaluation, used the same retrieval pipeline (Roth et al., 2014) as our model and achieved an F1 score of 32.1."}, {"heading": "5.2 Spanish TAC Slot-filling Results", "text": "Table 4 presents 2012 Spanish TAC slot-filling results for our multilingual relation extractors trained using zeroannotation transfer learning. Tying word embeddings between the two languages results in substantial improvements for the LSTM. We see that ensembling the nondictionary LSTM with USchema gives a slight boost over USchema alone, but ensembling the dictionary-tied LSTM with USchema provides a significant increase of nearly 4 F1 points over the highest-scoring single model, USchema. Clearly, grounding the Spanish data using a translation dictionary provides much better Spanish word representations. These improvements are complementary to the baseline USchema model, and yield impressive results when ensembled.\nIn addition to embedding semantically similar phrases from English and Spanish to have high similarity, our models also learn high-quality multilingual word embeddings. In Table 5 we compare Spanish nearest neighbors of English query words learned by the LSTM with dictionary ties versus the LSTM with no ties, using no unsupervised pre-training for the embeddings. Both approaches jointly embed Spanish and English word types, using shared entity embeddings, but the dictionary-tied model learns qualitatively better multilingual embeddings."}, {"heading": "5.3 USchema vs LSTM", "text": "We further analyze differences between USchema and LSTM in order to better understand why ensembling the models results in the best performing system. Figure 3 depicts precision-recall curves for the two models on the 2013 slot-filling task. As observed in earlier results, the LSTM achieves higher recall at the loss of\nsome precision, whereas USchema can make more precise predictions at a lower threshold for recall. In Figure 4 we observe evidence for these different precisionrecall trade-offs: USchema scores higher in terms of F1 on shorter patterns whereas the LSTM scores higher on longer patterns. As one would expect, USchema successfully matches more short patterns than the LSTM, making more precise predictions at the cost of being unable to predict on patterns unseen during training. The LSTM can predict using any text between entities observed at test time, gaining recall at the loss of precision. Combining the two models makes the most of their strengths and weaknesses, leading to the highest overall F1.\nQualitative analysis of our English models also suggests that our encoder-based models (LSTM) extract relations based on a wide range of semantically similar patterns that the pattern-matching model (USchema) is unable to score due to a lack of exact string match in the test data. For example, Table 6 lists three examples of the per:children relation that the LSTM finds which USchema does not, as well as three patterns that USchema does find. Though the LSTM patterns are all semantically and syntactically similar, they each contain different specific noun phrases, e.g. Lori, four children, toddler daughter, Lee and Albert, etc. Because these specific nouns weren\u2019t seen during training, USchema fails to find these patterns whereas the LSTM learns to ignore the specific nouns in favor of the overall pattern, that of a parent-child relationship in an obituary. USchema is limited to finding the relations represented by patterns observed during training, which limits the patterns matched at test-time to short and common patterns; all the USchema patterns matched at test time were similar to those listed in Table 6: variants of \u2019s son, \u2019."}, {"heading": "6 Conclusion", "text": "By jointly embedding English and Spanish corpora along with a KB, we can train an accurate Spanish relation extraction model using no direct annotation for relations in the Spanish data. This approach has the added benefit of providing significant accuracy improvements for the English model, outperforming the top system on the 2013 TAC KBC slot filling task, without using the hand-coded rules or additional annotations of alternative systems. By using deep sentence encoders, we can perform prediction for arbitrary input text and for entities unseen in training. Sentence encoders also provides opportunities to improve cross-lingual transfer learning by sharing word embeddings across languages. In future work we will apply this model to many more languages and domains besides newswire text. We would also like to avoid the entity detection problem by using a deep architecture to both identify entity mentions and identify relations between them."}, {"heading": "Acknowledgments", "text": "Many thanks to Arvind Neelakantan for good ideas and discussions. We also appreciate a generous hardware grant from nVidia. This work was supported in part by the Center for Intelligent Information Retrieval, in part by Defense Advanced Research Projects Agency (DARPA) under agreement #FA8750-13-2-0020 and contract #HR0011-15-2-0036, and in part by the National Science Foundation (NSF) grant numbers DMR1534431, IIS-1514053 and CNS-0958392. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon, in part by DARPA via agreement #DFA8750-13-2-0020 and NSF grant #CNS0958392. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}, {"heading": "7 Appendix", "text": ""}, {"heading": "7.1 Additional Qualitative Results", "text": "Qualitative analysis of our multilingual models further suggests that they successfully embed semantically similar relations across languages using tied entity pairs and translation dictionary as grounding. Table 7 lists three top nearest neighbors in English for several Spanish patterns from the text. In each case, the English patterns capture the relation represented in the Spanish text.\nOur model jointly embeds KB relations together with English and Spanish text. We demonstrate that plausible textual patterns are embedded close to the KB relations they express. Table 8 shows top scoring English and Spanish patterns given sample relations from our TAC KB."}, {"heading": "7.2 Implementation and Hyperparameters", "text": "We performed a small grid search over learning rate 0.0001, 0.005, 0.001, dropout 0.0, 0.1, 0.25, 0.5, dimension 50, 100, `2 gradient clipping 1, 10, 50, and epsilon 1e-8, 1e-6, 1e-4. All models are trained for a maximum of 15 epochs. The CNN and LSTM both use 100d embeddings while USchema uses 50d. The CNN and LSTM both learned 100-dimensional word embeddings which were randomly initialized. Using pre-trained embeddings did not substantially affect the results. Entity pair embeddings for the baseline USchema model are randomly\ninitialized. For the models with LSTM and CNN text encoders, entity pair embeddings are initialized using vectors from the baseline USchema model. This performs better than random initialization. We perform `2 gradient clipping to 1 on all models. Universal Schema uses a batch size of 1024 while the CNN and LSTM use 128. All models are optimized using ADAM (Kingma and Ba, 2015) with = 1e \u2212 8, \u03b21 = 0.9, and \u03b22 = 0.999 with a learning rate of .001 for USchema and .0001 for CNN and LSTM. The CNN and LSTM also use dropout of 0.1 after the embedding layer."}, {"heading": "7.3 Details Concerning Cosine Similarity Computation", "text": "We measure the similarity between rtext and rschema by computing the vectors\u2019 cosine similarity. However, such a distance is not well-defined, since the model was trained using inner products between entity vectors and rela-\ntion vectors, not between two relation vectors. The US likelihood is invariant to invertible transformations of the latent coordinate system, since \u03c3 ( u>s,ovr ) =\n\u03c3 ( (A>us,o) >A\u22121vr )\nfor any invertible A. When taking inner products between two v terms, however, the implicit A\u22121 terms do not cancel out. We found that this issue can be minimized, and high quality predictive accuracy can be achieved, simply by using sufficient `2 regularization to avoid implicitly learning an A that substantially stretches the space."}, {"heading": "7.4 Data Pre-processing, Distant Supervision and Extraction Pipeline", "text": "We replace tokens occurring less than 5 times in the corpus with UNK and normalize all digits to # (e.g. Oct11-1988 becomes Oct-##-####). For each sentence, we then extract all entity pairs and the text between them as surface patterns, ignoring patterns longer than 20 tokens. This results in 48 million English \u2018relations\u2019. In Section 7.6, we describe a technique for normalizing the surface patterns. We filter out entity pairs that occurred less than 10 times in the data and extract the largest connected component in this entity co-occurrence graph. This is necessary for the baseline US model, as otherwise learning decouples into independent problems per connected component. Though the components are connected when using sentence encoders, we use only a single component to facilitate a fair comparison between modeling approaches. We add the distant supervision training facts from the RelationFactory system, i.e. 352,236 entitypair-relation tuples obtained from Freebase and high precision seed patterns. The final training data contains a set of 3,980,164 (KB and openIE) facts made up of 549,760 unique entity pairs, 1,285,258 unique relations and 62,841 unique tokens.\nWe perform the same preprocessing on the Spanish data, resulting in 34 million raw surface patterns between entities. We then filter patterns that never occur with an entity pair found in the English data. This yields 860,502 Spanish patterns. Our multilingual model is trained on a combination of these Spanish patterns, the English surface patterns, and the distant supervision data described above. We learn word embeddings for 39,912 unique Spanish word types. After parameter tying for translation pairs (Section 3.5), there are 33,711 additional Spanish words not tied to English."}, {"heading": "7.5 Generation of Cross-Lingual Tied Word Types", "text": "We follow the same procedure for generating translation pairs as (Mikolov et al., 2013). First, we select the top 6000 words occurring in the lowercased Europarl dataset for each language and obtain a Google translation. We then filter duplicates and translations resulting in multiword phrases. We also remove English past participles\n(ending in -ed) as we found the Google translation interprets these as adjectives (e.g., \u2018she read the borrowed book\u2019 rather than \u2018she borrowed the book\u2019) and much of the relational structure in language we seek to model is captured by verbs. This resulted in 6201 translation pairs that occurred in our text corpus. Though higher quality translation dictionaries would likely improve this technique, our experimental results show that such automatically generated dictionaries perform well."}, {"heading": "7.6 Open IE Pattern Normalization", "text": "To improve US generalization, our US relations use logshortened patterns where the middle tokens in patterns longer than five tokens are simplified. For each long pattern we take the first two tokens and last two tokens, and replace all k remaining tokens with the number log k. For example, the pattern Barack Obama is married to a person named Michelle Obama would be converted to: Barack Obama is married [1] person named Michell Obama. This shortening performs slightly better than whole patterns. LSTM and CNN variants use the entire sequence of tokens."}], "references": [{"title": "Open information extraction from the web", "author": ["Banko et al.2007] Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matt Broadhead", "Oren Etzioni"], "venue": "In International Joint Conference on Artificial Intelligence", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "University of texas at austin kbp 2014 slot filling system: Bayesian logic programs for textual inference", "author": ["Bentor et al.2014] Yinon Bentor", "Vidhoon Viswanathan", "Raymond Mooney"], "venue": "In Proceedings of the Seventh Text Analysis Conference: Knowledge Base Population (TAC", "citeRegEx": "Bentor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bentor et al\\.", "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Question answering with subgraph embeddings", "author": ["Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1406.3676", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["Bunescu", "Mooney2007] Razvan Bunescu", "Raymond Mooney"], "venue": "In Annual meeting-association for Computational Linguistics,", "citeRegEx": "Bunescu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2007}, {"title": "Toward an architecture for never-ending language learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Open information extraction from the web", "author": ["Etzioni et al.2008] Oren Etzioni", "Michele Banko", "Stephen Soderland", "Daniel S Weld"], "venue": "Communications of the ACM,", "citeRegEx": "Etzioni et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Etzioni et al\\.", "year": 2008}, {"title": "Multilingual open relation extraction using crosslingual projection", "author": ["Faruqui", "Kumar2015] Manaal Faruqui", "Shankar Kumar"], "venue": "arXiv preprint arXiv:1503.06450", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "arXiv preprint arXiv:1411.4166", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Combining two and three-way embeddings models for link prediction in knowledge bases. CoRR, abs/1506.00999", "author": ["Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet"], "venue": null, "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Incorporating vector space similarity in random walk inference over knowledge bases", "author": ["Gardner et al.2014] Matt Gardner", "Partha Talukdar", "Jayant Krishnamurthy", "Tom Mitchell"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Gardner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2014}, {"title": "Traversing knowledge graphs in vector space. arXiv preprint arXiv:1506.01094", "author": ["Gu et al.2015] Kelvin Gu", "John Miller", "Percy Liang"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Multilingual models for compositional distributed semantics. arXiv preprint arXiv:1404.4641", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "In Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. In 3rd International Conference for Learning Representations (ICLR)", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W. Cohen"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Reading the web with learned syntactic-semantic inference rules", "author": ["Lao et al.2012] Ni Lao", "Amarnag Subramanya", "Fernando Pereira", "William W. Cohen"], "venue": "In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning", "citeRegEx": "Lao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2012}, {"title": "Zero-data learning of new tasks", "author": ["Dumitru Erhan", "Yoshua Bengio"], "venue": "In National Conference on Artificial Intelligence", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "When are tree structures necessary for deep learning of representations? arXiv preprint arXiv:1503.00185", "author": ["Li et al.2015] Jiwei Li", "Dan Jurafsky", "Eudard Hovy"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin et al.2015] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "Proceedings of AAAI", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "FACTORIE: Probabilistic programming via imperatively defined factor graphs", "author": ["Karl Schultz", "Sameer Singh"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "McCallum et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2009}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "In arXiv preprint arXiv:1309.4168v1,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Association for Computational Linguistics and International Joint Conference on Natural Language Processing", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Compositional vector space models for knowledge base completion", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "HansPeter Kriegel"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "A review of relational machine learning for knowledge graphs: From multirelational link prediction to automated knowledge graph construction. arXiv preprint arXiv:1503.00759", "author": ["Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": null, "citeRegEx": "Nickel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rendle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "HLTNAACL", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Injecting logical background knowledge into embeddings for relation extraction", "author": ["Sameer Singh", "Sebastian Riedel"], "venue": "In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)", "citeRegEx": "Rocktaschel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rocktaschel et al\\.", "year": 2015}, {"title": "Relationfactory: A fast, modular and effective system for knowledge base population", "author": ["Roth et al.2014] Benjamin Roth", "Tassilo Barth", "Grzegorz Chrupa\u0142a", "Martin Gropp", "Dietrich Klakow"], "venue": null, "citeRegEx": "Roth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2014}, {"title": "Methods and metrics for cold-start recommendations", "author": ["Alexandrin Popescul", "Lyle H Ungar", "David M Pennock"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Schein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Schein et al\\.", "year": 2002}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Yago: A core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Overview of the english slot filling track at the tac2014", "author": ["Surdeanu", "Ji.2014] Mihai Surdeanu", "Heng Ji"], "venue": null, "citeRegEx": "Surdeanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2014}, {"title": "Multiinstance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. V Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (to appear)", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang et al.2015] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "International Conference on Learning Representations", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Collective cross-document relation extraction without labelled data", "author": ["Yao et al.2010] Limin Yao", "Sebastian Riedel", "Andrew McCallum"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}, {"title": "Universal schema for entity type prediction", "author": ["Yao et al.2013] Limin Yao", "Sebastian Riedel", "Andrew McCallum"], "venue": "In Proceedings of the 2013 workshop on Automated knowledge base construction,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Unsupervised resolution of objects and relations on the web", "author": ["Yates", "Etzioni2007] Alexander Yates", "Oren Etzioni"], "venue": "In North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Yates et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yates et al\\.", "year": 2007}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks. EMNLP", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "The goal of automatic knowledge base construction (AKBC) is to build a structured knowledge base (KB) of facts using a noisy corpus of raw text evidence, and perhaps an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008).", "startOffset": 201, "endOffset": 270}, {"referenceID": 40, "context": "The goal of automatic knowledge base construction (AKBC) is to build a structured knowledge base (KB) of facts using a noisy corpus of raw text evidence, and perhaps an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008).", "startOffset": 201, "endOffset": 270}, {"referenceID": 35, "context": "Universal schema (Riedel et al., 2013) along with its extensions (Yao et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 49, "context": ", 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns.", "startOffset": 34, "endOffset": 126}, {"referenceID": 12, "context": ", 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns.", "startOffset": 34, "endOffset": 126}, {"referenceID": 30, "context": ", 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns.", "startOffset": 34, "endOffset": 126}, {"referenceID": 36, "context": ", 2013) along with its extensions (Yao et al., 2013; Gardner et al., 2014; Neelakantan et al., 2015; Rocktaschel et al., 2015), avoids alignment by jointly embedding KB relations, entities, and surface text patterns.", "startOffset": 34, "endOffset": 126}, {"referenceID": 44, "context": "Recently, Toutanova et al. (2015) addressed this issue by embedding text patterns using a deep sentence encoder, which captures the compositional semantics of textual relations and allows for prediction on inputs never seen before.", "startOffset": 10, "endOffset": 34}, {"referenceID": 40, "context": "A knowledge base is naturally described as a graph, in which entities are nodes and relations are labeled edges (Suchanek et al., 2007; Bollacker et al., 2008).", "startOffset": 112, "endOffset": 159}, {"referenceID": 31, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 11, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 47, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 2, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 45, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 25, "context": "A variety of model variants have been suggested, where the probability of a given edge existing depends on a multi-linear form (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), or non-linear interactions between s, r, and o (Socher et al.", "startOffset": 127, "endOffset": 253}, {"referenceID": 39, "context": ", 2015), or non-linear interactions between s, r, and o (Socher et al., 2013).", "startOffset": 56, "endOffset": 77}, {"referenceID": 3, "context": "Other approaches model the compositionality of multi-hop paths, typically for question answering (Bordes et al., 2014; Gu et al., 2015; Neelakantan et al., 2015).", "startOffset": 97, "endOffset": 161}, {"referenceID": 13, "context": "Other approaches model the compositionality of multi-hop paths, typically for question answering (Bordes et al., 2014; Gu et al., 2015; Neelakantan et al., 2015).", "startOffset": 97, "endOffset": 161}, {"referenceID": 30, "context": "Other approaches model the compositionality of multi-hop paths, typically for question answering (Bordes et al., 2014; Gu et al., 2015; Neelakantan et al., 2015).", "startOffset": 97, "endOffset": 161}, {"referenceID": 25, "context": "See Nickel et al. (2015) for a review.", "startOffset": 4, "endOffset": 25}, {"referenceID": 29, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 34, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 48, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 16, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 42, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 51, "context": "A variety of one-shot and iterative methods have addressed the alignment problem (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Min et al., 2013; Zeng et al., 2015).", "startOffset": 81, "endOffset": 249}, {"referenceID": 24, "context": "Prior work has applied deep learning to small-scale relation extraction problems, where functional relationships are detected between common nouns (Li et al., 2015; dos Santos et al., 2015).", "startOffset": 147, "endOffset": 189}, {"referenceID": 7, "context": ", 2015; dos Santos et al., 2015). Xu et al. (2015) apply an LSTM to a parse path, while Zeng et al.", "startOffset": 12, "endOffset": 51}, {"referenceID": 7, "context": ", 2015; dos Santos et al., 2015). Xu et al. (2015) apply an LSTM to a parse path, while Zeng et al. (2015) use a CNN on the raw text, with a special temporal pooling operation to separately embed the text around each entity.", "startOffset": 12, "endOffset": 107}, {"referenceID": 0, "context": "In response, open-domain information extraction (OpenIE) lets the text speak for itself: R contains all possible patterns of text occurring between entities s and o (Banko et al., 2007; Etzioni et al., 2008; Yates and Etzioni, 2007).", "startOffset": 165, "endOffset": 232}, {"referenceID": 8, "context": "In response, open-domain information extraction (OpenIE) lets the text speak for itself: R contains all possible patterns of text occurring between entities s and o (Banko et al., 2007; Etzioni et al., 2008; Yates and Etzioni, 2007).", "startOffset": 165, "endOffset": 232}, {"referenceID": 35, "context": "When applying Universal Schema (Riedel et al., 2013) (USchema) to relation extraction, we combine the OpenIE and link-prediction perspectives.", "startOffset": 31, "endOffset": 52}, {"referenceID": 34, "context": "When applying Universal Schema (Riedel et al., 2013) (USchema) to relation extraction, we combine the OpenIE and link-prediction perspectives. By jointly modeling both OpenIE patterns and the elements of a target schema, the method captures broader relational structure than multi-class classification approaches that just model the target schema. Furthermore, the method avoids the distant supervision alignment difficulties of Section 2.2. Riedel et al. (2013) augment a knowledge graph from a seed KB with additional edges corresponding to OpenIE patterns observed in the corpus.", "startOffset": 32, "endOffset": 463}, {"referenceID": 34, "context": "When applying Universal Schema (Riedel et al., 2013) (USchema) to relation extraction, we combine the OpenIE and link-prediction perspectives. By jointly modeling both OpenIE patterns and the elements of a target schema, the method captures broader relational structure than multi-class classification approaches that just model the target schema. Furthermore, the method avoids the distant supervision alignment difficulties of Section 2.2. Riedel et al. (2013) augment a knowledge graph from a seed KB with additional edges corresponding to OpenIE patterns observed in the corpus. Even if the user does not seek to predict these new edges, a joint model over all edges can exploit regularities of the OpenIE edges to improve modeling of the labels from the target schema. The data still consist of (s, r, o) triples, which can be predicted using link-prediction techniques such as lowrank factorization. Riedel et al. (2013) explore a variety of approximations to the 3-mode (s, r, o) tensor.", "startOffset": 32, "endOffset": 927}, {"referenceID": 34, "context": "All of the exposition and results in this paper use this factorization, though many of the techniques we present later could be applied easily to the other factorizations described in Riedel et al. (2013). Note that learning unique embeddings for OpenIE relations does not guarantee that similar patterns, such as the final two in Table 1, will be embedded similarly.", "startOffset": 184, "endOffset": 205}, {"referenceID": 33, "context": "Riedel et al. (2013) employ the Bayesian Personalized Ranking (BPR) approach of Rendle et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 33, "context": "(2013) employ the Bayesian Personalized Ranking (BPR) approach of Rendle et al. (2009), which does not explicitly model unobserved edges as negative, but instead seeks to rank the probability of observed triples above unobserved triples.", "startOffset": 66, "endOffset": 87}, {"referenceID": 21, "context": "Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015).", "startOffset": 117, "endOffset": 201}, {"referenceID": 22, "context": "Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015).", "startOffset": 117, "endOffset": 201}, {"referenceID": 12, "context": "Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015).", "startOffset": 117, "endOffset": 201}, {"referenceID": 30, "context": "Additional work has modeled the regularities of multi-hop paths through knowledge graph augmented with text patterns (Lao et al., 2011; Lao et al., 2012; Gardner et al., 2014; Neelakantan et al., 2015).", "startOffset": 117, "endOffset": 201}, {"referenceID": 40, "context": "Recently, Toutanova et al. (2015) extended USchema to not learn individual pattern embeddings vr, but instead to embed text patterns using a deep architecture applied to word tokens.", "startOffset": 10, "endOffset": 34}, {"referenceID": 20, "context": "Most of this work uses aligned sentences from the Europarl dataset (Koehn, 2005) to align word embeddings across languages (Gouws et al.", "startOffset": 67, "endOffset": 80}, {"referenceID": 26, "context": "Most of this work uses aligned sentences from the Europarl dataset (Koehn, 2005) to align word embeddings across languages (Gouws et al., 2015; Luong et al., 2015; Hermann and Blunsom, 2014).", "startOffset": 123, "endOffset": 190}, {"referenceID": 28, "context": "Others (Mikolov et al., 2013; Faruqui et al., 2014) align separate singlelanguage embedding models using a word-level dictionary.", "startOffset": 7, "endOffset": 51}, {"referenceID": 10, "context": "Others (Mikolov et al., 2013; Faruqui et al., 2014) align separate singlelanguage embedding models using a word-level dictionary.", "startOffset": 7, "endOffset": 51}, {"referenceID": 9, "context": ", 2013; Faruqui et al., 2014) align separate singlelanguage embedding models using a word-level dictionary. Mikolov et al. (2013) use translation pairs to learn a linear transform from one embedding space to another.", "startOffset": 8, "endOffset": 130}, {"referenceID": 35, "context": "Similar to many link prediction approaches, (Riedel et al., 2013) perform transductive learning, where a model is learned jointly over train and test data.", "startOffset": 44, "endOffset": 65}, {"referenceID": 38, "context": "orative filtering (Schein et al., 2002): it is unclear how to form predictions for unseen entity pairs, without refactorizing the entire matrix or applying heuristics.", "startOffset": 18, "endOffset": 39}, {"referenceID": 42, "context": "Therefore, following Toutanova et al. (2015), we embed raw context tokens compositionally using a deep architecture.", "startOffset": 21, "endOffset": 45}, {"referenceID": 34, "context": "Unlike Riedel et al. (2013), this requires no manual rules to map text to OpenIE patterns and can embed any possible input string.", "startOffset": 7, "endOffset": 28}, {"referenceID": 6, "context": "CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 55, "endOffset": 117}, {"referenceID": 17, "context": "CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 55, "endOffset": 117}, {"referenceID": 18, "context": "CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).", "startOffset": 55, "endOffset": 117}, {"referenceID": 6, "context": "CNNs have been useful in a variety of NLP applications (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Unlike Toutanova et al. (2015), we also consider RNNs, specifically Long-Short Term Memory Networks", "startOffset": 56, "endOffset": 150}, {"referenceID": 43, "context": "LSTMs have proven successful in a variety of tasks requiring encoding sentences as vectors (Sutskever et al., 2014; Vinyals et al., 2014).", "startOffset": 91, "endOffset": 137}, {"referenceID": 44, "context": "There are two key differences between our sentence encoder and that of Toutanova et al. (2015). First, we use the encoder at test time, since we process the context tokens for held-out data.", "startOffset": 71, "endOffset": 95}, {"referenceID": 44, "context": "There are two key differences between our sentence encoder and that of Toutanova et al. (2015). First, we use the encoder at test time, since we process the context tokens for held-out data. On the other hand, Toutanova et al. (2015) adopt the transductive approach where the encoder is only used to help train better representations for the relations in the target schema; it is ignored when forming predictions.", "startOffset": 71, "endOffset": 234}, {"referenceID": 44, "context": "There are two key differences between our sentence encoder and that of Toutanova et al. (2015). First, we use the encoder at test time, since we process the context tokens for held-out data. On the other hand, Toutanova et al. (2015) adopt the transductive approach where the encoder is only used to help train better representations for the relations in the target schema; it is ignored when forming predictions. Second, we apply the encoder to the raw text between entities, while Toutanova et al. (2015) first perform syntactic dependency parsing on the data and then apply an encoder to the path between the two entities in the parse tree.", "startOffset": 71, "endOffset": 507}, {"referenceID": 34, "context": "Despite the coverage advantages of using a deep sentence encoder, separately embedding each OpenIE pattern, as in Riedel et al. (2013), has key advantages.", "startOffset": 114, "endOffset": 135}, {"referenceID": 23, "context": "Note that we are not performing zero-shot learning of a Spanish model (Larochelle et al., 2008).", "startOffset": 70, "endOffset": 95}, {"referenceID": 2, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 45, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 25, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 47, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 44, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015).", "startOffset": 18, "endOffset": 119}, {"referenceID": 2, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015). Here, relation extraction is posed as link prediction on a subset of Freebase. This task does not capture the particular difficulties we address: (1) evaluation on entities and text unseen during training, and (2) zero-annotation learning of a predictor for a low-resource language. Also, note both Toutanova et al. (2015) and Riedel et al.", "startOffset": 19, "endOffset": 444}, {"referenceID": 2, "context": "the FB15k dataset (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Yang et al., 2015; Toutanova et al., 2015). Here, relation extraction is posed as link prediction on a subset of Freebase. This task does not capture the particular difficulties we address: (1) evaluation on entities and text unseen during training, and (2) zero-annotation learning of a predictor for a low-resource language. Also, note both Toutanova et al. (2015) and Riedel et al. (2013) explore the pros and cons of learning embeddings for entity pairs vs.", "startOffset": 19, "endOffset": 469}, {"referenceID": 27, "context": "Our retrieval pipeline first generates all valid slot filler candidates for each query entity and slot, based on entities extracted from the corpus using FACTORIE (McCallum et al., 2009) to perform tokenization, segmentation, and entity extraction.", "startOffset": 163, "endOffset": 186}, {"referenceID": 41, "context": "1Following Surdeanu et al. (2012) we remove facts about undiscovered entities to correct for recall.", "startOffset": 11, "endOffset": 34}, {"referenceID": 37, "context": "The document retrieval and entity matching components of our relation extraction pipeline are based on RelationFactory (Roth et al., 2014), the top-ranked system of the 2013 English slot-filling task.", "startOffset": 119, "endOffset": 138}, {"referenceID": 33, "context": "As in Riedel et al. (2013), we train using the BPR loss of Rendle et al.", "startOffset": 6, "endOffset": 27}, {"referenceID": 33, "context": "(2013), we train using the BPR loss of Rendle et al. (2009). Our CNN is implemented as described in Toutanova et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 33, "context": "(2013), we train using the BPR loss of Rendle et al. (2009). Our CNN is implemented as described in Toutanova et al. (2015), using width-3 convolutions, followed by tanh and max pool layers.", "startOffset": 39, "endOffset": 124}, {"referenceID": 41, "context": "For this reason, Surdeanu et al. (2012) restrict the evaluation to answer candidates returned by their system and effectively rescaling recall.", "startOffset": 17, "endOffset": 40}, {"referenceID": 37, "context": "7 Roth et al. (2014) 35.", "startOffset": 2, "endOffset": 21}, {"referenceID": 37, "context": "LSTM+USchema ensemble outperforms any single model, including the highly-tuned top 2013 system of Roth et al. (2014), despite using no handwritten patterns.", "startOffset": 98, "endOffset": 117}, {"referenceID": 37, "context": "First, our RelationFactory (Roth et al., 2014) retrieval pipeline was a top retrieval pipeline on the 2013 task, but was outperformed on the 2014 task which introduced new challenges such as confusable entities.", "startOffset": 27, "endOffset": 46}, {"referenceID": 37, "context": "(2014), the 4th place team in the 2014 evaluation, used the same retrieval pipeline (Roth et al., 2014) as our model and achieved an F1 score of 32.", "startOffset": 84, "endOffset": 103}, {"referenceID": 1, "context": "Bentor et al. (2014), the 4th place team in the 2014 evaluation, used the same retrieval pipeline (Roth et al.", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "Universal schema builds a knowledge base (KB) of entities and relations by jointly embedding all relation types from input KBs as well as textual patterns expressing relations from raw text. In most previous applications of universal schema, each textual pattern is represented as a single embedding, preventing generalization to unseen patterns. Recent work employs a neural network to capture patterns\u2019 compositional semantics, providing generalization to all possible input text. In response, this paper introduces significant further improvements to the coverage and flexibility of universal schema relation extraction: predictions for entities unseen in training and multilingual transfer learning to domains with no annotation. We evaluate our model through extensive experiments on the English and Spanish TAC KBP benchmark, outperforming the top system from TAC 2013 slot-filling using no handwritten patterns or additional annotation. We also consider a multilingual setting in which English training data entities overlap with the seed KB, but Spanish text does not. Despite having no annotation for Spanish data, we train an accurate predictor, with additional improvements obtained by tying word embeddings across languages. Furthermore, we find that multilingual training improves English relation extraction accuracy. Our approach is thus suited to broad-coverage automated knowledge base construction in a variety of languages and domains.", "creator": "LaTeX with hyperref package"}}}