{"id": "1511.06522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Integrating Deep Features for Material Recognition", "abstract": "We propose a method for integration of features extracted using deep representations of Convolutional Neural Networks (CNNs) each of which is learned using a different image dataset of objects and materials for material recognition. Given a set of representations of multiple pre-trained CNNs, we first compute activations of features using the representations on the images to select a set of samples which are best represented by the features. Then, we measure the uncertainty of the features by computing the entropy of class distributions for each sample set. Finally, we compute the contribution of each feature to representation of classes for feature selection and integration. We examine the proposed method on three benchmark datasets for material recognition. Experimental results show that the proposed method achieves state-of-the-art performance by integrating deep features. Additionally, we introduce a new material dataset called EFMD by extending Flickr Material Database (FMD). By the employment of the EFMD with transfer learning for updating the learned CNN models, we achieve 84.0%+/-1.8% accuracy on the FMD dataset which is close to human performance that is 84.9%.", "histories": [["v1", "Fri, 20 Nov 2015 08:31:00 GMT  (905kb,D)", "https://arxiv.org/abs/1511.06522v1", "9 pages, 3 figures, Under review as a conference paper at ICLR 2016"], ["v2", "Sat, 28 Nov 2015 14:21:28 GMT  (905kb,D)", "http://arxiv.org/abs/1511.06522v2", "9 pages, 3 figures, Under review as a conference paper at ICLR 2016"], ["v3", "Sun, 13 Dec 2015 13:39:24 GMT  (905kb,D)", "http://arxiv.org/abs/1511.06522v3", "9 pages, 3 figures, Under review as a conference paper at ICLR 2016"], ["v4", "Mon, 22 Feb 2016 14:36:36 GMT  (905kb,D)", "http://arxiv.org/abs/1511.06522v4", "9 pages, 3 figures"], ["v5", "Tue, 5 Apr 2016 09:18:49 GMT  (905kb,D)", "http://arxiv.org/abs/1511.06522v5", "9 pages, 3 figures"], ["v6", "Thu, 21 Apr 2016 10:19:56 GMT  (2998kb,D)", "http://arxiv.org/abs/1511.06522v6", "6 pages"]], "COMMENTS": "9 pages, 3 figures, Under review as a conference paper at ICLR 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yan zhang", "mete ozay", "xing liu", "takayuki okatani"], "accepted": false, "id": "1511.06522"}, "pdf": {"name": "1511.06522.pdf", "metadata": {"source": "CRF", "title": "Integrating Deep Features for Material Recognition", "authors": ["Yan Zhang", "Mete Ozay", "Xing Liu", "Takayuki Okatani"], "emails": ["okatani}@vision.is.tohoku.ac.jp"], "sections": [{"heading": null, "text": "I. INTRODUCTION In this work, we consider the problem of material recognition, which is to identify material categories such as glass or fabric of an object (i.e., the material from which the object is made) from its single RGB image. We are particularly interested in interconnection between material recognition and object recognition, by utilizing which we aim to perform material recognition accurately. To be specific, we want to develop a method that can efficiently transfer feature representations learned on different tasks/datasets including object recognition to a target task of material recognition. Toward this end, using convolutional neural networks (CNNs) [1], [2], we study how to select and integrate multiple features obtained by different models of CNNs trained on different tasks/datasets.\nA few studies of psychophysics [3], [4] imply that material perception in human vision is interconnected with perception of object category. An observation is that human can perceive some material properties and material category of objects only after correct recognition of the object categories. An example is shown in Fig. 1a. This dependency can be reversed; object category of some objects can be correctly recognized only after accurate perception of material category of the objects. In this work, we conjecture that there exist mutual dependencies between perception of object and material. This could be further generalized to wider contexts such as perception of scene and texture, although we focus on the relationship between object and material categories in this work.\nHowever, it is challenging to model such dependencies and utilize them to accurately perform the task (material recognition in our case). For example, it is possible that an image\nwhich belongs to an object category flower will be classified as one of the material categories, plastic, paper, and foliage (see Fig. 1b). In addition to such category-level dependency, we consider representation-level dependency, which will be much more complicated. It is evident that not every learned representation for object recognition is beneficial to material recognition, due to divergent appearance that a material category may exhibit. Hence, it is important to select useful object and material features for the material recognition task.\nIn this paper, we propose a feature selection method to select and combine deep features learned in a transfer learning\nar X\niv :1\n51 1.\n06 52\n2v 6\n[ cs\n.C V\n] 2\n1 A\npr 2\n01 6\nsetting. The contributions of the paper are summarized as follows: \u2022 We propose a method for material recognition by select-\ning and integrating multiple features of different CNN models. They are pre-trained on different datasets/tasks and, if possible and necessary, further fine-tuned on the target dataset/task in advance. \u2022 We introduce an extended version of the benchmark material dataset (namely, FMD [5]), called EFMD. EFMD is ten times larger than the FMD dataset while the images of EFMD are selected to provide surface properties that are similar to the images of FMD. By the employment of EFMD for transfer learning, we achieve 84.0% \u00b1 1.8% accuracy on FMD, which is close to human performance (84.9%) [6].\nThe rest of this paper is organized as follows. We summarize some related work in Section II. The proposed method is introduced in Section III. Section IV provides experiments conducted on several benchmark datasets. Section V concludes this study."}, {"heading": "II. RELATED WORK", "text": "Surface properties of materials, and the relationship between perception of material and object categories are analyzed in [3], [6]. They first proposed a well-designed benchmark dataset called FMD. Then, they designed descriptors to extract hand-crafted features for representation of various surface properties such as color, texture and shape for material recognition in [6]. Moreover, they analyzed the relationship between object and material recognition for accurate and fast perception of materials in [3].\nIn [7], a filter bank based method was developed using CNNs for texture recognition. The authors achieved stateof-the-art performance on several benchmark datasets for texture recognition and material recognition, including 82.4% accuracy on the FMD dataset. In [8], a method was proposed to discover local material attributes from crowdsourced perceptual material distances. They show that without relying on object cues (e.g., outlines, shapes), material recognition can still be performed by employing the discovered local material attributes. In this work, we focus on utilizing object cues for material recognition.\nCommonly used feature selection methods can be categorized into wrapper methods [9]\u2013[11] and filter methods [12]\u2013 [18]. Wrapper methods are used to select features by training a classifier on a subset of features, and determine the utility of these features according to the accuracy of the classifier. Features are ranked according to particular criteria using filter methods. For instance, correlation coefficients were used in [13] to measure importance of each individual feature. Mutual information is also a popular criterion used for ranking features [16]\u2013[18]. A max-relevance and min-redundancy criterion was proposed in [16]. However, in this method, they compute mutual information of each pair of features at each iteration to evaluate redundancy of features. Therefore, the computation cost of the algorithm was increased with the number of\ncandidate features. In [19], a feature selection method was proposed based on a variant of Adaboost. In this method, each feature is considered as a weak classifier. At each iteration, a weak classifier that performs best is selected, and the weights assigned to samples are updated. An advantage of boosting based feature selection methods is running time for the training phase. In this paper, we propose a filter-based feature selection method in order to select the most discriminative features efficiently by minimizing an entropy criteria in a boosting scheme."}, {"heading": "III. FEATURE SELECTION AND INTEGRATION FOR DEEP REPRESENTATIONS", "text": "In this section, we propose a method to analyze and integrate multiple features extracted by different CNNs pretrained on different tasks."}, {"heading": "A. Outline of the method", "text": "Specifically, we first extract features from material images using multiple CNN models trained on different datasets/tasks. Here, by a feature we mean an activation value of a unit (neuron) at a chosen layer of the CNN. If it is possible and necessary, the CNN models may be fine-tuned on the training samples of the target task of material recognition. Similarly to boosting in [19], we then consider each individual feature as a weak classifier, although we do not explicitly train a weak classifier for each feature. Instead, we utilize the class entropy as criteria for measuring how discriminative each feature is. As suggested in [20], [21], the images maximizing a feature are the most representative samples for utilization of the feature. Thus, we calculate class entropy for each feature using a set of images on which the activation value of the feature is maximized, and then use a weighted sum of the class entropy over the image set. A weight is given to each sample in the training data. Starting with equal weights for all the samples, the proposed method updates the weights and selects features in an iterative way. At each iteration, the most discriminative feature is selected for integration according to the weighted class entropy. Meanwhile, the weights of images on which an integrated feature is activated are penalized according to the class entropy. This procedure enables us to select a set of the most discriminative features for a given length of features."}, {"heading": "B. Details of the algorithm", "text": "Suppose that we are given N image datasets Dn = {(I(i), Y (i))}Mni=1, n = 1, 2, . . . , N , where I(i) \u2208 Xn is the ith image, Y (i) \u2208 Yn is the corresponding class label, and the samples {(I(i), Y (i))}Mni=1 are independent and identically distributed (i.i.d.) according to a distribution Pn on Xn\u00d7Yn. We then train a CNN using each dataset Dn for the associated task. The CNNs may have an identical network architecture or different ones. Choosing a layer (or a combination of layers) of each CNN trained on Dn1, we use \u03a6n to denote the mapping from an input image to the activation value of the layer(s) computed by the CNN. Given a new dataset\n1In our experiments, we used VGG-D-16 [2] and the fc7 layer.\nDb = {(I(i), Y (i))}Mbi=1, we express the activation value of an image I(i) of the sample (I(i), Y (i)) \u2208 Db as xinb = \u03a6n(I(i)).\nIn this work, we aim to extract features from a given set of material images Db by employing different CNNs independently trained using images of objects and materials. Therefore, we consider only datasets Dn\u2019s consisting of images of objects or materials. For the sake of simplicity of the notation and concreteness, we denote xm , \u03a6n(I \u2208 Db) and xo , \u03a6n\u2032(I \u2208 Db) as features extracted from an image I \u2208 Db using \u03a6n learned on a dataset of material images Dn, and using \u03a6n\u2032 on a dataset of object images Dn\u2032 , respectively. Then the feature vectors xo and xm are concatenated to obtain a feature vector xc = [xo,xm].\nIn the proposed method, we model class discriminative features using a classifier that will be employed on concatenated features xc by improving discriminative properties of features xm and xo, for classification of material images belonging to a dataset Db. For this purpose, we consider each individual feature as a weak classifier as suggested in [19]. However, we do not explicitly train a weak classifier for each feature. Instead, we first analyze the discriminative property of each individual feature in xm and xo.\nIn CNNs implemented using ReLU activation functions ( [1], [2]), the features that are represented in an individual neuron are visualized by analyzing and selecting the images on which the activation value of the neuron is maximized [20], [21]. In our method, we also employ this idea to measure contribution of features to discrimination of classes. Suppose that we are given a set of concatenated features C = {(xic, Y i)} Mb i=1 extracted from Db. The jth individual feature xc,j of xic is investigated by searching for top K samples that have maximal positive feature values on xc,j :\nTj = {(xkc , Y k)}Kk=1 s.t. xkc,j > x i c,j , x k c,j > 0\n\u2200(xkc , Y k) \u2208 Tj \u2200(xic, Y i) \u2208 C \\ Tj .\n(1)\nNote that, the samples belonging to Tj can reveal some properties of the feature xc,j . Then, we compute the discriminative property of xc,j by computing the class entropy in the set Tj as\nHj(Y ) = \u2212 \u2211\n(xc,Y )\u2208Tj\np(Y ) log p(Y ). (2)\nIn the next part of the proposed method, we select most discriminative features for discrimination of the whole dataset. Given a set of individual features F = {xc,j \u2208 xc}|xc|j=1, we initialize a set of features to be integrated S = \u2205, and assign equal weights wi = 1 to each sample (xci, Y i) \u2208 C. Next, we select a feature that has minimum weighted class entropy as\nxc,\u03c3 = arg min xc,j\u2208F \u2211 (xck,Y k)\u2208Tj wk \u2217Hj(Y ), (3)\nand update the sets S = S\u222a{xc,\u03c3}, F = F\\{xc,\u03c3}. Then, we penalize the top K samples (xck, Y k) \u2208 Tj of the integrated feature using\nwk = wk \u2217 (1 +Hj(Y )\u22121). (4)\nA pseudo-code of our algorithm is shown in Algorithm 1.\nInput : - {\u03a6n}Nn=1: A set of representations each of which is learned using a CNN obtained on a dataset Dn. - Db = {(I(i), Y (i))}Mbi=1: A dataset of images that will be used for inference of representations. - T : The number of integrated features. - K: The number of samples that have maximal activation values on each feature. Output : - S: A set of integrated features. Initialization: - For each (I, Y ) \u2208 Db, extract features using a representation \u03a6n(\u00b7) such that xn = \u03a6n(I), \u2200n = 1, 2, . . . , N . - Concatenate xn, \u2200n and construct xc = [xn]Nn=1. - Define C = {(xic, Y i)} Mb i=1, and set equal weight wi = 1 to each sample in C. - For each individual feature of xc, define the sets F = {xc,j \u2208 xc}|xc|j=1, and S = \u2205.\n1 for j \u2190 1 to |F| do 2 Construct K samples that have maximal feature values on xc,j using (1); 3 Compute class entropy on the set of top K samples using (2); 4 end 5 for t\u2190 1 to T do 6 Normalize the sample weights: wt+1i =\nwti Mb\u2211 j=1 wtj ,\u2200i;\n7 Select the feature that minimizes the weighted class entropy using (3); 8 Penalize the samples using (4); 9 end Algorithm 1: Integration of features extracted using deep representations of CNNs."}, {"heading": "IV. EXPERIMENTAL ANALYSIS", "text": ""}, {"heading": "A. Datasets", "text": "In the experiments, we used three benchmark datasets (see Table I). FMD dataset [5] consists of 10 material categories each of which contains 100 images. In the FMD, the samples were selected manually from Flickr covering different illumination conditions, compositions, colors, and texture. MINC is a large scale material recognition dataset [22]. It contains 3 million images belonging to 23 categories.\nIn addition, we introduce a new dataset called EFMD which is an extended version of the FMD. EFMD consists of the same categories that are used in FMD, each of which contains\n1,000 images. Thus, the size of EFMD is 10 times larger than the size of FMD. While constructing EFMD, we try to make it as similar to FMD as possible in the context of visual perception and recognition. Specifically, we first pick 100,000 images from Flickr by text searching. Then we ask Amazon Mechanical Turkers to choose the images that are similar to FMD images. In each Human Intelligence Task, we present 10 candidate images to a Turker along with three good examples (FMD images) and six bad examples, and ask the Turker to select good ones. Publishing each Human Intelligence Task for three Turkers, we only select images that are selected by all the Turkers. We then manually crop these images to adjust the scale of an object appearing in the images. Note that the images belonging to FMD will not be selected and merged into EFMD to make sure that there is no overlapping between them.\nEach of FMD and EFMD datasets is randomly split into two subsets of equal size; one is used for training and the other is used for testing. This scenario is performed 10 times and the average classification accuracy was reported. For MINC dataset, we directly use the originally provided train, validation and test sets. The details of each dataset and the corresponding experimental setup are shown in Table I."}, {"heading": "B. Details of experimental setups", "text": "In our experiments, we consider four different tasks, which we will refer to as FMD, FMD-2, EFMD, and MINC(val/test), respectively. In each of them, we construct a material representation \u03a6n and an object representation \u03a6\u2032n in different ways described below.\nIn all the tasks, we start with two CNNs pre-trained on MINC and ILSVRC2012 [23], for which we use publicly available pre-trained Caffe [24] models of VGG-D consisting of 16 layers [2]. We then fine-tune these CNNs for the tasks FMD, FMD-2, and EFMD using different samples as shown in Table II. The fine-tuning is performed in such a way that the weights of Conv1-Conv4 layers are fixed, and those of the higher layers are updated. We have found that this selection of fixed/updating layers performs the best for FMD and EFMD. We do not conduct fine-tuning on MINC dataset. This is because MINC has a large number of samples that enables to train a CNN from scratch. Although it is possible to fine-tune the CNN pre-trained on ILSVRC2012 using MINC dataset, it will be no longer a \u2019fine-tuning\u2019 because of the size of the dataset, and thus we use the pre-trained model as is.\nIn the inference phase of each task, we extract material features xm , \u03a6n(I \u2208 Db) and object features xo , \u03a6n\u2032(I \u2208 Db) using the representations learned at the fc7 layer (e.g. using fully connected networks at the 7th layer) of the two\nCNNs, respectively. The concatenated object and material features xc = [xo,xm] are used to obtain a set of integrated features xs. We set K to 10% of the size of training images, and T to 3,000 throughout the experiments. The integrated features are fed into a support vector machines (SVM) with radial basis function (RBF) kernels [25] for training and testing of classifiers."}, {"heading": "C. Performance analysis", "text": "The results are shown in Table III; i) O, ii) M, iii) MO and iv) SMO columns denote the results for the case where only i) object features (xo), ii) material features (xm), iii) concatenated features (xc = [xo,xm]), and iv) the features integrated by proposed method (xs), are used, respectively. Several observations can be made from the results.\nFirstly, the concatenated features (MO) provide better performance than the individual material (M) and object (O) features for all the tasks, indicating that object features contribute to the material classification. Next, features integrated by our method (SMO) further boost the performance obtained using the concatenated features for the task FMD and MINC-val/test, indicating the effectiveness of the proposed method. However, there is practically no difference in accuracy between MO and SMO for the task FMD-2 and EFMD, which we will analyze later.\nIt should also be noted that 82.3 \u00b1 1.7% for FMD and 84.0\u00b11.8% for FMD-2 are better than the performances of the state-of-the-art methods reported in the literature. A method proposed in [26] achieves 82.4 \u00b1 1.5% on FMD, where they combine fc7 features and Fisher Vectors (FV) features pooled from the conv5 3 layer of VGG-D-19. However, we should note that their numbers are obtained in different conditions and thus we cannot make direct comparisons. Differences are that they do not use MINC or EFMD dataset, whereas they use a multi-scale approach, i.e., images with different scales are used\nto compute FV features. It is nonetheless important to note that 84.0 \u00b1 1.8% is the best performance for FMD and is close to human vision (84.9%) [6]. Additionally, 83.93%/83.60% on MINC val/test also outperform the previous ones [22]. In their work, 83.83%/83.4% are obtained by GoogLeNet and 82.45%/82.19% are achieved by VGG-D-16. Note that their CNN models are pre-trained on ILSVRC2012 [23].\nFinally, we can see from the table that the accuracy on FMD-2 is better than FMD. This can be explained by the use of more training samples in FMD-2, where 10,000 EFMD samples in addition to 500 FMD samples are used for training. This also indicates the similarity of EFMD samples to FMD samples, which is consistent with our intention of the creation of EFMD. That said, the accuracy on EFMD, i.e., training on a half of EFMD and test on the rest, is even more higher, indicating that EFMD is easier than FMD and there is some gap between them.\nNow, we analyze the results that the proposed method (SMO) does not improve the concatenated feature (MO) for FMD-2 and EFMD. In order to analyze shareability of class representations among features, we compute average entropy\nH\u0302F = 1|F| |F|\u2211 i=1 Hi(Y ) and H\u0302S = 1|S| |S|\u2211 i=1 Hi(Y ). The results given in Table IV show that entropy decreases as we employ the EFMD for training CNNs. However, we observe that the difference between entropy values of H\u0302F and H\u0302S is \u223c 0.21 for FMD and FMD-2, and \u223c 0.22 for EFMD.\nIn order to further analyze the shareability of class representations, we computed diversity of decisions of classifiers employed on features (xm and xo) extracted using individual representations of objects (O) and materials (M ). For this purpose, we employed five statistical measures [27], [28], namely i) inter-rater agreement (\u03ba) to measure the level of agreement of classifiers while correcting for chance, ii) Q statistics to measure statistical dependency of classifiers, iii) Kohavi-Wolpert variance to measure variance of agreement, iv) measurement of disagreement, v) generalized diversity to measure causal statistical diversity of classifiers.\nThe results are given in Table IV. Note that the performance boosts using the proposed method as the diversity of the classifiers employed on the features extracted using individual representations (O and M ) increases. For instance, the diversity of classifiers employed on the feature sets extracted using FMD dataset is larger than that of the classifiers employed using FMD-2 and EFMD datasets. In addition, the performance difference between classifiers employed on the integrated features (SMO) and the concatenated features (MO) is 3.2%, 0.1% and 0% for FMD, FMD-2 and EFMD, respectively (see Table III). Therefore, we observe that the performance boost obtained using our proposed method increases as diversity of the classifiers employed on individual representations increases."}, {"heading": "D. Robustness analysis", "text": "We analyse how the number of Top-K selected samples and the number (T ) of integrated features affect the classification performance on the FMD dataset; see Fig. 2a and Fig. 2b.\nIn Fig. 2a, the number of integrated features is fixed as 3000. Note that only the samples with positive activation are considered as the top selected samples. For example, if K is selected to cover 100% of the samples, then the ratio of samples selected by the algorithm may be less than 100%. This can be observed if most of the activation values of FMD is 0. In order to analyze the effect of T on the performance, K is fixed as 10%. Although selection of T = 3000 features provides the best performance, less number of integrated features, e.g. 100-400, provides comparable accuracy.\nWe also analyze the relationship between features that are extracted using representations of materials and objects for feature selection. We leave out one split of FMD, and record the number of integrated material and object features using our feature selection method. A comparative result is shown in Fig. 3. As we can see from the figure, there is a gap between the number of selected material and object features in FMD (Fig. 3a), EFMD (Fig. 3c) and MINC (Fig. 3d). However, in Fig. 3b, we observe that the difference between the number of material and object features is less for FMD2 compared to the other datasets. In FMD2, both object and material features are fully fine-tuned using 10,000 EFMD images. If the finetuned features are discriminative for material classification, then the features are equally selected by the proposed method using FMD2. On the other hand, if the features are not sufficiently fine-tuned, then material features may be more discriminative than object features for material recognition. For instance, Table III shows the results for the case where material and object features are not fine-tuned for MINC-val. We obtain 82.45% using individual material features, while we obtain 68.17% using individual object features. Hence, we may obtain larger number of material features than object features as shown in Fig. 3d."}, {"heading": "V. CONCLUSION", "text": "In this work, we propose a method to integrate deep features extracted from multiple CNNs trained on images of materials and objects for material recognition. For this purpose, we first employ a feature selection and integration method to analyze and select deep features by measuring their contribution to representation of material categories. Then, the integrated features are used for material recognition using classifiers. In the\nexperimental results, we obtain state-of-the-art performance by employing the features integrated using the proposed method on several benchmark datasets. In future work, we plan to investigate theoretical properties of the proposed methods for integration of deep representations using various deep learning algorithms such as autoencoders, to perform other tasks such as scene analysis, image classification and detection."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, vol. abs/1409.1556, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Accuracy and speed of material categorization in real-world images", "author": ["L. Sharan", "R. Rosenholtz", "E.H. Adelson"], "venue": "Journal of Vision, vol. 14, no. 10, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal properties of material categorization and material rating: visual vs non-visual material features", "author": ["T. Nagai", "T. Matsushima", "K. Koida", "Y. Tani", "M. Kitazaki", "S. Nakauchi"], "venue": "Vision Research, vol. 115, Part B, pp. 259 \u2013 270, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Material perception: What can you see in a brief glance?", "author": ["L. Sharan", "R. Rosenholtz", "E.H. Adelson"], "venue": "Journal of Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Recognizing materials using perceptually inspired features", "author": ["L. Sharan", "C. Liu", "R. Rosenholtz", "E.H. Adelson"], "venue": "International Journal of Computer Vision, vol. 108, no. 3, pp. 348\u2013371, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep filter banks for texture recognition and segmentation", "author": ["M. Cimpoi", "S. Maji", "A. Vedaldi"], "venue": "CVPR, June 2015, pp. 3828\u20133836.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatically discovering local visual material attributes", "author": ["G. Schwartz", "K. Nishino"], "venue": "CVPR, June 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G.H. John"], "venue": "Artificial Intelligence, vol. 97, no. 12, pp. 273 \u2013 324, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Irrelevant features and the subset selection problem", "author": ["G.H. John", "R. Kohavi", "K. Pfleger"], "venue": "Proc. 11th International Conference on Machine Learning. Morgan Kaufmann, 1994, pp. 121\u2013129.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "Overfitting in making comparisons between variable selection methods", "author": ["J. Reunanen"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1371\u20131382, Mar. 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Selection of relevant features and examples in machine learning", "author": ["A.L. Blum", "P. Langley"], "venue": "Artificial Intelligence, vol. 97, no. 12, pp. 245 \u2013 271, 1997.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Use of the zero norm with linear models and kernel methods", "author": ["J. Weston", "A. Elisseeff", "B. Sch\u00f6lkopf", "M. Tipping"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1439\u20131461, Mar. 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1157\u20131182, Mar. 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Feature selection and feature extraction for text categorization", "author": ["Lewis", "D. David"], "venue": "Proceedings of the Workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 212\u2013 217.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 8, pp. 1226\u20131238, Aug 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 4, pp. 537\u2013550, Jul 1994.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Input feature selection for classification problems", "author": ["N. Kwak", "C.-H. Choi"], "venue": "IEEE Transactions on Neural Networks, vol. 13, no. 1, pp. 143\u2013159, Jan 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "CVPR, vol. 1, 2001, pp. 511\u2013518.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, June 2014, pp. 580\u2013587.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "ECCV 2014, 2014, vol. 8689, pp. 818\u2013833.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Material recognition in the wild with the materials in context database", "author": ["S. Bell", "P. Upchurch", "N. Snavely", "K. Bala"], "venue": "CVPR, June 2015, pp. 3479\u20133487.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV), pp. 1\u201342, April 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Deep filter banks for texture recognition, description, and segmentation", "author": ["M. Cimpoi", "S. Maji", "I. Kokkinos", "A. Vedaldi"], "venue": "CoRR, vol. abs/1507.02620, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Mach. Learn., vol. 51, no. 2, pp. 181\u2013207, May 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Toward this end, using convolutional neural networks (CNNs) [1], [2], we study how to select and integrate multiple features obtained by different models of CNNs trained on different tasks/datasets.", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "Toward this end, using convolutional neural networks (CNNs) [1], [2], we study how to select and integrate multiple features obtained by different models of CNNs trained on different tasks/datasets.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "A few studies of psychophysics [3], [4] imply that material perception in human vision is interconnected with perception of object category.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "A few studies of psychophysics [3], [4] imply that material perception in human vision is interconnected with perception of object category.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "\u2022 We introduce an extended version of the benchmark material dataset (namely, FMD [5]), called EFMD.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "9%) [6].", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "Surface properties of materials, and the relationship between perception of material and object categories are analyzed in [3], [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "Surface properties of materials, and the relationship between perception of material and object categories are analyzed in [3], [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": "Then, they designed descriptors to extract hand-crafted features for representation of various surface properties such as color, texture and shape for material recognition in [6].", "startOffset": 175, "endOffset": 178}, {"referenceID": 2, "context": "Moreover, they analyzed the relationship between object and material recognition for accurate and fast perception of materials in [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 6, "context": "In [7], a filter bank based method was developed using CNNs for texture recognition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In [8], a method was proposed to discover local material attributes from crowdsourced perceptual material distances.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Commonly used feature selection methods can be categorized into wrapper methods [9]\u2013[11] and filter methods [12]\u2013 [18].", "startOffset": 80, "endOffset": 83}, {"referenceID": 10, "context": "Commonly used feature selection methods can be categorized into wrapper methods [9]\u2013[11] and filter methods [12]\u2013 [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Commonly used feature selection methods can be categorized into wrapper methods [9]\u2013[11] and filter methods [12]\u2013 [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "Commonly used feature selection methods can be categorized into wrapper methods [9]\u2013[11] and filter methods [12]\u2013 [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "[13] to measure importance of each individual feature.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Mutual information is also a popular criterion used for ranking features [16]\u2013[18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "Mutual information is also a popular criterion used for ranking features [16]\u2013[18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "A max-relevance and min-redundancy criterion was proposed in [16].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "In [19], a feature selection method was proposed based on a variant of Adaboost.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Similarly to boosting in [19], we then consider each individual feature as a weak classifier, although we do not explicitly train a weak classifier for each feature.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "As suggested in [20], [21], the images maximizing a feature are the most representative samples for utilization of the feature.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "As suggested in [20], [21], the images maximizing a feature are the most representative samples for utilization of the feature.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "1In our experiments, we used VGG-D-16 [2] and the fc7 layer.", "startOffset": 38, "endOffset": 41}, {"referenceID": 18, "context": "For this purpose, we consider each individual feature as a weak classifier as suggested in [19].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "In CNNs implemented using ReLU activation functions ( [1], [2]), the features that are represented in an individual neuron are visualized by analyzing and selecting the images on which the activation value of the neuron is maximized [20], [21].", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "In CNNs implemented using ReLU activation functions ( [1], [2]), the features that are represented in an individual neuron are visualized by analyzing and selecting the images on which the activation value of the neuron is maximized [20], [21].", "startOffset": 59, "endOffset": 62}, {"referenceID": 19, "context": "In CNNs implemented using ReLU activation functions ( [1], [2]), the features that are represented in an individual neuron are visualized by analyzing and selecting the images on which the activation value of the neuron is maximized [20], [21].", "startOffset": 233, "endOffset": 237}, {"referenceID": 20, "context": "In CNNs implemented using ReLU activation functions ( [1], [2]), the features that are represented in an individual neuron are visualized by analyzing and selecting the images on which the activation value of the neuron is maximized [20], [21].", "startOffset": 239, "endOffset": 243}, {"referenceID": 4, "context": "FMD dataset [5] consists of 10 material categories each of which contains 100 images.", "startOffset": 12, "endOffset": 15}, {"referenceID": 21, "context": "MINC is a large scale material recognition dataset [22].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "In all the tasks, we start with two CNNs pre-trained on MINC and ILSVRC2012 [23], for which we use publicly available pre-trained Caffe [24] models of VGG-D consisting of 16 layers [2].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "In all the tasks, we start with two CNNs pre-trained on MINC and ILSVRC2012 [23], for which we use publicly available pre-trained Caffe [24] models of VGG-D consisting of 16 layers [2].", "startOffset": 136, "endOffset": 140}, {"referenceID": 1, "context": "In all the tasks, we start with two CNNs pre-trained on MINC and ILSVRC2012 [23], for which we use publicly available pre-trained Caffe [24] models of VGG-D consisting of 16 layers [2].", "startOffset": 181, "endOffset": 184}, {"referenceID": 21, "context": "45 [22] 68.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "19 [22] 68.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "The integrated features are fed into a support vector machines (SVM) with radial basis function (RBF) kernels [25] for training and testing of classifiers.", "startOffset": 110, "endOffset": 114}, {"referenceID": 25, "context": "A method proposed in [26] achieves 82.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "9%) [6].", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "60% on MINC val/test also outperform the previous ones [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "Note that their CNN models are pre-trained on ILSVRC2012 [23].", "startOffset": 57, "endOffset": 61}, {"referenceID": 26, "context": "For this purpose, we employed five statistical measures [27], [28], namely i) inter-rater agreement (\u03ba) to measure the level of agreement of classifiers while correcting for chance, ii) Q statistics to measure statistical dependency of classifiers, iii) Kohavi-Wolpert variance to measure variance of agreement, iv) measurement of disagreement, v) generalized diversity to measure causal statistical diversity of classifiers.", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "This paper considers the problem of material recognition. Motivated by an observation that there is close interconnection between material recognition and object recognition, we study how to select and integrate multiple features obtained by different models of Convolutional Neural Networks (CNNs) trained in a transfer learning setting. To be specific, we first compute activations of features using representations on images to select a set of samples which are best represented by the features. Then, we measure uncertainty of the features by computing entropy of class distributions for each sample set. Finally, we compute contribution of each feature to representation of classes for feature selection and integration. Experimental results show that the proposed method achieves state-of-the-art performance on two benchmark datasets for material recognition. Additionally, we introduce a new material dataset, named EFMD, which extends Flickr Material Database (FMD). By the employment of the EFMD for transfer learning, we achieve 84.0%\u00b1 1.8% accuracy on the FMD dataset, which is close to reported human performance 84.9%.", "creator": "LaTeX with hyperref package"}}}