{"id": "1511.06939", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2015", "title": "Session-based Recommendations with Recurrent Neural Networks", "abstract": "We apply recurrent neural networks (RNN) on a new domain, namely recommendation system. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.", "histories": [["v1", "Sat, 21 Nov 2015 23:42:59 GMT  (97kb,D)", "http://arxiv.org/abs/1511.06939v1", "Submitted for ICLR2016 Updated missing values from table 2 on November. 22. 0:40 (CET)"], ["v2", "Thu, 7 Jan 2016 21:13:50 GMT  (98kb,D)", "http://arxiv.org/abs/1511.06939v2", "Submitted for ICLR2016 Updated missing values from table 2 on November. 22. 0:40 (CET)"], ["v3", "Wed, 17 Feb 2016 16:41:37 GMT  (98kb,D)", "http://arxiv.org/abs/1511.06939v3", "Camera ready version (17th February, 2016)"], ["v4", "Tue, 29 Mar 2016 14:52:58 GMT  (98kb,D)", "http://arxiv.org/abs/1511.06939v4", "Camera ready version (17th February, 2016) Affiliation update (29th March, 2016)"]], "COMMENTS": "Submitted for ICLR2016 Updated missing values from table 2 on November. 22. 0:40 (CET)", "reviews": [], "SUBJECTS": "cs.LG cs.IR cs.NE", "authors": ["bal\\'azs hidasi", "alexandros karatzoglou", "linas baltrunas", "domonkos tikk"], "accepted": true, "id": "1511.06939"}, "pdf": {"name": "1511.06939.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["Bal\u00e1zs Hidasi", "Alexandros Karatzoglou", "Domonkos Tikk"], "emails": ["balazs.hidasi@gravityrd.com", "alexandros.karatzoglou@telefonica.com", "linas.baltrunas@telefonica.com", "domonkos.tikk@gravityrd.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Session-based recommendation is a relatively unappreciated problem in the machine learning and recommender systems community. Many e-commerce recommender systems (particularly those of small retailers) and most of news and media sites do not typically track the user-id\u2019s of the users that visit their sites over a long period of time. While cookies and browser fingerprinting can provide some level of user recognizability, those technologies are often not reliable enough and moreover raise privacy concerns. Even if tracking is possible, lots of users have only one or two sessions on a smaller e-commerce site, and in certain domains (e.g. classified sites) the behavior of users often show session-based traits. Thus subsequent sessions of the same user should be handled independently. Consequently, most session-based recommendation systems deployed on e-commerce are based on relatively simple methods that do not make use of a user profile e.g. itemto-item similarity, co-occurrence, or transition probabilities. While effective, those methods often take only the last click or selection of the user into account ignoring the information of past clicks.\nThe most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al. (2007), Hidasi & Tikk (2012) and neighborhood methods Sarwar et al. (2001), Koren (2008). Factor models work by decomposing the sparse user-item interactions matrix to a set of d dimensional vectors one for each item and user in the dataset. The recommendation problem is then treated as a matrix completion/reconstruction problem whereby the latent factor vectors are then used to fill the missing entries by e.g. taking the dot product of the corresponding user,item latent factors. Factor models are hard to apply in session-based recommendation due to the absence of a user profile. On the other hand, neighborhood methods which rely on computing similarities\n\u2217The author spent 3 months at Telefo\u0301nica I+D during the research of this topic.\nar X\niv :1\n51 1.\n06 93\n9v 1\n[ cs\n.L G\n] 2\n1 N\nov 2\nbetween items (or users) are based on co-occurrences of items in sessions (or user profiles). Neighborhood methods have been used extensively in session-based recommendations.\nThe past few years have seen the tremendous success of deep neural networks in a number of tasks such as image and speech recognition (Russakovsky et al., 2014), (Hinton et al., 2012) where unstructured data is processed through several convolutional and standard layers of (usually rectified linear) units. Sequential data modeling has recently also attracted a lot of attention with various flavors of RNNs being the model of choice for this type of data. Applications of sequence modeling range from test-translation to conversation modeling to image captioning.\nWhile RNNs have been applied to the aforementioned domains with remarkable success little attention, has been paid to the area of recommender systems. In this work we argue that RNNs can be applied to session-based recommendation with remarkable results, we deal with the issues that arise when modeling such sparse sequential data and also adapt the RNN models to the recommender setting by introducing a new ranking loss function suited to the task of training these models. The session-based recommendation problem shares some similarities with some NLP-related problems in terms of modeling. In the session-based recommendation we can consider the first item a user is clicking when entering a web-site as the initial input of the RNN, we then would like to query the model based on this initial input for a recommendation. Each consecutive click of the user will then produce an output (a recommendation) that depends on all the previous clicks. Typically the item-set to choose from in recommenders systems can be in the tens of thousands or even hundreds of thousands. Apart from the large size of the item set another challenge is that click-stream datasets are typically quite large thus training time and scalability are really important. As in most information retrieval and recommendation settings we are interested in focusing the modeling power on the top-items that the user might be interested in, to this end we use ranking loss function to train the RNNs."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 SESSION-BASED RECOMMENDATION", "text": "Much of the work in the area of recommender systems has focused on models that work when a user identifier is available and a clear user profile can be built. In this setting, matrix factorization methods and neighborhood models have dominated the literature and are also employed on-line. One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user profile is the item-to-item recommendation approach (Sarwar et al., 2001),(Linden et al., 2003) in this setting an item to item similarity matrix is precomputed from the available session data, that is items that are often clicked together in sessions are deemed to be similar. This similarity matrix is then simply used during the session to recommend the most similar items to the one the user has currently clicked. While simple this method has been proven to be effective and is widely employed. While effective these methods are only taking into account the last click of the user, in effect ignoring the information of the past clicks.\nA somewhat different approach to session-based recommendation are Markov Decision Processes (MDPs) (Shani et al., 2002). MDPs are models of sequential stochastic decision problems. An MDP is defined as a four-tuple \u3008S,A,Rwd, tr\u3009 where S is the set of states, A is a set of actions Rwd is a reward function and tr is the state-transition function. In recommender systems actions equate to recommendations and the simplest MPDs are essentially first order Markov chains where the next recommendation can be simply computed on the basis of the transition probability between items. The main issue with applying Markov chains in session-based recommendation is that the state space quickly becomes unmanageable when trying to include all possible sequences of user selections.\nThe extended version of the General Factorization Framework (GFF) (Hidasi & Tikk, 2015) is capable of using session data for recommendations. It models a session by the sum of its events. It uses two kinds of latent representations for items, one represents the item itself, the other is for representing the item as part of the sessions. The session is then represented as the average of these latter feature vectors. However this approach does not consider any ordering within the session."}, {"heading": "2.2 DEEP LEARNING IN RECOMMENDERS", "text": "One of the first related methods in the neural networks literature where the use of Restricted Boltzmann Machines for Collaborative Filtering Salakhutdinov et al. (2007). In this work an RBM is used to model user-item interaction and perform recommendations. This model has been shown to be one of the best performing Collaborative Filtering models. Deep Models have been used to extract features from unstructured content such as music or images that are then used together with more conventional collaborative filtering models. In Van den Oord et al. (2013) a convolutional deep network is used to extract feature from music files that are then used in a factor model. More recently Wang et al. (2015) introduce a more generic approach whereby a deep network is used to extract generic content-features from any types of items, these features are then incorporated in a standard collaborative filtering model to enhance the recommendation performance. This approach seem to be particularly useful in settings where there is not sufficient user-item interaction information."}, {"heading": "3 RECOMMENDATIONS WITH RNNS", "text": "Recurrent Neural Networks have been devised to model variable-length sequence data. The main difference between RNNs and conventional feedforward deep models is the existance of an internal hidden state in the units that compose the network. Standart RNNs update their hidden state h using the following update function: ht = g(Wxt + Uht\u22121) (1) Where g is a smooth, bounded function such as a logistic sigmoid function xt is the input of the unit at time t. An RNN outputs a probability distribution over the next element of the sequence, given its current state ht.\nA Gated Recurrent Unit (GRU) Cho et al. (2014) is a more elaborate model of an RNN unit that aims at dealing with the vanishing gradient problems. GRU gates essentialy learn when and by how much to update the hidden state of the unit. The activation of the GRU is a linear interpolation between the previous activation and the candidate activation h\u0302t:\nht = (1\u2212 zt)ht\u22121 + zth\u0302t (2) where the update gate is given by:\nzt = \u03c3(Wzxt + Uzht\u22121) (3) while the candidate activation function h\u0302t is computed in a similar manner:\nh\u0302t = tanhWxt + U(rt ht\u22121) (4) and finaly the reset gate rt is given by:\nrt = \u03c3(Wrxt + Urht\u22121) (5)"}, {"heading": "3.1 CUSTOMIZING THE GRU MODEL", "text": "We used the GRU-based RNN in our models for session-based recommendations. The input of the network is the actual state of the session while the output is the item of the next event in the session. The state of the session can either be the item of the actual event or the events in the session so far. In the former case 1-of-N encoding is used, i.e. the input vector\u2019s length equals to the number of items and only the coordinate corresponding to the active item is one, the others are zeros. The latter setting uses a weighted sum of these representations, in which events are discounted if they have occurred earlier. For the stake of stability, the input vector is then normalized. We also experimented with adding an additional embedding layer, but the 1-of-N encoding always performed better.\nThe core of the network is the GRU layer(s) and additional feedforward layers can be added between the last layer and the output. The output is the predicted preference of the items, i.e. the likelihood of being the next in the session for each item. When multiple GRU layers are used, the hidden state of the previous layer is the input of the next one. The input can also be optionally connected to GRU layers deeper in the network as we found that this improves performance. See the whole architecture on Figure 1.\nSince recommender systems are not the primary application area of recurrent neural networks, we modified the base network to better suit the task. We also considered practical points so that our solution could be possibly applied in a live environment."}, {"heading": "3.1.1 SESSION-PARALLEL MINI-BATCHES", "text": "RNNs for natural language processing tasks usually use in-sequence mini-batches. For example it is common to use a sliding window over the words of sentences and put these windowed fragments next to each other to form mini-batches. This does not fit our task, because (1) the length of sessions can be very different, even more so than that of sentences: some sessions consist of only 2 events, while others may range over a few hundreds; (2) our goal is to capture how a session evolves over time, so breaking down into fragments would make no sense. Therefore we use session-parallel mini-batches. First, we create an order for the sessions. Then, we use the first event of the first X sessions to form the input of the first mini-batch (the desired output is the second events of our active sessions). The second mini-batch is formed from the second events and so on. If any of the sessions end, the next available session is put in its place. Sessions are assumed to be independent, thus we reset the appropriate hidden state when this switch occurs. See Figure 2 for more details."}, {"heading": "3.1.2 SAMPLING ON THE OUTPUT", "text": "Recommender systems are especially useful when the number of items is large. Even for a mediumsized webshop this is in the range of tens of thousands, but on larger sites it is not rare to have hundreds of thousands of items or even a few millions. Calculating a score for each item in each step would make the algorithm scale with the product of the number of items and the number of events. This would be unusable in practice. Therefore we have to sample the output and only compute the score for a small subset of the items. This also entails that only some of the weights will be updated. Besides the desired output we need to compute scores for some negative examples and modify the weights so that the desired output is highly ranked.\nThe natural interpretation of an arbitrary missing event is that the user did not know about the existence of the item and thus there was no interaction. However there is a low probability that the user did know about the item and chose not to interact, because she disliked the item. The more popular the item, the more probable it is that the user knows about it, thus it is more likely that a missing event expresses dislike. Therefore we should sample items in proportion of their popularity. Instead of generating separate samples for each training example, we use the items from the other training examples of the mini-batch as negative examples. The benefit1 of this approach is that we can further reduce computational times by skipping the sampling. Meanwhile, this approach is also a popularity-based sampling, because the likelihood of an item being in the other training examples of the mini-batch is proportional to its popularity."}, {"heading": "3.1.3 RANKING LOSS", "text": "The core of recommender systems is the relevance-based ranking of items. Although the task can also be interpreted as a classification task, learning-to-rank approaches Rendle et al. (2009); Shi et al. (2012); Steck (2015) generally outperform other approaches. Ranking can be pointwise, pairwise or listwise. Pointwise ranking estimates the score or the rank of items independently of each other and the loss is defined in a way so that the rank of relevant items should be low. Pairwise ranking compares the score or the rank of pairs of a positive and a negative item and the loss enforces that the rank of the positive item should be lower than that of the negative one. Listwise ranking uses the scores and ranks of all items and compares them to the perfect ordering. As it includes sorting, it is usually computationally more expensive and thus not used often. Also, if there is only one relevant item \u2013 as in our case \u2013 listwise ranking can be solved via pairwise ranking.\nWe included several pointwise and pairwise ranking losses into our solution. We found that pointwise ranking was unstable with this network (see Section 4 for more comments). Pairwise ranking losses on the other hand performed well. We use the following two.\n\u2022 BPR: Bayesian Personalized Ranking (Rendle et al., 2009) is a matrix factorization method that uses pairwise ranking loss. It compares the score of a positive and a sampled negative item. Here we compare the score of the positive item with several sampled items and use their average as the loss. The loss at a given point in one session is defined as: Ls = \u2212 1NS \u00b7 \u2211NS j=1 log (\u03c3 (r\u0302s,i \u2212 r\u0302s,j)), where NS is the sample size, r\u0302s,k is the score on item k\nat the given point of the session, i is the desired item (next item in the session) and j are the negative samples.\n\u2022 TOP1: This ranking loss was devised by us for this task. It is the regularized approximation of the relative rank of the relevant item. The relative rank of the relevant item is given by 1 NS \u00b7 \u2211NS\nj=1 I{r\u0302s,j > r\u0302s,i}. We approximate I{\u00b7} with a sigmoid. Optimizing for this would modify parameters so that the score for i would be high. However this is unstable as certain positive items also act as negative examples and thus scores tend to become increasingly higher. To avoid this, we want to force the scores of the negative examples to be around zero. This is a natural expectation towards the scores of negative items. Thus we added a regularization term to the loss. It is important that this term is in the same range as the relative rank and acts similarly to it. The final loss function is as follows: Ls = 1 NS \u00b7 \u2211NS j=1 \u03c3 (r\u0302s,j \u2212 r\u0302s,i) + \u03c3 ( r\u03022s,j )"}, {"heading": "4 EXPERIMENTS", "text": "We evaluate the proposed recursive neural network against popular baselines on two datasets.\nThe first dataset is the of RecSys Challenge 20152. This dataset contains click-streams of an ecommerce site that sometimes end in purchase events. We work with the training set of the challenge and keep only the click events. We filter out sessions of length 1. The network is trained on \u223c 6 months of data, containing 7,953,885 sessions of 31,579,006 clicks on 37,483 items. We use the sessions of the subsequent day for testing. Each session is assigned to either the training or the test\n1There are also benefits on the implementation side from making the code less complex to faster matrix operations.\n2http://2015.recsyschallenge.com/\nset, we do not split the data mid-session. Because of the nature of collaborative filtering methods, we filter out clicks from the test set where the item clicked is not in the train set. Sessions of length one are also removed from the test set. After the preprocessing we are left with 12,372 sessions of 58,233 events. This dataset will be referred to as RSC15.\nThe second dataset is collected from a Youtube-like OTT video service platform. Events of watching a video for at least a certain amount of time were collected. Only certain regions were subject to this collection that lasted for somewhat shorter than 2 months. During this time item-to-item recommendations were provided after each video at the left side of the screen. These were provided by a selection of different algorithms and influenced the behavior of the users. Preprocessing steps are similar to that of the other dataset with the addition of filtering out very long sessions as they were probably generated by bots. The training data consists of all but the last day of the aforementioned period and has \u223c 3 million sessions of \u223c 13 million watch events on 330 thousand videos. The test set contains the sessions of the last day of the collection period and has\u223c 37 thousand sessions with \u223c 180 thousand watch events. This dataset will be referred to as VIDEO. The evaluation is done by providing the events of a session one-by-one and checking the rank of the item of the next event. The hidden state of the GRU is reset to zero after a session finishes. Items are ordered in descending order by their score and their position in this list is their order. With RSC15, all of the 37,483 items of the train set were ranked. However, this would have been impractical with VIDEO, due to the large number of items. There we ranked the desired item against the most popular 30,000 items. This has negligible effect on the evaluations as rarely visited items often get low scores. Also, popularity based pre-filtering is common in practical recommender systems.\nAs recommender systems can only recommend a few items at once, the actual item a user might pick should be amongst the first few items of the list. Therefore our primary evaluation metric is recall@20 that is the proportion of cases when the desired item was amongst the top-20 items in all test cases. Recall does not consider the actual rank of the item as long as it is amongst the top-N. This models certain practical scenarios well where there is no highlighting of recommendations and the absolute order does not matter. Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)(Liu et al., 2012; Hidasi & Tikk, 2012). The second metric used in the experiments is MRR@20 (Mean Reciprocal Rank). That is the average of reciprocal ranks of the desired items. The reciprocal rank is set to zero if the rank is above 20. MRR takes into account the rank of the item, which is important in cases where the order of recommendations matter (e.g. the lower ranked items are only visible after scrolling)."}, {"heading": "4.1 BASELINES", "text": "We compare the proposed network to a set of commonly used baselines.\n\u2022 POP: Popularity predictor that always recommends the most popular items of the training set. Despite its simplicity it is often a strong baseline in certain domains. \u2022 S-POP: This baseline recommends the most popular items of the current session. The recommendation list changes during the session as items gain more events. Ties are broken up using global popularity values. This baseline is strong in domains with high repetitiveness. \u2022 Item-KNN: Items similar to the actual item are recommended by this baseline and similarity is defined as the cosine similarity between the vector of their sessions, i.e. it is the number co-occurrences of two items in sessions divided by the square root of the product of the numbers of sessions in which the individual items are occurred. Regularization is also included to avoid coincidental high similarities of rarely visited items. This baseline is one of the most common item-to-item solutions in practical systems, that provides recommendations in the \u201cothers who viewed this item also viewed these ones\u201d setting. Despite of its simplicity it is usually a strong baseline (Linden et al., 2003; Davidson et al., 2010). \u2022 BPR-MF: BPR-MF (Rendle et al., 2009) is one of the commonly used matrix factorization methods. It optimizes for a pairwise ranking objective function (see 3) via SGD. Matrix factorization can not be applied directly to session-based recommendations, because the new sessions do not have feature vectors precomputed. However we can overcome this by using the average of item feature vectors of the items that had occurred in the session so far as the user feature vector. In other words we average the similarities of the feature vectors between a recommendable item and the items of the session so far.\nTable 1 shows the results for the baselines. The item-KNN approach clearly dominates the other methods."}, {"heading": "4.2 PARAMETER & STRUCTURE OPTIMIZATION", "text": "We manually tuned some of the parameters of the proposed network on the RSC15 dataset. Due to the relatively long evaluation cycles and the number of possible architectures, full parameter optimization was not executed. Also the parametrization that we have found best for RSC15 was transferred directly to experiments with VIDEO. Thus there may exist parameterizations that yield better results. After the optimization, the mini-batch size was set to 200, we used dropout with the probability of 0.3 and the learning rate was set to 0.05. Weight matrices were initialized by random numbers drawn uniformly from [\u2212x, x] where x depends on the number of rows and columns of the matrix. We experimented with both rmsprop(Dauphin et al., 2015) and adagrad(Duchi et al., 2011). We found rmsprop to be unstable when we used the BPR based loss function, but it performed better than adagrad with the TOP1 loss.\nWe briefly experimented with other units than GRU. We found both the classic RNN unit and LSTM to perform worse.\nWe tried out several loss functions. Pointwise ranking based losses, such as cross-entropy and MRR optimization (as in (Steck, 2015)) were usually unstable, even with regularization. We assume that this is due to the independently trying to achieve high scores for the desired items. On the other hand pairwise ranking-based losses performed well. We found the ones introduced in Section 3 (BPR and TOP1) to perform the best.\nSeveral architectures were examined and a single layer of GRU units was found to be the best performer. Adding addition layers always resulted in worst performance w.r.t. both training loss and recall and MRR measured on the test set. The exact reason of this is unknown as of yet and requires further research. Using embedding of the items gave slightly worse results, therefore we kept the 1-of-N encoding. Adding additional feed-forward layers after the GRU layer did not help either. However increasing the size of the GRU layer improved the performance. We also found that it is beneficial to use tanh as the activation function of the output layer."}, {"heading": "4.3 RESULTS", "text": "Table 2 shows the results of the best performing networks. The results are compared to the best baseline (item-KNN). We show results with 100 and 1000 hidden units. The former is important from a practical standpoint as the full training on the RSC15 data finishes in \u223c 4 hours (using 8 CPU cores), thus it can be used in live systems as it can be retrained frequently. The training of 1000 hidden units takes \u223c 10 hours and while still can be trained daily, might be slow for certain application domains.\nOn the RSC15 data, the GRU-based approach has substantial gain over the item-KNN in both evaluation metrics, even if the number of units is 100. Increasing the number of units further improves the results. It is interesting to see that the TOP1 loss performs better with rmsprop as with adagrad, while BPR was unstable with rmsprop. This is due to the built-in regularization of TOP1. TOP1 also performs better than BPR if the number of units is higher.\nOn the VIDEO data, GRU with 100 units is a little bit better than item-KNN in terms of recall and worse in terms of MRR. It is harder to represent this data properly on 100 units, because the large\nnumber of items3. With 1000 units however, the results for GRU improve significantly and it is better than item-KNN by a large margin."}, {"heading": "5 CONCLUSION & FUTURE WORK", "text": "In this paper we applied a kind of modern recurrent neural network (GRU) to new application domain: recommender systems. We chose task of session based recommendations, because it is a practically important area, but not well researched. We modified the basic GRU in order to fit the task better by introducing session-parallel mini-batches, mini-batch based output sampling and ranking loss function. We showed that our method can significantly outperform popular baselines that used for this task. We think that our work can be the basis of both deep learning applications in recommender systems and session based recommendations in general.\nOur immediate future work will focus on the more thorough examination of the proposed network. We also plan to train the network on automatically extracted item representation that is built on content of the item itself (e.g. thumbnail, video, text) instead of the current input."}, {"heading": "ACKNOWLEDGMENTS", "text": "The work leading to these results has received funding from the European Union\u2019s Seventh Framework Programme (FP7/2007-2013) under CrowdRec Grant Agreement n\u25e6 610594."}], "references": [{"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization", "author": ["Dauphin", "Yann N", "de Vries", "Harm", "Chung", "Junyoung", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1502.04390,", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "The YouTube video recommendation system", "author": ["Davidson", "James", "Liebald", "Benjamin", "Liu", "Junning"], "venue": "In Recsys\u201910: ACM Conf. on Recommender Systems,", "citeRegEx": "Davidson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidson et al\\.", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback", "author": ["B. Hidasi", "D. Tikk"], "venue": "In ECML-PKDD\u201912, Part II,", "citeRegEx": "Hidasi and Tikk,? \\Q2012\\E", "shortCiteRegEx": "Hidasi and Tikk", "year": 2012}, {"title": "General factorization framework for context-aware recommendations", "author": ["Hidasi", "Bal\u00e1zs", "Tikk", "Domonkos"], "venue": "Data Mining and Knowledge Discovery, pp", "citeRegEx": "Hidasi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hidasi et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Y. Koren"], "venue": "ACM Int. Conf. on Knowledge Discovery and Data Mining, pp", "citeRegEx": "Koren,? \\Q2008\\E", "shortCiteRegEx": "Koren", "year": 2008}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Enlister: Baidu\u2019s recommender system for the biggest Chinese Q&A website", "author": ["Liu", "Qiwen", "Chen", "Tianjian", "Cai", "Jing", "Yu", "Dianhai"], "venue": "In RecSys-12: Proc. of the 6th ACM Conf. on Recommender Systems,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In UAI\u201909: 25 Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rendle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael S", "Berg", "Alexander C", "Li", "Fei-Fei"], "venue": "CoRR, abs/1409.0575,", "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["Sarwar", "Badrul", "Karypis", "George", "Konstan", "Joseph", "Riedl", "John"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Sarwar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sarwar et al\\.", "year": 2001}, {"title": "An mdp-based recommender system", "author": ["Shani", "Guy", "Brafman", "Ronen I", "Heckerman", "David"], "venue": "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Shani et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Shani et al\\.", "year": 2002}, {"title": "Climf: Learning to maximize reciprocal rank with collaborative less-is-more filtering", "author": ["Shi", "Yue", "Karatzoglou", "Alexandros", "Baltrunas", "Linas", "Larson", "Martha", "Oliver", "Nuria", "Hanjalic", "Alan"], "venue": "In Proceedings of the Sixth ACM Conference on Recommender Systems,", "citeRegEx": "Shi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "Gaussian ranking by matrix factorization", "author": ["Steck", "Harald"], "venue": "Proceedings of the 9th ACM Conference on Recommender Systems,", "citeRegEx": "Steck and Harald.,? \\Q2015\\E", "shortCiteRegEx": "Steck and Harald.", "year": 2015}, {"title": "Deep content-based music recommendation", "author": ["Van den Oord", "Aaron", "Dieleman", "Sander", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2013}, {"title": "Collaborative deep learning for recommender systems", "author": ["Wang", "Hao", "Naiyan", "Yeung", "Dit-Yan"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Maximum margin matrix factorization for collaborative ranking", "author": ["Weimer", "Markus", "Karatzoglou", "Alexandros", "Le", "Quoc Viet", "Smola", "Alex"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Weimer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weimer et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al. (2007), Hidasi & Tikk (2012) and neighborhood methods Sarwar et al.", "startOffset": 70, "endOffset": 112}, {"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al. (2007), Hidasi & Tikk (2012) and neighborhood methods Sarwar et al.", "startOffset": 70, "endOffset": 134}, {"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al. (2007), Hidasi & Tikk (2012) and neighborhood methods Sarwar et al. (2001), Koren (2008).", "startOffset": 70, "endOffset": 180}, {"referenceID": 7, "context": "The most common methods used in recommender systems are factor models Koren et al. (2009), Weimer et al. (2007), Hidasi & Tikk (2012) and neighborhood methods Sarwar et al. (2001), Koren (2008). Factor models work by decomposing the sparse user-item interactions matrix to a set of d dimensional vectors one for each item and user in the dataset.", "startOffset": 70, "endOffset": 194}, {"referenceID": 11, "context": "The past few years have seen the tremendous success of deep neural networks in a number of tasks such as image and speech recognition (Russakovsky et al., 2014), (Hinton et al.", "startOffset": 134, "endOffset": 160}, {"referenceID": 6, "context": ", 2014), (Hinton et al., 2012) where unstructured data is processed through several convolutional and standard layers of (usually rectified linear) units.", "startOffset": 9, "endOffset": 30}, {"referenceID": 13, "context": "One of the main approaches that is employed in session-based recommendation and a natural solution to the problem of a missing user profile is the item-to-item recommendation approach (Sarwar et al., 2001),(Linden et al.", "startOffset": 184, "endOffset": 205}, {"referenceID": 14, "context": "A somewhat different approach to session-based recommendation are Markov Decision Processes (MDPs) (Shani et al., 2002).", "startOffset": 99, "endOffset": 119}, {"referenceID": 12, "context": "One of the first related methods in the neural networks literature where the use of Restricted Boltzmann Machines for Collaborative Filtering Salakhutdinov et al. (2007). In this work an RBM is used to model user-item interaction and perform recommendations.", "startOffset": 142, "endOffset": 170}, {"referenceID": 12, "context": "One of the first related methods in the neural networks literature where the use of Restricted Boltzmann Machines for Collaborative Filtering Salakhutdinov et al. (2007). In this work an RBM is used to model user-item interaction and perform recommendations. This model has been shown to be one of the best performing Collaborative Filtering models. Deep Models have been used to extract features from unstructured content such as music or images that are then used together with more conventional collaborative filtering models. In Van den Oord et al. (2013) a convolutional deep network is used to extract feature from music files that are then used in a factor model.", "startOffset": 142, "endOffset": 560}, {"referenceID": 12, "context": "One of the first related methods in the neural networks literature where the use of Restricted Boltzmann Machines for Collaborative Filtering Salakhutdinov et al. (2007). In this work an RBM is used to model user-item interaction and perform recommendations. This model has been shown to be one of the best performing Collaborative Filtering models. Deep Models have been used to extract features from unstructured content such as music or images that are then used together with more conventional collaborative filtering models. In Van den Oord et al. (2013) a convolutional deep network is used to extract feature from music files that are then used in a factor model. More recently Wang et al. (2015) introduce a more generic approach whereby a deep network is used to extract generic content-features from any types of items, these features are then incorporated in a standard collaborative filtering model to enhance the recommendation performance.", "startOffset": 142, "endOffset": 704}, {"referenceID": 0, "context": "A Gated Recurrent Unit (GRU) Cho et al. (2014) is a more elaborate model of an RNN unit that aims at dealing with the vanishing gradient problems.", "startOffset": 29, "endOffset": 47}, {"referenceID": 10, "context": "Although the task can also be interpreted as a classification task, learning-to-rank approaches Rendle et al. (2009); Shi et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 10, "context": "Although the task can also be interpreted as a classification task, learning-to-rank approaches Rendle et al. (2009); Shi et al. (2012); Steck (2015) generally outperform other approaches.", "startOffset": 96, "endOffset": 136}, {"referenceID": 10, "context": "Although the task can also be interpreted as a classification task, learning-to-rank approaches Rendle et al. (2009); Shi et al. (2012); Steck (2015) generally outperform other approaches.", "startOffset": 96, "endOffset": 150}, {"referenceID": 10, "context": "\u2022 BPR: Bayesian Personalized Ranking (Rendle et al., 2009) is a matrix factorization method that uses pairwise ranking loss.", "startOffset": 37, "endOffset": 58}, {"referenceID": 9, "context": "Recall also usually correlates well with important online KPIs, such as click-through rate (CTR)(Liu et al., 2012; Hidasi & Tikk, 2012).", "startOffset": 96, "endOffset": 135}, {"referenceID": 2, "context": "Despite of its simplicity it is usually a strong baseline (Linden et al., 2003; Davidson et al., 2010).", "startOffset": 58, "endOffset": 102}, {"referenceID": 10, "context": "\u2022 BPR-MF: BPR-MF (Rendle et al., 2009) is one of the commonly used matrix factorization methods.", "startOffset": 17, "endOffset": 38}, {"referenceID": 1, "context": "We experimented with both rmsprop(Dauphin et al., 2015) and adagrad(Duchi et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 3, "context": ", 2015) and adagrad(Duchi et al., 2011).", "startOffset": 19, "endOffset": 39}], "year": 2015, "abstractText": "We apply recurrent neural networks (RNN) on a new domain, namely recommendation system. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.", "creator": "LaTeX with hyperref package"}}}