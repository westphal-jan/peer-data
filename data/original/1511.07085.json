{"id": "1511.07085", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "Multiple--Instance Learning: Christoffel Function Approach to Distribution Regression Problem", "abstract": "A two--step Christoffel function based solution is proposed to distribution regression problem. On the first step, to model distribution of observations inside a bag, build Christoffel function for each bag of observations. Then, on the second step, build outcome variable Christoffel function, but use the bag's Christoffel function value at given point as the weight for the bag's outcome. The approach allows the result to be obtained in closed form and then to be evaluated numerically. While most of existing approaches minimize some kind an error between outcome and prediction, the proposed approach is conceptually different, because it uses Christoffel function for knowledge representation, what is conceptually equivalent working with probabilities only. To receive possible outcomes and their probabilities Gauss quadrature for second--step measure can be built, then the nodes give possible outcomes and normalized weights -- outcome probabilities. A library providing numerically stable polynomial basis for these calculations is available, what make the proposed approach practical.", "histories": [["v1", "Sun, 22 Nov 2015 23:19:23 GMT  (34kb)", "http://arxiv.org/abs/1511.07085v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladislav gennadievich malyshkin"], "accepted": false, "id": "1511.07085"}, "pdf": {"name": "1511.07085.pdf", "metadata": {"source": "CRF", "title": "Multiple\u2013Instance Learning: Christoffel Function Approach to Distribution Regression Problem", "authors": ["Vladislav Gennadievich Malyshkin"], "emails": ["malyshki@ton.ioffe.ru"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n07 08\n5v 1\n[ cs\n.L G\n] 2\n2 N\nov 2\n01 5"}, {"heading": "Multiple\u2013Instance Learning: Christoffel Function Approach to", "text": ""}, {"heading": "Distribution Regression Problem.", "text": "Vladislav Gennadievich Malyshkin\u2217\nIoffe Institute, Politekhnicheskaya 26, St Petersburg, 194021, Russia\n(Dated: November, 19, 2015)\n$Id: DistReg.tex,v 1.53 2015/11/22 23:16:03 mal Exp $\nA two\u2013step Christoffel function based solution is proposed to distribution regression\nproblem. On the first step, to model distribution of observations inside a bag, build\nChristoffel function for each bag of observations. Then, on the second step, build\noutcome variable Christoffel function, but use the bag\u2019s Christoffel function value\nat given point as the weight for the bag\u2019s outcome. The approach allows the result\nto be obtained in closed form and then to be evaluated numerically. While most of\nexisting approaches minimize some kind an error between outcome and prediction,\nthe proposed approach is conceptually different, because it uses Christoffel function\nfor knowledge representation, what is conceptually equivalent working with proba-\nbilities only. To receive possible outcomes and their probabilities Gauss quadrature\nfor second\u2013step measure can be built, then the nodes give possible outcomes and\nnormalized weights \u2013 outcome probabilities. A library providing numerically stable\npolynomial basis for these calculations is available, what make the proposed approach\npractical.\n\u2217 malyshki@ton.ioffe.ru\n2 I. INTRODUCTION\nMultiple instance learning is an important Machine Learning (ML) concept having numerous applications[1]. In multiple instance learning class label is associated not with a single observation, but with a \u201cbag\u201d of observations. A very close problem is distribution regression problem, where a l-th sample distribution of x is mapped to a single y(l) value. There are numerous heuristics methods developed from both: ML and distribution regression sides, see [2] for review. As in any ML problem the most important part is not so much the learning algorithm, but the way how the learned knowledge is represented. Learned knowledge is often represented as a set of propositional rules, regression function, Neural Network weights, etc. Most of the approaches minimize an error between result and prediction, and some kind of L2 metric is often used as an error. The simplest example of such approach is least squares\u2013type approximation. However, there is exist different kind of approximation, Radon\u2013Nikodym type, that operates not with result error, but with sample probabilty, see the Ref. [3] as an example comparing two these approaches.\nSimilar transition from result error to probability of outcomes is made in this paper. In this work we use Christoffel function as a mean to store knowledge learned. Christoffel function is a very fundamental concept related to \u201cdistribution density\u201d, quadratures weights, number of observations, etc[4, 5]. Recent progress in numerical stability of high order distribution moments calculation[6] allows Christoffel function to be built to a very high order, what make practical the approach of using Christoffel function as way to represent knowledge learned.\nThe paper is organized as following: In Section II a general theory of distribution regression is discussed and close form result, Eq. (10), is presented. Then in Section III numerical example of Eq. (10) application is presented. In Section IV possible further development is discussed."}, {"heading": "II. CHRISTOFFEL FUNCTION APPROACH", "text": "Consider distribution regression problem where a bag of N observations x is mapped to\na single outcome observation y for l = [1..M ].\n(x1, x2, . . . , xj , . . . , xN) (l) \u2192 y(l) (1)\n3 A distribution regression problem can have a goal to estimate y, average of y, distribution of y, etc. given specific value of x. While the Christoffel function can be used as a proxy to probabilty estimation, but for \u201ctrue\u201d distribution estimation a complete Gauss quadrature should be built, then the nodes would give possible outcomes and normalized weights \u2013 outcome probabilities.\nFor further development we need x and y bases Qk(x) and Qm(y) and some x and y measure. For simplicity, not reducing the generality of the approach, we are going to assume that x measure is a sum over j index \u2211\nj, y measure is a \u2211 l, the basis functions Qk(x) are\npolynomials k = 0..dx \u2212 1, and Qm(y) are polynomials m = 0..dy \u2212 1 where dx and dy is the number of elements in x and y bases, typical value for dx and dy is below 10\u201315.\nIf no x observations exist in each bag (N = 0), how to estimate the number of observations\nfor given y value? The answer is Christoffel function \u03bb(y).\nGy = < QsQt >y= M \u2211\nl=1\nQs(y (l))Qt(y (l)) (2)\nK(z, y) = dy\u22121 \u2211\ns,t=0\nQs(z) (Gy) \u22121 st Qt(y) (3)\n\u03bb(y) = 1\nK(y, y) (4)\nThe Gy is Gramm matrix, K(y, y) is a positive quadratic form with matrix equal to Gramm matrix inverse and is a polynomial of 2dy\u22122 order, when the form is expanded. The K(z, y) is a reproducing kernel: P (z) = \u2211M\nl=1K(z, y (l))P (y(l)) for any polynomial P of degree dy \u2212 1\nor less. For numerical calculations of K(y, y) see Ref. [6], Appendix C.\nThe \u03bb(y) define a value similar in nature to \u201cthe number of observations\u201d, or \u201cprob-\nability\u201d, \u201cweight\u201d, etc[4, 5]. (The equation M = \u2211dy\u22121\ni=0 \u03bb(yi) holds, when yi correspond\nto quadrature nodes build on y\u2013distribution, the yi are eigenvalues of generalized eigenfunctions problem: \u2211dy\u22121\nt=0 < yQsQt >y \u03c8 (i) t = yi \u2211dy\u22121 t=0 < QsQt >y \u03c8 (i) t , also note that\n\u03bb(yi) = 1/K(yi, yi) = 1/ ( \u2211dy\u22121 t=0 \u03c8 (i) t Qt(yi) )2 and 0 = \u2211dy\u22121 t=0 \u03c8 (s) t Qt(yi) for s 6= i. The asympthotic of \u03bb(y) can also serve as important characteristics of distribution property[4].) The problem now is to modify \u03bb(y) to take into account given x value. If, in addition to y(l), we have a vector x (l) j as precondition, then the weight in (2) for each l, should be no longer equal to the constant for all terms, but instead, should be calculated based on the number of x (l) j observations that are close to given x value. Let us use Christoffel function\n4 once again, but now in x\u2013space under fixed l and estimate the weight for l-th observation of y as equal to \u03bb(l)(x)\nThe result for \u03bb(y|x) is:\n< Qk > (l) x =\nN \u2211\nj=1\nQk(x (l) j ) (5)\nG(l) = < QqQr > (l) x =\nN \u2211\nj=1\nQq(x (l) j )Qr(x (l) j ) (6)\n\u03bb(l)(x) = 1\n\u2211dx\u22121 k,m=0Qk(x) (G (l)) \u22121 kmQm(x)\n(7)\n< Qs >\u03bb = M \u2211\nl=1\n\u03bb(l)(x)Qs(y (l)) (8)\nGy|x = < QsQt >\u03bb= M \u2211\nl=1\n\u03bb(l)(x)Qs(y (l))Qt(y (l)) (9)\n\u03bb(y|x) = 1\n\u2211dy\u22121 s,t=0Qs(y)\n( Gy|x )\u22121\nst Qt(y)\n(10)\nThe \u03bb(y|x) is the answer. The Gy|x is very similar to (2), but now the l-th term weight is \u03bb(l)(x) instead of a constant. For a given x the (10) is a function of y, having the meaning of observations number (or \u201cprobability\u201d\u2013like value when scaled). The conceptual difference between regressing the value of y on x and x\u2013dependent weights is conceptually similar to the difference between least squares approximation, where observable value is interpolated and Radon\u2013Nikodym type of approximation, where the weights are interpolated[3]. In Christoffel function approach only the weights, not the values are interpolated, what gives a new turn to distribution regression problem.\nFor an estimation of possible y outcomes given x, this can be done either using the (8) measure and estimating, say, average y and dispersion, or more interesting, build dy\u2013point Gauss quadrature using the measure (8), see Ref. [6], Appendix B for numerical algorithm, and, for the measure (8), obtain quadrature nodes yi and weights \u03bb(yi|x). Then quadrature nodes yi can be treated as possible y\u2013outcomes and \u03bb(yi|x) can be treated as the weight, corresponding to yi outcome. Normalizing the weights one receive probabilities of each yi\u2013 outcome given x value. (The quadratures provide superior information about probabilities of each outcome, taking long\u2013tail information into account, but if one, for whatever reason, still need average y value, corresponding to (8) measure, it can be easily obtained from quadrature averaging yi with probabilities \u03bb(yi|x)/ \u2211dy\u22121 m=0 \u03bb(ym|x) of yi outcome. The result would match\n5 exactly sample average of y for the measure (8). Also note that \u2211dy\u22121\ni=0 \u03bb(yi|x) = \u2211M l=1 \u03bb (l)(x))."}, {"heading": "III. NUMERICAL ESTIMATION", "text": "The major problem of Christoffel function calculation is numerical instability. For given observations all polynomial bases give identical results, but numerical stability of calculations is drastically different, because Gramm matrix condition number depend strongly on basis choice. IfQk(x) andQm(y) are chosen as orthogonal polynomials with orthogonality measure support matching the x and y support then for discrete measures the Gramm matrix posses a good condition number[7]. The numerical library we developed, see[6] Appendix A, is able to manipulate polynomials in Chebyshev, Legendre, Laguerre and Hermite bases directly, what allows a stable basis to be used and calculate the moments to a very high order, see Ref. [3] as an example. The distribution regression problem does not require hundreds of moments as in [3], the dx and dy are typically lower than 10\u201315 and also should be substantially lower that N and M values respectively. The numerical calculations are typically stable as long as one of four stable bases from Ref. [6] is used.\nThe algorithm for \u03bb(y|x) calculation is this. For each l calculate: < Qk > (l) x = \u2211N j=1Qk(x (l) j )\nmoments for k = [0..2dx \u2212 1], then, using polynomials multiplication operation, from these moments obtain Gramm matrix (6) G(l) =< QqQr > (l) x for q, r = [0..dx \u2212 1], inverse it and build \u03bb(l)(x), a rational function (the nominator is a constant and the denominator is a polynomial of 2dx \u2212 2 order) as in (7), then calculate the \u03bb (l)(x) at given value of x, save these as the weight for l-th observation of y(l). Having the weights, conditional on given x value, calculate < Qm >\u03bb= \u2211M l=1 \u03bb (l)(x)Qm(y (l)) moments for m = [0..2dy \u2212 1], then, using polynomials multiplication operation, from these moments obtain Gramm matrix (9) Gy|x =< QsQt >\u03bb for s, t = [0..dy \u2212 1], inverse it and build \u03bb(y|x) as in (10). If possible y outcomes and their probabilities are required, then solve generalized eigenvalues problem \u2211dy\u22121\nt=0 < yQsQt >\u03bb \u03c8 (i) t = yi \u2211dy\u22121 t=0 < QsQt >\u03bb \u03c8 (i) t , the eigenvalues yi provide possible\ny\u2013outcomes and the weight for each outcome is \u03bb(yi|x) = 1/ ( \u2211dy\u22121 t=0 \u03c8 (i) t Qt(yi) )2 , the probability of i\u2013th outcome is normalized weight \u03bb(yi|x)/ \u2211dy\u22121 m=0 \u03bb(ym|x) The code performing these calculations is available[8], see the file ExampleDistributionDependence.scala.\nFor application of the algorithm consider the following simple numerical example. Let y be uniformly [\u22121 . . . 1] distributed random variable, l = [1..M ];M = 10000, and for each\ny(l) generate j = [1..N ];N = 1000 random x as x = y + R \u2217 \u01eb[\u22121 . . . 1], where \u01eb[\u22121 . . . 1] is uniformly [\u22121 . . . 1] distributed random variable. Then for given x, we want to estimate the distribution of y. Let us choose dx = dy = 10 and plot \u03bb(y|x), the function of y for three fixed x = {\u22120.5, 0, 0.5}. In the Fig. 1 we present the chart for \u03bb(y|x) for R = 0.5 and R = 0.1. Unconditional \u03bb(y) from (4) is also presented. (For some applications conditional \u03bb(y|x)/\u03bb(y) can be also considered).\n7 One can see that the y\u2013localization at given x is very clear, and the width of non\u2013vanishing area of \u03bb(y|x) track very close the value of randomness parameter R. The quadrature built on \u03bb measure give both: possible outcomes (quadrature nodes) and weights (outcome probability is normalized weight), presented in the Fig. 1 as vertical lines corresponding to specific y\u2013outcome (as we noted above \u2013 on quadrature nodes the quadrature weight match exactly Christoffel function value). These calculations can be applied to any kind of distribution, this simple example was used just to demonstrate application of Christoffel function to representation of learned knowledge and to find possible y\u2013outcomes and their probabilities."}, {"heading": "IV. DISCUSSION", "text": "In this work a Christoffel function approach to distribution regression problem is proposed. The main idea is to use Christoffel function for knowledge representation. Closed form answer (10) is available. The Christoffel function is used twice: first, to build distribution approximation withing a \u201cbag\u201d, then to model y\u2013value distribution of these \u201cbags\u201d using Christoffel function value from the first step as the weight for the observation of y. When required, possible y outcomes and their probabilities, can be calculated by building Gauss quadrature instead of using plain Christoffel function answer (10), the quadrature nodes give possible outcomes and normalized quadrature weights give each outcome probability. The method can be extended from real\u2013value to discrete attributes (the dx and dy should be properly adjusted).\nThe approach, proposed in this paper, as most Multiple\u2013Instance learning approaches, has two stages. The question arise, whether consistent one stage approach exist. For the case x and y being random variables two one\u2013stage interpolation approaches: least squares and Radon\u2013Nikodym have been have been studied in [3]. Now, let us try to find similar one\u2013 stage approach, but for random distribution to random variable mapping, same as the (1) problem, we study in this paper. The idea is to convert the problem \u201crandom distribution\u201d to \u201crandom variable\u201d to the problem \u201cvector of random variables\u201d to \u201crandom variable\u201d. The simplest way is to take the moments of random x distribution as \u201cinput vector of random variables\u201d. Then least squares and Radon\u2013Nikodym approximations from [3] can be directly applied. In contrast with the problem (1): given x, what can we tell about y, this,\n8 converted to vector moments problem, would be: given < Qk >x moments of x\u2013distribution (fixed x0 case can be modeled by NQk(x0)), what can we tell about y? This problem is solvable in one step. The one\u2013step solution is actually almost identical to 2D problem of image grayscale intensity interpolation we have considered in [3]. There is just one major difference: for image interpolation problem we used basis value at specific point of the raster, but now we would have to use as input the < Qk >x moments of x distribution on which we want to estimate output y. The question arise of numerical stability of one\u2013stage method and the problem of data overfitting. While two\u2013stages approach typically effectively have dx+dy elements in basis, the one\u2013stage approach effectively have dxdy elements in basis. By choosing stable basis in [3] we calculated the moments for dx = dy = 100 without catching an instability, so basis dimension should not be an issue, but the question of data overfitting for one\u2013stage method require more research and to be published separately. The major advantage of using Christoffel function for knowledge representation is that it stores pure weights, and data overfitting parameter can be estimated as dx/N on first stage and about dy/M on second stage, so in practical applications the problem can be always identified from the beginning.\n[1] Jun Yang, Review of multi-instance learning and its applications , Tech. Rep. (Tech. Rep,\n2005).\n[2] Zolta\u0301n Szabo\u0301, Arthur Gretton, Barnaba\u0301s Po\u0301czos, and Bharath Sriperumbudur, \u201cLearning\ntheory for distribution regression,\u201d arXiv preprint arXiv:1411.2066 (2014).\n[3] Vladislav Gennadievich Malyshkin, \u201cRadon\u2013Nikodym approximation in applica-\ntion to image analysis. http://arxiv.org/abs/1511.01887,\u201d ArXiv e-prints (2015),\narXiv:1511.01887 [cs.CV].\n[4] Vilmos Totik, \u201cOrthogonal polynomials,\u201d Surveys in Approximation Theory 1, 70\u2013125 (11 Nov. 2005).\n[5] Paul G Nevai, \u201cGe\u0301za Freud, Orthogonal Polynomials. Christoffel Functions. A Case Study,\u201d\nJournal Of Approximation Theory 48, 3\u2013167 (1986).\n[6] Vladislav Gennadievich Malyshkin and Ray Bakhramov, \u201cMathematical Foundations of Real-\ntime Equity Trading. Liquidity Deficit and Market Dynamics. Automated Trading Machines.\nhttp://arxiv.org/abs/1510.05510,\u201d ArXiv e-prints (2015), arXiv:1510.05510 [q-fin.CP].\n9 [7] Bernhard Beckermann, On the numerical condition of polynomial bases: estimates for the con-\ndition number of Vandermonde, Krylov and Hankel matrices, Ph.D. thesis, Habilitationsschrift,\nUniversita\u0308t Hannover (1996).\n[8] Vladislav Gennadievich Malyshkin, (2014), the code for polynomials calculation,\nhttp://www.ioffe.ru/LNEPS/malyshkin/code.html."}], "references": [{"title": "Review of multi-instance learning and its applications", "author": ["Jun Yang"], "venue": "Tech. Rep. (Tech. Rep,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Learning theory for distribution regression", "author": ["Zolt\u00e1n Szab\u00f3", "Arthur Gretton", "Barnab\u00e1s P\u00f3czos", "Bharath Sriperumbudur"], "venue": "arXiv preprint arXiv:1411.2066 (2014).", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Radon\u2013Nikodym approximation in application to image analysis. http://arxiv.org/abs/1511.01887", "author": ["Vladislav Gennadievich Malyshkin"], "venue": "ArXiv e-prints (2015), arXiv:1511.01887 [cs.CV].", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Orthogonal polynomials", "author": ["Vilmos Totik"], "venue": "Surveys in Approximation Theory 1, 70\u2013125 (11 Nov. 2005).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "G\u00e9za Freud, Orthogonal Polynomials. Christoffel Functions. A Case Study", "author": ["Paul G Nevai"], "venue": "Journal Of Approximation Theory 48, 3\u2013167 (1986).", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1986}, {"title": "Mathematical Foundations of Realtime Equity Trading. Liquidity Deficit and Market Dynamics. Automated Trading Machines. http://arxiv.org/abs/1510.05510", "author": ["Vladislav Gennadievich Malyshkin", "Ray Bakhramov"], "venue": "ArXiv e-prints (2015), arXiv:1510.05510 [q-fin.CP].  9", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "On the numerical condition of polynomial bases: estimates for the condition number of Vandermonde", "author": ["Bernhard Beckermann"], "venue": "Krylov and Hankel matrices, Ph.D. thesis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Multiple instance learning is an important Machine Learning (ML) concept having numerous applications[1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "There are numerous heuristics methods developed from both: ML and distribution regression sides, see [2] for review.", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "[3] as an example comparing two these approaches.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Christoffel function is a very fundamental concept related to \u201cdistribution density\u201d, quadratures weights, number of observations, etc[4, 5].", "startOffset": 134, "endOffset": 140}, {"referenceID": 4, "context": "Christoffel function is a very fundamental concept related to \u201cdistribution density\u201d, quadratures weights, number of observations, etc[4, 5].", "startOffset": 134, "endOffset": 140}, {"referenceID": 5, "context": "Recent progress in numerical stability of high order distribution moments calculation[6] allows Christoffel function to be built to a very high order, what make practical the approach of using Christoffel function as way to represent knowledge learned.", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "[6], Appendix C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The \u03bb(y) define a value similar in nature to \u201cthe number of observations\u201d, or \u201cprobability\u201d, \u201cweight\u201d, etc[4, 5].", "startOffset": 106, "endOffset": 112}, {"referenceID": 4, "context": "The \u03bb(y) define a value similar in nature to \u201cthe number of observations\u201d, or \u201cprobability\u201d, \u201cweight\u201d, etc[4, 5].", "startOffset": 106, "endOffset": 112}, {"referenceID": 3, "context": "The asympthotic of \u03bb(y) can also serve as important characteristics of distribution property[4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "The conceptual difference between regressing the value of y on x and x\u2013dependent weights is conceptually similar to the difference between least squares approximation, where observable value is interpolated and Radon\u2013Nikodym type of approximation, where the weights are interpolated[3].", "startOffset": 282, "endOffset": 285}, {"referenceID": 5, "context": "[6], Appendix B for numerical algorithm, and, for the measure (8), obtain quadrature nodes yi and weights \u03bb(yi|x).", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "IfQk(x) andQm(y) are chosen as orthogonal polynomials with orthogonality measure support matching the x and y support then for discrete measures the Gramm matrix posses a good condition number[7].", "startOffset": 192, "endOffset": 195}, {"referenceID": 5, "context": "The numerical library we developed, see[6] Appendix A, is able to manipulate polynomials in Chebyshev, Legendre, Laguerre and Hermite bases directly, what allows a stable basis to be used and calculate the moments to a very high order, see Ref.", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "[3] as an example.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The distribution regression problem does not require hundreds of moments as in [3], the dx and dy are typically lower than 10\u201315 and also should be substantially lower that N and M values respectively.", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "[6] is used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "For the case x and y being random variables two one\u2013stage interpolation approaches: least squares and Radon\u2013Nikodym have been have been studied in [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 2, "context": "Then least squares and Radon\u2013Nikodym approximations from [3] can be directly applied.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "The one\u2013step solution is actually almost identical to 2D problem of image grayscale intensity interpolation we have considered in [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "By choosing stable basis in [3] we calculated the moments for dx = dy = 100 without catching an instability, so basis dimension should not be an issue, but the question of data overfitting for one\u2013stage method require more research and to be published separately.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "[1] Jun Yang, Review of multi-instance learning and its applications , Tech.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Zolt\u00e1n Szab\u00f3, Arthur Gretton, Barnab\u00e1s P\u00f3czos, and Bharath Sriperumbudur, \u201cLearning theory for distribution regression,\u201d arXiv preprint arXiv:1411.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Vladislav Gennadievich Malyshkin, \u201cRadon\u2013Nikodym approximation in application to image analysis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Vilmos Totik, \u201cOrthogonal polynomials,\u201d Surveys in Approximation Theory 1, 70\u2013125 (11 Nov.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Paul G Nevai, \u201cG\u00e9za Freud, Orthogonal Polynomials.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Vladislav Gennadievich Malyshkin and Ray Bakhramov, \u201cMathematical Foundations of Realtime Equity Trading.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Bernhard Beckermann, On the numerical condition of polynomial bases: estimates for the condition number of Vandermonde, Krylov and Hankel matrices, Ph.", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "A two\u2013step Christoffel function based solution is proposed to distribution regression problem. On the first step, to model distribution of observations inside a bag, build Christoffel function for each bag of observations. Then, on the second step, build outcome variable Christoffel function, but use the bag\u2019s Christoffel function value at given point as the weight for the bag\u2019s outcome. The approach allows the result to be obtained in closed form and then to be evaluated numerically. While most of existing approaches minimize some kind an error between outcome and prediction, the proposed approach is conceptually different, because it uses Christoffel function for knowledge representation, what is conceptually equivalent working with probabilities only. To receive possible outcomes and their probabilities Gauss quadrature for second\u2013step measure can be built, then the nodes give possible outcomes and normalized weights \u2013 outcome probabilities. A library providing numerically stable polynomial basis for these calculations is available, what make the proposed approach practical.", "creator": "gnuplot 4.4 patchlevel 4"}}}