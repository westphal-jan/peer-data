{"id": "1511.07130", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions", "abstract": "We develop parallel predictive entropy search (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a batch of points which will maximize the information gain about the global maximizer of the objective. Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.", "histories": [["v1", "Mon, 23 Nov 2015 08:21:17 GMT  (311kb,D)", "http://arxiv.org/abs/1511.07130v1", "12 pages in Neural Information Processing Systems 2015"]], "COMMENTS": "12 pages in Neural Information Processing Systems 2015", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["amar shah", "zoubin ghahramani"], "accepted": true, "id": "1511.07130"}, "pdf": {"name": "1511.07130.pdf", "metadata": {"source": "CRF", "title": "Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions", "authors": ["Amar Shah", "Zoubin Ghahramani"], "emails": ["as793@cam.ac.uk", "zoubin@eng.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Finding the global maximizer of a non-concave objective function based on sequential, noisy observations is a fundamental problem in various real world domains e.g. engineering design [1], finance [2] and algorithm optimization [3]. We are interesed in objective functions which are unknown but may be evaluated pointwise at some expense, be it computational, economical or other. The challenge is to find the maximizer of the expensive objective function in as few sequential queries as possible, in order to minimize the total expense.\nA Bayesian approach to this problem would probabilistically model the unknown objective function, f . Based on posterior belief about f given evaluations of the the objective function, you can decide where to evaluate f next in order to maximize a chosen utility function. Bayesian optimization [4] has been successfully applied in a range of difficult, expensive global optimization tasks including optimizing a robot controller to maximize gait speed [5] and discovering a chemical derivative of a particular molecule which best treats a particular disease [6].\nTwo key choices need to be made when implementing a Bayesian optimization algorithm: (i) a model choice for f and (ii) a strategy for deciding where to evaluate f next. A common approach for modeling f is to use a Gaussian process prior [7], as it is highly flexible and amenable to analytic calculations. However, other models have shown to be useful in some Bayesian optimization tasks e.g. Student-t process priors [8] and deep neural networks [9]. Most research in the Bayesian optimization literature considers the problem of deciding how to choose a single location where f should be evaluated next. However, it is often possible to probe several points in parallel. For example, you may possess 2 identical robots on which you can test different gait parameters in parallel. Or your computer may have multiple cores on which you can run algorithms in parallel with different hyperparameter settings.\nWhilst there are many established strategies to select a single point to probe next e.g. expected improvement, probability of improvement and upper confidence bound [10], there are few well known strategies for selecting batches of points. To the best of our knowledge, every batch selection\nar X\niv :1\n51 1.\n07 13\n0v 1\n[ cs\n.L G\n] 2\n3 N\nov 2\nstrategy proposed in the literature involves a greedy algorithm, which chooses individual points until the batch is filled. Greedy choice making can be severely detrimental, for example, a greedy approach to the travelling salesman problem could potentially lead to the uniquely worst global solution [11]. In this work, our key contribution is to provide what we believe is the first non-greedy algorithm to choose a batch of points to probe next in the task of parallel global optimization.\nOur approach is to choose a set of points which in expectation, maximally reduces our uncertainty about the location of the maximizer of the objective function. The algorithm we develop, parallel predictive entropy search, extends the methods of [12, 13] to multiple point batch selection. In Section 2, we formalize the problem and discuss previous approaches before developing parallel predictive entropy search in Section 3. Finally, we demonstrate the benefit of our non-greedy strategy on synthetic as well as real-world objective functions in Section 4."}, {"heading": "2 Problem Statement and Background", "text": "Our aim is to maximize an objective function f : X \u2192 R, which is unknown but can be (noisily) evaluated pointwise at multiple locations in parallel. In this work, we assume X is a compact subset of RD. At each decision, we must select a set of Q points St = {xt,1, ...,xt,Q} \u2282 X , where the objective function would next be evaluated in parallel. Each evaluation leads to a scalar observation yt,q = f(xt,q) + t,q , where we assume t,q \u223c N (0, \u03c32) i.i.d. We wish to minimize a future regret, rT = [f(x\n\u2217) \u2212 f(x\u0303T )], where x\u2217 \u2208 argmaxx\u2208X f(x) is an optimal decision (assumed to exist) and x\u0303T is our guess of where the maximizer of f is after evaluating T batches of input points. It is highly intractable to make decisions T steps ahead in the setting described, therefore it is common to consider the regret of the very next decision. In this work, we shall assume f is a draw from a Gaussian process with constant mean \u03bb \u2208 R and differentiable kernel function k : X 2 \u2192 R. Most Bayesian optimization research focuses on choosing a single point to query at each decision i.e. Q = 1. A popular strategy in this setting is to choose the point with highest expected improvement over the current best evaluation, i.e. the maximizer of aEI(x|D) = E [ max(f(x) \u2212 f(xbest), 0)\n\u2223\u2223D] = \u03c3(x)[\u03c6(\u03c4(x)) + \u03c4(x)\u03a6(\u03c4(x))], where D is the set of observations, xbest is the best evaluation point so far, \u03c3(x) = \u221a Var[f(x)|D], \u00b5(x) = E[f(x)|D], \u03c4(x) = (\u00b5(x)\u2212 f(xbest))/\u03c3(x) and \u03c6(.) and \u03a6(.) are the standard Gaussian p.d.f. and c.d.f. Aside from being an intuitive approach, a key advantage of using the expected improvement strategy is in the fact that it is computable analytically and is infinitely differentiable, making the problem of finding argmaxx\u2208XaEI(x|D) amenable to a plethora of gradient based optimization methods. Unfortunately, the corresponding strategy for selecting Q > 1 points to evaluate in parallel does not lead to an analytic expression. [14] considered an approach which sequentially used the EI criterion to greedily choose a batch of points to query next, which [3] formalized and utilized by defining\naEI\u2212MCMC ( x|D, {xq\u2032}qq\u2032=1 ) = \u222b X q aEI ( x|D\u222a{xq\u2032 , yq\u2032}qq\u2032=1 ) p ( {yq\u2032}qq\u2032=1|D, {xq\u2032}qq\u2032=1 ) dy1..dyq, the expected gain in evaluating x after evaluating {xq\u2032 , yq\u2032}qq\u2032=1, which can be approximated using Monte Carlo samples, hence the name EI-MCMC. Choosing a batch of points St using the EIMCMC policy is doubly greedy: (i) the EI criterion is greedy as it inherently aims to minimize onestep regret, rt, and (ii) the EI-MCMC approach starts with an empty set and populates it sequentially (and hence greedily), deciding the best single point to include until |St| = Q. A similar but different approach called simulated matching (SM) was introduced by [15]. Let \u03c0 be a baseline policy which chooses a single point to evaluate next (e.g. EI). SM aims to select a batch St of size Q, which includes a point \u2018close to\u2019 the best point which \u03c0 would have chosen when applied sequentially Q times, with high probability. Formally, SM aims to maximize\naSM(St|D) = \u2212ESQ\u03c0 [ Ef [\nmin x\u2208St\n(x\u2212 argmaxx\u2032\u2208SQ\u03c0 f(x \u2032))2 \u2223\u2223\u2223D, SQ\u03c0 ]],\nwhere SQ\u03c0 is the set of Q points which policy \u03c0 would query if employed sequentially. A greedy k-medoids based algorithm is proposed to approximately maximize the objective, which the authors justify by the submodularity of the objective function.\nThe upper confidence bound (UCB) strategy [16] is another method used by practitioners to decide where to evaluate an objective function next. The UCB approach is to maximize aUCB(x|D) =\n\u00b5(x) + \u03b1 1/2 t \u03c3(x), where \u03b1t is a domain-specific time-varying positive parameter which trades off exploration and exploitation. In order to extend this approach to the parallel setting, [17] noted that the predictive variance of a Gaussian process depends only on where observations are made, and not the observations themselves. Therefore, they suggested the GP-BUCB method, which greedily populates the set St by maximizing a UCB type equation Q times sequentially, updating \u03c3 at each step, whilst maintaining the same \u00b5 for each batch. Finally, a variant of the GP-UCB was proposed by [18]. The first point of the set St is chosen by optimizing the UCB objective. Thereafter, a \u2018relevant region\u2019 Rt \u2282 X which contains the maximizer of f with high probability is defined. Points are greedily chosen from this region to maximize the information gain about f , measured by expected reduction in entropy, until |St| = Q. This method was named Gaussian process upper confidence bound with pure exploration (GP-UCB-PE).\nEach approach discussed resorts to a greedy batch selection process. To the best of our knowledge, no batch Bayesian optimization method to date has avoided a greedy algorithm. We avoid a greedy batch selection approach with PPES, which we develop in the next section."}, {"heading": "3 Parallel Predictive Entropy Search", "text": "Our approach is to maximize information [19] about the location of the global maximizer x\u2217, which we measure in terms of the negative differential entropy of p(x\u2217|D). Analogous to [13], PPES aims to choose the set of Q points, St = {xq}Qq=1, which maximizes\naPPES(St|D) = H [ p(x\u2217|D) ] \u2212 E p ( {yq}Qq=1 \u2223\u2223D,St)[H[p(x\u2217|D \u222a {xq, yq}Qq=1)]], (1) where H[p(x)] = \u2212 \u222b p(x) log p(x)dx is the differential entropy of its argument and the expectation above is taken with respect to the posterior joint predictive distribution of {yq}Qq=1 given the previous evaluations, D, and the set St. Evaluating (1) exactly is typically infeasible. The prohibitive aspects are that p ( x\u2217|D \u222a {xq, yq}Qq=1 ) would have to be evaluated for many different combinations of {xq, yq}Qq=1, and the entropy computations are not analytically tractable in themselves. Significant approximations need to be made to (1) before it becomes practically useful [12]. A convenient equivalent formulation of the quantity in (1) can be written as the mutual information between x\u2217 and {yq}Qq=1 given D [20]. By symmetry of the mutual information, we can rewrite aPPES as aPPES(St|D) = H [ p ( {yq}Qq=1|D,St )] \u2212 Ep(x\u2217|D) [ H [ p ( {yq}Qq=1|D,St,x\u2217 )]] , (2)\nwhere p ( {yq}Qq=1|D,St,x\u2217 ) is the joint posterior predictive distibution for {yq}Qq=1 given the observed data, D and the location of the global maximizer of f . The key advantage of the formulation in (2), is that the objective is based on entropies of predictive distributions of the observations, which are much more easily approximated than the entropies of distributions on x\u2217.\nIn fact, the first term of (2) can be computed analytically. Suppose p ( {fq}Qq=1|D,St ) is multi-\nvariate Gaussian with covariance K, then H [ p ( {yq}Qq=1|D,St )] = 0.5 log[det(2\u03c0e(K + \u03c32I))]. We develop an approach to approximate the expectation of the predictive entropy in (2), using an expectation propagation based method which we discuss in the following section."}, {"heading": "3.1 Approximating the Predictive Entropy", "text": "Assuming a sample of x\u2217, we discuss our approach to approximating H [ p ( {yq}Qq=1|D,St,x\u2217 )] in (2) for a set of query points St. Note that we can write\np ( {yq}Qq=1|D,St,x\u2217 ) = \u222b p ( {fq}Qq=1|D,St,x\u2217 ) Q\u220f q=1 p(yq|fq) df1...dfQ, (3)\nwhere p ( {fq}Qq=1|D,St,x\u2217 ) is the posterior distribution of the objective function at the locations xq \u2208 St, given previous evaluations D, and that x\u2217 is the global maximizer of f . Recall that p(yq|fq) is Gaussian for each q. Our approach will be to derive a Gaussian approximation to p ( {fq}Qq=1|D,St,x\u2217 ) , which would lead to an analytic approximation to the integral in (3).\nThe posterior predictive distribution of the Gaussian process, p ( {fq}Qq=1|D,St ) , is multivariate Gaussian distributed. However, by further conditioning on the location x\u2217, the global maximizer\nof f , we impose the condition that f(x) \u2264 f(x?) for any x \u2208 X . Imposing this constraint for all x \u2208 X is extremely difficult and makes the computation of p ( {fq}Qq=1|D,St,x\u2217 ) highly intractable. We instead impose the following two conditions (i) f(x) \u2264 f(x?) for each x \u2208 St, and (ii) f(x?) \u2265 ymax + , where ymax is the largest observed noisy objective function value and \u223c N (0, \u03c32). Constraint (i) is equivalent to imposing that f(x?) is larger than objective function values at current query locations, whilst condition (ii) makes f(x?) larger than previous objective function evaluations, accounting for noise. Denoting the two conditions C, and the variables f = [f1, ..., fQ] > and f+ = [f ; f ?], where f? = f(x\u2217), we incorporate the conditions as follows\np ( f |D,St,x\u2217 ) \u2248 \u222b p ( f+|D,St,x\u2217 ) \u03a6 (f? \u2212 ymax\n\u03c3\n) Q\u220f q=1 I(f? \u2265 fq) df?, (4)\nwhere I(.) is an indicator function. The integral in (4) can be approximated using expectation propagation [21]. The Gaussian process predictive p(f+|D,St,x\u2217) isN (f+; m+,K+). We approximate the integrand of (4) with w(f+) = N (f+; m+,K+) \u220fQ+1 q=1 Z\u0303qN (c>q f+; \u00b5\u0303q, \u03c4\u0303q), where each Z\u0303q and \u03c4\u0303q are positive, \u00b5\u0303q \u2208 R and for q \u2264 Q, cq is a vector of length Q+ 1 with qth entry\u22121, Q+ 1st entry 1, and remaining entries 0, whilst cQ+1 = [0, ..., 0, 1]>. The approximation w(f+) approximates the Gaussian CDF, \u03a6(.), and each indicator function, I(.), with a univariate, scaled Gaussian PDF. The site parameters, {Z\u0303q, \u00b5\u0303q, \u03c4\u0303q}Q+1q=1 , are learned using a fast EP algorithm, for which details are given in the Appendix, where we show that w(f+) = ZN (f+;\u00b5+,\u03a3+), where\n\u00b5+ = \u03a3+ ( K\u22121+ m+ + Q+1\u2211 q=1 \u00b5\u0303q \u03c4\u0303q cqc > q )\u22121 , \u03a3+ = ( K\u22121+ + Q+1\u2211 q=1 1 \u03c4\u0303q cqc > q )\u22121 , (5)\nand hence p ( f+|D,St,C ) \u2248 N (f+;\u00b5+,\u03a3+). Since multivariate Gaussians are consistent un-\nder marginalization, a convenient corollary is that p ( f |D,St,x\u2217 ) \u2248 N (f ;\u00b5,\u03a3), where \u00b5 is the vector containing the first Q elements of \u00b5+, and \u03a3 is the matrix containing the first Q rows and columns of \u03a3+. Since sums of independent Gaussians are also Gaussian distributed, we see that p ( {yq}Qq=1|D,St,x\u2217 ) \u2248 N ([y1, ..., yQ]>;\u00b5,\u03a3 + \u03c32I). The final convenient attribute of our Gaussian approximation, is that the differential entropy of a multivariate Gaussian can be computed analytically, such that H [ p ( {yq}Qq=1|D,St,x\u2217 )] \u2248 0.5 log[det(2\u03c0e(\u03a3 + \u03c32I))]."}, {"heading": "3.2 Sampling from the Posterior over the Global Maximizer", "text": "So far, we have considered how to approximate H [ p ( {yq}Qq=1|D,St,x\u2217 )] , given the global maximizer, x\u2217. We in fact would like the expected value of this quantity over the posterior distribution of the global maximizer, p(x\u2217|D). Literally, p(x\u2217|D) \u2261 p(f(x\u2217) = maxx\u2208X f(x)|D), the posterior probability that x\u2217 is the global maximizer of f . Computing the distribution p(x\u2217|D) is intractable, but it is possible to approximately sample from it and compute a Monte Carlo based approximation of the desired expectation. We consider two approaches to sampling from the posterior of the global maximizer: (i) a maximum a posteriori (MAP) method, and (ii) a random feaure approach.\nMAP sample from p(x\u2217|D). The MAP of p(x\u2217|D) is its posterior mode, given by x\u2217MAP = argmaxx\u2217\u2208X p(x\n\u2217|D). We may approximate the expected value of the predictive entropy by replacing the posterior distribution of x\u2217 with a single point estimate at x\u2217MAP. There are two key advantages to using the MAP estimate in this way. Firstly, it is simple to compute x\u2217MAP, as it is the global maximizer of the posterior mean of f given the observations D. Secondly, choosing to use x\u2217MAP assists the EP algorithm developed in Section 3.1 to converge as desired. This is because the condition f(x\u2217) \u2265 f(x) for x \u2208 X is easy to enforce when x\u2217 = x\u2217MAP, the global maximizer of the poserior mean of f . When x\u2217 is sampled such that the posterior mean at x\u2217 is significantly suboptimal, the EP approximation may be poor. Whilst using the MAP estimate approximation is convenient, it is after all a point estimate and fails to characterize the full posterior distribution. We therefore consider a method to draw samples from p(x\u2217|D) using random features.\nRandom Feature Samples from p(x\u2217|D). A naive approach to sampling from p(x\u2217|D) would be to sample g \u223c p(f |D), and choosing argmaxx\u2208X g. Unfortunately, this would require sampling g over an uncountably infinite space, which is infeasible. A slightly less naive method would be to sequentially construct g, whilst optimizing it, instead of evaluating it everywhere in X . However,\nthis approach would have costO(m3) wherem is the number of function evaluations of g necessary to find its optimum. We propose as in [13], to sample and optimize an analytic approximation to g.\nBy Bochner\u2019s theorem [22], a stationary kernel function, k, has a Fourier dual s(w), which is equal to the spectral density of k. Setting p(w) = s(w)/\u03b1, a normalized density, we can write\nk(x,x\u2032) = \u03b1Ep(w)[e\u2212iw >(x\u2212x\u2032)] = 2\u03b1Ep(w,b)[cos(w>x+ b) cos(w>x\u2032 + b)], (6)\nwhere b \u223c U [0, 2\u03c0]. Let \u03c6(x) = \u221a\n2\u03b1/m cos(Wx+b) denote anm-dimensional feature mapping where W and b consist of m stacked samples from p(w, b), then the kernel k can be approximated by the inner product of these features, k(x,x\u2032) \u2248 \u03c6(x)>\u03c6(x\u2032) [23]. The linear model g(x) = \u03c6(x)>\u03b8 + \u03bb where \u03b8|D \u223c N (A\u22121\u03c6>(y \u2212 \u03bb1), \u03c32A\u22121) is an approximate sample from p(f |D), where y is a vector of objective function evaluations, A = \u03c6>\u03c6+\u03c32I and \u03c6> = [\u03c6(x1)...\u03c6(xn)]. In fact, limm\u2192\u221e g is a true sample from p(f |D) [24]. The generative process above suggests the following approach to approximately sampling from p(x\u2217|D): (i) sample random features \u03c6(i) and corresponding posterior weights \u03b8(i) using the process above, (ii) construct g(i)(x) = \u03c6(i)(x)>\u03b8(i) + \u03bb, and (iii) finally compute x?(i) = argmaxx\u2208X g (i)(x) using gradient based methods."}, {"heading": "3.3 Computing and Optimizing the PPES Approximation", "text": "Let \u03c8 denote the set of kernel parameters and the observation noise variance, \u03c32. Our posterior belief about \u03c8 is summarized by the posterior distribution p(\u03c8|D) \u221d p(\u03c8)p(D|\u03c8), where p(\u03c8) is our prior belief about \u03c8 and p(D|\u03c8) is the GP marginal likelihood given the parameters \u03c8. For a fully Bayesian treatment of\u03c8, we must marginalize aPPES with respect to p(\u03c8|D). The expectation with respect to the posterior distribution of \u03c8 is approximated with Monte Carlo samples. A similar approach is taken in [3, 13]. Combining the EP based method to approximate the predictive entropy with either of the two methods discussed in the previous section to approximately sample from p(x\u2217|D), we can construct a\u0302PPES an approximation to (2), defined by\na\u0302PPES(St|D) = 1\n2M M\u2211 i=1 [ log[det(K(i) + \u03c32(i)I)]\u2212 log[det(\u03a3(i) + \u03c32(i)I)] ] , (7)\nwhere K(i) is constructed using \u03c8(i) the ith sample of M from p(\u03c8|D), \u03a3(i) is constructed as in Section 3.1, assuming the global maximizer is x\u2217(i) \u223c p(x\u2217|D,\u03c8(i)). The PPES approximation is simple and amenable to gradient based optimization. Our goal is to choose St = {x1, ...,xQ} which maximizes a\u0302PPES in (7). Since our kernel function is differentiable, we may consider taking the derivative of a\u0302PPES with respect to xq,d, the dth component of xq ,\n\u2202 a\u0302PPES \u2202 xq,d = 1 2M M\u2211 i=1 [ trace [ (K(i) + \u03c32(i)I)\u22121 \u2202K(i) \u2202xq,d ] \u2212 trace [ (\u03a3(i) + \u03c32(i)I)\u22121 \u2202\u03a3(i) \u2202xq,d ]] . (8)\nComputing \u2202K (i) \u2202xq,d is simple directly from the definition of the chosen kernel function. \u03a3(i) is a function of K(i), {cq}Q+1q=1 and {\u03c3\u0303 (i) q }Q+1q=1 , and we know how to compute \u2202K (i) \u2202xq,d , and that each cq is a constant vector. Hence our only concern is how the EP site parameters, {\u03c3\u0303(i)q }Q+1q=1 , vary with xq,d. Rather remarkably, we may invoke a result from Section 2.1 of [25], which says that converged site parameters, {Z\u0303q, \u00b5\u0303q, \u03c3\u0303q}Q+1q=1 , have 0 derivative with respect to parameters of p(f+|D,St,x\u2217). There is a key distinction between explicit dependencies (where \u03a3 actually depends on K) and implicit dependencies where a site parameter, \u03c3\u0303q , might depend implicitly on K. A similar approach is taken in [26], and discussed in [7]. We therefore compute\n\u2202\u03a3 (i) + \u2202xq,d = \u03a3 (i) + K (i)\u22121 + \u2202K (i) + \u2202xq,d K (i)\u22121 + \u03a3 (i) + . (9)\nOn first inspection, it may seem computationally too expensive to compute derivatives with respect to each q and d. However, note that we may compute and store the matrices K(i)\u22121+ \u03a3 (i) + , (K (i) +\n\u03c32(i)I)\u22121 and (\u03a3(i) + \u03c32(i)I)\u22121 once, and that \u2202K\n(i) +\n\u2202xq,d is symmetric with exactly one non-zero row\nand non-zero column, which can be exploited for fast matrix multiplication and trace computations.\n0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8 1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7 0.8\n0 0.2 0.4 0.6 0.8 1 \u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n1\n(a) Synthetic function 0 0.2 0.4 0.6 0.8 1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.4\n0.6\n0.8\n1\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0 0.2 0.4 0.6 0.8 1 \u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n1\n(b) aPPES(x, x\u2032)\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8 1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6 0.7\n0 0.2 0.4 0.6 0.8 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0\n0.5\n1\n1.5\n2\n1\n(c) a\u0302PPES(x, x\u2032)"}, {"heading": "4 Empirical Study", "text": "In this section, we study the performance of PPES in comparison to aforementioned methods. We model f as a Gaussian process with constant mean \u03bb and covariance kernel k. Observations of the objective function are considered to be independently drawn fromN (f(x), \u03c32). In our experiments, we choose to use a squared-exponential kernel of the form k(x,x\u2032) = \u03b32 exp [ \u2212 0.5\u2211d(xd \u2212 x\u2032d) 2/l2d ] . Therefore the set of model hyperparameters is {\u03bb, \u03b3, l1, ..., lD, \u03c3}, a broad Gaussian hyperprior is placed on \u03bb and uninformative Gamma priors are used for the other hyperparameters.\nIt is worth investigating how well a\u0302PPES (7) is able to approximate aPPES (2). In order to test the approximation in a manner amenable to visualization, we generate a sample f from a Gaussian process prior on X = [0, 1], with \u03b32 = 1, \u03c32 = 10\u22124 and l2 = 0.025, and consider batches of size Q = 2. We set M = 200. A rejection sampling based approach is used to compute the ground truth aPPES, defined on XQ = [0, 1]2. We first discretize [0, 1]2, and sample p(x\u2217|D) in (2) by evaluating samples from p(f |D) on the discrete points and choosing the input with highest function value. Given x\u2217, we compute H [ p ( y1, y2|D,x1,x2,x\u2217 )] using rejection sampling. Samples from p(f |D) are evaluted on discrete points in [0, 1]2 and rejected if the highest function value occurs not at x\u2217. We add independent Gaussian noise with variance \u03c32 to the non rejected samples from the previous step and approximate H [ p ( y1, y2|D,x1,x2,x\u2217 )] using kernel density estimation [27].\nFigure 1 includes illustrations of (a) the objective function to be maximized, f , with 5 noisy observations, (b) the aPPES ground truth obtained using the rejection sampling method and finally (c) a\u0302PPES using the EP method we develop in the previous section. The black squares on the axes of Figures 1(b) and 1(c) represent the locations in X = [0, 1] where f has been noisily sampled, and the darker the shade, the larger the function value. The lightly shaded horizontal and vertical lines in these figures along the points The figures representing aPPES and a\u0302PPES appear to be symmetric, as is expected, since the set St = {x, x\u2032} is not an ordered set, since all points in the set are probed in parallel i.e. St = {x, x\u2032} = {x\u2032, x}. The surface of a\u0302PPES is similar to that of aPPES. In paticular, the a\u0302PPES approximation often appeared to be an annealed version of the ground truth aPPES, in the sense that peaks were more pronounced, and non-peak areas were flatter. Since we are interested in argmax{x,x\u2032}\u2208X 2 aPPES({x, x\u2032}), our key concern is that the peaks of a\u0302PPES occur at the same input locations as aPPES. This appears to be the case in our experiment, suggesting that the argmax a\u0302PPES is a good approximation for argmaxaPPES.\nWe now test the performance of PPES in the task of finding the optimum of various objective functions. For each experiment, we compare PPES (M = 200) to EI-MCMC (with 100 MCMC samples), simulated matching with a UCB baseline policy, GP-BUCB and GP-UCB-PE. We use the random features method to sample from p(x\u2217|D), rejecting samples which lead to failed EP runs. An experiment of an objective function, f , consists of sampling 5 input points uniformly at random and running each algorithm starting with these samples and their corresponding (noisy) function values. We measure performance after t batch evaluations using immediate regret, rt = |f(x\u0303t) \u2212 f(x\u2217)|, where x\u2217 is the known optimizer of f and x\u0303t is the recommendation of an algorithm after t batch\n0 10 20 30 0\n1\n2\n3\n4\n5\nt\nre gr et\n0 10 20 30 40 50 0\n0.5\n1\n1.5\n2\n2.5\nt\nre gr et\n1\n0 10 20 30 0\n1\n2\n3\n4\n5\nt\nre gr et\n0 10 20 30 40 50 0\n0.5\n1\n1.5\n2\n2.5\nt\nre gr et\n1\n2\n4\n6\n8 10\nre gr et\nPPES EI-MCMC SMUCB BUCB UCBPE\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6 0.7 0.8\nre g re t\n1\n2\n4\n6\n8 10\nre g re t\nPPES EI-MC SMUCB BUCB UCBPE\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6 0.7 0.8\nre gr et\n1\nevaluations. We perform 100 experiments for each objective function, and report the median of the immediate regret obtained for each algorithm. The confidence bands represent one standard deviation obtained from bootstrapping. The empirical distribution of the immediate regret is heavy tailed, making the median more representative of where most data points lie than the mean.\nOur first set of experiments is on a set of synthetic benchmark objective functions including BraninHoo [28], a mixture of cosines [29], a Shekel function with 10 modes [30] (each defined on [0, 1]2) and the Hartmann-6 function [28] (defined on [0, 1]6). We choose batches of size Q = 3 at each decision time. The plots in Figure 2 illustrate the median immediate regrets found for each algorithm. The results suggest that the PPES algorithm performs close to best if not the best for each problem considered. EI-MCMC does significantly better on the Hartmann function, which is a relatively smooth function with very few modes, where greedy search appears beneficial. Entropy-based strategies are more exploratory in higher dimensions. Nevertheless, PPES does significantly better than GP-UCB-PE on 3 of the 4 problems, suggesting that our non-greedy batch selection procedure enhances performance versus a greedy entropy based policy.\nWe now consider maximization of real world objective functions. The first, boston, returns the negative of the prediction error of a neural network trained on a random train/text split of the Boston Housing dataset [31]. The weight-decay parameter and number of training iterations for the neural network are the parameters to be optimized over. The next function, hydrogen, returns the amount of hydrogen produced by particular bacteria as a function of pH and nitrogen levels of a growth medium [32]. Thirdly we consider a function, rocket, which runs a simulation of a rocket [33] being launched from the Earth\u2019s surface and returns the time taken for the rocket to land on the Earth\u2019s surface. The variables to be optimized over are the launch height from the surface, the mass of fuel to use and the angle of launch with respect to the Earth\u2019s surface. If the rocket does not return, the function returns 0. Finally we consider a function, robot, which returns the walking speed of a bipedal robot [34]. The function\u2019s input parameters, which live in [0, 1]8, are the robot\u2019s controller. We add Gaussian noise with \u03c3 = 0.1 to the noiseless function. Note that all of the functions we consider are not available analytically. boston trains a neural network and returns test error, whilst rocket and robot run physical simulations involving differential equations before returning a desired quantity. Since the hydrogen dataset is available only for discrete points, we define hydrogen to return the predictive mean of a Gaussian process trained on the dataset.\nFigure 3 show the median values of immediate regret by each method over 200 random initializations. We consider batches of size Q = 2 and Q = 4. We find that PPES consistently outperforms competing methods on the functions considered. The greediness and nonrequirement of MCMC sampling of the SM-UCB, GP-BUCB and GP-UCB-PE algorithms make them amenable to large batch experiments, for example, [17] consider optimization in R45 with batches of size 10. However, these three algorithms all perform poorly when selecting batches of smaller size. The performance on the hydrogen function illustrates an interesting phenemona; whilst the immediate regret of PPES is mediocre initially, it drops rapidly as more batches are evaluated.\nThis behaviour is likely due to the non-greediness of the approach we have taken. EI-MCMC makes good initial progress, but then fails to explore the input space as well as PPES is able to. Recall that after each batch evaluation, an algorithm is required to output x\u0303t, its best estimate for the maximizer\n1\n1\n1\n1\n1\n1\n1\n1\nof the objective function. We observed that whilst competing algorithms tended to evaluate points which had high objective function values compared to PPES, yet when it came to recommending x\u0303t, PPES tended to do a better job. Our belief is that this occured exactly because the PPES objective aims to maximize information gain rather than objective function value improvement.\nThe rocket function has a strong discontinuity making if difficult to maximize. If the fuel mass, launch height and/or angle are too high, the rocket would not return to the Earth\u2019s surface, resulting in a 0 function value. It can be argued that a stationary kernel Gaussian process is a poor model for this function, yet it is worth investigating the performance of a GP based models since a practitioner may not know whether or not their black-box function is smooth apriori. PPES seemed to handle this function best and had fewer samples which resulted in 0 function value than each of the competing methods and made fewer recommendations which led to a 0 function value. The relative increase in PPES performance from increasing batch size from Q = 2 to Q = 4 is small for the robot function compared to the other functions considered. We believe this is a consequence of using a slightly naive optimization procedure to save computation time. Our optimization procedure first computes a\u0302PPES at 1000 points selected uniformly at random, and performs gradient ascent from the best point. Since a\u0302PPES is defined on XQ = [0, 1]32, this method may miss a global optimum. Other methods all select their batches greedily, and hence only need to optimize in X = [0, 1]8. However, this should easily be avoided by using a more exhaustive gradient based optimizer."}, {"heading": "5 Conclusions", "text": "We have developed parallel predictive entropy search, an information theoretic approach to batch Bayesian optimization. Our method is greedy in the sense that it aims to maximize the one-step information gain about the location of x\u2217, but it is not greedy in how it selects a set of points to evaluate next. Previous methods are doubly greedy, in that they look one step ahead, and also select a batch of points greedily. Competing methods are prone to under exploring, which hurts their perfomance on multi-modal, noisy objective functions, as we demonstrate in our experiments."}], "references": [{"title": "Review of Metamodeling Techniques in Support of Engineering Design Optimization", "author": ["G. Wang", "S. Shan"], "venue": "Journal of Mechanical Design, 129(4):370\u2013380", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic Optimization Models in Finance", "author": ["W. Ziemba", "R. Vickson"], "venue": "World Scientific Singapore,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Practical Bayesian Optimization of Machine Learning Algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian Approach to Global Optimization: Theory and Applications", "author": ["J. Mockus"], "venue": "Kluwer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Automatic Gait Optimization with Gaussian Process Regression", "author": ["D. Lizotte", "T. Wang", "M. Bowling", "D. Schuurmans"], "venue": "IJCAI, pages 944\u2013949", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "The Knowledge-Gradient Algorithm for Sequencing Experiments in Drug Discovery", "author": ["D.M. Negoescu", "P.I. Frazier", "W.B. Powell"], "venue": "INFORMS Journal on Computing, 23(3):346\u2013 363", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian Processes for Machine Learning", "author": ["Carl Rasmussen", "Chris Williams"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Student-t Processes as Alternatives to Gaussian Processes", "author": ["A. Shah", "A.G. Wilson", "Z. Ghahramani"], "venue": "AISTATS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Mr Prabat", "author": ["J. Snoek", "O. Rippel", "K. Swersky", "R. Kiros", "N. Satish", "N. Sundaram", "M. Patwary"], "venue": "and R. P. Adams. Scalable Bayesian Optimization Using Deep Neural Networks. ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "and N", "author": ["E. Brochu", "M. Cora"], "venue": "de Freitas. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Applications to Active User Modeling and Hierarchical Reinforcement Learning. Technical Report TR-2009-23, University of British Columbia", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Traveling salesman should not be greedy:domination analysis of greedy-type heuristics for the TSP", "author": ["G. Gutin", "A. Yeo", "A. Zverovich"], "venue": "Discrete Applied Mathematics, 117:81\u201386", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Entropy Search for Information-Efficient Global Optimization", "author": ["P. Hennig", "C.J. Schuler"], "venue": "JMLR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions", "author": ["J.M. Hern\u00e1ndez-Lobato", "M.W. Hoffman", "Z. Ghahramani"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Dealing with Asynchronicity in Parallel Gaussian Process Based Optimization", "author": ["D. Ginsbourger", "J. Janusevskis", "R. Le Riche"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Batch Bayesian Optimization via Simulation Matching", "author": ["J. Azimi", "A. Fern", "X.Z. Fern"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design", "author": ["N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger"], "venue": "ICML", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization", "author": ["T. Desautels", "A. Krause", "J. Burdick"], "venue": "ICML", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel Gaussian Process Optimization with Upper Confidence Bound and Pure Exploration", "author": ["E. Contal", "D. Buffoni", "D. Robicquet", "N. Vayatis"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 225\u2013240. Springer Berlin Heidelberg", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Information-Based Objective Functions for Active Data Selection", "author": ["D.J. MacKay"], "venue": "Neural Computation, 4(4):590\u2013604", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1992}, {"title": "Collaborative Gaussian Processes for Preference Learning", "author": ["N. Houlsby", "J.M. Hern\u00e1ndez-Lobato", "F. Huszar", "Z. Ghahramani"], "venue": "NIPS", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "A Family of Algorithms for Approximate Bayesian Inference", "author": ["T.P. Minka"], "venue": "PhD thesis, Masachusetts Institute of Technology", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Lectures on Fourier Integrals", "author": ["S. Bochner"], "venue": "Princeton University Press", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1959}, {"title": "Random Features for Large-Scale Kernel Machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian Learning for Neural Networks", "author": ["R.M. Neal"], "venue": "PhD thesis, University of Toronto", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Expectation Propagation for Exponential Families", "author": ["M. Seeger"], "venue": "Technical Report, U.C. Berkeley", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Gaussian Probabilities and Expectation Propagation", "author": ["J.P. Cunningham", "P. Hennig", "S. Lacoste-Julien"], "venue": "arXiv", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "A Nonparametric Estimation of the Entropy for Absolutely Continuous Distributions", "author": ["I. Ahmad", "P.E. Lin"], "venue": "IEEE Trans. on Information Theory, 22(3):372\u2013375", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1976}, {"title": "Practical Bayesian Optimization", "author": ["D. Lizotte"], "venue": "PhD thesis, University of Alberta", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "A Nonparametric Approach to Noisy and Costly Optimization", "author": ["B.S. Anderson", "A.W. Moore", "D. Cohn"], "venue": "ICML", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Test Functions for Multimodal Search Techniques", "author": ["J. Shekel"], "venue": "Information Science and Systems", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1971}, {"title": "Optimization of ph and nitrogen for enhanced hydrogen production by synechocystis sp", "author": ["E.H. Burrows", "W.K. Wong", "X. Fern", "F.W.R. Chaplen", "R.L. Ely"], "venue": "pcc 6803 via statistical and machine learning methods. Biotechnology Progress, 25(4):1009\u20131017", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "In Classical Mechanics with MATLAB Applications", "author": ["J.E. Hasbun"], "venue": "Jones & Bartlett Learning", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Feedback Control of Dynamic Bipedal Robot Locomotion", "author": ["E. Westervelt", "J. Grizzle"], "venue": "Control and Automation Series. CRC PressINC", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "EP: A Quick Reference", "author": ["T. Minka"], "venue": "Technical Report", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "engineering design [1], finance [2] and algorithm optimization [3].", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "engineering design [1], finance [2] and algorithm optimization [3].", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "engineering design [1], finance [2] and algorithm optimization [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Bayesian optimization [4] has been successfully applied in a range of difficult, expensive global optimization tasks including optimizing a robot controller to maximize gait speed [5] and discovering a chemical derivative of a particular molecule which best treats a particular disease [6].", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "Bayesian optimization [4] has been successfully applied in a range of difficult, expensive global optimization tasks including optimizing a robot controller to maximize gait speed [5] and discovering a chemical derivative of a particular molecule which best treats a particular disease [6].", "startOffset": 180, "endOffset": 183}, {"referenceID": 5, "context": "Bayesian optimization [4] has been successfully applied in a range of difficult, expensive global optimization tasks including optimizing a robot controller to maximize gait speed [5] and discovering a chemical derivative of a particular molecule which best treats a particular disease [6].", "startOffset": 286, "endOffset": 289}, {"referenceID": 6, "context": "A common approach for modeling f is to use a Gaussian process prior [7], as it is highly flexible and amenable to analytic calculations.", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "Student-t process priors [8] and deep neural networks [9].", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "Student-t process priors [8] and deep neural networks [9].", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "expected improvement, probability of improvement and upper confidence bound [10], there are few well known strategies for selecting batches of points.", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "Greedy choice making can be severely detrimental, for example, a greedy approach to the travelling salesman problem could potentially lead to the uniquely worst global solution [11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "The algorithm we develop, parallel predictive entropy search, extends the methods of [12, 13] to multiple point batch selection.", "startOffset": 85, "endOffset": 93}, {"referenceID": 12, "context": "The algorithm we develop, parallel predictive entropy search, extends the methods of [12, 13] to multiple point batch selection.", "startOffset": 85, "endOffset": 93}, {"referenceID": 13, "context": "[14] considered an approach which sequentially used the EI criterion to greedily choose a batch of points to query next, which [3] formalized and utilized by defining aEI\u2212MCMC ( x|D, {xq\u2032}qq\u2032=1 ) = \u222b", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[14] considered an approach which sequentially used the EI criterion to greedily choose a batch of points to query next, which [3] formalized and utilized by defining aEI\u2212MCMC ( x|D, {xq\u2032}qq\u2032=1 ) = \u222b", "startOffset": 127, "endOffset": 130}, {"referenceID": 14, "context": "A similar but different approach called simulated matching (SM) was introduced by [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "The upper confidence bound (UCB) strategy [16] is another method used by practitioners to decide where to evaluate an objective function next.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "In order to extend this approach to the parallel setting, [17] noted that the predictive variance of a Gaussian process depends only on where observations are made, and not the observations themselves.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Finally, a variant of the GP-UCB was proposed by [18].", "startOffset": 49, "endOffset": 53}, {"referenceID": 18, "context": "3 Parallel Predictive Entropy Search Our approach is to maximize information [19] about the location of the global maximizer x\u2217, which we measure in terms of the negative differential entropy of p(x\u2217|D).", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "Analogous to [13], PPES aims to choose the set of Q points, St = {xq}Qq=1, which maximizes aPPES(St|D) = H [ p(x\u2217|D) ] \u2212 E p ( {yq}Qq=1 \u2223\u2223D,St)[H[p(x\u2217|D \u222a {xq, yq}Qq=1)]], (1) where H[p(x)] = \u2212 \u222b p(x) log p(x)dx is the differential entropy of its argument and the expectation above is taken with respect to the posterior joint predictive distribution of {yq}Qq=1 given the previous evaluations, D, and the set St.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "Significant approximations need to be made to (1) before it becomes practically useful [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "A convenient equivalent formulation of the quantity in (1) can be written as the mutual information between x\u2217 and {yq}Qq=1 given D [20].", "startOffset": 132, "endOffset": 136}, {"referenceID": 20, "context": "The integral in (4) can be approximated using expectation propagation [21].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "We propose as in [13], to sample and optimize an analytic approximation to g.", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "By Bochner\u2019s theorem [22], a stationary kernel function, k, has a Fourier dual s(w), which is equal to the spectral density of k.", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "Let \u03c6(x) = \u221a 2\u03b1/m cos(Wx+b) denote anm-dimensional feature mapping where W and b consist of m stacked samples from p(w, b), then the kernel k can be approximated by the inner product of these features, k(x,x\u2032) \u2248 \u03c6(x)>\u03c6(x\u2032) [23].", "startOffset": 223, "endOffset": 227}, {"referenceID": 23, "context": "In fact, limm\u2192\u221e g is a true sample from p(f |D) [24].", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "A similar approach is taken in [3, 13].", "startOffset": 31, "endOffset": 38}, {"referenceID": 12, "context": "A similar approach is taken in [3, 13].", "startOffset": 31, "endOffset": 38}, {"referenceID": 24, "context": "1 of [25], which says that converged site parameters, {Z\u0303q, \u03bc\u0303q, \u03c3\u0303q} q=1 , have 0 derivative with respect to parameters of p(f+|D,St,x).", "startOffset": 5, "endOffset": 9}, {"referenceID": 25, "context": "A similar approach is taken in [26], and discussed in [7].", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "A similar approach is taken in [26], and discussed in [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "(a) Synthetic objective function (blue line) defined on [0, 1], with noisy observations (black squares).", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "(b) Ground truth aPPES defined on [0, 1], obtained by rejection sampling.", "startOffset": 34, "endOffset": 40}, {"referenceID": 0, "context": "In order to test the approximation in a manner amenable to visualization, we generate a sample f from a Gaussian process prior on X = [0, 1], with \u03b3 = 1, \u03c3 = 10\u22124 and l = 0.", "startOffset": 134, "endOffset": 140}, {"referenceID": 0, "context": "A rejection sampling based approach is used to compute the ground truth aPPES, defined on XQ = [0, 1].", "startOffset": 95, "endOffset": 101}, {"referenceID": 0, "context": "We first discretize [0, 1], and sample p(x\u2217|D) in (2) by evaluating samples from p(f |D) on the discrete points and choosing the input with highest function value.", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "Samples from p(f |D) are evaluted on discrete points in [0, 1] and rejected if the highest function value occurs not at x\u2217.", "startOffset": 56, "endOffset": 62}, {"referenceID": 26, "context": "We add independent Gaussian noise with variance \u03c3 to the non rejected samples from the previous step and approximate H [ p ( y1, y2|D,x1,x2,x\u2217 )] using kernel density estimation [27].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "The black squares on the axes of Figures 1(b) and 1(c) represent the locations in X = [0, 1] where f has been noisily sampled, and the darker the shade, the larger the function value.", "startOffset": 86, "endOffset": 92}, {"referenceID": 27, "context": "Our first set of experiments is on a set of synthetic benchmark objective functions including BraninHoo [28], a mixture of cosines [29], a Shekel function with 10 modes [30] (each defined on [0, 1]) and the Hartmann-6 function [28] (defined on [0, 1]).", "startOffset": 104, "endOffset": 108}, {"referenceID": 28, "context": "Our first set of experiments is on a set of synthetic benchmark objective functions including BraninHoo [28], a mixture of cosines [29], a Shekel function with 10 modes [30] (each defined on [0, 1]) and the Hartmann-6 function [28] (defined on [0, 1]).", "startOffset": 131, "endOffset": 135}, {"referenceID": 29, "context": "Our first set of experiments is on a set of synthetic benchmark objective functions including BraninHoo [28], a mixture of cosines [29], a Shekel function with 10 modes [30] (each defined on [0, 1]) and the Hartmann-6 function [28] (defined on [0, 1]).", "startOffset": 169, "endOffset": 173}, {"referenceID": 0, "context": "Our first set of experiments is on a set of synthetic benchmark objective functions including BraninHoo [28], a mixture of cosines [29], a Shekel function with 10 modes [30] (each defined on [0, 1]) and the Hartmann-6 function [28] (defined on [0, 1]).", "startOffset": 191, "endOffset": 197}, {"referenceID": 27, "context": "Our first set of experiments is on a set of synthetic benchmark objective functions including BraninHoo [28], a mixture of cosines [29], a Shekel function with 10 modes [30] (each defined on [0, 1]) and the Hartmann-6 function [28] (defined on [0, 1]).", "startOffset": 227, "endOffset": 231}, {"referenceID": 0, "context": "Our first set of experiments is on a set of synthetic benchmark objective functions including BraninHoo [28], a mixture of cosines [29], a Shekel function with 10 modes [30] (each defined on [0, 1]) and the Hartmann-6 function [28] (defined on [0, 1]).", "startOffset": 244, "endOffset": 250}, {"referenceID": 30, "context": "The next function, hydrogen, returns the amount of hydrogen produced by particular bacteria as a function of pH and nitrogen levels of a growth medium [32].", "startOffset": 151, "endOffset": 155}, {"referenceID": 31, "context": "Thirdly we consider a function, rocket, which runs a simulation of a rocket [33] being launched from the Earth\u2019s surface and returns the time taken for the rocket to land on the Earth\u2019s surface.", "startOffset": 76, "endOffset": 80}, {"referenceID": 32, "context": "Finally we consider a function, robot, which returns the walking speed of a bipedal robot [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "The function\u2019s input parameters, which live in [0, 1], are the robot\u2019s controller.", "startOffset": 47, "endOffset": 53}, {"referenceID": 16, "context": "The greediness and nonrequirement of MCMC sampling of the SM-UCB, GP-BUCB and GP-UCB-PE algorithms make them amenable to large batch experiments, for example, [17] consider optimization in R with batches of size 10.", "startOffset": 159, "endOffset": 163}, {"referenceID": 0, "context": "Since \u00e2PPES is defined on XQ = [0, 1], this method may miss a global optimum.", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "Other methods all select their batches greedily, and hence only need to optimize in X = [0, 1].", "startOffset": 88, "endOffset": 94}], "year": 2015, "abstractText": "We develop parallel predictive entropy search (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a batch of points which will maximize the information gain about the global maximizer of the objective. Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first nongreedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.", "creator": "LaTeX with hyperref package"}}}