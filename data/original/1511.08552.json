{"id": "1511.08552", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Simultaneous Private Learning of Multiple Concepts", "abstract": "We investigate the direct-sum problem in the context of differentially private PAC learning: What is the sample complexity of solving $k$ learning tasks simultaneously under differential privacy, and how does this cost compare to that of solving $k$ learning tasks without privacy? In our setting, an individual example consists of a domain element $x$ labeled by $k$ unknown concepts $(c_1,\\ldots,c_k)$. The goal of a multi-learner is to output $k$ hypotheses $(h_1,\\ldots,h_k)$ that generalize the input examples.", "histories": [["v1", "Fri, 27 Nov 2015 03:57:22 GMT  (35kb,D)", "http://arxiv.org/abs/1511.08552v1", "29 pages. To appear in ITCS '16"]], "COMMENTS": "29 pages. To appear in ITCS '16", "reviews": [], "SUBJECTS": "cs.DS cs.CR cs.LG", "authors": ["mark bun", "kobbi nissim", "uri stemmer"], "accepted": false, "id": "1511.08552"}, "pdf": {"name": "1511.08552.pdf", "metadata": {"source": "CRF", "title": "Simultaneous Private Learning of Multiple Concepts", "authors": ["Mark Bun", "Kobbi Nissim", "Uri Stemmer"], "emails": ["mbun@seas.harvard.edu", "kobbi@cs.bgu.ac.il,", "kobbi@seas.harvard.edu", "stemmer@cs.bgu.ac.il"], "sections": [{"heading": null, "text": "Without concern for privacy, the sample complexity needed to simultaneously learn k concepts is essentially the same as needed for learning a single concept. Under differential privacy, the basic strategy of learning each hypothesis independently yields sample complexity that grows polynomially with k. For some concept classes, we give multi-learners that require fewer samples than the basic strategy. Unfortunately, however, we also give lower bounds showing that even for very simple concept classes, the sample cost of private multi-learning must grow polynomially in k.\nKeywords: differential privacy, PAC learning, agnostic learning, direct-sum \u2217John A. Paulson School of Engineering & Applied Sciences, Harvard University. Supported by an NDSEG fellowship and NSF grant CNS-1237235. Work done in part while the author was visiting Yale University. mbun@seas.harvard.edu \u2020Dept. of Computer Science, Ben-Gurion University and Center for Research on Computation & Society (CRCS), Harvard University. Supported by NSF grant CNS-1237235, a gift from Google, Inc., a Simons Investigator grant, and ISF grant 276/12. kobbi@cs.bgu.ac.il, kobbi@seas.harvard.edu \u2021Dept. of Computer Science, Ben-Gurion University. Supported by the Ministry of Science and Technology (Israel), by the IBM PhD Fellowship Awards Program, and by the Frankel Center for Computer Science. stemmer@cs.bgu.ac.il\nar X\niv :1\n51 1.\n08 55\n2v 1\n[ cs\n.D S]\n2 7\nN ov\n2 01"}, {"heading": "1 Introduction", "text": "The work on differential privacy [18] is aimed at providing useful analyses on privacy-sensitive data while providing strong individual-level privacy protection. One family of such analyses that has received a lot of attention is PAC learning [34]. These tasks abstract many of the computations performed over sensitive information [26].\nWe address the direct-sum problem \u2013 what is the cost of solving multiple instances of a computational task simultaneously as compared to solving each of them separately? \u2013 in the context of differentially private PAC learning. In our setting, individual examples are drawn from domain X and labeled by k unknown concepts (c1, . . . , ck) taken from a concept class C = {c : X \u2192 {0, 1}}, i.e., each example is of the form (x, y1, . . . , yk), where x \u2208 X and yi = ci(x). The goal of a multi-learner is to output k hypotheses (h1, . . . , hk) that generalize the input examples while preserving the privacy of individuals.\nThe direct-sum problem has its roots in complexity theory, and is a basic problem for many algorithmic tasks. It also has implications for the practical use of differential privacy. Consider, for instance, a hospital that collects information about its patients and wishes to use this information for medical research. The hospital records for each patient a collection of attributes such as age, sex, and the results of various diagnostic tests (for each patient, these attributes make up a point x in some domain X) and, for each of k diseases, whether the patient suffers from the disease (the k labels (y1, . . . , yk)). Based on this collection of data, the hospital researchers wish to learn good predictors for the k diseases. One option for the researchers is to perform each of the learning tasks on a fresh sample of patients, hence enlarging the number of patient examples needed (i.e. the sample complexity) by a factor of k, which can be very costly.\nWithout concern for privacy, the sample complexity that is necessary and sufficient for performing the k learning tasks is actually fully characterized by the VC dimension of the concept class C \u2013 it is independent of the number of learning tasks k. In this work, we set out to examine if the situation is similar when the learning is performed with differential privacy. Interestingly, we see that with differential privacy the picture is quite different, and in particular, the required number of examples can grow polynomially in k.\nPrivate learning. A private learner is an algorithm that is given an sample of labeled examples (x, c(x)) (each representing the information and label pertaining to an individual) and outputs a generalizing hypothesis h that guarantees differential privacy with respect to its examples. The first differentially private learning algorithms were given by Blum et al. [9] and the notion of private learning was put forward and formally researched by Kasiviswanathan et al. [26]. Among other results, the latter work presented a generic construction of differentially private learners with sample complexity O(log |C|).\nIn contrast, the sample complexity of (non-private) PAC learning is \u0398(VC(C)), which can be much lower than log |C| for specific concept classes. This gap led to a line of work examining the sample complexity of private learning, which has revealed a significantly more complex picture than there is for nonprivate learning. In particular, for pure differentially private learners, it is known that the sample complexity of proper learning (where the learner returns a hypothesis h taken from C) is sometimes higher than the sample complexity of improper learners (where h comes from an arbitrary hypothesis class H). The latter is characterized by the representation dimension of the concept class C, which is generally higher than the VC dimension [5, 4, 15, 6, 22]. By contrast, a sample complexity gap between proper and improper learners does not exist for non-private learning. In the case of approximate differential privacy no such combinatorial characterization is currently known. It is however known that the sample complexity of such learners can be significantly lower than that of pure-differentially private learners and yet higher than the VC dimension of C [6, 22, 13]. Furthermore, there exist (infinite) PAC-learnable concept classes for which no differentially private learner (pure or approximate) exists.\nPrivate multi-learning. In this work we examine the sample complexity of private multi-learning. Our work is motivated by the recurring research theme of the direct-sum, as well as by the need to understand whether multi-learning remains feasible under differential privacy, as it is without privacy constraints.\nAt first glance, private multi-learning appears to be similar to the query release problem, the goal of which is to approximate the average values of a large collection of predicates on a dataset. One surprising result in differential privacy is that it is possible to answer an exponential number of such queries on a dataset [10, 30, 24]. For example, Blum, Ligett, and Roth [10] showed that given a dataset D and a concept class C, it is possible to generate with differential privacy a dataset D\u0302 such that the average value of c on D approximates the average of c on D\u0302 for every c \u2208 C simultaneously. The sample complexity required, i.e., the size of the database D, to perform this sanitization is only logarithmic in |C|. Results of this flavor suggest that we can also learn exponentially many concepts simultaneously. However, we give negative results showing that this is not the case, and that multi-learning can have significantly higher sample complexity than query release."}, {"heading": "1.1 Our results", "text": "Prior work on privately learning the simple concept classes POINTX (of functions that evaluate to 1 on exactly one point of their domain X and to 0 otherwise) and THRESHX (of functions that evaluate to 1 on a prefix of the domain X and to 0 otherwise) has demonstrated a rather complex picture, depending on whether learners are proper or improper, and whether learning is performed with pure or approximate differential privacy [5, 4, 6, 7, 13]. We analyze the sample complexity of multi-learning of these simple concept classes, as well as general concept classes. We also consider the class PARd of parity functions, but in this case we restrict our attention to uniformly selected examples. We examine both proper and improper PAC and agnostic learning under pure and approximate differential privacy. For ease of reference, we include tables with our results in Section 1.3, where we omit the dependency on the privacy and accuracy parameters.\nTechniques for private k-learning. Composition theorems for differential privacy show that the sample complexity of learning k concepts simultaneously is at most a factor of k larger than the sample complexity of learning one concept (and may be reduced to \u221a k for approximate differential privacy). Unfortunately, privately learning one concept from a concept class C can sometimes be quite costly, requiring much higher sample complexity than VC(C) which is needed to learn non-privately. Building on techniques of Beimel, Nissim, and Stemmer [8], we show that the multiplicative dependence on k can always be reduced to the VC-dimension of C, at the expense of producing a one-time sanitization of the dataset.\nTheorem 1.1 (Informal). Let C be a concept class for which there is pure differentially private sanitizer for C\u2295 = {f \u2295 g : f, g \u2208 C} with sample complexity m. Then there is an pure differentially private agnostic k-learner for C with sample complexity O(m+ k \u00b7VC(C)).\nSimilarly, if C\u2295 has an approximate differentially private sanitizer with sample complexity m, then there is an approximate differentially private agnostic k-learner for C with sample complexity O(m+ \u221a k \u00b7 VC(C)).\nThe best known general-purpose sanitizers require sample complexity m = O(VC(C) log |X|) for pure differential privacy [10] and m = O(log |C| \u221a log |X|) for approximate differential privacy [24]. However, for specific concept classes (such as POINTX and THRESHX ), the sample complexity of sanitization can be much lower.\nIn the case of approximate differential privacy, the sample complexity of k-learning can be even lower than what is achievable with our generic learner. Using stability-based arguments, we show that point func-\ntions and parities under the uniform distribution can be PAC k-learned with sample complexity O(VC(C)) \u2013 independent of the number of concepts k (see Theorems 3.13 and 3.12).\nLower bounds. In light of the above results, one might hope to be able to reduce the dependence on k further, or to eliminate it entirely (as is possible in the case of non-private learning). We show that this is not possible, even for the simplest of concept classes. In the case of pure differential privacy, a packing argument [21, 25, 5] shows that any non-trivial concept class requires sample complexity \u2126(k) to privately k-learn (Theorem 5.1). For approximate differential privacy, we use fingerprinting codes [12, 14] to show that unlike points and parities, threshold functions require sample complexity \u2126\u0303(k1/3) to PAC learn privately (Corollary 4.6). Moreover, any non-trivial concept class requires sample complexity \u2126\u0303( \u221a k) to privately learn in the agnostic model (Theorem 4.7). In the case of point functions, this matches the upper bound achievable by our generic learner.\nWe highlight a few of the main takeaways from our results:\nA complex answer to the direct sum question. Our upper bounds show that solving k learning problems simultaneously can require substantially lower sample complexity than solving the problems individually. On the other hand, our lower bounds show that a significant dependence on k is generally necessary.\nSeparation between private PAC and private agnostic learning. Non-privately, the sample complexities of PAC and agnostic learning are of the same order (differing only in the dependency in the accuracy parameters). Beimel et al. [8] showed that this is also the case with differentially private learning (of one concept). Our results on learning point functions show that private PAC and agnostic multi-learning can be substantially different (even for learning up to constant error). In the case of approximate differential privacy, O(1) sample suffice to PAC-learn multiple point functions. However, \u2126\u0303( \u221a k) samples are needed to learn k points agnostically.\nSeparation between improper learning with approximate differential privacy and non-private learning. Recently, Bun et al. [13] showed that the sample complexity of learning one threshold function with approximate differential privacy exceeds the VC dimension, but only in the case of proper learning. Thus it remains possible that improper learning with approximate differential privacy can match the sample complexity of non-private learning. While we do not address this question directly, we exhibit a separation for multi-learning. In particular, learning k thresholds with approximate differential privacy requires \u2126\u0303(k1/3) samples, even improperly, while O(1) samples suffices non-privately."}, {"heading": "1.2 Related work", "text": "Differential privacy was defined in [18] and the relaxation to approximate differential privacy is from [16]. Most related to our work is the work on private learning and its sample complexity [9, 26, 15, 19, 4, 6, 7, 22, 8, 13] and the early work on sanitization [10]. That many \u201cnatural\u201d learning tasks can be performed privately was shown in the early work of Blum et al. [9] and Kasiviswanathan et al. [26]. A characterization for the sample complexity of pure-private learners was given in [6], in terms of a new combinatorial measure \u2013 the Representation Dimension, that is, given a classC, the number of samples needed and sufficient for privately learning C is \u0398(RepDim(C)). Building on [6], Feldman and Xiao [22] showed an equivalence between the representation dimension of a concept C and the randomized one-way communication complexity of the evaluation problem for concepts from C. Using this equivalence they separated the sample complexity of pure-private learners from that of non-private ones.\nThe problem of learning multiple concepts simultaneously (without privacy) has been considered before. Motivated by the problem of bridging computational learning and reasoning, Valiant [35] also observed that (without privacy) multiple concepts can be learned from a common dataset in a data efficient manner."}, {"heading": "1.3 Tables of results", "text": "The following tables summarize the results of this work. In the tables below C is a class of concepts (i.e., predicates) defined over domain X . Sample complexity upper and lower bounds is given in terms of |C| and |X|. Note that for POINTX , THRESHX , and PARd we have |C| = \u0398(|X|).\nWhere not explicitly noted, upper bounds hold for the setting of agnostic learning and lower bounds are for the (potentially easier) setting of PAC learning. Similarly, where not explicitly noted, upper bounds are for proper learning and lower bounds are for the (less restrictive) setting of improper learning. For simplicity, these tables hide constant and logarithmic factors, as well as dependencies on the learning and privacy parameters.\nMulti-learning with pure differential privacy.\nUpper bounds:\nPAC learning Agnostic learning C proper improper proper improper References\nPOINTX k + log |C| k k + log |C| k Thm. 3.1, Cor. 3.3 THRESHX k + log |C| Thm. 3.1 General min{k log |C|, kVC(C) + log |X|VC(C)} Thm. 3.1\nPARd (uniform) k log |C| Thm. 3.1\nLower bounds:\nPAC learning Agnostic learning C proper improper proper improper References\nPOINTX k + log |C| k k + log |C| k Thm. 5.1, [5] THRESHX k + log |C| Thm. 5.1, [5, 22]\nPARd (uniform) k log |C| Thm. 5.4\nMulti-learning with approximate differential privacy.\nUpper bounds:\nPAC learning Agnostic learning C (proper and improper) (proper and improper)\nPOINTX 1 (Thm. 3.13) \u221a k (Cor. 3.9)\nTHRESHX 2 log\u2217 |X| + \u221a k (Cor. 3.10) General C min{ \u221a k log |C|, \u221a kVC(C) + log |X|VC(C), \u221a kVC(C) + \u221a log |X| log |C|} (Thm. 3.1)\nPARd (uniform) log |C| (Thm. 3.12) \u221a k log |C| (Thm. 3.1)\nLower bounds:\nPAC learning Agnostic learning C proper improper proper improper References\nPOINTX 1 \u221a k Cor. 4.10\nTHRESHX log \u2217 |X|+ k1/3 k1/3 log\u2217 |X|+ \u221a k\n\u221a k Cor. 4.6, Cor. 4.10, [13]\nPARd (uniform) log |C| \u221a k + log |C| Cor. 4.10)"}, {"heading": "2 Preliminaries", "text": "We recall and extend standard definitions from learning theory and differential privacy."}, {"heading": "2.1 Multi-learners", "text": "In the followingX is some arbitrary domain. A concept (similarly, hypothesis) over domainX is a predicate defined over X . A concept class (similarly, hypothesis class) is a set of concepts.\nDefinition 2.1 (Generalization Error). Let P \u2208 \u2206(X \u00d7 {0, 1}) be a probability distribution over X \u00d7 {0, 1}. The generalization error of a hypothesis h : X \u2192 {0, 1} w.r.t. P is defined as errorP(h) = Pr(x,y)\u223cP [h(x) 6= y].\nLet D \u2208 \u2206(X) be a probability distribution over X and let c : x \u2192 {0, 1} be a concept. The generalization error of hypothesis h : X \u2192 {0, 1} w.r.t. c andD is defined as errorD(c, h) = Prx\u223cD[h(x) 6= c(x)]. If errorD(c, h) \u2264 \u03b1 we say that h is \u03b1-good for c and D.\nDefinition 2.2 (Multi-labeled database). A k-labeled database over a domain X is a database S \u2208 (X \u00d7 {0, 1}k)\u2217. That is, S contains |S| elements from X , each concatenated with k binary labels.\nLet A : ( X \u00d7 {0, 1}k )n \u2192 (2X)k be an algorithm that operates on a k-labeled database and returns k hypotheses. Let C be a concept class over a domainX and letH be a hypothesis class overX . We now give a generalization of the notion of PAC learning [34] to multi-labeled databases (the standard PAC definition is obtained by setting k = 1):\nDefinition 2.3 (PAC Multi-Learner). Algorithm A is an (\u03b1, \u03b2)-PAC k-learner for concept class C using hypothesis class H with sample complexity n if for every distribution D over X and for every fixture of (c1, . . . , ck) from C, given a k-labeled database as an input S = ((xi, c1(xi), . . . , ck(xi))) n i=1 where each xi is drawn i.i.d. from D, algorithm A outputs k hypotheses (h1, . . . , hk) from H satisfying\nPr [ max 1\u2264j\u2264k (errorD(cj , hj)) > \u03b1 ] \u2264 \u03b2.\nThe probability is taken over the random choice of the examples in S according to D and the coin tosses of the learner A. If H \u2286 C then A is called a proper learner; otherwise, it is called an improper learner.\nDefinition 2.4 (Agnostic PAC Multi-Learner). AlgorithmA is an (\u03b1, \u03b2)-PAC agnostic k-learner forC using hypothesis class H and sample complexity n if for every distribution P over X \u00d7{0, 1}k, given a k-labeled database S = ((xi, y1,i, . . . , yk,i)) n i=1 where each k-labeled sample (xi, y1,i . . . , yk,i) is drawn i.i.d. from P , algorithm A outputs k hypotheses (h1, . . . , hk) from H satisfying\nPr [ max 1\u2264j\u2264k ( errorPj (hj)\u2212min c\u2208C ( errorPj (c) )) > \u03b1 ] \u2264 \u03b2,\nwhere Pj is the marginal distribution of P on the examples and the jth label. The probability is taken over the random choice of the examples in S according to P and the coin tosses of the learner A. If H \u2286 C then A is called a proper learner; otherwise, it is called an improper learner."}, {"heading": "2.2 The Sample Complexity of Multi-Learning", "text": "Without privacy considerations, the sample complexities of PAC and agnostic learning are essentially characterized by a combinatorial quantity called the Vapnik-Chervonenkis (VC) dimension. We state these characterizations in the context of multi-learning."}, {"heading": "2.2.1 The Vapnik-Chervonenkis Dimension", "text": "Definition 2.5. Fix a concept classC over domainX . A set {x1, . . . , xd} \u2208 X is shattered byC if for every labeling b \u2208 {0, 1}d, there exists c \u2208 C such that b1 = c(x1), . . . , bd = c(xd). The Vapnik-Chervonenkis (VC) dimension of C, denoted VC(C), is the size of the largest set which is shattered by C.\nThe Vapnik-Chervonenkis (VC) dimension is an important combinatorial measure of a concept class. Classical results in statistical learning theory show that the generalization error of a hypothesis h and its empirical error (observed on a large enough sample) are similar.\nDefinition 2.6 (Empirical Error). Let S = ((xi, yi))ni=1 \u2208 (X \u00d7 {0, 1})n be a labeled sample from X . The empirical error of a hypothesis h : X \u2192 {0, 1} w.r.t. S is defined as errorS(h) = 1n |{i : h(xi) 6= yi}|.\nLet D \u2208 Xn be a (unlabeled) sample from X and let c : x\u2192 {0, 1} be a concept. The empirical error of hypothesis h : X \u2192 {0, 1} w.r.t. c and D is defined as errorD(c, h) = 1n |{i : h(xi) 6= c(xi)].\nTheorem 2.7 (VC-Dimension Generalization Bound, e.g. [11]). LetD andC be, respectively, a distribution and a concept class over a domain X , and let c \u2208 C. For a sample S = ((xi, c(xi)))ni=1 where n \u2265 64 \u03b1 (VC(C) ln( 64 \u03b1 ) + ln( 8 \u03b2 )) and the xi are drawn i.i.d. from D, it holds that\nPr [ \u2203h \u2208 C s.t. errorD(h, c) > \u03b1 \u2227 errorS(h) \u2264 \u03b1\n2\n] \u2264 \u03b2.\nThis generalization argument extends to the setting of agnostic learning, where a hypothesis with small empirical error might not exist.\nTheorem 2.8 (VC-Dimension Agnostic Generalization Bound, e.g. [1, 2]). Let H be a concept class over a domain X , and let P be a distribution over X \u00d7 {0, 1}. For a sample S = ((xi, yi))ni=1 containing n \u2265 64\n\u03b12 (VC(H) ln( 6\u03b1) + ln( 8 \u03b2 )) i.i.d. elements from P , it holds that\nPr [ \u2203h \u2208 H s.t. \u2223\u2223errorP(h)\u2212 errorS(h)\u2223\u2223 > \u03b1] \u2264 \u03b2. Using theorems 2.11 and 2.8, an upper bound of O(VC(C)) on the sample complexity of learning a concept class C follows by reduction to the empirical learning problem. The goal of empirical learning is similar to that of PAC learning, except accuracy is measured only with respect to a fixed input database. Theorems 2.11 and 2.8 state that when an empirical learner is run on sufficiently many samples, it is also accurate with respect to a distribution on inputs.\nDefinition 2.9 (Empirical Learner). Algorithm A is an (\u03b1, \u03b2)-accurate empirical k-learner for a concept class C using hypothesis class H with sample complexity n if for every collection of concepts (c1, . . . , ck)\nfromC and database S = ((xi, c1(xi), . . . , ck(xi)))ni=1 \u2208 (X\u00d7{0, 1}k)n, algorithmA outputs k hypotheses (h1, . . . , hk) from H satisfying\nPr [ max 1\u2264j\u2264k ( errorS|j (hj) ) > \u03b1 ] \u2264 \u03b2,\nwhere S|j = ((xi, cj(xi)))ni=1. The probability is taken over the coin tosses of A.\nDefinition 2.10 (Agnostic Empirical Learner). Algorithm A is an agnostic (\u03b1, \u03b2)-accurate empirical klearner for a concept class C using hypothesis class H with sample complexity n if for every database S = ((xi, y1,i, . . . , yk,i)) n i=1 \u2208 (X \u00d7 {0, 1}k)n, algorithm A outputs k hypotheses (h1, . . . , hk) from H satisfying\nPr [ max 1\u2264j\u2264k ( errorS|j (hj)\u2212minc\u2208C ( errorS|j (c) )) > \u03b1 ] \u2264 \u03b2,\nwhere S|j = ((xi, yj,i))ni=1. The probability is taken over the coin tosses of A.\nTheorem 2.11. Let A be an (\u03b1, \u03b2)-accurate empirical k-learner for a concept class C (resp. agnostic empirical k-learner) using hypothesis classH . ThenA is also a (2\u03b1, \u03b2+\u03b2\u2032)-accurate PAC learner forC when given at least max{n, 32\u03b1 (VC(H\u2295C) log(32/\u03b1)+log(8/\u03b2 \u2032))} samples (resp. max{n, 64 \u03b12\n(VC(H) log(6/\u03b1)+ log(8k/\u03b2\u2032)) samples). Here, H \u2295 C = {h\u2295 c : h \u2208 H, c \u2208 C}.\nProof. We begin with the non-agnostic case. Let A be an (\u03b1, \u03b2)-accurate empirical k-learner for C. Let D be a distribution over the example space X . Let S be a random i.i.d. sample of size m from D. The generalization bound for PAC learning (Theorem 2.7) states that if m \u2265 32\u03b1 (d log(32/\u03b1) + log(8/\u03b2\n\u2032))}, then\nPr[\u2203c \u2208 C, h \u2208 H : errorS(c, h) \u2264 \u03b1 \u2227 errorD(c, h) > 2\u03b1] \u2264 \u03b2\u2032,\nwhere d = VC(H \u2295 C). The result follows by a union bound over the failure probability of A and the failure of generalization.\nNow we turn to the agnostic case. Let A be an agnostic (\u03b1, \u03b2)-accurate empirical k-learner for C. Fix an index j \u2208 [k], and let Pj be a distribution over X \u00d7 {0, 1}. Let S be a random i.i.d. sample of size m from Pj . Then generalization for agnostic learning (Theorem 2.8) yields\nPr[\u2203h \u2208 H : |errorS(h)\u2212 errorPj (h)| > \u03b1] \u2264 \u03b2\u2032\nk\nfor m \u2265 64 \u03b12 (VC(H) log(6/\u03b1) + log(8k/\u03b2\u2032))}.The result follows by a union bound over the failure probability of A and the failure of generalization for each of the indices j = 1, . . . , k.\nApplying the above theorem in the special case where A finds the concept c \u2208 C that minimizes the empirical error on its given sample, we obtain the following sample complexity upper bound for proper multi-learning.\nCorollary 2.12. Let C be a concept class with VC dimension d. There exists an (\u03b1, \u03b2)-accurate proper PAC k-learner forC usingO( 1\u03b1(d log(1/\u03b1)+log(1/\u03b2)) samples. Moreover, there exists an (\u03b1, \u03b2)-accurate proper agnostic PAC k-learner for C using O( 1\n\u03b12 (d log(1/\u03b1) + log(k/\u03b2)) samples.\nProof. For the non-agnostic case, we simply let A be the (0, 0)-accurate empirical learner that outputs any vector of hypotheses that is consistent with its given examples (one is guaranteed to exist, since the target concept satisfies this condition). The claim follows from Theorem 2.11 noting that VC(C \u2295 C) = O(VC(C)).\nFor the agnostic case, consider the algorithm A that on input S outputs hypotheses (h1, . . . , hk) that minimize the quantities errorSj (hj). Applying the agnostic generalization bound [1], this is an (\u03b1/2, \u03b2/2)accurate agnostic empirical learner given O( 1\n\u03b12 (d log(1/\u03b1) + log(k/\u03b2)) samples. The claim then follows\nfrom Theorem 2.11.\nIt is known that even for k = 1, the sample complexities of PAC and agnostic learning are at least \u2126(VC(C)/\u03b1) and \u2126(VC(C)/\u03b12), respectively. Therefore, the above sample complexity upper bound is tight up to logarithmic factors.\nWe define a few specific concept classes which will play an important role in this work.\nPOINTX : Let X be any domain. The class of point functions is the set of all concepts that evaluate to 1 on exactly one element of X , i.e. POINTX = {cx : x \u2208 X} where cx(y) = 1 iff y = x. The VC-dimension of POINTX is 1 for any X .\nTHRESHX : LetX be any totally ordered domain. The class of threshold functions takes the form THRESHX = {cx : x \u2208 X} where cx(y) = 1 iff y \u2264 x. The VC-dimension of THRESHX is 1 for any X .\nPARd: Let X = {0, 1}d. The class of parity functions on X is given by PARd = {cx : x \u2208 X} where cx(y) = \u3008x, y\u3009 (mod 2). The VC-dimension of PARd is d.\nIn this work, we focus our study of the concept class PARd on the problem of learning parities under the uniform distribution. The PAC and agnostic learning problems are defined as before, except we only require a learner to be accurate when the marginal distribution on examples is the uniform distribution Ud over {0, 1}d.\nDefinition 2.13 (PAC Learning PARd under Uniform). Algorithm A is an (\u03b1, \u03b2)-PAC k-learner for PARd using hypothesis class H and sample complexity n if for every fixed (c1, . . . , ck) from C, given a k-labeled database as an input S = ((xi, c1(xi), . . . , ck(xi))) n i=1 where each xi is drawn i.i.d. from Ud, algorithm A outputs k hypotheses (h1, . . . , hk) from H satisfying\nPr [ max 1\u2264j\u2264k (errorUd(cj , hj)) > \u03b1 ] \u2264 \u03b2.\nDefinition 2.14 (Agnostically Learning PARd under Uniform). Algorithm A is an (\u03b1, \u03b2)-PAC agnostic k-learner for PARd using hypothesis class H and sample complexity n if for every distribution P over {0, 1}d \u00d7 {0, 1}k, with marginal distribution Ud over the data universe {0, 1}d, given a k-labeled database S = ((xi, y1,i, . . . , yk,i)) n i=1 where each k-labeled sample (xi, y1,i . . . , yk,i) is drawn i.i.d. from P , algorithm A outputs k hypotheses (h1, . . . , hk) from H satisfying\nPr [ max 1\u2264j\u2264k ( errorPj (hj)\u2212min c\u2208C ( errorPj (c) )) > \u03b1 ] \u2264 \u03b2,\nwhere Pj is the marginal distribution of P on the examples and the jth label."}, {"heading": "2.3 Differential privacy", "text": "Two k-labeled databases S, S\u2032 \u2208 (X \u00d7 {0, 1}k)n are called neighboring if they differ on a single (multilabeled) entry, i.e., |{i : (xi, y1,i, . . . , yk,i) 6= (x\u2032i, y\u20321,i, . . . , y\u2032k,i)}| = 1.\nDefinition 2.15 (Differential Privacy [18]). LetA : ( X \u00d7 {0, 1}k )n \u2192 (2X)k be an algorithm that operates on a k-labeled database and returns k hypotheses. Let , \u03b4 \u2265 0. Algorithm A is ( , \u03b4)-differentially private if for all neighboring S, S\u2032 and for all T \u2286 ( 2X )k,\nPr[A(S) \u2208 T ] \u2264 e \u00b7 Pr[A(S\u2032) \u2208 T ] + \u03b4,\nwhere the probability is taken over the coin tosses of the algorithm A. When \u03b4 = 0 we say that A satisfies pure differential privacy, otherwise (i.e., if \u03b4 > 0) we say that A satisfies approximate differential privacy.\nOur learning algorithms are designed via repeated applications of differentially private algorithms on a database. Composition theorems for differential privacy show that the price of privacy for multiple (adaptively chosen) interactions degrades gracefully.\nTheorem 2.16 (Composition of Differential Privacy [16, 17, 19]). Let 0 < , \u03b4\u2032 < 1 and \u03b4 \u2208 [0, 1]. Suppose an algorithm A accesses its input database S only through m adaptively chosen executions of ( , \u03b4)-differentially private algorithms. Then A is\n1. (m ,m\u03b4)-differentially private, and 2. ( \u2032,m\u03b4 + \u03b4\u2032)-differentially private for = \u221a 2m ln(1/\u03b4\u2032) \u00b7 + 2m 2."}, {"heading": "2.4 Differentially Private Sanitization", "text": "A fundamental task in differential privacy is the data sanitization problem. Given a databaseD = (x1, . . . , xn) \u2208 Xn, the goal of a sanitizer is to privately produce a synthetic database D\u0302 \u2208 Xm that captures the statistical properties of D. We are primarily interested in sanitization for boolean-valued functions (equivalently referred to as counting queries). Given a function c : X \u2192 {0, 1} and a database D = (x1, . . . , xn), we write c(D) = 1n \u2211n i=1 c(xi).\nDefinition 2.17 (Sanitization). An algorithm A : Xn \u2192 Xm is an (\u03b1, \u03b2)-accurate sanitizer for a concept class C if for every D \u2208 Xn, the algorithm A produces a database D\u0302 \u2208 Xm such that\nPr[\u2203c \u2208 C : |c(D)\u2212 c(D\u0302)| > \u03b1] \u2264 \u03b2.\nHere, the probability is taken over the coins of A.\nIn an influential result, Blum, Ligett, and Roth [10] showed that any concept class C admits a differentially private sanitizer with sample complexity O(VC(C) log |X|):\nTheorem 2.18 ([10]). For any concept class C over a domain X , there exists an (\u03b1, \u03b2)-accurate and ( , 0)- differentially private sanitizer A : Xn \u2192 Xm for C when\nn = O\n( VC(C) \u00b7 log |X| \u00b7 log(1/\u03b1)\n\u03b13 +\nlog(1/\u03b2)\n\u03b1\n) ,\nand m = O(VC(C) log(1/\u03b1)/\u03b12).\nWhen relaxing to ( , \u03b4)-differential privacy, the private multiplicative weights algorithm of Hardt and Rothblum [24] can sometimes achieve lower sample complexity (roughly O(log |C| \u221a log |X|)).\nTheorem 2.19 ([24]). For any concept class C over a domain X , there exists an (\u03b1, \u03b2)-accurate and ( , \u03b4)differentially private sanitizer A : Xn \u2192 Xm for C when\nn = O\n( (log |C|+ log(1/\u03b2)) \u00b7 \u221a log |X| \u00b7 log(1/\u03b4)\n\u03b12\n) ,\nand m = O(VC(C) log(1/\u03b1)/\u03b12).\nHowever, for specific concept classes, sanitizers are known to exist with much lower sample complexity. We first give a sanitizer for point functions with essentially optimal sample complexity, which improves and simplifies a result of [7].\nProposition 2.20. There exists an (\u03b1, \u03b2)-accurate and ( , \u03b4)-differentially private sanitizer for POINTX with sample complexity\nn = O\n( log(1/\u03b1\u03b2\u03b4)\n\u03b1\n) .\nProof. To give a (2\u03b1, \u03b2)-accurate sanitizer, it suffices to produces, for each point function cx, an approximate answer ax \u2208 [0, 1] with |ax \u2212 cx| \u2264 \u03b1. This is because given these approximate answers, one can reconstruct a database D\u0302 of size O(1/\u03b1) with |cx(D\u0302)\u2212 ax| \u2264 \u03b1 for every x \u2208 X .\nThe algorithm for producing the answers ax is as follows.\nAlgorithm 1 Query release for POINTX Input: Privacy parameters ( , \u03b4), database D \u2208 Xn For each x \u2208 X , do the following:\n1. If cx(D) \u2264 \u03b14 , release ax = 0\n2. Let a\u0302x = cx(D) + Lap(2/ n)\n3. If a\u0302x \u2264 \u03b12 , release ax = 0\n4. Otherwise, release ax = a\u0302x\nFirst, we argue that Algorithm 1 is ( , \u03b4)-differentially private. Below, we write X \u2248( ,\u03b4) Y to denote the fact that for every measurable set S in the union of the supports of X and Y , we have Pr[X \u2208 S] \u2264 e Pr[Y \u2208 S] + \u03b4.\nLet D \u223c D\u2032 be adjacent databases of size n, with x \u2208 D replaced by x\u2032 \u2208 D\u2032. Then the output distribution of the mechanism differs only on its answers to the queries cx and cx\u2032 . Let us focus on cx. If both cx(D) \u2264 \u03b1/4 and cx(D\u2032) \u2264 \u03b1/4, then the mechanism always releases 0 for both queries. If both cx(D) > \u03b1/4 and cx(D\u2032) > \u03b1/4, then ax(D) \u2248( /2,0) ax(D\u2032) by properties of the Laplace mechanism. Finally, if cx(D) > \u03b1/4 but cx(D\u2032) \u2264 \u03b1/4, then cx(D\u2032) = 0 with probability 1. Moreover, we must have POINTx(D) \u2264 \u03b1/4 + 1/n, so\nPr[ax(D) = 0] \u2265 Pr[Lap(2/ n) < \u03b1/4\u2212 1/n] = 1\u2212 1\n2 exp(\u2212 n\u03b1/8 + /2) \u2265 1\u2212 \u03b4/2.\nSo in this case, ax(D) \u2248(0,\u03b4/2) ax(D\u2032). Therefore, we conclude that overall ax(D) \u2248( /2,\u03b4/2) ax(D\u2032). An identical argument holds for ax\u2032 , so the mechanism is ( , \u03b4)-differentially private.\nNow we argue that the answers ax are accurate. First, the answers are trivially \u03b1-accurate for all queries cx on which cx(D) \u2264 \u03b1/4. For each of the remaining queries, it is \u03b1-accurate with probability at least\nPr[|Lap(2/ n)| < \u03b1/2] = 1\u2212 exp(\u2212 n\u03b1/4) \u2265 1\u2212 \u03b1\u03b2 4 .\nTaking a union bound over the at most 4/\u03b1 queries with POINTx(D) > \u03b1/4, we conclude that the mechanism is \u03b1-accurate for all queries with probability at least 1\u2212 \u03b2.\nBun et al. [13], improving on work of Beimel et al. [7], gave a sanitizer for threshold functions with sample complexity roughly 2log \u2217 |X|.\nProposition 2.21 ([13]). There exists an (\u03b1, \u03b2)-accurate and ( , \u03b4)-differentially private sanitizer for THRESHX with sample complexity\nn = O\n( 1\n\u03b1 \u00b7 2log\n\u2217 |X| \u00b7 log\u2217 |X| \u00b7 log (\nlog\u2217 |X| \u03b4\n) \u00b7 log(1/\u03b2) \u00b7 log2.5(1/\u03b1) ) ."}, {"heading": "2.5 Private learners and multi-learners", "text": "Generalizing on the concept of private learners [26], we say that an algorithm A is (\u03b1, \u03b2, , \u03b4)-private PAC k-learner for C using H if A is (\u03b1, \u03b2)-PAC k-learner for C using H , and A is ( , \u03b4)-differentially private (similarly with agnostic private PAC k-learners). We omit the parameter k when k = 1 and the parameter \u03b4 when \u03b4 = 0.\nFor the case k = 1, we have a generic construction with sample complexity proportional to log |C|:\nTheorem 2.22 ([26]). Let C be a concept class, and \u03b1, \u03b2, > 0. There exists an (\u03b1, \u03b2, )-private agnostic proper learner for C with sample complexity O ( (log |C|+ log 1/\u03b2)(1/( \u03b1) + 1/\u03b12) ) .\nBeimel, Nissim, and Stemmer [8] gave a generic transformation from data sanitization to private learning, which generally gives improved sample complexity upper bounds.\nTheorem 2.23 ([8]). Suppose there exists an (\u03b1, \u03b2)-accurate and ( , \u03b4)-differentially private sanitizer for C\u2295 with sample complexitym. Then there exists a proper (2\u03b1, 2\u03b2)-PAC and ( + \u2032, \u03b4)-differentially private learner for C with sample complexity\nO ( m+ VC(C)\n\u03b13 \u2032 log\n( 1\n\u03b1\n) + 1\n\u03b1 \u2032 log\n( 1\n\u03b2\n)) .\nA number of works [5, 6, 7, 22, 13] have established sharper upper and lower bounds for learning the specific concept classes POINTX and THRESHX . In the case of pure differential privacy, POINTX requires \u0398(log |X|) samples to learn properly [5], but can be learned improperly with O(1) samples. On the other hand, the class of threshold functions THRESHX require \u2126(log |X|) samples to learn, even improperly [22]. In the case of approximate differential privacy, POINTx and THRESHx can be learned properly with sample complexities O(1) [7] and O\u0303(2log\n\u2217 |X|) [13], respectively. Moreover, properly learning threshold functions requires sample complexity \u2126(log\u2217 |X|)."}, {"heading": "2.6 Private PAC learning vs. Empirical Learning", "text": "We saw by Theorem 2.11 that when an empirical k-learner A for a concept class C is run on a random sample of size \u2126(VC(C)), it is also a (agnostic) PAC k-learner. In particular, if an empirical k-learner A is differentially private, then it also serves as a differentially private (agnostic) PAC k-learner.\nGeneralizing a result of [13], the next theorem shows that the converse is true as well: a differentially private (agnostic) PAC k-learner yields a private empirical k-learner with only a constant factor increase in the sample complexity.\nTheorem 2.24. Let \u2264 1. Suppose A is an ( , \u03b4)-differentially private (\u03b1, \u03b2)-accurate (agnostic) PAC k-learner for a concept class C with sample complexity n. Then there is an ( , \u03b4)-differentially private (\u03b1, \u03b2)-accurate (agnostic) empirical k-learner A\u0303 for C with sample complexity m = 9n. Moreover, if A is proper, then so is the resulting empirical learner A\u0303.\nProof. We give the proof for the agnostic case; the non-agnostic case is argued identically, and is immediate from [13]. To construct the empirical learner A\u0303, we use the fact that the given learner A performs well on any distribution over labeled examples \u2013 in particular, it performs well on the uniform distribution over rows of the input database to A\u0303. Consider a database S = ((xi, y1,i, . . . , yk,i))mi=1 \u2208 (X \u00d7 {0, 1}k)m. On input S, define A\u0303 by sampling n rows from S (with replacement), and outputting the result of running A on the sample. Let S denote the uniform distribution over the rows of S, and let Sj be its marginal distribution which is uniform over S|j = ((xi, yj,i))mi=1. Then sampling n rows from S is equivalent to sampling n rows i.i.d. from S. Hence, if (h1, . . . , hk) is the output of A on the subsample, we have\nPr [ max 1\u2264j\u2264k ( errorS|j (hj)\u2212minc\u2208C ( errorSj (c) )) > \u03b1 ] = Pr [ max 1\u2264j\u2264k ( errorSj (hj)\u2212min c\u2208C ( errorSj (c) )) > \u03b1 ] \u2264 \u03b2.\nTo show that A\u0303 remains ( , \u03b4)-differentially private, we apply the following \u201csecrecy-of-the-sample\u201d lemma [26, 13], which shows that the sampling procedure does not hurt privacy.\nLemma 2.25. Fix \u2264 1 and let A be an ( , \u03b4)-differentially private algorithm with sample complexity n. For m \u2265 2n, the algorithm A\u0303 described above is (\u0303, \u03b4\u0303) for\n\u0303 = 6 m\nn and \u03b4\u0303 = 4 exp\n( 6 m\nn ) \u00b7 m n \u00b7 \u03b4."}, {"heading": "3 Upper Bounds on the Sample Complexity of Private Multi-Learners", "text": ""}, {"heading": "3.1 Generic Construction", "text": "In this section we present the following general upper bounds on the sample complexity of private k-learners.\nTheorem 3.1. Let C be a finite concept class, and let k \u2265 1. There exists a proper agnostic (\u03b1, \u03b2, )-private PAC k-learner for C with sample complexity\nO\u03b1,\u03b2,\n( k \u00b7 log k + min { k \u00b7 log |C| , (k + log |X|) \u00b7VC(C) }) ,\nand there exists a proper agnostic (\u03b1, \u03b2, , \u03b4)-private PAC k-learner for C with sample complexity\nO\u03b1,\u03b2, ,\u03b4\n(\u221a k \u00b7 log k + min {\u221a k \u00b7 log |C| , ( \u221a k + log |X|) \u00b7VC(C) , \u221a k \u00b7VC(C) + \u221a log |X| \u00b7 log |C| }) .\nThe straightforward approach for constructing a private k-learner for a class C is to separately apply a (standard) private learner for C for each of the k target concepts. Using composition theorem 2.16 to argue the overall privacy guarantee of the resulting learner, we get the following observation.\nObservation 3.2. Let C be a concept class and let k \u2265 1. If there is an (\u03b1, \u03b2, , \u03b4)-PAC learner for C with sample complexity n, then \u2022 There is an (\u03b1, k\u03b2, k , k\u03b4)-PAC k-learner for C with sample complexity n. \u2022 There is an (\u03b1, k\u03b2,O( \u221a k log(1\u03b4 ) + k 2), O(k\u03b4))-PAC k-learner for C with sample complexity n. Moreover, if the initial learner is proper and/or agnostic, then so is the resulting learner.\nIn cases where sample efficient private PAC learners exist, it might be useful to apply Observation 3.2 in order to obtain a private k-learner. For example, Beimel et al. [5, 6] gave an improper agnostic (\u03b1, \u03b2, )- PAC learner for POINTX with sample complexity O\u03b1(1 log 1 \u03b2 ). Using Observation 3.2 yields the following corollary.\nCorollary 3.3. There exists an improper agnostic (\u03b1, \u03b2, )-PAC k-learner for POINTX with sample complexity O\u03b1,\u03b2, (k log k).\nFor a general concept class C, we can use Observation 3.2 with the generic construction of Theorem 2.22, stating that for every concept class C there exists a private agnostic proper learner A that uses O(log |C|) labeled examples.\nCorollary 3.4. Let C be a concept class, and \u03b1, \u03b2, > 0. There exists an (\u03b1, \u03b2, )-private agnostic proper k-learner for C with sample complexity O\u03b1,\u03b2, (k \u00b7 log |C|+ k \u00b7 log k). Moreover, there exists an (\u03b1, \u03b2, , \u03b4)private agnostic proper k-learner for C with sample complexity O\u03b1,\u03b2, ,\u03b4( \u221a k \u00b7 log |C|+ \u221a k \u00b7 log k).\nExample 3.5. There exists a proper agnostic (\u03b1, \u03b2, )-PAC k-learner for PARd with sample complexity O\u03b1,\u03b2, (kd+ k log k).\nAs we will see in Section 5, the bounds of Corollary 3.3 and Example 3.5 on the sample complexity of k-learning POINTX and PARd are tight (up to logarithmic factors). That is, with pure-differential privacy, the direct sum gives (roughly) optimal bounds for improperly learning POINTX , and for (properly or improperly) learning PARd. This is not the case for learning THRESHX or for properly learning learning POINTX .\nIn order to avoid the factor k log |C| (or \u221a k log |C|) in Corollary 3.4, we now show how an idea used in [8] (in the context of semi-supervised learning) can be used to construct sample efficient private k-learners. In particular, this construction will achieve tight bounds for learning THRESHX and for properly learning learning POINTX under pure-differential privacy.\nFix a concept class C, target concepts c1, . . . , ck \u2208 C, and a k-labeled database S (we use D to denote the unlabeled portion of S). For every 1 \u2264 j \u2264 k, the goal is to identify a hypothesis hj \u2208 C with low errorD(cj , hj) (such a hypothesis also has good generalization). Beimel et al. [8] observed that given a sanitization D\u0302 of D w.r.t. C\u2295 = {f\u2295g : f, g \u2208 C}, for every f, g \u2208 C it holds that\nerrorD(f, g) = 1 |D| |{x \u2208 D : (f \u2295 g)(x) = 1}| \u2248 1 |D\u0302| |{x \u2208 D\u0302 : (f \u2295 g)(x) = 1}| = errorD\u0302(f, g).\nHence, a hypothesis h with low errorD\u0302(h, cj) also has low errorD(h, cj) and vice versa. Let H be the set of all dichotomies over D\u0302 realized by C. Note that \u2203f\u2217j \u2208 H that agrees with cj on D\u0302, i.e., \u2203f\u2217j \u2208 H s.t. errorD\u0302(f \u2217 j , cj) = 0, and hence errorD(f \u2217 j , cj) is also low. The thing that works in our favor here is that H is small \u2013 at most 2|D\u0302| \u2264 2VC(C) \u2013 and hence choosing a hypothesis out of H is easy. Therefore, for every j we can use the exponential mechanism to identify a hypothesis hj \u2208 H with low errorD(hj , cj).\nLemma 3.6. Let C be a concept class, and \u03b1, \u03b2, , \u03b4 > 0. There exists an (\u03b1, \u03b2, )-private agnostic k-learner for C with sample complexity O\u03b1,\u03b2, (VC(C) \u00b7 log |X| + k \u00b7 VC(C) + k \u00b7 log k). Moreover, there exists an (\u03b1, \u03b2, , \u03b4)-private agnostic k-learner for C with sample complexity O\u03b1,\u03b2, ,\u03b4(min{VC(C) \u00b7 log |X|, log |C| \u00b7 \u221a log |X|}+ \u221a k \u00b7VC(C) + \u221a k \u00b7 log k).\nLemma 3.6 follows from the following lemma.\nLemma 3.7. Let \u2032 > 0 and let A be an (\u03b15 , \u03b2 5 )-accurate ( , \u03b4)-private sanitizer for C \u2295 with sample complexity m. Then there is an (\u03b1, \u03b2)-PAC agnostic k-learner for C with sample complexity\nO ( m+ VC(C)\n\u03b13 \u2032 log(\n1 \u03b1 ) + 1 \u03b1 \u2032 log( k \u03b2 ) + 1 \u03b12 VC(C) log( k \u03b1\u03b2 )\n) .\nMoreover, it is both ( + k \u2032, \u03b4) and ( + \u221a 2k ln(1/\u03b4) \u2032 + 2k \u20322, 2\u03b4)-differentially private.\nUsing Lemma 3.7 with the generic sanitizer of Theorem 2.18 or Theorem 2.19 results in Lemma 3.6.\nAn important building block of our generic learner is the exponential mechanism of McSherry and Talwar [27]. A quality function q : X\u2217 \u00d7F \u2192 N defines an optimization problem over the domain X and a finite solution set F : Given a database S \u2208 X\u2217, choose f \u2208 F that (approximately) maximizes q(S, f). The exponential mechanism solves such an optimization problem sampling a random f \u2208 F with probability \u221d exp ( \u00b7 q(S, f)/2\u2206q). Here, the sensitivity of a quality function, \u2206q, is the maximum over all f \u2208 F of the sensitivity of the function q(\u00b7, f).\nProposition 3.8 (Properties of the Exponential Mechanism [27]).\n1. The exponential mechanism is ( , 0)-differentially private.\n2. Let q be a quality function with sensitivity at most 1. Fix a database S \u2208 Xn and let OPT = maxf\u2208F{q(S, f)}. Let t > 0. Then exponential mechanism outputs a solution f with q(S, f) \u2264 OPT\u2212tn with probability at most |F| \u00b7 exp(\u2212 tn/2).\nAlgorithm 2 GenericLearner Input: Concept class C, privacy parameters \u2032, , \u03b4, and a k-labeled database S = (xi, yi,1, . . . , yi,k)ni=1. We use D = (xi)ni=1 to denote the unlabeled portion of S. Used Algorithm: An (\u03b15 , \u03b2 5 )-accurate ( , \u03b4)-private sanitizer for C \u2295 with sample complexity m.\n1. Initialize H = \u2205. 2. Construct an ( , \u03b4)-private sanitization D\u0303 of D w.r.t. C\u2295, where |D\u0303| = O ( VC(C\u2295) \u03b12 log( 1\u03b1) ) =\nO ( VC(C) \u03b12 log( 1\u03b1) ) .\n3. Let B = {b1, . . . , b|B|} be the set of all points appearing at least once in D\u0303. 4. For every (z1, . . . , z|B|) \u2208 \u03a0C(B) = { ( c(b1), . . . , c(b|B|) ) : c \u2208 C}, add to H an arbitrary concept\nc \u2208 C s.t. c(b`) = z` for every 1 \u2264 ` \u2264 |B|. 5. For every 1 \u2264 j \u2264 k, use the exponential mechanism with privacy parameter \u2032 to choose and return\na hypothesis hj \u2208 H with (approximately) minimal error on the examples in S w.r.t. their jth label.\nProof of Lemma 3.7. The proof is via the construction ofGenericLearner (algorithm 2). Note thatGenericLearner only accesses S via a sanitizer (on Step 2) and using the exponential mechanism (on Step 5). Composition theorem 2.16 state thatGenericLearner is both ( +k \u2032, \u03b4)-differentially private and ( + \u221a 2k ln(1/\u03b4) \u2032+ 2k \u20322, 2\u03b4)-differentially private. We, thus, only need to prove that with high probability the learner returns \u03b1-good hypotheses.\nFix a distribution P over X \u00d7{0, 1}k, and let Pj denote the marginal distribution of P on the examples and the jth label. Let S consist of examples (xi, yi,1, . . . , yi,k) \u223c P . We use D = (xi)ni=1 to denote the unlabeled portion of S, and use S|j = ((xi, yj,i))ni=1 to denote a database containing the examples in S together with their jth label. Define the following three events:\nE1 : For every f, h \u2208 C it holds that |errorD(f, h)\u2212 errorD\u0303(f, h)| \u2264 2\u03b1 5 .\nE2 : For every f \u2208 C and for every 1 \u2264 j \u2264 k it holds that |errorS|j (f)\u2212 errorPj (f)| \u2264 \u03b1 5 .\nE3 : For every 1 \u2264 j \u2264 k, the hypothesis hj chosen by the exponential mechanism is such that errorS|j (hj) \u2264 \u03b1 5 + minf\u2208H { errorS|j (f) } .\nWe first argue that when these three events happen algorithmGenericLearner returns good hypotheses. Fix 1 \u2264 j \u2264 k, and let c\u2217j = argminf\u2208C{errorPj (f)}. We denote \u2206 = errorPj (c\u2217j ). We need to show that if E1 \u2229E2 \u2229E3 occurs, then the hypothesis hj returned by GenericLearner is s.t. errorPj (hj) \u2264 \u03b1+ \u2206.\nFor every (y1, . . . , y|B|) \u2208 \u03a0C(B), algorithm GenericLearner adds to H a hypothesis f s.t. \u22001 \u2264 ` \u2264 |B|, f(b`) = y`. In particular, H contains a hypothesis h\u2217j s.t. h\u2217j (x) = c\u2217j (x) for every x \u2208 B, that is, a hypothesis h\u2217j s.t. errorD\u0303(h \u2217 j , c \u2217 j ) = 0. As event E1 has occurred we have that this h \u2217 j satisfies errorD(h\u2217j , c \u2217 j ) \u2264 2\u03b15 . Using the triangle inequality (and event E2) we get that this h \u2217 j satisfies errorS|j (h \u2217 j ) \u2264 errorD(h\u2217j , c\u2217j ) + errorS|j (c\u2217j ) \u2264 3\u03b1 5 + \u2206. Thus, event E3 ensures that algorithm GenericLearner chooses (using the exponential mechanism) a hypothesis hj \u2208 H s.t. errorS|j (hj) \u2264 4\u03b1 5 + \u2206. Event E2 ensures, therefore, that this hj satisfies errorPj (hj) \u2264 \u03b1 + \u2206. We will now show E1 \u2229 E2 \u2229 E3 happens with high probability.\nStandard arguments in learning theory state that (w.h.p.) the empirical error on a (large enough) random sample is close to the generalization error. Specifically, by setting n \u2265 O( 1\n\u03b12 VC(C) log( k\u03b1\u03b2 )), Theorem 2.8\nensures that Event E2 occurs with probability at least (1\u2212 25\u03b2). Assuming that n \u2265 m (the sample complexity of the sanitizer used in Step 5), with probability at least (1\u2212 \u03b25 ) for every (h\u2295 f) \u2208 C \u2295 (i.e., for every h, f \u2208 C) it holds that\n\u03b1 5 \u2265 |Q(h\u2295f)(D)\u2212Q(h\u2295f)(D\u0303)|\n= \u2223\u2223\u2223\u2223\u2223 |{x \u2208 D : (h\u2295f)(x)=1}||D| \u2212 |{x \u2208 D\u0303 : (h\u2295f)(x)=1}||D\u0303| \u2223\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u2223 |{x \u2208 D : h(x) 6=f(x)}||D| \u2212 |{x \u2208 D\u0303 : h(x)6=f(x)}||D\u0303| \u2223\u2223\u2223\u2223\u2223\n= \u2223\u2223errorD(h, f)\u2212 errorD\u0303(h, f)\u2223\u2223 .\nEvent E1 occurs therefore with probability at least (1\u2212 \u03b25 ). The exponential mechanism ensures that the probability of eventE3 is at least 1\u2212k|H|\u00b7exp(\u2212 \u2032\u03b1m/10)\n(see Proposition 3.8). Note that log |H| \u2264 |B| \u2264 |D\u0303| = O ( VC(C) \u03b12 log( 1\u03b1) ) . Therefore, for n \u2265\nO ( VC(C) \u03b13 \u2032 log( 1 \u03b1) + 1 \u03b1 \u2032 log( k \u03b2 ) ) , Event E3 occurs with probability at least (1\u2212 \u03b25 ).\nAll in all, setting n \u2265 O ( m+ VC(C)\n\u03b13 \u2032 log( 1 \u03b1) + 1 \u03b1 \u2032 log( k \u03b2 ) + 1 \u03b12 VC(C) log( k\u03b1\u03b2 ) )\n, ensures that the probability of GenericLearner failing is at most \u03b2.\nTheorem 3.1 now follows by combining Lemma 3.6 and Corollary 3.4.\nFor certain concept classes, there are sanitizers with substantially lower sample complexity than the generic sanitizers. Combining Lemma 3.6 with Proposition 2.20, we obtain:\nCorollary 3.9. There is an (\u03b1, \u03b2)-PAC agnostic k-learner for POINTX with sample complexity\nO\n( log(1/\u03b1\u03b2\u03b4)\n\u03b1 +\nlog(1/\u03b1)\n\u03b13 \u2032 +\nlog(k/\u03b2)\n\u03b1 \u2032 +\nlog(k/\u03b1\u03b2)\n\u03b12\n) .\nMoreover, it is both ( + k \u2032, \u03b4) and ( + \u221a 2k ln(1/\u03b4) \u2032 + 2k \u20322, 2\u03b4)-differentially private.\nSimilarly, combining Lemma 3.6 with Proposition 2.21, we obtain:\nCorollary 3.10. There is an (\u03b1, \u03b2)-PAC agnostic k-learner for THRESHX with sample complexity\nO\n2log\u2217 |X| \u00b7 log\u2217 |X| \u00b7 log ( log\u2217 |X| \u03b4 ) \u00b7 log(1/\u03b2) \u00b7 log2.5(1/\u03b1)\n\u03b1 +\nlog(1/\u03b1)\n\u03b13 \u2032 +\nlog(k/\u03b2)\n\u03b1 \u2032 +\nlog(k/\u03b1\u03b2)\n\u03b12  . Moreover, it is both ( + k \u2032, \u03b4) and ( + \u221a 2k ln(1/\u03b4) \u2032 + 2k \u20322, 2\u03b4)-differentially private."}, {"heading": "3.2 Upper Bounds for Approximate Private Multi-Learners", "text": "In this section we give two examples of cases where the sample complexity of private k-learning is of the same order as that of non-private k-learning (the sample complexity does not depend on k). Our algorithms are ( , \u03b4)-differentially private, and rely on stability arguments: the identity of the best k concepts, as an entire vector, is unlikely to change on nearby k-labeled databases. Hence, it can be released privately.\nThe main technical tool we use is the Adist algorithm of Smith and Thakurta [31]. Our discussion follows the treatment of [7].\nRecall that a quality function q : X\u2217 \u00d7 F \u2192 N defines an optimization problem over the domain X and a finite solution set F : Given a database S \u2208 X\u2217, find f \u2208 F that (approximately) maximizes q(S, f). The sensitivity of a quality function, \u2206q, is the maximum over all f \u2208 F of the sensitivity of the function q(\u00b7, f). The algorithm Adist privately identifies the exact maximizer as long as it is sufficiently stable.\nAlgorithm 3 Adist Input: Privacy parameters , \u03b4, database S \u2208 X\u2217, sensitivity-1 quality function q\n1. Let f1, f2 \u2208 F be the highest scoring and second-highest scoring solutions to q(S, \u00b7), respectively. 2. Let gap = q(S, f1)\u2212 q(S, f2), and g\u0302ap = gap + Lap(1/ ). 3. If g\u0302ap < 1 log\n1 \u03b4 , output \u22a5. Otherwise, output f1.\nProposition 3.11 (Properties of Adist [31]). 1. Algorithm Adist is ( , \u03b4)-differentially private.\n2. When run on a database S with gap > 1 log 1 \u03b4\u03b2 , Algorithm Adist outputs the highest scoring solution\nf1 with probability at least 1\u2212 \u03b2."}, {"heading": "3.2.1 Learning Parities under the Uniform Distribution", "text": "Theorem 3.12. For every k, d there exists an (\u03b1=0, \u03b2, , \u03b4)-PAC (non-agnostic) k-learner for PARd under the uniform distribution with sample complexity O(d log( 1\u03b2 ) + 1 log( 1 \u03b2\u03b4 )).\nRecall that (even without privacy constraints) the sample complexity of PAC learning PARd under the uniform distribution is \u2126(d). Hence the sample complexity of privately k-learning PARd (non-agnostically) under the uniform distribution is of the same order as that of non-private k-learning.\nFor the intuition behind Theorem 3.12, let c1, . . . , ck denote the k target concepts, and consider the quality function q(D, (h1, . . . , hk)) = max1\u2264j\u2264k{errorD(hj , cj)}. On a large enough sample D we expect that q(D, (h1, . . . , hk)) \u2248 12 for every (h1, . . . , hk) 6= (c1, . . . , ck), while q(D, (c1, . . . , ck)) = 0. The k target concepts can hence be privately identified (exactly) using stability techniques.\nIn order to make our algorithm computationally efficient, we apply the \u201csubsample and aggregate\u201d idea of Nissim et al. [28]. We divide the input sample into a small number of subsamples, use Gaussian elimination to (non-privately) identify a candidate hypothesis vector on each subsample, and then select from these candidates privately.\nAlgorithm 4 ParityLearner Input: Parameters , \u03b4, and a k-labeled database S of size n = O(d log(\n1 \u03b2\u03b4 )).\nOutput: Hypotheses h1, . . . , hk.\n1. Split S into m = O(1 log( 1 \u03b2\u03b4 )) disjoint samples S1, . . . , Sm of size O(d) each. Initiate Y as the\nempty multiset.\n2. For every 1 \u2264 t \u2264 m: (a) For every 1 \u2264 j \u2264 k try to use Gaussian elimination to identify a parity function yj that agrees\nwith the labels of the jth column of St.\n(b) If a parity is identified for every j, then set Y = Y \u222a {(y1, ..., yk)}. Otherwise set Y = Y \u222a {\u22a5}.\n3. Use algorithm Adist with privacy parameters , \u03b4 to choose and return a vector of k parity functions (h1, . . . , hk) \u2208 (PARd)k with a large number of appearances in Y .\nProof of Theorem 3.12. The proof is via the construction of ParityLearner (algorithm 4). First note that changing a single input element in S can change (at most) one element of Y . Hence, applying (the ( , \u03b4)private) algorithm Adist on Y preserves privacy (applying ParityLearner on neighboring inputs amounts to executing Adist on neighboring inputs).\nNow fix k target concepts c1, . . . , ck \u2208 PARd and let S be a random k-labeled database containing n i.i.d. elements from the uniform distribution Ud over X = {0, 1}d, each labeled by c1, . . . , ck. Observe that (for every 1 \u2264 t \u2264 m) we have that St contains i.i.d. elements from Ud labeled by c1, . . . , ck. We use Dt to denote the unlabeled portion of St. Standard arguments in learning theory (cf. Theorem 2.7) state that for |St| \u2265 O(d),\nPr [ \u2203h, f \u2208 PARd s.t. errorUd(h, f) \u2265 1\n4 \u2227 errorDt(h, f) \u2264\n1\n40\n] \u2264 1\n8 .\nThe above inequality holds, in particular, for every hypothesis h \u2208 PARd and every target concept cj , and hence,\nPr [ \u2203h \u2208 PARd and j s.t. errorUd(h, cj) \u2265 1\n4 \u2227 errorDt(h, cj) \u2264\n1\n40\n] \u2264 1\n8 .\nRecall that under the uniform distribution, the only h \u2208 PARd s.t. errorUd(h, cj) 6= 12 is cj itself, and hence\nPr [ \u2203h \u2208 PARd and j s.t. h 6= cj \u2227 errorDt(h, cj) \u2264 1\n40\n] \u2264 1\n8 .\nSo, for every 1 \u2264 t \u2264 m, with probability 7/8 we have that for every label column j the only hypothesis with empirical error less than 140 on St is the j\nth target concept itself (with empirical error 0). In such a case, step 2a (Gaussian elimination) identifies exactly the vector of k target concepts (c1, . . . , ck). Since m \u2265 O(log( 1\u03b2 )), the Chernoff bound ensures that except with probability \u03b2/2, the vector (c1, . . . , ck) is identified in at least 3/4 of the iterations of step 2. Assuming that this is the case, the vector (c1, . . . , ck) appears in Y at least 3m/4 times, while every other vector can appear at most m/4 times. Provided that m \u2265 O(1 log(\n1 \u03b2\u03b4 )), algorithm Adist ensures that the k target concepts are chosen with probability 1\u2212 \u03b2/2.\nAll in all, algorithm ParityLearner identifies the k target concepts (exactly) with probability 1 \u2212 \u03b2, provided that n \u2265 O(d log(\n1 \u03b2\u03b4 ))."}, {"heading": "3.2.2 Learning Points", "text": "We next show that the class of POINTX can be (non-agnostically) k-learned using constant sample complexity, matching the non-private sample complexity.\nTheorem 3.13. For every domain X and every k \u2208 N there exists an (\u03b1, \u03b2, , \u03b4)-PAC (non-agnostic) klearner for POINTX with sample complexity O( 1\u03b1 log( 1 \u03b1\u03b2\u03b4 )).\nThe proof is via the construction of Algorithm 5. The algorithm begins by privately identifying (using sanitization) a set of O(1/\u03b1) \u201cheavy\u201d elements in the input database, appearing \u2126(\u03b1) times. The k labels of such a heavy element can be privately identified using stability arguments (since their duplicity in the database is large). The labels of a \u201cnon-heavy\u201d element can be set to 0 since a target concept can evaluate to 1 on at most one such non-heavy element, in which case the error is small.\nNotation. We use #S(x) to denote the duplicity of a domain element x in a database S. For a distribution \u00b5 we denote \u00b5(x) = Prx\u0302\u223c\u00b5[x\u0302 = x].\nProof. The proof is via the construction of PointLearner (algorithm 5). First note the algorithm only access the input database using sanitization on step 1, and using algorithm Adist on step 4. By composition theorem 2.16, algorithm PointLearner is ( , \u03b4)-differentially private.\nLet \u00b5 be a distribution over X , and let c1, . . . , ck \u2208 POINTX be the fixed target concepts. Consider the execution of PointLearner on a database S = (xi, yi,1, . . . , yi,k)ni=1 sampled from \u00b5 and labeled by c1, . . . , ck. We useD to denote the unlabeled portion of S, D\u0302 for the sanitization ofD constructed on step 1, and write m = |D\u0302|. Define the following good events.\nE1 : For every x \u2208 X s.t. \u00b5(x) \u2265 \u03b1 it holds that 1n#S(x) \u2265 \u03b1/10.\nE2 : For every x \u2208 X we have that | 1m#D\u0302(x)\u2212 1 n#S(x)| \u2264 \u03b1/30.\nAlgorithm 5 PointLearner Input: Privacy parameters , \u03b4, and a k-labeled database S = (xi, yi,1, . . . , yi,k)ni=1. We use D = (xi)ni=1 to denote the unlabeled portion of S. Output: Hypotheses h1, . . . , hk.\n1. Let D\u0302 \u2208 Xm be an ( 2 , \u03b4 2)-private ( \u03b1 30 , \u03b2 4 )-accurate sanitization of D w.r.t. POINTX (e.g., using\nProposition 2.20).\n2. Let G = {x \u2208 X : 1m#D\u0302(x) \u2265 \u03b1/15} be the set of all \u201c \u03b1 15 -heavy\u201d domain elements w.r.t. the\nsanitization D\u0302. Note that |G| \u2264 15/\u03b1. 3. Let q be the quality function that on input a k-labeled database S, a domain element x, and\na binary vector ~v \u2208 {0, 1}k, returns the number of appearances of (x,~v) in S. That is, q(S, x, (v1, . . . , vk)) = |{i : xi = x \u2227 yi,1 = v1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 yi,k = vk}|.\n4. Use algorithm Adist with privacy parameters 2 , \u03b4 2 to choose a set of vectors V = {~vx \u2208 {0, 1} k : x \u2208 G} maximizing Q(S, V ) = min~vx\u2208V {q(S, x,~vx)}. That is, we use algorithm Adist to choose a set of |G| vectors \u2013 a vector ~vx for every x \u2208 G \u2013 such that the minimal number of appearances of an entry (x,~vx) in the database S is maximized.\n5. For 1 \u2264 j \u2264 k: If the jth entry of every ~vx \u2208 V is 0, then set hj \u2261 0. Otherwise, let x be s.t. ~vx \u2208 V has 1 as its jth entry, and define hj : X \u2192 {0, 1} as hj(y) = 1 iff y = x.\n6. Return h1, . . . , hk.\nE3 : Algorithm Adist returns a vector set V s.t. q(S, x,~vx) \u2265 1 for every x \u2208 G.\nWe now argue that when these three events happen algorithm PointLearner returns good hypotheses. First, observe that the set G contains every element x s.t. \u00b5(x) \u2265 \u03b1: Let x be s.t. \u00b5(x) \u2265 \u03b1. As event E1 has occurred, we have that 1n#S(x) \u2265 \u03b1/10. As event E2 has occurred, we have that 1 m#D\u0302(x) \u2265 \u03b1/15, and therefore x \u2208 G. Note that if q(S, x,~v) \u2265 1 then the example x is labeled as ~v by the target concepts. Thus, as event E3 has occurred, for every ~vx \u2208 V it holds that ~vx = (c1(x), . . . , ck(x)). Now let hj be the jth returned hypothesis. We next show that hj is \u03b1-good. If h 6\u2261 0, then let x be the unique element s.t. hj(x) = 1, and note that (according to step 5) the jth entry of ~vx is 1, and hence, cj(x) = 1. So hj = cj (since cj is a concept in POINTX ).\nIf hj \u2261 0 then the jth entry of every ~vx \u2208 V is 0. Note that in such a case hj only errs on the unique element x s.t. cj(x) = 1, and it suffices to show that \u00b5(x) < \u03b1. Assume towards contradiction that \u00b5(x) \u2265 \u03b1. As before, event E1 \u2229 E2 implies that x \u2208 G. As event E3 has occurred, we also have that ~vx \u2208 V is s.t. q(S, x,~vx) \u2265 1, and the example x is labeled as ~vx by the target concepts. This contradicts the assumption that the jth entry of ~vx \u2208 V is 0.\nThus, whenever E1 \u2229 E2 \u2229 E3 happens, algorithm PointLearner returns \u03b1-good hypotheses. We will now show E1 \u2229 E2 \u2229 E3 happens with high probability. Provided n \u2265 O( 1\u03b1 log( 1 \u03b1\u03b4 )), event E2 is guaranteed to hold with all but probability \u03b2/4 by the utility properties of the sanitizer used on step 1. See Proposition 2.20.\nTheorem 2.7 (VC bound) ensures that event E1 holds with probability 1 \u2212 \u03b2/4, provided that n \u2265 O( 1\u03b1 log( 1 \u03b1\u03b2 )). To see this, let z \u2261 0 denote the constant 0 hypothesis, and consider the class C = POINTX \u222a{z}. Note that VC(C) = 1. Hence, Theorem 2.11 states that, with all but probability 1\u2212\u03b2/4, for every c \u2208 POINTx s.t. error\u00b5(c, z) \u2265 \u03b1 it holds that errorD(c, z) \u2265 \u03b1/10. That is, with all but probability\n1\u2212 \u03b2/4, for every x \u2208 X s.t. \u00b5(x) \u2265 \u03b1 it holds that 1n#D(x) = 1 n#S(x) \u2265 \u03b1/10.\nBefore analyzing event E3, we show that if E2 occurs, then every x \u2208 G is s.t. #S(x) \u2265 \u03b1/30. Let x \u2208 G, that is, x s.t. 1m#D\u0302(x) \u2265 \u03b1/15. Assuming eventE2 has occurred, we therefore have that 1 n#S(x) \u2265 \u03b1/30. So every x \u2208 G appears in S at least \u03b1n/30 times with the labels (c1(x), . . . , ck(x)) , ~c(x). Thus, q(S, x,~c(x)) \u2265 \u03b1n/30. In addition, for every ~v 6= ~c(x) it holds that q(S, x,~v) = 0, since every appearance of the example x is labeled by the target concepts. Hence, provided that n \u2265 O( 1\u03b1 log( 1 \u03b2\u03b4 )), algorithmAdist ensures that event E3 happens with probability at least 1\u2212 \u03b2/2. Overall, E1 \u2229 E2 \u2229 E3 happens with probability at least 1\u2212 \u03b2."}, {"heading": "4 Approximate Privacy Lower Bounds from Fingerprinting Codes", "text": "In this section, we show how fingerprinting codes can be used to obtain poly(k) lower bounds against privately learning k concepts, even for very simple concept classes. Fingerprinting codes were introduced by Boneh and Shaw [12] to address the problem of watermarking digital content. The connection between fingerprinting codes and differential privacy lower bounds was established by Bun, Ullman, and Vadhan [14] in the context of private query release, and has since been extended to a number of other differentially private analyses [3, 20, 32, 13].\nA (fully-collusion-resistant) fingerprinting code is a scheme for distributing codewords w1, . . . , wn to n users that can be uniquely traced back to each user. Moreover, if any group of users combines its codewords into a pirate codeword w\u2032, then the pirate codeword can still be traced back to one of the users who contributed to it. Of course, without any assumption on how the pirates can produce their combined codeword, no secure tracing is possible. To this end, the pirates are constrained according to a marking assumption, which asserts that the combined codeword must agree with at least one of the pirates\u2019 codeword in each position. Namely, at an index j where wij = b for every i \u2208 b, the pirates are constrained to output w\u2032 with w\u2032j = b as well.\nTo illustrate our technique, we start with an informal discussion of how the original Boneh-Shaw fingerprinting code yields an \u2126\u0303(k1/3) sample complexity lower bound for multi-learning threshold functions. For parameters n and k, the (n, k)-Boneh-Shaw codebook is a matrix W \u2208 {0, 1}n\u00d7k, whose rows wi are the codewords given to users i = 1, . . . , n. The codebook is built from a number of highly structured columns, where a \u201ccolumn of type i\u201d consists of n bits where the first i bits are set to 1 and the last n\u2212 i bits are set to 0. For i = 1, . . . , n \u2212 1, each column of type i is repeated a total of k/(n \u2212 1) times, and the codebook W is obtained as a random permutation of these k columns. The security of the Boneh-Shaw code is a consequence of the secrecy of this random permutation. If a coalition of pirates is missing the codeword of user i, then it is unable to distinguish columns of type i \u2212 1 from columns of type i. Hence, if a pirate codeword is too consistent with a user i\u2019s codeword in both the columns of type i \u2212 1 and the columns of type i, a tracing algorithm can reasonably conclude that user i contributed to it. Boneh and Shaw showed that such a code is indeed secure for k = O\u0303(n3).\nTo see how this fingerprinting code gives a lower bound for multi-learning thresholds, consider thresholds over the data universe X = {1, . . . , |X|} for |X| \u2265 n. The key observation is that each column of the Boneh-Shaw codebook can be obtained as a labeling of the examples 1, . . . , n by a threshold concept. Namely, a column of type i is the labeling of 1, . . . , n by the concept ci. Now suppose a coalition of users T \u2286 [n] constructs a database S where each row is an example i \u2208 T together with the labels wi1, . . . , wik coming from the codeword given to user i. Let (h1, . . . , hk) be the hypotheses produced by running a threshold multi-learner on the database. If every user has a bit b at index j of her codeword, then the hypothesis produced by the learner must also evaluate to b on most of the examples. Thus, the empirical averages of the hypotheses (h1, . . . , hk) on the examples can be used to obtain a pirate codeword satisfying the marking\nassumption. The security of the fingerprinting code, i.e. the fact that this codeword can be traced back to a user i \u2208 T , implies that the learner cannot be differentially private. Hence, n samples is insufficient for privately learning k = O\u0303(n3) threshold concepts, giving a sample complexity lower bound of \u2126\u0303(k1/3).\nThe lower bounds in this section are stated for empirical learning, but extend to PAC learning by Theorem 2.24. We also remark that they hold against the relaxed privacy notion of label privacy, where differential privacy only needs to hold with respect to changing the labels of one example."}, {"heading": "4.1 Fingerprinting Codes", "text": "An (n, k)-fingerprinting code consists of a pair of randomized algorithms (Gen,Trace). The parameter n is the number of users supported by the fingerprinting code, and k is the length of the code. The codebook generator Gen produces a codebook W \u2208 {0, 1}n\u00d7k. Each row wi \u2208 {0, 1}k of W is the codeword of user i. For a subset T \u2286 [n], we let WT denote the set {wi : i \u2208 T} of codewords belonging to users in T . The accusation algorithm Trace takes as input a pirate codeword w\u2032 and accuses some i \u2208 [n] (or \u22a5 if it fails to accuse any user).\nWe define the feasible set of pirate codewords for a coalition T and codebook W by\nF (WT ) = {w\u2032 \u2208 {0, 1}k : \u2200j = 1, . . . , k \u2203i \u2208 S s.t. wij = w\u2032j}.\nThe basic marking assumption is that the pirate codeword w\u2032 \u2208 F (WT ). We say column j is b-marked if wij = b for every i \u2208 [n].\nDefinition 4.1 (Fingerprinting Codes). For n, k \u2208 N and \u03be \u2208 (0, 1], a pair of algorithms (Gen,Trace) is an (n, k)-fingerprinting code with security \u03be if Gen outputs a codebookW \u2208 {0, 1}n\u00d7k and for every (possibly randomized) adversary AFP , and every coalition T \u2286 [n], if we take w\u2032 \u2190R AFP (WT ), then the following properties hold.\nCompleteness: Pr [w\u2032 \u2208 F (WT ) \u2227 Trace(w\u2032) = \u22a5] \u2264 \u03be,\nSoundness: Pr [Trace(w\u2032) \u2208 [n] \\ T ] \u2264 \u03be,\nEach probability is taken over the coins of Gen,Trace, andAFP . The algorithms Gen and Trace may share a common state, which is hidden to ease notation."}, {"heading": "4.2 Lower Bound for Improper PAC Learning", "text": "Our lower bounds for multi-learning follow from constructions of fingerprinting codes with additional structural properties.\nDefinition 4.2. Let C be a concept class over a domain X . An (n, k)-fingerprinting code (Gen,Trace) is compatible with concept classC if there exist x1, . . . , xn \u2208 X such that for every codebookW in the support of Gen, there exist concepts c1, . . . , ck such that wij = cj(xi) for every i = 1, . . . , n and j = 1, . . . , k.\nTheorem 4.3. Suppose there exists an (n, k)-fingerprinting code compatible with a concept class C with security \u03be. Let \u03b1 \u2264 1/3, \u03b2, > 0, and \u03b4 < 1\u2212\u03be\u2212\u03b2n \u2212 e\n\u03be. Then every (improper) (\u03b1, \u03b2)-accurate and ( , \u03b4)-differentially private empirical k-learner for C requires sample complexity greater than n.\nThe proof of Theorem 4.3 follows the ideas sketched above.\nProof. Let (Gen,Trace) be an (n, k)-fingerprinting code compatible with the concept class C, and let x1, . . . , xn \u2208 X be its associated universe elements. Let D = (x1, . . . , xn) and let A be an (\u03b1, \u03b2)-accurate empirical k-learner for C with sample complexity n. We will useA to design an adversaryAFP against the fingerprinting code.\nLet T \u2286 [n] be a coalition of users, and consider a codebook W \u2190R Gen. The adversary strategy AFP (WT ) begins by constructing a labeled database S = (Si)ni=1 by setting Si = (xi, wi1, . . . , wik) for each i \u2208 T and to a nonce row for i /\u2208 T . It then runs A(S) obtaining hypotheses (h1, . . . , hk). Finally, it computes for each j = 1, . . . , k the averages\nhj(D) = 1\nn n\u2211 i=1 hj(xi)\nand produces a pirate word w\u2032 by setting each w\u2032j to the value of aj rounded to 0 or 1. Now consider the coalition T = [n]. Since the fingerprinting code is compatible with C, each column (w1j , . . . , wnj) = (cj(x1), . . . , cj(xn)) for some concept cj \u2208 C. Thus, if the hypotheses (h1, . . . , hk) are \u03b1-accurate for (c1, . . . , ck) on S, then w\u2032 \u2208 F (WT ) = F (W ). Therefore, by the completeness property of the code and the (\u03b1, \u03b2)-accuracy of A, we have\nPr [Trace(AFP (W )) 6= \u22a5] \u2265 1\u2212 \u03be \u2212 \u03b2.\nIn particular, there exists an i\u2217 for which\nPr [Trace(AFP (W )) = i\u2217] \u2265 1\u2212 \u03be \u2212 \u03b2\nn .\nOn the other hand, by the soundness property of the code,\nPr [Trace(AFP (W\u2212i\u2217)) = i\u2217] \u2264 \u03be.\nThus, A cannot be ( , \u03b4)-differentially private whenever\n1\u2212 \u03be \u2212 \u03b2 n > e \u00b7 \u03be + \u03b4.\nRemark 4.4. If we additionally assume that there exists an element x0 \u2208 X with c1(x0) = c2(x0) = \u00b7 \u00b7 \u00b7 = ck(x0), then we can use a \u201cpadding\u201d argument to obtain a stronger lower bound of n/3\u03b1. More specifically, suppose c1(x0) = \u00b7 \u00b7 \u00b7 = ck(x0) = 0. We pad the database S constructed above with (1/3\u03b1 \u2212 1)n copies of the junk row (x0, 0, . . . , 0). Now if a hypothesis h is \u03b1-accurate for a 0-marked column, it\u2019s empirical average will be at most \u03b1. On the other hand, an \u03b1-accurate hypothesis for a 1-marked column will have empirical average at least 2\u03b1. Since there is a gap between these two quantities, a pirate algorithm can still turn an accurate vector of k hypotheses into a feasible codeword.\nAs observed earlier, the (n, k)-Boneh-Shaw code is compatible with the concept class THRESHX for any |X| \u2265 n. Thus, instantiating Theorem 4.3 (and Remark 4.4) with the Boneh-Shaw code yields a lower bound for k-learning thresholds.\nLemma 4.5 ([12]). Let X be a totally ordered domain with |X| \u2265 n for some n \u2208 N. Then there exists an (n, k)-fingerprinting code compatible with the concept class THRESHX with security \u03be as long as k \u2265 2n3 log(2n/\u03be).\nCorollary 4.6. Every improper (\u03b1, \u03b2)-accurate and ( = O(1), \u03b4 = o(1/n))-differentially private empirical k-learner for THRESHX requires sample complexity min{|X|, \u2126\u0303(k1/3/\u03b1)}.\nDiscussion. Compatibility with a concept class is an interesting measure of the complexity of a fingerprinting code which warrants further attention. Peikert, shelat, and Smith [29] showed that structural constraints (related to compatibility) on a fingerprinting code give a lower bound on its length beyond the general lower bound of k = \u2126\u0303(n2) for arbitrary fingerprinting codes. In particular, they showed that the length k = O\u0303(n3) of the Boneh-Shaw code is essentially tight for the \u201cmultiplicity paradigm\u201d, where a codebook is a random permutation of a fixed set of columns, each repeated the same number of times. We take this as evidence that our \u2126\u0303(k1/3) lower bound for THRESHX cannot be improved via compatible fingerprinting codes. However, closing the gap between our lower bound and the upper bound of roughly \u221a k remains an intriguing open question. A natural avenue for obtaining stronger poly(k) lower bounds for private k-learning is to identify compatible fingerprinting codes with shorter length. Tardos [33] showed the existence of an (n, k)-fingerprinting code of optimal length k = O\u0303(n2) (see Proposition 4.9). The construction of his code differs significantly from multiplicity paradigm: for each column j of the Tardos code, a bias pj \u2208 (0, 1) is sampled from a fixed distribution, and then each bit of the column is sampled i.i.d. with bias pj . Hence, the columns of the Tardos code are supported on all bit vectors in {0, 1}n. This means that for a concept class C to be compatible with the (n, k)-Tardos code, it must be the case that VC(C) \u2265 n. Thus, the lower bound one obtains against k-learning C only matches the lower bound for PAC learning C (without privacy). It would be very interesting to construct a fingerprinting code of optimal length k = O\u0303(n2) with substantially fewer than 2n column types (and hence compatible with a concept class of VC-dimension smaller than n)."}, {"heading": "4.3 Lower Bound for Agnostic Learning", "text": "In the agnostic learning model, a learner has to perform well even when the columns of a multi-labeled database do not correspond to any concept. This allows us to apply the argument of Theorem 4.3 without the constraint of compatibility. The result is that any fingerprinting code, in particular one with optimal length, gives an agnostic learning lower bound for any non-trivial concept class.\nTheorem 4.7. Suppose there exists an (n, k)-fingerprinting code with security \u03be. Let C be a concept class with at least two distinct concepts. Let \u03b1 \u2264 1/3, \u03b2, > 0, and \u03b4 < 1\u2212\u03be\u2212\u03b2n \u2212 e\n\u03be. Then every (improper) agnostic (\u03b1, \u03b2)-accurate and ( , \u03b4)-differentially private empirical k-learner forC requires sample complexity greater than n.\nProof. The proof follows in much the same way as that of Theorem 4.3. Let (Gen,Trace) be an (n, k)fingerprinting code, and let x \u2208 X be such that there exist c0, c1 \u2208 C with c0(x) = 0 and c1(x) = 1. Let A be an agnostic (\u03b1, \u03b2)-accurate empirical k-learner for C with sample complexity n. Define a the fingerprinting code adversary AFP just as in Theorem 4.3. Namely, AFP constructs examples of the form (x,wi1, . . . , wij) with the available rows of the fingerprinting code, runs A on the result, and returns the rounded empirical averages of the k resulting hypotheses.\nTo show that A cannot be ( , \u03b4)-differentially private, it suffices to show that if A produces accurate hypotheses h1, . . . , hk, then the pirate codeword produced byAFP is feasible. To see this, suppose h1, . . . , hk are accurate, i.e.\nmax 1\u2264j\u2264k ( errorS|j (hj)\u2212minc\u2208C ( errorS|j (c) )) \u2264 \u03b1.\nLet column j of the codebook W be 0-marked, i.e. wij = 0 for all i \u2208 [n]. Recall that c0(x) = 0, and hence errorS|j (c0) = 0. Therefore, since hypothesis hj is \u03b1-accurate, we have errorS|j (hj) \u2264 \u03b1. This implies that bit w\u2032j of the pirate codeword is 0. An identical argument shows that the bits of the pirate codeword in the 1-marked columns are also 1. Thus, if A produces accurate hypotheses, the pirate codeword produced by AFP is feasible. The rest of the argument in the proof of Theorem 4.3 completes the proof.\nRemark 4.8. Just as in Remark 4.4, a padding argument shows how to obtain a lower bound of n/3\u03b1 under some additional assumptions on C, e.g. if the distinct concepts also share a common point x\u2032 with c0(x \u2032) = c1(x \u2032).\nProposition 4.9 ([33]). For n \u2208 N and \u03be \u2208 (0, 1), there exists an (n, k)-fingerprinting code with security \u03be as long as k = O(n2 log(n/\u03be)).\nCorollary 4.10. Every improper agnostic (\u03b1, \u03b2)-accurate and ( = O(1), \u03b4 = o(1/n))-differentially private empirical k-learner for POINTX , THRESHX , PARd requires sample complexity min{|X|, \u2126\u0303(k1/2)}.\nThe same proof yields a lower bound for agnostically learning parities under the uniform distribution.\nProposition 4.11. Suppose there exists an (n, k)-fingerprinting code with security \u03be. Let \u03b1 \u2264 1/6, \u03b2 > 0 and d = log n. Then every (improper) agnostic (\u03b1, \u03b2, = O(1), \u03b4 = o(1/n))-PAC k-learner for PARd requires sample complexity \u2126(n).\nProof sketch. By Lemma 2.24, it is enough to rule out a private empirical learner for a database whose n examples are the distinct binary strings in {0, 1}d. To do so, we follow the proof of Theorem 4.7, highlighting the changes that need to be made. First, we let c0 be the all-zeroes concept, and let c1 be an arbitrary other parity function. Second, AFP instead constructs examples of the form (xi, wi1, . . . , wik) where xi is the ith binary string. Finally, when converting the hypotheses (h1, . . . , hk) into a feasible codeword, we instead set w\u2032j to 0 if hj(D) \u2264 \u03b1, and set w\u2032j to 1 if hj(D) \u2265 12 \u2212 \u03b1. This works because, while errorS|j (c0) = 0 with respect to 0-marked columns, any concept (and in particular, c1) has error 12 with respect to 1-marked columns."}, {"heading": "5 Examples where Direct Sum is Optimal", "text": "In this section we show several examples for cases where the direct sum is (roughly) optimal. As we saw in Section 4, with ( , \u03b4)-differential privacy, every non-trivial agnostic k-learner requires sample complexity \u2126( \u221a k). We can prove a similar result for -private learners, that holds even for non-agnostic learners:\nTheorem 5.1. Let C be any non-trivial concept class over a domain X (i.e., |C| \u2265 2). Every proper or improper (\u03b1, \u03b2=12 , )-private PAC k-learner for C requires sample complexity \u2126(k/ ).\nIn [5, 6, 7], Beimel et al. presented an agnostic proper learner for POINTX with sample complexity O\u03b1,\u03b2, ,\u03b4(1) under ( , \u03b4)-privacy, and an agnostic improper learner for POINTX with sample complexity O\u03b1,\u03b2, ,\u03b4(1) under -privacy. Hence, using Observation 3.2 (direct sum) with their results yields an (\u03b1, \u03b2, , \u03b4)-PAC agnostic proper k-learner for POINTX with sample complexity O\u0303\u03b1,\u03b2, ,\u03b4( \u221a k), and an (\u03b1, \u03b2, )- PAC agnostic improper k-learner for POINTX with sample complexity O\u0303\u03b1,\u03b2, (k). As supported by our lower bounds (Corollary 4.10 and Theorem 5.1), those learners have roughly optimal sample complexity (ignoring the dependency in \u03b1, \u03b2, , \u03b4 and logarithmic factors in k).\nProof of Theorem 5.1. The proof is based on a packing argument [25, 5]. Let x \u2208 X and f, g \u2208 C be s.t. f(x) 6= g(x). Let \u00b5 denote the constant distribution over X giving probability 1 to the point x. Note that error\u00b5(f, g) = 1. Moreover, observe that for every concept h, if error\u00b5(h, f) < 1 then h(x) = f(x), and similarly with h, g.\nLet A be an (\u03b1, \u03b2, )-private PAC k-learner for C with sample complexity n. For every choice of k target functions (c1, . . . , ck) = ~c \u2208 {f, g}k, let S~c denote the k-labeled database containing n copies of the point x, each of which is labeled by c1, . . . , ck. Without loss of generality, we can assume that on such\ndatabases A returns hypotheses in {f, g} (since under \u00b5 we can replace an arbitrarily chosen hypothesis h with f if f(x) = h(x) or with g if g(x) = h(x)). Therefore, by the utility properties of A, for every ~c = (c1, . . . , ck) \u2208 {f, g}k we have that PrA[A(S~c) = (c1, . . . , ck)] \u2265 12 . By changing the database S~c to S~c\u2032 one row at a time while applying the differential privacy constraint, we see that\nPr A\n[A(S~c) = (c\u20321, . . . , c\u2032k)] \u2265 1 2 e\u2212 n.\nSince the above inequality holds for every two databases S~c and S~c\u2032 , we get\n1 2 \u2265 Pr A [A(S~c) 6= (c1, . . . , ck)] \u2265 (2k \u2212 1) 1 2 e\u2212 n.\nSolving for n, this yields n = \u2126(k/ ).\nRemark 5.2. The above proof could easily be strengthened to show that n = \u2126( k\u03b1 ), provided that C contains two concepts f, g s.t. \u2203x, y \u2208 X for which f(x) 6= g(x) and f(y) = g(y).\nThe following lemma shows that the sample complexities of properly and improperly learning parities under the uniform distribution are the same. Thus, for showing lower bounds, it is without loss of generality to consider proper learners.\nLemma 5.3. Let \u03b1 < 1/4. Let A be a (possibly improper) (\u03b1, \u03b2, , \u03b4)-PAC k-learner for PARd under the uniform distribution with sample complexity n. Then there exists a proper (\u03b1\u2032 = 0, \u03b2, , \u03b4)-PAC k-learner A\u2032 for PARd (under the uniform distribution) with sample complexity n.\nProof. The algorithm A\u2032 runs A and \u201crounds\u201d each hypothesis hj produced to the nearest parity function. That is, it outputs (h\u20321, . . . , h \u2032 k) where h \u2032 j is a parity function that minimizes Prx\u223cUd [h \u2032 j(x) 6= hj(x)]. Since this is just post-processing of the differentially private algorithm A, the proper learner A remains ( , \u03b4)differentially private.\nNow suppose (h1, . . . , hk) is \u03b1-accurate for parity functions (c1, . . . , ck) on the uniform distribution. Then for each j,\nPr x\u223cUd [h\u2032j(x) 6= cj(x)] \u2264 Pr x\u223cUd [h\u2032j(x) 6= hj(x)] + Pr x\u223cUd [hj(x) 6= cj(x)]\n\u2264 2 Pr x\u223cUd [hj(x) 6= cj(x)] \u2264 2\u03b1.\nHence, errorUd(h \u2032 j , cj) < 1/2. Since the error of any parity function from cj (other than cj itself) is exactly 1/2 under the uniform distribution, we conclude that (h\u20321, . . . , h \u2032 k) is in fact 0-accurate for (c1, . . . , ck).\nTheorem 5.4. Let \u03b1 < 14 . Every (\u03b1, \u03b2= 1 2 , )-PAC k-learner for PARd (under the uniform distribution) requires sample complexity \u2126(kd/ ).\nAs we saw in Example 3.5, applying direct sum for k-learning parities results in a proper agnostic (\u03b1, \u03b2, )-PAC k-learner for PARd with sample complexity O\u03b1,\u03b2, (kd + k log k). As stated by Theorem 5.4, this is the best possible (ignoring logarithmic factors and the dependency in \u03b1, \u03b2, ).\nProof of Theorem 5.4. The proof is based on a packing argument [25, 5]. Let A be an (\u03b1, \u03b2, )-PAC klearner for PARd with sample complexity n. By Lemma 5.3, we may assume A is proper and learns the hidden concepts exactly.\nFor every choice of k parity functions (c1, . . . , ck) = ~c \u2208 (PARd)k, let S~c denote a random k-labeled database containing n i.i.d. elements from Ud, each labeled by (c1, . . . , ck). By the utility properties of A we have that PrUd,A[A(S~c) = ~c] \u2265 12 . In particular, for every ~c \u2208 (PARd)\nk there exists a databaseD~c labeled by ~c s.t. PrA[A(S~c) = ~c] \u2265 12 . By changing the database D~c to D~c\u2032 one row at a time while applying the differential privacy constraint, we see that\nPr A\n[A(D~c) = ~c\u2032] \u2265 1 2 e\u2212 n.\nSince the above inequality holds for every two databases D~c and D~c\u2032 , we get\n1 2 \u2265 Pr A [A(D~c) 6= ~c] \u2265 (| PARd |k \u2212 1) 1 2 e\u2212 n.\nSolving for n, this yields n = \u2126(kd/ )."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We investigate the direct-sum problem in the context of differentially private PAC learning: What<lb>is the sample complexity of solving k learning tasks simultaneously under differential privacy, and how<lb>does this cost compare to that of solving k learning tasks without privacy? In our setting, an individual<lb>example consists of a domain element x labeled by k unknown concepts (c1, . . . ,<lb>ck). The goal of a<lb>multi-learner is to output k hypotheses (h1, . . . , hk) that generalize the input examples.<lb>Without concern for privacy, the sample complexity needed to simultaneously learn k concepts is<lb>essentially the same as needed for learning a single concept. Under differential privacy, the basic strategy<lb>of learning each hypothesis independently yields sample complexity that grows polynomially with k.<lb>For some concept classes, we give multi-learners that require fewer samples than the basic strategy.<lb>Unfortunately, however, we also give lower bounds showing that even for very simple concept classes,<lb>the sample cost of private multi-learning must grow polynomially in k.", "creator": "LaTeX with hyperref package"}}}