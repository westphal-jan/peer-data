{"id": "1511.08842", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "Efficient Sum of Outer Products Dictionary Learning (SOUP-DIL) - The $\\ell_0$ Method", "abstract": "The sparsity of natural signals and images in a transform domain or dictionary has been extensively exploited in several applications such as compression, denoising and inverse problems. More recently, data-driven adaptation of synthesis dictionaries has shown promise in many applications compared to fixed or analytical dictionary models. However, dictionary learning problems are typically non-convex and NP-hard, and the usual alternating minimization approaches for these problems are often computationally expensive, with the computations dominated by the NP-hard synthesis sparse coding step. In this work, we investigate an efficient method for $\\ell_{0}$ \"norm\"-based dictionary learning by first approximating the training data set with a sum of sparse rank-one matrices and then using a block coordinate descent approach to estimate the unknowns. The proposed block coordinate descent algorithm involves efficient closed-form solutions. In particular, the sparse coding step involves a simple form of thresholding. We provide a convergence analysis for the proposed block coordinate descent approach. Our numerical experiments show the promising performance and significant speed-ups provided by our method over the classical K-SVD scheme in sparse signal representation and image denoising.", "histories": [["v1", "Fri, 27 Nov 2015 22:32:43 GMT  (1903kb)", "http://arxiv.org/abs/1511.08842v1", "A short version of this work [arXiv:1511.06333] has also been submitted to ICLR 2016"], ["v2", "Fri, 21 Apr 2017 02:31:13 GMT  (1904kb)", "http://arxiv.org/abs/1511.08842v2", "This work is cited by the IEEE Transactions on Computational Imaging PaperarXiv:1511.06333(DOI: 10.1109/TCI.2017.2697206)"]], "COMMENTS": "A short version of this work [arXiv:1511.06333] has also been submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["saiprasad ravishankar", "raj rao nadakuditi", "jeffrey a fessler"], "accepted": false, "id": "1511.08842"}, "pdf": {"name": "1511.08842.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["fessler)@umich.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n08 84\n2v 1\n[ cs\n.L G\n] 2\n7 N\nov 2\n01 5\nIndex Terms\u2014Sparse representations, Dictionary learning, Image denoising, Fast algorithms, Machine learning, Convergence analysis, Block coordinate descent, Nonconvex optimization\nI. INTRODUCTION\nThe sparsity of natural signals and images in a transform domain or dictionary has been extensively exploited in several applications such as compression [1], denoising, compressed sensing and other inverse problems. Well-known models for sparsity include the synthesis, analysis [2], and the transform [3], [4] (or generalized analysis) models. More recently, the data-driven adaptation of sparse signal models has benefited many applications [4]\u2013[12] compared to fixed or analytical models. In this work, we focus on the data-driven adaptation of the synthesis model and present a simple and highly efficient algorithm with convergence analysis and applications. In the following, we first briefly review the topic of synthesis dictionary learning before summarizing the contributions of this work."}, {"heading": "A. Synthesis Model and Dictionary Learning", "text": "The well-known synthesis model suggests that a signal y \u2208 R n is approximately a linear combination of a small subset of\nThis work was supported in part by the following grants: ONR grant N00014-15-1-2141, DARPA Young Faculty Award D14AP00086, ARO MURI grants W911NF-11-1-0391 and 2015-05174-05, NIH grant U01 EB01875301, and a UM-SJTU seed grant.\nS. Ravishankar, R. Nadakuditi, and J. Fessler are with the Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, 48109 USA emails: (ravisha, rajnrao, fessler)@umich.edu.\natoms or columns of a dictionary D \u2208 Rn\u00d7J , i.e., y = Dx+e with x \u2208 RJ sparse, and e is assumed to be a small modeling error or approximation error in the signal domain [13]. We say that x \u2208 RJ is sparse if \u2016x\u2016\n0 \u226a n, where the \u21130 \u201cnorm\u201d\ncounts the number of non-zero entries in x. Since different candidate signals may be approximately spanned by different subsets of columns in the dictionary D, the synthesis model is also known as a union of subspaces model [14], [15]. When n = J and D is full rank, it is a basis. Else when J > n, D is called an overcomplete dictionary. Because of their richness, overcomplete dictionaries can provide highly sparse (i.e., with few non-zeros) representations of data and are popular.\nFor a given signal y and dictionary D, the process of finding a sparse representation x involves solving the wellknown synthesis sparse coding problem. This problem is to minimize \u2016y \u2212Dx\u20162\n2 subject to \u2016x\u2016 0 \u2264 s, where s is\nsome set sparsity level. The synthesis sparse coding problem is NP-hard (Non-deterministic Polynomial-time hard) [16], [17]. Numerous algorithms [18]\u2013[25] including greedy and relaxation algorithms have been proposed for this problem. While some of these algorithms are guaranteed to provide the correct solution under certain conditions, these conditions are often restrictive and violated in applications. Moreover, these sparse coding algorithms typically tend to be computationally expensive for large-scale problems.\nMore recently, the data-driven adaptation of synthesis dictionaries, called dictionary learning, has been investigated in several works [8], [26]\u2013[29]. Dictionary learning has shown promise in several applications including compression, denoising, inpainting, deblurring, demosaicing, super-resolution, and classification [5], [6], [30]\u2013[37]. It has also been demonstrated to be useful in inverse problems such as those in tomography [38], and magnetic resonance imaging (MRI) [9], [39]\u2013[41].\nGiven a collection of training signals {yi} N\ni=1 that are represented as columns of the matrix Y \u2208 Rn\u00d7N , the dictionary learning problem is often formulated as follows [28]\n(P0) min D,X\n\u2016Y \u2212DX\u20162F s.t. \u2016xi\u20160 \u2264 s \u2200 i, \u2016dj\u20162 = 1 \u2200 j.\nHere, dj and xi denote the columns of the dictionary D \u2208 R\nn\u00d7J and sparse code matrix X \u2208 RJ\u00d7N , respectively, and s denotes the maximum sparsity level (non-zeros in representations xi) allowed for each training signal. The columns of the dictionary are constrained to have unit norm to avoid the scaling ambiguity [42]. Variants of Problem (P0) include replacing the \u21130 \u201cnorm\u201d for sparsity with an \u21131 norm or an alternative sparsity criterion, or enforcing additional properties\n2 (e.g., incoherence [35], [43]) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem [8].\nAlgorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D). Some of these algorithms (e.g., [28], [48], [50]) also partially update X in the dictionary update step. A few recent methods attempt to solve for D and X jointly in an iterative fashion [53], [54]. The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39]. However, Problem (P0) is highly nonconvex and NP-hard, and most dictionary learning approaches lack proven convergence guarantees. Moreover, the algorithms for (P0) tend to be computationally expensive (particularly alternating-type algorithms), with the computations usually dominated by the synthesis sparse coding step.\nSome recent works [52], [55]\u2013[59] have studied the convergence of (specific) synthesis dictionary learning algorithms. However, these dictionary learning methods have not been demonstrated to be useful in applications such as image denoising. Bao et al. [52] find that their method, although a fast proximal scheme, denoises less effectively than the KSVD method [5]. Many prior works use restrictive assumptions (e.g., noiseless data, etc.) for their convergence results."}, {"heading": "B. Contributions", "text": "This work focuses on synthesis dictionary learning and investigates a dictionary learning problem with an \u21130 sparsity penalty instead of constraints. Our approach first models the training data set as an approximate sum of sparse rank-one matrices (or outer products). Then, we use a simple and exact block coordinate descent approach to estimate the factors of the various rank-one matrices. In particular, the sparse coding step in the algorithm uses a simple form of thresholding, and the dictionary atom update step involves a matrix-vector product that is computed efficiently. We provide a convergence analysis of the proposed block coordinate descent method. Our numerical experiments demonstrate the promising performance and significant speed-ups provided by our method over the classicial K-SVD dictionary learning scheme in sparse signal representation and image denoising. Importantly, our algorithm is similar to just the dictionary update step of KSVD, and yet performs comparably or much better and faster than K-SVD in applications.\nOur work shares similarities with a recent dictionary learning approach [60] that exploits a sum of outer products model for the training data. However, the specific problem formulation and algorithm studied in this work differ from the prior work [60]. Importantly, unlike the previous approach [60], we provide a detailed convergence analysis of the proposed algorithm and also demonstrate its usefulness (both in terms of speed and quality of results) in sparse representation of natural signals and in image denoising. The sum of outer products model and related algorithm can accomodate various structures or constraints on the dictionary or sparse codes."}, {"heading": "C. Organization", "text": "The rest of this paper is organized as follows. Section II discusses the problem formulation for dictionary learning, along with some potential alternatives. Section III presents our dictionary learning algorithm and discusses its computational properties. Section IV presents a convergence analysis for the proposed algorithm. Section V illustrates the empirical convergence behavior of our method and demonstrates its usefulness for sparse signal representation and image denoising. Section VI presents conclusions and proposals for future work."}, {"heading": "II. PROBLEM FORMULATIONS", "text": ""}, {"heading": "A. \u21130 Penalized Formulation", "text": "We consider a sparsity penalized variant of Problem (P0) [52]. Specifically, by replacing the sparsity constraints in (P0) with an \u21130 penalty \u2211N\ni=1 \u2016xi\u20160 and introducing a variable C = XT \u2208 RN\u00d7J , where (\u00b7)T denotes the matrix transpose operation, we arrive at the following formulation:\nmin D,C\n\u2225 \u2225Y \u2212DCT \u2225 \u2225 2\nF + \u03bb2 \u2016C\u2016 0 s.t. \u2016dj\u20162 = 1 \u2200 j. (1)\nwhere \u2016C\u2016 0 counts the number of non-zeros in the matrix C, and \u03bb2 > 0 is a weight for the sparsity penalty that controls the number of non-zeros in the sparse representation.\nNext, we express the matrix DCT in (1) as a sum of (sparse) rank-one matrices or outer products \u2211J\nj=1 djc T j , where cj\nis the jth column of C. This is a natural representation of the training data Y because it separates out the contributions of the various atoms in representing the data. It may also provide a natural way to set the number of atoms (degrees of freedom) in the dictionary. In particular, atoms of a dictionary whose contributions to the data (Y ) representation error or modeling error are small could be dropped. Such a Sum of OUter Products (SOUP) representation has been exploited in previous dictionary learning algorithms [28], [48]. With this model, we rewrite (1) as follows, where \u2016C\u2016\n0 =\n\u2211J\nj=1 \u2016cj\u20160:\n(P1) min {dj ,cj}\n\u2225 \u2225 \u2225Y \u2212 \u2211J\nj=1 djc T j\n\u2225 \u2225 \u2225 2\nF + \u03bb2\nJ \u2211\nj=1\n\u2016cj\u20160\ns.t. \u2016dj\u20162 = 1, \u2016cj\u2016\u221e \u2264 L \u2200 j.\nAs in Problem (P0), the matrix djcTj in (P1) is invariant to joint scaling of dj and cj as \u03b1dj and (1/\u03b1)cj , for \u03b1 6= 0. The constraint \u2016dj\u20162 = 1 helps remove this scaling ambiguity. We also enforce the constraint \u2016cj\u2016\u221e \u2264 L, with L > 0, in (P1) [52] (e.g., L = \u2016Y \u2016F ). Consider a dictionary D that has a column dj that repeats. Then, in this case, the outer product expansion of Y in (P1) could have both the terms djcTj and \u2212djcTj with cj that is highly sparse, and the objective would be invariant to (arbitrarily) large scalings1 of cj (i.e., noncoercive objective). The \u2113\u221e constraints on the columns of C (that constrain the magnitudes of entries of C) prevent such a scenario. Problem (P1) is designed to learn the factors {dj} J\nj=1\nand {cj} J\nj=1 that enable the best SOUP sparse representation\nof Y . 1Such degenerate representations of Y , however, cannot be minimizers in the problem because they simply increase the \u21130 sparsity penalty without affecting the fitting error (the first term) in the cost.\n3"}, {"heading": "B. Alternative Formulations", "text": "Several variants of the SOUP learning Problem (P1) could be constructed, with interesting effects. For example, the \u21130 \u201cnorm\u201d for sparsity could be replaced by the \u21131 norm [60]. Another interesting alternative to (P1) involves enforcing p-block-orthogonality constraints on the dictionary D. The dictionary in this case is split into blocks (instead of individual atoms), each of which has p atoms that are orthogonal to each other. For p = 2, the constraints take the form dT2j\u22121d2j = 0, 1 \u2264 j \u2264 J/2. In the extreme (more constrained) case of p = n, the dictionary would be made of several (square) orthonormal blocks. For tensor-type data, Problem (P1) can be modified by enforcing the dictionary atoms to be in the form of a Kronecker product. The algorithm proposed in Section III can be easily extended to accomodate such variants of Problem (P1). We do not fully explore such alternatives in this work due to space constraints, and a more detailed investigation of these will be presented elsewhere."}, {"heading": "III. ALGORITHM AND PROPERTIES", "text": ""}, {"heading": "A. Algorithm", "text": "We propose a block coordinate descent method to estimate the unknown variables in Problem (P1). For each j (1 \u2264 j \u2264 J), our algorithm has two key steps. First, we solve (P1) with respect to cj keeping all the other variables fixed. We refer to this step as the sparse coding step in our method. Once cj is updated, we solve (P1) with respect to dj keeping all other variables fixed. This step is referred to as the dictionary atom update step or simply dictionary update step. The algorithm updates the factors of the various rank-one matrices one-byone. We next describe the sparse coding and dictionary atom update steps.\n1) Sparse Coding Step: Minimizing (P1) with respect to cj leads to the following non-convex problem, where Ej , Y \u2212 \u2211\nk 6=j dkc T k is a fixed matrix based on the most recent\nvalues of all other atoms and coefficients:\nmin cj\n\u2225 \u2225Ej \u2212 djcTj \u2225 \u2225 2 F + \u03bb2 \u2016cj\u20160 s.t. \u2016cj\u2016\u221e \u2264 L. (2)\nThe following proposition provides the solution to Problem (2), where the hard-thresholding operator H\u03bb(\u00b7) is defined as\n(H\u03bb(b))i =\n{\n0, |bi| < \u03bb bi, |bi| \u2265 \u03bb (3)\nwith b \u2208 RN , and the subscript i indexes vector entries. We assume that the bound L > \u03bb and let 1N denote a vector of ones of length N . The operation \u201c\u2299\u201d denotes element-wise multiplication, sign(\u00b7) computes the signs of the elements of a vector, and z = min(a, u) for vectors a, u \u2208 RN denotes the element-wise minimum operation, i.e., zi = min(ai, bi), 1 \u2264 i \u2264 N .\nProposition 1: Given Ej \u2208 Rn\u00d7N and dj \u2208 Rn, and assuming L > \u03bb, a global minimizer of the sparse coding problem (2) is obtained by the following truncated hardthresholding operation:\nc\u0302j = min (\u2223 \u2223H\u03bb ( ETj dj )\u2223 \u2223 , L1N ) \u2299 sign ( H\u03bb ( ETj dj )) (4)\nThe minimizer of (2) is unique if and only if the vector ETj dj has no entry with a magnitude of \u03bb. Proof: First, for a vector dj that has unit \u21132 norm, we have the following equality\n\u2225 \u2225Ej \u2212 djc T j \u2225 \u2225 2 F = \u2016Ej\u2016 2 F + \u2016cj\u2016 2 2 \u2212 2cTj E T j dj\n= \u2225 \u2225cj \u2212 E T j dj \u2225 \u2225 2 2 + \u2016Ej\u2016 2 F \u2212 \u2225 \u2225ETj dj \u2225 \u2225 2 F (5)\nBy substituting (5) into (2), it is clear that Problem (2) is equivalent to\nmin cj\n\u2225 \u2225cj \u2212 E T j dj \u2225 \u2225 2 2 + \u03bb2 \u2016cj\u20160 s.t. \u2016cj\u2016\u221e \u2264 L. (6)\nDefine b , ETj dj . Then, the objective in (6) simplifies to \u2211N\ni=1\n{\n|cji \u2212 bi| 2 + \u03bb2 \u03b8(cji)\n}\nwith\n\u03b8 (a) =\n{\n0, if a = 0 1, if a 6= 0 (7)\nTherefore, we solve for each entry cji of cj as\nc\u0302ji = argmin cji\n|cji \u2212 bi| 2 + \u03bb2 \u03b8(cji) s.t. |cji| \u2264 L. (8)\nIt is straightforward to show that when |bi| \u2264 L (case (a)),\nc\u0302ji =\n{\n0, if b2i < \u03bb 2 bi, if b 2 i > \u03bb 2 (9)\nWhen |bi| = \u03bb (\u03bb < L), the optimal c\u0302ji can be either bi or 0 (non-unique), and both these settings achieve the minimum objective value \u03bb2 in (8). Next, when |bi| > L (case (b)), we have\nc\u0302ji =\n{\n0, if b2i < (L sign(bi)\u2212 bi) 2 + \u03bb2\nL sign(bi), if b 2 i > (L sign(bi)\u2212 bi) 2 + \u03bb2\n(10)\nSince L > \u03bb, clearly b2i > (L sign(bi)\u2212 bi) 2 + \u03bb2 in (10).\nThus, an optimal c\u0302ji in (8) is compactly written as c\u0302ji = min (\u2223 \u2223H\u03bb (bi) \u2223 \u2223 , L )\n\u00b7 sign (bi), thereby establishing (4). The condition for uniqueness of the sparse coding solution follows from the arguments for case (a) above.\n2) Dictionary Atom Update Step: Minimizing (P1) with respect to dj leads to the following non-convex problem:\nmin dj\n\u2225 \u2225Ej \u2212 djc T j \u2225 \u2225 2 F s.t. \u2016dj\u20162 = 1 (11)\nProposition 2 provides the closed-form solution for (11). Proposition 2: Given Ej \u2208 Rn\u00d7N and cj \u2208 RN , a global minimizer of the dictionary atom update problem (11) is\nd\u0302j =\n{\nEjcj \u2016Ejcj\u20162 , if cj 6= 0\nv1, if cj = 0 (12)\nwhere v1 is the first column of the n\u00d7n identity matrix. The solution is unique if and only if cj 6= 0. Proof: First, for a vector dj that has unit \u21132 norm, the following holds:\n\u2225 \u2225Ej \u2212 djcTj \u2225 \u2225 2 F = \u2016Ej\u2016 2 F + \u2016cj\u2016 2 2 \u2212 2 dTj Ejcj (13)\n4 SOUP-DIL Algorithm Inputs : Training data Y \u2208 Rn\u00d7N , weight \u03bb, upper bound L, and number of iterations K . Outputs : Columns {\ndKj }J j=1 of the learned dictionary,\nand the learned sparse coefficients { cKj }J j=1 . Initial Estimates: {\nd0j , c 0 j\n}J\nj=1 . (Often c0j = 0 \u2200 j.)\nFor t = 1 : K Repeat For j = 1 : J Repeat 1) C = [\nct1, ..., c t j\u22121, c t\u22121 j , ..., c t\u22121 J\n]\n. D = [\ndt1, ..., d t j\u22121, d t\u22121 j , ..., d t\u22121 J\n]\n.\n2) Sparse coding:\nbt = Y Tdt\u22121j \u2212 CD T dt\u22121j + c t\u22121 j (17)\nctj = min ( \u2223 \u2223H\u03bb (b t) \u2223 \u2223 , L1N ) \u2299 sign ( H\u03bb ( bt )) (18)\n3) Dictionary atom update:\nht = Y ctj \u2212DC T ctj + d t\u22121 j\n( ct\u22121j )T ctj (19)\ndtj =\n{\nht\n\u2016ht\u2016 2\n, if ctj 6= 0 v1, if c t j = 0\n(20)\nEnd End\nFig. 1. The SOUP-DIL Algorithm (or SOUP-DILLO Algorithm, due to the use of the \u21130 \u201cnorm\u201d) for Problem (P1). Superscript of t denotes the iterates in the algorithm. Although we perform the sparse coding step prior to the dictionary update step, one could also potentially switch this order. The vectors bt and ht above can be computed very efficiently via sparse operations.\nUpon substituting (13) into (11), Problem (11) simplifies to\nmax dj\ndTj Ejcj s.t. \u2016dj\u20162 = 1. (14)\nBy the Cauchy Schwarz inequality dTj Ejcj \u2264 \u2016Ejcj\u20162 for unit norm dj . Thus, a solution to (14) that achieves the value \u2016Ejcj\u20162 for the objective is\nd\u0302j =\n{\nEjcj \u2016Ejcj\u20162 , if Ejcj 6= 0\nv1, if Ejcj = 0 (15)\nObviously, any d \u2208 Rn would be a minimizer (non-unique) in (14) when Ejcj = 0. In particular d\u0302j = v1 works.\nNext, we show that Ejcj = 0 in our algorithm if and only if cj = 0. This result together with (15) immediately establishes the proposition. Since the cj used in the dictionary atom update step (11) was obtained as a minimizer in the preceding sparse coding step (2), we have the following inequality for all c \u2208 R\nN with \u2016c\u2016\u221e \u2264 L, and d\u0303j denotes the jth atom in the preceding sparse coding step: \u2225 \u2225Ej \u2212 d\u0303jcTj \u2225 \u2225 2 F + \u03bb2 \u2016cj\u20160 \u2264 \u2225 \u2225Ej \u2212 d\u0303jcT \u2225 \u2225 2 F + \u03bb2 \u2016c\u2016 0 (16)\nIf Ejcj = 0, the left hand side above simplifies to \u2016Ej\u2016 2\nF\n+ \u2016cj\u2016 2 2 +\u03bb2 \u2016cj\u20160, which is clearly minimal when cj = 0. Thus, when Ejcj = 0, we must also have cj = 0. While Propositions 1 and 2 provide the minimizers of (2) and (11) for the case of real-valued matrices/vectors in\nthe problems, these solutions are trivially extended to the complex-valued case (that may be useful in applications such as magnetic resonance imaging [9]) by using a Hermitian transpose.\nFig. 1 shows the Sum of OUter Products DIctionary Learning (SOUP-DIL) Algorithm (or SOUP-DILLO Algorithm, due to the use of the \u21130 \u201cnorm\u201d) for Problem (P1). The algorithm assumes that an initial estimate {\nd0j , c 0 j\n}J\nj=1 for the variables\nis provided. For example, the initial sparse coefficients could be set to zero, and the initial dictionary could be a known analytical dictionary such as the overcomplete DCT [5]. When ctj = 0, setting d t j = v1 in (20) in the algorithm could also be replaced with other (equivalent) settings such as dtj = d t\u22121 j or setting dtj to a random unit norm vector. All of these settings have been observed to work well in practice. A random ordering of the atom/sparse coefficient updates in Fig. 1 (i.e., random j sequence) also helps in practice (in accelerating convergence) compared to cycling in the same order 1 through J every iteration. One could also alternate several times between the sparse coding and dictionary atom update steps for each j in Fig. 1. However, this would increase computation."}, {"heading": "B. Computational Cost", "text": "For each iteration t in Fig. 1, our algorithm involves J sparse code and dictionary atom updates. The sparse coding and atom update steps involve matrix-vector products for computing bt and ht, respectively. An alternative approach to the one in Fig. 1 involves computing the matrix Etj = Y \u2212 \u2211\nk<j d t k (c t k)\nT \u2212 \u2211 k>j d t\u22121 k ( ct\u22121k )T\n(as in Propositions 1 and 2) directly at the beginning of each inner j iteration. This matrix can be updated sequentially and efficiently for each j by adding and subtracting appropriate sparse rank-one matrices. However, this alternative approach requires storing Etj \u2208 R\nn\u00d7N , which is often a large matrix for large N and n. Instead, the procedure adopted by us in Fig. 1 helps save memory usage. We now discuss the cost of each sparse coding and atom update procedure in Fig. 1.\nConsider the tth iteration and the jth inner iteration in Fig. 1, consisting of the update of the jth dictionary atom and its corresponding sparse coefficients. As in Fig. 1, let D \u2208 Rn\u00d7J be the dictionary whose columns are the current estimates of the atoms (at the start of the jth inner iteration), and let C \u2208 RN\u00d7J be the corresponding sparse coefficients matrix. (The index t on D and C is dropped to keep the notation simple.) Assume that the matrix C has \u03b1Nn nonzeros, with \u03b1 \u226a 1 typically. This translates to an average of \u03b1n non-zeros per row of C or \u03b1Nn/J non-zeros per column of C. We refer to \u03b1 as the sparsity factor of C.\nThe sparse coding step involves computing the right hand side of (17). While computing Y Tdt\u22121j in (17) requires Nn multiply-add operations, computing CDT dt\u22121j using matrixvector products requires Jn+ \u03b1Nn multiply-add operations. Computing the difference of these vectors and summing that result with the sparse ct\u22121j requires less than 2N additions. The hard thresholding-type operation (18) to obtain ctj requires at most 2N comparisons.\n5 Next, when ctj 6= 0, the dictionary atom update step requires computing the right hand side of (19). Since ctj is sparse with say rj non-zeros, computing Y ctj in (19) requires nrj multiply-add operations, and computing DCT ctj using matrixvector products requires less than Jn + \u03b1Nn multiply-add operations. The cost of the remaining operations in (19) and for normalizing ht (20) is negligible.\nThus, the net cost of the J \u2265 n inner iterations in iteration t in Fig. 1 is dominated (for N \u226b J, n) by NJn+2\u03b1mNJn+ \u03b2Nn2, where \u03b1m is the maximum sparsity factor of the estimated C\u2019s during the inner iterations, and \u03b2 is the sparsity factor of the estimated C at the end of iteration t. Thus, the cost per iteration of the block coordinate descent SOUP-DIL Algorithm is about (1 + \u03b1\u2032)NJn, with \u03b1\u2032 \u226a 1 typically. On the other hand, the proximal alternating algorithm proposed very recently2 by Bao et al. [52], [61] has a per-iteration computational cost of at least 2NJn + 6\u03b1NJn + 4\u03b1Nn2. This is clearly more computation than SOUP-DIL.\nAssuming J \u221d n, the cost per iteration of the SOUP-DIL Algorithm scales as O(Nn2). This is lower than the periteration cost of learning an n \u00d7 J synthesis dictionary D using K-SVD [28], which scales3 (assuming that the synthesis sparsity level s \u221d n and J \u221d n in K-SVD) as O(Nn3).\nAs illustrated in Section V-B, our algorithm converges in few iterations in practice. Therefore, the per-iteration computational advantages also translate to net computational advantages in practice. The low computational cost of our approach could be particularly useful for big data applications, or higher dimensional (3D or 4D) applications."}, {"heading": "IV. CONVERGENCE ANALYSIS", "text": "This section presents a convergence analysis for the proposed SOUP-DIL Algorithm for Problem (P1). Problem (P1) is highly non-convex due to the \u21130 penalty for sparsity, the unit \u21132 norm constraints on atoms of D, and the term \u2225 \u2225 \u2225Y \u2212 \u2211J\nj=1 djc T j\n\u2225 \u2225 \u2225 2\nF that is a non-convex function involving\nthe products of multiple unknown vectors. The proposed algorithm is an exact block coordinate descent procedure for Problem (P1). However, due to the high degree of nonconvexity, standard results on convergence of block coordinate descent methods (e.g., [63]) do not apply here. More recent works [64] on the convergence of block coordinate descent schemes use assumptions (such as multi-convexity) that do not hold in our setting. Here, we discuss the convergence of our algorithm to the critical points in the problem. In the following, we first present some definitions and notations, before stating the main convergence results."}, {"heading": "A. Definitions and Notations", "text": "First, we review the Fre\u0301chet sub-differential of a function [65], [66]. The norm and inner product notation in Definition 2 correspond to the euclidean \u21132 settings.\n2This method also involves more parameters than our scheme. 3When s \u221d n and J \u221d n, the per-iteration computational cost of the\nefficient implementation of K-SVD [62] also scales similarly as O(Nn3).\nDefinition 1: For a function g : Rp 7\u2192 (\u2212\u221e,+\u221e], its domain is defined as domg = {x \u2208 Rp : g(x) < +\u221e}. Function g is proper if domg is nonempty.\nDefinition 2: Let g : Rp 7\u2192 (\u2212\u221e,+\u221e] be a proper function and let x \u2208 domg. The Fre\u0301chet sub-differential of the function g at x is the following set denoted as \u2202\u0302g(x): {\nh \u2208 Rp : lim inf b\u2192x,b6=x 1 \u2016b\u2212x\u2016 (g(b)\u2212 g(x)\u2212 \u3008b\u2212 x, h\u3009) \u2265 0 }\nIf x /\u2208 domg, then \u2202\u0302g(x) , \u2205, the empty set. The subdifferential of g at x is the set \u2202g(x) defined as {\nh\u0303 \u2208 Rp : \u2203xk \u2192 x, g(xk) \u2192 g(x), hk \u2208 \u2202\u0302g(xk) \u2192 h\u0303 } .\nA necessary condition for x \u2208 Rp to be a minimizer of the function g is that x is a critical point of g, i.e., 0 \u2208 \u2202g(x). Critical points are considered to be \u201cgeneralized stationary points\u201d [65].\nWe say that a sequence {zt} \u2282 Rp has an accumulation point z, if there is a subsequence that converges to z.\nThe constraints \u2016dj\u20162 = 1, 1 \u2264 j \u2264 J , in (P1) can instead be added as penalties in the cost by using barrier functions \u03c7(dj) (taking the value +\u221e when the norm constraint is violated, and is zero otherwise). The constraints \u2016cj\u2016\u221e \u2264 L, 1 \u2264 j \u2264 J , can also be similarly replaced with barrier penalties \u03c8(cj). Then, we rewrite (P1) in unconstrained form with the following objective:\nf(C,D) = f (c1, c2, ..., cJ , d1, d2, ..., dJ ) = \u03bb 2\nJ \u2211\nj=1\n\u2016cj\u20160\n+ \u2225 \u2225 \u2225Y \u2212 \u2211J\nj=1 djc T j\n\u2225 \u2225 \u2225 2\nF +\nJ \u2211\nj=1\n\u03c7(dj) +\nJ \u2211\nj=1\n\u03c8(cj) (21)\nFor the SOUP-DIL Algorithm, the iterates computed in the tth outer iteration are denoted by the 2J-tuple (ct1, d t 1, c t 2, d t 2, ..., c t J , d t J ), or alternatively by the pair of matrices (Ct, Dt)."}, {"heading": "B. Main Results", "text": "Assume that the initial (C0, D0) satisfies the constraints in (P1). We then have the following simple monotonicity and consistency result.\nTheorem 1: Let {Ct, Dt} denote the iterate sequence generated by the SOUP-DIL Algorithm with training data Y \u2208 R\nn\u00d7N and initial (C0, D0). Then, the objective sequence {f t} with f t , f (Ct, Dt) is monotone decreasing, and converges to a finite value, say f\u2217. Moreover, the iterate sequence {Ct, Dt} is bounded, and all its accumulation points are equivalent in the sense that they achieve the exact same value f\u2217 of the objective.\nProof: See Appendix A. Theorem 1 establishes that for each initial point (C0, D0), the bounded iterate sequence in the SOUP-DIL Algorithm is such that all its accumulation points achieve the same value f\u2217 of the objective. They are equivalent in that sense. The value of f\u2217 could vary with different initalizations. We thus have the following corollary of Theorem 1 that holds because\n6 the distance between a bounded sequence and its (non-empty and compact) set of accumulation points converges to zero.\nCorollary 1: For each (C0, D0), the iterate sequence in the SOUP-DIL Algorithm converges to an equivalence class of accumulation points.\nThe following Theorem 2 considers a very special case of SOUP dictionary learning, where the dictionary has a single atom. In this case, the SOUP learning Problem (P1) is the problem of obtaining a sparse rank-one approximation of the training matrix Y . In this case, Theorem 2 establishes that the iterates in the algorithm converge to the set of critical points (i.e., the distance between the iterates and the set converges to zero) of the objective f .\nTheorem 2: Consider the SOUP-DIL Algorithm with J = 1. Let {ct, dt} denote the bounded iterate sequence generated by the algorithm in this case with data Y \u2208 Rn\u00d7N and initial (c0, d0). Then, every accumulation point of the iterate sequence is a critical point of the objective f , i.e., the iterates converge to the set of critical points of f .\nProof: See Appendix B. For the general case (J 6= 1), we have the following result. Theorem 3: Let {Ct, Dt} denote the bounded iterate sequence generated by the SOUP-DIL Algorithm with training data Y \u2208 Rn\u00d7N and initial (C0, D0). Suppose each accumulation point (C,D) of the iterate sequence is such that the matrix B with columns bj = ETj dj and Ej = Y \u2212DC T + djc T j , has no entry with magnitude \u03bb. Then every accumulation point of the iterate sequence is a critical point of the objective f(C,D). Moreover, the two sequences with terms \u2225 \u2225Dt \u2212Dt\u22121 \u2225 \u2225\nF and \u2225 \u2225Ct \u2212 Ct\u22121 \u2225 \u2225\nF respectively, both converge to zero.\nProof: See Appendix C. Theorem 3 establishes that the iterates in SOUP-DIL converge to the set of critical points of f(C,D). For each initial (C0, D0), the iterate sequence in the algorithm converges (using Corollary 1) to an equivalence class of critical points of f . Theorem 3 also establishes that \u2225 \u2225Dt \u2212Dt\u22121 \u2225 \u2225\nF \u2192 0\nand \u2225 \u2225Ct \u2212 Ct\u22121 \u2225 \u2225\nF \u2192 0, thereby implying that the sparse\napproximation to the training data Zt = Dt (Ct)T is such that \u2225 \u2225Zt \u2212 Zt\u22121 \u2225 \u2225\nF \u2192 0. These are necessary but not sufficient\nconditions for the convergence of the entire sequences {Dt}, {Ct}, and {Zt}. The assumption on the entries of the matrix B in Theorem 3 is equivalent to assuming that for every 1 \u2264 j \u2264 J , there is a unique minimizer of f with respect to cj with all other variables fixed to their values in the accumulation point (C,D).\nAlthough Theorem 3 uses a uniqueness condition, the following conjecture postulates that provided the following Assumption 1 (that uses a probabilistic model for the data) holds, the uniqueness condition holds with probability 1, i.e., the probability of a tie in assigning sparse codes is zero.\nAssumption 1. The training signals yi \u2208 Rn for 1 \u2264 i \u2264 N , are drawn independently from an absolutely continuous probability measure over the n-dimensional ball S , {y \u2208 Rn : \u2016y\u2016\n2 \u2264 \u03b20} for some \u03b20 > 0.\nConjecture 1: Let Assumption 1 hold. Then, with probability 1, every accumulation point (C,D) of the iterate sequence in the SOUP-DIL Algorithm is such that for each 1 \u2264 j \u2264\nJ , the minimizer of f(c1, ..., cj\u22121, c\u0303j , cj+1, ..., cJ , d1, ..., dJ ) with respect to c\u0303j is unique.\nIf Conjecture 1 holds, then every accumulation point of the iterate sequence in the SOUP-DIL Algorithm is immediately a critical point of f(C,D) with probability 1."}, {"heading": "V. NUMERICAL EXPERIMENTS", "text": ""}, {"heading": "A. Framework", "text": "This section presents numerical results illustrating the practical convergence behavior of the proposed algorithm, as well as its usefulness in sparse signal representation and image denoising. For demonstrating the convergence behavior and quality of sparse signal representations, we consider data formed using vectorized 2D patches of natural images. The SOUP-DIL Algorithm is used to learn sparse representations for such data, and we study the signal representation ability of the proposed algorithm at various values of \u03bb (i.e., various sparsities). We also present results for patch-based image denoising using Problem (P1).\nOur dictionary learning implementation was coded in Matlab version R2015a. We compare the performance of dictionaries learned using our method to those learned using the classical K-SVD4 method [5], [28]. For K-SVD learning and denoising [5], [28], we consider the original Matlab implementation of the methods available from Michael Elad\u2019s website [68]. A fast version5 of K-SVD [62] that also uses MEX/C implementations of sparse coding and some sub-steps of dictionary update, is publicly available [69]. The Matlab implementation of our method is not currently optimized for efficiency. Therefore, we compare the runtimes achieved by our unoptimized Matlab implementation to both the original Matlab and the efficient (partial MEX/C) implementations of K-SVD for a fair and instructive comparison. For KSVD, we used the built-in parameter settings of the author\u2019s implementations, unless otherwise stated. All computations were performed with an Intel Xeon CPU X3230 at 2.66 GHz and 8 GB memory, employing a 64-bit Windows 7 operating system.\nFor sparse representation of data Y , we use the normalized sparse representation error (NSRE) \u2225 \u2225Y \u2212DCT \u2225 \u2225\nF / \u2016Y \u2016F to\nmeasure the performance of the learned dictonaries. For image denoising, similar to prior work, we measure the peak-signalto-noise ratio (PSNR) computed between the true noiseless reference and the noisy or denoised images."}, {"heading": "B. Convergence Experiment", "text": "To study the convergence behavior of the proposed SOUPDIL Algorithm, we extracted 3 \u00d7 104 patches of size 8 \u00d7 8 from randomly chosen locations in the images Barbara, Boat,\n4The K-SVD method is a highly popular dictionary learning scheme that has been applied to many image processing applications including denoising [5], [6] and MR image reconstruction [9]. Mairal et al. [33] proposed a nonlocal method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D [67] denoising method. Similar extensions to our proposed method for denoising and other applications are left for future work.\n5This version [69] was observed to typically provide similar quality of results in our experiments as [68].\n7 Barbara Boat Couple\nHill Lena\nFig. 2. The 512\u00d7512 standard images used in our experiments. The images are Barbara, Boat, Couple, Hill, and Lena.\nand Hill, shown in Fig. 2. The SOUP-DIL Algorithm for (P1) was then used to learn a 64 \u00d7 256 overcomplete dictionary, with \u03bb = 69. The algorithm is initialized as mentioned in Section III-A. Specifically, the initial estimate for C is an allzero matrix, and the initial estimate for D is the overcomplete DCT [5], [68].\nFig. 3 illustrates the convergence behavior of SOUP-DIL. The objective in our method (Fig. 3(a)) converged monotonically and quickly over the iterations. Fig. 3(b) shows the normalized sparse representation error and sparsity factor (for C), both expressed as percentages. Both these components of the objective converged quickly for the algorithm, and the NSRE improved by 1 dB beyond the first iteration, indicating the success of the SOUP-DIL approach in representing data using a small number of non-zero coefficients (sparsity factor of 3.14% at convergence).\nImportantly, both the quantities \u2225 \u2225Dt \u2212Dt\u22121 \u2225 \u2225\nF (Fig. 3(c))\nand \u2225 \u2225Ct \u2212 Ct\u22121 \u2225 \u2225\nF (Fig. 3(d)) converge towards 0, as pre-\ndicted by Theorem 3. This implies that \u2225 \u2225Zt \u2212 Zt\u22121 \u2225 \u2225\nF with\nZt = Dt (Ct) T converges towards zero too. The above results are indicative (are necessary but not sufficient conditions) of the convergence of the entire sequences {Dt}, {Ct}, and {Zt} for our algorithm in practice. In contrast, Bao et al. [52] showed that the distance between successive iterates may not converge to zero for popular algorithms such as K-SVD."}, {"heading": "C. Sparse Representation of Data", "text": "The second experiment worked with the same data as in Section V-B and learned dictionaries of size 64 \u00d7 256 for various choices of the parameter \u03bb in (P1) (i.e., corresponding to a variety of solution sparsity levels). We measure the quality of the learned data approximations DCT using the NSRE metric. We compare the NSRE values achieved by our algorithm to those achieved by the K-SVD dictionary learning scheme [28] for the same data. The K-SVD method was executed for several choices of sparsity (number of nonzeros) of the columns of CT . The \u03bb values for (P1) were chosen so as to achieve similar average column sparsity levels\n1 20 40 60 80 4.5\n4.6\n4.7\n4.8\n4.9\n5\n5.1\n5.2x 10 8\nIteration Number\nO bj\nec tiv\ne F\nun ct\nio n\n3.1\n3.15\n3.2\n3.25\n3.3\n3.35\nS pa\nrs ity\n( %\n)\n1 20 40 60 80 7.2\n7.4\n7.6\n7.8\n8\n8.2\nN S\nR E\n( %\n)\nIteration Number\nSparsity (%) NSRE (%)\n(a) (b)\n1 10 80 10\n\u22122\n10 \u22121\n10 0\n10 1\nIteration Number\n\u2225 \u2225\nD t \u2212 D\nt \u2212 1 \u2225 \u2225\nF\n1 10 80 10\n\u22124\n10 \u22123\n10 \u22122\n10 \u22121\n10 0\nIteration Number\n\u2225 \u2225\nC t \u2212 C\nt \u2212 1 \u2225 \u2225\nF / \u2016Y\n\u2016 F\n(c) (d)\nin CT as K-SVD. Both the SOUP-DIL Algorithm and K-SVD were initialized with the same overcomplete DCT dictionary in our experiment and both methods ran for 10 iterations.\nFig. 4 shows the behavior of the SOUP-DIL and K-SVD algorithms for average column sparsity levels of CT ranging from about 2% to 20%. As expected, the NSRE values for the SOUP-DIL Algorithm decreased monotonically (Fig. 4(a)) when the average column sparsity level in CT increased\n8 (i.e., as \u03bb decreased). Importantly, the SOUP-DIL Algorithm provided better data representations (Fig. 4(b)) than K-SVD at the various tested sparsity levels. Large improvements of about 14 dB and 1 dB are observed at low and mid sparsity levels, respectively.\nFinally, Fig. 4(c) compares the runtimes of our method to those of the unoptimized Matlab implementation of K-SVD [68] as well as the efficient (partial) MEX/C implementation [69] of K-SVD demonstrating large speedups of 40-50 times for SOUP-DIL over the first K-SVD implementation (at most sparsities), while the runtimes for the SOUP-DIL method are about the same as those of the second K-SVD implementation. Since these results were obtained using only an unoptimized Matlab implementation of SOUP-DIL, we expect significant speed-ups for our scheme with code optimization or C/C++ implementations.\nD. Image Denoising\nIn image denoising, the goal is to recover an estimate of an image x \u2208 RM (2D image represented as a vector) from its corrupted measurements y = x+ h, where h is the noise. For example, the entries of h may be i.i.d. Gaussian with zero mean and standard deviation \u03c3.\nTo perform image denoising using (P1), we first extract all the overlapping patches (with maximum overlap) of the noisy image y, and construct the training set Y \u2208 Rn\u00d7N as a matrix whose columns are those noisy patches. We then use Problem (P1) to learn a dictionary and sparse codes for Y . To obtain the denoised image estimate, we then solve the following least squares problem, where D\u0302 and \u03b1\u0302j denote the learned dictionary and patch sparse codes obtained from the noisy patches, and Pj is an operator that extracts a patch as a vector:\nmin x\nN \u2211\nj=1\n\u2225 \u2225Pjx\u2212 D\u0302\u03b1\u0302j \u2225 \u2225 2 2 + \u03bd \u2016x\u2212 y\u20162 2 (22)\nThe optimal x\u0302 is obtained [5] by summing together the denoised patch estimates D\u0302\u03b1\u0302j at their respective 2D locations, and computing a weighted average between this result and the noisy image.\nK-SVD based denoising [5] involves a similar methodology as described above for (P1), but differs in the dictionary learning procedure, where the \u21130 \u201cnorms\u201d of the sparse codes are minimized so that a fitting constraint or error constraint of \u2225 \u2225Pjy \u2212 D\u0302\u03b1\u0302j \u2225 \u2225 2\n2 \u2264 \u01eb is met for representing the noisy patches.\nIn particular, when the noise is i.i.d. Gaussian, \u01eb = nC2\u03c32 is used, with C > 1 a constant. Such a constraint serves as a strong prior [5], [6], and is a key reason for the denoising capability of K-SVD. Hence, in our method based on (P1), we set \u03bb \u221d \u03c3 (noise standard deviation) in (P1) during learning, and once the dictionary D\u0302 is learned from noisy patches, we estimate the patch sparse codes \u03b1\u0302j using a single pass (over the noisy patches) of orthogonal matching pursuit (OMP) by employing an error constraint criterion like in K-SVD. Although we only use information on the noise statistics in a sub-optimal way for (P1), we still show good denoising performance (vis-a-vis K-SVD) in the following with this\nImage \u03c3 Noisy O-DCT K-SVD SOUP-DIL\nCouple\n5 34.16 37.25 37.29 37.28 10 28.11 33.40 33.49 33.50 20 22.11 29.71 30.01 29.99 25 20.17 28.53 28.88 28.92 30 18.58 27.53 27.88 27.97\n100 8.13 22.59 22.58 22.71\nBarbara 5 34.15 37.94 38.08 38.04 10 28.14 33.96 34.43 34.37 20 22.13 29.95 30.83 30.79 25 20.17 28.68 29.63 29.64 30 18.59 27.62 28.54 28.63\n100 8.11 21.87 21.87 21.97\nBoat\n5 34.15 37.09 37.21 37.16 10 28.13 33.43 33.62 33.60 20 22.10 29.92 30.36 30.37 25 20.17 28.79 29.28 29.30 30 18.60 27.93 28.41 28.43\n100 8.13 22.79 22.81 22.96\nHill\n5 34.15 37.02 37.08 37.05 10 28.14 33.26 33.45 33.44 20 22.10 29.85 30.17 30.20 25 20.18 28.89 29.23 29.31 30 18.57 28.14 28.43 28.56\n100 8.16 24.00 23.98 24.03\nLena\n5 34.16 38.52 38.62 38.55 10 28.12 35.30 35.48 35.47 20 22.11 32.02 32.40 32.40 25 20.18 30.89 31.32 31.32 30 18.59 29.98 30.41 30.46\n100 8.14 24.45 24.51 24.63\nAvg.\n5 34.16 37.56 37.66 37.61 10 28.13 33.87 34.09 34.07 20 22.11 30.29 30.75 30.75 25 20.17 29.16 29.67 29.70 30 18.58 28.24 28.74 28.81\n100 8.13 23.14 23.15 23.26\nTABLE I PSNR VALUES IN DECIBELS FOR DENOISING WITH OVERCOMPLETE DCT (O-DCT), K-SVD [5], AND SOUP-DIL. ALL DICTIONARIES HAVE SIZE 64\u00d7 256. THE PSNR VALUES OF THE NOISY IMAGES (NOISY) ARE ALSO SHOWN. THE LAST BLOCK OF ENTRIES IN THE TABLE CORRESPONDS TO THE AVERAGE (OVER THE IMAGES) PSNR VALUES.\napproach. Note that for both our method and for K-SVD [5], [68], the means of the patches are removed prior to learning and added back afterwards to obtain the denoised estimates.\nFor the denoising experiments, we work with the images Couple, Barbara, Boat, Hill, and Lena in Fig. 2, and simulate i.i.d. Gaussian noise at six different noise levels (\u03c3 = 5, 10, 20, 25, 30, 100) for each of the images. We then denoise these images using both the SOUP-DIL denoising method outlined above and K-SVD [5], [68]. SOUP-DIL denoising uses patches of size 8 \u00d7 8, a 64 \u00d7 256 overcomplete dictionary, \u03bb = 5\u03c3, \u03bd = 20/\u03c3 (in (22)), an overcomplete DCT initial dictionary in learning, and 10 iterations of the learning algorithm. These parameter settings6 were found to work well for our method.\nTable I lists the denoising PSNRs obtained by the SOUP-\n6The built-in parameter settings in the K-SVD denoising implementation [5], [68] are very similar except that K-SVD uses a subset of the patches for training. We found that the denoising performance of K-SVD changes by at most few hundredths of a dB when all the patches are used in learning. However, this setting also leads to increased runtimes.\n9 (a) (b)\nFig. 5. Learned dictionaries for Barbara at \u03c3 = 20: (a) SOUP dictionary; and (b) K-SVD dictionary. The columns of the dictionaries are shown as patches.\nDIL denoising method, along with the PSNRs obtained by K-SVD. For comparison, we also list the denoising PSNRs obtained by employing the overcomplete DCT dictionary for denoising. In the latter case, the same strategy (and parameter settings [68]) as used by K-SVD based denoising is adopted but while skipping the learning process.\nThe various methods denoise about the same in Table I for a low noise level of \u03c3 = 5. For \u03c3 > 5, the SOUP-DIL method denoises about 0.4 dB better on the average than the overcomplete DCT. While the SOUP-DIL method and K-SVD denoise about the same at low and mid noise levels, the SOUPDIL scheme performs about 0.1 dB better on average at higher noise levels such as \u03c3 = 30 or 100 in Table I. Importantly, SOUP-DIL based denoising is highly efficient and the learning procedure has good convergence properties. We have observed similar effects as in Section V-C for the runtimes of SOUPDIL denoising vis-a-vis the unoptimized Matlab [68] or the efficient (partial MEX/C) [69] implementations of the K-SVD approach.\nFig. 5 shows the dictionaries learned from noisy patches using the SOUP-DIL method and K-SVD for the image Barbara at \u03c3 = 20. Both dictionaries show frequency and textural features that are specific to the image Barbara. By learning such image-specific features, the SOUP-DIL method (and K-SVD) easily outperforms fixed dictionaries such as the overcomplete DCT in denoising."}, {"heading": "VI. CONCLUSIONS", "text": "This paper proposed a fast method for synthesis dictionary learning. The training data set is approximated by a sum of sparse rank-one matrices. The proposed formulation learns the left and right singular vectors of these rank-one matrices using an \u21130 penalty to enforce sparsity. We adopted a block coordinate descent method to efficiently update the unknown sparse codes and dictionary atoms in the problem. In particular, the sparse coding step in our algorithm is performed cheaply by truncated hard-thresholding, and the dictionary atom update involves a single sparse vector-matrix product that is computed efficiently. A convergence analysis was presented for the proposed block coordinate descent algorithm for a highly nonconvex problem. The proposed approach had comparable or superior performance and significant speed-ups over the classical K-SVD method [5], [28] in sparse signal representation\nand image denoising. The usefulness of our method in other applications such as blind compressed sensing [70] or other inverse problems in imaging merits further study. Extensions of the proposed method for online learning [8] are also of potential interest."}, {"heading": "APPENDIX A PROOF OF THEOREM 1", "text": "First, we discuss the convergence of the objective sequence. At every iteration t and inner iteration j in Fig. 1, we solve the sparse coding and dictionary atom update subproblems exactly. Thus, we have the following two inequalities:\nf ( ct1, ..., c t j\u22121, c t j , c t\u22121 j+1, ..., c t\u22121 J , d t 1, ..., d t j\u22121, d t\u22121 j , ..., d t\u22121 J ) \u2264\nf ( ct1, ..., c t j\u22121, c t\u22121 j , c t\u22121 j+1, ..., c t\u22121 J , d t 1, ..., d t j\u22121, d t\u22121 j , ..., d t\u22121 J ) f (\nct1, ..., c t j , c t\u22121 j+1, ..., c t\u22121 J , d t 1, ..., d t j\u22121, d t j , d t\u22121 j+1, ..., d t\u22121 J\n)\n\u2264\nf ( ct1, ..., c t j , c t\u22121 j+1, ..., c t\u22121 J , d t 1, ..., d t j\u22121, d t\u22121 j , d t\u22121 j+1, ..., d t\u22121 J )\nCombining these inequalities for the tth iteration and for each 1 \u2264 j \u2264 J yields f(Ct, Dt) \u2264 f(Ct\u22121, Dt\u22121). Thus, the sequence {f(Ct, Dt)} is monotone decreasing. Because it is lower bounded (by 0), it converges to a finite value f\u2217 = f\u2217(C0, D0) (that may depend on the initial conditions).\nNext, the boundedness of the {Dt} and {Ct} sequences is obvious from the constraints in Problem (P1), so the accumulation points of the iterates form a non-empty and compact set.\nFinally, we show that each accumulation point achieves the same value f\u2217 of the objective. Consider a subsequence {Cqt , Dqt} of the iterate sequence that converges to the accumulation point (C\u2217, D\u2217). Because \u2225\n\u2225dqtj \u2225 \u2225 2 = 1 and \u2225 \u2225cqtj \u2225 \u2225 \u221e \u2264 L for 1 \u2264 j \u2264 J and every t, therefore, due to the continuity of the norms, we also have \u2225\n\u2225d\u2217j \u2225 \u2225 2 = 1 and \u2225 \u2225c\u2217j \u2225 \u2225 \u221e \u2264 L for all j. Thus, the barrier functions in (21) are inactive for the subsequence and its limit, i.e.,\n\u03c7(d\u2217j ) = 0, \u03c8(c \u2217 j ) = 0 \u2200 j. (23)\nThe formula for ctj in (18) implies that it cannot have any non-zero entry of magnitude less than \u03bb. Because {\ncqtj }\nconverges to c\u2217j entry-wise, therefore, for any entry of c \u2217 j that is zero, the corresponding entry in cqtj = 0 for large enough t. Clearly, wherever c\u2217j is non-zero, the corresponding entry in cqtj stays non-zero for large enough t values. Thus,\nlim t\u2192\u221e\n\u2225 \u2225cqtj \u2225 \u2225\n0 =\n\u2225 \u2225c\u2217j \u2225 \u2225 0 \u2200 j (24)\nand the convergence in (24) happens in a finite number of iterations. We then easily have the following result:\nlim t\u2192\u221e f(Cqt , Dqt) = lim t\u2192\u221e\n\u2225 \u2225 \u2225Y \u2212Dqt (Cqt)T \u2225 \u2225 \u2225 2\nF\n+ \u03bb2 J \u2211\nj=1\nlim t\u2192\u221e\n\u2225 \u2225cqtj \u2225 \u2225 0 = \u03bb2\nJ \u2211\nj=1\n\u2225 \u2225c\u2217j \u2225 \u2225 0 + \u2225 \u2225 \u2225Y \u2212D\u2217 (C\u2217)T \u2225 \u2225 \u2225 2\nF\nThe right hand side above coincides with f(C\u2217, D\u2217). Since the objective sequence converges to f\u2217, therefore, f(C\u2217, D\u2217) = limt\u2192\u221e f(C qt , Dqt) = f\u2217.\n10"}, {"heading": "APPENDIX B PROOF OF THEOREM 2", "text": "First, consider a subsequence {cqt , dqt} of the iterate sequence that converges to the accumulation point (c\u2217, d\u2217). Because of the optimality of dqt in the dictionary atom update step (11) of the qtth iteration, we have the following inequality for all d \u2208 Rn with \u2016d\u2016\n2 = 1:\n\u2225 \u2225 \u2225Y \u2212 dqt (cqt)T \u2225 \u2225 \u2225 2\nF +\u03bb2 \u2016cqt\u2016 0 \u2264\n\u2225 \u2225 \u2225Y \u2212 d (cqt)T \u2225 \u2225 \u2225 2\nF +\u03bb2 \u2016cqt\u2016 0 .\n(25) Taking the limit t \u2192 \u221e in (25) and using (24) leads to the following result that holds for each feasible d: \u2225 \u2225\n\u2225Y \u2212 d\u2217 (c\u2217)T \u2225 \u2225 \u2225 2\nF + \u03bb2 \u2016c\u2217\u2016 0 \u2264\n\u2225 \u2225 \u2225Y \u2212 d (c\u2217)T \u2225 \u2225 \u2225 2\nF + \u03bb2 \u2016c\u2217\u2016 0 .\nThe result implies that d\u2217 (which has unit \u21132 norm) is in fact a global minimizer of f(c\u2217, d) with respect to d, i.e., 0 \u2208 \u2202fd (c\n\u2217, d\u2217). Next, consider a convergent subsequence { cqnt+1 }\nof the bounded sequence { cqt+1 }\nthat has a limit c\u2217\u2217. Because of the optimality of cqnt+1 in the sparse coding step (2) of iteration qnt + 1, we have the following inequality for all c \u2208 R\nN and for f(c, d) = \u2225 \u2225Y \u2212 dcT \u2225 \u2225 2\nF +\u03bb2 \u2016c\u2016 0 +\u03c7(d) + \u03c8(c):\nf(cqnt+1, dqnt ) \u2264 f(c, dqnt ) (26)\nAs in (25), taking the limit t \u2192 \u221e above and using (24) and (23) yields the following result that holds for each c:\nf(c\u2217\u2217, d\u2217) \u2264 f(c, d\u2217) (27)\nThis implies that c\u2217\u2217 (that has \u2016c\u2217\u2217\u2016\u221e \u2264 L) is a global minimizer of f(c, d\u2217) with respect to c achieving the objective value f(c\u2217\u2217, d\u2217).\nSince f(cqnt+1, dqnt+1) \u2264 f(cqnt+1, dqnt ) \u2264 f(cqnt , cqnt ), and by Theorem 1, the first and last terms in this inequality converge to f\u2217, we therefore have that f(c\u2217\u2217, d\u2217) = f\u2217 too. By Theorem 1, f(c\u2217, d\u2217) = f\u2217 holds for the accumulation point (c\u2217, d\u2217) of the subsequence {cqt , dqt}, implying f(c\u2217, d\u2217) = f(c\u2217\u2217, d\u2217). Combining this with (27), it is clear that c\u2217 is also a global minimizer of f(c, d\u2217) with respect to c, i.e., 0 \u2208 \u2202fc (c\u2217, d\u2217).\nFinally, (see Proposition 3 in [71]) the subdifferential \u2202f at (c\u2217, d\u2217) satisfies \u2202f (c\u2217, d\u2217) = \u2202fc (c\u2217, d\u2217) \u00d7 \u2202fd (c\u2217, d\u2217). Since 0 \u2208 \u2202fd (c\u2217, d\u2217) and 0 \u2208 \u2202fc (c\u2217, d\u2217), we therefore have that 0 \u2208 \u2202f (c\u2217, d\u2217). Thus, when J = 1 in the SOUPDIL Algorithm, each accumulation point in the algorithm is a critical point of the objective f ."}, {"heading": "APPENDIX C PROOF OF THEOREM 3", "text": ""}, {"heading": "A. Critical Point Property", "text": "Consider a convergent subsequence {Cqt , Dqt} of the iterate sequence in the SOUP-DIL Algorithm that converges to (C\u2217, D\u2217). Let { Cqnt+1, Dqnt+1 }\nbe a convergent subsequence of the bounded { Cqt+1, Dqt+1 }\n, with limit (C\u2217\u2217, D\u2217\u2217). For each iteration t and inner iteration j in the algorithm, define the matrix Etj , Y \u2212 \u2211 k<j d t k (c t k) T \u2212 \u2211\nk>j d t\u22121 k\n( ct\u22121k )T . For the accumulation point (C\u2217, D\u2217),\ndefine E\u2217j , Y \u2212 D \u2217 (C\u2217)T +d\u2217j\n( c\u2217j )T\n. In this proof, for simplicity, we denote the objective f (21) in the jth sparse coding step of iteration t (Fig. 1) as\nf ( Etj , cj , d t\u22121 j ) , \u2225 \u2225Etj \u2212 d t\u22121 j c T j \u2225 \u2225 2 F + \u03bb2\n\u2211\nk<j\n\u2225 \u2225ctk \u2225 \u2225\n0\n+ \u03bb2 \u2211\nk>j\n\u2225 \u2225ct\u22121k \u2225 \u2225 0 + \u03bb2 \u2016cj\u20160 + \u03c8(cj) (28)\nAll but the jth atom and sparse vector cj are represented via Etj on the left hand side in this notation. The objective that is minimized in the dictionary atom update step is similarly denoted as f (\nEtj , c t j, dj\n)\nwith\nf ( Etj , c t j , dj ) ,\n\u2225 \u2225 \u2225Etj \u2212 dj ( ctj )T \u2225 \u2225 \u2225 2\nF + \u03bb2\n\u2211\nk\u2264j\n\u2225 \u2225ctk \u2225 \u2225\n0\n+ \u03bb2 \u2211\nk>j\n\u2225 \u2225ct\u22121k \u2225 \u2225 0 + \u03c7(dj). (29)\nFinally, the functions f ( E\u2217j , cj, d \u2217 j ) and f ( E\u2217j , c \u2217 j , dj )\nare defined in a similar way with respect to the accumulation point (C\u2217, D\u2217).\nTo establish the critical point property of (C\u2217, D\u2217), we first show the partial global optimality of each column of the matrices C\u2217 and D\u2217 for f . By partial global optimality, we mean that each column of C\u2217 or D\u2217 is a global minimizer of f , when all other variables are kept fixed to the values in (C\u2217, D\u2217). First, for j = 1 and iteration qnt + 1, we have the following result for the sparse coding step for all c1 \u2208 RN :\nf ( E qnt+1\n1 , c qnt+1 1 , d qnt 1\n) \u2264 f ( E qnt+1\n1 , c1, d qnt 1\n)\n(30)\nTaking the limit t \u2192 \u221e above and using (24) to obtain limits of \u21130 terms in the cost (28), and using (23), and the fact that E qnt+1\n1 \u2192 E \u2217 1 , we have\nf (E\u22171 , c \u2217\u2217 1 , d \u2217 1) \u2264 f (E \u2217 1 , c1, d \u2217 1) \u2200 c1 \u2208 R N . (31)\nThis means that c\u2217\u22171 is a minimizer of f with all other variables fixed to their values in (C\u2217, D\u2217). Because of the (uniqueness) assumption in the theorem, we have\nc\u2217\u22171 = argmin c1 f (E\u22171 , c1, d \u2217 1) (32)\nFurthermore, because of the equivalence of accumulation points, f (E\u22171 , c \u2217\u2217 1 , d \u2217 1) = f (E \u2217 1 , c \u2217 1, d \u2217 1) = f\n\u2217 holds. This result together with (32) implies that c\u2217\u22171 = c \u2217 1 and thus\nc\u22171 = argmin c1 f (E\u22171 , c1, d \u2217 1) (33)\nTherefore, c\u22171 is a partial global minimizer of f . Next, for the first dictionary atom update step (j = 1) in iteration qnt + 1, we have the following for all d1 \u2208 R n:\nf ( E qnt+1\n1 , c qnt+1 1 , d qnt+1 1\n) \u2264 f ( E qnt+1\n1 , c qnt+1 1 , d1\n)\n(34)\nJust like in (30), upon taking the limit t \u2192 \u221e above and using c\u2217\u22171 = c \u2217 1, we get\nf (E\u22171 , c \u2217 1, d \u2217\u2217 1 ) \u2264 f (E \u2217 1 , c \u2217 1, d1) \u2200 d1 \u2208 R n. (35)\nThus, d\u2217\u22171 is a minimizer of the cost f (E \u2217 1 , c \u2217 1, d1) with respect to d1. Because of the equivalence of accumulation points, we\n11\nhave f (E\u22171 , c \u2217 1, d \u2217\u2217 1 ) = f (E \u2217 1 , c \u2217 1, d \u2217 1) = f \u2217. This implies that d\u22171 is also a partial global minimizer of f in (35) satisfying\nd\u22171 \u2208 argmin d1 f (E\u22171 , c \u2217 1, d1) (36)\nBy Proposition 2, the minimizer of the dictionary atom update cost is unique as long as the corresponding sparse code (in (33)) is non-zero. Therefore, d\u2217\u22171 = d \u2217 1 is the unique minimizer in (36), except when c\u22171 = 0. When c\u22171 = 0, we use (24) to conclude that c qt 1 = 0 for all sufficiently large t values. Since c\u2217\u22171 = c \u2217 1, we must also have that c qnt+1\n1 = 0 for all large enough t. Therefore, for sufficiently large t, dqt1 and d qnt+1\n1 are the minimizers of dictionary atom update steps (11), wherein the corresponding sparse coefficients cqt1 and c qnt+1\n1 are zero, implying that dqt1 = d qnt+1\n1 = v1 (Fig. 1) for all sufficiently large t. Thus, the limits satisfy d\u22171 = d \u2217\u2217 1 = v1. Therefore, d \u2217\u2217 1 = d \u2217 1 holds, even when c\u22171 = 0. Therefore, for j = 1,\nd\u2217\u22171 = d \u2217 1, c \u2217\u2217 1 = c \u2217 1. (37)\nNext, we repeat the above procedure by considering first the sparse coding step and then the dictionary update step for j = 2 and iteration qnt +1. For j = 2, we consider the matrix\nE qnt+1 2 = Y \u2212 \u2211\nk>2\nd qnt k ( c qnt k )T \u2212 d qnt+1 1\n(\nc qnt+1\n1\n)T\nIt follows from (37) that E qnt+1\n2 \u2192 E \u2217 2 as t \u2192 \u221e. Then,\nby repeating the steps (30) - (37), we can easily show that c\u22172 and d \u2217 2 are each partial global minimizers of f when all other variables are fixed to their values in (C\u2217, D\u2217). Moreover, c\u2217\u22172 = c \u2217 2 and d \u2217\u2217 2 = d \u2217 2. Similar such arguments can be made sequentially for all other values of j until j = J . Finally, the partial global optimality of each column of C\u2217 and D\u2217 for the objective f implies that 0 \u2208 \u2202f (C\u2217, D\u2217), i.e., (C\u2217, D\u2217) is a critical point of the function f ."}, {"heading": "B. Convergence of the Difference between Successive Iterates", "text": "Consider the sequence {at} whose elements are at , \u2225 \u2225Dt \u2212Dt\u22121 \u2225 \u2225\nF . Clearly, this sequence is bounded. We will\nshow that every convergent subsequence of this sequence converges to zero, thereby implying that zero is the only accumulation point, i.e., {at} converges to 0. A similar argument establishes that \u2225 \u2225Ct \u2212 Ct\u22121 \u2225 \u2225\nF \u2192 0 as t \u2192 \u221e.\nConsider a convergent subsequence {aqt} of the sequence {at}. The bounded sequence {( Cqt\u22121, Dqt\u22121, Cqt , Dqt )} (whose elements are formed by pairing successive elements of the iterate sequence) must have a convergent subsequence {(\nCqnt\u22121, Dqnt\u22121, Cqnt , Dqnt )}\nthat converges to a point (C\u2217, D\u2217, C\u2217\u2217, D\u2217\u2217). Based on the results in Appendix C-A, we then have d\u2217\u2217j = d \u2217 j (and c \u2217\u2217 j = c \u2217 j ) for each 1 \u2264 j \u2264 J , or D\u2217\u2217 = D\u2217. (38)\nThus, clearly aqnt \u2192 0 as t \u2192 \u221e. Since, {aqnt } is a subsequence of the convergent {aqt}, we must have that {aqt} converges to zero too. Thus, we have shown that zero is the limit of any arbitrary convergent subsequence of {at}."}], "references": [{"title": "An overview of JPEG-2000", "author": ["M.W. Marcellin", "M.J. Gormish", "A. Bilgin", "M.P. Boliek"], "venue": "Proc. Data Compression Conf., 2000, pp. 523\u2013541.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Analysis versus synthesis in signal priors", "author": ["M. Elad", "P. Milanfar", "R. Rubinstein"], "venue": "Inverse Problems, vol. 23, no. 3, pp. 947\u2013968, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Hadamard transform image coding", "author": ["W.K. Pratt", "J. Kane", "H.C. Andrews"], "venue": "Proc. IEEE, vol. 57, no. 1, pp. 58\u201368, 1969.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1969}, {"title": "Learning sparsifying transforms", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Signal Process., vol. 61, no. 5, pp. 1072\u20131086, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Trans. Image Process., vol. 15, no. 12, pp. 3736\u20133745, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparse representation for color image restoration", "author": ["J. Mairal", "M. Elad", "G. Sapiro"], "venue": "IEEE Trans. on Image Processing, vol. 17, no. 1, pp. 53\u201369, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Image sequence denoising via sparse and redundant representations", "author": ["M. Protter", "M. Elad"], "venue": "IEEE Trans. on Image Processing, vol. 18, no. 1, pp. 27\u201336, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "J. Mach. Learn. Res., vol. 11, pp. 19\u2013 60, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "MR image reconstruction from highly undersampled k-space data by dictionary learning", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Med. Imag., vol. 30, no. 5, pp. 1028\u20131041, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis K-SVD: A dictionarylearning algorithm for the analysis sparse model", "author": ["R. Rubinstein", "T. Peleg", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 3, pp. 661\u2013677, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning doubly sparse transforms for images", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Image Process., vol. 22, no. 12, pp. 4598\u20134612, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Structured overcomplete sparsifying transform learning with convergence guarantees and applications", "author": ["B. Wen", "S. Ravishankar", "Y. Bresler"], "venue": "International Journal of Computer Vision, vol. 114, no. 2-3, pp. 137\u2013 167, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM Review, vol. 51, no. 1, pp. 34\u201381, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 52\u201368, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparsity in unions of subspaces for classification and clustering of high-dimensional data", "author": ["E. Elhamifar", "R. Vidal"], "venue": "49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), 2011, pp. 1085\u20131089.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Comput., vol. 24, no. 2, pp. 227\u2013234, Apr. 1995.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptive greedy approximations", "author": ["G. Davis", "S. Mallat", "M. Avellaneda"], "venue": "Journal of Constructive Approximation, vol. 13, no. 1, pp. 57\u201398, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition", "author": ["Y. Pati", "R. Rezaiifar", "P. Krishnaprasad"], "venue": "Asilomar Conf. on Signals, Systems and Comput., 1993, pp. 40\u201344 vol.1.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S.G. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397\u20133415, 1993.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}, {"title": "Neuromagnetic source imaging with FOCUSS: A recursive weighted minimum norm algorithm", "author": ["I.F. Gorodnitsky", "J. George", "B.D. Rao"], "venue": "Electrocephalography and Clinical Neurophysiology, vol. 95, pp. 231\u2013251, 1995.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "A new algorithm for computing sparse solutions to linear inverse problems", "author": ["G. Harikumar", "Y. Bresler"], "venue": "ICASSP, may 1996, pp. 1331\u2013 1334.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Atomic decomposition by basis pursuit", "author": ["S.S. Chen", "D.L. Donoho", "M.A. Saunders"], "venue": "SIAM J. Sci. Comput., vol. 20, no. 1, pp. 33\u201361, 1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics, vol. 32, pp. 407\u2013499, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Cosamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J. Tropp"], "venue": "Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301 \u2013 321, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Trans. Information Theory, vol. 55, no. 5, pp. 2230\u20132249, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Nature, vol. 381, no. 6583, pp. 607\u2013609, 1996.  12", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1996}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S. Aase", "J. Hakon-Husoy"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, 1999, pp. 2443\u20132446.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Transactions on signal processing, vol. 54, no. 11, pp. 4311\u20134322, 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Dictionary learning for sparse approximations with the majorization method", "author": ["M. Yaghoobi", "T. Blumensath", "M. Davies"], "venue": "IEEE Transaction on Signal Processing, vol. 57, no. 6, pp. 2178\u20132191, 2009.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Compression of facial images using the k-svd algorithm", "author": ["O. Bryt", "M. Elad"], "venue": "Journal of Visual Communication and Image Representation, vol. 19, no. 4, pp. 270\u2013282, 2008.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse and redundant modeling of image content using an image-signature-dictionary", "author": ["M. Aharon", "M. Elad"], "venue": "SIAM Journal on Imaging Sciences, vol. 1, no. 3, pp. 228\u2013247, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning multiscale sparse representations for image and video restoration", "author": ["J. Mairal", "G. Sapiro", "M. Elad"], "venue": "SIAM Multiscale Modeling and Simulation, vol. 7, no. 1, pp. 214\u2013241, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "IEEE International Conference on Computer Vision, Sept 2009, pp. 2272\u20132279.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Alternatively constrained dictionary learning for image superresolution", "author": ["X. Lu", "Y. Yuan", "P. Yan"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 3, pp. 366\u2013377, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": "Proc. IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) 2010, 2010, pp. 3501\u20133508.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "A dictionary learning approach for classification: Separating the particularity and the commonality", "author": ["S. Kong", "D. Wang"], "venue": "Proceedings of the 12th European Conference on Computer Vision, 2012, pp. 186\u2013199.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Label consistent k-svd: Learning a discriminative dictionary for recognition", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11, pp. 2651\u20132664, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse representations for limited data tomography", "author": ["H.Y. Liao", "G. Sapiro"], "venue": "Proc. IEEE International Symposium on Biomedical Imaging (ISBI), 2008, pp. 1375\u20131378.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiscale dictionary learning for MRI", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "Proc. ISMRM, 2011, p. 2830.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Undersampled dynamic magnetic resonance imaging using patch-based spatiotemporal dictionaries", "author": ["Y. Wang", "Y. Zhou", "L. Ying"], "venue": "2013 IEEE 10th International Symposium on Biomedical Imaging (ISBI), April 2013, pp. 294\u2013297.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian nonparametric dictionary learning for compressed sensing mri", "author": ["Y. Huang", "J. Paisley", "Q. Lin", "X. Ding", "X. Fu", "X.P. Zhang"], "venue": "IEEE Trans. Image Process., vol. 23, no. 12, pp. 5007\u20135019, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Dictionary identification\u2013sparse matrixfactorization via l1 -minimization", "author": ["R. Gribonval", "K. Schnass"], "venue": "IEEE Trans. Inform. Theory, vol. 56, no. 7, pp. 3523\u20133539, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning incoherent dictionaries for sparse approximation using iterative projections and rotations", "author": ["D. Barchiesi", "M.D. Plumbley"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 8, pp. 2055\u20132065, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1553\u20131564, 2010.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Recursive least squares dictionary learning algorithm", "author": ["K. Skretting", "K. Engan"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 4, pp. 2121\u20132130, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-scale dictionary learning using wavelets", "author": ["B. Ophir", "M. Lustig", "M. Elad"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 5, pp. 1014\u20131024, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Dictionary training for sparse representation as generalization of k-means clustering", "author": ["S.K. Sahoo", "A. Makur"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 6, pp. 587\u2013590, June 2013.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving dictionary learning: Multiple dictionary updates and coefficient reuse", "author": ["L.N. Smith", "M. Elad"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 1, pp. 79\u201382, Jan 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Dictionary learning for sparse representation: A novel approach", "author": ["M. Sadeghi", "M. Babaie-Zadeh", "C. Jutten"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 12, pp. 1195\u20131198, Dec 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "A sequential dictionary learning algorithm with enforced sparsity", "author": ["A.-K. Seghouane", "M. Hanif"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 3876\u2013 3880.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Correlation based online dictionary learning algorithm", "author": ["Y. Naderahmadian", "S. Beheshti", "M. Tinati"], "venue": "IEEE Transactions on Signal Processing, 2015.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "L0 norm based dictionary learning by proximal methods with global convergence", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 3858\u2013 3865.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Direct optimization of the dictionary learning problem", "author": ["A. Rakotomamonjy"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 22, pp. 5495\u20135506, 2013.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Separable dictionary learning", "author": ["S. Hawe", "M. Seibert", "M. Kleinsteuber"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 438\u2013445.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact recovery of sparselyused dictionaries", "author": ["D.A. Spielman", "H. Wang", "J. Wright"], "venue": "Proceedings of the 25th Annual Conference on Learning Theory, 2012, pp. 37.1\u201337.18.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning sparsely used overcomplete dictionaries via alternating minimization", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"], "venue": "2013, preprint: http://arxiv.org/abs/1310.7991.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "2013, preprint: http://arxiv.org/pdf/1308.6273v5.pdf.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast patch-dictionary method for whole-image recovery", "author": ["Y. Xu", "W. Yin"], "venue": "2013, UCLA CAM report 13-38. [Online]. Available: ftp://ftp.math.ucla.edu/pub/camreport/cam13-38.pdf", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning sparsely used overcomplete dictionaries", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"], "venue": "Journal of Machine Learning Research, vol. 35, pp. 1\u201315, 2014.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning overcomplete dictionaries based on atom-by-atom updating", "author": ["M. Sadeghi", "M. Babaie-Zadeh", "C. Jutten"], "venue": "IEEE Transactions on Signal Processing, vol. 62, no. 4, pp. 883\u2013891, 2014.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Dictionary learning for sparse coding: Algorithms and analysis", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient implementation of the k-svd algorithm using batch orthogonal matching pursuit", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "http://www.cs.technion.ac.il/\u223cronrubin/Publications/KSVD-OMP-v2.pdf, 2008, technion - Computer Science Department - Technical Report.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2008}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl., vol. 109, no. 3, pp. 475\u2013494, 2001.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2001}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion", "author": ["Y. Xu", "W. Yin"], "venue": "SIAM Journal on Imaging Sciences, vol. 6, no. 3, pp. 1758\u20131789, 2013.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2013}, {"title": "Variational Analysis and Generalized Differentiation", "author": ["B.S. Mordukhovich"], "venue": "Vol. I: Basic theory. Springer-Verlag,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2006}, {"title": "Image denoising by sparse 3D transform-domain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. on Image Processing, vol. 16, no. 8, pp. 2080\u20132095, 2007.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2007}, {"title": "Michael Elad personal page", "author": ["M. Elad"], "venue": "http://www.cs.technion.ac.il/\u223celad/Various/KSVD Matlab ToolBox.zip, 2009, [Online; accessed Nov. 2015].", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient blind compressed sensing using sparsifying transforms with convergence guarantees and application to magnetic resonance imaging", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "SIAM Journal on Imaging Sciences, vol. 8, no. 4, pp. 2519\u20132557, 2015.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2015}, {"title": "Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the kurdyka-\u0142ojasiewicz inequality", "author": ["H. Attouch", "J. Bolte", "P. Redont", "A. Soubeyran"], "venue": "Math. Oper. Res., vol. 35, no. 2, pp. 438\u2013457, May 2010.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION The sparsity of natural signals and images in a transform domain or dictionary has been extensively exploited in several applications such as compression [1], denoising, compressed sensing and other inverse problems.", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "Well-known models for sparsity include the synthesis, analysis [2], and the transform [3], [4] (or generalized analysis) models.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "Well-known models for sparsity include the synthesis, analysis [2], and the transform [3], [4] (or generalized analysis) models.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "Well-known models for sparsity include the synthesis, analysis [2], and the transform [3], [4] (or generalized analysis) models.", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "More recently, the data-driven adaptation of sparse signal models has benefited many applications [4]\u2013[12] compared to fixed or analytical models.", "startOffset": 98, "endOffset": 101}, {"referenceID": 11, "context": "More recently, the data-driven adaptation of sparse signal models has benefited many applications [4]\u2013[12] compared to fixed or analytical models.", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": ", y = Dx+e with x \u2208 R sparse, and e is assumed to be a small modeling error or approximation error in the signal domain [13].", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "Since different candidate signals may be approximately spanned by different subsets of columns in the dictionary D, the synthesis model is also known as a union of subspaces model [14], [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "Since different candidate signals may be approximately spanned by different subsets of columns in the dictionary D, the synthesis model is also known as a union of subspaces model [14], [15].", "startOffset": 186, "endOffset": 190}, {"referenceID": 15, "context": "The synthesis sparse coding problem is NP-hard (Non-deterministic Polynomial-time hard) [16], [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "The synthesis sparse coding problem is NP-hard (Non-deterministic Polynomial-time hard) [16], [17].", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "Numerous algorithms [18]\u2013[25] including greedy and relaxation algorithms have been proposed for this problem.", "startOffset": 20, "endOffset": 24}, {"referenceID": 24, "context": "Numerous algorithms [18]\u2013[25] including greedy and relaxation algorithms have been proposed for this problem.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "More recently, the data-driven adaptation of synthesis dictionaries, called dictionary learning, has been investigated in several works [8], [26]\u2013[29].", "startOffset": 136, "endOffset": 139}, {"referenceID": 25, "context": "More recently, the data-driven adaptation of synthesis dictionaries, called dictionary learning, has been investigated in several works [8], [26]\u2013[29].", "startOffset": 141, "endOffset": 145}, {"referenceID": 28, "context": "More recently, the data-driven adaptation of synthesis dictionaries, called dictionary learning, has been investigated in several works [8], [26]\u2013[29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "Dictionary learning has shown promise in several applications including compression, denoising, inpainting, deblurring, demosaicing, super-resolution, and classification [5], [6], [30]\u2013[37].", "startOffset": 170, "endOffset": 173}, {"referenceID": 5, "context": "Dictionary learning has shown promise in several applications including compression, denoising, inpainting, deblurring, demosaicing, super-resolution, and classification [5], [6], [30]\u2013[37].", "startOffset": 175, "endOffset": 178}, {"referenceID": 29, "context": "Dictionary learning has shown promise in several applications including compression, denoising, inpainting, deblurring, demosaicing, super-resolution, and classification [5], [6], [30]\u2013[37].", "startOffset": 180, "endOffset": 184}, {"referenceID": 36, "context": "Dictionary learning has shown promise in several applications including compression, denoising, inpainting, deblurring, demosaicing, super-resolution, and classification [5], [6], [30]\u2013[37].", "startOffset": 185, "endOffset": 189}, {"referenceID": 37, "context": "It has also been demonstrated to be useful in inverse problems such as those in tomography [38], and magnetic resonance imaging (MRI) [9], [39]\u2013[41].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "It has also been demonstrated to be useful in inverse problems such as those in tomography [38], and magnetic resonance imaging (MRI) [9], [39]\u2013[41].", "startOffset": 134, "endOffset": 137}, {"referenceID": 38, "context": "It has also been demonstrated to be useful in inverse problems such as those in tomography [38], and magnetic resonance imaging (MRI) [9], [39]\u2013[41].", "startOffset": 139, "endOffset": 143}, {"referenceID": 40, "context": "It has also been demonstrated to be useful in inverse problems such as those in tomography [38], and magnetic resonance imaging (MRI) [9], [39]\u2013[41].", "startOffset": 144, "endOffset": 148}, {"referenceID": 27, "context": "Given a collection of training signals {yi} N i=1 that are represented as columns of the matrix Y \u2208 R , the dictionary learning problem is often formulated as follows [28]", "startOffset": 167, "endOffset": 171}, {"referenceID": 41, "context": "The columns of the dictionary are constrained to have unit norm to avoid the scaling ambiguity [42].", "startOffset": 95, "endOffset": 99}, {"referenceID": 34, "context": ", incoherence [35], [43]) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem [8].", "startOffset": 14, "endOffset": 18}, {"referenceID": 42, "context": ", incoherence [35], [43]) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem [8].", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": ", incoherence [35], [43]) for the dictionary D, or solving an online version (where the dictionary is updated sequentially as new training signals arrive) of the problem [8].", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 44, "endOffset": 47}, {"referenceID": 26, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 55, "endOffset": 59}, {"referenceID": 43, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 61, "endOffset": 65}, {"referenceID": 51, "context": "Algorithms for Problem (P0) or its variants [8], [27]\u2013 [29], [44]\u2013[52] typically alternate in some form between a sparse coding step (updating X), and a dictionary update step (solving for D).", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": ", [28], [48], [50]) also partially update X in the dictionary update step.", "startOffset": 2, "endOffset": 6}, {"referenceID": 47, "context": ", [28], [48], [50]) also partially update X in the dictionary update step.", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": ", [28], [48], [50]) also partially update X in the dictionary update step.", "startOffset": 14, "endOffset": 18}, {"referenceID": 52, "context": "A few recent methods attempt to solve for D and X jointly in an iterative fashion [53], [54].", "startOffset": 82, "endOffset": 86}, {"referenceID": 53, "context": "A few recent methods attempt to solve for D and X jointly in an iterative fashion [53], [54].", "startOffset": 88, "endOffset": 92}, {"referenceID": 27, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 17, "endOffset": 21}, {"referenceID": 4, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 86, "endOffset": 89}, {"referenceID": 29, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 91, "endOffset": 95}, {"referenceID": 38, "context": "The K-SVD method [28] has been particularly popular in numerous applications [5]\u2013[7], [9], [30], [39].", "startOffset": 97, "endOffset": 101}, {"referenceID": 51, "context": "Some recent works [52], [55]\u2013[59] have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 18, "endOffset": 22}, {"referenceID": 54, "context": "Some recent works [52], [55]\u2013[59] have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 24, "endOffset": 28}, {"referenceID": 58, "context": "Some recent works [52], [55]\u2013[59] have studied the convergence of (specific) synthesis dictionary learning algorithms.", "startOffset": 29, "endOffset": 33}, {"referenceID": 51, "context": "[52] find that their method, although a fast proximal scheme, denoises less effectively than the KSVD method [5].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[52] find that their method, although a fast proximal scheme, denoises less effectively than the KSVD method [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 59, "context": "Our work shares similarities with a recent dictionary learning approach [60] that exploits a sum of outer products model for the training data.", "startOffset": 72, "endOffset": 76}, {"referenceID": 59, "context": "However, the specific problem formulation and algorithm studied in this work differ from the prior work [60].", "startOffset": 104, "endOffset": 108}, {"referenceID": 59, "context": "Importantly, unlike the previous approach [60], we provide a detailed convergence analysis of the proposed algorithm and also demonstrate its usefulness (both in terms of speed and quality of results) in sparse representation of natural signals and in image denoising.", "startOffset": 42, "endOffset": 46}, {"referenceID": 51, "context": "l0 Penalized Formulation We consider a sparsity penalized variant of Problem (P0) [52].", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "Such a Sum of OUter Products (SOUP) representation has been exploited in previous dictionary learning algorithms [28], [48].", "startOffset": 113, "endOffset": 117}, {"referenceID": 47, "context": "Such a Sum of OUter Products (SOUP) representation has been exploited in previous dictionary learning algorithms [28], [48].", "startOffset": 119, "endOffset": 123}, {"referenceID": 51, "context": "We also enforce the constraint \u2016cj\u2016\u221e \u2264 L, with L > 0, in (P1) [52] (e.", "startOffset": 62, "endOffset": 66}, {"referenceID": 59, "context": "For example, the l0 \u201cnorm\u201d for sparsity could be replaced by the l1 norm [60].", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "While Propositions 1 and 2 provide the minimizers of (2) and (11) for the case of real-valued matrices/vectors in the problems, these solutions are trivially extended to the complex-valued case (that may be useful in applications such as magnetic resonance imaging [9]) by using a Hermitian transpose.", "startOffset": 265, "endOffset": 268}, {"referenceID": 4, "context": "For example, the initial sparse coefficients could be set to zero, and the initial dictionary could be a known analytical dictionary such as the overcomplete DCT [5].", "startOffset": 162, "endOffset": 165}, {"referenceID": 51, "context": "[52], [61] has a per-iteration computational cost of at least 2NJn + 6\u03b1NJn + 4\u03b1Nn.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[52], [61] has a per-iteration computational cost of at least 2NJn + 6\u03b1NJn + 4\u03b1Nn.", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "This is lower than the periteration cost of learning an n \u00d7 J synthesis dictionary D using K-SVD [28], which scales3 (assuming that the synthesis sparsity level s \u221d n and J \u221d n in K-SVD) as O(Nn).", "startOffset": 97, "endOffset": 101}, {"referenceID": 62, "context": ", [63]) do not apply here.", "startOffset": 2, "endOffset": 6}, {"referenceID": 63, "context": "More recent works [64] on the convergence of block coordinate descent schemes use assumptions (such as multi-convexity) that do not hold in our setting.", "startOffset": 18, "endOffset": 22}, {"referenceID": 64, "context": "Definitions and Notations First, we review the Fr\u00e9chet sub-differential of a function [65], [66].", "startOffset": 92, "endOffset": 96}, {"referenceID": 61, "context": "3When s \u221d n and J \u221d n, the per-iteration computational cost of the efficient implementation of K-SVD [62] also scales similarly as O(Nn).", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "We compare the performance of dictionaries learned using our method to those learned using the classical K-SVD4 method [5], [28].", "startOffset": 119, "endOffset": 122}, {"referenceID": 27, "context": "We compare the performance of dictionaries learned using our method to those learned using the classical K-SVD4 method [5], [28].", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "For K-SVD learning and denoising [5], [28], we consider the original Matlab implementation of the methods available from Michael Elad\u2019s website [68].", "startOffset": 33, "endOffset": 36}, {"referenceID": 27, "context": "For K-SVD learning and denoising [5], [28], we consider the original Matlab implementation of the methods available from Michael Elad\u2019s website [68].", "startOffset": 38, "endOffset": 42}, {"referenceID": 66, "context": "For K-SVD learning and denoising [5], [28], we consider the original Matlab implementation of the methods available from Michael Elad\u2019s website [68].", "startOffset": 144, "endOffset": 148}, {"referenceID": 61, "context": "A fast version5 of K-SVD [62] that also uses MEX/C implementations of sparse coding and some sub-steps of dictionary update, is publicly available [69].", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "Convergence Experiment To study the convergence behavior of the proposed SOUPDIL Algorithm, we extracted 3 \u00d7 10 patches of size 8 \u00d7 8 from randomly chosen locations in the images Barbara, Boat, 4The K-SVD method is a highly popular dictionary learning scheme that has been applied to many image processing applications including denoising [5], [6] and MR image reconstruction [9].", "startOffset": 339, "endOffset": 342}, {"referenceID": 5, "context": "Convergence Experiment To study the convergence behavior of the proposed SOUPDIL Algorithm, we extracted 3 \u00d7 10 patches of size 8 \u00d7 8 from randomly chosen locations in the images Barbara, Boat, 4The K-SVD method is a highly popular dictionary learning scheme that has been applied to many image processing applications including denoising [5], [6] and MR image reconstruction [9].", "startOffset": 344, "endOffset": 347}, {"referenceID": 8, "context": "Convergence Experiment To study the convergence behavior of the proposed SOUPDIL Algorithm, we extracted 3 \u00d7 10 patches of size 8 \u00d7 8 from randomly chosen locations in the images Barbara, Boat, 4The K-SVD method is a highly popular dictionary learning scheme that has been applied to many image processing applications including denoising [5], [6] and MR image reconstruction [9].", "startOffset": 376, "endOffset": 379}, {"referenceID": 32, "context": "[33] proposed a nonlocal method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D [67] denoising method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 65, "context": "[33] proposed a nonlocal method for image denoising that also exploits learned dictionaries and achieves denoising performance comparable to the well-known BM3D [67] denoising method.", "startOffset": 161, "endOffset": 165}, {"referenceID": 66, "context": "5This version [69] was observed to typically provide similar quality of results in our experiments as [68].", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "Specifically, the initial estimate for C is an allzero matrix, and the initial estimate for D is the overcomplete DCT [5], [68].", "startOffset": 118, "endOffset": 121}, {"referenceID": 66, "context": "Specifically, the initial estimate for C is an allzero matrix, and the initial estimate for D is the overcomplete DCT [5], [68].", "startOffset": 123, "endOffset": 127}, {"referenceID": 51, "context": "[52] showed that the distance between successive iterates may not converge to zero for popular algorithms such as K-SVD.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "We compare the NSRE values achieved by our algorithm to those achieved by the K-SVD dictionary learning scheme [28] for the same data.", "startOffset": 111, "endOffset": 115}, {"referenceID": 66, "context": "Sparsity Percentage R un tim es ( se co nd s) K\u2212SVD [68] SOUP\u2212DIL K\u2212SVD [69]", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "Behavior of the SOUP-DIL Algorithm and K-SVD [28] at various sparsities (or sparsity factors): (a) NSRE (percentage) for SOUP-DIL; (b) Improvements in NSRE (in decibels) provided by SOUP-DIL over K-SVD; and (c) runtimes for SOUP-DIL, the matlab implementation of K-SVD [28], [68], and the efficient (partial) MEX/C implementation of K-SVD [62], [69].", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "Behavior of the SOUP-DIL Algorithm and K-SVD [28] at various sparsities (or sparsity factors): (a) NSRE (percentage) for SOUP-DIL; (b) Improvements in NSRE (in decibels) provided by SOUP-DIL over K-SVD; and (c) runtimes for SOUP-DIL, the matlab implementation of K-SVD [28], [68], and the efficient (partial) MEX/C implementation of K-SVD [62], [69].", "startOffset": 269, "endOffset": 273}, {"referenceID": 66, "context": "Behavior of the SOUP-DIL Algorithm and K-SVD [28] at various sparsities (or sparsity factors): (a) NSRE (percentage) for SOUP-DIL; (b) Improvements in NSRE (in decibels) provided by SOUP-DIL over K-SVD; and (c) runtimes for SOUP-DIL, the matlab implementation of K-SVD [28], [68], and the efficient (partial) MEX/C implementation of K-SVD [62], [69].", "startOffset": 275, "endOffset": 279}, {"referenceID": 61, "context": "Behavior of the SOUP-DIL Algorithm and K-SVD [28] at various sparsities (or sparsity factors): (a) NSRE (percentage) for SOUP-DIL; (b) Improvements in NSRE (in decibels) provided by SOUP-DIL over K-SVD; and (c) runtimes for SOUP-DIL, the matlab implementation of K-SVD [28], [68], and the efficient (partial) MEX/C implementation of K-SVD [62], [69].", "startOffset": 339, "endOffset": 343}, {"referenceID": 66, "context": "4(c) compares the runtimes of our method to those of the unoptimized Matlab implementation of K-SVD [68] as well as the efficient (partial) MEX/C implementation [69] of K-SVD demonstrating large speedups of 40-50 times for SOUP-DIL over the first K-SVD implementation (at most sparsities), while the runtimes for the SOUP-DIL method are about the same as those of the second K-SVD implementation.", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "The optimal x\u0302 is obtained [5] by summing together the denoised patch estimates D\u0302\u03b1\u0302j at their respective 2D locations, and computing a weighted average between this result and the noisy image.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "K-SVD based denoising [5] involves a similar methodology as described above for (P1), but differs in the dictionary learning procedure, where the l0 \u201cnorms\u201d of the sparse codes are minimized so that a fitting constraint or error constraint of", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "Such a constraint serves as a strong prior [5], [6], and is a key reason for the denoising capability of K-SVD.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Such a constraint serves as a strong prior [5], [6], and is a key reason for the denoising capability of K-SVD.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "26 TABLE I PSNR VALUES IN DECIBELS FOR DENOISING WITH OVERCOMPLETE DCT (O-DCT), K-SVD [5], AND SOUP-DIL.", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": "Note that for both our method and for K-SVD [5], [68], the means of the patches are removed prior to learning and added back afterwards to obtain the denoised estimates.", "startOffset": 44, "endOffset": 47}, {"referenceID": 66, "context": "Note that for both our method and for K-SVD [5], [68], the means of the patches are removed prior to learning and added back afterwards to obtain the denoised estimates.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "We then denoise these images using both the SOUP-DIL denoising method outlined above and K-SVD [5], [68].", "startOffset": 95, "endOffset": 98}, {"referenceID": 66, "context": "We then denoise these images using both the SOUP-DIL denoising method outlined above and K-SVD [5], [68].", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "6The built-in parameter settings in the K-SVD denoising implementation [5], [68] are very similar except that K-SVD uses a subset of the patches for training.", "startOffset": 71, "endOffset": 74}, {"referenceID": 66, "context": "6The built-in parameter settings in the K-SVD denoising implementation [5], [68] are very similar except that K-SVD uses a subset of the patches for training.", "startOffset": 76, "endOffset": 80}, {"referenceID": 66, "context": "In the latter case, the same strategy (and parameter settings [68]) as used by K-SVD based denoising is adopted but while skipping the learning process.", "startOffset": 62, "endOffset": 66}, {"referenceID": 66, "context": "We have observed similar effects as in Section V-C for the runtimes of SOUPDIL denoising vis-a-vis the unoptimized Matlab [68] or the efficient (partial MEX/C) [69] implementations of the K-SVD approach.", "startOffset": 122, "endOffset": 126}, {"referenceID": 4, "context": "The proposed approach had comparable or superior performance and significant speed-ups over the classical K-SVD method [5], [28] in sparse signal representation and image denoising.", "startOffset": 119, "endOffset": 122}, {"referenceID": 27, "context": "The proposed approach had comparable or superior performance and significant speed-ups over the classical K-SVD method [5], [28] in sparse signal representation and image denoising.", "startOffset": 124, "endOffset": 128}, {"referenceID": 67, "context": "The usefulness of our method in other applications such as blind compressed sensing [70] or other inverse problems in imaging merits further study.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "Extensions of the proposed method for online learning [8] are also of potential interest.", "startOffset": 54, "endOffset": 57}, {"referenceID": 68, "context": "Finally, (see Proposition 3 in [71]) the subdifferential \u2202f at (c, d) satisfies \u2202f (c, d) = \u2202fc (c, d) \u00d7 \u2202fd (c, d).", "startOffset": 31, "endOffset": 35}], "year": 2017, "abstractText": "The sparsity of natural signals and images in a transform domain or dictionary has been extensively exploited in several applications such as compression, denoising and inverse problems. More recently, data-driven adaptation of synthesis dictionaries has shown promise in many applications compared to fixed or analytical dictionary models. However, dictionary learning problems are typically non-convex and NP-hard, and the usual alternating minimization approaches for these problems are often computationally expensive, with the computations dominated by the NP-hard synthesis sparse coding step. In this work, we investigate an efficient method for l0 \u201cnorm\u201d-based dictionary learning by first approximating the training data set with a sum of sparse rank-one matrices and then using a block coordinate descent approach to estimate the unknowns. The proposed block coordinate descent algorithm involves efficient closed-form solutions. In particular, the sparse coding step involves a simple form of thresholding. We provide a convergence analysis for the proposed block coordinate descent approach. Our numerical experiments show the promising performance and significant speed-ups provided by our method over the classical K-SVD scheme in sparse signal representation and image denoising.", "creator": "LaTeX with hyperref package"}}}