{"id": "1512.00170", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2015", "title": "Augmenting Phrase Table by Employing Lexicons for Pivot-based SMT", "abstract": "Pivot language is employed as a way to solve the data sparseness problem in machine translation, especially when the data for a particular language pair does not exist. The combination of source-to-pivot and pivot-to-target translation models can induce a new translation model through the pivot language. However, the errors in two models may compound as noise, and still, the combined model may suffer from a serious phrase sparsity problem. In this paper, we directly employ the word lexical model in IBM models as an additional resource to augment pivot phrase table. In addition, we also propose a phrase table pruning method which takes into account both of the source and target phrasal coverage. Experimental result shows that our pruning method significantly outperforms the conventional one, which only considers source side phrasal coverage. Furthermore, by including the entries in the lexicon model, the phrase coverage increased, and we achieved improved results in Chinese-to-Japanese translation using English as pivot language.", "histories": [["v1", "Tue, 1 Dec 2015 08:10:49 GMT  (511kb)", "http://arxiv.org/abs/1512.00170v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yiming cui", "conghui zhu", "xiaoning zhu", "tiejun zhao"], "accepted": false, "id": "1512.00170"}, "pdf": {"name": "1512.00170.pdf", "metadata": {"source": "CRF", "title": "Augmenting Phrase Table by Employing Lexicons for Pivot-based SMT", "authors": ["Yiming Cui", "Conghui Zhu", "Xiaoning Zhu", "Tiejun Zhao"], "emails": ["ymcui@mtlab.hit.edu.cn", "chzhu@mtlab.hit.edu.cn", "xnzhu@mtlab.hit.edu.cn", "tjzhao@mtlab.hit.edu.cn"], "sections": [{"heading": null, "text": "Pivot language is employed as a way to solve the data sparseness problem in machine translation, especially when the data for a particular language pair does not exist. The combination of source-to-pivot and pivot-to-target translation models can induce a new translation model through the pivot language. However, the errors in two models may compound as noise, and still, the combined model may suffer from a serious phrase sparsity problem. In this paper, we directly employ the word lexical model in IBM models as an additional resource to augment pivot phrase table. In addition, we also propose a phrase table pruning method which takes into account both of the source and target phrasal coverage. Experimental result shows that our pruning method significantly outperforms the conventional one, which only considers source side phrasal coverage. Furthermore, by including the entries in the lexicon model, the phrase coverage increased, and we achieved improved results in Chinese-to-Japanese translation using English as pivot language."}, {"heading": "1 Introduction", "text": "The recent improvement in statistical machine translation (SMT) strongly relies on the availability of large parallel data, in order to estimate parameters more precisely. For frequently used language pairs, like English-French, there are large amounts of parallel corpus readily available. However some language pairs, such as English-Dutch, has limited amount of parallel data, due to the popularity of the language pair. In order to solve limitations of parallel data, pivot language method was introduced (de Gispert and Marino, 2006), which bridges two languages through an intermediate language. Furthermore, the pivot language method can also be applied to those popular language pairs, such as ChineseEnglish, when the bilingual resources are limited for a particular domain.\nPhrase pivoting, also called triangulation, is one of the state of the art pivoting method, in which a source-to-target translation model is induced by combining a source-to-pivot and pivot-to-target translation models (Utiyama and Isahara, 2007; Cohn and Lapata, 2007). It aims to obtain a source-totarget model by combining source-to-pivot and pivot-to-target translation models, which has been shown to be generally better than the other pivot methods. However, it has been already reported that the phrase pivoting method may generate a very large phrase table (Utiyama and Isahara, 2007), bringing much noise and losing some phrase pairs if they are not connected to the same pivot phrase (Cui et al., 2013), which will affect the overall translation quality. Moreover, the pivot-based machine translation also suffers from a severe phrase sparsity problem since the loss of translation phrase pairs. Through some simple experiments, we discovered that generally the pivot model is inferior to standard model in phrase coverage, which trained with the same corpus size.\nIn this paper, we directly employ the word lexical model in IBM models as an additional resource to augment pivot phrase table. Furthermore, we also propose a pruning method which takes into account both of the source and target phrasal coverage, in order to diminish the noise to our phrase table extension method."}, {"heading": "2 Related Work", "text": "Many researchers have investigated pivot language method in statistical machine translation. The first is the sentence translation, also called transfer or cascade method (Khalilov et al., 2008). We first translate the source sentences to pivot language with source-to-pivot translation system, and then translate the pivot language sentences to target language with pivot-to-target translation system. Though very simple, it is time consuming, because it should pass two translation systems consecutively.\nThe second is the phrase pivoting, also called triangulation method. It combines source-to-pivot and pivot-to-target phrase table to induce a source-to-target phrase table for pivot translation (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007). It has been shown that phrase pivoting outperforms other pivoting methods in general (Wu and Wang, 2009).\nThe third is the synthetic method, also called the pseudo corpus method. It aims to build a sourceto-target parallel corpus, using existing source-to-pivot and pivot-to-target corpus (Bertoldi et al., 2008).\nFurthermore, Michael Paul and Eiichiro Sumita (2011) investigated the factors that affect the quality of pivot-based machine translation. Zhu et al. (2013a) used random walk method to enhance the connectivity of pivot phrases, creating much more candidates for source phrases and alleviating the OOV problems. Kholy et al. (2013) added connectivity strength features to indicate the quality of alignment projection, and proposed a simple phrase table pruning technique to solve noise problem."}, {"heading": "3 Phrase Pivoting Method", "text": "Given a source-to-pivot and pivot-to-target corpus, we can train two translation models respectively, and induce a pivot translation model, i.e. source-to-target translation model, by combining the sourceto-pivot and pivot-to-target translation models through the matched pivot language phrases. In the combining procedure, there are two elements should be taken into account.\nThe first element is phrase translation probability. Because the source phrases and target phrases are extracted from different corpus, we assume that source phrases are independent with target phrases. When given pivot phrases, we can induce the phrase translation probability ( | )s t\u03d5 as Equation 1. ( | ) ( | ) ( | )\np s t s p p t\u03d5 \u03d5 \u03d5= \u22c5\u2211 (1)\nwhere the s, p and t denotes the phrases in the source, pivot, and target respectively. The second element is lexical weight. We assume 1a and 2a be the alignment information inside phrase pair ( , )s p and ( , )p t respectively. And we can get the word alignment a of phrase pair ( , )s t by the following Equation 2. 1 2{( , ) | : ( , ) &( , ) }a s t p s p a p t a= \u2203 \u2208 \u2208 (2) Then we can estimate the lexical translation probability ( | )w s t by induced word alignment information. ( , )( | )\n( , ) s count s tw s t count s t \u2032 = \u2032\u2211\n(3)"}, {"heading": "4 Approach", "text": "As we mentioned above, the pivot-based machine translation suffers from serious noise problem caused by the compounded error in each translation model. In addition, the induced pivot translation model could be order of magnitude larger than unmerged models, which is almost occupied by noise phrase pairs. We tackle this problem by first pruning the enlarged translation model in order to exclude noise phrase pairs. Secondly, we augment translation model by employing additional entries from lexicon models."}, {"heading": "4.1 Modified Top-N Pruning Method", "text": "In pivot-based machine translation, a basic phrase table pruning technique is the top-N pruning. That is to say, we select the top-N candidates for a given source phrase in pivot phrase table. It is proved to be a simple and useful phrase table pruning strategy (Zhu et al., 2013a; Kholy et al., 2013). However, we have noticed that most of the pruning methods seldom take the source phrase coverage into account, and we believe that the noise not only exists in target phrases, but also in source phrases. Motivated by this, we propose a modified top-N pruning method, which will lead to a much compact phrase table, and significant improvements in translation performance.\nWhen given a source phrase S, we select its top-N scored phrases, and discard those who are not in the top-N list. To select these top-N candidates, based on the log-linear model, we multiply each feature (such as translation probabilities and lexical weights) by its optimized decoding weight, which are computed in the tuning procedure. Formally, when a phrase pair (S,T) is given, its score can be calculated as Equation 4.\n1 ( , ) log( )\nn\ni i score S T W F =\n= \u22c5\u2211 i (4)\nwhere Wi is the ith optimized decoding weight of (S,T), Fi is the ith feature of (S,T), and n represents the number of features. Through above steps, we have done the basic top-N pruning. That is to say, we prunes phrases with respect to the source side of the phrase pairs, so that only N phrases for each source phrase are preserved. Then we further applied similar pruning method to the target phrases. We performed top-M pruning with respect to the target side of the phrase pairs after top-N pruning, so that only M phrases for each target phrase are kept as translation candidates. Note that, some attention should be paid to the selection of pruning threshold N and M. We will have a further discussion about this in Section 5."}, {"heading": "4.2 Augmenting Phrase Table by Adding Lexicons", "text": "As we mentioned in Section 1, the pivot-based machine translation suffers from serious phrase sparsity problem, and even a basic translation unit, unigrams, may not be covered after inducing phrase pairs of pivot model. From a macro perspective, this is because the pivot method cannot make full use of the source-topivot and pivot-to-target corpus resources. When we combine the source-to-pivot and pivot-to-target phrase table, those phrases which are not connected through the same pivot phrases will be discarded, even though similar phrases may be matched in the pivot side. However, after we look into this problem, we found that the phrase coverage problem might result from word alignment error, which might prevent us from extracting useful phrase pairs. To solve this problem, we introduce a novel approach to enrich the phrase pairs in pivot phrase table, in order to increase the phrase coverage in pivot-based machine translation and improve overall translation performance. Besides the phrase-based translation model, we directly employ the word lexical model in IBM models (Brown et al., 1993) as an additional resource to extend pivot phrase table. After the word alignment is done, we can get a lexical model, which contains word-to-word translation table with its conditional translation probability (word-based model). So a source-to-target word lexical model can be rebuilt in a similar way to induce a pivot phrase table, which is to combine source-to-pivot and pivot-to-target lexical models. Formally, given a source-to-pivot word pair <s,p> and pivot-to-target word pair <p,t>, if the pivot word is the same, we can get a source-to-target word pair <s,t> with its conditional probabilities calculated in Equation 5 and 6. ( | ) ( | ) ( | )\np s t s p p t\u03c8 \u03c8 \u03c8= \u22c5\u2211 (5)\n( | ) ( | ) ( | ) p t s p s t p\u03c8 \u03c8 \u03c8= \u22c5\u2211 (6) where ( | )s t\u03c8 and ( | )t s\u03c8 denotes the direct and inverse conditional probability respectively. Note that, it is possible to project a NULL word to a target word in word alignment, but here we say that the word pairs such as <s,NULL> and <NULL,t> are not participated in generating the pivot lexical model. Because they cannot be used to produce reliable source-to-target word pairs. To formulate a phrase table, there are two extra things should be calculated, that is lexical weight. In this paper, we just copy the corresponding translation probabilities, as shown in Equation 7 and 8. We also tried conventional lexical weight calculation method (similar to Equation 3), as well as constant values for lexical weights. The results and further discussions are shown in Section 5.\n( | ) ( | )lex s t s t\u03c8= (7)\n( | ) ( | )lex t s t s\u03c8= (8)\nUntil now, we can combine the lexical translation table and original pivot phrase table to form a new pivot phrase table. Given a word pair <s,t> in lexical translation table, we just check if it is in the original pivot phrase table. If exists, we do not add <s,t> into it, or we just add this item into the original pivot phrase table."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experiment Setup", "text": "In this paper, we build a Chinese-Japanese translation system without using parallel corpus. We select English as pivot language, because of its large availability of bilingual corpus. We randomly select 50K Chinese-English and 59K Japanese-English sentences from an in-house parallel corpus respectively, containing spoken corpus of various domains.\nWe used Chinese-Japanese parallel corpus to make a 2K tuning set and 1K test set with single reference each. Note that the training set, tuning set and test set are independent each other. We carry out our experiments using an open-source phrase-based SMT toolkit Moses (Koehn et al., 2007). Word alignment and phrase extraction are done by GIZA++ (Och and Ney, 2003). 5-gram language models are trained by SRILM toolkit (Stolcke, 2002). We use MERT (minimum error training) for parameter tuning (Och, 2003). Because of MERT\u2019s instability, we tune every translation system 5 times independently and take the average BLEU score (Clark et al., 2011; Zhu et al., 2013b). The translation quality evaluation is done by case-insensitive BLEU-4 metric (Papineni et al., 2002). The statistical significance test is also carried out with paired bootstrap resampling method (Koehn, 2004)."}, {"heading": "5.2 Experiment of Modified Top-N Pruning", "text": "We applied several experiments to show the performance of modified top-N pruning method. As illustrated in Section 4.1, it is hard to choose proper values for N and M (represents direct/inverse pruning threshold respectively). So we carried out a series of experiments to try various combinations of N and M values. The experiment results are shown in Table 1. Because the size of the original pivot phrase table is huge (about 252M), we set a pruned phrase table with top-100 as our baseline system. The values in the Size column represent the number of phrase pairs. Inv represents inverse top-N pruning, which is the second step of modified top-N pruning method. The BLEU scores that statistically significant than the baseline (above 95% level) are marked with bold face. From Table 1, we observed that: (1) Top-N pruning proved to be a simple and useful phrase table pruning method. The pruned phrase\ntable is much more compact than original one. As the noise in phrase table has been removed, the translation performance is also improved. (2) Modified top-N pruning method generally outperform the traditional one. The scale of best system (+Top50+Inv100) is reduced to 31.2% of baseline system, and BLEU score raised 1.79 points. The best system also exceed the conventional top-N pruning method, both in scale and translation performance. When compared to the original pivot phrase table, the scale of best system significantly reduced to 0.01%, which will save much storage space and training time.\n(3) With the growing of Inv (see row 5 to 7), the translation performance also improved, but the scale of phrase table slightly increased. This is because, the higher we set an M value, the more we get source phrase candidates.\nCompared to M value, it is easy to set N value. Because for one source phrase, the number of target phrase candidates cannot be infinitively growing, even if the training corpus increases. So we empirically conclude that the N value should not exceed 100 in most cases. On the contrary, the inverse pruning threshold M should not set to a small value, in case of losing diversity of source phrases and increasing the OOVs. As we can see in Table 1, when the M value decreases to 20, the BLEU score drops dramatically. So we suggest that, for most circumstances, the M value should not be less than 100. We will investigate how to automatically optimize pruning thresholds for a given pivot-based translation system in the future."}, {"heading": "5.3 Experiment of Augmenting Phrase Table", "text": "In this experiment, we will test the performance of the phrase table augmenting. As we mentioned in Section 4.2, we try different lexical weight calculation methods to test their performances. To make the results more precise, we use top-N pruning method to lexical translation table, and only 20 candidates for each source word are preserved. Finally, we obtain a 426K lexical translation table.\nIn Table 2, the LEX column shows the method to calculate lexical weight. The re-estimate means conventional lexical weight calculation method, and copy means the approach we mentioned in Section 4.2, i.e. copy corresponding probabilities, and 10e\u2212 means constant value 10e\u2212 . Baseline_OPT represents the best system we obtained in Section 5.2 (with N=50, M=100). Besides the BLEU metric, we also show the number of OOVs, which indicates the performance of phrase coverage. As we can see that, after applying phrase table augmenting (not matter how we calculate lexical weights), these systems still outperform the baseline (Baseline_Top100) significantly. Furthermore, it is worth mentioning that as the phrase coverage improved by adding lexical translation table, the number of OOVs are significantly reduced from 400 to 191 (47.75% of its origin). Here, our experimental results can be summarized as follows: (1) The system performance changes slightly with different lexical weight calculation method (see\nrow 3 to 5). One of the reasons is that the number of lexical phrases added to our phrase table is far smaller than the induced phrase table, and thus we observe little impact to the system. (2) The phrase extended systems (see row 3 to 5) are not significantly better than Baseline_OPT in BLEU scores (only improved 0.06~0.42). In our experiments, we used a test set consisting only single reference, which may potentially underestimate the gains by reducing OOVs. To verify this, we analyzed the rank of translation candidates during decoding. We randomly selected some examples of negative results in Table 3.\nIn the test sentence \u201c\u6211\u60f3\u559d\u4e00\u676f\u6c7d\u6c34\u201d (I want a glass of soda.), the test word \u201c\u6c7d\u6c34\u201d (soda) is translated the same in the baseline and test set, but our system result \u201c\u30bd\u30fc\u30c0\u201d is another representa-\ntion of test word \u201c\u6c7d\u6c34\u201d, so as another case \u201c\u53e4\u5178\u201d (classical). If we use a multi-referenced test set, the BLEU score may raise up, due to the test set covered with various synonyms. However, on the other side, the phrase coverage problem relived (OOV reduced) by our method, with minor phrases translated to its synonyms, is acceptable in practical use. Furthermore, sometimes it is much more meaningful to cover more phrases than a slight improvement in BLEU score, which will convey more informations."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we presented a modified phrase table pruning method, which takes both of the source and target phrasal coverage into account, to alleviate the noise problem in pivot-based machine translation. In addition, we also introduced a novel approach to improve phrase coverage in phrase table, via employing the word lexical model in IBM models as an additional resource. Experiment results show that our pruning method significantly outperforms the original one, not only in reducing the size of phrase table, but also in improving overall translation performance. And after applying our phrase table extension strategy, the phrase coverage increased, in terms of OOV reduced to under 50% of its origin. In our future work, we are planning to integrate linguistic information to the phrase table augmenting method, in order to obtain a better pivot phrase table. Furthermore, we are also going to investigate new way to integrate lexical model into pivot model, such as linear interpolation etc."}], "references": [{"title": "Phrase-based Statistical Machine Translation with Pivot Languages", "author": ["Nicola Bertoldi", "Madalina Barbaiani", "Marcello Federico", "Roldano Cattoni."], "venue": "Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 143-149.", "citeRegEx": "Bertoldi et al\\.,? 2008", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2008}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer."], "venue": "Computational linguistics, volume 19(2), pages 263311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Phrase Table Combination Deficiency Analyses in Pivot-based SMT", "author": ["Yiming Cui", "Conghui Zhu", "Xiaoning Zhu", "Tiejun Zhao", "Dequan Zheng."], "venue": "Proceedings of 18th International Conference on Application of Natural Language to Information Systems (NLDB), pages 355-358.", "citeRegEx": "Cui et al\\.,? 2013", "shortCiteRegEx": "Cui et al\\.", "year": 2013}, {"title": "Machine Translation by Triangulation: Making Effective Use of MultiParallel Corpora", "author": ["Tevor Cohn", "Mirella Lapata."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguisitics (ACL), pages 348-355.", "citeRegEx": "Cohn and Lapata.,? 2007", "shortCiteRegEx": "Cohn and Lapata.", "year": 2007}, {"title": "Better hypothesis testing for statistical machine translation: controlling for optimizer instability", "author": ["Jonathon H. Clark", "Chris Dyer", "Alon Lavie", "Noah A.Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers, pages 176-181.", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Catalan-English statistical machine translation without parallel corpus: bridging through Spanish", "author": ["Adria de Gispert", "Jose B. Marino."], "venue": "Proceedings of 5th International Conference on Language Resources and Evaluation (LREC), pages 65-68.", "citeRegEx": "Gispert and Marino.,? 2006", "shortCiteRegEx": "Gispert and Marino.", "year": 2006}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL) on Interactive Poster and Demonstration Sessions, pages 177-180.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of Human Language Technology conference of the North American chapter of the Association for Computational Linguistics (HLT-NAALC), pages 127-133.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the Empirical Methods in Natural Language Processing Conference (EMNLP\u201904), pages 388-395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "The TALP & I2R SMT Systems for IWSLT 2008", "author": ["M. Khalilov", "Marta R. Costa-juss", "Jos A.R. Fonollosa", "Rafael E. Banchs", "B. Chen", "M. Zhang", "A. Aw", "H. Li", "Jos B. Mario", "Adolfo Hernndez", "Carlos A. Henrquez Q."], "venue": "Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 116\u2013 123.", "citeRegEx": "Khalilov et al\\.,? 2008", "shortCiteRegEx": "Khalilov et al\\.", "year": 2008}, {"title": "Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation", "author": ["Ahmed El Kholy", "Nizar Habash", "Gregor Leusch", "Evgeny Matusov."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 412-418.", "citeRegEx": "Kholy et al\\.,? 2013", "shortCiteRegEx": "Kholy et al\\.", "year": 2013}, {"title": "A Systematic Comparison of Various Statistical Alignment Models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational Linguistics, volume 29(1), pages 19-52.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st annual Meeting of Association for Computational Linguistics (ACL), pages 160-167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Translation Quality Indicators for Pivot-based Statistical MT", "author": ["Michael Paul", "Eiichiro Sumita."], "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP), pages 811-818.", "citeRegEx": "Paul and Sumita.,? 2011", "shortCiteRegEx": "Paul and Sumita.", "year": 2011}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Weijing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311-318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "SRILM - an Extensible Language Modeling Toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of the International Conference on Spoken Language Processing (ICSLP), volume 2, pages 901-904.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "A Comparison of pivot methods for phrase-based statistical machine translation", "author": ["Masao Utiyama", "Hitoshi Isahara."], "venue": "Proceedings of Human Language Technologies (HLT), pages 484-491.", "citeRegEx": "Utiyama and Isahara.,? 2007", "shortCiteRegEx": "Utiyama and Isahara.", "year": 2007}, {"title": "Revisiting Pivot Language Approach for Machine Translation", "author": ["Hua Wu", "Haifeng Wang."], "venue": "Proceedings of the 47th Annual Meeting of Association for Computational Linguistics and the 4th International Joint Conference of Natural Language Processing of the AFNLP, pages 154-162.", "citeRegEx": "Wu and Wang.,? 2009", "shortCiteRegEx": "Wu and Wang.", "year": 2009}, {"title": "Pivot Language Approach for Phrase-based Statistical Machine Translation", "author": ["Hua Wu", "Haifeng Wang."], "venue": "Proceedings of 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 856863.", "citeRegEx": "Wu and Wang.,? 2007", "shortCiteRegEx": "Wu and Wang.", "year": 2007}, {"title": "Improving Pivotbased statistical machine translation using random walk", "author": ["Xiaoning Zhu", "Zhongjun He", "Hua Wu", "Haifeng Wang", "Conghui Zhu", "Tiejun Zhao."], "venue": "Proceedings of the conference on Empirical Methods in Natural Language Processing (EMNLP), pages 524-534.", "citeRegEx": "Zhu et al\\.,? 2013a", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}, {"title": "Hierarchical Phrase Table Combination for Machine Translation", "author": ["Conghui Zhu", "Taro Watanabe", "Eiichiro Sumita", "Tiejun Zhao."], "venue": "Proceedings of 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 802-810.", "citeRegEx": "Zhu et al\\.,? 2013b", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 16, "context": "Phrase pivoting, also called triangulation, is one of the state of the art pivoting method, in which a source-to-target translation model is induced by combining a source-to-pivot and pivot-to-target translation models (Utiyama and Isahara, 2007; Cohn and Lapata, 2007).", "startOffset": 219, "endOffset": 269}, {"referenceID": 3, "context": "Phrase pivoting, also called triangulation, is one of the state of the art pivoting method, in which a source-to-target translation model is induced by combining a source-to-pivot and pivot-to-target translation models (Utiyama and Isahara, 2007; Cohn and Lapata, 2007).", "startOffset": 219, "endOffset": 269}, {"referenceID": 16, "context": "However, it has been already reported that the phrase pivoting method may generate a very large phrase table (Utiyama and Isahara, 2007), bringing much noise and losing some phrase pairs if they are not connected to the same pivot phrase (Cui et al.", "startOffset": 109, "endOffset": 136}, {"referenceID": 2, "context": "However, it has been already reported that the phrase pivoting method may generate a very large phrase table (Utiyama and Isahara, 2007), bringing much noise and losing some phrase pairs if they are not connected to the same pivot phrase (Cui et al., 2013), which will affect the overall translation quality.", "startOffset": 238, "endOffset": 256}, {"referenceID": 9, "context": "The first is the sentence translation, also called transfer or cascade method (Khalilov et al., 2008).", "startOffset": 78, "endOffset": 101}, {"referenceID": 3, "context": "It combines source-to-pivot and pivot-to-target phrase table to induce a source-to-target phrase table for pivot translation (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007).", "startOffset": 125, "endOffset": 194}, {"referenceID": 16, "context": "It combines source-to-pivot and pivot-to-target phrase table to induce a source-to-target phrase table for pivot translation (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007).", "startOffset": 125, "endOffset": 194}, {"referenceID": 18, "context": "It combines source-to-pivot and pivot-to-target phrase table to induce a source-to-target phrase table for pivot translation (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007).", "startOffset": 125, "endOffset": 194}, {"referenceID": 17, "context": "It has been shown that phrase pivoting outperforms other pivoting methods in general (Wu and Wang, 2009).", "startOffset": 85, "endOffset": 104}, {"referenceID": 0, "context": "It aims to build a sourceto-target parallel corpus, using existing source-to-pivot and pivot-to-target corpus (Bertoldi et al., 2008).", "startOffset": 110, "endOffset": 133}, {"referenceID": 0, "context": "It aims to build a sourceto-target parallel corpus, using existing source-to-pivot and pivot-to-target corpus (Bertoldi et al., 2008). Furthermore, Michael Paul and Eiichiro Sumita (2011) investigated the factors that affect the quality of pivot-based machine translation.", "startOffset": 111, "endOffset": 188}, {"referenceID": 0, "context": "It aims to build a sourceto-target parallel corpus, using existing source-to-pivot and pivot-to-target corpus (Bertoldi et al., 2008). Furthermore, Michael Paul and Eiichiro Sumita (2011) investigated the factors that affect the quality of pivot-based machine translation. Zhu et al. (2013a) used random walk method to enhance the connectivity of pivot phrases, creating much more candidates for source phrases and alleviating the OOV problems.", "startOffset": 111, "endOffset": 292}, {"referenceID": 0, "context": "It aims to build a sourceto-target parallel corpus, using existing source-to-pivot and pivot-to-target corpus (Bertoldi et al., 2008). Furthermore, Michael Paul and Eiichiro Sumita (2011) investigated the factors that affect the quality of pivot-based machine translation. Zhu et al. (2013a) used random walk method to enhance the connectivity of pivot phrases, creating much more candidates for source phrases and alleviating the OOV problems. Kholy et al. (2013) added connectivity strength features to indicate the quality of alignment projection, and proposed a simple phrase table pruning technique to solve noise problem.", "startOffset": 111, "endOffset": 465}, {"referenceID": 19, "context": "It is proved to be a simple and useful phrase table pruning strategy (Zhu et al., 2013a; Kholy et al., 2013).", "startOffset": 69, "endOffset": 108}, {"referenceID": 10, "context": "It is proved to be a simple and useful phrase table pruning strategy (Zhu et al., 2013a; Kholy et al., 2013).", "startOffset": 69, "endOffset": 108}, {"referenceID": 1, "context": "Besides the phrase-based translation model, we directly employ the word lexical model in IBM models (Brown et al., 1993) as an additional resource to extend pivot phrase table.", "startOffset": 100, "endOffset": 120}, {"referenceID": 6, "context": "We carry out our experiments using an open-source phrase-based SMT toolkit Moses (Koehn et al., 2007).", "startOffset": 81, "endOffset": 101}, {"referenceID": 11, "context": "Word alignment and phrase extraction are done by GIZA++ (Och and Ney, 2003).", "startOffset": 56, "endOffset": 75}, {"referenceID": 15, "context": "5-gram language models are trained by SRILM toolkit (Stolcke, 2002).", "startOffset": 52, "endOffset": 67}, {"referenceID": 12, "context": "We use MERT (minimum error training) for parameter tuning (Och, 2003).", "startOffset": 58, "endOffset": 69}, {"referenceID": 4, "context": "Because of MERT\u2019s instability, we tune every translation system 5 times independently and take the average BLEU score (Clark et al., 2011; Zhu et al., 2013b).", "startOffset": 118, "endOffset": 157}, {"referenceID": 20, "context": "Because of MERT\u2019s instability, we tune every translation system 5 times independently and take the average BLEU score (Clark et al., 2011; Zhu et al., 2013b).", "startOffset": 118, "endOffset": 157}, {"referenceID": 14, "context": "The translation quality evaluation is done by case-insensitive BLEU-4 metric (Papineni et al., 2002).", "startOffset": 77, "endOffset": 100}, {"referenceID": 8, "context": "The statistical significance test is also carried out with paired bootstrap resampling method (Koehn, 2004).", "startOffset": 94, "endOffset": 107}], "year": 2015, "abstractText": "Pivot language is employed as a way to solve the data sparseness problem in machine translation, especially when the data for a particular language pair does not exist. The combination of source-to-pivot and pivot-to-target translation models can induce a new translation model through the pivot language. However, the errors in two models may compound as noise, and still, the combined model may suffer from a serious phrase sparsity problem. In this paper, we directly employ the word lexical model in IBM models as an additional resource to augment pivot phrase table. In addition, we also propose a phrase table pruning method which takes into account both of the source and target phrasal coverage. Experimental result shows that our pruning method significantly outperforms the conventional one, which only considers source side phrasal coverage. Furthermore, by including the entries in the lexicon model, the phrase coverage increased, and we achieved improved results in Chinese-to-Japanese translation using English as pivot language.", "creator": "Word"}}}