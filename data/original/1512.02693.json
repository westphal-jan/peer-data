{"id": "1512.02693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2015", "title": "Reinforcement Control with Hierarchical Backpropagated Adaptive Critics", "abstract": "Present incremental learning methods are limited in the ability to achieve reliable credit assignment over a large number time steps (or events). However, this situation is typical for cases where the dynamical system to be controlled requires relatively frequent control updates in order to maintain stability or robustness yet has some action-consequences which must be established over relatively long periods of time. To address this problem, the learning capabilities of a control architecture comprised of two Backpropagated Adaptive Critics (BACs) in a two-level hierarchy with continuous actions are explored. The high-level BAC updates less frequently than the low-level BAC and controls the latter to some degree. The response of the low-level to high-level signals can either be determined a priori or it can emerge during learning. A general approach called Response Induction Learning is introduced to address the latter case.", "histories": [["v1", "Tue, 8 Dec 2015 23:11:54 GMT  (1002kb,D)", "http://arxiv.org/abs/1512.02693v1", "16 pages, 5 figures"]], "COMMENTS": "16 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.SY", "authors": ["john w jameson"], "accepted": false, "id": "1512.02693"}, "pdf": {"name": "1512.02693.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Control with Hierarchical Backpropagated Adaptive Critics\u2217", "authors": ["John W. Jameson"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "There has recently been considerable interest in the application of reinforcement learning methods to problems of adaptive (or intelligent) control. Perhaps the most recognized early work on this subject is a 1983 paper by Barto, Sutton, and Anderson [1] which demonstrated stabilization of the oft-studied cart-pole problem (see Figure 2). In this paper the authors introduced the \u201cadaptive heuristic critic,\u201d the function of which is to evaluate the current state in terms of expected future (cumulative) reinforcements. Sutton\u2019s work on temporal differences [5] refined these notions, and extended their application to nonlinear feedforward neural networks. His work is the basis for the present \u201ccritic network,\u201d the goal of which is to estimate the sum of future reinforcements (pt):\npt = \u221e\u2211 k=0 \u03b3krt+k+1, (1)\n\u2217This work was supported by Jameson Robotics and the Lyndon Baynes Johnson Space Center. Submitted to Neural Networks.\n1\nar X\niv :1\n51 2.\n02 69\n3v 1\n[ cs\n.N E\n] 8\nD ec\nwhere rt+k+1 is the reinforcement received at time t + k + 1 and \u03b3 is the discount factor (0 < \u03b3 < 1). Note that the closer the discount factor is to one the greater the \u201cforesight\u201d of the critic, i.e., the more future reinforcements should incorporated in the current prediction.\nFor many applications, in order to achieve dynamic stability and control efficacy the interval between controller updates are likely to be small compared to the time between some actions and their consequences, especially for more difficult \u201cintelligent\u201d control problems. However, the greater the number of time steps between initial states and goal states, the more difficult it is to achieve reliable credit assignment for actions which incurred early on, i.e., the more \u201cforesight\u201d required by the critic. A reasonable solution is to have motion primitives which require control actions to \u201csteer\u201d them in some way. These steering commands would occur less frequently than the motor commands issued by the primitives. Watkins [13] presented an interesting paradigm for hierarchical control based on ship navigation which has several parallels to, the present work. This idea is also similar to the principle of \u201ccascade control\u201d developed for process control.\nIn this paper a simple two-level hierarchy of reinforcement learning modules is implemented in such a way that the \u201clow level\u201d module controls the actuators directly and is rewarded for maintaining stability while the \u201chigh level\u201d module steers the low level module in order to maximize longer term positive reinforcement. Two cases are considered: one in which the role of the steering command is predefined in terms of the reinforcement to the low level controller, and the much more difficult cases where the role is not defined a priori but is developed as learning progresses."}, {"heading": "2 The Backpropagated Adaptive Critic (BAC)", "text": ""}, {"heading": "2.1 Background", "text": "In the Barto etal. work discussed in the last section, the state space of the plant was broken up into \u201cboxes\u201d and the control actions were binary. While their results were impressive, the discrete nature of the method is impractical for problems which either require high fidelity control or which have a high dimensional state space. Nevertheless, they achieved stabilization of the cart-pole system within seventy trials on average\u2014considerably better than any of the continuous approaches appearing in the literature (or here).\nAnderson [4] later used critic and action modules based on backpropagated multilayer perceptrons for stabilization of the cart-pole problem. Although inputs to the modules were continuous, the associative learning method for training the action network still required (stochastic) binary actions. Around 8,000 trials were required on average to stabilize the system. Note that a \u201ctrial\u201d starts with the cart and pole randomly within the state space ends when the position of either exceed its bounds.\nWerbos [2, 3] introduced an architecture called the Backpropagated Adaptive Critic (BAC) for which a utility function similar to a critic is used as a guide for training the action module. In this approach continuous inputs and actions are allowed. Jordan and Jacobs [12] and Jameson [11] provided some of the first\nresults with this approach, and both studied the cart-pole problem. Results for these cases are presented in Table 1 (and shall be discussed further)."}, {"heading": "2.2 Architecture: Indirect BAC", "text": "A schematic of the BAC is shown in Fig. 1, and consists of three networks: the action, model, and critic networks, indicated by the letters \u201cA,\u201d \u201cM,\u201d and \u201cC,\u201d respectively. This architecture, for reasons to be explained shortly, is called the Indirect BAC. For a more detailed description of this architecture, see [3,11]. Note that arrows crossing the left border of the BAC in Fig. 1 correspond to signals to and from the same BAC but for the previous time step, and similarly arrows that cross the right border refer to the next time step. Dashed lines represent feedback errors which may pass through a network with or without affecting its weights; if a dashed line is visible inside the network block the weights are not affected, otherwise they are.\nFor the experiments discussed below all the networks contained a single hidden layer with (seven) Gaussian activation functions and an output layer with linear activation functions.\nThe model network performs system identification. In the preferred form, it predicts the change in state \u2206x as a function of the current state and action, i.e., its weights are adapted to minimize, for all times,\nkm (\u2206x \u2212 \u2206xpred), (2)\nwhere \u2206xpred is the output of the model network, and km is a scaling factor; the feedback path for training the model network is not shown in Fig. 1. Training of this network is best achieved before training the controller by starting the system in a random place in the state space and letting the system evolve with random actions. After this network has adapted satisfactory, its weights are frozen.\nFrom eq. 1 it is easy to show that if the critic predictions are correct then pt = rt+1 + \u03b3pt. Hence, for training the critic, if rt+1 + \u03b3pt is the target and pt the actual output, then the critic error is:\nr\u0302t \u2261 rt+1 + \u03b3pt+1 \u2212 pt, (3)\nwhich corresponds to the following equation for changing the weights of the critic network (see [5] for further clarification):\n\u2206wc,t = \u03b7 r\u0302t+1\u2207pt + momentum, (4)\nwhere wc,t is the weight vector for the critic network, \u03b7 is the learning rate, and \u2207pt is the gradient of pt with respect to the weights. 1\nThe action (controller) network determines which action yt to take as a function of the current state xt (note that in general yt is a vector, but for the cart-pole problem it is just a scalar quantity).\nTo adapt the action network in order to increase pt, a path is needed from the critic network to the action network in order to feed back the gradient of pt. However, since the critic output is a function only of the state (xt), the model network is needed to provide a feedback pathway to the action network. Note that the gradient of pt corresponds to a \u201cerror signal\u201d of one (1) at the output of the critic. In order to establish more accurate gradients (and enhance robustness) a small Gaussian noise component is added to the outputs of the action network. However, feedback is performed with activities corresponding to zero noise (see [3])."}, {"heading": "2.3 Direct BAC Architecture", "text": "Another form of the BAC, called the \u201cDirect BAC,\u201d is virtually the same as the BAC just described except that the critic input consists of the current state and action\u2014no model network is needed. For this case the action is included as input to the critic network.\n1This particular form of the error gradient is referred to as TD(0) by Sutton [5], where the \u201c0\u201d refers to the fact that no past values of the gradient are used in eq. 4."}, {"heading": "3 Single Level BAC Control with the Cart-Pole Prob-", "text": "lem"}, {"heading": "3.1 The Cart-pole System", "text": "The cart-pole system, studied extensively in the literature in connection with reinforcement learning, is shown in the Fig. 2. The state of this system is completely described by four quantities which comprise the state vector xt: the position and velocity of the cart, and angular position and rate of the pole. The goal is to keep the inverted pole balanced and the cart from hitting either end of the track by controlling the force (proportional to yt) exerted on the cart. The equations of motion of the system are:\nx\u0308 = fc + mpL\n[ \u03b8\u03072 sin \u03b8 \u2212 \u03b8\u0308 cos \u03b8 ] mc + mp\n(5)\n\u03b8\u0308 = g sin \u03b8 + cos \u03b8\n[ \u2212fc \u2212 mpL\u03b8\u03072 sin \u03b8\nmc+mp ] L [ 4 3 \u2212 mp cos2 \u03b8 mc+mp\n] (6) where,\nmc = 1.0 kg = mass of the cart\nmp = 0.1 kg = mass of the pole\nL = 1.0m = (nominal) length of the pole g = 9.8m/s2 = acceleration of gravity\nThe external reinforcement signal (r) used for the experiments was:\nr =\n\u221a( \u03b8\n\u03b8r\n)2 + ( x\nxr\n)2 , (7)\nwhere \u03b8r = 12 o and xr = 2.4m are the ranges of the motion allowed for the pole angle and cart position, respectively; if either range is exceeded, a new trial begins. Note that I generally found the learning to be roughly half as fast if the system was failure driven, i.e., r = \u22121 when a failure occurs, or r = 0 otherwise (see [11] for more detailed performance comparisons).\nTo balance the pole it is necessary to have this force adjusted fairly frequently. However, the gross motion of the cart depends on the short-term averaged angle of the pole; if it is generally tilted to the right the cart will slowly accelerate to the right (actually, a small oscillatory motion which keeps the pole balanced will be superimposed on this acceleration\u2014note that if the pole is \u201cbalanced\u201d at a constant angle \u03b8b, the acceleration of the cart is x\u0308 = g tan \u03b8b). The point is that the time constant of this gross motion of the cart is much longer than that of the pole. In other words, this problem requires strong critic foresight in order to keep the cart centered. This fact motivated me to perform similar experiments as my earlier paper [11] but with a slower servo rate, i.e., all other parameters were kept constant except for the servo rate."}, {"heading": "3.2 Results for the Single-Level BAC with Different Servo Rates", "text": "The results of these experiments are presented in Table 1. An experiment consisted of letting the BAC learn until it either stabilized the system (called a \u201csuccess\u201d in Table 1) or until the number of trials reached 1,200 (or 3,000 for the Direct BAC), constituting a \u201cfailure.\u201d The parameters for all the experiments the (except case 4) were identical; the learning rates for the action and critic networks were .01 and .02, respectively. The model network was trained in each case for 1,000 time steps.\nThe results in Table 1 show that both BAC\u2019s (Indirect and Direct) had superior performance for the faster servo rates. This was mostly likely for the following reason: although the shortened pole is harder to balance for a slower servo rate (the gains must be more finely tuned), less foresight is required from the critic to center the cart.\nThe data for case 4 in Table 1 was obtained from Jordan and Jacobs [12], who used an architecture very similar to the Direct BAC but with a different reinforcement signal (related to the time to failure). However, whereas the criteria for a \u201csuccess\u201d was 20,000 time steps for other cases in Table 1, only 1,000 time steps qualified as a success in Jordan and Jacobs paper.\nNote that the control logic involved in stabilizing the cart-pole system is not very complicated. The main problem seems to be the long time constant of the cart\u2019s motion. The results of Table 1 strongly corroborate this hypothesis."}, {"heading": "4 The Two-Level BAC with Explicit Low-level Role", "text": "Put most simply, the principle of the two-level BAC is this: use a low-level (LL ) BAC with a fast servo rate to directly control the actuators for short term stability, and a high-level (HL ) BAC with a slower servo rate to control the LL BAC to\nmaximize the (long term) reinforcement. The preferred form of the two-level BAC is shown in Fig. 3. Only one HL BAC and one LL BAC are used in this architecture. The HL BAC updates once for every N LL BAC updates, where N was 40 for the simulations below.2\nThe HL BAC is virtually identical to the \u201cstandard\u201d BAC shown in Fig. 2, except the HL action, Yt, is used as part of the input to the LL instead of determining the force on the cart. Yt is referred to as the \u201cplan\u201d; in general it can be a vector, but for examples in this paper it is just a scalar quantity. Note that the HL receives the usual reinforcement from the environment.\nFig. 4 shows the architecture of the LL BAC, which has the reinforcement signal\nr\u2032t = (Yt\u2212L \u2212 \u03b8t)2 (8)\nwhere \u03b8t is the angle of the pole from top center, and L is the number of elapsed time steps since the last HL update. In other words, the LL controller learns to perform actions (forces on the cart) that keep the pole at an angle equal to the plan input. Note that the HL action, Y , is input to the LL action and critic networks.\nIt is most desirable that the environment of each BAC have the Markov property, i.e., that future states and reinforcement signals depend only on the current state and action(s). As Watkins pointed out [13], if the LL controller is subject to state transitions that are not entirely caused by its own actions then the Markov property does not hold for the LL controller. However, the case here is a little more subtle. Suppose that, instead of predicting the value of the cumulative reinforcement (p),\n2Actually, any value N between 10 and 50 works for the most of the examples below.\nthe LL critic predicted just r for the next time step (this is equivalent to predicting p for \u03b3 = 0). Since the LL critic has no way of knowing whether Y is going to change on the next time step, it would do a poor job of predicting r since the latter depends on Y . However, because Y does stay constant for many (N) LL time steps between each change in its value, the LL critics prediction would be somewhere in the \u201cball park;\u201d it would tend to yield a short term average of the oncoming values of r. The fact that an accumulation of r is predicted rather than just r significantly mitigates the prediction difficulty because of the averaging effect of the accumulation.\nIt was found for the simulations described below that the LL controller failed to work if the HL action was not included in the LL critic input space. Otherwise the LL learned to control the pole angle reliably.\nNote that the LL model network does not require the HL action as part of its input. This would not be the case, however, if the high level actions affected the cart-pole system through some other channel than the low level controller, such as through additional forces on the cart.\nThe single-level Indirect BAC (of the previous section) works best with two phases of learning\u2014the model network is trained in the first phase for random actions, then the action network is trained via the feedback from the critic. A similar situation occurs for the two-level BAC except that two phases are utilized for each level. The phases are reiterated below for clarity.\nPhase I: The LL model network learns to predict the change in state for random LL actions over time step. The LL action weights are then frozen.\nPhase II: The LL action network learns to balance the pole angle at an angle proportional to the HL action (plan) for random values of the latter. The LL action weights are then frozen.\nPhase III: The HL model network is trained to predict the change in state for random HL actions N time steps ahead. The HL model weights are then frozen.\nPhase IV: The HL action network learns to keep the cart centered.\nFor the two-level Direct BAC only two phases of learning are required, where phases I and II for the Direct BAC correspond to phases II and IV for the Indirect BAC, respectively."}, {"heading": "4.1 Experimental Results", "text": "Table 2 presents results obtained for the cart-pole system with a servo rate of 50 hz\u2014corresponding to the most difficult of the cases presented in Table 1. Note that the performance of the HL controller in phase IV learning was virtually identical to the performance of the system described in [11], where a proportional derivative control loop essentially fulfilled the role of the LL controller in the present example. The learning performance in phases I through III was very reliable, i.e., no local minima were encountered over the ten experiments.\nTABLE 2 Results for Two-Level BAC Architectures with Explicit Low-Level Function over Ten Experiments (50 hz Servo Rate)\nArchitecture Phase Typical No. of Trials Average No. of Time Steps\nIndirect BAC I 800 10,000 \u201d \u201d \u201d II 900 130,000 \u201d \u201d \u201d III 400 150,000 \u201d \u201d \u201d IV 160 10,000\nDirect BAC I 1,600 650,000 \u201d \u201d \u201d II 450 50,000\nOne way to make a direct comparison between the results shown in Table 2 for the two-level BAC with the corresponding single-level BAC (for the 50 hz servo rate) is to compare the total number of time steps prior to success. This yields 300,000 steps for the two-level BAC and 360,000 steps for the single-level BAC (10,000 steps are included in the latter to account for phase I learning\u2014which is not included in Table 1). However, of the thirty experiments in Table 1, only six succeeded, whereas the success ratio for the two-level BAC was much greater (nine successes out of ten experiments for phase IV and no failures in the prior phases).\nGiven the additional complexity associated with the model network, a reasonable conclusion might be that having a model is not worth the effort for a two-level BAC. For the cart-pole problem this might be the case. However, it seems likely that there are many problems where a Direct BAC would be inadequate for a given single level of control. In fact, the single-level BAC experiments of Table 1 indicate that the Indirect BAC was far inferior to the Direct BAC for the 50 hz servo rate."}, {"heading": "5 Response Induction (RI) Learning", "text": "What happens if, instead of forcing the pole to \u201cfollow\u201d the HL plan through an internal LL reinforcement, the LL controller is provided the same (external) reinforcement as the HL ? What role does the HL plan, with its influence on the LL controller, take on for this case, i.e., what \u201csubtask\u201d does the LL controller learn? I generally found that, given enough time, the effects of the HL plans on the LL actions became negligible. Since there was no inclusion of the plans in the reinforcement provided to the LL controller, the plans became disturbances to be rejected.\nOne way to induce the LL to learn to react to the HL plan signals without expressing explicitly how to react to them is to affect the weights of the action network such that each\n\u03b4i \u2261 \u2202p\n\u2202Yi , \u2200 i \u2208 P (9)\nis some significant positive value, where p is the LL critic output (see eq. 1), Yi is the ith HL plan input, and P is the set of units in the LL action network corresponding to the plan inputs; note that The term \u201c\u03b4\u201d is used in the same sense as Rumelhart etal. [9], Vol. 1, Chapter 8, with respect to backpropagation. Again, for this case the LL BAC receives the same reinforcement signal from the environment as the HL BAC.\nPerhaps the more obvious approach for inducing a response would be to replace \u2202p/\u2202Yj with \u2202yi/\u2202Yj in eq. 9, where yi and Yj correspond to the ith and jth LL and HL action signals, respectively. Yet this was found experimentally to be much less effective. The disturbing effect of the response induction usually kept the LL BAC from keeping the system stable for any appreciable amount of time. This was probably because response induction based on the action network output requires an immediate effect, whereas the induction based on the critic output elicits a response which can be mitigated over several time steps (since the critic output reflects present and future events). It is fortuitous that most of the factors for critic-based RI learning are already computed during LL action network learning.\nTo see how the LL action network weights are adapted to increase \u03b4i (for i \u2208 P), consider the following error functon for the output of the LL action network:\nE = \u2212Ecritic + Einfluence, (10)\nwhere\nEinfluence = \u2212 k1 np \u2211 i\u2208P e\u2212\u03b4 2 i /k 2 2 , (11)\nwhere P is the set of input units corresponding to the plan inputs, Einfluence is the \u201cinfluence error,\u201d and Ecritic is the effective error of the LL action output due to feedback from the critic network (during phase II learning for the two-level Indirect BAC or phase I learning for the two-level Direct BAC). The weights of the LL action network are adapted such as to maximize E in eq. 10. The constant k1 determines the maximal degree of the plans influence and the constant k2 determines the rate at which the influence is induced (I generally obtained adequate results with k2 = k1/2; note that if k2 is set too large the maximal induction, \u03b4i \u2248 k1, will not be achieved).\nFor the experiments, the weight of each connection between units in the hidden layer to the (single) plan input unit was adjusted an amount proportional to \u2202E/\u2202wji (the rest were adjusted normally):\n\u2206wji = \u03b7 [ \u03b4jYi + k1 k2 \u03b4i\u03b4j e \u2212\u03b42i /k 2 2 ] + momentum, (12)\nwhere \u03b7 is the learning rate, wji is the weight connecting the jth hidden unit to the ith input unit (the plan unit), and \u03b4j corresponds to the jth hidden unit and is defined in the manner of Rumelhart etal.\nWith the approach just described the LL action network essentially learns to maximize its reinforcement while responding in some fashion to theHL plans (which\nchanges every N LL time steps). Experiments with the cart-pole system revealed that the LL action network reliably produced smooth and significant responses to theHL plans while keeping the pole balanced. At the very beginning of the LL BAC learning \u03b4i was typically small, and hence the rate of the response induction was small because operation was in the central (flatter) region of the Gaussian \u201cbowl.\u201d This allowed the system to achieve reasonable stability before stronger responses to the HL actions were induced.\nFigure 5 shows the evolution of \u03b4i for three typical experiments during phase II two-level Indirect BAC learning; note that i = 5, i.e., the first four inputs to the action network were the state variables, and the fifth was the single plan input. Both curves were smoothed by averaging the steps per trial (and the \u03b4i) into bins of 50 trials. This plot shows how the response induction occurs after moderate stability of the system has been achieved.\nI have ignored so far the implications of RI learning for case when the plan input has more than one component. For this case, it is desirable that the response trajectory due to each plan component be different than the response trajectories due to the remaining plan components, i.e., that the same response trajectory for one plan component value cannot be achieved for some value of another plan component. The fact that the plan components vary independently with respect to each other during RI learning does not guarantee that the trajectories of the responses will be be different, although it seems highly likely that they will be somewhat different. Finding methods which enhance the diversity of the response trajectories is certainly one area for future research."}, {"heading": "5.1 Experimental Results with RI Learning", "text": "During the experiments it was usually the case that a change in the plan input caused a fairly rapid shift in the pole angle, after which the pole moved smoothly from the new (shifted) position, usually in the direction from which it started. In several experiments, however, the LL was observed to balance the pole at an angle\nproportional to the plan signal, just as in the more supervised case in Sec. 1. After performing some degree of (RI) parameter optimization, I performed fifteen experiments using RI learning during phase II for the two-level Indirect BAC; all other parameters were identical to those used for obtaining the results given in Table 2. During phase II RI learning learning and phase III HL model learning, uniformly random HL actions in the ranges [-.3, .3] and [-.7, .7], respectively, were provided to the LL BAC. The values of k1 and k2 were .35 and .14, respectively.\nOver these fifteen experiments, the average number of trials/(time steps) deemed adequate for phase II RI learning learning was 1,300/190,000. The average number of trials/(time steps) deemed adequate for phase III (HL model) learning was 1,000/140,000. For phase IV learning, five of the experiments resulted in finding a solution within 700 trials (otherwise the experiment was halted). The average number of trials/(time steps) for the solutions was 470/25,000.\nA common problem observed during the (ten) failures was that the state space was covered insufficiently during RI learning. This problem was tacitly eliminated for the explicit role case of the previous section in that the corresponding response did cover the solution space, i.e., the random plan signals corresponded to random pole angles, distributed around its top center position."}, {"heading": "5.2 Outlook on RI Learning", "text": "Inducing the LL action network simply to react to the plan signals while maximizing its local reinforcement seems crude in the sense that the type of reaction learned may not necessarily be useful for HL performance. However, because it is so general it lends considerable leeway to the LL action network for reinforcement maximization. This leeway could be utilized most effectively, perhaps, by augmenting the \u201cenvironmental\u201d reinforcement signal with terms related to such factors as state space exploration and/or to trajectory smoothing. It may be useful to construct separate critics solely for the purpose of response induction, letting a new critic respond to some of the factors just mentioned. In any case, for most applications it seems most desirable that the LL behavior with respect to HL actions be deterministic, and that the state space is adequately covered."}, {"heading": "6 Role Learning Based on Time Constant Identification", "text": "Another potential way to induce useful LL role emergence is more directly related to the \u201cexplicit function\u201d learning in Sec. 3. This would be to have the intelligent controller figure out itself which components of the plant\u2019s state variables are \u201cslow\u201d and which are \u201cfast\u201d and apportion control appropriately (e.g., as the control was apportioned in Sec. 3). This would not be as general as RI learning in that specific state variables must be identified and routed to different levels of control, whereas in RI learning different functions of the state variables may implicitly be routed. However, it seems like this might be more practical than RI learning for the near term."}, {"heading": "7 Conclusion", "text": "In this paper I have addressed the problem of extending the time span over which causes and effects can be accounted for reliably by a Backpropagated Adaptive Critic. The general ideas, of course, are applicable to other incremental learning methods (e.g., the feedback in time method [8]).\nThe functional allocation within the hierarchical architecture has been approached from two extremes: in single case, Explicit Role Learning (ERL), the role of the low level controller is predefined based on knowledge of the system, and in the other case, Response Induction Learning (RIL), no knowledge and very little structure is imbedded in the architecture.\nResults for the two-level BAC with ERL show that it is considerably more reliable than a corresponding single-level BAC for the cart-pole problem, although at a cost of more user interaction. It is likely that the model and action/critic learning phases within each BAC level can be meshed together in a form of learning that has attributes of both of the original (different) phases. This is certainly one area for future work (for single-level Indirect BAC learning as well as the hierarchical case). Elimination of the phases related to separate learning for the low and high level BAC\u2019s seems to be fundamentally much more difficult because of the desirability of surrounding each BAC (level) with a stationary environment. Perhaps the most promise lies in providing mechanisms for automatically detecting when to switch these phases, rather than finding ways to eliminate the phases themselves.\nRIL was introduced as an example of how the emergence of functional roles might be achieved within a hierarchical (reinforcement) control system. The method is very reliable at inducing responses while allowing the controller to \u201cmind\u201d the reinforcement signals. However, such responses, while usually quite deterministic, were not always conducive to solving the problem. However, the generality of the method leaves room for many varieties of enhancement.\nThe extension of the present approach to more than two levels of control seems straightforward in principle. For this case, the servo rates of each level would be progressively slower as the hierarchy is ascended, and each level would control the level beneath it. Also, the present discussion has also been limited to cases where control signals are issued at equal time intervals. The extension to intermittent control is another important area for future research.\nIs this hierarchical BAC approach valid for more challenging problems than the cart-pole system? The difficulty with the latter seems not to be with the control logic required (which is fairly simple), but rather with the widely differing time constants of the motion of the cart and pole, a hypothesis which is strongly corroborated by results in Table 1. It is likely that a single-level BAC can learn much more (logically) difficult problems than the cart-pole as long as causes and effects occur within reasonable time spans. The multi-level BAC is just a way of extending the performance when large differences in time constants are present, whether the \u201clogical\u201d problem addressed within each level is difficult or not.\nI"}], "references": [{"title": "Neuronlike Elements That Can Solve Difficult Control Problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1983}, {"title": "Beyond Regression, New Tools for Prediction and Analysis in the Behavioral Sciences, Ph.D", "author": ["P.J. Werbos"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1989}, {"title": "A Menu of Designs for Reinforcement Learning Over Time, Neural Networks for Control", "author": ["P.J. Werbos"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Strategy Learning with Multilayer Connectionist Representations", "author": ["C.W. Anderson"], "venue": "Tech. Report TR87-509.3,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Learning to Predict by the Methods of Temporal Differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Temporal Credit Assignment in Reinforcement Learning, Ph.D", "author": ["R.S. Sutton"], "venue": "Thesis, Dept. of Computer and Information Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1984}, {"title": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming,", "author": ["Sutton", "R. S"], "venue": "Proc. 7th Int. Conf. on Machine Learning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "The Truck Backer-Upper: An Example of Self-Learning", "author": ["D. Nguyen", "B. Widrow"], "venue": "Proc. IEEE Int. Conf. on Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Neural Network Based Process Optimization and Control", "author": ["Sofge", "D.A.D.A. White"], "venue": "Proc. 29th Conf. on Decision and Control,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "A Neurocontroller Based on Model Feedback and the Adaptive Heuristic Critic", "author": ["J.W. Jameson"], "venue": "Proc. IEEE Int. Conf. on Neural Networks, San Diego,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Learning to Control an Unstable System with Forward Modeling", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1989}, {"title": "Learning from Delayed Rewards, Ph.D", "author": ["C.J. Watkins"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Real Time Application of Neural Networks for Sensor- Based Control of Robots with Vision", "author": ["W.T. Miller"], "venue": "IEEE Trans. on Sys., Man, and Cybernetics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1989}, {"title": "Making the World Differentiable: On Using Supervised Learning Fully Recurrent Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments, Report FKI-126-90", "author": ["J.H. Schmidhuber"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "Perhaps the most recognized early work on this subject is a 1983 paper by Barto, Sutton, and Anderson [1] which demonstrated stabilization of the oft-studied cart-pole problem (see Figure 2).", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "Sutton\u2019s work on temporal differences [5] refined these notions, and extended their application to nonlinear feedforward neural networks.", "startOffset": 38, "endOffset": 41}, {"referenceID": 11, "context": "Watkins [13] presented an interesting paradigm for hierarchical control based on ship navigation which has several parallels to, the present work.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "Anderson [4] later used critic and action modules based on backpropagated multilayer perceptrons for stabilization of the cart-pole problem.", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "Werbos [2, 3] introduced an architecture called the Backpropagated Adaptive Critic (BAC) for which a utility function similar to a critic is used as a guide for training the action module.", "startOffset": 7, "endOffset": 13}, {"referenceID": 2, "context": "Werbos [2, 3] introduced an architecture called the Backpropagated Adaptive Critic (BAC) for which a utility function similar to a critic is used as a guide for training the action module.", "startOffset": 7, "endOffset": 13}, {"referenceID": 10, "context": "Jordan and Jacobs [12] and Jameson [11] provided some of the first", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "Jordan and Jacobs [12] and Jameson [11] provided some of the first", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "For a more detailed description of this architecture, see [3,11].", "startOffset": 58, "endOffset": 64}, {"referenceID": 9, "context": "For a more detailed description of this architecture, see [3,11].", "startOffset": 58, "endOffset": 64}, {"referenceID": 4, "context": "which corresponds to the following equation for changing the weights of the critic network (see [5] for further clarification):", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "However, feedback is performed with activities corresponding to zero noise (see [3]).", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "This particular form of the error gradient is referred to as TD(0) by Sutton [5], where the \u201c0\u201d refers to the fact that no past values of the gradient are used in eq.", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": ", r = \u22121 when a failure occurs, or r = 0 otherwise (see [11] for more detailed performance comparisons).", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "This fact motivated me to perform similar experiments as my earlier paper [11] but with a slower servo rate, i.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "The data for case 4 in Table 1 was obtained from Jordan and Jacobs [12], who used an architecture very similar to the Direct BAC but with a different reinforcement signal (related to the time to failure).", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "of time steps prior to successful trial dfrom Jordan and Jacobs [12]", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "As Watkins pointed out [13], if the LL controller is subject to state transitions that are not entirely caused by its own actions then the Markov property does not hold for the LL controller.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "Note that the performance of the HL controller in phase IV learning was virtually identical to the performance of the system described in [11], where a proportional derivative control loop essentially fulfilled the role of the LL controller in the present example.", "startOffset": 138, "endOffset": 142}, {"referenceID": 7, "context": ", the feedback in time method [8]).", "startOffset": 30, "endOffset": 33}], "year": 2015, "abstractText": "Present incremental learning methods are limited in the ability to achieve reliable credit assignment over a large number time steps (or events). However, this situation is typical for cases where the dynamical system to be controlled requires relatively frequent control updates in order to maintain stability or robustness yet has some action/consequences which must be established over relatively long periods of time. To address this problem, the learning capabilities of a control architecture comprised of two Backpropagated Adaptive Critics (BAC\u2019s) in a two-level hierarchy with continuous actions are explored. The high-level BAC updates less frequently than the low-level BAC and controls the latter to some degree. The response of the low-level to high-level signals can either be determined a priori or it can emerge during learning. A general approach called Response Induction Learning is introduced to address the latter case.", "creator": "LaTeX with hyperref package"}}}