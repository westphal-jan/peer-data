{"id": "1512.04280", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Small-footprint Deep Neural Networks with Highway Connections for Speech Recognition", "abstract": "For speech recognition, deep neural network (DNN) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models(GMMs), DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g. mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are thinner and deeper and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 70 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy.", "histories": [["v1", "Mon, 14 Dec 2015 12:29:32 GMT  (86kb,D)", "http://arxiv.org/abs/1512.04280v1", "5 pages, 2 figures"], ["v2", "Thu, 3 Mar 2016 12:14:06 GMT  (110kb,D)", "http://arxiv.org/abs/1512.04280v2", "5 pages, 3 figures"], ["v3", "Mon, 20 Jun 2016 10:30:54 GMT  (110kb,D)", "http://arxiv.org/abs/1512.04280v3", "5 pages, 3 figures, accepted by Interspeech 2016"], ["v4", "Wed, 14 Jun 2017 15:17:27 GMT  (110kb,D)", "http://arxiv.org/abs/1512.04280v4", "5 pages, 3 figures, fixed typo, accepted by Interspeech 2016"]], "COMMENTS": "5 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["liang lu", "steve renals"], "accepted": false, "id": "1512.04280"}, "pdf": {"name": "1512.04280.pdf", "metadata": {"source": "CRF", "title": "Small-footprint Deep Neural Networks with Highway Connections for Speech Recognition", "authors": ["Liang Lu", "Steve Renals"], "emails": ["s.renals}@ed.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5]. A typical architecture is the deep neural network (DNN) [1, 2], which is a feedforward neural network with multiple hidden layers (e.g. 4 \u223c 9), and each layer has a large number of hidden units (e.g. 512 \u223c 2048). Compared to the conventional Gaussian mixture models (GMMs), DNN acoustic models usually have much larger number of model parameters, which explains their large statistical modelling capacities and high recognition accuracies. However, it becomes challenging for the applications of DNN-based speech recognition systems in resource constrained scenarios. For instance, it is highly desirable that the speech recognition system can still function in wearable computing and mobile devices when the internet connection is unavailable. This requires smaller size of acoustic models that can still achieve high recognition accuracy.\nThere have been a number of works on small footprint DNNs for this purpose. For instance, Xue et al. [6] and Sainath et al. [7] approximate the weight matrix between two hidden layers by a product of two low-rank matrices, which may be equivalent to insert a bottleneck layer in between without the nonlinear activation. Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12]. In this approach, the teacher may be a large-size network or an ensemble of several different models, which is used to predict the soft targets for training the student model that is much smaller. As discussed in [12], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these labels is observed to perform better than a small model trained in the usual way [8, 9, 10]. Recently, [13] investigated the use of low rank displacement of structured matrices (e.g. Teoplitz matrix) for small-footprint neural networks. This work is in line with the argument that the neural networks with dense connections are overparameterised, and the linear layer may be replaced by structured efficient linear layers (SELLs) [14, 15, 16].\nIn this paper, we investigate the thin and deep architectures for small-footprint neural network acoustic models. However, as the depth increases, training DNNs by stochastic gradient decent (SGD) becomes increasingly difficult due to the highly non-convexity of the error surface. One approach is to pre-train the neural network by unsupervised [17] or greedy layer-wise fashion [18]. However, this approach cannot circumvent the difficulty arise in the fine tuning stage. Another approach is to rely on the teacher-student architecture, e.g. the FitNet [10], but it requires the additional effort to train the teacher model beforehand. Our work in this paper builds on the recently proposed highway networks [19], where the transform gate is used to scale the output of a hidden layer and the carry gate is used to pass through the input directly after elementwise rescaling. Similar idea also has been tried on long short-term memory recurrent neural networks (LSTM-RNN) for speech recognition [20]. In our study, we observe that the highway connections ar X iv :1\n51 2.\n04 28\n0v 1\n[ cs\n.C L\n] 1\n4 D\nec 2\n01 5\ncan be successfully applied to training thinner and deeper networks, while still retraining the recognition accuracy. Our experiments were performed on the AMI meeting speech transcription corpus, which contains around 70 hours of training data. Using highway neural networks, we managed to cut down the number of model parameters by over 80% with marginal accuracy loss compared to our baseline DNN acoustic model."}, {"heading": "2. Highway Deep Neural Network", "text": ""}, {"heading": "2.1. Deep neural networks", "text": "A DNN is a feed-forward neural network with multiple hidden layers that performs cascaded layer-wise nonlinear transformations of the input. For a network with L hidden layers, the model may be represented as\nh1 = f(x, \u03b81) (1) hl = f(hl\u22121, \u03b8l), for l = 2, . . . , L (2) y = g(hL, \u03d5) (3)\nwhere x is an input vector to the network; f(hl\u22121, \u03b8l) denotes the transformation of the input hl\u22121 with the parameter \u03b8l followed by a nonlinear activation function (e.g. sigmoid or tanh); g(\u00b7, \u03d5) is the output function(e.g. softmax) which is parameterised by \u03d5 in the output layer. Given the ground truth target y\u0302, the network is usually trained by gradient decent to minimise a loss function L(y, y\u0302) (e.g. cross-entropy). However, as the number of hidden layers increases, the error surface becomes increasingly non-convex, and it is more possible to find a poor local minima using gradient-based optimisation algorithms with random initialisation [21]. Furthermore, [22] showed that the variance of the back-propagated gradients may become small in the lower layers if the model parameters are not initialised properly."}, {"heading": "2.2. Highway connections for deep networks", "text": "There have been numerous studies on overcoming the difficulties in training very deep neural networks, including pre-training [17, 18], normalised initialisation [22], deeply-supervised networks [23], etc. Recently, Srivastav et al. [19] proposed the highway network and demonstrated the impressive results of using this approach to train very deep networks, e.g. up to 100 hidden layers. In the highway network, the hidden layers are augmented with two gating functions, which can be represented as\nhl = f(hl\u22121, \u03b8l) \u25e6 T (hl\u22121,WT ) +hl\u22121 \u25e6 C(hl\u22121,Wc) (4)\nwhere T (\u00b7) is the transform gate that scales the original hidden activations; C(\u00b7) is the carry gate, which scales the input before passing it directly to the next hidden layer; \u25e6 denotes elementwise (Hadamad) product; The outputs of T (\u00b7) and C(\u00b7) are constrained to be\n[0, 1], and we use sigmoid functions for both gates parameterised by WT and Wc respectively. Unlike [19], in this work, we do not use any bias vector in the two gate functions. In [19], the carry gate is constrained to be C(\u00b7) = 1\u2212T (\u00b7), while in this work, we evaluate the generalisation ability of highway networks with and without this constraint.\nWithout the transform gate, i.e. T (\u00b7) = 1, the highway network is similar to a network with skip connections, with the main difference that the input is firstly scaled by the carry gate. Without the carry gate, i.e. C(\u00b7) = 0, the hidden layer is\nhl = f(hl\u22121, \u03b8l) \u25e6 T (hl\u22121,WT ). (5)\nIt looks similar to the dropout regularisation for neural networks [24], which may be represented as\nhl = f(hl\u22121, \u03b8l) \u25e6 , i \u223c p( i), (6)\nwhere p( i) is a Bernoulli distribution for each element in as originally proposed in [24], while it was shown later that using a continuous distribution with well designed mean and variance works as well or better [25]. From this perspective, the transform gate may work as a regulariser, but with the key difference that T (\u00b7) is a deterministic function, while i is drawn stochastically from a predefined distribution in dropout. Nevertheless, our empirical results (cf. Section 3.3) indicate that the transform gate and the carry gate can speedup the convergence rate. In addition, the highway networks also generalise better when measured in terms of recognition accuracy, which is presumably due to the regularisation effect of the two gating functions. Further more rigorous analysis on this aspect is required."}, {"heading": "2.3. Small-footprint networks", "text": "The aim of this paper is to train small-footprint neural networks for resource constrained speech recognition. From Eq. (4), the highway network is not directly suitable for this purpose, because it introduces additional computational cost and model parameters in the two gating functions. The rationale is that the computational complexity and the number of model parameters for each layer in a densely connected network are in the order of O(n2), where n is the size of hidden units. Increasing the depth of the network only linearly increases the computational cost and the model size, while reducing the width will lead to the quadratic reduction. Highway connections make it feasible to train very thin and deep networks, and therefore the overall model sizes are much smaller. To further save the model parameters, in this work, we shared the two gates for all hidden layers so that the additional number of model parameters for T (\u00b7) and C(\u00b7) is relatively small."}, {"heading": "3. Experiments", "text": ""}, {"heading": "3.1. System setup", "text": "Our experiments were performed on the AMI meeting speech transcription dataset [26]. The amount of training data is around 70 hours, corresponding to roughly 28 million frames. This dataset is much larger than most of the datasets (e.g. MNIST, CIFAR, etc.) where other types of thin and deep networks were evaluated [10, 19]. We used 40-dimensional fMLLR adapted features vectors normalised on per-speaker level, which were then spliced by a context window of 15 frames (i.e. \u00b17) for all the systems. The number of tied HMM states is 3972, and all the DNN systems were trained with the same alignment. The results reported in this paper were obtained using the CNTK toolkit [27] with the Kaldi decoder [28], and the networks were trained using the cross entropy (CE) criterion without pre-training unless specified otherwise. We set the momentum to be 0.9 after the 1st epoch, and we used the sigmoid activation for the hidden layers. The weights in each hidden layer were randomly initialised with a uniform distribution in the range of [\u22120.5, 0.5] and the bias parameters were initialised to be 0 for CNTK systems. We used a trigram language model for decoding."}, {"heading": "3.2. Depth vs. Width", "text": "Table 1 shows the word error rate (WERs) of plain DNNs and highway networks (HDNNs) with different configurations. As the number of hidden units decrease, the\naccuracy of plain DNNs degrade rapidly, and the accuracy loss cannot be compensated by increasing the depth of the network. We faced the difficulty to train thin and deep networks directly without RBM pre-training, where the CE loss did not decrease at all after many epochs. However, with highway connections we did not have this difficulty. The HDNNs achieved consistent lower WERs compared to the plain DNN counterparts, and the margin of the gain also increases as the number of hidden units becomes smaller as shown in Figure 1. With the highway connections, we can cut down the number of model parameters by around 80% with marginal accuracy loss, and with less than 1 million model parameters, the CE trained HDNN can achieve comparable or slight higher accuracy compared to a strong GMM baseline with speaker adaptive training (SAT) and bMMI-based discriminative training. The accuracy of smaller-size HDNN models may be further improved by the teacher-student style training, which will be investigated in the future."}, {"heading": "3.3. Transform gate vs. Carry gate", "text": "We then evaluated the specific role of the transform and carry gate in the highway architectures. The results are shown in Table 2, where we disabled one of both of the gates. We observed that using only one of the two gates, the HDNN can still achieved lower WER compared to the plain DNN, but the best results were obtained using both of the gates, indicating that the two gates are complementary to each other. Figure 2 shows the convergence curve of training HDNNs with and without the transform and carry gate. We observed that it converged fast when both of the gates were turned on, and with only the transform gate, the convergence rate was much slower. As discussed before, the carry gate can be viewed as a particular type of skip connection, and it was observed to deliver much faster convergence rate compared to the transform gate."}, {"heading": "3.4. Constrained carry gate", "text": "We also evaluated using the constrained carry gate in our experiments, where C(\u00b7) = 1 \u2212 T (\u00b7) as studied in [19]. In this approach, the computational cost is reduced since the matrix-vector multiplication for the carry gate is not required. We evaluated this configuration with 10-layer neural networks, and the results are shown in Table 3. Contrary to our expectations, with the constrained carry gate we obtained worse results when the networks were relatively wide, while the accuracy gap was reduced when the number of hidden units was smaller. The reason may be that in the constrained setting, the transform gate T (\u00b7) learns the scaling function for both the input and output at the same time. As regularisation is expected to be more important for training wide and deep networks, this may\nnot be achieved by using a single gating function. For instance, both the input and output of one hidden layer may require larger or smaller scaling weights at the same time, which is impossible in the constrained setting. In the future, we shall look into the regularisation and generalisation properties of the two gating functions more closely."}, {"heading": "4. Conclusions", "text": "In this paper, we investigate the training of thin and deep neural networks for small-footprint acoustic models. Our study is build on the recently proposed highway neural network, which introduces an additional transform and carry gate for each hidden layer. Our experiments indicate that the highway connections can facilitate the information flow and mitigate the difficulty in training very deep feedforward networks. The thin and deep architecture with highway connections achieved consistently lower WERs compared to plain DNNs, and by reducing the number of hidden units, we can significantly cut down the total number of model parameters with negligible accuracy loss. We also evaluated the specific role of the transform and carry gate, and the carry gate was more important to speedup the convergence rate in our experiment. The small-footprint highway networks may be further improved by the teacher-student style training, which will be investigated in our future work.\nAcknowledgement: This work is funded by the EPSRC Programme Grant EP/I031022/1, Natural Speech Technology (NST). The NST research data collection may be accessed at http://datashare.is.ed.ac.uk/handle/10283/786. We thank Yu Zhang and Dong Yu for helpful discussions on using the CNTK toolkit."}, {"heading": "5. References", "text": "[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed,\nN. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.\n[2] G. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContextdependent pre-trained deep neural networks for largevocabulary speech recognition,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.\n[3] H. A. Bourlard and N. Morgan, Connectionist speech recognition: a hybrid approach. Springer, 1994, vol. 247.\n[4] N. Morgan and H. A. Bourlard, \u201cNeural networks for statistical recognition of continuous speech,\u201d Proceedings of the IEEE, vol. 83, no. 5, pp. 742\u2013772, 1995.\n[5] S. Renals, N. Morgan, H. Bourlard, M. Cohen, and H. Franco, \u201cConnectionist probability estimators in HMM speech recognition,\u201d IEEE Transactions on Speech and Audio Processing, vol. 2, no. 1, pp. 161\u2013174, 1994.\n[6] J. Xue, J. Li, and Y. Gong, \u201cRestructuring of deep neural network acoustic models with singular value decomposition.\u201d in Proc. INTERSPEECH, 2013, pp. 2365\u20132369.\n[7] T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran, \u201cLow-rank matrix factorization for deep neural network training with high-dimensional output targets,\u201d in Proc. ICASSP. IEEE, 2013, pp. 6655\u20136659.\n[8] J. Li, R. Zhao, J.-T. Huang, and Y. Gong, \u201cLearning smallsize dnn with output-distribution-based criteria,\u201d in Proc. INTERSPEECH, 2014.\n[9] J. Ba and R. Caruana, \u201cDo deep nets really need to be deep?\u201d in Proc. NIPS, 2014, pp. 2654\u20132662.\n[10] R. Adriana, B. Nicolas, K. Samira Ebrahimi, C. Antoine, G. Carlo, and B. Yoshua, \u201cFitnets: Hints for thin deep nets,\u201d in Proc. ICLR, 2015.\n[11] C. Bucilua\u030c, R. Caruana, and A. Niculescu-Mizil, \u201cModel compression,\u201d in Proc. ACM SIGKDD, 2006.\n[12] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d in Proc. NIPS Deep Learning and Representation Learning Workshop, 2015.\n[13] V. Sindhwani, T. N. Sainath, and S. Kumar, \u201cStructured transforms for small-footprint deep learning,\u201d in Proc. NIPS, 2015.\n[14] Q. Le, T. Sarlo\u0301s, and A. Smola, \u201cFastfood-approximating kernel expansions in loglinear time,\u201d in Proc. ICML, 2013.\n[15] Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola, L. Song, and Z. Wang, \u201cDeep fried convnets,\u201d in Proc. ICCV, 2015.\n[16] M. Moczulski, M. Denil, J. Appleyard, and N. de Freitas, \u201cACDC: A Structured Efficient Linear Layer,\u201d arXiv preprint arXiv:1511.05946, 2015.\n[17] G. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[18] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle et al., \u201cGreedy layer-wise training of deep networks,\u201d in Proc. NIPS, vol. 19, 2007, p. 153.\n[19] R. K. Srivastava, K. Greff, and J. Schmidhuber, \u201cTraining very deep networks,\u201d in Proc. NIPS, 2015.\n[20] Y. Zhang, G. Chen, D. Yu, K. Yao, S. Khudanpur, and J. Glass, \u201cHighway Long Short-Term Memory RNNs for Distant Speech Recognition,\u201d arXiv preprint arXiv:1510.08983, 2015.\n[21] D. Erhan, P.-A. Manzagol, Y. Bengio, S. Bengio, and P. Vincent, \u201cThe difficulty of training deep architectures and the effect of unsupervised pre-training,\u201d in International Conference on artificial intelligence and statistics, 2009, pp. 153\u2013160.\n[22] X. Glorot and Y. Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks,\u201d in International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.\n[23] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, \u201cDeeply-supervised nets,\u201d arXiv preprint arXiv:1409.5185, 2014.\n[24] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d arXiv preprint arXiv:1207.0580, 2012.\n[25] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: A simple way to prevent neural networks from overfitting,\u201d The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\n[26] S. Renals, T. Hain, and H. Bourlard, \u201cRecognition and understanding of meetings the AMI and AMIDA projects,\u201d in Proc. ASRU. IEEE, 2007, pp. 238\u2013247.\n[27] D. Yu, A. Eversole, M. Seltzer, K. Yao, Z. Huang, B. Guenter, O. Kuchaiev, Y. Zhang, F. Seide, H. Wang et al., \u201cAn introduction to computational networks and the computational network toolkit,\u201d Tech. Rep. MSR, Microsoft Research, Tech. Rep., 2014.\n[28] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motl\u0131cek, Y. Qian, P. Schwarz, J. Silovsky\u0301, G. Semmer, and K. Vesely\u0301, \u201cThe Kaldi speech recognition toolkit,\u201d in Proc. ASRU, 2011."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Contextdependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist speech recognition: a hybrid approach", "author": ["H.A. Bourlard", "N. Morgan"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Neural networks for statistical recognition of continuous speech", "author": ["N. Morgan", "H.A. Bourlard"], "venue": "Proceedings of the IEEE, vol. 83, no. 5, pp. 742\u2013772, 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["S. Renals", "N. Morgan", "H. Bourlard", "M. Cohen", "H. Franco"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 2, no. 1, pp. 161\u2013174, 1994.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition.", "author": ["J. Xue", "J. Li", "Y. Gong"], "venue": "in Proc. INTERSPEECH,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning smallsize dnn with output-distribution-based criteria", "author": ["J. Li", "R. Zhao", "J.-T. Huang", "Y. Gong"], "venue": "Proc. INTERSPEECH, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Do deep nets really need to be deep?", "author": ["J. Ba", "R. Caruana"], "venue": "in Proc. NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Fitnets: Hints for thin deep nets", "author": ["R. Adriana", "B. Nicolas", "K. Samira Ebrahimi", "C. Antoine", "G. Carlo", "B. Yoshua"], "venue": "Proc. ICLR, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Model compression", "author": ["C. Bucilu\u01ce", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proc. ACM SIGKDD, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "Proc. NIPS Deep Learning and Representation Learning Workshop, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured transforms for small-footprint deep learning", "author": ["V. Sindhwani", "T.N. Sainath", "S. Kumar"], "venue": "Proc. NIPS, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "Proc. ICML, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep fried convnets", "author": ["Z. Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang"], "venue": "Proc. ICCV, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "ACDC: A Structured Efficient Linear Layer", "author": ["M. Moczulski", "M. Denil", "J. Appleyard", "N. de Freitas"], "venue": "arXiv preprint arXiv:1511.05946, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Proc. NIPS, vol. 19, 2007, p. 153.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Proc. NIPS, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "author": ["Y. Zhang", "G. Chen", "D. Yu", "K. Yao", "S. Khudanpur", "J. Glass"], "venue": "arXiv preprint arXiv:1510.08983, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["D. Erhan", "P.-A. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "International Conference on artificial intelligence and statistics, 2009, pp. 153\u2013160.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Deeply-supervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "arXiv preprint arXiv:1409.5185, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1929}, {"title": "Recognition and understanding of meetings the AMI and AMIDA projects", "author": ["S. Renals", "T. Hain", "H. Bourlard"], "venue": "Proc. ASRU. IEEE, 2007, pp. 238\u2013247.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "Z. Huang", "B. Guenter", "O. Kuchaiev", "Y. Zhang", "F. Seide", "H. Wang"], "venue": "Tech. Rep. MSR, Microsoft Research, Tech. Rep., 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motl\u0131cek", "Y. Qian", "P. Schwarz", "J. Silovsk\u00fd", "G. Semmer", "K. Vesel\u00fd"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 1, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 2, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 3, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 4, "context": "Modern state-of-the-art speech recognition systems are based on the neural network acoustic models [1, 2, 3, 4, 5].", "startOffset": 99, "endOffset": 114}, {"referenceID": 0, "context": "A typical architecture is the deep neural network (DNN) [1, 2], which is a feedforward neural network with multiple hidden layers (e.", "startOffset": 56, "endOffset": 62}, {"referenceID": 1, "context": "A typical architecture is the deep neural network (DNN) [1, 2], which is a feedforward neural network with multiple hidden layers (e.", "startOffset": 56, "endOffset": 62}, {"referenceID": 5, "context": "[6] and Sainath et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] approximate the weight matrix between two hidden layers by a product of two low-rank matrices, which may be equivalent to insert a bottleneck layer in between without the nonlinear activation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 72, "endOffset": 82}, {"referenceID": 8, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 72, "endOffset": 82}, {"referenceID": 9, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 72, "endOffset": 82}, {"referenceID": 10, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Another branch of studies are based on the teacher-student architecture [8, 9, 10], which is also refereed to as model compression [11] or knowledge distillation [12].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "As discussed in [12], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these labels is observed to perform better than a small model trained in the usual way [8, 9, 10].", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "As discussed in [12], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these labels is observed to perform better than a small model trained in the usual way [8, 9, 10].", "startOffset": 234, "endOffset": 244}, {"referenceID": 8, "context": "As discussed in [12], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these labels is observed to perform better than a small model trained in the usual way [8, 9, 10].", "startOffset": 234, "endOffset": 244}, {"referenceID": 9, "context": "As discussed in [12], the soft targets provided by the teacher encode the generalisation power of the teacher, and the student model trained using these labels is observed to perform better than a small model trained in the usual way [8, 9, 10].", "startOffset": 234, "endOffset": 244}, {"referenceID": 12, "context": "Recently, [13] investigated the use of low rank displacement of structured matrices (e.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "This work is in line with the argument that the neural networks with dense connections are overparameterised, and the linear layer may be replaced by structured efficient linear layers (SELLs) [14, 15, 16].", "startOffset": 193, "endOffset": 205}, {"referenceID": 14, "context": "This work is in line with the argument that the neural networks with dense connections are overparameterised, and the linear layer may be replaced by structured efficient linear layers (SELLs) [14, 15, 16].", "startOffset": 193, "endOffset": 205}, {"referenceID": 15, "context": "This work is in line with the argument that the neural networks with dense connections are overparameterised, and the linear layer may be replaced by structured efficient linear layers (SELLs) [14, 15, 16].", "startOffset": 193, "endOffset": 205}, {"referenceID": 16, "context": "One approach is to pre-train the neural network by unsupervised [17] or greedy layer-wise fashion [18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "One approach is to pre-train the neural network by unsupervised [17] or greedy layer-wise fashion [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "the FitNet [10], but it requires the additional effort to train the teacher model beforehand.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "Our work in this paper builds on the recently proposed highway networks [19], where the transform gate is used to scale the output of a hidden layer and the carry gate is used to pass through the input directly after elementwise rescaling.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Similar idea also has been tried on long short-term memory recurrent neural networks (LSTM-RNN) for speech recognition [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "However, as the number of hidden layers increases, the error surface becomes increasingly non-convex, and it is more possible to find a poor local minima using gradient-based optimisation algorithms with random initialisation [21].", "startOffset": 226, "endOffset": 230}, {"referenceID": 21, "context": "Furthermore, [22] showed that the variance of the back-propagated gradients may become small in the lower layers if the model parameters are not initialised properly.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "There have been numerous studies on overcoming the difficulties in training very deep neural networks, including pre-training [17, 18], normalised initialisation [22], deeply-supervised networks [23], etc.", "startOffset": 126, "endOffset": 134}, {"referenceID": 17, "context": "There have been numerous studies on overcoming the difficulties in training very deep neural networks, including pre-training [17, 18], normalised initialisation [22], deeply-supervised networks [23], etc.", "startOffset": 126, "endOffset": 134}, {"referenceID": 21, "context": "There have been numerous studies on overcoming the difficulties in training very deep neural networks, including pre-training [17, 18], normalised initialisation [22], deeply-supervised networks [23], etc.", "startOffset": 162, "endOffset": 166}, {"referenceID": 22, "context": "There have been numerous studies on overcoming the difficulties in training very deep neural networks, including pre-training [17, 18], normalised initialisation [22], deeply-supervised networks [23], etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "[19] proposed the highway network and demonstrated the impressive results of using this approach to train very deep networks, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "where T (\u00b7) is the transform gate that scales the original hidden activations; C(\u00b7) is the carry gate, which scales the input before passing it directly to the next hidden layer; \u25e6 denotes elementwise (Hadamad) product; The outputs of T (\u00b7) and C(\u00b7) are constrained to be [0, 1], and we use sigmoid functions for both gates parameterised by WT and Wc respectively.", "startOffset": 272, "endOffset": 278}, {"referenceID": 18, "context": "Unlike [19], in this work, we do not use any bias vector in the two gate functions.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "In [19], the carry gate is constrained to be C(\u00b7) = 1\u2212T (\u00b7), while in this work, we evaluate the generalisation ability of highway networks with and without this constraint.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "It looks similar to the dropout regularisation for neural networks [24], which may be represented as", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "where p( i) is a Bernoulli distribution for each element in as originally proposed in [24], while it was shown later that using a continuous distribution with well designed mean and variance works as well or better [25].", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "where p( i) is a Bernoulli distribution for each element in as originally proposed in [24], while it was shown later that using a continuous distribution with well designed mean and variance works as well or better [25].", "startOffset": 215, "endOffset": 219}, {"referenceID": 25, "context": "Our experiments were performed on the AMI meeting speech transcription dataset [26].", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": ") where other types of thin and deep networks were evaluated [10, 19].", "startOffset": 61, "endOffset": 69}, {"referenceID": 18, "context": ") where other types of thin and deep networks were evaluated [10, 19].", "startOffset": 61, "endOffset": 69}, {"referenceID": 26, "context": "The results reported in this paper were obtained using the CNTK toolkit [27] with the Kaldi decoder [28], and the networks were trained using the cross entropy (CE) criterion without pre-training unless specified otherwise.", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "The results reported in this paper were obtained using the CNTK toolkit [27] with the Kaldi decoder [28], and the networks were trained using the cross entropy (CE) criterion without pre-training unless specified otherwise.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "We also evaluated using the constrained carry gate in our experiments, where C(\u00b7) = 1 \u2212 T (\u00b7) as studied in [19].", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "For speech recognition, deep neural network (DNN) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models(GMMs), DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g. mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are thinner and deeper and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 70 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy.", "creator": "LaTeX with hyperref package"}}}