{"id": "1512.05030", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2015", "title": "Morpho-syntactic Lexicon Generation Using Graph-based Semi-supervised Learning", "abstract": "Morpho-syntactic lexicons provide information about the morphological and syntactic roles of words in a language. Such lexicons are not available for all languages and even when available, their coverage can be limited. We present a graph-based semi-supervised learning method that uses the morphological, syntactic and semantic relations between words to automatically construct wide coverage lexicons from small seed sets. Our method is language-independent, and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing.", "histories": [["v1", "Wed, 16 Dec 2015 02:27:14 GMT  (129kb,D)", "http://arxiv.org/abs/1512.05030v1", null], ["v2", "Tue, 12 Jan 2016 19:02:56 GMT  (132kb,D)", "http://arxiv.org/abs/1512.05030v2", "in Transactions of the Association for Computational Linguistics (TACL) 2016"], ["v3", "Sun, 24 Jan 2016 04:28:46 GMT  (132kb,D)", "http://arxiv.org/abs/1512.05030v3", "Transactions of the Association for Computational Linguistics (TACL) 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["manaal faruqui", "ryan mcdonald", "radu soricut"], "accepted": true, "id": "1512.05030"}, "pdf": {"name": "1512.05030.pdf", "metadata": {"source": "CRF", "title": "Morpho-syntactic Lexicon Generation Using Graph-based Semi-supervised Learning", "authors": ["Manaal Faruqui", "Ryan McDonald", "Radu Soricut"], "emails": ["mfaruqui@cs.cmu.edu", "ryanmcd@google.com", "rsoricut@google.com"], "sections": [{"heading": "1 Introduction", "text": "Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nie\u00dfen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis et al., 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (Mu\u0308ller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1)\nThey are often constructed manually and are expensive to obtain (Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small.\nIn this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-syntactic lexicon generation as a graph-based semi-supervised learning problem (Zhu et al., 2005; Bengio et al., 2006; Subramanya and Talukdar, 2014). We construct a graph where nodes represent word types and the goal is to label them with attributes. The seed lexicon provides attributes for a subset of these nodes. Nodes are connected to each other through edges that denote features shared between them or surface morphological transformation between them.\nOur entire framework of lexicon generation, including the label propagation algorithm and the feature extraction module is language independent. We only use word-level morphological, semantic and syntactic relations between words that can be induced from unannotated corpora in an unsuper-\nar X\niv :1\n51 2.\n05 03\n0v 1\n[ cs\n.C L\n] 1\n6 D\nec 2\n01 5\nvised manner. One particularly novel aspect of our graph-based framework is that edges are featurized. Some of these features measure similarity, e.g., singular nouns tend to occur in similar distributional contexts as other singular nouns, but some also measure transformations from one inflection to another, e.g., adding a \u2018s\u2019 suffix could indicate flipping the NUM:SING attribute to NUM:PLUR (in English). For every attribute to be propagated, we learn weights over features on the edges separately. This is in contrast to traditional label propagation, where edges indicate similarity exclusively (Zhu et al., 2005).\nWe construct lexicons in 11 languages of varying morphological complexity. We perform intrinsic evaluation of the quality of generated lexicons obtained from either the universal dependency treebank or created manually by humans (\u00a74). We show that these automatically created lexicons provide useful features in two extrinsic NLP tasks which require identifying the contextually plausible morphological and syntactic roles: morphological tagging (Hajic\u030c and Hladka\u0301, 1998a; Hajic\u030c, 2000) and syntactic dependency parsing (Ku\u0308bler et al., 2009). We obtain an average of 15.4% and 5.3% error reduction across 11 languages for morphological tagging and dependency parsing respectively on a set of publicly available treebanks (\u00a75). We anticipate that the lexicons thus created will be useful in a variety of NLP problems."}, {"heading": "2 Graph Construction", "text": "The approach we take propagates information over lexical graphs (\u00a73). In this section we describe how\nto construct the graph that serves as the backbone of our model. We construct a graph in which nodes are word types and directed edges are present between nodes that share one or more features. Edges between nodes denote that there might be a relationship between the attributes of the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English.\nWord Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pado\u0301, 2010; Ta\u0308ckstro\u0308m et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in Ta\u0308ckstro\u0308m et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word pair sharing the same word cluster and a feature for the cluster is fired. Thus, there are 256 possible cluster features on an edge, though in our case only a single one can fire.\nSuffix & Prefix. Suffixes are often strong indicators of the morpho-syntactic attributes of a word (Ratnaparkhi, 1996; Clark, 2003). For example, in English, -ing denotes gerund verb forms like, studying, playing and -ed denotes past tense like studied, played etc. Prefixes like un-, in- often denote ad-\n1Some of these features can cause the graph to become very dense making label propagation prohibitive. We keep the size of the graph in check by only allowing a word node to be connected to at most 100 other (randomly selected) word nodes sharing one particular feature. This reduces edges while still keeping the graph connected.\njectives. Thus we include both 2-gram and 3-gram suffix and prefix as edge features.2 We introduce an edge between two words sharing a particular suffix or prefix feature.\nMorphological Transformations. Soricut and Och (2015) presented an unsupervised method of inducing prefix- and suffix-based morphological transformations between words using word embeddings. In their method, statistically, most of the transformations are induced between words with the same lemma (without using any prior information about the word lemma). For example, their method induces the transformation between played and playing as suffix:ed:ing. This feature indicates TENSE:PAST to turn off and TENSE:PRES to turn on.3 We train the morphological transformation prediction tool of Soricut and Och (2015) on the news corpus (same as the one used for training word clusters) for each language. An edge is introduced between two words that exhibit a morphological transformation feature from one word to another as indicated by the tool\u2019s output.\nMotivation for the Model. To motivate our model, consider the words played and playing. They have a common attribute POS:VERB but they differ in tense, showing TENSE:PAST and TENSE:PRES resp. Typical graph-propagation algorithms model similarity (Zhu et al., 2005) and thus propagate all attributes along the edges. However, we want to model if an attribute should propagate or change across an edge. For example, having a shared cluster feature, is an indication of similar POS tag (Clark, 2003; Koo et al., 2008; Turian et al., 2010), but a surface morphological transformation feature like suffix:ed:ing possibly indicates a change in the tense of the word. Thus, we will model attributes propagation/transformation as a function of the features shared on the edges between words. The features described in this section are specially suitable for languages that exhibit concatenative morphology, like English, German, Greek etc. and might not work very well with languages that exhibit nonconcatenative morphology i.e, where root modifica-\n2We only include those suffix and prefix which appear at least twice in the seed lexicon.\n3Our model will learn the following transformation: TENSE:PAST: 1\u2192 -1, TENSE:PRESENT: -1\u2192 1 (\u00a73).\ntion is highly frequent like in Arabic and Hebrew. However, it is important to note that our framework is not limited to just the features described here, but can incorporate any arbitrary information over word pairs (\u00a78)."}, {"heading": "3 Graph-based Label Propagation", "text": "We now describe our model. Let W = {w1, w2, . . . , w|W|} be the vocabulary with |W| words and A = {a1, a2, . . . , a|A|} be the set of lexical attributes that words in W can express; e.g. W = {played, playing, . . .} and A = {NUM:SING,NUM:PLUR, TENSE:PAST, . . .}. Each word type w \u2208 W is associated with a vector aw \u2208 [\u22121, 1]|A|, where ai,w = 1 indicates that word w has attribute i and ai,w = \u22121 indicates its absence; values in between are treated as degrees of uncertainty. For example, TENSE:PASTplayed = 1 and TENSE:PASTplaying = \u22121.4\nThe vocabulary W is divided into two disjoint subsets, the labeled words L for which we know their aw\u2019s (obtained from seed lexicon)5 and the unlabeled words U whose attributes are unknown. In general |U| |L|. The words in W are organized into a directed graph with edges E between words. Let, vector \u03c6(w, v) \u2208 [0, 1]|F| denote the features on the directed edge between words w and v, with 1 indicating the presence and 0 the absence of feature fk \u2208 F , where, F = {f1, f2, . . . , f|F|} are the set of possible binary features shared between two words in the graph. For example, the features on edges between played and playing from Fig. 1 are:\n\u03c6k(played, playing) =  1, iffk = suffix:ed:ing 1, iffk = prefix:pl 0, iffk = suffix:ly . . .\nWe seek to determine which subsets ofA are valid for each word w \u2208 W . We learn how a particular attribute of a node is a function of that particular attribute of its neighboring nodes and features on the edge connecting them. Let ai,w be an attribute of\n4We constrain ai,w \u2208 [\u22121, 1] as its easier to model the flipping of an attribute value from\u22121 to 1 as opposed to [0, 1]. This will be clearer soon.\n5We use labeled, seed, and training lexicon to mean the same thing interchangeably.\nword w and let a\u0302i,w be the empirical estimate of that attribute. We posit that a\u0302i,w can be estimated from the neighbors N (w) of w as follows:\na\u0302i,w = tanh  \u2211 v\u2208N (w) (\u03c6(w, v) \u00b7 \u03b8i)\u00d7 ai,v  (1) where, \u03b8i \u2208 R|F| is weight vector of the edge features for estimating attribute ai. \u2018\u00b7\u2019 represents dot product betwen two vectors. We use tanh as the nonlinearity to make sure that a\u0302i,w \u2208 [\u22121, 1]. The set of such weights \u03b8 \u2208 R|A|\u00d7|F| for all attributes are the model parameters that we learn. Our graph resembles the Ising model, which is a lattice model proposed for describing intermolecular forces (Ising, 1925), and equ. 1 solves the naive mean field approximation of the Ising model (Wang et al., 2007).\nIntuitively, one can view the node to node message function from v tow: \u03c6(w, v)\u00b7\u03b8i\u00d7ai,v as either (1) supporting the value ai,v when \u03c6(w, v) \u00b7 \u03b8i > 0; (2) inverting ai,v when \u03c6(w, v) \u00b7 \u03b8i < 0; or (3) dampening or neutering ai,v when \u03c6(w, v) \u00b7 \u03b8i \u2248 0. Returning to our motivation, if w = played and v = playing, a feature indicating the suffix substitution suffix:ed:ing should have a highly negative weight for TENSE:PAST, indicating a change in value. This is because TENSE:PAST = -1 for playing, and a negative value of \u03c6(w, v) \u00b7 \u03b8i will push it to positive for played.\nIt should be noted that this framework for constructing lexicons does not explicitly distinguish between morpho-syntactic paradigms, but simply identifies all possible attribute-values a word can take. If we consider an example like \u201cgames\u201d and two attributes, the syntactic part-of-speech, POS, and number, NUM, games can either be 1) {POS:VERB,NUM:SING}, as in John games the system; or {POS:NOUN,NUM:PLUR}, as in The games have started. Our framework will mereley return that all the above attribute-values are possible, which implies that the singluar noun and plural verb interpretations are valid. One possible way to account for this is to make full morphological paradigms the \u201cattributes\u201d in or model. But this leads to slower runtimes and sparser learning. We leave as future work extensions to full paradigm prediction.\nOur framework has three critical components, each described below: (1) model estimation, i.e., learning \u03b8; (2) label propagation to U ; and optionally (3) paradigm projection to known valid morphological paradigms. The overall procedure is illustrated in Figure 2 and made concrete in Algorithm 1."}, {"heading": "3.1 Model Estimation", "text": "We estimate all individual elements of an attribute vector using eq. 1. We define loss as the squared loss between the empirical and observed attribute vectors\non every labeled node in the graph, thus the total loss can be computed as:\u2211\nw\u2208L \u2016aw \u2212 a\u0302w\u201622 (2)\nWe train the edge feature weights \u03b8 by minimizing the loss function in eq. 2. In this step, we only use labeled nodes and the edge connections between labeled nodes. As such, this is strictly a supervised learning setup. We minimize the loss function using online adaptive gradient descent (Duchi et al., 2010) with `2 regularization on the feature weights \u03b8. This is the first step in Algorithm 1 (lines 1\u20135)."}, {"heading": "3.2 Label Propagation", "text": "In the second step, we use the learned weights of the edge features to estimate the attribute values over unlabeled nodes iteratively. The attribute vector of all unlabeled words is initialized to null, \u2200w \u2208 U ,aw = \u30080, 0, . . . , 0\u3009. In every iteration, an unlabeled node estimates its empirical attributes by looking at the corresponding attributes of its labeled and unlabeled neighbors using eq. 1, thus this is the semi-supervised step. We stop after the squared euclidean distance between the attribute vectors at two consecutive iterations for a node becomes less than 0.1 (averaged over all unlabeled nodes). This is the second step in Algorithm 1 (lines 6\u20139). After convergence, we can directly obtain attributes for a word by thresholding: a word w is said to possess an attribute ai if ai,w > 0."}, {"heading": "3.3 Paradigm Projection", "text": "Since a word can be labeled with multiple lexical attributes, this is a multi-label classification problem. For such a task, several advanced methods that take into account the correlation between attributes have been proposed (Ghamrawi and McCallum, 2005; Tsoumakas and Katakis, 2006; Fu\u0308rnkranz et al., 2008; Read et al., 2011), here we have adopted the binary relevance method which trains a classifier for every attribute independently of the other attributes, for its simplicity (Godbole and Sarawagi, 2004; Zhang and Zhou, 2005).\nHowever, as the decision for the presence of an attribute over a word is independent of all the other attributes, the final set of attributes obtained for a\nword in \u00a73.2 might not be a valid paradigm.6 For example, a word cannot only exhibit the two attributes POS:NOUN and TENSE:PAST, since the presence of the tense attribute implies POS:VERB should also be true. Further, we want to utilize the inherent correlations between attribute labels to obtain better solutions. We thus present an alternative, simpler method to account for this problem. To ensure that we obtain a valid attribute paradigm, we project the empirical attribute vector obtained after propagation to the space of all valid paradigms.\nWe first collect all observed and thus valid attribute paradigms from the seed lexicon (P = {aw|w \u2208 L}). We replace the empirical attribute vector obtained in \u00a73.2 by a valid attribute paradigm vector which is nearest to it according to euclidean distance. This projection step is inspired from the decoding step in label-space transformation approaches to multilabel classification (Hsu et al., 2009; Ferng and Lin, 2011; Zhang and Schneider, 2011). This is the last step in Algorithm 1 (lines 10\u2013 17). We investigate for each language if paradigm projection is helpful (\u00a74.1)."}, {"heading": "4 Intrinsic Evaluation", "text": "To ascertain how our graph-propagation framework predicts morphological attributes for words, we provide an intrinsic evaluation where we compare predicted attributes to gold lexicons that have been either read off from a treebank or derived manually."}, {"heading": "4.1 Dependency Treebank Lexicons", "text": "The universal dependency treebank (McDonald et al., 2013; De Marneffe et al., 2014; Agic\u0301 et al., 2015) contains dependency annotations for sentences and morpho-syntactic annotations for words in context for a number of languages.7 A word can display different attributes depending on its role in a sentence. In order to create morpho-syntactic lexicon for every language, we take the union of all the attributes that the word realizes in the entire treebank. Although, it is possible that this lexicon might not contain all realizable attributes if a particular attribute or paradigm is not seen in the treebank (we address this issue in \u00a74.2). The utility of evaluating against\n6A paradigm is defined as a set of attributes. 7We use version 1.1 released in May 2015.\nData: W , L, U , A, F , P Result: \u03b8, labeled U\n1 model optimization 2 while convergence do 3 for w \u2208 L do 4 loss\u2190 \u2016aw \u2212 a\u0302w\u201622 5 Update \u03b8 using \u2202loss\u2202\u03b8\n6 label propagation 7 while convergence do 8 for w \u2208 U do 9 aw \u2190 a\u0302w\n10 paradigm projection 11 for w \u2208 U do 12 mindist\u2190\u221e, closest\u2190 \u2205 13 for p \u2208 P do 14 dist\u2190 \u2016aw \u2212 p\u201622 15 if dist < mindist then 16 mindist\u2190 dist, closest\u2190 p\n17 aw \u2190 closest Algorithm 1: Graph-based semi-supervised label propagation algorithm.\ntreebank derived lexicons is that it allows us to evaluate on a large set of languages. In particular, in the universal dependency treebanks v1.1 (Agic\u0301 et al.,\n2015), 11 diverse languages contain the morphology layer, including Romance, Germanic and Slavic languages plus isolates like Basque and Greek.\nWe use the train/dev/test set of the treebank to create training (seed),8 development and test lexicons for each language. We exclude words from the dev and test lexicon that have been seen in seed lexicon. For every language, we create a graph with the features described in \u00a72 with words in the seed lexicon as labeled nodes. The words from development and test set are included as unlabeled nodes for the propagation stage.9 Table 2 shows statistics about the constructed graph for different languages.10\nWe perform feature selection and hyperparameter tuning by optimizing prediction on words in the development lexicon and then report results on the test lexicon. The decision whether paradigm projection (\u00a73.3) is useful or not is also taken by tuning performance on the development lexicon. Table 3 shows the features that were selected for each language. Now, for every word in the test lexicon we obtain predicted lexical attributes from the graph. For a given attribute, we count the number of words for which it was correctly predicted (true positive), wrongly predicted (false positive) and not predicted (false negative). Aggregating these counts over all attributes (A), we compute the micro-averaged F1 score and achieve 74.3% on an average across 11 languages (cf. Table 4). Note that this systematically underestimates performance due to the effect of missing attributes/paradigms that were not observed in the treebank.\nPropagated Lexicons. The last column in Table 2 shows the number of words in the propagated lexicon, and the first column shows the number of words in the seed lexicon. The ratio of the size of propagated and seed lexicon is different across languages, which presumably depends on how densely connected each language\u2019s graph is. For example, for\n8We only include those words in the seed lexicon that occur at least twice in the training set of the treebank.\n9Words from the news corpus used for word clustering are also used as unlabeled nodes.\n10Note that the size of the constructed lexicon (cf. Table 2) is always less than or equal to the total number of unlabeled nodes in the graph because some unlabeled nodes are not able to collect enough mass for acquiring an attribute i.e, \u2200a \u2208 A : aw < 0 and thus they remain unlabeled (cf. \u00a73.2).\nEnglish the propagated lexicon is around 240 times larger than the seed lexicon, whereas for Czech, its 8 times larger. We can individually tune how densely connected graph we want for each language depending on the seed size and feature sparsity, which we leave for future work.\nSelected Edge Features. The features most frequently selected across all the languages are the word cluster and the surface morphological transformation features. This essentially translates to having a graph that consists of small connected components of words having the same lemma (discovered in an unsupervised manner) with semantic links connecting such components using word cluster features. Suffix features are useful for highly inflected languages like Czech and Greek, while the prefix feature is only useful for Czech. Overall, the selected edge features for different languages correspond well to the morphological structure of these languages (Dryer, 2013).\nCorpus Baseline. We compare our results to a corpus-based method of obtaining morpho-syntactic lexicons. We hypothesize that if we use a morphological tagger of reasonable quality to tag the entire wikipedia corpus of a language and take the union of all the attributes for a word type across all its occurrences in the corpus, then we can acquire all possible attributes for a given word. Hence, producing a lexicon of reasonable quality. Moore (2015) used this technique to obtain a high quality tag dictionary for\nPOS-tagging. We thus train a morphological tagger (detail in \u00a75.1) on the training portion of the dependency treebank and use it to tag the entire wikipedia corpus. For every word, we add an attribute to the lexicon if it has been seen at least k times for the word in the corpus, where k \u2208 [2, 20]. This threshold on the frequency of the word-attribute pair helps prevent noisy judgements. We tune k for each language on the development set and report results on the test set in Table 4. We call this method the Corpus baseline. It can be seen that for every language we outperform this baseline, which on average has an F1 score of 67.1%."}, {"heading": "4.2 Manually Curated Lexicons", "text": "We have now showed that its possible to automatically construct large lexicons from smaller seed lexicons. However, the seed lexicons used in \u00a74.1 have been artifically constructed from aggregating attributes of word types over the treebank. Thus, it\ncan be argued that these constructed lexicons might not be complete i.e, the lexicon might not exhibit all possible attributes for a given word. On the other hand, manually curated lexicons are unavailable for many languages, inhibiting proper evaluation.\nTo test the utility of our approach on manually curated lexicons, we investigate publicly available lexicons for Finnish (Pirinen, 2011), Czech (Hajic\u030c and Hladka\u0301, 1998b) and Hungarian (Tro\u0301n et al., 2006). We eliminate numbers and punctuation from all lexicons. For each of these languages, we select 10000 words for training and the rest of the word types for evaluation. We train models obtained in \u00a74.1 for a given language using suffix, brown and morphological transformation features with paradigm projection. The only difference is the source of the seed lexicon and test set. Results are reported in Table 5 averaged over 10 different randomly selected seed set for every language. For each language we obtain more than 70% F1 score and on an average obtain 79.7%. Critically, the F1 score on human curated lexicons is higher for each language than the treebank constructed lexicons, in some cases as high as 9% absolute. This shows that the average 74.3% F1 score across all 11 languages is likely underestimated."}, {"heading": "5 Extrinsic Evaluation", "text": "We now show that the automatically generated lexicons provide informative features that are useful in two downstream NLP tasks: morphological tagging (\u00a75.1) and syntactic dependency parsing (\u00a75.2)."}, {"heading": "5.1 Morphological Tagging", "text": "Morphological tagging is the task of assigning a morphological reading to a token in context. The morphological reading consists of features such as part of speech, case, gender, person, tense etc.\n(Oflazer and Kuruo\u0308z, 1994; Hajic\u030c and Hladka\u0301, 1998a). The model we use is a standard atomic sequence classifier, that classifies the morphological bundle for each word independent of the others (with the exception of features derived from these words). Specifically, we use a linear SVM model classifier with hand tuned features. This is similar to commonly used analyzers like SVMTagger (Ma\u0300rquez and Gime\u0301nez, 2004) and MateTagger (Bohnet and Nivre, 2012).\nOur taggers are trained in a language independent manner (Hajic\u030c, 2000; Smith et al., 2005; Mu\u0308ller et al., 2013). The list of features used in training the tagger are listed in Table 6. In addition to the standard features, we use the morpho-syntactic attributes present in the lexicon for every word as features in the tagger. As shown in Mu\u0308ller and Schuetze (2015), this is typically the most important feature for morphological tagging, even more useful than clusters or word embeddings. While predicting the contextual morphological tags for a given word, the morphological attributes present in the lexicon for the current word, the previous word and the next word are used as features.\nWe use the same 11 languages from the universal dependency treebanks (Agic\u0301 et al., 2015) that contain morphological tags to train and evaluate the morphological taggers. We use the pre-specified train/dev/test splits that come with the data. Table 7 shows the macro-averaged F1 score over all\nattributes for each language on the test lexicon. The three columns show the F1 score of the tagger when no lexicon is used; when the seed lexicon derived from the training data is used; and when label propagation is applied.\nOverall, using lexicons provides a significant improvement in accuracy, even when just using the seed lexicon. For 9 out of 11 language, the highest accuracy is obtained using the lexicon derived from graph propagation. In some cases the gain is quite substantial, e.g., 94.6% \u2192 95.9% for Bulgarian. Overall there is 1.0% and 0.3% absolute improvement over the baseline and seed resp., which corresponds roughly to a 15% and 5% relative reduction in error. It is not surprising that the seed lexicon performs on par with the derived lexicon for some languages, as it is derived from the training corpus, which likely contains the most frequent words of the language."}, {"heading": "5.2 Dependency Parsing", "text": "We train dependency parsers for the same 11 universal dependency treebanks that contain the morphological layer (Agic\u0301 et al., 2015). We again use the supplied train/dev/test split of the dependency treebank to develop the models. Our parsing model is the transition-based parsing system of Zhang and Nivre (2011) with identical features and a beam of size 8.\nWe augment the features of Zhang and Nivre\n(2011) in two ways: using the context-independent morphological attributes present in the different lexicons; and using the corresponding morphological taggers from \u00a75.1 to generate context-dependent attributes. For each of the above two kinds of features, we fire the attributes for the word on top of the stack and the two words on at the front of the buffer. Additionally we take the cross product of these features between the word on the top of the stack and at the front of the buffer.\nTable 8 shows the labeled accuracy score (LAS) for all languages. Overall, the generated lexicon gives an improvement of absolute 1.5% point over the baseline (5.3% relative reduction in error) and 0.5% over the seed lexicon on an average across 11 languages. Critically this improvement holds for 10/11 languages over the baseline and 8/11 languages over the system that uses seed lexicon only."}, {"heading": "6 Further Analysis", "text": "In this section we further investigate our model and results in detail.\nSize of seed lexicon. We first test how the size of the seed lexicon affects performance of attribute prediction on the test set. We use the manually constructed lexicons described in \u00a74.2 for experiments. For each language, instead of using the full seed lexicon of 10000 words, we construct subsets of this lexicon by taking 1000 and 5000 randomly sampled words. We then train models obtained in \u00a74.1 on these lexicons and plot the performance on the test set in Figure 3. On average across"}, {"heading": "S + C + MT 87.5 79.9 71.6", "text": ""}, {"heading": "S + C 86.5 78.8 68.2", "text": ""}, {"heading": "S + MT 85.7 77.0 68.7", "text": ""}, {"heading": "C + MT 75.7 57.4 62.2", "text": ""}, {"heading": "S + C + MT + P 86.7 66.0 61.3", "text": "three languages, we observe that the absolute performance improvement from 1000 to 5000 seed words is \u224810% whereas it reduces to \u22482% from 5000 to 10000 words.\nFeature analysis. Table 9 shows the highest and the lowest weighted features for predicting a given attribute of English words. The highest weighted features for both VFORM:GER and NUM:PLUR are word clusters, indicating that word clusters exhibit strong syntactic and semantic coherence. More interestingly, it can be seen that for predicting VFORM:GER i.e, continuous verb forms, the lowest weighted features are those morphological transformations that substitute \u201cing\u201d with something else. Thus, if there exists an edge between the words studying and study, containing the feature: suffix:ing:{null}, the model would correctly predict that studying is VFORM:GER as study is not so and the negative feature weight can flip the label values. The same observation holds true for NUM:PLUR.\nFeature ablation. One key question is which of the features in our graph are important for projecting morphological attribute-values. Table 3 suggests\nthat this is language specific, which is intuitive, as morphology can be represented more or less regularly through the surface form depending on the language. To understand this, we did a feature ablation study for the three languages with manually curated lexicons (\u00a74.2) using the same feature set as before: clusters, suffix and morphological transformations with paradigm projection. We then leave out each feature to measure how performance drops. Unlike \u00a74.2, we do not average over 10 runs but use a single static graph where features (edges) are added or removed as necessary.\nTable 10 contains the results. Critically, all features are required for top accuracy across all languages and leaving out suffix features has the most detrimental effect. This is not surprising considering all three language primarily express morphological properties via suffixes. Furthermore, suffix features help to connect the graph and assist label propagation. Note that the importance of suffix features here is in contrast to the evaluation on treebank derived lexicons in \u00a74.1, where suffix features were only selected for 4 out of 11 languages based on the development data (Table 3), and not for Hungarian and Finnish. This could be due to the nature of the lexicons derived from treebanks versus complete lexicons constructed by humans.\nAdditionally, we also added back prefix features and found that for all languages, this resulted in a drop in accuracy, particularly for Finnish and Hungarian. The primary reason for this is that prefix features often create spurious edges in the graph. This in and of itself is not a problem for our model, as the edge weights should learn to discount this feature. However, the fact that we sample edges to make inference tractable means that more informative edges could be dropped in favor of those that are only connected via a prefix features.\nPrediction examples. Table 11 shows examples of predictions made by our model for English and Italian. For each language, we first select a random word from the seed lexicon, then we pick one syntactic and one semantically related word to the selected word from the set of unlabeled words. For e.g., in Italian tavola means table, whereas tavoli is the plural form and divano means sofa. We correctly identify attributes for these words."}, {"heading": "7 Related Work", "text": "We now review the areas of related work.\nLexicon generation. Natural language lexicons have often been created from smaller seed lexcions using various methods. Thelen and Riloff (2002) use patterns extracted over a large corpus to learn semantic lexicons from smaller seed lexicons using bootstrapping. Alfonseca et al. (2010) use distributional similarity scores across instances to propagate attributes using random walks over a graph. Das and Smith (2012) learn potential semantic frames for unknown predicates by expanding a seed frame lexicon. Sentiment lexicons containing semantic polarity labels for words and phrases have been created using bootstrapping and graph-based learning (Banea et al., 2008; Mohammad et al., 2009; Velikovich et al., 2010; Takamura et al., 2007; Lu et al., 2011).\nGraph-based learning. In general, graph-based semi-supervised learning is heavily used in NLP (Talukdar and Cohen, 2013; Subramanya and Talukdar, 2014). Graph-based learning has been used for class-instance acquisition (Talukdar and Pereira, 2010), text classification (Subramanya and Bilmes, 2008), summarization (Erkan and Radev, 2004), structured prediction problems (Subramanya et al., 2010; Das and Petrov, 2011; Garrette et al., 2013) etc. Our work differs from most of these approaches in that we specifically learn how different features shared between the nodes can correspond to either the propagation of an attribute or an inversion of the attribute value (cf. equ 1). In terms of the capability of inverting an attribute value, our method is close to Goldberg et al. (2007), who present a framework to include dissimilarity between nodes\nand Talukdar et al. (2012), who learn which edges can be excluded for label propagation. In terms of featurizing the edges, our work resembles previous work which measured similarity between nodes in terms of similarity between the feature types that they share (Muthukrishnan et al., 2011; Saluja and Navra\u0301til, 2013). Our work is also related to graphbased metric learning, where the objective is to learn a suitable distance metric between the nodes of a graph for solving a given problem (Weinberger et al., 2005; Dhillon et al., 2010; Dhillon et al., 2012).\nMorphology. High morphological complexity exacerbates the problem of feature sparsity in many NLP applications and leads to poor estimation of model parameters, emphasizing the need of morphological analysis. Morphological analysis encompasses fields like morphological segmentation (Creutz and Lagus, 2005; Demberg, 2007; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015), and inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004). Such models of segmentation and inflection generation are used to better understand the meaning and relations between words. Our task is complementary to the task of morphological paradigm generation. Paradigm generation requires generating all possible morphological forms of a given base-form according to different linguistic transformations (Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015), whereas our task requires identifying linguistic transformations between two different word forms.\nLow-resourced languages. Our algorithm can be used to generate morpho-syntactic lexicons for lowresourced languages, where the seed lexicon can\nbe constructed, for example, using crowdsourcing (Callison-Burch and Dredze, 2010; Irvine and Klementiev, 2010). Morpho-syntactic resources have been developed for east-european languages like Slovene (Dzeroski et al., 2000; Erjavec, 2004), Bulgarian (Simov et al., 2004) and highly agglutinative languages like Turkish (Sak et al., 2008). Morpho-syntactic lexicons are crucial components in acousting modeling and automatic speech recognition, where they have been developed for lowresourced languages (Huet et al., 2008; Besacier et al., 2014).\nOne alternative method to extract morphosyntactic lexicons is via parallel data (Das and Petrov, 2011). However, such methods assume that both the source and target langauges are isomorphic with respect to morphology. This can be the case with attributes like coarse part-of-speech or case, but is rarely true for other attributes like gender, which is very language specific."}, {"heading": "8 Future Work", "text": "There are three major ways in which the current model can be possibly improved.\nJoint learning and propagation. In the current model, we are first learning the weights in a supervised manner (\u00a73.1) and then propagating labels across nodes in a semi-supervised step with fixed feature weights (\u00a73.2). These can also be performed jointly: perform one iteration of weight learning, propagate labels using these weights, perform another iteration of weight learning assuming empirical labels as gold labels and continue to learn and propagate until convergence. This joint learning would be slower than the current approach as propagating labels across the graph is an expensive step.\nMulti-label classifcation. We are currently using the binary relevance method which trains a binary classifier for every attribute independently (Godbole and Sarawagi, 2004; Zhang and Zhou, 2005) with paradigm projection as a post-processing step (\u00a73.3). Thus we are accounting for attribute correlations only at the end. We can instead model such correlations as constraints during the learning step to obtain better solutions (Ghamrawi and McCallum, 2005; Tsoumakas and Katakis, 2006; Fu\u0308rnkranz et\nal., 2008; Read et al., 2011).\nRicher feature set. In addition our model can benefit from a richer set of features. Word embeddings can be used to connect word node which are similar in meaning (Mikolov et al., 2013). We can use existing morphological segmentation tools to discover the morpheme and inflections of a word to connect it to word with similar inflections which might be better than the crude suffix or prefix features. We can also use rich lexical resources like Wiktionary11 to extract relations between words that can be encoded on our graph edges."}, {"heading": "9 Conclusion", "text": "We have presented a graph-based semi-supervised method to construct large annotated morphosyntactic lexicons from small seed lexicons. Our method is language independent and we have constructed lexicons for 11 different languages. We showed that the lexicons thus constructed help improve performance in morphological tagging and dependency parsing, when used as features."}, {"heading": "Acknowledgement", "text": "This work was performed when the first author was an intern at Google. We thank David Talbot for his help in developing the propagation framework and helpful discussions about evaluation. We thank Avneesh Saluja, Chris Dyer and Partha Pratim Talukdar for their comments on drafts of this paper."}], "references": [{"title": "Universal dependencies 1.1. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["Tsarfaty", "Veronika Vincze", "Daniel Zeman"], "venue": null, "citeRegEx": "Tsarfaty et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsarfaty et al\\.", "year": 2015}, {"title": "Semi-supervised learning of morphological paradigms and lexicons", "author": ["Markus Forsberg", "Mans Hulden"], "venue": "In Proc. of EACL", "citeRegEx": "Ahlberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2014}, {"title": "Paradigm classification in supervised learning of morphology", "author": ["Markus Forsberg", "Mans Hulden"], "venue": "In Proc. of NAACL", "citeRegEx": "Ahlberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2015}, {"title": "Acquisition of instance attributes via labeled and related instances", "author": ["Marius Pasca", "Enrique Robledo-Arnuncio"], "venue": "In Proc. of SIGIR", "citeRegEx": "Alfonseca et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Alfonseca et al\\.", "year": 2010}, {"title": "Syntactic and sublexical features for turkish discriminative language models", "author": ["Arisoy et al.2010] Ebru Arisoy", "Murat Sara\u00e7lar", "Brian Roark", "Izhak Shafran"], "venue": "In Proc. of ICASSP", "citeRegEx": "Arisoy et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Arisoy et al\\.", "year": 2010}, {"title": "A bootstrapping method for building subjectivity lexicons for languages with scarce resources", "author": ["Banea et al.2008] Carmen Banea", "Janyce M Wiebe", "Rada Mihalcea"], "venue": "In Proc. of LREC", "citeRegEx": "Banea et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2008}, {"title": "Label propagation and quadratic criterion. In Semi-Supervised Learning", "author": ["Bengio et al.2006] Yoshua Bengio", "Olivier Delalleau", "Nicolas Le Roux"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Automatic speech recognition for under-resourced languages: A survey", "author": ["Etienne Barnard", "Alexey Karpov", "Tanja Schultz"], "venue": "Speech Communication,", "citeRegEx": "Besacier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Besacier et al\\.", "year": 2014}, {"title": "A transition-based system for joint part-ofspeech tagging and labeled non-projective dependency parsing", "author": ["Bohnet", "Nivre2012] Bernd Bohnet", "Joakim Nivre"], "venue": "In Proc. of EMNLP", "citeRegEx": "Bohnet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2012}, {"title": "Creating speech and language data with amazon\u2019s mechanical turk", "author": ["Callison-Burch", "Dredze2010] Chris Callison-Burch", "Mark Dredze"], "venue": "In Proc. of NAACL Workshop on Creating Speech and Language Data with Amazon\u2019s", "citeRegEx": "Callison.Burch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2010}, {"title": "Combining distributional and morphological information for part of speech induction", "author": ["Alexander Clark"], "venue": "In Proc. of EACL", "citeRegEx": "Clark.,? \\Q2003\\E", "shortCiteRegEx": "Clark.", "year": 2003}, {"title": "Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0", "author": ["Creutz", "Lagus2005] Mathias Creutz", "Krista Lagus"], "venue": "Helsinki University of Technology", "citeRegEx": "Creutz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2005}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Das", "Petrov2011] Dipanjan Das", "Slav Petrov"], "venue": "In Proc. of ACL", "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "Graph-based lexicon expansion with sparsityinducing penalties", "author": ["Das", "Smith2012] Dipanjan Das", "Noah A. Smith"], "venue": "In Proc. of NAACL", "citeRegEx": "Das et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Das et al\\.", "year": 2012}, {"title": "Universal stanford dependencies: A cross-linguistic typology", "author": ["Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D Manning"], "venue": "Proceedings of LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "A languageindependent unsupervised model for morphological segmentation", "author": ["Vera Demberg"], "venue": "In Proc. of ACL", "citeRegEx": "Demberg.,? \\Q2007\\E", "shortCiteRegEx": "Demberg.", "year": 2007}, {"title": "Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art pos tagging with less human effort", "author": ["Denis et al.2009] Pascal Denis", "Beno\u0131\u0302t Sagot"], "venue": "In Proc. of PACLIC", "citeRegEx": "Denis and Sagot,? \\Q2009\\E", "shortCiteRegEx": "Denis and Sagot", "year": 2009}, {"title": "Inference driven metric learning (idml) for graph construction", "author": ["Partha Pratim Talukdar", "Koby Crammer"], "venue": "UPenn Tech Report", "citeRegEx": "Dhillon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2010}, {"title": "Metric learning for graphbased domain adaptation", "author": ["Partha Talukdar", "Koby Crammer"], "venue": "In Proc. of COLING", "citeRegEx": "Dhillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2012}, {"title": "Discovering morphological paradigms from plain text using a dirichlet process mixture model", "author": ["Dreyer", "Eisner2011] Markus Dreyer", "Jason Eisner"], "venue": "In Proc. of EMNLP", "citeRegEx": "Dreyer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dreyer et al\\.", "year": 2011}, {"title": "Prefixing vs. Suffixing in Inflectional Morphology. Max Planck Institute for Evolutionary Anthropology", "author": ["Matthew S. Dryer"], "venue": null, "citeRegEx": "Dryer.,? \\Q2013\\E", "shortCiteRegEx": "Dryer.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2010] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS-2010-24, UC Berkeley", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Morphological annotation of quranic arabic", "author": ["Dukes", "Habash2010] Kais Dukes", "Nizar Habash"], "venue": "In Proc. of LREC", "citeRegEx": "Dukes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dukes et al\\.", "year": 2010}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Durrett", "DeNero2013] Greg Durrett", "John DeNero"], "venue": "In Proc. of NAACL", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "Morphosyntactic tagging of slovene: Evaluating taggers and tagsets", "author": ["Tomaz Erjavec", "Jakub Zavrel"], "venue": "In Proc. of LREC", "citeRegEx": "Dzeroski et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Dzeroski et al\\.", "year": 2000}, {"title": "Multext-east version 3: Multilingual morphosyntactic specifications, lexicons and corpora", "author": ["Tomaz Erjavec"], "venue": "In Proc. of LREC", "citeRegEx": "Erjavec.,? \\Q2004\\E", "shortCiteRegEx": "Erjavec.", "year": 2004}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["Erkan", "Radev2004] G\u00fcnes Erkan", "Dragomir R Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Erkan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Erkan et al\\.", "year": 2004}, {"title": "Training and evaluating a german named", "author": ["Faruqui", "Pad\u00f32010] Manaal Faruqui", "Sebastian Pad\u00f3"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2010}, {"title": "Multi-label classification with errorcorrecting codes", "author": ["Ferng", "Lin2011] Chun-Sung Ferng", "Hsuan-Tien Lin"], "venue": "In Proc. of ACML", "citeRegEx": "Ferng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ferng et al\\.", "year": 2011}, {"title": "Multilabel classification via calibrated label ranking", "author": ["Eyke H\u00fcllermeier", "Eneldo Loza Menc\u0131\u0301a", "Klaus Brinker"], "venue": "Machine learning,", "citeRegEx": "F\u00fcrnkranz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "F\u00fcrnkranz et al\\.", "year": 2008}, {"title": "Real-world semi-supervised learning of pos-taggers for low-resource languages", "author": ["Jason Mielens", "Jason Baldridge"], "venue": "In Proc. of ACL", "citeRegEx": "Garrette et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Garrette et al\\.", "year": 2013}, {"title": "Collective multi-label classification", "author": ["Ghamrawi", "McCallum2005] Nadia Ghamrawi", "Andrew McCallum"], "venue": "In Proc. of CIKM", "citeRegEx": "Ghamrawi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ghamrawi et al\\.", "year": 2005}, {"title": "Discriminative methods for multi-labeled classification", "author": ["Godbole", "Sarawagi2004] Shantanu Godbole", "Sunita Sarawagi"], "venue": "In Proc. of KDD", "citeRegEx": "Godbole et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Godbole et al\\.", "year": 2004}, {"title": "Dissimilarity in graphbased semi-supervised classification", "author": ["Xiaojin Zhu", "Stephen J Wright"], "venue": "In Proc. of AISTATS", "citeRegEx": "Goldberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2007}, {"title": "Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and em-hmmbased lexical probabilities", "author": ["Reut Tsarfaty", "Meni Adler", "Michael Elhadad"], "venue": "In Proc. of EACL", "citeRegEx": "Goldberg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2009}, {"title": "A class-based agreement model for generating accurately inflected translations", "author": ["Green", "DeNero2012] Spence Green", "John DeNero"], "venue": "In Proc. of ACL", "citeRegEx": "Green et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Green et al\\.", "year": 2012}, {"title": "Multi-label prediction via compressed sensing", "author": ["Hsu et al.2009] Daniel Hsu", "Sham Kakade", "John Langford", "Tong Zhang"], "venue": "In Proc. of NIPS", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Morphosyntactic resources for automatic speech recognition", "author": ["Huet et al.2008] St\u00e9phane Huet", "Guillaume Gravier", "Pascale S\u00e9billot"], "venue": "In Proc. of LREC", "citeRegEx": "Huet et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Huet et al\\.", "year": 2008}, {"title": "Using mechanical turk to annotate lexicons for less commonly used languages", "author": ["Irvine", "Klementiev2010] Ann Irvine", "Alexandre Klementiev"], "venue": "In Proce. of NAACL Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk", "citeRegEx": "Irvine et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Irvine et al\\.", "year": 2010}, {"title": "Beitrag zur theorie des ferromagnetismus", "author": ["Ernst Ising"], "venue": "Zeitschrift fu\u0308r Physik A Hadrons and Nuclei,", "citeRegEx": "Ising.,? \\Q1925\\E", "shortCiteRegEx": "Ising.", "year": 1925}, {"title": "Improved clustering techniques for classbased statistical language modelling", "author": ["Kneser", "Ney1993] Reinhard Kneser", "Hermann Ney"], "venue": "In Proc. of Eurospeech", "citeRegEx": "Kneser et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1993}, {"title": "Annotating, disambiguating & automatically extending the coverage of the swedish simple lexicon", "author": ["Maria Toporowska-Gronostaj", "Karin Warmenius"], "venue": "In Proc. of LREC", "citeRegEx": "Kokkinakis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kokkinakis et al\\.", "year": 2000}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo et al.2008] Terry Koo", "Xavier Carreras", "Michael Collins"], "venue": "In Proc. of ACL", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Dependency parsing. Synthesis Lectures on Human Language Technologies", "author": ["K\u00fcbler et al.2009] Sandra K\u00fcbler", "Ryan McDonald", "Joakim Nivre"], "venue": null, "citeRegEx": "K\u00fcbler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2009}, {"title": "Automatic construction of a context-aware sentiment lexicon: An optimization approach", "author": ["Lu et al.2011] Yue Lu", "Malu Castellanos", "Umeshwar Dayal", "ChengXiang Zhai"], "venue": "In Proceedings of WWW", "citeRegEx": "Lu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2011}, {"title": "A general pos tagger generator based on support vector machines", "author": ["M\u00e0rquez", "Gim\u00e9nez2004] L M\u00e0rquez", "J Gim\u00e9nez"], "venue": "Journal of Machine Learning Research", "citeRegEx": "M\u00e0rquez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "M\u00e0rquez et al\\.", "year": 2004}, {"title": "Algorithms for bigram and trigram word clustering", "author": ["Martin et al.1998] Sven Martin", "J\u00f6rg Liermann", "Hermann Ney"], "venue": "Speech communication,", "citeRegEx": "Martin et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Martin et al\\.", "year": 1998}, {"title": "Universal dependency annotation", "author": ["Joakim Nivre", "Yvonne Quirmbach-Brundage", "Yoav Goldberg", "Dipanjan Das", "Kuzman Ganchev", "Keith B Hall", "Slav Petrov", "Hao Zhang", "Oscar T\u00e4ckstr\u00f6m"], "venue": null, "citeRegEx": "McDonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proc. of NAACL", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Generating complex morphology for machine translation", "author": ["Minkov et al.2007] Einat Minkov", "Kristina Toutanova", "Hisami Suzuki"], "venue": "In Proc. of ACL", "citeRegEx": "Minkov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Minkov et al\\.", "year": 2007}, {"title": "Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus", "author": ["Cody Dunne", "Bonnie Dorr"], "venue": "In Proc. of EMNLP", "citeRegEx": "Mohammad et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2009}, {"title": "An improved tag dictionary for faster part-of-speech tagging", "author": ["Robert Moore"], "venue": "In Proc. of EMNLP", "citeRegEx": "Moore.,? \\Q2015\\E", "shortCiteRegEx": "Moore.", "year": 2015}, {"title": "Robust morphological tagging with word representations", "author": ["M\u00fcller", "Schuetze2015] Thomas M\u00fcller", "Hinrich Schuetze"], "venue": "In Proceedings of NAACL", "citeRegEx": "M\u00fcller et al\\.,? \\Q2015\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2015}, {"title": "Efficient higher-order crfs for morphological tagging", "author": ["M\u00fcller et al.2013] Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze"], "venue": "Proc. of EMNLP", "citeRegEx": "M\u00fcller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Simultaneous similarity learning and feature-weight learning for document clustering", "author": ["Dragomir Radev", "Qiaozhu Mei"], "venue": "In Proc. of TextGraphs", "citeRegEx": "Muthukrishnan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Muthukrishnan et al\\.", "year": 2011}, {"title": "An unsupervised method for uncovering morphological chains. TACL", "author": ["Regina Barzilay", "Tommi Jaakkola"], "venue": null, "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Inflection generation as discriminative string transduction", "author": ["Colin Cherry", "Grzegorz Kondrak"], "venue": "In Proc. of NAACL", "citeRegEx": "Nicolai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "Statistical machine translation with scarce resources using morpho-syntactic information", "author": ["Nie\u00dfen", "Ney2004] Sonja Nie\u00dfen", "Hermann Ney"], "venue": "Computational linguistics,", "citeRegEx": "Nie\u00dfen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nie\u00dfen et al\\.", "year": 2004}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Brendan OConnor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A Smith"], "venue": "In Proc. of NAACL", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "Modularisation of finnish finite-state language description\u2014towards wide collaboration in open source development of a morphological analyser", "author": ["Tommi A Pirinen"], "venue": "In Proc. of Nodalida", "citeRegEx": "Pirinen.,? \\Q2011\\E", "shortCiteRegEx": "Pirinen.", "year": 2011}, {"title": "Unsupervised morphological segmentation with log-linear models", "author": ["Poon et al.2009] Hoifung Poon", "Colin Cherry", "Kristina Toutanova"], "venue": "In Proc. of NAACL", "citeRegEx": "Poon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["Adwait Ratnaparkhi"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ratnaparkhi.,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi.", "year": 1996}, {"title": "Classifier chains for multi-label classification", "author": ["Read et al.2011] Jesse Read", "Bernhard Pfahringer", "Geoff Holmes", "Eibe Frank"], "venue": "Machine Learning,", "citeRegEx": "Read et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Read et al\\.", "year": 2011}, {"title": "Turkish language resources: Morphological parser, morphological disambiguator and web corpus", "author": ["Sak et al.2008] Ha\u015fim Sak", "Tunga G\u00fcng\u00f6r", "Murat Sara\u00e7lar"], "venue": "In Proc. of ANLP", "citeRegEx": "Sak et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2008}, {"title": "Graph-based unsupervised learning of word similarities using heterogeneous feature types", "author": ["Saluja", "Navr\u00e1til2013] Avneesh Saluja", "Jir\u0131 Navr\u00e1til"], "venue": "In Proc. of TextGraphs", "citeRegEx": "Saluja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saluja et al\\.", "year": 2013}, {"title": "Probabilistic partof-speech tagging using decision trees", "author": ["Helmut Schmid"], "venue": "In Proc. of the international conference on new methods in language processing", "citeRegEx": "Schmid.,? \\Q1994\\E", "shortCiteRegEx": "Schmid.", "year": 1994}, {"title": "A language resources infrastructure for bulgarian", "author": ["Petya Osenova", "Sia Kolkovska", "Elisaveta Balabanova", "Dimitar Doikoff"], "venue": "In Proc. of LREC", "citeRegEx": "Simov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Simov et al\\.", "year": 2004}, {"title": "Context-based morphological disambiguation with random fields", "author": ["Smith et al.2005] Noah A Smith", "David A Smith", "Roy W Tromble"], "venue": "In Proc. of EMNLP", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Unsupervised multilingual learning for morphological segmentation", "author": ["Snyder", "Barzilay2008] Benjamin Snyder", "Regina Barzilay"], "venue": "In Proc. of ACL", "citeRegEx": "Snyder et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2008}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Soricut", "Och2015] Radu Soricut", "Franz Och"], "venue": "In Proc. of NAACL", "citeRegEx": "Soricut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2015}, {"title": "Soft-supervised learning for text classification", "author": ["Subramanya", "Bilmes2008] Amarnag Subramanya", "Jeff Bilmes"], "venue": "In Proc. of EMNLP", "citeRegEx": "Subramanya et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Subramanya et al\\.", "year": 2008}, {"title": "Graph-based semi-supervised learning", "author": ["Subramanya", "Talukdar2014] Amarnag Subramanya", "Partha Pratim Talukdar"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,", "citeRegEx": "Subramanya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Subramanya et al\\.", "year": 2014}, {"title": "Efficient graphbased semi-supervised learning of structured tagging models", "author": ["Slav Petrov", "Fernando Pereira"], "venue": "In Proc. of EMNLP", "citeRegEx": "Subramanya et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Subramanya et al\\.", "year": 2010}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proc. of NAACL", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Extracting semantic orientations of phrases from dictionary", "author": ["Takashi Inui", "Manabu Okumura"], "venue": "In Proc. of NAACL", "citeRegEx": "Takamura et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Takamura et al\\.", "year": 2007}, {"title": "Scaling graph-based semi supervised learning to large number of labels using count-min sketch", "author": ["Talukdar", "William Cohen"], "venue": "In Proc. of AISTATS", "citeRegEx": "Talukdar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Talukdar et al\\.", "year": 2013}, {"title": "Experiments in graph-based semi-supervised learning methods for class-instance acquisition", "author": ["Talukdar", "Fernando Pereira"], "venue": "In Proc. of ACL", "citeRegEx": "Talukdar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Talukdar et al\\.", "year": 2010}, {"title": "Acquiring temporal constraints between relations", "author": ["Derry Wijaya", "Tom Mitchell"], "venue": "In Proc. of CIKM", "citeRegEx": "Talukdar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Talukdar et al\\.", "year": 2012}, {"title": "A bootstrapping method for learning semantic lexicons using extraction pattern contexts", "author": ["Thelen", "Riloff2002] Michael Thelen", "Ellen Riloff"], "venue": "In Proc. of ACL", "citeRegEx": "Thelen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Thelen et al\\.", "year": 2002}, {"title": "Morphdb. hu: Hungarian lexical database and morphological grammar", "author": ["Tr\u00f3n et al.2006] Viktor Tr\u00f3n", "P\u00e9ter Hal\u00e1csy", "P\u00e9ter Rebrus", "Andr\u00e1s Rung", "P\u00e9ter Vajda", "Eszter Simon"], "venue": "In Proc. of LREC", "citeRegEx": "Tr\u00f3n et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tr\u00f3n et al\\.", "year": 2006}, {"title": "Multi-label classification: An overview", "author": ["Tsoumakas", "Katakis2006] Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "Dept. of Informatics, Aristotle University of Thessaloniki, Greece", "citeRegEx": "Tsoumakas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2006}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc. of ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Distributed word clustering for large scale class-based language modeling in machine translation", "author": ["Uszkoreit", "Brants2008] Jakob Uszkoreit", "Thorsten Brants"], "venue": "In Proc. of ACL", "citeRegEx": "Uszkoreit et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Uszkoreit et al\\.", "year": 2008}, {"title": "The viability of web-derived polarity lexicons", "author": ["Sasha BlairGoldensohn", "Kerry Hannan", "Ryan McDonald"], "venue": "In Proc. of NAACL", "citeRegEx": "Velikovich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Velikovich et al\\.", "year": 2010}, {"title": "Semi-supervised mean fields", "author": ["Wang et al.2007] Fei Wang", "Shijun Wang", "Changshui Zhang", "Ole Winther"], "venue": "In Proc. of AISTATS", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["John Blitzer", "Lawrence K Saul"], "venue": "In Proc. of NIPS", "citeRegEx": "Weinberger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2005}, {"title": "Multilingual noise-robust supervised morphological analysis using the wordframe model", "author": ["Richard Wicentowski"], "venue": "In Proc. of SIGPHON", "citeRegEx": "Wicentowski.,? \\Q2004\\E", "shortCiteRegEx": "Wicentowski.", "year": 2004}, {"title": "Minimally supervised morphological analysis by multimodal alignment", "author": ["Yarowsky", "Wicentowski2000] David Yarowsky", "Richard Wicentowski"], "venue": "In Proc. of ACL", "citeRegEx": "Yarowsky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2000}, {"title": "Transition-based dependency parsing with rich non-local features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": "In Proc. of ACL", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Multi-label output codes using canonical correlation analysis", "author": ["Zhang", "Schneider2011] Yi Zhang", "Jeff G Schneider"], "venue": "In Proc. of AISTATS", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "A k-nearest neighbor based algorithm for multi-label classification", "author": ["Zhang", "Zhou2005] Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "In Proc. of IEEE Conf. on Granular Computing", "citeRegEx": "Zhang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2005}, {"title": "Semi-supervised learning with graphs", "author": ["Zhu et al.2005] Xiaojin Zhu", "John Lafferty", "Ronald Rosenfeld"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 49, "context": "As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nie\u00dfen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis et al.", "startOffset": 132, "endOffset": 199}, {"referenceID": 65, "context": ", 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis et al., 2009; Moore, 2015), dependency parsing (Goldberg et al.", "startOffset": 56, "endOffset": 103}, {"referenceID": 51, "context": ", 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis et al., 2009; Moore, 2015), dependency parsing (Goldberg et al.", "startOffset": 56, "endOffset": 103}, {"referenceID": 34, "context": ", 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al.", "startOffset": 41, "endOffset": 64}, {"referenceID": 4, "context": ", 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M\u00fcller and Schuetze, 2015) inter alia.", "startOffset": 27, "endOffset": 48}, {"referenceID": 41, "context": "They are often constructed manually and are expensive to obtain (Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small.", "startOffset": 64, "endOffset": 113}, {"referenceID": 91, "context": "We model the problem of morpho-syntactic lexicon generation as a graph-based semi-supervised learning problem (Zhu et al., 2005; Bengio et al., 2006; Subramanya and Talukdar, 2014).", "startOffset": 110, "endOffset": 180}, {"referenceID": 6, "context": "We model the problem of morpho-syntactic lexicon generation as a graph-based semi-supervised learning problem (Zhu et al., 2005; Bengio et al., 2006; Subramanya and Talukdar, 2014).", "startOffset": 110, "endOffset": 180}, {"referenceID": 91, "context": "This is in contrast to traditional label propagation, where edges indicate similarity exclusively (Zhu et al., 2005).", "startOffset": 98, "endOffset": 116}, {"referenceID": 43, "context": "We show that these automatically created lexicons provide useful features in two extrinsic NLP tasks which require identifying the contextually plausible morphological and syntactic roles: morphological tagging (Haji\u010d and Hladk\u00e1, 1998a; Haji\u010d, 2000) and syntactic dependency parsing (K\u00fcbler et al., 2009).", "startOffset": 283, "endOffset": 304}, {"referenceID": 10, "context": "Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad\u00f3, 2010; T\u00e4ckstr\u00f6m et al., 2012; Owoputi et al., 2013).", "startOffset": 168, "endOffset": 290}, {"referenceID": 42, "context": "Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad\u00f3, 2010; T\u00e4ckstr\u00f6m et al., 2012; Owoputi et al., 2013).", "startOffset": 168, "endOffset": 290}, {"referenceID": 81, "context": "Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad\u00f3, 2010; T\u00e4ckstr\u00f6m et al., 2012; Owoputi et al., 2013).", "startOffset": 168, "endOffset": 290}, {"referenceID": 73, "context": "Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad\u00f3, 2010; T\u00e4ckstr\u00f6m et al., 2012; Owoputi et al., 2013).", "startOffset": 168, "endOffset": 290}, {"referenceID": 58, "context": "Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad\u00f3, 2010; T\u00e4ckstr\u00f6m et al., 2012; Owoputi et al., 2013).", "startOffset": 168, "endOffset": 290}, {"referenceID": 46, "context": "We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language.", "startOffset": 63, "endOffset": 134}, {"referenceID": 10, "context": "Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad\u00f3, 2010; T\u00e4ckstr\u00f6m et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T\u00e4ckstr\u00f6m et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters.", "startOffset": 169, "endOffset": 634}, {"referenceID": 61, "context": "Suffixes are often strong indicators of the morpho-syntactic attributes of a word (Ratnaparkhi, 1996; Clark, 2003).", "startOffset": 82, "endOffset": 114}, {"referenceID": 10, "context": "Suffixes are often strong indicators of the morpho-syntactic attributes of a word (Ratnaparkhi, 1996; Clark, 2003).", "startOffset": 82, "endOffset": 114}, {"referenceID": 91, "context": "Typical graph-propagation algorithms model similarity (Zhu et al., 2005) and thus propagate all attributes along the edges.", "startOffset": 54, "endOffset": 72}, {"referenceID": 10, "context": "feature, is an indication of similar POS tag (Clark, 2003; Koo et al., 2008; Turian et al., 2010), but a surface morphological transformation feature like suffix:ed:ing possibly indicates a change in the tense of the word.", "startOffset": 45, "endOffset": 97}, {"referenceID": 42, "context": "feature, is an indication of similar POS tag (Clark, 2003; Koo et al., 2008; Turian et al., 2010), but a surface morphological transformation feature like suffix:ed:ing possibly indicates a change in the tense of the word.", "startOffset": 45, "endOffset": 97}, {"referenceID": 81, "context": "feature, is an indication of similar POS tag (Clark, 2003; Koo et al., 2008; Turian et al., 2010), but a surface morphological transformation feature like suffix:ed:ing possibly indicates a change in the tense of the word.", "startOffset": 45, "endOffset": 97}, {"referenceID": 39, "context": "Our graph resembles the Ising model, which is a lattice model proposed for describing intermolecular forces (Ising, 1925), and equ.", "startOffset": 108, "endOffset": 121}, {"referenceID": 84, "context": "1 solves the naive mean field approximation of the Ising model (Wang et al., 2007).", "startOffset": 63, "endOffset": 82}, {"referenceID": 21, "context": "We minimize the loss function using online adaptive gradient descent (Duchi et al., 2010) with `2 regularization on the feature weights \u03b8.", "startOffset": 69, "endOffset": 89}, {"referenceID": 29, "context": "For such a task, several advanced methods that take into account the correlation between attributes have been proposed (Ghamrawi and McCallum, 2005; Tsoumakas and Katakis, 2006; F\u00fcrnkranz et al., 2008; Read et al., 2011), here we have adopted the binary relevance method which trains a classifier for every attribute independently of the other attributes, for its simplicity (Godbole and Sarawagi, 2004; Zhang and Zhou, 2005).", "startOffset": 119, "endOffset": 220}, {"referenceID": 62, "context": "For such a task, several advanced methods that take into account the correlation between attributes have been proposed (Ghamrawi and McCallum, 2005; Tsoumakas and Katakis, 2006; F\u00fcrnkranz et al., 2008; Read et al., 2011), here we have adopted the binary relevance method which trains a classifier for every attribute independently of the other attributes, for its simplicity (Godbole and Sarawagi, 2004; Zhang and Zhou, 2005).", "startOffset": 119, "endOffset": 220}, {"referenceID": 36, "context": "This projection step is inspired from the decoding step in label-space transformation approaches to multilabel classification (Hsu et al., 2009; Ferng and Lin, 2011; Zhang and Schneider, 2011).", "startOffset": 126, "endOffset": 192}, {"referenceID": 47, "context": "The universal dependency treebank (McDonald et al., 2013; De Marneffe et al., 2014; Agi\u0107 et al., 2015) contains dependency annotations for sentences and morpho-syntactic annotations for words in context for a number of languages.", "startOffset": 34, "endOffset": 102}, {"referenceID": 20, "context": "Overall, the selected edge features for different languages correspond well to the morphological structure of these languages (Dryer, 2013).", "startOffset": 126, "endOffset": 139}, {"referenceID": 51, "context": "Moore (2015) used this technique to obtain a high quality tag dictionary for words Corpus Propagation", "startOffset": 0, "endOffset": 13}, {"referenceID": 59, "context": "To test the utility of our approach on manually curated lexicons, we investigate publicly available lexicons for Finnish (Pirinen, 2011), Czech (Haji\u010d and Hladk\u00e1, 1998b) and Hungarian (Tr\u00f3n et al.", "startOffset": 121, "endOffset": 136}, {"referenceID": 79, "context": "To test the utility of our approach on manually curated lexicons, we investigate publicly available lexicons for Finnish (Pirinen, 2011), Czech (Haji\u010d and Hladk\u00e1, 1998b) and Hungarian (Tr\u00f3n et al., 2006).", "startOffset": 184, "endOffset": 203}, {"referenceID": 67, "context": "Our taggers are trained in a language independent manner (Haji\u010d, 2000; Smith et al., 2005; M\u00fcller et al., 2013).", "startOffset": 57, "endOffset": 111}, {"referenceID": 53, "context": "Our taggers are trained in a language independent manner (Haji\u010d, 2000; Smith et al., 2005; M\u00fcller et al., 2013).", "startOffset": 57, "endOffset": 111}, {"referenceID": 52, "context": ", 2005; M\u00fcller et al., 2013). The list of features used in training the tagger are listed in Table 6. In addition to the standard features, we use the morpho-syntactic attributes present in the lexicon for every word as features in the tagger. As shown in M\u00fcller and Schuetze (2015), this is typically the most important feature for morphological tagging, even more useful than clusters or word embeddings.", "startOffset": 8, "endOffset": 283}, {"referenceID": 5, "context": "Sentiment lexicons containing semantic polarity labels for words and phrases have been created using bootstrapping and graph-based learning (Banea et al., 2008; Mohammad et al., 2009; Velikovich et al., 2010; Takamura et al., 2007; Lu et al., 2011).", "startOffset": 140, "endOffset": 248}, {"referenceID": 50, "context": "Sentiment lexicons containing semantic polarity labels for words and phrases have been created using bootstrapping and graph-based learning (Banea et al., 2008; Mohammad et al., 2009; Velikovich et al., 2010; Takamura et al., 2007; Lu et al., 2011).", "startOffset": 140, "endOffset": 248}, {"referenceID": 83, "context": "Sentiment lexicons containing semantic polarity labels for words and phrases have been created using bootstrapping and graph-based learning (Banea et al., 2008; Mohammad et al., 2009; Velikovich et al., 2010; Takamura et al., 2007; Lu et al., 2011).", "startOffset": 140, "endOffset": 248}, {"referenceID": 74, "context": "Sentiment lexicons containing semantic polarity labels for words and phrases have been created using bootstrapping and graph-based learning (Banea et al., 2008; Mohammad et al., 2009; Velikovich et al., 2010; Takamura et al., 2007; Lu et al., 2011).", "startOffset": 140, "endOffset": 248}, {"referenceID": 44, "context": "Sentiment lexicons containing semantic polarity labels for words and phrases have been created using bootstrapping and graph-based learning (Banea et al., 2008; Mohammad et al., 2009; Velikovich et al., 2010; Takamura et al., 2007; Lu et al., 2011).", "startOffset": 140, "endOffset": 248}, {"referenceID": 3, "context": "Alfonseca et al. (2010) use distributional similarity scores across instances to propagate attributes using random walks over a graph.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "Alfonseca et al. (2010) use distributional similarity scores across instances to propagate attributes using random walks over a graph. Das and Smith (2012) learn potential semantic frames for unknown predicates by expanding a seed frame lexicon.", "startOffset": 0, "endOffset": 156}, {"referenceID": 72, "context": "Graph-based learning has been used for class-instance acquisition (Talukdar and Pereira, 2010), text classification (Subramanya and Bilmes, 2008), summarization (Erkan and Radev, 2004), structured prediction problems (Subramanya et al., 2010; Das and Petrov, 2011; Garrette et al., 2013) etc.", "startOffset": 217, "endOffset": 287}, {"referenceID": 30, "context": "Graph-based learning has been used for class-instance acquisition (Talukdar and Pereira, 2010), text classification (Subramanya and Bilmes, 2008), summarization (Erkan and Radev, 2004), structured prediction problems (Subramanya et al., 2010; Das and Petrov, 2011; Garrette et al., 2013) etc.", "startOffset": 217, "endOffset": 287}, {"referenceID": 54, "context": "In terms of featurizing the edges, our work resembles previous work which measured similarity between nodes in terms of similarity between the feature types that they share (Muthukrishnan et al., 2011; Saluja and Navr\u00e1til, 2013).", "startOffset": 173, "endOffset": 228}, {"referenceID": 85, "context": "Our work is also related to graphbased metric learning, where the objective is to learn a suitable distance metric between the nodes of a graph for solving a given problem (Weinberger et al., 2005; Dhillon et al., 2010; Dhillon et al., 2012).", "startOffset": 172, "endOffset": 241}, {"referenceID": 17, "context": "Our work is also related to graphbased metric learning, where the objective is to learn a suitable distance metric between the nodes of a graph for solving a given problem (Weinberger et al., 2005; Dhillon et al., 2010; Dhillon et al., 2012).", "startOffset": 172, "endOffset": 241}, {"referenceID": 18, "context": "Our work is also related to graphbased metric learning, where the objective is to learn a suitable distance metric between the nodes of a graph for solving a given problem (Weinberger et al., 2005; Dhillon et al., 2010; Dhillon et al., 2012).", "startOffset": 172, "endOffset": 241}, {"referenceID": 28, "context": ", 2010; Das and Petrov, 2011; Garrette et al., 2013) etc. Our work differs from most of these approaches in that we specifically learn how different features shared between the nodes can correspond to either the propagation of an attribute or an inversion of the attribute value (cf. equ 1). In terms of the capability of inverting an attribute value, our method is close to Goldberg et al. (2007), who present a framework to include dissimilarity between nodes and Talukdar et al.", "startOffset": 30, "endOffset": 398}, {"referenceID": 28, "context": ", 2010; Das and Petrov, 2011; Garrette et al., 2013) etc. Our work differs from most of these approaches in that we specifically learn how different features shared between the nodes can correspond to either the propagation of an attribute or an inversion of the attribute value (cf. equ 1). In terms of the capability of inverting an attribute value, our method is close to Goldberg et al. (2007), who present a framework to include dissimilarity between nodes and Talukdar et al. (2012), who learn which edges can be excluded for label propagation.", "startOffset": 30, "endOffset": 489}, {"referenceID": 15, "context": "Morphological analysis encompasses fields like morphological segmentation (Creutz and Lagus, 2005; Demberg, 2007; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015), and inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004).", "startOffset": 74, "endOffset": 184}, {"referenceID": 60, "context": "Morphological analysis encompasses fields like morphological segmentation (Creutz and Lagus, 2005; Demberg, 2007; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015), and inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004).", "startOffset": 74, "endOffset": 184}, {"referenceID": 55, "context": "Morphological analysis encompasses fields like morphological segmentation (Creutz and Lagus, 2005; Demberg, 2007; Snyder and Barzilay, 2008; Poon et al., 2009; Narasimhan et al., 2015), and inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004).", "startOffset": 74, "endOffset": 184}, {"referenceID": 86, "context": ", 2015), and inflection generation (Yarowsky and Wicentowski, 2000; Wicentowski, 2004).", "startOffset": 35, "endOffset": 86}, {"referenceID": 1, "context": "Paradigm generation requires generating all possible morphological forms of a given base-form according to different linguistic transformations (Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015), whereas our task requires identifying linguistic transformations between two different word forms.", "startOffset": 144, "endOffset": 261}, {"referenceID": 2, "context": "Paradigm generation requires generating all possible morphological forms of a given base-form according to different linguistic transformations (Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015), whereas our task requires identifying linguistic transformations between two different word forms.", "startOffset": 144, "endOffset": 261}, {"referenceID": 56, "context": "Paradigm generation requires generating all possible morphological forms of a given base-form according to different linguistic transformations (Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Ahlberg et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015), whereas our task requires identifying linguistic transformations between two different word forms.", "startOffset": 144, "endOffset": 261}, {"referenceID": 24, "context": "Morpho-syntactic resources have been developed for east-european languages like Slovene (Dzeroski et al., 2000; Erjavec, 2004), Bulgarian (Simov et al.", "startOffset": 88, "endOffset": 126}, {"referenceID": 25, "context": "Morpho-syntactic resources have been developed for east-european languages like Slovene (Dzeroski et al., 2000; Erjavec, 2004), Bulgarian (Simov et al.", "startOffset": 88, "endOffset": 126}, {"referenceID": 66, "context": ", 2000; Erjavec, 2004), Bulgarian (Simov et al., 2004) and highly agglutinative languages like Turkish (Sak et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 63, "context": ", 2004) and highly agglutinative languages like Turkish (Sak et al., 2008).", "startOffset": 56, "endOffset": 74}, {"referenceID": 37, "context": "Morpho-syntactic lexicons are crucial components in acousting modeling and automatic speech recognition, where they have been developed for lowresourced languages (Huet et al., 2008; Besacier et al., 2014).", "startOffset": 163, "endOffset": 205}, {"referenceID": 7, "context": "Morpho-syntactic lexicons are crucial components in acousting modeling and automatic speech recognition, where they have been developed for lowresourced languages (Huet et al., 2008; Besacier et al., 2014).", "startOffset": 163, "endOffset": 205}, {"referenceID": 29, "context": "We can instead model such correlations as constraints during the learning step to obtain better solutions (Ghamrawi and McCallum, 2005; Tsoumakas and Katakis, 2006; F\u00fcrnkranz et al., 2008; Read et al., 2011).", "startOffset": 106, "endOffset": 207}, {"referenceID": 62, "context": "We can instead model such correlations as constraints during the learning step to obtain better solutions (Ghamrawi and McCallum, 2005; Tsoumakas and Katakis, 2006; F\u00fcrnkranz et al., 2008; Read et al., 2011).", "startOffset": 106, "endOffset": 207}, {"referenceID": 48, "context": "Word embeddings can be used to connect word node which are similar in meaning (Mikolov et al., 2013).", "startOffset": 78, "endOffset": 100}], "year": 2017, "abstractText": "Morpho-syntactic lexicons provide information about the morphological and syntactic roles of words in a language. Such lexicons are not available for all languages and even when available, their coverage can be limited. We present a graph-based semi-supervised learning method that uses the morphological, syntactic and semantic relations between words to automatically construct wide coverage lexicons from small seed sets. Our method is language-independent, and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing.", "creator": "LaTeX with hyperref package"}}}