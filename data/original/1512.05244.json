{"id": "1512.05244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2015", "title": "Learning Games and Rademacher Observations Losses", "abstract": "It has recently been shown that supervised learning with the popular logistic loss is equivalent to optimizing the exponential loss over sufficient statistics about the class: Rademacher observations (rados). We first show that this unexpected equivalence can actually be generalized to other example / rado losses, with necessary and sufficient conditions for the equivalence, exemplified on four losses that bear popular names in various fields: exponential (boosting), mean-variance (finance), Linear Hinge (on-line learning), ReLU (deep learning), and unhinged (statistics). Second, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (with Minkowski sums) in the equivalent rado loss. This brings simple and powerful rado-based learning algorithms for sparsity-controlling regularization, that we exemplify on a boosting algorithm for the regularized exponential rado-loss, which formally boosts over four types of regularization, including the popular ridge and lasso, and the recently coined slope --- we obtain the first proven boosting algorithm for this last regularization. Through our first contribution on the equivalence of rado and example-based losses, Omega-R.AdaBoost~appears to be an efficient proxy to boost the regularized logistic loss over examples using whichever of the four regularizers. Experiments display that regularization consistently improves performances of rado-based learning, and may challenge or beat the state of the art of example-based learning even when learning over small sets of rados. Finally, we connect regularization to differential privacy, and display how tiny budgets can be afforded on big domains while beating (protected) example-based learning.", "histories": [["v1", "Wed, 16 Dec 2015 16:56:02 GMT  (1510kb)", "http://arxiv.org/abs/1512.05244v1", null], ["v2", "Sat, 13 Feb 2016 00:33:22 GMT  (1510kb)", "http://arxiv.org/abs/1512.05244v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["richard nock"], "accepted": false, "id": "1512.05244"}, "pdf": {"name": "1512.05244.pdf", "metadata": {"source": "CRF", "title": "Learning Games and Rademacher Observations Losses", "authors": ["Richard Nock"], "emails": ["richard.nock@nicta.com.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n05 24\n4v 1\n[ cs\n.L G\n] 1\n6 D\nec 2"}, {"heading": "1 Introduction", "text": "A recent result has shown that minimising the popular logistic loss over examples in supervised learning is equivalent to the minimisation of the exponential loss over sufficient statistics about the class known as Rademacher observations (rados, (Nock et al., 2015)), for the same classifier. In short, we fit a classifier over data that is different from examples, and the same classifier generalizes well to new observations. It is known that sufficient statistics carry the intractability of certain processes that would otherwise be easy with data (Montanari, 2014). In the case of rados, such a computational caveat turns out to be a big advantage as privacy is becoming crucial (Enserink & Chin, 2015). Indeed, rados allow to protect data not just from a computational complexity standpoint, but also from geometric, algebraic and statistical standpoints (Nock et al., 2015), while still allowing to learn accurate classifiers.\nTwo key problems remain: learning from rados can compete experimentally with learning from examples, but there is a gap to reduce for rados to be not just a good material to learn from in a privacy setting, but also a serious alternative to learning from examples at large, yielding new avenues to supervised learning.\n1\nSecond, theoretically speaking, it is crucial to understand if this equivalence holds only for the logistic and exponential losses, or if it can be generalised and shed new light on losses and their minimisation.\nIn this paper, we provide answers to these two questions, with four main contributions. Our first contribution is to show that this generalization indeed holds: other example losses admit equivalent losses in the rado world, meaning in particular that their minimiser classifier is the same, regardless of the dataset of examples. The technique we use exploits a two-player zero sum game representation of convex losses, that has been very useful to analyse boosting algorithms (Schapire, 2003; Telgarsky, 2012), with one key difference: payoffs are non-linear convex, eventually non-differentiable. These also resemble the entropic dual losses (Reid et al., 2015), with the difference that we do not enforce conjugacy over the simplex. The conditions of the game are slightly different for examples and rados. We provide necessary and sufficient conditions for the resulting losses over examples and rados to be equivalent. Informally, equivalence happens iff the convex functions of the games satisfy a symmetry relationship and the weights satisfy a linear system of equations. We give four cases of this equivalence. It turns out that the losses involved bear popular names in different communities, even when not all of them are systematically used as losses per se: exponential, logistic, square, mean-variance, ReLU, linear Hinge, and unhinged losses (Nair & Hinton, 2010; Gentile & Warmuth, 1998; Nock & Nielsen, 2008; Telgarsky, 2012; Vapnik, 1998; van Rooyen et al., 2015) (and many others).\nOur second contribution came unexpectedly through this equivalence. Regularizing a loss is common in machine learning (Bach et al., 2011). We show a sufficient condition for the equivalence under which regularizing the example loss is equivalent to regularizing the rados in the rado loss, i.e. making a Minkowski sum of the rado set with a classifier-based set. This property is independent of the regularizer, and holds for all four cases of equivalence.\nThird, we propose a boosting algorithm, \u2126-R.ADABOOST, that learns a classifier from rados using the exponential regularized rado loss, with regularization choice belonging to the ridge, lasso, \u2113\u221e, or the recently coined SLOPE (Bogdan et al., 2015). Experiments display that \u2126-R.ADABOOST is all the better vs ADABOOST (unregularized and \u21131-regularized) as the domain gets larger, and is able to learn both accurate and sparse classifiers, making it a good contender for supervised learning at large on big domains. From a theoretical standpoint, we show that for any of these four regularizations, \u2126-R.ADABOOST is a boosting algorithm \u2014 thus, through our first contribution, \u2126-R.ADABOOST is an efficient proxy to boost the regularized logistic loss over examples using whichever of the four regularizers, and by extension, any linear combination of them (e.g., for elastic net regularization (Zou & Hastie, 2005)). We are not aware of any regularized logistic loss formal boosting algorithm with such a wide spectrum of regularizers.\nOur fourth contribution is a direct application of our findings to \u03b5-differential privacy (DP). We protect directly the examples, granting the property that all subsequent stages are DP as well. We show theoretically that a most popular mechanism (Dwork & Roth, 2014) used to protect examples in rados amounts to a surrogate form of regularization of the clean examples\u2019 loss; furthermore, the amount of noise can be commensurate to the one for a direct protection of examples. In other words, since rados\u2019 norm may be much larger than examples\u2019 (e.g. on big domains), we can expect noise to be much less damaging if learning from protected rados, and afford tiny budgets (e.g. \u03b5 \u2248 10\u22124) at little cost in accuracy. Experiments validate this intuition.\nThe rest of this paper is as follows. \u00a72, 3 and 4 respectively present the equivalence between example and rado losses, its extension to regularized learning and \u2126-R.ADABOOST. \u00a75, 6 and 7 respectively present differential privacy vs regularized rado losses, detail experiments, and conclude. In order not to laden the paper\u2019s body, an appendix, starting page 15 of this draft, contains the proofs and additional theoretical and experimental results.\n2"}, {"heading": "2 Games and equivalent example/rado losses", "text": "We first start by defining and analysing our general two players game setting. To avoid notational load, we shall not put immediately the learning setting at play, considering for the moment that the learner fits a general vector z \u2208 Rm, which depends both on data (examples or rados) and classifier. Let [m] .= {1, 2, ...,m} and \u03a3m .= {\u22121, 1}m, for m > 0. Let \u03d5e : R \u2192 R and \u03d5r : R \u2192 R two convex and lower-semicontinuous generators. We define functions Le : Rm \u00d7 Rm \u2192 R and Lr : R2m \u00d7 Rm \u2192 R:\nLe(p,z) . =\n\u2211\ni\u2208[m]\npizi + \u00b5e \u2211\ni\u2208[m]\n\u03d5e(pi) , (1)\nLr(q,z) . =\n\u2211\nI\u2286[m]\nqI \u2211\ni\u2208I\nzi + \u00b5r \u2211\nI\u2286[m]\n\u03d5r(qI) , (2)\nwhere \u00b5e,\u00b5r > 0 do not depend on z. For the notation to be meaningful, the coordinates in q are assumed (wlog) to be in bijection with 2[m]. The dependence of both problems in their respective generators is implicit and shall be clear from context. The adversary\u2019s goal is to fit\np\u2217(z) . = arg min\np\u2208Rm Le(p,z) , (3)\nq\u2217(z) . = arg min\nq\u2208H2m Lr(q,z) , (4)\nwith H2 m . = {q \u2208 R2m : 1\u22a4q = 1}, so as to attain\nL\u2217e (z) . = Le(p \u2217(z),z) , (5) L\u2217r (z) . = Lr(q \u2217(z),z) , (6)\nand let \u2202L\u2217e (z) and \u2202L \u2217 r (z) denote their subdifferentials. We view the learner\u2019s task as the problem of maximising the corresponding problems in eq. (5) (with examples) or (6) (with rados), or equivalently minimising negative the corresponding function, then called a loss function. The question of when these two problems are equivalent from the learner\u2019s standpoint motivates the following definition.\nDefinition 1 Two generators \u03d5e, \u03d5r are said proportionate iff for any m > 0, there exists (\u00b5e,\u00b5r) such that\nL\u2217e (z) = L \u2217 r (z) + b ,\u2200z \u2208 Rm . (7)\n(b does not depend on z) \u2200m \u2208 N\u2217, let\nGm . =\n[ 0 \u22a4 2m\u22121 1 \u22a4 2m\u22121\nGm\u22121 Gm\u22121\n]\n(\u2208 {0, 1}m\u00d72m ) (8)\nif m > 1, and G1 . = [0 1] otherwise (zd denotes a vector in Rd). Each column of Gm is the binary indicator vector for the edge vectors summed in a rado; wlog, we let these to give the bijection between 2[m] and coordinates of q(\u2217)(z).\nTheorem 2 \u03d5e, \u03d5r are proportionate iff the optimal solutions p\u2217(z) and q\u2217(z) to eqs (3) and (4) satisfy\np\u2217(z) \u2208 \u2202L\u2217r (z) , (9) Gmq \u2217(z) \u2208 \u2202L\u2217e (z) . (10)"}, {"heading": "In the case where \u03d5e, \u03d5r are differentiable, they are proportionate iff p\u2217(z) = Gmq\u2217(z).", "text": "3\n(Proof in Appendix, Subsection 9.1) Theorem 2 gives a necessary and sufficient condition for two generators to be proportionate. It does not say how to construct one from the other, if possible. We now show that it is indeed possible and prune the search space: if \u03d5e is proportionate to some \u03d5r, then it has to be a \u201csymmetrized\u201d version of \u03d5r, according to the following definition.\nDefinition 3 Let \u03d5r such that dom(\u03d5r) \u2287 (0, 1). We call \u03d5s(r)(z) .= \u03d5r(z) + \u03d5r(1 \u2212 z) the symmetrisation of \u03d5r.\nLemma 4 If \u03d5e and \u03d5r are proportionate, then \u03d5e(z) = (\u00b5r/\u00b5e) \u00b7 \u03d5s(r)(z) + (b/\u00b5e), where b appears in eq. (7).\n(Proof in Appendix, Subsection 9.2) To summarize, \u03d5e and \u03d5r are proportionate iff (i) they meet the structural property that \u03d5e is (proportional to) the symmetrized version of \u03d5r (according to Definition 3), and (ii) the optimal solutions p\u2217(z) and q\u2217(z) to problems (1) and (2) satisfy the conditions of Theorem 2. Depending on the direction, we have two cases to craft proportionate generators. First, if we have \u03d5r, then necessarily \u03d5e \u221d \u03d5s(r) so we merely have to check Theorem 2. Second, if we have \u03d5e, then it matches Definition 31. In this case, we have to find \u03d5r = f + g where g(z) = \u2212g(1 \u2212 z) and \u03d5e(z) = f(z) + f(1\u2212 z).\nWe now come back to L\u2217e (z), L \u2217 r (z) as defined in Definition 1, and make the connection with example and rado losses. In the next definition, an e-loss \u2113e(z) is a function defined over the coordinates of z, and a r-loss \u2113r(z) is a function defined over the subsets of sums of coordinates. Functions can depend on other parameters as well.\nDefinition 5 Suppose e-loss \u2113e(z) and r-loss \u2113r(z) are such that there exist (i) fe : R \u2192 R and fr(z) : R \u2192 R both strictly increasing and such that \u2200z \u2208 Rm,\n\u2212 L\u2217e (z) = fe (\u2113e(z)) , (11) \u2212L\u2217r (z) = fr (\u2113r(z)) . (12)\nThen the couple (\u2113e, \u2113r) is called a couple of equivalent example-rado losses.\nHereafter, we just write \u03d5s instead of \u03d5s(r).\nLemma 6 \u03d5r(z) . = z log z \u2212 z is proportionate to \u03d5e .= \u03d5s = z log z + (1 \u2212 z) log(1 \u2212 z) \u2212 1, whenever \u00b5e = \u00b5r.\n(Proof in Appendix, Subsection 9.3)\nCorollary 7 The following example and rado losses are equivalent for any \u00b5 > 0:\n\u2113e(z,\u00b5) = \u2211\ni\u2208[m]\nlog\n(\n1 + exp\n(\n\u2212 1 \u00b5 \u00b7 zi\n))\n, (13)\n\u2113r(z,\u00b5) = \u2211\nI\u2286[m]\nexp\n(\n\u2212 1 \u00b5 \u00b7 \u2211\ni\u2208I\nzi\n)\n. (14)\n(Proof in Appendix, Subsection 9.4)\nLemma 8 \u03d5r(z) . = (1/2)\u00b7z2 is proportionate to \u03d5e .= \u03d5s = (1/2)\u00b7(1\u22122z(1\u2212z)) whenever \u00b5e = \u00b5r/2m\u22121.\n1Alternatively, \u2212\u03d5e is permissible (Kearns & Mansour, 1999).\n4\n(Proof in Appendix, Subsection 9.5)\nCorollary 9 The following example and rado losses are equivalent, for any \u00b5 > 0:\n\u2113e(z,\u00b5) = \u2211\ni\u2208[m]\n(\n1\u2212 1 \u00b5 \u00b7 zi\n)2\n, (15)\n\u2113r(z,\u00b5) = \u2212 ( EI [ 1\n\u00b5 \u00b7 \u2211\ni\u2208I\nzi\n] \u2212 \u00b5 \u00b7 VI [ 1\n\u00b5 \u00b7 \u2211\ni\u2208I\nzi\n])\n, (16)\nwhere EI[X(I)] and VI[X(I)] denote the expectation and variance of X wrt uniform weights on I \u2286 [m].\n(Proof in Appendix, Subsection 9.6) We now investigate cases of non differentiable proportionate generators, the first of which is self-proportionate (\u03d5e = \u03d5r). We let \u03c7A(z) be the indicator function: \u03c7A(z) . = 0 if z \u2208 A (and +\u221e otherwise), convex since A = [0, 1] is convex.\nLemma 10 \u03d5r(z) . = \u03c7[0,1](z) is self-proportionate,\u2200\u00b5e ,\u00b5r.\n(Proof in Appendix, Subsection 9.7)\nCorollary 11 The following example and rado losses are equivalent, for any \u00b5e,\u00b5r:\n\u2113e(z,\u00b5e) = \u2211\ni\u2208[m]\nmax\n{\n0,\u2212 1 \u00b5e\n\u00b7 zi } , (17)\n\u2113r(z,\u00b5r) = max\n{\n0, max I\u2286[m]\n{\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n}}\n. (18)\n(Proof in Appendix, Subsection 9.8)\nLemma 12 \u03d5r(z) . = \u03c7[ 12m , 1 2 ] (z) is proportionate to \u03d5e . = \u03d5s = \u03c7{ 12}(z), for any \u00b5e,\u00b5r.\n(Proof in Appendix, Subsection 9.9)\nCorollary 13 The following example and rado losses are equivalent, for any \u00b5e,\u00b5r:\n\u2113e(z,\u00b5e) = \u2211\ni\n\u2212 1 \u00b5e \u00b7 zi , (19)\n\u2113r(z,\u00b5r) = EI\n[\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n]\n. (20)\n(Proof in Appendix, Subsection 9.10) Table 1 summarizes the four equivalent example and rado losses."}, {"heading": "3 Learning with (rado) regularized losses", "text": "We now plug the learning setting. The learner is given a set of examples S = {(xi, yi), i = 1, 2, ...,m} where xi \u2208 Rd, yi \u2208 \u03a31 (for i = 1, 2, ...,m). It returns a classifier h : Rd \u2192 R from a predefined set H. Let zi(h) . = yh(xi) and define z(h) as the corresponding vector in Rm, which we plug in the losses of Table 1 to obtain the corresponding example and rado losses. Losses simplify conveniently when H\n5\nconsists of linear classifiers, h(x) . = \u03b8\u22a4x for some \u03b8 \u2208 \u0398 \u2286 Rd. In this case, the example loss can be described using edge vectors Se . = {yi \u00b7 xi, i = 1, 2, ...,m} since zi = \u03b8\u22a4(yi \u00b7 xi), and the rado loss can be described using rademacher observations (Nock et al., 2015), since \u2211\ni\u2208I zi = \u03b8 \u22a4\u03c0\u03c3 for \u03c3i = yi iff i \u2208 I\n(and \u2212yi otherwise) and \u03c0\u03c3 .= (1/2) \u00b7 \u2211 i(\u03c3i + yi) \u00b7 xi. Let us define S\u2217r . = {\u03c0\u03c3 ,\u03c3 \u2208 \u03a3m} the set of all rademacher observations. We rewrite any couple of equivalent example and rado losses as \u2113e(Se,\u03b8) and \u2113r(S \u2217 r ,\u03b8) respectively\n2 , omitting parameters \u00b5e and \u00b5r, assumed to be fixed beforehand for the equivalence to hold (see Table 1). Let us regularize the example loss, so that the learner\u2019s goal is to minimize\n\u2113e(Se,\u03b8,\u2126) . = \u2113e(Se,\u03b8) + \u2126(\u03b8) , (21)\nwith \u2126 a regularizer (Bach et al., 2011). The following shows that when fe in eq. (11) is linear, there is a rado-loss equivalent to this regularized loss, regardless of \u2126.\nTheorem 14 Suppose H contains linear classifiers. Let (\u2113e(Se,\u03b8), \u2113r(S\u2217r ,\u03b8)) be any couple of equivalent example-rado losses such that fe in eq. (11) is linear:\nfe(z) = ae \u00b7 z + be , (22)\nfor some ae > 0, be \u2208 R. Then for any regularizer \u2126(.), the regularized example loss \u2113e(Se,\u03b8,\u2126) is equivalent to rado loss \u2113r(S\u2217,\u2126,\u03b8r ,\u03b8) computed over regularized rados:\nS\u2217,\u2126,\u03b8r . = S\u2217r \u2295 {\u2212\u2126\u0303(\u03b8) \u00b7 \u03b8} , (23)\nwhere \u2295 is Minkowski sum and \u2126\u0303(\u03b8) .= ae\u00b7\u2126(\u03b8)/\u2016\u03b8\u201622 if \u03b8 6= 0 (and 0 otherwise, assuming wlog \u2126(0) = 0).\n(Proof in Appendix, Subsection 9.11) Theorem 14 applies to all rado losses (I-IV) in Table 1. The effect of regularization on rados is intuitive from the margin standpoint: assume that a \u201cgood\u201d classifier \u03b8 is one that ensures lowerbounded inner products \u03b8\u22a4z \u2265 \u03c4 for some margin threshold \u03c4 . Then any good classifier on a regularized rado \u03c0\u03c3 shall actually meet, over examples,\n\u2211\ni:yi=\u03c3i\n\u03b8\u22a4(yi \u00b7 xi) \u2265 \u03c4 + ae \u00b7 \u2126(\u03b8) . (24)\nNotice that ineq (24) ties an \u201daccuracy\u201d of \u03b8 (edges, left hand-side) and its sparsity (right-hand side). One important question is the way the minimisation of the regularized rado loss impacts the minimisation of the regularized examples loss when one subsamples the rados, and learns \u03b8 from some Sr \u2286 S\u2217r with eventually\n2To prevent notational overload, we blend the notions of (pointwise) loss and (samplewise) risk, as just \u201closses\u201d.\n6\n|Sr| \u226a |S\u2217r |. We give an answer for the log-loss (Nock et al., 2015) (row I in Table 1), and for this objective define the \u2126-regularized exp-rado-loss computed over Sr, with |Sr| = n and \u03c9 > 0 user-fixed:\n\u2113expr (Sr,\u03b8,\u2126)\n. =\n1 n \u00b7 \u2211\nj\u2208[n]\nexp\n( \u2212\u03b8\u22a4 ( \u03c0j \u2212\u03c9 \u00b7 \u2126(\u03b8)\n\u2016\u03b8\u201622 \u00b7 \u03b8\n))\n, (25)\nwhenever \u03b8 6= 0 (otherwise, we discard the factor depending on \u03c9 in the formula). We assume that \u2126 is a norm, and let \u2113expr (Sr,\u03b8) denote the unregularized loss (\u03c9 = 0 in eq. (25)), and we let \u2113 log e (Se,\u03b8,\u2126) . = (1/m) \u2211 i log ( 1 + exp ( \u2212\u03b8\u22a4(yi \u00b7 xi) )) +\u2126(\u03b8) denote the \u2126-regularized log-loss. Notice that we normalize losses. We define the open ball B\u2126(0, r) . = {x \u2208 Rd : \u2126(x) < r} and r\u22c6\u03c0 . = (1/m) \u00b7 maxS\u2217r \u2126\u22c6(\u03c0\u03c3), where \u2126\u22c6 is the dual norm of \u2126. The following Theorem is a direct application of Theorem 3 in (Nock et al., 2015), and shows mild conditions on Sr \u2286 S\u2217r for the minimization of \u2113expr (Sr,\u03b8,\u2126) to indeed yield that of \u2113loge (Se,\u03b8,\u2126).\nTheorem 15 Assume \u0398 \u2286 B\u2016.\u20162(0, r\u03b8), with r\u03b8 > 0. Let \u033a(\u03b8) . = (sup\u03b8\u2032\u2208\u0398max\u03c0\u03c3\u2208S\u2217r exp(\u2212\u03b8\u2032\u22a4\u03c0\u03c3))/\u2113expr (S\u2217r ,\u03b8). Then if m is sufficiently large, \u2200\u03b4 > 0, there is probability \u2265 1\u2212 \u03b4 over the sampling of Sr that any \u03b8 \u2208 \u0398 satisfies:\n\u2113loge (Se,\u03b8,\u2126) \u2264 log 2 + (1/m) \u00b7 log \u2113expr (Sr,\u03b8,\u2126)\n+O\n(\n\u033a(\u03b8) m\u03b2 \u00b7 \u221a r\u03b8r\u22c6\u03c0 n + d nm log n d\u03b4\n)\n,\nas long as \u03c9 \u2265 um for some constant u > 0."}, {"heading": "4 Boosting with (rado) regularized losses", "text": "\u2126-R.ADABOOST presents our approach to learning with rados regularized with regularizer \u2126 to minimise loss \u2113expr (Sr,\u03b8,\u2126) in eq. (25). Classifier \u03b8t is defined as \u03b8t . = \u2211t t\u2032=1 \u03b1\u03b9(t\u2032) \u00b7 1\u03b9(t\u2032), where 1k is the kth canonical basis vector. Frameboxes highlight the differences with RADOBOOST (Nock et al., 2015). The expected edge rt used to compute \u03b1t in eq. (27) is based on the following basis assignation:\nr\u03b9(t) \u2190 1\n\u03c0\u2217\u03b9(t)\nn\u2211\nj=1\nwtj\u03c0j\u03b9(t) (\u2208 [\u22121, 1]) . (32)\nThe computation of rt is eventually tweaked by the weak learner, as displayed in Algorithm \u2126-WL. We investigate four choices for \u2126. For each of them, we prove the boosting ability of \u2126-R.ADABOOST (\u0393 is symmetric positive definite, Sd is the symmetric group of order d, |\u03b8| is the vector whose coordinates are the absolute values of the coordinates of \u03b8):\n\u2126(\u03b8) =\n \n\n\u2016\u03b8\u20161 .= |\u03b8|\u22a41 Lasso \u2016\u03b8\u20162\u0393 . = \u03b8\u22a4\u0393\u03b8 Ridge \u2016\u03b8\u2016\u221e .= maxk |\u03b8k| \u2113\u221e \u2016\u03b8\u2016\u03a6 .= maxM\u2208Sd(M|\u03b8|)\u22a4\u03be SLOPE\n(33)\n(Bach et al., 2011; Bogdan et al., 2015; Duchi & Singer, 2009; Su & Cande\u0300s, 2015). The coordinates of \u03be in SLOPE are \u03bek . = \u03a6\u22121(1 \u2212 kq/(2d)) where \u03a6\u22121(.) is the quantile of the standard normal distribution and q \u2208 (0, 1); thus, the largest coordinates (in absolute value) of \u03b8 are more penalized. We now establish the boosting ability of \u2126-R.ADABOOST. We give no direction for Step 1 in \u2126-WL, which is consistent with the definition of a weak learner in the boosting theory: all we require from the weak learner is |r.| no smaller than some weak learning threshold \u03b3WL > 0.\n7\nAlgorithm 1 \u2126-R.ADABOOST Input rados Sr . = {\u03c01,\u03c02, ...,\u03c0n}; T \u2208 N\u2217; \u03c9 \u2208 R+; \u03b3 \u2208 (0, 1);\nStep 1 : let \u03b80 \u2190 0, w0 \u2190 (1/n)1 ; Step 2 : for t = 1, 2, ..., T\nStep 2.1 : call the weak learner\n(\u03b9(t), rt) \u2190 \u2126-WL(Sr,wt,\u03b3,\u03c9,\u03b8t\u22121) ; (26)\nStep 2.2 : let\n\u03b1\u03b9(t) \u2190 1\n2\u03c0\u2217\u03b9(t) log 1 + rt 1\u2212 rt ; (27)\n\u03b4t \u2190 \u03c9 \u00b7 (\u2126(\u03b8t)\u2212 \u2126(\u03b8t\u22121)) ; (28)\nStep 2.3 : for j = 1, 2, ..., n\nwtj \u2190 w(t\u22121)j\nZt \u00b7 exp\n( \u2212\u03b1t\u03c0j\u03b9(t) + \u03b4t ) ; (29)\nReturn \u03b8T ;\nDefinition 16 Fix any constant \u03b3WL \u2208 (0, 1). \u2126-WL is said to be a \u03b3WL-Weak Learner iff the feature \u03b9(t) it picks at iteration t satisfies |r\u03b9(t)| \u2265 \u03b3WL, for any t = 1, 2, ..., T .\nWe also provide an optional step for the weak learner in \u2126-WL, which we exploit in the experimentations, which gives a total preference order on features to optimise further the convergence of \u2126-R.ADABOOST.\nTheorem 17 (boosting with ridge). Take \u2126(.) = \u2016.\u20162\u0393. Fix any 0 < a < 1/5, and suppose that \u03c9 and the number of iterations T of \u2126-R.ADABOOST are chosen so that\n\u03c9 < (2amin k max j\n\u03c02jk)/(T\u03bb\u0393) , (34)\nwhere \u03bb\u0393 > 0 is the largest eigenvalue of \u0393. Then there exists some \u03b3 > 0 (depending on a, and given to \u2126-WL) such that for any fixed 0 < \u03b3WL < \u03b3, if \u2126-WL is a \u03b3WL-Weak Learner, then \u2126-R.ADABOOST returns at the end of the T boosting iterations a classifier \u03b8T which meets:\n\u2113expr (Sr,\u03b8T , \u2016.\u20162\u0393) \u2264 exp(\u2212a\u03b32WLT/2) . (35)\nFurthermore, if we fix a = 1/7, then we can fix \u03b3 = 0.98, and if we consider a = 1/10, then we can fix \u03b3 = 0.999.\n(Proof in Appendix, Subsection 9.12) Two remarks are in order. First, the cases a = 1/7, 1/10 show that \u2126-WL can still obtain large edges in eq. (32), so even a \u201cstrong\u201d weak learner might fit in for \u2126-WL, without clamping edges. Second, the right-hand side of ineq. (34) may be very large if we consider that\n8\nAlgorithm 2 \u2126-WL, for \u2126 \u2208 {\u2016.\u20161, \u2016.\u20162\u0393, \u2016.\u2016\u221e, \u2016.\u2016\u03a6} Input set of rados Sr . = {\u03c01,\u03c02, ...,\u03c0n}; weights w \u2208 \u25b3n; parameters \u03b3 \u2208 (0, 1), \u03c9 \u2208 R+; classifier\n\u03b8 \u2208 Rd; Step 1 : pick weak feature \u03b9\u2217 \u2208 [d];\nOptional \u2014 use preference order:\n\u03b9 \u03b9\u2032 \u21d4 |r\u03b9| \u2212 \u03b4\u03b9 \u2265 |r\u03b9\u2032 | \u2212 \u03b4\u03b9\u2032 ; (30) (\u03b4\u03b9 . = \u03c9 \u00b7 (\u2126(\u03b8 + \u03b1\u03b9 \u00b7 1\u03b9)\u2212 \u2126(\u03b8)))\n// r\u03b9 is given in (32), \u03b1\u03b9 is given in (27)\nStep 2 : if \u2126 = \u2016.\u20162\u0393 then\nr\u2217 \u2190 { r\u03b9\u2217 if r\u03b9\u2217 \u2208 [\u2212\u03b3,\u03b3] sign (r\u03b9\u2217) \u00b7 \u03b3 otherwise ; (31)\nelse r\u2217 \u2190 r\u03b9\u2217 ; Return (\u03b9\u2217, r\u2217);\nmink maxj \u03c0 2 jk may be proportional to m 2. So the constraint on \u03c9 is in fact loose, and \u03c9 may easily meet the constraint of Thm 15.\nTheorem 18 (boosting with lasso or \u2113\u221e). Take \u2126(.) \u2208 {\u2016.\u20161, \u2016.\u2016\u221e}. Suppose \u2126-WL is a \u03b3WL-Weak Learner for some \u03b3WL > 0. Suppose \u22030 < a < 3/11 s. t. \u03c9 satisfies:\n\u03c9 = a\u03b3WL min k max j\n|\u03c0jk| . (36)\nThen \u2126-R.ADABOOST returns at the end of the T boosting iterations a classifier \u03b8T which meets:\n\u2113expr (Sr,\u03b8T ,\u2126) \u2264 exp(\u2212T\u0303\u03b32WL/2) , (37)\nwhere\nT\u0303 . =\n{ a\u03b3WLT if \u2126 = \u2016.\u20161\n(T \u2212 T\u2217) + a\u03b3WL \u00b7 T\u2217 if \u2126 = \u2016.\u2016\u221e , (38)\nand T\u2217 is the number of iterations where the feature computing the \u2113\u221e norm was updated3.\n(Proof in Appendix, Subsection 9.13) We finally investigate the SLOPE choice. The Theorem is proven for \u03c9 = 1 in \u2126-R.ADABOOST, for two reasons: it matches the original definition (Bogdan et al., 2015) and furthermore it unveils an interesting connection between boosting and the SLOPE properties (Su & Cande\u0300s, 2015).\nTheorem 19 (boosting with SLOPE). Take \u2126(.) = \u2016.\u2016\u03a6. Suppose wlog |\u03b8Tk| \u2265 |\u03b8T (k+1)|,\u2200k, and fix \u03c9 = 1. Let\na . = min { 3\u03b3WL 11 , \u03a6\u22121(1\u2212 q/(2d)) mink maxj |\u03c0jk| } . (39)\n3If several features match this criterion, T\u2217 is the total number of iterations for all these features.\n9\nAlgorithm 3 DP-RADOS Input rados Sr . = {\u03c01,\u03c02, ...,\u03c0n}; budget \u03b5 > 0;\nStep 1 : let SDPr \u2190 \u2205; Step 2 : for j = 1, 2, ..., n\nStep 2.1 : sample zj as zjk \u223c Lap(z|nre/\u03b5) ,\u2200k; Step 2.2 : SDPr \u2190 SDPr \u222a {\u03c0j + zj};\nReturn SDPr ;\nSuppose (i) \u2126-WL is a \u03b3WL-Weak Learner for some \u03b3WL > 0, and (ii) the q-value is chosen to meet:\nq \u2265 2 \u00b7max k\n{( 1\u2212 \u03a6 ( 3\u03b3WL 11 \u00b7max j |\u03c0jk| ))/( k d )} .\nThen classifier \u03b8T returned by \u2126-R.ADABOOST at the end of the T boosting iterations satisfies:\n\u2113expr (Sr,\u03b8T , \u2016.\u2016\u03a6) \u2264 exp(\u2212a\u03b32WLT/2) . (40)\n(Proof in Appendix, Subsection 9.14) Constraint (ii) on q is interesting in the light of the properties of SLOPE (Bogdan et al., 2015; Su & Cande\u0300s, 2015). Modulo some assumptions, SLOPE yields a control the false discovery rate (FDR) \u2014 that is, sparsity errors, negligible coefficients in the \u201dtrue\u201d linear model \u03b8\u2217 that are actually found significant in the learned \u03b8 \u2014. Constraint (ii) links the \u201dsmall\u201d achievable FDR (upperbounded by q) to the \u201dboostability\u201d of the data: the fact that each feature k can be chosen by the weak learner for a \u201dlarge\u201d \u03b3WL, or has maxj |\u03c0jk| large, precisely flags potential significant features, thus reducing the risk of sparsity errors, and allowing small q, which is constraint (ii). Using the second order approximation of normal quantiles (Su & Cande\u0300s, 2015), a sufficient condition for (ii) is that, for some constant K ,\n\u03b3WL min j max j\n|\u03c0jk| \u2265 K \u00b7 \u221a log d+ log q\u22121 ; (41)\nbut minj maxj |\u03c0jk| is proportional to m, so ineq. (41), and thus (ii), may hold even for small samples and q-values.\nWe can now have a look at the regularized log-loss of \u03b8T over examples, as depicted in Theorem 15, and show that it is guaranteed a monotonic decrease with T , with high probability, for any applicable choice of regularization, since we get indeed that the regularized log-loss of \u03b8T output by \u2126-R.ADABOOST, computed on examples, satisfies with high probability \u2113loge (Se,\u03b8,\u2126) \u2264 log 2 \u2212 \u03ba \u00b7 T + \u03c4(m), with \u03c4(m) \u2192 0 when m \u2192 \u221e, and \u03ba does not depend on T . Hence, \u2126-R.ADABOOST is an efficient proxy to boost the regularized log-loss over examples, using whichever of the ridge, lasso, \u2113\u221e or SLOPE regularization, establishing the first boosting algorithm for this last choice. Notice finally that we can also choose any linear combinations of the regularizers and still keep the formal boosting property, thereby extending our results e.g., to the popular elastic nets regularization (Zou & Hastie, 2005)."}, {"heading": "5 Regularized losses and differential privacy", "text": "We show here that the standard differential privacy (DP) mechanism (Dwork & Roth, 2014) to protect examples in rados \u2014 not investigated in (Nock et al., 2015) \u2014, amounts to a surrogate form of randomized regularization over clean examples. We let Lap(z|b) .= (1/2b) exp(\u2212|z|/b) denote the pdf of the Laplace distribution. Algorithm DP-RADOS states the protection mechanism. Let us define two training samples Se and S\u2032e as being neighbours, noted Se \u2248 S\u2032e, iff they differ from one example. We show how the Laplace\n10\nmechanism of DP-RADOS can give \u03b5-DP, and furthermore the minimisation of a rado-loss over protected rados resembles the minimisation of an optimistic bound on a regularization of the equivalent example loss over clean examples. We make the assumption that any two edge vectors e,e\u2032 satisfy \u2016e\u2212e\u2032\u20161 \u2264 re, which is ensured e.g. if all examples belong to a \u21131-ball of diameter re.\nTheorem 20 DP-RADOS delivers \u03b5-differential privacy. Furthermore, pick (\u2126,\u2126\u22c6) any couple of dual norms and assume Sr = S\u2217r (|Sr| = 2m). Then \u2200\u03b8, \u2113expr (S\u2217,DPr ,\u03b8) \u2264 exp{m\u00b7\u2113loge (Se,\u03b8, (1/m) \u00b7maxj \u2126\u22c6(zj) \u00b7 \u2126)}. (Proof in Appendix, Subsection 9.15)"}, {"heading": "6 Experiments", "text": "We have implemented \u2126-WL using the order suggested to retrieve the topmost feature in the order. Hence, the weak learner returns the feature maximising |r\u03b9| \u2212 \u03b4\u03b9. The rationale for this comes from the proofs of Theorems 17 \u2014 19, showing that \u220f\nt exp(\u2212(r2\u03b9(t)/2\u2212\u03b4\u03b9(t))) is an upperbound on the exponential regularized rado-loss. We do not clamp the weak learner for \u2126(.) = \u2016.\u20162\u0393, so the weak learner is restricted to the framebox in \u2126-WL4. We have tested two types of random rados, the plain random rados (Nock et al., 2015), and class-wise rados, for which we first pick a class at random and then sample a subset of its examples to compute one rado (and repeat for n rados). The supplementary information (Appendix, Section 10) provides the complete experiments, whose summary is given here.\nExperiments I: (regularized) rados vs examples The objective of these experiments is to evaluate \u2126R.ADABOOST as a contender for supervised learning per se. We compared \u2126-R.ADABOOST to ADABOOST/\u21131-ADABOOST (Schapire & Singer, 1999; Xi et al., 2009). All algorithms are run for a total of\n4the values for \u03c9 that we test, in {10\u2212u, u \u2208 {0, 1, 2, 3, 4, 5}}, are very small with respect to the upperbound in ineq. (34) given the number of boosting iterations (T = 1000), and would yield on most domains a maximal \u03b3 \u2248 1.\n11\nT = 1000 iterations, and at the end of the iterations, the classifier in the sequence that minimizes the empirical loss is kept. Notice therefore that rado-based classifiers are evaluated on the training set which computes the rados (in a privacy setting, the learner send the sequence of classifiers to the data handler, which then selects the best according to its training sample). To obtain very sparse solutions for \u21131-ADABOOST, we pick its \u03c9 (\u03b2 in (Xi et al., 2009)) in {10\u22124, 1, 104}. The results we give, in Table 2, report only the lowest error of all of ADABOOST variants. The Appendix (Subsection 10.1) details the support results, that are summarized in Table 2. Experiments support several key observations. First, regularizing consistently reduces the test error of \u2126-R.ADABOOST, by more than 15% on Magic, and 20% on Kaggle. Second, \u2126-R.ADABOOST is able to obtain both very sparse and accurate classifiers (Magic, Hardware, Marketing, Kaggle). Third, with the sole exception of domain Banknote, \u2126-R.ADABOOST competes or beats ADABOOST on all domains, and is all the better as the domain gets bigger. Fourth, it is important to have several choices of regularizers at hand. Fifth, as already remarked (Nock et al., 2015), significantly subsampling rados (e.g. Marketing, Kaggle) still yields very accurate classifiers. Finally, regularization in \u2126-R.ADABOOST successfully reduces sparsity to learn more accurate classifiers on several domains (Transfusion, Banknote, Winered, Magic, Marketing), achieving efficient adaptive sparsity control.\nExperiments II: differential privacy We have tested DP-RADOS for a fixed number of rados of n = 100. Such a small number of rados has three advantages: (i) the privacy budget does not blow up, (ii) accurate classifiers can still be learned with a small number of rados (Nock et al., 2015), (iii) with such a small number of rados, we are within the reach of additional privacy guarantees (Nock et al., 2015). We have compared with ADABOOST, trained over a subset of n = 100 (protected) examples, randomly sampled out of the full training fold. To make sure that this does not impair the algorithm just because the sample is too small, we compute the test error for very large values of \u03b5 as a baseline. Last, for tight comparisons, we use the same set of random vectors z to protect the rados and the examples. This choice is justified and discussed at the end of the proof of Theorem 20 (Appendix, Subsection 9.15). Yet, as we shall see, the results are exceedingly in favor of \u2126-R.ADABOOST in this case. To give a more balanced picture, we chose to compute an \u201capproximate\u201d example-equivalent privacy budget \u03b5a = \u03b5a(\u03b5, n,m) for ADABOOST and n examples, which we fix to be\n\u03b5a . = n \u00b7 ln\n(\n1 + exp(\u03b5/n) \u2212 1\nm\n)\n. (42)\nWe always have \u03b5a < \u03b5. The \u201coptimal\u201d DP picture of ADABOOST shall thus be representable as a stretching of its curves in between the figures for \u03b5a and \u03b5. We insist on the fact that the noise for \u03b5 is conservative but always safe, while computing \u03b5 from \u03b5a would sometimes fail to provide \u03b5a-DP (Appendix, Subsection 9.15).\nTable 3 presents the results obtained for three big domains (m indicated in parenthesis), in which we have run unregularized algorithms for a fixed number of T = 1000 iterations, keeping the last classifier \u03b81000 for testing. GaussNLin is a d = 2 simulated domain, non linearly separable but for which the optimal linear classifier has error < 2%. The results are a clear advocacy in favor of using rados against examples for the straight DP protection: with plain random rados, test errors that compete with clean data can be observed for privacy budget \u03b5 \u2248 10\u22124, that is, more than a hundred times smaller than most reported studies (Hsu et al., 2014). In comparison, ADABOOST\u2019s results, even plotted against the weak protection budget \u03b5a, are very significantly worse. Finally, on UCI domains SuSy and Higgs, non-trivial protections (typically, \u03b5 \u2208 [0.01, 1]) allow to beat classification on clean data, as witnessed by a 6%+ test error reduction for Higgs. In addition to \u201ccoming for free\u201d (Wang et al., 2015) in machine learning, DP may thus also be a worthwhile companion to improve learning.\n12"}, {"heading": "7 Conclusion", "text": "We have shown that the equivalence between the log loss over examples and the exponential loss over rados, as shown in (Nock et al., 2015), can be generalized to other losses via a principled representation of a loss function in a two-player zero-sum game. Furthermore, we have shown that this equivalence extends to regularized losses, where the regularization in the rado loss is performed over the rados themselves with Minkowski sums. Because regularization with rados has such a simple form, it is relatively easy to derive efficient learning algorithms working with various forms of regularization, as exemplified with ridge, lasso, \u2113\u221e and SLOPE regularizations in a formal boosting algorithm that we introduce, \u2126-R.ADABOOST. Experiments confirm that this freedom in the choice of regularization is a clear strength of the algorithm, and that regularization dramatically improves the performances over non-regularized rado learning. \u2126R.ADABOOST efficiently controls sparsity, and may be a worthy contender for supervised learning at large outside the privacy framework. Experiments also display that SLOPE regularization tends to achieve top performances, and call for an extension to rados of the formal sparsity results already known (Su & Cande\u0300s, 2015)."}, {"heading": "8 Acknowledgments", "text": "Thanks are also due to Stephen Hardy and Giorgio Patrini for many stimulating discussions and feedback on the subject. NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Center of Excellence Program."}, {"heading": "Appendix \u2014 Table of contents", "text": "Proofs Pg 16 Proof of Theorem 2 Pg 16 Proof of Lemma 4 Pg 18 Proof of Lemma 6 Pg 19 Proof of Corollary 7 Pg 20 Proof of Lemma 8 Pg 21 Proof of Corollary 9 Pg 22 Proof of Lemma 10 Pg 24 Proof of Corollary 11 Pg 24 Proof of Lemma 12 Pg 25 Proof of Corollary 13 Pg 25 Proof of Theorem 14 Pg 25 Proof of Theorem 17 Pg 26 Proof of Theorem 18 Pg 28 Proof of Theorem 19 Pg 30 Proof of Theorem 20 Pg 31\nAdditional Experiments Pg 33 Supports for rados (complement to Table 2) Pg 33 Experiments on class-wise rados Pg 33 Test errors and supports for rados (comparison last vs best empirical classifier) Pg 33\n15"}, {"heading": "9 Proofs", "text": ""}, {"heading": "9.1 Proof of Theorem 2", "text": "We split the proof in two parts, the first concerning the case where both generators are differentiable since some of the derivations shall be used hereafter, and then the case where they are not. Remark that because of Lemma 4, we do not have to cover the case where just one of the two generators would be differentiable. Case 1: \u03d5e, \u03d5r are differentiable. We show in this case that being proportionate is equivalent to having:\np\u2217(z) = Gmq \u2217(z) . (43)\nSolving eqs. (3) and (4) bring respectively:\np\u2217i (z) = \u03d5 \u2032 e \u22121\n(\n\u2212 1 \u00b5e\n\u00b7 zi ) , (44)\nq\u2217I (z) = \u03d5 \u2032 r \u22121\n(\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi + \u03bb\n\u00b5r\n)\n, (45)\nwhere \u03bb is picked so that q\u2217(z) \u2208 H2m , that is,\n\u2211\nI\u2286[m]\n\u03d5\u2032r \u22121\n(\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi + \u03bb\n\u00b5r\n)\n= 1 . (46)\nWe obtain\nL\u2217e (z) = \u2212\u00b5e \u2211\ni\u2208[m]\n\u03d5\u22c6e\n(\n\u2212 1 \u00b5e\n\u00b7 zi ) , (47)\nL\u2217r (z) = \u03bb\u2212 \u00b5r \u2211\nI\u2286[m]\n\u03d5r \u22c6\n(\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi + \u03bb\n\u00b5r\n)\n, (48)\nwhere \u03d5\u22c6(z) . = supz\u2032{zz\u2032 \u2212 \u03d5(z\u2032)} denotes the convex conjugate of \u03d5. It follows from eq. (47) that:\n\u2202\n\u2202zi L\u2217e (z) = \u03d5 \u22c6 e \u2032\n(\n\u2212 1 \u00b5e\n\u00b7 zi )\n= \u03d5\u2032e \u22121\n(\n\u2212 1 \u00b5e\n\u00b7 zi )\n(49)\n= p\u2217i (z) , (50)\n16\nwhere eq. (49) follows from properties of \u03d5\u22c6. We also have\n\u2202\n\u2202zi L\u2217r (z)\n=\n\n1\u2212 \u2211\nI\u2286[m]\n\u03d5\u2032r \u22121\n\n\u2212 1 \u00b5r \u00b7 \u2211\nj\u2208I\nzj + \u03bb\n\u00b5r\n\n\n\n \u00b7 \u2202\u03bb \u2202zi\n+ \u2211\nI\u2286[m]:i\u2208I\n\u03d5\u2032r \u22121\n\n\u2212 1 \u00b5r \u00b7 \u2211\nj\u2208I\nzj + \u03bb\n\u00b5r\n\n\n= \u2202\u03bb\n\u2202zi\n+ \u2211\nI\u2286[m]\n(\n1i\u2208I \u2212 \u2202\u03bb\n\u2202zi\n)\n\u03d5\u2032r \u22121\n\n\u2212 1 \u00b5r \u00b7 \u2211\nj\u2208I\nzj + \u03bb\n\u00b5r\n\n\n= \u2202\u03bb\n\u2202zi +\n\u2211\nI\u2286[m]\n(\n1i\u2208I \u2212 \u2202\u03bb\n\u2202zi\n)\n\u00b7 q\u2217I (z)\n= \u2202\u03bb\n\u2202zi \u00b7\n\n1\u2212 \u2211\nI\u2286[m]\nq\u2217I (z)\n\n + \u2211\nI\u2286[m]\n1i\u2208I \u00b7 q\u2217I (z)\n= \u2211\nI\u2286[m]\n1i\u2208I \u00b7 q\u2217I (z) , (51)\nsince q\u2217(z) \u2208 H2m . Now suppose \u03d5e and \u03d5r proportionate. It comes that there exists (\u00b5e,\u00b5r) such that the gradients of eq. (7) yield \u2207L\u2217e (z) = \u2207L\u2217r (z), and from eqs. (50) and (51) we obtain p\u2217(z) = Gmq\u2217(z). Reciprocally, having p\u2217(z) = Gmq\u2217(z) for some \u03d5e, \u03d5r and \u00b5e,\u00b5r > 0 implies as well \u2207L\u2217e (z) = \u2207L\u2217r (z) from eqs. (50) and (51), and therefore eq. (7) holds as well. This ends the proof of Case 1 for Theorem 2.\nCase 2: \u03d5e, \u03d5r are not differentiable. To simplify the statement and proofs, we assume that \u00b5e = \u00b5r = 1. We define the following problems\nLe(z) . = inf p\u2208Rm z\u22a4p+ \u03d5e(p) , (52) Lr(z) . = inf\nq\u2208H2m z\u22a4Gmq + \u03d5r(q) , (53)\nwhere \u03d5e : Rm \u2192 R and \u03d5r : R2 m \u2192 R are convex. Recall that \u2202Le and \u2202Lr are their subdifferentials, and p(z) and q(z) the arguments of the infima, assuming without loss of generality that they are finite. We now show that being proportionate is equivalent to having, for any z,\np(z) \u2208 \u2202Lr(z) , (54) Gmq(z) \u2208 \u2202Le(z) . (55)\nThis property is an immediate consequence of the following property, which we shall in fact show:\np(z) \u2208 \u2202Le(z) , (56) Gmq(z) \u2208 \u2202Lr(z) . (57)\n17\nGranted all (54\u201457) hold, Eq. (43) of Theorem 2 follows whenever subgradients are singletons. To see why the statement of the Theorem follows from (54\u201355), if the functions are proportionate, then their subdifferentials match from Definition 1 and we immediately get (54) and (55) from (56) and (57). If, on the other hand, we have both (54) and (55), then we get from (56) and (57) that \u2202Le(z) \u2229 \u2202Lr(z) 6= \u2205,\u2200z and so 0 \u2208 \u2202(Le(z) \u2212 Lr(z)), yieding the fact that the epigraphs of Le(z) and Lr(z) match by a translation of some b that does not depend on z, and by extension, the fact that \u03d5e and \u03d5r meet Definition 1 and are proportionate.\nTo show (56), we first remark that \u2212z\u2032 \u2208 \u2202\u03d5e(p(z\u2032)) for any z\u2032 because of the definition of p in (52). So, from the definition of subdifferentials, for any z,\n\u03d5e(p(z \u2032)) + (\u2212z\u2032)\u22a4(p(z)\u2212 p(z\u2032)) \u2264 \u03d5e(p(z)) .\nReorganising and substracting z\u22a4p(z) to both sides, we get\n\u2212\u03d5e(p(z\u2032))\u2212 z\u2032\u22a4p(z\u2032) \u2265 \u2212\u03d5e(p(z))\u2212 z\u22a4p(z) + (\u2212p(z))\u22a4(z\u2032 \u2212 z) ,\nwhich shows that \u2212p(z) \u2208 \u2202 \u2212 (\u03d5e(p(z)) + z\u22a4p(z)), and so p(z) \u2208 \u2202Le(z). We then tackle (57). We show that there exists \u03bb \u2208 R such that \u03bb \u00b7 12m \u2212 G\u22a4mz \u2208 \u2202\u03d5r(q(z)) at the optimal q(z). Suppose it is not the case. Then because of the definition of subgradients, for any \u03bb \u2208 R, there exists q \u2208 H2m, q 6= q(z) such that\n\u03d5r(q(z)) + (\u03bb \u00b7 12m \u2212 G\u22a4mz)\u22a4(q \u2212 q(z)) > \u03d5r(q) .\nReorganising and using the fact that q, q\u2217 \u2208 H2 m , we get \u03d5r(q(z)) + z\u22a4Gmq(z) > \u03d5r(q) + z\u22a4Gmq, contradicting the optimality of q(z). Consider any z\u2032 and its corresponding optimal q(z\u2032). Since \u03bb\u2032 \u00b7 12m \u2212 G\u22a4mz \u2208 \u2202\u03d5r(q(z)) for some \u03bb\u2032 \u2208 R, we get from the definition of subgradients that\n\u03d5r(q(z))\n\u2265 \u03d5r(q(z\u2032)) + (\u03bb\u2032 \u00b7 12m \u2212 G\u22a4mz\u2032)\u22a4(q(z) \u2212 q(z\u2032)) .\nReorganising and using the fact that q(z), q(z\u2032) \u2208 H2m , we get\n\u2212(\u03d5r(q(z\u2032)) + z\u2032\u22a4Gmq(z\u2032)) \u2265 \u2212(\u03d5r(q(z)) + z\u22a4Gmq(z))\n+(\u2212Gmq(z))\u22a4(z\u2032 \u2212 z) , (58)\nshowing that \u2212Gmq(z) \u2208 \u2202 \u2212 (\u03d5r(q(z)) + z\u22a4Gmq(z)), and so Gmq(z) \u2208 \u2202Lr(z)."}, {"heading": "9.2 Proof of Lemma 4", "text": "Take m = 1, and replace z by real z1. We have Le(p, z1) = pz1+\u03d5e(z1) and Lr(q, z) = q{1}z1+\u03d5r(q{1})+ \u03d5r(q\u2205). Remark that we can drop the constraint q \u2208 H2 since then q\u2205 = 1\u2212 q{1}. So we get\nL\u2217r (q) = min q\u2208R qz1 + \u00b5r\u03d5r(q) + \u00b5r\u03d5r(1\u2212 q)\n= min q\u2208R qz1 + \u00b5r\u03d5s(r)(q) = \u2212\u00b5r\u03d5\u22c6s(r) (\n\u2212 1 \u00b5r\n\u00b7 z1 ) ,\n18\nwhereas\nL\u2217e (p) = \u2212\u00b5e\u03d5\u22c6r (\n\u2212 1 \u00b5e\n\u00b7 z1 ) ,\nand since \u03d5e and \u03d5r are proportionate, then\n\u03d5\u22c6r\n(\n\u2212 1 \u00b5e\n\u00b7 z1 ) = \u00b5r\n\u00b5e \u00b7 \u03d5\u22c6s(r)\n(\n\u2212 1 \u00b5r\n\u00b7 z1 )\n\u2212 b \u00b5e . (59)\nWe then make the variable change z . = \u2212z1/\u00b5e and get\n\u03d5\u22c6e (z) = \u00b5r\n\u00b5e \u00b7 \u03d5\u22c6s(r)\n( \u00b5e\n\u00b5r \u00b7 z\n)\n\u2212 b \u00b5e , (60)\nwhich yields, since \u03d5e, \u03d5r, and by extension \u03d5s(r), are all convex and lower-semicontinuous,\n\u03d5e(z) = \u00b5r\n\u00b5e \u00b7 \u03d5s(r)(z) +\nb\n\u00b5e , (61)\nas claimed."}, {"heading": "9.3 Proof of Lemma 6", "text": "We use the fact that whenever \u03d5 is differentiable, \u03d5\u22c6(z) . = z \u00b7\u03d5\u2032\u22121(z)\u2212\u03d5(\u03d5\u2032\u22121(z)). We have \u03d5\u2032r(z) = log z, \u03d5\u2032r \u22121(z) = exp z = \u03d5\u22c6r (z). Therefore, the Lagrange multiplier \u03bb in (46) is\n\u03bb = \u2212\u00b5r \u00b7 log\n\n \u2211\nI\u2286[m]\nexp\n(\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n)\n , (62)\nwhich yields from (54):\nq\u2217I (z) = exp\n(\n\u2212 1 \u00b5r \u00b7\u2211i\u2208I zi )\n\u2211 J\u2286[m] exp ( \u2212 1 \u00b5r \u00b7\u2211j\u2208J zj ) ,\u2200I \u2286 [m] .\nOn the other hand, we also have \u03d5\u2032s(z) = log(z/(1 \u2212 z)), \u03d5\u2032s\u22121(z) = exp(z)/(1 + exp(z)) and \u03d5\u22c6s (z) = 1 + log(1 + exp(z)), which yields from (94):\np\u2217i (z) = exp\n(\n\u2212 1 \u00b5e\n\u00b7 zi )\n1 + exp (\n\u2212 1 \u00b5e\n\u00b7 zi ) ,\u2200i \u2208 [m] . (63)\n19\nWe then check that for any i \u2208 [m], we indeed have \u2211\nI\u2286[m]\n1i\u2208I \u00b7 q\u2217I (z)\n= \u2211\nI\u2286[m]\n1i\u2208I \u00b7 exp\n(\n\u2212 1 \u00b5r \u00b7\u2211i\u2032\u2208I zi\u2032 )\n\u2211 J\u2286[m] exp ( \u2212 1 \u00b5r \u00b7\u2211j\u2208J zj )\n= exp\n(\n\u2212 1 \u00b5e\n\u00b7 zi ) \u00b7 \u2211\nJ\u2286[m]\\{i} exp ( \u2212 1 \u00b5r \u00b7\u2211j\u2208I zj )\n\u2211 J\u2286[m] exp ( \u2212 1 \u00b5r \u00b7\u2211j\u2208J zj )\n= exp\n(\n\u2212 1 \u00b5e\n\u00b7 zi )\n\u00b7 c( 1 + exp (\n\u2212 1 \u00b5e\n\u00b7 zi )) \u00b7 c\n= exp\n(\n\u2212 1 \u00b5r\n\u00b7 zi )\n1 + exp (\n\u2212 1 \u00b5r\n\u00b7 zi ) , (64)\nwith c . =\n\u2211 J\u2286[m]\\{i} exp ( \u2212 1 \u00b5r \u00b7\u2211j\u2208I zj ) . We check that eq. (64) equals eq. (63) whenever \u00b5e = \u00b5r.\nHence eq. (43) holds. We conclude that \u03d5r and \u03d5e = \u03d5s are proportionate whenever \u00b5e = \u00b5r."}, {"heading": "9.4 Proof of Corollary 7", "text": "Consider \u03d5r(z) . = z log z \u2212 z and \u03d5e = \u03d5s. We obtain from eq. (47):\n\u2212L\u2217e (z)\n= fe\n  \u2211\ni\u2208[m]\nlog\n(\n1 + exp\n(\n\u2212 1 \u00b5e\n\u00b7 zi ))\n\n ,\nwith fe(z) = \u00b5e \u00b7z+\u00b5em. We have also \u03d5\u22c6r (z) = exp(z), and so using \u03bb in eq. (62) and eq. (48), we obtain\n\u2212L\u2217r (z)\n= \u00b5r \u00b7 log\n\n \u2211\nI\u2286[m]\nexp\n(\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n)\n\n+\u00b5r \u00b7 exp ( \u03bb\n\u00b5r\n)\n\u00b7 \u2211\nI\u2286[m]\nexp\n(\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n)\n= \u00b5r \u00b7 log\n  \u2211\nI\u2286[m]\nexp\n(\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n)\n\n+\u00b5r \u00b7 \u2211\nI\u2286[m] exp ( \u2212 1 \u00b5r \u00b7\u2211i\u2208I zi )\n\u2211 I\u2286[m] exp ( \u2212 1 \u00b5r \u00b7\u2211i\u2208I zi ) \ufe38 \ufe37\ufe37 \ufe38\n=1\n= fr\n  \u2211\nI\u2286[m]\nexp\n(\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n)\n ,\n20\nwith fr(z) = \u00b5r \u00b7 log z+\u00b5r. We get from Lemma 6 that the following example and rado risks are equivalent whenever \u00b5e = \u00b5r:\n\u2113e(z,\u00b5e) = \u2211\ni\u2208[m]\nlog\n(\n1 + exp\n(\n\u2212 1 \u00b5e\n\u00b7 zi )) , (65)\n\u2113r(z,\u00b5r) = \u2211\nI\u2286[m]\nexp\n(\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n)\n, (66)\nfrom which we get the statement of the Corollary by fixing \u00b5 = \u00b5e = \u00b5r."}, {"heading": "9.5 Proof of Lemma 8", "text": "We proceed as in the proof of Lemma 6. We have \u03d5\u2032r(z) = z, \u03d5 \u2032 r \u22121(z) = z and \u03d5\u22c6r (z) = \u03d5r(z). Therefore, the Lagrange multiplier \u03bb in (46) is\n\u03bb = \u00b5r\n2m +\n1 2m \u00b7 \u2211\nI\u2286[m]\n\u2211\ni\u2208I\nzi (67)\n= \u00b5r\n2m +\n1 2 \u00b7 \u2211\ni\u2208[m]\nzi , (68)\nsince any i belongs exactly to half of the subsets of [m]. We obtain:\nq\u2217I (z) = 1 2m \u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi + 1 2\u00b5r \u00b7 \u2211\ni\u2208[m]\nzi ,\u2200I \u2286 [m] .\nOn the other hand, we also have \u03d5\u2032s(z) = 2z\u22121, \u03d5\u2032s\u22121(z) = (1+z)/2 and \u03d5\u22c6s (z) = \u2212(1/4)+(1/4)\u00b7(1+z)2 , which yields from (94):\np\u2217i (z) = 1 2 \u00b7 ( 1\u2212 1 \u00b5e \u00b7 zi ) ,\u2200i \u2208 [m] . (69)\n21\nWe then check that for any i \u2208 [m], we have \u2211\nI\u2286[m]\n1i\u2208I \u00b7 q\u2217I (z)\n= \u2211\nI\u2286[m]\n1i\u2208I \u00b7\n\n 1 2m \u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi + 1 2\u00b5r \u00b7 \u2211\ni\u2208[m]\nzi\n\n\n= 1 2 \u2212 1 \u00b5r \u00b7 \u2211\nI\u2286[m]\n1i\u2208I \u00b7 \u2211\ni\u2208I\nzi + 2m\u22122 \u00b5r \u00b7 \u2211\ni\u2208[m]\nzi\n= 1 2 \u2212 2\nm\u22121\n\u00b5r \u00b7 zi \u2212\n1 \u00b5r \u00b7\n\u2211\nI\u2286[m]\\{i}\n\u2211\ni\u2208I\nzi\n+ 2m\u22122 \u00b5r \u00b7 \u2211\ni\u2208[m]\nzi\n= 1 2 \u2212 2\nm\u22121\n\u00b5r \u00b7 zi \u2212\n2m\u22122\n\u00b5r \u00b7\n\u2211\ni\u2208[m]\\{i}\nzi\n+ 2m\u22122 \u00b5r \u00b7 \u2211\ni\u2208[m]\nzi\n= 1 2 \u2212 2\nm\u22121\n\u00b5r \u00b7 zi +\n2m\u22122\n\u00b5r \u00b7 zi\n= 1\n2\n(\n1\u2212 2 m\u22121\n\u00b5r \u00b7 zi\n)\n. (70)\nWe check that eq. (70) equals eq. (69) whenever \u00b5e = \u00b5r/2m\u22121. Hence eq. (43) holds. We conclude that \u03d5r is proportionate to \u03d5e = \u03d5s whenever \u00b5e = \u00b5r/2m\u22121."}, {"heading": "9.6 Proof of Corollary 9", "text": "Consider \u03d5r(z) . = (1/2) \u00b7 z2 and \u03d5e = \u03d5s. We obtain from eq. (47):\n\u2212L\u2217e (z)\n= fe\n\n \u2211\ni\u2208[m]\n(\n1\u2212 1 \u00b5e\n\u00b7 zi )2\n\n ,\n22\nwith fe(z) = (\u00b5e/4) \u00b7 z + (\u00b5em/4). We have also \u03d5\u22c6r (z) = (1/2) \u00b7 z2, and so using eq. (48) and \u03bb in eq. (67), we obtain\n\u2212L\u2217r (z)\n= \u2212 \u00b5r 2m \u2212 1 2m \u00b7 \u2211\nI\u2286[m]\n\u2211\ni\u2208I\nzi\n+ 1\n2\u00b5r\n\u2211\nI\u2286[m]\n  \u2211\ni\u2208I\nzi \u2212 \u00b5r 2m \u2212 1 2m \u00b7 \u2211\nI\u2286[m]\n\u2211\ni\u2208I\nzi\n\n\n2\n= \u2212 \u00b5r 2m \u2212 1 2m \u00b7 \u2211\nI\u2286[m]\n\u2211\ni\u2208I\nzi + \u00b5r\n2m+1\n\u2212 1 2m \u00b7 \u2211\nI\u2286[m]\n  \u2211\ni\u2208I\nzi \u2212 1 2m \u00b7 \u2211\nI\u2286[m]\n\u2211\ni\u2208I\nzi\n\n\n\ufe38 \ufe37\ufe37 \ufe38\n=0\n+ 1\n2\u00b5r\n\u2211\nI\u2286[m]\n  \u2211\ni\u2208I\nzi \u2212 1 2m \u00b7 \u2211\nI\u2286[m]\n\u2211\ni\u2208I\nzi\n\n\n2\n= \u2212 \u00b5r 2m+1 \u2212 1 2m \u00b7 \u2211\nI\u2286[m]\n\u2211\ni\u2208I\nzi\n+ 2m\u22121 \u00b5r \u00b7 1 2m \u00b7 \u2211\nI\u2286[m]\n  \u2211\ni\u2208I\nzi \u2212 1 2m \u00b7 \u2211\nI\u2286[m]\n\u2211\ni\u2208I\nzi\n\n\n2\n= \u2212 \u00b5r 2m+1\n\u2212EI\u223c[m] [ \u2211\ni\u2208I\nzi\n]\n+ 2m\u22121\n\u00b5r \u00b7 VI\u223c[m]\n[ \u2211\ni\u2208I\nzi\n]\n= \u2212 \u00b5r 2m+1\n+ \u00b5r\n2m\u22121 \u00b7\n\n\u2212\n\n EI\u223c[m]\n[ 2m\u22121\n\u00b5r \u00b7\u2211i\u2208I zi\n]\n\u2212 \u00b5r2m\u22121 \u00b7 VI\u223c[m] [ 2m\u22121 \u00b5r \u2211 i\u2208I zi ]\n\n\n\n\n= fr\n\n\u2212\n\n EI\u223c[m]\n[ 2m\u22121\n\u00b5r \u00b7\u2211i\u2208I zi\n]\n\u2212 \u00b5r 2m\u22121 \u00b7 VI\u223c[m] [ 2m\u22121 \u00b5r \u2211 i\u2208I zi ]\n\n\n\n , (71)\n23\nwith fr(z) = (\u00b5r/2m\u22121) \u00b7 z \u2212 (\u00b5r/2m+1). Therefore, it comes from Lemma 8 that the following example and rado risks are equivalent whenever \u00b5e = \u00b5r/2m\u22121:\n\u2113e(z,\u00b5e) = \u2211\ni\u2208[m]\n(\n1\u2212 1 \u00b5e\n\u00b7 zi )2 ,\n\u2113r(z,\u00b5r) = \u2212 ( EI [ 2m\u22121\n\u00b5r \u00b7 \u2211\ni\u2208I\nzi\n]\n\u2212 \u00b5r 2m\u22121\n\u00b7 VI [ 2m\u22121\n\u00b5r \u00b7 \u2211\ni\u2208I\nzi\n])\n.\nThere remains to fix \u00b5 . = \u00b5e = \u00b5r/2 m\u22121 to obtain the statement of the Corollary."}, {"heading": "9.7 Proof of Lemma 10", "text": "Define \u25b3d as the d-dimensional probability simplex. Then it comes with that choice of \u03d5r(qI): min\nq\u2208H2m Lr(q,z)\n= min q\u2208\u25b32m\n\u2211\nI\u2286[m]\nqI \u2211\ni\u2208I\nzi\n=\n{ 0 if \u2211\ni\u2208I zi > 0,\u2200I 6= \u2205 ,\u2211 i:zi<0 zi otherwise , (72)\nsince whenever no zi is negative, the minimum is achieved by putting all the mass (1) on q\u2205, and when some are negative, the minimum is achieved by putting all the mass on the smallest over all I of \u2211\ni\u2208I zi, which is the one which collects all the indexes of the negative coordinates in z.\nOn the other hand, remark that fixing \u03d5e . = \u03d5s still yields \u03d5e(z) = \u03c7[0,1](z) = \u03d5r(z), yet this time we\nhave the following on Le(p,z):\nmin p\u2208Rm Lr(q,z) = min p\u2208[0,1]m\n\u2211\ni\u2208[m]\npizi\n= \u2212\u00b5e \u00b7 \u2211\ni\u2208[m]\nmax\n{\n0,\u2212 1 \u00b5e\n\u00b7 zi } , (73)\nsince the optimal choice for p\u2217i is to put 1 only when zi is negative. We obtain p \u2217(z) = Gmq \u2217(z) for any choice of \u00b5e,\u00b5r, and so \u03d5r(z) is self-proportionate for any \u00b5e,\u00b5r. This ends the proof of Lemma 10."}, {"heading": "9.8 Proof of Corollary 11", "text": "We obtain from Lemma 10 that \u2212L\u2217r (z) = fr(\u2113r(z,\u00b5r)) with fr(z) = \u00b5r \u00b7 z and:\n\u2113r(z,\u00b5r) = max\n{\n0, max I\u2286[m]\n{\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n}}\n. (74)\nOn the other hand, it comes from eq. (73) that \u2212L\u2217e (z) = fr(\u2113e(z,\u00b5e)) with fe(z) = \u00b5e \u00b7 z and:\n\u2113e(z,\u00b5e) = \u2211\ni\u2208[m]\nmax\n{\n0,\u2212 1 \u00b5e\n\u00b7 zi } . (75)\nThis concludes the proof of Corollary 11.\n24"}, {"heading": "9.9 Proof of Lemma 12", "text": "The choice of\n\u03d5r(z) = \u03c7[ 12m , 1 2 ] (z) , (76)\nunder the constraint that q \u2208 H2m , enforces q\u2217 I = 1/2m,\u2200I \u2286 [m]. Furthermore, fixing \u03d5e .= \u03d5s indeed yields\n\u03d5e = \u03c7[ 12m , 1 2 ] (z) + \u03c7[ 12m , 1 2 ] (1\u2212 z)\n= \u03c7{ 12}(z) , (77)\nwhich enforces p\u2217i = 1/2, \u2200i. Since each i belongs to exactly 2m\u22121 subsets of [m], we obtain p\u2217(z) = Gmq \u2217(z), for any \u00b5e,\u00b5r, and so \u03d5r is proportionate to \u03d5e = \u03d5s for any \u00b5e,\u00b5r."}, {"heading": "9.10 Proof of Corollary 13", "text": "We obtain from Lemma 12 that \u2212L\u2217r (z) = fr(\u2113r(z,\u00b5r)) with fr(z) = z and:\n\u2113r(z,\u00b5r) = EI\n[\n\u2212 1 \u00b5r \u00b7 \u2211\ni\u2208I\nzi\n]\n.\nOn the other hand, it comes from eq. (73) that \u2212L\u2217e (z) = fr(\u2113e(z,\u00b5e)) with fe(z) = (1/2) \u00b7 z and:\n\u2113e(z,\u00b5e) = \u2211\ni\n\u2212 1 \u00b5e \u00b7 zi .\nThis concludes the proof of Corollary 11."}, {"heading": "9.11 Proof of Theorem 14", "text": "The key to the poof is the constraint q \u2208 Hm in eq. (4). Since fe(z) = ae \u00b7 z + be, we have L\u2217e (z) = ae \u00b7 (\u2113e(z) +\u03c9) + be \u2212 ae \u00b7\u03c9 for any \u03c9 \u2208 R. It follows from eq. (7) that ae \u00b7 (\u2113e(z) +\u03c9) + be \u2212 ae \u00b7\u03c9 = L\u2217r (z) + b = \u2211 I\u2286[m] q \u2217 I \u2211 i\u2208I zi + \u00b5r \u2211 I\u2286[m] \u03d5r(q \u2217 I ) + b, and so\nae \u00b7 (\u2113e(z) +\u03c9) + be\n= \u2212\n   min q\u2208Hm   \u2211\nI\u2286[m]\nqI \u2211\ni\u2208I\nzi + \u00b5r \u2211\nI\u2286[m]\n\u03d5r(qI)\n\n \u2212 ae\u03c9\n \n\n+b\n= \u2212 min q\u2208Hm\n\n \u2211\nI\u2286[m]\nqI\n( \u2211\ni\u2208I\nzi \u2212 ae\u03c9 ) + \u00b5r \u2211\nI\u2286[m]\n\u03d5r(qI)\n\n\n+b ,\nsince q \u2208 Hm and ae,\u03c9, a are not a function of q. We thus get ae \u00b7 (\u2113e(z) +\u03c9) + be = ar \u00b7 fr ( \u2113\u0303r(z) ) + br, where \u2113\u0303r(z) equals \u2113r(z) in which each \u2211 i\u2208I zi is replaced by \u2211\ni\u2208I zi \u2212 ae\u03c9. For zi = \u03b8\u22a4(yi \u00b7 xi) and \u03c9 = \u2126(\u03b8), we obtain that whenever \u03b8 6= 0, \u2200I \u2286 [m],\n\u2211\ni\u2208I\nzi + ae\u03c9 = \u03b8 \u22a4\n(\n\u03c0\u03c3 \u2212 ae\u2126(\u03b8)\n\u2016\u03b8\u201622 \u00b7 \u03b8\n)\n, (78)\nfor \u03c3i = yi iff i \u2208 I (and \u2212yi otherwise), and the statement of the Theorem follows.\n25"}, {"heading": "9.12 Proof of Theorem 17", "text": "The proof of the Theorem contains two parts, the first of which follows ADABOOST\u2019s exponential convergence rate proof, and the second departs from this proof to cover \u2126-R.ADABOOST.\nWe use the fact that \u03b1\u03b9(t)\u03c0j\u03b9(t) = \u03b1\u03b9(t) \u00b7 1\u22a4\u03b9(t)\u03c0j = (\u03b8T \u2212 \u03b8T\u22121)\u22a4\u03c0j to unravel the weights as:\nwTj\n= w(T\u22121)j\nZT \u00b7 exp\n( \u2212\u03b1\u03b9(T )\u03c0j\u03b9(T ) + \u03b4T )\n= w(T\u22121)j\nZT \u00b7 exp ( \u2212(\u03b8T \u2212 \u03b8T\u22121)\u22a4\u03c0j +\u03c9 \u00b7 (\u2016\u03b8T \u201622 \u2212 \u2016\u03b8T\u22121\u201622) )\n= w(T\u22121)j\nZT \u00b7 exp ( \u2212\u03b8\u22a4T (\u03c0j \u2212\u03c9 \u00b7 \u03b8T ) +\u03b8\u22a4T\u22121 (\u03c0j \u2212\u03c9 \u00b7 \u03b8T\u22121) )\n= w0\n\u220fT t=1 Zt\n\u00b7 exp (\n\u2212\u03b8\u22a4T (\u03c0j \u2212\u03c9 \u00b7 \u03b8T ) +\u03b8\u22a40 (\u03c0j \u2212\u03c9 \u00b7 \u03b80)\n)\n(79)\n= w0\n\u220fT t=1 Zt\n\u00b7 exp ( \u2212\u03b8\u22a4T (\u03c0j \u2212\u03c9 \u00b7 \u03b8T ) ) , (80)\nsince the sums telescope in eq. (79) when we unravel the weight update and \u03b80 = 0. We therefore get\n\u2113expr (Sr,\u03b8, \u2016.\u201622) = T\u220f\nt=1\nZt , (81)\nas in the classical ADABOOST analysis (Schapire & Singer, 1999). This time however, we have, letting \u03c0\u0303j\u03b9(t) . = \u03c0j\u03b9(t)/\u03c0\u2217\u03b9(t) \u2208 [\u22121, 1] and \u03b1\u0303\u03b9(t) . = \u03c0\u2217\u03b9(t) \u00b7 \u03b1t for short,\nZt+1\n= \u2211\nj\u2208[n]\nwtj \u00b7 exp ( \u2212\u03b1\u03b9(t)\u03c0j\u03b9(t) + \u03b4t )\n= exp(\u03b4t) \u00b7 \u2211\nj\u2208[n]\nwtj \u00b7 exp ( \u2212\u03b1\u03b9(t)\u03c0j\u03b9(t) )\n= exp(\u03b4t) \u00b7 \u2211\nj\u2208[n]\nwtj \u00b7 exp ( \u2212\u03b1\u0303\u03b9(t)\u03c0\u0303j\u03b9(t) )\n\u2264 exp(\u03b4t) 2\n\u00b7 \u2211\nj\u2208[n]\nwtj \u00b7 ( (1 + \u03c0\u0303j\u03b9(t)) \u00b7 exp ( \u2212\u03b1\u0303\u03b9(t) )\n+(1\u2212 \u03c0\u0303j\u03b9(t)) \u00b7 exp ( \u03b1\u0303\u03b9(t) )\n)\n(82)\n= exp(\u03b4t) \u00b7 \u221a 1\u2212 r2t (83)\n= exp\n(\n\u03c9 \u00b7 (\u2016\u03b8t\u201622 \u2212 \u2016\u03b8t\u22121\u201622)\u2212 1\n2 ln\n1\n1\u2212 r2t\n)\n.\nThis is where our proof follows a different path from ADABOOST\u2019s: in eq. (83), we do not upperbound the \u221a\n1\u2212 r2t term, so it can absorb more easily the new exp(\u03b4t) factor which appears because of regularization. Ineq. (82) holds because of the convexity of exp, and eq. (83) is an equality when rt < \u03b3. If rt > \u03b3 is\n26\nclamped to rt \u2190 \u03b3 by the weak learner in (31), then we have instead the derivation \u2211\nj\u2208[n]\nwtj \u00b7 ( (1 + \u03c0\u0303j\u03b9(t)) \u00b7 exp ( \u2212\u03b1\u0303\u03b9(t) )\n+(1\u2212 \u03c0\u0303j\u03b9(t)) \u00b7 exp ( \u03b1\u0303\u03b9(t) )\n)\n= (1 + rt) \u00b7 \u221a 1\u2212 \u03b3 1 + \u03b3 + (1\u2212 rt) \u00b7 \u221a 1 + \u03b3\n1\u2212 \u03b3 \u2264 2 \u221a 1\u2212 \u03b32 , (84)\nsince function in (84) is decreasing on rt > 0. If rt < \u2212\u03b3 is clamped to rt \u2190 \u2212\u03b3, we get the same conclusion as in ineq (84) because this time \u03b1\u0303\u03b9(t) = (1/2) \u00b7 ln((1 \u2212 \u03b3)/(1 + \u03b3)). Summarising, whether rt has been clamped or not by the weak learner in (31), we get\nZt+1\n\u2264 exp ( \u03c9 \u00b7 (\u2016\u03b8t\u201622 \u2212 \u2016\u03b8t\u22121\u201622)\u2212 1\n2 ln\n1\n1\u2212 r2t\n)\n, (85)\nwith the additional fact that |rt| \u2264 \u03b3. For any feature index k \u2208 [d], let Fk \u2286 [T ] the iteration indexes for which \u03b9(t) = k. Letting \u03bb\u0393 (> 0) the largest eigenvalue of \u0393 , we obtain:\nT\u220f\nt=1\nZt\n\u2264 exp ( \u03c9 \u00b7 \u2016\u03b8T \u20162\u0393 \u2212 \u2211\nt\n1 2 log 1\n1\u2212 r2t\n)\n\u2264 exp ( \u03c9\u03bb\u0393 \u00b7 \u2016\u03b8T \u201622 \u2212 \u2211\nt\n1 2 log 1\n1\u2212 r2t\n)\n= exp\n \u22121 2 \u00b7 \u2211\nk\u2208[d]\n\u039bk\n\n , (86)\nWith\n\u039bk . = log 1 \u220f\nt:\u03b9(t)\u2208Fk (1\u2212 r2t )\n\u2212\u03c9\u03bb\u0393 2\u03c02\u2217k log2 \u220f\nt:\u03b9(t)\u2208Fk\n( 1 + rt 1\u2212 rt ) . (87)\nSince ( \u2211u l=1 al) 2 \u2264 u\u2211ul=1 a2l and mink maxj |\u03c0jk| \u2264 |\u03c0\u2217k|, \u039bk satisfies:\n\u039bk \u2265 \u2211\nt:\u03b9(t)\u2208Fk\n{\nlog 1\n1\u2212 r2t\n\u2212Tk\u03c9\u03bb\u0393 2M2 log2 1 + rt 1\u2212 rt\n}\n, (88)\nwith Tk . = |Fk| and M .= mink maxj |\u03c0jk|. For any a > 0, let\nfa(z) . =\n1 az2 \u00b7 ( log 1 1\u2212 z2 \u2212 a \u00b7 log 2 1 + z 1\u2212 z ) \u2212 1 .\n27\nIt satisfies\nfa(z) \u22480 ( 1\na \u2212 5\n)\n+\n( 1\n2a \u2212 8 3\n)\n\u00b7 z2\n+\n( 1\n3a \u2212 92 45\n)\n\u00b7 z4 + o(z4) . (89)\nSince fa(z) is continuous for any a 6= 0, \u22000 < a < 1/5, \u2203z\u2217(a) > 0 such that fa(z) \u2265 0,\u2200z \u2208 [0, z\u2217]. So, for any such a < 1/5 and any \u03c9 satisfying \u03c9 < (2aM2)/(Tk\u03bb\u0393), as long as each rt \u2264 z\u2217(a), we shall obtain\n\u039bk \u2265 a \u2211\nt:\u03b9(t)\u2208Fk\nr2t . (90)\nThere remains to tune \u03b3 \u2264 z\u2217(a), and remark that if we fix a = 1/7, then numerical calculations reveal that z\u2217(a) > 0.98, and if a = 1/10 then numerical calculations give z\u2217(a) > 0.999, completing the statement of Theorem 17."}, {"heading": "9.13 Proof of Theorem 18", "text": "We consider the case \u2126(.) = \u2016.\u2016\u221e, from which we shall derive the case \u2126(.) = \u2016.\u20161. We proceed as in the proof of Theorem 17, with the main change that we have now \u03b4t = \u03c9 \u00b7 (\u2016\u03b8t\u2016\u221e \u2212 \u2016\u03b8t\u22121\u2016\u221e), so in place of \u039bk in ineq . (86) we have to use, letting k\u2217 any feature that gives the \u2113\u221e norm,\n\u039bk . =\n \n\n\u2211\nt:\u03b9(t)\u2208Fk log 1\n1\u2212r2 t\n\u2212 \u03c9 \u03c0\u2217k\n\u2223 \u2223 \u2223 \u2211\nt:\u03b9(t)\u2208Fk log 1+rt1\u2212rt\n\u2223 \u2223 \u2223 if k = k\u2217 \u2211\nt:\u03b9(t)\u2208Fk log 1\n1\u2212r2 t\notherwise\n. (91)\nIt also comes\n\u039bk\u2217\n\u2265 \u2211\nt:\u03b9(t)\u2208Fk\u2217\n{\nlog 1 1\u2212 r2t \u2212 \u03c9 \u03c0\u2217k\u2217 log 1 + |rt| 1\u2212 |rt|\n}\n\u2265 \u2211\nt:\u03b9(t)\u2208Fk\u2217\n{\nlog 1 1\u2212 r2t \u2212 \u03c9 M log 1 + |rt| 1\u2212 |rt|\n}\n, (92)\nwith M . = mink maxj |\u03c0jk|. Let us analyze \u039bk\u2217 and define for any b > 0\ngb(z) . = log\n1 1\u2212 z2 \u2212 b \u00b7 log 1 + z 1\u2212 z\n\u2212 ( \u22122bz + z2 \u2212 2bz 3\n3\n)\n. (93)\nInspecting gb shows that gb(0) = 0, g\u2032b(0) = 0 and gb(z) is convex over [0, 1) for any b \u2264 3, which shows that gb(z) \u2265 0, \u2200z \u2208 [0, 1), \u2200b \u2264 3, and so, after dividing by bz2 and reorganising, yields in these cases:\n1 bz2 \u00b7 ( log 1 1\u2212 z2 \u2212 b \u00b7 log 1 + z 1\u2212 z ) \u2212 1\n\u2265 (\n\u22122 z +\n( 1\nb \u2212 1\n)\n\u2212 2z 3\n)\n. (94)\n28\nHence, both functions being continuous on (0, 1), the function in the left-hand side zeroes before the one in the right-hand side (when this one does on (0, 1)). The zeroes of the polynomial\npb(z) . = \u22122z\n2\n3 +\n( 1\nb \u2212 1\n)\nz \u2212 2 (95)\nexist iff b \u2264 \u221a 3/(4 + \u221a 3), in which case any z \u2208 [0, 1) must satisfy\nz \u2265 3 4 \u00b7\n\n 1\nb \u2212 1\u2212\n\u221a ( 1\nb \u2212 1\n)2\n\u2212 16 3\n\n (96)\nto guarantee that pb(z) \u2265 0. Whenever this happens, we shall have from (94):\nlog 1 1\u2212 z2 \u2212 b \u00b7 log 1 + z 1\u2212 z \u2265 bz 2 . (97)\nSince \u2126-WL is a \u03b3WL-weak learner, if we can guarantee that the right-hand side of ineq. (96) is no more than \u03b3WL, then there is nothing more to require from the weak learner to have ineq. (97) \u2014 and therefore to have \u039bk\u2217 \u2265 b\u03b32WL \u00b7 |Fk\u2217 |. This yields equivalently the following constraint on b:\nb \u2264 8\u03b3WL 3\n16\u03b32WL 9 + 8\u03b3WL 3 + 16 3\n. (98)\nSince \u03b3WL \u2264 1, ineq (98) ensured as long as\nb \u2264 8\u03b3WL 3\n16 9 + 8 3 + 16 3\n= 3\u03b3WL 11 , (99)\nwhich also guarantees b \u2264 \u221a 3/(4 + \u221a 3). So, letting T\u2217 . = |Fk\u2217 | and recollecting\nb . =\n\u03c9\nmink maxj |\u03c0jk| (100)\nfrom eq. (92), we obtain from ineqs (92) and (97):\n\u039bk\u2217 \u2265 \u03c9T\u2217\u03b3\n2 WL\nmink maxj |\u03c0jk| . (101)\nWe need to ensure \u03c9 \u2264 3mink maxj |\u03c0jk|\u03b3WL/11 from ineq . (99), which holds if we pick it according to eq. (36). In this case, we finally obtain\n\u039bk\u2217 \u2265 (a\u03b3WLT\u2217) \u00b7 \u03b32WL . (102) Now, since log(1/(1 \u2212 x2)) \u2265 x2, we also have for k 6= k\u2217 in eq. (91),\n\u039bk = \u2211\nt:\u03b9(t)\u2208Fk\nlog 1\n1\u2212 r2t\n\u2265 \u2211\nt:\u03b9(t)\u2208Fk\nr2t\n\u2265 |Fk|\u03b32WL ,\u2200k 6= k\u2217 . (103) So, we finally obtain from eq. (84) and ineq. (86),\n\u2113expr (Sr,\u03b8, \u2016.\u201622) \u2264 exp ( \u2212 T\u0303\u03b3 2 WL\n2\n)\n, (104)\nwith T\u0303 . = (T \u2212 T\u2217) + a\u03b3WL \u00b7 T\u2217, as claimed when \u2126(.) = \u2016.\u2016\u221e. The case \u2126 = \u2016.\u20161 follows form the fact that all \u039bk match the bound of \u039bk\u2217 .\n29"}, {"heading": "9.14 Proof of Theorem 19", "text": "We use the proof of Theorem 18, since when \u2126(.) = \u2016.\u2016\u03a6, eq. (91) becomes\n\u039bk . =\n\u2211\nt:\u03b9(t)\u2208Fk\nlog 1\n1\u2212 r2t (105)\n\u2212 \u03bek \u03c0\u2217k \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2211\nt:\u03b9(t)\u2208Fk\nlog 1 + rt 1\u2212 rt \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2265 \u2211\nt:\u03b9(t)\u2208Fk\n{\nlog 1 1\u2212 r2t \u2212 \u03bek maxj |\u03c0jk| log 1 + |rt| 1\u2212 |rt|\n}\n, (106)\nassuming without loss of generality that the classifier at iteration T , \u03b8T , satisfies |\u03b8Tk| \u2265 |\u03b8T (k+1)| for k = 1, 2, ..., d \u2212 1. We recall that \u03bek .= \u03a6\u22121(1 \u2212 kq/(2d)) where \u03a6\u22121(.) is the quantile of the standard normal distribution and q \u2208 (0, 1) is the user-fixed q-value. The constraint b \u2264 3\u03b3WL/11 from ineq. (99) now has to hold with\nb = bk . = \u03bek maxj |\u03c0jk| . (107)\nNow, fix\na . = min { 3\u03b3WL 11 , \u03a6\u22121(1\u2212 q/(2d)) mink maxj |\u03c0jk| } . (108)\nRemark that\n\u03bek . = \u03a6\u22121\n(\n1\u2212 kq 2d\n)\n\u2265 \u03a6\u22121 ( 1\u2212 q 2d ) \u2265 amin k\u2032 max j |\u03c0jk\u2032 | . (109)\nSuppose q is chosen such that\n\u03bek \u2264 3\u03b3WL 11 \u00b7max j |\u03c0jk| ,\u2200k \u2208 [d] . (110)\nThis ensures bk \u2264 3\u03b3WL/11 (\u2200k \u2208 [d]) in ineq. (99), while ineq. (109) ensures\n\u039bk \u2265 bk \u2211\nt:\u03b9(t)\u2208Fk\nr2t (111)\n\u2265 \u03bek mink\u2032 maxj |\u03c0jk\u2032 | \u00b7 \u2211\nt:\u03b9(t)\u2208Fk\nr2t (112)\n\u2265 a|Fk|\u03b32WL . (113)\nIneq. (111) holds because of ineqs (106) and (97). Ineq. (113) holds because of the weak learning assumption and ineq. (110). So, we obtain, under the weak learning assumption,\n\u2113expr (Sr,\u03b8, \u2016.\u2016\u03a6) \u2264 exp ( \u2212aT\u03b3 2 WL\n2\n)\n. (114)\n30\nEnsuring ineq. (110) is done if, after replacing \u03bek by its expression and reorganising, we can ensure\nq \u2265 2 \u00b7max k qN,k qD,k , (115)\nwith\n(0, 1) \u220b qN,k .= 1\u2212\u03a6 ( 3\u03b3WL 11 \u00b7max j |\u03c0jk| ) , (116)\n(0, 1] \u220b qD,k .= k\nd . (117)\n(118)"}, {"heading": "9.15 Proof of Theorem 20", "text": "Suppose wlog that the example index on which Se and S\u2032e differ is m, and let em and e \u2032 m denote the two distinct edge vectors of the neighbouring datasets. For n = 1, let \u03c0 denote a rado created from first picking uniformly at random I \u2208 2m and then using DP-RADOS on the singleton Sr .= {\u03c0I} with:\n\u03c0I . =\n\u2211\ni\u2208I\nyi \u00b7 xi . (119)\nLet a(Se) . =\n\u2211\nI\u2032\u2286[m\u22121] \u00b5(\u03c0|\u03c01 = \u03c0I\u2032 , S = Se), where \u00b5(\u03c0|.) is the density of the singleton output of DP-RADOS, and b(Se) . = \u2211 I\u2032\u2286[m],m\u2208I \u00b5(\u03c0|\u03c01 = \u03c0I\u2032 , S = Se). We have:\n\u00b5(\u03c0|Se) \u00b5(\u03c0|S\u2032e) = a(Se) + b(Se) a(S\u2032e) + b(S \u2032 e)\n= a(Se) + b(Se)\na(Se) + b(S\u2032e) (120)\n\u2264 max { b(Se) b(S\u2032e) , b(S\u2032e) b(Se) } . (121)\nEq. (120) follows from the fact that when I\u2032 \u2286 [m\u2212 1], \u00b5(\u03c0|\u03c01 = \u03c0I\u2032 , S = Se) = \u00b5(\u03c0|\u03c01 = \u03c0I\u2032 , S = S\u2032e). Now, for any fixed I\u2032 \u2286 [m] such that m \u2208 I\u2032, we have\n\u00b5(\u03c0|\u03c01 = \u03c0I\u2032 , S = Se) . = ( \u03b5\n2re\n)d\nexp(\u2212\u03b5 \u00b7 \u2016\u03c0I\u2032 \u2212 \u03c0\u20161/re)\n=\n( \u03b5\n2re\n)d\nexp(\u2212\u03b5 \u00b7 \u2016\u03c0\u2032I\u2032 \u2212 \u03c0+ em \u2212 e\u2032m\u20161/re) (122)\n\u2264 ( \u03b5\n2re\n)d\nexp\n(\n\u2212 \u03b5 re\n\u00b7 \u2016\u03c0\u2032I\u2032 \u2212 \u03c0\u20161 )\n\ufe38 \ufe37\ufe37 \ufe38 . =\u00b5(\u03c0|\u03c01=\u03c0I\u2032 ,S=S \u2032 e) \u00b7 exp ( \u03b5\nre \u00b7 \u2016em \u2212 e\u2032m\u20161\n)\n= exp\n( \u03b5\nre \u00b7 \u2016em \u2212 e\u2032m\u20161\n)\n\u00b7\u00b5(\u03c0|\u03c01 = \u03c0I\u2032 , S = S\u2032e) .\n31\nwhere \u03c0\u2032 I\u2032 . = \u03c0I\u2032 \u2212 em + e\u2032m in eq. (122) is the rado that is created from the same I\u2032 but using S\u2032e and its potentially different example e\u2032m. The inequality holds because of the triangle inequality. Since \u2016em \u2212 e\u2032m\u20161 \u2264 re by assumption, we get \u00b5(\u03c0|\u03c01 = \u03c0I\u2032 , S = Se) \u2264 exp(\u03b5) \u00b7 \u00b5(\u03c0|\u03c01 = \u03c0I\u2032 , S = S\u2032e), and so, summing over all I\u2032 \u2286 [m] such that m \u2208 I\u2032, we get b(Se)/b(S\u2032e) \u2264 exp(\u03b5). Furthermore, we also have by symmetry b(S\u2032e)/b(Se) \u2264 exp(\u03b5). So the delivery of one rado is \u03b5-differentially private. The composition Theorem (Dwork & Roth, 2014) achieves the proof of the first point of Theorem 20. To prove the second point, we first define the (unregularized) log-loss,\n\u2113loge (Se,\u03b8) . =\n1 m \u00b7 \u2211\ni\nlog ( 1 + exp ( \u2212\u03b8\u22a4(yi \u00b7 xi) )) . (123)\nWe exploit the following inequalities that hold for the log-loss and exp-rado loss:\n1\nm \u00b7 log \u2113expr (S\u2217,DPr ,\u03b8)\n= 1 m \u00b7 log 1 2m\n\u2211\n\u03c3\u2208\u03a3m\nexp ( \u2212\u03b8\u22a4 (\u03c0\u03c3 + z\u03c3) )\n\u2264 1 m\n\u00b7 log (( 1\n2m\n\u2211\n\u03c3\u2208\u03a3m\nexp ( \u2212\u03b8\u22a4\u03c0\u03c3 ) ) \u00b7 ( \u2211\n\u03c3\u2208\u03a3m\nexp ( \u2212\u03b8\u22a4z\u03c3 ) ))\n(124)\n= 1 m \u00b7 log 1 2m\n\u2211\n\u03c3\u2208\u03a3m\nexp ( \u2212\u03b8\u22a4\u03c0\u03c3 ) + 1\nm \u00b7 log\n\u2211\n\u03c3\u2208\u03a3m\nexp ( \u2212\u03b8\u22a4z\u03c3 )\n= log 2 + 1 m \u00b7 log 1 2m\n\u2211\n\u03c3\u2208\u03a3m\nexp ( \u2212\u03b8\u22a4\u03c0\u03c3 ) + 1 m \u00b7 log 1 2m \u2211\n\u03c3\u2208\u03a3m\nexp ( \u2212\u03b8\u22a4z\u03c3 )\n\u2264 log 2 + 1 m \u00b7 log 1 2m\n\u2211\n\u03c3\u2208\u03a3m\nexp ( \u2212\u03b8\u22a4\u03c0\u03c3 ) + 1\nm max \u03c3\n\u03b8\u22a4z\u03c3\n\u2264 log 2 + 1 m \u00b7 log 1 2m\n\u2211\n\u03c3\u2208\u03a3m\nexp ( \u2212\u03b8\u22a4\u03c0\u03c3 ) + 1\nm max \u03c3\n\u2126\u22c6(z\u03c3)\u2126(\u03b8) (125)\n= \u2113loge (Se,\u03b8) + 1\nm max \u03c3\n\u2126\u22c6(z\u03c3)\u2126(\u03b8) (126)\n= \u2113loge\n(\nSe,\u03b8, (1/m) \u00b7max \u03c3\n\u2126\u22c6(z\u03c3) \u00b7 \u2126 ) ,\nwhere ineq. (124) comes from the fact that \u2211 i aibi \u2264 ( \u2211 i ai)( \u2211\ni bi) when all ai, bi \u2265 0, ineq. (125) is Cauchy-Schwartz and eq. (126) is Lemma 2 in (Nock et al., 2015).\nRemarks on \u03b5a: let us explain why the protection of examples using the same noise level as rados is conservative but in fact necessary in the worst case, considering for simplicity the protection of a single rado / example. The proof of Theorem 20 exploits a conservative upperbound for the likelihood ratio:\n\u00b5(\u03c0|Se) \u00b5(\u03c0|S\u2032e) = a(Se) + b(Se) a(Se) + b(S\u2032e) \u2264 max\n{ b(Se)\nb(S\u2032e) , b(S\u2032e) b(Se)\n}\n, (127)\nand then upperbounds the max by exp \u03b5 to get the DP requirement. The same strategy can be used to protect the example, but the bound is sometimes more conservative in this case. Indeed, whereas one examples participates in generating half the total number of DP rados, one example participates in only 1/m of the\n32\ngeneration of DP examples. For a single DP example e, the equality in (127) becomes \u00b5(e|Se)/\u00b5(e|S\u2032e) = (a\u2032(Se) + b \u2032(Se)/(a \u2032(Se) + b \u2032(S\u2032e) with a \u2032(Se) . = (1 \u2212 (1/m)) \u00b7 \u00b5(e|Se\\{em}) and:\nb\u2032(Se) . =\n1\nm \u00b7 \u00b5(e|{em}) , b\u2032(S\u2032e) . =\n1\nm \u00b7 \u00b5(e|{e\u2032m}) . (128)\nLet u . = \u00b5(e|{e\u2032m})/\u00b5(e|S\u2032e\\{e\u2032m}) (= \u00b5(e|{e\u2032m})/\u00b5(e|Se\\{em})). If we use the same amount of protection as for one rado, then we get\n\u00b5(e|Se) \u00b5(e|S\u2032e) \u2264 fu(\u03b5) , (129)\nwhere \u03b5 is the rado privacy budget and\nfu(\u03b5) . = (m\u2212 1) + u exp(\u03b5) m\u2212 1 + u . (130)\nfu(\u03b5) is always < exp(\u03b5), so if we use this exp(\u03b5) bound to pick the noise level, then we are in fact putting more protection over examples than necessary (remember that the protection is also conservative for rados, but to a lesser extent). However, this choice would not be so bad in the worst case since limu\u2192\u221e fu(\u03b5) = exp(\u03b5). To summarise, without constraints on u, and to be sure to meet the DP requirements in any case, we would err on the conservative side, as we did for rados in ineq. (121), and pick \u03b5e = \u03b5, i.e. the same amount of noise for examples. Yet, as we explain in the body of the paper, the results are exceedingly in favor of \u2126-R.ADABOOST in this case. To give a more balanced picture, we chose to compute an \u201capproximate\u201d privacy budget \u03b5e = \u03b5a for n examples, which we simply fix to be \u03b5a . = n \u00b7 ln(fu .=1(\u03b5/n)) (< \u03b5) where \u03b5 is the privacy budget for n rados. So, we have\n\u03b5a = n \u00b7 ln ( 1 + exp(\u03b5/n) \u2212 1\nm\n)\n. (131)\nAgain, when u > 1, fixing \u03b5 = \u03b5a to protect examples would fail to achieve \u03b5-differential privacy. Nevertheless, one can reasonably consider that the \u201coptimal\u201d differentially private picture of ADABOOST shall thus be representable as a stretching of its curves in between the figures for \u03b5a and \u03b5."}, {"heading": "10 Additional experiments", "text": ""}, {"heading": "10.1 Supports for rados (complement to Table 2)", "text": "Table 4 in this Appendix provides the supports used to summarize Table 2.\n10.2 Experiments on class-wise rados\nTables 5 and 6 provide the test errors and supports for \u2126-R.ADABOOST when trained with class-wise rados, that is, rados that sum examples of the same class. The experiments do not display that class-wise rados allow for a better training of \u2126-R.ADABOOST, as test errors are on par with \u2126-R.ADABOOST trained with plain random rados (see Table 2)."}, {"heading": "10.3 Test errors and supports for rados (comparison last vs best empirical classifier)", "text": "In the paper\u2019s main experiments, the classifier kept out of the sequence, for both ADABOOST and \u2126R.ADABOOST, is the best empirical classifier, that is, the classifier which minimizes the empirical risk\n33\nout of the training sample. This setting makes sense if the objective is just the minimization of the test error without any constraint, and it is also applicable in a privacy setting where the data and the learner are distant parties (in this case, the learner sends the sequence of classifiers \u03b81,\u03b82, ...,\u03b8T to the party holding the data, which can then select the best in the sequence). Yet, one may wonder how the algorithms compare when the classifier returned is just the last one in the sequence, that is, \u03b8T .\nTables 7 and 8 provide errors and supports comparing the versions of \u2126-R.ADABOOST when the best empirical classifier is selected (\u22c6), or when the last classifier in the sequence is kept (\u2020). They are therefore subsuming Tables 2 (for test errors) and 4 (for supports).\nThe intuition tells that not selecting the classifier in the sequence produced (\u2020) should produce either no better, or eventually worse results than when selecting the classifier to keep from the sequence \u03b80,\u03b81, ...,\u03b8T . The results display that it is the case, for both ADABOOST and \u2126-R.ADABOOST, and the phenomenon is more visible as the domain size increases. The degradation for \u2126-R.ADABOOST appears to be significantly worse than that for ADABOOST on three domains, Fertility, Firmteacher and Kaggle, since not selecting the classifier using the training data incurs an increase of 8% and more on the test error for these domains. However, for the majority of the domains, the variation in test error does not exceed 1%, and on three domains (Winewhite, Smartphone, Eeg), the absence of selection of the classifier actually does not increase the test error at all.\nTherefore, even when not marginal, the fact that the test error significantly increases only on a minority of the domains for \u2126-R.ADABOOST calls for a rather domain-specific selection procedure of the classifier in the sequence, rather than an all-purpose selection procedure. Furthermore, on domains for which not selecting the classifier produces the worst results, such a more efficient selection procedure of the classifier might actually be bypassed by a more careful crafting of the rados, since when class-wise random rados are used (results not shown), picking the last classifier for domain Kaggle reduces the test error by approximately 10% compared to random rados (the test error drops to 32.68\u00b110.9 instead of 42.41\u00b19.32 for SLOPE). Such a specific crafting of rados is an interesting and non trivial problem that deserves further attention.\n34\n35\n36\n37\n38\n39\n40"}], "references": [{"title": "Optimization with sparsity-inducing penalties", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2011}, {"title": "SLOPE \u2013 adaptive variable selection via convex optimization", "author": ["M Bogdan", "E. van den Berg", "C. Sabatti", "W. Su", "Cand\u00e8s", "E.-J"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Bogdan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bogdan et al\\.", "year": 2015}, {"title": "Efficient learning using forward-backward splitting", "author": ["Duchi", "J.-C", "Y. Singer"], "venue": "In NIPS*22,", "citeRegEx": "Duchi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2009}, {"title": "The algorithmic foudations of differential privacy", "author": ["C. Dwork", "A. Roth"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Dwork and Roth,? \\Q2014\\E", "shortCiteRegEx": "Dwork and Roth", "year": 2014}, {"title": "Linear hinge loss and average margin", "author": ["C. Gentile", "M. Warmuth"], "venue": "In NIPS*11,", "citeRegEx": "Gentile and Warmuth,? \\Q1998\\E", "shortCiteRegEx": "Gentile and Warmuth", "year": 1998}, {"title": "Differential privacy: An economic method for choosing epsilon", "author": ["J. Hsu", "M. Gaboardi", "A. Haeberlen", "S. Khanna", "A. Narayan", "Pierce", "B.-C", "A. Roth"], "venue": "In Proc. of the 27 IEEE CSFS,", "citeRegEx": "Hsu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2014}, {"title": "On the boosting ability of top-down decision tree learning algorithms", "author": ["M.J. Kearns", "Y. Mansour"], "venue": "J. Comp. Syst. Sc.,", "citeRegEx": "Kearns and Mansour,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Mansour", "year": 1999}, {"title": "Computational implications of reducing data to sufficient statistics", "author": ["A. Montanari"], "venue": "Technical Report 201412, Stanford U.,", "citeRegEx": "Montanari,? \\Q2014\\E", "shortCiteRegEx": "Montanari", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "ICML, pp", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "On the efficient minimization of classification-calibrated surrogates", "author": ["R. Nock", "F. Nielsen"], "venue": "In NIPS*21,", "citeRegEx": "Nock and Nielsen,? \\Q2008\\E", "shortCiteRegEx": "Nock and Nielsen", "year": 2008}, {"title": "Generalized mixability via entropic duality", "author": ["Reid", "M.-D", "Frongillo", "R.-M", "Williamson", "R.-C", "Mehta", "N.-A"], "venue": "In 28 th COLT, pp", "citeRegEx": "Reid et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reid et al\\.", "year": 2015}, {"title": "The boosting approach to machine learning: An overview", "author": ["Schapire", "R.-E"], "venue": "Notes in Statistics,", "citeRegEx": "Schapire and R..E.,? \\Q2003\\E", "shortCiteRegEx": "Schapire and R..E.", "year": 2003}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Schapire", "Y. Singer"], "venue": "MLJ,", "citeRegEx": "Schapire and Singer,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer", "year": 1999}, {"title": "SLOPE is adaptive to unkown sparsity and asymptotically minimax", "author": ["W. Su", "Cand\u00e8s", "E.-J"], "venue": "CoRR, abs/1503.08393,", "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "A primal-dual convergence analysis of boosting", "author": ["M. Telgarsky"], "venue": "JMLR, 13:561\u2013606,", "citeRegEx": "Telgarsky,? \\Q2012\\E", "shortCiteRegEx": "Telgarsky", "year": 2012}, {"title": "Learning with symmetric label noise: The importance of being unhinged", "author": ["B. van Rooyen", "A. Menon", "Williamson", "R.-C"], "venue": "In NIPS*28,", "citeRegEx": "Rooyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rooyen et al\\.", "year": 2015}, {"title": "Privacy for free: Posterior sampling and stochastic gradient Monte Carlo", "author": ["Wang", "Y.-X", "S.E. Fienberg", "Smola", "A.-J"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Speed and sparsity of regularized boosting", "author": ["Xi", "Y.-T", "Xiang", "Z.-J", "Ramadge", "P.-J", "Schapire", "R.-E"], "venue": "AISTATS, pp", "citeRegEx": "Xi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xi et al\\.", "year": 2009}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Zou and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "It is known that sufficient statistics carry the intractability of certain processes that would otherwise be easy with data (Montanari, 2014).", "startOffset": 124, "endOffset": 141}, {"referenceID": 14, "context": "The technique we use exploits a two-player zero sum game representation of convex losses, that has been very useful to analyse boosting algorithms (Schapire, 2003; Telgarsky, 2012), with one key difference: payoffs are non-linear convex, eventually non-differentiable.", "startOffset": 147, "endOffset": 180}, {"referenceID": 10, "context": "These also resemble the entropic dual losses (Reid et al., 2015), with the difference that we do not enforce conjugacy over the simplex.", "startOffset": 45, "endOffset": 64}, {"referenceID": 14, "context": "It turns out that the losses involved bear popular names in different communities, even when not all of them are systematically used as losses per se: exponential, logistic, square, mean-variance, ReLU, linear Hinge, and unhinged losses (Nair & Hinton, 2010; Gentile & Warmuth, 1998; Nock & Nielsen, 2008; Telgarsky, 2012; Vapnik, 1998; van Rooyen et al., 2015) (and many others).", "startOffset": 237, "endOffset": 361}, {"referenceID": 0, "context": "Regularizing a loss is common in machine learning (Bach et al., 2011).", "startOffset": 50, "endOffset": 69}, {"referenceID": 1, "context": "ADABOOST, that learns a classifier from rados using the exponential regularized rado loss, with regularization choice belonging to the ridge, lasso, l\u221e, or the recently coined SLOPE (Bogdan et al., 2015).", "startOffset": 182, "endOffset": 203}, {"referenceID": 0, "context": "= le(Se,\u03b8) + \u03a9(\u03b8) , (21) with \u03a9 a regularizer (Bach et al., 2011).", "startOffset": 46, "endOffset": 65}, {"referenceID": 0, "context": "(Bach et al., 2011; Bogdan et al., 2015; Duchi & Singer, 2009; Su & Cand\u00e8s, 2015).", "startOffset": 0, "endOffset": 81}, {"referenceID": 1, "context": "(Bach et al., 2011; Bogdan et al., 2015; Duchi & Singer, 2009; Su & Cand\u00e8s, 2015).", "startOffset": 0, "endOffset": 81}, {"referenceID": 1, "context": "ADABOOST, for two reasons: it matches the original definition (Bogdan et al., 2015) and furthermore it unveils an interesting connection between boosting and the SLOPE properties (Su & Cand\u00e8s, 2015).", "startOffset": 62, "endOffset": 83}, {"referenceID": 1, "context": "14) Constraint (ii) on q is interesting in the light of the properties of SLOPE (Bogdan et al., 2015; Su & Cand\u00e8s, 2015).", "startOffset": 80, "endOffset": 120}, {"referenceID": 17, "context": "Table 2: Best result of ADABOOST/l1-ADABOOST (Schapire & Singer, 1999; Xi et al., 2009), vs \u03a9R.", "startOffset": 45, "endOffset": 87}, {"referenceID": 17, "context": "ADABOOST to ADABOOST/l1-ADABOOST (Schapire & Singer, 1999; Xi et al., 2009).", "startOffset": 33, "endOffset": 75}, {"referenceID": 17, "context": "To obtain very sparse solutions for l1-ADABOOST, we pick its \u03c9 (\u03b2 in (Xi et al., 2009)) in {10\u22124, 1, 104}.", "startOffset": 69, "endOffset": 86}, {"referenceID": 5, "context": "The results are a clear advocacy in favor of using rados against examples for the straight DP protection: with plain random rados, test errors that compete with clean data can be observed for privacy budget \u03b5 \u2248 10\u22124, that is, more than a hundred times smaller than most reported studies (Hsu et al., 2014).", "startOffset": 287, "endOffset": 305}, {"referenceID": 16, "context": "In addition to \u201ccoming for free\u201d (Wang et al., 2015) in machine learning, DP may thus also be a worthwhile companion to improve learning.", "startOffset": 33, "endOffset": 52}, {"referenceID": 17, "context": "Table 7: Best result of ADABOOST/l1-ADABOOST Schapire & Singer (1999); Xi et al. (2009), vs \u03a9-R.", "startOffset": 71, "endOffset": 88}, {"referenceID": 17, "context": "Table 7: Best result of ADABOOST/l1-ADABOOST Schapire & Singer (1999); Xi et al. (2009), vs \u03a9-R.ADABOOST (with or without regularization, trained with n = m random rados (above bold horizontal line) / n = 10000 rados (below bold horizontal line)), according to the expected true error of \u03b8T , when the classifier \u03b8T returned is the last classifier of the sequence (\u201d\u2020\u201d; \u03b8T = \u03b81000), or when it is the classifier minimizing the empirical risk in the sequence (\u201d\u201d; \u03b8T = \u03b8\u22641000). Table shows the best result over all \u03c9s, as well as the difference between the worst and best (\u2206). Shaded cells display the best result of \u03a9-R.ADABOOST. All domains but Kaggle are UCI Bache & Lichman (2013). 39", "startOffset": 71, "endOffset": 684}], "year": 2015, "abstractText": "It has recently been shown that supervised learning with the popular logistic loss is equivalent to optimizing the exponential loss over sufficient statistics about the class: Rademacher observations (rados). We first show that this unexpected equivalence can actually be generalized to other example / rado losses, with necessary and sufficient conditions for the equivalence, exemplified on four losses that bear popular names in various fields: exponential (boosting), mean-variance (finance), Linear Hinge (on-line learning), ReLU (deep learning), and unhinged (statistics). Second, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (with Minkowski sums) in the equivalent rado loss. This brings simple and powerful rado-based learning algorithms for sparsity-controlling regularization, that we exemplify on a boosting algorithm for the regularized exponential rado-loss, which formally boosts over four types of regularization, including the popular ridge and lasso, and the recently coined SLOPE \u2014 we obtain the first proven boosting algorithm for this last regularization. Through our first contribution on the equivalence of rado and example-based losses, \u03a9R.ADABOOST appears to be an efficient proxy to boost the regularized logistic loss over examples using whichever of the four regularizers (and any linear combination of them, e.g., for elastic net regularization). We are not aware of any regularized logistic loss formal boosting algorithm with such a wide spectrum of regularizers. Experiments display that regularization consistently improves performances of rado-based learning, and may challenge or beat the state of the art of example-based learning even when learning over small sets of rados. Finally, we connect regularization to \u03b5-differential privacy, and display how tiny budgets (e.g. \u03b5 < 10) can be afforded on big domains while beating (protected) example-based learning.", "creator": "gnuplot 4.6 patchlevel 5"}}}