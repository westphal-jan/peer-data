{"id": "1512.07048", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "Beauty and Brains: Detecting Anomalous Pattern Co-Occurrences", "abstract": "Our world is filled with both beautiful and brainy people, but how often does a Nobel Prize winner also wins a beauty pageant? Let us assume that someone who is both very beautiful and very smart is more rare than what we would expect from the combination of the number of beautiful and brainy people. Of course there will still always be some individuals that defy this stereotype; these beautiful brainy people are exactly the class of anomaly we focus on in this paper. They do not posses rare qualities, but it is the unexpected combination of factors that makes them stand out.", "histories": [["v1", "Tue, 22 Dec 2015 12:15:12 GMT  (397kb,D)", "https://arxiv.org/abs/1512.07048v1", null], ["v2", "Wed, 10 Feb 2016 15:55:56 GMT  (404kb,D)", "http://arxiv.org/abs/1512.07048v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["roel bertens", "jilles vreeken", "arno siebes"], "accepted": false, "id": "1512.07048"}, "pdf": {"name": "1512.07048.pdf", "metadata": {"source": "CRF", "title": "Beauty and Brains: Detecting Anomalous Pattern Co-Occurrences", "authors": ["Roel Bertens", "Jilles Vreeken", "Arno Siebes"], "emails": ["R.Bertens@uu.nl", "jilles@mpi-inf.mpg.de", "A.P.J.M.Siebes@uu.nl"], "sections": [{"heading": null, "text": "In this paper we define the above described class of anomaly and propose a method to quickly identify them in transaction data. Further, as we take a pattern set based approach, our method readily explains why a transaction is anomalous. The effectiveness of our method is thoroughly verified with a wide range of experiments on both real world and synthetic data."}, {"heading": "1. INTRODUCTION", "text": "The recognition of anomalies provides useful application-specific insights [1]. More specifically, the field of anomaly detection focusses on the identification of data that significantly differ from the rest of the dataset \u2014 so different that it gives rise to the suspicion that it was generated by a different mechanism. Such an anomaly may, e.g., occur because of an error, it may be an outlier, or it may be a highly unexpected data point. It is hard, if not impossible, to distinguish between such different reasons automatically. Hence, anomalies should be inspected manually to decide whether it should, e.g., be removed, corrected, or simply remain in the data \u201cas is\u201d. One should thus preferably not report an overly large list of potentially anomalous data points and, at the very least, that list should be ordered such that the most anomalous data points appear on top.\nFor transactional data anomaly detection usually boils down to pointing out those transactions that show unexpected behaviour. This unexpected behaviour can manifest itself in different ways and each detection algorithm is limited to find only those anomalies which fit the corresponding framework. For example, much work has been done to detect unexpected behaviour which can be expressed by the compressed size of a transaction given a preprocessed model [22, 3]. That is, transactions that badly fit the norm of the data are deemed to be anomalous. Another example is to score transactions based on the number of frequent patterns that reside in it [12]. Yet another method scores transactions based on items missing from a transaction which were expected given the set of mined association rules [18]. All these methods have their own advantages, however, none of them is able to detect an anomaly based on the presence of multiple items in a single transaction that are not expected to occur together. Therefore in this work we focus on this\nclass of anomalies, not to improve existing methods, but to improve the field of anomaly detection by making it more comprehensive.\nIn addition to only highlighting the transactions that show anomalous behaviour, our method describes anomalies in more detail by providing the most unlikely co-occurrence of patterns in a transaction. As an example consider a dataset containing people\u2019s drinking habits where roughly half of the people drinks Coca Cola and the other half drinks Pepsi Cola. Now each individual who drinks Coke or Pepsi is not surprising. Moreover, someone drinking both Coke and Pepsi also does not seem surprising as it can be compressed well using the methods of [22, 3], it contains multiple frequent patterns [12] and there is nothing missing [18]. However, in this dataset almost everyone drinks either Pepsi or Coke, but not both. Therefore, someone drinking both Coke and Pepsi is an anomaly by definition, drinking both is unexpected. We propose to score each transaction based on the most unlikely co-occurrence between patterns and therefore our method will be able to find the described class of anomalies.\nFor this example, the score we introduce is minus the log of the lift of the association rule Pepsi\u2192 Coke and, except the log, has been introduced before in [13] as the novelty of an association rule. The difference is that we do not score a rule, but a transaction and do so by the maximal novelty of the rules that apply to this transaction. Perhaps even more important is that we give an algorithm that does not require one to mine for all association rules with a support of 1 to find the most surprising transactions.\nOur new class of anomalies, their score, and the algorithm to discover these anomalies introduced in this paper is complementary to the two other well-known anomaly classes for transaction data. The first class describes transactions with a deviating length (i.e. the number of items in the transaction) and is quite trivial. The second class, for which the most work has been done [22, 3], describes unexpected transactions given a model of the data. To illustrate the complementarity of this latter class and our new class we show in our experiments that where current methods fail to identify our new class of anomalies, our method quickly finds them. Note, however, that our method is not intended to replace any existing methods that can discover other classes of anomalies. Rather, one should use different methods, for different classes of anomalies, that are complementary to each other.\nThe remainder of this paper is organised as follows. We first introduce notation in Section 2. In Section 3 we discuss the concept of anomalies in transaction data, introducing a novel class. Section 4 explains how to use our score in practice. We discuss related work in Section 5, and empirically evaluate our score in Section 6. We round up with discussion and conclusions in Sections 7 and 8, respectively.\nar X\niv :1\n51 2.\n07 04\n8v 2\n[ cs\n.A I]\n1 0\nFe b\n20 16"}, {"heading": "2. NOTATION", "text": "In this section we provide the notation used throughout the paper. We consider transaction datasets D containing |D| transactions. Each transaction t contains a subset, of size |t|, of the items from the alphabet \u2126. Categorical data consists of |A| attributes, where each attribute Ai \u2208 A has a domain \u2126i, and can also be regarded as transaction data by mapping each attribute value pair to a different item. For categorical data each transaction will have the same length as each attribute should be specified. All logarithms are to base 2, and by convention 0 log 0 = 0. Further, we use P (\u00b7) to denote a probability density function."}, {"heading": "3. ANOMALIES IN TRANSACTION DATA", "text": "What is an Anomaly? Anomalies are also referred to as abnormalities, discordants, deviants, or outliers in the data mining and statistics literature [1]. As we consider transaction data we use the following definition.\nDEFINITION 3.1. A transaction is anomalous when it deviates from what we expect considering the whole dataset.\nGiven this definition an anomaly can manifest itself in different ways, resulting in multiple classes of anomalies for transaction data. In this section we recall 2 classes of anomalies, define 1 new class, and we show how to identify all of them by formalising appropriate anomaly scores. We want to emphasise again that the scores for different classes of anomalies are complementary to each other. Further, for ease of interpretation and computation we take the negative log-likelihood for the scores in each class."}, {"heading": "3.1 Class 0: Unexpected Transaction Lengths", "text": "A transaction can be anomalous not as a result of the patterns it contains, but solely on its deviating length.\nDEFINITION 3.2. A class 0 anomaly is a transaction with significantly deviating transaction length.\nWe propose an anomaly score which represents the number of bits needed to describe the transaction length given all transaction lengths in the data, i.e. for a transaction t we have\nscore0(t) = \u2212 log(P (|t|)) = \u2212 log ( |{t\u2032 \u2208 D | |t\u2032| = |t|}|\n|D|\n) .\nThe intuition behind the subscript 0 for this score is that we have to take no patterns into account at all to identify these anomalies. As a result, using this score we will not be able to identify interesting co-occurrences. Further, as it is a fairly trivial score we will not further evaluate it."}, {"heading": "3.2 Class 1: Unexpected Transactions", "text": "When a transaction contains none or only few frequent patterns, which do occur in (almost) all other transactions, it can be regarded to be anomalous.\nDEFINITION 3.3. A class 1 anomaly is a transaction that contains very little regularity.\nThe state-of-the-art in transaction anomaly detection focusses on what we call class 1 anomalies. For example, OC3 [22] scores transactions using a descriptive pattern set S. Transactions containing few of these patterns but mostly singletons will get a higher score. That is, because such a transaction cannot be explained well by the pattern set that is descriptive for the data. We generalise this idea by defining a score based on the probability of a transaction. More\nformally, score1 scores each transaction based on the number of bits needed to describe it, i.e. for a transaction t we have\nscore1(t) = \u2212 logP (t) .\nFor compression based methods such as OC3 this score is defined by the compressed length of the transaction given the model of the data. However, any method that can assign a probability to a transaction based on the whole data can be used here. Note that, as transactions are scored as a whole, this approach will unlikely detect unexpected co-occurrences of patterns. For example, using OC3, all patterns that describe a transaction will contribute to a its score independently. As much work has been done to detect these anomalies we will not further evaluate their identification, but focus on the next class of anomalies."}, {"heading": "3.3 Class 2: Unexpected Co-occurrences", "text": "The focus of this paper lies on identifying unexpected co-occurrences of patterns. To the best of our knowledge we are the first to address these class 2 anomalies. Before we explain how to identify them, we start with a definition.\nDEFINITION 3.4. A transaction contains a class 2 anomaly when it contains two patterns which occur much less together in the data than what could be expected from their individual supports.\nAs this definition is somehow the opposite of that of a pattern, which is formed when two smaller patterns occur together more frequently than expected, we can also use the terms negative pattern or negative interaction pattern for a class 2 anomaly. These negative patterns cannot be identified using currently available methods as these do not consider negative interaction patterns. An example of a class 2 anomaly is someone drinking both Coke and Pepsi when everybody else only drinks either Pepsi or Coke. In other words, a very rare co-occurrence of the two frequent patterns Coke and Pepsi.\nNow to identify anomalous transactions based on class 2 anomalies we would like to score a transaction higher the more unexpected a co-occurrence of patterns it contains is. That is, we propose to rank a transaction based on its most unexpected co-occurrence. Intuitively this means that for each transaction we compute the number of bits we need to explain the most unlikely co-occurrence given a pattern set S and the data. For a transaction t we thus have\nscore2(t) = max {X,Y \u2208S|X,Y\u2286t}\n\u2212 logP (XY ) + log ( P (X)\u00d7 P (Y ) ) .\nIn the remainder of this paper we refer to score2 as the BNB (Beauty and Brains) score. We compute P (X) as X\u2019s support or relative frequency in the data. A compression based approach similar to OC3 to compute P (X) based on its relative usage makes no sense here as we are looking for unexpected co-occurrences and are not trying to describe the entire transaction.\nGiven a BNB score for a transaction we can readily explain its anomalousness as we know which co-occurrence of patterns is responsible for the score. Therefore our method has the nice property of producing very interpretable rankings.\nOur score is related to the concept of lift [20] used in the context of association rules. In our setting we use it to describe the difference between two patterns appearing together in a transaction and what would be expected if they were statistically independent. Therefore, the higher our score the more unexpected the pattern co-occurrence.\nScores that are constructed to identify class 1 anomalies are not able to detect these class 2 anomalies as they look at all patterns independently. For example, OC3 [22] and COMPREX [3] will not give a class 2 anomaly a higher score as both individual patterns are frequent and will add little to the anomaly score. Similarly, the\nfrequent pattern based method from He et al. [12] and the method from Narita et al. [18] have no means to give higher scores to class 2 anomalies. As a result, methods for identifying class 1 anomalies do not identify unexpected co-occurrences, while these actually do indicate anomalous behaviour.\nWhich patterns to consider Given the relation between our score and the lift of association rules, a straightforward way to find high scoring transactions may seem to simply mine for high-lift association rules. However, to maximize the score the individual patterns X and Y should have a support as high as possible while XY should have a support as low as possible. That is we should mine for all rules including those with a support of 1 to ensure that we don\u2019t miss the most interesting transactions.\nClearly this becomes infeasible for all but the smallest data sets quickly. Not only because discovering all these rules will take an inordinate amount of time, but also since the post-processing of all these rules necessary to identify the most surprising transactions becomes an even more daunting task.\nThe alternative we take is by starting from a set of patterns S . We compute the score of each pair of patterns from S and identify those transaction in which pairs with a (very) high score occur.\nClearly, not just any pattern set will do as want to find the highest scoring transactions. The set of all frequent patterns F will be very descriptive, yet far too large to be able to consider the interactions between each pair of patterns. For all but the smallest of datasets this will quickly yield infeasibly large pattern sets. That is, worst case we need to consider each co-occurrence of patterns for each transaction, thus leading to a computational complexity of\nO(|D| \u00d7 |F| \u00d7 |F|) .\nChoosing a higher minimum support will yield smaller pattern sets but as a result we might miss important patterns. We could use condensed representations such as closed [19] or non-derivable [5] frequent patterns to remove as much redundancy as possible, however these sets will still be too large. By sampling [11] patterns we can attain small sets of patterns, however, the choice of the size of the sample determines which anomalies one will (likely) find. A set that is too small might miss some important patterns, but a set that is too large probably contains redundancy and becomes a bottleneck in our approach as we need to look at each combination of patterns in the set. Since it is not straightforward to choose the right size for the required pattern set, we choose to use KRIMP [25] or SLIM [23] to automatically find small descriptive pattern sets that describe the data well without containing noise or redundancy. Using these pattern sets it will hold that |S| |F|. We thus dramatically reduce the complexity, making the BNB score practically feasible as we will show in our experiments in Section 6. Using such a vastly smaller set induces, of course, the risk that we miss anomalies. However in other research we have seen that the pattern sets chosen by KRIMP and SLIM are highly characteristic for the data. The experiments in Section 6 bear out that this is also the case here: all anomalies we inject in synthetic data are discovered using these small sets only.\nNext to computational complexity, another advantage of small descriptive pattern sets is that they are more easily interpretable, which is convenient when explaining the identified anomalies."}, {"heading": "4. HOW TO USE OUR SCORES", "text": "In the process of explorative data mining, one has to consider that all 3 classes of anomalies we identify give different insight. In practice, one should instantiate all 3 scores and investigate the\ntop-ranked anomalies for each class. Here, our focus is of course on class 2 anomalies.\nTo determine which of the BNB top-ranked transactions to investigate, as well as to verify the significance of the scores, we propose the following two methods based on the Bootstrap. Recall that bootstrap methods consider the given data as a sample, and generate a number of pseudo-samples from it; for each pseudosample calculate the statistic of interest, and use the distribution of this statistic across pseudo-samples to infer the distribution of the original sample statistic [6]."}, {"heading": "4.1 Significance test", "text": "For a synthetic dataset it is easy to test the significance of anomaly scores, as we can generate data with and without anomalies for which the resulting scores must clearly differ. For real world data this is unfortunately not the case as we do not know which and how much (negative) patterns the data comprises. Nevertheless, to give a measure of significance we use the following bootstrap approach. We randomly sample transactions from our original dataset (with replacement) to retrieve an equally sized new dataset. We repeat this a thousand times and save the highest anomaly score for each dataset. Then we repeat this process, but we first remove the transaction with the highest BNB score from the sample set. That is, the top-ranked anomaly is definitely not present in the bootstrap samples of the second kind and may or may not be present in the bootstrap samples of the first kind. The bigger the difference between the distributions of scores with and without the top-ranked transaction, the more significant the top-ranked anomaly."}, {"heading": "4.2 Which transactions to investigate", "text": "Choosing the right parameter value is never easy in explorative data mining. As the BNB score produces a ranking on al transactions, where higher scores indicate a higher chance on being anomalous, it does not need any parameters. To determine which transactions to investigate based on this ranking we employ Cantelli\u2019s inequality to identify the transactions that significantly differ from the norm.\nTHEOREM 4.1. Cantelli\u2019s inequality [9]. Let X be a random variable with expectation \u00b5X and standard deviation \u03c3X . Then for any k \u2208 R+,\nP (X \u2212 \u00b5X \u2265 k\u03c3X) \u2264 1\n1 + k2 .\nSmets and Vreeken [22] proposed a well-founded way to determine threshold values to distinguish between \u2018normal\u2019 and anomalous transactions. The positive class comprises anomaly scores for \u2018normal\u2019 transactions and based on the distribution of these scores we can choose a threshold by choosing an upper bound on the falsenegative rate (FNR). For example, if we choose a confidence level of\n10%, Cantelli\u2019s inequality tells us that this corresponds to a threshold \u03b8 at 3 standard deviations from the mean, given by \u03b8 = \u00b5+ k\u03c3. This means that the chance on a future transaction with an anomaly score above the threshold is less than 10%, see Figure 1.\nTo compute these thresholds we need the distribution of the positive class, i.e. the anomaly scores for all \u2018normal\u2019 transactions. Unfortunately, we only have the one dataset available which can contain both transactions from the positive and negative (actual anomalies) class. As by definition the number of anomalies must be relatively small we use the entire dataset to estimate the distribution of the positive class again using a bootstrap approach. That is, we generate bootstrap datasets by randomly sampling transactions (with replacement) from the original dataset. We then use all anomaly scores from all bootstrap datasets to estimate the distribution."}, {"heading": "5. RELATED WORK", "text": "In this paper we study anomaly detection in binary transaction data. As anomalies are referred to in many different ways, mostly with slightly different definitions, we refer to [15] and [1] for indepth overviews on this field of research. In general, most anomaly detection methods rely on distances. Here we focus on discrete data, nominal attributes, for which meaningful distance measures are typically not available.\nOf the methods that are applicable on transaction data, that of Smets and Vreeken [22] is perhaps the most relevant. They propose to identify anomalies as those transactions that cannot be described well by the model of the data, where as models they use small descriptive pattern sets. Their method OC3 works very well for one-class classification, however it is not able to identify unexpected co-occurrences in the data. Akoglu et al. [3] proposed COMPREX which takes a similar approach in that they also rank transactions based on their encoded length. The difference is that they do not use a single code table to describe the data, but a code table for each partition of correlated features. Although this method achieves very good results it is only suitable for categorical data and not for transaction data in general. Note that, following our generalised anomaly score for class 1 anomalies, any method that provides a probability for a transaction can be used. Examples based on pattern sets are those of Wang and Parthasarathy [26] and Mampaey et al. [14].\nHe et al. [12] rank transactions based on the number of frequent patterns they contain given only the top-k frequent patterns, and Narita et al. [18] rank transactions based on confidence of association rules but need a minimum confidence level as parameter. All these methods have no means to identify class 2 anomalies.\nIn the Introduction we already mentioned the relation between our score and novelty as introduced in [13]. As stated there, the difference is that we score transactions rather than rules and we give an algorithm to quickly discover the highest scoring transactions. Our notion of anomaly is also related to the conditional anomalies introduced in [24]. In our running example, Pepsi could be seen as the context that makes a purchase of Coke unexpected in their terminology. The difference is that we do not expect the user to define such contexts, they are discovered automatically. Moreover, we use a small set of patterns to discover all the class-2 anomalies rather than probabilistic models on context and other attributes.\nTo compute the BNB score we need a small pattern set that contains the key patterns of the data. In general, we can use the result of standard frequent pattern mining [2, 19] although this incurs a high computational cost. Instead, we can resort to pattern sampling techniques [11, 4], yet then we have to choose the number of patterns to be sampled. Instead, Siebes et al. [21] proposed to mine such pattern sets by the Minimum Description Length principle [10].\nThat is, they identify the best set of patterns as the set of patterns that together most succinctly describe the data. By definition this set is not redundant and does not contain noise. KRIMP [25] and SLIM [23] are two deterministic algorithms that heuristically optimise this score. Other pattern set mining techniques, especially those that mine patterns characteristic for the data such as [14, 8, 26], are also meaningful choices to be used with BNB."}, {"heading": "6. EXPERIMENTS", "text": "In this section we evaluate the power of the BNB score to identify class 2 anomalies. Firstly, we show how we generated synthetic data needed for some of the experiments. Secondly, we provide a baseline comparison where we show that the size of the input set of patterns is of great importance. Next we show the performance of BNB on synthetic data and show its statistical power. Lastly, we show some nice results of class 2 anomalies on a wide variety of real world datasets.\nWe implemented our algorithms in C++ and generated our synthetic data using Python. Our code, both to compute anomaly scores and to generate synthetic data, is available for research purposes.1 All experiments were conducted on a 2.6 GHz system with 64 GB of memory."}, {"heading": "6.1 Generating Synthetic Data", "text": "Here we describe how we generated both transaction and categorical synthetic data.\nTransaction Data To generate synthetic transaction datasets we first choose the number of transactions |D| and the size of the alphabet |\u2126|. Further, we generate a set of random patterns P and for each pattern in this set we choose a random support in the range [5-10%] and a random size from 3 to 6 items. In addition we similarly generate 2 more patterns with a support of 20%, which we call the anomaly generators. Then we build our dataset by first adding the 2 anomaly generators for which we make sure that they only occur together in the same transaction once; that is the anomaly. Thereafter, we do the following for each transaction. With the probability corresponding to its support each pattern from P is added to the transaction as long as it does not interfere with the anomaly. In addition, each singleton from \u2126 is added to each transaction similarly with a probability of 10%.\nCategorical Data To generate categorical data we take a similar approach. Firstly, we choose the number of transactions |D|, the number of attributes |A| and the alphabet size per attributes |\u2126i|. We generate random patterns with the same settings as for transaction data and again first add the anomaly generators to the dataset. We then try to add the other patterns as long as they fit and do not interfere with the anomaly. Then we fill the unspecified attributes for each transaction with random singletons."}, {"heading": "6.2 Baseline Comparison", "text": "In the following sections we will show that BNB gives very reliable scores, but first we show its efficiency here. To emphasise the necessity for using small pattern sets as input for our BNB score we compare the use of SLIM [23] pattern sets with a minimum support of 1 against the use of all closed frequent patterns with a minimum support at 5%. We generated random transaction data with |D| = 5 000, |\u2126| = 50 and we ranged |P| from 10 to 35 patterns. We then ran our method on both input sets keeping track of the runtimes 1http://eda.mmci.uni-saarland.de/bnb/\nand the size of the input set S for which we have to consider all |S| \u00d7 |S| possible combinations. Both approaches always rank the anomaly highest, therefore further we can focus on the runtime and the number of patterns that were considered. Note that, the runtimes include the time needed to compute the used pattern sets, however, these are negligible as the exponential growth in runtime for the baseline approach is caused by the exponential growth of S. Figure 2 shows the results which are averages over 5 runs per setting. For higher minimum support thresholds the baseline approach starts missing important patterns and it cannot identify the anomaly. Other settings for generating synthetic data lead to a similar figure. Since using a SLIM pattern set as input set for BNB we attain similar results compared to the baseline approach, that is we correctly identify the planted anomaly, in the remainder of this paper we always use the SLIM pattern set to compute the BNB score."}, {"heading": "6.3 Performance on Synthetic Transaction Data", "text": "The goal of this experiment is twofold. Firstly, we show that our method is able to identify class 2 anomalies in transaction data. Secondly, we justify the definition of the different classes of anomalies as we show that the class 2 anomalies are not identified by the state-of-the-art class 1 anomaly detector, which is OC3 [22]. We emphasise again that as a result both scores should not be further compared as they are complementary to each other.\nWe generated random datasets as described in Section 6.1 with various settings. The results in Table 1 show that BNB always ranks the anomaly highest and that OC3 does not identify them."}, {"heading": "6.4 Performance on Synthetic Categorical Data", "text": "Knowing that BNB correctly identifies class 2 anomalies for transaction data, here we compared it to the state-of-the-art on categorical data, which is COMPREX [3]. Again we note that we\nonly compare these methods to show that class 2 anomalies are different from class 1 anomalies and that these two methods thus should be used complementary to each other.\nWe generated random datasets as described in Section 6.1 with various settings. The results in Table 2 show that BNB always ranks the anomalous transaction first and COMPREX is not able to identify it (gives it a much lower rank)."}, {"heading": "6.5 Statistical Power", "text": "Our aim here is to examine the power of the BNB score for identifying class 2 anomalies. For this purpose, we perform statistical tests using synthetic data. To this end, the null hypothesis is that the data contains no class 2 anomalies. To determine the cutoff for testing the null hypothesis, we first generate 100 transaction datasets without the single co-occurrence between the 2 anomaly generators, whereafter we generate another 100 datasets with this co-occurrence included. For all datasets we choose |D| = 5 000, |\u2126| = 25 items and |P| = 100. Next, we report the highest BNB score for all 100 datasets without anomaly. Subsequently, we set the cutoff according to the significance level \u03b1 = 0.05. The power of the BNB score is the proportion of the highest scores from the 100 datasets with anomaly that exceed the cutoff. Note that, we only look at the highest score for each dataset as we know that this must be the anomaly for the datasets containing it. We show the results in Figure 3 while varying the the range from which we randomly choose the supports for the patterns in P from [4-8%] to [8-16%] and the support for the anomaly generators from 16% to 32%. In Figure 3 we label these linearly growing supports with their growth factor from 1 to 2. With other settings to generate the data we observe the same trend. Again, only to emphasise that methods to identify class 1 anomalies are not suitable to discover class 2 anomalies, in Figure 3 we also plotted the statistical power of OC3 regarding class 2 anomalies. As COMPREX is not applicable to transaction data we performed a similar experiment on categorical data. This resulted in a similar plot with BNB at the top and COMPREX at the bottom.\nIn Figure 4 we show the distribution of the highest scores for both the datasets with and without an anomaly and with pattern supports in range [7-14%] and an anomaly generator support at 28%. We can see a clear distinction between the scores for \u2018normal\u2019 and anomalous transactions."}, {"heading": "6.6 Real World Data", "text": "To show that class 2 anomalies actually exist, are not identified by the state-of-the-art in anomaly detection, and can give much insight we performed multiple experiments on real world datasets from various domains. We used the Adult, Zoo and Bike Sharing datasets\nfrom the UCI repository,2 together with the Mammals [17] en ICDM Abstracts [7] datasets.\nAdult The Adult dataset contains information about 48 842 people about their age, education, occupation, marital-status and more and is used to predict whether someone\u2019s income exceeds $50K a year.\nWe computed a ranking based on the BNB score and found some interesting anomalies. The top-ranked transaction contains the very unexpected co-occurrence of someone for which the attribute sex is female yet for whom the relationship status has the value of husband. The following 3 anomalies are persons with a similar situation but with the patterns reversed. That is, the dataset contains 3 persons who\u2019s sex is male and who\u2019s relationship is wife. The OC3 rankings of these first 4 people are 115, 148, 89 and 4 090, respectively. These examples show that class-2 anomalies indeed exist in real datasets, and that BNB is effective at identifying these \u2013 whether for further investigation, or data cleaning.\nTo get an idea of the significance of these results we performed the significance test as described in Section 4.1. Figure 5 shows the difference in the distribution of highest scores for bootstrap samples without (blue) and possibly with (red) the top-ranked transaction from the original dataset. Figure 5 gives insight in how much this transaction deviates from the norm, as the difference between the two distributions can only be caused by this transaction.\n2http://archive.ics.uci.edu/ml/datasets.html\nZoo The Zoo data contains 17 attributes describing 101 different animals.\nWe performed the bootstrap method described in Section 4.2 to determine which transactions are worth investigating. To this end, we generated 1 000 bootstrap samples for which we computed the anomaly scores for all transactions. In Figure 6 we show the distribution of all these scores with a histogram. Further, in the left plot we show the threshold (\u03b8) values corresponding to false-negative rates (FNR) of 50%, 20%, 10% and 5% respectively, together with the number of transactions from the original dataset that score above \u03b8. In the right plot we show the anomaly scores of the 5 highest ranked transactions in the original dataset together with the FNR corresponding to a \u03b8 equal to their score. In Figure 6 in the left plot we see that with an FNR below 10% only the top-ranked transaction scores above \u03b8. This transaction contains information about the platypus (duck bill) and from our results we found that the cooccurrence causing this high score is that the platypus is the only oviparous mammal in the dataset. In Figure 6 in the right plot we see that the chance that the second ranked animal belongs to the positive class is less than 11%. This is the scorpion for which BNB found that it is the only animal without teeth that is not oviparous.\nICDM Abstracts Next we ran our algorithm on the ICDM Abstracts dataset. This dataset consists of the abstracts from the ICDM conference, after stemming, and removing stopwords.3\nFor this data we would expect co-occurrences of terms used in different research (sub)fields to rank highly. In Table 3 we show the top 5 highest ranked abstracts with their explanation. That is, we show the unexpected co-occurrence responsible for the high BNB score. Further, only to show that these class 2 anomalies are not identified by the state-of-of-the-art, we show their OC3 rank. The abstract with the highest BNB rank contains both the frequently used words \u2018pattern mining\u2019 and \u2018training\u2019, which is an unexpected combination. After reading the corresponding abstract it appears that the term \u2018training\u2019 was used to refer to physical exercise rather than that of an algorithm. Other highly ranked abstracts show similar unexpected co-occurrences, for example \u2018learning\u2019 on one side and \u2018frequent pattern mining\u2019 on the other or \u2018frequent pattern mining\u2019 and \u2018compare\u2019, which suggest that exploratory algorithms are difficult to compare.\n3The data is available upon request from the author of [7].\nMammals The Mammals dataset consists of presence/absence records of 121 European mammals within 2 183 geographical areas of 50\u00d7 50 kilometres.4 In this dataset an anomaly constitutes two large territories of (groups of) animals which only overlap in a small region.\nFigure 7 shows two top-ranked area\u2019s (in red and pointed to by arrows) and readily explains why these are anomalous. For each of these two area\u2019s two groups of animals share this territory where the rest of their territory is completely separated. On the left in Figure 7 we see that the large habitat of the beech marten intersects with that of the moose, the European hedgehog and the mountain hare only in this single area. On the right in Figure 7 we see a similar phenomenon for the Etruscan shrew on one side and the raccoon dog on the other. The ranks of these two areas using OC3 are 591 and 294 out of the 2 183, respectively. There are also top-ranked areas that are explained by two groups of animals which habitat intersects in multiple areas (of course including the area that has received this score).\nBike Sharing The Bike Sharing dataset contains the daily count of rental bikes in the years 2011 and 2012 in the Capital Bikeshare system with corresponding weather and seasonal information.\nThe BNB score of the highest ranked day in the dataset is not the result of any rental behaviour, but shows a very rare co-occurrence\n4The full dataset [16] is available upon request from the Societas Europea Mammalogica, http://www.european-mammals. org.\nof a relatively low real temperature in combination with a relatively high apparent (perceived) temperature. This indeed seems strange as people more often feel colder as a result of wind-chill. Although this anomaly gives us no information about bike sharing, it is an actual class 2 anomaly present in the data."}, {"heading": "7. DISCUSSION", "text": "The experiments show that although the state-of-the-art in anomaly detection is not able to identify the newly defined class 2 anomalies, we can identify them using our new BNB score. We demonstrated that a naive baseline approach using closed frequent items as input set quickly becomes infeasible when the number of patterns present in the data grows. Using a SLIM pattern set to compute our BNB score, however, we attain similar results in a fraction of the time. We showed the statistical power of our method which scores transactions containing planted class 2 anomalies significantly higher than \u2018normal\u2019 transactions. Moreover, both on transaction and categorical synthetic data we showed that BNB always ranked the planted anomaly at the top.\nFrom our experiments on real world datasets we find that the class 2 anomalies do actually exist and can provide useful insights. That is, because next to identifying interesting transactions the BNB score also readily explains which co-occurrence of patterns is responsible for the transaction\u2019s anomaly score. For example, in the Adult dataset we found a very unexpected individual who is described as being a female husband. Further we showed how a BNB ranking can be used to study the significance of identified anomalies using a bootstrap approach. For example, in the Zoo dataset we found that the platypus, which is special because its the only oviparous\nmammal, has a less than 8% chance on being \u2018normal\u2019 given the data. Each of these class 2 anomalies were not identified, i.e. ranked low, using OC3 or COMPREX."}, {"heading": "8. CONCLUSION", "text": "In this paper we introduced a new class of anomalies which we refer to as unexpected co-occurrences of patterns. We showed that the anomalies identified by state-of-the-art in anomaly detection are of a different class and that these methods are not able to identify unexpected co-occurrences of patterns. We introduced the BNB score which intuitively scores a transactions based on its most unexpected co-occurrence of patterns. Using BNB we ably identify all planted anomalies in synthetic data and find interesting explanations for anomalous transactions in real world data. Besides useful for identifying interesting behaviour, BNB also makes it possible to detect errors in data that previous methods cannot, making it also very suited for data cleaning purposes."}, {"heading": "Acknowledgments", "text": "Roel Bertens and Arno Siebes are supported by the Dutch national program COMMIT. Jilles Vreeken is supported by the Cluster of Excellence \u201cMultimodal Computing and Interaction\u201d within the Excellence Initiative of the German Federal Government."}, {"heading": "9. REFERENCES", "text": "[1] C. C. Aggarwal, editor. Outlier Analysis. Springer, 2013. [2] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I.\nVerkamo. Fast discovery of association rules. In Advances in Knowledge Discovery and Data Mining, pages 307\u2013328. AAAI/MIT Press, 1996.\n[3] L. Akoglu, H. Tong, J. Vreeken, and C. Faloutsos. COMPREX: Compression based anomaly detection. In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM), Maui, HI. ACM, 2012.\n[4] M. Boley, C. Lucchese, D. Paurat, and T. G\u00e4rtner. Direct local pattern sampling by efficient two-step random procedures. In Proceedings of the 17th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD), San Diego, CA, pages 582\u2013590. ACM, 2011.\n[5] T. Calders and B. Goethals. Non-derivable itemset mining. Data Mining and Knowledge Discovery, 14(1):171\u2013206, 2007.\n[6] A. C. Cameron, J. B. Gelbach, and D. L. Miller. Bootstrap-based improvements for inference with clustered errors. The Review of Economics and Statistics, 90:414\u2013427, 2008.\n[7] T. De Bie. Maximum entropy models and subjective interestingness: an application to tiles in binary databases. Data Mining and Knowledge Discovery, 23(3):407\u2013446, 2011.\n[8] F. Geerts, B. Goethals, and T. Mielik\u00e4inen. Tiling databases. In Proceedings of Discovery Science, pages 278\u2013289, 2004.\n[9] G. Grimmett and D. Stirzaker. Probability and Random Processes. Oxford university press, 2001.\n[10] P. Gr\u00fcnwald. The Minimum Description Length Principle. MIT Press, 2007.\n[11] M. A. Hasan and M. Zaki. Musk: Uniform sampling of k maximal patterns. In Proceedings of the 9th SIAM International Conference on Data Mining (SDM), Sparks, NV, pages 650\u2013661. SIAM, 2009.\n[12] Z. He, X. Xu, J. Z. Huang, and S. Deng. Fp-outlier: Frequent pattern based outlier detection. Computer Science and Information Systems, 2(1):103\u2013118, 2005.\n[13] N. Lavrac\u030c, P. Flach, and B. Zupan. Rule evaluation measures: A unifying view. In International Conference on Inductive Logic Programming (ILP), pages 174\u2013185, 1999.\n[14] M. Mampaey, J. Vreeken, and N. Tatti. Summarizing data succinctly with the most informative itemsets. ACM Transactions on Knowledge Discovery from Data, 6:1\u201344, 2012.\n[15] M. Markou and S. Singh. Novelty detection: a review. Signal processing, 83(12), 2003.\n[16] A. Mitchell-Jones, G. Amori, W. Bogdanowicz, B. Krystufek, P. H. Reijnders, F. Spitzenberger, M. Stubbe, J. Thissen, V. Vohralik, and J. Zima. The Atlas of European Mammals. Academic Press, 1999.\n[17] T. Mitchell-Jones. Societas europaea mammalogica. http://www.european-mammals.org, 1999.\n[18] K. Narita and H. Kitagawa. Outlier detection for transaction databases using association rules. In Web-Age Information Management, 2008. WAIM \u201908. The Ninth International Conference on, pages 373\u2013380, July 2008.\n[19] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules. In Proceedings of the 7th International Conference on Database Theory (ICDT), Jerusalem, Israel, pages 398\u2013416. ACM, 1999.\n[20] G. Piatetsky-Shapiro. Discovery, analysis, and presentation of strong rules. In G. Piatetsky-Shapiro and W. Frawley, editors, Knowledge Discovery in Databases. AAAI/MIT Press, 1991.\n[21] A. Siebes, J. Vreeken, and M. van Leeuwen. Item sets that compress. In Proceedings of the 6th SIAM International Conference on Data Mining (SDM), Bethesda, MD, pages 393\u2013404. SIAM, 2006.\n[22] K. Smets and J. Vreeken. The odd one out: Identifying and characterising anomalies. In Proceedings of the 11th SIAM International Conference on Data Mining (SDM), Mesa, AZ, pages 804\u2013815. Society for Industrial and Applied Mathematics (SIAM), 2011.\n[23] K. Smets and J. Vreeken. SLIM: Directly mining descriptive patterns. In Proceedings of the 12th SIAM International Conference on Data Mining (SDM), Anaheim, CA, pages 236\u2013247. Society for Industrial and Applied Mathematics (SIAM), 2012.\n[24] X. Song, M. Wu, C. Jermaine, and S. Ranka. Conditional anomaly detection. TKDE, 19(5):631\u2013645, 2007.\n[25] J. Vreeken, M. van Leeuwen, and A. Siebes. KRIMP: Mining itemsets that compress. Data Mining and Knowledge Discovery, 23(1):169\u2013214, 2011.\n[26] C. Wang and S. Parthasarathy. Summarizing itemset patterns using probabilistic models. In Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD), Philadelphia, PA, pages 730\u2013735, 2006."}], "references": [{"title": "Fast discovery of association rules. In Advances in Knowledge Discovery and Data Mining, pages 307\u2013328", "author": ["R. Agrawal", "H. Mannila", "R. Srikant", "H. Toivonen", "A.I. Verkamo"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "COMPREX: Compression based anomaly detection", "author": ["L. Akoglu", "H. Tong", "J. Vreeken", "C. Faloutsos"], "venue": "In Proceedings of the 21st ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Direct local pattern sampling by efficient two-step random procedures", "author": ["M. Boley", "C. Lucchese", "D. Paurat", "T. G\u00e4rtner"], "venue": "In Proceedings of the 17th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Non-derivable itemset mining", "author": ["T. Calders", "B. Goethals"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Bootstrap-based improvements for inference with clustered errors", "author": ["A.C. Cameron", "J.B. Gelbach", "D.L. Miller"], "venue": "The Review of Economics and Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Maximum entropy models and subjective interestingness: an application to tiles in binary databases", "author": ["T. De Bie"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Tiling databases", "author": ["F. Geerts", "B. Goethals", "T. Mielik\u00e4inen"], "venue": "In Proceedings of Discovery Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Probability and Random Processes", "author": ["G. Grimmett", "D. Stirzaker"], "venue": "Oxford university press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "The Minimum Description Length Principle", "author": ["P. Gr\u00fcnwald"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Musk: Uniform sampling of k maximal patterns", "author": ["M.A. Hasan", "M. Zaki"], "venue": "In Proceedings of the 9th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Fp-outlier: Frequent pattern based outlier detection", "author": ["Z. He", "X. Xu", "J.Z. Huang", "S. Deng"], "venue": "Computer Science and Information Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Rule evaluation measures: A unifying view", "author": ["N. Lavra\u010d", "P. Flach", "B. Zupan"], "venue": "In International Conference on Inductive Logic Programming (ILP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Summarizing data succinctly with the most informative itemsets", "author": ["M. Mampaey", "J. Vreeken", "N. Tatti"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Novelty detection: a review", "author": ["M. Markou", "S. Singh"], "venue": "Signal processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "The Atlas of European Mammals", "author": ["A. Mitchell-Jones", "G. Amori", "W. Bogdanowicz", "B. Krystufek", "P.H. Reijnders", "F. Spitzenberger", "M. Stubbe", "J. Thissen", "V. Vohralik", "J. Zima"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Societas europaea mammalogica", "author": ["T. Mitchell-Jones"], "venue": "http://www.european-mammals.org,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Outlier detection for transaction databases using association rules", "author": ["K. Narita", "H. Kitagawa"], "venue": "In Web-Age Information Management,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Discovering frequent closed itemsets for association rules", "author": ["N. Pasquier", "Y. Bastide", "R. Taouil", "L. Lakhal"], "venue": "In Proceedings of the 7th International Conference on Database Theory (ICDT),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Discovery, analysis, and presentation of strong rules", "author": ["G. Piatetsky-Shapiro"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1991}, {"title": "Item sets that compress", "author": ["A. Siebes", "J. Vreeken", "M. van Leeuwen"], "venue": "In Proceedings of the 6th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "The odd one out: Identifying and characterising anomalies", "author": ["K. Smets", "J. Vreeken"], "venue": "In Proceedings of the 11th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "SLIM: Directly mining descriptive patterns", "author": ["K. Smets", "J. Vreeken"], "venue": "In Proceedings of the 12th SIAM International Conference on Data Mining (SDM),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Conditional anomaly detection", "author": ["X. Song", "M. Wu", "C. Jermaine", "S. Ranka"], "venue": "TKDE, 19(5):631\u2013645,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "KRIMP: Mining itemsets that compress", "author": ["J. Vreeken", "M. van Leeuwen", "A. Siebes"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Summarizing itemset patterns using probabilistic models", "author": ["C. Wang", "S. Parthasarathy"], "venue": "In Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}], "referenceMentions": [{"referenceID": 20, "context": "processed model [22, 3].", "startOffset": 16, "endOffset": 23}, {"referenceID": 1, "context": "processed model [22, 3].", "startOffset": 16, "endOffset": 23}, {"referenceID": 10, "context": "Another example is to score transactions based on the number of frequent patterns that reside in it [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Yet another method scores transactions based on items missing from a transaction which were expected given the set of mined association rules [18].", "startOffset": 142, "endOffset": 146}, {"referenceID": 20, "context": "Moreover, someone drinking both Coke and Pepsi also does not seem surprising as it can be compressed well using the methods of [22, 3], it contains multiple frequent patterns [12] and there is nothing missing [18].", "startOffset": 127, "endOffset": 134}, {"referenceID": 1, "context": "Moreover, someone drinking both Coke and Pepsi also does not seem surprising as it can be compressed well using the methods of [22, 3], it contains multiple frequent patterns [12] and there is nothing missing [18].", "startOffset": 127, "endOffset": 134}, {"referenceID": 10, "context": "Moreover, someone drinking both Coke and Pepsi also does not seem surprising as it can be compressed well using the methods of [22, 3], it contains multiple frequent patterns [12] and there is nothing missing [18].", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "Moreover, someone drinking both Coke and Pepsi also does not seem surprising as it can be compressed well using the methods of [22, 3], it contains multiple frequent patterns [12] and there is nothing missing [18].", "startOffset": 209, "endOffset": 213}, {"referenceID": 11, "context": "For this example, the score we introduce is minus the log of the lift of the association rule Pepsi\u2192 Coke and, except the log, has been introduced before in [13] as the novelty of an association rule.", "startOffset": 157, "endOffset": 161}, {"referenceID": 20, "context": "class, for which the most work has been done [22, 3], describes unexpected transactions given a model of the data.", "startOffset": 45, "endOffset": 52}, {"referenceID": 1, "context": "class, for which the most work has been done [22, 3], describes unexpected transactions given a model of the data.", "startOffset": 45, "endOffset": 52}, {"referenceID": 20, "context": "For example, OC [22] scores transactions using a descriptive pattern set S.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "Our score is related to the concept of lift [20] used in the context", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "For example, OC [22] and COMPREX [3] will not give a class 2 anomaly a higher score as both individual patterns are frequent and will add little to the anomaly score.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "For example, OC [22] and COMPREX [3] will not give a class 2 anomaly a higher score as both individual patterns are frequent and will add little to the anomaly score.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "[12] and the method from Narita et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] have no means to give higher scores to class 2 anomalies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "We could use condensed representations such as closed [19] or non-derivable [5] frequent patterns to remove as much redundancy as possible, however these sets will still be too large.", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "We could use condensed representations such as closed [19] or non-derivable [5] frequent patterns to remove as much redundancy as possible, however these sets will still be too large.", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "By sampling [11] patterns we can attain small sets of patterns, however, the choice of the size of the sample determines which anomalies one will (likely) find.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "for the required pattern set, we choose to use KRIMP [25] or SLIM [23] to automatically find small descriptive pattern sets that describe the data well without containing noise or redundancy.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "for the required pattern set, we choose to use KRIMP [25] or SLIM [23] to automatically find small descriptive pattern sets that describe the data well without containing noise or redundancy.", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "Recall that bootstrap methods consider the given data as a sample, and generate a number of pseudo-samples from it; for each pseudosample calculate the statistic of interest, and use the distribution of this statistic across pseudo-samples to infer the distribution of the original sample statistic [6].", "startOffset": 299, "endOffset": 302}, {"referenceID": 7, "context": "Cantelli\u2019s inequality [9].", "startOffset": 22, "endOffset": 25}, {"referenceID": 20, "context": "Smets and Vreeken [22] proposed a well-founded way to determine threshold values to distinguish between \u2018normal\u2019 and anomalous transactions.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "As anomalies are referred to in many different ways, mostly with slightly different definitions, we refer to [15] and [1] for indepth overviews on this field of research.", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "Of the methods that are applicable on transaction data, that of Smets and Vreeken [22] is perhaps the most relevant.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "[3] proposed COMPREX which takes a similar approach in that they also rank transactions based on their encoded length.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "sets are those of Wang and Parthasarathy [26] and Mampaey et al.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] rank transactions based on the number of frequent patterns they contain given only the top-k frequent patterns, and Narita et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] rank transactions based on confidence of association rules but need a minimum confidence level as parameter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In the Introduction we already mentioned the relation between our score and novelty as introduced in [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "Our notion of anomaly is also related to the conditional anomalies introduced in [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "In general, we can use the result of standard frequent pattern mining [2, 19] although this incurs a high computational cost.", "startOffset": 70, "endOffset": 77}, {"referenceID": 17, "context": "In general, we can use the result of standard frequent pattern mining [2, 19] although this incurs a high computational cost.", "startOffset": 70, "endOffset": 77}, {"referenceID": 9, "context": "Instead, we can resort to pattern sampling techniques [11, 4], yet then we have to choose the number of patterns to be sampled.", "startOffset": 54, "endOffset": 61}, {"referenceID": 2, "context": "Instead, we can resort to pattern sampling techniques [11, 4], yet then we have to choose the number of patterns to be sampled.", "startOffset": 54, "endOffset": 61}, {"referenceID": 19, "context": "[21] proposed to mine such pattern sets by the Minimum Description Length principle [10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[21] proposed to mine such pattern sets by the Minimum Description Length principle [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 23, "context": "KRIMP [25] and SLIM [23] are two deterministic algorithms that heuristically optimise this score.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "KRIMP [25] and SLIM [23] are two deterministic algorithms that heuristically optimise this score.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "Other pattern set mining techniques, especially those that mine patterns characteristic for the data such as [14, 8, 26], are also meaningful choices to be used with BNB.", "startOffset": 109, "endOffset": 120}, {"referenceID": 6, "context": "Other pattern set mining techniques, especially those that mine patterns characteristic for the data such as [14, 8, 26], are also meaningful choices to be used with BNB.", "startOffset": 109, "endOffset": 120}, {"referenceID": 24, "context": "Other pattern set mining techniques, especially those that mine patterns characteristic for the data such as [14, 8, 26], are also meaningful choices to be used with BNB.", "startOffset": 109, "endOffset": 120}, {"referenceID": 21, "context": "we compare the use of SLIM [23] pattern sets with a minimum support of 1 against the use of all closed frequent patterns with a minimum support at 5%.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Secondly, we justify the definition of the different classes of anomalies as we show that the class 2 anomalies are not identified by the state-of-the-art class 1 anomaly detector, which is OC [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 1, "context": "Knowing that BNB correctly identifies class 2 anomalies for transaction data, here we compared it to the state-of-the-art on categorical data, which is COMPREX [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 15, "context": "from the UCI repository, together with the Mammals [17] en ICDM Abstracts [7] datasets.", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "from the UCI repository, together with the Mammals [17] en ICDM Abstracts [7] datasets.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "The data is available upon request from the author of [7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "The full dataset [16] is available upon request from the Societas Europea Mammalogica, http://www.", "startOffset": 17, "endOffset": 21}], "year": 2016, "abstractText": "Our world is filled with both beautiful and brainy people, but how often does a Nobel Prize winner also wins a beauty pageant? Let us assume that someone who is both very beautiful and very smart is more rare than what we would expect from the combination of the number of beautiful and brainy people. Of course there will still always be some individuals that defy this stereotype; these beautiful brainy people are exactly the class of anomaly we focus on in this paper. They do not posses intrinsically rare qualities, it is the unexpected combination of factors that makes them stand out. In this paper we define the above described class of anomaly and propose a method to quickly identify them in transaction data. Further, as we take a pattern set based approach, our method readily explains why a transaction is anomalous. The effectiveness of our method is thoroughly verified with a wide range of experiments on both real world and synthetic data.", "creator": "LaTeX with hyperref package"}}}