{"id": "1512.07734", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2015", "title": "RDF2Rules: Learning Rules from RDF Knowledge Bases by Mining Frequent Predicate Cycles", "abstract": "Recently, several large-scale RDF knowledge bases have been built and applied in many knowledge-based applications. To further increase the number of facts in RDF knowledge bases, logic rules can be used to predict new facts based on the existing ones. Therefore, how to automatically learn reliable rules from large-scale knowledge bases becomes increasingly important. In this paper, we propose a novel rule learning approach named RDF2Rules for RDF knowledge bases. RDF2Rules first mines frequent predicate cycles (FPCs), a kind of interesting frequent patterns in knowledge bases, and then generates rules from the mined FPCs. Because each FPC can produce multiple rules, and effective pruning strategy is used in the process of mining FPCs, RDF2Rules works very efficiently. Another advantage of RDF2Rules is that it uses the entity type information when generates and evaluates rules, which makes the learned rules more accurate. Experiments show that our approach outperforms the compared approach in terms of both efficiency and accuracy.", "histories": [["v1", "Thu, 24 Dec 2015 07:19:01 GMT  (39kb)", "http://arxiv.org/abs/1512.07734v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["zhichun wang", "juanzi li"], "accepted": false, "id": "1512.07734"}, "pdf": {"name": "1512.07734.pdf", "metadata": {"source": "CRF", "title": "RDF2Rules: Learning Rules from RDF Knowledge Bases by Mining Frequent Predicate Cycles", "authors": ["Zhichun Wang", "Juanzi Li"], "emails": ["zcwang@bnu.edu.cn", "lijuanzi@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n07 73\n4v 1\n[ cs\n.A I]\n2 4\nD ec\n2 01"}, {"heading": "1. INTRODUCTION", "text": "Recently, a growing number of large-scale Knowledge Bases (KBs) have been created and published by using the Resource Description Framework (RDF)1, such as DBpedia [1], YAGO [24], and Freebase [2] etc. These KBs contain not only huge number of entities but also rich entity relations, which makes them successfully used in many applications such as Question Answering [25], Semantic Relatedness Computation [11] and Entity Linking[22].\nThe coverage of entities and the amount of facts are two important factors that determine the quality of RDF KBs. In order to enrich the knowledge in an RDF KB, information extraction techniques are usually used to extract more entities and their relations from plain text or semi-structured text. For example, DBpedia regularly extracts facts from Wikipedia\u2019s infoboxes to update its contents. Yet another promising way to expand a KB is to infer new facts from the existing ones by using inference rules. For example, by using the following rule we can predict that entity B is the child of\n1http://www.w3.org/RDF/\nentity C if we have already known that A is the parent of B and A is the spouse of C.\nhasChild(A,B) \u2227 hasSpouse(A,C) \u21d2 hasChild(C,B)\nAlthough the coverage of entities in a KB cannot be expanded in this way, inferring new facts by rules is more efficient and accurate than information extraction, especially when the KB has already accumulated substantial facts about entities. It is reported that the new version of YAGO used logic rules to deduce new facts from the existing ones [9].\nOne challenging problem of inferring new facts is how to define all the possible inference rules for a KB. Since manually setting all the rules is not possible, how to automatically learning inference rules from the existing facts in KBs becomes an interesting and important problem. Learning rules from KBs has been studied for years in the domain of Inductive Logic Programming (ILP) [18], but ILP approaches usually need negative facts of target relations and typically do not scale well on large-scale KBs. Recently, Gal\u00e1rraga et al. have proposed a system AMIE for learning rules from RDF data [7]. AMIE starts with the most general rules having only heads, and gradually extends rules by using four operators. Each time an operator is executed, a projection query is submitted to the KB to select entities and relations for that operator. Most recently, AMIE has been Extended to AMIE+ by a series of improvements to make it more efficient [8]. Compared with several state-of-the-art ILP systems, AMIE+ runs much more efficiently and can generate more rules with high quality.\nIn spite of good performance of AMIE+, there are still several challenging problems that need to be studied. First, how to use entity type information when learning rules has not been well studied. In [8], the authors do discuss adding types in rules, but how to automatically learn rules with types is not detailedly explained in the paper; and according to our experiments, the released AMIE+ tool can not learn rules with type information. Second, in the manner of learning one rule at a time, AMIE+ still needs a very long running time when the RDF KB is really large and long rules are to be learned. Third, the PCA confidence used by AMIE+ sometimes over-estimates unknown facts as true ones; rules with high PCA confidence may predict lots of incorrect facts.\nIn this paper, we propose a new rule learning approach named RDF2Rules. RDF2Rules works in a very different way from AMIE+ when learning inference rules. Instead of learning one rule at a time, RDF2Rules first mines a kind of interesting frequent patterns in KBs, which are called Frequent Predicate Cycles (FPCs); then multiple rules are generated from each mined FPC. With prop-\nerly designed pruning strategy, RDF2Rules can running faster than AMIE+ does. In addition, RDF2Rules can use entity type information when generating and evaluating learned rules, which results in rules having more accurate predictions.Specifically, our work has the following contributions:\n\u2022 We introduce the concept of Frequent Predicate Cycle (FPC) in RDF KBs, and manage to show that FPCs have corresponding relations with inference rules. An efficient algorithm for mining FPCs from RDF data is proposed; effective prune strategy is proposed to ensure the mining efficiency, and our FPC mining algorithm supports parallel execution on multi-core machines.\n\u2022 We propose a method for generating rules from the mined FPCs. Entity type information is utilized when our approach generates rules, and rules with entity type constraints are produced automatically.\n\u2022 To precisely evaluate the reliability of rules, we design a new confidence measure to evaluate rules under the open world assumption. Our new confidence measure also takes the entity type information into account, and can evaluate rules more accurately.\n\u2022 We evaluate our approach on YAGO2 and DBpedia. The experimental results show that our approach runs more efficiently and gets more reliable rules than the compared approach.\nThe rest of this paper is organized as follows, Section 2 first introduces some preliminary knowledge and then defines the concept of FPC; Section 3 presents the FPC mining algorithm; Section 4 presents the rule generation and evaluation methods; Section 5 describes the RDF KB indexing methods in our approach; Section 6 presents the evaluation results; Section 7 discusses some related work and finally Section 8 concludes this work."}, {"heading": "2. FREQUENT PREDICATE CYCLES", "text": "In this section, we first introduce some basics of RDF KBs; and then define the concept of Frequent Predicate Cycles (FPCs), and discuss the relation between FPCs and logic rules."}, {"heading": "2.1 RDF and RDF KB", "text": "The Resource Description Framework (RDF) is a framework for the conceptual description or modeling of information in Web resources. RDF expresses information by making statements about resources in the form of\n\u3008subject\u3009\u3008predicate\u3009\u3008object\u3009.\nThe subject and the object represent two resources, the predicate represents the relationship (directional) between the subject and the object. RDF statements are called triples because they consist of three elements. RDF is a graph-based data model; a set of RDF triples constitutes an RDF graph, where nodes represent resources and directed vertices represent predicates. There can be three kinds of nodes (resources) in an RDF graph: IRIs, literals, and blank nodes. An IRI is a global identifier for a resource, such as people, organization and place; literals are basic values including strings, dates and numbers, etc.; blank nodes in RDF represent recourses without global identifiers. Predicates in RDF are also represented by IRIs, since they can be considered as resources specifying binary relations. Figure 1 shows an example of small RDF\ndbo:spouse dbo:children\ndbr:George_W._Bush dbr:Laura_Bush dbr:Barbara_Pierce_Bush\n(a) Path\ngraph built from three triples about Barack Obama in DBpedia; dbr and dbo stand for the IRI prefixes http://dbpedia.org/resource/ and http://dbpedia.org/ontology/, respectively.\nAn RDF KB is a well-defined RDF dataset that consists of RDF statements (triples). The statements in an RDF KB are usually divided into two groups: T-box statements that define a set of domain specific concepts and predicates, and A-box statements that describe facts about instances of the concepts. The A-box triples excluding triples with literals are used by our approach to learn inference rules. Unlike AMIE, our approach also takes triples having rdf:type predicate as input. rdf:type is a special predicate that is used to state that a resource is an instance of a concept. The entity type information specified by rdf:type predicate is very useful and important to rule learning from RDF KBs, which is verified by our experiments."}, {"heading": "2.2 Definitions", "text": "RDF is a graph-based data model, so we represent an RDF KB as a graph G = (E,P, T ), where E is the set of vertexes representing all the vertexes (entities), P is the set of predicates, and T \u2286 E \u00d7 P \u00d7 E are directed and labeled edges between vertexes (entities). Based on this graph representation of KB, we define the concept of Path and Predicate Path as follows.\nDEFINITION 1 (PATH). A path in an RDF KB G = (E,P, T ) is a sequence of consecutive entities and predicates (v1, p d1 1 , v2, p d2 2 , ..., p dk\u22121 k\u22121 , vk), where vi \u2208 E, pi \u2208 P ; di \u2208 {1,\u22121} denotes the direction of predicate pi, if di = 1 then \u3008vi, pi, vi+1\u3009 \u2208 T ; otherwise, \u3008vi+1, pi, vi\u3009 \u2208 T . The length of a path is the number of\npredicates in it.\nPaths in an RDF KB show how entities are linked by various relations. Figure 2(a) shows an example of path in DBpedia, it starts from Grorge W. Bush via Lara Bush and ends at Barbara Pierce Bush. As shown in some studies, the starting and ending entities of a path sometimes also have some interesting relations. In the example of Figure 2(a), Barbara Pierce Bush actually is the children of Grorge W. Bush according to the facts in DBpedia. If we add this relation to the original path, we get a special kind of path as shown in Figure 2(b), the cycle.\nDEFINITION 2 (CYCLE). A cycle in an RDF graph is a special path that starts and ends at the same node.\nCycles in RDF graphs show very interesting connection patterns among entities. The connection pattern shown in Figure 2(b) reflects how Bush\u2019s family members are connected together by different relations. This pattern usually also holds for another family. In order to represent the interesting connection patterns in RDF graphs, we introduct the concept of Predicate Path and Predicate Cycle.\nDEFINITION 3 (PREDICATE PATH). A predicate path is a sequence of entity variables and predicates (x1, p d1 1 , x2, ..., p dk k , xk+1), where di \u2208 {1,\u22121} denotes the direction of predicate edge pi.\nDEFINITION 4 (PREDICATE CYCLE). A predicate cycle is a special predicate path that starts and ends at the same entity variable.\nAccording to the above definitions, predicate paths and predicate cycles can be obtained by replacing entities in paths and cycles with entity variables. Figure 3(a) and 3(b) show the predicate path and predicate cycle corresponding to the path and cycle in Figure 2(a) and 2(b).\nWe find that the interesting patterns represented by predicate cycles can be used to infer new facts in KBs. For example, if we have three entities that are connected by any two edges in the predicate cycle shown in Figure 3(b), a new fact identified by the third edge can be inferred. A more straightforward way is to generate inference rules from predicate cycles. As an example, the following two rules can be generated from the predicate cycle shown in Figure 3(b):\ndbo : spouse(x1, x2) \u2227 dbo : children(x2, x3)\n\u21d2 dbo : children(x1, x3)\ndbo : children(x1, x3) \u2227 dbo : children(x2, x3)\n\u21d2 dbo : spouse(x1, x2)\nBased on the above observation, we propose to learn inference rules by finding predicate cycles. However, not all predicate cycles can generate reliable rules, because some of them may present rare\nconnection pattern among entities. Intuitively, the more frequent a predicate cycle occurs in the RDF graph, the more reliable and useful the pattern of the predicate cycle is. Therefore, we define the concept of Frequent Predicate Path/Cycle.\nDEFINITION 5 (FREQUENT PREDICATE PATH/CYCLE). A path (cycle) is called the instance of a predicate path (cycle) if it can be generated by instantiating variables with entities in the predicate path (cycle). For a predicate path (cycle), the number of its instances that exists in the given RDF KB is called the support of it. If the support of a predicate path (cycle) is not less than a specified threshold, it is called the frequent predicate path (cycle).\nFrequent predicate cycles (FPCs) are patterns that frequently appear in the KB, rules generated from FPCs are prone to be reliable. So our proposed approach RDF2Rules first mines frequent predicate cycles from RDF KBs, and then generates inference rules from FPCs."}, {"heading": "3. FREQUENT PREDICATE CYCLE MINING", "text": "This section presents our proposed FPC mining algorithm. The basic idea is first to find all the Frequent Predicate Paths (FPPs) of specified maximum length, and then to discover FPCs by checking which FPPs can form predicate cycles. One big challenge here is that there are huge number of FPP candidates even in a small-sized RDF KB. If there are N predicates, then the number of all possible k-predicate paths (i.e. paths having k predicates) is (2N)k . As an example, there will be 8 million 3-predicate paths if we have 100 predicates. Because counting the supports of predicate paths is the most time-consuming work in the mining process, we have to prune the searching space of predicate paths if we want to find all the FPCs in a reasonable time.\nHere we use similar searching strategy in association rule mining and frequent subgraph mining algorithms. The basic idea is to first find frequent 1-predicate paths, and then iteratively find frequent k-predicate paths from frequent (k-1)-predicate paths. Algorithm 1 outlines the proposed algorithm. Our algorithm enumerates every different predicates in the KB, and searches predicate paths that start from it. For a starting predicate, two 1-predicate paths are first generated and evaluated to find whether they are frequent. After that, our algorithm starts a loop (line 12-20 in Algorithm 1) that discovers frequent k-predicate paths by extending frequent (k-1)predicate paths iteratively. In each iteration, once the frequent kpredicate paths are found, FPCs are discovered from them.\nAlgorithm 1: Frequent predicate cycles mining algorithm\nInput: An RDF graph G = (V, P, T ), the minimum support count \u03c4 , maximum length \u03be Output: Frequent predicate cycles \u0398\n1 Set \u0398 = \u2205; 2 Execute in parallel: 3 for each predicate pi \u2208 P do 4 Let \u03a81 = \u2205 5 Generate 1-predicate paths \u03b81 = (p+i ), \u03b82 = (p \u2212 i ); 6 if sup(\u03b81) \u2265 \u03c4 then 7 \u03a81 = \u03a81 \u222a {\u03b81}; 8 end 9 if sup(\u03b82) \u2265 \u03c4 then\n10 \u03a81 = \u03a81 \u222a {\u03b82}; 11 end 12 for j = 2, ..., \u03be do 13 Let \u03a8j = \u2205; 14 for each path \u03b8 \u2208 \u03a8j\u22121 do 15 \u03a8\u2032j = pathGrowth(\u03b8, \u03c4 ); 16 \u03a8j = \u03a8j \u222a\u03a8 \u2032 j ; 17 end 18 \u0398j = findCycles(\u03a8j); 19 \u0398 = \u0398 \u222a\u0398j 20 end 21 end 22 return \u0398\np1 pk\u22121 L\n(a) Original (k-1)-predicate path\nThe function pathGrowth in Algorithm 1 (line 15) extends a frequent (k-1)-predicate path to a set of frequent k-predicate paths by adding new predicates to the end of the (k-1)-predicate path. Figure 4 illustrates the process of pathGrowth. Given a (k-1)-predicate path \u03b8 in Figure 4(a), the instance paths of it are first find in the KB, let L(\u03b8) denotes the set of last entities in the instance paths of \u03b8. Predicates that connect entities in L(\u03b8) to other entities are then added to the original (k-1)-predicate path, as shown in Figure 4(b).\nAlgorithm 2: Predicate path growth algorithm (pathGrowth)\nInput: An RDF graph G = (V, P, T ), a (k-1)-predicate path \u03b8, the minimum support count \u03c4 Output: A set of frequent k-predicate path \u03a8\n1 \u03a8 = \u2205; 2 for each entity e \u2208 L(\u03b8) do 3 for each edge (e, p, e \u2032\n) \u2208 T do 4 Generate a new k-predicate path \u03b8 \u2032 = (\u03b8, p1); 5 if \u03b8 \u2032\n\u2208 \u03a8 then 6 \u03b8 \u2032 \u00b7 count = \u03b8 \u2032\n\u00b7 count + 1; 7 else 8 \u03b8 \u2032\n\u00b7 count = 1;\n9 \u03a8 = \u03a8 \u222a {\u03b8 \u2032\n}; 10 end 11 end 12 for each edge (e \u2032\n, p, e) \u2208 E do 13 Generate a new k-predicate path \u03b8 \u2032 = (\u03b8, p\u22121); 14 if \u03b8 \u2032\n\u2208 \u03a8 then 15 \u03b8 \u2032 \u00b7 count = \u03b8 \u2032\n\u00b7 count + 1; 16 else 17 \u03b8 \u2032\n\u00b7 count = 1;\n18 \u03a8 = \u03a8 \u222a {\u03b8 \u2032\n}; 19 end 20 end 21 end 22 for each predicate path \u03b8 \u2032\n\u2208 \u03a8 do 23 if \u03b8 \u2032\n\u00b7 count < \u03c4 then 24 \u03a8 = \u03a8/{\u03b8 \u2032\n}; 25 end 26 end 27 return \u03a8\nIn order to prune the search space and always get useful predicate paths, each time a new predicate is added to the original predicate path \u03b8, the following conditions should be satisfied:\n\u2022 The added predicate should appear in the frequent 1-predicate paths (i.e. the added predicate itself is frequent).\n\u2022 If the last predicate of the (k-1)-predicate path is p1i , the added predicate can not be p\u22121i ; if the last predicate of the (k1)-predicate path is p\u22121i , the added predicate can not be p 1 i .\nThis constraint can eliminate meaningless predicate paths like (x1, p1i , x2, p \u22121 i , x3, p 1 i , ....).\n\u2022 The added predicate should connect a required number (i.e. the minimum support) of entities to entities in L(\u03b8);\nAfter adding new predicates, a number of new k-predicate paths are obtained, as shown in Figure 4(c). Only the new predicate paths that are also frequent are kept as the output of the function pathGrowth. In the process of extending predicate paths, the set of last entities in the instance paths are always kept, which facilitates adding new predicates and counting the support of predicate paths. Algorithm 2 outlines the detailed steps of the pathGrowth algorithm.\nThe function findCycles in Algorithm 1 (line 18) finds a set of FPCs from the discovered FPPs. For a FPP \u03b8 = (x1, p d1 i1 , ..., xk+1), the following steps are performed to decide whether a FPC can be generated from it:\n\u2022 Find the set of instance paths I\u03b8 of \u03b8 in the KB, set the support of \u03b8 as a cycle supcycle(\u03b8) = 0.\n\u2022 For each path \u03c6 in I\u03b8 , if the first entity and the last entity in path \u03c6 are the same, supcycle(\u03b8) = supcycle(\u03b8) + 1.\n\u2022 If supcycle(\u03b8) is not less than the minimum support threshold, then a predicate cycle \u03b8\u2032 = (x1, p d1 i1 , ..., p dk ik , x1) is gen-\nerated by changing the last entity variable xk+1 to x1 in \u03b8; \u03b8\u2032 is kept as a FPC.\nAfter performing the above steps for each discovered FPP, the findCycles function obtains a set of FPCs, which is returned as the result of FPC mining algorithm.\nIn order to accelerate the mining process, our algorithm can run parallelly in a multi-core machine. The steps from line 3 to line 21 in Algorithm 1 can be executed independently for each starting predicate. If our algorithm runs on a machine with m cores, m threads can be created to find paths starting with m different predicates in parallel.\nDiscussion of the Support Measure\nAccording to Definition 5, the support of a predicate path equals to the number of its instance paths. This standard support measure, however, does not meet the Downward-Closure Property in frequent pattern mining, which requires that the support of a pattern must not exceed that of its sub-patterns. But in the problem of FPC mining, the support of a k-predicate path can be larger than that of its sub predicate paths. It is because many predicates in an RDF KB are not functional or inverse-functional2. For example, dbo : children predicate in DBpeida is not functional, one people can have more than one child; so it is very likely that predicate path (x1, dbo : spouse 1, x2, dbo : children 1, x3) has more instance paths than (x1, dbo : spouse1, x2) does.\nTherefore, extending FPPs by adding new frequent predicates can not ensure safely pruning of infrequent predicate paths. Some FPPs will also be pruned if we determine a predicate path is frequent or not based on the support measure in Definition 5. Actually, our approach can ensure finding all the FPPs if frequent is defined based on the following support measure:\nsupvar(\u03b8) = mini\u2208{1,...,k+1}(|\u03a0xi(\u03b8)|) (1)\nwhere \u03b8 = (x1, p d1 1 , x2, ..., p dk k , xk+1) is a k-predicate path, and \u03a0xi(\u03b8) be the set of entities that instantiate variable xi in the instance paths of \u03b8. It is easy to find that supvar satisfies the DownwardClosure Property; let sup(\u03b8) be the standard support defined in Definition 5, then we have supvar(\u03b8) \u2264 sup(\u03b8).\nBy the above discussion, we show that our approach actually finds FPPs having supvar no less than a given threshold; the discovered\n2For functional or inverse-functional predicates, please refer to http://www.w3.org/TR/owl-ref/#FunctionalProperty-def\nFPPs are also frequent in terms of the standard support measure defined in Definition 5.\nDealing with Duplicate FPCs\nIn the mining process of our approach, the same FPC may be generated from different FPPs. For example, given the following four different predicate paths:\n(x1, p 1 1, x2, p 1 2, x3), (x1, p 1 2, x2, p 1 1, x3)\n(x1, p \u22121 1 , x2, p \u22121 2 , x3), (x1, p \u22121 2 , x2, p \u22121 1 , x3)\nwe can generate the following predicate cycles from them:\n(x1, p 1 1, x2, p 1 2, x1), (x1, p 1 2, x2, p 1 1, x1)\n(x1, p \u22121 1 , x2, p \u22121 2 , x1), (x1, p \u22121 2 , x2, p \u22121 1 , x1)\nActually, they represent the same predicate cycle shown in Figure 5. To avoid produce duplicate rules, duplicate FPCs should be first detected and eliminated. So a predicate cycle normalization method is used to ensure one predicate cycle can have only one unique representation. To achieve that, we first assign each predicate in the KB a unique id. Then for a predicate path, the predicate pmin having the minimum id in it is found and taken as the first predicate in the representation, and the direction of pmin is set to 1; the second predicate is the next one follows the forward direction of pmin, and so forth. Assuming the subscript index of a predicate is its id, the predicate cycle in Figure 5 has a unique representation as (x1, p11, x2, p 1 2, x1) according to our normalization method.\nIt is very important to remove duplicate FPCs before generating rules. Because evaluating rules is a time consuming task, excluding redundant FPCs enables our approach to run more efficiently."}, {"heading": "4. RULE GENERATION AND EVALUATION", "text": ""}, {"heading": "4.1 Generate Rules from FPCs", "text": "Once all the FPCs \u0398 = {\u03b81, ..., \u03b8n} are found by Algorithm 1, rules are generated from them. As discussed in Section 2, one FPC having k predicates can generates k rules. Formally, for a FPC \u03b8 = (x1, p d1 1 , x2, p d2 2 , ..., p dk k , x1), the jth rule generated from it is\n\u2227\ni\u2208[1,k],i6=j\n\u3008xi, p di i , xi+1\u3009 \u21d2 \u3008xj , p dj j , xj+1\u3009 (2)\nFor example, a FPC (x1, hasParent1, x2, hasChildren\u22121, x1) can generate the following two rules:\n\u3008x1, hasParent,x2\u3009 \u21d2 \u3008x2, hasChildren, x1\u3009\n\u3008x2, hasChildren, x1\u3009 \u21d2 \u3008x1, hasParent, x2\u3009\nIn some cases, one predicate may appear multiple times in a FPC, duplicate rules might be generated from it. In the example shown in Figure 3(b), we can generate three rules from this 3-predicate cycle, but two of them are logically the same. Our approach will detect this problem and filter out duplicate rules."}, {"heading": "4.2 Add Type Information", "text": "Adding entity type information to rules can produce more accurate rules. For example, without entity type information, we may get rules like \u3008x1, bornIn, x2\u3009 \u21d2 \u3008x1, diedIn, x2\u3009. Based on this rule, if we already know that someone was born in someplace, then we can predict that this man also died in the same place. However, if the entity instantiating x2 is a small town, the prediction of this rule is prone to be incorrect, because most people would not stay in the same town in their whole lives. If the entity instantiating x2 becomes a country, the prediction will have a higher probability to be correct. Based on this observation, the following rule is more preferable.\n\u3008x1, bornIn, x2\u3009 \u2227 \u3008x1, typeOf, People\u3009\n\u2227\u3008x2, typeOf,Country\u3009 \u21d2 \u3008x1, dieIn, x2\u3009\nTo generate rules with entity type information, we propose an algorithm to find frequent types for FPCs and adds type information in FPCs, which is outlined in Algorithm 3. Given a FPC \u03b8, our algorithm first finds the frequent types of entities for each variable xi (line 4-17 in Algorithm 3). Then triples like \u3008xi, typeOf, type\u3009 are generated as type constraint for variable xi (line 21-25 in Algorithm 3). At last, type constraints for all the variables are combined and added to the original FPC \u03b8, resulting in a number of new FPCs having type constraints (line 27-30 in Algorithm 3). Two points in our algorithm should be noticed:\n\u2022 A \u3008xi, typeOf, Thing\u3009 triple is taken as a possible type constraint for xi in default; Thing is the most general type, any entity is an instance of Thing. If this constraint is selected and put over xi, it actually makes no different for xi.\n\u2022 For each variable xi, there could be lots of frequent types, especially when many entities have more than one type. In order to avoid generating too many constraint combinations, we only keep the top-k frequent types for each variable. k is a parameter and needs to be set manually.\nBy using Algorithm 3, we may get a FPC with type information as shown in Figure 6. When we generate rules from a FPC with type information, Equation 2 is first used to get rules without type information; then type constraints in the FPC are added in the body of each rule. For example, the FPC in Figure 6 can produce the rule which is previously presented in this section. In RDF2Rules, rules with and without type information will all be generated."}, {"heading": "4.3 Rule Evaluation", "text": "The measure of support defined in Section 2 measures how frequent a pattern appears in the concerned KB. The support of a rule is equal to the support of its corresponding FPC, i.e. the number of all possible instantiations of the variables in the rule. However, rules with high support may also make inaccurate predications. To evaluate how accurate a rule is, we can use the confidence measure which is used in association rule mining.\nAlgorithm 3: Add type information to FPCs\nInput: A FPC \u03b8 = (x1, pd11 , x2, p d2 2 , ..., p dk k , x1), the minimum\nsupport count \u03c4 , the maximum number of kept types for each variable k\nOutput: A set of FPCs with type information \u0398\u0304\n1 Set \u0398\u0304 = \u2205; 2 Get all the instance cycles of \u03b8; 3 for i = 1, ..., k do 4 Set Ci = \u2205; 5 Get \u03a0xi(\u03b8), the set of entities instantiating variable xi 6 for each entity e \u2208 \u03a0xi(\u03b8) do 7 Get the set of types Type(e) of e 8 for each type t \u2208 Type(e) do 9 if t \u2208 Ci then\n10 t.count = t.count + 1; 11 else 12 t.count = 1; 13 Ci = Ci \u222a {t}; 14 end 15 end 16 end 17 Remove types from Ci whose counts are less than \u03c4 ; 18 if |Ci| > k then 19 Only keep the top-k frequent types in Ci; 20 end 21 Set Hi = {\u3008xi, typeOf, Thing\u3009}; 22 for each type t \u2208 Ci do 23 Genenrate a triple h = \u3008xi, typeOf, t\u3009; 24 Hi = Hi \u222a {h}; 25 end 26 end 27 for each h\u0304 \u2208 H1 \u00d7H2 \u00d7 ...\u00d7Hk do 28 Generate a FPC with type information \u03b8\u0304 = \u03b8 \u2295 h\u0304; 29 \u0398\u0304 = \u0398\u0304 \u222a {\u03b8\u0304}; 30 end 31 return \u0398\u0304\nconf(Rbody \u21d2 Rhead) = sup(Rbody \u21d2 Rhead)\nsup(Rbody) (3)\nIf we use this standard confidence to evaluate rules in RDF2Rules, it will treat all that facts (entity relations) that do not exist in the given KB as false ones. It is not suitable for the scenario of enriching knowledge in KBs, because KBs are incomplete and unknown facts can not be simply taken as incorrect. In order to evaluate rules under the Open World Assumption (OWA), AMIE [7] uses a PCA confidence measure, which is defined as Equation 4.\nconfpca(Rbody \u21d2 \u3008x, p, y\u3009)\n= sup(Rbody \u21d2 \u3008x, p, y\u3009)\nsuppca(Rbody \u2227 \u3008x, p, y\u3009)\n(4)\nwhere\nsuppca(Rbody \u2227 \u3008x, p, y\u3009)\n= |{(x, y)|\u2203z1, ..., zm, y \u2032 : (Rbody) \u2227 \u3008x, p, y\u3009}|. (5)\nPCA confidence is based on the assumption that if an entity x has relation p with other entities, i.e. \u3008x, p, y\u3009 \u2208 KB, y \u2208 Y , then all the relations p of entity x are contained in the KB. If a relation \u3008x, p, y \u2032 \u3009 is predicted while y \u2032\n/\u2208 Y , the new predicted relation will be treated as a false fact. If there is no relation p of x in the KB, then any predicated relation p of x is considered as a true fact. This assumption is also adopted in KnowledgeVault [6]. The PCA confidence works well for rules having function predicate in the rule heads, and also holds for predicates having high functionality [7]. However, PCA confidence does not work well in some cases. For example, the following rule is mined by AMIE on YAGO23.\n\u3008x1, livesIn, x2\u3009 \u2227 \u3008x2, isLocatedIn, x3\u3009\n\u21d2 \u3008x1, isPoliticianOf, x3\u3009\nAccording to the evaluation results on the webpage of AMIE, this rule has a 85.38% PCA confidence, but only 13.33% predictions of this rule is correct. This is because that only a small number of people are politicians, it is not accurate to consider all the predicted isPoliticianOf relations of an entity e as true when e has no isPoliticianOf relations in the origin KB. Actually, if an entity is an instance of class Politician, then a new predicted isPoliticianOf relation about this entity is more likely to be true; otherwise, the new predicted fact might be wrong. Therefore, if we can estimate the probability of an entity having a specific relation of predicate, the confidence can be more accurately evaluated.\nLet P (e, p) denote the probability of entity e having a relation specified by p, we use the entity type information to estimate it, which is computed as\nP (e, p) = maxc\u2208Ce |Instp(c)|\n|Inst(c)| (6)\nwhere Ce is the set of types of e (one entity can have more than one types), Inst(c) is the set of instances of c, and Instp(c) is the set of instances of c that have relations of p. It is easy to find that 0 \u2264 P (e, p) \u2264 1. Based on Equation 6, we define a new confidence measure called soft confidence\nconfst(Rbody \u21d2 \u3008x, p, y\u3009)\n= sup(Rbody \u21d2 \u3008x, p, y\u3009)\nsup(Rbody)\u2212 \u2211\ne\u2208U P (e, p)\n(7)\nwhere U is the set of entities that previously have no relations of p, but have new predicted relations of p by the rule. This new 3This rule is from the webpage of AMIE, http://resources.mpiinf.mpg.de/yago-naga/amie/\nconfidence can be computed when the entity type information is available, which can evaluate rules more accurately. We will compare our new confidence with both standard confidence and PCA confidence in the experiments."}, {"heading": "5. RDF INDEXING FOR MINING ALGORITHM", "text": "In order to ensure the efficiency of our approach, we propose to use a in-memory indexing structure to support the mining algorithm instead of using the existing RDF storage systems. Generally, there are three types of queries over the RDF graph in our mining algorithm:\n(1) Given a predicate, find all the entity pairs that it connects;\n(2) Given an entity, find all its incident edges and its neighbor entities;\n(3) Given a predicate path, find all of its instances path.\nThe query of the first type is used to generate frequent 1-predicate paths in Algorithm 1. The second one is used in the predicate path growth process. The third one is used for counting the supports of predicate paths and finding predicate cycles. In order to support the above queries, two indexes are used in our approach, the PredicateEntity-Entity index and the Entity-Predicate-Entity index.\n\u2022 The Predicate-Entity-Entity index uses all the predicates as keys, and entity pairs connected by predicates as valules. Figure 7(a) shows a general example entry of the PredicateEntity-Entity index; a predicate key pi is associated to a vector of ki entity pairs; an entity pair \u3008eij1, e i j2\u3009 in the vector\ncorresponds to an edge \u3008eij1, pi, e i j2\u3009 in the RDF graph. The Predicate-Entity-Entity index is used for queries of type 1.\n\u2022 The Entity-Predicate-Entity index maps each entity to a subindex, where the keys are the predicates and the values are vectors of entities. Figure 7(b) shows an example entry of the Entity-Predicate-Entity index; an entity key ei is mapped to a vector of 2ki predicate keys \u3008p1i1, p \u22121 i1 , ..., p 1 iki , p\u22121iki\u3009;\neach predicate key p1ij or p \u22121 ij is linked to a vector of entities, which are connected to ei by p1ij or p \u22121 ij . The EntityPredicate-Entity index is used for queries of type 2 and 3."}, {"heading": "6. EXPERIMENTS", "text": ""}, {"heading": "6.1 Experiment Setup", "text": "Datasets. We evaluate our approach on YAGO2 and DBpedia 2014 (English version). YAGO2 is an extension of YAGO, which is built automatically from Wikipedia, GeoNames, and WordNet. DBpedia is a large-scale RDF KB, which is built by extracting structured content from the information contained the Wikipedia. Details of the used datasets are outlined in Table 1. Entity types of YAGO2 are obtained from YAGO3 [13] (the latest version of YAGO), as Gal\u00e1rraga et al. did in their work [8]. For DBpedia 2014, we use the dataset of mapping based properties, entity types are from the DBpedia ontology. The third column in Table 1 are the numbers of facts excluding the rdf:type statements. For each used KB, one dataset without entity types and another dataset with entity types are generated, which are used for AMIE+ separately in experiments. RDF2Rules always takes the dataset with types as input, but whether learning rules with types or not can be controlled.\npi \u3008ei11, e i 12\u3009 \u3008e i 21, e i 22\u3009 ... \u3008e i ki1 , eiki2\u3009\n(a) Predicate-Entity-Entity Index\nSettings. According to the experimental results reported in [8], AMIE+ outperforms both AMIE and several state-of-the-art ILP approaches. So we just compare our approach to AMIE+ in the experiments. All the experiments are run on a server with two 6- Core CPUs (Intel Xeon 2.4GHz) and 48 GB RAM, the operation system is Ubuntu 14.04. In all the experiments, the parameter k in Algorithm 3 is set to 1 for RDF2Rules; we set all the thresholds on confidence measures to 0 for RDF2Rules to let it output all the learned rules. For AMIE+, except for the minimum support and the max depth (i.e. maximum number of predicates) of rules, all its parameters are set to its default values (head coverage threshold minHC = 0.01, confidence threshold minConf = 0, PCA confidence threshold minpca = 0, etc.); the number of threads that AMIE+ uses is set to the actual number of cores."}, {"heading": "6.2 Mining Efficiency Analysis", "text": "In this sub-section, YAGO2 are used to evaluate the efficiency of RDF2Rules. The running time and the number of learned rules of RDF2Rules are compared with those of AMIE+. YAGO2 is a relatively small KB, so we set the support threshold to 50 for both RDF2Rules and AMIE+. We also let both systems run with three different maximum rule lengths, i.e. 2, 3, and 4. The experimental results are outlined in Table 2.\nRuntime. When the entity type information is not taken into account, both AMIE+ and RDF2Rules can learn rules very quickly. As shown in Table 2, RDF2Rules runs faster than AMIE+ does; as the maximum length of rules increases, the efficiency superiority of RDF2Rules is more obvious. When the rules of length 2\nare learned, AMIE+ takes about 1 more second than RDF2Rules; when the maximum length of rules is 4, AMIE+ takes about twice the time that RDF2Rules takes. This is because RDF2Rules first discovers FPPs and then generates rules, a FPP of length k can generate k rules. So when longer rules are to be learned, RDF2Rules has more advantage over AMIE+ in terms of running time.\nWhen the entity type information is considered in rule learning process, longer time is consumed for both approaches. But for AMIE+, it can not finish mining in 2 days when the maximum length of rules is 3 or 4. And we find that none of the rules learned by AMIE+ contain type constraints, which are the just the same rules as learned on the dataset without entity types. There is not a parameter of AMIE+ that controls using types or not. We just follow the method that is used in [8] to allow AMIE+ to learn rules with types, i.e. augmenting YAGO2 dataset by adding the rdf:type statements. But AMIE+ can not return rules with types in our experiments, adding the rdf:type statements can only augment the dataset and slow AMIE+ down. RDF2Rules can get rules with type constraints in acceptable time for different maximum rule lengths. When the maximum length of rule is set to 4, RDF2Rules finds more than 8 thousand rules and 6,585 of them are rules with types. Table 3 lists some examples of these rules.\nNumber of Rules. As for the number of rules, RDF2Rules always gets more rules than AMIE+ does given the same maximum rule length. This is because AMIE+ uses a measure called head coverage to prune the search space, which requires the rules covering a certain ratio of facts of the predicate in rule head. AMIE+ also has a relation size threshold to filter out rules with small sized rule head (i.e. the number of facts of predicate in the rule head is small). RDF2Rules uses different pruning strategy from AMIE+ does, there is no threshold on head coverage or the size of rule head, so more rules are searched and returned by RDF2Rules. And the extra rules found by RDF2Rules are also useful for predicting new facts."}, {"heading": "6.3 Confidence Measure Evaluation", "text": "We propose a new confidence measure call soft confidence in Section 4.3. Table 5 and Table 6 list some rules learned by AMIE+ and RDF2Rules from YAGO2 respectively. Rules in Table 5 are the top-10 rules with highest PCA confidences, and rules in Table 6 are the top-10 rules with highest soft confidence. We find that the second and third rules in Table 5 predicate isPoliticianOf facts; as we discussed in Section 4.3, there might be many people having\ndiedIn and isLocatedIn relations, but only a small number of them are politicians. So most predictions of these two rules would be wrong. Using soft confidence, the above two rules are penalized and there are not in the top-10 list.\nIn order to further evaluate the effectiveness of our new confidence measure, we let RDF2Rules and AMIE+ learn rules from DBpedia KB; we select the top rules returned by two approaches, and use them to predict new facts. The quality of new predicted facts can indirectly reflect the reliability of confidence measures. To perform the experiment, we randomly selected 40% triples from DBpedia dataset as test data, and then removed the test triples before the DBpedia data is fed to RDF2Rules and AMIE+. The support threshold is set to 500 for both two approaches, and only rules of length 2 are learned. Since AMIE+ fails to learn rules with types in the former experiments, we only let RDF2Rules learn rules with types. The top-500 rules are selected each time to predict new facts.\nTable 4 shows the results. When rules with no types are learned, AMIE+ and RDF2Rules take very close time; but RDF2Rules generates more rules than AMIE+. When rules with types are learned, RDF2Rules takes about 25 minutes and gets more than 8 thousand rules. The sixth column of Table 4 lists the number of predicated new facts by the top-500 rules; and the last column shows that number of new facts that are found in the holdout test data. Although rules of AMIE+ generate more predictions, rules of RDF2Rules get more facts found in the test data. It seems that soft confidence can evaluate rules more precisely, which generate more accurate predictions. Comparing rules without types and with types learned by RDF2Rules, we also find that adding type information in rules results in less predictions but more hits in the test data."}, {"heading": "7. RELATED WORK", "text": "As mentioned above, AMIE [7] is the most related work to our approach. AMIE mainly focuses on how to evaluate rules under the Open World Assumption, and its searching strategy becomes inefficient when dealing with large-scale KBs and long rules. AMIE learns one rule at a time by gradually adding new atoms to the rule body, while our approach first mines Frequent Predicate Paths and then generates several rules from each Frequent Predicate Path. Most recently, AMIE has been Extended to AMIE+ by a series of improvements to make it more efficient [8]. Lots of work has been done in the domain of Inductive Logic Programming to learn first-order Horn clauses from KBs. The learned rules can also be used for inferring new facts in KBs. Typical ILP systems, such as FOIL [21] and Progol [16], need a set of training examples that con-\ntain both positive and negative examples of the target concepts or relations. Although ILP systems such as [17] are proposed to learn rules from only positive examples, the main problem with these approaches is the low efficiency when dealing with large-scale KBs.\nBesides using rules to infer new facts in KB, there are also some other methods proposed for enriching an existing KB. Nickel et al. proposed a tensor factorization method RESCAL [19] for relational learning; RESCAL represents entities as low dimensional vectors and relations by low rank matrices, which are learned using a collective learning process. RESCAL has been applied to YAGO and it can predict unknown facts with high accuracy [20]; and there is also an improved work on RESCAL [15]. Bordes et al. proposed a series of embedding approaches of KB that predict new facts from the existing ones [5, 3, 4]; these approaches embed entities and relations in a KB into a continuous vector space while preserving the original knowledge; new facts are predicted by manipulating vectors and matrices. Socher et al. [23] proposed to use neural tensor network for reasoning over relationships between entities in KBs. Both tensor factorization approaches and embedding approaches all need to learn large number of parameters, which is very time consuming; and what these approaches learned is difficult for human experts to understand; however, they can be valuable complementation of the rule based approaches for enriching new facts in KB. Because the goal of this work is to efficiently learn inference rules from KB and accurately evaluate the learned rules, we do not compare our approaches with the above mentioned ones in the experiments.\nThere are also some work that uses similar structures as predicate path to predict new facts in knowledge bases, such as [10, 14]. But these work focus on how to accurately predicate relations based on multiple predicate/relation paths. How to effectively discover useful predicate paths are not discussed in these work. Our work focus on how to learn frequent predicate paths and use them to generate rules; the paths discovered by our approach can be used as input for the above two approaches. Most recently, there have been some work try to combine logic rules and knowledge embedding to predict new facts, such as [26, 12]. These work also do not focus on how to learn rules, but on how to use rules to make accurate predictions. So rules learned by our approach can also used in these approaches."}, {"heading": "8. CONCLUSION", "text": "In this paper, we propose a novel rule learning approach RDF2Rules for RDF KBs. Rules are learned by finding frequent predicate cycles in RDF graphs. A new confidence measure is also proposed for evaluating the reliability of the mined rules. Experiments show that our approach outperforms the compared approach in terms of both the quality of predictions and the running time."}, {"heading": "9. ACKNOWLEDGEMENTS", "text": "The work is supported by NSFC (No. 61202246), NSFC-ANR(No. 61261130588), and the Fundamental Research Funds for the Central Universities (2013NT56)."}, {"heading": "10. REFERENCES", "text": "[1] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,\nR. Cyganiak, and S. Hellmann. Dbpedia-a crystallization\npoint for the web of data. Web Semantics: Science, Services and Agents on the World Wide Web, 7(3):154\u2013165, 2009.\n[2] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247\u20131250. ACM, 2008.\n[3] A. Bordes, X. Glorot, J. Weston, and Y. Bengio. A semantic matching energy function for learning with multi-relational data. Machine Learning, 94(2):233\u2013259, 2014.\n[4] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, pages 2787\u20132795, 2013.\n[5] A. Bordes, J. Weston, R. Collobert, Y. Bengio, et al. Learning structured embeddings of knowledge bases. In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI 2015), pages 301\u2013306, 2011.\n[6] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy, T. Strohmann, S. Sun, and W. Zhang. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 601\u2013610. ACM, 2014.\n[7] L. A. Gal\u00e1rraga, C. Teflioudi, K. Hose, and F. Suchanek. Amie: association rule mining under incomplete evidence in ontological knowledge bases. In Proceedings of the 22nd international conference on World Wide Web, pages 413\u2013422. International World Wide Web Conferences Steering Committee, 2013.\n[8] L. Gal\u00c3a\u0328rraga, C. Teflioudi, K. Hose, and F. Suchanek. Fast rule mining in ontological knowledge bases with amie+. The VLDB Journal, pages 1\u201324, 2015.\n[9] J. Hoffart, F. M. Suchanek, K. Berberich, and G. Weikum. Yago2: A spatially and temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194(0):28 \u2013 61, 2013.\n[10] N. Lao, T. Mitchell, and W. W. Cohen. Random walk inference and learning in a large scale knowledge base. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911, pages 529\u2013539, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics.\n[11] J. P. Leal, V. Rodrigues, and R. Queir\u00f3s. Computing semantic relatedness using dbpedia. In OASIcs-OpenAccess Series in Informatics, volume 21, 2012.\n[12] Y. Lin, Z. Liu, H. Luan, M. Sun, S. Rao, and S. Liu. Modeling relation paths for representation learning of knowledge bases. In Proceedings of the 15th Conference on Empirical Methods in Natural Language Processing, pages 705\u2013714, 2015.\n[13] F. Mahdisoltani, J. Biega, and F. Suchanek. Yago3: A knowledge base from multilingual wikipedias. In 7th Biennial Conference on Innovative Data Systems Research.\nCIDR 2015, 2014. [14] C. Meng, R. Cheng, S. Maniu, P. Senellart, and W. Zhang.\nDiscovering meta-paths in large heterogeneous information networks. In 24th International World Wide Web Conference, pages 754\u2013764, 2015.\n[15] P. Minervini, C. d\u00e2A\u0306Z\u0301Amato, N. Fanizzi, and F. Esposito. A gaussian process model for knowledge propagation in web ontologies. In IEEE International Conference on Data Mining, pages 929\u2013934, 2014.\n[16] S. Muggleton. Inverse entailment and progol. New Generation Computing, 13(3-4):245\u2013286, 1995.\n[17] S. Muggleton. Learning from positive data. In Inductive logic programming, pages 358\u2013376. Springer, 1997.\n[18] S. Muggleton and L. D. Raedt. Inductive logic programming: Theory and methods. JOURNAL OF LOGIC PROGRAMMING, 19(20):629\u2013679, 1994.\n[19] M. Nickel, V. Tresp, and H.-P. Kriegel. A Three-Way Model for Collective Learning on Multi-Relational Data. In Proceedings of the 28th International Conference on Machine Learning (ICML 2011), pages 809\u2013816, 2011.\n[20] M. Nickel, V. Tresp, and H.-P. Kriegel. Factorizing yago: Scalable machine learning for linked data. In Proceedings of the 21st International Conference on World Wide Web, WWW \u201912, pages 271\u2013280, New York, NY, USA, 2012. ACM.\n[21] J. R. Quinlan. Learning logical definitions from relations. Mach. Learn., 5(3):239\u2013266, Sept. 1990.\n[22] W. Shen, J. Wang, P. Luo, and M. Wang. Linden: linking named entities with knowledge base via semantic knowledge. In Proceedings of the 21st international conference on World Wide Web, pages 449\u2013458. ACM, 2012.\n[23] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning with neural tensor networks for knowledge base completion. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 926\u2013934. Curran Associates, Inc., 2013.\n[24] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203\u2013217, 2008.\n[25] C. Unger, L. B\u00fchmann, J. Lehmann, A.-C. Ngonga Ngomo, D. Gerber, and P. Cimiano. Template-based question answering over rdf data. In Proceedings of the 21st International Conference on World Wide Web, WWW \u201912, pages 639\u2013648, New York, NY, USA, 2012. ACM.\n[26] Q. Wang, B. Wang, and L. Guo. Knowledge base completion using embeddings and rules. In Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI\u201915, pages 1859\u20131865. AAAI Press, 2015."}], "references": [{"title": "Dbpedia-a crystallization  point for the web of data", "author": ["C. Bizer", "J. Lehmann", "G. Kobilarov", "S. Auer", "C. Becker", "R. Cyganiak", "S. Hellmann"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Amie: association rule mining under incomplete evidence in ontological knowledge bases", "author": ["L.A. Gal\u00e1rraga", "C. Teflioudi", "K. Hose", "F. Suchanek"], "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Fast rule mining in ontological knowledge bases with amie+", "author": ["L. Gal\u00c3\u0105rraga", "C. Teflioudi", "K. Hose", "F. Suchanek"], "venue": "The VLDB Journal,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Yago2: A spatially and temporally enhanced knowledge base from wikipedia", "author": ["J. Hoffart", "F.M. Suchanek", "K. Berberich", "G. Weikum"], "venue": "Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["N. Lao", "T. Mitchell", "W.W. Cohen"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Computing semantic relatedness using dbpedia", "author": ["J.P. Leal", "V. Rodrigues", "R. Queir\u00f3s"], "venue": "In OASIcs-OpenAccess Series in Informatics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Y. Lin", "Z. Liu", "H. Luan", "M. Sun", "S. Rao", "S. Liu"], "venue": "In Proceedings of the 15th Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Yago3: A knowledge base from multilingual wikipedias", "author": ["F. Mahdisoltani", "J. Biega", "F. Suchanek"], "venue": "In 7th Biennial Conference on Innovative Data Systems Research", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Discovering meta-paths in large heterogeneous information networks", "author": ["C. Meng", "R. Cheng", "S. Maniu", "P. Senellart", "W. Zhang"], "venue": "In 24th International World Wide Web Conference,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A gaussian process model for knowledge propagation in web ontologies", "author": ["P. Minervini", "C. d\u00e2\u0102\u0179Amato", "N. Fanizzi", "F. Esposito"], "venue": "In IEEE International Conference on Data Mining,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Inverse entailment and progol", "author": ["S. Muggleton"], "venue": "New Generation Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Learning from positive data", "author": ["S. Muggleton"], "venue": "In Inductive logic programming,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Inductive logic programming: Theory and methods", "author": ["S. Muggleton", "L.D. Raedt"], "venue": "JOURNAL OF LOGIC PROGRAMMING,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1994}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Factorizing yago: Scalable machine learning for linked data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In Proceedings of the 21st International Conference on World Wide Web,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Learning logical definitions from relations", "author": ["J.R. Quinlan"], "venue": "Mach. Learn.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1990}, {"title": "Linden: linking named entities with knowledge base via semantic knowledge", "author": ["W. Shen", "J. Wang", "P. Luo", "M. Wang"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Yago: A large ontology from wikipedia and wordnet", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Template-based question answering over rdf data", "author": ["C. Unger", "L. B\u00fchmann", "J. Lehmann", "A.-C. Ngonga Ngomo", "D. Gerber", "P. Cimiano"], "venue": "In Proceedings of the 21st International Conference on World Wide Web, WWW", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Knowledge base completion using embeddings and rules", "author": ["Q. Wang", "B. Wang", "L. Guo"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Recently, a growing number of large-scale Knowledge Bases (KBs) have been created and published by using the Resource Description Framework (RDF), such as DBpedia [1], YAGO [24], and Freebase [2] etc.", "startOffset": 176, "endOffset": 179}, {"referenceID": 23, "context": "INTRODUCTION Recently, a growing number of large-scale Knowledge Bases (KBs) have been created and published by using the Resource Description Framework (RDF), such as DBpedia [1], YAGO [24], and Freebase [2] etc.", "startOffset": 186, "endOffset": 190}, {"referenceID": 1, "context": "INTRODUCTION Recently, a growing number of large-scale Knowledge Bases (KBs) have been created and published by using the Resource Description Framework (RDF), such as DBpedia [1], YAGO [24], and Freebase [2] etc.", "startOffset": 205, "endOffset": 208}, {"referenceID": 24, "context": "These KBs contain not only huge number of entities but also rich entity relations, which makes them successfully used in many applications such as Question Answering [25], Semantic Relatedness Computation [11] and Entity Linking[22].", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "These KBs contain not only huge number of entities but also rich entity relations, which makes them successfully used in many applications such as Question Answering [25], Semantic Relatedness Computation [11] and Entity Linking[22].", "startOffset": 205, "endOffset": 209}, {"referenceID": 21, "context": "These KBs contain not only huge number of entities but also rich entity relations, which makes them successfully used in many applications such as Question Answering [25], Semantic Relatedness Computation [11] and Entity Linking[22].", "startOffset": 228, "endOffset": 232}, {"referenceID": 8, "context": "It is reported that the new version of YAGO used logic rules to deduce new facts from the existing ones [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 17, "context": "Learning rules from KBs has been studied for years in the domain of Inductive Logic Programming (ILP) [18], but ILP approaches usually need negative facts of target relations and typically do not scale well on large-scale KBs.", "startOffset": 102, "endOffset": 106}, {"referenceID": 6, "context": "have proposed a system AMIE for learning rules from RDF data [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "Most recently, AMIE has been Extended to AMIE+ by a series of improvements to make it more efficient [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "In [8], the authors do discuss adding types in rules, but how to automatically learn rules with types is not detailedly explained in the paper; and according to our experiments, the released AMIE+ tool can not learn rules with type information.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In order to evaluate rules under the Open World Assumption (OWA), AMIE [7] uses a PCA confidence measure, which is defined as Equation 4.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "This assumption is also adopted in KnowledgeVault [6].", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "The PCA confidence works well for rules having function predicate in the rule heads, and also holds for predicates having high functionality [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 12, "context": "Entity types of YAGO2 are obtained from YAGO3 [13] (the latest version of YAGO), as Gal\u00e1rraga et al.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "did in their work [8].", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "According to the experimental results reported in [8], AMIE+ outperforms both AMIE and several state-of-the-art ILP approaches.", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "We just follow the method that is used in [8] to allow AMIE+ to learn rules with types, i.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "RELATED WORK As mentioned above, AMIE [7] is the most related work to our approach.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "Most recently, AMIE has been Extended to AMIE+ by a series of improvements to make it more efficient [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 20, "context": "Typical ILP systems, such as FOIL [21] and Progol [16], need a set of training examples that conTable 5: Top-10 Rules learned by AMIE+ from YAGO2 (according to PCA confidence, rule length\u2264 3) \u3008x1, isMarriedTo, x2\u3009 \u21d2 \u3008x2, isMarriedTo, x1\u3009 \u3008x1, diedIn, x2\u3009 \u2227 \u3008x2, isLocatedIn, x3\u3009 \u21d2 \u3008x1, isPoliticianOf, x3\u3009 \u3008x1, isLocatedIn, x2\u3009 \u2227 \u3008x3, livesIn, x1\u3009 \u21d2 \u3008x3, isPoliticianOf, x2\u3009 \u3008x1, hasOfficialLanguage, x2\u3009 \u2227 \u3008x3, isLocatedIn, x1\u3009 \u21d2 \u3008x3, hasOfficialLanguage, x2\u3009 \u3008x1, isMarriedTo, x2\u3009 \u2227 \u3008x2, livesIn, x3\u3009 \u21d2 \u3008x1, livesIn, x3\u3009 \u3008x1, isMarriedTo, x2\u3009 \u2227 \u3008x1, livesIn, x3\u3009 \u21d2 \u3008x2, livesIn, x3\u3009 \u3008x1, hasOfficialLanguage, x2\u3009 \u2227 \u3008x1, isLocatedIn, x3\u3009 \u21d2 \u3008x3, hasOfficialLanguage, x2\u3009 \u3008x1, created, x2\u3009 \u2227 \u3008x1, produced, x2\u3009 \u21d2 \u3008x1, directed, x2\u3009", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "Typical ILP systems, such as FOIL [21] and Progol [16], need a set of training examples that conTable 5: Top-10 Rules learned by AMIE+ from YAGO2 (according to PCA confidence, rule length\u2264 3) \u3008x1, isMarriedTo, x2\u3009 \u21d2 \u3008x2, isMarriedTo, x1\u3009 \u3008x1, diedIn, x2\u3009 \u2227 \u3008x2, isLocatedIn, x3\u3009 \u21d2 \u3008x1, isPoliticianOf, x3\u3009 \u3008x1, isLocatedIn, x2\u3009 \u2227 \u3008x3, livesIn, x1\u3009 \u21d2 \u3008x3, isPoliticianOf, x2\u3009 \u3008x1, hasOfficialLanguage, x2\u3009 \u2227 \u3008x3, isLocatedIn, x1\u3009 \u21d2 \u3008x3, hasOfficialLanguage, x2\u3009 \u3008x1, isMarriedTo, x2\u3009 \u2227 \u3008x2, livesIn, x3\u3009 \u21d2 \u3008x1, livesIn, x3\u3009 \u3008x1, isMarriedTo, x2\u3009 \u2227 \u3008x1, livesIn, x3\u3009 \u21d2 \u3008x2, livesIn, x3\u3009 \u3008x1, hasOfficialLanguage, x2\u3009 \u2227 \u3008x1, isLocatedIn, x3\u3009 \u21d2 \u3008x3, hasOfficialLanguage, x2\u3009 \u3008x1, created, x2\u3009 \u2227 \u3008x1, produced, x2\u3009 \u21d2 \u3008x1, directed, x2\u3009", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "Although ILP systems such as [17] are proposed to learn rules from only positive examples, the main problem with these approaches is the low efficiency when dealing with large-scale KBs.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "proposed a tensor factorization method RESCAL [19] for relational learning; RESCAL represents entities as low dimensional vectors and relations by low rank matrices, which are learned using a collective learning process.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "RESCAL has been applied to YAGO and it can predict unknown facts with high accuracy [20]; and there is also an improved work on RESCAL [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "RESCAL has been applied to YAGO and it can predict unknown facts with high accuracy [20]; and there is also an improved work on RESCAL [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "proposed a series of embedding approaches of KB that predict new facts from the existing ones [5, 3, 4]; these approaches embed entities and relations in a KB into a continuous vector space while preserving the original knowledge; new facts are predicted by manipulating vectors and matrices.", "startOffset": 94, "endOffset": 103}, {"referenceID": 2, "context": "proposed a series of embedding approaches of KB that predict new facts from the existing ones [5, 3, 4]; these approaches embed entities and relations in a KB into a continuous vector space while preserving the original knowledge; new facts are predicted by manipulating vectors and matrices.", "startOffset": 94, "endOffset": 103}, {"referenceID": 3, "context": "proposed a series of embedding approaches of KB that predict new facts from the existing ones [5, 3, 4]; these approaches embed entities and relations in a KB into a continuous vector space while preserving the original knowledge; new facts are predicted by manipulating vectors and matrices.", "startOffset": 94, "endOffset": 103}, {"referenceID": 22, "context": "[23] proposed to use neural tensor network for reasoning over relationships between entities in KBs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "There are also some work that uses similar structures as predicate path to predict new facts in knowledge bases, such as [10, 14].", "startOffset": 121, "endOffset": 129}, {"referenceID": 13, "context": "There are also some work that uses similar structures as predicate path to predict new facts in knowledge bases, such as [10, 14].", "startOffset": 121, "endOffset": 129}, {"referenceID": 25, "context": "Most recently, there have been some work try to combine logic rules and knowledge embedding to predict new facts, such as [26, 12].", "startOffset": 122, "endOffset": 130}, {"referenceID": 11, "context": "Most recently, there have been some work try to combine logic rules and knowledge embedding to predict new facts, such as [26, 12].", "startOffset": 122, "endOffset": 130}], "year": 2015, "abstractText": "Recently, several large-scale RDF knowledge bases have been built and applied in many knowledge-based applications. To further increase the number of facts in RDF knowledge bases, logic rules can be used to predict new facts based on the existing ones. Therefore, how to automatically learn reliable rules from large-scale knowledge bases becomes increasingly important. In this paper, we propose a novel rule learning approach named RDF2Rules for RDF knowledge bases. RDF2Rules first mines frequent predicate cycles (FPCs), a kind of interesting frequent patterns in knowledge bases, and then generates rules from the mined FPCs. Because each FPC can produce multiple rules, and effective pruning strategy is used in the process of mining FPCs, RDF2Rules works very efficiently. Another advantage of RDF2Rules is that it uses the entity type information when generates and evaluates rules, which makes the learned rules more accurate. Experiments show that our approach outperforms the compared approach in terms of both efficiency and accuracy.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}