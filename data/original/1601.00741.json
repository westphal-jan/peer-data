{"id": "1601.00741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "Learning Preferences for Manipulation Tasks from Online Coactive Feedback", "abstract": "We consider the problem of learning preferences over trajectories for mobile manipulators such as personal robots and assembly line robots. The preferences we learn are more intricate than simple geometric constraints on trajectories; they are rather governed by the surrounding context of various objects and human interactions in the environment. We propose a coactive online learning framework for teaching preferences in contextually rich environments. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this coactive preference feedback can be more easily elicited than demonstrations of optimal trajectories. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms.", "histories": [["v1", "Tue, 5 Jan 2016 05:47:09 GMT  (16943kb,D)", "http://arxiv.org/abs/1601.00741v1", "IJRR accepted (Learning preferences over trajectories from coactive feedback)"]], "COMMENTS": "IJRR accepted (Learning preferences over trajectories from coactive feedback)", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["ashesh jain", "shikhar sharma", "thorsten joachims", "ashutosh saxena"], "accepted": false, "id": "1601.00741"}, "pdf": {"name": "1601.00741.pdf", "metadata": {"source": "CRF", "title": "Learning Preferences for Manipulation Tasks from Online Coactive Feedback", "authors": ["Ashesh Jain", "Shikhar Sharma", "Thorsten Joachims", "Ashutosh Saxena"], "emails": ["asaxena}@cs.cornell.edu"], "sections": [{"heading": null, "text": "We implement our algorithm on two high degree-of-freedom robots, PR2 and Baxter, and present three intuitive mechanisms for providing such incremental feedback. In our experimental evaluation we consider two context rich settings \u2013 household chores and grocery store checkout \u2013 and show that users are able to train the robot with just a few feedbacks (taking only a few minutes).1\nI. INTRODUCTION Recent advances in robotics have resulted in mobile manipulators with high degree of freedom (DoF) arms. However, the use of high DoF arms has so far been largely successful only in structured environments such as manufacturing scenarios, where they perform repetitive motions (e.g., recent deployment of Baxter on assembly lines). One challenge in the deployment of these robots in unstructured environments (such as a grocery checkout counter or at our homes) is their lack of understanding of user preferences and thereby not producing desirable motions. In this work we address the problem of learning preferences over trajectories for high DoF robots such as Baxter or PR2. We consider a variety of household chores for PR2 and grocery checkout tasks for Baxter.\nA key problem for high DoF manipulators lies in identifying an appropriate trajectory for a task. An appropriate trajectory not only needs to be valid from a geometric point (i.e., feasible and obstacle-free, the criteria that most path planners focus on), but it also needs to satisfy the user\u2019s preferences. Such users\u2019 preferences over trajectories can be common across users or they may vary between users, between tasks, and between the environments the trajectory is performed in. For\n1Parts of this work has been published at NIPS and ISRR conferences [22, 21]. This journal submission presents a consistent full paper, and also includes the proof of regret bounds, more details of the robotic system, and a thorough related work.\nexample, a household robot should move a glass of water in an upright position without jerks while maintaining a safe distance from nearby electronic devices. In another example, a robot checking out a knife at a grocery store should strictly move it at a safe distance from nearby humans. Furthermore, straight-line trajectories in Euclidean space may no longer be the preferred ones. For example, trajectories of heavy items should not pass over fragile items but rather move around them. These preferences are often hard to describe and anticipate without knowing where and how the robot is deployed. This makes it infeasible to manually encode them in existing path planners (e.g., Zucker et al. [63], Sucan et al. [57], Schulman et al. [50]) a priori.\nIn this work we propose an algorithm for learning user preferences over trajectories through interactive feedback from the user in a coactive learning setting [51]. In this setting the robot learns through iterations of user feedback. At each iteration robot receives a task and it predicts a trajectory. The user responds by slightly improving the trajectory but not necessarily revealing the optimal trajectory. The robot use this feedback from user to improve its predictions for future iterations. Unlike in other learning settings, where a human first demonstrates optimal trajectories for a task to the robot [4], our learning model does not rely on the user\u2019s ability to demonstrate optimal trajectories a priori. Instead, our learning algorithm explicitly guides the learning process and merely requires the user to incrementally improve the robot\u2019s trajectories, thereby learning preferences of user and not the expert. From these interactive improvements the robot learns a general model of the user\u2019s preferences in an online fashion. We realize this learning algorithm on PR2 and Baxter robots, and also leverage robot-specific design to allow users to easily give preference feedback.\nOur experiments show that a robot trained using this approach can autonomously perform new tasks and if need be, only a small number of interactions are sufficient to tune the robot to the new task. Since the user does not have to demonstrate a (near) optimal trajectory to the robot, the feedback is easier to provide and more widely applicable. Nevertheless, it leads to an online learning algorithm with provable regret bounds that decay at the same rate as for optimal demonstration algorithms (eg. Ratliff et al. [45]).\nIn our empirical evaluation we learn preferences for two high DoF robots, PR2 and Baxter, on a variety of household and grocery checkout tasks respectively. We design expressive trajectory features and show how our algorithm learns pref-\nar X\niv :1\n60 1.\n00 74\n1v 1\n[ cs\n.R O\n] 5\nJ an\n2 01\n6\nerences from online user feedback on a broad range of tasks for which object properties are of particular importance (e.g., manipulating sharp objects with humans in the vicinity). We extensively evaluate our approach on a set of 35 household and 16 grocery checkout tasks, both in batch experiments as well as through robotic experiments wherein users provide their preferences to the robot. Our results show that our system not only quickly learns good trajectories on individual tasks, but also generalizes well to tasks that the algorithm has not seen before. In summary, our key contributions are:\n1) We present an approach for teaching robots which does not rely on experts\u2019 demonstrations but nevertheless gives strong theoretical guarantees. 2) We design a robotic system with multiple easy to elicit feedback mechanisms to improve the current trajectory. 3) We implement our algorithm on two robotic platforms, PR2 and Baxter, and support it with a user study. 4) We consider preferences that go beyond simple geometric criteria to capture object and human interactions. 5) We design expressive trajectory features to capture contextual information. These features might also find use in other robotic applications.\nIn the following section we discuss related works. In section III we describe our system and feedback mechanisms. Our learning algorithm and trajectory features are discussed in sections IV and IV, respectively. Section VI gives our experiments and results. We discuss future research directions and conclude in section VII."}, {"heading": "II. RELATED WORK", "text": "Path planning is one of the key problems in robotics. Here, the objective is to find a collision free path from a start to goal location. Over the last decade many planning algorithms have been proposed, such as sampling based planners by Lavalle and Kuffner [33], and Karaman and Frazzoli [26], search based planners by Cohen et al. [10], trajectory optimizers by Schulman et al. [50], and Zucker et al. [63] and many more [27]. However, given the large space of possible trajectories in most robotic applications simply a collision free trajectory might not suffice, instead the trajectory should satisfy certain constraints\nand obey the end user preferences. Such preferences are often encoded as a cost which planners optimize [26, 50, 63]. We address the problem of learning a cost over trajectories for context-rich environments, and from sub-optimal feedback elicited from non-expert users. We now describe related work in various aspects of this problem. Learning from Demonstration (LfD): Teaching a robot to produce desired motions has been a long standing goal and several approaches have been studied. In LfD an expert provides demonstrations of optimal trajectories and the robot tries to mimic the expert. Examples of LfD includes, autonomous helicopter flights [1], ball-in-a-cup game [30], planning 2-D paths [43, 44], etc. Such settings assume that kinesthetic demonstrations are intuitive to an end-user and it is clear to an expert what constitutes a good trajectory. In many scenarios, especially involving high DoF manipulators, this is extremely challenging to do [2].2 This is because the users have to give not only the end-effector\u2019s location at each time-step, but also the full configuration of the arm in a spatially and temporally consistent manner. In Ratliff et al. [45] the robot observes optimal user feedback but performs approximate inference. On the other hand, in our setting, the user never discloses the optimal trajectory or feedback, but instead, the robot learns preferences from sub-optimal suggestions for how the trajectory can be improved.\nNoisy demonstrations and other forms of user feedback: Some later works in LfD provided ways for handling noisy demonstrations, under the assumption that demonstrations are either near optimal, as in Ziebart et al. [62], or locally optimal, as in Levine et al. [36]. Providing noisy demonstrations is different from providing relative preferences, which are biased and can be far from optimal. We compare with an algorithm for noisy LfD learning in our experiments. Wilson et al. [60] proposed a Bayesian framework for learning rewards of a Markov decision process via trajectory preference queries. Our approach advances over [60] and Calinon et. al. [9] in\n2Consider the following analogy. In search engine results, it is much harder for the user to provide the best web-pages for each query, but it is easier to provide relative ranking on the search results by clicking.\nthat we model user as a utility maximizing agent. Further, our score function theoretically converges to user\u2019s hidden function despite recieving sub-optimal feedback. In the past, various interactive methods (e.g. human gestures) [8, 56] have been employed to teach assembly line robots. However, these methods required the user to interactively show the complete sequence of actions, which the robot then remembered for future use. Recent works by Nikolaidis et al. [39, 40] in human-robot collaboration learns human preferences over a sequence of sub-tasks in assembly line manufacturing. However, these works are agnostic to the user preferences over robot\u2019s trajectories. Our work could complement theirs to achieve better human-robot collaboration.\nLearning preferences over trajectories: User preferences over robot\u2019s trajectories have been studied in human-robot interaction. Sisbot et. al. [55, 54] and Mainprice et. al. [37] planned trajectories satisfying user specified preferences in form of constraints on the distance of the robot from the user, the visibility of the robot and the user\u2019s arm comfort. Dragan et. al. [14] used functional gradients [47] to optimize for legibility of robot trajectories. We differ from these in that we take a data driven approach and learn score functions reflecting user preferences from sub-optimal feedback.\nPlanning from a cost function: In many applications, the goal is to find a trajectory that optimizes a cost function. Several works build upon the sampling-based planner RRT [33] to optimize various cost heuristics [17, 11, 20]. Additive cost functions with Lipschitz continuity can be optimized using optimal planners such as RRT* [26]. Some approaches introduce sampling bias [35] to guide the sampling based planner. Recent trajectory optimizers such as CHOMP [47] and TrajOpt [50] provide optimization based approaches to finding optimal trajectory. Our work is complementary to these in that we learn a cost function while the above approaches optimize a cost.\nOur work is also complementary to few works in path planning. Berenson et al. [5] and Phillips et al. [41] consider the problem of trajectories for high-dimensional manipulators. For computational reasons they create a database of prior trajectories, which we could leverage to\ntrain our system. Other recent works consider generating human-like trajectories [15, 14, 58]. Humans-robot interaction is an important aspect and our approach could incorporate similar ideas.\nApplication domain: In addition to above mentioned differences we also differ in the applications we address. We capture the necessary contextual information for household and grocery store robots, while such context is absent in previous works. Our application scenario of learning trajectories for high DoF manipulations performing tasks in presence of different objects and environmental constraints goes beyond the application scenarios that previous works have considered. Some works in mobile robotics learn context-rich perception-driven cost functions, such as Silver et al. [53], Kretzschmar et al. [32] and Kitani et al. [28]. In this work we use features that consider robot configurations, object-object relations, and temporal behavior, and use them to learn a score function representing the preferences over trajectories."}, {"heading": "III. COACTIVE LEARNING WITH INCREMENTAL FEEDBACK", "text": "We first give an overview of our robot learning setup and then describe in detail three mechanisms of user feedback."}, {"heading": "A. Robot learning setup", "text": "We propose an online algorithm for learning preferences in trajectories from sub-optimal user feedback. At each step the robot receives a task as input and outputs a trajectory that maximizes its current estimate of some score function. It then observes user feedback \u2013 an improved trajectory \u2013 and updates the score function to better match the user preferences. This procedure of learning via iterative improvement is known as coactive learning. We implement the algorithm on PR2 and Baxter robots, both having two 7 DoF arms. In the process of training, the initial trajectory proposed by the robot can be far-off the desired behavior. Therefore, instead of directly executing trajectories in human environments, users first visualize them in the OpenRAVE simulator [13] and then decide the kind of feedback they would like to provide."}, {"heading": "B. Feedback mechanisms", "text": "Our goal is to learn even from feedback given by non-expert users. We therefore require the feedback only to be incrementally better (as compared to being close to optimal) in expectation, and will show that such feedback is sufficient for the algorithm\u2019s convergence. This stands in contrast to learning from demonstration (LfD) methods [1, 30, 43, 44] which require (near) optimal demonstrations of the complete trajectory. Such demonstrations can be extremely challenging and nonintuitive to provide for many high DoF manipulators [2]. Instead, we found [21, 22] that it is more intuitive for users to give incremental feedback on high DoF arms by improving upon a proposed trajectory. We now summarize three feedback mechanisms that enable the user to iteratively provide improved trajectories.\n(a) Re-ranking: We display the ranking of trajectories using OpenRAVE [13] on a touch screen device and ask the user to identify whether any of the lower-ranked trajectories is better than the top-ranked one. The user sequentially observes the trajectories in order of their current predicted scores and clicks on the first trajectory which is better than the top ranked trajectory. Figure 2 shows three trajectories for moving a knife. As feedback, the user moves the trajectory at rank 3 to the top position. Likewise, Figure 3 shows three trajectories for moving an egg carton. Using the current estimate of score function robot ranks them as red (1st), green (2nd) and blue (3rd). Since eggs are fragile the user selects the green trajectory.\n(b) Zero-G: This is a kinesthetic feedback. It allows the user to correct trajectory waypoints by physically changing the robot\u2019s arm configuration as shown in Figure 1. High DoF arms such as the Barrett WAM and Baxter have zero-force gravity-compensation (zero-G) mode, under which\nthe robot\u2019s arms become light and the users can effortlessly steer them to a desired configuration. On Baxter, this zero-G mode is automatically activated when a user holds the robot\u2019s wrist (see Figure 1, right). We use this zero-G mode as a feedback method for incrementally improving the trajectory by correcting a waypoint. This feedback is useful (i) for bootstrapping the robot, (ii) for avoiding local maxima where the top trajectories in the ranked list are all bad but ordered correctly, and (iii) when the user is satisfied with the top ranked trajectory except for minor errors.\n(c) Interactive: For the robots whose hardware does not permit zero-G feedback, such as PR2, we built an alternative interactive Rviz-ROS [18] interface for allowing the users to improve the trajectories by waypoint correction. Figure 4 shows a robot moving a bowl with one bad waypoint (in red), and the user provides a feedback by correcting it. This feedback serves the same purpose as zero-G.\nNote that, in all three kinds of feedback the user never reveals the optimal trajectory to the algorithm, but only provides a slightly improved trajectory (in expectation)."}, {"heading": "IV. LEARNING AND FEEDBACK MODEL", "text": "We model the learning problem in the following way. For a given task, the robot is given a context x that describes the environment, the objects, and any other input relevant to the problem. The robot has to figure out what is a good trajectory y for this context. Formally, we assume that the user has a scoring function s\u2217(x, y) that reflects how much he values each trajectory y for context x. The higher the score, the better the trajectory. Note that this scoring function cannot be observed directly, nor do we assume that the user can actually provide cardinal valuations according to this function. Instead, we merely assume that the user can provide us with preferences that reflect this scoring function. The robot\u2019s goal is to learn a function s(x, y;w)\n(where w are the parameters to be learned) that approximates the user\u2019s true scoring function s\u2217(x, y) as closely as possible.\nInteraction Model. The learning process proceeds through the following repeated cycle of interactions. \u2022 Step 1: The robot receives a context x and uses a planner\nto sample a set of trajectories, and ranks them according to its current approximate scoring function s(x, y;w). \u2022 Step 2: The user either lets the robot execute the topranked trajectory, or corrects the robot by providing an improved trajectory y\u0304. This provides feedback indicating that s\u2217(x, y\u0304) > s\u2217(x, y). \u2022 Step 3: The robot now updates the parameter w of s(x, y;w) based on this preference feedback and returns to step 1.\nRegret. The robot\u2019s performance will be measured in terms of regret, REGT = 1T \u2211T t=1[s \u2217(xt, y \u2217 t ) \u2212 s\u2217(xt, yt)], which compares the robot\u2019s trajectory yt at each time step t against the optimal trajectory y\u2217t maximizing the user\u2019s unknown scoring function s\u2217(x, y), y\u2217t = argmaxys\n\u2217(xt, y). Note that the regret is expressed in terms of the user\u2019s true scoring function s\u2217, even though this function is never observed. Regret characterizes the performance of the robot over its whole lifetime, therefore reflecting how well it performs throughout the learning process. We will employ learning algorithms with theoretical bounds on the regret for scoring functions that are linear in their parameters, making only minimal assumptions about the difference in score between s\u2217(x, y\u0304) and s\u2217(x, y) in Step 2 of the learning process. Expert Vs Non-expert user. We refer to an expert user as someone who can demonstrate the optimal trajectory y\u2217 to the robot. For example, robotics experts such as, the pilot demonstrating helicopter maneuver in Abbeel et al. [1]. On the other hand, our non-expert users never demonstrate y\u2217. They can only provide feedback y\u0304 indicating s\u2217(x, y\u0304) > s\u2217(x, y). For example, users working with assistive robots on assembly lines."}, {"heading": "V. LEARNING ALGORITHM", "text": "For each task, we model the user\u2019s scoring function s\u2217(x, y) with the following parametrized family of functions. s(x, y;w) = w \u00b7 \u03c6(x, y) (1)\nw is a weight vector that needs to be learned, and \u03c6(\u00b7) are features describing trajectory y for context x. Such linear representation of score functions have been previously used for generating desired robot behaviors [1, 44, 62].\nWe further decompose the score function in two parts, one only concerned with the objects the trajectory is interacting with, and the other with the object being manipulated and the environment\ns(x, y;wO, wE) = sO(x, y;wO) + sE(x, y;wE)\n= wO \u00b7 \u03c6O(x, y) + wE \u00b7 \u03c6E(x, y) (2) We now describe the features for the two terms, \u03c6O(\u00b7) and\n\u03c6E(\u00b7) in the following."}, {"heading": "A. Features Describing Object-Object Interactions", "text": "This feature captures the interaction between objects in the environment with the object being manipulated. We enumerate waypoints of trajectory y as y1, .., yN and objects in the environment as O = {o1, .., oK}. The robot manipulates the object o\u0304 \u2208 O. A few of the trajectory waypoints would be affected by the other objects in the environment. For example in Figure 5, o1 and o2 affect the waypoint y3 because of proximity. Specifically, we connect an object ok to a trajectory waypoint if the minimum distance to collision is less than a threshold or if ok lies below o\u0304. The edge connecting yj and ok is denoted as (yj , ok) \u2208 E .\nSince it is the attributes [31] of the object that really matter in determining the trajectory quality, we represent each object with its attributes. Specifically, for every object ok, we consider a vector of M binary variables [l1k, .., l M k ], with each lmk = {0, 1} indicating whether object ok possesses property m or not. For example, if the set of possible properties are {heavy, fragile, sharp, hot, liquid, electronic}, then a\nlaptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively. The binary variables lpk and l q indicates whether ok and o\u0304 possess property p and q respectively.3 Then, for every (yj , ok) edge, we extract following four features \u03c6oo(yj , ok): projection of minimum distance to collision along x, y and z (vertical) axis and a binary variable, that is 1, if ok lies vertically below o\u0304, 0 otherwise.\nWe now define the score sO(\u00b7) over this graph as follows:\nsO(x, y;wO) = \u2211\n(yj ,ok)\u2208E\nM\u2211 p,q=1 lpkl q[wpq \u00b7 \u03c6oo(yj , ok)] (3)\nHere, the weight vector wpq captures interaction between objects with properties p and q. We obtain wO in eq. (2) by concatenating vectors wpq . More formally, if the vector at position i of wO is wuv then the vector corresponding to position i of \u03c6O(x, y) will be \u2211 (yj ,ok)\u2208E l u k l v[\u03c6oo(yj , ok)]."}, {"heading": "B. Trajectory Features", "text": "We now describe features, \u03c6E(x, y), obtained by performing operations on a set of waypoints. They comprise the following three types of the features:\n1) Robot Arm Configurations: While a robot can reach the same operational space configuration for its wrist with different configurations of the arm, not all of them are preferred [61]. For example, the contorted way of holding the cup shown in Figure 5 may be fine at that time instant, but would present problems if our goal is to perform an activity with it, e.g. doing the pouring activity. Furthermore, humans like to anticipate robots move and to gain users\u2019 confidence, robot should produce predictable and legible robotic motions [14].\nWe compute features capturing robot\u2019s arm configuration using the location of its elbow and wrist, w.r.t. to its shoulder, in cylindrical coordinate system, (r, \u03b8, z). We divide a trajectory into three parts in time and compute 9 features for each of the parts. These features encode the maximum and minimum r, \u03b8 and z values for wrist and elbow in that part of the trajectory, giving us 6 features. Since at the limits of the manipulator configuration, joint locks may happen, therefore we also add 3 features for the location of robot\u2019s elbow whenever the endeffector attains its maximum r, \u03b8 and z values respectively. Thus obtaining \u03c6robot(\u00b7) \u2208 R9 (3+3+3=9) features for each one-third part and \u03c6robot(\u00b7) \u2208 R27 for the complete trajectory.\n2) Orientation and Temporal Behaviour of the Object to be Manipulated: Object orientation during the trajectory is crucial in deciding its quality. For some tasks, the orientation must be strictly maintained (e.g., moving a cup full of coffee); and for some others, it may be necessary to change it in a particular fashion (e.g., pouring activity). Different parts of the trajectory may have different requirements over time. For example, in the placing task, we may need to bring the object closer to obstacles and be more careful.\nWe therefore divide trajectory into three parts in time. For each part we store the cosine of the object\u2019s maximum\n3In this work, our goal is to relax the assumption of unbiased and close to optimal feedback. We therefore assume complete knowledge of the environment for our algorithm, and for the algorithms we compare against. In practice, such knowledge can be extracted using an object attribute labeling algorithms such as in [31].\ndeviation, along the vertical axis, from its final orientation at the goal location. To capture object\u2019s oscillation along trajectory, we obtain a spectrogram for each one-third part for the movement of the object in x, y, z directions as well as for the deviation along vertical axis (e.g. Figure 6). We then compute the average power spectral density in the low and high frequency part as eight additional features for each. This gives us 9 (=1+4*2) features for each one-third part. Together with one additional feature of object\u2019s maximum deviation along the whole trajectory, we get \u03c6obj(\u00b7) \u2208 R28 (=9*3+1).\n3) Object-Environment Interactions: This feature captures temporal variation of vertical and horizontal distances of the object o\u0304 from its surrounding surfaces. In detail, we divide the trajectory into three equal parts, and for each part we compute object\u2019s: (i) minimum vertical distance from the nearest surface below it. (ii) minimum horizontal distance from the surrounding surfaces; and (iii) minimum distance from the table, on which the task is being performed, and (iv) minimum distance from the goal location. We also take an average, over all the waypoints, of the horizontal and vertical distances between the object and the nearest surfaces around it.4 To capture temporal variation of object\u2019s distance from its surrounding we plot a time-frequency spectrogram of the object\u2019s vertical distance from the nearest surface below it, from which we extract six features by dividing it into grids. This feature is expressive enough to differentiate whether an object just grazes over table\u2019s edge (steep change in vertical distance) versus, it first goes up and over the table and then moves down (relatively smoother change). Thus, the features obtained from objectenvironment interaction are \u03c6obj\u2212env(\u00b7) \u2208 R20 (3*4+2+6=20).\nFinal feature vector is obtained by concatenating \u03c6obj\u2212env , \u03c6obj and \u03c6robot, giving us \u03c6E(\u00b7) \u2208 R75.\n4We query PQP collision checker plugin of OpenRave for these distances."}, {"heading": "C. Computing Trajectory Rankings", "text": "For obtaining the top trajectory (or a top few) for a given task with context x, we would like to maximize the current scoring function s(x, y;wO, wE).\ny\u2217 = arg max y s(x, y;wO, wE). (4)\nSecond, for a given set {y(1), . . . , y(n)} of discrete trajectories, we need to compute (4). Fortunately, the latter problem is easy to solve and simply amounts to sorting the trajectories by their trajectory scores s(x, y(i);wO, wE). Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12]. Previously, both approaches have been studied. However, for high DoF manipulators the sampling based approach [6, 12] maintains tractability of the problem, hence we take this approach. More precisely, similar to Berg et al. [6], we sample trajectories using rapidly-exploring random trees (RRT) [33].5 However, naively sampling trajectories could return many similar trajectories. To get diverse samples of trajectories we use various diversity introducing methods. For example, we introduce obstacles in the environment which forces the planner to sample different trajectories. Our methods also introduce randomness in planning by initizaling goalsample bias of RRT planner randomly. To avoid sampling similar trajectories multiple times, one of our diversity method introduce obstacles to block waypoints of already sampled trajectories. Recent work by Ross et al. [48] propose the use of sub-modularity to achieve diversity. For more details on sampling trajectories we refer interested readers to the work by Erickson and LaValle [16], and Green and Kelly [19]. Since our primary goal is to learn a score function on trajectories we now describe our learning algorithm."}, {"heading": "D. Learning the Scoring Function", "text": "The goal is to learn the parameters wO and wE of the scoring function s(x, y;wO, wE) so that it can be used to rank trajectories according to the user\u2019s preferences. To do so,\n5When RRT becomes too slow, we switch to a more efficient bidirectionalRRT.The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [47] or optimal planners like RRT* [26] to produce reasonably good trajectories.\nwe adapt the Preference Perceptron algorithm [51] as detailed in Algorithm 1, and we call it the Trajectory Preference Perceptron (TPP). Given a context xt, the top-ranked trajectory yt under the current parameters wO and wE , and the user\u2019s feedback trajectory y\u0304t, the TPP updates the weights in the direction \u03c6O(xt, y\u0304t)\u2212\u03c6O(xt, yt) and \u03c6E(xt, y\u0304t)\u2212\u03c6E(xt, yt) respectively. Our update equation resembles to the weights update equation in Ratliff et al. [44]. However, our update does not depends on the availability of optimal demonstraions. Figure 7 shows an overview of our system design.\nDespite its simplicity and even though the algorithm typically does not receive the optimal trajectory y\u2217t = arg maxy s\n\u2217(xt, y) as feedback, the TPP enjoys guarantees on the regret [51]. We merely need to characterize by how much the feedback improves on the presented ranking using the following definition of expected \u03b1-informative feedback: Et[s\n\u2217(xt, y\u0304t)] \u2265 s\u2217(xt, yt) + \u03b1(s\u2217(xt, y\u2217t )\u2212 s\u2217(xt, yt))\u2212 \u03bet This definition states that the user feedback should have a score of y\u0304t that is \u2013 in expectation over the users choices \u2013 higher than that of yt by a fraction \u03b1 \u2208 (0, 1] of the maximum possible range s\u2217(xt, y\u0304t) \u2212 s\u2217(xt, yt). It is important to note that this condition only needs to be met in expectation and not deterministically. This leaves room for noisy and imperfect user feedback. If this condition is not fulfilled due to bias in the feedback, the slack variable \u03bet captures the amount of violation. In this way any feedback can be described by an appropriate combination of \u03b1 and \u03bet. Using these two parameters, the proof by Shivaswamy and Joachims [51] can be adapted (for proof see Appendix A & B) to show that average regret of TPP is upper bounded by:\nE[REGT ] \u2264 O( 1\n\u03b1 \u221a T\n+ 1\n\u03b1T T\u2211 t=1 \u03bet)\nIn practice, over feedback iterations the quality of trajectory y proposed by robot improves. The \u03b1-informative criterion only requires the user to improve y to y\u0304 in expectation."}, {"heading": "VI. EXPERIMENTS AND RESULTS", "text": "We first describe our experimental setup, then present quantitative results (Section VI-B) , and then present robotic experiments on PR2 and Baxter (Section VI-D)."}, {"heading": "A. Experimental Setup", "text": "Task and Activity Set for Evaluation. We evaluate our approach on 35 robotic tasks in a household setting and 16 pick-and-place tasks in a grocery store checkout setting. For household activities we use PR2, and for the grocery store setting we use Baxter. To assess the generalizability of our approach, for each task we train and test on scenarios with different objects being manipulated and/or with a different environment. We evaluate the quality of trajectories after the robot has grasped the item in question and while the robot moves it for task completion. Our work complements previous works on grasping items [49, 34], pick and place tasks [23], and detecting bar codes for grocery checkout [29]. We consider the following three most commonly occurring activities in household and grocery stores:\n1) Manipulation centric: These activities are primarily concerned with the object being manipulated. Hence the object\u2019s properties and the way the robot moves it in the environment are more relevant. Examples of such household activities are pouring water into a cup or inserting pen into a pen holder, as in Figure 8 (Left). While in a grocery store, such activities could include moving a flower vase or moving fruits and vegetables, which could be damaged when dropped or pushed into other items. We consider pick-and-place, pouring and inserting activities with following objects: cup, bowl, bottle, pen, cereal box, flower vase, and tomato. Further, in every environment we place many objects, along with the object to be manipulated, to restrict simple straight line trajectories. 2) Environment centric: These activities are also concerned with the interactions of the object being manipulated with the surrounding objects. Our object-object interaction features (Section V-A) allow the algorithm to learn preferences on trajectories for moving fragile objects like egg cartons or moving liquid near electronic devices, as in Figure 8 (Middle). We consider moving fragile items like egg carton, heavy metal boxes near a glass table, water near laptop and other electronic devices. 3) Human centric: Sudden movements by the robot put the human in danger of getting hurt. We consider activities where a robot manipulates sharp objects such as knife, as in Figure 8 (Right), moves a hot coffee cup or a bowl of water with a human in vicinity.\nExperiment setting. Through experiments we will study: \u2022 Generalization: Performance of robot on tasks that it has\nnot seen before. \u2022 No demonstrations: Comparison of TPP to algorithms\nthat also learn in absence of expert\u2019s demonstrations. \u2022 Feedback: Effectiveness of different kinds of user feed-\nback in absence of expert\u2019s demonstrations.\nBaseline algorithms. We evaluate algorithms that learn preferences from online feedback under two settings: (a) untrained, where the algorithms learn preferences for a new task from scratch without observing any previous feedback; (b) pretrained, where the algorithms are pre-trained on other similar\ntasks, and then adapt to a new task. We compare the following algorithms:\n\u2022 Geometric: The robot plans a path, independent of the task, using a Bi-directional RRT (BiRRT) [33] planner. \u2022 Manual: The robot plans a path following certain manually coded preferences. \u2022 TPP: Our algorithm, evaluated under both untrained and pre-trained settings. \u2022 MMP-online: This is an online implementation of the Maximum Margin Planning (MMP) [44, 46] algorithm. MMP attempts to make an expert\u2019s trajectory better than any other trajectory by a margin. It can be interpreted as a special case of our algorithm with 1-informative i.e. optimal feedback. However, directly adapting MMP [44] to our experiments poses two challenges: (i) we do not have knowledge of the optimal trajectory; and (ii) the state space of the manipulator we consider is too large, discretizing which makes intractable to train MMP. To ensure a fair comparison, we follow the MMP algorithm from [44, 46] and train it under similar settings as TPP. Algorithm 2 shows our implementation of MMPonline. It is very similar to TPP (Algorithm 1) but with a different parameter update step. Since both algorithms only observe user feedback and not demonstrations, MMP-online treats each feedback as a proxy for optimal demonstration. At every iteration MMP-online trains a structural support vector machine (SSVM) [25] using all previous feedback as training examples, and use the learned weights to predict trajectory scores in the next iteration. Since the argmax operation is performed on a set of trajectories it remains tractable. We quantify closeness of trajectories by the L2\u2212norm of the difference in their feature representations, and choose the regularization parameter C for training SSVM in hindsight, giving an unfair advantage to MMP-online.\nAlgorithm 2 MMP-online\nInitialize w(1)O \u2190 0, w (1) E \u2190 0, T = {} for t = 1 to T do Sample trajectories {y(1), ..., y(n)} yt = argmaxys(xt, y;w (t) O , w (t) E )\nObtain user feedback y\u0304t T = T \u222a {(xt, y\u0304t)} w\n(t+1) O , w (t+1) E = Train-SSVM(T ) (Joachims et al. [25])\nend for\nEvaluation metrics. In addition to performing a user study (Section VI-D), we also designed two datasets to quantitatively evaluate the performance of our online algorithm. We obtained experts labels on 1300 trajectories in a grocery setting and 2100 trajectories in a household setting. Labels were on the basis of subjective human preferences on a Likert scale of 1- 5 (where 5 is the best). Note that these absolute ratings are never provided to our algorithms and are only used for the quantitative evaluation of different algorithms.\nWe evaluate performance of algorithms by measuring how well they rank trajectories, that is, trajectories with higher Likert score should be ranked higher. To quantify the quality of a ranked list of trajectories we report normalized discounted cumulative gain (nDCG) [38] \u2014 criterion popularly used in Information Retrieval for document ranking. In particular we report nDCG at positions 1 and 3, equation (6). While nDCG@1 is a suitable metric for autonomous robots that execute the top ranked trajectory (e.g., grocery checkout), nDCG@3 is suitable for scenarios where the robot is supervised by humans, (e.g., assembly lines). For a given ranked list of items (trajectories here) nDCG at position k is defined as:\nDCG@k = k\u2211 i=1 li log2(i+ 1)\n(5)\nnDCG@k = DCG@k\nIDCG@k , (6)\nwhere li is the Likert score of the item at position i in the ranked list. IDCG is the DCG value of the best possible ranking of items. It is obtained by ranking items in decreasing order of their Likert score."}, {"heading": "B. Results and Discussion", "text": "We now present quantitative results where we compare TPP against the baseline algorithms on our data set of labeled trajectories.\nHow well does TPP generalize to new tasks? To study generalization of preference feedback we evaluate performance of TPP-pre-trained (i.e., TPP algorithm under pre-trained setting) on a set of tasks the algorithm has not seen before. We study generalization when: (a) only the object being manipulated changes, e.g., a bowl replaced by a cup or an egg carton replaced by tomatoes, (b) only the surrounding environment changes, e.g., rearranging objects in the environment or changing the start location of tasks, and (c) when both change. Figure 9 shows nDCG@3 plots averaged over tasks for all types of activities for both household and grocery store settings.6 TPP-pre-trained starts-off with higher nDCG@3 values than TPP-untrained in all three cases. However, as more feedback is provided, the performance of both algorithms improves, and they eventually give identical performance. We further observe that generalizing to tasks with both new environment and object is harder than when\n6Similar results were obtained with nDCG@1 metric.\nonly one of them changes.\nHow does TPP compare to MMP-online? MMP-online while training assumes all user feedback is optimal, and hence over time it accumulates many contradictory/suboptimal training examples. We empirically observe that MMP-online generalizes better in grocery store setting than the household setting (Figure 9), however under both settings its performance remains much lower than TPP. This also highlights the sensitivity of MMP to sub-optimal demonstrations.\nHow does TPP compare to Manual? For the manual baseline we encode some preferences into the planners, e.g., keep a glass of water upright. However, some preferences are difficult to specify, e.g., not to move heavy objects over fragile items. We empirically found (Figure 9) that the resultant manual algorithm produces poor trajectories in comparison with TPP, with an average nDCG@3 of 0.44 over all types of household activities. Table I reports nDCG values averaged over 20 feedback iterations in untrained setting. For both household and grocery activities, TPP performs better than other baseline algorithms.\nHow does TPP perform with weaker feedback? To study the robustness of TPP to less informative feedback we consider the following variants of re-rank feedback: \u2022 Click-one-to-replace-top: User observes the trajectories\nsequentially in order of their current predicted scores and clicks on the first trajectory which is better than the top ranked trajectory. \u2022 Click-one-from-5: Top 5 trajectories are shown and user clicks on the one he thinks is the best after watching all 5 of them. \u2022 Approximate-argmax: This is a weaker feedback, here instead of presenting top ranked trajectories, five random trajectories are selected as candidate. The user selects the best trajectory among these 5 candidates. This simulates\na situation when computing an argmax over trajectories is prohibitive and therefore approximated.\nFigure 10 shows the performance of TPP-untrained receiving different kinds of feedback and averaged over three types of activities in grocery store setting. When feedback is more \u03b1informative the algorithm requires fewer iterations to learn preferences. In particular, click-one-to-replace-top and clickone-from-5 are more informative than approximate-argmax and therefore require less feedback to reach a given nDCG@1 value. Approximate-argmax improves slowly since it is least informative. In all three cases the feedback is \u03b1-informative, for some \u03b1 > 0, therefore TPP-untrained eventually learns the user\u2019s preferences."}, {"heading": "C. Comparison with fully-supervised algorithms", "text": "The algorithms discussed so far only observes ordinal feedback where the users iteratively improves upon the proposed trajectory. In this section we compare TPP to a fullysupervised algorithm that observes expert\u2019s labels while training. Eliciting such expert labels on the large space of trajectories is not realizable in practice. However, empirically it nonetheless provides an upper-bound on the generalization to new tasks. We refer to this algorithm as Oracle-svm and it learns to rank trajectories using SVM-rank [24]. Since expert labels are not available while prediction, on test set Oraclesvm predicts once and does not learn from user feedback.\nFigure 11 compares TPP and Oracle-svm on new tasks. Without observing any feedback on new tasks Oracle-svm performs better than TPP. However, after few feedback iterations TPP improves over Oracle-svm, which is not updated since it requires expert\u2019s labels on test set. On average, we observe, it takes 5 feedback iterations for TPP to improve over Oracle-svm. Furthermore, learning from demonstration (LfD) can be seen as a special case of Oracle-svm where, instead of providing an expert label for every sampled trajectory, the expert directly demonstrates the optimal trajectory."}, {"heading": "D. Robotic Experiment: User Study in learning trajectories", "text": "We perform a user study of our system on Baxter and PR2 on a variety of tasks of varying difficulties in grocery store and\nhousehold settings, respectively. Thereby we show a proofof-concept of our approach in real world robotic scenarios, and that the combination of re-ranking and zero-G/interactive feedback allows users to train the robot in few feedback iterations.\nExperiment setup: In this study, users not associated with this work, used our system to train PR2 and Baxter on household and grocery checkout tasks, respectively. Five users independently trained Baxter, by providing zero-G feedback kinesthetically on the robot, and re-rank feedback in a simulator. Two users participated in the study on PR2. On PR2, in place of zero-G, users provided interactive waypoint correction feedback in the Rviz simulator. The users were undergraduate students. Further, both users training PR2 on household tasks were familiar with Rviz-ROS.7 A set of 10 tasks of varying difficulty level was presented to users one at a time, and they were instructed to provide feedback until they were satisfied with the top ranked trajectory. To quantify the quality of learning each user evaluated their own trajectories (self score), the trajectories learned by the other users (cross score), and those predicted by Oracle-svm, on a Likert scale of 1-5. We also recorded the total time a user spent on each task \u2013 from start of training till the user was satisfied with the top ranked trajectory. This includes time taken for both re-rank and zero-G feedback.\nIs re-rank feedback easier to elicit from users than zero-G or interactive? In our user study, on average a user took 3 re-rank and 2 zero-G feedback per task to train a robot (Table II). From this we conjecture, that for high DoF manipulators re-rank feedback is easier to provide than zero-G \u2013 which requires modifying the manipulator joint angles. However, an increase in the count of zero-G (interactive) feedback with task difficulty suggests (Figure 12 (Right)), users rely more on zero-G feedback for difficult tasks since it allows precisely rectifying erroneous waypoints. Figure 13 and Figure 14 show two example trajectories learned by a user.\nHow many feedback iterations a user takes to improve over Oracle-svm? Figure 12 (Left) shows that the quality of trajectory improves with feedback. On average, a user took 5 feedback to improve over Oracle-svm, which is also consistent with our quantitative analysis (Section VI-C). In grocery setting, users 4 and 5 were critical towards trajectories learned by Oracle-svm and gave them low scores. This indicates a possible mismatch in preferences between our expert (on whose labels Oracle-svm was trained) and users 4 and 5.\nHow do users\u2019 unobserved score functions vary? An average difference of 0.6 between users\u2019 self and cross score (Table II) in the grocery checkout setting suggests preferences\n7The smaller user size on PR2 is because it requires users with experience in Rviz-ROS. Further, we also observed users found it harder to correct trajectory waypoints in a simulator than providing zero-G feedback on the robot. For the same reason we report training time only on Baxter for grocery store setting.\nvaried across users, but only marginally. In situations where this difference is significant and a system is desired for\na user population, a future work might explore coactive learning for satisfying user population, similar to Raman and Joachims [42]. For household setting, the sample size is small to draw a similar conclusion.\nHow long does it take for users to train a robot? We report training time for only the grocery store setting, because the interactive feedback in the household setting requires users with experience in Rviz-ROS. Further, we observed that users found it difficult to modify the robot\u2019s joint angles in a simulator to their desired configuration. In the grocery checkout setting, among all the users, user 1 had the strictest preferences and also experienced some early difficulties in using the system and therefore took longer than others. On an average, a user took 5.5 minutes per task, which we believe is acceptable for most applications. Future research in human-computer interaction, visualization and\nbetter user interfaces [52] could further reduce this time. For example, simultaneous visualization of top ranked trajectories instead of sequentially showing them to users (the scheme we currently adopt) could bring down the time for re-rank feedback. Despite its limited size, through our user study we show that our algorithm is realizable in practice on high DoF manipulators. We hope this motivates researchers to build robotic systems capable of learning from non-expert users. For more details, videos and code, visit:\nhttp://pr.cs.cornell.edu/coactive/"}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "When manipulating objects in human environments, it is important for robots to plan motions that follow users\u2019 preferences. In this work, we considered preferences that go beyond simple geometric constraints and that considered surrounding context of various objects and humans in the environment. We presented a coactive learning approach for teaching robots these preferences through iterative improvements from nonexpert users. Unlike in standard learning from demonstration approaches, our approach does not require the user to provide optimal trajectories as training data. We evaluated our approach on various household (with PR2) and grocery store checkout (with Baxter) settings. Our experiments suggest that it is indeed possible to train robots within a few minutes with just a few incremental feedbacks from non-expert users.\nFuture research could extend coactive learning to situations with uncertainty in object pose and attributes. Under\nuncertainty the trajectory preference perceptron will admit a belief space update form, and theoretical guarantees will also be different. Coactive feedback might also find use in other interesting robotic applications such as assistive cars, where a car learns from humans steering actions. Scaling up coactive feedback by crowd-sourcing and exploring other forms of easy-to-elicit learning signals are also potential future directions."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This research was supported by ARO award W911NF-121-0267, Microsoft Faculty fellowship and NSF Career award (to Saxena)."}, {"heading": "APPENDIX A PROOF FOR AVERAGE REGRET", "text": "This proof builds upon Shivaswamy & Joachims [51]. We assume the user hidden score function s\u2217(x, y) is contained in the family of scoring functions s(x, y;w\u2217O, w \u2217 E) for some unknown w\u2217O and w \u2217 E . Average regret for TPP over T rounds of interactions can be written as:\nREGT = 1\nT T\u2211 t=1 (s\u2217(xt, y \u2217 t )\u2212 s\u2217(xt, yt))\n= 1\nT T\u2211 t=1 (s(xt, y \u2217 t ;w \u2217 O, w \u2217 E)\u2212 s(xt, yt;w\u2217O, w\u2217E))\nWe further assume the feedback provided by the user is strictly \u03b1-informative and satisfy following inequality:\ns(xt, y\u0304t;w \u2217 O, w \u2217 E) \u2265s(xt, yt;w\u2217O, w\u2217E) + \u03b1[s(xt, y\u2217t ;w\u2217O, w\u2217E)\n\u2212 s(xt, yt;w\u2217O, w\u2217E)]\u2212 \u03bet (7) Later we relax this constraint and requires it to hold only in expectation. This definition states that the user feedback should have a score of y\u0304t that is higher than that of yt by a fraction \u03b1 \u2208 (0, 1] of the maximum possible range s(xt, y \u2217 t ;w \u2217 O, w \u2217 E)\u2212 s(xt, yt;w\u2217O, w\u2217E).\nTheorem 1: The average regret of trajectory preference perceptron receiving strictly \u03b1-informative feedback can be upper bounded for any [w\u2217O;w \u2217 E ] as follows:\nREGT \u2264 2C \u2016[w\u2217O;w\u2217E ]\u2016\n\u03b1 \u221a T\n+ 1\n\u03b1T T\u2211 t=1 \u03bet (8)\nwhere C is constant such that \u2016[\u03c6O(x, y);\u03c6E(x, y)]\u20162 \u2264 C. Proof: After T rounds of feedback, using weight update equations of wE and wO we can write:\nw\u2217O \u00b7 w (T+1) O = w \u2217 O \u00b7 w (T ) O + w \u2217 O \u00b7 (\u03c6O(xT , y\u0304T )\u2212 \u03c6O(xT , yT )) w\u2217E \u00b7 w (T+1) E = w \u2217 E \u00b7 w (T ) E + w \u2217 E \u00b7 (\u03c6E(xT , y\u0304T )\u2212 \u03c6E(xT , yT ))\nAdding the two equations and recursively reducing the right gives:\nw\u2217O \u00b7 w (T+1) O + w \u2217 E \u00b7 w (T+1) E = T\u2211 t=1 (s(xt, y\u0304t;w \u2217 O, w \u2217 E)\n\u2212 s(xt, yt;w\u2217O, w\u2217E)) (9) Using Cauchy-Schwarz inequality the left hand side of\nequation (9) can be bounded as:\nw\u2217O \u00b7w (T+1) O +w \u2217 E \u00b7w (T+1) E \u2264 \u2016[w \u2217 O;w \u2217 E ]\u2016 \u2225\u2225\u2225[w(T+1)O ;w(T+1)E ]\u2225\u2225\u2225 (10)\u2225\u2225\u2225[w(T+1)O ;w(T+1)E ]\u2225\u2225\u2225 can be bounded by using weight update equations:\nw (T+1) O \u00b7 w (T+1) O + w (T+1) E \u00b7 w (T+1) E = w (T ) O \u00b7 w (T ) O + w (T ) E \u00b7 w (T ) E\n+ 2w (T ) O \u00b7 (\u03c6O(xT , y\u0304T )\u2212 \u03c6O(xT , yT )) + 2w (T ) E \u00b7 (\u03c6E(xT , y\u0304T )\u2212 \u03c6E(xT , yT )) + (\u03c6O(xT , y\u0304T )\u2212 \u03c6O(xT , yT )) \u00b7 (\u03c6O(xT , y\u0304T )\u2212 \u03c6O(xT , yT )) + (\u03c6E(xT , y\u0304T )\u2212 \u03c6E(xT , yT )) \u00b7 (\u03c6E(xT , y\u0304T )\u2212 \u03c6E(xT , yT )) \u2264 w(T )O \u00b7 w (T ) O + w (T ) E \u00b7 w (T ) E + 4C 2 \u2264 4C2T (11)\n\u2234 \u2225\u2225\u2225[w(T+1)O ;w(T+1)E ]\u2225\u2225\u2225 \u2264 2C\u221aT (12) Eq. (11) follows from the fact that s(xT , yT ;w (T ) O , w (T ) E ) >\ns(xT , y\u0304T ;w (T ) O , w (T ) E ) and \u2016[\u03c6O(x, y);\u03c6E(x, y)]\u20162 \u2264 C. Using equations (10) and (12) gives following bound on (9): T\u2211\nt=1\n(s(xt, y\u0304t;w \u2217 O, w \u2217 E)\u2212 s(xt, yt;w\u2217O, w\u2217E))\n\u2264 2C \u221a T \u2016[w\u2217O;w\u2217E ]\u2016 (13)\nAssuming strictly \u03b1-informative feedback and re-writing equation (7) as:\ns(xt, y \u2217 t ;w \u2217 O, w \u2217 E)\u2212 s(xt, yt;w\u2217O, w\u2217E)\n\u2264 1 \u03b1 ((s(xt, y\u0304t;w \u2217 O, w \u2217 E)\u2212 s(xt, yt;w\u2217O, w\u2217E))\u2212 \u03bet) (14)\nCombining equations (13) and (14) gives the bound on average regret (8)."}, {"heading": "APPENDIX B PROOF FOR EXPECTED REGRET", "text": "We now show the regret bounds for TPP under a weaker feedback assumption \u2013 expected \u03b1-informative feedback:\nEt[s(xt, y\u0304t;w \u2217 O, w \u2217 E)] \u2265 s(xt, yt;w\u2217O, w\u2217E)\n+ \u03b1[s(xt, y \u2217 t ;w \u2217 O, w \u2217 E)\u2212 s(xt, yt;w\u2217O, w\u2217E)]\u2212 \u03bet\nwhere the expectation is under choices y\u0304t when yt and xt are known.\nCorollary 2: The expected regret of trajectory preference perceptron receiving expected \u03b1-informative feedback can be upper bounded for any [w\u2217O;w \u2217 E ] as follows:\nE[REGT ] \u2264 2C \u2016[w\u2217G;w\u2217O]\u2016\n\u03b1 \u221a T\n+ 1\n\u03b1T T\u2211 t=1 \u03be\u0304t (15)\nProof: Taking expectation on both sides of equation (9), (10) and (11) yields following equations respectively:\nE[w\u2217O \u00b7 w (T+1) O + w \u2217 E \u00b7 w (T+1) E ] = T\u2211 t=1 E[(s(xt, y\u0304t;w \u2217 O, w \u2217 E)\n\u2212 s(xt, yt;w\u2217O, w\u2217E))] (16)\nE[w\u2217O \u00b7 w (T+1) O +w \u2217 E \u00b7 w (T+1) E ] \u2264 \u2016[w\u2217O;w\u2217E ]\u2016E [\u2225\u2225\u2225[w(T+1)O ;w(T+1)E ]\u2225\u2225\u2225]\nE[w (T+1) O \u00b7 w (T+1) O + w (T+1) E \u00b7 w (T+1) E ] \u2264 4C 2T\nApplying Jensen\u2019s inequality on the concave function \u221a \u00b7\nwe get:\nE[w\u2217O \u00b7 w (T+1) O + w \u2217 E \u00b7 w (T+1) E ] \u2264 \u2016[w\u2217O;w\u2217E ]\u2016E [\u2225\u2225\u2225[w(T+1)O ;w(T+1)E ]\u2225\u2225\u2225]\n\u2264 \u2016[w\u2217O;w\u2217E ]\u2016 \u221a E[w (T+1) O \u00b7 w (T+1) O + w (T+1) E \u00b7 w (T+1) E ]\nUsing (16) gives the following bound: T\u2211\nt=1\nE[s(xt, y\u0304t;w \u2217 O, w \u2217 E)\u2212 s(xt, yt;w\u2217O, w\u2217E)]\n\u2264 2C \u221a T \u2016[w\u2217O;w\u2217E ]\u2016\nNow using the fact that the user feedback is expected \u03b1informative gives the regret bound (15)."}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A.Y. Ng"], "venue": "International Journal of Robotics Research, 29(13)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Keyframe-based learning from demonstration", "author": ["B. Akgun", "M. Cakmak", "K. Jiang", "A.L. Thomaz"], "venue": "International Journal of Social Robotics, 4(4):343\u2013355", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The stochastic motion roadmap: A sampling framework for planning with markov motion uncertainty", "author": ["R. Alterovitz", "T. Sim\u00e9on", "K. Goldberg"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems, 57(5):469\u2013483", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A robot path planning framework that learns from experience", "author": ["D. Berenson", "P. Abbeel", "K. Goldberg"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information", "author": ["J.V.D. Berg", "P. Abbeel", "K. Goldberg"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Identification and representation of homotopy classes of trajectories for search-based path planning in 3d", "author": ["S. Bhattacharya", "M. Likhachev", "V. Kumar"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "The morpha style guide for icon-based programming", "author": ["R. Bischoff", "A. Kazi", "M. Seyfarth"], "venue": "Proceedings. 11th IEEE International Workshop on RHIC.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "On learning", "author": ["S. Calinon", "F. Guenter", "A. Billard"], "venue": "representing, and generalizing a task in a humanoid robot. Sys., Man, and Cybernetics, Part B: Cybernetics, IEEE Trans. on", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Search-based planning for manipulation with motion primitives", "author": ["B.J. Cohen", "S. Chitta", "M. Likhachev"], "venue": "Proceedings of the International Conference on Robotics and Automation, pages 2902\u20132908", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Anytime rrts", "author": ["Dave D. Ferguson", "A. Stentz"], "venue": "In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Contextual sequence prediction with application to control library optimization", "author": ["D. Dey", "T.Y. Liu", "M. Hebert", "J.A. Bagnell"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Automated Construction of Robotic Manipulation Programs", "author": ["R. Diankov"], "venue": "PhD thesis, CMU,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Generating legible motion", "author": ["A. Dragan", "S. Srinivasa"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Legibility and predictability of robot motion", "author": ["A. Dragan", "K. Lee", "S. Srinivasa"], "venue": "Human Robot Interaction", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Survivability: Measuring and ensuring path diversity", "author": ["L.H. Erickson", "S.M. LaValle"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Randomised rough-terrain robot motion planning", "author": ["A. Ettlin", "H. Bleuler"], "venue": "Proceedings of the IEEE/RSJ  Conference on Intelligent Robots and Systems", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "A", "author": ["D. Gossow"], "venue": "Leeperand D. Hershberger, and M. Ciocarlie. Interactive markers: 3-d user interfaces for ros applications [ros topics]. Robotics & Automation Magazine, IEEE, 18(4):14\u201315", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Toward optimal sampling in the space of paths", "author": ["C.J. Green", "A. Kelly"], "venue": "In Robotics Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Sampling-based path planning on configuration-space costmaps", "author": ["L. Jaillet", "J. Cort\u00e9s", "T. Sim\u00e9on"], "venue": "26(4)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Beyond geometric path planning: Learning context-driven user preferences via sub-optimal feedback", "author": ["A. Jain", "S. Sharma", "A. Saxena"], "venue": "Proceedings of the International Symposium on Robotics Research", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning trajectory preferences for manipulators via iterative improvement", "author": ["A. Jain", "B. Wojcik", "T. Joachims", "A. Saxena"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to place new objects in a scene", "author": ["Y. Jiang", "M. Lim", "C. Zheng", "A. Saxena"], "venue": "International Journal of Robotics Research, 31(9):1021\u20131043", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the ACM Special Interest Group on Knowledge Discovery and Data Mining", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning, 77(1): 27\u201359", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Incremental sampling-based algorithms for optimal motion planning", "author": ["S. Karaman", "E. Frazzoli"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Sampling-based algorithms for optimal motion planning", "author": ["S. Karaman", "E. Frazzoli"], "venue": "International Journal of Robotics Research, 30(7):846\u2013894", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Activity forecasting", "author": ["K.M. Kitani", "B.D. Ziebart", "J.A. Bagnell", "M. Hebert"], "venue": "In Proceedings of the European Conference on Computer Vision", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Grasping with application to an autonomous checkout robot", "author": ["E. Klingbeil", "D. Rao", "B. Carpenter", "V. Ganapathi", "A.Y. Ng", "O. Khatib"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "ML, 84(1)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H.S. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to predict trajectories of cooperatively navigating agents", "author": ["H. Kretzschmar", "M. Kuderer", "W. Burgard"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Randomized kinodynamic planning", "author": ["S.M. LaValle", "J.J. Kuffner"], "venue": "International Journal of Robotics Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Using manipulability to  bias sampling during the construction of probabilistic roadmaps", "author": ["P. Leven", "S. Hutchinson"], "venue": "IEEE Trans. on Robotics and Automation, 19(6)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["S. Levine", "V. Koltun"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Planning human-aware motions using a sampling-based costmap planner", "author": ["J. Mainprice", "E.A. Sisbot", "L. Jaillet", "J. Cort\u00e9s", "R. Alami", "T. Sim\u00e9on"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "volume 1. Cambridge University Press Cambridge", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Human-robot teaming using shared mental models", "author": ["S. Nikolaidis", "J. Shah"], "venue": "HRI, Workshop on Human- Agent-Robot Teamwork", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-robot cross-training: Computational formulation", "author": ["S. Nikolaidis", "J. Shah"], "venue": "modeling and evaluation of a human team training strategy. In IEEE/ACM ICHRI", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Egraphs: Bootstrapping planning with experience graphs", "author": ["M. Phillips", "B. Cohen", "S. Chitta", "M. Likhachev"], "venue": "Proceedings of Robotics: Science and Systems", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning socially optimal information systems from egoistic users", "author": ["K. Raman", "T. Joachims"], "venue": "Proceedings of the European Conference on Machine Learning", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to Search: Structured Prediction Techniques for Imitation Learning", "author": ["N. Ratliff"], "venue": "PhD thesis, CMU, RI", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximum margin planning", "author": ["N. Ratliff", "J.A. Bagnell", "M. Zinkevich"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "online) subgradient methods for structured prediction", "author": ["N. Ratliff", "J.A. Bagnell", "M. Zinkevich"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["N. Ratliff", "D. Silver", "J.A. Bagnell"], "venue": "Autonomous Robots, 27(1):25\u201353", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Chomp: Gradient optimization techniques for efficient motion planning", "author": ["N. Ratliff", "M. Zucker", "J.A. Bagnell", "S. Srinivasa"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning policies for contextual submodular prediction", "author": ["S. Ross", "J. Zhou", "Y. Yue", "D. Dey", "J.A. Bagnell"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Robotic grasping of novel objects using vision", "author": ["A. Saxena", "J. Driemeyer", "A.Y. Ng"], "venue": "International Journal of Robotics Research, 27(2):157\u2013173", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding locally optimal", "author": ["J. Schulman", "J. Ho", "A. Lee", "I. Awwal", "H. Bradlow", "P. Abbeel"], "venue": "collision-free trajectories with sequential convex optimization. In Proceedings of Robotics: Science and Systems", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Online structured  prediction via coactive learning", "author": ["P. Shivaswamy", "T. Joachims"], "venue": "Proceedings of the International Conference on Machine Learning", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "Designing The User Interface: Strategies for Effective Human-Computer Interaction", "author": ["B. Shneiderman", "C. Plaisant"], "venue": "Addison-Wesley Publication", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning from demonstration for autonomous navigation in complex unstructured terrain", "author": ["D. Silver", "J.A. Bagnell", "A. Stentz"], "venue": "International Journal of Robotics Research", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatial reasoning for human robot interaction", "author": ["E.A. Sisbot", "L.F. Marin", "R. Alami"], "venue": "Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2007}, {"title": "A human aware mobile robot motion planner", "author": ["E.A. Sisbot", "L.F. Marin-Urias", "R. Alami", "T. Simeon"], "venue": "IEEE Transactions on Robotics", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards interactive learning for manufacturing assistants", "author": ["A. Stopp", "S. Horstmann", "S. Kristensen", "F. Lohnert"], "venue": "Proceedings. 10th IEEE International Workshop on RHIC.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2001}, {"title": "The Open Motion Planning Library", "author": ["I.A. Sucan", "M. Moll", "L.E. Kavraki"], "venue": "IEEE Robotics & Automation Magazine, 19(4):72\u201382", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "Synthesizing object receiving motions of humanoid robots with human motion database", "author": ["K. Tamane", "M. Revfi", "T. Asfour"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient high dimensional maximum entropy modeling via symmetric partition functions", "author": ["P. Vernaza", "J.A. Bagnell"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2012}, {"title": "A bayesian approach for policy learning from trajectory preference queries", "author": ["A. Wilson", "A. Fern", "P. Tadepalli"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2012}, {"title": "Making planned paths look more human-like in humanoid robot manipulation planning", "author": ["F. Zacharias", "C. Schlette", "F. Schmidt", "C. Borst", "J. Rossmann", "G. Hirzinger"], "venue": "Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "AAAI", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2008}, {"title": "and S", "author": ["M. Zucker", "N. Ratliff", "A.D. Dragan", "M. Pivtoraiko", "M. Klingensmith", "C.M. Dellin", "J.A. Bagnell"], "venue": ".S. Srinivasa. Chomp: Covariant hamiltonian optimization for motion planning. International Journal of Robotics Research, 32", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "1Parts of this work has been published at NIPS and ISRR conferences [22, 21].", "startOffset": 68, "endOffset": 76}, {"referenceID": 20, "context": "1Parts of this work has been published at NIPS and ISRR conferences [22, 21].", "startOffset": 68, "endOffset": 76}, {"referenceID": 62, "context": "[63], Sucan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[57], Schulman et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[50]) a priori.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "In this work we propose an algorithm for learning user preferences over trajectories through interactive feedback from the user in a coactive learning setting [51].", "startOffset": 159, "endOffset": 163}, {"referenceID": 3, "context": "Unlike in other learning settings, where a human first demonstrates optimal trajectories for a task to the robot [4], our learning model does not rely on the user\u2019s ability to demonstrate optimal trajectories a priori.", "startOffset": 113, "endOffset": 116}, {"referenceID": 44, "context": "[45]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Over the last decade many planning algorithms have been proposed, such as sampling based planners by Lavalle and Kuffner [33], and Karaman and Frazzoli [26], search based planners by Cohen et al.", "startOffset": 121, "endOffset": 125}, {"referenceID": 25, "context": "Over the last decade many planning algorithms have been proposed, such as sampling based planners by Lavalle and Kuffner [33], and Karaman and Frazzoli [26], search based planners by Cohen et al.", "startOffset": 152, "endOffset": 156}, {"referenceID": 9, "context": "[10], trajectory optimizers by Schul-", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[50], and Zucker et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "[63] and many more [27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[63] and many more [27].", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "Such preferences are often encoded as a cost which planners optimize [26, 50, 63].", "startOffset": 69, "endOffset": 81}, {"referenceID": 49, "context": "Such preferences are often encoded as a cost which planners optimize [26, 50, 63].", "startOffset": 69, "endOffset": 81}, {"referenceID": 62, "context": "Such preferences are often encoded as a cost which planners optimize [26, 50, 63].", "startOffset": 69, "endOffset": 81}, {"referenceID": 0, "context": "Examples of LfD includes, autonomous helicopter flights [1], ball-in-a-cup game [30], planning 2-D paths [43, 44], etc.", "startOffset": 56, "endOffset": 59}, {"referenceID": 29, "context": "Examples of LfD includes, autonomous helicopter flights [1], ball-in-a-cup game [30], planning 2-D paths [43, 44], etc.", "startOffset": 80, "endOffset": 84}, {"referenceID": 42, "context": "Examples of LfD includes, autonomous helicopter flights [1], ball-in-a-cup game [30], planning 2-D paths [43, 44], etc.", "startOffset": 105, "endOffset": 113}, {"referenceID": 43, "context": "Examples of LfD includes, autonomous helicopter flights [1], ball-in-a-cup game [30], planning 2-D paths [43, 44], etc.", "startOffset": 105, "endOffset": 113}, {"referenceID": 1, "context": "In many scenarios, especially involving high DoF manipulators, this is extremely challenging to do [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 44, "context": "[45] the robot observes optimal user feedback but performs approximate inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[62], or locally optimal, as in Levine et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[60] proposed a Bayesian framework for learning rewards of a", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "Our approach advances over [60] and Calinon et.", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "[9] in", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "human gestures) [8, 56] have been employed to teach assembly line robots.", "startOffset": 16, "endOffset": 23}, {"referenceID": 55, "context": "human gestures) [8, 56] have been employed to teach assembly line robots.", "startOffset": 16, "endOffset": 23}, {"referenceID": 38, "context": "[39, 40] in human-robot collaboration learns human preferences over a sequence of sub-tasks in assembly line manufacturing.", "startOffset": 0, "endOffset": 8}, {"referenceID": 39, "context": "[39, 40] in human-robot collaboration learns human preferences over a sequence of sub-tasks in assembly line manufacturing.", "startOffset": 0, "endOffset": 8}, {"referenceID": 54, "context": "[55, 54] and Mainprice et.", "startOffset": 0, "endOffset": 8}, {"referenceID": 53, "context": "[55, 54] and Mainprice et.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[37] planned trajectories satisfying user specified preferences in form of constraints on the distance of the robot from the user, the visibility of the robot and the user\u2019s arm comfort.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] used functional gradients [47] to optimize for legibility of robot trajectories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[14] used functional gradients [47] to optimize for legibility of robot trajectories.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "Several works build upon the sampling-based planner RRT [33] to optimize various cost heuristics [17, 11, 20].", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "Several works build upon the sampling-based planner RRT [33] to optimize various cost heuristics [17, 11, 20].", "startOffset": 97, "endOffset": 109}, {"referenceID": 10, "context": "Several works build upon the sampling-based planner RRT [33] to optimize various cost heuristics [17, 11, 20].", "startOffset": 97, "endOffset": 109}, {"referenceID": 19, "context": "Several works build upon the sampling-based planner RRT [33] to optimize various cost heuristics [17, 11, 20].", "startOffset": 97, "endOffset": 109}, {"referenceID": 25, "context": "Additive cost functions with Lipschitz continuity can be optimized using optimal planners such as RRT* [26].", "startOffset": 103, "endOffset": 107}, {"referenceID": 34, "context": "Some approaches introduce sampling bias [35] to guide the sampling based planner.", "startOffset": 40, "endOffset": 44}, {"referenceID": 46, "context": "Recent trajectory optimizers such as CHOMP [47] and TrajOpt [50] provide optimization based approaches to finding optimal trajectory.", "startOffset": 43, "endOffset": 47}, {"referenceID": 49, "context": "Recent trajectory optimizers such as CHOMP [47] and TrajOpt [50] provide optimization based approaches to finding optimal trajectory.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "[5] and Phillips et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "[41] consider the problem of trajectories for high-dimensional manipulators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Other recent works consider generating human-like trajectories [15, 14, 58].", "startOffset": 63, "endOffset": 75}, {"referenceID": 13, "context": "Other recent works consider generating human-like trajectories [15, 14, 58].", "startOffset": 63, "endOffset": 75}, {"referenceID": 57, "context": "Other recent works consider generating human-like trajectories [15, 14, 58].", "startOffset": 63, "endOffset": 75}, {"referenceID": 52, "context": "[53], Kretzschmar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] and Kitani et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "first visualize them in the OpenRAVE simulator [13] and then decide the kind of feedback they would like to provide.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "This stands in contrast to learning from demonstration (LfD) methods [1, 30, 43, 44] which require (near) optimal demonstrations of the complete trajectory.", "startOffset": 69, "endOffset": 84}, {"referenceID": 29, "context": "This stands in contrast to learning from demonstration (LfD) methods [1, 30, 43, 44] which require (near) optimal demonstrations of the complete trajectory.", "startOffset": 69, "endOffset": 84}, {"referenceID": 42, "context": "This stands in contrast to learning from demonstration (LfD) methods [1, 30, 43, 44] which require (near) optimal demonstrations of the complete trajectory.", "startOffset": 69, "endOffset": 84}, {"referenceID": 43, "context": "This stands in contrast to learning from demonstration (LfD) methods [1, 30, 43, 44] which require (near) optimal demonstrations of the complete trajectory.", "startOffset": 69, "endOffset": 84}, {"referenceID": 1, "context": "Such demonstrations can be extremely challenging and nonintuitive to provide for many high DoF manipulators [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 20, "context": "Instead, we found [21, 22] that it is more intuitive for users to give incremental feedback on high DoF arms by improving upon a proposed trajectory.", "startOffset": 18, "endOffset": 26}, {"referenceID": 21, "context": "Instead, we found [21, 22] that it is more intuitive for users to give incremental feedback on high DoF arms by improving upon a proposed trajectory.", "startOffset": 18, "endOffset": 26}, {"referenceID": 12, "context": "(a) Re-ranking: We display the ranking of trajectories using OpenRAVE [13] on a touch screen device and ask the user to identify whether any of the lower-ranked trajectories is better than the top-ranked one.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "(c) Interactive: For the robots whose hardware does not permit zero-G feedback, such as PR2, we built an alternative interactive Rviz-ROS [18] interface for allowing the users to improve the trajectories by waypoint correction.", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Such linear representation of score functions have been previously used for generating desired robot behaviors [1, 44, 62].", "startOffset": 111, "endOffset": 122}, {"referenceID": 43, "context": "Such linear representation of score functions have been previously used for generating desired robot behaviors [1, 44, 62].", "startOffset": 111, "endOffset": 122}, {"referenceID": 61, "context": "Such linear representation of score functions have been previously used for generating desired robot behaviors [1, 44, 62].", "startOffset": 111, "endOffset": 122}, {"referenceID": 30, "context": "Since it is the attributes [31] of the object that really", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 41, "endOffset": 59}, {"referenceID": 0, "context": "laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 41, "endOffset": 59}, {"referenceID": 0, "context": "laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 64, "endOffset": 82}, {"referenceID": 60, "context": "They comprise the following three types of the features: 1) Robot Arm Configurations: While a robot can reach the same operational space configuration for its wrist with different configurations of the arm, not all of them are preferred [61].", "startOffset": 237, "endOffset": 241}, {"referenceID": 13, "context": "Furthermore, humans like to anticipate robots move and to gain users\u2019 confidence, robot should produce predictable and legible robotic motions [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 30, "context": "In practice, such knowledge can be extracted using an object attribute labeling algorithms such as in [31].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 89, "endOffset": 99}, {"referenceID": 6, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 89, "endOffset": 99}, {"referenceID": 58, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 89, "endOffset": 99}, {"referenceID": 5, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 160, "endOffset": 167}, {"referenceID": 11, "context": "Two effective ways of solving the former problem are either discretizing the state space [3, 7, 59] or directly sampling trajectories from the continuous space [6, 12].", "startOffset": 160, "endOffset": 167}, {"referenceID": 5, "context": "However, for high DoF manipulators the sampling based approach [6, 12] maintains tractability of the problem, hence we take this approach.", "startOffset": 63, "endOffset": 70}, {"referenceID": 11, "context": "However, for high DoF manipulators the sampling based approach [6, 12] maintains tractability of the problem, hence we take this approach.", "startOffset": 63, "endOffset": 70}, {"referenceID": 5, "context": "[6], we sample trajectories using rapidly-exploring random trees (RRT) [33].", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "[6], we sample trajectories using rapidly-exploring random trees (RRT) [33].", "startOffset": 71, "endOffset": 75}, {"referenceID": 47, "context": "[48] propose the use of sub-modularity to achieve diversity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For more details on sampling trajectories we refer interested readers to the work by Erickson and LaValle [16], and Green and Kelly [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "For more details on sampling trajectories we refer interested readers to the work by Erickson and LaValle [16], and Green and Kelly [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 46, "context": "The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [47] or optimal planners like RRT* [26] to produce reasonably good trajectories.", "startOffset": 97, "endOffset": 101}, {"referenceID": 25, "context": "The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [47] or optimal planners like RRT* [26] to produce reasonably good trajectories.", "startOffset": 132, "endOffset": 136}, {"referenceID": 50, "context": "we adapt the Preference Perceptron algorithm [51] as detailed in Algorithm 1, and we call it the Trajectory Preference Perceptron (TPP).", "startOffset": 45, "endOffset": 49}, {"referenceID": 43, "context": "[44].", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "Despite its simplicity and even though the algorithm typically does not receive the optimal trajectory y\u2217 t = arg maxy s (xt, y) as feedback, the TPP enjoys guarantees on the regret [51].", "startOffset": 182, "endOffset": 186}, {"referenceID": 50, "context": "Using these two parameters, the proof by Shivaswamy and Joachims [51] can be adapted (for proof see Appendix A & B) to show that average regret of TPP is upper bounded by:", "startOffset": 65, "endOffset": 69}, {"referenceID": 48, "context": "Our work complements previous works on grasping items [49, 34], pick and place tasks [23], and detecting bar codes for grocery checkout [29].", "startOffset": 54, "endOffset": 62}, {"referenceID": 33, "context": "Our work complements previous works on grasping items [49, 34], pick and place tasks [23], and detecting bar codes for grocery checkout [29].", "startOffset": 54, "endOffset": 62}, {"referenceID": 22, "context": "Our work complements previous works on grasping items [49, 34], pick and place tasks [23], and detecting bar codes for grocery checkout [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 28, "context": "Our work complements previous works on grasping items [49, 34], pick and place tasks [23], and detecting bar codes for grocery checkout [29].", "startOffset": 136, "endOffset": 140}, {"referenceID": 32, "context": "\u2022 Geometric: The robot plans a path, independent of the task, using a Bi-directional RRT (BiRRT) [33] planner.", "startOffset": 97, "endOffset": 101}, {"referenceID": 43, "context": "\u2022 MMP-online: This is an online implementation of the Maximum Margin Planning (MMP) [44, 46] algorithm.", "startOffset": 84, "endOffset": 92}, {"referenceID": 45, "context": "\u2022 MMP-online: This is an online implementation of the Maximum Margin Planning (MMP) [44, 46] algorithm.", "startOffset": 84, "endOffset": 92}, {"referenceID": 43, "context": "However, directly adapting MMP [44] to our experiments poses two challenges: (i) we do not have knowledge of the optimal trajectory; and (ii) the state space of the manipulator we consider is too large, discretizing which makes intractable to train MMP.", "startOffset": 31, "endOffset": 35}, {"referenceID": 43, "context": "To ensure a fair comparison, we follow the MMP algorithm from [44, 46] and train it under similar settings as TPP.", "startOffset": 62, "endOffset": 70}, {"referenceID": 45, "context": "To ensure a fair comparison, we follow the MMP algorithm from [44, 46] and train it under similar settings as TPP.", "startOffset": 62, "endOffset": 70}, {"referenceID": 24, "context": "At every iteration MMP-online trains a structural support vector machine (SSVM) [25] using all previous feedback as training examples, and use the learned weights to predict trajectory scores in the next iteration.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "[25]) end for", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "To quantify the quality of a ranked list of trajectories we report normalized discounted cumulative gain (nDCG) [38] \u2014 criterion popularly used in Information Retrieval for document ranking.", "startOffset": 112, "endOffset": 116}, {"referenceID": 23, "context": "We refer to this algorithm as Oracle-svm and it learns to rank trajectories using SVM-rank [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 41, "context": "In situations where this difference is significant and a system is desired for a user population, a future work might explore coactive learning for satisfying user population, similar to Raman and Joachims [42].", "startOffset": 206, "endOffset": 210}, {"referenceID": 51, "context": "better user interfaces [52] could further reduce this time.", "startOffset": 23, "endOffset": 27}, {"referenceID": 50, "context": "This proof builds upon Shivaswamy & Joachims [51].", "startOffset": 45, "endOffset": 49}], "year": 2016, "abstractText": "We consider the problem of learning preferences over trajectories for mobile manipulators such as personal robots and assembly line robots. The preferences we learn are more intricate than simple geometric constraints on trajectories; they are rather governed by the surrounding context of various objects and human interactions in the environment. We propose a coactive online learning framework for teaching preferences in contextually rich environments. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this coactive preference feedback can be more easily elicited than demonstrations of optimal trajectories. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We implement our algorithm on two high degree-of-freedom robots, PR2 and Baxter, and present three intuitive mechanisms for providing such incremental feedback. In our experimental evaluation we consider two context rich settings \u2013 household chores and grocery store checkout \u2013 and show that users are able to train the robot with just a few feedbacks (taking only a few minutes).", "creator": "LaTeX with hyperref package"}}}