{"id": "1601.05647", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2016", "title": "On Structured Sparsity of Phonological Posteriors for Linguistic Parsing", "abstract": "The speech signal conveys information on different time scales from short (20-40 ms) time scale or segmental, associated to phonological and phonetic information to long (150-250 ms) time scale or supra segmental, associated to syllabic and prosodic information. Linguistic and neurocognitive studies recognize the phonological classes at segmental level as the essential and invariant representations used in speech temporal organization. In the context of speech processing, a deep neural network (DNN) is an effective computational method to infer the probability of individual phonological classes from a short segment of speech signal. A vector of all phonological class probabilities is referred to as phonological posterior. There are only very few classes comprising a short term speech signal; hence, the phonological posterior is a sparse vector. Although the phonological posteriors are estimated at segmental level, we claim that they convey supra-segmental information. Namely, we demonstrate that phonological posteriors are indicative of syllabic and prosodic events. Building on findings from converging linguistic evidence on the gestural model of Articulatory Phonology as well as neural basis of speech perception, we hypothesize that phonological posteriors convey properties of linguistic classes at multiple time scales, and this information is embedded in their support (index) of active coefficients. To verify this hypothesis, we obtain a binary representation of phonological posteriors at segmental level which is referred to as first-order sparsity structure; the high-order structures are obtained by concatenation of first-order binary vectors. It is then confirmed that classification of supra-segmental linguistic events, the problem known as linguistic parsing, can be achieved with high accuracy using a simple binary pattern matching of first-order or high-order structures.", "histories": [["v1", "Thu, 21 Jan 2016 14:15:41 GMT  (803kb,D)", "https://arxiv.org/abs/1601.05647v1", null], ["v2", "Wed, 18 May 2016 14:08:02 GMT  (818kb,D)", "http://arxiv.org/abs/1601.05647v2", null], ["v3", "Tue, 30 Aug 2016 09:23:58 GMT  (836kb,D)", "http://arxiv.org/abs/1601.05647v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["milos cernak", "afsaneh asaei", "herv\\'e bourlard"], "accepted": false, "id": "1601.05647"}, "pdf": {"name": "1601.05647.pdf", "metadata": {"source": "CRF", "title": "On Structured Sparsity of Phonological Posteriors for Linguistic Parsing", "authors": ["Milos Cernaka", "Afsaneh Asaeia", "Herv\u00e9 Bourlard"], "emails": ["milos.cernak@idiap.ch", "afsaneh.asaei@idiap.ch", "herve.bourlard@idiap.ch"], "sections": [{"heading": null, "text": "The speech signal conveys information on different time scales from short (20\u201340 ms) time scale or segmental, associated to phonological and phonetic information to long (150\u2013250 ms) time scale or supra segmental, associated to syllabic and prosodic information. Linguistic and neurocognitive studies recognize the phonological classes at segmental level as the essential and invariant representations used in speech temporal organization.\nIn the context of speech processing, a deep neural network (DNN) is an effective computational method to infer the probability of individual phonological classes from a short segment of speech signal. A vector of all phonological class probabilities is referred to as phonological posterior. There are only very few classes comprising a short term speech signal; hence, the phonological posterior is a sparse vector. Although the phonological posteriors are estimated at segmental level, we claim that they convey supra-segmental information. Specifically, we demonstrate that phonological posteriors are indicative of syllabic and prosodic events.\nBuilding on findings from converging linguistic evidence on the gestural model of Articulatory Phonology as well as the neural basis of speech perception, we hypothesize that phonological posteriors convey properties of linguistic classes at multiple time scales, and this information is embedded in their support (index) of active coefficients. To verify this hypothesis, we obtain a binary representation of phonological posteriors at the segmental level which is referred to as first-order sparsity structure; the high-order structures are obtained by the concatenation of first-order binary vectors. It is then confirmed that the classification of supra-segmental linguistic events, the problem known as linguistic parsing, can be achieved with high accuracy using a simple binary pattern matching of first-order or high-order structures.\nKeywords: Phonological posteriors, Structured sparse representation, Deep neural network (DNN), Binary pattern matching, Linguistic parsing.\n\u2217Corresponding authors; both authors contributed equally to this manuscript. Email addresses: milos.cernak@idiap.ch (Milos Cernak), afsaneh.asaei@idiap.ch (Afsaneh Asaei),\nherve.bourlard@idiap.ch (Herve\u0301 Bourlard)\nar X\niv :1\n60 1.\n05 64\n7v 3\n[ cs\n.C L\n] 3\n0 A\nug 2\n01 6"}, {"heading": "1. Introduction", "text": "A theory of Articulatory Phonology (Browman and Goldstein, 1986) suggests that an utterance is described by temporally overlapped distinctive constriction actions of the vocal tract organs, known as gestures. Gestures are changes in the vocal tract, such as opening and closing, widening and narrowing, and they are phonetic in nature (Fowler et al., 2015). Gestures compose units of information and can be used to distinguish words in all languages. Recent work on Articulatory Phonology (Goldstein and Fowler, 2003) further suggests an existence of coupling/synchronisation of gestures that influence the syllable structure of an utterance.\nPhonological classes (e.g., (Jakobson and Halle, 1956; Chomsky and Halle, 1968)) emerge during the phonological encoding process \u2013 the processes of speech planning for articulation, namely the preparation of an abstract phonological code and its transformation into speech motor plans that guide articulation (Levelt, 1993). Stevens (2005) reviews evidence about a universal set of phonological classes that consists of articulator-bound classes and articulator-free classes ([continuant], [sonorant], [strident]). We support the hypothesis in this work and consider phonological classes in our work as essential and invariant acousticphonetic elements used in both linguistics and cognitive neuroscience studies for speech temporal organization.\nIn the present paper, we study inferred phonological posterior features that consist of phonological class probabilities given a segment of input speech signal. The class-conditional posterior probabilities are estimated using a Deep Neural Network (DNN). Cernak et al. (2015b) introduce the phonological posterior features for phonological analysis and synthesis, and we hypothesise their relation to the linguistic gestural model. Saltzman and Munhall (1989) describe the constriction dynamics model as a computational system that incorporates the theory of articulatory phonology. This gestural model defines gestural scores as the temporal activation of each gesture in an utterance. Thus, we hypothesise that phonological posteriors are related to gestural scores and that the trajectories of phonological posteriors correspond to the distal representation of articulatory gestures. In a broader view, we consider the trajectories of phonological posteriors as articulatory-bound and articulatory-free gestures. Since gestures are linguistically relevant (Liberman and Whalen, 2000), we hypothesize that phonological posteriors should convey supra-segmental information through their inter-dependency low-dimensional structures. Hence, by characterizing the structure of phonological posteriors, it should be possible to perform a top-down linguistic parsing, i.e., by knowing a priori where linguistic boundaries lie.\nPreviously in (Asaei et al., 2015), we have shown that phonological posteriors admit sparsity structures underlying short-term segmental representations where the structures are quantified as sparse binary vectors. In this work, we explore this idea further and consider trajectories of phonological posteriors for supra-segmental structures. We show that unique structures (codes) exist for distinct linguistic classes and\nidentification of these structures enables us to perform linguistic parsing. The linguistic parsing is thus achieved through identification of low dimensional sparsity structures of phonological posteriors followed by binary pattern matching. This idea is in line with an assumption that physical and cognitive speech structures are, in fact, the low and high dimensional descriptions of a single (complex) system1.\nOur contribution to advance the study of phonological posteriors is two-fold: First, we review converging evidence from linguistic and neural basis of speech perception, that support the hypothesis about phonological posteriors conveying properties of linguistic classes at multiple time scales. Second, we propose linguistic parsing based on structured sparsity as low dimensional characterization of phonological posteriors.\nThe rest of the paper is organized as follows. Section 2 provides review of the definition and relation of phonological posteriors to the linguistic gestural model and subsequently to cognitive neuroscience, Section 3 introduces linguistic parsing, and Section 4 presents the details of experimental analysis. Finally, Section 5 concludes the paper and discusses the results in a broader context."}, {"heading": "2. Phonological Class-conditional Posteriors", "text": "Figure 1 illustrates the process of the phonological analysis (Yu et al., 2012; Cernak et al., 2015b). The phonological posterior features are extracted starting with converting a segment of speech samples into a sequence of acoustic features X = {~x1, . . . , ~xn, . . . , ~xN} where N denotes the number of segments in the utterance. Conventional cepstral coefficients can be used as acoustic features. Then, a bank of phonological class analysers realised via neural network classifiers converts the acoustic feature observation sequence X into a sequence of phonological posterior probabilities Z = {~z1, . . . , ~zn, . . . , ~zN}; a posterior probability ~zn = [p(c1|xn), . . . , p(ck|xn), . . . , p(cK |xn)]> consists of K phonological class-conditional posterior probabilities where ck denotes the phonological class and . > stands for the transpose operator.\n1http://www.haskins.yale.edu/research/gestural.html\nThe phonological posteriors Z yield a parametric speech representation, and we hypothesise that the trajectories of the articulatory-bound phonological posteriors correspond to the distal representation of the gestures in the gestural model of speech production (and perception). For example, Figure 2 shows a comparison of articulatory tongue tip gestures (vertical direction with respect to the occlusal plane) and the phonological anterior posterior features, on an electromagnetic articulography (EMA) recording (Lee et al., 2005). The articulatory gesture and phonological posteriors trajectory have the same number of maximums, and their relation is evident. A more asynchronous relation toward the end of utterance is caused by asynchronous relations of fast tongue tip movements causing an obstruction in the mouth and generated acoustics.\nThe hypothesis of correspondence of the phonological posterior features to the gestural trajectories is also motivated by the analogy to the constriction dynamics model (Saltzman and Munhall, 1989) that takes gestural scores as input and generates articulator trajectories and acoustic output. Alternatively to this constriction dynamics model, we generate acoustic output using a phonological synthesis DNN described in Cernak et al. (2015b).\nIn the following sections, we outline converging evidence from linguistics as well as the neural basis of speech perception, that support the hypothesis about phonological posteriors conveying properties of linguistic classes at multiple time scales."}, {"heading": "2.1. Linguistic Evidence", "text": "Linguistics defines two traditional components of speech structures:\n1. Cognitive structure consisting of system primitives, that is, the units of representation for cognitively\nrelevant objects such as phonemes or syllables. The system primitives are represented by canonical phonological features (classes) that emerge during the phonological encoding process (Levelt, 1993).\n2. Physical structure generated by a set of permissible operations over cognitive system primitives that\nyield the observed (surface) patterns. The physical structure is represented by surface phonological features, continuous variables that may be partially estimated from the speech signal by inverse filtering. Phonological posteriors can also be classified as surface phonological features.\nThe canonical (discrete) phonological features have been used over the last 60 years to describe cognitive structures of speech sounds. Miller and Nicely (1955) have experimentally shown that consonant confusions were perceived similarly for the isolated phonemes and the phonemes within the perceptual groups they form (for example /t/, /p/ and /k/ form a perceptual group, characterised by binary features of voiceless stops). Canonical features are extensively studied in phonology. In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles \u2013 the Sound Pattern of English (SPE). Later advanced phonological systems were proposed, such as multi-valued phonological features of Ladefoged and Johnson (2014), and monovalent Government Phonology features of Harris and Lindsey (1995) that describe sounds by fusing and splitting of primes.\nThe surface code includes co-articulated canonical code, with further intrinsic (speaker-based) and extrinsic (channel-based) speech variabilities that contribute to the opacity of the function operating between the two codes. The surface features may contain additional gestures dependent on the prosodic context, such as position within a syllable, word, and sentence. Other changes in surface phonological features at different time granularities are due to phonotactic constraints. For example, glides are always syllable-initial, and consonants that follow a non-tense vowel are always in the coda of the syllable (Stevens, 2005).\nRelation of the canonical and surface code can be investigated by a linguistic theory of Articulatory Phonology (Browman and Goldstein, 1986, 1989, 1992) that introduced articulatory gestures as a basis for human speech production. Although it is generally claimed that gestures convey segmental-level information (for example, Fowler et al. (2015) say that gestures are phonetic in nature), recent developments suggest that the timing of articulatory gestures encodes syllabic (and thus linguistic) information as well (Browman and Goldstein, 1988; Nam et al., 2009). Liberman and Whalen (2000) provides theoretic claims about linguisticlly relevant articulatory gestures, and Saltzman and Munhall (1989) implement a syllable structure-based gesture coupling model. Thus, from prior evidence that articulatory gestures convey linguistic properties, and the hypothesis of correspondence of the phonological posteriors to the gestural trajectories, we claim\nthat phonological posteriors may convey linguistic information as well."}, {"heading": "2.2. Cognitive Neuroscience Evidence", "text": "Modern cognitive neuroscience studies use phonological classes as essential and invariant acoustic-phonetic primitives for speech temporal organization (Poeppel, 2014). Neurological data from the brain activity during speech planning, production or perception are increasingly used to inform such cognitive models of speech and language.\nThe auditory pre-processing is done in the cochlea, and then split into two parallel pathways leading from the auditory system (Wernicke, 1874/1969). For example, the dual-stream model of the functional anatomy of language (Hickok and Poeppel, 2007) consists of a ventral stream: sound to meaning function using phonological classes, phonological-level processing at superior temporal sulcus bilaterally, and a dorsal stream: sound to action, a direct link between sensory and motor representations of speech based again on the phonological classes. The former stream supports the speech perception, and the latter stream reflects the observed disruptive effects of altered auditory feedback on speech production. Phillips et al. (2000); Mesgarani et al. (2014) present evidence of discrete phonological classes available in the human auditory cortex.\nRecent evidence from psychoacoustics and neuroimaging studies indicate that auditory cortex segregates information emerging from the cochlea on at least three discrete time-scales processed in the auditory cortical hierarchy: (1) \u201cstress\u201d \u03b4 frequency (1\u20133 Hz), (2) \u201csyllabic\u201d \u03b8 frequency (4\u20138 Hz) and (3) \u201cphonetic\u201d low \u03b3 frequency (25\u201335 Hz) (Giraud and Poeppel, 2012). Leong et al. (2014) show that phase relations between the phonetic and syllabic amplitude modulations, known as hierarchical phase locking and nesting or synchronization across different temporal granularity (Lakatos et al., 2005), is a good indication of the syllable stress. Intelligible speech representation with stress and accent information can be constructed by asynchronous fusion of phonetic and syllabic information (Cernak et al., 2015a).\nIn addition, not only phase locking across different temporal granularity has linguistic interpretation. Bouchard et al. (2013) claim that functional organisation of ventral sensorimotor cortex supports the gestural model developed in Articulatory Phonology. Analysis of spatial patterns of activity showed a hierarchy of network states that organizes phonemes by articulatory-bound phonological features. Leonard et al. (2015) further show how listeners use phonotactic knowledge (phoneme sequence statistics) to process spoken input and to link low-level acoustic representations (the coarticulatory dynamics of the sounds through the encoding of combination of phonological features) with linguistic information about word identity and meaning. This is converging evidence on the relation of the linguistic gestural model and speech and language cognitive neuroscience models with the phonological class-conditional posteriors used in our work."}, {"heading": "3. Sparse Phonological Structures for Linguistic Parsing", "text": "Building on linguistic and cognitive findings, the phonological representation of speech lies at the center of human speech processing. Speech analysis is performed at different time granularities broadly categorized as segmental and supra-segmental levels. The phonological classes define the sub-phonetic and phonetic attributes recognized at the segmental level whereas the syllables, lexical stress and prosodic accent are the basic supra-segmental events - c.f. Figure 3. The phonological representations are often studied at segmental level and their supra-segmental properties are not investigated. This supra-segmental characterization of phonological posteriors will be explored in this work."}, {"heading": "3.1. Structured Sparsity of Phonological Posteriors", "text": "Phonological posteriors are indicators of the physiological posture of human articulation machinery. Due to the physical constraints, only few combinations can be realized in our vocalization. This physical limitation leads to a small number of unique patterns exhibited over the entire speech corpora (Asaei et al., 2015). We refer to this structure as first-order structure which is exhibited at the segmental level.\nMoreover, the dynamics of the structured sparsity patterns is slower than the short segments and it is indicative of supra-segmental information, leading to a higher order structure underlying a sequence (trajectory) of phonological posteriors. This structure is exhibited at supra-segmental level by analyzing a long duration of phonological posteriors, and it is associated to the syllabic information or more abstract linguistic attributes. We refer to this structure as high-order structure.\nWe hypothesize that the first-order and high-order structures underlying phonological posteriors can be exploited as indicators of supra-segmental linguistic events. To test this hypothesis, we identify all structures exhibited in different linguistic classes. The set of class-specific structures is referred to as the codebook."}, {"heading": "3.2. Codebook of Linguistic Structures", "text": "The goal of codebook construction is to collect all the structures associated to a particular linguistic event. To that end, we consider binary phonological posteriors where the probabilities above 0.5 are normalized\nto 1 and the probabilities less than 0.5 are forced to zero. This rounding procedure enables us to identify the active phonological components as indicators of linguistic events. It also alleviates the speaker and environmental variability encoded in the continuous probabilities. An immediate extension to this approach is multi-valued quantization of phonological posteriors as opposed to 1-bit quantization. We consider this extension for our future studies and focus on binary phonological indicators to obtain linguistic structures.\nDifferent codebooks are constructed for different classes. Namely, one codebook encapsulates all the binary structures of the consonants whereas another codebook has all the binary structures of the vowels. These two codebooks will be used for binary pattern matching to classify consonants versus vowels as will be explained in the next Section 3.3. Likewise, one codebook encapsulates all the binary structures of stressed syllables whereas another codebook has all the binary structures of unstressed syllables, and these two codebooks are used for stress detection; a similar procedure holds for accent detection.\nThe codebook can be constructed from the first-order structures as well as the high-order structures. For example, a second-order codebook is formed from all the binary structures of second-order phonological posteriors obtained by concatenation of two adjacent phonological posteriors to form a super vector from the segmental representations.\nThe procedure of codebook construction for classification of linguistic events relies on the assumption that there are unique structures per class (consonant, stressed or syllable) and the number of permissible patterns is small (see 4.2.2 for empirical results). Hence, classification of any phonological posterior can be performed by finding the closest match to its binary structure from the codebooks characterizing different linguistic classes."}, {"heading": "3.3. Pattern Matching for Linguistic Parsing", "text": "Figure 3 illustrates different time granularity identified for processing of speech. Inferring the suprasegmental properties such as syllable type or accented / stressed pronunciation is known as linguistic parsing (Poeppel, 2003). Parsing can be performed in a top-down procedure, driven by a-priori known segment boundaries.\nHaving the codebooks of structures underlying phonological posteriors, linguistic parsing amounts to binary pattern matching. The similarity metric plays a critical role in classification accuracy. Hence, we investigate several metrics found effective in different binary classification settings. The definition of binary similarity measures are expressed by operational taxonomic units (Dunn and Everitt, 1982). Consider two binary vectors i, j: a denotes the number of elements where the values of both i, j are 1, meaning \u201cpositive match\u201d; b denotes the number of elements where the values of i, j is (0, 1), meaning \u201ci absence mismatch\u201d; c denotes the number of elements where the values of i, j is (1, 0), meaning \u201cj absence mismatch\u201d; d denotes the number of elements where the values of both i, j are 0, meaning \u201cnegative match\u201d. The definition of binary similarity measures used for our evaluation of linguistic parsing is as follows (Choi and Cha, 2010):\nSJACCARD = a\na+ b+ c (1)\nSINNERPRODUCT = a+ d (2)\nSHAMMING = b+ c (3) SAMPLE = a(c+ d)\nc(a+ b) (4)\nSSIMPSON = a\nmin(a+ b, a+ c) (5)\nSHELLINGER = 2\n\u221a 1\u2212 a\u221a\n(a+ b)(a+ c) (6)\nDifferent metrics are motivated due to different treatment of positive/negative match and mismatches in indicators of phonological classes. The most effective similarity measure for linguistic parsing can imply different cognitive mechanisms governing the human perception of linguistic attributes.\nIn the top-down approach to linguistic parsing, syllable boundaries are first estimated from the speech signal. Then, the similarity between the class-specific codebook members and a phonological posterior is measured. The class label is determined based on the maximum similarity. We provide empirical results on linguistic parsing in the following Section 4."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Experimental setup", "text": "We use an open-source phonological vocoding platform (Cernak and Garner, 2016) to obtain phonological posteriors. Briefly, the platform is based on cascaded speech analysis and synthesis that works internally with the phonological speech representation. In the phonological analysis part, phonological posteriors are detected directly from the speech signal by DNNs. Binary (Yu et al., 2012) or multi-valued classification (Stouten and Martens, 2006; Rasipuram and Magimai.-Doss, 2011) might be used. In the latter case, the phonological classes are grouped together based on place or manner of articulation. We followed the binary classification approach in our work, and thus each DNN determines the probability of a particular phonological class. To confirm independence of the proposed methodology on a phonological system, two different phonological speech representations are considered: the SPE feature set (Chomsky and Halle, 1968), and the extended SPE (eSPE) feature set (Cernak et al., 2015b) are used in training of the DNNs for phonological posterior estimation on English and French data respectively. The mapping used to map from phonemes to SPE phonological class is taken from Cernak et al. (2016b). The distribution of the phonological labels is non-uniform, driven by mapping different numbers of phonemes to the phonological classes.\nFor French eSPE feature set, we started from pseudo-phonological feature classification designed for American English (Yu et al., 2012). We deleted the glottal and dental classes corresponding to English phonemes [h, D, T], replaced [+Retroflex] with [+Uvular] to represent a French rhotic consonant, and replaced the broad classes [+Continuant, +Tense] with:\n\u2022 Fortis and Lenis, as an alternative to [+Tense] class, to distinguish consonants produced with greater\nand lesser energy, or articulation strength.\n\u2022 Alveolar and Postalveolar, to distinguish between sibilants articulated by anterior portion of the tongue,\n\u2022 Dorsal, to group consonants articulated by the central and posterior portions of the tongue,\n\u2022 Central, to group vowels in the central position of the portion of tongue that is involved in the\narticulation and to the tongue\u2019s position relative to the palate (Bauman-Waengler, 2011),\n\u2022 Unround, to group vowels with an opposite degree of lip rounding to the [+Round] class.\nIn the following, we describe the databases and DNN training procedure to estimate the phonological\nposterior features."}, {"heading": "4.1.1. Speech Databases", "text": "To confirm cross-lingual property of phonological posteriors (Siniscalchi et al., 2012), we conducted our\nevaluations on English and French speech corpora. Table 1 lists data used in the experimental setup.\nTo train the DNNs for phonological posterior estimation on English data, we use the Wall Street Journal (WSJ0 and WSJ1) continuous speech recognition corpora (Paul and Baker, 1992). To train the DNNs for phonological posterior estimation on French data, we use the Ester database (Galliano et al., 2006) containing standard French radio broadcast news in various recording conditions.\nOnce DNNs are trained, the phonological posterior features are estimated for the Nancy and SIWIS\nrecordings which is used for the subsequent cross-database linguistic parsing experiments.\nThe Nancy database is provided in Blizzard Challenge2. The speaker is known as \u201cNancy\u201d, and she is a US English native female speaker. The database consists of 16.6 hours of high quality recordings of natural expressive human speech recorded in an anechoic chamber at a 96 kHz sampling rate. The audio of the last 1.5 hours of the recordings was selected and re-sampled to sampling frequency of 16 kHz for our experiments. The transcription of the audio data comprised of 12 095 utterances. The text was processed by a conventional and freely available text to speech synthesis (TTS) front-end (Black et al., 1997), resulting in segmental (quinphone phonetic context) and supra-segmental (full-context) labels. The full-context labels included binary lexical stress and prosodic accents. In this work, by the term stress, we refer to the lexical stress of a word, which is the stress placed on syllables within words. On the other hand, accent refers to the phrase- or sentence- level prominence given to a syllable. The syllables conveying phrasal prominence are called pitch accented syllables. In some cases, a stressed syllable can be promoted to pitch accented syllable based just on its position in a phrase or on the focus/emphasis the speaker intends to give to the specific part of the sentence to convey a specific message (Matt, 2014). Accents are predicted from the text transcriptions, using features that affect accenting, such as lexical stress, part-of-speech, and ToBI labels. The labels were force-aligned with the audio recordings.\nThe SIWIS database3 consists of 26 native French speakers. The labels were obtained using forced alignment. We generated full-context labels using the French text analyzer eLite (Roekhaut et al., 2014). Unlike Nancy speech recordings, SIWIS data is noisy and recorded in less restricted acoustic conditions. Evaluations on both English and French corpora enables us to confirm and compare the applicability of our linguistic parsing method across languages with different phonological classes as well as different recording scenarios."}, {"heading": "4.1.2. DNN Training for Phonological Posterior Estimation", "text": "First, we trained a phoneme-based automatic speech recognition system using mel frequency cepstral coefficients (MFCC) as acoustic features. The phoneme set comprising of 40 phonemes (including \u201csil\u201d, representing silence) was defined by the CMU pronunciation dictionary. The three-state, cross-word triphone models were trained with the HMM-based speech synthesis system (HTS) variant (Zen et al., 2007) of the Hidden Markov Model Toolkit (HTK) on the 90% subset of the WSJ si tr s 284 set. The remaining 10% subset was used for cross-validation. We tied triphone models with decision tree state clustering based on the minimum description length (MDL) criterion (Shinoda and Watanabe, 1997). The MDL criterion allows an unsupervised determination of the number of states. We used 12 685 tied models, and modeled each state with a GMM consisting of 16 Gaussians. The acoustic models were used to get boundaries of the phoneme labels.\n2http://www.cstr.ed.ac.uk/projects/blizzard/2011/lessac_blizzard2011 3https://www.idiap.ch/project/siwis/downloads/siwis-database\nThen, the labels of phonemes were mapped to the SPE phonological classes. In total, K DNNs were trained as the phonological analyzers using the short segment (frame) alignment with two output labels indicating whether the k-th phonological class exists for the aligned phoneme or not. In other words, the two DNN outputs correspond to the target class v/s the rest. Some classes might seem to have unbalanced training data, for example, the two labels for the nasal class are associated with the speech samples from just 3 phonemes /m/, /n/, and /N/, and with the remaining 36 phonemes. However, this split is necessary to train a discriminative classifier well. Each DNN was trained on the whole training set. The number of classifiers K is determined from the set of phonological classes and it is equal to 15 for the English data, and 24 for the French data. The DNNs have the architecture of 351 \u00d7 1024 \u00d7 1024 \u00d7 1024 \u00d7 2 neurons, determined empirically (we tried different sizes, from 500 to 2000, and deep, from 3 to 5 hidden layers, and different context of the input features). The input vectors are 39 order MFCC features with the temporal context of 9 successive frames. The parameters were initialized using deep belief network pre-training done by single-step contrastive divergence (CD-1) procedure of Hinton et al. (2006) on randomly selected 5000 utterances from the training data. The DNNs with the softmax output function were then trained using a mini-batch based stochastic gradient descent algorithm with the cross-entropy cost function of the KALDI toolkit (Povey et al., 2011). Table 2 lists the detection accuracy for different phonological classes. The DNN outputs provide the phonological posterior probabilities for each phonological class. Detection accuracy on cross-database (Nancy) test data is lower, however following the accuracy on training and CV data. The consonantal and voice DNN performed worse, and we speculate that this might be caused by difficulty to train good classifiers for such broad (hierarchical) phonological classes. Recent work of Nagamine et al. (2015) suggests that a DNN learns selective phonological classes in different nodes and hidden layers. Thus, using features from the hidden layers could improve classification accuracy of some specific phonological classes.\nSimilarly, we trained French phonological posterior estimators. In this study, we retained a subset of Ester consisting of native speakers in low noise conditions. The three-state, cross-word triphone models were trained on 93% of the data. The remaining 7% subset was used for cross-validation. The acoustic models were used to get boundaries of the phoneme labels. We tied triphone models with the MDL criterion that resulted in the 11 504 tied models, modelling each state with a GMM consisting of 16 Gaussians.\nThe phoneme set comprising 38 phonemes (including \u201csil\u201d) was defined by the BDLex (Perennou, 1986) lexicon. The aligned phoneme labels were mapped to the French eSPE phonological classes. The DNN architecture is similar to the English data, and it is initialized by deep belief network pre-training. Table 3 lists the detection accuracy for various eSPE classes. Similarly as for English, some class detectors evaluated on cross-database (Siwis) test data show lower performance. The detectors for the \u201cbroader\u201d classes, such as vowel or voiced, perform worse."}, {"heading": "4.2. Linguistic Parsing", "text": "In this section, we present the evaluation results of our proposed method of top-down linguistic parsing. We provide empirical results on sparsity of phonological posteriors and confirm validity of class-specific codebooks to classify supra-segmental linguistic events based on binary pattern matching."}, {"heading": "4.2.1. Binary Sparsity of Phonological Posteriors", "text": "Figure 4 illustrates a histogram of phonological posteriors distribution. We can see that the distribution exhibits the binary nature of phonological posterior being valued in the range of [0\u2212 1], and mostly concentrated very close to either 1 or 0. This binary pattern is visible for both stressed and unstressed syllables as demonstrated in the right and left plots, respectively.\nThe 1-bit discretization, achieved by rounding of posteriors results in a very small number of unique phonological binary structures, 0.1% of all possible structures. This implies that the binary patterns may encode particular shapes of the vocal tract. Since a limited number of these shapes can be created for human speech, the number of unique patterns is very small.\nThis property encouraged us also to use this binary approximation in low bit-rate speech coding (Cernak et al., 2015b; Asaei et al., 2015); these studies confirmed that binary approximation has only a negligible impact on perceptual speech quality.\nFurthermore, comparing Figures 4a and 4b, we can observe that at least the [low] (6th), [round] (9th) and [rising] (10th) classes are significantly more present in stressed binary-ones than in unstressed syllables. This observation indicates that stressed syllables are more prominent in prosodic typology (e.g., (Jun, 2005)). We use the [rising] feature to differentiate diphthongs from monophthongs, that is also more prominent in stressed syllables."}, {"heading": "4.2.2. Class-specific Linguistic Structures", "text": "structures which can be used for identification of supra-segmental linguistic events.\nFollowing the procedure of codebook construction elaborated in Section 3.2, we obtain six different\ncodebooks to address the following parsing scenarios:\n\u2022 Consonant vs. vowel (C-V) detection.\n\u2022 Stress vs. unstressed detection.\n\u2022 Accented vs. unaccented detection.\nThe cardinality of each codebook equals the number of unique class-specific binary structures. The number of unique structures is indeed a small fraction of the whole speech data. For example, the ratio of unique binary structures for the whole Nancy database (16.6 hours of speech) is about 0.08% of the total number of phonological posteriors. Figure 5 illustrates the distribution of binary phonological posteriors in the individual codebooks along with the number of codes. Comparing the codebook pairs, we can identify distinct patterns, for example more frequent consonantal features in the consonantal codebook, and voice features in the vowel codebook. Furthermore, we can see the number of codes in Stressed vs Unstressed\ncodebooks are far less balanced compared to the C-V and Accented/Unaccented codebooks. It might imply that the stressed phonological codes are far more restricted, and phonological pattern matching can potentially be an effective method for this task. This idea is further investigated in Cernak et al. (2016a) where a highly competitive emphasis detection system is achieved.\nThe detection method relies on binary pattern matching and the codebook with a member which possesses maximum similarity to the phonological posterior determines its supra-segmental linguistic property, i.e. being a consonant or vowel, stressed or unstressed and accented or unaccented. The three parsing scenarios are tested separately so the linguistic parsing amounts to a binary classification problem.\nWe process each speech segment independently. To obtain a decision for the supra-segmental events from the segmental labels, the labels of all the segments comprising a supra-segmental event are pulled to form a decision based on majority counting. In other words, the number of segments being recognized as a particular event is counted, and the final supra-segmental label is decided according to the maximum count. If the similarities of a binary phonological posterior to both codebooks are equal, the segment is not labeled.\nSince we devise a top-down parsing mechanism, we use the knowledge of supra-segmental boundaries to determine the underlying linguistic event. Beyond the supra-segmental boundaries, no other information is available, and the linguistic event is classified merely based on majority count of the segments classified by pattern matching within the a priori known boundary.\nTo perform pattern matching, the similarity measure of binary structures must be quantified. There\nare many metrics formulated for this purpose (Choi and Cha, 2010) which differ mainly in the way that positive/negative match or different mismatches are addressed. We conducted thorough tests on the metrics defined in (Choi and Cha, 2010); Figure 6 compares and contrasts a few representative results.\nWe can see that the fast and simple innerproduct is the most effective similarity metric; it quantifies the positive and negative matches between the two binary structures. On the other hand, Hamming similarity measure that quantifies the mismatches does not perform well for linguistic parsing. The Jaccard (2) formula yields similar results to innerproduct. Hence, we choose the innerproduct for its efficiency in our linguistic parsing evaluation. Table 4 lists the accuracy of different parsing scenarios for English data provided in recordings from Nancy and French data available in SIWIS database.\nThe results are averaged over 5-fold random selection of length 1000 consecutive segments. Accordingly, the codebooks are constructed using the selected data. The high-order structured sparsity patterns are obtained by concatenating each segment with its adjacent segments on the right, and the context size denotes the number of extra segments concatenated. We can see that the higher order structured sparsity patterns enables more accurate linguistic parsing. It also confirms that the proposed structured sparsity principle is independent of language as well as phonological class definitions.\nThe differences in stress and accent detection for English and French might be related to our test data and their ground truth labels. Although French stress prediction from text is probably easier, there might still be some mismatch between the labels and recordings. In addition, the SIWIS recordings used for evaluation\nof prosodic parsing typically contained one emphasized word per utterance. Because stress is the relative prominence within words and utterances, the SIWIS speakers might emphasize certain words to express word prominence that could lead to de-emphasis of some syllable prominences we evaluated. The lower performance of C-V detection might be caused by lower performance of consonantal- and vocalic-related DNNs, that is partially compensated by longer context. On the other hand, stress and accent detection perform better at the segmental level, and relative improvement by longer context is smaller than in C-V detection. Overall performance of linguistic parsing can be improved by adding supplementary features, such as features related to tonal and energy contours."}, {"heading": "4.2.3. Dependency of Linguistic Events", "text": "Finally, we test the dependency between different supra-segmental attributes captured in codebook structures. Both stressed and accented syllables convey similar information on linguistic emphasis, the former denotes it at a lexical level while the latter designates it at a prosodic level. Hence, we hypothesize that the codebook constructed from stressed structures can be used for accent detection, and vice versa. Table 5 lists the accuracies using these linguistically relevant codebooks.\nWe can see that a codebook constructed from either of stress/accent structures can be used for detection of the other with high accuracy. This study confirms the hypothesis that codebooks encapsulate linguistically relevant structures and demonstrates that accented structures are indeed highly correlated with the stressed structures."}, {"heading": "5. Concluding Remarks", "text": "The theories of linguistics and cognitive neuroscience suggest that the phonological representation of speech places at the heart of speech temporal organization. We devised a methodology to quantify the phonological based supra-segmental primitives as essential building blocks for detection of various linguistic events. Our proposed approach relies on the identification of structured sparsity patterns to learn classspecific codebooks characterizing different supra-segmental attributes. The experiments confirmed that indeed phonological posteriors convey supra-segmental information which is encoded in their support of active components, and these structures can be used as indicators of their higher level linguistic attributes.\nIn this context, we also verified that the class-specific structures of phonological posteriors is a property independent of language as well as definition of different phonological classes. In addition, it is robust to unconstrained and noisy recording conditions. Furthermore, the dependency of different linguistic properties such as stress and accent is captured in their codebooks which confirm the high correlation between their underlying structures. Indeed, the stress and accent TTS labels are related, for example, for English the placement of accents on stressed syllables in all content words is a reasonable approximation achieving high accuracy on typical databases 4. In general, we cannot draw broad conclusions on presented linguistic parsing, as we do not have ground truth reference labels (the labels predicted from text do not necessarily match speech recordings), and we parsed only a few linguistic classes. However, the goal of this study has not been to present a complete linguistic parser, rather we focused on showing supra-segmental properties of phonological posteriors.\nThis work quantified the supra-segmental events through the binary representation of posteriors. This quantification can be more accurate if multi-level discretization is considered to find a compromise between speaker and environmental variability encoded in the probabilities and the actual contribution of phonological classes.\nIn our future work, we plan to investigate more closely the relationship of the trajectories of the articulatory-bound phonological posterior features to the task dynamic model of inter-articulator coordination in speech (Saltzman and Munhall, 1989). This study will strengthen our knowledge about the interpretation of phonological posteriors, when applied to different speech processing tasks. Applications include detection of syllable boundaries and subsequent bottom-up linguistic parsing (i.e., parsing without\n4http://www.festvox.org/bsv/x1750.html\nproviding the segment boundaries as discussed by Ghitza (2011); Giraud and Poeppel (2012)), as well as phonetic posterior estimation for automatic speech recognition and synthesis systems, parametric speech coding, and automatic assessment of speech production."}, {"heading": "6. Acknowledgment", "text": "Afsaneh Asaei is supported by funding from SNSF project on \u201cParsimonious Hierarchical Automatic Speech Recognition (PHASER)\u201d grant agreement number 200021-153507. The authors are grateful to the anonymous reviewers for their time and comments to improve the clarity and quality of the manuscript."}, {"heading": "7. References", "text": ""}], "references": [{"title": "On Compressibility of Neural Network Phonological Features for Low Bit Rate", "author": ["A. References Asaei", "M. Cernak", "H. Bourlard", "Sep"], "venue": null, "citeRegEx": "Asaei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Asaei et al\\.", "year": 2015}, {"title": "The Festival Speech Synthesis System", "author": ["A. Pearson. Black", "P. Taylor", "R. Caley"], "venue": "Communication Sciences and Disorders),", "citeRegEx": "Black et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Black et al\\.", "year": 1997}, {"title": "Functional organization of human sensorimotor cortex", "author": ["K.E. Edinburgh. Bouchard", "N. Mesgarani", "K. Johnson", "E.F. Chang", "Mar"], "venue": null, "citeRegEx": "Bouchard et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bouchard et al\\.", "year": 2013}, {"title": "Towards an articulatory phonology", "author": ["C. P", "L.M. Goldstein"], "venue": "for speech articulation. Nature", "citeRegEx": "P. and Goldstein,? \\Q1986\\E", "shortCiteRegEx": "P. and Goldstein", "year": 1986}, {"title": "2016a. Sound Pattern Matching for Automatic Prosodic", "author": ["M. Cernak", "A. Asaei", "Honnet", "P.-E", "P.N. Garner", "H. Bourlard"], "venue": null, "citeRegEx": "Cernak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cernak et al\\.", "year": 2016}, {"title": "Speech vocoding for laboratory phonology", "author": ["M. Cernak", "S. Benus", "A. Lazaridis"], "venue": "Event Detection. In: Proc. of Interspeech. San Francisco,", "citeRegEx": "Cernak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cernak et al\\.", "year": 2016}, {"title": "PhonVoc: A Phonetic and Phonological Vocoding Toolkit", "author": ["M. Cernak", "P.N. Garner"], "venue": "Proc. of Interspeech", "citeRegEx": "Cernak and Garner,? \\Q2016\\E", "shortCiteRegEx": "Cernak and Garner", "year": 2016}, {"title": "Incremental Syllable-Context Phonetic Vocoding", "author": ["CA Francisco", "M. USA. Cernak", "P.N. Garner", "A. Lazaridis", "P. Motlicek", "X. Na", "Jun"], "venue": null, "citeRegEx": "Francisco et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Francisco et al\\.", "year": 2015}, {"title": "Phonological vocoding using artificial neural networks", "author": ["M. Cernak", "B. Potard", "P.N. Garner", "Apr"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing", "citeRegEx": "Cernak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cernak et al\\.", "year": 2015}, {"title": "The Sound Pattern of English", "author": ["N. Chomsky", "M. Halle"], "venue": null, "citeRegEx": "Chomsky and Halle,? \\Q1968\\E", "shortCiteRegEx": "Chomsky and Halle", "year": 1968}, {"title": "Corpus description of the ester evaluation", "author": ["S. Galliano", "E. Geoffrois", "G. Gravier", "J. f. Bonastre", "D. Mostefa", "K. Choukri"], "venue": null, "citeRegEx": "Galliano et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Galliano et al\\.", "year": 2006}, {"title": "Articulatory phonology: a phonology for public language use", "author": ["L. Goldstein", "C. Fowler"], "venue": null, "citeRegEx": "Goldstein and Fowler,? \\Q2003\\E", "shortCiteRegEx": "Goldstein and Fowler", "year": 2003}, {"title": "The elements of phonological representation", "author": ["J. Harris", "G. Lindsey"], "venue": "Nature Reviews Neuroscience", "citeRegEx": "Harris and Lindsey,? \\Q1995\\E", "shortCiteRegEx": "Harris and Lindsey", "year": 1995}, {"title": "Fundamentals of Language", "author": ["R. 1527\u20131554. Jakobson", "M. Halle"], "venue": "The Hague: Mouton. Jun, S.-A.,", "citeRegEx": "Jakobson and Halle,? \\Q1956\\E", "shortCiteRegEx": "Jakobson and Halle", "year": 1956}, {"title": "A Course in Phonetics, 7th Edition", "author": ["P. Ladefoged", "K. Johnson", "Jan"], "venue": "Oxford University Press,", "citeRegEx": "Ladefoged et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ladefoged et al\\.", "year": 2014}, {"title": "neuronal excitability and stimulus processing in the auditory cortex", "author": ["S. Lee", "S. Yildirim", "A. Kazemzadeh", "S. Narayanan"], "venue": "Journal of neurophysiology", "citeRegEx": "Lee et al\\.,? \\Q1904\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1904}, {"title": "Dynamic encoding of speech sequence probability in human", "author": ["M.K. Leonard", "K.E. Bouchard", "C. Tang", "E.F. Chang"], "venue": null, "citeRegEx": "Leonard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leonard et al\\.", "year": 2015}, {"title": "A role for amplitude modulation phase relationships in speech", "author": ["V. Leong", "M.A. Stone", "R.E. Turner", "U. Goswami", "Jul"], "venue": "temporal cortex. The Journal of Neuroscience", "citeRegEx": "Leong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leong et al\\.", "year": 2014}, {"title": "On the relation of speech to language", "author": ["A.M. Bradford Book. Liberman", "D.H. Whalen", "May"], "venue": "Trends in cognitive sciences 4 (5), 187\u2013196. Matt, G., 2014. Disentangling stress and pitch accent: Toward a typology of prominence at different prosodic levels. in Harry", "citeRegEx": "Liberman et al\\.,? 2000", "shortCiteRegEx": "Liberman et al\\.", "year": 2000}, {"title": "An Analysis of Perceptual Confusions Among Some English Consonants", "author": ["G.A. Miller", "P.E. Nicely", "Mar"], "venue": "Gyrus. Science", "citeRegEx": "Miller et al\\.,? \\Q1955\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1955}, {"title": "Exploring how deep neural networks form phonemic categories", "author": ["Soc. Am"], "venue": null, "citeRegEx": "Am.,? \\Q2015\\E", "shortCiteRegEx": "Am.", "year": 2015}, {"title": "Self-organization of syllable structure: A coupled oscillator model", "author": ["H. Nam", "L. Goldstein", "E. Saltzman"], "venue": null, "citeRegEx": "Nam et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nam et al\\.", "year": 2009}, {"title": "The design for the wall street journal-based", "author": ["D.B. 299\u2013328. Paul", "J.M. Baker"], "venue": "CSR corpus. In: Proceedings of the workshop on", "citeRegEx": "Paul and Baker,? \\Q1992\\E", "shortCiteRegEx": "Paul and Baker", "year": 1992}, {"title": "Auditory Cortex Accesses Phonological Categories: An MEG Mismatch Study", "author": ["C. Phillips", "T. Pellathy", "A. Marantz", "E. Yellin", "K. Wexler", "D. Poeppel", "M. McGinnis", "T. Roberts", "Nov."], "venue": "Journal of Cognitive Neuroscience 12 (6), 1038\u20131055.", "citeRegEx": "Phillips et al\\.,? 2000", "shortCiteRegEx": "Phillips et al\\.", "year": 2000}, {"title": "The Analysis of Speech in Different Temporal Integration Windows: Cerebral Lateralization As \u2019Asymmetric Sampling in Time", "author": ["D. Poeppel", "Aug."], "venue": "Speech Communication 41 (1), 245\u2013255.", "citeRegEx": "Poeppel and Aug.,? 2003", "shortCiteRegEx": "Poeppel and Aug.", "year": 2003}, {"title": "The neuroanatomic and neurophysiological infrastructure for speech and language", "author": ["D. Poeppel", "Oct."], "venue": "Current Opinion in Neurobiology 28, 142\u2013149.", "citeRegEx": "Poeppel and Oct.,? 2014", "shortCiteRegEx": "Poeppel and Oct.", "year": 2014}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely", "Dec."], "venue": "In: Proc. of ASRU. IEEE SPS, iEEE Catalog No.: CFP11SRW-USB.", "citeRegEx": "Povey et al\\.,? 2011", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Integrating articulatory features using Kullback-Leibler divergence based acoustic model for phoneme recognition", "author": ["R. Rasipuram", "M. Magimai.-Doss", "May"], "venue": "In: Proc. of ICASSP. IEEE, pp. 5192\u20135195.", "citeRegEx": "Rasipuram et al\\.,? 2011", "shortCiteRegEx": "Rasipuram et al\\.", "year": 2011}, {"title": "eLite-HTS: A NLP tool for French HMM-based speech synthesis", "author": ["S. Roekhaut", "S. Brognaux", "R. Beaufort", "T. Dutoit"], "venue": "Proc. of Interspeech", "citeRegEx": "Roekhaut et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roekhaut et al\\.", "year": 2014}, {"title": "A dynamical approach to gestural patterning in speech production", "author": ["E.L. Saltzman", "K.G. Munhall"], "venue": "Ecological Psychology", "citeRegEx": "Saltzman and Munhall,? \\Q1989\\E", "shortCiteRegEx": "Saltzman and Munhall", "year": 1989}, {"title": "Acoustic modeling based on the MDL principle for speech recognition", "author": ["K. Shinoda", "T. Watanabe"], "venue": "Proc. of Eurospeech", "citeRegEx": "Shinoda and Watanabe,? \\Q1997\\E", "shortCiteRegEx": "Shinoda and Watanabe", "year": 1997}, {"title": "Experiments on Cross-Language Attribute Detection and Phone Recognition With Minimal Target-Specific Training Data", "author": ["S.M. Siniscalchi", "Lyu", "D.-C.", "T. Svendsen", "Lee", "C.-H.", "Mar."], "venue": "IEEE Trans. on Audio, Speech, and Language Processing 20 (3), 875\u2013887.", "citeRegEx": "Siniscalchi et al\\.,? 2012", "shortCiteRegEx": "Siniscalchi et al\\.", "year": 2012}, {"title": "Features in Speech Perception and Lexical Access", "author": ["K.N. Stevens"], "venue": "The Handbook of Speech Perception. Blackwell Publishing,", "citeRegEx": "Stevens,? \\Q2005\\E", "shortCiteRegEx": "Stevens", "year": 2005}, {"title": "On The Use of Phonological Features for Pronunciation Scoring", "author": ["F. Stouten", "Martens", "J.-P.", "May"], "venue": "In: Proc. of ICASSP. Vol. 1. IEEE, p. I.", "citeRegEx": "Stouten et al\\.,? 2006", "shortCiteRegEx": "Stouten et al\\.", "year": 2006}, {"title": "Bonston studies in the phylosophy of science", "author": ["C. Wernicke"], "venue": null, "citeRegEx": "Wernicke,? \\Q1874\\E", "shortCiteRegEx": "Wernicke", "year": 1874}, {"title": "Boosting attribute and phone estimation accuracies with deep neural networks for detection-based speech recognition", "author": ["D. Yu", "S. Siniscalchi", "L. Deng", "Lee", "C.-H.", "March"], "venue": "In: Proc. of ICASSP. IEEE SPS.", "citeRegEx": "Yu et al\\.,? 2012", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "The HMM-based Speech Synthesis System Version 2.0", "author": ["H. Zen", "T. Nose", "J. Yamagishi", "S. Sako", "T. Masuko", "A. Black", "K. Tokuda"], "venue": "Proc. of ISCA SSW6", "citeRegEx": "Zen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 11, "context": "Recent work on Articulatory Phonology (Goldstein and Fowler, 2003) further suggests an existence of coupling/synchronisation of gestures that influence the syllable structure of an utterance.", "startOffset": 38, "endOffset": 66}, {"referenceID": 13, "context": ", (Jakobson and Halle, 1956; Chomsky and Halle, 1968)) emerge during the phonological encoding process \u2013 the processes of speech planning for articulation, namely the preparation of an abstract phonological code and its transformation into speech motor plans that guide articulation (Levelt, 1993).", "startOffset": 2, "endOffset": 53}, {"referenceID": 9, "context": ", (Jakobson and Halle, 1956; Chomsky and Halle, 1968)) emerge during the phonological encoding process \u2013 the processes of speech planning for articulation, namely the preparation of an abstract phonological code and its transformation into speech motor plans that guide articulation (Levelt, 1993).", "startOffset": 2, "endOffset": 53}, {"referenceID": 0, "context": "Previously in (Asaei et al., 2015), we have shown that phonological posteriors admit sparsity structures underlying short-term segmental representations where the structures are quantified as sparse binary vectors.", "startOffset": 14, "endOffset": 34}, {"referenceID": 5, "context": ", (Jakobson and Halle, 1956; Chomsky and Halle, 1968)) emerge during the phonological encoding process \u2013 the processes of speech planning for articulation, namely the preparation of an abstract phonological code and its transformation into speech motor plans that guide articulation (Levelt, 1993). Stevens (2005) reviews evidence about a universal set of phonological classes that consists of articulator-bound classes and articulator-free classes ([continuant], [sonorant], [strident]).", "startOffset": 29, "endOffset": 314}, {"referenceID": 3, "context": "Cernak et al. (2015b) introduce the phonological posterior features for phonological analysis and synthesis, and we hypothesise their relation to the linguistic gestural model.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Cernak et al. (2015b) introduce the phonological posterior features for phonological analysis and synthesis, and we hypothesise their relation to the linguistic gestural model. Saltzman and Munhall (1989) describe the constriction dynamics model as a computational system that incorporates the theory of articulatory phonology.", "startOffset": 0, "endOffset": 205}, {"referenceID": 35, "context": "Figure 1 illustrates the process of the phonological analysis (Yu et al., 2012; Cernak et al., 2015b).", "startOffset": 62, "endOffset": 101}, {"referenceID": 29, "context": "The hypothesis of correspondence of the phonological posterior features to the gestural trajectories is also motivated by the analogy to the constriction dynamics model (Saltzman and Munhall, 1989) that takes gestural scores as input and generates articulator trajectories and acoustic output.", "startOffset": 169, "endOffset": 197}, {"referenceID": 4, "context": "Alternatively to this constriction dynamics model, we generate acoustic output using a phonological synthesis DNN described in Cernak et al. (2015b). In the following sections, we outline converging evidence from linguistics as well as the neural basis of speech perception, that support the hypothesis about phonological posteriors conveying properties of linguistic classes at multiple time scales.", "startOffset": 127, "endOffset": 149}, {"referenceID": 32, "context": "For example, glides are always syllable-initial, and consonants that follow a non-tense vowel are always in the coda of the syllable (Stevens, 2005).", "startOffset": 133, "endOffset": 148}, {"referenceID": 21, "context": "(2015) say that gestures are phonetic in nature), recent developments suggest that the timing of articulatory gestures encodes syllabic (and thus linguistic) information as well (Browman and Goldstein, 1988; Nam et al., 2009).", "startOffset": 178, "endOffset": 225}, {"referenceID": 11, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles \u2013 the Sound Pattern of English (SPE).", "startOffset": 20, "endOffset": 46}, {"referenceID": 9, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles \u2013 the Sound Pattern of English (SPE).", "startOffset": 50, "endOffset": 75}, {"referenceID": 9, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles \u2013 the Sound Pattern of English (SPE). Later advanced phonological systems were proposed, such as multi-valued phonological features of Ladefoged and Johnson (2014), and monovalent Government Phonology features of Harris and Lindsey (1995) that describe sounds by fusing and splitting of primes.", "startOffset": 50, "endOffset": 291}, {"referenceID": 9, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles \u2013 the Sound Pattern of English (SPE). Later advanced phonological systems were proposed, such as multi-valued phonological features of Ladefoged and Johnson (2014), and monovalent Government Phonology features of Harris and Lindsey (1995) that describe sounds by fusing and splitting of primes.", "startOffset": 50, "endOffset": 366}, {"referenceID": 9, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles \u2013 the Sound Pattern of English (SPE). Later advanced phonological systems were proposed, such as multi-valued phonological features of Ladefoged and Johnson (2014), and monovalent Government Phonology features of Harris and Lindsey (1995) that describe sounds by fusing and splitting of primes. The surface code includes co-articulated canonical code, with further intrinsic (speaker-based) and extrinsic (channel-based) speech variabilities that contribute to the opacity of the function operating between the two codes. The surface features may contain additional gestures dependent on the prosodic context, such as position within a syllable, word, and sentence. Other changes in surface phonological features at different time granularities are due to phonotactic constraints. For example, glides are always syllable-initial, and consonants that follow a non-tense vowel are always in the coda of the syllable (Stevens, 2005). Relation of the canonical and surface code can be investigated by a linguistic theory of Articulatory Phonology (Browman and Goldstein, 1986, 1989, 1992) that introduced articulatory gestures as a basis for human speech production. Although it is generally claimed that gestures convey segmental-level information (for example, Fowler et al. (2015) say that gestures are phonetic in nature), recent developments suggest that the timing of articulatory gestures encodes syllabic (and thus linguistic) information as well (Browman and Goldstein, 1988; Nam et al.", "startOffset": 50, "endOffset": 1407}, {"referenceID": 9, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles \u2013 the Sound Pattern of English (SPE). Later advanced phonological systems were proposed, such as multi-valued phonological features of Ladefoged and Johnson (2014), and monovalent Government Phonology features of Harris and Lindsey (1995) that describe sounds by fusing and splitting of primes. The surface code includes co-articulated canonical code, with further intrinsic (speaker-based) and extrinsic (channel-based) speech variabilities that contribute to the opacity of the function operating between the two codes. The surface features may contain additional gestures dependent on the prosodic context, such as position within a syllable, word, and sentence. Other changes in surface phonological features at different time granularities are due to phonotactic constraints. For example, glides are always syllable-initial, and consonants that follow a non-tense vowel are always in the coda of the syllable (Stevens, 2005). Relation of the canonical and surface code can be investigated by a linguistic theory of Articulatory Phonology (Browman and Goldstein, 1986, 1989, 1992) that introduced articulatory gestures as a basis for human speech production. Although it is generally claimed that gestures convey segmental-level information (for example, Fowler et al. (2015) say that gestures are phonetic in nature), recent developments suggest that the timing of articulatory gestures encodes syllabic (and thus linguistic) information as well (Browman and Goldstein, 1988; Nam et al., 2009). Liberman and Whalen (2000) provides theoretic claims about linguisticlly relevant articulatory gestures, and Saltzman and Munhall (1989) implement a syllable structure-based gesture coupling model.", "startOffset": 50, "endOffset": 1654}, {"referenceID": 9, "context": "In the tradition of Jakobson and Halle (1956) and Chomsky and Halle (1968), phonemes are assumed to consist of feature bundles \u2013 the Sound Pattern of English (SPE). Later advanced phonological systems were proposed, such as multi-valued phonological features of Ladefoged and Johnson (2014), and monovalent Government Phonology features of Harris and Lindsey (1995) that describe sounds by fusing and splitting of primes. The surface code includes co-articulated canonical code, with further intrinsic (speaker-based) and extrinsic (channel-based) speech variabilities that contribute to the opacity of the function operating between the two codes. The surface features may contain additional gestures dependent on the prosodic context, such as position within a syllable, word, and sentence. Other changes in surface phonological features at different time granularities are due to phonotactic constraints. For example, glides are always syllable-initial, and consonants that follow a non-tense vowel are always in the coda of the syllable (Stevens, 2005). Relation of the canonical and surface code can be investigated by a linguistic theory of Articulatory Phonology (Browman and Goldstein, 1986, 1989, 1992) that introduced articulatory gestures as a basis for human speech production. Although it is generally claimed that gestures convey segmental-level information (for example, Fowler et al. (2015) say that gestures are phonetic in nature), recent developments suggest that the timing of articulatory gestures encodes syllabic (and thus linguistic) information as well (Browman and Goldstein, 1988; Nam et al., 2009). Liberman and Whalen (2000) provides theoretic claims about linguisticlly relevant articulatory gestures, and Saltzman and Munhall (1989) implement a syllable structure-based gesture coupling model.", "startOffset": 50, "endOffset": 1764}, {"referenceID": 17, "context": "Phillips et al. (2000); Mesgarani et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "Phillips et al. (2000); Mesgarani et al. (2014) present evidence of discrete phonological classes available in the human auditory cortex.", "startOffset": 0, "endOffset": 48}, {"referenceID": 12, "context": "Leong et al. (2014) show that phase relations between the phonetic and syllabic amplitude modulations, known as hierarchical phase locking and nesting or synchronization across different temporal granularity (Lakatos et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Bouchard et al. (2013) claim that functional organisation of ventral sensorimotor cortex supports the gestural model developed in Articulatory Phonology.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "Bouchard et al. (2013) claim that functional organisation of ventral sensorimotor cortex supports the gestural model developed in Articulatory Phonology. Analysis of spatial patterns of activity showed a hierarchy of network states that organizes phonemes by articulatory-bound phonological features. Leonard et al. (2015) further show how listeners use phonotactic knowledge (phoneme sequence statistics) to process spoken input and to link low-level acoustic representations (the coarticulatory dynamics of the sounds through the encoding of combination of phonological features) with linguistic information about word identity and meaning.", "startOffset": 0, "endOffset": 323}, {"referenceID": 0, "context": "This physical limitation leads to a small number of unique patterns exhibited over the entire speech corpora (Asaei et al., 2015).", "startOffset": 109, "endOffset": 129}, {"referenceID": 6, "context": "We use an open-source phonological vocoding platform (Cernak and Garner, 2016) to obtain phonological posteriors.", "startOffset": 53, "endOffset": 78}, {"referenceID": 35, "context": "Binary (Yu et al., 2012) or multi-valued classification (Stouten and Martens, 2006; Rasipuram and Magimai.", "startOffset": 7, "endOffset": 24}, {"referenceID": 9, "context": "To confirm independence of the proposed methodology on a phonological system, two different phonological speech representations are considered: the SPE feature set (Chomsky and Halle, 1968), and the extended SPE (eSPE) feature set (Cernak et al.", "startOffset": 164, "endOffset": 189}, {"referenceID": 4, "context": "To confirm independence of the proposed methodology on a phonological system, two different phonological speech representations are considered: the SPE feature set (Chomsky and Halle, 1968), and the extended SPE (eSPE) feature set (Cernak et al., 2015b) are used in training of the DNNs for phonological posterior estimation on English and French data respectively. The mapping used to map from phonemes to SPE phonological class is taken from Cernak et al. (2016b). The distribution of the phonological labels is non-uniform, driven by mapping different numbers of phonemes to the phonological classes.", "startOffset": 232, "endOffset": 466}, {"referenceID": 35, "context": "For French eSPE feature set, we started from pseudo-phonological feature classification designed for American English (Yu et al., 2012).", "startOffset": 118, "endOffset": 135}, {"referenceID": 31, "context": "To confirm cross-lingual property of phonological posteriors (Siniscalchi et al., 2012), we conducted our evaluations on English and French speech corpora.", "startOffset": 61, "endOffset": 87}, {"referenceID": 22, "context": "To train the DNNs for phonological posterior estimation on English data, we use the Wall Street Journal (WSJ0 and WSJ1) continuous speech recognition corpora (Paul and Baker, 1992).", "startOffset": 158, "endOffset": 180}, {"referenceID": 10, "context": "To train the DNNs for phonological posterior estimation on French data, we use the Ester database (Galliano et al., 2006) containing standard French radio broadcast news in various recording conditions.", "startOffset": 98, "endOffset": 121}, {"referenceID": 1, "context": "The text was processed by a conventional and freely available text to speech synthesis (TTS) front-end (Black et al., 1997), resulting in segmental (quinphone phonetic context) and supra-segmental (full-context) labels.", "startOffset": 103, "endOffset": 123}, {"referenceID": 28, "context": "We generated full-context labels using the French text analyzer eLite (Roekhaut et al., 2014).", "startOffset": 70, "endOffset": 93}, {"referenceID": 36, "context": "The three-state, cross-word triphone models were trained with the HMM-based speech synthesis system (HTS) variant (Zen et al., 2007) of the Hidden Markov Model Toolkit (HTK) on the 90% subset of the WSJ si tr s 284 set.", "startOffset": 114, "endOffset": 132}, {"referenceID": 30, "context": "We tied triphone models with decision tree state clustering based on the minimum description length (MDL) criterion (Shinoda and Watanabe, 1997).", "startOffset": 116, "endOffset": 144}, {"referenceID": 26, "context": "The DNNs with the softmax output function were then trained using a mini-batch based stochastic gradient descent algorithm with the cross-entropy cost function of the KALDI toolkit (Povey et al., 2011).", "startOffset": 181, "endOffset": 201}, {"referenceID": 26, "context": "The DNNs with the softmax output function were then trained using a mini-batch based stochastic gradient descent algorithm with the cross-entropy cost function of the KALDI toolkit (Povey et al., 2011). Table 2 lists the detection accuracy for different phonological classes. The DNN outputs provide the phonological posterior probabilities for each phonological class. Detection accuracy on cross-database (Nancy) test data is lower, however following the accuracy on training and CV data. The consonantal and voice DNN performed worse, and we speculate that this might be caused by difficulty to train good classifiers for such broad (hierarchical) phonological classes. Recent work of Nagamine et al. (2015) suggests that a DNN learns selective phonological classes in different nodes and hidden layers.", "startOffset": 182, "endOffset": 711}, {"referenceID": 0, "context": "This property encouraged us also to use this binary approximation in low bit-rate speech coding (Cernak et al., 2015b; Asaei et al., 2015); these studies confirmed that binary approximation has only a negligible impact on perceptual speech quality.", "startOffset": 96, "endOffset": 138}, {"referenceID": 4, "context": "This idea is further investigated in Cernak et al. (2016a) where a highly competitive emphasis detection system is achieved.", "startOffset": 37, "endOffset": 59}, {"referenceID": 29, "context": "In our future work, we plan to investigate more closely the relationship of the trajectories of the articulatory-bound phonological posterior features to the task dynamic model of inter-articulator coordination in speech (Saltzman and Munhall, 1989).", "startOffset": 221, "endOffset": 249}], "year": 2016, "abstractText": "The speech signal conveys information on different time scales from short (20\u201340 ms) time scale or segmental, associated to phonological and phonetic information to long (150\u2013250 ms) time scale or supra segmental, associated to syllabic and prosodic information. Linguistic and neurocognitive studies recognize the phonological classes at segmental level as the essential and invariant representations used in speech temporal organization. In the context of speech processing, a deep neural network (DNN) is an effective computational method to infer the probability of individual phonological classes from a short segment of speech signal. A vector of all phonological class probabilities is referred to as phonological posterior. There are only very few classes comprising a short term speech signal; hence, the phonological posterior is a sparse vector. Although the phonological posteriors are estimated at segmental level, we claim that they convey supra-segmental information. Specifically, we demonstrate that phonological posteriors are indicative of syllabic and prosodic events. Building on findings from converging linguistic evidence on the gestural model of Articulatory Phonology as well as the neural basis of speech perception, we hypothesize that phonological posteriors convey properties of linguistic classes at multiple time scales, and this information is embedded in their support (index) of active coefficients. To verify this hypothesis, we obtain a binary representation of phonological posteriors at the segmental level which is referred to as first-order sparsity structure; the high-order structures are obtained by the concatenation of first-order binary vectors. It is then confirmed that the classification of supra-segmental linguistic events, the problem known as linguistic parsing, can be achieved with high accuracy using a simple binary pattern matching of first-order or high-order structures.", "creator": "LaTeX with hyperref package"}}}