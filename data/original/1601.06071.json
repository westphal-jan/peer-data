{"id": "1601.06071", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2016", "title": "Bitwise Neural Networks", "abstract": "Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resource-constrained environments, since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations. Hence, the BNN requires for less spatial complexity, less memory bandwidth, and less power consumption in hardware. In order to design such networks, we propose to add a few training schemes, such as weight compression and noisy backpropagation, which result in a bitwise network that performs almost as well as its corresponding real-valued network. We test the proposed network on the MNIST dataset, represented using binary features, and show that BNNs result in competitive performance while offering dramatic computational savings.", "histories": [["v1", "Fri, 22 Jan 2016 16:59:01 GMT  (87kb,D)", "http://arxiv.org/abs/1601.06071v1", "This paper was presented at the International Conference on Machine Learning (ICML) Workshop on Resource-Efficient Machine Learning, Lille, France, Jul. 6-11, 2015"]], "COMMENTS": "This paper was presented at the International Conference on Machine Learning (ICML) Workshop on Resource-Efficient Machine Learning, Lille, France, Jul. 6-11, 2015", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["minje kim", "paris smaragdis"], "accepted": false, "id": "1601.06071"}, "pdf": {"name": "1601.06071.pdf", "metadata": {"source": "META", "title": "Bitwise Neural Networks", "authors": ["Minje Kim", "Paris Smaragdis"], "emails": ["MINJE@ILLINOIS.EDU", "PARIS@ILLINOIS.EDU"], "sections": [{"heading": "1. Introduction", "text": "According to the universal approximation theorem, a single hidden layer with a finite number of units can approximate a continuous function with some mild assumptions (Cybenko, 1989; Hornik, 1991). While this theorem implies a shallow network with a potentially intractable number of hidden units when it comes to modeling a compli-\nProceedings of the 31 st International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\ncated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009).\nAlthough DNNs are extending the state of the art results for various tasks, such as image classification (Goodfellow et al., 2013), speech recognition (Hinton et al., 2012), speech enhancement (Xu et al., 2014), etc, it is also the case that the relatively bigger networks with more parameters than before call for more resources (processing power, memory, battery time, etc), which are sometimes critically constrained in applications running on embedded devices. Examples of those applications span from context-aware computing, collecting and analysing a variety of sensor signals on the device (Baldauf et al., 2007), to always-on computer vision applications (e.g. Google glasses), to speechdriven personal assistant services, such as \u201cHey, Siri.\u201d A primary concern that hinders those applications from being more successful is that they assume an always-on pattern recognition engine on the device, which will drain the battery fast unless it is carefully implemented to minimize the use of resources. Additionally, even in an environment with the necessary resources being available, speeding up a DNN can greatly improve the user experience when it comes to tasks like searching big databases (Salakhutdinov & Hinton, 2009). In either case, a more compact yet still well-performing DNN is a welcome improvement.\nEfficient computational structures for deploying artificial neural networks have long been studied in the literature. Most of the effort is focused on training networks whose weights can be transformed into some quantized representations with a minimal loss of performance (Fiesler et al., 1990; Hwang & Sung, 2014). They typically use the quantized weights in the feedforward step at every training iteration, so that the trained weights are robust to the known quantization noise caused by a limited precision. It was also shown that 10 bits and 12 bits are enough to represent gradients and storing weights for implementing the stateof-the-art maxout networks even for training the network (Courbariaux et al., 2014). However, in those quantized\nar X\niv :1\n60 1.\n06 07\n1v 1\n[ cs\n.L G\n] 2\n2 Ja\nn 20\nnetworks one still needs to employ arithmetic operations, such as multiplication and addition, on fixed-point values. Even though faster than floating point, they still require relatively complex logic and can consume a lot of power.\nWith the proposed Bitwise Neural Networks (BNN), we take a more extreme view that every input node, output node, and weight, is represented by a single bit. For example, a weight matrix between two hidden layers of 1024 units is a 1024 \u00d7 1025 matrix of binary values rather than quantized real values (including the bias). Although learning those bitwise weights as a Boolean concept is an NPcomplete problem (Pitt & Valiant, 1988), the bitwise networks have been studied in the limited setting, such as \u00b5perceptron networks where an input node is allowed to be connected to one and only one hidden node and its final layer is a union of those hidden nodes (Golea et al., 1992). A more practical network was proposed in (Soudry et al., 2014) recently, where the posterior probabilities of the binary weights were sought using the Expectation Back Propagation (EBP) scheme, which is similar to backpropagation in its form, but has some advantages, such as parameterfree learning and a straightforward discretization of the weights. Its promising results on binary text classification tasks however, rely on the real-valued bias terms and averaging of predictions from differently sampled parameters.\nThis paper presents a completely bitwise network where all participating variables are bipolar binaries. Therefore, in its feedforward only XNOR and bit counting operations are used instead of multiplication, addition, and a nonlinear activation on floating or fixed-point variables. For training, we propose a two-stage approach, whose first part is typical network training with a weight compression technique that helps the real-valued model to easily be converted into a BNN. To train the actual BNN, we use those compressed weights to initialize the BNN parameters, and do noisy backpropagation based on the tentative bitwise parameters. To binarize the input signals, we can adapt any binarization techniques, e.g. fixed-point representations and hash codes. Regardless of the binarization scheme, each input node is given only a single bit at a time, as opposed to a bit packet representing a fixed-point number. This is significantly different from the networks with quantized inputs, where a real-valued signal is quantized into a set of bits, and then all those bits are fed to an input node in place of their corresponding single real value. Lastly, we apply the sign function as our activation function instead of a sigmoid to make sure the input to the next layer is bipolar binary as well. We compare the performance of the proposed BNN with its corresponding ordinary real-valued networks on hand-written digit recognition tasks, and show that the bitwise operations can do the job with a very small performance loss, while providing a large margin of improvement in terms of the necessary computational resources."}, {"heading": "2. Feedforward in Bitwise Neural Networks", "text": "It has long been known that any Boolean function, which takes binary values as input and produces binary outputs as well, can be represented as a bitwise network with one hidden layer (McCulloch & Pitts, 1943), for example, by merely memorizing all the possible mappings between input and output patterns. We define the forward propagation procedure as follows based on the assumption that we have trained such a network with bipolar binary parameters:\nali = b l i + Kl\u22121\u2211 j wli,j \u2297 zl\u22121j , (1)\nzli = sign ( ali ) , (2) zl \u2208 BK l ,Wl \u2208 BK l\u00d7Kl\u22121 ,bl \u2208 BK l , (3)\nwhere B is the set of bipolar binaries, i.e. \u00b111, and\u2297 stands for the bitwise XNOR operation (see Figure 1 (a)). l, j, and i indicate a layer, input and output units of the layer, respectively. We use bold characters for a vector (or a matrix if capicalized). Kl is the number of input units at l-th layer. Therefore, z0 equals to an input vector, where we omit the sample index for the notational convenience. We use the sign activation function to generate the bipolar outputs.\nWe can check the prediction error E by measuring the bitwise agreement of target vector t and the output units of L-th layer using XNOR as a multiplication operator,\nE = KL+1\u2211\ni\n( 1\u2212 ti \u2297 zL+1i ) /2, (4)\nbut this error function can be tentatively replaced by involving a softmax layer during the training phase.\nThe XNOR operation is a faster substitute of binary multiplication. Therefore, (1) and (2) can be seen as a special version of the ordinary feedforward step that only works when the inputs, weights, and bias are all bipolar binaries. Note that these bipolar bits will in practice be implemented using 0/1 binary values, where (2) activation is equivalent to counting the number of 1\u2019s and then checking if the accumulation is bigger than the half of the number of input units plus 1. With no loss of generality, in this paper we will use the \u00b11 bipolar representation since it is more flexible in defining hyperplanes and examining the network behavior.\nSometimes a BNN can solve the same problem as a realvalued network without any size modifications, but in general we should expect that a BNN could require larger network structures than a real-valued one. For example, the XOR problem in Figure 1 (b) can have an infinite number of solutions with real-valued parameters once a pair\n1In the bipolar binary representation, +1 stands for the \u201cTRUE\u201d status, while \u22121 is for \u201cFALSE.\u201d\nof hyperplanes can successfully discriminate (1, 1) and (\u22121,\u22121) from (1,\u22121) and (\u22121, 1). Among all the possible solutions, we can see that binary weights and bias are enough to define the hyperplanes, x1 \u2212 x2 + 1 > 0 and \u2212x1 + x2 + 1 > 0 (dashes). Likewise, the separation performance of the particular BNN defined in (c) has the same classification power once the inputs are binary as well.\nFigure 1 (d) shows another example where BNN requires more hyperplanes than a real-valued network. This linearly separable problem is solvable with only one hyperplane, such as \u22120.1x1 + x2 + 0.5 > 0, but it is impossible to describe such a hyperplane with binary coefficients. We can instead come up with a solution by combining multiple binary hyperplanes that will eventually increase the perceived complexity of the model. However, even with a larger number of nodes, the BNN is not necessarily more complex than the smaller real-valued network. This is because a parameter or a node of BNN requires only one bit to represent while a real-valued node generally requires more than that, up to 64 bits. Moreover, the simple XNOR and bit counting operations of BNN bypass the computational complica-\ntions of a real-valued system, such as the power consumption of multipliers and adders for the floating-point operations, various dynamic ranges of the fixed-point representations, erroneous flips of the most significant bits, etc. Note that if the bitwise parameters are sparse, we can further reduce the number of hyperplanes. For example, for an inactive element in the weight matrix W due to the sparsity, we can simply ignore the computation for it similarly to the operations on the sparse representations. Conceptually, we can say that those inactive weights serve as zero weights, so that a BNN can solve the problem in Figure 1 (d) by using only one hyperplane as in (e). From now on, we will use this extended version of BNN with inactive weights, yet there are some cases where BNN needs more hyperplanes than a real-valued network even with the sparsity."}, {"heading": "3. Training Bitwise Neural Networks", "text": "We first train some compressed network parameters, and then retrain them using noisy backpropagation for BNNs."}, {"heading": "3.1. Real-valued Networks with Weight Compression", "text": "First, we train a real-valued network that takes either bitwise inputs or real-valued inputs ranged between \u22121 and +1. A special part of this network is that we constrain the weights to have values between \u22121 and +1 as well by wrapping them with tanh. Similarly, if we choose tanh for the activation, we can say that the network is a relaxed version of the corresponding bipolar BNN. With this weight compression technique, the relaxed forward pass during training is defined as follows:\nali = tanh(b\u0304 l i) + Kl\u22121\u2211 j tanh(w\u0304li,j)z\u0304 l\u22121 j , (5)\nz\u0304li = tanh ( ali ) , (6)\nwhere all the binary values in (1) and (2) are real for the time being: W\u0304l \u2208 RKl\u00d7Kl\u22121 , b\u0304l \u2208 RKl , and z\u0304l \u2208 RKl . The bars on top of the notations are for the distinction.\nWeight compression needs some changes in the backpropagation procedure. In a hidden layer we calculate the error, \u03b4lj(n) = (Kl+1\u2211\ni\ntanh(w\u0304l+1i,j )\u03b4 l+1 i (n)\n) \u00b7 ( 1\u2212 tanh2 ( alj )) .\nNote that the errors fron the next layer are multiplied with the compressed versions of the weights. Hence, the gradients of the parameters in the case of batch learning are\n\u2207w\u0304li,j = (\u2211\nn\n\u03b4li(n)z\u0304 l\u22121 j\n) \u00b7 ( 1\u2212 tanh2 ( w\u0304li,j )) ,\n\u2207b\u0304li = (\u2211\nn\n\u03b4li(n) ) \u00b7 ( 1\u2212 tanh2 ( b\u0304li )) ,\nwith the additional term from the chain rule on the compressed weights."}, {"heading": "3.2. Training BNN with Noisy Backpropagation", "text": "Since we have trained a real-valued network with a proper range of weights, what we do next is to train the actual bitwise network. The training procedure is similar to the ones with quantized weights (Fiesler et al., 1990; Hwang & Sung, 2014), except that the values we deal with are all bits, and the operations on them are bitwise. To this end, we first initialize all the real-valued parameters, W\u0304 and b\u0304, with the ones learned from the previous section. Then, we setup a sparsity parameter \u03bb which says the proportion of the zeros after the binarization. Then, we divide the parameters into three groups: +1, 0, or \u22121. Therefore, \u03bb decides the boundaries \u03b2, e.g. wlij = \u22121 if w\u0304lij < \u2212\u03b2. Note that the number of zero weights |w\u0304lij | < \u03b2 equals to \u03bbKlKl\u22121.\nThe main idea of this second training phase is to feedforward using the binarized weights and the bit operations as in (1) and (2). Then, during noisy backpropagation the errors and gradients are calculated using those binarized weights and signals as well:\n\u03b4lj(n) = Kl+1\u2211 i wl+1i,j \u03b4 l+1 i (n),\n\u2207w\u0304li,j = \u2211 n \u03b4li(n)z l\u22121 j , \u2207b\u0304 l i = \u2211 n \u03b4li(n). (7)\nIn this way, the gradients and errors properly take the binarization of the weights and the signals into account. Since the gradients can get too small to update the binary parameters W and b, we instead update their corresponding realvalued parameters,\nw\u0304li,j \u2190 w\u0304li,j \u2212 \u03b7\u2207w\u0304li,j , b\u0304li,j \u2190 b\u0304li \u2212 \u03b7\u2207b\u0304li, (8)\nwith \u03b7 as a learning rate parameter. Finally, at the end of each update we binarize them again with \u03b2. We repeat this procedure at every epoch."}, {"heading": "4. Experiments", "text": "In this section we go over the details and results of the hand-written digit recognition task on the MNIST data set (LeCun et al., 1998) using the proposed BNN system. Throughout the training, we adopt the softmax output layer for these multiclass classification cases. All the networks have three hidden layers with 1024 units per layer.\nFrom the first round of training, we get a regular dropout network with the same setting suggested in (Srivastava et al., 2014), except the fact that we used the hyperbolic tangent for both weight compression and activation to make\nthe network suitable for initializing the following bipolar bitwise network. The number of iterations from 500 to 1, 000 was enough to build a baseline. The first row of Table 1 shows the performance of the baseline real-valued network with 64bits floating-point. As for the input to the real-valued networks, we rescale the pixel intensities into the bipolar range, i.e. from \u22121 to +1, for the bipolar case (the first column). In the second column, we use the original input between 0 and 1 as it is. For the third column, we encode the four equally spaced regions between 0 to 1 into two bits, and feed each bit into each input node. Hence, the baseline network for the third input type has 1, 568 binary input nodes rather than 784 as in the other cases.\nOnce we learn the real-valued parameters, now we train the BNN, but with binarized inputs. For instance, instead of real values between \u22121 and +1 in the bipolar case, we take their sign as the bipolar binary features. As for the 0/1 binaries, we simply round the pixel intensity. Fixed-point inputs are already binarized. Now we train the new BNN with the noisy backpropagation technique as described in 3.2. The second row of Table 1 shows the BNN results. We see that the bitwise networks perform well with very small additional errors. Note that the performance of the original real-valued dropout network with similar network topology (logistic units without max-norm constraint) is 1.35%."}, {"heading": "5. Conclusion", "text": "In this work we propose a bitwise version of artificial neural networks, where all the inputs, weights, biases, hidden units, and outputs can be represented with single bits and operated on using simple bitwise logic. Such a network is very computationally efficient and can be valuable for resource-constrained situations, particularly in cases where floating-point / fixed-point variables and operations are prohibitively expensive. In the future we plan to investigate a bitwise version of convolutive neural networks, where efficient computing is more desirable."}], "references": [{"title": "A survey on context-aware systems", "author": ["M. Baldauf", "S. Dustdar", "F. Rosenberg"], "venue": "International Journal of Ad Hoc and Ubiquitous Computing,", "citeRegEx": "Baldauf et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baldauf et al\\.", "year": 2007}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Low precision arithmetic for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Approximations by superpositions of sigmoidal functions", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals, and Systems,", "citeRegEx": "Cybenko,? \\Q1989\\E", "shortCiteRegEx": "Cybenko", "year": 1989}, {"title": "Weight discretization paradigm for optical neural networks", "author": ["E. Fiesler", "A. Choudry", "H.J. Caulfield"], "venue": "In The Hague\u201990,", "citeRegEx": "Fiesler et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Fiesler et al\\.", "year": 1990}, {"title": "On learning \u03bc-perceptron networks with binary weights", "author": ["M. Golea", "M. Marchand", "T.R. Hancock"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Golea et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Golea et al\\.", "year": 1992}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K. Hornik"], "venue": "Neural Networks,", "citeRegEx": "Hornik,? \\Q1991\\E", "shortCiteRegEx": "Hornik", "year": 1991}, {"title": "Fixed-point feedforward deep neural network design using weights", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "Hwang and Sung,? \\Q2014\\E", "shortCiteRegEx": "Hwang and Sung", "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W.H. Pitts"], "venue": "The Bulletin of Mathematical Biophysics,", "citeRegEx": "McCulloch and Pitts,? \\Q1943\\E", "shortCiteRegEx": "McCulloch and Pitts", "year": 1943}, {"title": "Computational limitations on learning from examples", "author": ["L. Pitt", "L.G. Valiant"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "Pitt and Valiant,? \\Q1988\\E", "shortCiteRegEx": "Pitt and Valiant", "year": 1988}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["D. Soudry", "I. Hubara", "R. Meir"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Soudry et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soudry et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "Dai", "L.-R", "Lee", "C.-H"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "According to the universal approximation theorem, a single hidden layer with a finite number of units can approximate a continuous function with some mild assumptions (Cybenko, 1989; Hornik, 1991).", "startOffset": 167, "endOffset": 196}, {"referenceID": 7, "context": "According to the universal approximation theorem, a single hidden layer with a finite number of units can approximate a continuous function with some mild assumptions (Cybenko, 1989; Hornik, 1991).", "startOffset": 167, "endOffset": 196}, {"referenceID": 6, "context": "cated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009).", "startOffset": 121, "endOffset": 156}, {"referenceID": 1, "context": "cated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009).", "startOffset": 121, "endOffset": 156}, {"referenceID": 14, "context": ", 2012), speech enhancement (Xu et al., 2014), etc, it is also the case that the relatively bigger networks with more parameters than before call for more resources (processing power, memory, battery time, etc), which are sometimes critically constrained in applications running on embedded devices.", "startOffset": 28, "endOffset": 45}, {"referenceID": 0, "context": "Examples of those applications span from context-aware computing, collecting and analysing a variety of sensor signals on the device (Baldauf et al., 2007), to always-on computer vision applications (e.", "startOffset": 133, "endOffset": 155}, {"referenceID": 4, "context": "Most of the effort is focused on training networks whose weights can be transformed into some quantized representations with a minimal loss of performance (Fiesler et al., 1990; Hwang & Sung, 2014).", "startOffset": 155, "endOffset": 197}, {"referenceID": 2, "context": "It was also shown that 10 bits and 12 bits are enough to represent gradients and storing weights for implementing the stateof-the-art maxout networks even for training the network (Courbariaux et al., 2014).", "startOffset": 180, "endOffset": 206}, {"referenceID": 5, "context": "Although learning those bitwise weights as a Boolean concept is an NPcomplete problem (Pitt & Valiant, 1988), the bitwise networks have been studied in the limited setting, such as \u03bcperceptron networks where an input node is allowed to be connected to one and only one hidden node and its final layer is a union of those hidden nodes (Golea et al., 1992).", "startOffset": 334, "endOffset": 354}, {"referenceID": 12, "context": "A more practical network was proposed in (Soudry et al., 2014) recently, where the posterior probabilities of the binary weights were sought using the Expectation Back Propagation (EBP) scheme, which is similar to backpropagation in its form, but has some advantages, such as parameterfree learning and a straightforward discretization of the weights.", "startOffset": 41, "endOffset": 62}, {"referenceID": 4, "context": "The training procedure is similar to the ones with quantized weights (Fiesler et al., 1990; Hwang & Sung, 2014), except that the values we deal with are all bits, and the operations on them are bitwise.", "startOffset": 69, "endOffset": 111}, {"referenceID": 9, "context": "In this section we go over the details and results of the hand-written digit recognition task on the MNIST data set (LeCun et al., 1998) using the proposed BNN system.", "startOffset": 116, "endOffset": 136}, {"referenceID": 13, "context": "From the first round of training, we get a regular dropout network with the same setting suggested in (Srivastava et al., 2014), except the fact that we used the hyperbolic tangent for both weight compression and activation to make Table 1.", "startOffset": 102, "endOffset": 127}], "year": 2016, "abstractText": "Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resourceconstrained environments, since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations. Hence, the BNN requires for less spatial complexity, less memory bandwidth, and less power consumption in hardware. In order to design such networks, we propose to add a few training schemes, such as weight compression and noisy backpropagation, which result in a bitwise network that performs almost as well as its corresponding realvalued network. We test the proposed network on the MNIST dataset, represented using binary features, and show that BNNs result in competitive performance while offering dramatic computational savings.", "creator": "LaTeX with hyperref package"}}}