{"id": "1601.06581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Character-Level Incremental Speech Recognition with Recurrent Neural Networks", "abstract": "In real-time speech recognition applications, the latency is an important issue. We have developed a character-level incremental speech recognition (ISR) system that responds quickly even during the speech, where the hypotheses are gradually improved while the speaking proceeds. The algorithm employs a speech-to-character unidirectional recurrent neural network (RNN), which is end-to-end trained with connectionist temporal classification (CTC), and an RNN-based character-level language model (LM). The output values of the CTC-trained RNN are character-level probabilities, which are processed by beam search decoding. The RNN LM augments the decoding by providing long-term dependency information. We propose tree-based online beam search with additional depth-pruning, which enables the system to process infinitely long input speech with low latency. This system not only responds quickly on speech but also can dictate out-of-vocabulary (OOV) words according to pronunciation. The proposed model achieves the word error rate (WER) of 8.90% on the Wall Street Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284 training set.", "histories": [["v1", "Mon, 25 Jan 2016 12:51:46 GMT  (86kb)", "https://arxiv.org/abs/1601.06581v1", "To appear in ICASSP 2016"], ["v2", "Thu, 28 Jan 2016 11:03:05 GMT  (86kb)", "http://arxiv.org/abs/1601.06581v2", "To appear in ICASSP 2016"]], "COMMENTS": "To appear in ICASSP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1601.06581"}, "pdf": {"name": "1601.06581.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kyuyeon.hwang@gmail.com;", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n06 58\n1v 2\n[ cs\n.C L\n] 2\n8 Ja\nn 20\nIndex Terms\u2014 Incremental speech recognition, character-level, recurrent neural networks, connectionist temporal classification, beam search\n1. INTRODUCTION\nIncremental speech recognition (ISR) allows a speech-based interaction system to react quickly while the utterance is being spoken. Unlike offline sentence-wise automatic speech recognition (ASR), where the decoding result is available after a user finishes speaking, ISR returns N -best decoding results with small latency during speech. These N -best results, or hypotheses, gradually improve as the system receives more speech data. Since ISR is usually employed for immediate reaction to speech, word stability [1, 2] and incremental lattice generation [3] have been important topics.\nIn this paper, we introduce an end-to-end character-level ISR system with two unidirectional recurrent neural networks (RNNs). An acoustic RNN roughly dictates the input speech and an RNNbased language model is employed to augment the dictation result through decoding. Compared to a conventional word-level backend for speech recognition system, the character-level ASR is capable of dictating out of vocabulary (OOV) words based on the pronuncia-\nThis work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2015R1A2A1A10056051).\ntion. Also, our model is trained directly from speech and text corpus and does not require external word dictionary or senone modeling.\nThere have been efforts to deal with OOV words in conventional HMM based ASR systems. In [4], graphemes are employed as basic units instead of phonemes. Also, a sub-lexical language model is proposed in [5] for detecting previously unseen words.\nRNN-based character-level end-to-end ASR systems were studied in [6, 7, 8, 9, 10]. However, they lack the capability of dictating OOV words since the decoding is performed with word-level LMs. Recently, a lexicon-free end-to-end ASR system is introduced in [11], where a character-level RNN LM is employed. We further improve this approach by employing prefix tree based online beam search with additional depth-pruning for ISR.\nThe character-level ISR system proposed in this paper is composed of an acoustic RNN and an RNN LM. The acoustic RNN is end-to-end trained with connectionist temporal classification (CTC) [12] using Wall Street Journal (WSJ) speech corpus [13]. The output of the acoustic RNN is the probability of characters, which are decoded with character-level beam search to generate N -best hypotheses. To improve the performance, a character-level RNN LM is employed to augment the beam search performance. Also, we propose depth-pruning for efficient tree-based beam search. The RNN LM is separately trained with large text corpus that is also included in WSJ corpus. Unlike for word-level language modeling, conventional statistical LMs such as n-gram back-off models cannot be used because much longer history window is required for character-level prediction. Both acoustic RNN and RNN LM have deep unidirectional long short-term memory (LSTM) network structures [14, 15]. For continuous ISR on infinitely long input speech, they are trained with virtually infinite training data streams that are generated by randomly concatenating training sequences.\nThe proposed model is evaluated on a single test sequence that is generated by concatenating all test utterances in WSJ eval92 (Nov\u201992 20k evaluation set) without any external reset of RNN states at the utterance boundaries. The ISR performance is examined by varying the beam width and depth. Generally, wider beam increases the accuracy. Under the same beam width, there is a trade-off between the accuracy and stability (or latency), where the balance between them can be adjusted by the beam depth.\n2. MODELS"}, {"heading": "2.1. Acoustic model", "text": "The acoustic model is a deep RNN trained with CTC [12]. The network consists of two LSTM layers with 768 cells each, where the\nnetwork has total 12.2 M trainable parameters. The model is similar to the one in the previous work about end-to-end speech recognition with RNNs [6] except a few major differences. In our case, the RNN is trained by online CTC [16] with very long training sequences that are generated by randomly concatenating several utterances. There is no need to reset the RNN states at the utterance boundary. This is necessary for ISR systems that runs continuously with an infinite input audio stream. Also, our model has a unidirectional structure since bidirectional networks that are usually employed for endto-end speech recognition are not suitable for low-latency speech recognition. This is because the backward layers in the bidirectional networks cannot be computed before the input utterance is finished.\nThe input of the network is a 40-dimensional log mel-frequency filterbank feature vector with energy and their delta and double-delta values, resulting in an 123-dimensional vector. The feature vectors are extracted every 10 ms with 25 ms Hamming window. The input vectors are element-wisely standardized based on the statistics obtained from the training set. The output is a 31-dimensional vector that consists of the probabilities of 26 upper case alphabets, 3 special characters, the end-of-sentence (EOS) symbol, and the CTC blank label.\nThe networks are trained with stochastic gradient descent (SGD) with 8 parallel input streams on a GPU [17]. The networks are unrolled 2048 times and weight updates are performed every 1024 forward steps. The network performances are evaluated at every 10 M training frames. The evaluation is performed on total 2 M frames from the development set. The learning rate starts from 1 \u00d7 10\u22125 and is reduced by the factor of 10 whenever the WER on the development set is not improved for 6 consecutive evaluations. The training ends when the learning rate drops below 1\u00d7 10\u22127.\nWe trained the networks on two training sets. The first one is the standard WSJ SI-284 set and the second one, SI-ALL, is the set of all speaker independent training utterances in the WSJ corpus. Note that the utterances with verbalized punctuations are removed from both training sets. Also, odd transcriptions are filtered out, which makes the final SI-284 and SI-ALL sets contain roughly 71 and 167 hours of speech, respectively. WSJ dev93 (Nov\u201993 20k development set) and eval92 (Nov\u201992 20k evaluation set) sets are used as the development set and the evaluation set, respectively."}, {"heading": "2.2. Language model", "text": "An RNN language model (LM) [18] is employed for the proposed ISR system since conventional statistical LMs such as n-gram backoff models are not suitable for character-level prediction since they cannot make use of very long history windows. Specifically, the\nRNN LM has a deep LSTM network structure with two LSTM layers where each of them has 512 memory cells, resulting in total 3.2 M parameters.\nThe input of the RNN LM is a 30-dimensional vector, where the current label (character) is one-hot encoded. The output is also a 30-dimensional vector which represents the probabilities of next labels. Although the RNN LM is trained to predict the next characters with only given the current character, the past character histories are internally stored inside the RNN and used for the prediction. It is well known that RNN LM can remember contexts for very long time steps.\nAs for the acoustic RNN, the RNN LM is trained on a very long text stream that is generated by attaching randomly picked sentences and inserting EOS labels between sentences. The RNN LM is trained with AdaDelta [19] based SGD method for accelerated training and better annealing. The WSJ LM training text with nonverbalized punctuation, which contains about 215 M characters, is used for training the RNN LM. Randomly selected 1% of the corpus is reserved for evaluation, on which the final bits-per-character (BPC) of the RNN LM is 1.167 (character-level perplexity of 2.245).\nRandom sentences can be generated following the method described in [20]. Briefly, the next label is randomly picked following the probabilities of the current output of the RNN LM and fed back to the RNN in the next step. By iterating these steps, texts can be sequentially generated as shown in Figure 1. From the example, it is clear that the RNN LM learned the linguistic structures as well as spellings of words that frequently appear.\n3. CHARACTER-LEVEL BEAM SEARCH"}, {"heading": "3.1. Tree-based CTC beam search", "text": "Let L be the set of labels without the CTC blank label. The label sequence z is a sequence of labels in L. The length of the label\nsequence z is less than or equal to the number of input frames. The objective of the beam search decoding is to find the label sequence that has the maximum posterior probability given the input features from time 1 to t generated by the acoustic RNNs, that is,\nzmax = argmax z P (z|x1:t), (1)\nwhere x1:t is the input features from time 1 to t. However, the CTC-trained RNN output has one more blank label. Let L\u2032 be the set of labels (or CTC states) with the additional CTC blank label, and the path \u03c0(i)t be a sequence of labels in L \u2032 from time 1 to t. The length of the path \u03c0(i)t is the same as t. By the definition of CTC, every \u03c0 can be reduced into the corresponding z. For example, \u03c0 with \u201caab-c\u2013a\u201d corresponds to z with \u201cabca\u201d, where \u201c-\u201d is the blank label.\nThere can be many paths, \u03c0(i)t , that can be reduced into the same z. Let F(\u00b7) be a function that maps a path to the corresponding label sequence, that is, F(\u03c0(i)t ) = z, then the posterior probability in (1) becomes,\nP (z|x1:t) = \u2211\n{\u2200i|F(\u03c0 (i) t )=z}\nP (\u03c0 (i) t |x1:t). (2)\nTherefore, if the two different paths \u03c0(j)t and \u03c0 (k) t in the decoding network are mapped to the same z, then they can be merged by summing their probabilities.\nFor the beam search, we first represent the lattice with a treebased structure so that each node has one of labels in L as depicted in Figure 2. Then, backtracking from any node generates a unique label sequence z. To deal with CTC state transitions, we need a statebased network that is represented with CTC states, L\u2032. As shown in Figure 3, this can be easily done by expanding each tree node, of which label is in L, into two CTC states, one with the corresponding label in L\u2032 followed by the blank CTC label. Since the label-level (L) search network is based on a tree structure, two different statelevel (L\u2032) paths with different label sequences never meet each other. This simplifies the problem since there is no interaction between two different sequence labelings (hypotheses) and (2) is the only equation that we should concern.\nAs proposed in [8, 11], external language models can be integrated by modifying the posterior probability term in (1) into:\nlog(P (z|x1:t)) = log(PCTC(z|x1:t)) (3)\n+ \u03b1log(PLM(z)) + \u03b2|z|,\nwhere \u03b1 is the LM weight and \u03b2 is the insertion bonus. This modification can be applied by adding the additional terms with \u03b1 and \u03b2 to the log probability of the destination state when a state transition between two different label nodes occurs.\nThe probability of the next label is computed using the RNN LM when a new active label node is added to the beam search tree. For this, the RNN LM context (hidden activations) is copied from the parent node to the child node and the RNN LM processes the new label of the child node with the copied context. Therefore, each active node has its own RNN LM context."}, {"heading": "3.2. Pruning", "text": "Pruning of the search tree is performed by the standard beam search approach. That is, at each frame, only the active nodes with the top N hypotheses and their ancestor nodes remain alive after the pruning with the beam width of N . However, this standard pruning, or width-pruning, cannot prevent the tree from growing indefinitely especially when the input speech is very long. This gradually degrades the efficiency of beam search on recent nodes since more and more hypotheses would be wasted to maintain the old part of the lattice that is already out of the context range of the RNN LMs.\nTo remedy this issue, we propose an additional pruning method called depth-pruning. The procedure is as follows. First, find the M -th ancestor of the node with the best hypothesis, where M is the beam depth. Then, the ancestor node becomes a new root node. The pruning is performed by removing the nodes that are not descendants of the new root node. In this way, a beam can be better utilized for recent hypotheses rather than older ones. Figure 4 shows an example of depth-pruning with the beam depth of 2. Note that the depth of some nodes can be larger than the beam depth. In the following experiments, depth-pruning is performed every 20 frames.\n4. EXPERIMENTS\nThe proposed ISR system is evaluated on a single 42-minute speech stream that is formed by concatenating all 333 utterances in the evaluation set, eval92 (WSJ Nov\u201992 20k evaluation set). We use \u03b1 = 2.0 and \u03b2 = 1.5 for the system trained with SI-284, and \u03b1 = 1.5 and \u03b2 = 2.0 for the other one trained with SI-ALL.\nThe effects of beam depth and width to the final WER are examined in Figure 5. The gap between the beam width of 128 and 512\nis roughly 0.5% to 1% WER. However, there was little difference when the beam width increases from 512 to 2048 in our preliminary experiments. The best performing beam depths are 50 and 30 for the SI-284 and SI-ALL systems, respectively. This means the SI-ALL system can recognize speech more immediately than the SI-284 system. We consider this is because the acoustic model of the SI-ALL system can embed stronger language model due to increased training data, and can make decision more precisely without relying on the external language model much. The character error rate (CER) and WER are reported in Table 1 with the optimal beam depths. For comparison, we also report sentence-wise offline decoding results without depth-pruning.\nThe proposed ISR system is compared with other end-to-end word-level speech recognition systems in Table 2. The other systems perform sentence-wise offline decoding with bidirectional RNNs. The best result was achieved by Miao et al. [9] with a CTC-trained deep bidirectional LSTM network and a retrained trigram LM with extended vocabulary. The systems with the original trigram model provided with the WSJ corpus perform worse than our ISR system with character-level RNN LM. On the other hand, our system is beaten by the other ones with extended trigram models. However, more precise comparison of the decoding stages should be done by employing the same CTC model.\nFigure 6 shows the incremental speech recognition result with the proposed ISR system. The best hypothesis is reported every 50 frames (500 ms). It is shown that the past best result can be corrected by making use of the additional speech input. For example, the word \u201cROCK\u201d is changed to \u201cDRAW\u201d in the frame 450 by listening the word \u201cRATE\u201d. Moreover, the correction of \u201cIN DRAW RATE\u201d to \u201cAND DRAW CROWD\u201d during hearing the word \u201cPEOPLE\u201d in the frame 500 is a good evidence that long term context can also be considered.\n5. CONCLUDING REMARKS\nA character-level incremental speech recognizer is proposed and analyzed throughout the paper. The proposed system combines a CTCtrained RNN with a character-level RNN LM through tree-based beam search decoding. For online decoding with very long input speech, depth-pruning is proposed to prevent indefinite growth of the search tree. When the proposed model is trained with WSJ SI-284, 8.90% WER can be achieved on the very long speech that is formed by concatenating all utterances in the WSJ eval92 evaluation set. The incremental recognition result shows the evidence that character-level RNN LM can learn dependencies between two words even when they are five words apart, which are hard to be caught using conventional n-gram back-off language models.\nNote that the proposed system only requires speech and text corpus for training. External lexicon or senone modeling is not needed for training, which is a huge advantage. Moreover, it is expected that OOV words or infrequent words such as names of places or people can be dictated as they are pronounced.\n6. REFERENCES\n[1] Ethan O Selfridge, Iker Arizmendi, Peter A Heeman, and Jason D Williams, \u201cStability and accuracy in incremental speech recognition,\u201d in Proceedings of the SIGDIAL 2011 Conference. Association for Computational Linguistics, 2011, pp. 110\u2013119.\n[2] Ian McGraw and Alexander Gruenstein, \u201cEstimating wordstability during incremental speech recognition,\u201d Training, vol. 17, no. 27,327, pp. 6\u20134, 2011.\n[3] Gerhard Sagerer, Heike Rautenstrauch, Gernot A Fink, Bernd Hildebrandt, A Jusek, and Franz Kummert, \u201cIncremental generation of word graphs.,\u201d in ICSLP. Citeseer, 1996.\n[4] Mirjam Killer, Sebastian Stu\u0308ker, and Tanja Schultz, \u201cGrapheme based speech recognition.,\u201d in INTERSPEECH, 2003.\n[5] Maximilian Bisani and Hermann Ney, \u201cOpen vocabulary speech recognition with flat hybrid models.,\u201d in INTERSPEECH, 2005, pp. 725\u2013728.\n[6] Alex Graves and Navdeep Jaitly, \u201cTowards end-to-end speech recognition with recurrent neural networks,\u201d in Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764\u20131772.\n[7] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeepSpeech: Scaling up endto-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014.\n[8] Awni Y Hannun, Andrew L Maas, Daniel Jurafsky, and Andrew Y Ng, \u201cFirst-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs,\u201d arXiv preprint arXiv:1408.2873, 2014.\n[9] Yajie Miao, Mohammad Gowayyed, and Florian Metze, \u201cEESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,\u201d arXiv preprint arXiv:1507.08240, 2015.\n[10] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio, \u201cEnd-to-end attentionbased large vocabulary speech recognition,\u201d arXiv preprint arXiv:1508.04395, 2015.\n[11] Andrew L Maas, Ziang Xie, Dan Jurafsky, and Andrew Y Ng, \u201cLexicon-free conversational speech recognition with neural networks,\u201d in NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA, May 31 - June 5, 2015, 2015, pp. 345\u2013354.\n[12] Alex Graves, Santiago Ferna\u0301ndez, Faustino Gomez, and Ju\u0308rgen Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.\n[13] Douglas B Paul and Janet M Baker, \u201cThe design for the Wall Street Journal-based CSR corpus,\u201d in Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 357\u2013362.\n[14] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[15] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed, \u201cHybrid speech recognition with deep bidirectional LSTM,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.\n[16] Kyuyeon Hwang and Wonyong Sung, \u201cOnline sequence training of recurrent neural networks with connectionist temporal classification,\u201d arXiv preprint arXiv:1511.06841, 2015.\n[17] Kyuyeon Hwang and Wonyong Sung, \u201cSingle stream parallelization of generalized LSTM-like RNNs on a GPU,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 1047\u2013 1051.\n[18] Toma\u0301s\u030c Mikolov, Stefan Kombrink, Luka\u0301s\u030c Burget, Jan Honza C\u030cernocky\u0300, and Sanjeev Khudanpur, \u201cExtensions of recurrent neural network language model,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5528\u20135531.\n[19] Matthew D Zeiler, \u201cADADELTA: An adaptive learning rate method,\u201d arXiv preprint arXiv:1212.5701, 2012.\n[20] Ilya Sutskever, James Martens, and Geoffrey E Hinton, \u201cGenerating text with recurrent neural networks,\u201d in Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1017\u20131024.\n[21] Phillip C Woodland, Julian J Odell, Valtcho Valtchev, and Steve J Young, \u201cLarge vocabulary continuous speech recognition using HTK,\u201d in Acoustics, Speech, and Signal Processing, 1994. ICASSP-94., 1994 IEEE International Conference on. IEEE, 1994, vol. 2, pp. II\u2013125."}], "references": [{"title": "Stability and accuracy in incremental speech recognition", "author": ["Ethan O Selfridge", "Iker Arizmendi", "Peter A Heeman", "Jason D Williams"], "venue": "Proceedings of the SIGDIAL 2011 Conference. Association for Computational Linguistics, 2011, pp. 110\u2013119.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating wordstability during incremental speech recognition", "author": ["Ian McGraw", "Alexander Gruenstein"], "venue": "Training, vol. 17, no. 27,327, pp. 6\u20134, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental generation of word graphs", "author": ["Gerhard Sagerer", "Heike Rautenstrauch", "Gernot A Fink", "Bernd Hildebrandt", "A Jusek", "Franz Kummert"], "venue": "ICSLP. Citeseer, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Grapheme based speech recognition", "author": ["Mirjam Killer", "Sebastian St\u00fcker", "Tanja Schultz"], "venue": "INTERSPEECH, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Open vocabulary speech recognition with flat hybrid models", "author": ["Maximilian Bisani", "Hermann Ney"], "venue": "INTER- SPEECH, 2005, pp. 725\u2013728.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764\u20131772.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "DeepSpeech: Scaling up endto-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs", "author": ["Awni Y Hannun", "Andrew L Maas", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "arXiv preprint arXiv:1408.2873, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "arXiv preprint arXiv:1507.08240, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end attentionbased large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1508.04395, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["Andrew L Maas", "Ziang Xie", "Dan Jurafsky", "Andrew Y Ng"], "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA, May 31 - June 5, 2015, 2015, pp. 345\u2013354.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "The design for the Wall Street Journal-based CSR corpus", "author": ["Douglas B Paul", "Janet M Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 357\u2013362.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Online sequence training of recurrent neural networks with connectionist temporal classification", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "arXiv preprint arXiv:1511.06841, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 1047\u2013 1051.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Honza \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 5528\u20135531.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1017\u20131024.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Large vocabulary continuous speech recognition using HTK", "author": ["Phillip C Woodland", "Julian J Odell", "Valtcho Valtchev", "Steve J Young"], "venue": "Acoustics, Speech, and Signal Processing, 1994. ICASSP-94., 1994 IEEE International Conference on. IEEE, 1994, vol. 2, pp. II\u2013125.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "Since ISR is usually employed for immediate reaction to speech, word stability [1, 2] and incremental lattice generation [3] have been important topics.", "startOffset": 79, "endOffset": 85}, {"referenceID": 1, "context": "Since ISR is usually employed for immediate reaction to speech, word stability [1, 2] and incremental lattice generation [3] have been important topics.", "startOffset": 79, "endOffset": 85}, {"referenceID": 2, "context": "Since ISR is usually employed for immediate reaction to speech, word stability [1, 2] and incremental lattice generation [3] have been important topics.", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "In [4], graphemes are employed as basic units instead of phonemes.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Also, a sub-lexical language model is proposed in [5] for detecting previously unseen words.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "RNN-based character-level end-to-end ASR systems were studied in [6, 7, 8, 9, 10].", "startOffset": 65, "endOffset": 81}, {"referenceID": 6, "context": "RNN-based character-level end-to-end ASR systems were studied in [6, 7, 8, 9, 10].", "startOffset": 65, "endOffset": 81}, {"referenceID": 7, "context": "RNN-based character-level end-to-end ASR systems were studied in [6, 7, 8, 9, 10].", "startOffset": 65, "endOffset": 81}, {"referenceID": 8, "context": "RNN-based character-level end-to-end ASR systems were studied in [6, 7, 8, 9, 10].", "startOffset": 65, "endOffset": 81}, {"referenceID": 9, "context": "RNN-based character-level end-to-end ASR systems were studied in [6, 7, 8, 9, 10].", "startOffset": 65, "endOffset": 81}, {"referenceID": 10, "context": "Recently, a lexicon-free end-to-end ASR system is introduced in [11], where a character-level RNN LM is employed.", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "The acoustic RNN is end-to-end trained with connectionist temporal classification (CTC) [12] using Wall Street Journal (WSJ) speech corpus [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "The acoustic RNN is end-to-end trained with connectionist temporal classification (CTC) [12] using Wall Street Journal (WSJ) speech corpus [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "Both acoustic RNN and RNN LM have deep unidirectional long short-term memory (LSTM) network structures [14, 15].", "startOffset": 103, "endOffset": 111}, {"referenceID": 14, "context": "Both acoustic RNN and RNN LM have deep unidirectional long short-term memory (LSTM) network structures [14, 15].", "startOffset": 103, "endOffset": 111}, {"referenceID": 11, "context": "The acoustic model is a deep RNN trained with CTC [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "The model is similar to the one in the previous work about end-to-end speech recognition with RNNs [6] except a few major differences.", "startOffset": 99, "endOffset": 102}, {"referenceID": 15, "context": "In our case, the RNN is trained by online CTC [16] with very long training sequences that are generated by randomly concatenating several utterances.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "The networks are trained with stochastic gradient descent (SGD) with 8 parallel input streams on a GPU [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "An RNN language model (LM) [18] is employed for the proposed ISR system since conventional statistical LMs such as n-gram backoff models are not suitable for character-level prediction since they cannot make use of very long history windows.", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "The RNN LM is trained with AdaDelta [19] based SGD method for accelerated training and better annealing.", "startOffset": 36, "endOffset": 40}, {"referenceID": 19, "context": "Random sentences can be generated following the method described in [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "As proposed in [8, 11], external language models can be integrated by modifying the posterior probability term in (1) into:", "startOffset": 15, "endOffset": 22}, {"referenceID": 10, "context": "As proposed in [8, 11], external language models can be integrated by modifying the posterior probability term in (1) into:", "startOffset": 15, "endOffset": 22}, {"referenceID": 5, "context": "90% Graves and Jaitly [6] CTC + Trigram (extended) 8.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "[9] CTC + Trigram (extended) 7.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] CTC + Trigram 9.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] CTC + Bigram 14.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Encoder-decoder + Trigram 11.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] GMM-HMM + Trigram 9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] DNN-HMM + Trigram 7.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] with a CTC-trained deep bidirectional LSTM network and a retrained trigram LM with extended vocabulary.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "In real-time speech recognition applications, the latency is an important issue. We have developed a character-level incremental speech recognition (ISR) system that responds quickly even during the speech, where the hypotheses are gradually improved while the speaking proceeds. The algorithm employs a speech-to-character unidirectional recurrent neural network (RNN), which is end-to-end trained with connectionist temporal classification (CTC), and an RNN-based character-level language model (LM). The output values of the CTC-trained RNN are character-level probabilities, which are processed by beam search decoding. The RNN LM augments the decoding by providing long-term dependency information. We propose tree-based online beam search with additional depth-pruning, which enables the system to process infinitely long input speech with low latency. This system not only responds quickly on speech but also can dictate out-of-vocabulary (OOV) words according to pronunciation. The proposed model achieves the word error rate (WER) of 8.90% on the Wall Street Journal (WSJ) Nov\u201992 20K evaluation set when trained on the WSJ SI-284 training set.", "creator": "LaTeX with hyperref package"}}}