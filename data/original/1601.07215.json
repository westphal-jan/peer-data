{"id": "1601.07215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2016", "title": "Recurrent Neural Network Postfilters for Statistical Parametric Speech Synthesis", "abstract": "In the last two years, there have been numerous papers that have looked into using Deep Neural Networks to replace the acoustic model in traditional statistical parametric speech synthesis. However, far less attention has been paid to approaches like DNN-based postfiltering where DNNs work in conjunction with traditional acoustic models. In this paper, we investigate the use of Recurrent Neural Networks as a potential postfilter for synthesis. We explore the possibility of replacing existing postfilters, as well as highlight the ease with which arbitrary new features can be added as input to the postfilter. We also tried a novel approach of jointly training the Classification And Regression Tree and the postfilter, rather than the traditional approach of training them independently.", "histories": [["v1", "Tue, 26 Jan 2016 22:53:45 GMT  (70kb,D)", "http://arxiv.org/abs/1601.07215v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["prasanna kumar muthukumar", "alan w black"], "accepted": false, "id": "1601.07215"}, "pdf": {"name": "1601.07215.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORK POSTFILTERS FOR STATISTICAL PARAMETRIC SPEECH SYNTHESIS", "authors": ["Prasanna Kumar Muthukumar", "Alan W Black"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Recurrent Neural network, Postfilter, Statistical Parametric Speech synthesis"}, {"heading": "1. INTRODUCTION", "text": "Deep Neural Networks have had a tremendous influence on Automatic Speech Recognition in the last few years. Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5]. Despite this, it would be difficult to argue as of now that deep neural networks have had the same success in synthesis that they have had in ASR. DNN-influenced improvements in synthesis have mostly been fairly moderate. This becomes fairly evident when looking at the submissions to the Blizzard Challenge[6] in the last 3 years. Few of the submitted systems use deep neural networks in any part of the pipeline, and those that do use DNNs, do not seem to have any advantage over traditional well-trained systems.\nEven in cases where the improvements look promising, the techniques have had to rely on the use of much larger datasets than is typically used. The end result of this is that Statistical Parametric Synthesis ends up having to lose the advantage it has over traditional unit-selection systems[7] in terms of the amount of data needed to build a reasonable system.\nThat being said, it is still be unwise to rule out the possibility of DNNs playing an important role in speech synthesis research in the future. DNNs are extremely powerful models, and like many algorithms at the forefront of machine learning research, it might be the case we haven\u2019t yet found the best possible way to use them. With this in mind, in this paper we explore the idea of using DNNs to supplement existing systems rather than as a replacement for any part of the system. More specifically, we will explore the possibility of using DNNs as a postfilter to try to correct the errors made by the Classification And Regression Trees (CARTs)."}, {"heading": "2. RELATION TO PRIOR WORK", "text": "The idea of using a postfilter to fix flaws in the output of the CARTs is not very new. Techniques such as Maximum Likelihood Parameter Generation (MLPG)[8] and Global variance[9] have become standard, and even the newer ideas like the use of Modulation Spectrum[10] have started moving into the mainstream. These techniques provide a significant improvement in quality, but suffer from the drawback that the post-filter has to be derived analytically for each feature that is used. MLPG for instance exclusively deals with the means, the deltas, and the delta-deltas for every state. Integrating non-linear combinations of the means across frames or arbitrary new features like wavelets into MLPG will be somewhat non-trivial, and requires revisiting the equations behind MLPG as well as a rewrite of the code.\nUsing a Deep Neural Network to perform this postfiltering can overcome many of these issues. Neural networks are fairly agnostic to the type of features provided as input. The input features can also easily be added or removed without having to do an extensive code rewrite.\nThere has been prior work in using DNN based postfilters for parametric synthesis in [11] and [12]. However, we differ from these in several ways. One major difference is in the use of Recurrent Neural Networks (RNNs) as opposed to standard feedforward networks or generative models like Deep Belief Nets. We believe that inherent structure of RNNs is particularly suited to the time sensitive nature of the speech signal. RNNs have been used before for synthesis in [5], but as a replacement for the existing acoustic model and not as a\nar X\niv :1\n60 1.\n07 21\n5v 1\n[ cs\n.C L\n] 2\n6 Ja\nn 20\n16\npostfilter. In addition to this, we also explore the use of lexical features as input to the postfilter. To our knowledge, this is the first attempt at building a postfilter (neural network based or otherwise) that can make use of text based features like phone and state information in addition to spectral features like the MCEP means and standard deviations. We also describe our efforts in making use of a novel algorithm called Method of Auxiliary Coordinates (MAC) to jointly train the CARTs and the postfilter, rather than the traditional approach of training the postfilter independent of the CART."}, {"heading": "3. RECURRENT NEURAL NETWORKS", "text": "The standard feedforward neural network processes data one sample at a time. While this may be perfectly appropriate for handling images, data such as speech or video has an inherent time element that is often ignored by these kind of networks. Each frame of a speech sample is heavily influenced by the frames that were produced before it. Both CARTs and feedforward networks generally tend to ignore these interdependencies between consecutive frames. Crude approximations like stacking of frames are typically used to attempt to overcome these problems.\nA more theoretically sound approach to handle the interdependencies between consecutive frames is to use a Recurrent Neural Network[13]. RNNs differ from basic feedforward neural networks in their hidden layers. Each RNN hidden layer receives inputs not only from its previous layer but also from activations of itself for previous inputs. A simplified version of this structure is shown in figure 1. In the actual RNN that we used, every node in the hidden layer is connected to the previous activation of every node in that layer. However, most of these links have been omitted in the figure for clarity. The structure we use for this paper is more or less identical to the one described in section 2.5 of [14].\nTo train an RNN to act as a postfilter, we start off by building a traditional Statistical Parametric Speech Synthesis system. The particular synthesizer we use is the Clustergen system[15]. Once Clustergen has been trained on a corpus in the standard way, we make Clustergen re-predict the entire\ntraining data. As a result, we will now have Clustergen\u2019s predictions of Mel Cepstral Coefficients (MCEPs) which is time aligned with the original MCEPs extracted from speech. We then train an RNN to learn to predict the original MCEP statics based on Clustergen\u2019s predictions of the MCEP statics and deltas. This trained RNN can then be applied to the CART\u2019s predictions of test data to remove some of the noise in prediction. We use 25 dimensional MCEPs in our experiments. So the RNN takes 50 inputs (25 MCEP statics + 25 deltas) and predicts 25 features (MCEP statics). The results of doing this on four different corpora are shown in table 1. Each voice had its own RNN. Mel Cepstral Distortion[16] is used as the objective metric.\nRMS and SLT are voices from the CMU Arctic speech databases[17], about an hour of speech each. CXB is an American female, and the corpus consists of various recordings from audiobooks. A 2-hour subset of this corpus was used for the experiments described in this paper. AXB is a 2-hour corpus of Hindi speech recorded by an Indian female. RMS, SLT, and AXB were designed to be phonetically balanced. CXB is not phonetically balanced, and is an audiobook unlike the others which are corpora designed for building synthetic voices.\nThe RNN was implemented using the Torch7 toolkit[18]. The hyper-parameters were tuned on the RMS voice for the experiment described in table 1. The result of this was an RNN with 500 nodes in a single recurrent hidden layer, with sigmoids as non-linearities, and a linear output layer. To train the RNN, we used the ADAGRAD[19] algorithm along with a two-step BackPropagation Through Time[20]. A batch size of 10, and a learning rate of 0.01 were used. Early stopping was used for regularization. L1, L2 regularization, and momentum did not improve performance when used in addition to early stopping. No normalization was done on either the output or the input MCEPs. The hyperparameters were not tuned for any other voice, and even the learning rate was left unchanged.\nThe results reported in table 1 are with the MLPG option in Clustergen turned off. The reason for this is that MLPG requires the existence of standard deviations, in addition to the means of the parameters that the CART predicts. There is no\ntrue set of standard deviations that can be provided as training data for the RNN to learn to predict; it only has access to the true means in the training data. That being said, we did however apply MLPG by taking the MCEP means from the RNN, and standard deviations from the original CART predictions. We found that it did not matter whether the means for the deltas were predicted by the RNN or if they were taken from the CART predictions themselves. The magnitudes of the deltas were typically so small that it did not influence the RNN training as much as the statics did. So, the deltas were omitted from the RNN predictions in favor of using the deltas predicted by the CARTs directly. This also made the training a lot faster as the size of the output layer was effectively halved. The results of applying MLPG on the results from table 1 are shown in table 2. Note that MLPG was applied on the baseline system as well as the RNN postfilter system."}, {"heading": "4. ADDING LEXICAL FEATURES", "text": "In all of the previous experiments, the RNN was only using the same input features that MLPG typically uses. This however does not leverage the full power of the RNN. Any arbitrary feature can be fed as input for the RNN to learn from. This is the advantage that an RNN has over traditional postfiltering methods such as MLPG or Modulation Spectrum.\nWe added all of Festival\u2019s lexical features as additional input features for the RNN, in addition to CART\u2019s predictions of f0, MCEP statics and deltas, and voicing. The standard deviations as well as the means of the CART predictions were used. This resulted in 776 input features for the English language voices and 1076 features for the Hindi one (the Hindi phoneset for synthesis is slightly larger). The output features were the same 25-dimensional MCEPs from the previous set of experiments. The results of applying this kind of RNN on various corpora are shown in the following tables. As for the previous set of experiments, each voice had its own RNN. Table 3 and table 4 show results without and with MLPG respectively. In addition to the voices tested in previous experiments, we also tested this approach on three additional corpora, KSP (Indian male, 1 hour of Arctic), GKA (Indian male, 30mins of Arctic), and AUP (Indian male, 30 mins of Arctic).\nAs can be seen in the tables, the RNN always gives an improvement when MLPG is turned off. [21] reports that an\nMCD decrease of 0.12 is equivalent to doubling the amount of training data. The MCD decrease in table 3 is far beyond that in all of the voices that are tested. It is especially heartening to see that the MCD decreases significantly even in the case where the corpus is only 30 minutes. With MLPG turned on, the RNN always improves the system or is no worse. The decrease in MCD is much smaller though.\nWith MLPG turned on, the decrease in MCD with the RNN system is not large enough for humans to be able to do reasonable listening tests. With MLPG turned off though, the RNN system was shown to be vastly better in informal listening tests. But our goal was to build a system much better than the baseline system with MLPG, and so no formal listening tests were done."}, {"heading": "5. JOINT TRAINING OF THE CART AND THE POSTFILTER", "text": "The traditional way to build any postfilter for Statistical Parametric Speech Synthesis is to start off by building the Classification And Regression Trees that predict the parameters of the speech signal, and then train or apply the postfilter on the output of the CART. The drawback of this is that the CART is unnecessarily agnostic to the existence of the postfilter. CARTs and postfilters need not necessarily work well together, given that each has its own set of idiosyncracies when dealing with data. MCEPs might not be the best representation that connects these two. In an ideal system, the CART and the postfilter should jointly agree upon a representation of the data. This representation should be easy for the CART to learn, as well as reasonable for the postfilter to correct.\nOne way of achieving this goal is to use a fairly new technique called Method of Auxiliary Coordinates (MAC)[22]. To understand how this technique works, we need to mathematically formalize our problem.\nLet X represent the linguistic features that are input to the CART. Let Y be the correct MCEPs that need to be predicted at the end of synthesis. Let f1() be the function that the CART represents, and f2() the function that the RNN represents. The CART and the RNN both have parameters that are learned as part of training. These can be concatenated into one set of parameters W . The objective function we will want to minimize can therefore be written as:\nE(W ) = \u2016f2(f1(X;W );W )\u2212 Y \u20162\nThe MAC algorithm basically rewrites the above equation to make it easier to solve. This is done by explicitly defining an intermediate representation Z that acts as the target for the CARTs and as input to the RNN. The previous equation can now be rewritten as:\nE(W,Z) = \u2016f2(Z;W )\u2212 Y \u20162\ns.t. Z = f1(X;W )\nThe hard constraint in the above equation is then converted to a quadratic penalty term:\nE(W,Z;\u00b5) = \u2016f2(Z;W )\u2212 Y \u20162 + \u00b5\u2016f1(X;W )\u2212 Z\u20162\nwhere \u00b5 is slowly increased towards\u221e. The original objective function in the first equation only had the parameters of the CART and the RNN as variables to be used for the minimization. The rewritten objective function adds the intermediate representation Z as an auxiliary variable that will also be used in the minimization. We minimize the objective function by alternatingly optimizing the parameters W and the intermediate variables Z. Minimizing with respect to W is more or less equivalent to training the CARTs and RNNs independently using a standard RMSE criterion. Minimization with respect to Z is done through Stochastic Gradient Descent. Intuitively, this alternating minimization has the effect of alternating between optimizing the CART and RNN, and optimizing for an intermediate representation that works well with both.\nWe applied this algorithm to the RNN and CARTs built for the SLT and RMS voices built for table 1. The results of\nthis are shown in table 5. Starting from the second iteration, any further optimization of either the W or the Z variables only results in a very small decrease in the value of the objective function. So, we did not run the code past the second iteration.\nWe did not try MAC for the RNNs which use lexical features. This is because running the Z optimization on those RNNs would have given us a new set of lexical features for which we would have no way of extracting from text.\nOn our experiments on the RMS and SLT voices, there was no significant improvement in MCD. There are marginal MCD improvements in the 1st iteration for the SLT voice but these are not significant. Preliminary experiments suggest that one reason for the suboptimal performance is the overfitting towards the training data. It is difficult to say why the MAC algorithm does not perform very well in this framework. The search space for the parameters and the number of possible experiments that can be run is extremely large though, and so it is likely that a more thorough investigation will provide positive results."}, {"heading": "6. DISCUSSION", "text": "In this paper, we have looked at the effect of using RNN based postfilters. The massive improvements we get in the absence of other postfilters such as MLPG indicate that the RNNs are definitely a viable option in the future, especially because of the ease with which random new features can be added. However the combination of MLPG and RNNs are slightly less convincing. This could mean that the RNNs have learned to do approximately the same thing as MLPG does. Or it could mean that MLPG is not really appropriate to be used on RNN outputs. We believe that the answer might actually be a combination of both. The right solution might ultimately be to find an algorithm akin to MAC which can tie various postfilters together for joint training. Future investigations in these directions might lead to insightful new results."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "Nearly all experiments reported in this paper were run on a Tesla K40 provided by an Nvidia hardware grant, or on g2.8xlarge EC2 instances funded by an Amazon AWS research grant. In the absence of these, it would have been impossible to run all of the experiments described above in a reasonable amount of time."}, {"heading": "8. REFERENCES", "text": "[1] Heiga Zen, Keiichi Tokuda, and Alan W Black, \u201cStatistical parametric speech synthesis,\u201d Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.\n[2] John Dines, Junichi Yamagishi, and Simon King, \u201cMeasuring the gap between HMM-based ASR and TTS,\u201d Selected Topics in Signal Processing, IEEE Journal of, vol. 4, no. 6, pp. 1046\u20131058, 2010.\n[3] Heiga Zen, Andrew Senior, and Mike Schuster, \u201cStatistical parametric speech synthesis using deep neural networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7962\u20137966.\n[4] Zhen-Hua Ling, Li Deng, and Dong Yu, \u201cModeling spectral envelopes using restricted boltzmann machines and deep belief networks for statistical parametric speech synthesis,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 10, pp. 2129\u20132139, 2013.\n[5] Yuchen Fan, Yao Qian, Fenglong Xie, and Frank K Soong, \u201cTTS synthesis with bidirectional LSTM based recurrent neural networks,\u201d in Proc. Interspeech, 2014, pp. 1964\u20131968.\n[6] \u201cBlizzard Challenge 2015,\u201d http://www.synsig. org/index.php/Blizzard_Challenge_ 2015.\n[7] Andrew J Hunt and Alan W Black, \u201cUnit selection in a concatenative speech synthesis system using a large speech database,\u201d in Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on. IEEE, 1996, vol. 1, pp. 373\u2013376.\n[8] Keiichi Tokuda, Takayoshi Yoshimura, Takashi Masuko, Takao Kobayashi, and Tadashi Kitamura, \u201cSpeech parameter generation algorithms for HMMbased speech synthesis,\u201d in Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on. IEEE, 2000, vol. 3, pp. 1315\u20131318.\n[9] Tomoki Toda and Keiichi Tokuda, \u201cA speech parameter generation algorithm considering global variance for HMM-based speech synthesis,\u201d IEICE TRANSACTIONS on Information and Systems, vol. 90, no. 5, pp. 816\u2013824, 2007.\n[10] Shinnosuke Takamichi, Tomoki Toda, Graham Neubig, Sakriani Sakti, and Shigenari Nakamura, \u201cA postfilter to modify the modulation spectrum in HMM-based speech synthesis,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 290\u2013294.\n[11] Ling-Hui Chen, Tuomo Raitio, Cassia ValentiniBotinhao, Junichi Yamagishi, and Zhen-Hua Ling, \u201cDNN-based stochastic postfilter for HMM-based speech synthesis,\u201d Proc. Interspeech, Singapore, Singapore, 2014.\n[12] Ling-Hui Chen, Tuomo Raitio, Cassia ValentiniBotinhao, Zhen-Hua Ling, and Junichi Yamagishi, \u201cA deep generative architecture for postfiltering in statistical parametric speech synthesis,\u201d Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 11, pp. 2003\u20132014, 2015.\n[13] Jeffrey L Elman, \u201cFinding structure in time,\u201d Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.\n[14] Ilya Sutskever, Training recurrent neural networks, Ph.D. thesis, University of Toronto, 2013.\n[15] Alan W Black, \u201cCLUSTERGEN: a statistical parametric synthesizer using trajectory modeling.,\u201d in INTERSPEECH, 2006.\n[16] Robert F Kubichek, \u201cMel-cepstral distance measure for objective speech quality assessment,\u201d in Communications, Computers and Signal Processing, 1993., IEEE Pacific Rim Conference on. IEEE, 1993, vol. 1, pp. 125\u2013 128.\n[17] John Kominek and Alan W Black, \u201cThe CMU Arctic speech databases,\u201d in Fifth ISCA Workshop on Speech Synthesis, 2004.\n[18] Ronan Collobert, Koray Kavukcuoglu, and Cle\u0301ment Farabet, \u201cTorch7: A matlab-like environment for machine learning,\u201d in BigLearn, NIPS Workshop, 2011, number EPFL-CONF-192376.\n[19] John Duchi, Elad Hazan, and Yoram Singer, \u201cAdaptive subgradient methods for online learning and stochastic optimization,\u201d The Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.\n[20] Paul J Werbos, \u201cBackpropagation through time: what it does and how to do it,\u201d Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.\n[21] John Kominek, Tanja Schultz, and Alan W Black, \u201cSynthesizer voice quality of new languages calibrated with mean mel cepstral distortion,\u201d in Spoken Languages Technologies for Under-Resourced Languages, 2008.\n[22] Miguel A Carreira-Perpina\u0301n and Weiran Wang, \u201cDistributed optimization of deeply nested systems,\u201d arXiv preprint arXiv:1212.5921, 2012."}], "references": [{"title": "Statistical parametric speech synthesis", "author": ["Heiga Zen", "Keiichi Tokuda", "Alan W Black"], "venue": "Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Measuring the gap between HMM-based ASR and TTS", "author": ["John Dines", "Junichi Yamagishi", "Simon King"], "venue": "Selected Topics in Signal Processing, IEEE Journal of, vol. 4, no. 6, pp. 1046\u20131058, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["Heiga Zen", "Andrew Senior", "Mike Schuster"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7962\u20137966.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling spectral envelopes using restricted boltzmann machines and deep belief networks for statistical parametric speech synthesis", "author": ["Zhen-Hua Ling", "Li Deng", "Dong Yu"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 10, pp. 2129\u20132139, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Yuchen Fan", "Yao Qian", "Fenglong Xie", "Frank K Soong"], "venue": "Proc. Interspeech, 2014, pp. 1964\u20131968.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Unit selection in a concatenative speech synthesis system using a large speech database", "author": ["Andrew J Hunt", "Alan W Black"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on. IEEE, 1996, vol. 1, pp. 373\u2013376.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Speech parameter generation algorithms for HMMbased speech synthesis", "author": ["Keiichi Tokuda", "Takayoshi Yoshimura", "Takashi Masuko", "Takao Kobayashi", "Tadashi Kitamura"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on. IEEE, 2000, vol. 3, pp. 1315\u20131318.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "A speech parameter generation algorithm considering global variance for HMM-based speech synthesis", "author": ["Tomoki Toda", "Keiichi Tokuda"], "venue": "IEICE TRANSAC- TIONS on Information and Systems, vol. 90, no. 5, pp. 816\u2013824, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A postfilter to modify the modulation spectrum in HMM-based speech synthesis", "author": ["Shinnosuke Takamichi", "Tomoki Toda", "Graham Neubig", "Sakriani Sakti", "Shigenari Nakamura"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 290\u2013294.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "DNN-based stochastic postfilter for HMM-based speech synthesis", "author": ["Ling-Hui Chen", "Tuomo Raitio", "Cassia Valentini- Botinhao", "Junichi Yamagishi", "Zhen-Hua Ling"], "venue": "Proc. Interspeech, Singapore, Singapore, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep generative architecture for postfiltering in statistical parametric speech synthesis", "author": ["Ling-Hui Chen", "Tuomo Raitio", "Cassia Valentini- Botinhao", "Zhen-Hua Ling", "Junichi Yamagishi"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 11, pp. 2003\u20132014, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Training recurrent neural networks, Ph.D", "author": ["Ilya Sutskever"], "venue": "thesis, University of Toronto,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "CLUSTERGEN: a statistical parametric synthesizer using trajectory modeling", "author": ["Alan W Black"], "venue": "INTER- SPEECH, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Mel-cepstral distance measure for objective speech quality assessment", "author": ["Robert F Kubichek"], "venue": "Communications, Computers and Signal Processing, 1993., IEEE Pacific Rim Conference on. IEEE, 1993, vol. 1, pp. 125\u2013 128.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "The CMU Arctic speech databases", "author": ["John Kominek", "Alan W Black"], "venue": "Fifth ISCA Workshop on Speech Synthesis, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "BigLearn, NIPS Workshop, 2011, number EPFL-CONF-192376.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1990}, {"title": "Synthesizer voice quality of new languages calibrated with mean mel cepstral distortion", "author": ["John Kominek", "Tanja Schultz", "Alan W Black"], "venue": "Spoken Languages Technologies for Under-Resourced Languages, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed optimization of deeply nested systems", "author": ["Miguel A Carreira-Perpin\u00e1n", "Weiran Wang"], "venue": "arXiv preprint arXiv:1212.5921, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 233, "endOffset": 242}, {"referenceID": 3, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 233, "endOffset": 242}, {"referenceID": 4, "context": "Statistical Parametric Speech Synthesis[1] has a tradition of borrowing ideas from the speech recognition community[2], and so there has been a flurry of papers in the last two years on using deep neural networks for speech synthesis[3, 4, 5].", "startOffset": 233, "endOffset": 242}, {"referenceID": 5, "context": "The end result of this is that Statistical Parametric Synthesis ends up having to lose the advantage it has over traditional unit-selection systems[7] in terms of the amount of data needed to build a reasonable system.", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "Techniques such as Maximum Likelihood Parameter Generation (MLPG)[8] and Global variance[9] have become standard, and even the newer ideas like the use of Modulation Spectrum[10] have started moving into the mainstream.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Techniques such as Maximum Likelihood Parameter Generation (MLPG)[8] and Global variance[9] have become standard, and even the newer ideas like the use of Modulation Spectrum[10] have started moving into the mainstream.", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Techniques such as Maximum Likelihood Parameter Generation (MLPG)[8] and Global variance[9] have become standard, and even the newer ideas like the use of Modulation Spectrum[10] have started moving into the mainstream.", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "There has been prior work in using DNN based postfilters for parametric synthesis in [11] and [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "There has been prior work in using DNN based postfilters for parametric synthesis in [11] and [12].", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "RNNs have been used before for synthesis in [5], but as a replacement for the existing acoustic model and not as a ar X iv :1 60 1.", "startOffset": 44, "endOffset": 47}, {"referenceID": 11, "context": "A more theoretically sound approach to handle the interdependencies between consecutive frames is to use a Recurrent Neural Network[13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "5 of [14].", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "The particular synthesizer we use is the Clustergen system[15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "Mel Cepstral Distortion[16] is used as the objective metric.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "RMS and SLT are voices from the CMU Arctic speech databases[17], about an hour of speech each.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "The RNN was implemented using the Torch7 toolkit[18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "To train the RNN, we used the ADAGRAD[19] algorithm along with a two-step BackPropagation Through Time[20].", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "To train the RNN, we used the ADAGRAD[19] algorithm along with a two-step BackPropagation Through Time[20].", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "[21] reports that an Table 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "One way of achieving this goal is to use a fairly new technique called Method of Auxiliary Coordinates (MAC)[22].", "startOffset": 108, "endOffset": 112}], "year": 2016, "abstractText": "In the last two years, there have been numerous papers that have looked into using Deep Neural Networks to replace the acoustic model in traditional statistical parametric speech synthesis. However, far less attention has been paid to approaches like DNN-based postfiltering where DNNs work in conjunction with traditional acoustic models. In this paper, we investigate the use of Recurrent Neural Networks as a potential postfilter for synthesis. We explore the possibility of replacing existing postfilters, as well as highlight the ease with which arbitrary new features can be added as input to the postfilter. We also tried a novel approach of jointly training the Classification And Regression Tree and the postfilter, rather than the traditional approach of training them independently.", "creator": "LaTeX with hyperref package"}}}