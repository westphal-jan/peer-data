{"id": "1602.01024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2016", "title": "On Deep Multi-View Representation Learning: Objectives and Optimization", "abstract": "We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for learning while only one view is available for downstream tasks. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them empirically on image, speech, and text tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). We also explore a stochastic optimization procedure for minibatch correlation-based objectives and discuss the time/performance trade-offs for kernel-based and neural network-based implementations.", "histories": [["v1", "Tue, 2 Feb 2016 17:51:43 GMT  (1335kb)", "http://arxiv.org/abs/1602.01024v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weiran wang", "raman arora", "karen livescu", "jeff bilmes"], "accepted": false, "id": "1602.01024"}, "pdf": {"name": "1602.01024.pdf", "metadata": {"source": "CRF", "title": "On Deep Multi-View Representation Learning: Objectives and Optimization", "authors": ["Weiran Wang", "Karen Livescu"], "emails": ["WEIRANWANG@TTIC.EDU", "ARORA@CS.JHU.EDU", "KLIVESCU@TTIC.EDU", "BILMES@EE.WASHINGTON.EDU"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n01 02"}, {"heading": "1. Introduction", "text": "In many applications, we have access to multiple \u201cviews\u201d of data at training time while only one view is available at test time, or for a downstream task. The views can be multiple measurement modalities, such as simultaneously recorded audio + video (Kidron et al., 2005; Chaudhuri et al., 2009), audio + articulation (Arora and Livescu, 2013; Wang et al., 2015a), images + text (Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al., 2014) or document text + text of\ninbound hyperlinks (Bickel and Scheffer, 2004). The presence of multiple information sources presents an opportunity to learn better representations (features) by analyzing the views simultaneously. Typical approaches are based on learning a feature transformation of the \u201cprimary\u201d view (the one available at test time) that captures useful information from the second view using a paired two-view training set. Under certain assumptions, theoretical results exist showing the advantages of multi-view techniques for downstream tasks (Kakade and Foster, 2007; Foster et al., 2009; Chaudhuri et al., 2009). Experimentally, prior work has shown the benefit of multi-view methods on tasks such as retrieval (Vinokourov et al., 2003; Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al., 2009), and classification/recognition (Dhillon et al., 2011; Arora and Livescu, 2013; Ngiam et al., 2011).\nRecent work has introduced several approaches for multi-view representation learning based on deep neural networks (DNNs), using two main training criteria (objectives). One type of objective is based on deep autoencoders (Hinton and Salakhutdinov, 2006), where the objective is to learn a compact representation that best reconstructs the inputs. In the multi-view learning scenario, it is natural to use an encoder to extract the shared representation from the primary view, and use different decoders to reconstruct each view\u2019s input features from the shared representation. This approach has been shown to be effective for speech and vision tasks (Ngiam et al., 2011).\nThe second main DNN-based multi-view approach is based on deep extensions of canonical correlation analysis (CCA, Hotelling, 1936), which learns features in two views that are maximally correlated. The CCA objective has been studied extensively and has a number of useful properties and interpretations (Borga, 2001; Bach and Jordan, 2002, 2005; Chechik et al., 2005), and the optimal linear projection mappings can be obtained by solving an eigenvalue system of a matrix whose dimensions equal the input dimensionalities. To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004). CCA and KCCA have long been the workhorse for multi-view feature learning and dimensionality reduction (Vinokourov et al., 2003; Kakade and Foster, 2007; Socher and Li, 2010; Dhillon et al., 2011). Several alternative nonlinear CCA-like approaches based on neural networks have also been proposed (Lai and Fyfe, 1999; Hsieh, 2000), but the full DNN extension of CCA, termed deep CCA (DCCA, Andrew et al., 2013) has been developed only recently. Compared to kernel methods, DNNs are more scalable to large amounts of training data and also have the potential advantage that the non-linear mapping can be learned, unlike with a fixed kernel approach.\nThe contributions of this paper are as follows.We present a head-to-head comparison of several DNN-based approaches, along with linear and kernel CCA, in the unsupervised multi-view feature learning setting where the second view is not available at test time. We compare approaches based on prior work, as well as developing and comparing new variants. Empirically, we find that CCAbased approaches tend to outperform unconstrained reconstruction-based approaches. One of the new methods we propose, a DNN-based model combining CCA and autoencoder-based terms, is the consistent winner across several tasks. Finally, we study a stochastic optimization approach for deep CCA, both empirically and theoretically. To facilitate future work, we have released our implementations and a new benchmark dataset of simulated two-view data based on MNIST.1\nAn early version of this work appeared in (Wang et al., 2015b). This paper expands on that work with expanded discussion of related work (Sections 3.2 and 3.3), additional experiments, and theo-\n1. Data and implementations can be found at http://ttic.uchicago.edu/\u02dcwwang5/dccae.html.\nretical analysis. In particular we include additional experiments on word similarity tasks with multilingual word embedding learning (Section 4.3); extensive empirical comparisons between batch and stochastic optimization methods for DCCA; and comparisons between DCCA and two popular low-rank approximate KCCA methods, demonstrating their computational trade-offs (Section 4.4). We also analyze the stochastic optimization approach for DCCA theoretically in Appendix A."}, {"heading": "2. DNN-based multi-view feature learning", "text": "In this section, we discuss several existing and new multi-view learning approaches based on deep feed-forward neural networks, including their objective functions and optimization procedures. Schematic diagrams summarizing the methods are given in Fig. 1.\nNotation In the multi-view feature learning scenario, we have access to paired observations from two views, denoted (x1,y1), . . . , (xN ,yN ), where N is the sample size and xi \u2208 RDx and yi \u2208 R Dy for i = 1, . . . , N . We also denote the data matrices for each view X = [x1, . . . ,xN ] and Y = [y1, . . . ,yN ]. We use bold-face letters, e.g. f , to denote multidimensional mappings implemented by DNNs. A DNN f of depth Kf implements a nested mapping of the form f(x) = fKf ((\u00b7 \u00b7 \u00b7 f1(x;W1) \u00b7 \u00b7 \u00b7 );WKf ), where Wj are the weight parameters\n2 of layer j, j = 1, . . . ,Kf , and fj is the mapping of layer j which takes the form of a linear mapping followed by a elementwise activation: fj(t) = s(W\u22a4j t), and typical choices for s include sigmoid, tanh, ReLU, etc. The collection of the learnable parameters in model f is denoted Wf , e.g. Wf = {W1, . . . ,WKf } in the DNN case. We write the f -projected (view 1) data matrix as f(X) = [f(x1), . . . , f(xN )]. The dimensionality of the projection (feature vector), which is equal to the number of output units in the DNN case, is denoted L."}, {"heading": "2.1 Split autoencoders (SplitAE)", "text": "Ngiam et al. (2011) propose to extract shared representations by learning to reconstruct both views from the one view that is available at test time. In this approach, the feature extraction network f is shared while the reconstruction networks p and q are separate for each view. We refer to this model as a split autoencoder (SplitAE), shown schematically in Fig. 1 (a). The objective of this model is\n2. Biases can be introduced by appending an extra 1 to the input.\nthe sum of reconstruction errors for the two views (we omit the \u21132 weight decay term for all models in this section):\nmin Wf ,Wp,Wq\n1\nN\nN \u2211\ni=1\n\u2016xi \u2212 p(f(xi))\u2016 2 + \u2016yi \u2212 q(f(xi))\u2016 2 .\nThe intuition for this model is that the shared representation can be extracted from a single view, and can be used to reconstruct all views.3 The autoencoder loss is the empirical expectation of the loss incurred at each training sample, and thus stochastic gradient descent (SGD) can be used to optimize the objective efficiently with the gradients estimated on small minibatches of samples."}, {"heading": "2.2 Deep canonical correlation analysis (DCCA)", "text": "Andrew et al. (2013) propose a DNN extension of CCA termed deep CCA (DCCA; see Fig. 1 (b)). In DCCA, two DNNs f and g are used to extract nonlinear features for each view and the canonical correlation between the extracted features f(X) and g(Y) is maximized:\nmax Wf ,Wg,U,V\n1 N tr ( U\u22a4f(X)g(Y)\u22a4V )\n(1)\ns.t. U\u22a4 ( 1\nN f(X)f(X)\u22a4 + rxI\n)\nU = I,\nV\u22a4 ( 1\nN g(Y)g(Y)\u22a4 + ryI\n)\nV = I,\nu\u22a4i f(X)g(Y) \u22a4vj = 0, for i 6= j,\nwhere U = [u1, . . . ,uL] and V = [v1, . . . ,vL] are the CCA directions that project the DNN outputs and (rx, ry) > 0 are regularization parameters added to the diagonal of the sample autocovariance matrices (Bie and Moor, 2003; Hardoon et al., 2004). In DCCA, U\u22a4f(\u00b7) is the final projection mapping used for downstream tasks.4 One intuition for CCA-based objectives is that, while it may be difficult to accurately reconstruct one view from the other view, it may be easier, and perhaps sufficient, to learn a predictor of a function (or subspace) of the second view. In addition, it should be helpful for the learned dimensions within each view to be uncorrelated so that they provide complementary information.\nOptimization The DCCA objective couples all training samples through the whitening constraints (i.e., it is a fully batch objective), so stochastic gradient descent (SGD) cannot be applied in a standard way. However, it is observed that DCCA can still be optimized effectively as long as the gradient is estimated using a sufficiently large minibatch (Wang et al., 2015a, with the gradient formulas given as in Andrew et al., 2013). Intuitively, this approach works because a large minibatch\n3. The authors also propose a bimodal deep autoencoder combining DNN transformed features from both views; this model is more natural for the multimodal fusion setting, where both views are available at test time, but can also be used in our multi-view setting (see Ngiam et al., 2011). Empirically, however, Ngiam et al. (2011) report that SplitAE tends to work better in the multi-view setting than bimodal autoencoders. 4. In principle there is no need for the final linear projection; we could define DCCA such that the correlation objective and constraints are imposed on the final nonlinear layer of the two DNNs. The final linear projection is equivalent to constraining the final DNN layer to be linear. While this is in principle not needed, it is crucial for practical algorithmic implementations such as ours, and matches the original formulation of DCCA (Andrew et al., 2013).\nof samples contains sufficient information for estimating the covariance matrices. We provide empirical analysis about the optimization of DCCA (in comparison with KCCA) in Section 4.4 and its theoretical analysis is given in Appendix A."}, {"heading": "2.3 Deep canonically correlated autoencoders (DCCAE)", "text": "Inspired by both CCA and reconstruction-based objectives, we propose a new model that consists of two autoencoders and optimizes the combination of canonical correlation between the learned \u201cbottleneck\u201d representations and the reconstruction errors of the autoencoders. In other words, we optimize the following objective\nmin Wf ,Wg,Wp,Wq,U,V\n\u2212 1 N tr ( U\u22a4f(X)g(Y)\u22a4V )\n+ \u03bb\nN\nN \u2211\ni=1\n\u2016xi \u2212 p(f(xi))\u2016 2 + \u2016yi \u2212 q(g(yi))\u2016 2 (2)\ns.t. the same constraints as in (1),\nwhere \u03bb > 0 is a trade-off parameter. Alternatively, this approach can be seen as adding an autoencoder regularization term to DCCA. We call this approach deep canonically correlated autoencoders (DCCAE). Fig. 1 (c) shows a schematic representation of the approach.\nOptimization We apply stochastic optimization to the DCCAE objective. Notice obtaining good stochastic estimates of the gradient for the correlation and autoencoder terms may involve different minibatch sizes. We explore the minibatch sizes for each term separately (by training DCCA and an autoencoder) and select them using a validation set. The stochastic gradient is then the sum of the gradient for the DCCA term (usually estimated using large minibatches) and the gradient for the autoencoder term (usually estimated using small minibatches).\nInterpretations CCA maximizes the mutual information between the projected views for certain distributions (Borga, 2001), while training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between inputs and learned features (Vincent et al., 2010). The DCCAE objective offers a trade-off between the information captured in the (input, feature) mapping within each view on the one hand, and the information in the (feature, feature) relationship across views."}, {"heading": "2.4 Correlated autoencoders (CorrAE)", "text": "In the next approach, we remove the uncorrelatedness constraints from the DCCAE objective, leaving only the sum of scalar correlations between pairs of learned dimensions and the reconstruction error term. This approach is intended to test how important the original CCA constraints are. We call this model correlated autoencoders (CorrAE), also represented by Fig. 1 (c). Its objective can\nbe written as\nmin Wf ,Wg,Wp,Wq,U,V\n\u2212 1 N tr ( U\u22a4f(X)g(Y)\u22a4V )\n+ \u03bb\nN\nN \u2211\ni=1\n\u2016xi \u2212 p(f(xi))\u2016 2 + \u2016yi \u2212 q(g(yi))\u2016 2 (3)\ns.t. u\u22a4i f(X)f(X) \u22a4ui = v \u22a4 i g(Y)g(Y) \u22a4vi = N, 1 \u2264 i \u2264 L.\nwhere \u03bb > 0 is a trade-off parameter. It is clear that the constraint set in (3) is a relaxed version of that of (2). Later we demonstrate that this difference results in a large performance gap. We apply the same optimization strategy of DCCAE to CorrAE.\nCorrAE is similar to the model of Chandar et al. (2014, 2015), who try to learn vectorial word representations using parallel corpora from two languages. They use a DNN in each view (language) to predict a bag-of-words representation of the input sentences, or that of the paired sentences from the other view, while encouraging the learned bottleneck layer representations to be highly correlated."}, {"heading": "2.5 Minimum-distance autoencoders (DistAE)", "text": "The CCA objective can be seen as minimizing the distance between the learned projections of the two views, while satisfying the whitening constraints for the projections (Hardoon et al., 2004). The constraints complicate the optimization of CCA-based objectives, as pointed out above. This observation motivates us to consider additional objectives that decompose into sums over training examples, while maintaining the intuition of the CCA objective as a reconstruction error between two mappings. Here we consider two variants that we refer to as minimum-distance autoencoders (DistAE).\nThe first variant DistAE-1 optimizes the following objective:\nmin Wf ,Wg,Wp,Wq\n1\nN\nN \u2211\ni=1\n\u2016f(xi)\u2212 g(yi)\u2016 2\n\u2016f(xi)\u2016 2 + \u2016g(yi)\u2016 2\n+ \u03bb\nN\nN \u2211\ni=1\n\u2016xi \u2212 p(f(xi))\u2016 2 + \u2016yi \u2212 q(g(yi))\u2016 2 (4)\nwhich is a weighted combination of reconstruction errors of two autoencoders and the average discrepancy between the projected sample pairs. The denominator of the discrepancy term is used to keep the optimization from improving the objective by simply scaling down the projections (although they can never become identically zero due to the reconstruction terms). This objective is unconstrained and is the empirical average of the loss incurred at each training sample, so normal SGD applies using small (or any size) minibatches.\nThe second variant DistAE-2 optimizes a somewhat different objective:\nmin Wf ,Wg,Wp,Wq,A,b\n1\nN\nN \u2211\ni=1\n\u2016f(xi)\u2212Ag(yi)\u2212 b\u2016 2\n+ \u03bb\nN\nN \u2211\ni=1\n\u2016xi \u2212 p(f(xi))\u2016 2 + \u2016yi \u2212 q(g(yi))\u2016 2 (5)\nwhere A \u2208 RL\u00d7L and b \u2208 RL. The underlying intuition is that the representation of the primary view can be linearly predicted from the representation of the other view. This relationship is motivated by the fact that when g(y) and f(x) are perfectly linearly correlated, then there exists an affine transformation that can map from one to the other. This approach, hence, alleviates the burden on g(y) being simultaneously predictive of the output and close to f(x) by itself."}, {"heading": "3. Related work", "text": "Here we focus on related work on multi-view feature learning using neural networks and the kernel extension of CCA."}, {"heading": "3.1 Neural network feature extraction using CCA-like objectives", "text": "There have been several approaches to multi-view representation learning using neural networks with an objective similar to that of CCA. Under the assumption that the two views share a common cause (e.g., depth is the common cause for adjacent patches of images), Becker and Hinton (1992) propose to maximize a sample-based estimate of mutual information between outputs of neural networks for the two views. In this work, the 1-dimensional output of each network can be considered to be \u201cinternal teaching signals\u201d for the other. It is less straightforward to extend their sample-based estimator of mutual information to higher dimensions, while the CCA objective is always closely related to maximal mutual information between the views (under the joint multivariate Gaussian distributions of the inputs, see Borga, 2001).\nLai and Fyfe (1999) propose to optimize the correlation (rather than canonical correlation) between the outputs of networks for each view, subject to scale constraints on each output dimension. Instead of directly solving this constrained formulation, the authors apply Lagrangian relaxation and solve the resulting unconstrained objective using SGD. Note, however, that their objective is different from that of CCA, as there are no constraints that the learned dimensions within each view be uncorrelated. Hsieh (2000) proposes a neural network-based model involving three modules: one module for extracting a pair of maximally correlated one-dimensional features for the two views; and a second and third module for reconstructing the original inputs of the two views from the learned features. In this model, the feature dimensions can be learned one after another, each learned using as input the reconstruction residual from previous dimensions. This approach is intuitively similar to CorrAE and DCCA, but the three modules are each trained separately, so there is no unified objective.\nKim et al. (2012) propose an algorithm that first uses deep belief networks and the autoencoder objective to extract features for two languages independently, and then applies linear CCA to the learned features (activations at the bottleneck layer of the autoencoders) to learn the final representation. In this two-step approach, the DNN weight parameters are not updated to optimize the CCA objective.\nThere has also been work on multi-view feature learning using deep Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014). The models in this work stack several layers of restricted Boltzmann machines (RBM) to represent each view, with an additional top layer that provides the joint representation. These are probabilistic graphical models, for which the maximum likelihood objective is intractable and the training procedures are more complex. Although probabilistic models have some advantages (e.g., dealing with missing values and generating sam-\nples in a natural way), DNN-based models have the advantages of a tractable objective and efficient training."}, {"heading": "3.2 Kernel CCA", "text": "Formulation (1) encompasses several variants. Obviously, (1) reduces to linear CCA when f and g are identity mappings. One popular nonlinear approach is kernel CCA (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004), corresponding to choosing f(x) and g(y) to be feature maps induced by positive definite kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7), respectively (e.g., Gaussian RBF kernel k(a,b) = e\u2212\u2016a\u2212b\u2016 2/2s2 where s is the kernel width). Following the derivation of Bach and Jordan (2002, Section 3.2.1), it suffices to consider projection mappings of the form f\u0303(x) =\n\u2211N i=1\u03b1ikx(x,xi) and g\u0303(y) = \u2211N i=1 \u03b2iky(y,yi), where \u03b1i,\u03b2i \u2208\nR L, i = 1, . . . , N .\nDenote by Kx the N \u00d7 N kernel matrix for view 1, i.e., (Kx)ij = kx(xi,xj), and similarly denote by Ky the kernel matrix for view 2. Then (1) can be written as a problem in the coefficient matrices A = [\u03b11, . . . ,\u03b1L]\u22a4 \u2208 RN\u00d7L and B = [\u03b21, . . . ,\u03b2L] \u22a4 \u2208 RN\u00d7L:\nmax A,B\n1 N tr ( A\u22a4KxKyB )\n(6)\ns.t. A\u22a4 ( 1\nN Kx\n2 + rxKx\n) A = B\u22a4 ( 1\nN Ky\n2 + ryKy\n)\nB = I,\n\u03b1\u22a4i KxKy\u03b2j = 0, for i 6= j.\nTherefore we can conveniently work with the kernel (Gram) matrices instead of possibly infinite dimensional RKHS space and optimize directly over the coefficients. Following a derivation similar to that of CCA, one can show that the optimal solution (A\u2217,B\u2217) satisfies\n(Kx +NrxI) \u22121Ky(Ky +NryI) \u22121KxA \u2217 = A\u2217\u03a32, (7) (Ky +NryI) \u22121Kx(Kx +NrxI) \u22121KyB \u2217 = B\u2217\u03a32, (8)\nwhere \u03a3 is a diagonal matrix containing the leading correlation coefficients. Thus the optimal projection can be obtained by solving an eigenvalue problem of size N \u00d7N .\nWe can make a few observations on the KCCA method. First, the non-zero regularization parameters (rx, ry) > 0 are needed to avoid trivial solutions and correlations. Second, in KCCA, the mappings are not optimized over except that the kernel parameters are usually cross-validated. Third, exact KCCA is computationally challenging for large data sets as it would require performing an eigendecomposition of an N \u00d7N matrix which is expensive both in memory (storing the kernel matrices) and time (solving the N \u00d7N eigenvalue systems naively costs O(N3)).\nVarious kernel approximation techniques have been proposed to scale up kernel machines. Two widely used approximation techniques are random Fourier features (Lopez-Paz et al., 2014) and the Nystro\u0308m approximation (Williams and Seeger, 2001). In random Fourier features, we randomly sample M Dx (respectively Dy)-dimensional vectors from a Gaussian distribution and map the original inputs to RM by computing the dot products with the random samples followed by an elementwise cosine. That is, f(x) = [cos(w1x + b1), . . . , cos(wMx + bM )] where wi is sampled from N (0, I/s2) (s is the width of the Gaussian kernel we wish to approximate), and bi is sampled uniformly from [0, 2\u03c0]. Inner products between transformed samples then approximate\nkernel similarities between original inputs. In the Nystro\u0308m approximation, we randomly select M training samples x\u03031, . . . , x\u0303M and construct the M \u00d7M kernel matrix K\u0303x based on these samples, i.e. (K\u0303x)ij = kx(x\u0303i, x\u0303j). We compute the eigenvalue decomposition K\u0303x = R\u0303\u039b\u0303R\u0303\u22a4, and then the N \u00d7N kernel matrix for the entire training set can be approximated as Kx \u2248 CK\u0303\u22121x C\n\u22a4 where C contains the columns of Kx corresponding to the selected subset, i.e., Cij = kx(xi, x\u0303j). This means Kx \u2248 ( CR\u0303\u039b\u0303 \u22121/2 )( CR\u0303\u039b\u0303 \u22121/2 )\u22a4 , so we can use the M \u00d7 N matrix F = ( CR\u0303\u039b\u0303 \u22121/2 )\u22a4 as a new feature representation for view 1, where inner products between samples approximate kernel similarities.\nBoth techniques produce rank-M approximations of the kernel matrices with computational complexity O(M3 +M2N); but random Fourier features are data independent and more efficient to generate while Nystro\u0308m tends to work better. Other approximation techniques such as incomplete Cholesky decomposition (Bach and Jordan, 2002), partial Gram-Schmidt (Hardoon et al., 2004), incremental SVD (Arora and Livescu, 2012) have also been proposed and applied to KCCA. However, for very large training sets, such as the ones in some of our tasks below, it remains difficult and costly to approximate KCCA well. Although recently iterative algorithms have been introduced for very large CCA problems (Lu and Foster, 2014), they are aimed at sparse matrices and do not have a natural out-of-sample extension."}, {"heading": "3.3 Other related models", "text": "CCA is related to metric learning in a broad sense. In metric learning, the task is to learn a metric in the input space (or equivalently a projection mapping) such that the learned distances (or equivalently Euclidean distances in the projected space) between \u201csimilar\u201d samples are small while distances between \u201cdissimilar\u201d samples are large. Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).\nNote that we can equivalently write the CCA objective as (by replacing max with min\u2212 and adding 1/2 times the left-hand side of the whitening constraints)\nmin U,V\n1\nN\n\u2225 \u2225 \u2225 U\u22a4F\u2212V\u22a4G \u2225 \u2225 \u2225 2\nF + rx 2 \u2016U\u20162F + ry 2 \u2016V\u20162F (9)\ns.t. the same constraints in (1).\nIn view of the above formulation, pairs of co-occurring two-view samples are mapped into similar locations in the projection, which are thus considered \u201csimilar\u201d in CCA, and the whitening constraints set the scales of the projections so that the dataset does not collapse into a constant and so that the projection dimensions are uncorrelated. Unlike the typical metric learning setting, the twoview data in CCA may come from different domains/modalities and thus each view has its own projection mapping. Also, CCA uses no information regarding \u201cdissimilar\u201d pairs of two-view data. In this sense the CCA setting is more similar to that of Shental et al. (2002) and Bar-Hillel et al. (2005), which use only side information regarding groups of similar samples (\u201cchunklets\u201d) for single-view data.\nFor multi-view data, Globerson et al. (2007) propose an algorithm for learning Euclidean embeddings by defining a joint or conditional distribution of the views based on Euclidean distance\nin the embedding space and maximizing the data likelihood. The objective they minimize is the weighted mean of squared distances between embeddings of co-occurring pairs with a regularization term depending on the partition function of the defined distribution. This model differs from CCA in the global constraints/regularization used.\nRecently, there has been increasing interest in learning (multi-view) representations using contrastive losses which aim to enforce that distances between dissimilar pairs are larger than distances between similar pairs by some margin (Hermann and Blunsom, 2014; Huang et al., 2013). In case there is no ground-truth information regarding similar/dissimilar pairs (as in our setting), random sampling is typically used to generate negative pairs. The sampling of negative pairs can be harmful in cases where the probability of mistakenly obtaining similar pairs from the sampling procedure is relatively high. In future work it would be interesting to compare contrastive losses with the CCA objective when only similar pairs are given, and to consider the effects of such sampling in a variety of data distributions.\nFinally, CCA has a connection with the information bottleneck method (Tishby et al., 1999). Indeed, in the case of Gaussian variables, the information bottleneck method finds the same subspace as CCA (Chechik et al., 2005)."}, {"heading": "4. Experiments", "text": "We first demonstrate the proposed algorithms and related work on several multi-view feature learning tasks (Sections 4.1\u20134.3). In our setting, the second view is not available during test time, so we try to learn a feature transformation of the first/primary view that captures useful information from the second view using a paired two-view training set. Then, in Section 4.4, we explore the stochastic optimization procedure for the DCCA objective (1).\nWe focus on several downstream tasks including noisy digit image classification, speech recognition, and word pair semantic similarity. On these tasks, we compare the following methods in the multi-view learning setting:\n\u2022 DNN-based models, including SplitAE, CorrAE, DCCA, DCCAE, and DistAE.\n\u2022 Linear CCA (CCA), corresponding to DCCA with only a linear network with no hidden layers for both views.\n\u2022 Kernel CCA approximations. Exact KCCA is computationally infeasible for our tasks since they are large; we instead implement two kernel approximation techniques, using Gaussian RBF kernels. The first implementation, denoted FKCCA, uses random Fourier features (Lopez-Paz et al., 2014) and the second implementation, denoted NKCCA, uses the Nystro\u0308m approximation (Williams and Seeger, 2001). As described in Section 3.2, in both FKCCA/NKCCA, we transform the original inputs to an M -dimensional feature space where the inner products between samples approximate the kernel similarities (Yang et al., 2012). We apply linear CCA to the transformed inputs to obtain the approximate KCCA solution."}, {"heading": "4.1 Noisy MNIST digits", "text": "In this task, we generate two-view data using the MNIST dataset (LeCun et al., 1998), which consists of 28 \u00d7 28 grayscale digit images, with 60K/10K images for training/testing. We generate a more challenging version of the dataset as follows (see Fig. 2 for examples). We first rescale the\npixel values to [0, 1] (by dividing the original values in [0, 255] by 255). We then randomly rotate the images at angles uniformly sampled from [\u2212\u03c0/4, \u03c0/4] and the resulting images are used as view 1 inputs. For each view 1 image, we randomly select an image of the same identity (0-9) from the original dataset, add independent random noise uniformly sampled from [0, 1] to each pixel, and truncate the pixel final values to [0, 1] to obtain the corresponding view 2 sample. The original training set is further split into training/tuning sets of size 50K/10K .\nSince, given the digit identity, observing a view 2 image does not provide any information about the corresponding view 1 image, a good multi-view learning algorithm should be able to extract features that disregard the noise. We measure the class separation in the learned feature spaces via both clustering and classification performance. First, we cluster the projected view 1 inputs into 10 clusters and evaluate how well the clusters agree with ground-truth labels. We use spectral clustering (Ng et al., 2002) so as to account for possibly non-convex cluster shapes. Specifically, we first build a k-nearest-neighbor graph on the projected view 1 tuning/test samples with a binary weighting scheme (edges connecting neighboring samples have a constant weight of 1), and then embed these samples in R10 using eigenvectors of the normalized graph Laplacian, and finally run K-means in the embedding to obtain a hard partition of the samples. In the last step, K-means is run 20 times with random initialization and the run with the best K-means objective is used. The size of the neighborhood graph k is selected from {5, 10, 20, 30, 50} using the tuning set. We measure clustering performance with two criteria, clustering accuracy (AC) and normalized mutual information (NMI) (Xu et al., 2003; Manning et al., 2008). The AC is defined as\nAC =\n\u2211N i=1 \u03b4(si,map(ri))\nN , (10)\nwhere si is the ground truth label of sample xi, ri is the cluster label of xi, and map(ri) is an optimal permutation mapping between cluster labels and ground truth labels obtained by solving\na linear assignment problem using the Hungarian algorithm (Munkres, 1957). The NMI considers the probability distribution over the ground truth label set C and cluster label set C \u2032 jointly, and is defined by the following set of equations\nNMI(C,C \u2032) = MI(C,C \u2032)\nmax(H(C),H(C \u2032)) (11)\nwhere MI(C,C \u2032) = \u2211\nci\u2208C\n\u2211\nc\u2032j\u2208C \u2032\np(ci, c \u2032 j) log2\np(ci, c \u2032 j)\np(ci)p(c \u2032 j)\n(12)\nH(C) = \u2212 \u2211\nci\u2208C\np(ci) log2 p(ci), H(C \u2032) = \u2212\n\u2211\nc\u2032j\u2208C \u2032\np(c\u2032j) log2 p(c \u2032 j), (13)\nwhere p(ci) is interpreted as the probability of a sample having label ci and p(ci, c\u2032j) the probability of a sample having label ci while being assigned to cluster c\u2032j (all of which can be computed by counting the samples in the joint partition of C and C \u2032). Larger values of these criteria (with an upper bound of 1) indicate better agreement between the clustering and ground-truth labeling.\nEach algorithm has hyperparameters that are selected using the tuning set. The final dimensionality L is selected from {5, 10, 20, 30, 50}. For CCA, the regularization parameters rx and ry are selected via grid search. For KCCAs, we fix both rx and ry at a small positive value of 10\u22124 (as suggested by Lopez-Paz et al. (2014), FKCCA is robust to rx, ry), and do grid search for the Gaussian kernel width over {2, 3, 4, 5, 6, 8, 10, 15, 20} for view 1 and {2.5, 5, 7.5, 10, 15, 20, 30} for view 2 at rank M = 5, 000, and then test with M = 20, 000. For DNN-based models, the feature mappings (f ,g) are implemented by networks of 3 hidden layers, each of 1024 sigmoid units, and a linear output layer of L units; reconstruction mappings (p,q) are implemented by networks of 3 hidden layers, each of 1024 sigmoid units, and an output layer of 784 sigmoid units. We fix rx = ry = 10\u22124 for DCCA and DCCAE. For SplitAE/CorrAE/DCCAE/DistAE we select the trade-off parameter \u03bb via grid search over {0.001, 0.01, 0.1, 1, 10}, allowing a trade-off between the correlation term (with value in [0, L]) and the reconstruction term (varying roughly in the range [60, 110] as the reconstruction error for the second view is always large). The networks (f ,p) are pre-trained in a layerwise manner using restricted Boltzmann machines (Hinton and Salakhutdinov, 2006) and similarly for (g,q) with inputs from the corresponding view.\nFor DNN-based models, we use SGD for optimization with minibatch size, learning rate and momentum tuned on the tuning set (more on this in Section 4.4). A small weight decay parameter of 10\u22124 is used for all layers. We monitor the objective on the tuning set for early stopping. For each algorithm, we select the model with the best AC on the tuning set, and report its results on the test set. The AC and NMI results (in percent) for each algorithm are given in Table 1. As a baseline, we also cluster the original 784-dimensional view 1 images.\nNext, we also measure the quality of the projections via classification experiments. If the learned features are clustered well into classes, then one might expect that a simple linear classifier can achieve high accuracy on these projections. We train one-versus-one linear SVMs (Chang and Lin, 2011) on the projected training set (now using the ground truth labels), and test on the projected test set, while using the projected tuning set for selecting the SVM hyperparameter (the penalty parameter for hinge loss). Test error rates on the optimal embedding of each algorithm (with highest AC) are provided in Table 1 (last column). These error rates agree with the clustering results. Multi-view feature learning makes classification much easier on this task: Instead of using a heavily\n(a) Inputs (a\u2019) \u20181\u2019 and \u20183\u2019\n(a) Inputs (f) SplitAE\nnonlinear classifier on the original inputs, a very simple linear classifier that can be trained efficiently on low-dimensional projections already achieves high accuracy.\nAll of the multi-view feature learning algorithms achieve some improvement over the baseline. The nonlinear CCA algorithms all perform similarly, and significantly better than SplitAE, CorrAE, and DistAE. We also qualitatively investigate the features by embedding the projected features in 2D using t-SNE (van der Maaten and Hinton, 2008); the resulting visualizations are given in Figures 3 and 4. Overall, the visual class separation qualitatively agrees with the relative clustering and classification performance in Table 1.\nIn the embedding of input images (Figure 3 (a)), samples of each digit form an approximately one-dimensional, stripe-shaped manifold, and the degree of freedom along each manifold corresponds roughly to the variation in rotation angle (see Figure 3 (a\u2019)). This degree of freedom does not change the identity of the image, which is common to both views. Projections by SplitAE/CorrAE/DistAE do achieve somewhat better separation for some classes, but the unwanted rotation variation is still prominent in the embeddings. On the other hand, without using any label information and with only paired noisy images, the nonlinear CCA algorithms manage to map digits of the same identity to similar locations while suppressing the rotational variation and separating images of different identities. Linear CCA also approximates the same behavior, but fails to separate the classes, presumably because the input variations are too complex to be captured by only linear mappings. Overall, DCCAE gives the cleanest embedding, with different digits pushed far apart and good separation achieved.\nThe different behavior of CCA-based methods from SplitAE/CorrAE/DistAE suggests two things. First, when the inputs are noisy, reconstructing the input faithfully may lead to unwanted degrees of freedom in the features (DCCAE tends to select a relatively small trade-off parameter \u03bb = 10\u22123 or 10\u22122), further supporting that it is not necessary to fully minimize reconstruction error. We show the clustering accuracy of DCCAE at L = 10 for different \u03bb values in Figure 5. Second, the hard CCA constraints, which enforce uncorrelatedness between different feature dimen-\nsions, appear essential to the success of CCA-based methods; these constraints are the difference between DCCAE and CorrAE. However, the constraints without the multi-view objective seem to be insufficient. To see this, we also visualize a 10-dimensional locally linear embedding (LLE, Roweis and Saul, 2000) of the test images in Fig. 3 (b). LLE satisfies the same uncorrelatedness constraints as in CCA-based methods, but without access to the second view, it does not separate the classes as nicely."}, {"heading": "4.2 Acoustic-articulatory data for speech recognition", "text": "We next experiment with the Wisconsin X-Ray Microbeam (XRMB) corpus (Westbury, 1994) of simultaneously recorded speech and articulatory measurements from 47 American English speakers. Multi-view feature learning via CCA/KCCA/DCCA has previously been shown to improve phonetic recognition performance when tested on audio alone (Arora and Livescu, 2013; Wang et al., 2015a).\nWe follow the setup of Arora and Livescu (2013) and use the learned features for speakerindependent phonetic recognition. Similarly to Arora and Livescu (2013), the inputs to multi-view feature learning are acoustic features (39D features consisting of mel frequency cepstral coefficients (MFCCs) and their first and second derivatives) and articulatory features (horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract) concatenated over a 7-frame window around each frame, giving 273D acoustic inputs and 112D articulatory inputs for each view.\nWe split the XRMB speakers into disjoint sets of 35/8/2/2 speakers for feature learning/recognizer training/tuning/testing. The 35 speakers for feature learning are fixed; the remaining 12 are used in a 6-fold experiment (recognizer training on 8 speakers, tuning on 2 speakers, and testing on the remaining 2 speakers). Each speaker has roughly 50K frames, giving 1.43M multi-view training frames; this is a much larger training set than those used in previous work on this data set. We remove the per-speaker mean and variance of the articulatory measurements for each training speaker. All of the learned feature types are used in a tandem approach (Hermansky et al., 2000), i.e., they are appended to the original 39D features and used in a standard hidden Markov model (HMM)-based recognizer with Gaussian mixture model observation distributions. The baseline is the recognition performance using the original MFCC features. The recognizer has one 3-state left-to-right HMM per phone, using the same language model as in Arora and Livescu (2013).\nFor each fold, we select the best hyperparameters based on recognition accuracy on the tuning speakers, and use the corresponding learned model for the test speakers. As before, models based on neural networks are trained via SGD with the optimization parameters tuned by grid search. Here we do not use pre-training for weight initialization. A small weight decay parameter of 5\u00d7 10\u22124 is used for all layers. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}. For DNN-based models, we use hidden layers of 1500 ReLUs. For DCCA, we vary the network depths (up to 3 nonlinear hidden layers) of each view. In the best DCCA architecture, f has 3 ReLU layers of 1500 units followed by a linear output layer while g has only a linear output layer.\nFor CorrAE/DistAE/DCCAE, we use the same architecture of DCCA for the encoders, and we set the decoders to have symmetric architectures to the encoders. For this domain, we find that the best choice of architecture for the encoders/decoders for View 2 is linear while for View 1 it is typically three layers deep. For SplitAE, the encoder f is similarly deep and the View 1 decoder p has the symmetric architecture, while its View 2 decoder q was set to linear to match the best choice for the other methods. We fix (rx, ry) to small values as before. The trade-off parameter \u03bb is tuned for each algorithm by grid search.\nFor FKCCA, we find it important to use a large number of random features M to get a competitive result, consistent with the findings of Huang et al. (2014) when using random Fourier features for speech data. We tune kernel widths at M = 5,000 with FKCCA, and test FKCCA with M = 30,000 (the largest M we could afford to obtain an exact SVD solution on a workstation with 32G main memory); We are not able to obtain results for NKCCA with M = 30,000 in 48 hours with our implementation, so we report its test performance at M = 20,000 with the optimal FKCCA hyperparameters. Notice that FKCCA has about 14.6 million parameters (random Gaussian samples + projection matrices from random Fourier features to the L-dimensional KCCA features, which is more than the number of weight parameters in the largest DCCA model, so it is slower than DCCA for testing (the cost of computing test features is linear in the number of parameters for both KCCA and DNNs).\nPhone error rates (PERs) obtained by different feature learning algorithms are given in Table 2. We see the same pattern as on MNIST: Nonlinear CCA-based algorithms outperform SplitAE/CorrAE/DistAE. Since the recognizer now is a nonlinear mapping (HMM), the performance of the linear CCA features is highly competitive. Again, DCCAE tends to select a relatively small \u03bb, indicating that the canonical correlation term is more important."}, {"heading": "4.3 Multilingual data for word embeddings", "text": "In this task, we learn a vectorial representation of English words from pairs of English-German word embeddings for improved semantic similarity. We follow the setup of Faruqui and Dyer (2014) and use as inputs 640-dimensional monolingual word vectors trained via latent semantic analysis on the WMT 2011 monolingual news corpora and use the same 36K English-German word pairs for multi-view learning. The learned mappings are applied to the original English word embeddings (180K words) and the projections are used for evaluation.\nWe evaluate learned features on two groups of tasks. The first group consists of the four word similarity tasks from Faruqui and Dyer (2014): WS-353 and the two splits WS-SIM and WS-REL, RG-65, MC-30, and MTurk-287. The second group of tasks uses the adjective-noun (AN) and verbobject (VN) subsets from the bigram similarity dataset of Mitchell and Lapata (2010), and tuning and test splits (of size 649/1972) for each subset (we exclude the noun-noun subset as we find that the NN human annotations often reflect \u201ctopical\u201d rather than \u201cfunctional\u201d similarity). We simply add the projections of the two words in each bigram to obtain an L-dimensional representation\nof the bigram, as done in prior work (Blacoe and Lapata, 2012). We compute the cosine similarity between the two vectors of each bigram pair, order the pairs by similarity, and report the Spearman\u2019s correlation (\u03c1) between the model\u2019s ranking and human rankings.\nWe tune the feature dimensionality L over {128, 384}; other hyperparameters are tuned as in previous experiments. DNN-based models use ReLU hidden layers of width 1,280. A small weight decay parameter of 10\u22124 is used for all layers. We use two ReLU hidden layers for encoders (f and g), and try both linear and nonlinear networks with two hidden layers for decoders (p and q). FKCCA/NKCCA are tested with M = 20,000 using kernel widths tuned at M = 4,000. We fix rx = ry = 10 \u22124 for nonlinear CCAs and tune them over {10\u22126, 10\u22124, 10\u22122, 1, 102} for CCA.\nFor word similarity tasks, we simply report the highest Spearman\u2019s correlation obtained by each algorithm. For bigram similarity tasks, we select for each algorithm the model with the highest Spearman\u2019s correlation on the 649 tuning bigram pairs, and we report its performance on the 1972 test pairs. The results are given in Table 3. Unlike MNIST and XRMB, it is important for the features to reconstruct the input monolingual word embeddings well, as can be seen from the superior performance of SplitAE over FKCCA/NKCCA/DCCA. This implies there is useful information in the original inputs that is not correlated across views. However, DCCAE still performs the best on the AN task, in this case using a relatively large \u03bb = 0.1."}, {"heading": "4.4 Empirical analysis of DCCA optimization", "text": "We now explore the issue of stochastic optimization for DCCA as discussed in Section 2.2. We use the same XRMB dataset as in the acoustic-articulatory experiment of Section 4.2. In this experiment, we select utterances from a single speaker \u2018JW11\u2019 and divide them into training/tuning/test splits of roughly 30K/11K/9K pairs of acoustic and articulatory frames.5 Since the sole purpose of this experiment is to study the optimization of nonlinear CCA algorithms rather than the usefulness of the features, we do not carry out any down-stream tasks or careful model selection for that purpose (e.g., a search for feature dimensionality L).\nWe now consider the effect of our stochastic optimization procedure for DCCA, denoted STO below, and demonstrate the importance of minibatch size. We use a 3-layer architecture where the acoustic and articulatory networks have two hidden layers of 1800 and 1200 rectified linear units (ReLUs) respectively, and the output dimensionality (and therefore the maximum possible total canonical correlation over dimensions) is L = 112. We use a small weight decay \u03b3 = 10\u22124, and do grid search for several hyperparameters: rx, ry \u2208 {10\u22124, 10\u22122, 1, 102}, constant learning rate in {10\u22124, 10\u22123, 10\u22122, 10\u22121}, fixed momentum in {0, 0.5, 0.9, 0.95, 0.99}, and minibatch size in {100, 200, 300, 400, 500, 750, 1000}. After learning the projection mappings on the training set, we apply them to the tuning/test set to obtain projections, and measure the canonical correlation between views. Figure 6 shows the learning curves on the tuning set for different minibatch sizes, each using the optimal values for the other hyperparameters. It is clear that for small minibatches (100, 200), the objective quickly plateaus at a low value, whereas for large enough minibatch size, there is always a steep increase at the beginning, which is a known advantage of stochastic firstorder algorithms (Bottou and Bousquet, 2008), and a wide range of learning rate/momentum give very similar results. The reason for such behavior is that the stochastic estimate of the DCCA objective becomes more accurate as minibatch size n increases. We provide theoretical analysis of the error between the true objective and its stochastic estimate in Appendix A.\nRecently, Wang et al. (2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al., 2015) for CCA. Each iteration of the NOI algorithm adaptively estimates the covariance matrices of the projections of each view, whitens the projections of a minibatch using the estimated covariance matrices, and takes a gradient step over DNN weight parameters of the nonlinear least squares problems of regressing each view\u2019s input against the whitened projection of the other view for the minibatch. The advantage of NOI is that it performs well with smaller minibatch sizes and thus reduces memory consumption. Wang et al. (2015c) have shown that NOI can achieve the same objective value as STO using smaller minibatches. For the problems considered in this paper, however, each epoch of NOI takes a longer time than that of STO, due to the whitening operations at each iteration (involving eigenvalue decomposition of L \u00d7 L covariance matrices). The smaller the minibatch size we use in NOI, the more frequently we run such operations. We report the learning curve of NOI with minibatch size n = 100 while all other hyper-parameters are tuned similarly to STO. NOI can eventually reach the\n5. Our split of the data is the same as the one used by Andrew et al. (2013). We note that Lopez-Paz et al. (2014) used the same speaker in their experiments, but they randomly shuffled all 50K frames before creating the splits. We suspect that DCCA (as well as their own algorithm) were under-tuned in their experiments. We ran experiments on a randomly shuffled dataset with careful tuning of kernel widths for FKCCA/NKCCA (using rank M = 6000) and obtained canonical correlations of 99.2/105.6/107.6 for FKCCA/NKCCA/DCCA, which are better than those reported in Lopez-Paz et al. (2014).\nsame objective as STO given more training time. Ultimately, the choice of optimization algorithm will depend on the task-specific data and time/memory constraints.\nFinally, we also train the same model using batch training with L-BFGS6 with the same random initial weight parameters and tune (rx, ry) on the same grid. While L-BFGS does well on the training set, its performance on tuning/test is usually worse than that of stochastic optimization with reasonable hyperparameters.\nWe also train FKCCA and NKCCA on this dataset. We tune (rx, ry) on the same grid as for DCCA, tune the kernel width for each view, and vary the approximation rank M from 1000 to 6000 for each method. We plot the best total canonical correlation achieved by each algorithm on the tuning set as a function of M in Figure 7. Clearly both algorithms require relatively large M to perform well on this task, but the return in further increasing M is diminishing. NKCCA achieves better results than FKCCA, although forming the low-rank approximation also becomes more costly as M increases. We plot the total canonical correlation achieved by each algorithm at different running times with various hyperparameters in Figure 8.\n6. We use the L-BFGS implementation of Schmidt (2012), which includes a good line-search procedure.\nFinally, we select for each algorithm the best model on the tuning set and give the corresponding total canonical correlation on the test set in Table 4."}, {"heading": "5. Conclusion", "text": "We have explored several approaches in the space of DNN-based multi-view representation learning. We have found that on several tasks, CCA-based models outperform autoencoder-based models (SplitAE) and models based on between-view squared distance (DistAE) or correlation (CorrAE) instead of canonical correlation. The best overall performer is a new DCCA extension, deep canonically correlated autoencoders (DCCAE). We have studied these objectives in the context of DNNs, but we expect that the same trends should apply also to other network architectures such as convolutional (LeCun et al., 1998) and recurrent (Elman, 1990; Hochreiter and Schmidhuber, 1997) networks, and this is one direction for future work.\nIn light of the empirical results, it is interesting to consider again the main features of each type of objective and corresponding constraints. Autoencoder-based approaches are based on the idea that the learned features should be able to accurately reconstruct the inputs (in the case of multiview learning, the inputs in both views). The CCA objective, on the other hand, focuses on how well\neach view\u2019s representation predicts the other\u2019s, ignoring the ability to reconstruct each view. CCA is expected to perform well for clustering and classification when the two views are uncorrelated given the class label (Chaudhuri et al., 2009). The noisy MNIST dataset used here simulates exactly this scenario, and indeed this is the task where deep CCA outperforms other objectives by the largest margins. Even in the other tasks, however, there usually seems to be only a small advantage to being able to reconstruct the inputs faithfully.\nThe constraints in the various methods also have an important effect. The performance difference between DCCA and CorrAE demonstrates that uncorrelatedness between learned dimensions is important. On the other hand, the stronger DCCA constraint may still not be sufficiently strong; an even better constraint may be to require the learned dimensions to be independent (or approximately so), and this is an interesting avenue for future work.\nWe believe the applicability of the DCCA objective goes beyond the unsupervised feature learning setting. For example, it can be used as a data-dependent regularizer in supervised or semisupervised learning settings where we have some labeled data as well as multi-view observations. The usefulness of CCA in such settings has previously been analyzed theoretically (Kakade and Foster, 2007; McWilliams et al., 2013) and has begun to be explored experimentally (Arora and Livescu, 2014)."}, {"heading": "Appendix A. Analysis of stochastic optimization for DCCA", "text": "Our SGD-like optimization for DCCA works as follows. We randomly pick a minibatch of n training pairs {(xi,yi)}ni=1, feed them forward through the networks f and g to obtain outputs {(fi,gi)} n i=1 where fi = f(xi) \u2208 R\ndx and gi = g(yi) \u2208 Rdy , and estimate the covariances of network outputs based on these samples, namely \u03a3\u0302xx = 1n \u2211n i=1 fif \u22a4 i + rxI, \u03a3\u0302yy = 1 n \u2211n i=1 gig \u22a4 i + ryI, and \u03a3\u0302xy = 1n \u2211n i=1 fig \u22a4 i , and finally compute a \u201cpartial\u201d objective which is the sum of the top L singular values of T\u0302 = \u03a3\u0302 \u22121/2 xx \u03a3\u0302xy\u03a3\u0302 \u22121/2 yy , together with its gradient with respect to the network outputs and the weights at each layer of each network (see Section 3 of Andrew et al., 2013 for the gradient formulas).\nWe denote by \u0398(n) the \u201cpartial\u201d objective based on the n samples, i.e.\n\u0398(n) =\nL \u2211\nk=1\n\u03c3k(T\u0302) =\nL \u2211\nk=1\n\u03c3k(\u03a3\u0302 \u22121/2 xx \u03a3\u0302xy\u03a3\u0302 \u22121/2 yy ).\nAnd recall that our true objective function is\n\u0398 = L \u2211\nk=1\n\u03c3k(T) = L \u2211\nk=1\n\u03c3k(\u03a3 \u22121/2 xx \u03a3xy\u03a3 \u22121/2 yy ),\nwhere \u03a3xx = 1n \u2211N i=1 fif \u22a4 i + rxI, \u03a3yy = 1 n \u2211N i=1 gig \u22a4 i + ryI, and \u03a3xy = 1 n \u2211N i=1 fig \u22a4 i are computed over the entire training set. An important observation is that\nE\u0398(n) 6= \u0398,\nwhere the expectation is taken over random selection of the n training samples (all expectations in this section are taken over the random sampling process). This fact indicates that in expectation,\nthe \u201cpartial\u201d objective we compute is not equal to the true objective we want to optimize and thus the naive implementation is not really a stochastic gradient algorithm (which requires the gradient estimate to be unbiased). The reason why we can not in general have E\u0398(n) = \u0398 for the naive implementation is that there exist three nonlinear operations in our objective: the sum of singular values operations, the multiplication of three matrices, and the inverse square root operations for auto-covariance matrices. These operations keep us from moving the expectation inside, even though we do have E \u03a3\u0302xx = \u03a3xx, E \u03a3\u0302yy = \u03a3yy , and E \u03a3\u0302xy = \u03a3xy.\nIn the following, we try to bound the error between E\u0398(n) and \u0398 for a slightly modified version of the naive implementation. Specifically, we make the two assumptions below which make the analysis easier.\n\u2022 A1: The samples used for estimating \u03a3xx, \u03a3yy and \u03a3xy are chosen independently from each other, each by sampling examples (for \u03a3xx and \u03a3yy) or example pairs (for \u03a3xy) uniformly at random with replacement from the training set. In practice, we could randomly pick n samples of f(x) for computing \u03a3\u0302xx, another n samples of g(y) for computing \u03a3\u0302yy, and n pairs of samples of (f(x),g(y)) for computing \u03a3\u0302xy, and the computational cost of this modified procedure is twice the cost of the original naive implementation. This simple modification allows the expectation of matrix multiplications to factorize.\n\u2022 A2: Additionally, we assume an upper bound on the magnitude of the neural network outputs, namely, max(\u2016fi\u2016 2 , \u2016gi\u2016 2) \u2264 B, \u2200i. This holds when nonlinear activations with a bounded\nrange are used; e.g., with logistic sigmoid or hyperbolic tangent activations, the upper bound B can be set to the output dimensionality (because the squared activation is bounded by 1 for each output unit), or when the inputs themselves are bounded and the functions f(x) and g(y) are Lipschitz.\nOur analysis is based on the following formulation of the linear CCA solution (Borga, 2001). Notice that the optimal (U,V) for the CCA objective (1) satisfies\n[\n0 T T\u22a4 0\n]\n[\n\u03a3 1/2 xx U \u03a3 1/2 yy V\n]\n=\n[\n\u03a3 1/2 xx U \u03a3 1/2 yy V\n]\n\u03a3, (14)\nwhere \u03a3 contains the top L singular values of T on the diagonal. This is easy to verify as \u03a31/2xx U and \u03a31/2yy V contain the top left/right singular vectors of T (see, e.g., Section 2 of Andrew et al., 2013).\nMultiplying both sides of (14) by\n[\n\u03a3 \u22121/2 xx 0\n0 \u03a3 \u22121/2 yy\n]\ngives\n[\n\u03a3\u22121xx 0\n0 \u03a3\u22121yy\n] [\n0 \u03a3xy \u03a3\u22a4xy 0\n] [\nU\nV\n]\n=\n[\nU\nV\n]\n\u03a3,\nwhich implies\n[\nU\nV\n]\ncorrespond to the top L eigenvectors of the matrix A\u22121B, where\nA =\n[\n\u03a3xx 0\n0 \u03a3yy\n]\n, B =\n[\n0 \u03a3xy \u03a3\u22a4xy 0\n]\n.\n(It is straightforward to check that eigenvalues of A\u22121B are \u00b1\u03c31(T),\u00b1\u03c32(T), . . . .) When using minibatches, the estimate of the objective is of course computed based on the randomly chosen subset of samples, and is the sum of the top eigenvalues of the matrix A\u0302\u22121B\u0302, where\nA\u0302 =\n[\n\u03a3\u0302xx 0\n0 \u03a3\u0302yy\n]\n, B\u0302 =\n[\n0 \u03a3\u0302xy\n\u03a3\u0302 \u22a4 xy 0\n]\n.\nWe now try to bound the expected error between A\u0302\u22121B\u0302 and A\u22121B measured in spectral norm. An important tool in our analysis is the matrix Bernstein inequality below.\nLemma 1 (Matrix Bernstein, Theorem 1.6.2 of Tropp, 2012) LetA1, . . . ,An be independent random matrices with common dimension d1 \u00d7 d2. Assume that each matrix has bounded deviation from its mean:\n\u2016Ak \u2212 EAk\u2016 \u2264 R \u2200k = 1, . . . , n.\nForm the sum B = \u2211n\nk=1Ak, and introduce a variance parameter\n\u03c32 = max { \u2225 \u2225 \u2225 E [ (B\u2212 EB)(B\u2212 EB)\u22a4 ] \u2225 \u2225 \u2225 , \u2225 \u2225 \u2225 E [ (B\u2212 EB)\u22a4(B\u2212 EB) ] \u2225 \u2225 \u2225 } .\nThen\nE \u2016B\u2212 EB\u2016 \u2264 \u221a 2\u03c32 log(d1 + d2) + 1\n3 R log(d1 + d2).\nThe following result and its proof are similar to that of Lopez-Paz et al. (2014, Theorem. 4).\nTheorem 1 Assume that A1 and A2 hold for our stochastic estimate of the true DCCA objective, and assume that the eigenvalues of autocovariance matrices \u03a3\u0302xx and \u03a3\u0302yy are lower bounded by \u03b3x > 0 and \u03b3y > 0 respectively, i.e.,\n\u03a3\u0302xx \u03b3xI, \u03a3\u0302yy \u03b3yI.\nThen we have the following bound for the expected error:\nE\n\u2225 \u2225 \u2225 A\u0302\u22121B\u0302\u2212A\u22121B \u2225 \u2225 \u2225 \u2264 max {e1, e2} (15)\nwhere the expectation is taken over random selection of training samples as described in A1, and\ne1 = B\n\u03b32x\n( \u221a\n2B2 log(2dx)\nn +\n2B log(2dx)\n3n\n)\n+ 1\n\u03b3x\n( \u221a\n2B2 log(dx + dy)\nn +\n2B log(dx + dy)\n3n\n)\n,\ne2 = B\n\u03b32y\n( \u221a\n2B2 log(2dy)\nn +\n2B log(2dy)\n3n\n)\n+ 1\n\u03b3y\n( \u221a\n2B2 log(dx + dy)\nn +\n2B log(dx + dy)\n3n\n)\n.\nProof Since A\u0302\u22121B\u0302 \u2212A\u22121B is block diagonal, its spectral norm is bounded by the maximum of the spectral norms of the individual blocks:\nE\n\u2225 \u2225 \u2225 A\u0302\u22121B\u0302\u2212A\u22121B \u2225 \u2225 \u2225 \u2264 max { E \u2225 \u2225 \u2225 \u03a3\u0302 \u22121 xx \u03a3\u0302xy \u2212\u03a3 \u22121 xx\u03a3xy \u2225 \u2225 \u2225 , E \u2225 \u2225 \u2225 \u03a3\u0302 \u22121 yy \u03a3\u0302 \u22a4 xy \u2212\u03a3 \u22121 yy \u03a3 \u22a4 xy \u2225 \u2225 \u2225 } .\nWe now focus on analyzing the first term in the above maximum as the other term follows analogously. Define the individual error terms\nEi = 1\nn (\u03a3\u0302\n\u22121 xx fig \u22a4 i \u2212\u03a3 \u22121 xx\u03a3xy), E =\nn \u2211\ni=1\nEi.\nThus our goal is to bound \u2225 \u2225 \u2225 \u03a3\u0302 \u22121 xx \u03a3\u0302xy \u2212\u03a3 \u22121 xx\u03a3xy \u2225 \u2225 \u2225 = \u2016E\u2016.\nNotice that the matrices \u03a3xx and \u03a3xy are not random. And due to assumption A1, the sam-\nples used for estimating \u03a3\u0302xx and \u03a3\u0302xy are selected independently, so the expectation of \u03a3\u0302 \u22121 xx fig \u22a4 i factorizes. Therefore we have\nEEi = 1\nn\n(\nE\n[\n\u03a3\u0302 \u22121 xx\n]\n\u03a3xy \u2212\u03a3 \u22121 xx\u03a3xy\n)\n,\nand the deviation of the individual error matrices from their expectation is\nZi := Ei \u2212 EEi = 1\nn\n(\n\u03a3\u0302 \u22121 xx fig \u22a4 i \u2212 E\n[\n\u03a3\u0302 \u22121 xx\n]\n\u03a3xy\n)\n.\nWe would like to bound \u2016 \u2211n\ni=1 Zi\u2016 and \u2016EEi\u2016 separately using the matrix Bernstein inequality in Lemma 1, and then use them to bound e1.\nBounding \u2016 \u2211n\ni=1 Zi\u2016. Notice that E [Zi] = 0, and we can bound its norm (denoted R)\nR := \u2016Zi\u2016 \u2264 1\nn\n(\u2225\n\u2225 \u2225 \u03a3\u0302 \u22121 xx fig \u22a4 i\n\u2225 \u2225 \u2225 + \u2225 \u2225 \u2225 E [ \u03a3\u0302 \u22121 xx ] \u03a3xy \u2225 \u2225 \u2225 )\n\u2264 1\nn\n( \u2225\n\u2225 \u2225 \u03a3\u0302 \u22121 xx\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 fig \u22a4 i \u2225 \u2225 \u2225 + E [ \u2225 \u2225 \u2225 \u03a3\u0302 \u22121 xx \u2225 \u2225 \u2225 ] \u2016\u03a3xy\u2016 )\n\u2264 1\nn\n(\n1\n\u03b3x\n\u2225 \u2225 \u2225 fig \u22a4 i \u2225 \u2225 \u2225 + 1\n\u03b3x max i\n\u2225 \u2225 \u2225 fig \u22a4 i \u2225 \u2225 \u2225\n)\n\u2264 2\nn\u03b3x max i\n\u2225 \u2225 \u2225 fig \u22a4 i \u2225 \u2225 \u2225\n\u2264 2\nn\u03b3x max i \u2016fi\u2016 \u2016gi\u2016\n\u2264 2B\nn\u03b3x ,\nwhere we have used the triangle inequality in the first inequality, and Jensen\u2019s inequality for the second and third inequality (since norms are convex functions). To apply the Matrix Bernstein inequality, we still need to bound the variance which is defined as\n\u03c32 :=max\n\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 E   ( n \u2211\ni=1\nZi\n)\n\u00b7\n(\nn \u2211\ni=1\nZi\n)\u22a4 \n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 , \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 E   ( n \u2211\ni=1\nZi\n)\u22a4\n\u00b7\n(\nn \u2211\ni=1\nZi\n)\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \n=max\n{ \u2225\n\u2225 \u2225 \u2225 \u2225\nn \u2211\ni=1\nE\n[\nZiZ \u22a4 i\n]\n\u2225 \u2225 \u2225 \u2225 \u2225 , \u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\ni=1\nE\n[\nZ\u22a4i Zi\n]\n\u2225 \u2225 \u2225 \u2225 \u2225 } ,\nwhere we have used the fact that the Zi\u2019s are independent and have mean zero in the second equality. Let us consider an individual term in the summand of the second term:\nZ\u22a4i Zi = 1\nn2\n(\ngif \u22a4 i \u03a3\u0302 \u22122 xx fig \u22a4 i \u2212 gif \u22a4 i \u03a3\u0302 \u22121 xxE\n[\n\u03a3\u0302 \u22121 xx\n]\n\u03a3xy\n\u2212\u03a3\u22a4xyE [ \u03a3\u0302 \u22121 xx ] \u03a3\u0302 \u22121 xx fig \u22a4 i +\u03a3 \u22a4 xyE [ \u03a3\u0302 \u22121 xx ] E [ \u03a3\u0302 \u22121 xx ] \u03a3xy ) .\nTaking expectations we see that\nE\n[\nZ\u22a4i Zi\n] = 1\nn2\n(\nE\n[\ngif \u22a4 i \u03a3\u0302 \u22122 xx fig \u22a4 i\n]\n\u2212\u03a3\u22a4xy\n(\nE\n[\n\u03a3\u0302 \u22121 xx ])2 \u03a3xy\n)\nwhere we have again used the assumption A1. Notice that \u03a3\u22a4xy ( E [ \u03a3\u0302 \u22121 xx ])2 \u03a3xy is positive semidefinite, so subtracting it only decreases the spectral norm. We now take norms on both sides of the above equality and obtain\n\u2225 \u2225 \u2225 E [ Z\u22a4i Zi ] \u2225 \u2225 \u2225 \u2264 1\nn2\n\u2225 \u2225 \u2225 E [ gif \u22a4 i \u03a3\u0302 \u22122 xx fig \u22a4 i ] \u2225 \u2225 \u2225 \u2264 1\nn2 E\n[ \u2225\n\u2225 \u2225 gif \u22a4 i \u03a3\u0302 \u22122 xx fig \u22a4 i\n] \u2225\n\u2225 \u2225\n\u2264 1\nn2 E\n[ \u2225\n\u2225 \u2225 gif \u22a4 i\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u03a3\u0302 \u22122 xx \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 fig \u22a4 i \u2225 \u2225 \u2225 ]\n\u2264 B2\nn2\u03b32x ,\nwhere Jensen\u2019s inequality is used in the second inequality. A similar argument shows that\n\u2225 \u2225 \u2225 E [ ZiZ \u22a4 i ] \u2225 \u2225 \u2225 \u2264 1\nn2\n\u2225 \u2225 \u2225 E [ \u03a3\u0302 \u22121 xx (fig \u22a4 i gif \u22a4 i )\u03a3\u0302 \u22121 xx ] \u2225 \u2225 \u2225 \u2264\nB2\nn2\u03b32x .\nAn invocation of the triangle inequality on the definition of \u03c32 along with the above two bounds gives\n\u03c32 \u2264 B2\nn\u03b32x .\nWe may now appeal to the matrix Bernstein inequality on the dx \u00d7 dy matrices {Zi}ni=1 to obtain the bound\nE\n\u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\ni=1\nZi\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2264 \u221a 2\u03c32 log(dx + dy) + 1 3 R log(dx + dy)\n= 1\n\u03b3x\n( \u221a\n2B2 log(dx + dy)\nn +\n2B log(dx + dy)\n3n\n)\n.\nBounding EEi. Notice that\n\u03a3\u0302 \u22121 xx \u2212\u03a3 \u22121 xx = \u03a3 \u22121 xx (\u03a3xx \u2212 \u03a3\u0302xx)\u03a3\u0302 \u22121 xx ,\nso that\n\u2016EEi\u2016 = 1\nn\n\u2225 \u2225 \u2225 E [ \u03a3\u0302 \u22121 xx ] \u03a3xy \u2212\u03a3 \u22121 xx\u03a3xy \u2225 \u2225 \u2225\n\u2264 1\nn\n\u2225 \u2225 \u2225 E [ \u03a3\u0302 \u22121 xx \u2212\u03a3 \u22121 xx ] \u2225 \u2225 \u2225 \u2016\u03a3xy\u2016\n\u2264 1\nn E\n[ \u2225\n\u2225 \u2225 \u03a3\u0302 \u22121 xx \u2212\u03a3 \u22121 xx\n\u2225 \u2225 \u2225 ] \u2016\u03a3xy\u2016\n\u2264 B\nn\u03b32x E\n\u2225 \u2225 \u2225 \u03a3\u0302xx \u2212\u03a3xx \u2225 \u2225 \u2225 .\nUsing the same matrix Bernstein technique (now applied to \u2211n i=1Z \u2032 i with Z \u2032 i = 1 n\n(\nfif \u22a4 i \u2212\u03a3xx\n)\n), we have\nE\n\u2225 \u2225 \u2225 \u03a3\u0302xx \u2212\u03a3xx \u2225 \u2225 \u2225 \u2264\n\u221a\n2B2 log(2dx)\nn +\n2B log(2dx)\n3n .\nWe are now ready to bound the quantity of interest as\ne1 = E \u2225 \u2225 \u2225 \u03a3\u0302 \u22121 xx \u03a3\u0302xy \u2212\u03a3 \u22121 xx\u03a3xy \u2225 \u2225 \u2225 = E\n\u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\ni=1\nEi\n\u2225 \u2225 \u2225 \u2225 \u2225\n= E\n\u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\ni=1\nEEi +\nn \u2211\ni=1\nZi\n\u2225 \u2225 \u2225 \u2225 \u2225\n\u2264\n\u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\ni=1\nEEi\n\u2225 \u2225 \u2225 \u2225 \u2225 + E \u2225 \u2225 \u2225 \u2225 \u2225 n \u2211\ni=1\nZi\n\u2225 \u2225 \u2225 \u2225 \u2225\n\u2264 B\n\u03b32x\n( \u221a\n2B2 log(2dx)\nn +\n2B log(2dx)\n3n\n)\n+ 1\n\u03b3x\n( \u221a\n2B2 log(dx + dy)\nn +\n2B log(dx + dy)\n3n\n)\n.\nThe bound for e2 = E \u2225 \u2225 \u2225 \u03a3\u0302 \u22121 yy \u03a3\u0302 \u22a4 xy \u2212\u03a3 \u22121 yy \u03a3 \u22a4 xy \u2225 \u2225 \u2225 is completely analogous.\nAssuming that dx = dy = d in our algorithm and \u03b3 = min(\u03b3x, \u03b3y), then we have the dependency of the error in spectral norm as\nE\n\u2225 \u2225 \u2225 A\u0302\u22121B\u0302\u2212A\u22121B \u2225 \u2225 \u2225 \u2264\n(\nB2 \u03b32 + B \u03b3\n)\n( \u221a\n2 log(2d)\nn +\n2 log(2d)\n3n\n)\n.\nAs expected, the error decreases as we use large minibatch size n. Also, we observe the error decreases as (\u03b3x, \u03b3y) increase. Notice that from the definition of \u03a3\u0302xx and \u03a3\u0302yy we are guaranteed that \u03b3x \u2265 rx and \u03b3y \u2265 ry . This means we can increase the regularization constants to improve the estimation error, and this is to be expected as larger (rx, ry) render the auto-covariance matrices less relevant for estimating T. In practice, as long as one uses a minibatch size n > d, the autocovariance matrices are typically non-singular and (rx, ry) are underestimate of (\u03b3x, \u03b3y)."}, {"heading": "Acknowledgment", "text": "The authors would like to thank Louis Goldstein for providing phonetic alignments for the data used in the recognition experiment; Manaal Faruqui, Chris Dyer, Ang Lu, Mohit Bansal, and Kevin Gimpel for sharing resources for the multi-lingual embedding experiments; Nati Srebro for input on the stochastic optimization of DCCA; and Geoff Hinton for helpful discussion on the CCA objective."}], "references": [{"title": "A kernel method for canonical correlation analysis", "author": ["Shotaro Akaho"], "venue": "In Proceedings of the International Meeting of the Psychometric Society (IMPS2001). Springer-Verlag,", "citeRegEx": "Akaho.,? \\Q2001\\E", "shortCiteRegEx": "Akaho.", "year": 2001}, {"title": "Deep canonical correlation analysis", "author": ["Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu"], "venue": "In Proc. of the 30th Int. Conf. Machine Learning (ICML", "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Kernel CCA for multi-view learning of acoustic features using articulatory measurements", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Symposium on Machine Learning in Speech and Language Processing (MLSLP),", "citeRegEx": "Arora and Livescu.,? \\Q2012\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2012}, {"title": "Multi-view CCA-based acoustic features for phonetic recognition across speakers and domains", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201913),", "citeRegEx": "Arora and Livescu.,? \\Q2013\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2013}, {"title": "Multi-view learning with supervision for transformed bottleneck features", "author": ["Raman Arora", "Karen Livescu"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201914),", "citeRegEx": "Arora and Livescu.,? \\Q2014\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2014}, {"title": "Kernel independent component analysis", "author": ["Francis R. Bach", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2002}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["Francis R. Bach", "Michael I. Jordan"], "venue": "Technical Report 688,", "citeRegEx": "Bach and Jordan.,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2005}, {"title": "Learning a Mahalanobis metric from equivalence constraints", "author": ["Aharon Bar-Hillel", "Tomer Hertz", "Noam Shental", "Daphna Weinshall"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bar.Hillel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bar.Hillel et al\\.", "year": 2005}, {"title": "Self-organizing neural network that discovers surfaces in random-dot stereograms", "author": ["Suzanna Becker", "Geoffrey E. Hinton"], "venue": "Nature, 355:161\u2013163,", "citeRegEx": "Becker and Hinton.,? \\Q1992\\E", "shortCiteRegEx": "Becker and Hinton.", "year": 1992}, {"title": "Multi-view clustering", "author": ["Steffen Bickel", "Tobias Scheffer"], "venue": "In Proc. of the 4th IEEE Int. Conf. Data Mining (ICDM\u201904),", "citeRegEx": "Bickel and Scheffer.,? \\Q2004\\E", "shortCiteRegEx": "Bickel and Scheffer.", "year": 2004}, {"title": "On the regularization of canonical correlation analysis", "author": ["Tijl De Bie", "Bart De Moor"], "venue": "Int. Sympos. ICA and BSS,", "citeRegEx": "Bie and Moor.,? \\Q2003\\E", "shortCiteRegEx": "Bie and Moor.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Blacoe and Lapata.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Correlational spectral clustering", "author": ["Mathew B. Blaschko", "Christoph H. Lampert"], "venue": "In Proc. of the 2008 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Blaschko and Lampert.,? \\Q2008\\E", "shortCiteRegEx": "Blaschko and Lampert.", "year": 2008}, {"title": "Canonical correlation: A tutorial", "author": ["Magnus Borga"], "venue": "Available at http://www.imt.liu.se/ magnus/cca/tutorial/tutorial.pdf,", "citeRegEx": "Borga.,? \\Q2001\\E", "shortCiteRegEx": "Borga.", "year": 2001}, {"title": "The tradeoffs of large scale learning", "author": ["Leon Bottou", "Olivier Bousquet"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bottou and Bousquet.,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet.", "year": 2008}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Sarath Chandar", "Stanislas Lauly", "Hugo Larochelle", "Mitesh M. Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "Correlational neural networks", "author": ["Sarath Chandar", "Mitesh M. Khapra", "Hugo Larochelle", "Balaraman Ravindran"], "venue": "[cs.CL], April", "citeRegEx": "Chandar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2015}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Trans. Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["Kamalika Chaudhuri", "Sham M. Kakade", "Karen Livescu", "Karthik Sridharan"], "venue": "In Proc. of the 26th Int. Conf. Machine Learning", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Information bottleneck for Gaussian variables", "author": ["Gal Chechik", "Amir Globerson", "Naftali Tishby", "Yair Weiss"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chechik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2005}, {"title": "Information-theoretic metric learning", "author": ["Jason V. Davis", "Brian Kulis", "Prateek Jain", "Suvrit Sra", "Inderjit S. Dhillon"], "venue": "In Proc. of the 24th Int. Conf. Machine Learning", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["Paramveer Dhillon", "Dean Foster", "Lyle Ungar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In Proc. EACL,", "citeRegEx": "Faruqui and Dyer.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Multi-view dimensionality reduction via canonical correlation analysis", "author": ["Dean P. Foster", "Rie Johnson", "Sham M. Kakade", "Tong Zhang"], "venue": "Technical report,", "citeRegEx": "Foster et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2009}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Globerson and Roweis.,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Euclidean embedding of co-occurrence data", "author": ["Amir Globerson", "Gal Chechik", "Fernando Pereira", "Naftali Tishby"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Globerson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2007}, {"title": "Neighbourhood components analysis", "author": ["Jacob Goldberger", "Sam Roweis", "Geoff Hinton", "Ruslan Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Goldberger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2005}, {"title": "Linear Algebra for Signal Processing, volume 69 of The IMA Volumes in Mathematics and its Applications, chapter The Canonical Correlations of Matrix Pairs and their Numerical Computation, pages 27\u201349", "author": ["Gene H. Golub", "Hongyuan Zha"], "venue": null, "citeRegEx": "Golub and Zha.,? \\Q1995\\E", "shortCiteRegEx": "Golub and Zha.", "year": 1995}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["David R. Hardoon", "Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Multilingual distributed representations without word alignment", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proc. of the 2nd Int. Conf. Learning Representations (ICLR", "citeRegEx": "Hermann and Blunsom.,? \\Q2014\\E", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2014}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["Hynek Hermansky", "Daniel P.W. Ellis", "Sangita Sharma"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc", "citeRegEx": "Hermansky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hermansky et al\\.", "year": 2000}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Learning distance metrics with contextual constraints for image retrieval", "author": ["Steven C.H. Hoi", "Wei Liu", "Michael R. Lyu", "Wei-Ying Ma"], "venue": "In Proc. of the 2006 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Hoi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2006}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, 28(3/4):321\u2013377,", "citeRegEx": "Hotelling.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling.", "year": 1936}, {"title": "Nonlinear canonical correlation analysis by neural networks", "author": ["W.W. Hsieh"], "venue": "Neural Networks,", "citeRegEx": "Hsieh.,? \\Q2000\\E", "shortCiteRegEx": "Hsieh.", "year": 2000}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proc. of the 22nd Int. Conf. Information and Knowledge Management", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Kernel methods match deep neural networks on TIMIT: Scalable learning in high-dimensional random Fourier spaces", "author": ["Po-Sen Huang", "Haim Avron", "Tara Sainath", "Vikas Sindhwani", "Bhuvana Ramabhadran"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201914),", "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["Sham M. Kakade", "Dean P. Foster"], "venue": "In Proc. of the 20th Annual Conference on Learning Theory (COLT\u201907),", "citeRegEx": "Kakade and Foster.,? \\Q2007\\E", "shortCiteRegEx": "Kakade and Foster.", "year": 2007}, {"title": "Pixels that sound", "author": ["Einat Kidron", "Yoav Y. Schechner", "Michael Elad"], "venue": "In Proc. of the 2005 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Kidron et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kidron et al\\.", "year": 2005}, {"title": "Learning semantics with deep belief network for cross-language information retrieval", "author": ["Jungi Kim", "Jinseok Nam", "Iryna Gurevych"], "venue": "In Proceedings of COLING 2012: Posters,", "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "A neural implementation of canonical correlation analysis", "author": ["Pei Ling Lai", "Colin Fyfe"], "venue": "Neural Networks,", "citeRegEx": "Lai and Fyfe.,? \\Q1999\\E", "shortCiteRegEx": "Lai and Fyfe.", "year": 1999}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["Pei Ling Lai", "Colin Fyfe"], "venue": "Int. J. Neural Syst.,", "citeRegEx": "Lai and Fyfe.,? \\Q2000\\E", "shortCiteRegEx": "Lai and Fyfe.", "year": 2000}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Randomized nonlinear component analysis", "author": ["David Lopez-Paz", "Suvrit Sra", "Alex Smola", "Zoubin Ghahramani", "Bernhard Schoelkopf"], "venue": "In Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Lopez.Paz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2014}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proc. NAACL,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Large scale canonical correlation analysis with iterative least squares", "author": ["Yichao Lu", "Dean P. Foster"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lu and Foster.,? \\Q2014\\E", "shortCiteRegEx": "Lu and Foster.", "year": 2014}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Zhuang Ma", "Yichao Lu", "Dean Foster"], "venue": "In Proc. of the 32nd Int. Conf. Machine Learning (ICML", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Introduction to Information Retrieval", "author": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Correlated random features for fast semi-supervised learning", "author": ["Brian McWilliams", "David Balduzzi", "Joachim Buhmann"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "McWilliams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McWilliams et al\\.", "year": 2013}, {"title": "Nonlinear feature extraction using generalized canonical correlation analysis", "author": ["Thomas Melzer", "Michael Reiter", "Horst Bischof"], "venue": "In Proc. of the 11th Int. Conf. Artificial Neural Networks", "citeRegEx": "Melzer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Melzer et al\\.", "year": 2001}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Algorithms for the assignment and transportation problems", "author": ["James Munkres"], "venue": "J. SIAM,", "citeRegEx": "Munkres.,? \\Q1957\\E", "shortCiteRegEx": "Munkres.", "year": 1957}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Ng"], "venue": "In Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": null, "citeRegEx": "Roweis and Saul.,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul.", "year": 2000}, {"title": "Code available at http://www.cs.ubc.ca/ \u0303schmidtm/Software/minFunc.html", "author": ["Mark Schmidt"], "venue": "minFunc,", "citeRegEx": "Schmidt.,? \\Q2012\\E", "shortCiteRegEx": "Schmidt.", "year": 2012}, {"title": "Learning a distance metric from relative comparisons", "author": ["Matthew Schultz", "Thorsten Joachims"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Schultz and Joachims.,? \\Q2004\\E", "shortCiteRegEx": "Schultz and Joachims.", "year": 2004}, {"title": "Adjustment learning and relevant component analysis", "author": ["Noam Shental", "Tomer Hertz", "Daphna Weinshall", "Misha Pavel"], "venue": "In Proc. 7th European Conf. Computer Vision (ECCV\u201902),", "citeRegEx": "Shental et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Shental et al\\.", "year": 2002}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["Richard Socher", "Fei-Fei Li"], "venue": "In Proc. of the 2010 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Socher and Li.,? \\Q2010\\E", "shortCiteRegEx": "Socher and Li.", "year": 2010}, {"title": "Improved multimodal deep learning with variation of information", "author": ["Kihyuk Sohn", "Wenling Shang", "Honglak Lee"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sohn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2014}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava and Salakhutdinov.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov.", "year": 2014}, {"title": "The information bottleneck method", "author": ["Naftali Tishby", "Fernando Pereira", "William Bialek"], "venue": "In Proc. 37th Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "Tishby et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 1999}, {"title": "User-friendly tools for random matrices: An introduction", "author": ["J.A. Tropp"], "venue": "NIPS Tutorials,", "citeRegEx": "Tropp.,? \\Q2012\\E", "shortCiteRegEx": "Tropp.", "year": 2012}, {"title": "Distance metric learning with kernels", "author": ["Ivor W. Tsang", "James T. Kwok"], "venue": "In Proc. of the 13th Int. Conf. Artificial Neural Networks", "citeRegEx": "Tsang and Kwok.,? \\Q2003\\E", "shortCiteRegEx": "Tsang and Kwok.", "year": 2003}, {"title": "Visualizing data using t-SNE", "author": ["Laurens J.P. van der Maaten", "Geoffrey E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Inferring a semantic representation of text via cross-language correlation analysis", "author": ["Alexei Vinokourov", "Nello Cristianini", "John Shawe-Taylor"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinokourov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vinokourov et al\\.", "year": 2003}, {"title": "Partial-Hessian strategies for fast learning of nonlinear embeddings", "author": ["Max Vladymyrov", "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "In Proc. of the 29th Int. Conf. Machine Learning (ICML", "citeRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.,? \\Q2012\\E", "shortCiteRegEx": "Vladymyrov and Carreira.Perpi\u00f1\u00e1n.", "year": 2012}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In Proc. of the IEEE Int. Conf. Acoustics, Speech and Sig. Proc. (ICASSP\u201915),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "On deep multi-view representation learning", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In Proc. of the 32nd Int. Conf. Machine Learning (ICML 2015),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Stochastic optimization for deep cca via nonlinear orthogonal iterations", "author": ["Weiran Wang", "Raman Arora", "Nati Srebro", "Karen Livescu"], "venue": "In 53nd Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Lawrence K. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "X-Ray Microbeam Speech Production Database User\u2019s Handbook Version 1.0", "author": ["John R. Westbury"], "venue": "Waisman Center on Mental Retardation & Human Development,", "citeRegEx": "Westbury.,? \\Q1994\\E", "shortCiteRegEx": "Westbury.", "year": 1994}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["Christopher K.I. Williams", "Matthias Seeger"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric Xing", "Andrew Ng", "Michael Jordan", "Stuart Russell"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Xing et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2003}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["Wei Xu", "Xin Liu", "Yihong Gong"], "venue": "In Proc. of the 26th ACM Conf. Research and Development in Information Retrieval (SIGIR", "citeRegEx": "Xu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2003}, {"title": "Deep correlation for matching images and text", "author": ["Fei Yan", "Krystian Mikolajczyk"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Yan and Mikolajczyk.,? \\Q2015\\E", "shortCiteRegEx": "Yan and Mikolajczyk.", "year": 2015}, {"title": "Nystr\u00f6m method vs random Fourier features: A theoretical and empirical comparison", "author": ["Tianbao Yang", "Yu-Feng Li", "Mehrdad Mahdavi", "Rong Jin", "Zhi-Hua Zhou"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 42, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video (Kidron et al., 2005; Chaudhuri et al., 2009), audio + articulation (Arora and Livescu, 2013; Wang et al.", "startOffset": 96, "endOffset": 141}, {"referenceID": 18, "context": "The views can be multiple measurement modalities, such as simultaneously recorded audio + video (Kidron et al., 2005; Chaudhuri et al., 2009), audio + articulation (Arora and Livescu, 2013; Wang et al.", "startOffset": 96, "endOffset": 141}, {"referenceID": 3, "context": ", 2009), audio + articulation (Arora and Livescu, 2013; Wang et al., 2015a), images + text (Hardoon et al.", "startOffset": 30, "endOffset": 75}, {"referenceID": 30, "context": ", 2015a), images + text (Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al.", "startOffset": 24, "endOffset": 115}, {"referenceID": 63, "context": ", 2015a), images + text (Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al.", "startOffset": 24, "endOffset": 115}, {"referenceID": 35, "context": ", 2015a), images + text (Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al.", "startOffset": 24, "endOffset": 115}, {"referenceID": 81, "context": ", 2015a), images + text (Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al.", "startOffset": 24, "endOffset": 115}, {"referenceID": 71, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 29, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 15, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 23, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 48, "context": ", 2013; Yan and Mikolajczyk, 2015), or parallel text in two languages (Vinokourov et al., 2003; Haghighi et al., 2008; Chandar et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al.", "startOffset": 70, "endOffset": 181}, {"referenceID": 58, "context": ", 2015), but may also be different information extracted from the same source, such as words + context (Pennington et al., 2014) or document text + text of 1", "startOffset": 103, "endOffset": 128}, {"referenceID": 9, "context": "inbound hyperlinks (Bickel and Scheffer, 2004).", "startOffset": 19, "endOffset": 46}, {"referenceID": 41, "context": "Under certain assumptions, theoretical results exist showing the advantages of multi-view techniques for downstream tasks (Kakade and Foster, 2007; Foster et al., 2009; Chaudhuri et al., 2009).", "startOffset": 122, "endOffset": 192}, {"referenceID": 24, "context": "Under certain assumptions, theoretical results exist showing the advantages of multi-view techniques for downstream tasks (Kakade and Foster, 2007; Foster et al., 2009; Chaudhuri et al., 2009).", "startOffset": 122, "endOffset": 192}, {"referenceID": 18, "context": "Under certain assumptions, theoretical results exist showing the advantages of multi-view techniques for downstream tasks (Kakade and Foster, 2007; Foster et al., 2009; Chaudhuri et al., 2009).", "startOffset": 122, "endOffset": 192}, {"referenceID": 71, "context": "Experimentally, prior work has shown the benefit of multi-view methods on tasks such as retrieval (Vinokourov et al., 2003; Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al.", "startOffset": 98, "endOffset": 187}, {"referenceID": 30, "context": "Experimentally, prior work has shown the benefit of multi-view methods on tasks such as retrieval (Vinokourov et al., 2003; Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al.", "startOffset": 98, "endOffset": 187}, {"referenceID": 63, "context": "Experimentally, prior work has shown the benefit of multi-view methods on tasks such as retrieval (Vinokourov et al., 2003; Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al.", "startOffset": 98, "endOffset": 187}, {"referenceID": 35, "context": "Experimentally, prior work has shown the benefit of multi-view methods on tasks such as retrieval (Vinokourov et al., 2003; Hardoon et al., 2004; Socher and Li, 2010; Hodosh et al., 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al.", "startOffset": 98, "endOffset": 187}, {"referenceID": 12, "context": ", 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al., 2009), and classification/recognition (Dhillon et al.", "startOffset": 20, "endOffset": 72}, {"referenceID": 18, "context": ", 2013), clustering (Blaschko and Lampert, 2008; Chaudhuri et al., 2009), and classification/recognition (Dhillon et al.", "startOffset": 20, "endOffset": 72}, {"referenceID": 21, "context": ", 2009), and classification/recognition (Dhillon et al., 2011; Arora and Livescu, 2013; Ngiam et al., 2011).", "startOffset": 40, "endOffset": 107}, {"referenceID": 3, "context": ", 2009), and classification/recognition (Dhillon et al., 2011; Arora and Livescu, 2013; Ngiam et al., 2011).", "startOffset": 40, "endOffset": 107}, {"referenceID": 57, "context": ", 2009), and classification/recognition (Dhillon et al., 2011; Arora and Livescu, 2013; Ngiam et al., 2011).", "startOffset": 40, "endOffset": 107}, {"referenceID": 33, "context": "One type of objective is based on deep autoencoders (Hinton and Salakhutdinov, 2006), where the objective is to learn a compact representation that best reconstructs the inputs.", "startOffset": 52, "endOffset": 84}, {"referenceID": 57, "context": "This approach has been shown to be effective for speech and vision tasks (Ngiam et al., 2011).", "startOffset": 73, "endOffset": 93}, {"referenceID": 13, "context": "The CCA objective has been studied extensively and has a number of useful properties and interpretations (Borga, 2001; Bach and Jordan, 2002, 2005; Chechik et al., 2005), and the optimal linear projection mappings can be obtained by solving an eigenvalue system of a matrix whose dimensions equal the input dimensionalities.", "startOffset": 105, "endOffset": 169}, {"referenceID": 19, "context": "The CCA objective has been studied extensively and has a number of useful properties and interpretations (Borga, 2001; Bach and Jordan, 2002, 2005; Chechik et al., 2005), and the optimal linear projection mappings can be obtained by solving an eigenvalue system of a matrix whose dimensions equal the input dimensionalities.", "startOffset": 105, "endOffset": 169}, {"referenceID": 45, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 0, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 53, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 5, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 30, "context": "To overcome the limiting power of linear projections, a nonlinear extension\u2013kernel canonical correlation analysis (KCCA)\u2013has also been proposed (Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004).", "startOffset": 144, "endOffset": 243}, {"referenceID": 71, "context": "CCA and KCCA have long been the workhorse for multi-view feature learning and dimensionality reduction (Vinokourov et al., 2003; Kakade and Foster, 2007; Socher and Li, 2010; Dhillon et al., 2011).", "startOffset": 103, "endOffset": 196}, {"referenceID": 41, "context": "CCA and KCCA have long been the workhorse for multi-view feature learning and dimensionality reduction (Vinokourov et al., 2003; Kakade and Foster, 2007; Socher and Li, 2010; Dhillon et al., 2011).", "startOffset": 103, "endOffset": 196}, {"referenceID": 63, "context": "CCA and KCCA have long been the workhorse for multi-view feature learning and dimensionality reduction (Vinokourov et al., 2003; Kakade and Foster, 2007; Socher and Li, 2010; Dhillon et al., 2011).", "startOffset": 103, "endOffset": 196}, {"referenceID": 21, "context": "CCA and KCCA have long been the workhorse for multi-view feature learning and dimensionality reduction (Vinokourov et al., 2003; Kakade and Foster, 2007; Socher and Li, 2010; Dhillon et al., 2011).", "startOffset": 103, "endOffset": 196}, {"referenceID": 44, "context": "Several alternative nonlinear CCA-like approaches based on neural networks have also been proposed (Lai and Fyfe, 1999; Hsieh, 2000), but the full DNN extension of CCA, termed deep CCA (DCCA, Andrew et al.", "startOffset": 99, "endOffset": 132}, {"referenceID": 38, "context": "Several alternative nonlinear CCA-like approaches based on neural networks have also been proposed (Lai and Fyfe, 1999; Hsieh, 2000), but the full DNN extension of CCA, termed deep CCA (DCCA, Andrew et al.", "startOffset": 99, "endOffset": 132}, {"referenceID": 57, "context": "1 Split autoencoders (SplitAE) Ngiam et al. (2011) propose to extract shared representations by learning to reconstruct both views from the one view that is available at test time.", "startOffset": 31, "endOffset": 51}, {"referenceID": 1, "context": "2 Deep canonical correlation analysis (DCCA) Andrew et al. (2013) propose a DNN extension of CCA termed deep CCA (DCCA; see Fig.", "startOffset": 45, "endOffset": 66}, {"referenceID": 10, "context": ",vL] are the CCA directions that project the DNN outputs and (rx, ry) > 0 are regularization parameters added to the diagonal of the sample autocovariance matrices (Bie and Moor, 2003; Hardoon et al., 2004).", "startOffset": 164, "endOffset": 206}, {"referenceID": 30, "context": ",vL] are the CCA directions that project the DNN outputs and (rx, ry) > 0 are regularization parameters added to the diagonal of the sample autocovariance matrices (Bie and Moor, 2003; Hardoon et al., 2004).", "startOffset": 164, "endOffset": 206}, {"referenceID": 1, "context": "While this is in principle not needed, it is crucial for practical algorithmic implementations such as ours, and matches the original formulation of DCCA (Andrew et al., 2013).", "startOffset": 154, "endOffset": 175}, {"referenceID": 1, "context": ", 2015a, with the gradient formulas given as in Andrew et al., 2013). Intuitively, this approach works because a large minibatch 3. The authors also propose a bimodal deep autoencoder combining DNN transformed features from both views; this model is more natural for the multimodal fusion setting, where both views are available at test time, but can also be used in our multi-view setting (see Ngiam et al., 2011). Empirically, however, Ngiam et al. (2011) report that SplitAE tends to work better in the multi-view setting than bimodal autoencoders.", "startOffset": 48, "endOffset": 458}, {"referenceID": 13, "context": "Interpretations CCA maximizes the mutual information between the projected views for certain distributions (Borga, 2001), while training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between inputs and learned features (Vincent et al.", "startOffset": 107, "endOffset": 120}, {"referenceID": 70, "context": "Interpretations CCA maximizes the mutual information between the projected views for certain distributions (Borga, 2001), while training an autoencoder to minimize reconstruction error amounts to maximizing a lower bound on the mutual information between inputs and learned features (Vincent et al., 2010).", "startOffset": 283, "endOffset": 305}, {"referenceID": 30, "context": "5 Minimum-distance autoencoders (DistAE) The CCA objective can be seen as minimizing the distance between the learned projections of the two views, while satisfying the whitening constraints for the projections (Hardoon et al., 2004).", "startOffset": 211, "endOffset": 233}, {"referenceID": 65, "context": "There has also been work on multi-view feature learning using deep Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014).", "startOffset": 86, "endOffset": 141}, {"referenceID": 64, "context": "There has also been work on multi-view feature learning using deep Boltzmann machines (Srivastava and Salakhutdinov, 2014; Sohn et al., 2014).", "startOffset": 86, "endOffset": 141}, {"referenceID": 8, "context": ", depth is the common cause for adjacent patches of images), Becker and Hinton (1992) propose to maximize a sample-based estimate of mutual information between outputs of neural networks for the two views.", "startOffset": 61, "endOffset": 86}, {"referenceID": 8, "context": ", depth is the common cause for adjacent patches of images), Becker and Hinton (1992) propose to maximize a sample-based estimate of mutual information between outputs of neural networks for the two views. In this work, the 1-dimensional output of each network can be considered to be \u201cinternal teaching signals\u201d for the other. It is less straightforward to extend their sample-based estimator of mutual information to higher dimensions, while the CCA objective is always closely related to maximal mutual information between the views (under the joint multivariate Gaussian distributions of the inputs, see Borga, 2001). Lai and Fyfe (1999) propose to optimize the correlation (rather than canonical correlation) between the outputs of networks for each view, subject to scale constraints on each output dimension.", "startOffset": 61, "endOffset": 642}, {"referenceID": 8, "context": ", depth is the common cause for adjacent patches of images), Becker and Hinton (1992) propose to maximize a sample-based estimate of mutual information between outputs of neural networks for the two views. In this work, the 1-dimensional output of each network can be considered to be \u201cinternal teaching signals\u201d for the other. It is less straightforward to extend their sample-based estimator of mutual information to higher dimensions, while the CCA objective is always closely related to maximal mutual information between the views (under the joint multivariate Gaussian distributions of the inputs, see Borga, 2001). Lai and Fyfe (1999) propose to optimize the correlation (rather than canonical correlation) between the outputs of networks for each view, subject to scale constraints on each output dimension. Instead of directly solving this constrained formulation, the authors apply Lagrangian relaxation and solve the resulting unconstrained objective using SGD. Note, however, that their objective is different from that of CCA, as there are no constraints that the learned dimensions within each view be uncorrelated. Hsieh (2000) proposes a neural network-based model involving three modules: one module for extracting a pair of maximally correlated one-dimensional features for the two views; and a second and third module for reconstructing the original inputs of the two views from the learned features.", "startOffset": 61, "endOffset": 1143}, {"referenceID": 8, "context": ", depth is the common cause for adjacent patches of images), Becker and Hinton (1992) propose to maximize a sample-based estimate of mutual information between outputs of neural networks for the two views. In this work, the 1-dimensional output of each network can be considered to be \u201cinternal teaching signals\u201d for the other. It is less straightforward to extend their sample-based estimator of mutual information to higher dimensions, while the CCA objective is always closely related to maximal mutual information between the views (under the joint multivariate Gaussian distributions of the inputs, see Borga, 2001). Lai and Fyfe (1999) propose to optimize the correlation (rather than canonical correlation) between the outputs of networks for each view, subject to scale constraints on each output dimension. Instead of directly solving this constrained formulation, the authors apply Lagrangian relaxation and solve the resulting unconstrained objective using SGD. Note, however, that their objective is different from that of CCA, as there are no constraints that the learned dimensions within each view be uncorrelated. Hsieh (2000) proposes a neural network-based model involving three modules: one module for extracting a pair of maximally correlated one-dimensional features for the two views; and a second and third module for reconstructing the original inputs of the two views from the learned features. In this model, the feature dimensions can be learned one after another, each learned using as input the reconstruction residual from previous dimensions. This approach is intuitively similar to CorrAE and DCCA, but the three modules are each trained separately, so there is no unified objective. Kim et al. (2012) propose an algorithm that first uses deep belief networks and the autoencoder objective to extract features for two languages independently, and then applies linear CCA to the learned features (activations at the bottleneck layer of the autoencoders) to learn the final representation.", "startOffset": 61, "endOffset": 1734}, {"referenceID": 0, "context": "One popular nonlinear approach is kernel CCA (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004), corresponding to choosing f(x) and g(y) to be feature maps induced by positive definite kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7), respectively (e.", "startOffset": 45, "endOffset": 150}, {"referenceID": 53, "context": "One popular nonlinear approach is kernel CCA (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004), corresponding to choosing f(x) and g(y) to be feature maps induced by positive definite kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7), respectively (e.", "startOffset": 45, "endOffset": 150}, {"referenceID": 5, "context": "One popular nonlinear approach is kernel CCA (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004), corresponding to choosing f(x) and g(y) to be feature maps induced by positive definite kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7), respectively (e.", "startOffset": 45, "endOffset": 150}, {"referenceID": 30, "context": "One popular nonlinear approach is kernel CCA (KCCA, Lai and Fyfe, 2000; Akaho, 2001; Melzer et al., 2001; Bach and Jordan, 2002; Hardoon et al., 2004), corresponding to choosing f(x) and g(y) to be feature maps induced by positive definite kernels kx(\u00b7, \u00b7) and ky(\u00b7, \u00b7), respectively (e.", "startOffset": 45, "endOffset": 150}, {"referenceID": 47, "context": "Two widely used approximation techniques are random Fourier features (Lopez-Paz et al., 2014) and the Nystr\u00f6m approximation (Williams and Seeger, 2001).", "startOffset": 69, "endOffset": 93}, {"referenceID": 78, "context": ", 2014) and the Nystr\u00f6m approximation (Williams and Seeger, 2001).", "startOffset": 38, "endOffset": 65}, {"referenceID": 5, "context": "Other approximation techniques such as incomplete Cholesky decomposition (Bach and Jordan, 2002), partial Gram-Schmidt (Hardoon et al.", "startOffset": 73, "endOffset": 96}, {"referenceID": 30, "context": "Other approximation techniques such as incomplete Cholesky decomposition (Bach and Jordan, 2002), partial Gram-Schmidt (Hardoon et al., 2004), incremental SVD (Arora and Livescu, 2012) have also been proposed and applied to KCCA.", "startOffset": 119, "endOffset": 141}, {"referenceID": 2, "context": ", 2004), incremental SVD (Arora and Livescu, 2012) have also been proposed and applied to KCCA.", "startOffset": 25, "endOffset": 50}, {"referenceID": 49, "context": "Although recently iterative algorithms have been introduced for very large CCA problems (Lu and Foster, 2014), they are aimed at sparse matrices and do not have a natural out-of-sample extension.", "startOffset": 88, "endOffset": 109}, {"referenceID": 79, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 62, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 7, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 68, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 61, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 27, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 36, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 25, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 20, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 76, "context": "Metric learning often uses side information in the form of pairs of similar/dissimilar samples, which may be known a priori or derived from class labels (Xing et al., 2003; Shental et al., 2002; Bar-Hillel et al., 2005; Tsang and Kwok, 2003; Schultz and Joachims, 2004; Goldberger et al., 2005; Hoi et al., 2006; Globerson and Roweis, 2006; Davis et al., 2007; Weinberger and Saul, 2009).", "startOffset": 153, "endOffset": 387}, {"referenceID": 60, "context": "In this sense the CCA setting is more similar to that of Shental et al. (2002) and Bar-Hillel et al.", "startOffset": 57, "endOffset": 79}, {"referenceID": 7, "context": "(2002) and Bar-Hillel et al. (2005), which use only side information regarding groups of similar samples (\u201cchunklets\u201d) for single-view data.", "startOffset": 11, "endOffset": 36}, {"referenceID": 7, "context": "(2002) and Bar-Hillel et al. (2005), which use only side information regarding groups of similar samples (\u201cchunklets\u201d) for single-view data. For multi-view data, Globerson et al. (2007) propose an algorithm for learning Euclidean embeddings by defining a joint or conditional distribution of the views based on Euclidean distance 9", "startOffset": 11, "endOffset": 186}, {"referenceID": 31, "context": "Recently, there has been increasing interest in learning (multi-view) representations using contrastive losses which aim to enforce that distances between dissimilar pairs are larger than distances between similar pairs by some margin (Hermann and Blunsom, 2014; Huang et al., 2013).", "startOffset": 235, "endOffset": 282}, {"referenceID": 39, "context": "Recently, there has been increasing interest in learning (multi-view) representations using contrastive losses which aim to enforce that distances between dissimilar pairs are larger than distances between similar pairs by some margin (Hermann and Blunsom, 2014; Huang et al., 2013).", "startOffset": 235, "endOffset": 282}, {"referenceID": 66, "context": "Finally, CCA has a connection with the information bottleneck method (Tishby et al., 1999).", "startOffset": 69, "endOffset": 90}, {"referenceID": 19, "context": "Indeed, in the case of Gaussian variables, the information bottleneck method finds the same subspace as CCA (Chechik et al., 2005).", "startOffset": 108, "endOffset": 130}, {"referenceID": 47, "context": "The first implementation, denoted FKCCA, uses random Fourier features (Lopez-Paz et al., 2014) and the second implementation, denoted NKCCA, uses the Nystr\u00f6m approximation (Williams and Seeger, 2001).", "startOffset": 70, "endOffset": 94}, {"referenceID": 78, "context": ", 2014) and the second implementation, denoted NKCCA, uses the Nystr\u00f6m approximation (Williams and Seeger, 2001).", "startOffset": 85, "endOffset": 112}, {"referenceID": 82, "context": "2, in both FKCCA/NKCCA, we transform the original inputs to an M -dimensional feature space where the inner products between samples approximate the kernel similarities (Yang et al., 2012).", "startOffset": 169, "endOffset": 188}, {"referenceID": 46, "context": "1 Noisy MNIST digits In this task, we generate two-view data using the MNIST dataset (LeCun et al., 1998), which consists of 28 \u00d7 28 grayscale digit images, with 60K/10K images for training/testing.", "startOffset": 85, "endOffset": 105}, {"referenceID": 56, "context": "We use spectral clustering (Ng et al., 2002) so as to account for possibly non-convex cluster shapes.", "startOffset": 27, "endOffset": 44}, {"referenceID": 80, "context": "We measure clustering performance with two criteria, clustering accuracy (AC) and normalized mutual information (NMI) (Xu et al., 2003; Manning et al., 2008).", "startOffset": 118, "endOffset": 157}, {"referenceID": 51, "context": "We measure clustering performance with two criteria, clustering accuracy (AC) and normalized mutual information (NMI) (Xu et al., 2003; Manning et al., 2008).", "startOffset": 118, "endOffset": 157}, {"referenceID": 55, "context": "a linear assignment problem using the Hungarian algorithm (Munkres, 1957).", "startOffset": 58, "endOffset": 73}, {"referenceID": 33, "context": "The networks (f ,p) are pre-trained in a layerwise manner using restricted Boltzmann machines (Hinton and Salakhutdinov, 2006) and similarly for (g,q) with inputs from the corresponding view.", "startOffset": 94, "endOffset": 126}, {"referenceID": 17, "context": "We train one-versus-one linear SVMs (Chang and Lin, 2011) on the projected training set (now using the ground truth labels), and test on the projected test set, while using the projected tuning set for selecting the SVM hyperparameter (the penalty parameter for hinge loss).", "startOffset": 36, "endOffset": 57}, {"referenceID": 45, "context": "For KCCAs, we fix both rx and ry at a small positive value of 10\u22124 (as suggested by Lopez-Paz et al. (2014), FKCCA is robust to rx, ry), and do grid search for the Gaussian kernel width over {2, 3, 4, 5, 6, 8, 10, 15, 20} for view 1 and {2.", "startOffset": 84, "endOffset": 108}, {"referenceID": 72, "context": "We run the t-SNE implementation of Vladymyrov and Carreira-Perpi\u00f1\u00e1n (2012) on projected test images for 300 iterations with a perplexity of 20.", "startOffset": 35, "endOffset": 75}, {"referenceID": 77, "context": "2 Acoustic-articulatory data for speech recognition We next experiment with the Wisconsin X-Ray Microbeam (XRMB) corpus (Westbury, 1994) of simultaneously recorded speech and articulatory measurements from 47 American English speakers.", "startOffset": 120, "endOffset": 136}, {"referenceID": 3, "context": "Multi-view feature learning via CCA/KCCA/DCCA has previously been shown to improve phonetic recognition performance when tested on audio alone (Arora and Livescu, 2013; Wang et al., 2015a).", "startOffset": 143, "endOffset": 188}, {"referenceID": 2, "context": "Multi-view feature learning via CCA/KCCA/DCCA has previously been shown to improve phonetic recognition performance when tested on audio alone (Arora and Livescu, 2013; Wang et al., 2015a). We follow the setup of Arora and Livescu (2013) and use the learned features for speakerindependent phonetic recognition.", "startOffset": 144, "endOffset": 238}, {"referenceID": 2, "context": "Multi-view feature learning via CCA/KCCA/DCCA has previously been shown to improve phonetic recognition performance when tested on audio alone (Arora and Livescu, 2013; Wang et al., 2015a). We follow the setup of Arora and Livescu (2013) and use the learned features for speakerindependent phonetic recognition. Similarly to Arora and Livescu (2013), the inputs to multi-view feature learning are acoustic features (39D features consisting of mel frequency cepstral coefficients (MFCCs) and their first and second derivatives) and articulatory features (horizontal/vertical displacement of 8 pellets attached to different parts of the vocal tract) concatenated over a 7-frame window around each frame, giving 273D acoustic inputs and 112D articulatory inputs for each view.", "startOffset": 144, "endOffset": 350}, {"referenceID": 32, "context": "All of the learned feature types are used in a tandem approach (Hermansky et al., 2000), i.", "startOffset": 63, "endOffset": 87}, {"referenceID": 2, "context": "The recognizer has one 3-state left-to-right HMM per phone, using the same language model as in Arora and Livescu (2013). For each fold, we select the best hyperparameters based on recognition accuracy on the tuning speakers, and use the corresponding learned model for the test speakers.", "startOffset": 96, "endOffset": 121}, {"referenceID": 2, "context": "The recognizer has one 3-state left-to-right HMM per phone, using the same language model as in Arora and Livescu (2013). For each fold, we select the best hyperparameters based on recognition accuracy on the tuning speakers, and use the corresponding learned model for the test speakers. As before, models based on neural networks are trained via SGD with the optimization parameters tuned by grid search. Here we do not use pre-training for weight initialization. A small weight decay parameter of 5\u00d7 10\u22124 is used for all layers. For each algorithm, the feature dimensionality L is tuned over {30, 50, 70}. For DNN-based models, we use hidden layers of 1500 ReLUs. For DCCA, we vary the network depths (up to 3 nonlinear hidden layers) of each view. In the best DCCA architecture, f has 3 ReLU layers of 1500 units followed by a linear output layer while g has only a linear output layer. For CorrAE/DistAE/DCCAE, we use the same architecture of DCCA for the encoders, and we set the decoders to have symmetric architectures to the encoders. For this domain, we find that the best choice of architecture for the encoders/decoders for View 2 is linear while for View 1 it is typically three layers deep. For SplitAE, the encoder f is similarly deep and the View 1 decoder p has the symmetric architecture, while its View 2 decoder q was set to linear to match the best choice for the other methods. We fix (rx, ry) to small values as before. The trade-off parameter \u03bb is tuned for each algorithm by grid search. For FKCCA, we find it important to use a large number of random features M to get a competitive result, consistent with the findings of Huang et al. (2014) when using random Fourier features for speech data.", "startOffset": 96, "endOffset": 1669}, {"referenceID": 23, "context": "We follow the setup of Faruqui and Dyer (2014) and use as inputs 640-dimensional monolingual word vectors trained via latent semantic analysis on the WMT 2011 monolingual news corpora and use the same 36K English-German word pairs for multi-view learning.", "startOffset": 23, "endOffset": 47}, {"referenceID": 23, "context": "We follow the setup of Faruqui and Dyer (2014) and use as inputs 640-dimensional monolingual word vectors trained via latent semantic analysis on the WMT 2011 monolingual news corpora and use the same 36K English-German word pairs for multi-view learning. The learned mappings are applied to the original English word embeddings (180K words) and the projections are used for evaluation. We evaluate learned features on two groups of tasks. The first group consists of the four word similarity tasks from Faruqui and Dyer (2014): WS-353 and the two splits WS-SIM and WS-REL, RG-65, MC-30, and MTurk-287.", "startOffset": 23, "endOffset": 528}, {"referenceID": 23, "context": "We follow the setup of Faruqui and Dyer (2014) and use as inputs 640-dimensional monolingual word vectors trained via latent semantic analysis on the WMT 2011 monolingual news corpora and use the same 36K English-German word pairs for multi-view learning. The learned mappings are applied to the original English word embeddings (180K words) and the projections are used for evaluation. We evaluate learned features on two groups of tasks. The first group consists of the four word similarity tasks from Faruqui and Dyer (2014): WS-353 and the two splits WS-SIM and WS-REL, RG-65, MC-30, and MTurk-287. The second group of tasks uses the adjective-noun (AN) and verbobject (VN) subsets from the bigram similarity dataset of Mitchell and Lapata (2010), and tuning and test splits (of size 649/1972) for each subset (we exclude the noun-noun subset as we find that the NN human annotations often reflect \u201ctopical\u201d rather than \u201cfunctional\u201d similarity).", "startOffset": 23, "endOffset": 751}, {"referenceID": 11, "context": "of the bigram, as done in prior work (Blacoe and Lapata, 2012).", "startOffset": 37, "endOffset": 62}, {"referenceID": 14, "context": "It is clear that for small minibatches (100, 200), the objective quickly plateaus at a low value, whereas for large enough minibatch size, there is always a steep increase at the beginning, which is a known advantage of stochastic firstorder algorithms (Bottou and Bousquet, 2008), and a wide range of learning rate/momentum give very similar results.", "startOffset": 253, "endOffset": 280}, {"referenceID": 28, "context": "(2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al.", "startOffset": 149, "endOffset": 191}, {"referenceID": 49, "context": "(2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al.", "startOffset": 149, "endOffset": 191}, {"referenceID": 50, "context": "(2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al., 2015) for CCA.", "startOffset": 219, "endOffset": 236}, {"referenceID": 13, "context": "It is clear that for small minibatches (100, 200), the objective quickly plateaus at a low value, whereas for large enough minibatch size, there is always a steep increase at the beginning, which is a known advantage of stochastic firstorder algorithms (Bottou and Bousquet, 2008), and a wide range of learning rate/momentum give very similar results. The reason for such behavior is that the stochastic estimate of the DCCA objective becomes more accurate as minibatch size n increases. We provide theoretical analysis of the error between the true objective and its stochastic estimate in Appendix A. Recently, Wang et al. (2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al.", "startOffset": 254, "endOffset": 633}, {"referenceID": 13, "context": "It is clear that for small minibatches (100, 200), the objective quickly plateaus at a low value, whereas for large enough minibatch size, there is always a steep increase at the beginning, which is a known advantage of stochastic firstorder algorithms (Bottou and Bousquet, 2008), and a wide range of learning rate/momentum give very similar results. The reason for such behavior is that the stochastic estimate of the DCCA objective becomes more accurate as minibatch size n increases. We provide theoretical analysis of the error between the true objective and its stochastic estimate in Appendix A. Recently, Wang et al. (2015c) have proposed a nonlinear orthogonal iterations (NOI) algorithm for the DCCA objective which extends the alternating least squares procedure (Golub and Zha, 1995; Lu and Foster, 2014) and its stochastic version (Ma et al., 2015) for CCA. Each iteration of the NOI algorithm adaptively estimates the covariance matrices of the projections of each view, whitens the projections of a minibatch using the estimated covariance matrices, and takes a gradient step over DNN weight parameters of the nonlinear least squares problems of regressing each view\u2019s input against the whitened projection of the other view for the minibatch. The advantage of NOI is that it performs well with smaller minibatch sizes and thus reduces memory consumption. Wang et al. (2015c) have shown that NOI can achieve the same objective value as STO using smaller minibatches.", "startOffset": 254, "endOffset": 1391}, {"referenceID": 1, "context": "Our split of the data is the same as the one used by Andrew et al. (2013). We note that Lopez-Paz et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 1, "context": "Our split of the data is the same as the one used by Andrew et al. (2013). We note that Lopez-Paz et al. (2014) used the same speaker in their experiments, but they randomly shuffled all 50K frames before creating the splits.", "startOffset": 53, "endOffset": 112}, {"referenceID": 1, "context": "Our split of the data is the same as the one used by Andrew et al. (2013). We note that Lopez-Paz et al. (2014) used the same speaker in their experiments, but they randomly shuffled all 50K frames before creating the splits. We suspect that DCCA (as well as their own algorithm) were under-tuned in their experiments. We ran experiments on a randomly shuffled dataset with careful tuning of kernel widths for FKCCA/NKCCA (using rank M = 6000) and obtained canonical correlations of 99.2/105.6/107.6 for FKCCA/NKCCA/DCCA, which are better than those reported in Lopez-Paz et al. (2014). 20", "startOffset": 53, "endOffset": 586}, {"referenceID": 60, "context": "We use the L-BFGS implementation of Schmidt (2012), which includes a good line-search procedure.", "startOffset": 36, "endOffset": 51}, {"referenceID": 46, "context": "We have studied these objectives in the context of DNNs, but we expect that the same trends should apply also to other network architectures such as convolutional (LeCun et al., 1998) and recurrent (Elman, 1990; Hochreiter and Schmidhuber, 1997) networks, and this is one direction for future work.", "startOffset": 163, "endOffset": 183}, {"referenceID": 22, "context": ", 1998) and recurrent (Elman, 1990; Hochreiter and Schmidhuber, 1997) networks, and this is one direction for future work.", "startOffset": 22, "endOffset": 69}, {"referenceID": 34, "context": ", 1998) and recurrent (Elman, 1990; Hochreiter and Schmidhuber, 1997) networks, and this is one direction for future work.", "startOffset": 22, "endOffset": 69}, {"referenceID": 18, "context": "CCA is expected to perform well for clustering and classification when the two views are uncorrelated given the class label (Chaudhuri et al., 2009).", "startOffset": 124, "endOffset": 148}, {"referenceID": 41, "context": "The usefulness of CCA in such settings has previously been analyzed theoretically (Kakade and Foster, 2007; McWilliams et al., 2013) and has begun to be explored experimentally (Arora and Livescu, 2014).", "startOffset": 82, "endOffset": 132}, {"referenceID": 52, "context": "The usefulness of CCA in such settings has previously been analyzed theoretically (Kakade and Foster, 2007; McWilliams et al., 2013) and has begun to be explored experimentally (Arora and Livescu, 2014).", "startOffset": 82, "endOffset": 132}, {"referenceID": 4, "context": ", 2013) and has begun to be explored experimentally (Arora and Livescu, 2014).", "startOffset": 52, "endOffset": 77}, {"referenceID": 13, "context": "Our analysis is based on the following formulation of the linear CCA solution (Borga, 2001).", "startOffset": 78, "endOffset": 91}], "year": 2016, "abstractText": "We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for learning while only one view is available for downstream tasks. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them empirically on image, speech, and text tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). We also explore a stochastic optimization procedure for minibatch correlation-based objectives and discuss the time/performance trade-offs for kernel-based and neural network-based implementations.", "creator": "LaTeX with hyperref package"}}}