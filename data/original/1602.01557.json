{"id": "1602.01557", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "An ensemble diversity approach to supervised binary hashing", "abstract": "Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.", "histories": [["v1", "Thu, 4 Feb 2016 04:59:54 GMT  (615kb)", "http://arxiv.org/abs/1602.01557v1", "17 pages, 5 figures"]], "COMMENTS": "17 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV math.OC stat.ML", "authors": ["miguel \u00e1 carreira-perpi\u00f1\u00e1n", "ramin raziperchikolaei"], "accepted": true, "id": "1602.01557"}, "pdf": {"name": "1602.01557.pdf", "metadata": {"source": "CRF", "title": "An Ensemble Diversity Approach to Supervised Binary Hashing", "authors": ["Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n", "Ramin Raziperchikolaei"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n01 55\n7v 1\n[ cs\n.L G\n] 4\nF eb"}, {"heading": "1 Introduction and related work", "text": "Information retrieval tasks such as searching for a query image or document in a database are essentially a nearest-neighbor search (Shakhnarovich et al., 2006). When the dimensionality of the query and the size of the database is large, approximate search is necessary. We focus on binary hashing (Grauman and Fergus, 2013), where the query and database are mapped onto low-dimensional binary vectors, where the search is performed. This has two speedups: computing Hamming distances (with hardware support) is much faster than computing distances between high-dimensional floating-point vectors; and the entire database becomes much smaller, so it may reside in fast memory rather than disk (for example, a database of 1 billion real vectors of dimension 500 takes 2 TB in floating point but 8 GB as 64-bit codes).\nConstructing hash functions that do well in retrieval measures such as precision and recall is usually done by optimizing an affinity-based objective function that relates Hamming distances to supervised neighborhood information in a training set. Many such objective functions have the form of a sum of pairwise terms that indicate whether the training points xn and xm are neighbors:\nmin h L(h) =\nN \u2211\nn,m=1\nL(zn, zm; ynm) where\n{\nzm = h(xm)\nzn = h(xn).\nHere, X = (x1, . . . ,xN ) is the dataset of high-dimensional feature vectors (e.g., SIFT features of an image), h: RD \u2192 {\u22121,+1}b are b binary hash functions and z = h(x) is the b-bit code vector for input x \u2208 RD, minh means minimizing over the parameters of the hash function h (e.g. over the weights of a linear SVM), and L(\u00b7) is a loss function that compares the codes for two images (often through their Hamming distance \u2016zn \u2212 zm\u2016) with the ground-truth value ynm that measures the affinity in the original space between the two images xn and xm (distance, similarity or other measure of neighborhood). The sum is often restricted to a subset of image pairs (n,m) (for example, within the k nearest neighbors of each other in the original space), to keep the runtime low. The output of the algorithm is the hash function h and the binary codes Z = (z1, . . . , zN ) for the training points, where zn = h(xn) for n = 1, . . . , N . Examples of these objective functions are Supervised Hashing with Kernels (KSH) (Liu et al., 2012), Binary Reconstructive Embeddings\n(BRE) (Kulis and Darrell, 2009) and the binary Laplacian loss (an extension of the Laplacian Eigenmaps objective; Belkin and Niyogi, 2003):\nLKSH(zn, zm; ynm) = (z T nzm \u2212 bynm) 2 (1) LBRE(zn, zm; ynm) = ( 1\nb \u2016zn \u2212 zm\u2016\n2 \u2212 ynm )2 (2)\nLLAP(zn, zm; ynm) = ynm \u2016zn \u2212 zm\u2016 2\n(3)\nwhere for KSH ynm is 1 if xn, xm are similar and \u22121 if they are dissimilar; for BRE ynm = 1\n2 \u2016xn \u2212 xm\u2016\n2\n(where the dataset X is scaled or normalized so the Euclidean distances are in [0, 1]); and for the Laplacian loss ynm > 0 if xn, xm are similar and < 0 if they are dissimilar (\u201cpositive\u201d and \u201cnegative\u201d neighbors). Other examples of these objective functions include models developed for dimension reduction, be they spectral such as Locally Linear Embedding (Roweis and Saul, 2000) or Anchor Graphs (Liu et al., 2011), or nonlinear such as the Elastic Embedding (Carreira-Perpin\u0303a\u0301n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al., 2012). They all can produce good hash functions. We will focus on the Laplacian loss in this paper.\nIn designing these objective functions, one needs to eliminate two types of trivial solutions. 1) In the Laplacian loss, mapping all points to the same code, i.e., z1 = \u00b7 \u00b7 \u00b7 = zN , is the global optimum of the positive neighbors term (this also arises if the codes zn are real-valued, as in Laplacian eigenmaps). This can be avoided by having negative neighbors. 2) Having all hash functions (all b bits of each vector) being identical to each other, i.e., zn1 = \u00b7 \u00b7 \u00b7 = znb for each n = 1, . . . , N . This can be avoided by introducing constraints, penalty terms or other mathematical devices that couple the b bit dimensions. For example, in the Laplacian loss (3) we can encourage codes to be orthogonal through a constraint ZTZ = NI (Weiss et al., 2009) or a penalty term \u2016ZTZ\u2212NI\u20162 (the latter requiring a hyperparameter that controls the weight of the penalty) (Ge et al., 2014), although this generates dense matrices of N \u00d7N . In the KSH or BRE losses (1), squaring the dot product or Hamming distance between the codes couples the b bits.\nAn important downside of these approaches is the difficulty of their optimization. This is due to the fact that the objective function is nonsmooth (implicitly discrete) because of the binary output of the hash function. There is a large number of such binary variables (bN), a larger number of pairwise interactions (O(N2), less if using sparse neighborhoods) and the variables are coupled by the said constraints or penalty terms. The optimization is approximated in different ways. Most papers ignore the binary nature of the Z codes and optimize over them as real values, then binarize them by truncation (possibly with an optimal rotation; Yu and Shi, 2003; Gong et al., 2013), and finally fit a classifier (e.g. linear SVM) to each of the b bits separately. For example, for the Laplacian loss with constraints this involves solving an eigenproblem on Z as in Laplacian eigenmaps (Belkin and Niyogi, 2003; Weiss et al., 2009; Zhang et al., 2010), or approximated using landmarks (Liu et al., 2011). This is fast, but relaxing the codes in the optimization is generally far from optimal. Some recent papers try to respect the binary nature of the codes during their optimization, using techniques such as alternating optimization, min-cut and GraphCut (Boykov et al., 2001; Lin et al., 2014b; Ge et al., 2014) or others (Lin et al., 2013), and then fit the classifiers, or use alternating optimization directly on the hash function parameters (Liu et al., 2012). Even more recently, one can optimize jointly over the binary codes and hash functions (Ge et al., 2014; Carreira-Perpin\u0303a\u0301n and Raziperchikolaei, 2015; Raziperchikolaei and Carreira-Perpin\u0303a\u0301n, 2015). Most of these approaches are slow and limited to small datasets (a few thousand points) because of the quadratic number of pairwise terms in the objective.\nWe propose a different, much simpler approach. Rather than coupling the b hash functions into a single objective function, we train each hash function independently from each other and using a single-bit objective function of the same form. We show that we can avoid trivial solutions by injecting diversity into each hash function\u2019s training using techniques inspired from classifier ensemble learning. Section 2 discusses relevant ideas from the ensemble learning literature, section 3 describes our independent Laplacian hashing algorithm, section 4 gives evidence with image retrieval datasets that this simple approach indeed works very well, and section 5 further discusses the connection between hashing and ensembles."}, {"heading": "2 Ideas from learning classifier ensembles", "text": "At first sight, optimizing (3) without constraints does not seem like a good idea: since \u2016zn \u2212 zm\u2016 2 separates over the b bits, we obtain b independent identical objectives, one over each hash function, and so they all have the same global optimum. And, if all hash functions are equal, they are equivalent to using just one of them, which will give a much lower precision/recall. In fact, the very same issue arises when training an ensemble of classifiers (Dietterich, 2000; Zhou, 2012; Kuncheva, 2014). Here, we have a training set of input vectors and output class labels, and want to train several classifiers whose outputs are then combined (usually by majority vote). If the classifiers are all equal, we gain nothing over a single classifier. Hence, it is necessary to introduce diversity among the classifiers so that they disagree in their predictions. The ensemble learning literature has identified several mechanisms to inject diversity. The most important ones that apply to our binary hashing setting are as follows:\nUsing different data for each classifier This can be done by: 1) Using different feature subsets for each classifier. This works best if the features are somewhat redundant. 2) Using different training sets for each classifier. This works best for unstable algorithms (whose resulting classifier is sensitive to small changes in the training data), such as decision trees or neural nets, and unlike linear or nearest neighbor classifiers. A prominent example is bagging (Breiman, 1996), which generates bootstrap datasets and trains a model on each.\nInjecting randomness in the training algorithm This is only possible if local optima exist (as for neural nets) or if the algorithm is randomized (as for decision trees). This can be done by using different initializations, adding noise to the updates or using different choices in the randomized operations in the algorithm (e.g. the choice of split in decision trees, as in random forests; Breiman, 2001).\nUsing different classifier models For example, different parameters (e.g. the number of neighbors in a nearest-neighbor classifier), different architectures (e.g. neural nets with different number of layers or hidden units), or different types of classifiers altogether.\nThere are other variations in addition to these techniques, as well as combinations of them."}, {"heading": "3 Independent Laplacian Hashing (ILH) with diversity", "text": "The connection of binary hashing with ensemble learning offers many possible options, in terms of the choice of type of hash function (\u201cbase learner\u201d), binary hashing (single-bit) objective function, optimization algorithm, and diversity mechanism. In this paper we focus on the following choices. We use linear and kernel SVMs as hash functions. Without loss of generality (see later), we use the Laplacian objective (3), which for a single bit takes the form\nE(z) =\nN \u2211\nn,m=1\nynm(zn \u2212 zm) 2\n{\nzn = h(xn) \u2208 {\u22121, 1} n = 1, . . . , N.\n(4)\nTo optimize it, we use a two-step approach, where we first optimize (4) over the N bits and then learn the hash function by fitting to it a binary classifier. (It is also possible to optimize over the hash function directly with the method of auxiliary coordinates; Carreira-Perpin\u0303a\u0301n and Raziperchikolaei, 2015; Raziperchikolaei and Carreira-Perpin\u0303a\u0301n, 2015, which essentially iterates over optimizing (4) and fitting the classifier.) The Laplacian objective (4) is NP-complete if we have negative neighbors (i.e., some ynm < 0). We approximately optimize it using a min-cut algorithm (as implemented by Boykov et al., 2001) applied in alternating fashion to submodular blocks as described in Lin et al. (2014a). This first partitions the N points into disjoint groups containing only nonnegative weights. Each group defines a submodular function (specifically, quadratic with nonpositive coefficients) whose global minimum can be found in polynomial time using min-cut. The order in which the groups are optimized over is randomized at each iteration (this improves over using a fixed order). The approximate optimizer found depends on the initial z \u2208 {\u22121, 1}N .\nFinally, we consider three types of diversity mechanism (as well as their combination):\nDifferent initializations (ILHi) Each hash function is initialized from a random N -bit vector z.\nDifferent training sets (ILHt) Each hash function uses a training set of N points that is different and (if possible) disjoint from that of other hash functions. We can afford to do this because in binary hashing the training sets are potentially very large, and the computational cost of the optimization limits the training sets to a few thousand points. Later we show this outperforms using bootstrapped training sets.\nDifferent feature subsets (ILHf) Each hash function is trained on a random subset of 1 \u2264 d \u2264 D features sampled without replacement (so the d features are distinct). The subsets corresponding to different hash functions may overlap.\nThese mechanisms are applicable to other objective functions beyond (4). We could also use the same training set but construct differently the weight matrix in (4) (e.g. using different numbers of positive and negative neighbors).\nEquivalence of objective functions in the single-bit case Several binary hashing objective functions that differ in the general case of b > 1 bits become essentially identical in the b = 1 case. For example, expanding the pairwise terms in (1)\u2013(3) (noting that z2n = 1 if zn \u2208 {\u22121,+1}):\nLKSH(zn, zm; ynm) = \u22122ynmznzm + constant LBRE(zn, zm; ynm) = \u22124(2\u2212 ynm)znzm + constant LLAP(zn, zm; ynm) = \u22122ynmznzm + constant.\nSo the Laplacian and KSH objectives are in fact identical, and all three can be written in the form of a binary quadratic function without linear term (or a Markov random field with quadratic potentials only):\nminzE(z) = z TAz with z \u2208 {\u22121,+1}N (5)\nwith an appropriate, data-dependent neighborhood symmetric matrix A of N \u00d7 N . This problem is NPcomplete in general (Garey and Johnson, 1979; Boros and Hammer, 2002; Kolmogorov and Zabih, 2003), when A has both positive and negative elements, as well as zeros. It is submodular if A has only nonpositive elements, in which case it is equivalent to a min-cut/max-flow problem and it can be solved in polynomial time (Boros and Hammer, 2002; Greig et al., 1989).\nMore generally, any objective function of a binary vector z that has the form E(z) = \u2211N\nn,m=1 fnm(zn, zm) and which only depends on Hamming distances between bits zn, zm can be written as fnm(zn, zm) = anmznzm+bnm. Even more, an arbitrary function of 3 binary variables that depends only on their Hamming distances can be written as a quadratic function of the 3 variables. However, for 4 variables or more this is not generally true (see appendix A).\nComputational advantages Training the hash functions independently has some important advantages. First, training the b functions can be parallelized perfectly. This is a speedup of one to two orders of magnitude for typical values of b (32 to 200 in our experiments). Coupled objective functions such as KSH do not exhibit obvious parallelism, because they are trained with alternating optimization, which is inherently sequential.\nSecond, even in a single processor, b binary optimizations over N variables each is generally easier than one binary optimization over bN variables. This is so because the search spaces contain b2N and 2bN states, resp., so enumeration is much faster in the independent case (even though it is still impractical). If using an approximate polynomial-time algorithm, the independent case is also faster if the runtime is superlinear on the number of variables: the asymptotic runtimes will be O(bN\u03b1) and O((bN)\u03b1) with \u03b1 > 1, respectively. This is the case for the best practical GraphCut (Boykov et al., 2001) and max-flow/min-cut algorithms (Cormen et al., 2009).\nThird, the solution exhibits \u201cnesting\u201d, that is, to get the solution for b + 1 bits we just need to take a solution with b bits and add one more bit (as happens with PCA). This is unlike most methods based on a coupled objective function (such as KSH), where the solution for b + 1 bits cannot be obtained by adding one more bit, we have to solve for b+ 1 bits from scratch.\nFor ILHf, both the training and test time are lower than if using all D features for each hash function. The test runtime for a query is d/D times smaller.\nModel selection for the number of bits b Selecting the number of bits (hash functions) to use has not received much attention in the binary hashing literature. The most obvious way to do this would be to maximize the precision on a test set over b (cross-validation) subject to b not exceeding a preset limit (so applying the hash function is fast with test queries). The nesting property of ILH makes this computationally easy: we simply keep adding bits until the test precision stabilizes or decreases, or until we reach the maximum b. We can still benefit from parallel processing: if P processors are available, we train P hash functions in parallel and evaluate their precision, also in parallel. If we still need to increase b, we train P more hash functions, etc."}, {"heading": "4 Experiments", "text": "We use the following labeled datasets (all using the Euclidean distance in feature space): (1) CIFAR (Krizhevsky, 2009) contains 60 000 images in 10 classes. We use D = 320 GIST features (Oliva and Torralba, 2001) from each image. We use 58 000 images for training and 2 000 for test. (2) Infinite MNIST (Loosli et al., 2007). We generated, using elastic deformations of the original MNIST handwritten digit dataset, 1 000 000 images for training and 2 000 for test, in 10 classes. We represent each image by a D = 784 vector of raw pixels. Appendix C contains experiments on additional datasets.\nBecause of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Liu et al., 2012; Lin et al., 2013; Ge et al., 2014). Unless otherwise indicated, we train the hash functions in a subset of 5 000 points of the training set, and report precision and recall by searching for a test query on the entire dataset (the base set). As hash functions (for each bit), we use linear SVMs (trained with LIBLINEAR; Fan et al., 2008) and kernel SVMs (with 500 basis functions centered at a random subset of training points).\nWe report precision and recall for the test set queries using as ground truth (set of true neighbors in original space) all the training points with the same label as the query. The retrieved set contains the k nearest neighbors of the query point in the Hamming space. We report precision for different values of k to test the robustness of different algorithms.\nDiversity mechanisms with ILH To understand the effect of diversity, we evaluate the 3 mechanisms ILHi, ILHt and ILHf, and their combination ILHitf, over a range of number of bits b (32 to 128) and training set size N (2 000 to 20 000). As baseline coupled objective, we use KSH (Liu et al., 2012) but using the same two-step training as ILH: first we find the codes using the alternating min-cut method described earlier (initialized from an all-ones code, and running one iteration of alternating min-cut) and then we fit the classifiers. This is faster and generally finds better optima than the original KSH optimization (Lin et al., 2014b). We denote it as KSHcut.\nFig. 1 shows the results. The clearly best diversity mechanism is ILHt, which works better than the other mechanisms, even when combined with them, and significantly better than KSHcut. We explain this as follows. Although all 3 mechanisms introduce diversity, ILHt has a distinct advantage (also over KSHcut): it effectively uses b times as much training data, because each hash function has its own disjoint dataset. Using bN training points in KSHcut would be orders of magnitude slower. ILHt is equal or even better than the combined ILHitf because 1) since there is already enough diversity in ILHt, the extra diversity from ILHi and ILHf does not help; 2) ILHf uses less data (it discards features), which can hurt the precision; this is also seen in fig. 2 (panel 2). The precision of all methods saturates as N increases; with b = 128 bits, ILHt achieves nearly maximum precision with only 5 000 points. In fact, if we continued to increase the per-bit training set size N in ILHt, eventually all bits would use the same training set (containing all available data), diversity would disappear and the precision would drop drastically to the precision of using a single bit (\u2248 12%). Practical image retrieval datasets are so large that this is unlikely to occur unless N is very large (which would make the optimization too slow anyway).\nLinear SVMs are very stable classifiers known to benefit less from ensembles than less stable classifiers such as decision trees or neural nets (Kuncheva, 2014). Remarkably, they strongly benefit from the ensemble in our case. This is because each hash function is solving a different classification problem (different output labels), so the resulting SVMs are in fact quite different from each other. The conclusions for kernel hash functions are similar. We tried two cases: all the hash functions using the same, common 500 centers for the\nradial basis functions vs each hash function using its own 500 centers. Nonlinear classifiers are less stable than linear ones. In our case they do not benefit much more than linear SVMs more from the diversity. They do achieve higher precision since they are more powerful models, particularly when using private centers.\nFig. 2 (panels 1\u20132) shows the results in ILHf of varying the number of features 1 \u2264 d \u2264 D used by each hash function. Intuitively, very low d is bad because each classifier receives too little information and will make near-random codes. Indeed, for low d the precision is comparable to that of LSH (random projections) in fig. 2 (panel 4). Very high d will also work badly because it would eliminate the diversity and drop to the precision of a single bit for d = D. This does not actually happen because there is an additional source of diversity: the randomization in the alternating min-cut iterations. This has an effect similar to that of ILHi, and indeed a comparable precision. The highest precision is achieved with a proportion d/D \u2248 50% for ILHf, indicating some redundancy in the features. When combined with the other diversity mechanisms (ILHitf, panel 2), the highest precision occurs for d = D, because diversity is already provided by the other mechanisms, and using more data is better.\nFig. 2 (panel 3) shows the results of constructing the b training sets for ILHt as a random sample from the base set such that they are \u201cbootstrapped\u201d (sampled with replacement), \u201cdisjoint\u201d (sampled without replacement) or \u201crandom\u201d (sampled without replacement but reset for each bit, so the training sets may overlap). As expected, \u201cdisjoint\u201d (closely followed by \u201crandom\u201d) is consistently and significantly better than \u201cbootstrap\u201d because it introduces more independence between the hash functions and learns from more data overall (since each hash function uses the same training set size N).\nPrecision as a function of b Fig. 2 (panel 4) shows the precision (in the test set) as a function of the number of bits b for ILHt, where the solution for b+1 bits is obtained by adding a new bit to the solution for b. Since the hash functions obtained depend on the order in which we add the bits, we show 5 such orders (red curves). Remarkably, the precision increases nearly monotonically and continues increasing beyond b = 200 bits (note the prediction error in bagging ensembles typically levels off after around 25\u201350 decision\ntrees; Kuncheva, 2014, p. 186). This is (at least partly) because the effective training set size is proportional to b. The variance in the precision decreases as b increases. In contrast, for KSHcut the variance is larger and the precision barely increases after b = 80. The higher variance for KSHcut is due to the fact that each b value involves training from scratch and we can converge to a relatively different local optimum. As with ILHt, adding LSH random projections (again 5 curves for different orders) increases precision monotonically, but can only reach a low precision at best, since it lacks supervision. We also show the curve for thresholded PCA (tPCA), whose precision tops at around b = 30 and decreases thereafter. A likely explanation is that high-order principal components essentially capture noise rather than signal, i.e., random variation in the data, and this produces random codes for those bits, which destroy neighborhood information. Bagging tPCA (Leng et al., 2014) does make tPCA improve monotonically with b, but the result is still far from competitive. The reason is that there is little diversity among the ensemble members, because the top principal components can be accurately estimated even from small samples. The result in fig. 2 uses tPCA ensembles where each member has 16 principal components, i.e., 16 bits. If using single-bit members, as with ILHt, the precision with b bits is barely better than with 1 bit.\nIs the precision gap between KSH and ILHt due to an incomplete optimization of the KSH objective, or to bad local optima? We verified that 1) random perturbations of the KSHcut optimum lower the precision; 2) optimizing KSHcut using the ILHt codes as initialization (\u201cKSHcut-ILHt\u201d curve) increases the precision but it still remains far from that of ILHt. This confirms that the optimization algorithm is doing its job, and that the ILHt diversity mechanism is superior to coupling the hash functions in a joint objective.\nAre the codes orthogonal? The result of learning binary hashing is b hash functions, represented by a matrix Wb\u00d7D of real weights for linear SVMs, and a matrix ZN\u00d7b of binary (\u22121,+1) codes for the entire dataset. We define a measure of code orthogonality as follows. Define b \u00d7 b matrices CZ = 1 N ZTZ for the codes and CW = WW T for the weights (assuming normalized SVM weights). Each C matrix has entries in [\u22121, 1], equal to a normalized dot product of codes or weight vectors, and diagonal entries equal to 1. (Note that any matrix SCS where S is diagonal with \u00b11 entries is equivalent, since reverting a hash function\u2019s output does not alter the Hamming distances.) Perfect orthogonality happens when C = I, and is encouraged (explicitly or not) by many binary hashing methods.\nFig. 3 shows this for ILHt in CIFAR (N = 58 000 training points of dimension D = 320) and Infinite MNIST (N = 1 000 000 training points of dimension D = 784). It plots CZ and CW as an image, as well as the histogram of the entries of CZ and CW. The histograms also contain, as a control, the histogram corresponding to normalized dot products of random vectors (of dimension N or D, respectively), which is known to tend to a delta function at 0 as the dimension grows. Although CW has some tendency to orthogonality as the number of bits b used increases, it is clear that, for both codes and weight vectors, the distribution of dot products is wide and far from strict orthogonality. Hence, enforcing orthogonality does not seem necessary to achieve good hash functions and codes.\nComparison with other binary hashing methods We compare with both the original KSH (Liu et al., 2012) and its min-cut optimization KSHcut (Lin et al., 2014b), and a representative subset of affinitybased and unsupervised hashing methods: Supervised Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009), Supervised Self-Taught Hashing (STH) (Zhang et al., 2010), Spectral Hashing (SH) (Weiss et al., 2009), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Autoencoder (BA) (Carreira-Perpin\u0303a\u0301n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008). We create affinities ynm for all the affinity-based methods using the dataset labels. For each training point xn, we use as similar neighbors 100 points with the same labels as xn; and as dissimilar neighbors 100 points chosen randomly among the points whose labels are different from that of xn. For all\ndatasets, all the methods are trained using a subset of 5 000 points. Given that KSHcut already performs well (Lin et al., 2014b) and that ILHt consistently outperforms it both in precision and runtime, we expect ILHt to be competitive with the state-of-the-art. Fig. 4 shows this is generally the case, particularly as the number of bits b increases, when ILHt beats all other methods, which are not able to increase precision as much as ILHt does.\nRuntime The runtime to train a single ILHt hash function (in a single processor) for CIFAR is as follows:\nNumber of points N 2 000 5 000 10 000 20 000 Time in seconds 1.2 2.8 7.1 22.5\nThis is much faster than other affinity-based hashing methods (for example, for 128 bits with 5 000 points, BRE did not converge after 12 hours). KSHcut is among the faster methods. Its runtime per min-cut pass over a single bit is comparable to ours, but it needs b sequential passes to complete just one alternating optimization iteration, while our b functions can be trained in parallel.\nSummary ILHt achieves a remarkably high precision compared to a coupled KSH objective using the same optimization algorithm but introducing diversity by feeding different data to independent hash functions rather than by jointly optimizing over them. It also compares well with state-of-the-art methods in precision/recall, being competitive if few bits are used and the clear winner as more bits are used, and is very fast and embarrassingly parallel."}, {"heading": "5 Discussion", "text": "We have revealed for the first time a connection between supervised binary hashing and ensemble learning that could open the door to many new hashing algorithms. Although we have focused on a specific objective (Laplacian) and identified as particularly successful with it a specific diversity mechanism (disjoint training sets), other choices may be better depending on the application. The core idea we propose is the independent training of the hash functions via the introduction of diversity by means other than coupling terms in the objective or constraints. This may come as a surprise in the area of learning binary hashing, where most work in the last few years has focused on proposing complex objective functions that couple all b hash functions and developing sophisticated optimization algorithms for them.\nAnother surprise is that orthogonality of the codes or hash functions seems unnecessary. ILHt creates codes and hash functions that do differ from each other but are far from being orthogonal, yet they achieve good precision that keeps growing as we add bits. Thus, introducing diversity through different training data seems a better mechanism to make hash functions differ than coupling the codes through an orthogonality constraint or otherwise. It is also far simpler and faster to train independent single-bit hash functions.\nA final surprise is that the wide variety of affinity-based objective functions in the b-bit case reduces to a binary quadratic problem in the 1-bit case regardless of the form of the b-bit objective (as long as it depends on Hamming distances only). In this sense, there is a unique objective in the 1-bit case.\nThere has been a prior attempt to use bagging (bootstrapped samples) with truncated PCA (Leng et al., 2014). Our experiments show that, while this improves truncated PCA, it performs poorly in supervised binary hashing. This is because PCA is unsupervised and does not use the user-provided similarity information, which may disagree with Euclidean distances in image space; and because estimating principal components from samples has low diversity. Also, PCA is computationally simple and there is little gain by bagging it, unlike the far more difficult optimization of supervised binary hashing.\nSome supervised binary hashing work (Liu et al., 2012; Wang et al., 2012) has proposed to learn the b hash functions sequentially, where the ith function has an orthogonality-like constraint to force it to differ from the previous functions. Hence, this does not learn the functions independently and can be seen as a greedy optimization of a joint objective over all b functions.\nBinary hashing does differ from ensemble learning in one important point: the predictions of the b classifiers (= b hash functions) are not combined into a single prediction, but are instead concatenated into a binary vector (which can take 2b possible values). The \u201clabels\u201d (the binary codes) for the \u201cclassifiers\u201d (the hash functions) are unknown, and are implicitly or explicitly learned together with the hash functions themselves. This means that well-known error decompositions such as the error-ambiguity decomposition (Krogh and Vedelsby, 1995) and the bias-variance decomposition (Geman et al., 1992) do not apply. Also, the real goal of binary hashing is to do well in information retrieval measures such as precision and recall, but hash functions do not directly optimize this. A theoretical understanding of why diversity helps in learning binary hashing is an important topic of future work.\nIn this respect, there is also a relation with error-correcting output codes (ECOC) (Dietterich and Bakiri, 1995), an approach for multiclass classification. In ECOC, we represent each of the K classes with a b-bit binary vector, ensuring that b is large enough for the vectors to be sufficiently separated in Hamming distance. Each bit corresponds to partitioning the K classes into two groups. We then train b binary classifiers, such as decision trees. Given a test pattern, we output as class label the one closest in Hamming distance to the b-bit output of the b classifiers. The redundant error-correcting codes allow for small errors in the individual classifiers and can improve performance. An ECOC can also be seen as an ensemble of classifiers where we manipulate the output targets (rather than the input features or training set) to obtain each classifier, and we apply majority vote on the final result (if the test output in classifier i is 1, then all classes associated with 1 get a vote). The main benefit of ECOC seems to be in variance reduction, as in other ensemble methods\n(James and Hastie, 1998). Binary hashing can be seen as an ECOC with N classes, one per training point, with the ECOC prediction for a test pattern (query) being the nearest-neighbor class codes in Hamming distance. However, unlike in ECOC, the binary hashing the codes are learned so they preserve neighborhood relations between training points. Also, while ideally all N codes should be different (since a collision makes two originally different patterns indistinguishable, which will degrade some searches), this is not guaranteed in binary hashing.\nA final, different example shows the important role of diversity, i.e., making the hash functions differ, in learning good hash functions. Some binary hashing methods optimize an objective essentially of the following form (Rastegari et al., 2015; Xia et al., 2015):\nmin W,B\n\u2016B\u2212WX\u2016 2 s.t. WTW = I, B \u2208 {\u22121,+1}bN (6)\nwhere W is a linear projection matrix of b\u00d7D. The idea is to force the projections to be as close as possible to binary values. The orthogonality constraint ensures that trivial solutions (which would make all b hash functions equal) are not optimal. Remarkably, the objective function (6) contains no explicit information about neighborhood preservation (as in affinity-based loss functions) or reconstruction of the input (as in autoencoders). Although orthogonal projections preserve Euclidean distances, this is not true if preserving only a few, binarized projections. Yet this can produce good hash functions if initialized from PCA or ITQ, which did learn projections that try to reconstruct the inputs optimally, and a local optimum of the (NP-complete) objective (6) may not be far from that. Thus, it would appear that part of the success of these approaches relies on the constraint providing a form of diversity among the hash functions."}, {"heading": "6 Conclusion", "text": "Much work in supervised binary hashing has focused on designing sophisticated objective functions of the hash functions that force them to compete with each other while trying to preserve neighborhood information. We have shown, surprisingly, that training hash functions independently is not just simpler, faster and parallel, but also can achieve better retrieval quality, as long as diversity is introduced into each hash function\u2019s objective function. This establishes a connection with ensemble learning and allows one to borrow techniques from it. We showed that having each hash function optimize a Laplacian objective on a disjoint subset of the data works well, and facilitates selecting the number of bits to use. Although our evidence is mostly empirical, the intuition behind it is sound and in agreement with the many results (also mostly empirical) showing the power of ensemble classifiers. The ensemble learning perspective suggests many ideas for future work, such as pruning a large ensemble or using other diversity techniques. It may also be possible to characterize theoretically the performance in precision of binary hashing depending on the diversity of the hash functions."}, {"heading": "A Equivalence of objective functions in the single-bit case: proofs", "text": "In the main paper, we state that, in the single bit case (b = 1), the Laplacian, KSH and BRE loss functions over the vector z of binary codes for each data point can be written in the form of a binary quadratic function without linear term (or a MRF with quadratic potentials only):\nmin z\nE(z) = zTAz with z \u2208 {\u22121,+1}N (7)\nwith an appropriate, data-dependent neighborhood symmetric matrix A of N \u00d7N . We can assume w.l.o.g. that ann = 0, i.e., the diagonal elements of A are zero, since any diagonal values simply add a constant to E(z).\nMore generally, consider an arbitrary objective function of a binary vector z \u2208 {\u22121,+1}N that has the\nform E(z) = \u2211N\nn,m=1 fnm(zn, zm) and which only depends on Hamming distances between bits zn, zm. This is the form of the affinity-based loss function used in many binary hashing papers, in the single-bit case. Each term of the function E(z) can be written as fnm(zn, zm) = anmznzm + bnm. This fact, already noted by Lin et al. (2013), is because a function of 2 binary variables f(x, y) can take 4 different values:\nx y f 1 1 a \u22121 1 b 1 \u22121 c \u22121 \u22121 d\nbut if f(x, y) only depends on the Hamming distance of x and y then we have a = d and b = c. This can be achieved by f(x, y) = 1\n2 (a\u2212 b)xy + 1 2 (a+ b), and the constant 1 2 (a+ b) can be ignored when optimizing.\nBy a similar argument we can prove that an arbitrary function of 3 binary variables that depends only on their Hamming distances can be written as a quadratic function of the 3 variables.\nHowever, this is not true in general. This can be seen by comparing the dimensions of the function spaces spanned by the arbitrary function and the quadratic function. Consider first a general quadratic function E(z) = 1\n2 zTAz+ bT z+ c of N binary variables z \u2208 {\u22121,+1}N . We can always take A symmetric (because\nzTAz = zT ( A+A T\n2\n)\nz) and absorb its diagonal terms into the constant c (because z2n = 1), so we can write\nw.l.o.g. E(z) = \u2211N n<m anmznzm + \u2211N n=1 bnzn + c. This has (n 2 + n + 2)/2 free parameters. The vector of 2n possible values of E for all possible binary vectors z is a linear function of these free parameters, Hence, the dimension of the space of all quadratic functions is at most (n2 + n+ 2)/2. Consider now an arbitrary function of b binary variables that depends only on their Hamming distances. Although there are n(n\u2212 1)/2 Hamming distances d(zn, zm), they are all determined just by the n \u2212 1 first distances d(z1, zn) for n > 1. This is because, given z1, the distance d(z1, zn) determines zn for each n > 1 and so the entire vector z and all the other distances. Also, given the distances d(z1, zn) for n > 1, the value z1 = \u22121 produces a vector z whose bits are reversed from that produced by z1 = +1, so both have the same Hamming distances. Hence, we have n\u2212 1 free binary variables (the values of d(z1, zn) for n > 1), which determine the vector of 2n possible values of E for all possible binary vectors z. Hence, the dimension of the space of all arbitrary functions of Hamming distances is 2n\u22121. Since 2n\u22121 > (n2 + n + 2)/2 for n > 5, the quadratic functions in general cannot represent all arbitrary binary functions of the Hamming distances using the same binary variables.\nFinally, note that some objective functions which make sense in the b-bit case with b > 1 become trivial in the single-bit case. For example, the loss function for Minimal Loss Hashing (Norouzi and Fleet, 2011):\nLMLH(zn, zm; ynm) =\n{\nmax(\u2016zn \u2212 zm\u2016 \u2212 \u03c1+ 1, 0), ynm = 1\n\u03bbmax(\u03c1\u2212 \u2016zn \u2212 zm\u2016+ 1, 0), ynm = 0\nuses a hinge loss to implement the goal that similar points (having ynm = 1) should differ by no more than \u03c1 \u2212 1 bits and dissimilar points (having ynm = 0) should differ by \u03c1 + 1 bits or more, where \u03c1 \u2265 1, \u03bb > 0, and \u2016zn \u2212 zm\u2016 is the Hamming distance between zn and zm. It is easy to see that in the single-bit case the loss LMLH(zn, zm; ynm) becomes constant, independent of the codes\u2014because using one bit the Hamming distance can be either 0 or 1 only."}, {"heading": "B Orthogonality measure: proofs", "text": "In paragraphAre the codes orthogonal? of the main paper, we define a measure of orthogonality for either the binary codes ZN\u00d7b or the hash function weight vectors Wb\u00d7D, based on the b\u00d7 b matrices of normalized dot products, CZ = 1 N ZTZ and CW = WW\nT (where the rows of W are normalized), respectively. Here we prove several statements we make in that paragraph.\nInvariance to sign reversals Given a matrix C of b\u00d7 b (either CZ or CW) with entries in [\u22121, 1], define as measure of orthogonality (where \u2016\u00b7\u2016F is the Frobenius norm):\n\u22a5 (C) = 1\nL(L\u2212 1) \u2016I\u2212C\u2016\n2 F \u2208 [0, 1]. (8)\nThat is, \u22a5 (C) is the average of the squared off-diagonal elements of C.\nTheorem B.1. \u22a5 (C) is independent of sign reversals of the hash functions.\nProof. Let S be a b\u00d7 b diagonal matrix with diagonal entries sii \u2208 {\u22121,+1}. S satisfies S TS = S2 = I so it is orthogonal. Hence, \u2016I\u2212 SCS\u20162F = \u2016S(I\u2212C)S\u2016 2 F = \u2016I\u2212C\u2016 2 F .\nDistribution of the dot products of random vectors As control hypothesis for the orthogonality of the binary codes or hash function vectors we used the distribution of dot products of random vectors. Here we give their mean and variance explicitly as a function of their dimension.\nTheorem B.2. Let x,y \u2208 {\u22121,+1}d be two random binary vectors of independent components, where x1, . . . , xd, y1, . . . , yd take the value +1 with probability 1 2 . Let z = 1 d xTy = 1 d \u2211d\ni=1 xiyi. Then E {z} = 0 and var {z} = 1\nd .\nProof. Let zi = xiyi \u2208 {\u22121,+1}. Clearly, zi takes the value +1 with probability 1\n2 , so its mean is 0 and\nits variance is 1, and z1, . . . , zd are iid. Hence, using standard properties of the expectation and variance, we have that E {z} = 1\nd\n\u2211d\ni=1 E {zi} = 0, and var {z} = 1 d2\n\u2211d\ni=1 var {zi} = 1 d . (Furthermore, 1 2 (zi + 1) is\nBernoulli and d 2 (z + 1) is binomial.)\nIt is also possible to prove that, for random unit vectors of dimension d with real components, their dot product has mean 0 and variance 1\nd .\nHence, as the dimension d increases, the variance decreases, and the distribution of z tends to a delta at 0. This means that random high-dimensional vectors are practically orthogonal. The \u201crandom\u201d histograms (black line) in fig. 3 are based on a sample of b random vectors (for W, we sample the component of each weight vector uniformly in [\u22121, 1] and then normalize the vector). They follow the theoretical distribution well."}, {"heading": "C Additional experiments", "text": "In fig. 5 we also include results for an additional, unsupervised dataset, the Flickr 1 million image dataset (Huiskes et al., 2010). For Flickr, we randomly select 2 000 images for test and the rest for training. We use D = 150 MPEG-7 edge histogram features. Since no labels are available, we create pseudolabels ynm for xn by declaring as similar points its 100 true nearest neighbors (using the Euclidean distance) and as dissimilar points a random subset of 100 points among the remaining points. As ground truth, we use the K = 10 000 nearest neighbors of the query in Euclidean space. All hash functions are trained using 5 000 points. Retrieved set: k nearest neighbors of the query point in the Hamming space, for a range of k.\nThe only important difference is that Locality-Sensitive Hashing (LSH) achieves a high precision in the Flickr dataset, considerably higher than that of KSHcut. This is understandable, for the following reasons: 1) Flickr is an unsupervised dataset, and the neighborhood information provided to KSHcut (and ILHt) in the form of affinities is limited to the small subset of positive and negative neighbors ynm, while LSH has access to the full feature vector of every image. 2) The dimensionality of the Flickr feature vectors is quite small: D = 150. Still, ILHt beats LSH by a significant margin.\nIn addition to the methods we used in the supervised datasets, we compare ILHt with Spectral Hashing (SH) (Weiss et al., 2009), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Autoencoder (BA) (Carreira-Perpin\u0303a\u0301n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008). Again, ILHt beats all other state-of-the-art methods, or is comparable to the best of them, particularly as the number of bits b increases."}, {"heading": "Acknowledgments", "text": "Work supported by NSF award IIS\u20131423515."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Comm. ACM,", "citeRegEx": "Andoni and Indyk.,? \\Q2008\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2008}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "Belkin and Niyogi.,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2003}, {"title": "Pseudo-boolean optimization", "author": ["E. Boros", "P.L. Hammer"], "venue": "Discrete Applied Math.,", "citeRegEx": "Boros and Hammer.,? \\Q2002\\E", "shortCiteRegEx": "Boros and Hammer.", "year": 2002}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "The elastic embedding algorithm for dimensionality reduction", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Proc. of the 27th Int. Conf. Machine Learning (ICML", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n.,? \\Q2010\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n.", "year": 2010}, {"title": "Hashing with binary autoencoders", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "R. Raziperchikolaei"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.,? \\Q2015\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.", "year": 2015}, {"title": "Distributed optimization of deeply nested systems", "author": ["M.\u00c1. Carreira-Perpi\u00f1\u00e1n", "W. Wang"], "venue": "Proc. of the 17th Int. Conf. Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.,? \\Q2014\\E", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.", "year": 2014}, {"title": "Introduction to Algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2009}, {"title": "Ensemble methods in machine learning", "author": ["T.G. Dietterich"], "venue": "In Multiple Classifier Systems,", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Solving multi-class learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "J. Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Computers and Intractability: A Guide to the Theory of NP-Completeness", "author": ["M.R. Garey", "D.S. Johnson"], "venue": "W.H. Freeman,", "citeRegEx": "Garey and Johnson.,? \\Q1979\\E", "shortCiteRegEx": "Garey and Johnson.", "year": 1979}, {"title": "Graph cuts for supervised binary coding", "author": ["T. Ge", "K. He", "J. Sun"], "venue": "In Proc. 13th European Conf. Computer Vision (ECCV\u201914),", "citeRegEx": "Ge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2014}, {"title": "Neural networks and the bias/variance dilemma", "author": ["S. Geman", "E. Bienenstock", "R. Doursat"], "venue": "Neural Computation,", "citeRegEx": "Geman et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1992}, {"title": "Iterative quantization: A Procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Learning binary hash codes for large-scale image search", "author": ["K. Grauman", "R. Fergus"], "venue": "Machine Learning for Computer Vision,", "citeRegEx": "Grauman and Fergus.,? \\Q2013\\E", "shortCiteRegEx": "Grauman and Fergus.", "year": 2013}, {"title": "Exact maximum a posteriori estimation for binary images", "author": ["D.M. Greig", "B.T. Porteous", "A.H. Seheult"], "venue": "Journal of the Royal Statistical Society, B,", "citeRegEx": "Greig et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Greig et al\\.", "year": 1989}, {"title": "New trends and ideas in visual concept detection: The MIR Flickr Retrieval Evaluation Initiative", "author": ["M.J. Huiskes", "B. Thomee", "M.S. Lew"], "venue": "In Proc. ACM Int. Conf. Multimedia Information Retrieval,", "citeRegEx": "Huiskes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huiskes et al\\.", "year": 2010}, {"title": "The error coding method and PICTs", "author": ["G. James", "T. Hastie"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "James and Hastie.,? \\Q1998\\E", "shortCiteRegEx": "James and Hastie.", "year": 1998}, {"title": "What energy functions can be minimized via graph cuts", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kolmogorov and Zabih.,? \\Q2003\\E", "shortCiteRegEx": "Kolmogorov and Zabih.", "year": 2003}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, Dept. of Computer Science, University of Toronto, Apr", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["A. Krogh", "J. Vedelsby"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krogh and Vedelsby.,? \\Q1995\\E", "shortCiteRegEx": "Krogh and Vedelsby.", "year": 1995}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kulis and Darrell.,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Darrell.", "year": 2009}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["L.I. Kuncheva"], "venue": null, "citeRegEx": "Kuncheva.,? \\Q2014\\E", "shortCiteRegEx": "Kuncheva.", "year": 2014}, {"title": "Learning binary codes with bagging PCA", "author": ["C. Leng", "J. Cheng", "T. Yuan", "X. Bai", "H. Lu"], "venue": "Proc. of the 25th European Conf. Machine Learning", "citeRegEx": "Leng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leng et al\\.", "year": 2014}, {"title": "Geodesic distance function learning via heat flows on vector fields", "author": ["B. Lin", "J. Yang", "X. He", "J. Ye"], "venue": "Proc. of the 31st Int. Conf. Machine Learning (ICML", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A general two-step approach to learning-based hashing", "author": ["G. Lin", "C. Shen", "D. Suter", "A. van den Hengel"], "venue": "In Proc. 14th Int. Conf. Computer Vision (ICCV\u201913),", "citeRegEx": "Lin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Fast supervised hashing with decision trees for high-dimensional data", "author": ["G. Lin", "C. Shen", "Q. Shi", "A. van den Hengel", "D. Suter"], "venue": "In Proc. of the 2014 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "In Proc. of the 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Training invariant support vector machines using selective sampling", "author": ["G. Loosli", "S. Canu", "L. Bottou"], "venue": "Large Scale Kernel Machines, Neural Information Processing Series,", "citeRegEx": "Loosli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2007}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D. Fleet"], "venue": "Proc. of the 28th Int. Conf. Machine Learning (ICML", "citeRegEx": "Norouzi and Fleet.,? \\Q2011\\E", "shortCiteRegEx": "Norouzi and Fleet.", "year": 2011}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "Int. J. Computer Vision,", "citeRegEx": "Oliva and Torralba.,? \\Q2001\\E", "shortCiteRegEx": "Oliva and Torralba.", "year": 2001}, {"title": "Computationally bounded retrieval", "author": ["M. Rastegari", "C. Keskin", "P. Kohli", "S. Izadi"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201915),", "citeRegEx": "Rastegari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2015}, {"title": "Learning hashing with affinity-based loss functions using auxiliary coordinates", "author": ["R. Raziperchikolaei", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "[cs.LG], Jan", "citeRegEx": "Raziperchikolaei and Carreira.Perpi\u00f1\u00e1n.,? \\Q2015\\E", "shortCiteRegEx": "Raziperchikolaei and Carreira.Perpi\u00f1\u00e1n.", "year": 2015}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science,", "citeRegEx": "Roweis and Saul.,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul.", "year": 2000}, {"title": "Nearest-Neighbor Methods in Learning and Vision. Neural Information Processing Series", "author": ["G. Shakhnarovich", "P. Indyk", "T. Darrell", "editors"], "venue": null, "citeRegEx": "Shakhnarovich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shakhnarovich et al\\.", "year": 2006}, {"title": "Visualizing data using t-SNE", "author": ["L.J.P. van der Maaten", "G.E. Hinton"], "venue": "J. Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Semi-supervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Weiss et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2009}, {"title": "Sparse projections for high-dimensional binary codes", "author": ["Y. Xia", "K. He", "P. Kohli", "J. Sun"], "venue": "In Proc. of the 2015 IEEE Computer Society Conf. Computer Vision and Pattern Recognition (CVPR\u201915),", "citeRegEx": "Xia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2015}, {"title": "Multiclass spectral clustering", "author": ["S.X. Yu", "J. Shi"], "venue": "In Proc. 9th Int. Conf. Computer Vision (ICCV\u201903),", "citeRegEx": "Yu and Shi.,? \\Q2003\\E", "shortCiteRegEx": "Yu and Shi.", "year": 2003}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. of the 33rd ACM Conf. Research and Development in Information Retrieval (SIGIR", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Ensemble Methods: Foundations and Algorithms. Chapman & Hall/CRC Machine Learning and Pattern Recognition Series", "author": ["Z.-H. Zhou"], "venue": "CRC Publishers,", "citeRegEx": "Zhou.,? \\Q2012\\E", "shortCiteRegEx": "Zhou.", "year": 2012}], "referenceMentions": [{"referenceID": 36, "context": "Information retrieval tasks such as searching for a query image or document in a database are essentially a nearest-neighbor search (Shakhnarovich et al., 2006).", "startOffset": 132, "endOffset": 160}, {"referenceID": 15, "context": "We focus on binary hashing (Grauman and Fergus, 2013), where the query and database are mapped onto low-dimensional binary vectors, where the search is performed.", "startOffset": 27, "endOffset": 53}, {"referenceID": 29, "context": "Examples of these objective functions are Supervised Hashing with Kernels (KSH) (Liu et al., 2012), Binary Reconstructive Embeddings", "startOffset": 80, "endOffset": 98}, {"referenceID": 22, "context": "(BRE) (Kulis and Darrell, 2009) and the binary Laplacian loss (an extension of the Laplacian Eigenmaps objective; Belkin and Niyogi, 2003):", "startOffset": 6, "endOffset": 31}, {"referenceID": 1, "context": "(BRE) (Kulis and Darrell, 2009) and the binary Laplacian loss (an extension of the Laplacian Eigenmaps objective; Belkin and Niyogi, 2003):", "startOffset": 62, "endOffset": 138}, {"referenceID": 35, "context": "Other examples of these objective functions include models developed for dimension reduction, be they spectral such as Locally Linear Embedding (Roweis and Saul, 2000) or Anchor Graphs (Liu et al.", "startOffset": 144, "endOffset": 167}, {"referenceID": 28, "context": "Other examples of these objective functions include models developed for dimension reduction, be they spectral such as Locally Linear Embedding (Roweis and Saul, 2000) or Anchor Graphs (Liu et al., 2011), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al.", "startOffset": 185, "endOffset": 203}, {"referenceID": 4, "context": ", 2011), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 38, "context": ", 2011), or nonlinear such as the Elastic Embedding (Carreira-Perpi\u00f1\u00e1n, 2010) or t-SNE (van der Maaten and Hinton, 2008); as well as objective functions designed specifically for binary hashing, such as Semi-supervised sequential Projection Learning Hashing (SPLH) (Wang et al., 2012).", "startOffset": 265, "endOffset": 284}, {"referenceID": 39, "context": "For example, in the Laplacian loss (3) we can encourage codes to be orthogonal through a constraint ZZ = NI (Weiss et al., 2009) or a penalty term \u2016ZTZ\u2212NI\u2016 (the latter requiring a hyperparameter that controls the weight of the penalty) (Ge et al.", "startOffset": 108, "endOffset": 128}, {"referenceID": 12, "context": ", 2009) or a penalty term \u2016ZTZ\u2212NI\u2016 (the latter requiring a hyperparameter that controls the weight of the penalty) (Ge et al., 2014), although this generates dense matrices of N \u00d7N .", "startOffset": 115, "endOffset": 132}, {"referenceID": 41, "context": "Most papers ignore the binary nature of the Z codes and optimize over them as real values, then binarize them by truncation (possibly with an optimal rotation; Yu and Shi, 2003; Gong et al., 2013), and finally fit a classifier (e.", "startOffset": 124, "endOffset": 196}, {"referenceID": 14, "context": "Most papers ignore the binary nature of the Z codes and optimize over them as real values, then binarize them by truncation (possibly with an optimal rotation; Yu and Shi, 2003; Gong et al., 2013), and finally fit a classifier (e.", "startOffset": 124, "endOffset": 196}, {"referenceID": 1, "context": "For example, for the Laplacian loss with constraints this involves solving an eigenproblem on Z as in Laplacian eigenmaps (Belkin and Niyogi, 2003; Weiss et al., 2009; Zhang et al., 2010), or approximated using landmarks (Liu et al.", "startOffset": 122, "endOffset": 187}, {"referenceID": 39, "context": "For example, for the Laplacian loss with constraints this involves solving an eigenproblem on Z as in Laplacian eigenmaps (Belkin and Niyogi, 2003; Weiss et al., 2009; Zhang et al., 2010), or approximated using landmarks (Liu et al.", "startOffset": 122, "endOffset": 187}, {"referenceID": 42, "context": "For example, for the Laplacian loss with constraints this involves solving an eigenproblem on Z as in Laplacian eigenmaps (Belkin and Niyogi, 2003; Weiss et al., 2009; Zhang et al., 2010), or approximated using landmarks (Liu et al.", "startOffset": 122, "endOffset": 187}, {"referenceID": 28, "context": ", 2010), or approximated using landmarks (Liu et al., 2011).", "startOffset": 41, "endOffset": 59}, {"referenceID": 3, "context": "Some recent papers try to respect the binary nature of the codes during their optimization, using techniques such as alternating optimization, min-cut and GraphCut (Boykov et al., 2001; Lin et al., 2014b; Ge et al., 2014) or others (Lin et al.", "startOffset": 164, "endOffset": 221}, {"referenceID": 12, "context": "Some recent papers try to respect the binary nature of the codes during their optimization, using techniques such as alternating optimization, min-cut and GraphCut (Boykov et al., 2001; Lin et al., 2014b; Ge et al., 2014) or others (Lin et al.", "startOffset": 164, "endOffset": 221}, {"referenceID": 26, "context": ", 2014) or others (Lin et al., 2013), and then fit the classifiers, or use alternating optimization directly on the hash function parameters (Liu et al.", "startOffset": 18, "endOffset": 36}, {"referenceID": 29, "context": ", 2013), and then fit the classifiers, or use alternating optimization directly on the hash function parameters (Liu et al., 2012).", "startOffset": 112, "endOffset": 130}, {"referenceID": 12, "context": "Even more recently, one can optimize jointly over the binary codes and hash functions (Ge et al., 2014; Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015; Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n, 2015).", "startOffset": 86, "endOffset": 195}, {"referenceID": 5, "context": "Even more recently, one can optimize jointly over the binary codes and hash functions (Ge et al., 2014; Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015; Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n, 2015).", "startOffset": 86, "endOffset": 195}, {"referenceID": 34, "context": "Even more recently, one can optimize jointly over the binary codes and hash functions (Ge et al., 2014; Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015; Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n, 2015).", "startOffset": 86, "endOffset": 195}, {"referenceID": 8, "context": "In fact, the very same issue arises when training an ensemble of classifiers (Dietterich, 2000; Zhou, 2012; Kuncheva, 2014).", "startOffset": 77, "endOffset": 123}, {"referenceID": 43, "context": "In fact, the very same issue arises when training an ensemble of classifiers (Dietterich, 2000; Zhou, 2012; Kuncheva, 2014).", "startOffset": 77, "endOffset": 123}, {"referenceID": 23, "context": "In fact, the very same issue arises when training an ensemble of classifiers (Dietterich, 2000; Zhou, 2012; Kuncheva, 2014).", "startOffset": 77, "endOffset": 123}, {"referenceID": 3, "context": "We approximately optimize it using a min-cut algorithm (as implemented by Boykov et al., 2001) applied in alternating fashion to submodular blocks as described in Lin et al. (2014a). This first partitions the N points into disjoint groups containing only nonnegative weights.", "startOffset": 74, "endOffset": 182}, {"referenceID": 11, "context": "This problem is NPcomplete in general (Garey and Johnson, 1979; Boros and Hammer, 2002; Kolmogorov and Zabih, 2003), when A has both positive and negative elements, as well as zeros.", "startOffset": 38, "endOffset": 115}, {"referenceID": 2, "context": "This problem is NPcomplete in general (Garey and Johnson, 1979; Boros and Hammer, 2002; Kolmogorov and Zabih, 2003), when A has both positive and negative elements, as well as zeros.", "startOffset": 38, "endOffset": 115}, {"referenceID": 19, "context": "This problem is NPcomplete in general (Garey and Johnson, 1979; Boros and Hammer, 2002; Kolmogorov and Zabih, 2003), when A has both positive and negative elements, as well as zeros.", "startOffset": 38, "endOffset": 115}, {"referenceID": 2, "context": "It is submodular if A has only nonpositive elements, in which case it is equivalent to a min-cut/max-flow problem and it can be solved in polynomial time (Boros and Hammer, 2002; Greig et al., 1989).", "startOffset": 154, "endOffset": 198}, {"referenceID": 16, "context": "It is submodular if A has only nonpositive elements, in which case it is equivalent to a min-cut/max-flow problem and it can be solved in polynomial time (Boros and Hammer, 2002; Greig et al., 1989).", "startOffset": 154, "endOffset": 198}, {"referenceID": 3, "context": "This is the case for the best practical GraphCut (Boykov et al., 2001) and max-flow/min-cut algorithms (Cormen et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 7, "context": ", 2001) and max-flow/min-cut algorithms (Cormen et al., 2009).", "startOffset": 40, "endOffset": 61}, {"referenceID": 20, "context": "We use the following labeled datasets (all using the Euclidean distance in feature space): (1) CIFAR (Krizhevsky, 2009) contains 60 000 images in 10 classes.", "startOffset": 101, "endOffset": 119}, {"referenceID": 32, "context": "We use D = 320 GIST features (Oliva and Torralba, 2001) from each image.", "startOffset": 29, "endOffset": 55}, {"referenceID": 30, "context": "(2) Infinite MNIST (Loosli et al., 2007).", "startOffset": 19, "endOffset": 40}, {"referenceID": 22, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Liu et al., 2012; Lin et al., 2013; Ge et al., 2014).", "startOffset": 131, "endOffset": 209}, {"referenceID": 29, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Liu et al., 2012; Lin et al., 2013; Ge et al., 2014).", "startOffset": 131, "endOffset": 209}, {"referenceID": 26, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Liu et al., 2012; Lin et al., 2013; Ge et al., 2014).", "startOffset": 131, "endOffset": 209}, {"referenceID": 12, "context": "Because of the computational cost of affinity-based methods, previous work has used training sets limited to a few thousand points (Kulis and Darrell, 2009; Liu et al., 2012; Lin et al., 2013; Ge et al., 2014).", "startOffset": 131, "endOffset": 209}, {"referenceID": 10, "context": "As hash functions (for each bit), we use linear SVMs (trained with LIBLINEAR; Fan et al., 2008) and kernel SVMs (with 500 basis functions centered at a random subset of training points).", "startOffset": 53, "endOffset": 95}, {"referenceID": 29, "context": "As baseline coupled objective, we use KSH (Liu et al., 2012) but using the same two-step training as ILH: first we find the codes using the alternating min-cut method described earlier (initialized from an all-ones code, and running one iteration of alternating min-cut) and then we fit the classifiers.", "startOffset": 42, "endOffset": 60}, {"referenceID": 23, "context": "Linear SVMs are very stable classifiers known to benefit less from ensembles than less stable classifiers such as decision trees or neural nets (Kuncheva, 2014).", "startOffset": 144, "endOffset": 160}, {"referenceID": 24, "context": "Bagging tPCA (Leng et al., 2014) does make tPCA improve monotonically with b, but the result is still far from competitive.", "startOffset": 13, "endOffset": 32}, {"referenceID": 29, "context": "Comparison with other binary hashing methods We compare with both the original KSH (Liu et al., 2012) and its min-cut optimization KSHcut (Lin et al.", "startOffset": 83, "endOffset": 101}, {"referenceID": 22, "context": ", 2014b), and a representative subset of affinitybased and unsupervised hashing methods: Supervised Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009), Supervised Self-Taught Hashing (STH) (Zhang et al.", "startOffset": 139, "endOffset": 164}, {"referenceID": 42, "context": ", 2014b), and a representative subset of affinitybased and unsupervised hashing methods: Supervised Binary Reconstructive Embeddings (BRE) (Kulis and Darrell, 2009), Supervised Self-Taught Hashing (STH) (Zhang et al., 2010), Spectral Hashing (SH) (Weiss et al.", "startOffset": 203, "endOffset": 223}, {"referenceID": 39, "context": ", 2010), Spectral Hashing (SH) (Weiss et al., 2009), Iterative Quantization (ITQ) (Gong et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 14, "context": ", 2009), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 33, "endOffset": 79}, {"referenceID": 0, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 142, "endOffset": 166}, {"referenceID": 24, "context": "There has been a prior attempt to use bagging (bootstrapped samples) with truncated PCA (Leng et al., 2014).", "startOffset": 88, "endOffset": 107}, {"referenceID": 29, "context": "Some supervised binary hashing work (Liu et al., 2012; Wang et al., 2012) has proposed to learn the b hash functions sequentially, where the ith function has an orthogonality-like constraint to force it to differ from the previous functions.", "startOffset": 36, "endOffset": 73}, {"referenceID": 38, "context": "Some supervised binary hashing work (Liu et al., 2012; Wang et al., 2012) has proposed to learn the b hash functions sequentially, where the ith function has an orthogonality-like constraint to force it to differ from the previous functions.", "startOffset": 36, "endOffset": 73}, {"referenceID": 21, "context": "This means that well-known error decompositions such as the error-ambiguity decomposition (Krogh and Vedelsby, 1995) and the bias-variance decomposition (Geman et al.", "startOffset": 90, "endOffset": 116}, {"referenceID": 13, "context": "This means that well-known error decompositions such as the error-ambiguity decomposition (Krogh and Vedelsby, 1995) and the bias-variance decomposition (Geman et al., 1992) do not apply.", "startOffset": 153, "endOffset": 173}, {"referenceID": 9, "context": "In this respect, there is also a relation with error-correcting output codes (ECOC) (Dietterich and Bakiri, 1995), an approach for multiclass classification.", "startOffset": 84, "endOffset": 113}, {"referenceID": 18, "context": "(James and Hastie, 1998).", "startOffset": 0, "endOffset": 24}, {"referenceID": 33, "context": "Some binary hashing methods optimize an objective essentially of the following form (Rastegari et al., 2015; Xia et al., 2015):", "startOffset": 84, "endOffset": 126}, {"referenceID": 40, "context": "Some binary hashing methods optimize an objective essentially of the following form (Rastegari et al., 2015; Xia et al., 2015):", "startOffset": 84, "endOffset": 126}, {"referenceID": 25, "context": "This fact, already noted by Lin et al. (2013), is because a function of 2 binary variables f(x, y) can take 4 different values:", "startOffset": 28, "endOffset": 46}, {"referenceID": 31, "context": "For example, the loss function for Minimal Loss Hashing (Norouzi and Fleet, 2011):", "startOffset": 56, "endOffset": 81}, {"referenceID": 17, "context": "5 we also include results for an additional, unsupervised dataset, the Flickr 1 million image dataset (Huiskes et al., 2010).", "startOffset": 102, "endOffset": 124}, {"referenceID": 39, "context": "In addition to the methods we used in the supervised datasets, we compare ILHt with Spectral Hashing (SH) (Weiss et al., 2009), Iterative Quantization (ITQ) (Gong et al.", "startOffset": 106, "endOffset": 126}, {"referenceID": 14, "context": ", 2009), Iterative Quantization (ITQ) (Gong et al., 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 33, "endOffset": 79}, {"referenceID": 0, "context": ", 2013), Binary Autoencoder (BA) (Carreira-Perpi\u00f1\u00e1n and Raziperchikolaei, 2015), thresholded PCA (tPCA), and Locality-Sensitive Hashing (LSH) (Andoni and Indyk, 2008).", "startOffset": 142, "endOffset": 166}], "year": 2016, "abstractText": "Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}