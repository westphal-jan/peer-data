{"id": "1602.01582", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "SDCA without Duality, Regularization, and Individual Convexity", "abstract": "Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularization and do not rely on duality. We prove linear convergence rates even if individual loss functions are non-convex, as long as the expected loss is strongly convex.", "histories": [["v1", "Thu, 4 Feb 2016 08:14:06 GMT  (11kb,D)", "https://arxiv.org/abs/1602.01582v1", null], ["v2", "Sat, 21 May 2016 12:33:05 GMT  (25kb)", "http://arxiv.org/abs/1602.01582v2", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz"], "accepted": true, "id": "1602.01582"}, "pdf": {"name": "1602.01582.pdf", "metadata": {"source": "META", "title": "SDCA without Duality, Regularization, and Individual Convexity", "authors": ["Shai Shalev-Shwartz"], "emails": ["SHAIS@CS.HUJI.AC.IL"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n01 58\n2v 2\n[ cs\n.L G\n] 2\n1 M\nay 2\n01 6"}, {"heading": "1. Introduction", "text": "We consider the following loss minimization problem:\nmin w\u2208Rd\nF (w) := 1\nn\nn \u2211\ni=1\nfi(w) .\nAn important sub-class of problems is when each fi can be written as fi(w) = \u03c6i(w) + \u03bb2 \u2016w\u20162, where \u03c6i is Lismooth and convex. A popular method for solving this sub-class of problems is Stochastic Dual Coordinate Ascent (SDCA), and (Shalev-Shwartz & Zhang, 2013) established the convergence rate of O\u0303((Lmax/\u03bb+ n) log(1/\u01eb)), where Lmax = maxi Li.\nAs its name indicates, SDCA is derived by considering a dual problem. In this paper, we consider the possibility of applying SDCA for problems in which individual fi do not necessarily have the form \u03c6i(w) + \u03bb2 \u2016w\u20162, and can even be non-convex (e.g., deep learning optimization problems, or problems arising in fast calculation of the top singular vectors (Jin et al., 2015)). In many such cases, the\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\ndual problem is meaningless. Instead of directly using the dual problem, we describe and analyze a variant of SDCA in which only gradients of fi are being used. Following (Johnson & Zhang, 2013), we show that SDCA is a member of the Stochastic Gradient Descent (SGD) family of algorithms, that is, its update is based on an unbiased estimate of the gradient, but unlike the vanilla SGD, for SDCA the variance of the estimation of the gradient tends to zero as we converge to a minimum.\nOur analysis assumes that F is \u03bb-strongly convex and each fi is Li-smooth. When each fi is also convex we establish the convergence rate of O\u0303(L\u0304/\u03bb + n), where L\u0304 is the average of Li and the O\u0303 notation hides logarithmic terms, including the factor log(1/\u01eb). This matches the best known bound for SVRG given in (Xiao & Zhang, 2014). Lower bounds have been derived in (Arjevani et al., 2015; Agarwal & Bottou, 2014). Applying an acceleration technique ((Shalev-Shwartz & Zhang, 2015; Lin et al., 2015)) we obtain the convergence rate O\u0303(n1/2 \u221a\nL\u0304/\u03bb + n). If fi are non-convex we first prove that SDCA enjoys the rate O\u0303(L\u03042/\u03bb2+n). Finally, we show how the acceleration technique yields the bound O\u0303 ( n3/4 \u221a L\u0304/\u03bb+ n ) . That is, we\nhave the same dependency on the square root of the condition number, \u221a L\u0304/\u03bb, but this term is multiplied by n3/4 rather than by n1/2. Understanding if this factor can be eliminated is left to future work.\nRelated work: In recent years, many randomized methods for optimizing average of functions have been proposed. For example, SAG (Le Roux et al., 2012), SVRG (Johnson & Zhang, 2013), Finito (Defazio et al., 2014b), SAGA (Defazio et al., 2014a), S2GD (Konec\u030cny\u0300 & Richta\u0301rik, 2013), and UniVr (Allen-Zhu & Yuan, 2015). All of these methods have similar convergence rates for strongly convex\nand smooth problems. Here we show that SDCA achieves the best known convergence rate for the case in which individual loss functions are convex, and a slightly worse rate for the case in which individual loss functions are non-convex. A systematic study of the convergence rate of the different methods under non-convex losses is left to future work.\nThis version of the paper improves upon a previous unpublished version of the paper (Shalev-Shwartz, 2015) in three aspects. First, the convergence rate here depends on L\u0304 as opposed to Lmax in (Shalev-Shwartz, 2015). Second, the version in (Shalev-Shwartz, 2015) only deals with the regularized case, while here we show that the same rate can be obtained for unregularized objectives. Last, for the non-convex case, here we derive the bound O\u0303 ( n3/4 \u221a L\u0304/\u03bb+ n ) while in (Shalev-Shwartz,\n2015) only the bound of O\u0303(L2max/\u03bb 2 + n) has been given.\n(Csiba & Richta\u0301rik, 2015) extended the work of (Shalev-Shwartz, 2015) to support arbitrary mini-batching schemes, and (He & Taka\u0301c\u030c, 2015) extended the work of (Shalev-Shwartz, 2015) to support adaptive sampling probabilities. A primal form of SDCA has been also given in (Defazio, 2014). Using SVRG for non-convex individual functions has been recently studied in (Shamir, 2015; Jin et al., 2015), in the context of fast computation of the top singular vectors of a matrix."}, {"heading": "2. SDCA without Duality", "text": "We start the section by describing a variant of SDCA that do not rely on duality. To simplify the presentation, we start in Section 2.1 with regularized loss minimization problems. In Section 2.2 we tackle the non-regularized case and in Section 2.3 we tackle the non-convex case.\nWe recall the following basic definitions: A (differentiable) function f is \u03bb-strongly convex if for every u,w we have f(w) \u2212 f(u) \u2265 \u2207f(u)\u22a4(w \u2212 u) + \u03bb2 \u2016w \u2212 u\u20162. We say that f is convex if it is 0-strongly convex. We say that f is L-smooth if \u2016\u2207f(w) \u2212 \u2207f(u)\u2016 \u2264 L\u2016w \u2212 u\u2016. It is well known that smoothness and convexity also implies that f(w) \u2212 f(u) \u2264 \u2207f(u)\u22a4(w \u2212 u) + L2 \u2016w \u2212 u\u20162."}, {"heading": "2.1. Regularized problems", "text": "In regularized problems, each fi can be written as fi(w) = \u03c6i(w)+ \u03bb 2 \u2016w\u20162. Similarly to the original SDCA algorithm, we maintain vectors \u03b11, . . . , \u03b1n, where each \u03b1i \u2208 Rd. We call these vectors pseudo-dual vectors. The algorithm is described below.\nAlgorithm 1: Dual-Free SDCA for Regularized Objectives\nGoal: Minimize F (w) = 1n \u2211n i=1 \u03c6i(w) + \u03bb 2 \u2016w\u20162 Input: Objective F , number of iterations T , step size \u03b7, Smoothness parameters L1, . . . , Ln Initialize: w(0) = 1\u03bbn \u2211n i=1 \u03b1 (0) i for some \u03b1(0) = (\u03b1(0)1 , . . . , \u03b1 (0) n )\n\u2200i \u2208 [n], qi = (Li + L\u0304)/(2nL\u0304) where L\u0304 = 1n \u2211n i=1 Li\nFor t = 1, . . . , T Pick i \u223c q, denote \u03b7i = \u03b7qin Update:\n\u03b1 (t) i = \u03b1 (t\u22121) i \u2212 \u03b7i\u03bbn\n( \u2207\u03c6i(w(t\u22121)) + \u03b1(t\u22121)i )\nw(t) = w(t\u22121) \u2212 \u03b7i ( \u2207\u03c6i(w(t\u22121)) + \u03b1(t\u22121)i )\nObserve that SDCA keeps the primal-dual relation\nw(t\u22121) = 1\n\u03bbn\nn \u2211\ni=1\n\u03b1 (t\u22121) i\nObserve also that the update of \u03b1 can be rewritten as\n\u03b1 (t) i = (1\u2212 \u03b2i)\u03b1 (t\u22121) i + \u03b2i\n( \u2212\u2207\u03c6i(w(t\u22121)) ) ,\nwhere \u03b2i = \u03b7i\u03bbn. Namely, the new value of \u03b1i is a convex combination of its old value and the negative gradient. Finally, observe that, conditioned on the value of w(t\u22121) and \u03b1(t\u22121), we have that\nEi\u223cq[w (t)] = w(t\u22121) \u2212 \u03b7\n\u2211\ni\nqi qin ( (\u2207\u03c6i(w(t\u22121)) + \u03b1(t\u22121)i ) )\n= w(t\u22121) \u2212 \u03b7 (\n\u2207 1 n\nn \u2211\ni=1\n\u03c6i(w (t\u22121)) + \u03bbw(t\u22121)\n)\n= w(t\u22121) \u2212 \u03b7\u2207P (w(t\u22121)) .\nThat is, SDCA is in fact an instance of Stochastic Gradient Descent (SGD). As we will see shortly, the advantage of\nSDCA over a vanilla SGD algorithm is because the variance of the update goes to zero as we converge to an optimum.\nOur convergence analysis relies on bounding the following potential function, defined for every t \u2265 0,\nCt = \u03bb 2 \u2016w(t) \u2212 w\u2217\u20162 + \u03b7 n2\nn \u2211\ni=1\n[ 1\nqi \u2016\u03b1(t)i \u2212 \u03b1\u2217i \u20162] , (1)\nwhere\nw\u2217 = argmin w F (w), and \u2200i, \u03b1\u2217i = \u2212\u2207\u03c6i(w\u2217) . (2)\nIntuitively, Ct measures the distance to the optimum both in primal and pseudo-dual variables. Observe that if F is LF -smooth and convex then\nF (w(t))\u2212 F (w\u2217) \u2264 LF 2 \u2016w(t) \u2212 w\u2217\u20162 \u2264 LF \u03bb Ct ,\nand therefore a bound on Ct immediately implies a bound on the sub-optimality of w(t).\nThe following theorem establishes the convergence rate of SDCA for the case in which each \u03c6i is convex.\nTheorem 1 Assume that each \u03c6i is Li-smooth and convex, and Algorithm 1 is run with \u03b7 \u2264 min {\n1 4L\u0304 , 14\u03bbn }\n. Then, for every t \u2265 1,\nE[Ct] \u2264 (1 \u2212 \u03b7\u03bb)t C0 ,\nwhere Ct is as defined in (1). In particular, to achieve E[F (w(T )) \u2212 F (w\u2217)] \u2264 \u01eb it suffices to set \u03b7 = min {\n1 4L\u0304 , 14\u03bbn } and\nT \u2265 \u2126\u0303 ( L\u0304\n\u03bb + n\n)\n.\nVariance Reduction: The lemma below tells us that the variance of the SDCA update decreases as we get closer to the optimum.\nLemma 1 Under the same conditions of Theorem 1, the expected value of \u2016w(t) \u2212w(t\u22121)\u20162 conditioned on w(t\u22121) satisfies:\nE[\u2016w(t)\u2212w(t\u22121)\u20162] \u2264 3 \u03b7 (\n1 2\u2016w (t\u22121) \u2212 w\u2217\u20162 + Ct\u22121 ) ."}, {"heading": "2.2. SDCA without regularization", "text": "We now turn to the case in which the objective is not explicitly regularized. The algorithm below tackles this problem by a reduction to the regularized case. In particular, we artificially add regularization to the objective and compensate for it by adding one more loss function that cancels out the regularization term. While the added function is not convex (in fact, it is concave), we prove that the same convergence rate holds due to the special structure of the added loss function.\nAlgorithm 2: Dual-Free SDCA for Non-Regularized Objectives\nGoal: Minimize F (w) = 1n \u2211n\ni=1 fi(w) Input: Objective F , number of iterations T ,\nstep size \u03b7, Strong convexity parameter \u03bb, Smoothness parameters L1, . . . , Ln\nDefine: For all i \u2208 [n], \u03c6i(w) = n+1n fi(w), L\u0303i = n+1n Li For i = n+ 1, \u03c6i(w) = \u2212\u03bb i2 \u2016w\u20162, L\u0303i = \u03bb i Solve: Rewrite F as F (w) = 1n+1 \u2211n+1 i=1 \u03c6i(w) + \u03bb 2 \u2016w\u20162\nCall Algorithm 1 with F above and with {L\u0303i}\nTheorem 2 Assume that F is \u03bb-strongly convex, that each fi is Li-smooth and convex, and that Algorithm 2 is run with \u03b7 \u2264 min {\n1 8(L\u0304+\u03bb) , 14 \u03bb(n+1)\n}\n. Then, for every t \u2265 1,\nE[Ct] \u2264 (1 \u2212 \u03b7\u03bb)t C0 ,\nwhere Ct is as defined in (1). In particular, to achieve E[F (w(T )) \u2212 F (w\u2217)] \u2264 \u01eb it suffices to set \u03b7 = min {\n1 8(L\u0304+\u03bb) , 14\u03bb(n+1)\n}\nand\nT \u2265 \u2126\u0303 ( L\u0304\n\u03bb + n\n)\n."}, {"heading": "2.3. The non-convex case", "text": "We now consider the non-convex case. For simplicity, we focus on the regularized setting. In the non-regularized setting we can simply replace every fi with \u03c6i(w) = fi(w)\u2212 \u03bb2 \u2016w\u20162 and apply the regularized setting. Note that this does not change significantly the smoothness (because \u03bb is typically much smaller than the average smoothness of the fi).\nWe can apply Algorithm 1 for the non-convex case, and the only change is the choice of \u03b7, as reflected in the theorem below.\nTheorem 3 Consider running algorithm 1 on F which is \u03bb-strongly convex, assume that each \u03c6i is Li-smooth, and \u03b7 \u2264 min {\n\u03bb 4L\u03042 , 14\u03bbn } . Then, for every t \u2265 1,\nE[Ct] \u2264 (1 \u2212 \u03b7\u03bb)t C0 ,\nwhere Ct is as defined in (1). In particular, to achieve E[F (w(T )) \u2212 F (w\u2217)] \u2264 \u01eb it suffices to set \u03b7 = min {\n\u03bb 4L\u03042 , 14\u03bbn } and\nT \u2265 \u2126\u0303 ( L\u03042\n\u03bb2 + n\n)\n.\nAs can be seen, the dependence of T on the condition number, L\u0304\u03bb , is quadratic for the non-convex case, as opposed to a linear dependency for the convex case. We next show how to improve the bound using acceleration."}, {"heading": "2.4. Acceleration", "text": "Accelerated SDCA (Shalev-Shwartz & Zhang, 2015) is obtained by solving (using SDCA) a sequence of problems, where at each iteration, we add an artificial regularization of the form \u03ba2 \u2016w \u2212 y(t\u22121)\u20162, where y(t\u22121) is a function of w(t\u22121) and w(t\u22122). The algorithm has been generalized in (Lin et al., 2015) to allow the inner solver to be any algorithm. For completeness, we provide the pseudo-code of the \u201cCatalyst\u201d algorithm of (Lin et al., 2015) and its analysis.\nAlgorithm 3: Acceleration\nGoal: Minimize a \u03bb-strongly convex function F (w) Parameters: \u03ba, T Initialize:\nInitial solution w(0) \u01eb0 s.t. \u01eb0 \u2265 F (w(0))\u2212 F (w\u2217) y(0) = w(0), q = \u03bb\u03bb+\u03ba\nFor: t = 1, . . . , T Define Gt(w) = F (w) + \u03ba2 \u2016w \u2212 y(t\u22121)\u20162 Set \u01ebt = (1 \u2212 0.9 \u221a q) \u01ebt\u22121\nFind w(t) s.t. Gt(w(t))\u2212minw Gt(w) \u2264 \u01ebt Set y(t) = w(t) + \u221a q\u2212q\u221a q+q (w\n(t) \u2212 w(t\u22121)) Output: w(T )\nLemma 2 Fix \u01eb > 0 and suppose we run the Acceleration algorithm (Algorithm 3) for\nT = \u2126\n(\n\u221a\n\u03bb+ \u03ba\n\u03bb log\n(\n\u03bb+ \u03ba\n\u03bb \u01eb\n)\n)\niterations. Then, F (w(T ))\u2212 F (w\u2217) \u2264 \u01eb.\nProof The lemma follows directly from Theorem 3.1 of (Lin et al., 2015) by observing that Algorithm 3 is a specification of Algorithm 1 in (Lin et al., 2015) with \u03b10 = \u221a q (which implies that \u03b1t = \u03b10 for every t), with \u01ebt = \u01eb0(1\u2212 \u03c1)t, and with \u03c1 = 0.9 \u221a q.\nTheorem 4 Let F = 1n \u2211n i=1 \u03c6i(w) + \u03bb 2 \u2016w\u20162, assume that each \u03c6i is Li smooth and that F is \u03bb-strongly convex. Assume also that (L\u0304/\u03bb)2 \u2265 3n (otherwise we can simply apply O\u0303(n) iterations of Algorithm 1). Then, running Algorithm 3 with parameters \u03ba = L\u0304/ \u221a n, T = \u2126\u0303 ( 1 + n\u22121/4 \u221a L\u0304/\u03bb ) , and while at each iteration of Al-\ngorithm 3 using \u2126\u0303 (n) iterations of Algorithm 1 to minimize Gt, guarantees that F (w(T ))\u2212F (w\u2217) \u2264 \u01eb (with high probability). The total required number of iterations of Algorithm 1 is therefore bounded by O\u0303 ( n+ n3/4 \u221a L\u0304/\u03bb ) .\nObserve that for the case of convex individual functions, accelerating Algorithm 1 yields the upper bound O\u0303 ( n+ n1/2 \u221a L\u0304/\u03bb ) . Therefore, the convex and non-\nconvex cases have the same dependency on the condition number, but the non-convex case has a worse dependence on n."}, {"heading": "3. Proofs", "text": ""}, {"heading": "3.1. Proof of Theorem 1", "text": "Observe that 0 = \u2207F (w\u2217) = 1n \u2211 i \u2207\u03c6i(w\u2217) + \u03bbw\u2217, which implies that w\u2217 = 1\u03bbn \u2211 i \u03b1 \u2217 i , where \u03b1 \u2217 i = \u2212\u2207\u03c6i(w\u2217).\nDefine ui = \u2212\u2207\u03c6i(w(t\u22121)) and vi = \u2212ui + \u03b1(t\u22121)i . We also denote two potentials:\nAt =\nn \u2211\nj=1\n1 qj \u2016\u03b1(t)j \u2212 \u03b1\u2217j\u20162 , Bt = \u2016w(t) \u2212 w\u2217\u20162 .\nWe will first analyze the evolution of At and Bt. If on round t we update using element i then \u03b1(t)i = (1 \u2212 \u03b2i)\u03b1 (t\u22121) i + \u03b2iui. It follows that,\nAt\u22121 \u2212At = \u2212 1\nqi \u2016\u03b1(t)i \u2212 \u03b1\u2217i \u20162 +\n1 qi \u2016\u03b1(t\u22121)i \u2212 \u03b1\u2217i \u20162\n(3)\n= \u2212 1 qi \u2016(1\u2212 \u03b2i)(\u03b1(t\u22121)i \u2212 \u03b1\u2217i ) + \u03b2i(ui \u2212 \u03b1\u2217i )\u20162\n+ 1 qi \u2016\u03b1(t\u22121)i \u2212 \u03b1\u2217i \u20162\n= 1 qi (\u2212(1\u2212 \u03b2i)\u2016\u03b1(t\u22121)i \u2212 \u03b1\u2217i \u20162 \u2212 \u03b2i\u2016ui \u2212 \u03b1\u2217i \u20162\n+ \u03b2i(1 \u2212 \u03b2i)\u2016\u03b1(t\u22121)i \u2212 ui\u20162 + \u2016\u03b1 (t\u22121) i \u2212 \u03b1\u2217i \u20162 )\n= \u03b2i qi ( \u2016\u03b1(t\u22121)i \u2212 \u03b1\u2217i \u20162 \u2212 \u2016ui \u2212 \u03b1\u2217i \u20162 + (1\u2212 \u03b2i)\u2016vi\u20162 ) = \u03b7 \u03bb\nq2i\n( \u2016\u03b1(t\u22121)i \u2212 \u03b1\u2217i \u20162 \u2212 \u2016ui \u2212 \u03b1\u2217i \u20162 + (1\u2212 \u03b2i)\u2016vi\u20162 ) .\n(4)\nTaking expectation w.r.t. i \u223c q we obtain\nE[At\u22121 \u2212At] =\n\u03b7\u03bb n \u2211\ni=1\n1\nqi\n( \u2016\u03b1(t\u22121)i \u2212 \u03b1\u2217i \u20162 \u2212 \u2016ui \u2212 \u03b1\u2217i \u20162 + (1\u2212 \u03b2i)\u2016vi\u20162 )\n(5)\n= \u03b7\u03bb\n(\nAt\u22121 +\nn \u2211\ni=1\n1\nqi\n( \u2212\u2016ui \u2212 \u03b1\u2217i \u20162 + (1\u2212 \u03b2i)\u2016vi\u20162 )\n)\n.\n(6)\nAs to the second potential, we have\nBt\u22121 \u2212Bt = \u2212\u2016w(t) \u2212 w\u2217\u20162 + \u2016w(t\u22121) \u2212 w\u2217\u20162 (7) = 2 (w(t\u22121) \u2212 w\u2217)\u22a4(\u03b7 vi)\u2212 \u03b72i \u2016vi\u20162 .\nTaking expectation w.r.t. i \u223c q and noting that Ei\u223cq(\u03b7ivi) = \u03b7\u2207F (w(t\u22121)) we obtain\nE[Bt\u22121 \u2212Bt] =2\u03b7 (w(t\u22121) \u2212 w\u2217)\u22a4\u2207F (w(t\u22121)) (8)\n\u2212 \u03b7 2\nn2\n\u2211\ni\n1 qi \u2016vi\u20162 .\nWe now take a potential of the form Ct = caAt + cbBt.\nCombining (6) and (8) we obtain\nE[Ct\u22121 \u2212 Ct] = ca\u03b7\u03bbAt\u22121 \u2212 ca\u03b7\u03bb \u2211\ni\n1 qi \u2016ui \u2212 \u03b1\u2217i \u20162\n+ 2cb\u03b7(w (t\u22121) \u2212 w\u2217)\u22a4\u2207F (w(t\u22121))\n+ \u2211\ni\n1 qi \u2016vi\u20162\n(\nca\u03b7\u03bb(1 \u2212 \u03b2i)\u2212 cb\u03b7\n2\nn2\n)\n(9)\nWe will choose the parameters \u03b7, ca, cb such that\n\u03b7 \u2264 min { qi 2\u03bb , 1 4L\u0304 } and cb ca = \u03bbn2 2\u03b7 (10)\nThis implies that \u03b2i = \u03b7i\u03bbn = \u03b7\u03bb qi \u2264 1/2, and therefore the term in (9) is non-negative. Next, due to strong convexity of F we have that\n(w(t\u22121) \u2212 w\u2217)\u22a4\u2207F (w(t\u22121))\n\u2265 F (w(t\u22121))\u2212 F (w\u2217) + \u03bb 2 \u2016w(t\u22121) \u2212 w\u2217\u20162 .\nTherefore,\nE[Ct\u22121 \u2212 Ct] = ca\u03b7\u03bbAt\u22121 \u2212 ca\u03b7\u03bb \u2211\ni\n1 qi \u2016ui \u2212 \u03b1\u2217i \u20162\n+ 2cb\u03b7(F (w (t\u22121))\u2212 F (w\u2217)) + cb\u03b7\u03bbBt\u22121\n= \u03b7 \u03bbCt\u22121+\n\u03b7\n(\n2cb(F (w (t\u22121))\u2212 F (w\u2217))\u2212 ca\u03bb\n\u2211\ni\n1 qi \u2016ui \u2212 \u03b1\u2217i \u20162\n)\n.\n(11)\nNote that ui\u2212\u03b1\u2217i = \u2207\u03c6i(w(t\u22121))\u2212\u2207\u03c6i(w\u2217). In Lemma 3 we show that when \u03c6i is Li smooth and convex then\n\u2016\u2207\u03c6i(w(t\u22121))\u2212\u2207\u03c6i(w\u2217)\u20162 (12) \u2264 2Li (\u03c6i(w(t\u22121))\u2212 \u03c6i(w\u2217)\u2212\u2207\u03c6i(w\u2217)\u22a4(w(t\u22121) \u2212 w\u2217))\nTherefore, denoting \u03c4 = (\n2 maxi Li qi\n)\nwe obtain that\n\u2211\ni\n1 qi \u2016ui \u2212 \u03b1\u2217i \u20162 = \u2211\ni\n1 qi \u2016\u2207\u03c6i(w(t\u22121))\u2212\u2207\u03c6i(w\u2217)\u20162\n(13)\n\u2264 \u03c4 \u2211\ni\n(\u03c6i(w (t\u22121))\u2212 \u03c6i(w\u2217)\u2212\u2207\u03c6i(w\u2217)\u22a4(w(t\u22121) \u2212 w\u2217))\n= \u03c4 n\n(\nF (w(t\u22121))\u2212 F (w\u2217)\u2212 \u03bb 2 \u2016w(t\u22121) \u2212 w\u2217\u20162\n)\n\u2264 \u03c4 n ( F (w(t\u22121))\u2212 F (w\u2217) ) . (14)\nThe definition of qi implies that for every i,\nLi qi = 2nL\u0304 Li Li + L\u0304 \u2264 2nL\u0304 . (15)\nCombining this with (13) and (11) we obtain\nE[Ct\u22121 \u2212 Ct] \u2265 \u03b7 \u03bbCt\u22121 + \u03b7 ( 2cb \u2212 4n2L\u0304\u03bbca ) (F (w(t\u22121))\u2212 F (w\u2217))\nPlugging the value of cb = ca\u03bbn 2\n2\u03b7 yields that the coefficient in the last term is\n2 ca\u03bbn\n2\n2\u03b7 \u2212 4n2L\u0304\u03bbca = ca\u03bbn2\n(\n1 \u03b7 \u2212 4L\u0304\n)\n\u2265 0 ,\nwhere we used the choice of \u03b7 \u2264 1 4L\u0304 . In summary, we have shown that E[Ct\u22121 \u2212 Ct] \u2265 \u03b7 \u03bbCt\u22121, which implies that\nE[Ct] \u2264 (1\u2212 \u03b7 \u03bb)Ct\u22121 .\nTaking expectation over Ct\u22121 and continue recursively, we obtain that E[Ct] \u2264 (1\u2212 \u03b7 \u03bb)t C0 \u2264 e\u2212\u03b7 \u03bb t C0. Finally, since qi \u2265 1/(2n) for every i, we can choose\n\u03b7 = min\n{\n1\n4L\u0304 ,\n1\n4\u03bbn\n}\nand therefore 1\n\u03b7\u03bb \u2264 4\n(\nn+ L\u0304\n\u03bb\n)\n.\nThe proof is concluded by choosing cb = \u03bb/2 and ca = \u03b7/n2."}, {"heading": "3.2. Proof of Lemma 1", "text": "We have:\nE[\u2016w(t) \u2212 w(t\u22121)\u20162] = \u2211\ni\nqi\u03b7 2 i \u2016\u2207\u03c6i(w(t\u22121)) + \u03b1 (t\u22121) i \u20162\n\u2264 3\u03b7 2\nn2\n\u2211\ni\n1 qi (\u2016\u2207\u03c6i(w(t\u22121)) + \u03b1\u2217i \u20162\n+ \u2016\u03b1(t\u22121)i \u2212 \u03b1\u2217i \u20162) (triangle inequality)\n= 3\u03b72\nn2\n\u2211\ni\n( 1qi \u2016\u2207\u03c6i(w (t\u22121))\u2212\u2207\u03c6i(w\u2217)\u20162\n+ 1qi \u2016\u03b1 (t\u22121) i \u2212 \u03b1\u2217i \u20162)\n\u2264 3\u03b7 2\nn2\n\u2211\ni\n(\n2nL\u0304 \u2016w(t\u22121) \u2212 w\u2217\u20162 + 1qi \u2016\u03b1 (t\u22121) i \u2212 \u03b1\u2217i \u20162\n)\n(smoothness and (15))\n\u2264 3 \u03b7 (\n1 2\u2016w (t\u22121) \u2212 w\u2217\u20162 + Ct\u22121 )\n(because \u03b7 \u2264 1 4L\u0304 ) ."}, {"heading": "3.3. Proof of Theorem 2", "text": "The beginning of the proof is identical to the proof of Theorem 1. The change starts in (13), where we cannot apply (12) to \u03c6n+1 because it is not convex. To overcome this, we first apply (12) to \u03c61, . . . , \u03c6n, and obtain that\nn \u2211\ni=1\n1 qi \u2016ui \u2212 \u03b1\u2217i \u20162 =\nn \u2211\ni=1\n1 qi \u2016\u2207\u03c6i(w(t\u22121))\u2212\u2207\u03c6i(w\u2217)\u20162\n\u2264 (\n2 max i L\u0303i qi\n)\n\u00b7\nn \u2211\ni=1\n(\u03c6i(w (t\u22121))\u2212 \u03c6i(w\u2217)\u2212\u2207\u03c6i(w\u2217)\u22a4(w(t\u22121) \u2212 w\u2217))\n= 2 (n+ 1)\n(\nmax i L\u0303i qi\n)\n(F (w(t\u22121))\u2212 F (w\u2217)) ,\nwhere the last equality follows from the fact that \u2211n\ni=1 \u03c6i(w) = (n + 1)F (w), which also implies that \u2211\ni\u2207\u03c6i(w\u2217) = 0. In addition, since \u03c6n+1(w) =\n\u2212\u03bb(n+1)2 \u2016w\u20162, we have 1\nqn+1 \u2016\u2207\u03c6n+1(w) \u2212\u2207\u03c6n+1(w\u2217)\u20162 = \u03bb2(n+ 1)2\nqn+1 \u2016w \u2212 w\u2217\u20162\n= 2 (n+ 1) L\u0303n+1 qn+1 \u00b7 \u03bb 2 \u2016w \u2212 w\u2217\u20162\n\u2264 2 (n+ 1) L\u0303n+1 qn+1 (F (w) \u2212 F (w\u2217)) ,\nwhere the last inequality is because of the \u03bb-strong convexity of F . Combining the two inequalities, we obtain an analogue of (13),\nn+1 \u2211\ni=1\n1 qi \u2016ui \u2212 \u03b1\u2217i \u20162\n\u2264 4 (n+ 1) (\nmax i\u2208[n+1] L\u0303i qi\n)\n(F (w(t\u22121))\u2212 F (w\u2217)) .\nThe rest of the proof is almost identical, except that we have n replaced by n+1 and L\u0304 replaced by L\u0303 := 1n+1 \u2211n i=1 L\u0303i. We now need to choose\n\u03b7 = min\n{\n1\n8L\u0303 ,\n1\n4\u03bb(n+ 1)\n}\n.\nObserve that,\n(n+1)L\u0303 = n+ 1\nn\n(\nn \u2211\ni=1\nLi\n)\n+\u03bb(n+1) = (n+1)(L\u0304+\u03bb) ,\nso we can rewrite\n\u03b7 = min\n{\n1\n8(L\u0304+ \u03bb) ,\n1\n4\u03bb(n+ 1)\n}\n.\nThis yields 1\n\u03b7\u03bb \u2264 4\n(\nn+ 3 + 2L\u0304\n\u03bb\n)\n."}, {"heading": "3.4. Proof of Theorem 3", "text": "The beginning of the proof is identical to the proof of Theorem 1 up to (9).\nWe will choose the parameters \u03b7, ca, cb such that\n\u03b7 \u2264 min { qi 2\u03bb , 1 4L\u0304 } and cb ca = \u03bbn2 2\u03b7 (16)\nThis implies that \u03b2i = \u03b7i\u03bbn = \u03b7\u03bb qi \u2264 1/2, and therefore the term in (9) is non-negative. Next, due to strong convexity of F we have that\n(w(t\u22121) \u2212 w\u2217)\u22a4\u2207F (w(t\u22121))\n\u2265 F (w(t\u22121))\u2212 F (w\u2217) + \u03bb 2 \u2016w(t\u22121) \u2212 w\u2217\u20162 \u2265 \u03bb\u2016w(t\u22121) \u2212 w\u2217\u20162 .\nTherefore,\nE[Ct\u22121 \u2212 Ct]\n= ca\u03b7\u03bbAt\u22121 \u2212 ca\u03b7\u03bb \u2211\ni\n1 qi \u2016ui \u2212 \u03b1\u2217i \u20162 + 2cb\u03b7\u03bbBt\u22121\n= \u03b7 \u03bbCt\u22121 + \u03b7 \u03bb\n(\ncbBt\u22121 \u2212 ca \u2211\ni\n1 qi \u2016ui \u2212 \u03b1\u2217i \u20162\n)\n.\n(17)\nNext, we use the smoothness of the \u03c6i to get\n\u2211\ni\n1 qi \u2016ui \u2212 \u03b1\u2217i \u20162 = \u2211\ni\n1 qi \u2016\u2207\u03c6i(w(t\u22121))\u2212\u2207\u03c6i(w\u2217)\u20162\n\u2264 \u2211\ni\nL2i qi \u2016w(t\u22121) \u2212 w\u2217\u20162 = Bt\u22121 \u2211\ni\nL2i qi .\nThe definition of qi implies that for every i,\nLi qi = 2nL\u0304 Li Li + L\u0304 \u2264 2nL\u0304 ,\nso by combining with (17) we obtain\nE[Ct\u22121 \u2212 Ct] \u2265 \u03b7 \u03bbCt\u22121 + \u03b7\u03bb ( cb \u2212 2n2L\u03042ca ) Bt\u22121\nThe last term will be non-negative if cbca \u2265 2n 2L\u03042. Since we chose cbca = \u03bbn2 2\u03b7 we obtain the requirement\n\u03bbn2\n2\u03b7 \u2265 2n2L\u03042 \u21d2 \u03b7 \u2264 \u03bb 4L\u03042 .\nIn summary, we have shown that E[Ct\u22121\u2212Ct] \u2265 \u03b7 \u03bbCt\u22121. The rest of the proof is identical, but the requirement on \u03b7 is\n\u03b7 \u2264 min { \u03bb\n4L\u03042 ,\n1\n4\u03bbn\n}\n,\nand therefore 1\n\u03b7\u03bb \u2264 4\n(\nn+ L\u03042\n\u03bb2\n)\n."}, {"heading": "4. Proof of Theorem 4", "text": "Proof Each iteration of Algorithm 3 requires to minimize Gt to accuracy \u01ebt \u2264 O(1) (1 \u2212 \u03c1)t, where \u03c1 = 0.9 \u221a q. If t \u2264 T where T is as defined in Lemma 2, then we have that,\n\u2212t log(1\u2212\u03c1) \u2264 \u2212T log(1\u2212\u03c1) = \u2212 log(1 \u2212 \u03c1) \u03c1 log\n(\n800\nq \u01eb\n)\nUsing Lemma 4, \u2212 log(1\u2212\u03c1)\u03c1 \u2264 2 for every \u03c1 \u2208 (0, 1/2). In our case, \u03c1 is indeed in (0, 1/2) because of the definition of \u03ba and our assumption that (L\u0304/\u03bb)2 \u2265 3n. Hence,\nlog( 1\u01ebt ) = O(log((\u03bb+ \u03ba)/(\u03bb\u01eb))) .\nCombining this with Theorem 3, and using the definition of Gt, we obtain that the number of iterations required1 by each application of Algorithm 3 is\nO\u0303\n(\n(L\u0304+ \u03ba)2 (\u03bb+ \u03ba)2 + n\n)\n= O\u0303(n) ,\nwhere in the equality we used the definition of \u03ba. Finally, multiplying this by the value of T as given in Lemma 2 we obtain (ignoring log-terms):\n\u221a\n1 + \u03ba\n\u03bb n \u2264 (1 +\n\u221a\n\u03ba \u03bb )n = n+ n3/4\n\u221a\nL\u0304 \u03bb ."}, {"heading": "4.1. Technical Lemmas", "text": "Lemma 3 Assume that \u03c6 is L-smooth and convex. Then, for every w and u,\n\u2016\u2207\u03c6(w)\u2212\u2207\u03c6(u)\u20162 \u2264 2L [ \u03c6(w) \u2212 \u03c6(u)\u2212\u2207\u03c6(u)\u22a4(w \u2212 u) ] .\nProof For every i, define\ng(w) = \u03c6(w) \u2212 \u03c6(u)\u2212\u2207\u03c6(u)\u22a4(w \u2212 u) .\nClearly, since \u03c6 is L-smooth so is g. In addition, by convexity of \u03c6 we have g(w) \u2265 0 for all w. It follows that g is nonnegative and smooth, and therefore, it is self-bounded (see\n1While Theorem 3 bounds the expected sub-optimality, by techniques similar to (Shalev-Shwartz & Zhang, 2015) it can be converted to a bound that holds with high probability.\nSection 12.1.3 in (Shalev-Shwartz & Ben-David, 2014)):\n\u2016\u2207g(w)\u20162 \u2264 2Lg(w) .\nUsing the definition of g, we obtain\n\u2016\u2207\u03c6(w) \u2212\u2207\u03c6(u)\u20162\n= \u2016\u2207g(w)\u20162 \u2264 2Lg(w) = 2L [ \u03c6(w) \u2212 \u03c6(u)\u2212\u2207\u03c6(u)\u22a4(w \u2212 u) ] .\nLemma 4 For a \u2208 (0, 1/2)we have\u2212 log(1\u2212a)/a \u2264 1.4.\nProof Denote g(a) = \u2212 log(1 \u2212 a)/a. It is easy to verify that the derivative of g in (0, 1/2) is positive and that g(0.5) \u2264 1.4. The proof follows."}, {"heading": "5. Summary", "text": "We have described and analyzed a dual free version of SDCA that supports non-regularized objectives and nonconvex individual loss functions. Our analysis shows a linear rate of convergence for all of these cases. Two immediate open questions are whether the worse dependence on the condition number for the non-accelerated result for the non-convex case is necessary, and whether the factor n3/4 in Theorem 4 can be reduced to n1/2.\nAcknowledgements: In a previous draft of this paper, the bound for the non-convex case was n5/4+n3/4 \u221a\nL\u0304/\u03bb. We thank Ohad Shamir for showing us how to derive the improved bound of n + n3/4 \u221a\nL\u0304/\u03bb. The work is supported by ICRI-CI and by the European Research Council (TheoryDL project)."}], "references": [{"title": "A lower bound for the optimization of finite sums", "author": ["Agarwal", "Alekh", "Bottou", "Leon"], "venue": "In ICML,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Univr: A universal variance reduction framework for proximal stochastic gradient method", "author": ["Allen-Zhu", "Zeyuan", "Yuan", "Yang"], "venue": "arXiv preprint arXiv:1506.01972,", "citeRegEx": "Allen.Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allen.Zhu et al\\.", "year": 2015}, {"title": "On lower and upper bounds for smooth and strongly convex optimization problems", "author": ["Arjevani", "Yossi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "arXiv preprint arXiv:1503.06833,", "citeRegEx": "Arjevani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjevani et al\\.", "year": 2015}, {"title": "Primal method for erm with flexible mini-batching schemes and nonconvex losses", "author": ["Csiba", "Dominik", "Richt\u00e1rik", "Peter"], "venue": "arXiv preprint arXiv:1506.02227,", "citeRegEx": "Csiba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Csiba et al\\.", "year": 2015}, {"title": "New Optimisation Methods for Machine Learning", "author": ["Defazio", "Aaron"], "venue": "PhD thesis, Australian National Univer- sity,", "citeRegEx": "Defazio and Aaron.,? \\Q2014\\E", "shortCiteRegEx": "Defazio and Aaron.", "year": 2014}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Finito: A faster, permutable incremental gradient method for big data problems", "author": ["Defazio", "Aaron J", "Caetano", "Tib\u00e9rio S", "Domke", "Justin"], "venue": "arXiv preprint arXiv:1407.2710,", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Dual free sdca for empirical risk minimization with adaptive probabilities", "author": ["He", "Xi", "Tak\u00e1\u010d", "Martin"], "venue": "arXiv preprint arXiv:1510.06684,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Jin", "Chi", "Kakade", "Sham M", "Musco", "Cameron", "Netrapalli", "Praneeth", "Sidford", "Aaron"], "venue": "arXiv preprint arXiv:1510.08896,", "citeRegEx": "Jin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["Kone\u010dn\u1ef3", "Jakub", "Richt\u00e1rik", "Peter"], "venue": "arXiv preprint arXiv:1312.1666,", "citeRegEx": "Kone\u010dn\u1ef3 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kone\u010dn\u1ef3 et al\\.", "year": 2013}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Le Roux", "Nicolas", "Schmidt", "Mark", "Bach", "Francis"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "A universal catalyst for first-order optimization", "author": ["Lin", "Hongzhou", "Mairal", "Julien", "Harchaoui", "Zaid"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Mathematical Programming SERIES A and B (to appear),", "citeRegEx": "Shalev.Shwartz and Zhang,? \\Q2015\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2015}, {"title": "Sdca without duality", "author": ["Shalev-Shwartz", "Shai"], "venue": "arXiv preprint arXiv:1502.06177,", "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2015\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2015}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shalev-Shwartz", "Shai", "Ben-David"], "venue": "Cambridge university press,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2014}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2013}, {"title": "A stochastic pca and svd algorithm with an exponential convergence rate", "author": ["Shamir", "Ohad"], "venue": "In ICML,", "citeRegEx": "Shamir and Ohad.,? \\Q2015\\E", "shortCiteRegEx": "Shamir and Ohad.", "year": 2015}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Xiao", "Lin", "Zhang", "Tong"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": ", deep learning optimization problems, or problems arising in fast calculation of the top singular vectors (Jin et al., 2015)).", "startOffset": 107, "endOffset": 125}, {"referenceID": 2, "context": "Lower bounds have been derived in (Arjevani et al., 2015; Agarwal & Bottou, 2014).", "startOffset": 34, "endOffset": 81}, {"referenceID": 12, "context": "Applying an acceleration technique ((Shalev-Shwartz & Zhang, 2015; Lin et al., 2015)) we obtain the convergence rate \u00d5(n \u221a", "startOffset": 36, "endOffset": 84}, {"referenceID": 8, "context": "Using SVRG for non-convex individual functions has been recently studied in (Shamir, 2015; Jin et al., 2015), in the context of fast computation of the top singular vectors of a matrix.", "startOffset": 76, "endOffset": 108}, {"referenceID": 12, "context": "The algorithm has been generalized in (Lin et al., 2015) to allow the inner solver to be any algorithm.", "startOffset": 38, "endOffset": 56}, {"referenceID": 12, "context": "For completeness, we provide the pseudo-code of the \u201cCatalyst\u201d algorithm of (Lin et al., 2015) and its analysis.", "startOffset": 76, "endOffset": 94}, {"referenceID": 12, "context": "1 of (Lin et al., 2015) by observing that Algorithm 3 is a specification of Algorithm 1 in (Lin et al.", "startOffset": 5, "endOffset": 23}, {"referenceID": 12, "context": ", 2015) by observing that Algorithm 3 is a specification of Algorithm 1 in (Lin et al., 2015) with \u03b10 = \u221a q (which implies that \u03b1t = \u03b10 for every t), with \u01ebt = \u01eb0(1\u2212 \u03c1), and with \u03c1 = 0.", "startOffset": 75, "endOffset": 93}], "year": 2016, "abstractText": "Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularization and do not rely on duality. We prove linear convergence rates even if individual loss functions are non-convex, as long as the expected loss is strongly convex.", "creator": "LaTeX with hyperref package"}}}