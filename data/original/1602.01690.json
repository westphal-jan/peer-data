{"id": "1602.01690", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2016", "title": "Minimizing the Maximal Loss: How and Why?", "abstract": "A commonly used learning rule is to approximately minimize the \\emph{average} loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the \\emph{maximal} loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We prove that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers.", "histories": [["v1", "Thu, 4 Feb 2016 14:32:23 GMT  (47kb,D)", "https://arxiv.org/abs/1602.01690v1", null], ["v2", "Sun, 22 May 2016 15:13:56 GMT  (307kb,D)", "http://arxiv.org/abs/1602.01690v2", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz", "yonatan wexler"], "accepted": true, "id": "1602.01690"}, "pdf": {"name": "1602.01690.pdf", "metadata": {"source": "META", "title": "Minimizing the Maximal Loss: How and Why", "authors": ["Shai Shalev-Shwartz", "Yonatan Wexler"], "emails": ["SHAIS@CS.HUJI.AC.IL", "YONATAN.WEXLER@ORCAM.COM"], "sections": [{"heading": "1. Introduction", "text": "In a typical supervised learning scenario, we have training examples, S = ((x1, y1), . . . , (xm, ym)) \u2208 (X \u00d7Y)m, and our goal is to learn a function h : X \u2192 Y . We focus on the case in which h is parameterized by a vectorw \u2208 W \u2282 Rd, and we use hw to denote the function induced by w. The performance of w on an example (x, y) is assessed using a loss function, ` :W \u00d7X \u00d7 Y \u2192 [0, 1]. A commonly used learning rule is to approximately minimize the average loss, namely,\nmin w\u2208W\nLavg(w) := 1\nm\nm\u2211\ni=1\n`(w, xi, yi) . (1)\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nAnother option is to approximately minimize the maximal loss, namely,\nmin w\u2208W Lmax(w) := max i\u2208[m] `(w, xi, yi) . (2)\nObviously, if there existsw\u2217 \u2208 W such that `(w\u2217, xi, yi) = 0 for every i then the minimizers of both problems coincide. However, approximate solutions can be very different. In particular, since Lmax(w) \u2265 Lavg(w) for every w, the guarantee Lmax(w) < is stronger than the guarantee Lavg(w) < . Furthermore, for binary classification with the zero-one loss, any vector for which Lmax(w) < 1 must predict all the labels on the training set correctly, while the guarantee Lavg(w) < 1 is meaningless.\nSome classical machine learning algorithms can be viewed as approximately minimizing Lmax. For example, HardSVM effectively solves Lmax with respect to the loss function `(w, xi, yi) = \u03bb\u2016w\u20162 + 1[yi\u3008w, xi\u3009 < 1]. However, minimizingLavg is a more popular approach, especially for deep learning problems, in whichw is the vector of weights of a neural network and the optimization is performed using variants of stochastic gradient descent (SGD). There are several reasons to prefer Lavg over Lmax:\n1. If m is very large, it is not practical to perform operations on the entire training set. Instead, we prefer iterative algorithms that update w based on few examples at each iteration. This can be easily done for Lavg by observing that if we sample i uniformly at random from [m], then the gradient of `(w, xi, yi) with respect to w is an unbiased estimator of the gradient of Lavg(w). This property, which lies at the heart of the SGD algorithm, does not hold for Lmax.\n2. Our ultimate goal is not to minimize the loss on the training set but instead to have a small loss on unseen examples. As argued before, approximately minimizing Lmax can lead to a smaller loss on the training set, but it is not clear if this added accuracy will also be reflected in performance on unseen examples. Formal\nar X\niv :1\n60 2.\n01 69\n0v 2\n[ cs\n.L G\n] 2\n2 M\nay 2\n01 6\narguments of this nature were given in (Bousquet & Bottou, 2008; Shalev-Shwartz & Srebro, 2008).\n3. The objective Lmax is not robust to outliers. It is easy to see that even a single outlier can make the minimizer of Lmax meaningless.\nIn this paper we tackle the aforementioned disadvantages of Lmax, and by doing so, we show cases in which Lmax is preferable. In particular:\n1. We describe and analyze a meta algorithm that can take any online learner for w and convert it to a minimizer of Lmax. A detailed description of our meta algorithm, its analysis, and a comparison to other approaches, are given in Section 2.\n2. The arguments in (Bousquet & Bottou, 2008; ShalevShwartz & Srebro, 2008) rely on a comparison of upper bounds. We show that these upper bounds are not tight in many cases. Furthermore, we analyze the sample complexity of learning in situations where the training examples are divided to \u201ctypical\u201d scenarios and \u201crare\u201d scenarios. We argue that in many practical cases, our goal is to have a high accuracy on both typical and rare examples. We show conditions under which minimizing even few rare examples suffice to guarantee good performance on unseen examples from the rare scenario. In other words, few examples can have a dramatic effect on the performance of the learnt classifier on unseen examples. This is described and analyzed in Section 3.\n3. Finally, in Section 4 we review standard techniques for generalizing the results from realizable cases to scenarios in which there might be outliers in the data.\nTo summarize, we argue that in some situations minimizing Lmax is better than minimizing Lavg. We address the \u201chow\u201d question in Section 2, the \u201cwhy\u201d question in Section 3, and the issue of robustness in Section 4. Finally, in Section 5 we provide some empirical evidence, showing the effectiveness of our algorithm on real world learning problems."}, {"heading": "2. How", "text": "In this section we describe and analyze an algorithmic framework for approximately solving the optimization problem given in (2).\nDenote by Sm = {p \u2208 [0, 1]m : \u2016p\u20161 = 1} the probabilistic simplex over m items. We also denote by \u039b : W \u2192 [0, 1]m the function defined by\n\u039b(w) = (`(w, x1, y1), . . . , `(w, xm, ym)) .\nThe first step is to note that the optimization problem given in (2) is equivalent to\nmin w\u2208W max p\u2208Sm\n\u3008p,\u039b(w)\u3009 . (3)\nThis is true because for every w, the p that maximizes the inner optimization is the all zeros vector except 1 in the coordinate for which `(w, xi, yi) is maximal.\nWe can now think of (3) as a zero-sum game between twoplayers. The p player tries to maximize \u3008p,\u039b(w)\u3009 while the w player tries to minimize \u3008p,\u039b(w)\u3009. The optimization process is comprised of T game rounds. At round t, the p player defines pt \u2208 Sm and the w player defines wt \u2208 W . We then sample it \u223c pt and define the value of the round to be `(wt, xit , yit).\nTo derive a concrete algorithm we need to specify how player p picks pt and how player w picks wt. For the w player one can use any online learning algorithm. We specify the requirement from the algorithm below.\nDefinition 1 (Mistake bound for the w player) We say that the w player enjoys a mistake bound of C if for every sequence of indices (i1, . . . , iT ) \u2208 [m]T we have that\nT\u2211\nt=1\n`(wt, xit , yit) \u2264 C . (4)\nExample 1 Consider a binary classification problem in which the data is linearly separable by a vector w\u2217 with a margin of 1. Let the loss function be the zero-one loss, namely, `(w, x, y) = 1[y\u3008w, x\u3009 \u2264 0], where 1[boolean expression] is 1 if the boolean expression holds and 0 otherwise. We can use the online Perceptron algorithm as our w learner and it is well known that the Perceptron enjoys the mistake bound of C = \u2016w\u2217\u20162 maxi\u2208[m] \u2016xi\u20162 (for a reference, see for example (Shalev-Shwartz, 2011)).\nFor the p player, we use the seminal work of (Auer et al., 2002). In particular, recall that the goal of the p player is to maximize the loss, `(wt, xit , yit , where it \u223c pt. The basic idea of the construction is therefore to think of the m examples as m slot machines, where at round t the gain of pulling the arms of the different machines is according to \u039b(wt) \u2208 [0, 1]m. Crucially, the work of (Auer et al., 2002) does not assume that \u039b(wt) are sampled from a fixed distribution, but rather the vectors \u039b(wt) can be chosen by an adversary. As observed in Auer et al. (2002, Section 9), this naturally fits zero-sum games, as we consider here.\nIn (Auer et al., 2002) it is proposed to rely on the algorithm EXP3.P.1 as the strategy for the p-player. The acronym EXP3 stands for Exploration-Exploitation-Exponent, because the algorithm balances between exploration and ex-\nploitation and rely on an exponentiated gradient framework. The \u201cP\u201d in EXP3.P.1 stands for a regret bound that holds with high probability. This is essential for our analysis because we will later apply a union bound over the m examples. While the EXP3.P.1 algorithm gives the desired regret analysis, the runtime per iteration of this algorithm scales with m. Here, we propose another variant of EXP3 for which the runtime per iteration is O(log(m)).\nTo describe our strategy for the p player, recall that it maintains pt \u2208 Sm. We will instead maintain another vector, qt \u2208 Sm, and will set pt to be the vector such that pt,i = 1 2qt,i + 1 2m . That is, pt is a half-half mix of qt with the uniform distribution. While in general such a strong mix with the uniform distribution can hurt the regret, in our case it only affects the convergence rate by a constant factor. On the up side, this strong exploration helps us having an update step that takes O(log(m)) per iteration.\nRecall that at round t of the algorithm, we sample it \u223c pt and the value of the round is `(wt, xit , yit). Denote zt = \u2212 `it (wt)pit eit , then it is easy to verify that Eit\u223cpt [zt] = \u2212\u039b(wt). Therefore, applying gradient descent with respect to the linear function \u3008\u00b7, zt\u3009 is in expectation equivalent to applying gradient descent with respect to the linear function \u2212\u3008\u00b7,\u039b(wt)\u3009, which is the function the p player aims at minimizing. Instead of gradient descent, we use the exponentiated gradient descent approach which applies gradient descent in the log space, namely, the update can be written as log(qt+1) = log(qt) + \u03b7zt.\nA pseudo-code of the resulting algorithm is given in Section 2.3. Observe that we use a tree structure to hold the vector q, and since all but the it coordinate of zt are zeros, we can implement the update of q in O(log(m)) time per iteration. The following theorem summarizes the convergence of the resulting algorithm.\nTheorem 1 Suppose the we have an oracle access to an online algorithm that enjoys a mistake bound of C with respect to the training examples (x1, y1), . . . , (xm, ym). Fix , \u03b4, and suppose we run the FOL algorithm with T, k such that C/T \u2264 /8, T = \u2126(m log(m/\u03b4)/ ), and k = \u2126(log(m/\u03b4)/ ), and with \u03b7 = 1/(2m). Then, with probability of at least 1\u2212 \u03b4,\nmax i\n1\nk\nk\u2211\nj=1\n`(wtj , xi, yi) \u2264 .\nThe proof of the theorem is given in Appendix A.\nThe above theorem tells us that we can find an ensemble of O(log(m)/ ) predictors, such that the ensemble loss is smaller than for all of the examples.\nWe next need to show that we can construct a single pre-\ndictor with a small loss. To do so, we consider two typical scenarios. The first is classification settings, in which `(w, x, y) is the zero-one loss and the second is convex losses in which `(w, x, y) has the form \u03c6y(hw(x)), where for every y, \u03c6y is a convex function."}, {"heading": "2.1. Classification", "text": "In classification, `(w, x, y) is the zero-one loss, namely, it equals to zero if hw(x) = y and it equals to 1 if hw(x) 6= y. We will take to be any number strictly smaller than 1/2, say 0.499.\nObserve that Theorem 1 tells us that the average loss of the classifiers wt1 , . . . , wtk is smaller than = 0.499. Since the values of the loss are either 1 or 0, it means that the loss of more than 1/2 of the classifiers is 0, which implies that the majority classifier has a zero loss.\nCorollary 1 Assume that `(w, x, y) is the zero-one loss function, namely, `(w, x, y) = 1[hw(x) 6= y]. Apply Theorem 1 with = 0.49. Then, with probability of at least 1 \u2212 \u03b4, the majority classifier of hwt1 , . . . , hwtk is consistent, namely, it makes no mistakes on the entire training set.\nExample 2 Consider again the linear binary classification problem given in Example 1, where we use the online Perceptron algorithm as our w learner, and its mistake bound is C as given in Example 1. Then, after O\u0303 (m+ C) iterations, we will find an ensemble of O(log(m)) halfspaces, whose majority vote is consistent with all the examples. In Section 2.4 we compare the runtime of the method to stateof-the-art approaches. Here we just note that to obtain a consistent hypothesis using SGD one needs order of mC iterations, which is significantly larger in most scenarios."}, {"heading": "2.2. Convex Losses", "text": "Consider now the case in which `(w, x, y) has the form \u03c6y(hw(x)), where for every y, \u03c6y is a convex function. Note that this assumption alone does not imply that ` is a convex function of w (this will be true only if hw(x) is an affine function).\nIn the case of convex \u03c6y , combining Theorem 1 with Jensen\u2019s inequality we obtain:\nCorollary 2 Under the assumptions of Theorem 1, if `(w, x, y) has the form \u03c6y(hw(x)), where for every y, \u03c6y is a convex function, then the predictor h(x) = 1 k \u2211k j=1 hwtj (x) satisfies \u2200i, \u03c6yi(h(xi)) \u2264 . If we further assume that hw(x) is an affine function of w, and let w = 1k \u2211k j=1 wtj , then we also have that\n\u2200i, \u03c6yi(hw(xi)) \u2264 ."}, {"heading": "2.3. Pseudo-code", "text": "Below we describe a pseudo-code of the algorithm. We rely on a tree data structure for maintaining the probability of the p-player. It is easy to verify the correctness of the implementation. Observe that the runtime of each iteration is the time required to perform one step of the online learner plusO(log(m)) for sampling from pt and updating the tree structure.\nFocused Online Learning (FOL)\nInput: Training examples (x1, y1), . . . , (xm, ym) Loss function ` :W \u00d7X \u00d7 Y \u2192 [0, 1] Parameters \u03b7, T, k Oracle access to online learning algorithm OLA Initialization: Tree.initialize(m) (see the Tree pseudo-code) w1 = OLA.initialize() Loop over t \u2208 {1, . . . , T}: (it, pit) = Tree.sample(1/2) OLA.step(xit , yit) Tree.update(it, exp(\u03b7 `(wt, xit , yit)/pit)) Output: Sample (t1, . . . , tk) indices uniformly from [T ] Output Majority/Average of (hwt1 , . . . , hwtk )\nTree\ninitialize(m) Build a full binary tree of height h = dlog2(m)e Set value of the first m leaves to 1 and the rest to 0 Set the value of each internal node to be\nthe sum of its two children Let qi be the value of the i\u2019th leaf divided by\nthe value of the root sample(\u03b3)\nSample b \u2208 {0, 1} s.t. P[b = 0] = \u03b3 If b = 0\nSample i uniformly at random from [m] Else\nSet v to be the root node of the tree While v is not a leaf:\nGo to the left/right child by sampling according to their values\nLet i be the obtained leaf Return: (i, \u03b3/m+ (1\u2212 \u03b3)qi)\nupdate(i, f ) Let v be the current value of the i\u2019th leaf of the tree Let \u03b4 = f v \u2212 v Add \u03b4 to the values of all nodes on\nthe path from the i\u2019th leaf to the root"}, {"heading": "2.4. Related Work", "text": "As mentioned before, our algorithm is a variant of the approach given in Auer et al. (2002, Section 9), but has the advantage that the update of the p player at each iteration scales with log(m) rather than with m. Phrasing the maxloss minimization as a two players game has also been proposed by (Clarkson et al., 2012; Hazan et al., 2011). These works focus on the specific case of binary classification with a linear predictor, namely, they tackle the problem minw\u2208Rd:\u2016w\u20162\u22641 maxp\u2208Sm \u2211 i pi\u3008w, xi\u3009. Assuming the setup of Example 1, (Clarkson et al., 2012) presents an algorithm that finds a consistent hypothesis in runtime of O\u0303((m + d) \u00b7 C). For the same problem, our algorithm (with the Perceptron as the weak learner) finds a consistent hypothesis in runtime of O\u0303((m + C) \u00b7 d). Furthermore, if the instances are d\u0304-sparse (meaning that the number of nonzeros in each xi is at most d\u0304), then the term d in our bound can be replaced by d\u0304. In any case, our bound is sometimes better and sometimes worse than the one in (Clarkson et al., 2012). We note that we can also use AdaBoost (Freund & Schapire, 1995) on top of the Perceptron algorithm for the same problem. It can be easily verified that the resulting runtime will be identical to our bound. In this sense, our algorithm can be seen as an online version of AdaBoost.\nFinally, several recent works use sampling strategies for speeding up optimization algorithms for minimizing the average loss. See for example (Bengio & Sene\u0301cal, 2008; Bouchard et al., 2015; Zhao & Zhang, 2014; Allen-Zhu & Yuan, 2015)."}, {"heading": "3. Why", "text": "In this section we tackle the \u201cWhy\u201d question, namely, why should we prefer minimizing the maximal loss instead of the average loss. For simplicity of presentation, throughout this section we deal with binary classification problems with the zero-one loss, in the realizable setting. In this context, minimizing the maximal loss to accuracy of < 1 leads to a consistent hypothesis1. On the other hand, minimizing the average loss to any accuracy of > 1/m does not guarantee to return a consistent hypothesis. Therefore, in this context, the \u201cwhy\u201d question becomes: why should we find a consistent hypothesis and not be satisfied with a hypothesis with Lavg(h) \u2264 for some > 1/m. In the usual PAC learning model (see (Shalev-Shwartz & Ben-David, 2014) for an overview), there is a distribution D over X \u00d7Y and the training examples are assumed to be\n1Recall that a consistent hypothesis is a hypothesis that makes no mistakes on the training set. We also use the term Empirical Risk Minimization (ERM) to describe the process of finding a consistent hypothesis, and use ERM(S) to denote any hypothesis which is consistent with a sample S.\nsampled i.i.d. from D. The goal of the learner is to minimize LD(h) := E(x,y)\u223cD[`(h, x, y)] = P(x,y)\u223cD[h(x) 6= y]. For a fixed h \u2208 H, the random variable Lavg(h) is an unbiased estimator ofLD(h). Furthermore, it can be shown (Boucheron et al. (2005, Section 5.1.2)) that with probability of at least 1\u2212 \u03b4 over the choice of the sample S \u223c Dm we have that:\n\u2200h \u2208 H, LD(h) \u2264 Lavg(h)+\nO\u0303 (\u221a Lavg(h)\nVC(H)\u2212 log(\u03b4) m + VC(H)\u2212 log(\u03b4) m\n)\nwhere VC(H) is the VC dimension of the class H and the notation O\u0303 hides constants and logarithmic terms.\nFrom the above bound we get that any h with Lavg(h) = 0 (i.e., a consistent h) guarantees that LD(h) = O\u0303 (\nVC(H)+log(1/\u03b4) m\n) . However, we will obtain the same\nguarantee (up to constants) if we will choose any h with Lavg(h) \u2264 , for = O\u0303 ( VC(H)+log(1/\u03b4)\nm\n) . Based on this\nobservation, it can be argued that it is enough to minimize Lavg to accuracy of = O\u0303 ( VC(H)+log(1/\u03b4)\nm\n) > 1m , be-\ncause a better accuracy on the training set will in any case get lost by the sampling noise.\nFurthermore, because of either computational reasons or high dimensionality of the data, we often do not directly minimize the zero-one loss, and instead minimize a convex surrogate loss, such as the hinge-loss. In such cases, we often rely on a margin based analysis, which means that the term VC(H) is replaced byB2, whereB is an upper bound on the norm of the weight vector that defines the classifier. It is often the case that the convergence rate of SGD is of the same order, and therefore there is no added value of solving the ERM problem over performing a single SGD pass over the data (or few epochs over the data). Formal arguments of this nature were given in (Bousquet & Bottou, 2008; Shalev-Shwartz & Srebro, 2008).\nDespite of these arguments, we show below reasons to prefer the max loss formulation over the average loss formulation. The first reason is straightforward: arguments that are based on worst case bounds are problematic, since in many cases the behavior is rather different than the worst case bounds. In subsection 3.1 we present a simple example in which there is a large gap between the sample complexity of SGD and the sample complexity of ERM, and we further show that the runtime of our algorithm will be much better than the runtime of SGD for solving this problem.\nNext, we describe a family of problems in which the distribution from which the training data is being sampled is a mix of \u201ctypical\u201d examples and \u201crare\u201d examples. We show that in such a case, few \u201crare\u201d examples may be sufficient for learning a hypothesis that has a high accuracy on both\nthe \u201ctypical\u201d and \u201crare\u201d examples, and therefore, it is really required to solve the ERM problem as opposed to being satisfied with a hypothesis for which Lavg(h) is small."}, {"heading": "3.1. A Simple Example of a Gap", "text": "Consider the following distribution. Let z1 = (\u03b1, 1) and z2 = (\u03b1,\u22122\u03b1) for some small \u03b1 > 0. To generate an example (x, y) \u223c D, we first sample a label y uniformly at random from {\u00b11}, then we set x = yz1 with probability 1 \u2212 and set x = yz2 with probability . The hypothesis class is halfspaces: H = {x\u2192 sign(\u3008w, x\u3009) : w \u2208 R2}. The following three lemmas, whose proofs are given in the appendix, establish the gap between the different approaches.\nLemma 1 For every \u03b4 \u2208 (0, 1), if m \u2265 2 log(4/\u03b4) then, with probability of at least 1 \u2212 \u03b4 over the choice of the training set, S \u223c Dm, any hypothesis in ERM(S) has a generalization error of 0.\nLemma 2 Suppose we run SGD with the hinge-loss and any \u03b7 > 0 for less than T = \u2126(1/(\u03b1 )) iterations. Then, with probability of 1\u2212O( ) we have that SGD will not find a solution with error smaller than .\nLemma 3 Running FOL (with the Perceptron as its w player) takes O\u0303 ( 1 +\n1 \u03b1\n) iterations."}, {"heading": "3.2. Typical vs. Rare Distributions", "text": "To motivate the learning setting, consider the problem of face detection, in which the goal is to take an image crop and determine whether it is an image of a face or not. An illustration of typical random positive and negative examples is given in Figure 1 (top row). By having enough training examples, we can learn that the discrimination between face and non-face is based on few features like \u201can ellipse shape\u201d, \u201ceyes\u201d, \u201cnose\u201d, and \u201cmouth\u201d. However, from the typical examples it is hard to tell whether an image of a watermelon is a face or not \u2014 it has the ellipse shape like a face, and something that looks like eyes, but it doesn\u2019t have a nose, or a mouth. The bottom row of Figure 1 shows some additional \u201crare\u201d examples.\nSuch a phenomenon can be formally described as follows. There are two distributions over the examples, D1 and D2. Our goal is to have an error of at most on both distributions, namely, we would like to find h such thatLD1(h) \u2264 and LD2(h) \u2264 . However, the training examples that we observe are sampled i.i.d. from a mixed distribution, D = \u03bb1D1+\u03bb2D2, where \u03bb1, \u03bb2 \u2208 (0, 1) and \u03bb1+\u03bb2 = 1. We assume that \u03bb2 \u03bb1, namely, typical examples in the training set are from D1 while examples from D2 are rare. Fix some . If \u03bb2 < , then a hypothesis with Lavg(h) \u2264\nMinimizing the Maximal Loss: How and WhyMinimizing the Maximal Loss: How and Why\nFigure 1. Top: typical positive (left) and negative (right) examples. Bottom: rare negative examples.\nmight err on most of the \u201crare\u201d examples, and is therefore likely to have LD2(h) > \u270f. If we want to guarantee a good performance on D2 we must optimize to a very high accuracy, or put another way, we would like to minimize Lmax instead of Lavg. The question is how many examples do we need in order to guarantee that a consistent hypothesis on S will have a small error on both D1 and D2. A naive approach is to require order of VC(H)/( 2\u270f) examples, thus ensuring that we have order of VC(H)/\u270f examples from both D1 and D2. However, this is a rough estimate and the real sample complexity might be much smaller. Intuitively, we can think of the typical examples from D1 as filtering out most of the hypotheses in H, and the goal of the rare examples is just to fine tune the exact hypothesis. In the example of face detection, the examples from D1 will help us figure out what is an \u201cellipse like shape\u201d, what is an \u201ceye\u201d, and what is a \u201cmouth\u201d and a \u201cnose\u201d. After we understand all this, the rare examples from D2 will tell us the exact requirement of being a face (e.g., you need an ellipse like shape and either eyes or a mouth). We can therefore hope that the number of required \u201crare\u201d examples is much smaller than the number of required \u201ctypical\u201d examples. This intuition is formalized in the following theorem.\nTheorem 2 Fix \u270f, 2 (0, 1), distributions D1, D2, and let D = 1D1 + 2D2 where 1 + 2 = 1, 1, 2 2 [0, 1], and 2 < 1. Define H1,\u270f = {h 2 H : LD1(h)  \u270f} and c = max{c0 2 [\u270f, 1) : 8h 2 H1,\u270f, LD2(h)  c0 ) LD2(h)  \u270f}. Then, if\nm \u2326 \u2713\nVC(H) log(1/\u270f) + log(1/ ) \u270f +\nVC(H1,\u270f) log(1/c) + log(1/ ) c 2\n\u25c6\nwe have, with probability of at least 1 over the sampling of a sample S \u21e0 Dm:\nLD1(ERM(S))  \u270f and LD2(ERM(S))  \u270f\nThe proof of the theorem is given in the appendix. The first term in the sample complexity is a standard VC-based sample complexity. The second term makes two crucial\nimprovement. First, we measure the VC dimension of a reduced class (H1,\u270f), containing only those hypotheses in H that have a small error on the \u201ctypical\u201d distribution. Intuitively, this will be a much smaller hypothesis class compared to the original class. Second, we apply an analysis of the sample complexity similar to the \u201cshell analysis\u201d of (Haussler et al., 1996), and assume that the error of all hypotheses in H1,\u270f on D2 is either smaller than \u270f or larger than c, where we would like to think of c as being significantly larger than \u270f. Naturally, this will not always be the case. But, Theorem 2 provides data dependent conditions, under which a much smaller number of examples from D2 is sufficient. As a motivation, consider again Figure 1, and suppose H1,\u270f contains conjunctions over all subsets of the features \u201chas eyes\u201d, \u201chas nose\u201d, \u201chas mouth\u201d, \u201chas skin color\u201d. Let h\u21e4 be the conjunction of all these 4 features. It is reasonable to assume that examples in D2 lack one of these features. Let us also assume for simplicity that each lacking feature takes at least 1/8 of the mass of D2. Hence, the error of all \u201cwrong\u201d functions in H1,\u270f on D2 is at least 1/8, while the error of h\u21e4 is 0. We see that in this simple example, c = 1/8.\nAll in all, the theorem shows that a small number of \u201crare\u201d examples in the training set can have a dramatic effect on the performance of the algorithm on the rare distribution D2. But, we will see this effect only if we will indeed find a hypothesis consistent with all (or most) examples from D2, which requires an algorithm for minimizing Lmax and not Lavg.\n4. Robustness In the previous section we have shown cases in which minimizing Lmax is better than minimizing Lavg. However, in the presence of outliers, minimizing Lmax might lead to meaningless results \u2014 even a single outlier can change the value of Lmax and might lead to a trivial, non-interesting, solution. In this section we describe two tricks for addressing this problem. The first trick replaces the original sample with a new sample whose examples are sampled from the original sample. The second trick relies on slack variables. We note that these tricks are not new and appears in the literature in various forms. See for example (Huber & Ronchetti, 2009; Maronna et al., 2006). The goal of this section is merely to show how to apply known tricks to the max loss problem.\nRecall that in the previous section we have shown that a small amount of \u201crare\u201d examples can have a dramatic effect on the performance of the algorithm on the \u201crare\u201d distribution. Naturally, if the number of outliers is larger than the number of rare examples we cannot hope to enjoy the benefit of rare examples. Therefore, throughout this section we assume that the number of outliers, denoted k, is smaller\nFigure 1. op: t i l iti l ft) and negative (right) examples. Botto : r r ti l s.\nmight err on ost of the \u201crare\u201d exa ples, and is therefore likely to have LD2(h) > . If we want to guarantee a good performance on D2 we must optimize to a very high accuracy, or put another way, we would like to minimize Lmax instead of Lavg. The question is how many examples do we need in order to guarantee that a consistent hypothesis on S will have a small error on both D1 and D2. A naive approach is to require order of VC(H)/(\u03bb2 ) examples, thus ensuring that we have order of VC(H)/ examples from both D1 and D2. However, this is a rough estimate and the real sample complexity might be much smaller. Intuitively, we can think of the typical examples from D1 as filtering out most of the hypotheses in H, and the goal of the rare examples is just to fine tune the exact hypothesis. In the example of face detection, the examples from D1 will help us figure out what is an \u201cellipse like shape\u201d, what is an \u201ceye\u201d, and what is a \u201cmouth\u201d and a \u201cnose\u201d. After we understand all this, the rare examples from D2 will tell us the exact requirement of being a face (e.g., you need an ellipse like shape and either eyes or a mouth). We can therefore hope that the number of required \u201crare\u201d examples is much smaller than the number of required \u201ctypical\u201d examples. This intuition is formalized in the following theor m.\nTheorem 2 Fix , \u03b4 \u2208 (0, 1), distributions D1, D2, and let D = \u03bb1D1 + \u03bb2D2 where \u03bb1 + \u03bb2 = 1, \u03bb1, \u03bb2 \u2208 [0, 1], and \u03bb2 < \u03bb1. Define H1, = {h \u2208 H : LD1(h) \u2264 } and c = max{c\u2032 \u2208 [ , 1) : \u2200h \u2208 H1, , LD2(h) \u2264 c\u2032 \u21d2 LD2(h) \u2264 }. Then, if\nm \u2265 \u2126 ( VC(H) log(1/ ) + log(1/\u03b4) +\nVC(H1, ) log(1/c) + log(1/\u03b4) c \u03bb2\n)\nwe have, with probability of at least 1\u2212\u03b4 over the sampling of a sample S \u223c Dm:\nLD1(ERM(S)) \u2264 and LD2(ERM(S)) \u2264\nThe proof of the theorem is given in the appendix. The first term in the sample complexity is a standard VC-based sample complexity. The second term makes two crucial\nimprovement. First, we measure the VC dimension of a reduced class (H1, ), containing only those hypotheses in H that have a small error on the \u201ctypical\u201d distribution. Intuitively, this will be a much smaller hypothesis class compared to the original class. Second, we apply an alysis of the sample complexity similar to he \u201cshell analysis\u201d of (Haussler et al., 1996), and assume that the erro f all hypotheses in H1, on D2 is either smaller than or large than c, wher we would like to think of c as being sign ficantly larger than . Naturally, this will not always be the case. But, Theorem 2 provides data dep ndent conditions, under which a much smaller number of examples from D2 is sufficient. As a motivation, consider again Figure 1, and suppose H1, contains conjunctions over all subsets of the features \u201chas eyes\u201d, \u201chas nose\u201d, \u201chas mouth\u201d, \u201chas skin color\u201d. Let h\u2217 be the conjunction of all these 4 features. It is reasonable to assume that examples in D2 lack one of these features. Let us also assume for simplicity that each lacking feature takes at least 1/8 of the mass ofD2. Hence, the error of all \u201cwrong\u201d functions in H1, on D2 is at least 1/8, while the error of h\u2217 is 0. We see that in this simple example, c = 1/8.\nAll in all, the theorem shows that a small number of \u201crare\u201d examples in the training set can have a dramatic effect on the performance of the algorithm on the rare distribution D2. But, we will see this effect only if we will indeed find a hypothesis consistent with all (or most) examples from D2, which requires an algorithm for minimizing Lmax and not Lavg."}, {"heading": "4. Robustness", "text": "In the previous section we have shown cases in which minimizing Lmax is better than minimizing Lavg. However, in the presence of outliers, mi i izing Lmax might lead to meaningles results \u2014 ev n a si gle outlier can change the value of Lmax and might lead to a trivial, non-interesting, solution. In this section we describe two tricks for addressing this problem. The first trick replaces the original sample with a new sample whose examples are sampled from the original sample. Th sec nd trick relies on slack variables. We ote that these tricks are not new and appears in the literature in various forms. See for example (Huber & Ronchetti, 2009; Maronna et al., 2006). The goal of this section is merely to show how to apply known tricks to the max loss problem.\nRecall that in the previous section we have shown that a small amount of \u201crare\u201d examples can have a dramatic effect on the performance of the algorithm on the \u201crare\u201d distribution. Naturally, if the number of outliers is larger than the number of rare examples we cannot hope to enjoy the benefit of rare examples. Therefore, throughout this section we assume that the number of outliers, denoted k, is smaller\nthan the number of \u201crare\u201d examples, which we denote by m2."}, {"heading": "4.1. Sub-sampling with repetitions", "text": "The first trick we consider is to simply take a new sample of n examples, where each example in the new sample is sampled independently according to the uniform distribution over the original m examples. Then, we run our algorithm on the obtained sample of n examples.\nIntuitively, if there are k outliers, and the size of the new sample is significantly smaller than m/k, then there is a good chance that no outliers will fall into the new sample. On the other hand, we want that enough \u201crare\u201d examples will fall into the new sample. The following theorem, whose proof is in the appendix, shows for which values of k and m2 this is possible.\nTheorem 3 Let k be the number of outliers, m2 be the number of rare examples, m be the size of the original sample, and n be the size of the new sample. Assume that m \u2265 10k. Then, the probability that the new sample contains outliers and/or does not contain at least m2/2 rare examples is at most 0.01 + 0.99kn/m+ e\u22120.1nm2/m.\nFor example, if n = m/(100k) and m2 \u2265 1000 log(100) k, then the probability of the bad event is at most 0.03."}, {"heading": "4.2. Slack variables", "text": "Another common trick, often used in the SVM literature, is to introduce a vector of slack variables, \u03be \u2208 Rm, such that \u03bei > 0 indicates that example i is an outlier. We first describe the ideal version of outlier removal. Suppose we restrict \u03bei to take values in {0, 1}, and we restrict the number of outliers to be at most K. Then, we can write the following optimization problem:\nmin w\u2208W,\u03be\u2208Rm max i\u2208[m]\n(1\u2212 \u03bei) `(w, xi, yi) s.t.\n\u03be \u2208 {0, 1}m, \u2016\u03be\u20161 \u2264 K .\nThis optimization problem minimizes the max loss over a subset of examples of size at leastm\u2212K. That is, we allow the algorithm to refer to at most K examples as outliers.\nNote that the above problem can be written as a max-loss minimization:\nmin w\u0304\u2208W\u0304 max i\n\u00af\u0300(w\u0304, xi, yi) where\nW\u0304 = {(w, \u03be) : w \u2208 W, \u03be \u2208 {0, 1}m, \u2016\u03be\u20161 \u2264 K} and \u00af\u0300((w, \u03be), xi, yi) = (1\u2212 \u03bei)`(w, xi, yi)\nWe can now apply our framework on this modified problem. The p player remains as before, but now the w\u0304\nplayer has a more difficult task. To make the task easier we can perform several relaxations. First, we can replace the non-convex constraint \u03be \u2208 {0, 1}m with the convex constraint \u03be \u2208 [0, 1]m. Second, we can replace the multiplicative slack with an additive slack, and re-define: \u00af\u0300((w, \u03be), xi, yi) = `(w, xi, yi) \u2212 \u03bei. This adds a convex term to the loss function, and therefore, if the original loss was convex we end up with a convex loss. The new problem can often be solved by combining gradient updates with projections of \u03be onto the set \u03be \u2208 [0, 1]m, \u2016\u03be\u20161 \u2264 K. For efficient implementations of this projection see for example (Duchi et al., 2008). We can further replace the constraint \u2016\u03be\u20161 \u2264 K with a constraint of \u2016\u03be\u201622 \u2264 K, because projection onto the Euclidean ball is a simple scaling, and the operation can be done efficiently with an adequate data structure (as described, for example, in (Shalev-Shwartz et al., 2011))."}, {"heading": "5. Experiments", "text": "In this section we demonstrate several merits of our approach on the well studied problem of face detection. Detection problems in general have a biased distribution as they are often expected to detect few needles in a haystack. Furthermore, a mix of typical and rare distributions is to be expected. For example, users of smartphones won\u2019t be in the same continent as the manufacturers who collect data for training. This domain requires weighting of examples, and therefore is a good playground to examine our algorithm.\nTo create a dataset we downloaded 30k photos from Google images that are tagged with \u201cface\u201d. We then applied an off-the-shelf face detector, and it found 32k rectangles that aligned on faces. This was the base of our positive examples. For negative examples we randomly sampled 250k rectangles in the same images that do not overlap faces. Each rectangle was cropped and scaled to 28\u00d728 pixels. Using a fixed size simplifies the experiments so we can focus on the merits of our method rather than justify various choices that are not relevant here, such as localization and range of scale.\nRecall that our FOL algorithm relies on an online algorithm as the w player. In the experiments, w is the vector containing all the weights of a convolutional neural network (CNN) with a variant of the well known LeNet architecture. The layers of the network are as follows. Convolution with a 5\u00d75 kernel, stride of 1, and 40 output channels, followed by ReLU and max-pooling. Then a convolution with a 5\u00d75 kernel, stride of 1, and 80 output channels, followed by ReLU and max-pooling. Then a convolution with a 7\u00d77 kernel, stride of 1, and 160 output channels, followed by ReLU. Finally, a linear prediction over the resulting 160 channels yields the binary prediction for the input 28\u00d728\nimage crop. Overall the model has 710, 642 weights. We denote byH the resulting hypothesis class. In the comparison, we focus on the case in which the data is realizable by H. To guarantee that, we first used vanilla SGD to find a network in H. We then kept only samples that were labeled correctly by the network. This yielded 28k positive examples and 246k negative examples. This set was then randomly mixed and split 9 : 1 for train and test sets.\nFor the w player we used the SGD algorithm with Nesterov\u2019s momentum, as this is a standard solver for learning convolutional neural networks. The parameters we used are a batch size of 64, an `2 regularization of 0.0005, momentum parameter of 0.9, and a learning rate of \u03b7t = 0.01(1 + 0.0001 t)\u22120.75. We used the logistic loss as a surrogate loss for the classification error.\nWe performed two experiments to highlight different properties of our algorithm. The first experiment shows that FOL is much faster than SGD, and this is reflected both in train and test errors. The second experiment compares FOL to the application of AdaBoost on top of the same base learner.\nExperiment 1: Convergence speed In this experiment we show that FOL is faster than SGD. Figure 2 shows the train and test errors of SGD and FOL. Both models were initialized with the same randomly selected weights. As mentioned before, FOL relies on SGD as its w player. Observe that FOL essentially solves the problem (zero training error) after 30 epochs, whereas SGD did not converge to a zero training error even after 14, 000 epochs and achieved 0.1313% error. While the logarithmic scale in the figure shows that SGD is still improving, it is doing so at a decreasing rate. This is reflected by our theoretical analysis.\nTo understand why SGD slows down, observe that when the error of SGD is as small as here (0.13%), only one example in 769 is informative. Even when at classification error of 0.4% (left-side of the graph), only 4 in 1, 000 examples are informative. Hence, even with batch size of 64, SGD picks one useful sample only once every fifteen iterations. FOL expects an average of 32 useful samples in every iteration so every iteration is informative. In our case, since the training set size is 246k, only 984 examples are informative and FOL makes sure to focus on these rather than waste time on solved examples.\nAs can be seen in Figure 2, the faster convergence of FOL on the training set is also translated to a better test error. Indeed, FOL achieves a test error of 0.14% (after 27 epochs) whereas even after 14k epochs SGD results 0.35% error.\nExperiment 2: Comparison to AdaBoost As mentioned in Section 2.4, FOL can be seen as an online version of AdaBoost. Specifically, we can apply the AdaBoost algorithm while using SGD as its weak learner. For concreteness and reproducibility of our experiment, we briefly describe the resulting algorithm. We initialize a uniform distribution over the m examples, p = (1/m, . . . , 1/m). At iteration t of AdaBoost, we run one epoch of SGD over the data, while sampling examples according to p. Let ht be the resulting classifier. We then calculate ht(xi) over all the m examples, calculate the averaged zero-one error of ht, with weights based on p, define a weight \u03b1t = 0.5 log(1/ t\u22121), and update p such that pi \u221d pi exp(\u2212\u03b1tyiht(xi)). We repeat this process for T iterations, and output the hypothesis h(x) = sign( \u2211T t=1 \u03b1tht(x)).\nObserve that each such iteration of AdaBoost is equivalent to 2 epochs of our algorithm. In Figure 3 we show the train error of AdaBoost and FOL as a function of the number of epochs over the data. The behavior on the test set shows a similar trend. As can be seen in the figure, AdaBoost finds a consistent hypothesis after 20 epochs, while FOL requires 27 epochs to converge to a consistent hypothesis. However, once FOL converged, its last hypothesis has a zero training error. In contrast, the output hypothesis of AdaBoost is a weighted majority of T hypotheses (T = 10 in this case). It follows that at prediction time, applying AdaBoost\u2019s predictor is 10 times slower than applying FOL\u2019s predictor. Often, we prefer to spend more time during training, for the sake of finding a hypothesis which can be evaluated faster at test time. While based on our theory, the output hypothesis of FOL should also be a majority of log(m) hypotheses, we found out that in practice, the last hypothesis of FOL converges to a zero classification error at almost the same rate as the majority classifier.\nAcknowledgements: S. Shalev-Shwartz is supported by ICRI-CI and by the European Research Council (Theo-\nryDL project)."}, {"heading": "A. Proof of Theorem 1", "text": "A.1. Background\nBernstein\u2019s type inequality for martingales: A sequence B1, . . . , BT of random variables is Markovian if for every t, given Bt\u22121 we have that Bt is independent of B1, . . . , Bt\u22122. A sequence A1, . . . , AT of random variables is a martingale difference sequence with respect to B1, . . . , BT if for every t we have E[At|B1, . . . , Bt] = 0.\nLemma 4 (Hazan et al. (2011, Lemma C.3) and Fan et al. (2012, Theorem 2.1)) Let B1, . . . , BT be a Markovian sequence and let A1, . . . , AT be a martingale difference sequence w.r.t. B1, . . . , BT . Assume that for every t we have |At| \u2264 V and E[A2t |B1, . . . , Bt] \u2264 s. Then, for every \u03b1 > 0 we have\nP\n( 1\nT\nT\u2211\nt=1\nAt \u2265 \u03b1 ) \u2264 exp ( \u2212T \u03b1 2/2\ns+ \u03b1V/3\n)\nIn particular, for every \u03b4 \u2208 (0, 1), if\nT \u2265 2(s+ \u03b1V/3) log(1/\u03b4) \u03b12 ,\nthen with probability of at least 1\u2212 \u03b4 we have that 1T \u2211T t=1At \u2264 \u03b1.\nThe EG algorithm: Consider a sequence of vectors, z1, . . . , zT , where every zt \u2208 Rm. Consider the following sequence of vectors, parameterized by \u03b7 > 0. The first vector is q\u03031 = (1, . . . , 1) \u2208 Rm and for t \u2265 1 we define q\u0303t+1 to be such that:\n\u2200i \u2208 [m], q\u0303t+1,i = q\u0303t,i exp(\u2212\u03b7zt,i) .\nIn addition, for every t define qt = q\u0303/( \u2211m i=1 q\u0303i) \u2208 Sm. The algorithm that generates the above sequence is known as the EG algorithm (Kivinen & Warmuth, 1997).\nLemma 5 (Theorem 2.22 in (Shalev-Shwartz, 2011)) Assume that \u03b7zt,i \u2265 \u22121 for every t and i. Then, for every u \u2208 Sm we have:\nT\u2211\nt=1\n\u3008qt \u2212 u, zt\u3009 \u2264 log(m)\n\u03b7 + \u03b7\nT\u2211\nt=1\nm\u2211\ni=1\nqt,iz 2 t,i .\nA.2. Proof\nTo simplify our notation we denote `i(wt) = `(wt, xi, yi). We sometimes omit the time index t when it is clear from the context (e.g., we sometime use qi instead of qt,i).\nA.2.1. THE w PLAYER\nBy our assumption that C/T \u2264 /8 we have that, for every i1, . . . , iT ,\n1\nT\nT\u2211\nt=1\n`it(wt) \u2264 /8 (5)\nA.2.2. THE p PLAYER\nRecall that pi = 12m + qi 2 . Note that, for every i,\n1 pi \u2264 2m and qi pi \u2264 2\nDefine zt = \u2212 `it (wt)pit eit . Observe that the p player applies the EG algorithm w.r.t. the sequence z1, . . . , zT . Since\nzt,i \u2265 \u22122m we obtain from Lemma 5 that if \u03b7 \u2264 1/(2m) then, for every u \u2208 Sm,\n1\nT\nT\u2211\nt=1\n\u3008qt \u2212 u, zt\u3009 \u2264 log(m) \u03b7T + \u03b7 T\nT\u2211\nt=1\nm\u2211\ni=1\nqt,iz 2 t,i\n\u2264 log(m) \u03b7T + \u03b7 T\nT\u2211\nt=1\nqt,it `it(wt)\n2\np2t,it\n\u2264 log(m) \u03b7T + \u03b7 T\nT\u2211\nt=1\n4m`it(wt) 2\n\u2264 log(m) \u03b7T + \u03b74m T\nT\u2211\nt=1\n`it(wt)\n\u2264 /8 + 2 T\nT\u2211\nt=1\n`it(wt) ,\nwhere in the last inequality we used \u03b7 = 1/(2m) and T = \u2126(m log(m)/ ). Rearranging, and combining with (5) we obtain\n1\nT\nT\u2211\nt=1\n\u3008u, 1 pt,it `it(wt)eit\u3009 \u2264 1 T\nT\u2211\nt=1\n( qt,it pt,it + 2 ) `it(wt) + /8 \u2264 4 T T\u2211\nt=1\n`it(wt) + /8 \u2264 5\n8 . (6)\nA.2.3. MEASURE CONCENTRATION\nNote that, if u = ei, then\nE[\u3008u, 1 pt,it\n`it(wt)eit\u30092|qt, wt] = m\u2211\nj=1\npt,j p2t,j `j(wt) 2uj \u2264 1 pt,i \u2264 2m .\nDefine the martingale difference sequence A1, . . . , AT where At = `i(wt) \u2212 \u3008u, 1pt,it `it(wt)eit\u3009. We have that |At| \u2264 (2m + 1) and E[A2t |qt, wt] \u2264 2m. Therefore, the conditions of Lemma 4 holds and we obtain that if T \u2265 6m log(m/\u03b4)/( /8)2 then with probability of at least 1 \u2212 \u03b4/m we have that 1T \u2211 tAt \u2264 /8. Applying a union bound over i \u2208 [m] we obtain that with probability of at least 1\u2212 \u03b4 it holds that\n\u2200i \u2208 [m], 1 T\n\u2211\nt\n`i(wt) \u2264 1\nT\n\u2211\nt\n\u3008ei, 1\npit `it(wt)eit\u3009+ /8 .\nCombining with (6) we obtain that, with probability of at least 1\u2212 \u03b4,\n\u2200i \u2208 [m], 1 T\nT\u2211\nt=1\n`i(wt) \u2264 6\n8 .\nFinally, relying on Bernstein\u2019s inequality (see Lemma B.10 in (Shalev-Shwartz & Ben-David, 2014)), it is not hard to see that if k = \u2126(log(m/\u03b4)/ ) then, with probability of at least 1\u2212 \u03b4 we have that\n\u2200i \u2208 [m], 1 k\nk\u2211\nj=1\n`i(wtj ) \u2264 1\nT\nT\u2211\nt=1\n`i(wt) + 4 ,\nand this concludes our proof."}, {"heading": "B. Proofs of Lemmas in Section 3.1", "text": "Proof [Proof of Lemma 1] There are only 4 possible examples, so an ERM will have a generalization error of 0 provided we see all the 4 examples. By a simple direct calculation together with the union bound over the 4 examples it\nis easy to verify that the probability not to see all the examples is at most 4(1\u2212 /2)m \u2264 4e\u2212m /2, and the claim follows.\nProof [Proof of Lemma 2] For SGD, we can assume (due to symmetry) that y is always 1. Therefore, there are only two possible examples z1, z2. With probability 1\u2212 the first examples is z1. Also, wT has always the form\n\u03b7(kz1 + rz2) = \u03b7((k + r)\u03b1, k \u2212 2r\u03b1) ,\nwhere k is the number of times we had a margin error on z1 and r is the number of times we had a margin error on z2. To make sure that \u3008wT , z2\u3009 > 0 we must have that\n(k + r)\u03b12 \u2212 2(k \u2212 2r\u03b1)\u03b1 > 0 \u21d2 r > k 2\u03b1\u2212 \u03b1 2 5\u03b12 \u2248 k 2 5\u03b1 (7)\nNote that the first example is z1 with probability of 1 \u2212 , hence we have that k \u2265 1 with probability of at least 1 \u2212 . In addition, r is upper bounded by the number of times we saw z2 as the example, and by Chernoff\u2019s bound we have that that the probability that this number is greater than 2m is at most e\u2212 /3 \u2248 (1\u2212 /3). Therefore, with probability of 1\u2212O( ) we have the requirement that m must be at least \u2126(1/(\u03b1 )), which concludes our proof.\nProof [Proof of Lemma 3] We have shown that m = 1/ examples suffices. Specifying our general analysis to classification with the zero-one loss, it suffices to ensure that the regret of both players will be smaller than 1/2. The regret of the sampling player is bounded by O(m log(m)). As for the halfspace player, to simplify the derivation, lets use the Perceptron as the underlying player. It is easy to verify that the vector wT has the form kz1 + rz2 = ((k + r)\u03b1, k \u2212 2r\u03b1), for some integers k, r. Lets consider two regimes. The first is the first time when r, k satisfies \u3008wT , z2\u3009 > 0. As we have shown before, this happens when r is roughly 2k/(5\u03b1). Once this happens we also have that\n\u3008wT , z1\u3009 = ((k + r)\u03b12 + k \u2212 2r\u03b1) \u2248 (k \u2212 2r\u03b1) \u2248 > 4k/5 > 0 ,\nSo, the Perceptron will stop making changes and will give us an optimal halfspace. Next, suppose that we have a pair r, k for which \u3008wT , z2\u3009 \u2264 0. If we now encounter z2 then we increase r. If we encounter z1 then\n\u3008wT , z1\u3009 \u2248 (k \u2212 2r\u03b1) \u2248 > 4k/5 > 0 ,\nso we\u2019ll not increase k. Therefore, k will increase only up to a constant, while r will continue to increase until roughly 2k/(5\u03b1), and then the Perceptron will stop making updates. This implies that the mistake bound of the Perceptron is bounded by O(1/\u03b1), which concludes our proof."}, {"heading": "C. Proof of Theorem 2", "text": "We can think of the ERM algorithm as following the following three steps. First, we sample (i1, . . . , im) \u2208 {1, 2}m, where P[ir = j] = \u03bbj . Let m1 be the number of indices for which ir = 1 and let m2 = m\u2212m1. Second, we sample S1 \u223c Dm11 , and define H\u03021 to be all hypotheses in H which are consistent with S1. Last, we sample S2 \u223c Dm22 and set the output hypotheses to be some hypothesis in H\u03021 which is consistent with S2. The proof relies on the following three claims, where we use C to denote a universal constant:\n\u2022 Claim 1: With probability of at least 1 \u2212 \u03b4/3 over the choice of (i1, . . . , im) we have that both m1 \u2265 \u03bb1m/2 and m2 \u2265 \u03bb2m/2. \u2022 Claim 2: Assuming that m1 \u2265 C ( VC(H) log(1/ )+log(1/\u03b4) ) , then with probability of at least 1\u2212 \u03b4/3 over the choice\nof S1 we have that H\u03021 \u2286 H1, .\n\u2022 Claim 3: Assume that m2 \u2265 C ( VC(H1, ) log(1/c)+log(1/\u03b4) c ) , then with probability of at least 1\u2212 \u03b4/3 over the choice\nof S2, any hypothesis inH1, which is consistent with S2 must have LD2(h) \u2264 c.\nClaim 1 follows directly from Chernoff\u2019s bound, while Claim 2-3 follows directly from standard VC bounds (see for example Shalev-Shwartz & Ben-David (2014, Theorem 6.8)).\nEquipped with the above three claims we are ready to prove the theorem. First, we apply the union bound to get that with probability of at least 1 \u2212 \u03b4, the statements in all the above three claims hold. This means that H\u03021 \u2286 H1, hence LD1(ERM(S)) \u2264 . It also means that ERM(S) must be inH1, , and therefore from the third claim and the assumption in the theorem we have that LD2(ERM(S)) \u2264 as well, which concludes our proof."}, {"heading": "D. Proof of Theorem 3", "text": "The probability that all of the outliers do not fall into the sample of n examples is\n(1\u2212 k/m)n \u2265 0.99 e\u2212kn/m .\nTherefore, the probability that at least one outlier falls into the sample is at most\n1\u2212 0.99 e\u2212kn/m \u2264 1\u2212 0.99(1\u2212 kn/m) = 0.01 + 0.99kn/m\nOn the other hand, the expected number of rare examples in the sample is nm2/m and by Chernoff\u2019s bound, the probability that less than half of the rare examples fall into the sample is at most exp(\u22120.1nm2/m). Applying the union bound we conclude our proof."}], "references": [{"title": "Even faster accelerated coordinate descent using non-uniform sampling", "author": ["Allen-Zhu", "Zeyuan", "Yuan", "Yang"], "venue": "arXiv preprint arXiv:1512.09103,", "citeRegEx": "Allen.Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Allen.Zhu et al\\.", "year": 2015}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Bengio", "Yoshua", "Sen\u00e9cal", "Jean-S\u00e9bastien"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2008}, {"title": "Accelerating stochastic gradient descent via online learning to sample", "author": ["Bouchard", "Guillaume", "Trouillon", "Th\u00e9o", "Perez", "Julien", "Gaidon", "Adrien"], "venue": "arXiv preprint arXiv:1506.09016,", "citeRegEx": "Bouchard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bouchard et al\\.", "year": 2015}, {"title": "Theory of classification: A survey of some recent advances", "author": ["Boucheron", "St\u00e9phane", "Bousquet", "Olivier", "Lugosi", "G\u00e1bor"], "venue": "ESAIM: probability and statistics,", "citeRegEx": "Boucheron et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2005}, {"title": "The tradeoffs of large scale learning", "author": ["Bousquet", "Olivier", "Bottou", "L\u00e9on"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Bousquet et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2008}, {"title": "Sublinear optimization for machine learning", "author": ["Clarkson", "Kenneth L", "Hazan", "Elad", "Woodruff", "David P"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Clarkson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2012}, {"title": "Efficient projections onto the l 1-ball for learning in high dimensions", "author": ["Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Hoeffding\u2019s inequality for supermartingales", "author": ["Fan", "Xiequan", "Grama", "Ion", "Liu", "Quansheng"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2012}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "In Computational learning theory,", "citeRegEx": "Freund et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1995}, {"title": "Rigorous learning curve bounds from statistical mechanics", "author": ["Haussler", "David", "Kearns", "Michael", "Seung", "H Sebastian", "Tishby", "Naftali"], "venue": "Machine Learning,", "citeRegEx": "Haussler et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1996}, {"title": "Beating sgd: Learning svms in sublinear time", "author": ["Hazan", "Elad", "Koren", "Tomer", "Srebro", "Nati"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hazan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2011}, {"title": "Robust Statistics (second edition)", "author": ["Huber", "Peter J", "Ronchetti", "Elvezio M"], "venue": "J. Wiley,", "citeRegEx": "Huber et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huber et al\\.", "year": 2009}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Kivinen and Warmuth,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth", "year": 1997}, {"title": "Online learning and online convex optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2011}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shalev-Shwartz", "Shai", "Ben-David"], "venue": "Cambridge university press,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2014}, {"title": "On the equivalence of weak learnability and linear separability: New relaxations and efficient boosting algorithms", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "Machine learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Svm optimization: inverse dependence on training set size", "author": ["Shalev-Shwartz", "Shai", "Srebro", "Nathan"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2008}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew"], "venue": "Mathematical programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Stochastic optimization with importance sampling", "author": ["Zhao", "Peilin", "Zhang", "Tong"], "venue": "arXiv preprint arXiv:1401.2753,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "AT be a martingale difference sequence w.r.t", "author": ["Hazan"], "venue": "Lemma C.3) and Fan et al. (2012,", "citeRegEx": "Hazan,? \\Q2011\\E", "shortCiteRegEx": "Hazan", "year": 2011}, {"title": "The algorithm that generates the above sequence is known as the EG algorithm (Kivinen", "author": ["\u2208 Sm"], "venue": "(Shalev-Shwartz,", "citeRegEx": "Sm.,? \\Q1997\\E", "shortCiteRegEx": "Sm.", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "For the p player, we use the seminal work of (Auer et al., 2002).", "startOffset": 45, "endOffset": 64}, {"referenceID": 1, "context": "Crucially, the work of (Auer et al., 2002) does not assume that \u039b(wt) are sampled from a fixed distribution, but rather the vectors \u039b(wt) can be chosen by an adversary.", "startOffset": 23, "endOffset": 42}, {"referenceID": 1, "context": "In (Auer et al., 2002) it is proposed to rely on the algorithm EXP3.", "startOffset": 3, "endOffset": 22}, {"referenceID": 6, "context": "Phrasing the maxloss minimization as a two players game has also been proposed by (Clarkson et al., 2012; Hazan et al., 2011).", "startOffset": 82, "endOffset": 125}, {"referenceID": 11, "context": "Phrasing the maxloss minimization as a two players game has also been proposed by (Clarkson et al., 2012; Hazan et al., 2011).", "startOffset": 82, "endOffset": 125}, {"referenceID": 6, "context": "Assuming the setup of Example 1, (Clarkson et al., 2012) presents an algorithm that finds a consistent hypothesis in runtime of \u00d5((m + d) \u00b7 C).", "startOffset": 33, "endOffset": 56}, {"referenceID": 6, "context": "In any case, our bound is sometimes better and sometimes worse than the one in (Clarkson et al., 2012).", "startOffset": 79, "endOffset": 102}, {"referenceID": 3, "context": "See for example (Bengio & Sen\u00e9cal, 2008; Bouchard et al., 2015; Zhao & Zhang, 2014; Allen-Zhu & Yuan, 2015).", "startOffset": 16, "endOffset": 107}, {"referenceID": 10, "context": "Second, we apply an analysis of the sample complexity similar to the \u201cshell analysis\u201d of (Haussler et al., 1996), and assume that the error of all hypotheses in H1,\u270f on D2 is either smaller than \u270f or larger than c, where we would like to think of c as being significantly larger than \u270f.", "startOffset": 89, "endOffset": 112}, {"referenceID": 10, "context": "Second, we apply an alysis of the sample complexity similar to he \u201cshell analysis\u201d of (Haussler et al., 1996), and assume that the erro f all hypotheses in H1, on D2 is either smaller than or large than c, wher we would like to think of c as being sign ficantly larger than .", "startOffset": 86, "endOffset": 109}, {"referenceID": 7, "context": "For efficient implementations of this projection see for example (Duchi et al., 2008).", "startOffset": 65, "endOffset": 85}, {"referenceID": 18, "context": "We can further replace the constraint \u2016\u03be\u20161 \u2264 K with a constraint of \u2016\u03be\u20162 \u2264 K, because projection onto the Euclidean ball is a simple scaling, and the operation can be done efficiently with an adequate data structure (as described, for example, in (Shalev-Shwartz et al., 2011)).", "startOffset": 247, "endOffset": 276}], "year": 2016, "abstractText": "A commonly used learning rule is to approximately minimize the average loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the maximal loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We prove that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers.", "creator": "LaTeX with hyperref package"}}}