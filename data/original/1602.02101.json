{"id": "1602.02101", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Variance-Reduced and Projection-Free Stochastic Optimization", "abstract": "The Frank-Wolfe optimization algorithm has recently regained popularity for machine learning applications due to its projection-free property and its ability to handle structured constraints. However, in the stochastic learning setting, it is still relatively understudied compared to the gradient descent counterpart. In this work, leveraging a recent variance reduction technique, we propose two stochastic Frank-Wolfe variants which substantially improve previous results in terms of the number of stochastic gradient evaluations needed to achieve $1-\\epsilon$ accuracy. For example, we improve from $O(\\frac{1}{\\epsilon})$ to $O(\\ln\\frac{1}{\\epsilon})$ if the objective function is smooth and strongly convex, and from $O(\\frac{1}{\\epsilon^2})$ to $O(\\frac{1}{\\epsilon^{1.5}})$ if the objective function is smooth and Lipschitz. The theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application.", "histories": [["v1", "Fri, 5 Feb 2016 17:14:59 GMT  (46kb,D)", "http://arxiv.org/abs/1602.02101v1", null], ["v2", "Thu, 14 Sep 2017 00:03:37 GMT  (53kb,D)", "http://arxiv.org/abs/1602.02101v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad hazan", "haipeng luo"], "accepted": true, "id": "1602.02101"}, "pdf": {"name": "1602.02101.pdf", "metadata": {"source": "CRF", "title": "Variance-Reduced and Projection-Free Stochastic Optimization", "authors": ["Elad Hazan", "Haipeng Luo"], "emails": ["ehazan@cs.princeton.edu", "haipengl@cs.princeton.edu"], "sections": [{"heading": null, "text": ") to O(ln 1 ) if the objective function is smooth and strongly convex, and from O( 1 2 ) to O( 1 1.5 )\nif the objective function is smooth and Lipschitz. The theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application."}, {"heading": "1 Introduction", "text": "We consider the following optimization problem\nmin w\u2208\u2126 f(w) = min w\u2208\u2126\n1\nn n\u2211 i=1 fi(w)\nwhich is an extremely common objective in machine learning. We are interested in the case where 1) n, usually corresponding to the number of training examples, is very large and therefore stochastic optimization is much more efficient; and 2) the domain \u2126 admits fast linear optimization, while projecting onto it is much slower, necessitating projection-free optimization algorithms. Examples of such problem include multiclass classification, multitask learning, recommendation systems, matrix learning and many more (see for example (Hazan & Kale, 2012; Hazan et al., 2012; Jaggi, 2013; Dudik et al., 2012; Zhang et al., 2012; Harchaoui et al., 2015)).\nThe Frank-Wolfe algorithm (Frank & Wolfe, 1956) (also known as conditional gradient) and it variants are natural candidates for solving these problems, due to its projection-free property and its ability to handle structured constraints. However, despite gaining more popularity recently, its applicability and efficiency in the stochastic learning setting, where computing stochastic gradients is much faster than computing exact gradients, is still relatively understudied compared to variants of projected gradient descent methods.\nIn this work, we thus try to answer the following question: how fast can a projection-free algorithm achieve in terms of the number of stochastic gradient evaluations and the number of linear optimizations needed to achieve a certain accuracy? Utilizing Nesterov\u2019s acceleration technique (Nesterov, 1983) and the recent variance reduction idea (Johnson & Zhang, 2013; Mahdavi et al., 2013), we propose two new algorithms that are substantially faster than previous work. Specifically, to achieve 1\u2212 accuracy, while the number of linear optimization is the same as previous work, the improvement of the number of stochastic gradient evaluations is summarized in Table 1:\nThe extra overhead of our algorithms is computing at most O(ln 1 ) exact gradients, which is computationally insignificant compared to the other operations. A more detailed comparisons to previous work is included in Table 2, which will be further explained in Section 2.\nar X\niv :1\n60 2.\n02 10\n1v 1\n[ cs\n.L G\n] 5\nF eb\n2 01\nWhile the idea of our algorithms is quite straightforward, we emphasize that our analysis is non-trivial, especially for the second algorithm where the convergence of a sequence of auxiliary points in Nesterov\u2019s algorithm needs to be shown.\nTo support our theoretical results, we also conducted experiments on three large real-word datasets for a multiclass classification application. These experiments show significant improvement over both previous projection-free algorithms and algorithms such as projected stochastic gradient descent and its variance-reduced version.\nThe rest of the paper is organized as follows: Section 2 setups the problem more formally and discusses related work. Our two new algorithms are presented and analyzed in Section 3 and 4, followed by experiment details in Section 5."}, {"heading": "2 Preliminary and Related Work", "text": "We assume each function fi is convex and L-smooth, that is, for any w,v \u2208 \u2126,\n\u2207fi(v)>(w \u2212 v) \u2264 fi(w)\u2212 fi(v)\n\u2264 \u2207fi(v)>(w \u2212 v) + L\n2 \u2016w \u2212 v\u20162 .\nWe will use two more important properties of smoothness. The first one is\n\u2016\u2207fi(w)\u2212\u2207fi(v)\u20162 \u2264 2L(fi(w)\u2212 fi(v)\u2212\u2207fi(v)>(w \u2212 v))\n(1)\n(proven in Appendix A for completeness), and the second one is\nfi(\u03bbw + (1\u2212 \u03bb)v) \u2265\n\u03bbfi(w) + (1\u2212 \u03bb)fi(v)\u2212 L\n2 \u03bb(1\u2212 \u03bb) \u2016w \u2212 v\u20162\n(2)\nfor any w,v \u2208 \u2126 and \u03bb \u2208 [0, 1]. Notice that f = 1n \u2211n i=1 fi is also L-smooth since smoothness is preserved under convex combinations. For some cases, we also assume each fi is G-Lipschitz: \u2016\u2207fi(w)\u2016 \u2264 G for any w \u2208 \u2126, and f (although not necessarily each fi) is \u03b1-strongly convex, that is,\nf(w)\u2212 f(v) \u2264 \u2207f(w)>(w \u2212 v)\u2212 \u03b1 2 \u2016w \u2212 v\u20162\nfor any w,v \u2208 \u2126. As usual, \u00b5 = L\u03b1 is called the condition number of f . We assume the domain \u2126 \u2208 Rd is a compact convex set with diameter D. We are interested in the case where linear optimization on \u2126, formally argminv\u2208\u2126w >v for any w \u2208 Rd, is much faster than projection onto \u2126, formally argminv\u2208\u2126 \u2016w \u2212 v\u2016 2. Examples of such domains include the set of all bounded trace norm matrices, the convex hull of all rotation matrices, flow polytope and many more (see for instance (Hazan & Kale, 2012))."}, {"heading": "2.1 Example Application: Multiclass Classification", "text": "Consider a multiclass classification problem where a set of training examples (ei, yi)i=1,...,n is given beforehand. Here ei \u2208 Rm is a feature vector and yi \u2208 {1, . . . , h} is the label. Our goal is to find an accurate linear predictor, a matrixw = [w>1 ; . . . ,w > h ] \u2208 Rh\u00d7m that predicts argmax`w>` e for any example e. Note that here the dimensionality d is hm. Previous work (Dudik et al., 2012; Zhang et al., 2012) found that finding w by minimizing a regularized multivariate logistic loss gives a very accurate predictor in general. Specifically, the objective can be written in our notation with\nfi(w) = log\n( 1 + \u2211 6\u0300=yi exp(w>` ei \u2212w>yiei) )\nand \u2126 = {w \u2208 Rh\u00d7m : \u2016w\u2016\u2217 \u2264 \u03c4} where \u2016\u00b7\u2016\u2217 denotes the matrix trace norm. In this case, projecting onto \u2126 is equivalent to performing an SVD, which takes O(hmmin{h,m}) time, while linear optimization on \u2126 amounts to finding the top singular vectors, which can be done in time linear to the number of non-zeros in the corresponding h by m matrix, and is thus much faster. One can also verify that each fi is smooth. The number of examples n can be prohibitively large for non-stochastic methods (for instance, tens of millions for the ImageNet dataset (Deng et al., 2009)), which makes stochastic optimization necessary."}, {"heading": "2.2 Detailed Efficiency Comparisons", "text": "We call \u2207fi(w) a stochastic gradient for f at some w, where i is picked from {1, . . . , n} uniformly at random. Note that a stochastic gradient\u2207fi(w) is an unbiased estimator of the exact gradient\u2207f(w). The efficiency of a projectionfree algorithm is measured by how many numbers of exact gradient evaluations, stochastic gradient evaluations and linear optimizations respectively are needed to achieve 1 \u2212 accuracy, that is, to output a point w \u2208 \u2126 such that E[f(w)\u2212 f(w\u2217)] \u2264 where w\u2217 \u2208 argminw\u2208\u2126 f(w) is any optimum.\nIn Table 2, we summarize the efficiency (and extra assumptions needed beside convexity and smoothness1) of\n1In general, condition \u201cG-Lipschitz\u201d in Table 2 means each fi is G-Lipschitz, except for our STORC algorithm which only requires f being G-Lipschitz.\nexisting algorithms in the literature as well as the two new algorithms we propose. Below we briefly explain these results from top to bottom.\nThe standard Frank-Wolfe algorithm:\nvk = argmin v\u2208\u2126\n\u2207f(wk\u22121)>v\nwk = (1\u2212 \u03b3k)wk\u22121 + \u03b3kvk (3)\nfor some appropriate chosen \u03b3k requires O( 1 ) iteration without additional conditions (Frank & Wolfe, 1956; Jaggi, 2013). In a recent paper, Garber & Hazan (2013) give a variant that requiresO(d\u00b5\u03c1 ln 1 ) iterations when f is strongly convex and smooth, and \u2126 is a polytope2. Although the dependence on is much better, the geometric constant \u03c1 depends on the polyhedral set and can be very large. Moreover, each iteration of the algorithm requires further computation besides the linear optimization step.\nThe most obvious way to obtain a stochastic Frank-Wolfe variant is to replace\u2207f(wk\u22121) by some\u2207fi(wk\u22121), or more generally the average of some number of iid samples of\u2207fi(wk\u22121) (mini-batch approach). We call this method SFW and include its analysis in Appendix B since we do not find it explicitly analyzed before. SFW needs O( 1 3 ) stochastic gradients and O( 1 ) linear optimization steps to reach an -approximate optimum.\nThe work by Hazan & Kale (2012) focuses on a online learning setting. One can extract two results from this work for the setting studied here.3 In any case, the result is worse than SFW for both the number of stochastic gradients and the number of linear optimizations.\nStochastic Condition Gradient Sliding (SCGS), recently proposed by Lan & Zhou (2014), uses Nesterov\u2019s acceleration technique to speed up Frank-Wolfe. Without strong convexity, SCGS needs O( 1 2 ) stochastic gradients, improving SFW. With strong convexity, this number can even be improved to O( 1 ). In both cases, the number of linear optimization steps is O( 1 ).\nThe key idea of our algorithms is to combine the variance reduction technique proposed in (Johnson & Zhang, 2013; Mahdavi et al., 2013) with some of the above-mentioned algorithms. For example, our algorithm SVRF combines this technique with SFW, also improving the number of stochastic gradients from O( 1 3 ) to O( 1 2 ), but without any extra conditions (such as Lipschitzness required for SCGS). More importantly, despite having seemingly same convergence rate, SVRF substantially outperforms SCGS empirically (see Section 5).\nOn the other hand, our second algorithm STORC combines variance reduction with SCGS, providing even further improvements. Specifically, the number of stochastic gradients is improved to: O( 1 1.5 ) when f is Lipschitz; O( 1 ) when \u2207f(w\u2217) = 0; and finally O(ln 1 ) when f is strongly convex. Note that the condition \u2207f(w \u2217) = 0 essentially means that w\u2217 is in the interior of \u2126, but it is still an interesting case when the optimum is not unique and doing unconstraint optimization would not necessary return a point in \u2126.\nBoth of our algorithms require O( 1 ) linear optimization steps as previous work, and overall require computing O(ln LD 2\n) exact gradients. However, we emphasize that this extra overhead is much more affordable compared to non-stochastic Frank-Wolfe (that is, computing exact gradients every iteration) since it does not have any polynomial dependence on parameters such as d, L or \u00b5."}, {"heading": "2.3 Variance-Reduced Stochastic Gradients", "text": "Originally proposed in (Johnson & Zhang, 2013) and independently in (Mahdavi et al., 2013), the idea of variancereduced stochastic gradients is proven to be highly useful and has been extended to various different algorithms (such as (Frostig et al., 2015; Moritz et al., 2016)).\nA variance-reduced stochastic gradient at some point w \u2208 \u2126 with some snapshot w0 \u2208 \u2126 is defined as\n\u2207\u0303f(w;w0) = \u2207fi(w)\u2212 (\u2207fi(w0)\u2212\u2207f(w0)),\nwhere i is again picked from {1, . . . , n} uniformly at random. The snapshotw0 is usually a decision point from some previous iteration of the algorithm and its exact gradient \u2207f(w0) has been pre-computed before, so that computing \u2207\u0303f(w;w0) only requires two standard stochastic gradient evaluations: \u2207fi(w) and \u2207fi(w0).\n2See also recent follow up work Lacoste-Julien & Jaggi (2015). 3The first result comes from the setting where the online loss functions are stochastic, and the second one comes from a completely online\nsetting with the standard online-to-batch conversion.\nAlgorithm 1 Stochastic Variance-Reduced Frank-Wolfe (SVRF) 1: Input: Objective function f = 1n \u2211n i=1 fi.\n2: Input: Parameters \u03b3k, mk and Nk. 3: Initialize: w0 = minw\u2208\u2126\u2207f(x)>w for some arbitrary x \u2208 \u2126. 4: for t = 1, 2, . . . , T do 5: Take snapshot: x0 = wt\u22121 and compute \u2207f(x0). 6: for k = 1 to Nt do 7: Compute \u2207\u0303k, the average of mk iid samples of \u2207\u0303f(xk\u22121,x0). 8: Compute vk = minv\u2208\u2126 \u2207\u0303>k v. 9: Compute xk = (1\u2212 \u03b3k)xk\u22121 + \u03b3kvk.\n10: end for 11: Set wt = xNt . 12: end for\nA variance-reduced stochastic gradient is clearly also unbiased, that is, E[\u2207\u0303f(w;w0)] = \u2207f(w). More importantly, the term \u2207fi(w0) \u2212 \u2207f(w0) serves as a correction term to reduce the variance of the stochastic gradient. Formally, one can prove the following\nLemma 1. For any w,w0 \u2208 \u2126, we have\nE[\u2016\u2207\u0303f(w;w0)\u2212\u2207f(w)\u20162] \u2264 6L(2E[f(w)\u2212 f(w\u2217)] + E[f(w0)\u2212 f(w\u2217)]).\nIn words, the variance of the variance-reduced stochastic gradient is bounded by how close the current point and the snapshot are to the optimum. The original work proves a bound on E[\u2016\u2207\u0303f(w;w0)\u20162] under the assumption \u2207f(w\u2217) = 0, which we do not require here. However, the main idea of the proof is similar and we defer it to Section 6."}, {"heading": "3 Stochastic Variance-Reduced Frank-Wolfe", "text": "With the previous discussion, our first algorithm is pretty straightforward: compared to the standard Frank-Wolfe, we simply replace the exact gradient with the average of a mini-batch of variance-reduced stochastic gradients, and take snapshots every once in a while. We call this algorithm Stochastic Variance-Reduced Frank-Wolfe (SVRF), whose pseudocode is presented in Alg 1. The convergence rate of this algorithm is shown in the following theorem.\nTheorem 1. With the following parameters,\n\u03b3k = 2\nk + 1 , mk = 96(k + 1), Nt = 2\nt+3 \u2212 2,\nAlgorithm 1 ensures E[f(wt)\u2212 f(w\u2217)] \u2264 LD 2\n2t+1 for any t.\nBefore proving this theorem, we first show a direct implication of this convergence result.\nCorollary 1. To achieve 1 \u2212 accuracy, Algorithm 1 requires O(ln LD 2 ) exact gradient evaluations, O( L2D4\n2 )\nstochastic gradient evaluations and O(LD 2 ) linear optimizations. Proof. According to the algorithm and the choice of parameters, it is clear that these three numbers are T + 1,\u2211T t=1 \u2211Nt k=1mk = O(4T ) and \u2211T t=1Nt = O(2T ) respectively. Theorem 1 implies that T should be of order \u0398(log2 LD2 ). Plugging in all parameters concludes the proof.\nTo prove Theorem 1, we first consider a fixed iteration t and prove the following lemma:\nLemma 2. For any k, we have E[f(xk)\u2212 f(w\u2217)] \u2264 4LD 2 k+2 if E[\u2016\u2207\u0303s \u2212\u2207f(xs\u22121)\u2016 2] \u2264 L 2D2 (s+1)2 for all s \u2264 k.\nWe defer the proof of this lemma to Section 6 for coherence. With the help of Lemma 2, we are now ready to prove the main convergence result.\nProof of Theorem 1. We prove by induction. For t = 0, by smoothness, the optimality of w0 and convexity, we have\nf(w0) \u2264 f(x) +\u2207f(x)>(w0 \u2212 x) + L\n2 \u2016w0 \u2212 x\u20162\n\u2264 f(x) +\u2207f(x)>(w\u2217 \u2212 x) + LD 2\n2\n\u2264 f(w\u2217) + LD 2\n2 .\nNow assuming E[f(wt\u22121)\u2212f(w\u2217)] \u2264 LD 2\n2t , we consider iteration t of the algorithm and use another induction to show E[f(xk)\u2212f(w\u2217)] \u2264 4LD 2 k+2 for any k \u2264 Nt. The base case is trivial sincex0 = wt\u22121. Suppose E[f(xs\u22121)\u2212f(w \u2217)] \u2264 4LD2\ns+1 for any s \u2264 k. Now because \u2207\u0303s is the average of ms iid samples of \u2207\u0303f(xs\u22121;x0), its variance is reduced by a factor of ms. That is, with Lemma 1 we have\nE[\u2016\u2207\u0303s \u2212\u2207f(xs\u22121)\u20162]\n\u2264 6L ms (2E[f(xs\u22121)\u2212 f(w\u2217)] + E[f(x0)\u2212 f(w\u2217)])\n\u2264 6L ms\n( 8LD2\ns+ 1 + LD2 2t ) \u2264 6L ms ( 8LD2 s+ 1 + 8LD2 s+ 1 ) = L2D2 (s+ 1)2 ,\nwhere the last inequality is by the fact s \u2264 Nt = 2t+3 \u2212 2 and the last equality is by plugging the choice of ms. Therefore the condition of Lemma 2 is satisfied and the induction is completed. Finally with the choice of Nt we thus prove E[f(wt)\u2212 f(w\u2217)] = E[f(xNt)\u2212 f(w\u2217)] \u2264 4LD 2 Nt+2 = LD 2 2t+1 .\nWe remark that in Alg 1, we essentially restart the algorithm (that is, reseting k to 1) after taking a new snapshot. However, another option is to continue increasing k and never reset it. Although one can show that this only leads to constant speed up for the convergence, it provides more stable update and is thus what we implement in experiments."}, {"heading": "4 Stochastic Variance-Reduced Conditional Gradient Sliding", "text": "Our second algorithm apply variance reduction to the SCGS algorithm (Lan & Zhou, 2014). Again, the key difference is that we replace the stochastic gradients with the average of a mini-batch of variance-reduced stochastic gradients, and take snapshots every once in a while. See pseudocode in Alg 2 for details.\nThe algorithm makes use of two auxiliary sequences xk and zk (Line 8 and 12), which is standard for Nesterov\u2019s algorithm. xk is obtained by approximately solving a square norm regularized linear optimization so that it is close to xk\u22121 (Line 11). Note that this step does not require computing any extra gradients of f or fi, and is done by performing the standard Frank-Wolfe algorithm (Eq. (3)) until the duality gap is at most a certain value \u03b7t,k. The duality gap is a certificate of approximate optimality (see (Jaggi, 2013)), and is a side product of the linear optimization performed at each step, requiring no extra cost.\nAlso note that the stochastic gradients are computed at the sequence zk instead of yk, which is also standard in Nesterov\u2019s algorithm. However, according to Lemma 1, we thus need to show the convergence rate of the auxiliary sequence zk, which appears to be rarely studied previously to the best our knowledge. This is one of the key steps in our analysis.\nThe main convergence result of STORC is the following:\nAlgorithm 2 STOchastic variance-Reduced Conditional gradient sliding (STORC) 1: Input: Objective function f = 1n \u2211n i=1 fi.\n2: Input: Parameters \u03b3k, \u03b2k, \u03b7t,k, mt,k and Nt. 3: Initialize: w0 = minw\u2208\u2126\u2207f(x)>w for some arbitrary x \u2208 \u2126. 4: for t = 1, 2, . . . do 5: Take snapshot: y0 = wt\u22121 and compute \u2207f(y0). 6: Initialize x0 = y0. 7: for k = 1 to Nt do 8: Compute zk = (1\u2212 \u03b3k)yk\u22121 + \u03b3kxk\u22121. 9: Compute \u2207\u0303k, the average of mt,k iid samples of \u2207\u0303f(zk;y0).\n10: Let g(x) = \u03b2k2 \u2016x\u2212 xk\u22121\u2016 2 + \u2207\u0303>k x. 11: Compute xk, the output of using standard Frank-Wolfe to solve minx\u2208\u2126 g(x) until the duality gap is at most\n\u03b7t,k, that is, max x\u2208\u2126 \u2207g(xk)>(xk \u2212 x) \u2264 \u03b7t,k . (4)\n12: Compute yk = (1\u2212 \u03b3k)yk\u22121 + \u03b3kxk. 13: end for 14: Set wt = yNt . 15: end for\nTheorem 2. With the following parameters (where Dt is defined later below):\n\u03b3k = 2\nk + 1 , \u03b2k =\n3L\nk , \u03b7t,k = 2LD2t Ntk ,\nAlgorithm 2 ensures E[f(wt)\u2212 f(w\u2217)] \u2264 LD 2\n2t+1 for any t if any of the following three cases holds:\n(a) \u2207f(w\u2217) = 0 and Dt = D,Nt = d2 t 2 +2e,mt,k = 900Nt.\n(b) f is G-Lipschitz and Dt = D,Nt = d2 t 2 +2e,mt,k = 700Nt + 24NtG(k+1)LD .\n(c) f is \u03b1-strongly convex and D2t = \u00b5D2 2t\u22121 , Nt = d \u221a 32\u00b5e,mt,k = 5600Nt\u00b5 where \u00b5 = L\u03b1 .\nAgain we first give a direct implication of the above result:\nCorollary 2. To achieve 1 \u2212 accuracy, Algorithm 2 requires O(ln LD 2 ) exact gradient evaluations and O( LD2 ) linear optimizations. The numbers of stochastic gradient evaluations for Case (a), (b) and (c) are respectivelyO(LD 2 ), O(LD 2 + \u221a LD2G 1.5 ) and O(\u00b5 2 ln LD 2 ).\nProof. Line 11 requires O(\u03b2kD 2\n\u03b7t,k ) iterations of the standard Frank-Wolfe algorithm since g(x) is \u03b2k-smooth (see e.g.\n(Jaggi, 2013, Theorem 2)). So the numbers of exact gradient evaluations, stochastic gradient evaluations and linear optimizations are respectively T + 1, \u2211T t=1 \u2211Nt k=1mt,k andO( \u2211T t=1 \u2211Nt k=1 \u03b2kD 2 \u03b7t,k ). Theorem 2 implies that T should be of order \u0398(log2 LD2 ). Plugging in all parameters proves the corollary.\nTo prove Theorem 2, we again first consider a fixed iteration t and use the following lemma, which is essentially proven in (Lan & Zhou, 2014). We include a distilled proof in Appendix C for completeness.\nLemma 3. Suppose 0 \u2264 Dt \u2264 D is such that E[\u2016y0\u2212w\u2217\u20162] \u2264 D2t . For any k, we have E[f(yk)\u2212f(w\u2217)] \u2264 8LD2t k(k+1) if E[\u2016\u2207\u0303s \u2212\u2207f(zs)\u20162] \u2264 L 2D2t\nNt(s+1)2 for all s \u2264 k.\nProof of Theorem 2. We prove by induction. The base case t = 0 holds by the exact same argument as in the proof of Theorem 1. Suppose E[f(wt\u22121)\u2212 f(w\u2217)] \u2264 LD 2\n2t and consider iteration t. Below we use another induction to prove\nE[f(yk)\u2212 f(w\u2217)] \u2264 8LD2t k(k+1) for any 1 \u2264 k \u2264 Nt, which will concludes the proof since for any of the three cases, we have E[f(wt)\u2212 f(w\u2217)] = E[f(yNt)\u2212 f(w \u2217)] \u2264 8LD 2 t N2t \u2264 LD 2\n2t+1 . We first show that the condition E[\u2016y0 \u2212w\u2217\u20162] \u2264 D2t holds. This is trivial for Case (a) and (b) when Dt = D. For Case (c), by strong convexity and the inductive assumption, we have E[\u2016y0 \u2212w\u2217\u20162] \u2264 2\u03b1E[f(y0) \u2212 f(w \u2217)] \u2264\nLD2\n\u03b12t\u22121 = D 2 t .\nNext note that Lemma 1 implies\nE[\u2016\u2207\u0303s \u2212\u2207f(zs)\u20162]\n\u2264 6L mt,s (2E[f(zs)\u2212 f(w\u2217)] + E[f(y0)\u2212 f(w\u2217)]).\nSo the key is to bound E[f(zs)\u2212 f(w\u2217)]. With z1 = y0 one can verify that E[\u2016\u2207\u03031 \u2212\u2207f(z1)\u20162] \u2264 18Lmt,1E[f(y0)\u2212 f(w\u2217)] \u2264 18L 2D2\nmt,12t \u2264 L 2D2t 4Nt holds for all three cases, and thus E[f(ys) \u2212 f(w\u2217)] \u2264 8LD2t s(s+1) holds for s = 1 by\nLemma 3. Now suppose it holds for any s < k, below we discuss the three cases separately to show that it also holds for s = k.\nCase (a). By smoothness, the condition \u2207f(w\u2217) = 0, the construction of zs, and Cauchy-Schwarz inequality, we have for any 1 < s \u2264 k,\nf(zs) \u2264 f(ys\u22121) + (\u2207f(ys\u22121)\u2212\u2207f(w\u2217))>(zs \u2212 ys\u22121)\n+ L\n2 \u2016zs \u2212 ys\u22121\u20162\n= f(ys\u22121) + \u03b3s(\u2207f(ys\u22121)\u2212\u2207f(w\u2217))>(xs\u22121 \u2212 ys\u22121)\n+ L\u03b32s\n2 \u2016xs\u22121 \u2212 ys\u22121\u20162\n\u2264 f(ys\u22121) + \u03b3sD\u2016\u2207f(ys\u22121)\u2212\u2207f(w\u2217)\u2016+ LD2\u03b32s\n2 .\nProperty (1) and the optimality ofw\u2217 implies: \u2016\u2207f(ys\u22121)\u2212\u2207f(w\u2217)\u20162 \u2264 2L(f(ys\u22121)\u2212f(w\u2217)\u2212\u2207f(w\u2217)>(ys\u22121\u2212 w\u2217)) \u2264 2L(f(ys\u22121) \u2212 f(w\u2217)). So subtracting f(w\u2217) and taking expectation on both sides, and applying Jensen\u2019s inequality and the inductive assumption, we have\nE[f(zs)\u2212 f(w\u2217)] \u2264 E[f(ys\u22121)\u2212 f(w\u2217)] + \u03b3sD \u221a 2LE[f(ys\u22121)\u2212 f(w\u2217)]\n+ 2LD2\n(s+ 1)2\n\u2264 8LD 2\n(s\u2212 1)s +\n8LD2 (s+ 1) \u221a (s\u2212 1)s + 2LD2 (s+ 1)2 < 55LD2 (s+ 1)2 .\nOn the other hand, we have E[f(y0) \u2212 f(w\u2217)] \u2264 LD 2 2t \u2264 16LD2 (Nt\u22121)2 < 40LD2 (Nt+1)2 \u2264 40LD 2 (s+1)2 . So E[\u2016\u2207\u0303s \u2212 \u2207f(zs)\u2016 2 is at most 900L 2D2\nmt,s(s+1)2 , and the choice of mt,s ensures that this bound is at most L\n2D2\nNt(s+1)2 , satisfying the condition of\nLemma 3 and thus completing the induction.\nCase (b). With the G-Lipschitz condition we proceed similarly as follows:\nf(zs) \u2264 f(ys\u22121) +\u2207f(ys\u22121)>(zs \u2212 ys\u22121) + L\n2 \u2016zs \u2212 ys\u22121\u20162\n= f(ys\u22121) + \u03b3s\u2207f(ys\u22121)>(xs\u22121 \u2212 ys\u22121) + LD2\u03b32s\n2\n\u2264 f(ys\u22121) + \u03b3sGD + LD2\u03b32s\n2 .\nSo using bounds derived previously and the choice of mt,s, we bound E[\u2016\u2207\u0303s \u2212\u2207f(zs)\u20162 as follows:\n6L\nmt,s\n( 16LD2\n(s\u2212 1)s +\n4GD s+ 1 +\n4LD2\n(s+ 1)2 +\n40LD2\n(s+ 1)2 ) < 6L\nmt,s\n( 4GD\ns+ 1 +\n116LD2\n(s+ 1)2\n) < L2D2\nNt(s+ 1)2 ,\nagain completing the induction.\nCase (c). Using the definition of zs and ys and direct calcalution, one can remove the dependence of xs and verify ys\u22121 = s+1 2s\u22121zs + s\u22122 2s\u22121ys\u22122 for any s \u2265 2. Now we apply Property (2) with \u03bb = s+1 2s\u22121 :\nf(ys\u22121) \u2265 s+ 1\n2s\u2212 1 f(zs) + s\u2212 2 2s\u2212 1 f(ys\u22122)\n\u2212 L 2 (s+ 1)(s\u2212 2) (2s\u2212 1)2 \u2016zs \u2212 ys\u22122\u20162\n= f(w\u2217) + s+ 1\n2s\u2212 1 (f(zs)\u2212 f(w\u2217))+\ns\u2212 2 2s\u2212 1 (f(ys\u22122)\u2212 f(w\u2217))\u2212 L(s\u2212 2) 2(s+ 1) \u2016ys\u22121 \u2212 ys\u22122\u20162\n\u2265 f(w\u2217) + 1 2 (f(zs)\u2212 f(w\u2217))\u2212 L 2 \u2016ys\u22121 \u2212 ys\u22122\u20162,\nwhere the equality is by adding and subtracting f(w\u2217) and the fact ys\u22121 \u2212 ys\u22122 = s+12s\u22121 (zs \u2212 ys\u22122), and the last inequality is by f(ys\u22122) \u2265 f(w\u2217) and trivial relaxations.\nRearranging gives f(zs) \u2212 f(w\u2217) \u2264 2(f(ys\u22121 \u2212 f(w\u2217)) + L\u2016ys\u22121 \u2212 ys\u22122\u20162. Applying Cauchy-Schwarz inequality, strong convexity and the fact \u00b5 \u2265 1, we continue with\nf(zs)\u2212 f(w\u2217) \u2264 2(f(ys\u22121 \u2212 f(w\u2217)) + 2L(\u2016ys\u22121 \u2212w\u2217\u20162 + \u2016ys\u22122 \u2212w\u2217\u20162) \u2264 2(f(ys\u22121 \u2212 f(w\u2217)) + 4\u00b5(f(ys\u22121)\u2212 f(w\u2217) + f(ys\u22122)\u2212 f(w\u2217)) \u2264 6\u00b5(f(ys\u22121 \u2212 f(w\u2217)) + 4\u00b5(f(ys\u22122)\u2212 f(w\u2217)),\nFor s \u2265 3, we use the inductive assumption to show E[f(zs) \u2212 f(w\u2217)] \u2264 48\u00b5LD 2 t (s\u22121)s + 32\u00b5LD2t (s\u22122)(s\u22121) \u2264 448\u00b5LD2t (s+1)2 . The case for s = 2 can be verified similarly using the bound on E[f(y0) \u2212 f(w\u2217)]. Finally we bound the term E[f(y0) \u2212 f(w\u2217)] \u2264 LD 2 2t = LD2t 2\u00b5 \u2264 32LD2t (Nt+1)2 \u2264 32LD 2 t (s+1)2 , and conclude that the variance E[\u2016\u2207\u0303s \u2212 \u2207f(zs)\u2016 2 is at most 6Lmt,s ( 896\u00b5LD2t (s+1)2 + 32LD2t (s+1)2 ) \u2264 L2D2t Nt(s+1)2 , completing the induction by Lemma 3."}, {"heading": "5 Experiments", "text": "To support our theory, we conduct experiments in the multiclass classification problem mentioned in Sec 2.1. Three datasets are selected from the LIBSVM repository4 with relatively large number of features, categories and examples, summarized in the Table 3.\nRecall that the loss function is multivariate logistic loss and \u2126 is the set of matrices with bounded trace norm \u03c4 . We focus on how fast the loss decreases instead of the final test error rate so that the tuning of \u03c4 is less important, and is fixed to 50 throughout.\nWe compare six algorithms. Four of them (SFW, SCGS, SVRF, STORC) are projection-free as discussed, and the other two are standard projected stochastic gradient descent (SGD) and its variance-reduced version (SVRG (Johnson & Zhang, 2013)), both of which require expensive projection.\nFor most of the parameters in these algorithms, we roughly follow what the theory suggests. For example, the size of mini-batch of stochastic gradients at round k is set to k2, k3 and k respectively for SFW, SCGS and SVRF, and is fixed to 100 for the other three. The number of iterations between taking two snapshots for variance-reduced methods (SVRG, SVRF and STORC) are fixed to 50. The learning rate is set to the typical decaying sequence c/ \u221a k for SGD and a constant c\u2032 for SVRG as the original work suggests for some best tuned c and c\u2032. Since the complexity of computing gradients, performing linear optimization and projecting are very different, we measure the actual running time of the algorithms and see how fast the loss decreases. Results can be found in Figure 1, where one can clearly observe that for all datasets, SGD and SVRG are significantly slower compared to the others, due to the expensive projection step, highlighting the usefulness of projection-free algorithms. Moreover, we also observe large improvement gained from the variance reduction technique, especially when comparing SCGS and STORC, as well as SVF and SVRF on the aloi dataset. Interestingly, even though the STORC algorithm gives the best theoretical results, empirically the simpler algorithms SFW and SVRF tend to have consistent better performance.\n4https://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/"}, {"heading": "6 Omitted Proofs", "text": "This section includes some previously omitted proofs."}, {"heading": "6.1 Proof of Lemma 1", "text": "Proof. Let Ei denotes the conditional expectation given all the past except the realization of i. We have\nEi[\u2016\u2207\u0303f(w;w0)\u2212\u2207f(w)\u20162] = Ei[\u2016\u2207fi(w)\u2212\u2207fi(w0) +\u2207f(w0)\u2212\u2207f(w)\u20162] = Ei[\u2016(\u2207fi(w)\u2212\u2207fi(w\u2217))\u2212 (\u2207fi(w0)\u2212\u2207fi(w\u2217))\n+ (\u2207f(w0)\u2212\u2207f(w\u2217))\u2212 (\u2207f(w)\u2212\u2207f(w\u2217))\u20162] \u2264 3Ei[\u2016\u2207fi(w)\u2212\u2207fi(w\u2217)\u20162 + \u2016(\u2207fi(w0)\u2212\u2207fi(w\u2217)) \u2212 (\u2207f(w0)\u2212\u2207f(w\u2217))\u20162 + \u2016\u2207f(w)\u2212\u2207f(w\u2217)\u20162] \u2264 3Ei[\u2016\u2207fi(w)\u2212\u2207fi(w\u2217)\u20162 + \u2016\u2207fi(w0)\u2212\u2207fi(w\u2217)\u20162\n+ \u2016\u2207f(w)\u2212\u2207f(w\u2217)\u20162]\nwhere the first inequality is Cauchy-Schwarz inequality, and the second one is by the fact Ei[\u2207fi(w0)\u2212\u2207fi(w\u2217)] = \u2207f(w0)\u2212\u2207f(w\u2217) and that the variance of a random variable is bounded by its second moment.\nWe now apply Property (1) to bound each of the three terms above. For example, Ei\u2016\u2207fi(w) \u2212 \u2207fi(w\u2217)\u20162 \u2264 2LEi[fi(w)\u2212 fi(w\u2217)\u2212\u2207fi(w\u2217)>(w\u2212w\u2217)] = 2L(f(w)\u2212 f(w\u2217)\u2212\u2207f(w\u2217)>(w\u2212w\u2217)) \u2264 2L(f(w)\u2212 f(w\u2217)) where the last step is by the optimality of w\u2217. Proceeding similarly for the other two terms concludes the proof."}, {"heading": "6.2 Proof of Lemma 2", "text": "Proof. For any s \u2264 k, by smoothness we have f(xs) \u2264 f(xs\u22121) + \u2207f(xs\u22121)>(xs \u2212 xs\u22121) + L2 \u2016xs \u2212 xs\u22121\u2016 2. Plugging in xs = (1\u2212 \u03b3s)xs\u22121 + \u03b3svs gives f(xs) \u2264 f(xs\u22121) + \u03b3s\u2207f(xs\u22121)>(vs \u2212 xs\u22121) + L\u03b3 2 s 2 \u2016vs \u2212 xs\u22121\u2016 2. Rewriting and using the fact that \u2016vs\u2212xs\u22121\u2016 \u2264 D leads to f(xs) \u2264 f(xs\u22121)+\u03b3s\u2207\u0303>s (vs\u2212xs\u22121)+\u03b3s(\u2207f(xs\u22121)\u2212 \u2207\u0303s)>(vs \u2212 xs\u22121) + LD 2\u03b32s 2 .\nThe optimality of vs implies \u2207\u0303>s vs \u2264 \u2207\u0303>s w\u2217. So with further rewriting we arrive at f(xs) \u2264 f(xs\u22121) + \u03b3s\u2207f(xs\u22121)>(w\u2217 \u2212 xs\u22121) + \u03b3s(\u2207f(xs\u22121)\u2212 \u2207\u0303s)>(vs \u2212w\u2217) + LD 2\u03b32s 2 .\nBy convexity, term\u2207f(xs\u22121)>(w\u2217\u2212xs\u22121) is bounded by f(w\u2217)\u2212f(xs\u22121), and by Cauchy-Schwarz inequality, term (\u2207f(xs\u22121)\u2212 \u2207\u0303s)>(vs \u2212w\u2217) is bounded by D\u2016\u2207\u0303s \u2212\u2207f(xs\u22121)\u2016, which in expectation is at most LD 2\ns+1 by the condition on E[\u2016\u2207\u0303s \u2212\u2207f(xs\u22121)\u20162] and Jensen\u2019s inequality. Therefore we arrive at\nE[f(xs)\u2212 f(w\u2217)]\n\u2264 (1\u2212 \u03b3s)E[f(xs\u22121)\u2212 f(w\u2217)] + LD2\u03b3s s+ 1 + LD2\u03b32s 2 = (1\u2212 \u03b3s)E[f(xs\u22121)\u2212 f(w\u2217)] + LD2\u03b32s .\nFinally we prove E[f(xk) \u2212 f(w\u2217)] \u2264 4LD 2 k+2 by induction. The base case is trival: E[f(x1) \u2212 f(w \u2217)] \u2264 (1 \u2212 \u03b31)E[f(x0) \u2212 f(w\u2217)] + LD2\u03b321 = LD2 since \u03b31 = 1. Suppose E[f(xs\u22121) \u2212 f(w\u2217)] \u2264 4LD 2 s+1 then with \u03b3s = 2 s+1 we have\nE[f(xs)\u2212 f(w\u2217)] \u2264 4LD2\ns+ 1\n( 1\u2212 2\ns+ 1 +\n1\ns+ 1\n) \u2264 4LD 2\ns+ 2 ,\ncompleting the induction."}, {"heading": "7 Conclusion and Open Problems", "text": "We conclude that the variance reduction technique, previously shown to be highly useful for gradient descent variants, can also be very helpful in speeding up projection-free algorithms. The main open question is, in the strongly convex case, whether the number of stochastic gradients for STORC can be improved fromO(\u00b52 ln 1 ) toO(\u00b5 ln 1 ), which is typical for gradient descent methods, and whether the number of linear optimizations can be improved from O( 1 ) to O(ln 1 )."}, {"heading": "B Analysis for SFW", "text": "The concrete update of SFW is\nvk = argmin v\u2208\u2126\n\u2207\u0303>k v\nwk = (1\u2212 \u03b3k)wk\u22121 + \u03b3kvk\nwhere \u2207\u0303k is the average of mk iid samples of stochastic gradient \u2207fi(wk\u22121). The convergence rate of SFW is presented below.\nTheorem 3. If each fi is G-Lipschitz, then with \u03b3k = 2k+1 and mk = ( G(k+1) LD )2 , SFW ensures for any k,\nE[f(wk)\u2212 f(w\u2217)] \u2264 4LD2\nk + 2 .\nProof. Similar to the proof of Lemma 2, we first proceed as follows,\nf(wk) \u2264 f(wk\u22121) +\u2207f(wk\u22121)>(wk \u2212wk\u22121) + L\n2 \u2016wk \u2212wk\u22121\u20162 (smoothness)\n= f(wk\u22121) + \u03b3k\u2207f(wk\u22121)>(vk \u2212wk\u22121) + L\u03b32k\n2 \u2016vk \u2212 xk\u22121\u20162 (wk \u2212wk\u22121 = \u03b3k(vk \u2212wk\u22121))\n\u2264 f(wk\u22121) + \u03b3k\u2207\u0303>k (vk \u2212wk\u22121) + \u03b3k(\u2207f(wk\u22121)\u2212 \u2207\u0303k)>(vk \u2212wk\u22121) + LD2\u03b32k\n2 (\u2016vk \u2212wk\u22121\u2016 \u2264 D)\n\u2264 f(wk\u22121) + \u03b3k\u2207\u0303>k (w\u2217 \u2212wk\u22121) + \u03b3k(\u2207f(wk\u22121)\u2212 \u2207\u0303k)>(vk \u2212wk\u22121) + LD2\u03b32k\n2 (by optimality of vk)\n= f(wk\u22121) + \u03b3k\u2207f(wk\u22121)>(w\u2217 \u2212wk\u22121) + \u03b3k(\u2207f(wk\u22121)\u2212 \u2207\u0303k)>(vk \u2212w\u2217) + LD2\u03b32k\n2\n\u2264 f(wk\u22121) + \u03b3k(f(w\u2217)\u2212 f(wk\u22121)) + \u03b3kD\u2016\u2207\u0303k \u2212\u2207f(wk\u22121)\u2016+ LD2\u03b32k\n2 ,\nwhere the last step is by convexity and Cauchy-Schwarz inequality. Since fi is G-Lipschitz, with Jensen\u2019s inequality, we further have E[\u2016\u2207\u0303k\u2212\u2207f(wk\u22121)\u2016] \u2264 \u221a E[\u2016\u2207\u0303k \u2212\u2207f(wk\u22121)\u20162] \u2264 G\u221amk , which is at most LD\u03b3k 2 with the choice of \u03b3k and mk. So we arrive at E[f(wk) \u2212 f(w\u2217)] \u2264 (1 \u2212 \u03b3k)E[f(wk\u22121) \u2212 f(w\u2217)] + LD2\u03b32k . It remains to use a simple induction to conclude the proof.\nNow it is clear that to achieve 1 \u2212 accuracy, SFW needs O(LD 2 ) iterations, and in total O( G2 L2D2 ( LD2 ) 3) =\nO(G 2LD4 3 ) stochastic gradients."}, {"heading": "C Proof of Lemma 3", "text": "Proof. Let \u03b4s = \u2207\u0303s \u2212\u2207f(zs). For any s \u2264 k, we proceed as follows:\nf(ys) \u2264 f(zs) +\u2207f(zs)>(ys \u2212 zs) + L\n2 \u2016ys \u2212 zs\u2016 2 (by smoothness)\n= (1\u2212 \u03b3s)(f(zs) +\u2207f(zs)>(ys\u22121 \u2212 zs)) + \u03b3s(f(zs) +\u2207f(zs)>(w\u2217 \u2212 zs)) + \u03b3s\u2207f(zs)>(xs \u2212w\u2217)\n+ L\u03b32s\n2 \u2016xs \u2212 xs\u22121\u20162 (by definition of ys and zs)\n\u2264 (1\u2212 \u03b3s)f(ys\u22121) + \u03b3sf(w\u2217) + \u03b3s\u2207f(zs)>(xs \u2212w\u2217) + L\u03b32s\n2 \u2016xs \u2212 xs\u22121\u20162 (by convexity)\n= (1\u2212 \u03b3s)f(ys\u22121) + \u03b3sf(w\u2217) + \u03b3s\u2207\u0303>s (xs \u2212w\u2217) + L\u03b32s\n2 \u2016xs \u2212 xs\u22121\u20162 + \u03b3s\u03b4>s (w\u2217 \u2212 xs)\n\u2264 (1\u2212 \u03b3s)f(ys\u22121) + \u03b3sf(w\u2217) + \u03b3s\u03b7t,s \u2212 \u03b3s\u03b2s(xs \u2212 xs\u22121)>(xs \u2212w\u2217) + L\u03b32s\n2 \u2016xs \u2212 xs\u22121\u20162 + \u03b3s\u03b4>s (w\u2217 \u2212 xs)\n(by Eq. (4))\n= (1\u2212 \u03b3s)f(ys\u22121) + \u03b3sf(w\u2217) + \u03b3s\u03b7t,s + \u03b2s\u03b3s\n2 (\u2016xs\u22121 \u2212w\u2217\u20162 \u2212 \u2016xs \u2212w\u2217\u20162)+\n\u03b3s 2\n( (L\u03b3s \u2212 \u03b2s) \u2016xs \u2212 xs\u22121\u20162 + 2\u03b4>s (xs\u22121 \u2212 xs) + 2\u03b4 > s (w \u2217 \u2212 xs\u22121) )\n\u2264 (1\u2212 \u03b3s)f(ys\u22121) + \u03b3sf(w\u2217) + \u03b3s\u03b7t,s + \u03b2s\u03b3s\n2 (\u2016xs\u22121 \u2212w\u2217\u20162 \u2212 \u2016xs \u2212w\u2217\u20162) + \u03b3s 2\n( \u2016\u03b4s\u20162\n\u03b2s \u2212 L\u03b3s + 2\u03b4>s (w \u2217 \u2212 xs\u22121)\n) ,\nwhere the last inequality is by the fact \u03b2s \u2265 L\u03b3s and thus\n(L\u03b3s \u2212 \u03b2s) \u2016xs \u2212 xs\u22121\u20162 + 2\u03b4>s (xs\u22121 \u2212 xs) = \u2016\u03b4s\u20162\n\u03b2s \u2212 L\u03b3s \u2212 (\u03b2s \u2212L\u03b3s) \u2225\u2225\u2225\u2225xs \u2212 xs\u22121 \u2212 \u03b4s\u03b2s \u2212 L\u03b3s \u2225\u2225\u2225\u22252 \u2264 \u2016\u03b4s\u20162\u03b2s \u2212 L\u03b3s .\nNote that E[\u03b4>s (w\u2217 \u2212 xs\u22121)] = 0. So with the condition E[\u2016\u03b4s\u2016 2 ] \u2264 L 2D2t Nt(s+1)2 def = \u03c32s we arrive at\nE[f(ys)\u2212f(w\u2217)] \u2264 (1\u2212\u03b3s)E[f(ys\u22121)\u2212f(w\u2217)]+\u03b3s ( \u03b7t,s +\n\u03b2s 2 (E[\u2016xs\u22121 \u2212w\u2217\u20162]\u2212 E[\u2016xs \u2212w\u2217\u20162]) + \u03c32s 2(\u03b2s \u2212 L\u03b3s)\n) .\nNow define \u0393s = \u0393s\u22121(1 \u2212 \u03b3s) when s > 1 and \u03931 = 1. By induction, one can verify \u0393s = 2s(s+1) and the following:\nE[f(yk)\u2212 f(w\u2217)] \u2264 \u0393k k\u2211 s=1 \u03b3s \u0393s ( \u03b7t,s + \u03b2s 2 (E[\u2016xs\u22121 \u2212w\u2217\u20162]\u2212 E[\u2016xs \u2212w\u2217\u20162]) + \u03c32s 2(\u03b2s \u2212 L\u03b3s) ) ,\nwhich is at most\n\u0393k k\u2211 s=1 \u03b3s \u0393s ( \u03b7s + \u03c32s 2(\u03b2s \u2212 L\u03b3s) ) + \u0393k 2 ( \u03b31\u03b21 \u03931 E[\u2016x0 \u2212w\u2217\u20162] + k\u2211 s=2 ( \u03b3s\u03b2s \u0393s \u2212 \u03b3s\u22121\u03b2s\u22121 \u0393s\u22121 ) E[\u2016xs\u22121 \u2212w\u2217\u20162] ) .\nFinally plugging in the parameters \u03b3s, \u03b2s, \u03b7t,s, \u0393s and the bound E[\u2016x0 \u2212w\u2217\u20162] \u2264 D2t concludes the proof:\nE[f(yk)\u2212 f(w\u2217)] \u2264 2\nk(k + 1) k\u2211 s=1 k ( 2LD2t Ntk + LD2t 2Nt(k + 1) ) + 3LD2t k(k + 1) \u2264 8LD 2 t k(k + 1) ."}], "references": [{"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Lifted coordinate descent for learning with trace-norm regularization", "author": ["Dudik", "Miro", "Harchaoui", "Zaid", "Malick", "J\u00e9r\u00f4me"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Dudik et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2012}, {"title": "An algorithm for quadratic programming", "author": ["Frank", "Marguerite", "Wolfe", "Philip"], "venue": "Naval research logistics quarterly,", "citeRegEx": "Frank et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Frank et al\\.", "year": 1956}, {"title": "Competing with the empirical risk minimizer in a single pass", "author": ["Frostig", "Roy", "Ge", "Rong", "Kakade", "Sham M", "Sidford", "Aaron"], "venue": "In Proceedings of the 28th Annual Conference on Learning Theory,", "citeRegEx": "Frostig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frostig et al\\.", "year": 2015}, {"title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "author": ["Garber", "Dan", "Hazan", "Elad"], "venue": "arXiv preprint arXiv:1301.4666,", "citeRegEx": "Garber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Garber et al\\.", "year": 2013}, {"title": "Conditional gradient algorithms for norm-regularized smooth convex optimization", "author": ["Harchaoui", "Zaid", "Juditsky", "Anatoli", "Nemirovski", "Arkadi"], "venue": "Mathematical Programming,", "citeRegEx": "Harchaoui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Harchaoui et al\\.", "year": 2015}, {"title": "Projection-free online learning", "author": ["Hazan", "Elad", "Kale", "Satyen"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Near-optimal algorithms for online matrix prediction", "author": ["Hazan", "Elad", "Kale", "Satyen", "Shalev-Shwartz", "Shai"], "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["Jaggi", "Martin"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Jaggi and Martin.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi and Martin.", "year": 2013}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "On the global linear convergence of frank-wolfe optimization variants", "author": ["Lacoste-Julien", "Simon", "Jaggi", "Martin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2015}, {"title": "Conditional gradient sliding for convex optimization", "author": ["Lan", "Guanghui", "Zhou", "Yi"], "venue": "Optimization-Online preprint (4605),", "citeRegEx": "Lan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2014}, {"title": "Mixed optimization for smooth functions", "author": ["Mahdavi", "Mehrdad", "Zhang", "Lijun", "Jin", "Rong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mahdavi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2013}, {"title": "A linearly-convergent stochastic l-bfgs algorithm", "author": ["Moritz", "Philipp", "Nishihara", "Robert", "Jordan", "Michael I"], "venue": "In Proceedings of the Nineteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Moritz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moritz et al\\.", "year": 2016}, {"title": "Accelerated training for matrix-norm regularization: A boosting approach", "author": ["Zhang", "Xinhua", "Schuurmans", "Dale", "Yu", "Yao-liang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "Examples of such problem include multiclass classification, multitask learning, recommendation systems, matrix learning and many more (see for example (Hazan & Kale, 2012; Hazan et al., 2012; Jaggi, 2013; Dudik et al., 2012; Zhang et al., 2012; Harchaoui et al., 2015)).", "startOffset": 151, "endOffset": 268}, {"referenceID": 1, "context": "Examples of such problem include multiclass classification, multitask learning, recommendation systems, matrix learning and many more (see for example (Hazan & Kale, 2012; Hazan et al., 2012; Jaggi, 2013; Dudik et al., 2012; Zhang et al., 2012; Harchaoui et al., 2015)).", "startOffset": 151, "endOffset": 268}, {"referenceID": 14, "context": "Examples of such problem include multiclass classification, multitask learning, recommendation systems, matrix learning and many more (see for example (Hazan & Kale, 2012; Hazan et al., 2012; Jaggi, 2013; Dudik et al., 2012; Zhang et al., 2012; Harchaoui et al., 2015)).", "startOffset": 151, "endOffset": 268}, {"referenceID": 5, "context": "Examples of such problem include multiclass classification, multitask learning, recommendation systems, matrix learning and many more (see for example (Hazan & Kale, 2012; Hazan et al., 2012; Jaggi, 2013; Dudik et al., 2012; Zhang et al., 2012; Harchaoui et al., 2015)).", "startOffset": 151, "endOffset": 268}, {"referenceID": 12, "context": "In this work, we thus try to answer the following question: how fast can a projection-free algorithm achieve in terms of the number of stochastic gradient evaluations and the number of linear optimizations needed to achieve a certain accuracy? Utilizing Nesterov\u2019s acceleration technique (Nesterov, 1983) and the recent variance reduction idea (Johnson & Zhang, 2013; Mahdavi et al., 2013), we propose two new algorithms that are substantially faster than previous work.", "startOffset": 344, "endOffset": 389}, {"referenceID": 1, "context": "Previous work (Dudik et al., 2012; Zhang et al., 2012) found that finding w by minimizing a regularized multivariate logistic loss gives a very accurate predictor in general.", "startOffset": 14, "endOffset": 54}, {"referenceID": 14, "context": "Previous work (Dudik et al., 2012; Zhang et al., 2012) found that finding w by minimizing a regularized multivariate logistic loss gives a very accurate predictor in general.", "startOffset": 14, "endOffset": 54}, {"referenceID": 0, "context": "The number of examples n can be prohibitively large for non-stochastic methods (for instance, tens of millions for the ImageNet dataset (Deng et al., 2009)), which makes stochastic optimization necessary.", "startOffset": 136, "endOffset": 155}, {"referenceID": 12, "context": "The key idea of our algorithms is to combine the variance reduction technique proposed in (Johnson & Zhang, 2013; Mahdavi et al., 2013) with some of the above-mentioned algorithms.", "startOffset": 90, "endOffset": 135}, {"referenceID": 12, "context": "3 Variance-Reduced Stochastic Gradients Originally proposed in (Johnson & Zhang, 2013) and independently in (Mahdavi et al., 2013), the idea of variancereduced stochastic gradients is proven to be highly useful and has been extended to various different algorithms (such as (Frostig et al.", "startOffset": 108, "endOffset": 130}, {"referenceID": 3, "context": ", 2013), the idea of variancereduced stochastic gradients is proven to be highly useful and has been extended to various different algorithms (such as (Frostig et al., 2015; Moritz et al., 2016)).", "startOffset": 151, "endOffset": 194}, {"referenceID": 13, "context": ", 2013), the idea of variancereduced stochastic gradients is proven to be highly useful and has been extended to various different algorithms (such as (Frostig et al., 2015; Moritz et al., 2016)).", "startOffset": 151, "endOffset": 194}, {"referenceID": 3, "context": ", 2013), the idea of variancereduced stochastic gradients is proven to be highly useful and has been extended to various different algorithms (such as (Frostig et al., 2015; Moritz et al., 2016)). A variance-reduced stochastic gradient at some point w \u2208 \u03a9 with some snapshot w0 \u2208 \u03a9 is defined as \u2207\u0303f(w;w0) = \u2207fi(w)\u2212 (\u2207fi(w0)\u2212\u2207f(w0)), where i is again picked from {1, . . . , n} uniformly at random. The snapshotw0 is usually a decision point from some previous iteration of the algorithm and its exact gradient \u2207f(w0) has been pre-computed before, so that computing \u2207\u0303f(w;w0) only requires two standard stochastic gradient evaluations: \u2207fi(w) and \u2207fi(w0). 2See also recent follow up work Lacoste-Julien & Jaggi (2015). 3The first result comes from the setting where the online loss functions are stochastic, and the second one comes from a completely online setting with the standard online-to-batch conversion.", "startOffset": 152, "endOffset": 718}], "year": 2016, "abstractText": "The Frank-Wolfe optimization algorithm has recently regained popularity for machine learning applications due to its projection-free property and its ability to handle structured constraints. However, in the stochastic learning setting, it is still relatively understudied compared to the gradient descent counterpart. In this work, leveraging a recent variance reduction technique, we propose two stochastic Frank-Wolfe variants which substantially improve previous results in terms of the number of stochastic gradient evaluations needed to achieve 1 \u2212 accuracy. For example, we improve from O( 1 ) to O(ln 1 ) if the objective function is smooth and strongly convex, and from O( 1 2 ) to O( 1 1.5 ) if the objective function is smooth and Lipschitz. The theoretical improvement is also observed in experiments on real-world datasets for a multiclass classification application.", "creator": "LaTeX with hyperref package"}}}