{"id": "1602.02151", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters", "abstract": "The amount of data available in the world is growing faster and bigger than our ability to deal with it. However, if we take advantage of the internal structure, data may become much smaller for machine learning purposes. In this paper we focus on one of the most fundamental machine learning tasks, empirical risk minimization (ERM), and provide faster algorithms with the help from the clustering structure of the data.", "histories": [["v1", "Fri, 5 Feb 2016 20:58:18 GMT  (818kb,D)", "http://arxiv.org/abs/1602.02151v1", null], ["v2", "Fri, 4 Nov 2016 17:10:04 GMT  (2630kb,D)", "http://arxiv.org/abs/1602.02151v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["zeyuan allen zhu", "yang yuan", "karthik sridharan"], "accepted": true, "id": "1602.02151"}, "pdf": {"name": "1602.02151.pdf", "metadata": {"source": "CRF", "title": "Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters", "authors": ["Zeyuan Allen-Zhu", "Yang Yuan", "Karthik Sridharan"], "emails": ["zeyuan@csail.mit.edu", "yangyuan@cs.cornell.edu", "sridharan@cs.cornell.edu"], "sections": [{"heading": null, "text": "We introduce a simple notion of raw clustering that can be efficiently obtained with just one pass of the data, and propose two algorithms. Our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using the clustering information, and our accelerated algorithm ClusterACDM is built on a novel Haar transformation applied to the dual space of each cluster. Our algorithms outperform their classical counterparts both in theory and practice."}, {"heading": "1 Introduction", "text": "For large-scale machine learning applications, n, the number of training data examples, is usually very large. Therefore, to search for the optimal solution, it is often desirable to use stochastic gradient methods which only requires one (or a batch of) random example(s) from the given training set per iteration as an estimator of the true gradient.\nFor empirical risk minimization problems (ERM) in particular, stochastic gradient methods have received a lot of attention in the past decade. The original stochastic gradient descent (SGD) methods [4, 27] simply defines the estimator using one random data example but converges slowly. Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].\nNone of the above cited results, however, have considered the internal structure of the dataset, i.e., using the stochastic gradient with respect to one data vector p to estimate the stochastic gradients of other data vectors close to p. To illustrate why internal structure could be very\n\u2217These two authors equally contribute to this paper.\nar X\niv :1\n60 2.\n02 15\n1v 1\n[ cs\n.L G\nhelpful, consider the following extreme case: if all the data vectors are located at the same spot, then, every stochastic gradient could represent the full gradient of the entire dataset. In a nonextreme case, if data vectors form clusters, then the stochastic gradient of one data vector could provide a rough estimation for its neighbors. For this reason, one should expect empirical risk minimization problems to be easier if the data vectors are clustered.\nMore importantly, well-clustered datasets are abundant in the big-data era. For instance, although there are more than 1 billion of users on Facebook, the intrinsic \u201cfeature vectors\u201d of these users can be naturally categorized by the users\u2019 occupations, nationalities, etc. As another example, although there are 581,012 data vectors in the famous Covtype dataset [8], each representing a 30m x 30m cell in the Roosevelt National Forest of northern Colorado, these feature vectors (after normalization) can be easily categorized into 1,445 clusters of diameter 0.1 \u2014 see Section 5. With these examples in mind, we wish to investigate how to train an ERM problem faster using clustering information.\nIn a seminal work by Hofmann et al. [11], they introduced N-SAGA, the first ERM training algorithm that takes into account the similarities between data vectors. In each iteration, NSAGA computes the stochastic gradient of one data vector p, and uses this information as a biased representative for a small neighborhood of p (say, for 20 nearby data vectors of p). However, NSAGA has several limitations. First, it is biased and does not converge to the global minimum of the training objective. Secondly, in order to keep the bias small, it can only exploit a small neighborhood for every vector. Moreover, although it only needs one gradient computation per iteration, it may require time O(20d) per iteration to update the full gradient if 20 is the average neighborhood size and d is the dimension of the vectors. This could be 20 times slower than a single iteration of SGD."}, {"heading": "1.1 Our Results and Techniques", "text": "In this paper we focus on a very simple notion of clustering that captures the minimum requirement for a cluster to have similar vectors. Assume without loss of generality that each data vector has norm at most 1. We say that a partition of the data vectors is an (s, \u03b4) raw clustering if the vectors are divided into s disjoint sets, and the average distance between vectors in each set is at most \u03b4.\nRaw clusterings certainly exist for all the datasets: an (n, 0) raw clustering corresponds to each data vector belonging to its own cluster, and a (1, 2) raw clustering corresponds to all the vectors belonging to the same cluster. For different values of \u03b4, one can obtain an (s\u03b4, \u03b4) raw clustering where s\u03b4 is a function on \u03b4. For example, a (1445, 0.1) raw clustering exists for the Covtype dataset.\nRaw clustering enjoys the following nice properties.\n\u2022 It is a weak definition, allowing a few outliers to exist in each cluster as well as nearby vectors to be split into multiple clusters. This makes raw clustering easily obtainable in practice.\n\u2022 It allows large clusters. This is very different from N-SAGA, which requires each cluster to be very small due to the bias in the algorithm.\n\u2022 It can be computed very efficiently using approximate nearest neighbor algorithms such as LSH [2] and product quantization [10, 12], see Section 5.1.\nWe explore in this paper how a given (s, \u03b4) raw clustering can improve the performance of an ERM training algorithm, for different choices of s and \u03b4, both theoretically and empirically. We propose two unbiased algorithms, which we call ClusterSVRG and ClusterACDM, that make novel use of clustering information to reduce the running time of SVRG [13] and ACDM [1, 15] respectively. Our two algorithms use very different techniques.\nNon-Accelerated Methods and Our ClusterSVRG. Our ClusterSVRG method is a nonaccelerated stochastic gradient method just like SVRG [13], SAGA [6], SDCA [24], etc.\nRecall that in a stochastic gradient method one usually defines a gradient estimator \u2207\u0303t at iteration t, and performs an update xt+1 \u2190 xt \u2212 \u03b7\u2207\u0303t for some step size \u03b7 > 0. If the objective f(x) = 1n \u2211 i fi(x) is separable, a natural choice of \u2207\u0303t is to set \u2207\u0303t = \u2207fi(xt) for some random i \u2208 [n]. This is known as the SGD method but is usually slow because the variance of this estimator can be large.\nThe key idea behind most non-accelerated methods is to carefully define an estimator \u2207\u0303t with diminishing variance. For instance, SVRG chooses \u2207\u0303t = 1n \u2211 j \u2207fj(x\u0303) + \u2207fi(xt) \u2212 \u2207fi(x\u0303) where x\u0303 is a snapshot iterate that can be n iterations ago. We observe in this paper that, in the SVRG estimator \u2207\u0303t, each term \u2207fj(x\u0303) can be viewed as a \u201cguess term\u201d of the true gradient \u2207fj(xt). These guess terms may be very outdated because x\u0303 may be n iterations away from xt, and therefore contribute to a large variance.\nWe use the raw clustering to further improve these guess terms and thus reduce the variance. More specifically, if vector j belongs to cluster c, our ClusterSVRG uses \u2207fj(x\u0303)+\u2207fk(xt)\u2212\u2207fk(x\u0303) as the new guess of \u2207fj(xt) where t is the last time cluster c was accessed and k is the index of the vector in this cluster that was accessed. This new guess only has an outdatedness of roughly s that could be much smaller than n.1\nAccelerated Methods and Our ClusterACDM. Our ClusterACDM method is an accelerated stochastic gradient method just like AccSDCA [25], APCG [17], ACDM [1, 15], SPDC [28], etc.\nRecall that accelerated methods outperform non-accelerated ones in certain parameter regimes. For instance, consider in ridge regression where the `2 regularizer has weight \u03bb > 0. The best non-accelerated methods and accelerated methods run in time\nnon-accelerated: O\u0303 ( nd+ d\n\u03bb\n) and accelerated: O\u0303 ( nd+ \u221a nd\u221a \u03bb ) (1.1)\nwhere d is the dimension of the data vectors and the O\u0303 notation hides the log 1\u03b5 factor that depends on the accuracy.\nJust by looking at the asymptotic worst-case running times in (1.1), we argue that the clustering information can have drastically different impacts on the two types of methods.\n\u2022 In a non-accelerated method, even if all the data vectors were identical so a stochastic gradient would be the same as the full gradient, a non-accelerated method still had to run O\u0303( 1\u03bb)\niterations, yielding a total running time O\u0303(d/\u03bb). Therefore, there is little room to improve the asymptotic worst-case running time of a non-accelerated method, even if good clustering information is provided. 2\n\u2022 In contrast, for an accelerated method, if all the data vectors were identical, an accelerated method could converge in O\u0303( 1\u221a\n\u03bb ) iterations, yielding a total running time O(d/\n\u221a \u03bb). In other\nwords, one could expect the running time of an accelerated method to be improved from O\u0303 ( nd+ \u221a nd\u221a \u03bb ) to O\u0303 ( nd+ d\u221a \u03bb ) at best.\n1For instance, if all the vectors in this cluster c were identical, then \u2207fj(x\u0303) +\u2207fk(xt)\u2212\u2207fk(x\u0303) would be equal to \u2207fj(xt) and thus outdated from fj(xt) by only s iterations if all clusters are of equal size. If the vectors in cluster c are different from each other by at most \u03b4 in distance, then in addition to the outdatedness s, this parameter \u03b4 also contributes to the variance computation. See equation (UB1) in Lemma 4.3 for details.\n2However, as we show in Section 4, ClusterSVRG still improves the running time especially at the first a few passes of the dataset.\nOur ClusterACDM method is designed to precisely match this suggested performance above. Given an (s, \u03b4) raw clustering, ClusterACDM enjoys a worst-case running time\nO\u0303 ( nd+ max{\u221as, \u221a \u03b4n}\u221a \u03bb d )\n(1.2)\nfor ridge regression, Lasso, and more generally for all regularized least-square problems. In the ideal case when all the feature vectors are identical, ClusterACDM converges in the aforementioned O\u0303 ( nd+ d\u221a\n\u03bb\n) running time. Otherwise, our running time is always asymptotically better than known\naccelerated methods by a factor O(min{ \u221a\nn s , 1\u221a \u03b4 }) that depends on the clustering quality.3\nOur high-level idea for ClusterACDM is simple. Since a cluster of vectors have almost identical directions if \u03b4 is small, we wish to create an auxiliary vector for each cluster representing \u201cmoving in the average direction of all vectors in this cluster\u201d. Next, we design a stochastic gradient method that, instead of uniformly choosing a random vector, selects those auxiliary vectors with a much higher probability compared with ordinary ones. This could lead to a running time improvement because moving in the direction of an auxiliary vector only costs O(d) running time but exploits the information of the entire cluster.\nWe implement the above intuition using optimization insights. In the dual space of the ERM problem, each variable corresponds to a data example in the primal, and the objective is known to be coordinate-wise smooth with the same smoothness parameter per coordinate. In the preprocessing step, ClusterACDM applies a novel Haar transformation on each cluster of the dual coordinates. Haar transformation rotates the dual space, and for each cluster, it automatically reserves a new dual variable that corresponds to the auxiliary primal vector mentioned above. Furthermore, these new dual variables have significantly larger smoothness parameters and therefore will be selected with probability much larger than 1/n if one applies a state-of-the-art accelerated coordinate descent method such as ACDM."}, {"heading": "1.2 To Practitioners", "text": "SVRG vs. SAGA vs. ClusterSVRG. The original SVRG method becomes a special case of ClusterSVRG when all the data vectors belong to the same cluster; the SAGA method also becomes a special case when each data vector belongs to its own cluster. We hope that this interpolation helps experimentalists decide between these methods: (1) if the data vectors are pairwise close to each other then use SVRG; (2) if the data vectors are all very separated from each other then use SAGA; and (3) if the data vectors have nice clustering structures then use our ClusterSVRG.\nClusterSVRG vs. ClusterACDM. Based on both theoretical and experimental results, we have the following suggestions for choosing between ClusterSVRG and ClusterACDM for experimentalists. When the given clusters has large average sizes (thus large cluster diameter), we recommend ClusterACDM and vice versa because ClusterACDM exploits large clusters better. When the regularization parameter \u03bb is large, we recommend ClusterSVRG and vice versa. In practice, one may first pick the best \u03bb using cross validation, and then decide between ClusterSVRG and ClusterACDM. See Section 5.2 for detailed comparison and reasoning.\nMini Batch. Recall that both non-accelerated and accelerated gradient methods have their mini-batch variants, such as mini-batch SVRG [14], mini-batch AccSDCA [23], and mini-batch SPDC [28]. Instead of computing a stochastic gradient with respect to a single training data\n3Since s increases as \u03b4 decreases, this speed up factor is maximized when s/\u03b4 = n. In practice, however, the performance of ClusterACDM is not very sensitive to the quality of clusters. As we shown in Section 5.3, as long as the given clustering is not degenerated, we obtain similarly fast convergence rates.\nexample, these algorithms perform updates with respect to a random subset of the data examples, known as a mini batch. These results are therefore orthogonal to the present work, and adopting mini-batch updates can further improve performance for a clustering-based method. For instance, by selecting a random data example from each cluster as a mini batch in each iteration, one can further reduce the variance of the gradient estimator in ClusterSVRG and improve its convergence."}, {"heading": "1.3 Other Related Work", "text": "Reddi et al. [20] also proposes a framework unifying SVRG and SAGA. ClusterSVRG is different from theirs as we compare SVRG and SAGA from the perspective of data clustering, while their framework generalizes several algorithms using a universal ScheduleUpdate subroutine. With the help from this clustering perspective, a more fundamental difference between SAGA and SVRG is revealed and verified by experiments in this paper: SAGA implicitly treats each data vector as a separate cluster, while SVRG implicitly treats all the vectors as a single cluster.\nResearchers also design accelerated stochastic methods via a black-box reduction to non-accelerated ones [9, 16]. The worst-case running times of these methods match that of AccSDCA. However, we can not use these reductions to directly get ClusterACDM from ClusterSVRG, since in ClusterACDM we use different techniques such as the Haar transformation and non-uniform sampling, which do not appear in either ClusterSVRG or the reduction frameworks."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Raw Clustering", "text": "Given a dataset consisting of n vectors {a1, . . . , an} \u2282 Rd, we assume without loss of generality that \u2016ai\u20162 \u2264 1 for each i \u2208 [n]. Let a clustering of this dataset denote a partition of the indices [n] = S1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ss. We call each set Sc a cluster and use nc = |Sc| to denote its size. It satisfies\u2211s\nc=1 nc = n. We are interested in the following quantification that estimates the quality of a clustering:\nDefinition 2.1 (raw clustering on vectors). We say a partition [n] = S1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ss is an (s, \u03b4) raw clustering for the vectors {a1, . . . , an} if, on average, the pairwise distances between vectors in each cluster Sc are at most \u03b4; or more precisely, for every cluster Sc it satisfies\n1 |Sc|2 \u2211\ni,j\u2208Sc \u2016ai \u2212 aj\u20162 \u2264 \u03b4 .\nWe call it a raw clustering because the above definition captures the minimum requirement for each cluster to have similar vectors. For instance, the above \u201caverage\u201d definition allows a few outliers to exist in each cluster as long as there are not too many. In addition, nearby vectors can be split into different clusters.\nRaw clustering of the dataset is very easy to obtain: we include in Section 5.1 a simple and efficient algorithm for computing an (s\u03b4, \u03b4) raw clustering of any quality \u03b4.\nWhile our accelerated method ClusterACDM works only for regularized least-square problems in theory, our non-accelerated method ClusterSVRG, like its parent method SVRG [13], focuses on a more general stochastic setting, consisting of a set of n convex functions {f1(x), . . . , fn(x)} rather than n vectors. In this case, we consider the following quantification of the clustering quality:\nDefinition 2.2 (raw clustering on functions). We say a partition [n] = S1\u222a\u00b7 \u00b7 \u00b7\u222aSs is an (s, \u03c3) raw clustering of the functions {f1(x), . . . , fn(x)} if for every cluster Sc and every x that has bounded norms, it satisfies\n1 |Sc|2 \u2211\ni,j\u2208Sc \u2016\u2207fi(x)\u2212\u2207fj(x)\u20162 \u2264 \u03c3 .\nDefinition 2.2 is a natural generalization of Definition 2.1 for the following reason. Consider the case when each function fi(x) = 1 2(\u3008ai, x\u3009 \u2212 1)2 is a least square loss function, where ai is the a feature vector with \u2016ai\u2016 \u2264 1. Then, it satisfies that \u2207fi(x) = (\u3008ai, x\u3009 \u2212 1)ai and therefore \u2016\u2207fi(x)\u2212\u2207fj(x)\u20162 = \u2016\u3008ai, x\u3009ai \u2212 \u3008aj , x\u3009aj \u2212 ai + aj\u20162\n= \u2016 ( \u3008ai, x\u3009 \u2212 1 ) (ai \u2212 aj) + \u3008ai \u2212 aj , x\u3009aj\u20162 \u2264 2 (( \u3008ai, x\u3009 \u2212 1 )2 \u00b7 \u2016ai \u2212 aj\u20162 + ( \u3008ai \u2212 aj , x\u3009 )2 \u00b7 \u2016aj\u20162 ) \u2264 O ( \u2016x\u20162 ) \u00b7 \u2016ai \u2212 aj\u20162 .\nSince we have assumed x to be bounded, it follows that an (s, \u03b4) raw clustering on the dataset implies an (s, \u03c3) raw clustering on the corresponding least-square loss functions where the parameter \u03c3 is on the same order as \u03b4. Similar results can be deduced for other loss functions including logistic loss, smoothed hinge losses, etc.\nWe note that a very similar assumption on functions like our (s, \u03c3) raw clustering assumption in Definition 2.2 was also introduced by Hofmann et al. [11]."}, {"heading": "2.2 Convex Optimization", "text": "Recall some classical definitions on the strong convexity and the smoothness of a function.\nDefinition 2.3 (Smoothness and strong convexity). For a convex function f : Rn \u2192 R, \u2022 f is \u03c3-strongly convex if \u2200x, y \u2208 Rn, it satisfies f(y) \u2265 f(x) + \u3008\u2207f(x), y \u2212 x\u3009+ \u03c32 \u2016x\u2212 y\u20162. \u2022 f is L-smooth if \u2200x, y \u2208 Rn, it satisfies \u2016\u2207f(x)\u2212\u2207f(y)\u2016 \u2264 L\u2016x\u2212 y\u2016. \u2022 f is coordinate-wise smooth with parameters (L1, L2, . . . , Ln), if\n\u2200x \u2208 Rn, \u2200\u03b4 > 0, \u2200i \u2208 [n] : |\u2207if(x+ \u03b4ei)\u2212\u2207if(x)| \u2264 Li \u00b7 \u03b4 . If f is twice differentiable, this requirement can be simplified as \u22072iif(x) \u2264 Li for all x \u2208 Rn.\nFor strongly convex and coordinate-wise smooth functions f , one can apply the accelerated coordinate descent algorithm (ACDM) to minimize f :\nTheorem 2.4 (ACDM). If f(x) is \u03c3-strongly convex and coordinate-wise smooth with parameters (L1, . . . , Ln), the non-uniform accelerated coordinate descent method of [1] produces an output y satisfying f(y)\u2212minx f(x) \u2264 \u03b5 in\nO (\u2211\ni\n\u221a Li/\u03c3 \u00b7 log(1/\u03b5)\n)\niterations. Each iteration runs in time proportional to the computation of a coordinate gradient \u2207if(\u00b7) of f .\nWe remark here that accelerated coordinate descent method admits several variants such as APCG [17], ACDM [15], and NU ACDM [1]. These variants agree on the convergence rate in the case when L1 = \u00b7 \u00b7 \u00b7 = Ln, but NU ACDM has reported a faster convergence when L1, . . . , Ln are not identical. Therefore, we refer to NU ACDM as the accelerated coordinate descent method (ACDM) in this paper."}, {"heading": "3 ClusterACDM Algorithm", "text": "Consider a regularized least-square problem\nPrimal: min x\u2208Rd\n{ P (x) def = 1\n2n\nn\u2211\ni=1\n(\u3008ai, x\u3009 \u2212 li)2 + r(x) } , (3.1)\nwhere each ai \u2208 Rd is the feature vector of a training example and li is the label of ai. Problem (3.1) becomes ridge regression when r(x) = \u03bb2\u2016x\u201622, and becomes Lasso when r(x) = \u03bb\u2016x\u20161.\nOne of the state-of-the-art accelerated stochastic gradient methods to solve (3.1) is through its dual. Consider the following equivalent dual formulation of (3.1) (see for instance [17] for the detailed proof):\nDual: min y\u2208Rn\n{ D(y) def = 1\n2n \u2016y\u20162 + 1 n \u3008y, l\u3009+ r\u2217 ( \u2212 1 n Ay ) = 1 n n\u2211\ni=1\n(1 2 y2i + yi \u00b7 li ) + r\u2217 ( \u2212 1 n n\u2211\ni=1\nyiai\n)} ,\n(3.2)\nwhere A = [a1, a2, . . . , an] \u2208 Rd\u00d7n is the data matrix and r\u2217(y) def= maxw yTw \u2212 r(w) is the Fenchel dual of r(w)."}, {"heading": "3.1 Previous Solutions", "text": "If r(x) is \u03bb-strongly convex in the primal objective P (x), the dual objective D(y) is both strongly convex and smooth. The following lemma is due to [17] but is anyways proved for completeness\u2019 sake.\nLemma 3.1. If r(x) is \u03bb-strongly convex, then D(y) is \u03c3 = 1n strongly convex and coordinate-wise smooth with parameters (L1, . . . , Ln) for Li = 1 n + 1 \u03bbn2 \u2016ai\u20162.\nProof. Since there is a component 12n\u2016y\u20162 in the definition of D(y), we know that D(y) is 1n -strongly convex. To prove the smoothness, recall that if r(\u00b7) is \u03bb strongly convex, then r\u2217(\u00b7) is 1/\u03bb smooth, or mathematically, \u22072r\u2217(x) 1\u03bbI (see for instance the text book [5]). Here we denote by A B the spectral dominance of matrices, which means xTAx \u2264 xTBx for every x \u2208 Rn.\nUsing this fact, we compute the Hessian of D(\u00b7)\n\u22072D(y) = 1 n I +\u22072r\u2217 ( \u2212 1 n Ay ) 1 n I + 1 n2 AT I \u03bb A = 1 n + 1 \u03bbn2 ATA .\nNow, since (ATA)ii = \u2016ai\u20162 for each i \u2208 [n], we have that \u22072iiD(y) \u2264 1n + 1\u03bbn2 \u2016ai\u20162 which gives the desired smoothness property.\nFor this reason, the authors of [17] proposed to apply accelerated coordinate descent (such as their APCG method) to minimize D(y).4 Assuming without loss of generality \u2016ai\u20162 \u2264 1 for each feature vector, we have Li \u2264 1n + 1\u03bbn2 . Owing to Theorem 2.4, one can conclude that the ACDM method produces an output y satisfying D(y)\u2212minzD(z) \u2264 \u03b5 in\nO (\u2211\ni\n\u221a Li/\u03c3 log(1/\u03b5) ) = O(n\n\u221a 1 n + 1 \u03bbn2\n1/n log(1/\u03b5)) = O\u0303(n+\n\u221a n/\u03bb)\n4They showed that defining x = \u2207r\u2217(\u2212Ay/n), if y is a good approximate minimizer of the dual objective D(y), x is also a good approximate minimizer of the primal objective P (x).\nAlgorithm 1 ClusterACDM Input: a raw clustering S1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ss. 1: Apply cluster-based Haar transformation Hcl to get the transformed objective D\n\u2032(y\u2032). 2: Run ACDM to minimize D\u2032(y\u2032) 3: Transform the solution of D\u2032(y\u2032) back to the original space.\niterations, and each iteration runs in time proportional to the computation of \u2207iD(y) which is usually O(d) for ERM problems. This total running time O\u0303(nd + \u221a n/\u03bbd) matches the fastest known running time (originally obtained by AccSDCA [25]) for solving (3.1) when r(x) is \u03bb-strongly convex.\nAbove, we only focused on the case when r(x) is strongly convex. If r(x) is not strongly convex, we can always make r(x) it strongly convex by a standard reduction. For instance, in Lasso we may have r(x) = \u03bb1\u2016x\u20161 in the primal objective (3.1) which is not strongly convex. In this case, it suffices to look at an alternative regularizer r\u2032(x) def= r(x) + \u03bb2\u2016x\u201622 and its corresponding objective function P \u2032(x) def= P (x)+ \u03bb2\u2016x\u201622. Since for every x it satisfies |P (x)\u2212P \u2032(x)| \u2264 O(\u03bb), one can specify a small enough parameter \u03bb proportional to the desired error \u03b5, and minimize P \u2032(x) instead. This auxiliary 2-norm regularizer introduces error to the objective, but allows the function P (x) to be minimized in O\u0303(nd+ \u221a n/\u03bbd) = O\u0303(nd+ \u221a n/\u03b5d) time, again matching the fastest known running time by AccSDCA. Thus, in this section we always assume that r(x) is \u03bb-strongly convex for some \u03bb > 0."}, {"heading": "3.2 Our New Algorithm", "text": "Since each dual coordinate yi corresponds to the i-th feature vector ai, given a raw clustering [n] = S1 \u222a S2 \u222a \u00b7 \u00b7 \u00b7 \u222a Ss of the dataset {a1, . . . , an}, we can naturally partition the coordinates of the dual vector y \u2208 Rn into s blocks corresponding to the clusters. Without loss of generality, we assume that the coordinates of y are sorted in the order of the cluster indices. In other words, we write y = (yS1 , . . . , ySs) where each ySc \u2208 Rnc .\nOur ClusterACDM method transforms the dual objective (3.2) into its equivalent form, by performing nc-dimensional Haar transformation on its c-th block of coordinates, for every c = 1, 2, . . . , s. Formally,5 Definition 3.2. Let R2 def = [ 1/ \u221a 2 \u22121/ \u221a 2 ] ,\nR3 def = [ \u221a2/ \u221a 3 \u2212 \u221a 2/(2 \u221a 3) \u2212 \u221a 2/(2 \u221a 3)\n0 1/ \u221a 2 \u22121/ \u221a 2\n] ,\nand more generally\nRn def =   1/a\u221a 1/a+1/b . . . 1/a\u221a 1/a+1/b \u22121/b\u221a 1/a+1/b . . . \u22121/b\u221a 1/a+1/b\nRa 0 0 Rb\n  \u2208 R(n\u22121)\u00d7n\nfor a = bn/2c and b = dn/2e. Then, define the n-dimensional (normalized) Haar matrix as\nHn def = [ 1/ \u221a n \u00b7 \u00b7 \u00b7 1/\u221an Rn ] \u2208 Rn\u00d7n\n5We note that an n-dimensional Haar transformation is often defined in the literature only for n being an integral power of 2; in contrast, we generalize them into arbitrary integral dimensions for our application in this paper.\nWe give a few examples of the Haar matrices we introduced in Definition 3.5.\nExample 3.3. We have\nH4 =   1/2 1/2 1/2 1/2 1/2 1/2 \u22121/2 \u22121/2 1/ \u221a 2 \u22121/ \u221a 2 0 0\n0 0 1/ \u221a 2 \u22121/ \u221a 2\n \nand\nH7 =   1/ \u221a 7 1/ \u221a 7 1/ \u221a 7 1/ \u221a 7 1/ \u221a 7 1/ \u221a 7 1/ \u221a 7 2/ \u221a 21 2/ \u221a 21 2/ \u221a 21 \u2212 \u221a 21/14 \u2212 \u221a 21/14 \u2212 \u221a 21/14 \u2212 \u221a 21/14\u221a 2/ \u221a 3 \u2212 \u221a 2/(2 \u221a 3) \u2212 \u221a 2/(2 \u221a 3) 0 0 0 0 0 1/ \u221a 2 \u22121/ \u221a 2 0 0 0 0 0 0 0 1/2 1/2 \u22121/2 \u22121/2 0 0 0 1/ \u221a 2 \u22121/ \u221a 2 0 0\n0 0 0 0 0 1/ \u221a 2 \u22121/ \u221a 2\n \nIt is easy to verify that\nLemma 3.4. For every n, HTnHn = HnH T n = I, so Hn is a unitary matrix.\nDefinition 3.5. Given a clustering [n] = S1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ss, define the following cluster-based Haar transformation Hcl \u2208 Rn that is a block diagonal matrix:\nHcl =   H|S1| 0 0 \u00b7 \u00b7 \u00b7 0 0 0 H|S2| 0 \u00b7 \u00b7 \u00b7 0 0 ... ... . . .\n... 0 0 0 \u00b7 \u00b7 \u00b7 0 H|Ss|\n  .\nAccordingly, we apply the unitary transformation Hcl on (3.2) and consider\nmin y\u2032\u2208Rn\n{ D\u2032(y\u2032) def= 1\n2n \u2016y\u2032\u20162 + 1 n \u3008y\u2032, Hcll\u3009+ r\u2217 ( \u2212 1 n AHTcl y \u2032)} . (3.3)\nWe call D\u2032(y\u2032) the transformed objective function.\nIt is clear that the minimization problem (3.3) is equivalent to (3.2) by making transformation y = HTcl y\n\u2032. Our ClusterACDM algorithm now simply applies ACDM on minimizing this transformed objective D\u2032(y\u2032).\nHigh-Level Intuition. To see why cluster-based Haar transformation is helpful, let us focus on one cluster c \u2208 [s]. Assume without loss of generality that this cluster has feature vectors a1, a2, \u00b7 \u00b7 \u00b7 , anc . After applying the Haar transformation on this cluster, the new columns 1, 2, . . . , nc of matrix AHTcl become weighted combinations of a1, a2, \u00b7 \u00b7 \u00b7 , anc , and the weights are determined by the entries in the corresponding row of Hnc .\nObserve that every row except the first one in Hnc has its entries sum up to 0. Therefore, columns 2, . . . , nc in AH T cl will be close to zero vectors and have small norms. In contrast, since the\nfirst row in Hnc has all entries equal to 1/ \u221a nc, the first column of AH T cl becomes \u221a nc \u00b7 a1+\u00b7\u00b7\u00b7+ancnc , the scaled average of all vectors in this cluster. It has a large Euclidean norm. See the following illustration: [\na1, a2, . . . , anc ] Hnc = [\u221a nc \u00b7\na1 + \u00b7 \u00b7 \u00b7+ anc nc\n, \u2248 0, \u00b7 \u00b7 \u00b7 ,\u2248 0 ]\nThe first column after Haar transformation can be seen as an auxiliary feature vector representing the entire cluster. If we runs ACDM with respect to this new matrix, and whenever an auxiliary column is selected, it represents \u201cmoving in the average direction of all vectors in this cluster\u201d. Of course, this single auxiliary column cannot represent the entire cluster, and therefore the remaining nc \u2212 1 columns serve as helpers that ensure that the algorithm is unbiased (i.e., it converges to the exact minimizer).\nMost importantly, ACDM is a stochastic method that samples a feature vector with a probability roughly proportional to its Euclidean norm. Thus, we expect the auxiliary vector to be sampled with probability much larger 1/n. This intuition can be formalized as the following theorem:\nTheorem 3.6. Given an (s, \u03b4) raw clustering on vectors, ClusterACDM converges to an \u03b5-approximate minimizer of D\u2032(\u00b7) (thus an \u03b5-approximate minimizer of D(\u00b7)) in T = O ( n + max{ \u221a s, \u221a \u03b4n}\u221a\n\u03bb\n) itera-\ntions, each costing O(d) time.\nComparing this to the complexity of APCG, ACDM, or AccSDCA (see Section 3.1), ClusterACDM is faster by a factor that is at least \u2126 ( min{ \u221a n/s, \u221a 1/\u03b4} ) for small \u03bb > 0. Detailed analysis can be found in the following subsections."}, {"heading": "3.3 Proof Details", "text": "The following statement is a simple corollary of Lemma 3.1 on the transformed objective:\nCorollary 3.7. Letting b1, . . . , bn \u2208 Rd be the column vectors of AHT , then the transformed objective D\u2032(\u00b7) is coordinate-wise smooth with parameters (L\u20321, . . . , L\u2032n) for L\u2032i = 1n + 1\u03bbn2 \u2016bi\u20162.\nWe are now ready to relate the smoothness parameters of the transformed objective to how good a raw clustering is. Suppose that \u2016a1\u20162 = \u00b7 \u00b7 \u00b7 \u2016an\u20162 = 1 for the sake of simplicity.6 We consider the following definition of raw clustering:\nDefinition 3.8 (raw clustering on vectors). We say that a partition S = S1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ss is a (s, \u03b4) raw clustering, if for every cluster c, it satisfies that 1\u2016Sc\u20162 \u2211 i,j\u2208Sc\u3008ai, aj\u3009 \u2265 1\u2212 \u03b4 2 . In other words, in average, the pairwise inner products of features vectors in each cluster Sc are at least 1\u2212 \u03b42 .\nSince \u2016ai \u2212 aj\u20162 = 2 \u2212 2\u3008ai, aj\u3009, the above definition is equivalent to Definition 2.1. It implies the following property on the coordinate smoothness of the transformed objective:\nLemma 3.9. If the transformed objective is built on a (s, \u03b4) raw clustering, then it satisfies\n\u2211\ni\u2208[n]\n\u221a L\u2032i \u2264 \u221a n+ \u221a c/n+ \u03b4/2\u221a\n\u03bb\nProof. For each cluster c \u2208 [s], recall that the first row of the Haar matrix Hnc is all 1/ \u221a nc. Let ic \u2208 [n] be the index of this first row in our matrix H for each cluster c, which are \u201cimportant\u201d indices. Since bic is the i-th column vector of AH T = [a1, . . . , an]H T , we have\nbic = 1\u221a nc\n\u2211 j\u2208Sc aj .\n6We remark here that in some machine learning applications, the feature vectors are already normalized so this already holds. If not, one can treat vectors with different magnitudes of norms separately and perform raw clustering.\nTherefore, we have \u2016bic\u20162 = 1nc \u2211\nj1,j2\u2208Sc\u3008aj1 , aj2\u3009 \u2265 (1 \u2212 \u03b4/2)nc due to our definition of the (s, \u03b4) raw clustering. Summing them up, we have\n\u2211\ni\u2208{i1,...,is} \u2016bi\u20162 \u2265\n( 1\u2212 \u03b4/2 ) n\nand we define \u03b4\u2032 \u2264 \u03b4 to be the parameter that satisfies \u2211i\u2208{i1,...,is} \u2016bi\u20162 = (1\u2212 \u03b4\u2032 2 )n.\nNow we compute the summation of \u221a L\u2032i by dividing indices into important indices i1, i2, . . . , is\nand others. We first sum them up for important indices:\n\u2211\ni\u2208{i1,...,is}\n\u221a L\u2032i \u2264 \u221a s \u00b7 ( \u2211 i\u2208{i1,...,is} L\u2032i )1/2 = \u221a s \u00b7 ( s n + (1\u2212 \u03b4\u20322 )n \u03bbn2 )1/2\nOn the other hand, for non-important indices, we have\n\u2211\ni 6\u2208{i1,...,is}\n\u221a L\u2032i \u2264 \u221a n\u2212 s \u00b7 ( \u2211 i 6\u2208{i1,...,is} L\u2032i )1/2 = \u221a n\u2212 s \u00b7 (n\u2212 s n + \u03b4\u2032n 2\u03bbn2 )1/2\nTogether, we have\n\u2211\ni\u2208[n]\n\u221a L\u2032i \u2264 \u221a s \u00b7 \u221a s n + \u221a n\u2212 s \u00b7 \u221a n\u2212 s n + \u221a s \u00b7 \u221a 1\u2212 \u03b4\u20322 \u03bbn + \u221a n\u2212 s \u00b7 \u221a \u03b4\u2032 2\u03bbn \u2264 \u221an+ \u221a s/n+ \u03b4/2\u221a \u03bb .\nFinally, using the fact that D\u2032(\u00b7) is \u03c3 = 1n strongly convex, we can apply Theorem 2.4 and deduce Theorem 3.6."}, {"heading": "4 ClusterSVRG Algorithm", "text": "Consider the following composite convex minimization:\nmin x\u2208Rd\n{ F (x) def = f(x) + \u03a8(x) def = 1\nn\nn\u2211\ni=1\nfi(x) + \u03a8(x) } . (4.1)\nHere, f(x) = 1n \u2211n\ni=1 fi(x) is the finite average of n functions, each fi(x) is convex and L-smooth, and \u03a8(x) is a simple (but possibly non-differentiable) convex function, sometimes called the proximal function. We denote x\u2217 as a minimizer of (4.1).\nThis setting is studied by many stochastic gradient methods including SVRG, and captures all known empirical risk minimization problems such as Lasso and ridge regression."}, {"heading": "4.1 Previous Solutions", "text": "As illustrated in the introduction, variance-reduction based stochastic gradient methods iteratively perform the following update\nxt = arg min x { 1 2\u03b7 \u2016x\u2212 xt\u22121\u20162 + \u3008\u2207\u0303t\u22121, x\u3009+ \u03a8(x) } ,\nwhere \u2207\u0303t\u22121 is the so-called gradient estimator and usually its expectation has to equal to the full gradient \u2207f(xt\u22121) in order to make the algorithm unbiased. The central idea behind all such methods is to ensure that the variance Var[\u2207\u0303t\u22121] decreases quickly.\nIn particular, SVRG has an outer loop of epochs, where at the beginning of each epoch, SVRG records the position of the current iterate x as the snapshot point x\u0303,7and computes its full gradient \u2207f(x\u0303). Each epoch consists of m inner iterations, and in each inner iteration, SVRG picks a random i and defines the gradient estimator\n\u2207\u0303t\u22121SVRG def =\n1\nn\nn\u2211\nj=1\n\u2207fj(x\u0303) +\u2207fi(xt\u22121)\u2212\u2207fi(x\u0303) . (4.2)\nSAGA works differently from SVRG. It maintains a table of n vectors that stores the gradient \u2207fi(\u03c6i) at position \u03c6i for each i \u2208 [n]. In each iteration t, SAGA picks a random i and define\n\u2207\u0303t\u22121SAGA def =\n1\nn\nn\u2211\nj=1\n\u2207fj(\u03c6j) +\u2207fi(xt\u22121)\u2212\u2207fi(\u03c6i) .\nAfter updating xt using this estimator, SAGA records this new position \u03c6i \u2190 xt\u22121 and update the corresponding \u2207fi(\u03c6i) in the table."}, {"heading": "4.2 Our New Algorithm", "text": "Our ClusterSVRG computes the gradient estimator \u2207\u0303 based on the clustering information. Given a clustering [n] = S1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ss, we denote by ci \u2208 [s] is the cluster that index i belongs to. Using the same snapshot notation x\u0303 as SVRG, we define\n\u2207\u0303t\u22121 def= 1 n\nn\u2211\nj=1\n( \u2207fj(x\u0303) + \u03b6cj ) +\u2207fi(xt\u22121)\u2212 ( \u2207fi(x\u0303) + \u03b6ci ) (4.3)\nAbove, i is chosen uniformly at random from [n], and for each cluster c, we introduce an additional \u03b6c that we shall define shortly. The existence of \u03b6c for different clusters makes our \u2207\u0303t\u22121 different from \u2207\u0303t\u22121SVRG. However, if there were only one cluster, then \u2207\u0303t\u22121 would be identical to \u2207\u0303t\u22121SVRG no matter how \u03b6c is defined.\nBefore defining \u03b6c, we note that Ei[\u03b6ci ] = \u2211s c=1 nc n \u03b6c = \u2211n j=1 \u03b6cj n and therefore our estimator is unbiased: that is, Ei[\u2207\u0303t\u22121] = \u2207f(xt\u22121). Similar to SVRG, we propose two slightly different definitions for \u03b6c, and call them Option I and Option II. In both options, we initialize \u03b6c = 0 at the beginning of the epoch.\n\u2022 In Option I, after each iteration t is completed and suppose i is the random index chosen at iteration t, we update \u03b6ci \u2190 \u2207fi(xt\u22121)\u2212\u2207fi(x\u0303).\n\u2022 In Option II, we divide an epoch of length m into subepochs of length s. At the beginning of each subepoch, for each cluster c \u2208 [s], we define \u03b6c \u2190 \u2207fj(x) \u2212\u2207fj(x\u0303). Here, x is the last iterate of the previous subepoch and j is a random index in Sc.\nWe summarize both options in Algorithm 2.\nHigh Level Intuition. In the estimator \u2207\u0303t\u22121SVRG, each \u2207fj(x\u0303) intuitively serves as a \u201crough guess\u201d of the gradient \u2207fj(xt\u22121) with respect to the iterate xt\u22121. However, this guess may be quite\n7More precisely, SVRG provides two options, one defining x\u0303 as the average iterates of the previous epoch, and the other defining x\u0303 to be the last iterate of the previous epoch.\nAlgorithm 2 ClusterSVRG Input: Epoch length m and learning rate \u03b7, a raw clustering S1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ss. 1: x0, x\u2190 initial point, t\u2190 0. 2: for epoch\u2190 0 to MaxEpoch do 3: x\u0303\u2190 xt, and (\u03b61, . . . , \u03b6s)\u2190 (0, . . . , 0) 4: for iter\u2190 1 to m do 5: t\u2190 t+ 1 and choose i uniformly at random from {1, \u00b7 \u00b7 \u00b7 , n} 6: xt \u2190 xt\u22121 \u2212 \u03b7 ( 1 n \u2211n j=1 ( \u2207fj(x\u0303) + \u03b6cj ) +\u2207fi(xt\u22121)\u2212 ( \u2207fi(x\u0303) + \u03b6ci ))\n7: Option I: \u03b6ci \u2190 \u2207fi(xt\u22121)\u2212\u2207fi(x\u0303) 8: Option II: if iter mod s = 0 then for all c = 1, . . . , s, 9: \u03b6c \u2190 \u2207fj(xt\u22121)\u2212\u2207fj(x\u0303) where j is randomly chosen from Sc.\n10: end for 11: end for\ndifferent from \u2207fj(xt\u22121) because at most m iterations may have passed between x\u0303 and xt\u22121. In this case, we say \u2207fj(x\u0303) has an outdatedness of O(m). Since the epoch length m is usually of the same magnitude as n [13], this outdatedness may be very large. Our newly introduced \u03b6c, as a cluster-dependent correction term, will help us decrease this outdatedness.\nMore specifically, in Option I, define lt(c) to be the most recent time t when any index i in cluster Sc was randomly chosen, and define li(c) to be this index i. Then, we can write\n\u03b6c = \u2207fli(c)(xlt(c))\u2212\u2207fli(c)(x\u0303) .\nUnder this notation, we claim that \u2207fj(x\u0303) + \u03b6cj is a better guess of \u2207fj(xt\u22121) comparing to \u2207fj(x\u0303). Indeed, if all the gradients \u2207fk(x) for k \u2208 Scj are close enough (which follows from the raw clustering definition in Definition 2.2), then \u2207fj(x\u0303) \u2248 \u2207fli(c)(x\u0303) and therefore\n\u2207fj(x\u0303) + \u03b6cj = \u2207fj(x\u0303) +\u2207fli(c)(xlt(c))\u2212\u2207fli(c)(x\u0303) \u2248 \u2207fli(c)(xlt(c)) \u2248 \u2207fj(xlt(c)) .\nHere, the right hand side \u2207fj(xlt(c)) has only an outdatedness of t\u2212 lt(c) compared with \u2207fj(xt\u22121), and t \u2212 lt(c) can be as small as s if all the s clusters are of equal size. In sum, this smaller outdatedness implies that our choice of \u2207\u0303t\u22121 is a better estimator of the full gradient.\nCompared with Option I which updates \u03b6c incrementally, our Option II can be seen as a batchupdate version. An almost identical argument from the one above implies \u2207fj(x\u0303) + \u03b6cj in Option II is a guess of \u2207fj(xt\u22121) with outdatedness at most s, because x and xt\u22121 differ by at most s iterations. This is simpler to analyze because in Option I, the outdatedness is at most s only when the clusters are of equal size. For the sake of cleanness, in this paper we only provide theoretical proofs for Option II. From an efficiency perspective, Option I is preferred in practice because it only needs one stochastic gradient computation per iteration where Option II needs two in the amortized sense.\nFull analysis of ClusterSVRG can be found in in the rest of this section."}, {"heading": "4.3 Convergence Analysis", "text": "We now formalize the high level intuitions provided in Section 4.2. We make the following assumption in this section:\nAssumption 4.1. We assume that in each epoch of ClusterSVRG, between two consecutive iterations t\u2212 1 and t, it always satisfies \u2016xt\u22121 \u2212 xt\u20162 \u2264 \u03be.\nNote that this is a reasonable assumption because in most interesting applications the gradients are bounded. Under this assumption, we have the following measurement of the outdatedness:\nClaim 4.2 (Outdatedness). (a) For every i \u2208 [n], it satisfies \u2016\u2207fi(x)\u2212\u2207fi(xt\u22121)\u20162 \u2264 L2s2\u03be. (b) Given any (s, \u03c3) raw clustering of the functions, for indices i and j that are randomly selected\nfrom the same cluster Sc, it satisfies Ei,j [ \u2016\u2207fi(x)\u2212\u2207fj(xt\u22121)\u20162 ] \u2264 2\u03c3 + 2L2s2\u03be.\nThe above claim captures the \u201coutdatedness\u201d parameter that we discussed before. Indeed, since x and xt\u22121 can be at most s iterations apart, the gradient \u2207fi(x) is outdated from \u2207fi(xt\u22121) by at most s iterations. However, knowing only the number of iterations it remains difficult to measure the actual distance \u2016\u2207fi(x) \u2212 \u2207fi(xt\u22121)\u20162. In Claim 4.2.a, we simply use the smoothness of the function fi(\u00b7) to provide a very loose upper bound on this distance, which turns out to be already quite useful when providing a theoretical comparison between ClusterSVRG and SVRG later in this section. In addition, Claim 4.2.b simply combines Claim 4.2.a with the \u03c3-rawness of the given clustering. A formal proof is as follows.\nProof of Claim 4.2. The first half of the claim follows from\n\u2016\u2207fi(x)\u2212\u2207fi(xt\u22121)\u20162 \u2264 L2\u2016x\u2212 xt\u22121\u20162 \u2264 L2s2\u03be2 .\nAbove, the first inequality is due to the smoothness of function fi(\u00b7), and the second inequality is because there are at most s iterations between x and xt\u22121.\nTo prove the second half, we compute that\nE[\u2016\u2207fi(x)\u2212\u2207fj(xt\u22121)\u20162] = E[\u2016\u2207fi(x)\u2212\u2207fj(x) +\u2207fj(x)\u2212\u2207fj(xt\u22121)\u20162]\n\u2264 2E[\u2016\u2207fi(x)\u2212\u2207fj(x)\u20162] + 2E[\u2016\u2207fj(x)\u2212\u2207fj(xt\u22121)\u20162]\n\u2264 2\u03c3 + 2L2s2\u03be . We are now ready to state the main lemma of this section. It requires a few careful applications\nof Claim 4.2, and we defer its proof to Appendix B.\nLemma 4.3 (Variance of ClusterSVRG). In ClusterSVRG with Option II, if a (s, \u03c3) raw clustering on the functions is provided and Assumption 4.1 is satisfied, then we have the following upper bound on the variance of the gradient estimator at iteration t:\nE[Vari[\u2207\u0303t\u22121]] = E [\u2225\u2225\u2207\u0303t\u22121 \u2212\u2207f(xt\u22121) \u2225\u22252] \u2264 O ( \u03c3 + L2s2\u03be ) . (UB1)\nWhere the expectation is over all the randomness in the current epoch. In addition, we also have\nE[Vari[\u2207\u0303t\u22121]] \u2264 O(L) \u00b7 E [( f(x\u0303)\u2212 f(x\u2217) ) + ( f(x)\u2212 f(x\u2217) ) + ( f(xt\u22121)\u2212 f(x\u2217) )] . (UB2)\nWe compare our main lemma to its counterpart for SVRG:\nLemma 4.4 (Variance of SVRG). In SVRG, we have the following upper bound on the variance of the gradient estimator at iteration t:\nE[Vari[\u2207\u0303t\u22121]] = E [\u2225\u2225\u2207\u0303t\u22121 \u2212\u2207f(xt\u22121) \u2225\u22252] \u2264 O ( L2m2\u03be ) . (UB1\u2019)\nIn addition, we also have\nE[Vari[\u2207\u0303t\u22121]] \u2264 O(L) \u00b7 E [( f(xt\u22121)\u2212 f(x\u2217) ) + ( f(x\u0303)\u2212 f(x\u2217) )] (UB2\u2019)\nProof. The proof for (UB2\u2019) can be found on page 6 of [13]. The proof for (UB1\u2019) is almost identical to that for (UB1).\nFollowing the main intuition behind all the variance-reduction methods, we know that a stochastic gradient method converges faster if the variance of the gradient estimator \u2207\u0303t\u22121 is small. It is easy to see that (UB1) is a better upper bound than (UB1\u2019) if an (s, \u03c3) raw clustering is provided, and at the same time our (UB2) has the same form as (UB2\u2019). Therefore, the variance of the gradient estimator in ClusterSVRG is better than that in SVRG.\nFor interested readers, we compare these upper bounds more carefully in the next subsection."}, {"heading": "4.4 Discussion", "text": "Upper bounds (UB2) and (UB2\u2019) are smaller than (UB1) and (UB1\u2019) respectively for large iterations t, because our objective value f(x) approaches to f(x\u2217) as t increases. For this reason, they are better choices in order to obtain an asymptotic worst-case running time.\nFor instance, if only (UB2\u2019) is used in the analysis, Johnson and Zhang [13] showed that for \u03c3-strongly convex objectives F (x) in (4.1), by setting m = L/\u03c3, the SVRG method converges to an \u03b5-approximate minimizer of F (x) in T = O((n+ L\u03c3 ) log(1/\u03b5)) iterations. This matches the best known running time performance for non-accelerated methods on solving (4.1). For ClusterSVRG, using (UB2) and an analogous analysis to [13], it matches the worst case running time of SVRG.\nIn contrast, in the first a few epochs of the algorithm, we expect (UB1) and (UB1\u2019) to be smaller than (UB2) and (UB2\u2019) respectively, thus providing tighter upper bounds on the variance. For this reason, if \u03c3 is relatively small, because L2s2\u03be is significantly smaller than L2m2\u03be, we expect ClusterSVRG to have a much smaller variance than SVRG in the first a few epochs, thus outperform SVRG. Such outperforance is indeed observed in our experiment in Section 5. Interpolation Between SVRG and SAGA. When every index i is itself a cluster Si = {i}, and if we choose m =\u221e so there is only one epoch, ClusterSVRG with Option I reduces to SAGA, because \u2207fj(x\u0303) + \u03b6cj becomes exactly \u2207fj(lt(j)) where lt(j) is the last time index j was chosen. In this case, it is not necessary to compute snapshots anymore. Also, recall that if there is only one cluster S1 = [n], our ClusterSVRG is identical to SVRG by comparing (4.3) with (4.2).\nIn sum, SAGA and SVRG are two extreme variants of ClusterSVRG. Our experiments in Section 5 confirm that SVRG outperforms SAGA when the data points are well clustered and vice versa."}, {"heading": "5 Experiments", "text": "We conduct experiments for three datasets that can be found on the LibSVM website [8]: Covtype.binary, SensIT (combined scale), and News20.binary. To make easier comparison across datasets, we scale every vector by the average Euclidean norm of all the vectors, ensuring that the data vectors have an average Euclidean norm 1. This step is for comparison only and not necessary in practice. Note that Covtype and SensIT are two datasets where the feature vectors have a nice clustering structure; in contrast, dataset News20 cannot be well clustered and we include it for comparison purpose."}, {"heading": "5.1 Clustering and Haar Transformation", "text": "We use the existing approximate nearest neighbor algorithm LSH [2] to compute raw clusterings, and include our implementation in Section 5.1.1.\nThe running time needed for our raw clustering algorithm is reasonable. (In Table 1, we list the average running time of clustering with \u03b4 = 0.05, 0.10, 0.15, \u00b7 \u00b7 \u00b7 , 2.00, as well as the one-pass running time of SAGA using sparse implementation.) To sum up, the clustering time is about the same as SAGA performing 5 passes on the datasets for Covtype and SensIT, and 38 passes for News20. Clustering on News20 is slower because it does not have a nice clustering structure so our implementation Algorithm 3 keeps inserting new clusters.8\nWe view the time needed for clustering as negligible: in real-life applications, one only needs to compute clustering once, and this additional cost is usually amortized over multiple runs of the training algorithm due to different data analysis tasks, parameter tunings, etc. Even if there were only one run of the training algorithm, our new algorithms are usually faster than known ones by more than 10 passes of the dataset (see Figure 1 later), a speed-up that is faster than the clustering time alone!\nAlso, in ClusterACDM, we need to preprocess and compute the matrix AHTcl using Haar transformation. This matrix multiplication can be efficiently implemented thanks to the sparsity of Haar matrices. In Table 2, we see that the time needed to do so roughly 2 passes of the dataset for the 9 test cases we are considering in the subsequent subsections."}, {"heading": "5.1.1 Our Particular Clustering Algorithm", "text": "Let FindNearestNeighbor(v,DB,R) be an oracle that outputs, with high probability, a close neighbor of v with distance at most R in the set DB. Then, given n data vectors a1, . . . , an, we iteratively call FindNearestNeighbor(ai, DB,R) for each i = 1, 2, . . . , n. If FindNearestNeighbor outputs a neighbor, we add ai into the corresponding cluster of that neighbor; otherwise we create a new cluster that only contains ai. See Algorithm 3.\nThere are many fast nearest neighbor algorithms, such as LSH [2] and product quantization [10, 12]. In our experiments, we use E2LSH package [2], with small modification to make it support sparse vectors. There are three parameters that we need to set for E2LSH (for other parameters we set to the default). The first parameter is the radius R. R controls the size of a cluster and\n8Recall that we anyways include News20 in our experiment only for comparison purpose.\nAlgorithm 3 Compute a Raw Clustering Input: n data samples a1, \u00b7 \u00b7 \u00b7 , an. 1: DB \u2190 {}. 2: for i\u2190 1 to n do 3: aj \u2190 FindNearestNeighbor(ai, DB,R) 4: if aj exists then 5: add ai into the cluster of aj 6: else 7: add ai into DB as a new cluster 8: end if 9: end for\nthere will be fewer clusters when R gets larger. In our experiment, we enumerate R to get a series of different clusterings to test the performance of our algorithms. The second and third parameters are K, the size of each hash function and L, the number of hash functions. These two parameters control the accuracy of the oracle. In [3], it is described how to select good K and L to make the program run faster. Since we only need raw clustering in this paper, it is not a big deal if we miss some true neighbors. We simply set K = 12 and L = 30 for all the datasets.\nRemarks. One can of course get faster clustering time using algorithms other than LSH, or using pre-existing clustering information. This is out of the scope of the present paper. Also, in Algorithm 3, we have ensured that there is no outlier in the raw clustering we found. However, both ClusterSVRG and ClusterACDM are robust against a few outliers, see Section 2."}, {"heading": "5.2 Performance Comparison", "text": "We compare our new algorithms with SVRG, SAGA, and ACDM. We use the default setting m = 2n with Option I for SVRG, and m = 2n with Option I for ClusterSVRG. We consider both ridge regression and Lasso regression, and denote by \u03bb > 0 the weight of the `2 regularizer for ridge regression or that of the `1 regularizer for Lasso. Prameters. For both SVRG and SAGA, we tune the best step size for each test case.9 To make our comparison even stronger, instead of tuning the best step size for ClusterSVRG, we simply set it to be either the best of SVRG or the best of SAGA in each test case. For ACDM and ClusterACDM, the step size is computed automatically so tuning is unnecessary.\nFor the Lasso problem, because the objective is not strongly convex, one has to add a dummy `2 regularizer on the objective in order to run ACDM or ClusterACDM. (In fact, this step is needed for every accelerated method including AccSDCA, APCG or SPDC.) We simply choose this dummy regularizer to have weight 10\u22127 for Covtype and SenseIT, and weight 10\u22126 for News20.\nLegend Format. In the legend, we use the format\n\u2022 \u201cClusterSVRG\u2013s\u2013\u03b4\u2013stepsize\u201d for ClusterSVRG,\n\u2022 \u201cClusterACDM\u2013s\u2013\u03b4\u201d for ClusterACDM.\n\u2022 \u201cSVRG/SAGA\u2013stepsize\u201d for SVRG or SAGA.\n\u2022 \u201cACDM (no Cluster)\u201d for the vanilla ACDM without using any clustering info.10\nResults. Our comprehensive experimental plots are included only in the appendix, and here in the main body of the paper, we simply compare all the algorithms on training ride regression for datasets SensIT and Covtype by choosing only one representative clustering, see Figure 1.\nGenerally, ClusterSVRG outperforms SAGA/SVRG when the regularizing parameter \u03bb is large.11\nClusterACDM outperforms all other algorithms when the regularizing parameter \u03bb is small. This is because accelerated methods outperform non-accelerated ones with smaller values of \u03bb (see (1.1)), and the complexity of ClusterACDM outperforms ACDM more when \u03bb is smaller (compare (1.1) with (1.2)).12\nIn the appendix, we pick three most representative (s, \u03b4)-raw clusterings and perform both Lasso and ridge regression for each dataset. We compare ClusterSVRG with SVRG and SAGA on datasets Covtype (see Figure 3), SensIT (see Figure 5), and News20 (see Figure 7). We compare ClusterACDM with SVRG, SAGA and ACDM on datasets Covtype (see Figure 4), SensIT (see Figure 6), and News20 (see Figure 8). We summarize some of the findings that can be observed from these plots as follows.\nFirstly, dataset News20 does not have a nice clustering structure and therefore ClusterSVRG and ClusterACDM do not outperform their classical counterparts SVRG and ACDM respectively.\n9We tune the best step size in the set {3\u00d7 10k, 10k : k \u2208 Z} for each dataset, for each training problem, and for each \u03bb.\n10ACDM has the same worst-case performance compared with APCG or SPDC (see [1]). Moreover, ACDM supports the non-uniform sampling that is needed in order to obtain the result of ClusterACDM. In our experiments, to make a fair comparison, instead of comparing ClusterACDM with APCG or SPDC, we simply compare it with ACDM that is implemented in the same manner as ClusterACDM.\n11This is because for a better regularized objective, the updates between consecutive iterates are smaller. Therefore, the \u03be parameter in Assumption 4.1 is smaller, and (UB1) dominates (UB2) for a longer time in Lemma 4.3.\n12Of course, the best choice of \u03bb usually depends on the dataset, and this requires cross-validation. For instance, by performing a 10-fold cross validation on SensIT and Covtype, one can figure out that the best choice of \u03bb is around 10\u22126 for SensIT Ridge, 10\u22125 for SensIT Lasso, 10\u22127 for Covtype Ridge, and 10\u22126 for Covtype Lasso. Therefore, at least for these two data sets, ClusterACDM is preferred.\nHowever, our methods still perform comparably well in such cases. Secondly, the performance of ClusterSVRG is slightly better with clustering that has smaller diameter \u03b4. In contrast, ClusterACDM with larger \u03b4 performs slightly better. This is because ClusterACDM can take advantage of very large but low-quality clusters, and this is a very appealing feature in practice. Thirdly, SensIT is a dataset where all feature vectors are close in space. In this case, SVRG performs very well because it implicitly treats all the vectors as a single cluster, as discussed in the introduction."}, {"heading": "5.3 Sensitivity on the Clustering Information", "text": "In Figure 2, we plot the performance curves of ClusterSVRG and ClusterACDM for SensIT and Covtype, with 7 different clusterings.\nFrom the plots we claim that ClusterSVRG and ClusterACDM are very insensitive to the clustering quality. As long as one does not choose the most extreme clustering, the performance improvement due to clustering can be significant. Moreover, ClusterSVRG is slightly faster if the clustering has relatively smaller diameter \u03b4 (say, below 0.1), while the ClusterACDM can be fast even for very large \u03b4 (say, around 0.6)."}, {"heading": "Acknowledgement", "text": "We thank Jinyang Gao for fruitful discussions."}, {"heading": "A Missing Experimental Figures", "text": ""}, {"heading": "B Proof of Lemma 4.3", "text": "First half of Lemma 4.3. We first prove inequality (UB1), the first half of Lemma 4.3. Suppose that i \u2208 Sc so the random index i belongs to the c-th cluster. In this case, we can rewrite\n\u2207\u0303t\u22121 \u2212\u2207f(xt\u22121)\n= 1\nn\nn\u2211\nj=1\n( \u2207fj(x\u0303) + \u03b6cj ) +\u2207fi(xt\u22121)\u2212\u2207fi(x\u0303)\u2212 \u03b6ci \u2212\u2207f(xt\u22121)\n= 1\nn\n\u2211\nj\u2208[n]\\Sc\n( \u2207fj(x\u0303) + \u03b6cj \u2212\u2207fj(xt\u22121) )\n+ 1\nn\n\u2211\nj\u2208Sc\n( \u2207fj(x\u0303) + \u03b6cj \u2212\u2207fj(xt\u22121) ) +\u2207fi(xt\u22121)\u2212\u2207fi(x\u0303)\u2212 \u03b6ci\n= 1\nn\n\u2211\nc\u2032 6=c\n\u2211\nj\u2208Sc\u2032\n( \u2207fj(x\u0303) + \u03b6c\u2032 \u2212\u2207fj(xt\u22121) ) + n\u2212 nc n ( \u2207fi(x\u0303)\u2212 \u03b6c \u2212 fi(xt\u22121) )\n+ 1\nn\n\u2211\nj\u2208Sc\n( \u2207fj(x\u0303)\u2212\u2207fi(x\u0303) ) + 1\nn\n\u2211\nj\u2208Sc\n( \u2207fi(xt\u22121)\u2212\u2207fj(xt\u22121) ) .\nTherefore, conditioning on that i belongs to Sc, we can upper bound the variance as follows:\nE [\u2225\u2225\u2225\u2207\u0303t\u22121 \u2212\u2207f(xt\u22121) \u2225\u2225\u2225 2 \u2223\u2223\u2223\u2223i \u2208 Sc ]\n\u2264 4E [\u2225\u2225\u2225 1 n \u2211\nc\u2032 6=c\n\u2211\nj\u2208Sc\u2032\n( \u2207fj(x\u0303) + \u03b6c\u2032 \u2212\u2207fj(xt\u22121) )\u2225\u2225\u2225 2 ]\n\ufe38 \ufe37\ufe37 \ufe38 (\u2660)\n+4E [\u2225\u2225\u2225n\u2212 nc\nn\n( \u2207fi(x\u0303)\u2212 \u03b6c \u2212 fi(xt\u22121) )\u2225\u2225\u2225 2 \u2223\u2223\u2223\u2223i \u2208 Sc ]\n\ufe38 \ufe37\ufe37 \ufe38 (\u2665)\n+ 4E [\u2225\u2225\u2225 1 n \u2211\nj\u2208Sc\n( \u2207fj(x\u0303)\u2212\u2207fi(x\u0303) )\u2225\u2225\u2225 2 \u2223\u2223\u2223\u2223i \u2208 Sc ]\n\ufe38 \ufe37\ufe37 \ufe38 (\u2663)\n+4E [\u2225\u2225\u2225 1 n \u2211\nj\u2208Sc\n( \u2207fi(xt\u22121)\u2212\u2207fj(xt\u22121) )\u2225\u2225\u2225 2 \u2223\u2223\u2223\u2223i \u2208 Sc ]\n\ufe38 \ufe37\ufe37 \ufe38 (\u2666)\n.\n(B.1)\nAbove, the inequality is because for every q vectors v1, . . . , vq, it satisfies \u2225\u2225\u2211q\nk=1 vk \u2225\u22252 \u2264 q\u2211qk=1 \u2016vk\u20162 . (B.2)\nNow, the definition of \u03c3 raw clustering on functions immediately imply that (\u2663) = E [\u2225\u2225\u2225 1 n \u2211\nj\u2208Sc\n( \u2207fj(x\u0303)\u2212\u2207fi(x\u0303) )\u2225\u2225\u2225 2 \u2223\u2223\u2223\u2223i \u2208 Sc ] \u2264 n 2 c\nn2 Ei,j\u2208RSc\n[ \u2016\u2207fi(x\u0303)\u2212\u2207fj(x\u0303)\u20162 ] \u2264 n 2 c\nn2 \u03c3 .\nAbove, the first inequality follows from (B.2). Similarly, we also have\n(\u2666) = E [\u2225\u2225\u2225 1 n \u2211\nj\u2208Sc\n( \u2207fi(xt\u22121)\u2212\u2207fj(xt\u22121) )\u2225\u2225\u2225 2 \u2223\u2223\u2223\u2223i \u2208 Sc ] \u2264 n 2 c\nn2 \u03c3 .\nTo upper bound the other two terms in (B.1), we first compute that\n(\u2665) = (n\u2212 nc) 2\nn2 Ei,k\u2208RSc\n[\u2225\u2225(\u2207fi(x\u0303)\u2212\u2207fk(x\u0303) ) \u2212 ( \u2207fi(xt\u22121)\u2212\u2207fk(x) )\u2225\u22252 ]\n\u2264 2(n\u2212 nc) 2\nn2\n( Ei,k\u2208RSc [\u2225\u2225\u2207fi(x\u0303)\u2212\u2207fk(x\u0303) \u2225\u22252 ] + Ei,k\u2208RSc [\u2225\u2225\u2207fi(xt\u22121)\u2212\u2207fk(x) \u2225\u22252 ])\n\u2264 O ( \u03c3 + L2s2\u03be ) .\nHere, the last inequality uses Claim 4.2. Similarly, we can compute that\n(\u2660) \u00ac \u2264 n\u2212 nc n2 E [\u2211\nc\u2032 6=c\n\u2211\nj\u2208Sc\u2032\n\u2225\u2225\u2225\u2207fj(x\u0303) + \u03b6c\u2032 \u2212\u2207fj(xt\u22121) \u2225\u2225\u2225 2 ]\n= n\u2212 nc n2\n\u2211 c\u2032 6=c nc\u2032 \u00b7 Ej,k\u2208RSc\u2032 [\u2225\u2225\u2225 ( \u2207fj(x\u0303)\u2212\u2207fk(x\u0303) ) \u2212 ( \u2207fj(xt\u22121)\u2212\u2207fk(x) )\u2225\u2225\u2225 2 ]\n\u2264 2(n\u2212 nc) n2\n\u2211 c\u2032 6=c nc\u2032 \u00b7 Ej,k\u2208RSc\u2032 [\u2225\u2225\u2207fj(x\u0303)\u2212\u2207fk(x\u0303) )\u2225\u22252 + \u2225\u2225\u2207fj(xt\u22121)\u2212\u2207fk(x) \u2225\u22252 ]\n \u2264 O ( \u03c3 + L2s2\u03be ) .\nAbove, \u00ac uses inequality (B.2), and  uses Claim 4.2. Substituting all of the four upper bound back to inequality (B.1), we immediately have that\nE [\u2225\u2225\u2225\u2207\u0303t\u22121 \u2212\u2207f(xt\u22121) \u2225\u2225\u2225 2 \u2223\u2223\u2223\u2223i \u2208 Sc ] \u2264 O ( (\u2660) + (\u2665) + (\u2663) + (\u2666) ) = O ( \u03c3 + 2L2s2\u03be ) .\nTaking expectation over all possible c = 1, 2, . . . , s, we get our desired upper bound.\nSecond half of Lemma 4.3. We now prove inequality (UB2), the second half of Lemma 4.3. This time, we rewrite\n\u2207\u0303t\u22121 \u2212\u2207f(xt\u22121)\n= 1\nn\nn\u2211\nj=1\n( \u2207fj(x\u0303) + \u03b6cj ) +\u2207fi(xt\u22121)\u2212\u2207fi(x\u0303)\u2212 \u03b6ci \u2212\u2207f(xt\u22121)\n= ( f(x\u0303)\u2212 f(xt\u22121) ) + ( \u2207fi(xt\u22121)\u2212\u2207fi(x\u0303) ) \u2212 \u03b6ci + 1\nn\ns\u2211\nc\u2032=1\nnc\u2032\u03b6c\u2032 ,\nand therefore\nE [\u2225\u2225\u2207\u0303t\u22121 \u2212\u2207f(xt\u22121) \u2225\u22252 ]\n\u2264 4E [\u2225\u2225\u2207f(x\u0303)\u2212\u2207f(xt\u22121) \u2225\u22252 + \u2225\u2225\u2207fi(xt\u22121)\u2212\u2207fi(x\u0303) \u2225\u22252 + \u2016\u03b6ci\u20162 + \u2225\u2225\u2225 1 n s\u2211\nc\u2032=1\nnc\u2032\u03b6c\u2032 \u2225\u2225\u2225 2] . (B.3)\nWe next upper bound the four terms on the right hand side of (B.3). The first two terms enjoy the following simple upper bounds due to the SVRG paper [13]:\n\u2225\u2225\u2207f(x\u0303)\u2212\u2207f(xt\u22121) \u2225\u22252 \u00ac\u2264 2 \u2225\u2225\u2207f(x\u0303)\u2212\u2207f(x\u2217) \u2225\u22252 + 2 \u2225\u2225\u2207f(xt\u22121)\u2212\u2207f(x\u2217) \u2225\u22252\n \u2264 4L ( f(x\u0303)\u2212 f(x\u2217) ) + 4L ( f(xt\u22121)\u2212 f(x\u2217) ) .\nE [\u2225\u2225\u2207fi(xt\u22121)\u2212\u2207fi(x\u0303) \u2225\u22252]\n\u00ae \u2264 2E [\u2225\u2225\u2207fi(xt\u22121)\u2212\u2207fi(x\u2217) \u2225\u22252]+ 2E [\u2225\u2225\u2207fi(x\u0303)\u2212\u2207fi(x\u2217) \u2225\u22252]\n\u00af \u2264 2E [ fi(x t\u22121)\u2212 fi(x\u2217)\u2212 \u3008fi(x\u2217), xt\u22121 \u2212 x\u2217\u3009 ] + 2E [ fi(x\u0303)\u2212 fi(x\u2217)\u2212 \u3008fi(x\u2217), x\u0303\u2212 x\u2217\u3009 \u2225\u22252] = 4L \u00b7 E [( f(xt\u22121)\u2212 f(x\u2217) ) + ( f(x\u0303)\u2212 f(x\u2217) )] .\nAbove, \u00ac and \u00ae follow from (B.2), while  and \u00af follow from a classical inequality for smooth functions.13\nIn fact, the other two terms can be bounded using similar ideas from the ones above. For instance,\nE [\u2225\u2225\u03b6ci \u2225\u22252] = E [ \u2016\u2207fk(x)\u2212\u2207fk(x\u0303)\u20162 \u2223\u2223 k is randomly chosen in [n] ]\n\u2264 2E [\u2225\u2225\u2207fk(x)\u2212\u2207fk(x\u2217) \u2225\u22252]+ 2E [\u2225\u2225\u2207fk(x\u0303)\u2212\u2207fk(x\u2217) \u2225\u22252] \u2264 2E [ fk(x)\u2212 fk(x\u2217)\u2212 \u3008fk(x\u2217), x\u2212 x\u2217\u3009 ] + 2E [ fk(x\u0303)\u2212 fk(x\u2217)\u2212 \u3008fk(x\u2217), x\u0303\u2212 x\u2217\u3009 \u2225\u22252] = 4L \u00b7 E [( f(x)\u2212 f(x\u2217) ) + ( f(x\u0303)\u2212 f(x\u2217) )] .\nOne can similarly prove that\nE [\u2225\u2225\u2225 1 n s\u2211\nc\u2032=1\nnc\u2032\u03b6c\u2032 \u2225\u2225\u2225 2] \u2264 O(L) \u00b7 E [( f(x)\u2212 f(x\u2217) ) + ( f(x\u0303)\u2212 f(x\u2217) )] .\nSubstituting these upper bounds back to (B.3) concludes the proof. 13See for instance in Theorem 2.1.5 of the textbook [19]: if g is convex and L-smooth, with minimizer x\u2217, then for every x, y we we have \u2016\u2207g(x)\u2212\u2207g(y)\u20162 \u2264 2L(g(x)\u2212 g(y)\u2212 \u3008\u2207g(y), x\u2212 y\u3009)."}], "references": [{"title": "Even faster accelerated coordinate descent using non-uniform sampling", "author": ["Zeyuan Allen-Zhu", "Zheng Qu", "Peter Richt\u00e1rik", "Yang Yuan"], "venue": "ArXiv e-prints,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Nearest Neighbor Search: the Old, the New, and the Impossible", "author": ["Alexandr Andoni"], "venue": "PhD thesis,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Stochastic gradient descent", "author": ["L\u00e9on Bottou"], "venue": "http://leon.bottou.org/projects/sgd,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems", "author": ["Aaron J. Defazio", "Tib\u00e9rio S. Caetano", "Justin Domke"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "LIBSVM Data: Classification, Regression and Multi-label", "author": ["Rong-En Fan", "Chih-Jen Lin"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M. Kakade", "Aaron Sidford"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Optimized product quantization", "author": ["Tiezheng Ge", "Kaiming He", "Qifa Ke", "Jian Sun"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Variance reduced stochastic gradient descent with neighbors", "author": ["Thomas Hofmann", "Aurelien Lucchi", "Simon Lacoste-Julien", "Brian McWilliams"], "venue": "NIPS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Product quantization for nearest neighbor search", "author": ["Herv\u00e9 J\u00e9gou", "Matthijs Douze", "Cordelia Schmid"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Semi-Stochastic Gradient Descent Methods", "author": ["J. Kone\u010dn\u00fd", "P. Richt\u00e1rik"], "venue": "ArXiv e-prints,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems", "author": ["Yin Tat Lee", "Aaron Sidford"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A Universal Catalyst for First-Order Optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "An Accelerated Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization", "author": ["Qihang Lin", "Zhaosong Lu", "Lin Xiao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning", "author": ["Julien Mairal"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["Sashank J. Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnab\u00e1s P\u00f3czos", "Alexander J. Smola"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Proximal Stochastic Dual Coordinate Ascent", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "arXiv preprint arXiv:1211.2717,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Accelerated Mini-Batch Stochastic Dual Coordinate Ascent", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "A Proximal Stochastic Gradient Method with Progressive Variance Reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "The original stochastic gradient descent (SGD) methods [4, 27] simply defines the estimator using one random data example but converges slowly.", "startOffset": 55, "endOffset": 62}, {"referenceID": 25, "context": "The original stochastic gradient descent (SGD) methods [4, 27] simply defines the estimator using one random data example but converges slowly.", "startOffset": 55, "endOffset": 62}, {"referenceID": 4, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 100, "endOffset": 130}, {"referenceID": 5, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 100, "endOffset": 130}, {"referenceID": 11, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 100, "endOffset": 130}, {"referenceID": 16, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 100, "endOffset": 130}, {"referenceID": 19, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 100, "endOffset": 130}, {"referenceID": 20, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 100, "endOffset": 130}, {"referenceID": 22, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 100, "endOffset": 130}, {"referenceID": 24, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 100, "endOffset": 130}, {"referenceID": 7, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 283, "endOffset": 302}, {"referenceID": 14, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 283, "endOffset": 302}, {"referenceID": 15, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 283, "endOffset": 302}, {"referenceID": 23, "context": "Recently, variance-reduction methods were introduced, which greatly improve the running time of SGD [6, 7, 13, 18, 21, 22, 24, 26], and accelerated gradient methods were also introduced to further improve the running time when the strong convexity of the objective function is small [9, 16, 17, 25, 28].", "startOffset": 283, "endOffset": 302}, {"referenceID": 6, "context": "As another example, although there are 581,012 data vectors in the famous Covtype dataset [8], each representing a 30m x 30m cell in the Roosevelt National Forest of northern Colorado, these feature vectors (after normalization) can be easily categorized into 1,445 clusters of diameter 0.", "startOffset": 90, "endOffset": 93}, {"referenceID": 9, "context": "[11], they introduced N-SAGA, the first ERM training algorithm that takes into account the similarities between data vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "\u2022 It can be computed very efficiently using approximate nearest neighbor algorithms such as LSH [2] and product quantization [10, 12], see Section 5.", "startOffset": 125, "endOffset": 133}, {"referenceID": 10, "context": "\u2022 It can be computed very efficiently using approximate nearest neighbor algorithms such as LSH [2] and product quantization [10, 12], see Section 5.", "startOffset": 125, "endOffset": 133}, {"referenceID": 11, "context": "We propose two unbiased algorithms, which we call ClusterSVRG and ClusterACDM, that make novel use of clustering information to reduce the running time of SVRG [13] and ACDM [1, 15] respectively.", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "We propose two unbiased algorithms, which we call ClusterSVRG and ClusterACDM, that make novel use of clustering information to reduce the running time of SVRG [13] and ACDM [1, 15] respectively.", "startOffset": 174, "endOffset": 181}, {"referenceID": 13, "context": "We propose two unbiased algorithms, which we call ClusterSVRG and ClusterACDM, that make novel use of clustering information to reduce the running time of SVRG [13] and ACDM [1, 15] respectively.", "startOffset": 174, "endOffset": 181}, {"referenceID": 11, "context": "Our ClusterSVRG method is a nonaccelerated stochastic gradient method just like SVRG [13], SAGA [6], SDCA [24], etc.", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "Our ClusterSVRG method is a nonaccelerated stochastic gradient method just like SVRG [13], SAGA [6], SDCA [24], etc.", "startOffset": 96, "endOffset": 99}, {"referenceID": 22, "context": "Our ClusterSVRG method is a nonaccelerated stochastic gradient method just like SVRG [13], SAGA [6], SDCA [24], etc.", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "Our ClusterACDM method is an accelerated stochastic gradient method just like AccSDCA [25], APCG [17], ACDM [1, 15], SPDC [28], etc.", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "Our ClusterACDM method is an accelerated stochastic gradient method just like AccSDCA [25], APCG [17], ACDM [1, 15], SPDC [28], etc.", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "Our ClusterACDM method is an accelerated stochastic gradient method just like AccSDCA [25], APCG [17], ACDM [1, 15], SPDC [28], etc.", "startOffset": 108, "endOffset": 115}, {"referenceID": 13, "context": "Our ClusterACDM method is an accelerated stochastic gradient method just like AccSDCA [25], APCG [17], ACDM [1, 15], SPDC [28], etc.", "startOffset": 108, "endOffset": 115}, {"referenceID": 12, "context": "Recall that both non-accelerated and accelerated gradient methods have their mini-batch variants, such as mini-batch SVRG [14], mini-batch AccSDCA [23], and mini-batch SPDC [28].", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "Recall that both non-accelerated and accelerated gradient methods have their mini-batch variants, such as mini-batch SVRG [14], mini-batch AccSDCA [23], and mini-batch SPDC [28].", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "[20] also proposes a framework unifying SVRG and SAGA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Researchers also design accelerated stochastic methods via a black-box reduction to non-accelerated ones [9, 16].", "startOffset": 105, "endOffset": 112}, {"referenceID": 14, "context": "Researchers also design accelerated stochastic methods via a black-box reduction to non-accelerated ones [9, 16].", "startOffset": 105, "endOffset": 112}, {"referenceID": 11, "context": "While our accelerated method ClusterACDM works only for regularized least-square problems in theory, our non-accelerated method ClusterSVRG, like its parent method SVRG [13], focuses on a more general stochastic setting, consisting of a set of n convex functions {f1(x), .", "startOffset": 169, "endOffset": 173}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": ", Ln), the non-uniform accelerated coordinate descent method of [1] produces an output y satisfying f(y)\u2212minx f(x) \u2264 \u03b5 in", "startOffset": 64, "endOffset": 67}, {"referenceID": 15, "context": "We remark here that accelerated coordinate descent method admits several variants such as APCG [17], ACDM [15], and NU ACDM [1].", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "We remark here that accelerated coordinate descent method admits several variants such as APCG [17], ACDM [15], and NU ACDM [1].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "We remark here that accelerated coordinate descent method admits several variants such as APCG [17], ACDM [15], and NU ACDM [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 15, "context": "1) (see for instance [17] for the detailed proof):", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "The following lemma is due to [17] but is anyways proved for completeness\u2019 sake.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "To prove the smoothness, recall that if r(\u00b7) is \u03bb strongly convex, then r\u2217(\u00b7) is 1/\u03bb smooth, or mathematically, \u22072r\u2217(x) 1 \u03bbI (see for instance the text book [5]).", "startOffset": 157, "endOffset": 160}, {"referenceID": 15, "context": "For this reason, the authors of [17] proposed to apply accelerated coordinate descent (such as their APCG method) to minimize D(y).", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "This total running time \u00d5(nd + \u221a n/\u03bbd) matches the fastest known running time (originally obtained by AccSDCA [25]) for solving (3.", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "Since the epoch length m is usually of the same magnitude as n [13], this outdatedness may be very large.", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "The proof for (UB2\u2019) can be found on page 6 of [13].", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "For instance, if only (UB2\u2019) is used in the analysis, Johnson and Zhang [13] showed that for \u03c3-strongly convex objectives F (x) in (4.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "For ClusterSVRG, using (UB2) and an analogous analysis to [13], it matches the worst case running time of SVRG.", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "We conduct experiments for three datasets that can be found on the LibSVM website [8]: Covtype.", "startOffset": 82, "endOffset": 85}, {"referenceID": 8, "context": "There are many fast nearest neighbor algorithms, such as LSH [2] and product quantization [10, 12].", "startOffset": 90, "endOffset": 98}, {"referenceID": 10, "context": "There are many fast nearest neighbor algorithms, such as LSH [2] and product quantization [10, 12].", "startOffset": 90, "endOffset": 98}, {"referenceID": 1, "context": "In [3], it is described how to select good K and L to make the program run faster.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "ACDM has the same worst-case performance compared with APCG or SPDC (see [1]).", "startOffset": 73, "endOffset": 76}], "year": 2017, "abstractText": "The amount of data available in the world is growing faster and bigger than our ability to deal with it. However, if we take advantage of the internal structure, data may become much smaller for machine learning purposes. In this paper we focus on one of the most fundamental machine learning tasks, empirical risk minimization (ERM), and provide faster algorithms with the help from the clustering structure of the data. We introduce a simple notion of raw clustering that can be efficiently obtained with just one pass of the data, and propose two algorithms. Our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using the clustering information, and our accelerated algorithm ClusterACDM is built on a novel Haar transformation applied to the dual space of each cluster. Our algorithms outperform their classical counterparts both in theory and practice.", "creator": "LaTeX with hyperref package"}}}