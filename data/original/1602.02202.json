{"id": "1602.02202", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Efficient Second Order Online Learning by Sketching", "abstract": "We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a linear running time. We further improve the computational complexity to linear in the number of nonzero entries by creating sparse forms of the sketching methods (such as Oja's rule) for top eigenvector extraction. Together, these algorithms eliminate all computational obstacles in previous second order online learning approaches.", "histories": [["v1", "Sat, 6 Feb 2016 02:33:53 GMT  (51kb,D)", "https://arxiv.org/abs/1602.02202v1", null], ["v2", "Sat, 21 May 2016 15:10:31 GMT  (65kb,D)", "http://arxiv.org/abs/1602.02202v2", null], ["v3", "Tue, 4 Oct 2016 15:22:07 GMT  (85kb,D)", "http://arxiv.org/abs/1602.02202v3", "NIPS2016 camera-ready version"], ["v4", "Tue, 17 Oct 2017 05:01:59 GMT  (85kb,D)", "http://arxiv.org/abs/1602.02202v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haipeng luo", "alekh agarwal", "nicol\u00f2 cesa-bianchi", "john langford"], "accepted": true, "id": "1602.02202"}, "pdf": {"name": "1602.02202.pdf", "metadata": {"source": "CRF", "title": "Efficient Second Order Online Learning by Sketching", "authors": ["Haipeng Luo", "Alekh Agarwal"], "emails": ["haipengl@cs.princeton.edu", "alekha@microsoft.com", "nicolo.cesa-bianchi@unimi.it", "jcl@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Online learning methods are highly successful at rapidly reducing the test error on large, highdimensional datasets. First order methods are particularly attractive in such problems as they typically enjoy computational complexity linear in the input size. However, the convergence of these methods crucially depends on the geometry of the data; for instance, running the same algorithm on a rotated set of examples can return vastly inferior results. See Fig. 1 for an illustration.\nSecond order algorithms such as Online Newton Step [17] have the attractive property of being invariant to linear transformations of the data, but typically require space and update time quadratic in the number of dimensions. Furthermore, the dependence on dimension is not improved even if the examples are sparse. These issues lead to the key question in our work: Can we develop (approximately) second order online learning algorithms with efficient updates? We show that the answer is \u201cyes\u201d by developing efficient sketched second order methods with regret guarantees. Specifically, the three main contributions of this work are:\n1. Invariant learning setting and optimal algorithms (Section 2). The typical online regret minimization setting evaluates against a benchmark that is bounded in some fixed norm (such as the `2-norm), implicitly putting the problem in a nice geometry. However, if all the features are scaled down, it is desirable to compare with accordingly larger weights, which is precluded by an apriori fixed norm bound. We study an invariant learning setting similar to the paper [30] which compares the learner to a benchmark only constrained to generate bounded predictions on the sequence of examples. We show that a variant of the Online Newton Step [17], while quadratic in computation, stays regret-optimal with a nearly matching lower bound in this more general setting.\n2. Improved efficiency via sketching (Section 3). To overcome the quadratic running time, we next develop sketched variants of the Newton update, approximating the second order information using a small number of carefully chosen directions, called a sketch. While the idea of data sketching is widely studied [33], as far as we know our work is the first one to apply it to a general adversarial\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 2.\n02 20\n2v 4\n[ cs\n.L G\n] 1\n7 O\nonline learning setting and provide rigorous regret guarantees. Two different sketching methods are considered: Frequent Directions [11, 20] and Oja\u2019s algorithm [25, 26], both of which allow linear running time per round. For the first method, we prove regret bounds similar to the full second order update whenever the sketch-size is large enough. Our analysis makes it easy to plug in other sketching and online PCA methods (e.g. [10]).\n3. Sparse updates (Section 4). For practical implementation, we further develop sparse versions of these updates with a running time linear in the sparsity of the examples. The main challenge here is that even if examples are sparse, the sketch matrix still quickly becomes dense. These are the first known sparse implementations of the Frequent Directions1 and Oja\u2019s algorithm, and require new sparse eigen computation routines that may be of independent interest.\nEmpirically, we evaluate our algorithm using the sparse Oja sketch (called Oja-SON) against first order methods such as diagonalized ADAGRAD [5, 22] on both ill-conditioned synthetic and a suite of real-world datasets. As Fig. 1 shows for a synthetic problem, we observe substantial performance gains as data conditioning worsens. On the real-world datasets, we find\nimprovements in some instances, while observing no substantial second-order signal in the others.\nRelated work Our online learning setting is closest to the one proposed in [30], which studies scale-invariant algorithms, a special case of the invariance property considered here (see also [28, Section 5]). Computational efficiency, a main concern in this work, is not a problem there since each coordinate is scaled independently. Orabona and P\u00e1l [27] study unrelated notions of invariance. Gao et al. [8] study a specific randomized sketching method for a special online learning setting.\nThe L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically. Recent works [6, 14, 13, 29] employ sketching in stochastic optimization, but do not provide sparse implementations or extend in an obvious manner to the online setting. The FrankWolfe algorithm [7, 18] is also invariant to linear transformations, but with worse regret bounds [16] without further assumptions and modifications [9].\nNotation Vectors are represented by bold letters (e.g., x, w, . . . ) and matrices by capital letters (e.g., M , A, . . . ). Mi,j denotes the (i, j) entry of matrix M . Id represents the d\u00d7 d identity matrix, 0m\u00d7d represents the m\u00d7 d matrix of zeroes, and diag{x} represents a diagonal matrix with x on the diagonal. \u03bbi(A) denotes the i-th largest eigenvalue of A, \u2016w\u2016A denotes \u221a w>Aw, |A| is the\ndeterminant of A, TR(A) is the trace of A, \u3008A,B\u3009 denotes \u2211 i,j Ai,jBi,j , and A B means that B \u2212A is positive semidefinite. The sign function SGN(a) is 1 if a \u2265 0 and \u22121 otherwise."}, {"heading": "2 Setup and an Optimal Algorithm", "text": "We consider the following setting. On each round t = 1, 2 . . . , T : (1) the adversary first presents an example xt \u2208 Rd, (2) the learner chooseswt \u2208 Rd and predictsw>t xt, (3) the adversary reveals a loss function ft(w) = `t(w>xt) for some convex, differentiable `t : R\u2192 R+, and (4) the learner suffers loss ft(wt) for this round.\nThe learner\u2019s regret to a comparatorw is defined asRT (w) = \u2211T t=1 ft(wt)\u2212 \u2211T t=1 ft(w). Typical results study RT (w) against all w with a bounded norm in some geometry. For an invariant update, 1Recent work by [12] also studies sparse updates for a more complicated variant of Frequent Directions which is randomized and incurs extra approximation error. 2Stochastic setting assumes that the examples are drawn i.i.d. from a distribution.\nwe relax this requirement and only put bounds on the predictions w>xt. Specifically, for some pre-chosen constant C we define Kt def = { w : |w>xt| \u2264 C } . We seek to minimize regret to all comparators that generate bounded predictions on every data point, that is:\nRT = sup w\u2208K\nRT (w) where K def = T\u22c2 t=1 Kt = { w : \u2200t = 1, 2, . . . T, |w>xt| \u2264 C } .\nUnder this setup, if the data are transformed to Mxt for all t and some invertible matrix M \u2208 Rd\u00d7d, the optimal w\u2217 simply moves to (M\u22121)>w\u2217, which still has bounded predictions but might have significantly larger norm. This relaxation is similar to the comparator set considered in [30].\nWe make two structural assumptions on the loss functions.\nAssumption 1. (Scalar Lipschitz) The loss function `t satisfies |` \u2032\nt(z)| \u2264 L whenever |z| \u2264 C. Assumption 2. (Curvature) There exists \u03c3t \u2265 0 such that for all u,w \u2208 K, ft(w) is lower bounded by ft(u) +\u2207ft(u)>(w \u2212 u) + \u03c3t2 ( \u2207ft(u)>(u\u2212w) )2 .\nNote that when \u03c3t = 0, Assumption 2 merely imposes convexity. More generally, it is satisfied by squared loss ft(w) = (w>xt \u2212 yt)2 with \u03c3t = 18C2 whenever |w\n>xt| and |yt| are bounded by C, as well as for all exp-concave functions (see [17, Lemma 3]).\nEnlarging the comparator set might result in worse regret. We next show matching upper and lower bounds qualitatively similar to the standard setting, but with an extra unavoidable \u221a d factor. 3\nTheorem 1. For any online algorithm generatingwt \u2208 Rd and all T \u2265 d, there exists a sequence of T examples xt \u2208 Rd and loss functions `t satisfying Assumptions 1 and 2 (with \u03c3t = 0) such that the regret RT is at least CL \u221a dT/2.\nWe now give an algorithm that matches the lower bound up to logarithmic constants in the worst case but enjoys much smaller regret when \u03c3t 6= 0. At round t+ 1 with some invertible matrix At specified later and gradient gt = \u2207ft(wt), the algorithm performs the following update before making the prediction on the example xt+1:\nut+1 = wt \u2212A\u22121t gt, and wt+1 = argmin w\u2208Kt+1 \u2016w \u2212 ut+1\u2016At . (1)\nThe projection onto the set Kt+1 differs from typical norm-based projections as it only enforces boundedness on xt+1 at round t+ 1. Moreover, this projection step can be performed in closed form.\nLemma 1. For any x 6= 0,u \u2208 Rd and positive definite matrix A \u2208 Rd\u00d7d, we have\nargmin w : |w>x|\u2264C\n\u2016w \u2212 u\u2016A = u\u2212 \u03c4C(u\n>x)\nx>A\u22121x A\u22121x, where \u03c4C(y) = SGN(y) max{|y| \u2212 C, 0}.\nIf At is a diagonal matrix, updates similar to those of Ross et al. [30] are recovered. We study a choice of At that is similar to the Online Newton Step (ONS) [17] (though with different projections):\nAt = \u03b1Id + t\u2211 s=1 (\u03c3s + \u03b7s)gsg > s (2)\nfor some parameters \u03b1 > 0 and \u03b7t \u2265 0. The regret guarantee of this algorithm is shown below: Theorem 2. Under Assumptions 1 and 2, suppose that \u03c3t \u2265 \u03c3 \u2265 0 for all t, and \u03b7t is non-increasing. Then using the matrices (2) in the updates (1) yields for all w \u2208 K,\nRT (w) \u2264 \u03b1\n2 \u2016w\u201622 + 2(CL) 2 T\u2211 t=1 \u03b7t + d 2(\u03c3 + \u03b7T ) ln\n( 1 + (\u03c3 + \u03b7T ) \u2211T t=1 \u2016gt\u2016 2 2\nd\u03b1\n) .\n3In the standard setting where wt and xt are restricted such that \u2016wt\u2016 \u2264 D and \u2016xt\u2016 \u2264 X , the minimax regret is O(DXL \u221a T ). This is clearly a special case of our setting with C = DX .\nAlgorithm 1 Sketched Online Newton (SON) Input: Parameters C, \u03b1 and m.\n1: Initialize u1 = 0d\u00d71. 2: Initialize sketch (S,H)\u2190 SketchInit(\u03b1,m). 3: for t = 1 to T do 4: Receive example xt. 5: Projection step: compute x\u0302 = Sxt, \u03b3 = \u03c4C(u > t xt)\nx>t xt\u2212x\u0302>Hx\u0302 and setwt = ut \u2212 \u03b3(xt \u2212 S>Hx\u0302).\n6: Predict label yt = w>t xt and suffer loss `t(yt). 7: Compute gradient gt = ` \u2032 t(yt)xt and the to-sketch vector g\u0302 = \u221a \u03c3t + \u03b7tgt. 8: (S,H)\u2190 SketchUpdate(g\u0302). 9: Update weight: ut+1 = wt \u2212 1\u03b1 (gt \u2212 S\n>HSgt). 10: end for\nThe dependence on \u2016w\u201622 implies that the method is not completely invariant to transformations of the data. This is due to the part \u03b1Id in At. However, this is not critical since \u03b1 is fixed and small while the other part of the bound grows to eventually become the dominating term. Moreover, we can even set \u03b1 = 0 and replace the inverse with the Moore-Penrose pseudoinverse to obtain a truly invariant algorithm, as discussed in Appendix D. We use \u03b1 > 0 in the remainder for simplicity.\nThe implication of this regret bound is the following: in the worst case where \u03c3 = 0, we set \u03b7t = \u221a d/C2L2t and the bound simplifies to\nRT (w) \u2264 \u03b1\n2 \u2016w\u201622 +\nCL\n2\n\u221a Td ln ( 1 + \u2211T t=1 \u2016gt\u2016 2 2\n\u03b1CL \u221a Td\n) + 4CL \u221a Td ,\nessentially only losing a logarithmic factor compared to the lower bound in Theorem 1. On the other hand, if \u03c3t \u2265 \u03c3 > 0 for all t, then we set \u03b7t = 0 and the regret simplifies to\nRT (w) \u2264 \u03b1\n2 \u2016w\u201622 +\nd\n2\u03c3 ln\n( 1 + \u03c3 \u2211T t=1 \u2016gt\u2016 2 2\nd\u03b1\n) , (3)\nextending the O(d lnT ) results in [17] to the weaker Assumption 2 and a larger comparator set K."}, {"heading": "3 Efficiency via Sketching", "text": "Our algorithm so far requires \u2126(d2) time and space just as ONS. In this section we show how to achieve regret guarantees nearly as good as the above bounds, while keeping computation within a constant factor of first order methods. Let Gt \u2208 Rt\u00d7d be a matrix such that the t-th row is g\u0302>t where we define g\u0302t = \u221a \u03c3t + \u03b7tgt to be the to-sketch vector. Our previous choice of At (Eq. (2)) can be written as \u03b1Id +G>t Gt. The idea of sketching is to maintain an approximation of Gt, denoted by St \u2208 Rm\u00d7d where m d is a small constant called the sketch size. If m is chosen so that S>t St approximates G > t Gt well, we can redefine At as \u03b1Id + S>t St for the algorithm.\nTo see why this admits an efficient algorithm, notice that by the Woodbury formula one has A\u22121t = 1 \u03b1 ( Id \u2212 S>t (\u03b1Im + StS>t )\u22121St ) . With the notation Ht = (\u03b1Im + StS>t )\n\u22121 \u2208 Rm\u00d7m and \u03b3t = \u03c4C(u > t+1xt+1)/(x > t+1xt+1 \u2212 x>t+1S>t HtStxt+1), update (1) becomes:\nut+1 = wt \u2212 1\u03b1 ( gt \u2212 S>t HtStgt ) , and wt+1 = ut+1 \u2212 \u03b3t ( xt+1 \u2212 S>t HtStxt+1 ) .\nThe operations involving Stgt or Stxt+1 require only O(md) time, while matrix vector products with Ht require onlyO(m2). Altogether, these updates are at most m times more expensive than first order algorithms as long as St and Ht can be maintained efficiently. We call this algorithm Sketched Online Newton (SON) and summarize it in Algorithm 1.\nWe now discuss two sketching techniques to maintain the matrices St and Ht efficiently, each requiring O(md) storage and time linear in d.\nAlgorithm 2 FD-Sketch for FD-SON Internal State: S and H . SketchInit(\u03b1,m)\n1: Set S = 0m\u00d7d and H = 1\u03b1Im. 2: Return (S,H).\nSketchUpdate(g\u0302) 1: Insert g\u0302 into the last row of S. 2: Compute eigendecomposition: V >\u03a3V = S>S and set S = (\u03a3\u2212 \u03a3m,mIm) 1 2V .\n3: Set H = diag {\n1 \u03b1+\u03a31,1\u2212\u03a3m,m , \u00b7 \u00b7 \u00b7 , 1 \u03b1\n} .\n4: Return (S,H).\nAlgorithm 3 Oja\u2019s Sketch for Oja-SON Internal State: t, \u039b, V and H . SketchInit(\u03b1,m)\n1: Set t = 0,\u039b = 0m\u00d7m, H = 1\u03b1Im and V to anym\u00d7dmatrix with orthonormal rows. 2: Return (0m\u00d7d, H).\nSketchUpdate(g\u0302) 1: Update t\u2190 t+ 1, \u039b and V as Eqn. 4. 2: Set S = (t\u039b) 1 2V .\n3: Set H = diag {\n1 \u03b1+t\u039b1,1 , \u00b7 \u00b7 \u00b7 , 1\u03b1+t\u039bm,m }\n. 4: Return (S,H).\nFrequent Directions (FD). Frequent Directions sketch [11, 20] is a deterministic sketching method. It maintains the invariant that the last row of St is always 0. On each round, the vector g\u0302 > t is inserted into the last row of St\u22121, then the covariance of the resulting matrix is eigendecomposed into V >t \u03a3tVt and St is set to (\u03a3t \u2212 \u03c1tIm) 1 2Vt where \u03c1t is the smallest eigenvalue. Since the rows of St are orthogonal to each other, Ht is a diagonal matrix and can be maintained efficiently (see Algorithm 2). The sketch update works inO(md) time (see [11] and Appendix F) so the total running time is O(md) per round. We call this combination FD-SON and prove the following regret bound with notation \u2126k = \u2211d i=k+1 \u03bbi(G > TGT ) for any k = 0, . . . ,m\u2212 1. Theorem 3. Under Assumptions 1 and 2, suppose that \u03c3t \u2265 \u03c3 \u2265 0 for all t and \u03b7t is non-increasing. FD-SON ensures that for any w \u2208 K and k = 0, . . . ,m\u2212 1, we have\nRT (w) \u2264 \u03b1\n2 \u2016w\u201622 + 2(CL) 2 T\u2211 t=1 \u03b7t + m 2(\u03c3 + \u03b7T ) ln ( 1 + TR(S>T ST ) m\u03b1 ) + m\u2126k 2(m\u2212 k)(\u03c3 + \u03b7T )\u03b1 .\nThe bound depends on the spectral decay \u2126k, which essentially is the only extra term compared to the bound in Theorem 2. Similarly to previous discussion, if \u03c3t \u2265 \u03c3, we get the bound \u03b12 \u2016w\u2016 2 2 + m 2\u03c3 ln ( 1 + TR(S>T ST ) m\u03b1 ) + m\u2126k2(m\u2212k)\u03c3\u03b1 . With \u03b1 tuned well, we pay logarithmic regret for the top m eigenvectors, but a square root regret O( \u221a\n\u2126k) for remaining directions not controlled by our sketch. This is expected for deterministic sketching which focuses on the dominant part of the spectrum. When \u03b1 is not tuned we still get sublinear regret as long as \u2126k is sublinear.\nOja\u2019s Algorithm. Oja\u2019s algorithm [25, 26] is not usually considered as a sketching algorithm but seems very natural here. This algorithm uses online gradient descent to find eigenvectors and eigenvalues of data in a streaming fashion, with the to-sketch vector g\u0302t\u2019s as the input. Specifically, let Vt \u2208 Rm\u00d7d denote the estimated eigenvectors and the diagonal matrix \u039bt \u2208 Rm\u00d7m contain the estimated eigenvalues at the end of round t. Oja\u2019s algorithm updates as:\n\u039bt = (Im \u2212 \u0393t)\u039bt\u22121 + \u0393t diag{Vt\u22121g\u0302t} 2 , Vt orth\u2190\u2212\u2212 Vt\u22121 + \u0393tVt\u22121g\u0302tg\u0302 > t (4)\nwhere \u0393t \u2208 Rm\u00d7m is a diagonal matrix with (possibly different) learning rates of order \u0398(1/t) on the diagonal, and the \u201c orth\u2190\u2212\u2212\u201d operator represents an orthonormalizing step.4 The sketch is then St = (t\u039bt) 1 2Vt. The rows of St are orthogonal and thus Ht is an efficiently maintainable diagonal matrix (see Algorithm 3). We call this combination Oja-SON.\nThe time complexity of Oja\u2019s algorithm is O(m2d) per round due to the orthonormalizing step. To improve the running time to O(md), one can only update the sketch every m rounds (similar to the block power method [15, 19]). The regret guarantee of this algorithm is unclear since existing analysis for Oja\u2019s algorithm is only for the stochastic setting (see e.g. [1, 19]). However, Oja-SON provides good performance experimentally.\n4For simplicity, we assume that Vt\u22121 + \u0393tVt\u22121g\u0302tg\u0302 > t is always of full rank so that the orthonormalizing step\ndoes not reduce the dimension of Vt.\nAlgorithm 4 Sparse Sketched Online Newton with Oja\u2019s Algorithm Input: Parameters C, \u03b1 and m.\n1: Initialize u\u0304 = 0d\u00d71 and b = 0m\u00d71. 2: (\u039b, F, Z,H)\u2190 SketchInit(\u03b1,m) (Algorithm 5). 3: for t = 1 to T do 4: Receive example xt. 5: Projection step: compute x\u0302 = FZxt and \u03b3 = \u03c4C(u\u0304 >xt+b >Zxt)\nx>t xt\u2212(t\u22121)x\u0302>\u039bHx\u0302 .\nObtain w\u0304 = u\u0304\u2212 \u03b3xt and b\u2190 b+ \u03b3(t\u2212 1)F>\u039bHx\u0302 (Equation 6). 6: Predict label yt = w\u0304>xt + b>Zxt and suffer loss `t(yt). 7: Compute gradient gt = ` \u2032 t(yt)xt and the to-sketch vector g\u0302 = \u221a \u03c3t + \u03b7tgt. 8: (\u039b, F , Z, H , \u03b4)\u2190 SketchUpdate(g\u0302) (Algorithm 5). 9: Update weight: u\u0304 = w\u0304 \u2212 1\u03b1gt \u2212 (\u03b4 >b)g\u0302 and b\u2190 b+ 1\u03b1 tF >\u039bHFZgt (Equation 5).\n10: end for"}, {"heading": "4 Sparse Implementation", "text": "In many applications, examples (and hence gradients) are sparse in the sense that \u2016xt\u20160 \u2264 s for all t and some small constant s d. Most online first order methods enjoy a per-example running time depending on s instead of d in such settings. Achieving the same for second order methods is more difficult since A\u22121t gt (or sketched versions) are typically dense even if gt is sparse.\nWe show how to implement our algorithms in sparsity-dependent time, specifically, in O(m2 +ms) for FD-SON and in O(m3 + ms) for Oja-SON. We emphasize that since the sketch would still quickly become a dense matrix even if the examples are sparse, achieving purely sparsity-dependent time is highly non-trivial and may be of independent interest. Due to space limit, below we only briefly mention how to do it for Oja-SON. Similar discussion for the FD sketch can be found in Appendix F. Note that mathematically these updates are equivalent to the non-sparse counterparts and regret guarantees are thus unchanged.\nThere are two ingredients to doing this for Oja-SON: (1) The eigenvectors Vt are represented as Vt = FtZt, where Zt \u2208 Rm\u00d7d is a sparsely updatable direction (Step 3 in Algorithm 5) and Ft \u2208 Rm\u00d7m is a matrix such that FtZt is orthonormal. (2) The weightswt are split as w\u0304t +Z>t\u22121bt, where bt \u2208 Rm maintains the weights on the subspace captured by Vt\u22121 (same as Zt\u22121), and w\u0304t captures the weights on the complementary subspace which are again updated sparsely.\nWe describe the sparse updates for w\u0304t and bt below with the details for Ft and Zt deferred to Appendix G. Since St = (t\u039bt) 1 2Vt = (t\u039bt) 1 2FtZt and wt = w\u0304t + Z>t\u22121bt, we know ut+1 is\nwt \u2212 ( Id \u2212 S>t HtSt )gt \u03b1 = w\u0304t \u2212 gt \u03b1 \u2212 (Zt \u2212 Zt\u22121)\n>bt\ufe38 \ufe37\ufe37 \ufe38 def = u\u0304t+1 +Z>t (bt + 1 \u03b1F > t (t\u039btHt)FtZtgt\ufe38 \ufe37\ufe37 \ufe38 def = b\u2032t+1 ) . (5)\nSince Zt \u2212 Zt\u22121 is sparse by construction and the matrix operations defining b\u2032t+1 scale with m, overall the update can be done in O(m2 +ms). Using the update forwt+1 in terms of ut+1, wt+1 is equal to\nut+1 \u2212 \u03b3t(Id \u2212 S>t HtSt)xt+1 = u\u0304t+1 \u2212 \u03b3txt+1\ufe38 \ufe37\ufe37 \ufe38 def = w\u0304t+1 +Z>t (b \u2032 t+1 + \u03b3tF > t (t\u039btHt)FtZtxt+1\ufe38 \ufe37\ufe37 \ufe38 def = bt+1 ) . (6)\nAgain, it is clear that all the computations scale with s and not d, so both w\u0304t+1 and bt+1 require only O(m2 +ms) time to maintain. Furthermore, the prediction w>t xt = w\u0304 > t xt + b > t Zt\u22121xt can also be computed in O(ms) time. The O(m3) in the overall complexity comes from a Gram-Schmidt step in maintaining Ft (details in Appendix G).\nThe pseudocode is presented in Algorithms 4 and 5 with some details deferred to Appendix G. This is the first sparse implementation of online eigenvector computation to the best of our knowledge.\nAlgorithm 5 Sparse Oja\u2019s Sketch Internal State: t, \u039b, F , Z, H and K. SketchInit(\u03b1,m)\n1: Set t = 0,\u039b = 0m\u00d7m, F = K = \u03b1H = Im and Z to any m\u00d7 d matrix with orthonormal rows. 2: Return (\u039b, F , Z, H).\nSketchUpdate(g\u0302) 1: Update t\u2190 t+1. Pick a diagonal stepsize matrix \u0393t to update \u039b\u2190 (I\u2212\u0393t)\u039b+\u0393t diag{FZg\u0302}2. 2: Set \u03b4 = A\u22121\u0393tFZg\u0302 and update K \u2190 K + \u03b4g\u0302>Z> + Zg\u0302\u03b4> + (g\u0302>g\u0302)\u03b4\u03b4>. 3: Update Z \u2190 Z + \u03b4g\u0302>. 4: (L,Q) \u2190 Decompose(F,K) (Algorithm 11), so that LQZ = FZ and QZ is orthogonal. Set F = Q.\n5: Set H \u2190 diag {\n1 \u03b1+t\u039b1,1 , \u00b7 \u00b7 \u00b7 , 1\u03b1+t\u039bm,m }\n. 6: Return (\u039b, F , Z, H , \u03b4)."}, {"heading": "5 Experiments", "text": "Preliminary experiments revealed that out of our two sketching options, Oja\u2019s sketch generally has better performance (see Appendix H). For more thorough evaluation, we implemented the sparse version of Oja-SON in Vowpal Wabbit.5 We compare it with ADAGRAD [5, 22] on both synthetic and real-world datasets. Each algorithm takes a stepsize parameter: 1\u03b1 serves as a stepsize for Oja-SON and a scaling constant on the gradient matrix for ADAGRAD. We try both methods with the parameter set to 2j for j = \u22123,\u22122, . . . , 6 and report the best results. We keep the stepsize matrix in Oja-SON fixed as \u0393t = 1t Im throughout. All methods make one online pass over data minimizing square loss."}, {"heading": "5.1 Synthetic Datasets", "text": "To investigate Oja-SON\u2019s performance in the setting it is really designed for, we generated a range of synthetic ill-conditioned datasets as follows. We picked a random Gaussian matrix Z \u223c RT\u00d7d (T = 10,000 and d = 100) and a random orthonormal basis V \u2208 Rd\u00d7d. We chose a specific spectrum \u03bb \u2208 Rd where the first d\u2212 10 coordinates are 1 and the rest increase linearly to some fixed condition number parameter \u03ba. We let X = Zdiag{\u03bb} 1 2 V > be our example matrix, and created a binary classification problem with labels y = sign(\u03b8>x), where \u03b8 \u2208 Rd is a random vector. We generated 20 such datasets with the same Z, V and labels y but different values of \u03ba \u2208 {10, 20, . . . , 200}. Note that if the algorithm is truly invariant, it would have the same behavior on these 20 datasets.\nFig. 1 (in Section 1) shows the final progressive error (i.e. fraction of misclassified examples after one pass over data) for ADAGRAD and Oja-SON (with sketch size m = 0, 5, 10) as the condition number increases. As expected, the plot confirms the performance of first order methods such as ADAGRAD degrades when the data is ill-conditioned. The plot also shows that as the sketch size increases,\n5An open source machine learning toolkit available at http://hunch.net/~vw\nOja-SON becomes more accurate: when m = 0 (no sketch at all), Oja-SON is vanilla gradient descent and is worse than ADAGRAD as expected; when m = 5, the accuracy greatly improves; and finally when m = 10, the accuracy of Oja-SON is substantially better and hardly worsens with \u03ba.\nTo further explain the effectiveness of Oja\u2019s algorithm in identifying top eigenvalues and eigenvectors, the plot in Fig. 2 shows the largest relative difference between the true and estimated top 10 eigenvalues as Oja\u2019s algorithm sees more data. This gap drops quickly after seeing just 500 examples."}, {"heading": "5.2 Real-world Datasets", "text": "Next we evaluated Oja-SON on 23 benchmark datasets from the UCI and LIBSVM repository (see Appendix H for description of these datasets). Note that some datasets are very high dimensional but very sparse (e.g. for 20news, d \u2248 102, 000 and s \u2248 94), and consequently methods with running time quadratic (such as ONS) or even linear in dimension rather than sparsity are prohibitive.\nIn Fig. 3(a), we show the effect of using sketched second order information, by comparing sketch size m = 0 and m = 10 for Oja-SON (concrete error rates in Appendix H). We observe significant improvements in 5 datasets (acoustic, census, heart, ionosphere, letter), demonstrating the advantage of using second order information. However, we found that Oja-SON was outperformed by ADAGRAD on most datasets, mostly because the diagonal adaptation of ADAGRAD greatly reduces the condition number on these datasets. Moreover, one disadvantage of SON is that for the directions not in the sketch, it is essentially doing vanilla gradient descent. We expect better results using diagonal adaptation as in ADAGRAD in off-sketch directions.\nTo incorporate this high level idea, we performed a simple modification to Oja-SON: upon seeing example xt, we feed D \u2212 12 t xt to our algorithm instead of xt, where Dt \u2208 Rd\u00d7d is the diagonal part of\nthe matrix \u2211t\u22121 \u03c4=1 g\u03c4g > \u03c4 .\n6 The intuition is that this diagonal rescaling first homogenizes the scales of all dimensions. Any remaining ill-conditioning is further addressed by the sketching to some degree, while the complementary subspace is no worse-off than with ADAGRAD. We believe this flexibility in picking the right vectors to sketch is an attractive aspect of our sketching-based approach.\nWith this modification, Oja-SON outperforms ADAGRAD on most of the datasets even for m = 0, as shown in Fig. 3(b) (concrete error rates in Appendix H). The improvement on ADAGRAD at m = 0 is surprising but not impossible as the updates are not identical\u2013our update is scale invariant like Ross et al. [30]. However, the diagonal adaptation already greatly reduces the condition number on all datasets except splice (see Fig. 4 in Appendix H for detailed results on this dataset), so little improvement is seen for sketch size m = 10 over m = 0. For several datasets, we verified the accuracy of Oja\u2019s method in computing the top-few eigenvalues (Appendix H), so the lack of difference between sketch sizes is due to the lack of second order information after the diagonal correction.\nThe average running time of our algorithm when m = 10 is about 11 times slower than ADAGRAD, matching expectations. Overall, SON can significantly outperform baselines on ill-conditioned data, while maintaining a practical computational complexity.\nAcknowledgements This work was done when Haipeng Luo and Nicol\u00f2 Cesa-Bianchi were at Microsoft Research, New York. We thank Lijun Zhang for pointing out our mistake in the regret proof of another sketching method that appeared in an earlier version.\n6D1 is defined as 0.1\u00d7 Id to avoid division by zero."}, {"heading": "A Proof of Theorem 1", "text": "Proof. Assuming T is a multiple of d without loss of generality, we pick xt from the basis vectors {e1, . . . , ed} so that each ei appears T/d times (in an arbitrary order). Note that now K is just a hypercube:\nK = { w : |w>xt| \u2264 C, \u2200t } = {w : \u2016w\u2016\u221e \u2264 C} .\nLet \u03be1, . . . , \u03beT be independent Rademacher random variables such that Pr(\u03bet = +1) = Pr(\u03bet = \u22121) = 12 . For a scalar \u03b8, we define loss function\n7 `t(\u03b8) = (\u03betL)\u03b8, so that Assumptions 1 and 2 are clearly satisfied with \u03c3t = 0. We show that, for any online algorithm,\nE[RT ] = E [ T\u2211 t=1 `t ( w>t xt ) \u2212 inf w\u2208K T\u2211 t=1 `t ( w>xt )] \u2265 CL \u221a dT 2\nwhich implies the statement of the theorem. First of all, note that E [ `t ( w>t xt ) \u2223\u2223\u2223 \u03be1, . . . , \u03bet\u22121] = 0 for any wt. Hence we have E\n[ T\u2211 t=1 `t ( w>t xt ) \u2212 inf w\u2208K T\u2211 t=1 `t ( w>xt )] = E [ sup w\u2208K T\u2211 t=1 \u2212`t ( w>xt )] = LE [ sup w\u2208K w> T\u2211 t=1 \u03betxt ] ,\nwhich, by the construction of xt, is\nCLE [\u2225\u2225\u2225\u2225\u2225 T\u2211 t=1 \u03betxt \u2225\u2225\u2225\u2225\u2225 1 ] = CLdE \u2223\u2223\u2223\u2223\u2223\u2223 T/d\u2211 t=1 \u03bet \u2223\u2223\u2223\u2223\u2223\u2223  \u2265 CLd\u221a T 2d = CL \u221a dT 2 ,\nwhere the final bound is due to the Khintchine inequality (see e.g. Lemma 8.2 in [3]). This concludes the proof."}, {"heading": "B Projection", "text": "We prove a more general version of Lemma 1 which does not require invertibility of the matrix A here. Lemma 2. For any x 6= 0,u \u2208 Rd\u00d71 and positive semidefinite matrix A \u2208 Rd\u00d7d, we have\nw\u2217 = argmin w:|w>x|\u2264C \u2016w \u2212 u\u2016A =  u\u2212 \u03c4C(u >x) x>A\u2020x A\u2020x if x \u2208 range(A)\nu\u2212 \u03c4C(u >x)\nx>(I\u2212A\u2020A)x (I \u2212A \u2020A)x if x /\u2208 range(A)\nwhere \u03c4C(y) = SGN(y) max{|y| \u2212 C, 0} and A\u2020 is the Moore-Penrose pseudoinverse of A. (Note that when A is rank deficient, this is one of the many possible solutions.)\nProof. First consider the case when x \u2208 range(A). If |u>x| \u2264 C, then it is trivial thatw\u2217 = u. We thus assume u>x \u2265 C below (the last case u>x \u2264 \u2212C is similar). The Lagrangian of the problem is\nL(w, \u03bb1, \u03bb2) = 1\n2 (w \u2212 u)>A(w \u2212 u) + \u03bb1(w>x\u2212 C) + \u03bb2(w>x+ C)\nwhere \u03bb1 \u2265 0 and \u03bb2 \u2264 0 are Lagrangian multipliers. Since w>x cannot be C and \u2212C at the same time, The complementary slackness condition implies that either \u03bb1 = 0 or \u03bb2 = 0. Suppose the latter case is true, then setting the derivative with respect tow to 0, we getw\u2217 = u\u2212\u03bb1A\u2020x+(I\u2212A\u2020A)z\n7By adding a suitable constant, these losses can always be made nonnegative while leaving the regret unchanged.\nwhere z \u2208 Rd\u00d71 can be arbitrary. However, since A(I \u2212 A\u2020A) = 0, this part does not affect the objective value at all and we can simply pick z = 0 so that w\u2217 has a consistent form regardless of whether A is full rank or not. Now plugging w\u2217 back, we have\nL(w\u2217, \u03bb1, 0) = \u2212 \u03bb1\n2\n2 x>A\u2020x+ \u03bb1(u >x\u2212 C)\nwhich is maximized when \u03bb1 = u >x\u2212C x>A\u2020x \u2265 0. Plugging this optimal \u03bb1 into w\u2217 gives the stated solution. On the other hand, if \u03bb1 = 0 instead, we can proceed similarly and verify that it gives a smaller dual value (0 in fact), proving the previous solution is indeed optimal.\nWe now move on to the case when x /\u2208 range(A). First of all the stated solution is well defined since x>(I \u2212A\u2020A)x is nonzero in this case. Moreover, direct calculation shows that w\u2217 is in the valid space: |w\u2217>x| = |u>x \u2212 \u03c4C(u>x)| \u2264 C, and also it gives the minimal possible distance value \u2016w\u2217 \u2212 u\u2016A = 0, proving the lemma."}, {"heading": "C Proof of Theorem 2", "text": "We first prove a general regret bound that holds for any choice of At in update 1:\nut+1 = wt \u2212A\u22121t gt wt+1 = argmin\nw\u2208Kt+1 \u2016w \u2212 ut+1\u2016At .\nThis bound will also be useful in proving regret guarantees for the sketched versions. Proposition 1. For any sequence of positive definite matrices At and sequence of losses satisfying Assumptions 1 and 2, the regret of updates (1) against any comparator w \u2208 K satisfies\n2RT (w) \u2264 \u2016w\u20162A0 + T\u2211 t=1\ngTt A \u22121 t gt\ufe38 \ufe37\ufe37 \ufe38\n\u201cGradient Bound\u201dRG\n+ T\u2211 t=1\n(wt \u2212w)>(At \u2212At\u22121 \u2212 \u03c3tgtg>t )(wt \u2212w)\ufe38 \ufe37\ufe37 \ufe38 \u201cDiameter Bound\u201dRD\nProof. Since wt+1 is the projection of ut+1 onto Kt+1, by the property of projections (see for example [16, Lemma 8]), the algorithm ensures\n\u2016wt+1 \u2212w\u20162At \u2264 \u2016ut+1 \u2212w\u2016 2 At = \u2016wt \u2212w\u20162At + g > t A \u22121 t gt \u2212 2g>t (wt \u2212w)\nfor all w \u2208 K \u2286 Kt+1. By the curvature property in Assumption 2, we then have that\n2RT (w) \u2264 T\u2211 t=1 2g>t (wt \u2212w)\u2212 \u03c3t ( g>t (wt \u2212w) )2 \u2264\nT\u2211 t=1 g>t A \u22121 t gt + \u2016wt \u2212w\u2016 2 At \u2212 \u2016wt+1 \u2212w\u20162At \u2212 \u03c3t ( g>t (wt \u2212w) )2 \u2264 \u2016w\u20162A0 + T\u2211 t=1 g>t A \u22121 t gt + (wt \u2212w)>(At \u2212At\u22121 \u2212 \u03c3tgtg>t )(wt \u2212w),\nwhich completes the proof.\nProof of Theorem 2. We apply Proposition 1 with the choice: A0 = \u03b1Id and At = At\u22121 + (\u03c3t + \u03b7t)gtg T t , which gives \u2016w\u2016 2 A0 = \u03b1 \u2016w\u201622 and\nRD = T\u2211 t=1 \u03b7t(wt \u2212w)>gtg>t (wt \u2212w) \u2264 4(CL)2 T\u2211 t=1 \u03b7t ,\nwhere the last equality uses the Lipschitz property in Assumption 1 and the boundedness ofw>t xt and w>xt.\nFor the term RG, define A\u0302t = \u03b1\u03c3+\u03b7T Id + \u2211t s=1 gsg > s . Since \u03c3t \u2265 \u03c3 and \u03b7t is non-increasing, we have A\u0302t 1\u03c3+\u03b7T At, and therefore:\nRG \u2264 1\n\u03c3 + \u03b7T T\u2211 t=1 g>t A\u0302 \u22121 t gt =\n1\n\u03c3 + \u03b7T T\u2211 t=1 \u2329 A\u0302t \u2212 A\u0302t\u22121, A\u0302\u22121t \u232a \u2264 1 \u03c3 + \u03b7T T\u2211 t=1 ln |A\u0302t| |A\u0302t\u22121| = 1 \u03c3 + \u03b7T ln |A\u0302T | |A\u03020|\n= 1\n\u03c3 + \u03b7T d\u2211 i=1 ln\n1 + (\u03c3 + \u03b7T )\u03bbi (\u2211T t=1 gtg > t ) \u03b1  \u2264 d \u03c3 + \u03b7T ln 1 + (\u03c3 + \u03b7T )\u2211di=1 \u03bbi (\u2211T t=1 gtg > t ) d\u03b1\n = d\n\u03c3 + \u03b7T ln\n( 1 + (\u03c3 + \u03b7T ) \u2211T t=1 \u2016gt\u2016 2 2\nd\u03b1 ) where the second inequality is by the concavity of the function ln |X| (see [17, Lemma 12] for an alternative proof), and the last one is by Jensen\u2019s inequality. This concludes the proof."}, {"heading": "D A Truly Invariant Algorithm", "text": "In this section we discuss how to make our adaptive online Newton algorithm truly invariant to invertible linear transformations. To achieve this, we set \u03b1 = 0 and replace A\u22121t with the MoorePenrose pseudoinverse A\u2020t : 8\nut+1 = wt \u2212A\u2020tgt, wt+1 = argmin\nw\u2208Kt+1 \u2016w \u2212 ut+1\u2016At .\n(7)\nWhen written in this form, it is not immediately clear that the algorithm has the invariant property. However, one can rewrite the algorithm in a mirror descent form:\nwt+1 = argmin w\u2208Kt+1 \u2225\u2225\u2225w \u2212wt +A\u2020tgt\u2225\u2225\u22252 At\n= argmin w\u2208Kt+1\n\u2016w \u2212wt\u20162At + 2(w \u2212wt) >AtA \u2020 tgt\n= argmin w\u2208Kt+1\n\u2016w \u2212wt\u20162At + 2w >gt\nwhere we use the fact that gt is in the range of At in the last step. Now suppose all the data xt are transformed to Mxt for some unknown and invertible matrix M , then one can verify that all the weights will be transformed to M\u2212Twt accordingly, ensuring the prediction to remain the same.\nMoreover, the regret bound of this algorithm can be bounded as below. First notice that even when At is rank deficient, the projection step still ensures the following: \u2016wt+1 \u2212w\u20162At \u2264 \u2016ut+1 \u2212w\u2016 2 At\n, which is proven in [17, Lemma 8]. Therefore, the entire proof of Theorem 2 still holds after replacing A\u22121t with A \u2020 t , giving the regret bound:\n1\n2 T\u2211 t=1 g>t A \u2020 t gt + 2(CL) 2\u03b7t . (8)\nThe key now is to bound the term \u2211T t=1 g > t A\u0302 \u2020 t gt where we define A\u0302t = \u2211t s=1 gsg > s . In order to do this, we proceed similarly to the proof of [4, Theorem 4.2] to show that this term is of order O(d2 lnT ) in the worst case.\n8See Appendix B for the closed form of the projection step.\nTheorem 4. Let \u03bb\u2217 be the minimum among the smallest nonzero eigenvalues of A\u0302t (t = 1, . . . , T ) and r be the rank of A\u0302T . We have\nT\u2211 t=1 g>t A\u0302 \u2020 t gt \u2264 r + (1 + r)r 2 ln\n( 1 + 2 \u2211T t=1 \u2016gt\u2016 2 2\n(1 + r)r\u03bb\u2217\n) .\nProof. First by Cesa-Bianchi et al. [4, Lemma D.1], we have\ng>t A\u0302 \u2020 t gt =\n{ 1 if gt /\u2208 range(A\u0302t\u22121)\n1\u2212 det+(A\u0302t\u22121) det+(A\u0302t) < 1 if gt \u2208 range(A\u0302t\u22121)\nwhere det+(M) denotes the product of the nonzero eigenvalues of matrix M . We thus separate the steps t such that gt \u2208 range(A\u0302t\u22121) from those where gt /\u2208 range(A\u0302t\u22121). For each k = 1, . . . , r let Tk be the first time step t in which the rank of At is k (so that T1 = 1). Also let Tr+1 = T + 1 for convenience. With this notation, we have\nT\u2211 t=1 g>t A\u0302 \u2020 t gt = r\u2211 k=1 g>TkA\u0302\u2020TkgTk + Tk+1\u22121\u2211 t=Tk+1 g>t A\u0302 \u2020 t gt  =\nr\u2211 k=1 1 + Tk+1\u22121\u2211 t=Tk+1 ( 1\u2212 det+(A\u0302t\u22121) det+(A\u0302t) ) = r +\nr\u2211 k=1 Tk+1\u22121\u2211 t=Tk+1\n( 1\u2212 det+(A\u0302t\u22121)\ndet+(A\u0302t)\n)\n\u2264 r + r\u2211\nk=1 Tk+1\u22121\u2211 t=Tk+1 ln det+(A\u0302t) det+(A\u0302t\u22121)\n= r + r\u2211 k=1 ln det+(A\u0302Tk+1\u22121) det+(A\u0302Tk) .\nFix any k and let \u03bbk,1, . . . , \u03bbk,k be the nonzero eigenvalues of A\u0302Tk and \u03bbk,1 + \u00b5k,1, . . . , \u03bbk,k + \u00b5k,k be the nonzero eigenvalues of A\u0302Tk+1\u22121. Then\nln det+(A\u0302Tk+1\u22121)\ndet+(A\u0302Tk) = ln k\u220f i=1 \u03bbk,i + \u00b5k,i \u03bbk,i = k\u2211 i=1 ln ( 1 + \u00b5k,i \u03bbk,i ) .\nHence, we arrive at T\u2211 t=1 g>t A\u0302 + t gt \u2264 r + r\u2211 k=1 k\u2211 i=1 ln ( 1 + \u00b5k,i \u03bbk,i ) .\nTo further bound the latter quantity, we use \u03bb\u2217 \u2264 \u03bbk,i and Jensen\u2019s inequality : r\u2211\nk=1 k\u2211 i=1 ln ( 1 + \u00b5k,i \u03bbk,i ) \u2264 r\u2211 k=1 k\u2211 i=1 ln ( 1 + \u00b5k,i \u03bb\u2217 ) \u2264 (1 + r)r\n2 ln\n( 1 + 2 \u2211r k=1 \u2211k i=1 \u00b5k,i\n(1 + r)r\u03bb\u2217\n) .\nFinally noticing that\nk\u2211 i=1 \u00b5k,i = TR(A\u0302Tk+1\u22121)\u2212 TR(A\u0302Tk) = Tk+1\u22121\u2211 t=Tk+1 TR(gtg > t ) = Tk+1\u22121\u2211 t=Tk+1 \u2016gt\u2016 2 2\ncompletes the proof.\nTaken together, Eq. (8) and Theorem 4 lead to the following regret bounds (recall the definitions of \u03bb\u2217 and r from Theorem 4).\nCorollary 1. If \u03c3t = 0 for all t and \u03b7t is set to be 1CL \u221a d t , then the regret of the algorithm defined by Eq. (7) is at most\nCL\n2\n\u221a T\nd\n( r + (1 + r)r\n2 ln\n( 1 + 2 \u2211T t=1 \u2016gt\u2016 2 2\n(1 + r)r\u03bb\u2217\n)) + 4CL \u221a Td.\nOn the other hand, if \u03c3t \u2265 \u03c3 > 0 for all t and \u03b7t is set to be 0, then the regret is at most\n1\n2\u03c3\n( r + (1 + r)r\n2 ln\n( 1 + 2 \u2211T t=1 \u2016gt\u2016 2 2\n(1 + r)r\u03bb\u2217\n)) ."}, {"heading": "E Proof of Theorem 3", "text": "Proof. We again first apply Proposition 1 (recall the notation RG and RD stated in the proposition). By the construction of the sketch, we have\nAt \u2212At\u22121 = S>t St \u2212 S>t\u22121St\u22121 = g\u0302tg\u0302 > t \u2212 \u03c1tV >t Vt g\u0302tg\u0302 > t .\nIt follows immediately that RD is again at most 4(CL)2 \u2211T t=1 \u03b7t. For the term RG, we will apply the\nfollowing guarantee of Frequent Directions (see the proof of Theorem 1.1 of [11]): \u2211T t=1 \u03c1t \u2264 \u2126k m\u2212k . Specifically, since TR(VtA\u22121t V > t ) \u2264 1\u03b1TR(VtV > t ) = m \u03b1 we have\nRG = T\u2211 t=1\n1\n\u03c3t + \u03b7t\n\u2329 A\u22121t , At \u2212At\u22121 + \u03c1tV >t Vt \u232a \u2264 1 \u03c3 + \u03b7T T\u2211 t=1 (\u2329 A\u22121t , At \u2212At\u22121 + \u03c1tV >t Vt\n\u232a) = 1\n\u03c3 + \u03b7T T\u2211 t=1 (\u2329 A\u22121t , At \u2212At\u22121 \u232a + \u03c1tTR(VtA \u22121 t V > t ) )\n\u2264 1 (\u03c3 + \u03b7T ) T\u2211 t=1 \u2329 A\u22121t , At \u2212At\u22121 \u232a + m\u2126k (m\u2212 k)(\u03c3 + \u03b7T )\u03b1 .\nFinally for the term \u2211T t=1 \u2329 A\u22121t , At \u2212At\u22121 \u232a , we proceed similarly to the proof of Theorem 2:\nT\u2211 t=1 \u2329 A\u22121t , At \u2212At\u22121 \u232a \u2264 T\u2211 t=1 ln |At| |At\u22121| = ln |AT | |A0| = d\u2211 i=1 ln ( 1 + \u03bbi(S > T ST ) \u03b1 )\n= m\u2211 i=1 ln ( 1 + \u03bbi(S > T ST ) \u03b1 ) \u2264 m ln ( 1 + TR(S>T ST ) m\u03b1 ) where the first inequality is by the concavity of the function ln |X|, the second one is by Jensen\u2019s inequality, and the last equality is by the fact that S>T ST is of rank m and thus \u03bbi(S > T ST ) = 0 for any i > m. This concludes the proof."}, {"heading": "F Sparse updates for FD sketch", "text": "The sparse version of our algorithm with the Frequent Directions option is much more involved. We begin by taking a detour and introducing a fast and epoch-based variant of the Frequent Directions algorithm proposed in [11]. The idea is the following: instead of doing an eigendecomposition immediately after inserting a new g\u0302 every round, we double the size of the sketch (to 2m), keep up to m recent g\u0302\u2019s, do the decomposition only at the end of every m rounds and finally keep the top m\neigenvectors with shrunk eigenvalues. The advantage of this variant is that it can be implemented straightforwardly in O(md) time on average without doing a complicated rank-one SVD update, while still ensuring the exact same guarantee with the only price of doubling the sketch size.\nAlgorithm 6 shows the details of this variant and how we maintain H . The sketch S is always represented by two parts: the top part (DV ) comes from the last eigendecomposition, and the bottom part (G) collects the recent to-sketch vector g\u0302\u2019s. Note that within each epoch, the update of H\u22121 is a rank-two update and thus H can be updated efficiently using Woodbury formula (Lines 3 and 4 of Algorithm 6).\nAlgorithm 6 Frequent Direction Sketch (epoch version) Internal State: \u03c4,D, V,G and H . SketchInit(\u03b1,m)\n1: Set \u03c4 = 1, D = 0m\u00d7m, G = 0m\u00d7d, H = 1\u03b1I2m and let V be any m\u00d7 d matrix whose rows are orthonormal. 2: Return (02m\u00d7d, H).\nSketchUpdate(g\u0302) 1: Insert g\u0302 into the \u03c4 -th row of G. 2: if \u03c4 < m then 3: Let e be the 2m\u00d7 1 basis vector whose (m+ \u03c4)-th entry is 1 and q = Sg\u0302 \u2212 g\u0302\n>g\u0302 2 e.\n4: Update H \u2190 H \u2212 Hqe >H\n1+e>Hq and H \u2190 H \u2212 Heq >H 1+q>He\n. 5: Update \u03c4 \u2190 \u03c4 + 1. 6: else 7: (V,\u03a3)\u2190 ComputeEigenSystem (( DV G )) (Algorithm 7).\n8: Set D to be a diagonal matrix with Di,i = \u221a\n\u03a3i,i \u2212 \u03a3m,m, \u2200i \u2208 [m]. 9: Set H \u2190 diag { 1\n\u03b1+D21,1 , \u00b7 \u00b7 \u00b7 , 1\u03b1+D2m,m , 1 \u03b1 , . . . , 1 \u03b1\n} .\n10: Set G = 0m\u00d7d. 11: Set \u03c4 = 1. 12: end if 13: Return (( DV G ) , H ) .\nAlthough we can use any available algorithm that runs in O(m2d) time to do the eigendecomposition (Line 7 in Algorithm 6), we explicitly write down the procedure of reducing this problem to eigendecomposing a small square matrix in Algorithm 7, which will be important for deriving the sparse version of the algorithm. Lemma 3 proves that Algorithm 7 works correctly for finding the top m eigenvector and eigenvalues.\nAlgorithm 7 ComputeEigenSystem(S) Input: S = ( DV G ) . Output: V \u2032 \u2208 Rm\u00d7d and diagonal matrix \u03a3 \u2208 Rm\u00d7m such that the i-th row of V \u2032 and the i-th entry of the diagonal of \u03a3 are the i-th eigenvector and eigenvalue of S>S respectively.\n1: Compute M = GV >. 2: Decompose G\u2212MV into the form LQ where L \u2208 Rm\u00d7r, Q is a r \u00d7 d matrix whose rows are\northonormal and r is the rank of G\u2212MV (e.g. by a Gram-Schmidt process). 3: Compute the top m eigenvectors (U \u2208 Rm\u00d7(m+r)) and eigenvalues (\u03a3 \u2208 Rm\u00d7m) of the matrix(\nD2 0m\u00d7r 0r\u00d7m 0r\u00d7r\n) + ( M>\nL>\n) ( M L ).\n4: Return (V \u2032,\u03a3) where V \u2032 = U ( V Q ) .\nLemma 3. The outputs of Algorithm 7 are such that the i-th row of V \u2032 and the i-th entry of the diagonal of \u03a3 are the i-th eigenvector and eigenvalue of S>S respectively.\nProof. Let W> \u2208 Rd\u00d7(d\u2212m\u2212r) be an orthonormal basis of the null space of ( V Q ) . By Line 2, we know that GW> = 0 and E = (V > Q> W>) forms an orthonormal basis of Rd. Therefore, we have\nS>S = V >D2V +G>G\n= E  D2 0 00 0 0 0 0 0 E> + EE>G>GEE> = E\n D2 0 00 0 0 0 0 0 +  V G>QG> WG>  (GV > GQ> GW>) E>\n= (V > Q>) (( D2 0 0 0 ) + ( M> L> ) ( M L ) ) \ufe38 \ufe37\ufe37 \ufe38\n=C\n( V Q )\nwhere in the last step we use the fact GQ> = (MV + LQ)Q> = L. Now it is clear that the eigenvalue of C will be the eigenvalue of S>S and the eigenvector of C will be the eigenvector of S>S after left multiplied by matrix (V > Q>), completing the proof.\nWe are now ready to present the sparse version of SON with Frequent Direction sketch (Algorithm 8). The key point is that we represent Vt as FtZt for some Ft \u2208 Rm\u00d7m and Zt \u2208 Rm\u00d7d, and the weight vectorwt as w\u0304t +Z>t\u22121bt and ensure that the update of Zt and w\u0304t will always be sparse. To see this,\ndenote the sketch St by ( DtFtZt Gt ) and let Ht,1 and Ht,2 be the top and bottom half of Ht. Now the update rule of ut+1 can be rewritten as\nut+1 = wt \u2212 ( Id \u2212 S>t HtSt )gt \u03b1\n= w\u0304t + Z > t\u22121bt \u2212\n1 \u03b1 gt + 1 \u03b1 (Z>t F > t Dt, G > t ) ( Ht,1Stgt Ht,2Stgt ) = w\u0304t + 1\n\u03b1 (G>t Ht,2Stgt \u2212 gt)\u2212 (Zt \u2212 Zt\u22121)>bt\ufe38 \ufe37\ufe37 \ufe38\nu\u0304t+1\n+Z>t (bt + 1\n\u03b1 F>t DtHt,1Stgt)\ufe38 \ufe37\ufe37 \ufe38\nb\u2032t+1\nWe will show that Zt \u2212 Zt\u22121 = \u2206tGt for some \u2206t \u2208 Rm\u00d7m shortly, and thus the above update is efficient due to the fact that the rows of Gt are collections of previous sparse vectors g\u0302.\nSimilarly, the update of wt+1 can be written as\nwt+1 = ut+1 \u2212 \u03b3t(xt+1 \u2212 S>t HtStxt+1)\n= u\u0304t+1 + Z > t b \u2032 t+1 \u2212 \u03b3txt+1 + \u03b3t(Z>t F>t Dt, G>t ) ( Ht,1Stxt+1 Ht,2Stxt+1 ) = u\u0304t+1 + \u03b3t(G\n> t Ht,2Stxt+1 \u2212 xt+1)\ufe38 \ufe37\ufe37 \ufe38\nw\u0304t+1\n+Z>t (b \u2032 t+1 + \u03b3tF > t DtHt,1Stxt+1)\ufe38 \ufe37\ufe37 \ufe38 bt+1 .\nIt is clear that \u03b3t can be computed efficiently, and thus the update of wt+1 is also efficient. These updates correspond to Line 6 and 10 of Algorithm 8.\nIt remains to perform the sketch update efficiently. Algorithm 9 is the sparse version of Algorithm 6. The challenging part is to compute eigenvectors and eigenvalues efficiently. Fortunately, in light of Algorithm 7, using the new representation V = FZ one can directly translate the process to Algorithm 10 and find that the eigenvectors can be expressed in the form N1Z +N2G. To see this, first note that Line 1 of both algorithms compute the same matrix M = GV > = GZ>F>. Then Line 2 decomposes the matrix\nG\u2212MV = G\u2212MFZ = ( \u2212MF Im ) ( Z G ) def = PR\nAlgorithm 8 Sparse Sketched Online Newton with Frequent Directions Input: Parameters C, \u03b1 and m.\n1: Initialize u\u0304 = 0d\u00d71, b = 0m\u00d71 and (D,F,Z,G,H)\u2190 SketchInit(\u03b1,m) (Algorithm 9). 2: Let S denote the matrix ( DFZ G ) throughout the algorithm (without actually computing it).\n3: Let H1 and H2 denote the upper and lower half of H , i.e. H = ( H1 H2 ) . 4: for t = 1 to T do 5: Receive example xt. 6: Projection step: compute x\u0302 = Sxt and \u03b3 = \u03c4C(u\u0304 >xt+b >Zxt)\nx>t xt\u2212x\u0302>Hx\u0302 .\nObtain w\u0304 = u\u0304+ \u03b3(G>H2x\u0302\u2212 xt) and b\u2190 b+ \u03b3F>DH1x\u0302. 7: Predict label yt = w\u0304>xt + b>Zxt and suffer loss `t(yt). 8: Compute gradient gt = ` \u2032 t(yt)xt and the to-sketch vector g\u0302 = \u221a \u03c3t + \u03b7tgt.\n9: (D,F,Z,G,H,\u2206)\u2190 SketchUpdate(g\u0302) (Algorithm 9). 10: Update u\u0304 = w\u0304 + 1\u03b1 (G >H2Sg \u2212 g)\u2212G>\u2206>b and b\u2190 b+ 1\u03b1F >DH1Sg. 11: end for\nusing Gram-Schmidt into the form LQR such that the rows of QR are orthonormal (that is, QR corresponds toQ in Algorithm 7). While directly applying Gram-Schmidt to PR would takeO(m2d) time, this step can in fact be efficiently implemented by performing Gram-Schmidt to P (instead of PR) in a Banach space where inner product is defined as \u3008a, b\u3009 = a>Kb with\nK = RR> =\n( ZZ> ZG>\nGZ> GG> ) being the Gram matrix of R. Since we can efficiently maintain the Gram matrix of Z (see Line 10 of Algorithm 9) and GZ> and GG> can be computed sparsely, this decomposing step can be done efficiently too. This modified Gram-Schmidt algorithm is presented in Algorithm 11 (which will also be used in sparse Oja\u2019s sketch), where Line 4 is the key difference compared to standard Gram-Schmidt (see Lemma 4 below for a formal proof of correctness).\nLine 3 of Algorithms 7 and 10 are exactly the same. Finally the eigenvectorsU ( V Q ) in Algorithm 7\nnow becomes (with U1, U2, Q1, Q2, N1, N2 defined in Line 4 of Algorithm 10)\nU ( FZ QR ) = (U1, U2) ( FZ QR ) = U1FZ + U2(Q1, Q2) ( Z G ) = (U1FZ + U2Q1)Z + U2Q2G = N1Z +N2G.\nTherefore, having the eigenvectors in the form N1Z +N2G, we can simply update F as N1 and Z as Z +N\u221211 N2G so that the invariant V = FZ still holds (see Line 11 of Algorithm 9). The update of Z is sparse since G is sparse.\nWe finally summarize the results of this section in the following theorem. Theorem 5. The average running time of Algorithm 8 is O ( m2 + ms ) per round, and the regret bound is exactly the same as the one stated in Theorem 3.\nLemma 4. The output of Algorithm 11 ensures that LQR = PR and the rows of QR are orthonormal.\nProof. It suffices to prove that Algorithm 11 is exactly the same as using the standard Gram-Schmidt to decompose the matrix PR into L and an orthonormal matrix which can be written as QR. First note that when K = In, Algorithm 11 is simply the standard Gram-Schmidt algorithm applied to P . We will thus go through Line 1-10 of Algorithm 11 with P replaced by PR and K by In and show that it leads to the exact same calculations as running Algorithm 11 directly. For clarity, we add \u201c\u02dc\u201d to symbols to distinguish the two cases (so P\u0303 = PR and K\u0303 = In). We will inductively prove\nAlgorithm 9 Sparse Frequent Direction Sketch Internal State: \u03c4,D, F, Z,G,H and K. SketchInit(\u03b1,m)\n1: Set \u03c4 = 1, D = 0m\u00d7m, F = K = Im, H = 1\u03b1I2m, G = 0m\u00d7d, and let Z be any m\u00d7 d matrix whose rows are orthonormal. 2: Return (D,F,Z,G,H).\nSketchUpdate(g\u0302) 1: Insert g\u0302 into the \u03c4 -th row of G. 2: if \u03c4 < m then 3: Let e be the 2m\u00d7 1 basic vector whose (m+ \u03c4)-th entry is 1 and compute q = Sg\u0302 \u2212 g\u0302\n>g\u0302 2 e.\n4: Update H \u2190 H \u2212 Hqe >H\n1+e>Hq and H \u2190 H \u2212 Heq >H 1+q>He\n. 5: Set \u2206 = 0m\u00d7m. 6: Set \u03c4 \u2190 \u03c4 + 1. 7: else 8: (N1, N2,\u03a3)\u2190 ComputeSparseEigenSystem (( DFZ G ) ,K ) (Algorithm 10).\n9: Compute \u2206 = N\u221211 N2. 10: Update Gram matrix K \u2190 K + \u2206GZ> + ZG>\u2206> + \u2206GG>\u2206>. 11: Update F = N1, Z \u2190 Z + \u2206G, and let D be such that Di,i = \u221a \u03a3i,i \u2212 \u03a3m,m, \u2200i \u2208 [m].\n12: Set H \u2190 diag {\n1 \u03b1+D21,1 , \u00b7 \u00b7 \u00b7 , 1\u03b1+D2m,m , 1 \u03b1 , . . . , 1 \u03b1\n} .\n13: Set G = 0m\u00d7d. 14: Set \u03c4 = 1. 15: end if 16: Return (D,F,Z,G,H,\u2206).\nAlgorithm 10 ComputeSparseEigenSystem(S,K) Input: S = ( DFZ G ) and Gram matrix K = ZZ>. Output: N1, N2 \u2208 Rm\u00d7m and diagonal matrix \u03a3 \u2208 Rm\u00d7m such that the i-th row of N1Z +N2G and the i-th entry of the diagonal of \u03a3 are the i-th eigenvector and eigenvalue of the matrix S>S.\n1: Compute M = GZ>F>. 2: (L,Q)\u2190 Decompose ( ( \u2212MF Im ) , ( K ZG>\nGZ> GG>\n)) (Algorithm 11).\n3: Let r be the number of columns of L. Compute the top m eigenvectors (U \u2208 Rm\u00d7(m+r)) and eigenvalues (\u03a3 \u2208 Rm\u00d7m) of the matrix (\nD2 0m\u00d7r 0r\u00d7m 0r\u00d7r\n) + ( M>\nL>\n) ( M L ).\n4: Set N1 = U1F + U2Q1 and N2 = U2Q2 where U1 and U2 are the first m and last r columns of U respectively, and Q1 and Q2 are the left and right half of Q respectively. 5: Return (N1, N2,\u03a3).\nthe invariance Q\u0303 = QR and L\u0303 = L. The base case Q\u0303 = QR = 0 and L\u0303 = L = 0 is trivial. Now assume it holds for iteration i\u2212 1 and consider iteration i. We have\n\u03b1\u0303 = Q\u0303K\u0303p\u0303 = QRR>p = QKp = \u03b1,\n\u03b2\u0303 = p\u0303\u2212 Q\u0303>\u03b1\u0303 = R>p\u2212 (QR)>\u03b1 = R>(p\u2212Q>\u03b1) = R>\u03b2,\nc\u0303 = \u221a \u03b2\u0303 > K\u0303\u03b2\u0303 = \u221a (R>\u03b2)>(R>\u03b2) = \u221a \u03b2>K\u03b2 = c,\nwhich clearly implies that after execution of Line 5-9, we again have Q\u0303 = QR and L\u0303 = L, finishing the induction.\nAlgorithm 11 Decompose(P, K) Input: P \u2208 Rm\u00d7n, K \u2208 Rm\u00d7m such that K is the Gram matrix K = RR> for some matrix\nR \u2208 Rn\u00d7d where n \u2265 m, d \u2265 m, Output: L \u2208 Rm\u00d7r and Q \u2208 Rr\u00d7n such that LQR = PR where r is the rank of PR and the rows\nof QR are orthonormal. 1: Initialize L = 0m\u00d7m and Q = 0m\u00d7n. 2: for i = 1 to m do 3: Let p> be the i-th row of P .\n4: Compute \u03b1 = QKp,\u03b2 = p\u2212Q>\u03b1 and c = \u221a \u03b2>K\u03b2.\n5: if c 6= 0 then 6: Insert 1c\u03b2\n> to the i-th row of Q. 7: end if 8: Set the i-th entry of \u03b1 to be c and insert \u03b1 to the i-th row of L. 9: end for\n10: Delete the all-zero columns of L and all-zero rows of Q. 11: Return (L,Q)."}, {"heading": "G Details for sparse Oja\u2019s algorithm", "text": "We finally provide the missing details for the sparse version of the Oja\u2019s algorithm. Since we already discussed the updates for w\u0304t and bt in Section 4, we just need to describe how the updates for Ft and Zt work. Recall that the dense Oja\u2019s updates can be written in terms of F and Z as\n\u039bt = (Im \u2212 \u0393t)\u039bt\u22121 + \u0393t diag{Ft\u22121Zt\u22121g\u0302t} 2\nFtZt orth\u2190\u2212\u2212 Ft\u22121Zt\u22121 + \u0393tFt\u22121Zt\u22121g\u0302tg\u0302 > t = Ft\u22121(Zt\u22121 + F \u22121 t\u22121\u0393tFt\u22121Zt\u22121g\u0302tg\u0302 > t ) .\n(9)\nHere, the update for the eigenvalues is straightforward. For the update of eigenvectors, first we let Zt = Zt\u22121 + \u03b4tg\u0302 > t where \u03b4t = F \u22121 t\u22121\u0393tFt\u22121Zt\u22121g\u0302t (note that under the assumption of Footnote 4, Ft is always invertible). Now it is clear that Zt \u2212 Zt\u22121 is a sparse rank-one matrix and the update of u\u0304t+1 is efficient. Finally it remains to update Ft so that FtZt is the same as orthonormalizing Ft\u22121Zt, which can in fact be achieved by applying the Gram-Schmidt algorithm to Ft\u22121 in a Banach space where inner product is defined as \u3008a, b\u3009 = a>Ktb where Kt is the Gram matrix ZtZ>t (see Algorithm 11). Since we can maintain Kt efficiently based on the update of Zt:\nKt = Kt\u22121 + \u03b4tg\u0302 > t Z > t\u22121 + Zt\u22121g\u0302t\u03b4 > t + (g\u0302 > t g\u0302t)\u03b4t\u03b4 > t ,\nthe update of Ft can therefore be implemented in O(m3) time."}, {"heading": "H Experiment Details", "text": "This section reports some detailed experimental results omitted from Section 5.2. Table 1 includes the description of benchmark datasets; Table 2 reports error rates on relatively small datasets to show that Oja-SON generally has better performance; Table 3 reports concrete error rates for the experiments described in Section 5.2; finally Table 4 shows that Oja\u2019s algorithm estimates the eigenvalues accurately.\nAs mentioned in Section 5.2, we see substantial improvement for the splice dataset when using Oja\u2019s sketch even after the diagonal adaptation. We verify that the condition number for this dataset before and after the diagonal adaptation are very close (682 and 668 respectively), explaining why a large improvement is seen using Oja\u2019s sketch. Fig. 4 shows the decrease of error rates as Oja-SON with different sketch sizes sees more examples. One can see that even with m = 1 Oja-SON already performs very well. This also matches our expectation since there is a huge gap between the top and second eigenvalues of this dataset (50.7 and 0.4 respectively)."}], "references": [{"title": "The fast convergence of incremental pca", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "NIPS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic quasi-newton method for large-scale optimization", "author": ["R.H. Byrd", "S. Hansen", "J. Nocedal", "Y. Singer"], "venue": "SIAM Journal on Optimization, 26:1008\u20131031,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "A second-order perceptron algorithm", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "SIAM Journal on Computing, 34(3):640\u2013668,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Convergence rates of sub-sampled newton methods", "author": ["M.A. Erdogdu", "A. Montanari"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval research logistics quarterly, 3 (1-2):95\u2013110,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1956}, {"title": "One-pass auc optimization", "author": ["W. Gao", "R. Jin", "S. Zhu", "Z.-H. Zhou"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "author": ["D. Garber", "E. Hazan"], "venue": "SIAM Journal on Optimization, 26:1493\u20131528,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Online learning of eigenvectors", "author": ["D. Garber", "E. Hazan", "T. Ma"], "venue": "ICML,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Frequent directions: Simple and deterministic matrix sketching", "author": ["M. Ghashami", "E. Liberty", "J.M. Phillips", "D.P. Woodruff"], "venue": "SIAM Journal on Computing, 45:1762\u20131792,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient frequent directions algorithm for sparse matrices", "author": ["M. Ghashami", "E. Liberty", "J.M. Phillips"], "venue": "KDD,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster sgd using sketched conditioning", "author": ["A. Gonen", "S. Shalev-Shwartz"], "venue": "arXiv:1506.02649,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Solving ridge regression using sketched preconditioned svrg", "author": ["A. Gonen", "F. Orabona", "S. Shalev-Shwartz"], "venue": "ICML,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "The noisy power method: A meta algorithm with applications", "author": ["M. Hardt", "E. Price"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Projection-free online learning", "author": ["E. Hazan", "S. Kale"], "venue": "ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning, 69(2-3):169\u2013192,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "ICML,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Rivalry of two families of algorithms for memory-restricted streaming pca", "author": ["C.-L. Li", "H.-T. Lin", "C.-J. Lu"], "venue": "arXiv:1506.01490,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple and deterministic matrix sketching", "author": ["E. Liberty"], "venue": "KDD,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming, 45(1-3):503\u2013528,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H.B. McMahan", "M. Streeter"], "venue": "COLT,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Global convergence of online limited memory bfgs", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "JMLR, 16:3151\u20133181,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A linearly-convergent stochastic l-bfgs algorithm", "author": ["P. Moritz", "R. Nishihara", "M.I. Jordan"], "venue": "AISTATS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology, 15 (3):267\u2013273,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1982}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["E. Oja", "J. Karhunen"], "venue": "Journal of mathematical analysis and applications, 106(1):69\u201384,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1985}, {"title": "Scale-free algorithms for online linear optimization", "author": ["F. Orabona", "D. P\u00e1l"], "venue": "ALT,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "A generalized online mirror descent with applications to classification and regression", "author": ["F. Orabona", "K. Crammer", "N. Cesa-Bianchi"], "venue": "Machine Learning, 99(3):411\u2013435,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Newton sketch: A linear-time optimization algorithm with linearquadratic convergence", "author": ["M. Pilanci", "M.J. Wainwright"], "venue": "arXiv:1505.02250,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Normalized online learning", "author": ["S. Ross", "P. Mineiro", "J. Langford"], "venue": "UAI,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["N.N. Schraudolph", "J. Yu", "S. G\u00fcnter"], "venue": "AISTATS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["J. Sohl-Dickstein", "B. Poole", "S. Ganguli"], "venue": "ICML,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["D.P. Woodruff"], "venue": "Foundations and Trends in Machine Learning, 10(1-2):1\u2013157,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Second order algorithms such as Online Newton Step [17] have the attractive property of being invariant to linear transformations of the data, but typically require space and update time quadratic in the number of dimensions.", "startOffset": 51, "endOffset": 55}, {"referenceID": 29, "context": "We study an invariant learning setting similar to the paper [30] which compares the learner to a benchmark only constrained to generate bounded predictions on the sequence of examples.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "We show that a variant of the Online Newton Step [17], while quadratic in computation, stays regret-optimal with a nearly matching lower bound in this more general setting.", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "While the idea of data sketching is widely studied [33], as far as we know our work is the first one to apply it to a general adversarial", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "Two different sketching methods are considered: Frequent Directions [11, 20] and Oja\u2019s algorithm [25, 26], both of which allow linear running time per round.", "startOffset": 68, "endOffset": 76}, {"referenceID": 19, "context": "Two different sketching methods are considered: Frequent Directions [11, 20] and Oja\u2019s algorithm [25, 26], both of which allow linear running time per round.", "startOffset": 68, "endOffset": 76}, {"referenceID": 24, "context": "Two different sketching methods are considered: Frequent Directions [11, 20] and Oja\u2019s algorithm [25, 26], both of which allow linear running time per round.", "startOffset": 97, "endOffset": 105}, {"referenceID": 25, "context": "Two different sketching methods are considered: Frequent Directions [11, 20] and Oja\u2019s algorithm [25, 26], both of which allow linear running time per round.", "startOffset": 97, "endOffset": 105}, {"referenceID": 9, "context": "[10]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Empirically, we evaluate our algorithm using the sparse Oja sketch (called Oja-SON) against first order methods such as diagonalized ADAGRAD [5, 22] on both ill-conditioned synthetic and a suite of real-world datasets.", "startOffset": 141, "endOffset": 148}, {"referenceID": 21, "context": "Empirically, we evaluate our algorithm using the sparse Oja sketch (called Oja-SON) against first order methods such as diagonalized ADAGRAD [5, 22] on both ill-conditioned synthetic and a suite of real-world datasets.", "startOffset": 141, "endOffset": 148}, {"referenceID": 29, "context": "Related work Our online learning setting is closest to the one proposed in [30], which studies scale-invariant algorithms, a special case of the invariance property considered here (see also [28, Section 5]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "Orabona and P\u00e1l [27] study unrelated notions of invariance.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "[8] study a specific randomized sketching method for a special online learning setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 22, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 23, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 30, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 31, "context": "The L-BFGS algorithm [21] has recently been studied in the stochastic setting2 [2, 23, 24, 31, 32], but has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches empirically.", "startOffset": 79, "endOffset": 98}, {"referenceID": 5, "context": "Recent works [6, 14, 13, 29] employ sketching in stochastic optimization, but do not provide sparse implementations or extend in an obvious manner to the online setting.", "startOffset": 13, "endOffset": 28}, {"referenceID": 13, "context": "Recent works [6, 14, 13, 29] employ sketching in stochastic optimization, but do not provide sparse implementations or extend in an obvious manner to the online setting.", "startOffset": 13, "endOffset": 28}, {"referenceID": 12, "context": "Recent works [6, 14, 13, 29] employ sketching in stochastic optimization, but do not provide sparse implementations or extend in an obvious manner to the online setting.", "startOffset": 13, "endOffset": 28}, {"referenceID": 28, "context": "Recent works [6, 14, 13, 29] employ sketching in stochastic optimization, but do not provide sparse implementations or extend in an obvious manner to the online setting.", "startOffset": 13, "endOffset": 28}, {"referenceID": 6, "context": "The FrankWolfe algorithm [7, 18] is also invariant to linear transformations, but with worse regret bounds [16] without further assumptions and modifications [9].", "startOffset": 25, "endOffset": 32}, {"referenceID": 17, "context": "The FrankWolfe algorithm [7, 18] is also invariant to linear transformations, but with worse regret bounds [16] without further assumptions and modifications [9].", "startOffset": 25, "endOffset": 32}, {"referenceID": 15, "context": "The FrankWolfe algorithm [7, 18] is also invariant to linear transformations, but with worse regret bounds [16] without further assumptions and modifications [9].", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": "The FrankWolfe algorithm [7, 18] is also invariant to linear transformations, but with worse regret bounds [16] without further assumptions and modifications [9].", "startOffset": 158, "endOffset": 161}, {"referenceID": 11, "context": "For an invariant update, Recent work by [12] also studies sparse updates for a more complicated variant of Frequent Directions which is randomized and incurs extra approximation error.", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "This relaxation is similar to the comparator set considered in [30].", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "[30] are recovered.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We study a choice of At that is similar to the Online Newton Step (ONS) [17] (though with different projections):", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "extending the O(d lnT ) results in [17] to the weaker Assumption 2 and a larger comparator set K.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "Frequent Directions sketch [11, 20] is a deterministic sketching method.", "startOffset": 27, "endOffset": 35}, {"referenceID": 19, "context": "Frequent Directions sketch [11, 20] is a deterministic sketching method.", "startOffset": 27, "endOffset": 35}, {"referenceID": 10, "context": "The sketch update works inO(md) time (see [11] and Appendix F) so the total running time is O(md) per round.", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "Oja\u2019s algorithm [25, 26] is not usually considered as a sketching algorithm but seems very natural here.", "startOffset": 16, "endOffset": 24}, {"referenceID": 25, "context": "Oja\u2019s algorithm [25, 26] is not usually considered as a sketching algorithm but seems very natural here.", "startOffset": 16, "endOffset": 24}, {"referenceID": 14, "context": "To improve the running time to O(md), one can only update the sketch every m rounds (similar to the block power method [15, 19]).", "startOffset": 119, "endOffset": 127}, {"referenceID": 18, "context": "To improve the running time to O(md), one can only update the sketch every m rounds (similar to the block power method [15, 19]).", "startOffset": 119, "endOffset": 127}, {"referenceID": 0, "context": "[1, 19]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[1, 19]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "5 We compare it with ADAGRAD [5, 22] on both synthetic and real-world datasets.", "startOffset": 29, "endOffset": 36}, {"referenceID": 21, "context": "5 We compare it with ADAGRAD [5, 22] on both synthetic and real-world datasets.", "startOffset": 29, "endOffset": 36}, {"referenceID": 29, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "References [1] A.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "2 in [3]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "1 of [11]): \u2211T t=1 \u03c1t \u2264 \u03a9k m\u2212k .", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "We begin by taking a detour and introducing a fast and epoch-based variant of the Frequent Directions algorithm proposed in [11].", "startOffset": 124, "endOffset": 128}], "year": 2017, "abstractText": "We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a running time linear in the dimension and sketch size. We further develop sparse forms of the sketching methods (such as Oja\u2019s rule), making the computation linear in the sparsity of features. Together, the algorithm eliminates all computational obstacles in previous second order online learning approaches.", "creator": "LaTeX with hyperref package"}}}