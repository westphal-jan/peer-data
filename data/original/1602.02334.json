{"id": "1602.02334", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2016", "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution", "abstract": "Entity resolution (ER), an important and common data cleaning problem, is about detecting data duplicate representations for the same external entities, and merging them into single representations. Relatively recently, declarative rules called \"matching dependencies\" (MDs) have been proposed for specifying similarity conditions under which attribute values in database records are merged. In this work we show the process and the benefits of integrating four components of ER: (a) Building a classifier for duplicate/non-duplicate record pairs built using machine learning (ML) techniques; (b) Use of MDs for supporting the blocking phase of ML; (c) Record merging on the basis of the classifier results; and (d) The use of the declarative language \"LogiQL\" -an extended form of Datalog supported by the \"LogicBlox\" platform- for all activities related to data processing, and the specification and enforcement of MDs.", "histories": [["v1", "Sun, 7 Feb 2016 03:06:40 GMT  (1385kb)", "https://arxiv.org/abs/1602.02334v1", "Extended version ofarXiv:1508.06013"], ["v2", "Sun, 27 Nov 2016 21:09:37 GMT  (482kb,D)", "http://arxiv.org/abs/1602.02334v2", "Revised journal submission after acceptance with minor revisions. Extended version ofarXiv:1508.06013"], ["v3", "Wed, 18 Jan 2017 17:43:43 GMT  (457kb,D)", "http://arxiv.org/abs/1602.02334v3", "Final journal version, with some minor technical corrections. Extended version ofarXiv:1508.06013"]], "COMMENTS": "Extended version ofarXiv:1508.06013", "reviews": [], "SUBJECTS": "cs.DB cs.AI cs.LG", "authors": ["zeinab bahmani", "leopoldo bertossi", "nikolaos vasiloglou"], "accepted": false, "id": "1602.02334"}, "pdf": {"name": "1602.02334.pdf", "metadata": {"source": "CRF", "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution", "authors": ["Zeinab Bahmani", "Leopoldo Bertossia", "Nikolaos Vasiloglou"], "emails": ["bertossi@scs.carleton.ca"], "sections": [{"heading": null, "text": "Entity resolution (ER), an important and common data cleaning problem, is about detecting data duplicate representations for the same external entities, and merging them into single representations. Relatively recently, declarative rules called matching dependencies (MDs) have been proposed for specifying similarity conditions under which attribute values in database records are merged. In this work we show the process and the benefits of integrating four components of ER: (a) Building a classifier for duplicate/non-duplicate record pairs built using machine learning (ML) techniques; (b) Use of MDs for supporting the blocking phase of ML; (c) Record merging on the basis of the classifier results; and (d) The use of the declarative language LogiQL -an extended form of Datalog supported by the LogicBlox platform- for all activities related to data processing, and the specification and enforcement of MDs.\nKeywords: Entity resolution, matching dependencies, support-vector machines, classification, Datalog 2010 MSC: 00-01, 99-00"}, {"heading": "1. Introduction", "text": "Entity resolution (ER) is a common and difficult problem in data cleaning that has to do with handling unintended multiple representations in a database of the same external objects. This problem is also known as deduplication, reference reconciliation, merge-purge, etc. Multiple representations lead to uncertainty in data and the problem of managing it. Cleaning the database reduces uncertainty. In more precise terms, ER is about the identification and fusion of database records (think of rows or tuples in tables) that represent the same real-world entity [13, 25]. As a consequence, ER usually goes through two main consecutive phases: (a) detecting duplicates, and (b) merging them into single representations.\n\u2217Corresponding author Email address: bertossi@scs.carleton.ca (Leopoldo Bertossi)\nPreprint submitted to Journal of LATEX Templates January 19, 2017\nar X\niv :1\n60 2.\n02 33\n4v 3\n[ cs\n.D B\n] 1\n8 Ja\nn 20\nFor duplicate detection, one must first analyze multiple pairs of records, comparing the two records in them, and discriminating between: pairs of duplicate records and pairs of non-duplicate records. This classification problem is approached with machine learning (ML) methods, to learn from previously known or already made classifications (a training set for supervised learning), building a classification model (a classifier) for deciding about other record pairs [16, 25].\nIn principle, in ER every two records (forming a pair) have to be compared through the classifier. Most of the work on applying ML to ER work at the record level [41, 16, 17], and only some of the attributes, or their features, i.e. numerical values associated to them, may be involved in duplicate detection. The choice of relevant sets of attributes and features is application dependent.\nWith a classifier at hand, ER may be a task of quadratic complexity since it requires comparing every two records. To reduce the large number of two-record comparisons, blocking techniques are used [45, 9, 32, 50]. Commonly, a single record attribute, or a combination of attributes, the so-called blocking key, is used to split the database records into blocks. Next, under the assumption that any two records in different blocks are unlikely to be duplicates, only every two records in a same block are compared for duplicate detection.\nAlthough blocking will discard many record pairs that are obvious nonduplicates, some true duplicate pairs might be missed (by putting them in different blocks), due to errors or typographical variations in attribute values or the rigidity and low sensitivity of blocking keys. More interestingly, similarity between blocking key values alone may fail to capture the relationships that naturally hold in the data and could be used for blocking. Thus, entity blocking based only on similarities of blocking key values may cause low recall. This is a major drawback of traditional blocking techniques.\nIn this work we consider different and coexisting entities, for example Author and Paper. For each of them, there is a collection of records. For entity Author, records may have the form a = \u3008name, . . . , affiliation, . . . , paper title, . . .\u3009; and for Paper entity, records may be of the form p = \u3008title, . . . , author name, . . .\u3009.1\nRecords for different entities may be related via attributes in common and referential constraints, something the blocking mechanism could take advantage of. Blocking can be performed on each of the participating entities, and the way records for an entity, say Author, are placed in blocks may influence the way the records for another entity, say Paper, are assigned to blocks. This is called \u201ccollective blocking\u201d. Semantic, relational information, in addition to that provided by blocking keys for single entities, can be used to state relationships between different entities and their corresponding similarity criteria. So, blocking decision making forms a collective and intertwined process involving several entities. In the end, the records for each individual entity will be placed in blocks associated to that entity.\nIn our work, collective blocking is based on blocking keys and the enforcement\n1For all practical purposes, think of records as database tuples in a single table.\nof semantic information about the relational closeness of entities Author and Paper, which is captured by a set of matching dependencies (MDs) [27]. So, we propose \u201cMD-based collective blocking\u201d.\nAfter records are divided in blocks, the proper duplicate detection process starts, and is carried out by comparing every two records in a block, and classifying the pair as \u201cduplicates\u201d or \u201cnon-duplicates\u201d using the trained ML model at hand. In the end, records in duplicate pairs are considered to represent the same external entity, and have to be merged into a single representation, i.e. into a single record. This second phase is also application dependent. MDs were originally proposed to support this kind of task, and their use in blocking is somehow unexpected.\nMatching dependencies are declarative logical rules that tell us under what conditions of similarity between attribute values, any two records must have certain attribute values merged (or matched), i.e. made identical [26, 27]. For example, the MD:\nDeptB [Dep] \u2248 DeptB [Dep] \u2192 DeptB [City ] . = DeptB [City ] (1)\ntells us that, for any two records for entity (or relation or table) DeptB that have similar values for attribute Dep, their values for attribute City should be merged, i.e. made the same.\nMDs as introduced in [27] do not specify how to merge values. In [11], MDs were extended with matching functions (MFs). For a data domain, a MF specifies how to assign a value in common to two values. In this work, we adopt MDs with MFs. In the end, the enforcement of MDs with MFs should produce a duplicate-free instance (cf. Section 2 for more details).\nMDs have to be specified in a declarative manner, and at some point enforced, by producing changes on the data. For this purpose, we use the LogicBlox platform, a data management system developed by the LogicBlox2 company, that is centered around its declarative language, LogiQL [31]. LogiQL supports relational data management and, among several other features [3], an extended form of Datalog with stratified negation [15]. This language is expressive enough for the kind of MDs considered in this work.3\nIn this paper, we describe our ERBlox system. It is built on top of the LogicBlox platform, and implements entity resolution (ER) applying LogiQL for the specification and enforcement of MDs, and built-in ML techniques for building the classifier. More specifically, ERBlox has four main components or modules:\n(a) MD-based collective blocking: This phase is just about clustering together records that might be duplicates of each other. Additional comparisons between two records between will be performed within block, and never with records from different blocks. Blocking can be used before learning\n2 www.logicblox.com 3For arbitrary sets of MDs, we need higher expressive power [11], such as that provided by\nanswer set programming [4].\na classifier, to ascribe labels (duplicate/non-duplicate or \u00b11) to pairs of records that will become training examples, or after the classifier has been learned, with new records in the database that have to be checked for duplication with other records in the database.4 It may be the case that two records in a same block may end up not being considered as duplicates of each other. However, through blocking the number of pairwise record comparisons of is reduced.\n(b) ML-based classification model construction: At this point any supervised technique for classification, i.e. for building the mathematical model for classification, could be used. This is the proper machine learning phase. We used the support-vector machine (SVM) approach [21, 48].\n(c) Duplicate detection: Having the new records in the database (as opposed to training examples) already clustered in blocks, this phase is about applying the classification model obtained in the previous phase to new pairs of records, obtaining for each pair the outcome \u00b11. The classifier could be applied to any two records, or -if blocking techniques have been applied to the database- only to two records in a same block. In our case, we did the latter.\n(d) MD-based duplicate merging: The output of the preceding phase is just a set of record-pairs with their newly created labels, \u00b11, indicating that they are duplicates of each other, or not. This last phase merges duplicates into single records. In our case, when and how to merge is specified by matching dependencies, which invoke matching functions to find the values in common to be used in the records created by merging.\nThe blocking phase, (a) above, uses MDs to specify the blocking strategy. They express conditions in terms of blocking key similarities and also relational closeness -the semantic knowledge- to assign two records to a same block, by making their block identifiers identical. Then, under MD-based collective blocking different records of possibly several related entities are simultaneously assigned to blocks through the enforcement of MDs (cf. Section 5 for details). This is a non-traditional, novel use of MDs, whereas their intended use is the application to proper merging phase, (d) above, [26].\nIt is important to emphasize that, in our work, MDs are not used for the whole ER process, but only in two of the phases above. In principle, the whole ER process could be based only on MDs. However, this would be a complete different problem, in particular, a completely different machine learning problem: the MDs for this application would have to be learned from scratch (implicitly learning similarity relationships and a classifier). Learning MDs is a rather unexplored area of research (cf. [43, 44] for some work in this direction), which is somehow closer to the areas of rule learning [29] and discovery of database\n4In our case, the training data already came with labels. So, blocking was applied to the unlabeled records before, but independently, from the learning and execution of the classifier.\ndependencies [40]. With our approach we can leverage, at a particular phase of the ER process, available machine learning techniques that are fully integrated with database management systems, as in the case of LogicBlox.\nThe sets of MDs used in (a) and (d) are different, and play different roles. In both cases, they are application-dependent, and have a canonical representation in the system, as Datalog rules. The MDs are then enforced by applying (running) those rules. Although in general a set of MDs may lead to alternative final instances through its enforcement [11], in our application of MDs both sets of MDs lead to a single instance.\nIn the case of (a), this means that, for each entity, a unique set of disjoint blocks is generated. The reason is that the combination of the set of MDs and the initial database instance falls into a newly identified, well-behaved class, the SFAI class, that we introduce in this work. (The main ideas and intuitions around it are presented in the main body of this paper, but more specific details are given in Appendix A.) In the case of (d), the set of \u201cmerge\u201d MDs also leads to a single, duplicate-free instance (as captured by the classifier and the merge MDs). This is because the MDs in the set turn out to be interaction-free [11](cf. also Appendix A).\nWe use LogiQL to declaratively implement the two MD-based components of ERBlox. As shown in [4, 6] in general, sets of MDs can be expressed by means of answer-set programs (ASPs) [14]. However, both classes of MDs used by ERBlox can be expressed by computationally efficient fragments of ASPs, namely Datalog with stratified negation [15], which is supported by LogiQL.\nOn the machine learning side (item (b) above), the problem is about building and implementing a model for the detection of pairs of duplicate records. The classification model is trained using record-pairs known to be duplicates or non-duplicates. We independently used three established classification algorithms: SVM, k-nearest neighbor (K-NN) [20], and non-parametric Bayes classifier (NBC) [8]. We used the Ismion5 implementation of them due to the in-house expertise at LogicBlox. Since the emphasis of this work is on the use of LogiQL and MDs, we will refer only to our use of SVM.\nFor experimentation with the ERBlox system, we used as dataset a snapshot of Microsoft Academic Search (MAS)6 that includes 250K authors, 2.5M papers, and a training set. We also used, independently, datasets from DBLP and Cora Citation. The experimental results show that our system improves ER recall and precision over traditional, standard blocking techniques [33], where just blocking-key similarities are used. Actually, MD-based collective blocking leads to higher precision and recall on the given datasets.\nOur work also shows the integration under a single system of different forms of data retrieval, storage and transformation, on one side, and machine learning techniques, on the other. All this is enabled by the use of optimized Datalog-rule declaration and execution as supported by the LogicBlox platform.\n5http://www.ismion.com 6http://academic.research.microsoft.com. As of January 2013.\nThis paper is structured as follows. Section 2 introduces background on: matching dependencies (including a brief description of the new SFAI class), classification, and collective blocking. A general overview of the ERBlox system is presented in Section 3. Specific details about the components of our methodology and ERBlox are given and discussed in Sections 4, 5, 6, and 7. Experimental results are shown in Section 8. Sections 9 and 10 present related work and conclusions, respectively. In Appendix A we provide the definitions and more details about relational MDs, the SFAI class, and other classes with the unique clean instance property.7 This paper is a revised and extended version of [5]."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Matching dependencies", "text": "We consider an application-dependent relational schema R, with a data domain U . For an attribute A, Dom(A) \u2286 U is its domain. We assume predicates do not share attributes, but different attributes may share a domain. An instance D for R is a finite set of ground atoms of the form R(c1, . . . , cn), with R \u2208 R, ci \u2208 U . The active domain of an instance D, denoted Adom(D), is the finite set of all constants from U that appear in D.\nWe assume that each entity is represented by a relational predicate, and its tuples or rows in its extension correspond to records for the entity. As in [11], we assume records have unique, fixed, global record identifiers (rids), which are positive integers. This allows us to trace changes of attribute values in records. When records are represented as tuples in a database, which is usually the case, we talk about global tuple identifiers (tids). Record and tuple ids are placed in an extra, first attribute for R \u2208 R that acts as a key. Then, records take the form R(t, c\u0304), with t the identifier. Sometimes we leave tids and rids implicit. If A is a sublist of the attributes for a predicate R, R[A] denotes the restriction of an R-tuple (or the predicate R) to attributes in A.\nMDs are formulas of the form [26, 27]: \u03d5 : \u2227 j R1[X j 1 ] \u2248j R2[X j 2 ] \u2212\u2192 \u2227 k R1[Y k 1 ] . = R2[Y k 2 ], (2)\nwhere attributes (treated as variables) Xj1 and X j 2 (also Y k 1 , Y k 2 ) are comparable, in the sense that they share the same data domain Domj on which a binary similarity (i.e., reflexive and symmetric) relation \u2248j is defined. R1, R2 could be the same predicate. The MD in (2) states that, for every pair of tuples (one in relation R1, the other in relation R2) where the left-hand side (LHS) of the arrow is true, the attribute values in them on the right-hand side (RHS) have to be made identical. We can consider only MDs with a single identity atom\n7The material in Appendix A is all new, but, although important for ERBlox, departs from the main thread of the paper.\n(with . =) in the RHSs. Accordingly, an explicit formulation of the MD in (2) in classical predicate logic is:8\n\u03d5 : \u2200t1t2 \u2200x\u03041x\u03042(R1(t1, x\u03041) \u2227R2(t2, x\u03042) \u2227 \u2227 j xj1 \u2248j x j 2 \u2212\u2192 y1 . = y2), (3) with xj1, y1 \u2208 x\u03041, x j 2, y2 \u2208 x\u03042. The ti are used as variables for tuple IDs. We usually leave the universal quantifiers implicit. LHS (\u03d5) and RHS (\u03d5) denote the sets of atoms on the LHS and RHS of \u03d5, respectively. LHS (\u03d5) contains, apart from similarity atoms, atoms R1(t1, x\u03041) and R2(t2, x\u03042), which contain all the variables in the MD, including those in the RHS (\u03d5). So, similarity and identity atoms in \u03d5 involve one variable from predicate R1, and one from predicate R2.\nExample 1. Consider Paper(PID ,Title,Year ,CID , JID ,Keyword ,Bl#), a relational predicate representing records for entity Paper. It includes a first attribute for a tuple identifier, and a last indicating the block the tuple (record) has been assigned to. The MD\nPaper(pid1, x1, y1, z1, w1, v1, bl1) \u2227 Paper(pid2, x2, y2, z2, w2, v2, bl2) \u2227 x1 \u2248Title x2 \u2227 y1 = y2 \u2227 z1 = z2 \u2212\u2192 bl1 . = bl2, (4)\ninvolves a similarity relation on the Title attribute, and equality as similarity relation on attributes Year and CID. The MD specifies that, when the conditions expressed in the LHS are satisfied, the two block values have to be made the same, i.e. the two records should be (re)assigned to the same block.\nA dynamic, chase-based semantics for MDs with matching functions (MFs) was introduced in [11], and we briefly summarize it here. Given an initial instance D, the set \u03a3 of MDs is iteratively enforced until they cannot be be applied any further, at which point a resolved instance has been produced.\nIn order to enforce (the RHSs of) MDs, there are binary matching functions (MFs) mA : Dom(A) \u00d7 Dom(A) \u2192 Dom(A); and mA(a, a\u2032) is used to replace two values a, a\u2032 \u2208 Dom(A) that have to be made identical. For example, for an attribute Address, we might have a MF mAddress , such that mAddress(\u201cMainSt., Ottawa\u201d, \u201c25 Main St.\u201d) := \u201c25 MainSt., Ottawa\u201d.\nMFs are idempotent, commutative, and associative, and then induce a partialorder structure \u3008Dom(A), A\u3009, with: a A a\u2032 :\u21d4 mA(a, a\u2032) = a\u2032 [11, 10]. It always holds: a, a\u2032 A mA(a, a\u2032). Actually, the relationship a A a\u2032 can be thought in terms of information contents: a\u2032 is at least as informative as a.9 This partial order allows to define a partial order v on instances [11]. Accordingly, when MDs are applied, a chain of increasingly more informative (or less uncertain) instances is generated: D0 v D1 v \u00b7 \u00b7 \u00b7 v Dclean . In this work, MFs are treated as built-in relations.\n8Similarity symbols can be treated as regular, built-in, binary predicates, but the identity symbol, . =, would be non-classical.\n9Of course, this claim assumes that MFs locally assign an at least as informative value as both of the two input values. MFs are application dependent.\nGiven a database instance D and a set of MDs \u03a3, there may be several resolved instances for D and \u03a3 [11]. However, there is a unique resolved instance if one of the following holds [11, 6]:\n(a) MFs used by \u03a3 are similarity-preserving, i.e., for every a, a\u2032, a\u2032\u2032 : a \u2248 a\u2032 implies a \u2248 mA(a\u2032, a\u2032\u2032). When MDs use similarity-preserving MFs, we also say that the MDs are similarity-preserving.\n(b) \u03a3 is interaction-free, i.e. no attribute (with its predicate) appears both in a RHS and a LHS of MDs in \u03a3.\nFor example, the set \u03a31 = {R[A] \u2248 T [B] \u2192 R[C] . = T [D], T [D] \u2248 S[A] \u2192 T [A] .= S[B]} is not interaction-free due to the presence of attribute T [D]. \u03a32 = {R[A] \u2248 T [B] \u2192 R[C] . = T [D], T [A] \u2248 S[A] \u2192 T [C] . = S[C]} is interaction-free.\n(c) The combination of \u03a3 and the initial instance D is similarity-free attribute intersection (we say it is SFAI), if \u03a3 is interaction-free, or, otherwise, for every pair of interacting MDs \u03d51, \u03d52 in \u03a3, and, for every t1, t2, t3 \u2208 D, it holds LHS (\u03d51) is not true in instance {t1, t2} or LHS (\u03d52) is not true in instance {t2, t3}. Consider, for example, predicate R(A,B,C), the instance D below, and the set \u03a3 of interacting MDs:\n\u03d51 : R [A] \u2248 R [A] \u2212\u2192 R [B] . = R [B] , \u03d52 : R [B] \u2248 R [B] \u2212\u2192 R [C] . = R [C] .\nR(D) A B C t1 a1 b1 c1 t2 a2 b2 c2 t3 a3 b3 c3\nAssume that the only similarities that holds in the data domain U are a1 \u2248A a2 and b1 \u2248B b4, with b4 \u2208 Dom(B) r Adom(D).\nSince \u03d52 is not applicable in D (i.e. there is no pair of tuples making it true), the combination of \u03a3 and D is SFAI. Notice that b1 \u2248B b4 does not matter, because there is no tuple in D with b4 as value for R[B].\nWith general sets of MDs, different orders of MD enforcements may result in different clean instances, because tuple similarities may be broken during the chase with interacting, non-similarity-preserving MDs, without reappearing again [11]. With SFAI combinations, two similar tuples in the original instance D -or becoming similar along a chase sequence- may have the similarities broken in a chase sequence, but they will reappear later on in the same and the other chase sequences. Thus, different orders of MD enforcements cannot lead in the end to different clean instances.\nThe SFAI class had not been investigated before. It is a semantic class, as opposed to syntactic, in that there is a dependency upon the initial instance. See Appendix A for more details on this class.\nThe three classes above have the unique clean instance (UCI) property, i.e. iteratively and exhaustively enforcing them leads to a single clean, stable instance. Even more, in these three cases, the single clean instance can be computed in polynomial time in data, i.e. in polynomial time in |D|, the size of the initial instance, leaving the set of MDs as a fixed, external parameter for the computational problem that here receives database instances as inputs.10\nIn this work, for collective-blocking purposes, we will introduce and use a new class of MDs, that of relational MDs, that extends the class of \u201cclassical\u201d MDs introduced earlier in this section. Actually, the three UCI classes of classical MDs listed above can be extended to relational MDs, and preserving the UCI property (cf. Appendix A for more details).\nRelational MDs, the SFAI class, and the UCI property are all relevant for this work. However, a detailed analysis of them is somehow beyond the scope of this work. For this reason, and in order not to break the natural flow of the presentation, we provide in Appendix A, mainly for reference, some more details about all these subjects."}, {"heading": "2.2. Classification with support-vector models", "text": "The support-vector machines technique (SVM) [48] is a form of kernel-based learning. SVM can be used for classifying vectors in an inner-product vector space V over R. Vectors are classified in two classes, say with labels 0 or 1. The classification model is a hyper-plane in V: vectors are classified depending on the side of the hyperplane they fall.\nThe hyper-plane has to be learned through an algorithm applied to a training set of examples, say E = {(e1, f(e1)), (e2, f(e2)), (e3, f(e3)), . . . , (en, f(en))}. Here, ei \u2208 V, and for the real-valued feature (function) f : f(ei) \u2208 {0, 1}.\nThe SVM algorithm finds an optimal hyperplane, H, in V that separates the two classes in which the training vectors are classified. Hyperplane H has an equation of the form w \u2022x + b, where \u2022 denotes the inner product, x is a vector variable, w is a weight-vector of real values, and b is a real number. Now, a new\n10In data management it is common to measure computational complexity (in our case, time complexity) in terms of the size of the underlying dataset, which is usually much larger than that of any other ingredient, such as a query, a set of integrity constrains, a set of view definitions, etc. If we bring the sizes of the latter into the complexity analysis, we talk of combined complexity [1].\nvector e in V can be classified as positive or negative depending on the side of H it lies. This is determined by computing h(e) := sign(w \u2022 e + b). If h(e) > 0, e belongs to class 1; otherwise, to class 0.\nIt is possible to compute real numbers \u03b11, . . . , \u03b1n, the coefficients of the \u201csupport vectors\u201d, such that the classifier h can be computed through: h(e) = sign( \u2211 i \u03b1i \u00b7 f(ei) \u00b7 ei \u2022 e + b) [28].\nAs Figure 1 shows, in our case, we need to classify pairs of records, that is our vectors are of record-pairs of the form e = \u3008r1, r2\u3009. If h(e) = 1, the classifier returns as output \u3008r1, r2, 1\u3009, meaning that they two records are duplicates (of each other). Otherwise, it returns \u3008r1, r2, 0\u3009, meaning that the records are nonduplicates (of each other). For the moment we do not need more than this about the SVM technique."}, {"heading": "2.3. Collective blocking", "text": "Entity-resolution (and other machine learning tasks) use blocking techniques, to group together input values for further processing. In the case of ER, records that might be duplicates of each other are grouped under a same block, and only records within the same block are compared. Any two records in different blocks will never be declared as duplicates.\nCommonly, a single attribute in records, or a combination of attributes, called a blocking key, is used to split records into blocks. If two records share the same (or have similar) values for the blocking-key attributes, they are put into the same block. For example, we could block employee records according to the name and the city. If two of them share (or have similar) name and city values, they go to the same block. Additional analysis, or the use of a classifier, will eventually determine if they are duplicates or not.\nBlocking keys are rather rigid, and \u201clocal\u201d, in that they are applied to records for a single entity (other entities may have other blocking keys). Their use may cause low recall. For this reason, it may be useful to apply blocking techniques that take advantage of additional semantics and/or domain knowledge. Actually, collective blocking creates blocks for different entities by exploiting the relational relationships between entities. Records for different entities are separately, but simultaneously blocked, in interaction. Accordingly, this approach can be called semantic collective blocking.\nExample 2. Consider two entities, Author and Paper. For each of them, there is a set of records. For Author, they are of the form a = \u3008name, . . . , affiliation, . . . , paper title, . . .\u3009, with {name, affiliation} the blocking key; and for Paper, records are the form p = \u3008title, . . . , author name, . . .\u3009, with title the blocking key.\nWe can block together two Author records on the basis of the similarities of their values for the blocking key, in this case of authors\u2019 names and affiliations. (This blocking policy can be specified by means of an MD of the form (4) in Example 1.) However, if two Author records, say a2,a5, have similar names, but not similar affiliations, they will not be assigned to the same block.\nAn alternative approach, could create and assign blocks of Author records, and also blocks of Paper records, at the same time, separately for each entity, but in an intertwined process. In this case, the same Author records a2,a5, that were assigned to different blocks, may be the authors of papers, represented as Paper records, say p3,p8, resp., which have been already be put in the same block (of papers) on the basis of similarities of paper titles (cf. Figure 2). With this additional information, we might assign a2 and a5 to the same block.\nThe additional knowledge comes in two forms: (a) semantic knowledge, about the relational relationships between records for different entities, in this case, the reference of paper titles appearing in Author records to paper titles in Paper entities, and (b) \u201cprocedural\u201d knowledge that tells us about blocks certain entities have been assigned to. As we will see, MDs allow us to express both, simultaneously. In this case, we will be able to express that \u201cif two papers are in the same block, then the corresponding Author records that have similar author names should be put in the same block too\u201d. So, we are blocking Author and Paper entities, separately, but collectively and in interaction. Similarly, and the other way around, we could block Papers records according to the blocking results for their authors (Author records).\n3. Overview of ERBlox\nA high-level description of the components and workflow of ERBlox is given in Figure 3. In the rest of this section, numbers in boldface refer to the edges in that figure. ERBlox\u2019s main four components are: 1. MD-based collective blocking (path 1,3,5, {6,8}), 2. Classification-model construction (all the tasks up to 12, inclusive), 3. Duplicate detection (continues with edge 13), and 4. MD-based merging (previous path extended with 14,15). All the tasks in the figure, except for the classification model construction (that applying the SVM algorithm), are supported by LogiQL.11\n11The implementation of in-house developed ML algorithms as components of the LogicBlox platform is ongoing work.\nTS\nSimilarity\u00a0 C t ti\nML\u2010 Based\u00a0\u00a0 Classification\nFiles\nT\nImport\nTrained\u00a0 Classifier\n(training)1\n2 4 7 9 10\n11 13\nMD\u2010Based\u00a0 Blocking\nompu a on\nD Similarity\u00a0 Computation\nM D\u00d7D\n3 5\n6\n8\n12\nMD\u2010Based\u00a0 Merging\n14\nER Result\n15\nFigure 3: Overview of ERBlox\nThe initial input data is stored in structured text files, which are initially standardized and free of misspellings, etc. However, there may be duplicates. The general LogiQL program supporting the above workflow contains rules for importing data from the files into the extensions of relational predicates (tables). This is edge 1. This results in a relational database instance T containing the training data (edge 2), and instance D to be subject to ER (edge 3).\nEntity records are represented as relational tuples as shown in Figure 4. However, we will keep referring to them as records, and they will be generally denoted with r, r1, ....\nThe next tasks require similarity computation of pairs of records \u3008r1, r2\u3009 in T and (separately) in D (edges 4 and 5). Similarity computation is based on two-argument similarity functions on the domain of a record attribute, say fi : Dom(Ai) \u00d7 Dom(Ai) \u2192 [0, 1], each of which assigns a numerical value to (the comparison of) two values for attribute Ai, in two different records.\nThese similarity functions, being real-valued functions of the objects under classification, correspond to features in the general context of machine learning. They are considered only for a pre-chosen subset of record attributes. Weight-vectors w(r1, r2) = \u3008\u00b7 \u00b7 \u00b7 , wi(fi(r1[Ai], r2[Ai])), \u00b7 \u00b7 \u00b7 \u3009 are formed by applying predefined weights, wi, on real-valued similarity functions, fi, on a pair of of values for attributes Ai (edges 4 and 5), as in Figure 5. (For more details on similarity computation see Section 4.)\nSome record-pairs in the training dataset T are considered as duplicates and\nothers as non-duplicates, which results (according to path 4,7) in a \u201csimilarityenhanced\u201d training database Ts of tuples of the form \u3008r1, r2, w(r1, r2), L\u3009, with label L \u2208 {0, 1}. Label L indicates if the two records are duplicates (L = 1) or not (L = 0). These labels are consistent with the corresponding weight vectors. The classifier is trained using Ts, leading, through the application of the SVM algorithm, to the classification model (edges 9,10) to be used for ER.\nBlocking is applied to instance D, pre-classifying records into blocks, so that only records in a same block will form input pairs for the trained classification model. Accordingly, two records in a same block may end up as duplicates (of each other) or not, but two records in different blocks will never be duplicates.\nWe assume each record r \u2208 D has two extra, auxiliary attributes: a unique and global (numerical) record identifier (rid) whose value is originally assigned and never changes; and a block number that initially takes the rid as value. This block number is subject to changes.\nFor the records in D, similarity measures are used for blocking (see sub-path 5,8). To decide if two records, r1, r2, go into the same block, the weight-vector w(r1, r2) can be used: it can be read off from it if their values for certain attributes are similar enough or not. However, the similarity computations required for blocking may be different from those involved in the computation of the weight-vectors w(r1, r2), which are related to the classification model. Either way, this similarity information is used by the blocking-matching dependencies, which are pre-declared and domain-dependent.\nBlocking-MDs specify and enforce (through their RHSs) that the blocks (block numbers) of two records have to be made identical. This happens when certain similarities between pairs of attribute values appearing in the LHSs of the MDs hold. For example, (4) is a blocking-MD that requires the computation of similarities of string values for attribute Title. The similarity-atoms on the LHS of a blocking-MD are considered to be true when the similarity values are above thresholds that have been predefined for blocking purposes only.\nExample 3. (ex. 2 cont.) With schema {Author(AID ,Name, PTitle, ABlock), Paper(PID ,PTitle,Venue,PBlock) (including ID and block attributes), the following is a relational MD that captures a blocking policy that is similar to (but\nmore refined than) that in Example 2:\n\u03d5 : Author(t1, x1, y1, bl1) \u2227 Paper(t3, y\u20321, z1, bl4) \u2227 y1 \u2248 y\u20321 \u2227 Author(t2, x2, y2, bl2) \u2227 Paper(t4, y\u20322, z2, bl4) \u2227 y2 \u2248 y\u20322 \u2227\nx1 \u2248 x2 \u2227 y1 \u2248 y2 \u2212\u2192 bl1 . = bl2, (5)\nwith Author-atoms as \u201cleading atoms\u201d (they contain the identified variables on the RHS). It contains similarity comparisons involving attribute values for both relations Author and Paper. It specifies that when the Author-tuple similarities on the LHS hold, and their papers are similar to those in corresponding Papertuples that are in the same block (equality as an implicit similarity is captured by the join variable bl4), then blocks bl1, bl2 have to be made identical. This blocking policy uses relational knowledge (the relationships between Author and Paper tuples), plus the blocking decisions already made about Paper tuples.\nWe can see from (5) that information about classifications in blocks of records for the entity at hand (Author in this case) and for others entities (Paper in this case) may simultaneously appear as conditions in the LHSs of blocking-MDs. Furthermore, blocking-MDs may involve in their LHSs inter-entity similarity conditions, such as y1 \u2248 y\u20321 in (5)). All this is the basis for our \u201csemanticallyenhanced\u201d collective blocking process.\nThe MD-based collective blocking stage (steps 5,8,6) consists in the enforcement of the blocking-MDs on D, which results in database D enhanced with information about the blocks to which the records are assigned. Pairs of records with the same block form candidate duplicate record-pairs.\nWe emphasize that some blocking-MDs, such as (5), are more general than those of the form (2) introduced in [27] or Section 2.1: In their LHSs, they may contain regular database atoms, from more that one relation, that are used to give context to the similarity atoms in the MD, to capture additional relational knowledge. MDs of this kind are called relational MDs, and extend the so-called classical MDs of Section 2.1. (Cf. Appendix A for more details on relational MDs.)\nA unique assignment of blocks to records is obtained after the enforcement of the blocking-MDs. Uniqueness is guaranteed by the properties of the class of MDs we use for blocking. Actually, blocking-MDs will turn out to have the UCI property (cf. Section 2.1). (More details on this are given in Sections 5 and Appendix A.)\nAfter the records have been assigned to blocks, record-pairs \u3008r1, r2\u3009, with r1, r2 in the same block, are considered for the duplicate test. At this point, we proceed as we did for the training data: the weight-vectors w(r1, r2), which represent the record-pairs in the \u201cfeature vector space\u201d, are computed and passed over to the classifier (edges 11,12).12\nThe result of applying the trained ML-based classifier to the record-pairs is a set of triples \u3008r1, r2, 1\u3009 containing records that come from the same block and\n12Similarity computations are kept in appropriate program predicates. So, similarity values computed before blocking can be reused at this stage, or whenever needed.\nare considered to be duplicates. Equivalently, the output is a set M \u2286 D \u00d7D containing pairs of duplicate records (edge 13). The records in pairs in M are merged by enforcing an application-dependent set of (merge-)MDs (edge 14). This set of MDs is different from that used for blocking.\nSince records have kept their rids, we define a \u201csimilarity\u201d predicate \u2248id on the domain of rids as follows: r1[rid ] \u2248id r2[rid ] iff \u3008r1, r2\u3009 \u2208 M , i.e. iff the corresponding records are considered to be duplicates by the classifier. We informally denote r1[rid ] \u2248id r2[rid ] by r1 \u2248 r2. Using this notation, the mergeMDs are usually and informally written in the form: r1 \u2248 r2 \u2192 r1 . = r2. Here, the RHS is a shorthand for r1[A1] . = r2[A1] \u2227 \u00b7 \u00b7 \u00b7 \u2227 r1[Am] . = r2[Am], where A1, . . . , Am are all the record attributes, excluding the first and last, i.e. ignoring the identifier and the block number (cf. Figure 4). Putting all together, merge-MDs take the official form:\nr1[rid ] \u2248id r2[rid ] \u2212\u2192 r1[A1] . = r2[A1] \u2227 \u00b7 \u00b7 \u00b7 \u2227 r1[Am] . = r2[Am]. (6)\nMerging at the attribute level, as required by the RHS, uses the predefined and domain-dependent matching functions mAi .\nAfter applying the merge-MDs, a single duplicate-free instance is obtained from D (edge 15). Uniqueness is guaranteed by the fact that the classes of merge-MDs that we use in our generic approach turn out to be interactionfree. (More details are given in Section 7 and Appendix A. See also the brief discussion in Section 2.1.)\nMore details about the ERBlox system and our approach to ER are found in the subsequent sections."}, {"heading": "4. Datasets and Similarity Computation", "text": "We now describe some aspects of the MAS dataset that are relevant for the description of the ERBlox system components,13 and the way the initial data is processed and created for their use with the LogiQL language of LogicBlox."}, {"heading": "4.1. Data files and relational data", "text": "In the initial, structured data files, entries (non-relational records) for entity Author relation contain author names and their affiliations. The entries for entity Paper contain: paper titles, years of publication, conference IDs, journal IDs, and keywords. Entries for the PaperAuthor relationship between Paper and Author entities contain: paper IDs, author IDs, author names, and their affiliations. The entries for the Journal and Conference entities contain both short names of the publication venue, their full names, and their home pages.\nThe dataset is preprocessed by means of Python scripts, in preparation for proper ERBlox tasks. This is necessary because the data gathering methods\n13We also independently experimented with the DBLP and Cora Citation datasets, but we will concentrate on MAS.\nin general, and for the MAS dataset in particular, are often loosely controlled, resulting in out-of-range values, impossible data combinations, missing values, etc. For example, non-word characters are replaced by blanks, some strings are converted into lower case, etc. Not solving these problems may lead to later execution problems and, in the end, to misleading ER results. This preprocessing produces updated structured data files. As expected, there is no ER at this stage, and in the new files there may be many authors who publish under several variations of their names; also the same paper may appear under slightly different titles, etc. This kind of cleaning will be performed with ERBlox.\nNext, from the data in (the preprocessed) structured files, relational predicates and their extensions are created and computed, by means of a generic Datalog program in LogiQL [3, 31]. For example, these rules are part of the program:\nfile in(x1, x2, x3) \u2212\u2192 string(x1), string(x2), string(x3). (7) lang : physical : filePath[\u2018 file in] = \u201dauthor .csv\u201d. (8)\n+author(id1, x2, x3) \u2190 file in(x1, x2, x3), string : int64:convert [x1] = id1. (9)\nHere, (7) is a predicate schema declaration, in this case of the \u201c file in\u201d predicate with three string-valued attributes. It is used to automatically store the contents extracted from the source file \u201dauthor.csv\u201d, as specified in (8). In LogiQL in general, metadata declarations use \u201c\u2192\u201d. (In LogiQL, each predicate\u2019s schema has to be declared, unless it can be inferred from the rest of the program.) Derivation rules, such as (9), use \u201c\u2190\u201d, as usual in Datalog. It defines the author predicate, and the \u201c+\u201d in the rule head inserts the data into the predicate extension. The rule also makes the first attribute a tuple identifier.\nFigure 6 shows three relational predicates that are created and populated in this way: Author(AID ,Name,Affiliation,Bl#), Paper(PID ,Title,Year ,CID , JID ,Keyword ,Bl#), PaperAuthor(PID ,AID ,Name,Affiliation). The (partial) tables show that there may be missing attribute values."}, {"heading": "4.2. Features and similarity computation", "text": "From the general description of our methodology in Section 3, a crucial component is similarity computation. It is needed for: (a) blocking, and (b) building the classification model. Similarity measures are related to features, which are numerical functions of the data, more precisely of the values of some specially chosen attributes. Feature selection is a fundamental task in machine learning [22, 46]; going in detail into this subject is beyond the scope of this work. Example 4 shows some specific aspects of this task as related to our dataset.\nIn relation to blocking, in order to decide if two records, r1, r2 in D, go into the same block, similarity of values for certain attributes are computed, those that are appear in similarity conditions in the LHSs of blocking-MDs. All is needed is whether they are similar enough or not, which is determined by predefined numerical thresholds.\nFor model building, similarity values are computed to build the weightvectors, w(r1, r2), for records r1, r2 from the training data in T . The numerical values in those vectors depend on the values taken by some selected record attributes (cf. Figure 5).\nExample 4. (ex. 2 cont.) Bibliographic datasets, such as MAS, have been commonly used for evaluation of machine learning techniques, in particular, classification for ER. In our case, the features chosen in our work for the classification of records for entities Paper and Author from the MAS dataset (and the other datasets) correspond to those previously used in [47, 17]. Experiments in [35] show that the chosen features enhance generalization power of the classification model, by reducing over-fitting.\nIn the case of Paper-records, if the \u201cjournal ID\u201d values are null in both records, but not their \u201cconference ID\u201d values, \u201cjournal ID\u201d is not considered for feature computation, because it does not contribute to the recall or precision of the classifier under construction. Similarly, when the \u201cconference ID\u201d values are null. However, the values for \u201cjournal ID\u201d and \u201cconference ID\u201d are replaced by \u201cjournal full name\u201d and \u201cconference full name\u201d values that are found in Conference- and Journal-records, resp. Attributes Title, Year, ConfFullName or JourFullName, and Keyword are chosen for feature computation.\nFor feature computation in the case of Author-records, the Name attribute is split in two, the Fname and Lname attributes, to increase recall and precision of the classifier under construction. Accordingly, features are computed for attributes Fname, Lname and Affiliation.\nOnce the classifier has been built, also weight-vectors, w(r1, r2) are computed as inputs for the classifier, but this time for records from the data under\nclassification (in D).14\nNotice that numerical values, associated to similarities, in a weight-vector w(r1, r2) for r1, r2 under classification, could be used as similarity information for blocking. However, the attributes and features used for blocking may be different from those used for weight-vectors. For example, in our experiments with the MAS dataset, the classification of Author-records is based on attributes Fname, Lname, and Affiliation. For blocking, the latter is reused as such (cf. MD (13) below), but also the combination of Fname and Lname is reused, as attribute Name in MDs (cf. MDs (13) and (15) below).\nThere is a class of well-known and widely applied similarity functions that are used in data cleaning and machine learning [19]. For our application with ERBlox we used three of them, depending on the attribute domains for the MAS dataset. Long-text-valued attributes, in our case, e.g. for the Affiliation attribute, their values are represented as lists of strings. For computing similarities between these kinds of attribute values, the \u201cTF-IDF cosine\u201d measure was used [42]. It assigns low weights to frequent strings and high weights to rare strings. For example, affiliation values usually contain multiple strings, e.g. \u201cCarleton University, School of Computer Science\u201d. Among them, some are frequent, e.g. \u201cSchool\u201d, and others are rare, e.g. \u201cCarleton\u201d.\nFor attributes with \u201cshort\u201d string values, such as author names, \u201cJaroWinkler\u201d similarity was used [34, 51]. This measure counts the characters in common in two strings, even if they are misplaced by a short distance. For example, this measure gives a high similarity value to the pair of first names \u201cZeinab\u201d and \u201cZienab\u201d. In the MAS dataset, there are many author first names and last names presenting this kind of misspellings.\nFor numerical attributes, such as publication year, the \u201cLevenshtein distance\u201d was used [37]. The similarity of two numbers is based on the minimum number of operations required to transform one into the other.\nAs already mentioned in Section 3, these similarity measures are used, but differently, for blocking and the creation and application of the classification algorithm. In the former case, similarity values related to LHSs of blocking-MDs are compared with user-defined thresholds, in essence, making them boolean variables. In the latter case, they are used for computing the similarity vectors, which contain numerical values (in R). Notice that similarity measures are not used beyond the output of the classification algorithm, in particular, not for MD-based record merging.\nSimilarity computation for ERBlox is done through LogiQL-rules that define the similarity functions. In particular, similarity computations are kept in extensions of program-defined predicates. For example, if the similarity value for the pair of values, a1, a2, for attribute Title is above the threshold, a tuple\n14In our experiments, we did not care about null values in records under classification. Learning, inference, and prediction in the presence of missing values are pervasive problems in machine learning and statistical data analysis. Dealing with missing values is beyond the scope of this work.\nTitle-Sim(a1, a2) is created by the program."}, {"heading": "5. MD-Based Collective Blocking", "text": "As described in Section 3, the Block attribute, Bl , in records takes integer numerical values; and before the blocking process starts (or blocking-MDs are enforced), each record in the instance D has a unique block number that coincides with its rid. Blocking policies are specified by blocking-MDs, all of which use the same matching function for identity enforcement, given by:\nFor i, j \u2208 N, with j \u2264 i, mBl(i, j) := i. (10)\nA blocking MD that identifies block numbers (i.e. makes them identical) in two records (tuples) for database relation R (cf. Figure 4) takes the form:\nR(x\u03041, bl1) \u2227 R(x\u03042, bl2) \u2227 \u03c8(x\u03043) \u2212\u2192 bl1 . = bl2. (11)\nHere, bl1, bl2 are variables for block numbers, R is a database predicate (representing an entity), the lists of variables x\u03041, x\u03042 stand for all the attributes in R but Bl#, for which variables bl i are used. The MD in (11) is relational when formula \u03c8 in it is a conjunction of relational atoms plus comparison atoms via similarity predicates; including implicit equalities of block numbers (but not \u2248- similarities between block numbers). The variables in \u03c8(x\u03043) may appear among those in x\u03041, x\u03042 (in R) or in another database predicate or in a similarity atom. We assume that (x\u03041 \u222a x\u03042) \u2229 x\u03043 6= \u2205. (Cf. Appendix A for more details on relational MDs.)\nAn example is the MD in (5), where the leading R1, R2-atoms are Author tuples, the extra conjunction contains Paper atoms, non-block-similarities, and an implicit equality of blocks through the shared use of variable bl4. There, \u03c8 is Paper(t3, y \u2032 1, z1, bl4) \u2227 y1 \u2248 y\u20321 \u2227 Paper(t4, y\u20322, z2, bl4) \u2227 y2 \u2248 y\u20322 \u2227 x1 \u2248 x2 \u2227 y1 \u2248 y2.\nExample 5. These are some of the blocking-MDs used with the MAS dataset. The first two are classical blocking-MDs, and the last two are properly relational blocking-MDs:\nPaper(pid1, x1, y1, z1, w1, v1, bl1) \u2227 Paper(pid2, x2, y2, z2, w2, v2, bl2) \u2227 (12) x1 \u2248Title x2 \u2227 y1 = y2 \u2227 z1 = z2 \u2212\u2192 bl1 . = bl2. Author(aid1, x1, y1, bl1) \u2227 Author(aid2, x2, y2, bl2) \u2227 (13) x1 \u2248Name x2 \u2227 y1 \u2248Aff y2 \u2212\u2192 bl1 . = bl2.\nPaper(pid1, x1, y1, z1, w1, v1, bl1) \u2227 Paper(pid2, x2, y2, z2, w2, v2, bl2) \u2227 (14) PaperAuthor(pid1, aid1, x \u2032 1, y \u2032 1) \u2227 PaperAuthor(pid2, aid2, x\u20322, y\u20322) \u2227\nAuthor(aid1, x \u2032 1, y \u2032 1, bl3) \u2227Author(aid2, x\u20322, y\u20322, bl3) \u2227 x1 \u2248Title x2\n\u2212\u2192 bl1 . = bl2.\nAuthor(aid1, x1, y1, bl1) \u2227Author(aid2, x2, y2, bl2) \u2227 x1 \u2248Name x2 \u2227 (15) PaperAuthor(pid1, aid1, x1, y1) \u2227 PaperAuthor(pid2, aid2, x2, y2) \u2227 Paper(pid1, x \u2032 1, y \u2032 1, z \u2032 1, w \u2032 1, v \u2032 1, bl3) \u2227 Paper(pid2, x\u20322, y\u20322, z\u20322, w\u20322, v\u20322, bl3)\n\u2212\u2192 bl1 . = bl2.\nIn informal terms, (12) requires that, for every two Paper entities p1,p2 for which the values for attribute Title are similar, and with the same publication year and conference ID, the values for attribute Bl# must be made identical. According to (13), whenever there are similar values for name and affiliation in Author, the corresponding authors should go into the same block.\nThe relational blocking-MDs in (14) and (15) collectively block Paper and Author entities. According to (14), a blocking-MD for Paper, if two authors are in the same block, their papers p1, p2 having similar titles must be in the same block too. Notice that if papers p1 and p2 have similar titles, but they do not have same publication year or conference ID, we cannot block them together using (12) alone. The blocking-MD (15) for Author is similar to that discussed in Example 3.\nFor the application-dependent set, \u03a3Bl , of blocking-MDs we adopt the chasebased semantics [11], which may lead, in general, to several, alternative final instances. In each of them, every record is assigned to a unique block, but now records may share block numbers, which is interpreted as belonging to the same block. In principle, there might be two final instances where the same pair of records is put in the same block in one of them, but not in the other one. However, with a set of the relational blocking-MDs of the form (11) acting on an initial instance D (created with LogicBlox as described above), the chasebased enforcement of the MDs results in a single, final instance, DBl . This is because the combination of the blocking-MDs with the initial instance D turns out to belong to the SFAI class, which has the UCI property (cf. Section 2.1 and Appendix A).\nThat the initial instance and the blocking-MDs form a SFAI combination is easy to see. In fact, initially the block numbers in tuples (or records) are all different, they are the same as their tids. Now, the only relevant attributes in records (for SFAI membership) are \u201cblock attributes\u201d, those appearing in RHSs of blocking-MDs (cf. (11)). In the LHSs of blocking-MDs they may appear only in implicit equality atoms. Since all initial block numbers in D are different, no relevant similarity holds in D.\nDue to the SFAI property of blocking-MDs in combination with the initial instance, MD enforcement leads to a single instance that can be computed in polynomial time in data, which gives us the hope to use a computationally well-behaved extension of plain Datalog for MD enforcement (and blocking). It turns out that the representation and enforcement of these MDs can be done by means of Datalog with stratified negation [15, 1], which is supported by LogiQL. Stratified Datalog programs have a unique stable model, which can be computed in a bottom-up manner in polynomial time in the size of the\nextensional database.15\nIn LogiQL, blocking-MDs take the form as Datalog rules:\nR[X\u03041]=Bl2, R[X\u03042]=Bl2 \u2190\u2212 R[X\u03041] = Bl1, R[X\u03042] = Bl2, (16) \u03c8(X\u03043), Bl1 < Bl2,\nsubject to the same conditions as for (11). The condition Bl1 < Bl2 in the rule body corresponds to the use of the MF mBl in (10).\nAn atom of the form R[X\u0304]=Bl not only declares Bl as an attribute value for R, but also that predicate R is functional on X\u0304 [3]: Each record in R can have only one block number.\nIn addition to the blocking-MDs, we need some auxiliary rules, which we introduce and discuss next. Given an initial instance D and a set of blocking-MDs \u03a3Bl , the LogiQL-program \u03a0Bl(D) that specifies MD-based collective blocking contains the following rules:\n1. For every atom R(rid , x\u0304, bl) \u2208 D, the fact R[rid , x\u0304] = bl . That is, initially, the block number, bl , is functionally assigned the value rid .\n2. Facts of the form A-Sim(a1, a2), where a1, a2 \u2208 Dom(A), the finite attribute domain of an attribute A. They state that the two values are similar, which is determined by similarity computation. (Cf. Section 4.2 for more on similarity computation.)\n3. Rules for the blocking-MDs, as in (16).\n4. Rules specifying older versions of entity records (in relation R) after MDenforcement:\nR-OldVer(r, x\u0304, bl1) \u2190\u2212 R[r, x\u0304] = bl1, R[r, x\u0304] = bl2, bl1 < bl2.\nHere, variable r stands for the rid. Since for each rid, r, there could be several atoms of the form R[r, x\u0304] = bl , corresponding to the evolution of the record identified by r through an MD-based chase sequence, the rule specifies as old those versions of the record with a block number that is smaller than the last one obtained for it.\n5. Rules that collect the records\u2019 latest versions, to form blocks:\nR-Block [r, x\u0304] = bl \u2190\u2212 R[r, x\u0304] = bl , not R-OldVer(r, x\u0304, bl).\nThe rule collects R-records that are not old versions.16\n15General sets of MDs can be specified and enforced by means of disjunctive, stratified answer set programs, with the possibly multiple resolved instances corresponding to the stable models of the program [4]. These programs can be specialized, via an automated rewriting mechanism, for the SFAI case, obtaining residual programs in Datalog with stratified negation [6].\n16LogiQL, uses \u201c!\u201d instead of not for Datalog negation [3].\nProgram \u03a0Bl(D) as above is a Datalog program with stratified negation (there is no recursion through negation). In computational terms, this means that the program computes old version of records (using negation), and next definitive blocks are computed. As expected from the SFAI property of blockingMDs in combination with the initial instance, the program has and computes a single model, in polynomial time in the size of the initial instance. From it, the final block numbers of records can be read off.\nExample 6. (ex. 5 cont.) We consider only blocking-MDs (12) and (14). The portion of \u03a0Bl(D) that does the blocking of records for the Paper entity has the following rules (we follow the numbering used in the generic program):\n2. Facts such as: Title-Sim(\u201cIllness entities in West Africa\u201d, \u201cIllness entities in Africa\u201d). Title-Sim(\u201cDLR Simulation Environment m3 \u201d, \u201cDLR Simulation\nEnvironment\u201d).\n3. Paper [pid1, x1, y1, z1, w1, v1] = bl2,Paper [pid2, x2, y2, z2, w2, v2] = bl2 \u2190 Paper [pid1, x1, y1, z1, w1, v1] = bl1,Paper [pid2, x2, y2, z2, w2, v2] = bl2,\nTitle-Sim(x1, x2), y1 = y2, z1 = z2, bl1 < bl2.\nPaper [pid1, x1, y1, z1, w1, v1] = bl2,Paper [pid2, x2, y2, z2, w2, v2] = bl2 \u2190 Paper [pid1, x1, y1, z1, w1, v1] = bl1,Paper [pid2, x2, y2, z2, w2, v2] = bl2,\nTitle-Sim(x1, x2),PaperAuthor(pid1, aid1, x \u2032 1, y \u2032 1), bl1 < bl2,\nPaperAuthor(pid2, aid2, x \u2032 2, y \u2032 2),Author [aid1, x \u2032 1, y \u2032 1] = bl3,\nAuthor [aid2, x \u2032 2, y \u2032 2] = bl3.\n4. Paper-OldVer(pid , x, y, z, w, v, bl1) \u2190 Paper [pid , x, y, z, w, v] = bl1, Paper [pid , x, y, z, w, v] = bl2, bl1 < bl2.\n5. Paper-Block [pid , x, y, z, w, v] = bl \u2190 Paper [pid , x, y, z, w, v] = bl , not Paper-OldVer(pid , x, y, z, w, v, bl).\nBy restricting the model of the program to attributes PID and Block# of predicate Paper-Block , we obtain blocks: {123, 205}, {195, 769}, . . .. That is, the papers with pids 123 and 205 are blocked together; similarly for those with pids 195 and 769, etc.\nThe execution of the blocking-program \u03a0Bl(D) will return in the end, for each entity-relation R a list of subsets of the extension of R in D. These subsets are blocks of R-records. Pairs of records in a same block will be inputs to the classification model, which has to be independently constructed first."}, {"heading": "6. Classification Model Construction", "text": "For both, the classification model construction and duplicate detection that uses it, weight-vectors for record-pairs have to be computed. The numerical values for these vectors come from features related to similarity comparisons\nbetween attribute values for two records r1, r2. Only a subset of record attributes are chosen, those attributes that have strong discriminatory power, to achieve maximum classification recall and precision (cf. Section 4.2).\nThe input to the SVM algorithm (that will produce the classification model) is a set of tuples of the form \u3008r1, r2, w(r1, r2), L\u3009, where r1, r2 are records (for the same entity) in the training dataset T , L \u2208 {0, 1}, and w(r1, r2) is the computed weight-vector for the record-pair. In the LogiQL program, that input uses two defined predicates. Predicate TrainLabel has two arguments: One for pairs of rids, r1r2, together, which is called \u201cthe vector id\u201d for vector w(r1, r2) = \u3008w1, . . . , wn\u3009, and another to represents label L associated to w(r1, r2). Predicate TrainVector contains one argument for vector ids, and n arguments to represent entries wi in the weight-vectors w(r1, r2).\nSeveral ML techniques are accessible from (or within) the LogicBlox platform, through the BloxMLPack library that provides a generic Datalog interface. Then, ERBlox can call a SVM-based classification model constructor, through the general LogiQL program.\nIn particular, the BloxMLPack wraps calls to the machine learning library in a predicate-to-predicate mapping called mlpack, and manages marshalling the inputs and outputs to the machine learning library from/to LogiQL predicates. This is done via special rules in LogiQL that come in two modes: the learning mode (when a model is being learned, in our case, a SVM classification model), and the evaluation mode (when the model is applied, for record-pair classification in our case) [36, 3]. We do not give here the formal syntax and semantics for these rules, but just the gist by means of an example.\nAssume that we want to train a SVM-model for Author-record classification. For invoking SVM from LogiQL, a relation InputMatrix [j, i] is needed. It contains tabular data where each column (j) represents a feature of Author-records, while each row i represents a vector id for which the tuple TrainVector(i, w1, w2, w3) exists. So, InputMatrix [j, i] represents the value of the feature j in the weight-vector i. The following rules are used in LogiQl to populate relation InputMatrix : (They involve predicates Feature(\u201cFname\u201d), Feature(\u201cLname\u201d), and Feature(\u201cAffiliation\u201d), associated to the three chosen attributes for Authorrecords. They appear in quotes, because they are constants, i.e. attribute names.)\nInputMatrix[\u201cFname\u201d, i] = w1, InputMatrix[\u201cLname\u201d, i] = w2,\nInputMatrix[\u201cAffiliation\u201d, i] = w3 \u2190\u2212 TrainVector(i, w1, w2, w3), Feature(\u201cFname\u201d), Feature(\u201cLname\u201d), Feature(\u201cAffiliation\u201d).\nThe following learning rule learns a SVM model for Author, and stores the resulting model in the predicate SVMsModel(model):\nSVMsModel(m) \u2190\u2212 mlpack m = SVM (p\u0304), train InputMatrix [j, i] = v, Feature(j),TrainLabel(i, l).\nHere, the head of the rule defines a predicate where the ML algorithm outputs its results, while the body of the rule lists a collection of predicates that supplies\ndata for the ML algorithm. In the above rule, the required parameters p\u0304 for running the SVM algorithm are specified by the user. The above rule is in the training mode."}, {"heading": "7. Duplicate Detection and MD-Based Merging", "text": "The input to the trained classifier is a set of tuples of the form \u3008r1, r2, w(r1, r2)\u3009, where r1, r2 are record (ids) in a same block for a relation R, and w(r1, r2) is the weight-vector for the record-pair \u3008r1, r2\u3009. The output is a set of triples of the form \u3008r1, r2, 1\u3009 or \u3008r1, r2, 0\u3009.\nUsing LogiQL rules, the triples \u3008r1, r2, 1\u3009 form the extension of a defined predicate R-Duplicate.\nExample 7. (ex. 6 and 4 cont.) Considering the previous Paper-records, the input to the trained classifier consists of: \u3008123, 205, w(123, 205)\u3009, with w(123, 205) = [0.8, 1.0, 1.0, 0.7]; and \u3008195, 769, w(195, 769)\u3009, with w(195, 769) = [0.93, 1.0, 1.0, 0.5].\nIn this case, the SVM-based classifier returns \u3008[0.8, 1.0, 1.0, 0.7], 1\u3009 and \u3008[0.93, 1.0, 1.0, 0.5], 1\u3009. Accordingly, the tuples Paper -Duplicate(123, 205) and Paper -Duplicate(195, 769) are created.\nThe extensions of predicates R-Duplicate will be the input to the merging process.\nRecord merging is carried out through the enforcement of merge-MDs, as described in Section 3, where we showed that they form an interaction-free set. Consequently, there is a single instance resulting from their enforcement. These MDs use application-dependent matching functions (MFs), and can be expressed by means of LogiQL rules. Actually, the generic merge-MDs in (6) can be expressed in their Datalog versions by means of the above mentioned R-Duplicate predicates. The RHSs of MDs in (6) have to be expressed in terms of MFs, mAi . All these become ingredients of a Datalog merge-program \u03a0 M .\nExample 8. (ex. 4 cont.) Duplicate Paper-records are merged by enforcing the merge-MD:\nPaper [pid1] \u2248 Paper [pid2] \u2212\u2192 Paper [Title,Year ,CID ,Keyword ] . =\nPaper [Title, Year ,CID , Keyword ].\nThe general LogiQL program, \u03a0M , for MD-based merging contains rules as in 1.-4. below:\n1. The ground atoms of the form R-Duplicate(r1, r2) mentioned above, and those representing MFs, of the form mA(a1, a2) = a3.\n2. For an MD R[r1] \u2248 R[r2] \u2212\u2192 R[r\u03041] . = R[r\u03042], the rules:\nR(r1, x\u03043), R(r2, x\u03043) \u2190\u2212 R-Duplicate(r1, r2), R(r1, x\u03041), R(r2, x\u03042), m(x\u03041, x\u03042) = x\u03043,\nwhere x\u03041, x\u03042, x\u03043 stand for all attributes of relation R, except for the rid and the block number (block numbers play no role in merging). m(x\u03041, x\u03042) = x\u03043 is just a shorthand to denote the componentwise application of m individual MFs mAi (cf. (6)).\nAt the end of the iterative application of these rules, there may be several tuples with different rids but identical \u201ctails\u201d. Only one of those tuples is kept in the resolved instance.\n3. As for the blocking-program \u03a0Bl(D) of Section 5, we need rules specifying the old versions of a record:\nR-OldVer(r1, x\u03041) \u2190\u2212 R(r1, x\u03041), R(r1, x\u03042), x\u03041 \u227a x\u03042,\nwhere x\u03041 stands for all attributes other than rid and the block number; and x\u03041 \u227a x\u03042 means componentwise comparison of values according to the partial orders defined by the MFs. (Recall from Section 2.1, that each application of a MF makes us grow in the information lattice: the highest values are the newest values.)\n4. Finally, we introduce rules to collect, in a new predicate R-ER, the latest version of each record, to build the final resolved instance:\nR-ER(r, x\u0304) \u2190 R(r, x\u0304), not R-OldVer(r, x\u0304).\nThis is a stratified Datalog program that computes a single resolved instance in polynomial time in the size of the extensional database, in this case formed by the contents of relations R-Duplicate and D.17\nIn our application to bibliographic datasets, we used as matching functions \u201cthe union case\u201d [10], which was investigated in detail in [11] in terms of MDs. The idea is to treat attribute values as objects, i.e. sets of pairs attribute/value. For example, the address \u201c250 Hamilton Str., Peterbrook, K2J5G3\u201d could be represented as the set {\u3008number, 250\u3009, \u3008stName,Hamilton Str .\u3009, \u3008city,Peterbrook\u3009, \u3008areaCode,K2J5G3 \u3009}. When two values of this kind are merged, their union is computed. For example, the two strings \u201c250 Hamilton Str., K2J5G3\u201d and \u201cHamilton Str., Peterbook\u201d, represented as objects, are merged into \u201c250 Hamilton Str., Peterbook, K2J5G3\u201d [11]. This generic merge function has the advantage that, in essence, the older pieces of information are preserved, and combined into a more complete value. In this example, the string \u201c250 Hamilton Str., Peterbook, K2J5G3\u201d is more informative than the two strings initial strings, \u201c250 Hamilton Str., K2J5G3\u201d and \u201cHamilton Str., Peterbook\u201d. In the case of two alternative values, the two versions will be kept in the union, which may require some sort of domain-dependent postprocessing, essentially making choices and possibly edits. In any case, working with the union case for matching\n17As with the blocking-programs, the merge-programs can be obtained particularizing the general programs in [4] to the case of interaction-free MDs [6].\ndependencies is good enough for our purposes, namely to compare traditional techniques with ours.\nWe point out that MD-based merging takes care of \u201ctransitive cases\u201d produced by the classifier. More precisely, if it returns \u3008r1, r2, 1\u3009 and \u3008r2, r3, 1\u3009, but not \u3008r1, r3, 1\u3009, we still merge r1, r3 (even when r1 \u2248 r3 does not hold). Indeed, if MD-enforcement first merges r1, r2 into the same record, the similarity between r2 and r3 still holds (it was pre-computed and stored, and not destroyed by the updating of attribute values of r2). Then, the merge-MD will be applied to r3 and the new version of r2. Iteratively, r1, r2, r3 will end up having the same attribute values (except for the rid).18\nThere might be applications where we do not want this form of full entity resolution triggered by transitivity. If that is the case, we could use semantic constraints on the ER result (or process). Actually, negative rules have been proposed in [49], and discussed in [4] in the context of general answer set programs for MD-based ER. However, the introduction of constraints into Datalog changes the entire picture. Under a common approach, if the intended model of the program does not satisfy the constraint, it is rejected. This is not particularly appealing from the application point of view. An alternative is to transform constraints into non-stratified program rules, which would take us in general to the realm of ASPs [14]. In any case, developing this case in full is outside the scope of this work."}, {"heading": "8. Experimental Results", "text": "In comparison with standard blocking (SB) techniques, our experiments with the MAS dataset show that our approach to ER, in particular, through the use of semantically rich matching dependencies for blocking result in lower reduction ratio for blocking, and higher recall and precision for classification. These are positive results that can also be observed in the experimental results with the DBLP and Cora Citation datasets. Cf. Figures 7, 9, and 8 (more details follow below).\nWe considered three different blocking techniques, shown, respectively, in the sets of columns in Figure 7: (a) Standard Blocking (SB), (b) MD-based Standard Blocking (MDSB), and (c) MD-based Collective Blocking (MDCB), which we now describe:\n(a) According to SB, records are clustered into a same block when they share the identical values for blocking keys [33].\n(b) MDSB generalizes standard blocking through the use of blocking-MDs that consider on the LHS exactly the same attributes (actually, keys) as in SB. However, for some of the attributes, equality is replaced by similarity,\n18Notice that there is certain similarity with the argument around the SFAI case of MDs in Section 5. This not a coincidence: non-interacting MDs form a case of SFAI, for any initial instance.\nReduction ratio refers to the record-blocking task of ER, and is defined by 1 \u2212 SN , where S is the number of candidate duplicate record-pairs produced by the blocking technique, and N is the total number of possible candidate\nduplicate record-pairs in the entire dataset. If there are n records for an entity, then N = n\u00d7 n for that entity.\nReduction ratio is the relative reduction in the number of candidate duplicate record-pairs to be compared. The higher the reduction ratio, the fewer the candidate record-pairs that are generated, but the quality of the generated candidate record-pairs is not considered [18].\nThat the reduction ratio decreases from left to right in Figure 7 shows that the use of blocking-MDs increasingly captures more potential record-pairs comparisons that would be missed otherwise.\nRecall and precision are measures of goodness of the result of the classification task [18], in this case, of record-pairs as duplicate (i.e. containing duplicates of each other) or not. Recall is defined by true positives/(true positives + false negatives), whereas precision is defined by true positives/ (true positives + false positives).\nHigher recall means more true (i.e. duplicate) candidate record-pairs have been actually found; and higher precision, that more of the retrieved candidate duplicate record-pairs are actually true. Since we want high recall and precision, then we prefer a blocking technique that generates a small number of candidates for false positives and false negatives.\nOur experiments focused mainly on the recall and precision of the overall results after classification (and before merging). They indirectly allows for the evaluation of the blocking techniques, as well. Actually, recall measures the effectiveness a blocking technique through non-dismissal of true candidate duplicate record-pairs. Similarly, a high precision reflects that the blocking technique generates mostly true candidate duplicate record-pairs. Inversely, a low precision shows a large number of non-duplicate record-pairs is also considered, through blocking, as candidate duplicate record-pairs. We can see that it becomes crucial to verify that filtering out record-pairs by a particular blocking technique does not affect the quality of the results obtained after classification.\nAll the three above mentioned measures were computed by cross-validation, on the basis of the training data. Approximately 70% of the training data was used for training, and the other 30%, for testing. The MAS dataset includes 250K authors, 2.5M papers, and a training set. For the authors dataset, the\ntraining and test sets contain 3,739 and 2,244 cases (author ids), respectively. Figures 7, 9 and 8 show the comparative performances of ERBlox with the three forms of blocking mentioned above, for three different datasets. In all cases, the same SVM technique was applied.\nIn our concrete application domain, standard blocking based on key-equalities of Paper-records of the MAS dataset used attributes Title, Publication Year, and Conference ID, together, as one blocking key. The MD-version of this key, for MD-based standard blocking and MD-based collective blocking, is the MD (12) in Example 5. According to it, if two records have similar titles, with the same publication year and conference ID, they have the same block numbers. Deciding which attribute equalities become similarities is domain-dependent.\nStandard blocking based on key-equalities has higher reduction ratio than MD-based standard blocking, i.e. the former generates fewer candidate duplicate record-pairs. Standard blocking also leads to higher precision than MD-based standard blocking, i.e. we can trust more candidate duplicate record-pairs judgements obtained via standard blocking. However, this standard blocking is very conservative, and has a very low rate of recall, i.e. many of the true candidate duplicate record-pairs are not identified as such. All this makes sense since with standard blocking based we only consider equalities of blocking keys, not similarities.\nPrecision and recall of MD-based collective blocking are higher than the two standard blocking techniques. This emphasizes the importance of MDs that support collective blocking, and shows that blocking based on string similarity alone fails to capture the semantic interrelationships that naturally hold in the data. On the other side, MD-based collective blocking has lower reduction ratio than standard MD-based blocking, which may lead to better ER results, but may impact computational cost: larger blocks may be produced, and then, more candidate duplicate record-pairs become inputs for the classifier. In blocking, this is a common tradeoff that needs to be considered [18].\nOverall, the quality of MD-based collective blocking dominates standard blocking, both in its key-based and MD-based forms, for the three datasets."}, {"heading": "9. Related Work", "text": "An unsupervised clustering-based approach to collective deduplication is proposed in [12]. While traditional deduplication techniques assume that only similarities between attribute values are available, in relational data the entities are assumed to have additional relational information that can be used to improve the deduplication process. This approach falls in the context of relational learning [30]. More precisely, in [12], a relationship graph is built whose nodes are the entities (records), and edges indicate entities which co-occur. The graph supports the propagation of similarity information to related entities. In particular, the similarity between two nodes is calculated as the weighted sum of the attribute-value similarity and their relational similarity (as captured through the graph). Experimental results [12] show that this form of collective deduplication outperforms traditional deduplication.\nThe approach to ER in [12] could be seen as implicitly involving collective blocking, where relationships between entities and similarities between attribute values are used to create the blocks of records. However, this form of collective blocking does not take advantage of a declarative, logic-based semantics. In contrast, a relationship graph is used for collective deduplication. In our case, semantic information for this task is captured by matching dependencies. Most importantly, the main focus of our approach to ER is MD-based collective blocking. For this reason, our experiments compare this approach with other blocking techniques. A comparison of our whole approach to ER with other (whole) collective approaches to ER, such as that in [12] has to be left for future research. However, the results of such a comparison may not be very eloquent, because our approach is based on crucial intermediate techniques, such as the use of SVM for the classification task, which is somehow orthogonal to the blocking approach.\nDedupalog, a declarative approach to collective entity deduplication in the presence of constraints, is proposed in [2]. Constraints are represented by a form of Datalog language. The focus of this work is unsupervised clustering, where constraints are an additional element. Clusters of records make their elements candidates for merging, but blocking per se or the actual merging are not main objectives. However, this kind of clustering could be interpreted as a form of blocking. The additional use of constraints could be seen as a form of collective clustering. In [2], equality-generating dependencies were used as hard constraints, and clustering-rules as weak constraints.\nOur approach can also be seen as a form of relational learning. However, in our case, the semantic relational information (constraints) are, in some sense, implicitly captured through matching dependencies. Their semantics is nonclassical (it is chase-based as seen in Section 2.1), and involves directly the blocking or merging processes, as opposed to having higher-level logical constraints \u201ccompiled\u201d into them. In our case, the proper learning part of the\nprocess, i.e. classification-model learning via SVM, is supervised,19 but it does not use any kind of additional relational knowledge. In this regard, it is worth pointing out to quite recent research proposing supervised ML-techniques for classification that involve semantic knowledge in the form of logical formulas in kernels for kernel-based methods (such as SVM) [23].\nVarious blocking techniques have been proposed, investigated and applied. See [9, 18, 24, 39, 45] for comprehensive surveys and comparative studies. To the best of our knowledge, existing approaches to blocking are inflexible and limited in that they: (a) allow blocking on only single entity types, in isolation from other entity types, or (b) do not take advantage of valuable domain or semantic knowledge. Possible exceptions are [38, 41]. Collective blocking in [38] disregards blocking keys and creates blocks by considering exclusively the relationships between entities. The relationships correspond to links in a graph connecting entities, and blocks are formed by groping together entities within neighborhoods with a predefined (path) \u201cdiameter\u201d. Under this approach, in contrast with ours (cf. Example 2), relationships are not declarative, and blocking decisions on one entity do not have a direct, explicit impact on blocking decisions to be made on another related entity.\nIn [41], similarity of blocking keys and relational relationships are considered for blocking in the context of identification of duplicates (not the merging). However, the semantics of relational relationships (or closeness) between blocking keys and entities is not fully developed."}, {"heading": "10. Conclusions", "text": "We have shown that matching dependencies, a new class of semantic constraints for data quality and cleaning, can be profitably integrated with traditional ML-methods, in our case for developing classification models for entity resolution. These dependencies play a role not only in their intended goal, that of merging duplicate representations, but also in the record-blocking process that precedes the proper learning task. At that stage they declaratively capture semantic information that can be used to enrich the blocking activity.\nMDs declaration and enforcement, data processing in general, and machine learning can all be integrated using the LogiQL language. Actually, all the data extraction, movement and transformation tasks are carried out via LogiQL, a form of extended Datalog supported by the LogicBlox platform.\nIn this regards it is interesting to mention that Datalog has been around since the early 80s, as a declarative and executable rule-based language for relational databases. It was used mostly in DB research, until recently. In the last few years Datalog has experienced a revival, and many new applications have been found.\nLogiQL, in particular, is being extended in such a way it can smoothly interact with optimization and machine learning algorithms, on top of a single\n19We refer to [35] for a discussion on supervised vs. unsupervised approaches.\n46 \u2022 Datalog enables declarative and executable specifications\nof data-related domains\nAn extension of relational algebra/calculus/databases\n\u2022 LogicQL is being extended with interaction with optimization\nand machine learning packages and systems!\nplatform. Data for optimization and ML problems stored as \u201cextensions\u201d for a relational database (that is a component of LogicBlox), and Datalog predicates. The results of those algorithms can be automatically stored in existing database predicates or newly defined Datalog predicates, for additional computations or query answering. Currently new ML methods are being implemented as components of the LogicBlox system (cf. Figure 10).\nOur work can be extended in several directions, some of which have been mentioned in previous sections. A most interesting extension would consider the use of more expressive blocking-MDs than those of the form 11. Actually, they could have in their RHSs attributes other than Bl#, the block attributes. As a consequence, blocking-MDs, together with making block numbers identical, would make identical pairs of application-dependent values for some other attributes. Doing this would refine the blocking process itself (modifying the data for the next applications of blocking-MDs), but would also prepare the data for the next task, that of classification for entity resolution.\nAcknowledgments: Part of this research was funded by NSERC Discovery Grant #250279-2011, and the NSERC Strategic Network on Business Intelligence (BIN). Z. Bahmani and L. Bertossi are very much grateful for the support from LogicBlox during their internship and sabbatical visit."}, {"heading": "Appendix A. Relational MDs and the UCI Property", "text": "Here, we formally extend the class of matching dependencies (MDs) introduced in Section 2.1, which we will call classical MDs, to the larger class of relational MDs. This extension is motivated by the application of MDs to blocking for entity resolution, but applications can be easily foreseen in other areas where declarative relational knowledge may be useful in combination with matching and merging.\nWe also identify classes of relational MDs for which a single clean instance exists, no matter how the MDs are enforced, that can be computed through the chase procedure in polynomial time in the size of the database on which the MDs are enforced. We say that the MDs (in some cases in combination with an initial instance) have the unique clean instance property (UCI property). More details can be found in [11, 6, 7].\nDefinition 1. Given a relational schema R, a relational MD is a formula of the form:\n\u03d5 : \u2200t1t2t\u03043 \u2200y1y2x\u03041x\u03042x\u0304 (R1(t1, y1, x\u03041) \u2227 R2(t2, y2, x\u03042) \u2227 \u03c8(t\u03043, z\u0304) \u2212\u2192 y1 . = y2). (A.1)\nHere, R1, R2 \u2208 R, the x\u0304i, etc. are lists of variables, and the yi are single variables, the ti are tid variables, and the t\u0304i are lists of tid variables. R1(t1, y1, x\u03041),\nR2(t2, y2, x\u03042), are the leading atoms. Formula \u03c8(t\u03043, z\u0304) is a conjunction of similarity atoms and relational atoms (with predicates inR), with z\u0304\u2229(x\u03041\u222a{y1}) 6= \u2205, z\u0304 \u2229 (x\u03042 \u222a {y2}) 6= \u2205.\nIt is worth comparing classical MDs in (3) with this extended form. Here, the arguments in the relational part of the MD, namely in \u03c8(t\u03043, z\u0304), interact via variables in common (joins) with the arguments in the leading atoms.\nExample 9. (ex. 2 cont.) For schema Author(Name,Aff ,PapTitle,Bl#), Paper(PTitle,Kwd ,Venue,Bl#), the following is a (properly) relational MD:\n\u03d5 : Author(t1, x1, y1, p1, bl1) \u2227 Author(t2, x2, y2, p2, bl2) \u2227 x1 \u2248 x2 \u2227 Paper(t3, p \u2032 1, z1, w1, bl4) \u2227 Paper(t4, p\u20322, z2, w2, bl4) \u2227 p1 \u2248 p\u20321 \u2227 p2 \u2248 p\u20322\n\u2212\u2192 bl1 . = bl2. (A.2)\nHere, the leading atoms are underlined. They contain the two variables that appear in the identification atom on the RHS. Notice that there is an implicit similarity atom (an equality) represented by the use of the shared (join) variable bl4.\nThe chase-based semantics developed for classical MDs can be applied to relational MDs, without any relevant change: the new relational conditions on the RHSs have to be made true when MDs are enforced.\nOn the classical side of MDs (cf. Section 2.1), two special classes of MDs were identified: similarity-preserving MDs, and interaction-free (IF) MDs. They have the UCI property. On the relational side of MDs, similarity-preserving MDs (i.e. that use similarity-preserving matching functions) are clearly UCI, because only new additional conditions have to be verified before enforcing an MD. We proceed now to generalize the interaction-free class to the relational case, and prepare the ground for introducing a new class of relational MDs, the SFAI class.\nDefinition 2. (a) For a relational MD \u03d5, ALHS (\u03d5) denotes the sets of attributes (with their predicates) appearing in similarity atoms in its LHS. ARHS (\u03d5) denotes the set of attributes appearing in the identity atom (with . =) in its RHS.20 (Notice from (A.1) that variables y1, y2 in the RHS have implicit predicates, say R1[Y1], R2[Y2].) (b) A set of relational MDs \u03a3 is interaction-free (IF) if, for every \u03d51, \u03d52 \u2208 \u03a3, ALHS (\u03d51) \u2229ARHS (\u03d52) = \u2205. Here, \u03d51 and \u03d52 can be the same.\nIn Example 9, ALHS (\u03d5) = {Author [Name],Author [PTitle],Paper [PTitle], Paper [Bl#]} and ARHS (\u03d5) = {Author [Bl#]}. Since ALHS (\u03d5)\u2229ARHS (\u03d5) = \u2205, \u03a3 = {\u03d5} is IF.\n20We are making a distinction with LHS(\u03d5) and RHS(\u03d5) that denote the set of atoms in the LHS and RHS side of \u03d5, respectively.\nFor the same reasons as for similarity-preserving relational MDs, enforcing IF sets of relational MDs on an initial instance results in a single clean instance that can be computed in polynomial time in the size of the initial instance. Accordingly, set of IF relational MDs have the UCI property.\nThe class of relational MDs we will introduce next requires its combination with the initial instance.\nDefinition 3. Let \u03a3 be a set of relational MDs and D an initial instance. The combination of \u03a3 and D is similarity-free attribute intersection (sometimes we simply say that (\u03a3, D) is SFAI) if one of the following holds: (below \u03d51, \u03d52 can be the same) (a) There are no \u03d51, \u03d52 \u2208 \u03a3, with ALHS (\u03d52) \u2229 ARHS (\u03d51) 6= \u2205, i.e., \u03a3 is interaction-free. (b) For every \u03d51, \u03d52 \u2208 \u03a3 and attribute R[A] \u2208 ALHS (\u03d52) \u2229 ARHS (\u03d51), it holds: If S1, S2 \u2286 D with R(c\u0304) \u2208 S1 \u2229 S2, then LHS (\u03d51) is not true in S1 or LHS (\u03d52) is not true in S2. 21\nIn condition (b) above, S1 and S2 could be the same. Notice that condition (b) is checked only against the initial instance, and not on later instances obtained along a chase sequence. Since the SFAI notion depends on instances, we consider SFAI to be a semantic class, as opposed to the two syntactic ones we considered before in this section.\nIn general, different orders of MD enforcements may result in different clean instances, because tuple similarities may be broken during the chase with interacting MDs and non-similarity-preserving MFs, without reappearing again [11]. Intuitively, with SFAI combinations, two similar tuples in the original instance D -or becoming similar along a chase sequence- may have the similarities broken in a chase sequence, but they will reappear later on in the same and the other chase sequences. Thus, different orders of MD enforcements cannot lead in the end to different clean instances. This behavior can be better appreciated in Example 10 below.\nAs expected, the notion of SFAI class can be applied to classical MDs. Notice that for a classical MD, the LHS of an MD is verified against pairs of tuples from the instance. Thus, for a set of classical MDs, S1 and S2 in condition (b) of Definition 3 take the form {t1, t2} and {t2, t3}, respectively.\nRemark 1. A combination formed by a set of classical MDs \u03a3 and an instance D is SFAI if there are no \u03d51, \u03d52 \u2208 \u03a3, with ALHS (\u03d52) \u2229 ARHS (\u03d51) 6= \u2205, or, otherwise, for every \u03d51, \u03d52 \u2208 \u03a3 and attribute R[A] \u2208 ALHS (\u03d52)\u2229ARHS (\u03d51) it holds: If t1, t2, t3 \u2208 D, then LHS (\u03d51) is not true in {t1, t2} or LHS (\u03d52) is not true in {t2, t3}.\nExample 10. Consider predicate R(A,B,C), the instance D0, and the set of classical MDs \u03a3 below:\n21We informally say that \u03d51 is not applicable in S1, etc.\n\u03d51 : R [A] \u2248 R [A]\u2192 R [B] . = R [B], \u03d52 : R [B] \u2248 R [B]\u2192 R [C] . = R [C].\nR(D0) A B C\nt1 a1 b1 c1 t2 a2 b2 c2 t3 a3 b3 c3\nSometimes we use tids to denote a whole tuple (or record): if t is a tuple identifier in instance D, t\u0304 denotes the tuple in D identified by t: t\u0304 = R(c1, . . . , cn). If A is a sublist of the attributes of predicate R, then t[A] denotes the restriction of t\u0304 to A.\n\u03a3 is interacting (i.e. not IF), because ARHS (\u03d51) \u2229 ALHS (\u03d52) = {R[B]}. Assume now that the only similarities that hold in the data domain U are a1 \u2248 a2, a1 \u2248 a3 and b3 \u2248 b4, with b4 \u2208 Dom(B) r Adom(D0).\nSince \u03d52 is not applicable in D0 (i.e., its LHS is not true), (\u03a3, D0) is SFAI. Notice that b3 \u2248 b4 does not matter, because there is no tuple in D0 with b4 as value for R[B]. If we had b2 \u2248 b3, with t2[B] = b2, t3[B] = b3 in D0, LHS (\u03d51) would be true in {t1, t2}, and LHS (\u03d52) would be true in {t2, t3}.\nWe will show that the enforcement of \u03a3 on D0 generates a unique clean instance, through different chase sequences. First, we show a possibly chase sequence D0, D1, D2, D3, D4, D5, D6, with D6 a stable instance. The matching functions are as follows:\nMB(b1, b2) = b12, MB(b2, b3) = b23, MB(b12, b123) = b123, MB(b12, b3) = b123, MC(c1, c2) = c12, MC(c2, c3) = c23, MC(c12, c3) = c123, MC(c12, c123) = c123.\nAs a result of enforcing \u03d51 on D0 first, the tuples t1, t2 get the identical values for R[B], as shown in the new instance D1 (cf. Figure A.11). Next, since t1 and t2 have same value for R[B], we can enforce \u03d52, leading to t1, t2 getting the same value for R[C], as shown in instance D2 (cf. Figure A.11). As we can see, through MD enforcement new similarities may be created, in this case t1[B] = t2[B] in D1. Furthermore, the equality of values for attribute R[B] feeds the LHS of \u03d52.\nNow, enforcing \u03d51 on t1, t3 in D2 makes the tuples get the same value for attribute R[B], as shown in instance D3 (cf. Figure A.12). At this stage we have broken the equality of t1[B], t2[B] we had in D2, as shown underlined in Figure A.12. This is a crucial point in relation to the SFAI property: \u03d51 is still applicable on D3 with t1, t2, because there are no MDs with attribute R[A] in their RHSs that could destroy the initial similarities that held in D0, in particular t1[A] = a1 \u2248 a3 = t3[A]: they keep holding along the enforcement path. So, enforcing \u03d51 makes t1[B], t2[B] identical again, as shown in instance D4 (cf. Figure A.12).\nNotice that the initial similarities of attribute values we have in the initial instances are not destroyed later along a chase sequences. This is a general property for SFAI combinations.\nNext, applying \u03d52 on t2, t3 in D4 makes the tuples get the same value for attribute R[C], as shown in instance D5 (cf. Figure A.13). Enforcing \u03d52 on t1, t2 in D5 results in instance D6, as shown in Figure A.13. No further applications of MDs are possible, and we have reached a stable instance.\nActually, D6 is the only instance that can be reached through any chase sequence. For example, we will now show another chase sequence leading to the same clean instance D6.\nThe above chase sequence started applying \u03d51 with t1, t2. We could have started with enforcing \u03d51 on t1, t3 in D0. This makes the tuples get the same value for attribute R[B], as shown in instance D\u2032\u20321 (cf. Figure A.14). Next, enforcing \u03d51 on t1, t2 in D \u2032\u2032 1 results in instance D \u2032\u2032 2 , where t1, t2 have identical values for attribute R[B], as shown in Figure A.14.\nAgain, we have broken the equality of t1[B], t3[B] we had in D \u2032\u2032 1 , as shown underlined in Figure A.14. MD \u03d51 is still applicable onD \u2032\u2032 2 with t1, t3. So, enforcing \u03d51 makes t1[B], t3[B] identical again, as shown in instance D \u2032\u2032 3 (cf. Figure A.15).\nNext, applying \u03d52 on t2, t3 in D \u2032\u2032 3 , makes the tuples get the same value for attribute R[C], as shown in instance D\u2032\u20324 (cf. Figure A.15). Now, enforcing \u03d52 on t1, t3 in D \u2032\u2032 4 makes the tuples get the same value for attribute R[C], as shown in\ninstance D\u2032\u20325 (cf. Figure A.16). Enforcing \u03d52 on t1, t2 in D \u2032\u2032 5 results in instance D6 which we had obtained before through a different chase sequence.\nActually, no matter in what order the MDs are enforced in this case, the final, clean instance will be D6, which is due to (\u03a3, D0) having the SFAI property.\nWe illustrated the definition of SFAI with a classical set of MDs.\nExample 11. Consider the set of relational MDs \u03a3 = {\u03d51, \u03d52} with:\n\u03d51 : Author(t1, x1, y1, p1, bl1) \u2227 Author(t2, x2, y2, p2, bl2) \u2227 x1 \u2248 x2 \u2227 Paper(t3, p1, z1, w1, bl4) \u2227 Paper(t4, p2, z2, w2, bl4) \u2212\u2192 bl1 . = bl2. \u03d52 : Paper(t1, p1, z1, w1, bl1) \u2227 Paper(t2, p2, z2, w2, bl2) \u2227 z1 \u2248 z2 \u2227 Author(t3, x1, y1, p1, bl3) \u2227 Author(t4, x2, y2, p2, bl3) \u2212\u2192 bl1 . = bl2.\nAssume that the only similarities that hold in the data domain U , apart from equalities, are: n2 \u2248 n3 and title1 \u2248 title3. Consider the initial instance D0:\nAuthor(D0) Name Aff PID Bl# t1 n1 a1 120 250 t2 n2 a2 121 251 t3 n3 a3 122 252\nPaper(D0) PID Title Key Bl# t4 120 title1 k1 302 t5 122 title2 k2 300 t6 121 title3 k3 300\nHere, to check the SFAI property, we find two cases of interaction: (1) ALHS (\u03d52) \u2229ARHS (\u03d51) = {Author [Bl#]}, (2) ALHS (\u03d51) \u2229ARHS (\u03d52) = {Paper [Bl#]}. We have to check both cases according to condition (b) in Definition 3. For example, for the first case, (\u03a3, D0) is not SFAI if there are S1, S2 \u2286 D0 with a tuple Author(a\u0304) \u2208 S1 \u2229 S2, such that LHS (\u03d51) is true in S1 and LHS (\u03d52) is true in S2.\nIn the general relational case of MDs, one would wonder how difficult is checking the SFAI property. First, notice that only the active domain, Adom(D), of the initial instance D matters for condition (b), because S1, S2 are subsets of D. Actually, checking the SFAI property is decidable, because, for condition (a), a finite set of MDs has to be checked, for interaction; and, for condition (b),\nAdom(D) is finite. Even more, the test can be performed in polynomial time in the size of D (i.e. in data), by posing one Boolean conjunctive query (BCQ) (with similarity built-ins) for each case of interaction between any two MDs in \u03a3.22\nIf one of those queries gets the value true in D, the SFAI property does not hold. We illustrate this claim with an example.\nExample 12. (ex. 11 cont.) For the first case of interaction between the MDs, the following BCQ is posed to D0:\nQ\u03d51,\u03d52 : \u2203t\u0304 \u2203x\u0304 \u2203y\u0304 \u2203p\u0304 \u2203b\u0304l \u2203z\u0304 \u2203w\u0304 (Author(t1, x1, Y1, p3, bl1) \u2227 Author(t2, x2, y2, p4, bl2) \u2227 Paper(t3, p3, z3, w3, bl3) \u2227 Paper(t4, p4, z4, w4, bl3) \u2227 Author(t5, x5, y5, p6, bl2) \u2227 Paper(t6, p6, z6, w6, bl4) \u2227 x1 \u2248 x2 \u2227 z4 \u2248 z6).\nQ\u03d51,\u03d52 takes the value false in D0, then this case (case (1) in Example 11) does not lead to a violation of the SFAI property.\nFor the second case of interaction, we consider the following BCQ:\nQ\u03d52,\u03d51 : \u2203t\u0304 \u2203x\u0304 \u2203y\u0304 \u2203p\u0304 \u2203b\u0304l \u2203z\u0304 \u2203w\u0304 (Author(t1, x1, y1, p3, bl1) \u2227 Author(t2, x2, y2, p4, bl1) \u2227 Paper(t3, p3, z3, w3, bl3) \u2227 Paper(t4, p4, z4, w4, bl4) \u2227 Author(t5, x5, y5, p6, bl2) \u2227 Paper(t6, p6, z6, w6, bl4) \u2227 Author(t7, x7, y7, p6, bl7) \u2227 x2 \u2248 x7 \u2227 z4 \u2248 z3).\nQ\u03d52,\u03d51 also takes the value false in D0. Then, (\u03a3, D0) is SFAI. For a negative example of SFAI, with the same MDs \u03d51, \u03d52, consider a different initial instance D1, :\nAuthor(D1) Name Aff PID Bl# t1 n1 a1 120 250 t2 n2 a2 121 250 t3 n3 a3 122 252 t4 n4 a4 121 253\nPaper(D1) PID Title Key Bl# t5 120 title1 k1 302 t6 122 title2 k2 300 t7 121 title3 k3 300\nIn this case, (\u03a3, D1) is not SFAI, because the query Q\u03d52,\u03d51 above takes the value true in D1.\nAs shown in Example 10 with a set of classical MDs \u03a3, when (\u03a3, D) is SFAI, the initial similarities that held in D can not be destroyed during a complete chase sequence. In particular, the initial similarities keep holding along the enforcement path. The same holds for relational SFAI combinations. As a\n22The MD R [B] \u2248 R [B] \u2192 R [B] .= R [B] interacts with itself, and gives rise to one SFAI test (one query). The interacting MDs R [B] \u2248 R [B] \u2192 R [A] .= R [A] , R [A] \u2248 R [A] \u2192 R [B] . = R [B] give rise to two SFAI tests (two queries).\nconsequence, enforcements of SFAI combinations behave similarly to the case of non-interacting MDs. Actually, it is possible to prove that SFAI combinations have the SCI property. Even more, it is possible to automatically rewrite answer set programs [14] that specify the clean instances obtained with general sets of MDs [4] into Datalog programs with stratified negation [7], which have a single (standard) model that coincides with the single clean instance."}], "references": [{"title": "Large-Scale Deduplication with Con- 32  straints using Dedupalog", "author": ["A. Arasu", "R\u00e9", "Ch", "D. Suciu"], "venue": "Proc. of the 25th International Conference on Data Engineering (ICDE),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Design and Implementation of the LogicBlox System", "author": ["M. Aref", "B. ten Cate", "T.J. Green", "B. Kimelfeld", "D. Olteanu", "E. Pasalic", "T. Veldhuizen", "G. Washburn"], "venue": "Proc. of the ACM International Conference on Management of Data (SIGMOD),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Declarative Entity Resolution via Matching Dependencies and Answer Set Programs", "author": ["Z. Bahmani", "L. Bertossi", "S. Kolahi", "L. Lakshmanan"], "venue": "Proc. of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning (KR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity Resolution", "author": ["Z. Bahmani", "L. Bertossi", "N. Vasiloglou"], "venue": "Proc. of the 9th International Conference on Scalable Uncertainty Management (SUM),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Declarative Entity Resolution via Matching Dependencies and Answer Set Programs. Extended version", "author": ["Z. Bahmani", "L. Bertossi", "S. Kolahi", "L. Lakshmanan"], "venue": "In preparation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Enforcing Relational Matching Dependencies with Datalog for Entity Resolution", "author": ["Z. Bahmani", "L. Bertossi"], "venue": "Submitted. Posted as Corr Arxiv Paper cs.DB/1611.06951,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Generalized Discriminant Analysis using a Kernel Approach", "author": ["Baudat G", "F. Anouar"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "A Comparison of Fast Blocking Methods for Record Linkage", "author": ["R. Baxter", "P. Christen", "T. Churches"], "venue": "Proc. ACM SIGKDD Workshop on Data Cleaning, Record Linkage, and Object Identification,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Swoosh: A Generic Approach to Entity Resolution", "author": ["O. Benjelloun", "H. Garcia-Molina", "D. Menestrina", "Q. Su", "S. EuijongWhang", "J. Widom"], "venue": "VLDB Journal,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Data Cleaning and Query Answering with Matching Dependencies and Matching Functions", "author": ["L. Bertossi", "S. Kolahi", "L. Lakshmanan"], "venue": "Th. Comp. Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Collective Entity Resolution in Relational Data", "author": ["I. Bhattacharya", "L. Getoor"], "venue": "ACM Transaction Knowledge Discovery Data,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Answer Set Programming at a Glance", "author": ["G. Brewka", "Eiter", "Th", "M. Truszczynski"], "venue": "Commun. ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Quality and Complexity Measures for Data Linkage and Deduplication", "author": ["P. Christen", "K. Goiser"], "venue": "In Quality Measures in Data Mining, Guillet,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Automatic Record Linkage using Seeded Nearest Neighbour and Support Vector Machine Classification", "author": ["P. Christen"], "venue": "Proc. of the 14th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "A Survey of Indexing Techniques for Scalable Record Linkage and Deduplication", "author": ["P. Christen"], "venue": "IEEE Transactions in Knowledge and Data Engineering,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "A Comparison of String Metrics for Matching Names and Records", "author": ["W.C. Cohen", "P.D. Ravikumar", "S.E. Fienberg"], "venue": "Proc. KDD Workshop on Data Cleaning and Object Consolidation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Nearest Neighbor Pattern Classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1967}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Feature Selection for Classification", "author": ["M. Dash", "H. Liu"], "venue": "Intelligent Data Analysis,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Bridging Logic and Kernel Machines", "author": ["M Diligenti", "M. Gori", "M. Maggini", "L. Rigutini"], "venue": "Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "A Comparison and Generalization of Blocking and Windowing Algorithms for Duplicate Detection", "author": ["U. Draisbach", "F. Naumann"], "venue": "Proc. QDB Workshop at VLDB,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Duplicate Record Detection: a Survey", "author": ["A. Elmagarmid", "P. Ipeirotis", "V. Verykios"], "venue": "IEEE Transactions in Knowledge and Data Engineering,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Dependencies Revisited for Improving Data Quality", "author": ["W. Fan"], "venue": "Proc. of the Twenty-Seventh ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Machine Learning", "author": ["P. Flach"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Introduction to Statistical Relational Learning", "author": ["L. Getoor", "Taskar", "B. (eds"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "LogiQL: A Query Language for Smart Databases", "author": ["T. Halpin", "S. Rugaber"], "venue": "CRC Press,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Data Quality and Record Linkage Techniques", "author": ["T.N. Herzog", "F.J. Scheuren", "W.E. Winkler"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Advances in Record Linkage Methodology as Applied to Matching the 1985 Census of Tampa", "author": ["M. Jaro"], "venue": "Journal of the American Statistical Society,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1989}, {"title": "Probabilistic Linkage of Large Public Health Data Files", "author": ["M.A. Jaro"], "venue": "Journal of Statistics in Medicine,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1995}, {"title": "Frameworks for Entity Matching: a Comparison", "author": ["H. Kopcke", "E. Rahm"], "venue": "Journal of Data and Knowledge Engineering,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "A Guided Tour to Approximate String Matching", "author": ["G. Navarro"], "venue": "ACM Computing Surveys,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2001}, {"title": "On the Use of Semantic Blocking Techniques for Data Cleansing and Integration", "author": ["J. Nin", "V. Muntes", "N. Martinez-Bazan", "J. Larriba"], "venue": "Proc. IDEAS, IEEE Press,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Comparative Analysis of Approximate Blocking Techniques for Entity Resolution", "author": ["G. Papadakis", "J. Svirsky", "A. Gal", "T. Palpanas"], "venue": "PVLDB, 2016,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "A Hybrid Approach to Functional Dependency Discovery", "author": ["T. Papenbrock", "F. Felix Naumann"], "venue": "Proc. SIGMOD 2016,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Large-scale Collective Entity Matching", "author": ["V. Rastogi", "N.N. Dalvi", "M.N. Garofalakis"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Term-weighting Approaches in Automatic Text Retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information Processing and Management,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1988}, {"title": "Discovering Matching Dependencies", "author": ["S. Song", "L. Chen"], "venue": "Proc. CIKM,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2009}, {"title": "Efficient Discovery of Similarity Constraints for Matching Dependencies", "author": ["S. Song", "L. Chen"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "A Comparison of Blocking Methods for Record Linkage", "author": ["R. Steorts", "S. Ventura", "M. Sadinle", "S. Fienberg"], "venue": "Proc. Privacy in Statistical Databases (PSD), Springer LNCS", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Feature Selection for Classification: A Review", "author": ["J. Tang", "S. Alelyani", "H. Liu"], "venue": "In Data Classification: Algorithms and Applications, CRC Press,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Author Name Disambiguation in Medline", "author": ["I. Torvik", "R. Smalheiser"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Generic Entity Resolution with Negative Rules", "author": ["S Whang", "O. Benjelloun", "H. Garcia-Molina"], "venue": "VLDB Journal,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2009}, {"title": "Entity Resolution with Iterative Blocking", "author": ["S. Whang", "D. Menestrina", "G. Koutrika", "M. Theobald", "H. Garcia-Molina"], "venue": "Proc. of the ACM International Conference on Management of Data (SIGMOD),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2009}], "referenceMentions": [{"referenceID": 21, "context": "In more precise terms, ER is about the identification and fusion of database records (think of rows or tuples in tables) that represent the same real-world entity [13, 25].", "startOffset": 163, "endOffset": 171}, {"referenceID": 12, "context": "This classification problem is approached with machine learning (ML) methods, to learn from previously known or already made classifications (a training set for supervised learning), building a classification model (a classifier) for deciding about other record pairs [16, 25].", "startOffset": 268, "endOffset": 276}, {"referenceID": 21, "context": "This classification problem is approached with machine learning (ML) methods, to learn from previously known or already made classifications (a training set for supervised learning), building a classification model (a classifier) for deciding about other record pairs [16, 25].", "startOffset": 268, "endOffset": 276}, {"referenceID": 34, "context": "Most of the work on applying ML to ER work at the record level [41, 16, 17], and only some of the attributes, or their features, i.", "startOffset": 63, "endOffset": 75}, {"referenceID": 12, "context": "Most of the work on applying ML to ER work at the record level [41, 16, 17], and only some of the attributes, or their features, i.", "startOffset": 63, "endOffset": 75}, {"referenceID": 13, "context": "Most of the work on applying ML to ER work at the record level [41, 16, 17], and only some of the attributes, or their features, i.", "startOffset": 63, "endOffset": 75}, {"referenceID": 38, "context": "To reduce the large number of two-record comparisons, blocking techniques are used [45, 9, 32, 50].", "startOffset": 83, "endOffset": 98}, {"referenceID": 7, "context": "To reduce the large number of two-record comparisons, blocking techniques are used [45, 9, 32, 50].", "startOffset": 83, "endOffset": 98}, {"referenceID": 26, "context": "To reduce the large number of two-record comparisons, blocking techniques are used [45, 9, 32, 50].", "startOffset": 83, "endOffset": 98}, {"referenceID": 42, "context": "To reduce the large number of two-record comparisons, blocking techniques are used [45, 9, 32, 50].", "startOffset": 83, "endOffset": 98}, {"referenceID": 22, "context": "made identical [26, 27].", "startOffset": 15, "endOffset": 23}, {"referenceID": 9, "context": "In [11], MDs were extended with matching functions (MFs).", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "For this purpose, we use the LogicBlox platform, a data management system developed by the LogicBlox company, that is centered around its declarative language, LogiQL [31].", "startOffset": 167, "endOffset": 171}, {"referenceID": 1, "context": "LogiQL supports relational data management and, among several other features [3], an extended form of Datalog with stratified negation [15].", "startOffset": 77, "endOffset": 80}, {"referenceID": 9, "context": "com 3For arbitrary sets of MDs, we need higher expressive power [11], such as that provided by answer set programming [4].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "com 3For arbitrary sets of MDs, we need higher expressive power [11], such as that provided by answer set programming [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 17, "context": "We used the support-vector machine (SVM) approach [21, 48].", "startOffset": 50, "endOffset": 58}, {"referenceID": 22, "context": "This is a non-traditional, novel use of MDs, whereas their intended use is the application to proper merging phase, (d) above, [26].", "startOffset": 127, "endOffset": 131}, {"referenceID": 36, "context": "[43, 44] for some work in this direction), which is somehow closer to the areas of rule learning [29] and discovery of database", "startOffset": 0, "endOffset": 8}, {"referenceID": 37, "context": "[43, 44] for some work in this direction), which is somehow closer to the areas of rule learning [29] and discovery of database", "startOffset": 0, "endOffset": 8}, {"referenceID": 33, "context": "dependencies [40].", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "Although in general a set of MDs may lead to alternative final instances through its enforcement [11], in our application of MDs both sets of MDs lead to a single instance.", "startOffset": 97, "endOffset": 101}, {"referenceID": 9, "context": "This is because the MDs in the set turn out to be interaction-free [11](cf.", "startOffset": 67, "endOffset": 71}, {"referenceID": 2, "context": "As shown in [4, 6] in general, sets of MDs can be expressed by means of answer-set programs (ASPs) [14].", "startOffset": 12, "endOffset": 18}, {"referenceID": 4, "context": "As shown in [4, 6] in general, sets of MDs can be expressed by means of answer-set programs (ASPs) [14].", "startOffset": 12, "endOffset": 18}, {"referenceID": 11, "context": "As shown in [4, 6] in general, sets of MDs can be expressed by means of answer-set programs (ASPs) [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "We independently used three established classification algorithms: SVM, k-nearest neighbor (K-NN) [20], and non-parametric Bayes classifier (NBC) [8].", "startOffset": 98, "endOffset": 102}, {"referenceID": 6, "context": "We independently used three established classification algorithms: SVM, k-nearest neighbor (K-NN) [20], and non-parametric Bayes classifier (NBC) [8].", "startOffset": 146, "endOffset": 149}, {"referenceID": 27, "context": "The experimental results show that our system improves ER recall and precision over traditional, standard blocking techniques [33], where just blocking-key similarities are used.", "startOffset": 126, "endOffset": 130}, {"referenceID": 3, "context": "This paper is a revised and extended version of [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "As in [11], we assume records have unique, fixed, global record identifiers (rids), which are positive integers.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "MDs are formulas of the form [26, 27]:", "startOffset": 29, "endOffset": 37}, {"referenceID": 9, "context": "A dynamic, chase-based semantics for MDs with matching functions (MFs) was introduced in [11], and we briefly summarize it here.", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "MFs are idempotent, commutative, and associative, and then induce a partialorder structure \u3008Dom(A), A\u3009, with: a A a\u2032 :\u21d4 mA(a, a\u2032) = a\u2032 [11, 10].", "startOffset": 135, "endOffset": 143}, {"referenceID": 8, "context": "MFs are idempotent, commutative, and associative, and then induce a partialorder structure \u3008Dom(A), A\u3009, with: a A a\u2032 :\u21d4 mA(a, a\u2032) = a\u2032 [11, 10].", "startOffset": 135, "endOffset": 143}, {"referenceID": 9, "context": "This partial order allows to define a partial order v on instances [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "Given a database instance D and a set of MDs \u03a3, there may be several resolved instances for D and \u03a3 [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "However, there is a unique resolved instance if one of the following holds [11, 6]:", "startOffset": 75, "endOffset": 82}, {"referenceID": 4, "context": "However, there is a unique resolved instance if one of the following holds [11, 6]:", "startOffset": 75, "endOffset": 82}, {"referenceID": 9, "context": "With general sets of MDs, different orders of MD enforcements may result in different clean instances, because tuple similarities may be broken during the chase with interacting, non-similarity-preserving MDs, without reappearing again [11].", "startOffset": 236, "endOffset": 240}, {"referenceID": 23, "context": ", \u03b1n, the coefficients of the \u201csupport vectors\u201d, such that the classifier h can be computed through: h(e) = sign( \u2211 i \u03b1i \u00b7 f(ei) \u00b7 ei \u2022 e + b) [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "Next, from the data in (the preprocessed) structured files, relational predicates and their extensions are created and computed, by means of a generic Datalog program in LogiQL [3, 31].", "startOffset": 177, "endOffset": 184}, {"referenceID": 25, "context": "Next, from the data in (the preprocessed) structured files, relational predicates and their extensions are created and computed, by means of a generic Datalog program in LogiQL [3, 31].", "startOffset": 177, "endOffset": 184}, {"referenceID": 18, "context": "Feature selection is a fundamental task in machine learning [22, 46]; going in detail into this subject is beyond the scope of this work.", "startOffset": 60, "endOffset": 68}, {"referenceID": 39, "context": "Feature selection is a fundamental task in machine learning [22, 46]; going in detail into this subject is beyond the scope of this work.", "startOffset": 60, "endOffset": 68}, {"referenceID": 40, "context": "In our case, the features chosen in our work for the classification of records for entities Paper and Author from the MAS dataset (and the other datasets) correspond to those previously used in [47, 17].", "startOffset": 194, "endOffset": 202}, {"referenceID": 13, "context": "In our case, the features chosen in our work for the classification of records for entities Paper and Author from the MAS dataset (and the other datasets) correspond to those previously used in [47, 17].", "startOffset": 194, "endOffset": 202}, {"referenceID": 29, "context": "Experiments in [35] show that the chosen features enhance generalization power of the classification model, by reducing over-fitting.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "There is a class of well-known and widely applied similarity functions that are used in data cleaning and machine learning [19].", "startOffset": 123, "endOffset": 127}, {"referenceID": 35, "context": "For computing similarities between these kinds of attribute values, the \u201cTF-IDF cosine\u201d measure was used [42].", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": "For attributes with \u201cshort\u201d string values, such as author names, \u201cJaroWinkler\u201d similarity was used [34, 51].", "startOffset": 99, "endOffset": 107}, {"referenceID": 30, "context": "For numerical attributes, such as publication year, the \u201cLevenshtein distance\u201d was used [37].", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "For the application-dependent set, \u03a3 , of blocking-MDs we adopt the chasebased semantics [11], which may lead, in general, to several, alternative final instances.", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "An atom of the form R[X\u0304]=Bl not only declares Bl as an attribute value for R, but also that predicate R is functional on X\u0304 [3]: Each record in R can have only one block number.", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "15General sets of MDs can be specified and enforced by means of disjunctive, stratified answer set programs, with the possibly multiple resolved instances corresponding to the stable models of the program [4].", "startOffset": 205, "endOffset": 208}, {"referenceID": 4, "context": "These programs can be specialized, via an automated rewriting mechanism, for the SFAI case, obtaining residual programs in Datalog with stratified negation [6].", "startOffset": 156, "endOffset": 159}, {"referenceID": 1, "context": "16LogiQL, uses \u201c!\u201d instead of not for Datalog negation [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "This is done via special rules in LogiQL that come in two modes: the learning mode (when a model is being learned, in our case, a SVM classification model), and the evaluation mode (when the model is applied, for record-pair classification in our case) [36, 3].", "startOffset": 253, "endOffset": 260}, {"referenceID": 8, "context": "In our application to bibliographic datasets, we used as matching functions \u201cthe union case\u201d [10], which was investigated in detail in [11] in terms of MDs.", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "In our application to bibliographic datasets, we used as matching functions \u201cthe union case\u201d [10], which was investigated in detail in [11] in terms of MDs.", "startOffset": 135, "endOffset": 139}, {"referenceID": 9, "context": ", Peterbook, K2J5G3\u201d [11].", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "17As with the blocking-programs, the merge-programs can be obtained particularizing the general programs in [4] to the case of interaction-free MDs [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 4, "context": "17As with the blocking-programs, the merge-programs can be obtained particularizing the general programs in [4] to the case of interaction-free MDs [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 41, "context": "Actually, negative rules have been proposed in [49], and discussed in [4] in the context of general answer set programs for MD-based ER.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "Actually, negative rules have been proposed in [49], and discussed in [4] in the context of general answer set programs for MD-based ER.", "startOffset": 70, "endOffset": 73}, {"referenceID": 11, "context": "An alternative is to transform constraints into non-stratified program rules, which would take us in general to the realm of ASPs [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 27, "context": "(a) According to SB, records are clustered into a same block when they share the identical values for blocking keys [33].", "startOffset": 116, "endOffset": 120}, {"referenceID": 14, "context": "The higher the reduction ratio, the fewer the candidate record-pairs that are generated, but the quality of the generated candidate record-pairs is not considered [18].", "startOffset": 163, "endOffset": 167}, {"referenceID": 14, "context": "Recall and precision are measures of goodness of the result of the classification task [18], in this case, of record-pairs as duplicate (i.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "In blocking, this is a common tradeoff that needs to be considered [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "An unsupervised clustering-based approach to collective deduplication is proposed in [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "This approach falls in the context of relational learning [30].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "More precisely, in [12], a relationship graph is built whose nodes are the entities (records), and edges indicate entities which co-occur.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "Experimental results [12] show that this form of collective deduplication outperforms traditional deduplication.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "The approach to ER in [12] could be seen as implicitly involving collective blocking, where relationships between entities and similarities between attribute values are used to create the blocks of records.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "A comparison of our whole approach to ER with other (whole) collective approaches to ER, such as that in [12] has to be left for future research.", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "Dedupalog, a declarative approach to collective entity deduplication in the presence of constraints, is proposed in [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "In [2], equality-generating dependencies were used as hard constraints, and clustering-rules as weak constraints.", "startOffset": 3, "endOffset": 6}, {"referenceID": 19, "context": "In this regard, it is worth pointing out to quite recent research proposing supervised ML-techniques for classification that involve semantic knowledge in the form of logical formulas in kernels for kernel-based methods (such as SVM) [23].", "startOffset": 234, "endOffset": 238}, {"referenceID": 7, "context": "See [9, 18, 24, 39, 45] for comprehensive surveys and comparative studies.", "startOffset": 4, "endOffset": 23}, {"referenceID": 14, "context": "See [9, 18, 24, 39, 45] for comprehensive surveys and comparative studies.", "startOffset": 4, "endOffset": 23}, {"referenceID": 20, "context": "See [9, 18, 24, 39, 45] for comprehensive surveys and comparative studies.", "startOffset": 4, "endOffset": 23}, {"referenceID": 32, "context": "See [9, 18, 24, 39, 45] for comprehensive surveys and comparative studies.", "startOffset": 4, "endOffset": 23}, {"referenceID": 38, "context": "See [9, 18, 24, 39, 45] for comprehensive surveys and comparative studies.", "startOffset": 4, "endOffset": 23}, {"referenceID": 31, "context": "Possible exceptions are [38, 41].", "startOffset": 24, "endOffset": 32}, {"referenceID": 34, "context": "Possible exceptions are [38, 41].", "startOffset": 24, "endOffset": 32}, {"referenceID": 31, "context": "Collective blocking in [38] disregards blocking keys and creates blocks by considering exclusively the relationships between entities.", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "In [41], similarity of blocking keys and relational relationships are considered for blocking in the context of identification of duplicates (not the merging).", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "19We refer to [35] for a discussion on supervised vs.", "startOffset": 14, "endOffset": 18}], "year": 2017, "abstractText": "Entity resolution (ER), an important and common data cleaning problem, is about detecting data duplicate representations for the same external entities, and merging them into single representations. Relatively recently, declarative rules called matching dependencies (MDs) have been proposed for specifying similarity conditions under which attribute values in database records are merged. In this work we show the process and the benefits of integrating four components of ER: (a) Building a classifier for duplicate/non-duplicate record pairs built using machine learning (ML) techniques; (b) Use of MDs for supporting the blocking phase of ML; (c) Record merging on the basis of the classifier results; and (d) The use of the declarative language LogiQL -an extended form of Datalog supported by the LogicBlox platformfor all activities related to data processing, and the specification and enforcement of MDs.", "creator": "LaTeX with hyperref package"}}}