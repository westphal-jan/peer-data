{"id": "1602.02867", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Value Iteration Networks", "abstract": "We introduce the value iteration network: a fully differentiable neural network with a `planning module' embedded within. Value iteration networks are suitable for making predictions about outcomes that involve planning-based reasoning, such as predicting a desired trajectory from an observation of a map. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate our value iteration networks on the task of predicting optimal obstacle-avoiding trajectories from an image of a landscape, both on synthetic data, and on challenging raw images of the Mars terrain.", "histories": [["v1", "Tue, 9 Feb 2016 05:44:36 GMT  (309kb,D)", "http://arxiv.org/abs/1602.02867v1", null], ["v2", "Sun, 29 May 2016 18:33:04 GMT  (416kb,D)", "http://arxiv.org/abs/1602.02867v2", "Update: new experiments on continuous domain and language task, more focus on generalization property of value iteration networks"], ["v3", "Sun, 5 Feb 2017 20:06:14 GMT  (1195kb,D)", "http://arxiv.org/abs/1602.02867v3", "NIPS final version"], ["v4", "Mon, 20 Mar 2017 21:41:51 GMT  (1195kb,D)", "http://arxiv.org/abs/1602.02867v4", "Fixed missing table values"]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE stat.ML", "authors": ["aviv tamar", "sergey levine", "pieter abbeel", "yi wu", "garrett thomas"], "accepted": true, "id": "1602.02867"}, "pdf": {"name": "1602.02867.pdf", "metadata": {"source": "META", "title": "Value Iteration Networks", "authors": ["Aviv Tamar", "Sergey Levine", "Pieter Abbeel"], "emails": ["AVIVT@BERKELEY.EDU", "SLEVINE@EECS.BERKELEY.EDU", "PABBEEL@CS.BERKELEY.EDU"], "sections": [{"heading": "1. Introduction", "text": "In recent years, deep convolutional neural networks (CNNs) have shown remarkable successes in computer vision tasks such as object recognition, action recognition, and scene labeling (Krizhevsky et al., 2012; Farabet et al., 2013; Ji et al., 2013). Broadly speaking, common to these tasks is the goal of answering the question: \u2018what is in the image ?\u2019 1.\nIn this work, we consider prediction tasks with complex structured output that, broadly speaking, answer the question \u2018what to do given the image ?\u2019. In particular, we are interested in predicting labels that are the result of some planning procedure. As an example, consider the pathplanning problem depicted in Figure 1. Here, the observation is an image of a terrain and, given some start and goal positions, the label encodes the shortest-length trajectory between them. Such structured prediction tasks\n1In general, the observation does not have to be an image, and CNNs have been applied to various inputs such as audio data and natural language text. In our presentation, we focus on image inputs, but our work can be extended to other types of observations.\nFigure 1. Trajectory prediction problem on the Mars dataset. The input observation is a map of a terrain with obstacles, a starting position (blue marker), and a goal position (orange marker). The objective is to predict the shortest path to the goal between the obstacles.\n(Bakir et al., 2007; Nowozin & Lampert, 2011) are important for learning from demonstration in robotics (Hersch et al., 2008; Ratliff et al., 2009; Sung et al., 2015), activity forcasting (Kitani et al., 2012), and natural language processing (Chang et al., 2015).\nA na\u0131\u0308ve approach for solving such problems is to ignore the underlying planning process altogether, and solve them using any standard multi-class classification model. For all but the smallest of problems, such an approach is doomed to fail, since the decisions made by the planning algorithm may have a very complex and non-smooth dependency on the observation, making generalization extremely difficult. For example, the trajectory in Figure 1 could change dramatically by adding just one more obstacle near its end.\nIn this work, we propose a neural network model that is suitable for learning such planning-based reasoning. Our model, termed a value-iteration network, has a \u2018planning module\u2019 embedded within the neural network structure. This embedding allows the network to implicitly encode an approximation of the planning process that generated the data, thereby enabling complex predictions that depend\nar X\niv :1\n60 2.\n02 86\n7v 1\n[ cs\n.A I]\n9 F\neb 2\n01 6\non such planning.\nThe key to our approach is an observation that the classic value-iteration (VI) planning algorithm (Bellman, 1957; Bertsekas, 2012) may be represented by a specific type of convolutional neural network. By embedding such a VI network block inside a standard feed-forward classification network, we obtain a model that implicitly encodes a planning process. The VI block is differentiable, and the whole network can be trained end-to-end using standard backpropagation. After training, the network learns to map an observation to an underlying planning process, and generate predictions based on the resulting plan. In addition, we investigate a hierarchical extension of the VI block, which learns more efficiently on larger domains.\nWe show that value-iteration networks can learn to predict non-trivial trajectories from images of synthetic grid-world domains, and directly from raw overhead images of Mars terrain, as in Figure 1. Our approach performs significantly better than feed-forward classification networks, and does not require knowing the planning model in advance.\nWe emphasize that our contribution is a new representation for planning-based predictions, and not a new learning algorithm. Indeed, in this work we used standard supervised learning methods for training our models. Previous approaches to the structured-prediction problem, such as inverse reinforcement learning (IRL) address the underlying planning process in the learning algorithm (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Ross et al., 2011). Our contribution is therefore complimentary to these methods, and can be easily incorporated within more advanced learning algorithms, such as those proposed by Ross et al. (2011). We further discuss related work in Section 5."}, {"heading": "2. Background", "text": "In this section we provide background on planning and convolutional neural networks, and present our problem formulation."}, {"heading": "2.1. Value Iteration", "text": "A standard model for sequential decision making and planning is the Markov decision process (MDP) (Bellman, 1957; Bertsekas, 2012). An MDP consists of states s \u2208 S , actions a \u2208 A, a reward function R(s, a), and a transition kernel P (s\u2032|s, a) that encodes the probability of the next state given the current state and action. A policy \u03c0(a|s) prescribes an action distribution for each state. The goal in an MDP is to find a policy that obtains high rewards in the long term. Formally, the value V \u03c0(s) of a state under policy \u03c0 is the expected discounted sum of rewards when\nstarting from that state and executing policy \u03c0,\nV \u03c0(s) . = E\u03c0 [ \u221e\u2211 t=0 \u03b3tr(st, at) \u2223\u2223\u2223\u2223\u2223 s0 = s ] ,\nwhere \u03b3 \u2208 (0, 1) is a discount factor, and E\u03c0 denotes an expectation over trajectories of states and actions s0, a0, s1, a1 . . . , in which actions are selected according to \u03c0, and states evolve according to the transition kernel P (s\u2032|s, a). The optimal value function V \u2217(s) . = max\u03c0 V\n\u03c0(s) is the maximal long-term return possible from a state. A policy \u03c0\u2217 is said to be optimal if V \u03c0 \u2217 (s) = V \u2217(s) \u2200s. A popular algorithm for calculating V \u2217 and \u03c0\u2217 is value iteration (VI):\nVn+1(s) = max a\nQn(s, a) \u2200s, Qn(s, a) = R(s, a) + \u03b3 \u2211 s\u2032 P (s\u2032|s, a)Vn(s\u2032). (1)\nIt is well known that the value function Vn in VI converges as n \u2192 \u221e to V \u2217, from which an optimal policy may be derived as \u03c0\u2217(s) = argmaxaQ\u221e(s, a). In practice, VI can only be applied for a finite number of iterations, yielding an approximate solution, for which error bounds can be derived (Bertsekas, 2012)."}, {"heading": "2.2. Convolutional Neural Networks", "text": "Convolutional neural networks (CNNs; LeCun et al. 1998; Bengio 2009) are neural networks with a particular architecture that has proved useful for computer vision, among other domains. A CNN is comprised of stacked convolution and max-pooling layers. The input to each convolution layer is a 3-dimensional signal X , typically, an image with l channels, m horizontal pixels, and n vertical pixels, and its output h is a l\u2032-channel convolution of the image with kernels W 1, . . . ,W l \u2032 ,\nhl\u2032,i\u2032,j\u2032 = \u03c3 \u2211 l,i,j W l \u2032 l,i,jXl,i\u2032\u2212i,j\u2032\u2212j  , (2) where \u03c3 is a non-linear activation function, typically a rectified linear unit (ReLU): \u03c3(x) = max(x, 0). A maxpooling layer selects, for each channel l and pixel i, j in h, the maximum value among its neighbors N(i, j)\nhmaxpooll,i,j = max i\u2032,j\u2032\u2208N(i,j) hl,i\u2032,j\u2032 . (3)\nTypically, the neighbors N(i, j) are chosen as a k \u00d7 k image patch around pixel i, j. After max-pooling, the image is down-sampled by a constant factor d, commonly 2 or 4, resulting in an output signal with l\u2032 channels, m/d horizontal pixels, and n/d vertical pixels. CNNs are typically trained using stochastic gradient descent (SGD), and the gradients of a CNN can be calculated efficiently using backpropagation."}, {"heading": "2.3. Problem Formulation", "text": "We consider a structured prediction (Bakir et al., 2007; Nowozin & Lampert, 2011) problem in which the input consists of a dataset of images { xi } i=1,...,N\n, states{ si } i=1,...,N , and action labels { ai } i=1,...,N\n. We assume that the actions are generated from an optimal policy ai = \u03c0\u2217(si) with respect to some unknown MDP, encoded in the image xi. For example, in the path planning domains we consider, the state s encodes the position, and the image shows the obstacles and a goal location. The action then encodes the direction of the shortest path to the goal between the obstacles. We emphasize that the reward and transitions in the MDP are unknown, and are not an explicit part of the input in any way.\nOur goal is to learn a mapping y(x, s) that best predicts the correct action label for an image and state. Na\u0131\u0308vely, this problem can be cast as a standard supervised learning problem (Bishop, 2006), by concatenating x and s into a single observation. With enough training data, it is conceivable that a standard supervised learning algorithm would learn a suitable solution. However, such a na\u0131\u0308ve approach ignores the knowledge about the process that generates the data. By exploiting this knowledge, a solution may be found more efficiently. In the following, we propose a class of neural network representations that incorporate a \u2018planning module\u2019 in their structure. We shall show that this structure enables much more efficient learning than standard feedforward networks in this problem domain."}, {"heading": "3. The Value Iteration Network Model", "text": "In this section we introduce the value iteration network \u2013 a model that encodes a differentiable planning procedure.\nOur starting point is the VI algorithm (1). Our main observation, is that each iteration of VI may be seen as passing the previous value function Vn and reward function R through a convolution layer and max-pooling layer. In this analogy, each channel in the convolution layer corresponds to the Q-function for a specific action, and convolution-kernel weights correspond to the discounted transition probabilities. Thus, by recurrently applying a convolution layer K times, K iterations of VI can be performed.\nFollowing this idea, we propose the VI network block model, as depicted in Figure 2. The inputs to the VIN are a \u2018reward image\u2019 r of dimensions l,m, n, and a state (is, js) \u2208 (1, . . . ,m)\u00d7 (1, . . . , n). Thus, we explicitly assume that the state space maps to a 2-dimensional map2.\n2Our approach can be extended to general state spaces by adding a transformation layer between the state and the value function (Jaderberg et al., 2015). This extension will be explored\nThe reward is fed into to a convolutional layer q with A channels and a linear activation function,\nqa,i\u2032,j\u2032 = \u2211 l,i,j W al,i,jrl,i\u2032\u2212i,j\u2032\u2212j .\nEach channel in this layer corresponds to Q(s, a) for a particular action a. This layer is then max-pooled along the actions channel to produce the next-iteration value function layer v,\nvi,j = max a q(a, i, j).\nThe next-iteration value function layer v is then stacked with the reward, and fed back into the convolutional layer and max-pooling layer K times, to perform K iterations of value iteration. After K such recurrences, the A channels of the q layer in the is, js position are fed into a fully connected softmax output layer y,\ny(a) = exp\n(\u2211 a\u2032W out a\u2032,aq(a \u2032, is, js) )\n\u2211 a exp (\u2211 a\u2032W out a\u2032,aq(a \u2032, is, js) ) .\nThe VI block is simply a neural-network based module for approximate-VI based planning. Nevertheless, we emphasize that this representation makes learning the MDP parameters and reward function natural \u2013 by backpropagating through the network, similarly to a standard CNN. Thus, the weights in a VI block, which implicitly encode the MDP transitions and reward, need not be known in advance, and are trained directly from data. This makes the VI block a suitable model for learning action predictions that are the output of some planning process, as in the trajectory prediction problems described above. We additionally emphasize that the weights learned by the VI block can be very different from the true MDP transitions that generated the data, so long as they lead to similar action\nin future work.\npredictions. In particular, the number of channels A in the q layer can be different than the true number of actions in the true MDP. Indeed, in our experiments we found out that selecting a larger A than the number of actions in the true MDP actually improves learning.\nThe VI block can be connected to an image-processing network that takes as input a general image, and outputs a reward map. The key here is that the whole network can be trained end-to-end, by back-propagating gradient information through the network components. Thus, the imageprocessing network can be trained to learn a mapping from the image features to a reward map representing the planning process underlying the data. We emphasize that this learned reward mapping need not be the true MDP reward, so long as it leads to accurate action predictions. In this sense, the the network is learning a reward shaping (Ng et al., 1999) from data. In Figure 3 we depict a VI network with two additional convolution layers between the image input and the VI block. This specific network was used in our experiments, but other image-processing models can be easily incorporated instead."}, {"heading": "3.1. Hierarchical VI Networks", "text": "The difficulty of training deep recurrent models such as the VI block described above increases with the number of VI iterationsK (Pascanu et al., 2012). Unfortunately, for some planning domains, the number of VI iteration required to obtain a reasonable policy may be quite large. As an example, consider a grid-world, in which the goal is located L steps away from some state s. Then, at least L iterations of VI are required to convey the reward information from the goal to state s, and clearly, any action prediction obtained with less than L VI iterations at state s is unaware of the goal location, and therefore unacceptable.\nAt least potentially, a well-chosen reward shaping can help mitigate this issue. However, for a localized mapping from\nthe input image to the shaped reward, such as the CNN layers in the VI network suggested above, the rewardinformation problem may pertain.\nOur solution for conveying reward information faster in VI is to perform VI at multiple levels of resolution. We term this model a hierarchical VI Network (HVIN), due to its similarity with hierarchical planning algorithms. In a HVIN, a copy of the input down-sampled by a factor of d is first fed into a VI block termed the high-level VI block. The down-sampling offers a d\u00d7 speedup of information transmission in the map, at the price of reduced accuracy. The value layer of the high-level VI block is then up-sampled, and added as an additional input channel to the shaped reward of the standard VI network. Thus, the high-level VI block learns a mapping from down-sampled image features to a suitable reward-shaping for the nominal image. The full HVIN model is depicted in Figure 4. This model can easily be extended to include multiple levels of hierarchy."}, {"heading": "3.2. Approximate Planning Interpretation", "text": "Our presentation so far has been deep-learning oriented, viewing approximate VI as a neural-network architecture. In this section we provide a different interpretation, and discuss VI networks as a planning algorithm.\nIn this context, the VI network can be seen as an approximate value iteration algorithm, with the network weights approximating the MDP transitions, and the shaped reward function replacing the true reward. Most importantly, in this approximation VI is not iterated until convergence,\nbut only for K iterations. In the planning literature, several studies investigated the use of reward-shaping (Ng et al., 1999), hierarchical macro-actions (Mann & Mannor, 2014), and state-aggregation (Taylor et al., 2009; Bertsekas, 2012) for reducing the number of iterations required for VI to converge, with the goal of reducing computation time. However, while it is known that a well-chosen reward-shaping, state-aggregation, or macro-action set can indeed make VI converge faster, the problem of choosing such is challenging. While several useful heuristic rules and intractable optimality-driven approaches exist, to our knowledge, a principled yet tractable method is not yet known.\nOne interpretation of our VI networks is that, by training from optimal actions, we are in fact learning an optimal reward-shaping for the iteration-constrained VI. More interestingly, in the HVIN models, we are also learning to perform a hierarchical VI algorithm. In this algorithm, the network performs state-aggregation (by down-sampling), with the actions in the high-level VI block representing macro-actions in the original domain. Interestingly, the macro-actions here are learned from data. We are not aware of an analogous hierarchical VI algorithm in the dynamic programming literature that does not rely on hand-crafted macro-actions.\nAt least conceptually, our method may be used in the future to devise reward-shaping and macro-action discovery techniques for planning algorithms."}, {"heading": "4. Results", "text": "We evaluated the VIN model in two path-planning domains. The first domain is a synthetic grid-world with obstacles, in which the observation is a map of the obstacles and the goal position. The second domain models a navigation task for a Mars rover. In this domain, the observation is an overhead image of the terrain, along with a map of the goal position. Our goal in these experiments is twofold. First, we wish to show that VIN models are vastly superior to standard feed-forward networks in prediction tasks that involve planning. Second, we shall demonstrate that by embedding a VIN within an image-processing network, challenging structured-prediction tasks from real images can be tackled successfully."}, {"heading": "4.1. Synthetic Grid-World Domain", "text": "We consider a m\u00d7 n grid-world with randomly placed obstacles, and a border of size 1. Figure 5 shows an instance of the grid world with m = n = 16. The possible actions are moving in one of the 8 directions. Attempting to move into an obstacle results in no movement. Finding a shortest path between two locations in the grid-world may be\ndone using value iteration (among other standard planning algorithms), by using a constant negative reward for every non-goal state, and some positive reward for the goal.\nRecall that our goal in the experiment is to solve a classification problem \u2013 to learn a mapping from an image of the grid-world (as in Figure 5), the current position in the map, and some goal position, to an action that leads to the shortest path to the goal. Our training set consists of Ni = 5000 random grid-world instances, with Nt = 7 shortest-path trajectories from a random start-state to a random goalstate for each instance; a total of Ni \u00d7 Nt trajectories. For each state S = (i, j) in each trajectory, we produce a (2\u00d7m\u00d7 n)-sized observation image X . The first channel of X encodes the obstacle presence (1 for obstacle, 0 otherwise), while the second channel encodes the goal position (1 at the goal, 0 otherwise). In addition, for each state we produce a label A that encodes the action (one of 8 directions) that an optimal shortest-path policy would take in that state.\nWe trained several neural-network based multi-class logistic regression classifiers using stochastic gradient descent, with an RMSProp step size (Tieleman & Hinton, 2012), implemented in the Theano (Bastien et al., 2012) library.\nWe compare between the following neural network models:\nVIN network We used the VIN model of Section 3, with 3\u00d73 convolution kernels, 150 channels for the hidden layer H , and 10 channels for the q layer in the VI block. The recurrence K was set relative to the problem size: K = 10 for 8\u00d7 8 domains, K = 20 for 16\u00d7 16 domains, K = 36 for 28\u00d728 domains, andK = 44 for 36\u00d736 domains. The guideline for choosing these values was to keep the network small while guaranteeing that goal information can flow to every state in the map.\nHierarchical VIN network We used the hierarchical VIN model of Section 3.1, with a 2 \u00d7 2 down-sampling layer. Similarly to the VI network, we used 3 \u00d7 3 convolution kernels, 150 channels for each hidden layer H (for both the down-sampled image, and standard image), and 10 channels for the q layer in each VI block. Similary to the VIN networks, the recurrence K was set relative to the problem size, taking into account the down-sampling factor: K = 4 for 8\u00d78 domains,K = 10 for 16\u00d716 domains, K = 16 for 28\u00d7 28 domains, and K = 20 for 36\u00d7 36 domains.\nFully-connected feed-forward network In order to train a standard multi-layer fully-connected (FC) feed-forward network (also known as a multi-layer perceptron model), we flattened out the obstacle and goal images, and represented the state (i, j) as two 1-hot vectors (a vector for each coordinate), for a total of 2 \u00d7m \u00d7 n +m + n inputs. We constructed both a single hidden-layer FC network, with 256 hidden units, and a two-layer FC network, with 256 hidden units in each hidden layer, and ReLU non-linearity. The number of hidden units was chosen by trial and error.\nFeed-forward CNN This model is an extension of the single layer FC network, in which the input images are first fed into 2 convolution layers, with 3 \u00d7 3 convolution kernels, 8 and 16 channels, respectively, and 2 \u00d7 2 maxpooling. The output of these layers, along with the 1-hot state vectors, are fed into a single hidden-layer FC network, with 256 hidden units.\nIn Table 1 we present the average 0 \u2212 1 prediction loss of each model, evaluated on a held-out test-set. In addition, we report the actual performance in predicting a trajectory by measuring the average success rate in obstacle avoidance, and the length of the trajectory compared to the optimal shortest-path. We evaluated these measures on a held-out test-set of 1000 maps with random obstacles, goal positions, and initial states. For each map, a full trajectory from the initial state was predicted, by iteratively rollingout the next-states predicted by the network. A trajectory was said to succeed if it did not hit any obstacles. For each trajectory that succeeded, we also measured its difference in length from the optimal trajectory. The average difference and the average success rate are reported in Table 1.\nClearly, the VIN and hierarchical VIN significantly outperform the feed-forward networks. Note, however, that the performance gap increases dramatically with the problem size. The 8 \u00d7 8 domain is small enough for a feedforward network to \u2018remember\u2019 all possible obstacle configurations, and perform reasonably well. The number of possible configurations, however, increases exponentially with the problem size, leading to a complete failure on the larger maps. The VIN based models, on the other hand, are\nable to overcome this issue by exploiting the structure of the problem.\nNote that the hierarchical VIN has a better performance on the larger maps, even though it uses a smaller number of VI iterationsK. Moreover, the values ofK for hierarchical VIN were chosen to be smaller than the image size, making it impossible to predict correct labels far away from the goal without exploiting hierarchical information. Thus, the near-perfect performance of hierarchical VIN shows that the network indeed learned a useful reward-shaping from the down-sampled VI block.\nWe emphasize that the model (i.e., transition dynamics) used for planning the shortest-path trajectories was not given to the learning algorithm at any point. In such a setting, exact IRL is impossible."}, {"heading": "4.2. Mars Rover Navigation", "text": "The grid-world experiments in the previous section establish the capability of VIN networks to efficiently represent shortest-path predictions from image input. One very simplifying factor in those experiments, however, was that the image input was a prefect map of the planning domain. In this section we show that by piping VIN networks to a CNN feature extraction block, and training the whole network end-to-end, we can learn relevant features and make shortest-path predictions from raw image input. We demonstrate this on predicting obstacle-avoidance trajectories directly from overhead terrain images of a Mars landscape.\nWe consider the problem of autonomously navigating the surface of Mars by a rover such as the Mars Science Laboratory (MSL) (Lockwood, 2006) over long-distance trajectories. The MSL has a limited ability for climbing high-degree slopes, and its path-planning algorithm should therefore avoid navigating into high-slope areas. In our experiment, we plan trajectories that avoid slopes of 10 degrees or more, using digital terrain models from the High Resolution Imaging Science Experiment (HiRISE) (McEwen et al., 2007). The HiRISE data consists of grayscale images of the Mars terrain, and matching elevation data, accurate to tens of centimeters. We used an image of a 33.3km by 6.3km area at 49.96 degrees latitude and 219.2 degrees longitude, with a 10.5 sq. meters / pixel resolution. From this image, we randomly sampled 8m pixel by 8n pixel patches, and planned shortest-path trajectories on a m \u00d7 n grid-world, where each state was considered an obstacle if its corresponding 8 \u00d7 8 image patch contained an angle of 10 degrees or more, evaluated using the ground-truth elevation data. Thus, the MDP underlying shortest-path planning in this case is similar to the grid-world domain of Section 4.1. An example of the domain and terrain image is depicted in Figure 6.\nOur goal is to train a network that predicts the shortestpath trajectory directly from the terrain image data. We emphasize that the ground-truth elevation data is not part of the input, and the elevation therefore must be inferred (if needed) from the terrain image itself.\nOur network model is the hierarchical VIN model of Section 3.1, with a 2 \u00d7 2 down-sampling layer. In this case, however, instead of feeding in the obstacle map, we feed in the raw terrain image. This image is processed by two convolution layers: the first with a 5 \u00d7 5 kernel, 6 channels, and 4 \u00d7 4 max-pooling, and the second with a 3 \u00d7 3 kernel, 12 channels, and 2\u00d7 2 max-pooling. The resulting 12-channel m\u00d7n image is fed into the VI network instead of the obstacle map. The goal map, state inputs, and output labels remain as in the grid-world experiments. We emphasize that the whole network is trained end-to-end, without pre-training the input filters.\nIn Table 2 we present our results for training am = n = 16 map from a 10K image-patch dataset, with 7 random trajectories per patch, evaluated on a held-out test set of 1K patches. To put the 84.8% success rate in context, we compare with the best performance achievable without access to the elevation data. To make this comparison, we trained a CNN to classify whether an 8\u00d78 patch is an obstacle or not. This classifier was trained using the same image data as the VIN network, but its labels were the true obstacle classifications from the elevation map (we reiterate that the VIN network did not have access to these ground-truth obstacle classification labels during training or testing). Training this classifier is a standard binary classification problem, and its performance represents the best obstacle identification possible with our CNN in this domain. The bestachievable shortest-path prediction is then defined as the\nshortest path in an obstacle map generated by this classifier from the raw image. The results of this optimal predictor are reported in Table 1. The 90.3% success rate shows that obstacle identification from the raw image is indeed challenging. Thus, the success rate of the VIN network, which was trained without any obstacle labels, and had to \u2018figure out\u2019 the planning process is quite remarkable.\nIn Figure 6 we show an instance of the input image, the obstacles, the shortest-path trajectory, and the trajectory predicted by our method."}, {"heading": "5. Related Work", "text": "Learning to predict trajectories is an instance of structured prediction problems (Bakir et al., 2007; Nowozin & Lampert, 2011). In particular, structured prediction strategies such as maximum-margin prediction (Ratliff et al., 2006) have been applied to imitation learning of trajectories from demonstrations. In this setting, the transitions in the MDP underlying the planning process are assumed to be known,\nbut the cost has to be learned from data. A similar approach is also used in inverse reinforcement learning (IRL) (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011), which has been used for similar domains of predicting trajectories from overhead images as in this work (Kitani et al., 2012; Ratliff et al., 2009). Our approach is somewhat orthogonal to these methods, as we encode the planning process directly in our policy, relieving the need to use specialized learning algorithms that exploit the underlying MDP. To emphasize this point, in this work we trained our models using a standard classification-learning algorithm. However, our approach can be easily incorporated within a no-regret structured prediction method such DAgger (Ross et al., 2011) to potentially yield improved results. We remark that although we do not explicitly use a model, the local convolutions applied in our networks do exploit some structure of the underlying MDP. However, this prior knowledge is still considerably less than knowing the exact transitions.\nSimilar to our approach, Zucker & Bagnell (2012) suggested gradient-based approaches for learning better planners. In that work, a linearly parameterized cost function was learned using reinforcement-learning techniques. In a different study, Singh et al. (2010) used evolutionary algorithms to learn a cost function that accelerates learning. Our work is substantially different, in learning both costs and transitions of an approximate planning algorithm, and using highly expressive neural networks instead of linear function approximators.\nAnother relevant line of work is the recent trend of approximating algorithms using neural networks, such as the neural turing machines (Graves et al., 2014), pointer net-\nworks (Vinyals et al., 2015), and the recurrent networks of Zaremba et al. (2015). Our approach can be seen as a neural-network approximation of dynamic programming (VI), learned from examples."}, {"heading": "6. Conclusion and Outlook", "text": "We introduced the value-iteration network: a fully differentiable neural network with a \u2018planning module\u2019 embedded within. VI networks are particularly well-suited for making predictions about the outputs of a planning process, for example, trajectory planning, as explored in this work. By training a VI end-to-end, we were able to learn to predict shortest-path trajectories directly from raw images of a domain, both on synthetic data, and real overhead images of a Mars terrain. In addition, we proposed a hierarchical variant of the VI network, which is better suited for larger domains.\nIn this work we focused on planning problems that have a two-dimensional structure, in which the state can be easily mapped onto images, and the transitions are local. While such problems have important applications in activity forecasting and path-planning (Kitani et al., 2012; Ratliff et al., 2009), generalizing our approach to different problem structures is important. We now discuss several directions for achieving this. First, the mapping from the state input to the shaped-reward map can be generalized by adding a spatial transformation layer (Jaderberg et al., 2015) between them. Such a transformation can be learned simultaneously with the rest of the network by training endto-end. Second, the localized convolution kernels, which imply local state transitions in the MDP, can easily be replaced with different, arbitrary, kernel shapes. In addition,\nconditioning the kernels on the input (Oh et al., 2015) may provide an additional flexibility.\nWe conclude with an alternative view of our approach in a reinforcement learning (RL) context. The VI network can be seen as a policy representation, i.e., a mapping from states to actions, that explicitly performs some look-ahead computation by the VI block. Recent RL advances (Mnih et al., 2015; Schulman et al., 2015) showed that CNN-based policies can be trained successfully in a RL setting. Thus, with minimal modification, our VI networks may be incorporated into a RL algorithm, with the potential of providing stronger policy representations."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A. Ng"], "venue": "In ICML,", "citeRegEx": "Abbeel and Ng,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng", "year": 2004}, {"title": "Predicting Structured Data (Neural Information Processing)", "author": ["Bakir", "G. H", "T. Hofmann", "B. Sch\u00f6lkopf", "A. Smola", "B. Taskar", "S. Vishwanathan"], "venue": null, "citeRegEx": "Bakir et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bakir et al\\.", "year": 2007}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Dynamic Programming and Optimal Control, Vol II", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas,? \\Q2012\\E", "shortCiteRegEx": "Bertsekas", "year": 2012}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Learning to search better than your teacher", "author": ["K. Chang", "A. Krishnamurthy", "A. Agarwal", "H. Daum\u00e9 III", "J. Langford"], "venue": "In ICML,", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Farabet et al\\.,? \\Q1915\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 1915}, {"title": "Dynamical system modulation for robot learning via kinesthetic demonstrations. Robotics", "author": ["M. Hersch", "F. Guenter", "S. Calinon", "A. Billard"], "venue": "IEEE Transactions on,", "citeRegEx": "Hersch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hersch et al\\.", "year": 2008}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In NIPS,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ji et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "In NIPS,", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "Scaling up approximate value iteration with options: Better policies with fewer iterations", "author": ["T. Mann", "S. Mannor"], "venue": "In ICML,", "citeRegEx": "Mann and Mannor,? \\Q2014\\E", "shortCiteRegEx": "Mann and Mannor", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A. Ng", "D. Harada", "S. Russell"], "venue": "In ICML,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Structured learning and prediction in computer vision", "author": ["S. Nowozin", "C. Lampert"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Nowozin and Lampert,? \\Q2011\\E", "shortCiteRegEx": "Nowozin and Lampert", "year": 2011}, {"title": "Actionconditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R. Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "arXiv preprint arXiv:1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Maximum margin planning", "author": ["N. Ratliff", "A. Bagnell", "M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["N. Ratliff", "D. Silver", "A. Bagnell"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G. Gordon", "A. Bagnell"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M. Jordan", "P. Moritz"], "venue": "In ICML,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Intrinsically motivated reinforcement learning: An evolutionary perspective", "author": ["S. Singh", "R. Lewis", "A. Barto", "J. Sorg"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Singh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2010}, {"title": "Robobarista: Object partbased transfer of manipulation trajectories from crowdsourcing in 3d pointclouds", "author": ["J. Sung", "S. Jin", "A. Saxena"], "venue": "In International Symposium on Robotics Research (ISRR),", "citeRegEx": "Sung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2015}, {"title": "Bounding performance loss in approximate MDP homomorphisms", "author": ["J. Taylor", "D. Precup", "P. Panagaden"], "venue": "In NIPS,", "citeRegEx": "Taylor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2009}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Learning simple algorithms from examples", "author": ["W. Zaremba", "T. Mikolov", "A. Joulin", "R. Fergus"], "venue": "arXiv preprint arXiv:1511.07275,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B. Ziebart", "A. Maas", "A. Bagnell", "A. Dey"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}, {"title": "Reinforcement planning: RL for optimal planners", "author": ["M. Zucker", "A. Bagnell"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Zucker and Bagnell,? \\Q2012\\E", "shortCiteRegEx": "Zucker and Bagnell", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "In recent years, deep convolutional neural networks (CNNs) have shown remarkable successes in computer vision tasks such as object recognition, action recognition, and scene labeling (Krizhevsky et al., 2012; Farabet et al., 2013; Ji et al., 2013).", "startOffset": 183, "endOffset": 247}, {"referenceID": 11, "context": "In recent years, deep convolutional neural networks (CNNs) have shown remarkable successes in computer vision tasks such as object recognition, action recognition, and scene labeling (Krizhevsky et al., 2012; Farabet et al., 2013; Ji et al., 2013).", "startOffset": 183, "endOffset": 247}, {"referenceID": 1, "context": "(Bakir et al., 2007; Nowozin & Lampert, 2011) are important for learning from demonstration in robotics (Hersch et al.", "startOffset": 0, "endOffset": 45}, {"referenceID": 9, "context": ", 2007; Nowozin & Lampert, 2011) are important for learning from demonstration in robotics (Hersch et al., 2008; Ratliff et al., 2009; Sung et al., 2015), activity forcasting (Kitani et al.", "startOffset": 91, "endOffset": 153}, {"referenceID": 22, "context": ", 2007; Nowozin & Lampert, 2011) are important for learning from demonstration in robotics (Hersch et al., 2008; Ratliff et al., 2009; Sung et al., 2015), activity forcasting (Kitani et al.", "startOffset": 91, "endOffset": 153}, {"referenceID": 26, "context": ", 2007; Nowozin & Lampert, 2011) are important for learning from demonstration in robotics (Hersch et al., 2008; Ratliff et al., 2009; Sung et al., 2015), activity forcasting (Kitani et al.", "startOffset": 91, "endOffset": 153}, {"referenceID": 7, "context": ", 2012), and natural language processing (Chang et al., 2015).", "startOffset": 41, "endOffset": 61}, {"referenceID": 3, "context": "The key to our approach is an observation that the classic value-iteration (VI) planning algorithm (Bellman, 1957; Bertsekas, 2012) may be represented by a specific type of convolutional neural network.", "startOffset": 99, "endOffset": 131}, {"referenceID": 5, "context": "The key to our approach is an observation that the classic value-iteration (VI) planning algorithm (Bellman, 1957; Bertsekas, 2012) may be represented by a specific type of convolutional neural network.", "startOffset": 99, "endOffset": 131}, {"referenceID": 21, "context": "Previous approaches to the structured-prediction problem, such as inverse reinforcement learning (IRL) address the underlying planning process in the learning algorithm (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Ross et al., 2011).", "startOffset": 169, "endOffset": 251}, {"referenceID": 30, "context": "Previous approaches to the structured-prediction problem, such as inverse reinforcement learning (IRL) address the underlying planning process in the learning algorithm (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Ross et al., 2011).", "startOffset": 169, "endOffset": 251}, {"referenceID": 23, "context": "Previous approaches to the structured-prediction problem, such as inverse reinforcement learning (IRL) address the underlying planning process in the learning algorithm (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Ross et al., 2011).", "startOffset": 169, "endOffset": 251}, {"referenceID": 21, "context": "Previous approaches to the structured-prediction problem, such as inverse reinforcement learning (IRL) address the underlying planning process in the learning algorithm (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart et al., 2008; Ross et al., 2011). Our contribution is therefore complimentary to these methods, and can be easily incorporated within more advanced learning algorithms, such as those proposed by Ross et al. (2011). We further discuss related work in Section 5.", "startOffset": 189, "endOffset": 433}, {"referenceID": 3, "context": "A standard model for sequential decision making and planning is the Markov decision process (MDP) (Bellman, 1957; Bertsekas, 2012).", "startOffset": 98, "endOffset": 130}, {"referenceID": 5, "context": "A standard model for sequential decision making and planning is the Markov decision process (MDP) (Bellman, 1957; Bertsekas, 2012).", "startOffset": 98, "endOffset": 130}, {"referenceID": 5, "context": "In practice, VI can only be applied for a finite number of iterations, yielding an approximate solution, for which error bounds can be derived (Bertsekas, 2012).", "startOffset": 143, "endOffset": 160}, {"referenceID": 13, "context": "Convolutional neural networks (CNNs; LeCun et al. 1998; Bengio 2009) are neural networks with a particular architecture that has proved useful for computer vision, among other domains.", "startOffset": 30, "endOffset": 68}, {"referenceID": 4, "context": "Convolutional neural networks (CNNs; LeCun et al. 1998; Bengio 2009) are neural networks with a particular architecture that has proved useful for computer vision, among other domains.", "startOffset": 30, "endOffset": 68}, {"referenceID": 1, "context": "We consider a structured prediction (Bakir et al., 2007; Nowozin & Lampert, 2011) problem in which the input consists of a dataset of images { x }", "startOffset": 36, "endOffset": 81}, {"referenceID": 6, "context": "Na\u0131\u0308vely, this problem can be cast as a standard supervised learning problem (Bishop, 2006), by concatenating x and s into a single observation.", "startOffset": 77, "endOffset": 91}, {"referenceID": 10, "context": "Our approach can be extended to general state spaces by adding a transformation layer between the state and the value function (Jaderberg et al., 2015).", "startOffset": 127, "endOffset": 151}, {"referenceID": 17, "context": "In this sense, the the network is learning a reward shaping (Ng et al., 1999) from data.", "startOffset": 60, "endOffset": 77}, {"referenceID": 20, "context": "The difficulty of training deep recurrent models such as the VI block described above increases with the number of VI iterationsK (Pascanu et al., 2012).", "startOffset": 130, "endOffset": 152}, {"referenceID": 17, "context": "In the planning literature, several studies investigated the use of reward-shaping (Ng et al., 1999), hierarchical macro-actions (Mann & Mannor, 2014), and state-aggregation (Taylor et al.", "startOffset": 83, "endOffset": 100}, {"referenceID": 27, "context": ", 1999), hierarchical macro-actions (Mann & Mannor, 2014), and state-aggregation (Taylor et al., 2009; Bertsekas, 2012) for reducing the number of iterations required for VI to converge, with the goal of reducing computation time.", "startOffset": 81, "endOffset": 119}, {"referenceID": 5, "context": ", 1999), hierarchical macro-actions (Mann & Mannor, 2014), and state-aggregation (Taylor et al., 2009; Bertsekas, 2012) for reducing the number of iterations required for VI to converge, with the goal of reducing computation time.", "startOffset": 81, "endOffset": 119}, {"referenceID": 2, "context": "We trained several neural-network based multi-class logistic regression classifiers using stochastic gradient descent, with an RMSProp step size (Tieleman & Hinton, 2012), implemented in the Theano (Bastien et al., 2012) library.", "startOffset": 198, "endOffset": 220}, {"referenceID": 1, "context": "Learning to predict trajectories is an instance of structured prediction problems (Bakir et al., 2007; Nowozin & Lampert, 2011).", "startOffset": 82, "endOffset": 127}, {"referenceID": 21, "context": "In particular, structured prediction strategies such as maximum-margin prediction (Ratliff et al., 2006) have been applied to imitation learning of trajectories from demonstrations.", "startOffset": 82, "endOffset": 104}, {"referenceID": 30, "context": "A similar approach is also used in inverse reinforcement learning (IRL) (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011), which has been used for similar domains of predicting trajectories from overhead images as in this work (Kitani et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 14, "context": "A similar approach is also used in inverse reinforcement learning (IRL) (Abbeel & Ng, 2004; Ziebart et al., 2008; Levine et al., 2011), which has been used for similar domains of predicting trajectories from overhead images as in this work (Kitani et al.", "startOffset": 72, "endOffset": 134}, {"referenceID": 22, "context": ", 2011), which has been used for similar domains of predicting trajectories from overhead images as in this work (Kitani et al., 2012; Ratliff et al., 2009).", "startOffset": 113, "endOffset": 156}, {"referenceID": 23, "context": "However, our approach can be easily incorporated within a no-regret structured prediction method such DAgger (Ross et al., 2011) to potentially yield improved results.", "startOffset": 109, "endOffset": 128}, {"referenceID": 25, "context": "In a different study, Singh et al. (2010) used evolutionary algorithms to learn a cost function that accelerates learning.", "startOffset": 22, "endOffset": 42}, {"referenceID": 29, "context": ", 2015), and the recurrent networks of Zaremba et al. (2015). Our approach can be seen as a neural-network approximation of dynamic programming (VI), learned from examples.", "startOffset": 39, "endOffset": 61}, {"referenceID": 22, "context": "While such problems have important applications in activity forecasting and path-planning (Kitani et al., 2012; Ratliff et al., 2009), generalizing our approach to different problem structures is important.", "startOffset": 90, "endOffset": 133}, {"referenceID": 10, "context": "First, the mapping from the state input to the shaped-reward map can be generalized by adding a spatial transformation layer (Jaderberg et al., 2015) between them.", "startOffset": 125, "endOffset": 149}, {"referenceID": 19, "context": "conditioning the kernels on the input (Oh et al., 2015) may provide an additional flexibility.", "startOffset": 38, "endOffset": 55}, {"referenceID": 16, "context": "Recent RL advances (Mnih et al., 2015; Schulman et al., 2015) showed that CNN-based policies can be trained successfully in a RL setting.", "startOffset": 19, "endOffset": 61}, {"referenceID": 24, "context": "Recent RL advances (Mnih et al., 2015; Schulman et al., 2015) showed that CNN-based policies can be trained successfully in a RL setting.", "startOffset": 19, "endOffset": 61}], "year": 2016, "abstractText": "We introduce the value iteration network: a fully differentiable neural network with a \u2018planning module\u2019 embedded within. Value iteration networks are suitable for making predictions about outcomes that involve planning-based reasoning, such as predicting a desired trajectory from an observation of a map. Key to our approach is a novel differentiable approximation of the valueiteration algorithm, which can be represented as a convolutional neural network, and trained endto-end using standard backpropagation. We evaluate our value iteration networks on the task of predicting optimal obstacle-avoiding trajectories from an image of a landscape, both on synthetic data, and on challenging raw images of the Mars terrain.", "creator": "LaTeX with hyperref package"}}}