{"id": "1602.03960", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2016", "title": "TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice Questions", "abstract": "We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling.", "histories": [["v1", "Fri, 12 Feb 2016 03:54:43 GMT  (76kb,D)", "http://arxiv.org/abs/1602.03960v1", "Keywords: Data, General Knowledge, Tables, Question Answering, MCQ, Crowd-sourcing, Mechanical Turk"]], "COMMENTS": "Keywords: Data, General Knowledge, Tables, Question Answering, MCQ, Crowd-sourcing, Mechanical Turk", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sujay kumar jauhar", "peter turney", "eduard hovy"], "accepted": false, "id": "1602.03960"}, "pdf": {"name": "1602.03960.pdf", "metadata": {"source": "CRF", "title": "TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice Questions", "authors": ["Sujay Kumar Jauhar", "Peter Turney", "Eduard Hovy"], "emails": ["hovy}@cs.cmu.edu", "petert@allenai.org"], "sections": [{"heading": null, "text": "Keywords: General Knowledge, Tables, Question Answering, MCQ, Crowd-sourcing, Mechanical Turk"}, {"heading": "1. Introduction", "text": "The Aristo project at AI2 (Clark, 2015) uses standardized science exams as drivers for research in Artificial Intelligence. Aristo\u2019s question answering format exposes a variety of interesting problems and challenges in NLP (Weston et al., 2015), such as information extraction, semantic modelling and reasoning. An important component of a system that performs Question Answering (QA) is a store of background knowledge for fact-checking and reasoning. This store can be in a variety of modes and formalisms: largescale extracted and curated knowledge bases (Fader et al., 2014), structured models such as Markov Logic Networks (Khot et al., 2015), or simple text corpora in information retrieval approaches (Tellex et al., 2003). There is, however, a fundamental trade-off in the expressive power of a formalism and its ability to be curated easily using existing data and tools. In this paper we describe our work on building knowledge tables, whose semi-structured format affords a balance between expressive power and ease of creation. Additionally, we introduce our efforts at building a large bank of Multiple-Choice Questions (MCQs) using crowd-sourcing on Mechanical Turk (MTurk) by imposing structural constraints on the MCQs from the tables. These constraints lead to consistency in MCQ quality and additionally enable us to harvest alignment information between questions and table cells with little additional effort from annotators. The tables, the MCQs, and the alignments between the two constitute a trio of resources that are, to the best of our knowledge, a first of their kind. We are releasing this dataset publicly1, and we believe it will not only be useful to those working on question answering, but present interesting challenges to researchers exploring a number of related areas.\n1http://allenai.org/content/data/TabMCQ_ v_1.0.zip"}, {"heading": "2. Related Work", "text": "In related recent work Pasupat and Liang (2015) create a dataset of QA pairs over tables. However, their annotation setup does not impose structural constraints from tables, and produces simple QA pairs rather than MCQs. (Yin et al., 2015) and (Neelakantan et al., 2015) use tables in the context of question answering, but deal with synthetically generated query data for those tables. More generally tables have been related to QA in the context of queries over relational databases (Cafarella et al., 2008; Pimplikar and Sarawagi, 2012). Regarding crowd-sourcing for question creation, Aydin et al. (2014) harvest MCQs via a gamified app. However their work does not involve tables. Monolingual alignment datasets have also been explored separately, for example by Brockett (2007) in the context of Textual Entailment."}, {"heading": "3. Motivation for the Dataset", "text": "Our motivation for constructing and releasing this data stems from promising preliminary results on using tables and their alignments to MCQs for solving AI2\u2019s Aristo challenge. We manually annotated 77 of the 108 4th grade science exam questions in the Regents dataset2 for alignment to tables. The resulting data was exactly like the one in this paper, but at a fraction of the scale. Even with this small amount of data we were able to build a system that rivaled our best in-house solvers of the Aristo challenge. A larger dataset will enable us to exceed this performance, and also allow us to explore models that require greater amounts of data (such as Neural Networks). A detailed description of our system is beyond the scope of this paper, but in summary it uses a feature-rich log-linear model to score table cells on their relevance to answering a\n2http://allenai.org/content/data/Regents. zip\nar X\niv :1\n60 2.\n03 96\n0v 1\n[ cs\n.C L\n] 1\n2 Fe\nb 20\n16\nquestion. A combination of structural and textual features are used to capitalize on the semi-structured format of tables."}, {"heading": "4. General Knowledge Tables", "text": "In this section we present the set of tables of curated natural language facts. We discuss their semi-structured form and its utility for knowledge representation. We then detail the data and possibilities for its extension."}, {"heading": "4.1. Tables as a Form of Knowledge Representation", "text": "An example of the kind of table we construct is given in Table 1. This format is semi-structured: the rows of the table (with the exception of the header) are essentially a list of sentences, but with well-defined recurring filler patterns. Together with the header, these patterns divide the rows into meaningful columns. The resulting table structure has some interesting semantics. A row in a table corresponds to a fact in the world3. The cells in a row correspond to concepts, entities, or processes that participate in this fact. A content column4 corresponds to a group of concepts, entities, or processes that are the same type. The head of the column is an abstract description of the type. We may view the head as a hypernym and the cells in the column below as co-hyponyms of the head. The structure of tables also exposes sets of analogies. More formally, let rowi and rowj be any two rows in a table. Let colk and colm be any two content columns in a table. The four cells where the rows and columns intersect form an analogy: celli,k is to celli,m as cellj,k is to cellj,m. That is, the relation between celli,k and celli,m is highly similar to the relation between cellj,k and cellj,m. For example, in Table 1 one such analogy is: \u201cMelting\u201d is to \u201csolid\u201d as \u201cCondensation\u201d is to \u201cgas\u201d. Here the latent relation being represented is the one between a \u201cPhase Change\u201d and the \u201cinitial state\u201d of a substance upon which it acts. The table is essentially a compact representation of a large number of scientific analogies. The semi-structured nature of this data is flexible. Since facts are presented as sentences, the tables can simply act as a text corpus for information retrieval. Depending on the end application, more complex models can rely on the inherent structure in tables. They can be used as is for information extraction tasks that seek to zero in on specific nuggets of information, or they can be converted to logical expressions using rules defined over neighboring cells. Regarding construction, the recurring filler patterns can be used as templates to extend the tables semi-automatically by searching over large corpora for similar facts (Ravichandran and Hovy, 2002)."}, {"heading": "4.2. The Dataset", "text": "Our dataset of tables was, for the most part, constructed manually by the second author of this paper and a knowl-\n3Rows may also be seen as predicates, or more generally frames with typed arguments.\n4We differentiate these from filler columns, which only contain a recurring pattern, and no information in their header cells.\nedge engineer. The target domain for the tables was the Regents 4th grade science exam. The majority of tables were constructed to contain topics and facts in this exam (with additional facts being added once a coherent topic had been identified), with the rest being targeted at an additional inhouse question bank of 500 questions. The dataset consists of 65 hand-crafted tables organized topic-wise, from bounded tables such as the one about phase changes, to virtually unbounded tables, such as the kind of energy used in performing an action. An additional collection of 5 semi-automatically generated tables contains a more heterogeneous mix of facts across topics. A total of 3851 facts, or information rows, are present in the manually constructed tables, while an additional 4415 are present in the semi-automatically generated ones. The number of content columns in a given table varies from 2 to 5."}, {"heading": "4.3. Future Extensions", "text": "While the tables are reasonably complete with regard to the target domain for which they were constructed, they are not comprehensive for any open-ended knowledge retrieval task. We are in the process of investigating ways to extend the tables semi-automatically to cover a wider collection of facts, by using the set of existing facts as seeds. An inhouse fuzzy search engine allows us to search over patterns consisting not only of words and wild-card symbols, but parts-of-speech and semantically related concepts. An interesting avenue for future research is to completely automate the process of this extension, and even construct semi-structured knowledge tables on the fly for targeted queries."}, {"heading": "5. Crowd-sourcing Multiple-choice Questions from Tables", "text": "In this section we outline the MCQs generated from tables. We first describe the annotation task and argue for constraints imposed by table structure. We next describe the data we generated via crowd-sourcing. Finally, we also list some possible extensions to the kinds of questions we generate."}, {"heading": "5.1. The Annotation Task \u2013 Constraining MCQs with Tables", "text": "We use MTurk to generate MCQs by imposing constraints derived from the structure of the tables. These constraints help annotators create questions with scaffolding information, and lead to consistent quality in the generated output. An additional benefit of this format is the alignment information from cells in the tables to the MCQs that are generated as a by-product. The annotation task guides a Turker through the process of creating an MCQ. Given a table, we choose a target cell to be the correct answer for a new MCQ. First, we ask Turkers to create a question by primarily using information from the rest of the row containing the target cell, in such a way that the target cell is its correct answer. Then annotators must select all the cells in the row that they actually used to construct the question. Following this, Turkers must construct 4 succinct choices for the question, one of which is\nthe correct answer and the other 3 are distractors. These distractors must be formed from other cells in the column containing the target cell. If there are insufficient unique cells in the column Turkers may create their own. To simplify and reinforce table constraints over rows and columns, our interface highlights table cells as shown in Figure 1. Turkers are allowed to rephrase and shuffle the contents of cells to arrive at the 4 succinct choices. Consequently we require Turkers to indicate which of the 4 choices that they created is the correct one for their MCQ. In addition to an MCQ we obtain the following alignment information with no additional effort from annotators. We know which table, row, and column contain the answer, and thus also which header cells might be relevant to the question. Additionally, we know the cells of a row that were used to construct a question."}, {"heading": "5.2. The Dataset", "text": "We created an individual HIT for every content bearing cell from each one of the 65 manually constructed tables. We paid annotators a reward of 10 cents per MCQ, and asked for 1 annotation per HIT for most tables. However, for an initial set of 4 tables which we used in a pilot study, we asked for 3 annotations per HIT5. In terms of qualifications, we required Turkers to have a HIT approval rating of 95% or higher, with a minimum of at least 500 HITs approved. Additionally, we restricted the demographics of our workers to the US. The annotators were able to create an MCQ from a table in approximately 70 seconds. They were also largely successful in their attempts. Manual inspection of the generated output also revealed that questions are of consistently good quality. They are certainly good enough for training machine learning models and many are even good enough as evaluation data for QA. A sample of generated MCQs are presented in Table 2. Once annotations were completed we implemented some simple sanity checks to evaluate the data before approving HITs. These included things like checking if an MCQ has at least 3 choices, whether choices are repeated, etc. Through this process we rejected about 0.7% of submitted HITs. We had to further prune our data to discard some MCQs after accepting HITs due to corrupted data, or badly constructed MCQs. A total of 160 MCQs were lost through the cleanup. Thus our dataset of MCQs covers the overwhelming majority, but not all, of the cells in the collection of tables. In the end our complete data consists of 9091 MCQs, which is \u2013 to the best of our knowledge \u2013 orders of magnitude\n5The goal was to observe whether there was diversity in the MCQs created for a target cell. The results were not sufficiently conclusive to warrant a threefold increase in the cost of creation.\nlarger than any existing collection of science exam MCQs available for research. These MCQs also come with alignment information to tables, rows, columns and cells."}, {"heading": "5.3. Future Extensions", "text": "In the future we hope to generate MCQs with other kinds of constraints from tables. Currently the constraints are row-dominant: the questions are based on cells from the row in which a target cell occurs. Such questions are often information look-up questions \u2013 the simplest kind. We plan to explore column-dominant questions, which would lead to questions about abstraction or specification (based on the hypernym-hyponym relationship in columns). Other structural semantics we hope to investigate are multi-row constraints that result in comparisons, or joins on multiple tables with related columns that results in chaining or reasoning."}, {"heading": "6. Utility to the Research Community", "text": "We believe that the data we have collected will be useful to people with diverse research interests in the NLP community. While the tables were designed for facts covered in 4th grade science exams, the contents are general enough to be used as background knowledge in simple domains. The additional structure presents interesting challenges to people interested in information extraction, especially when considered with the alignments to MCQs. They can be used for applications such as question parsing and answer type extraction. The structural semantics of tables (as described in Section 4.1.) can also present a interesting challenges for those interested in lexical semantics and analogical reasoning. Jointly, the tables and MCQs can be used for QA, \u2013 with great effect as summarized in Section 3."}, {"heading": "7. Conclusion", "text": "We have presented a dataset of tables, MCQs and alignment information between the two. Our preliminary experiments with this trio of resources even on a much smaller scale showed promising results, rivaling the best current systems on the Aristo QA challenge. We are thus led to believe the data described in this paper will be very useful to researchers working on QA. However the usefulness of the dataset potentially extends to other areas of NLP as well. The tables and MCQs individually represent resources of interest to people requiring background knowledge or working on semantic modelling and information extraction. Moreover, alignment information is expensive and time-consuming to annotate and consequently scarce; yet alignment of textual fragments is a recurring theme in NLP. Our setup allows us to harvest this resource easily at scale, thus becoming useful to people working on problems like paraphrasing, textual entailment and question parsing."}, {"heading": "8. Acknowledgements", "text": "We\u2019d like to thank the Allen Institute for Artificial Intelligence for generously funding the creation of this dataset, and permitting its release. Thanks also go to Isaac Cowhey for his effort in painstakingly building the tables. The first and third authors of this paper were supported in part by the following grants: NSF grant IIS-1143703, NSF award IIS-1147810, DARPA grant FA87501220342."}, {"heading": "9. References", "text": "Aydin, B. I., Yilmaz, Y. S., Li, Y., Li, Q., Gao, J., and\nDemirbas, M. (2014). Crowdsourcing for multiplechoice question answering. In Twenty-Sixth IAAI Conference. Brockett, C. (2007). Aligning the rte 2006 corpus. Technical Report MSR-TR-2007-77, Microsoft Research, June. Cafarella, M. J., Halevy, A., Wang, D. Z., Wu, E., and Zhang, Y. (2008). Webtables: exploring the power of tables on the web. Proceedings of the VLDB Endowment, 1(1):538\u2013549. Clark, P. (2015). Elementary school science and math tests as a driver for ai: Take the aristo challenge. Proceedings of IAAI, 2015. Fader, A., Zettlemoyer, L., and Etzioni, O. (2014). Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1156\u20131165. ACM. Khot, T., Balasubramanian, N., Gribkoff, E., Sabharwal, A., Clark, P., and Etzioni, O. (2015). Exploring markov\nlogic networks for question answering. Proceedings of EMNLP, 2015. Neelakantan, A., Le, Q. V., and Sutskever, I. (2015). Neural programmer: Inducing latent programs with gradient descent. arXiv preprint arXiv:1511.04834. Pasupat, P. and Liang, P. (2015). Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305. Pimplikar, R. and Sarawagi, S. (2012). Answering table queries on the web using column keywords. Proceedings of the VLDB Endowment, 5(10):908\u2013919. Ravichandran, D. and Hovy, E. (2002). Learning surface text patterns for a question answering system. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41\u201347. Association for Computational Linguistics. Tellex, S., Katz, B., Lin, J., Fernandes, A., and Marton, G. (2003). Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 41\u201347. ACM. Weston, J., Bordes, A., Chopra, S., and Mikolov, T. (2015). Towards ai-complete question answering: a set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698. Yin, P., Lu, Z., Li, H., and Kao, B. (2015). Neural enquirer: Learning to query tables. arXiv preprint arXiv:1512.00965."}], "references": [{"title": "Crowdsourcing for multiplechoice question answering", "author": ["B.I. Aydin", "Y.S. Yilmaz", "Y. Li", "Q. Li", "J. Gao", "M. Demirbas"], "venue": "Twenty-Sixth IAAI Conference.", "citeRegEx": "Aydin et al\\.,? 2014", "shortCiteRegEx": "Aydin et al\\.", "year": 2014}, {"title": "Aligning the rte 2006 corpus", "author": ["C. Brockett"], "venue": "Technical Report MSR-TR-2007-77, Microsoft Research, June.", "citeRegEx": "Brockett,? 2007", "shortCiteRegEx": "Brockett", "year": 2007}, {"title": "Webtables: exploring the power of tables on the web", "author": ["M.J. Cafarella", "A. Halevy", "D.Z. Wang", "E. Wu", "Y. Zhang"], "venue": "Proceedings of the VLDB Endowment, 1(1):538\u2013549.", "citeRegEx": "Cafarella et al\\.,? 2008", "shortCiteRegEx": "Cafarella et al\\.", "year": 2008}, {"title": "Elementary school science and math tests as a driver for ai: Take the aristo challenge", "author": ["P. Clark"], "venue": "Proceedings of IAAI, 2015.", "citeRegEx": "Clark,? 2015", "shortCiteRegEx": "Clark", "year": 2015}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1156\u20131165. ACM.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint arXiv:1511.04834.", "citeRegEx": "Neelakantan et al\\.,? 2015", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["P. Pasupat", "P. Liang"], "venue": "arXiv preprint arXiv:1508.00305.", "citeRegEx": "Pasupat and Liang,? 2015", "shortCiteRegEx": "Pasupat and Liang", "year": 2015}, {"title": "Answering table queries on the web using column keywords", "author": ["R. Pimplikar", "S. Sarawagi"], "venue": "Proceedings of the VLDB Endowment, 5(10):908\u2013919.", "citeRegEx": "Pimplikar and Sarawagi,? 2012", "shortCiteRegEx": "Pimplikar and Sarawagi", "year": 2012}, {"title": "Learning surface text patterns for a question answering system", "author": ["D. Ravichandran", "E. Hovy"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41\u201347. Association for Computational Linguistics.", "citeRegEx": "Ravichandran and Hovy,? 2002", "shortCiteRegEx": "Ravichandran and Hovy", "year": 2002}, {"title": "Quantitative evaluation of passage retrieval algorithms for question answering", "author": ["S. Tellex", "B. Katz", "J. Lin", "A. Fernandes", "G. Marton"], "venue": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages", "citeRegEx": "Tellex et al\\.,? 2003", "shortCiteRegEx": "Tellex et al\\.", "year": 2003}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Neural enquirer: Learning to query tables", "author": ["P. Yin", "Z. Lu", "H. Li", "B. Kao"], "venue": "arXiv preprint arXiv:1512.00965.", "citeRegEx": "Yin et al\\.,? 2015", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "The Aristo project at AI2 (Clark, 2015) uses standardized science exams as drivers for research in Artificial Intelligence.", "startOffset": 26, "endOffset": 39}, {"referenceID": 10, "context": "Aristo\u2019s question answering format exposes a variety of interesting problems and challenges in NLP (Weston et al., 2015), such as information extraction, semantic modelling and reasoning.", "startOffset": 99, "endOffset": 120}, {"referenceID": 4, "context": "This store can be in a variety of modes and formalisms: largescale extracted and curated knowledge bases (Fader et al., 2014), structured models such as Markov Logic Networks (Khot et al.", "startOffset": 105, "endOffset": 125}, {"referenceID": 9, "context": ", 2015), or simple text corpora in information retrieval approaches (Tellex et al., 2003).", "startOffset": 68, "endOffset": 89}, {"referenceID": 11, "context": "(Yin et al., 2015) and (Neelakantan et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": ", 2015) and (Neelakantan et al., 2015) use tables in the context of question answering, but deal with synthetically generated query data for those tables.", "startOffset": 12, "endOffset": 38}, {"referenceID": 2, "context": "More generally tables have been related to QA in the context of queries over relational databases (Cafarella et al., 2008; Pimplikar and Sarawagi, 2012).", "startOffset": 98, "endOffset": 152}, {"referenceID": 7, "context": "More generally tables have been related to QA in the context of queries over relational databases (Cafarella et al., 2008; Pimplikar and Sarawagi, 2012).", "startOffset": 98, "endOffset": 152}, {"referenceID": 2, "context": "In related recent work Pasupat and Liang (2015) create a dataset of QA pairs over tables.", "startOffset": 23, "endOffset": 48}, {"referenceID": 0, "context": "Regarding crowd-sourcing for question creation, Aydin et al. (2014) harvest MCQs via a gamified app.", "startOffset": 48, "endOffset": 68}, {"referenceID": 0, "context": "Regarding crowd-sourcing for question creation, Aydin et al. (2014) harvest MCQs via a gamified app. However their work does not involve tables. Monolingual alignment datasets have also been explored separately, for example by Brockett (2007) in the context of Textual Entailment.", "startOffset": 48, "endOffset": 243}, {"referenceID": 8, "context": "Regarding construction, the recurring filler patterns can be used as templates to extend the tables semi-automatically by searching over large corpora for similar facts (Ravichandran and Hovy, 2002).", "startOffset": 169, "endOffset": 198}], "year": 2016, "abstractText": "We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling.", "creator": "LaTeX with hyperref package"}}}