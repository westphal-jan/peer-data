{"id": "1602.04621", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2016", "title": "Deep Exploration via Bootstrapped DQN", "abstract": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.", "histories": [["v1", "Mon, 15 Feb 2016 10:54:20 GMT  (5872kb,D)", "http://arxiv.org/abs/1602.04621v1", null], ["v2", "Fri, 1 Jul 2016 16:23:55 GMT  (7516kb,D)", "http://arxiv.org/abs/1602.04621v2", null], ["v3", "Mon, 4 Jul 2016 17:11:52 GMT  (7516kb,D)", "http://arxiv.org/abs/1602.04621v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY stat.ML", "authors": ["ian osband", "charles blundell", "alexander pritzel", "benjamin van roy"], "accepted": true, "id": "1602.04621"}, "pdf": {"name": "1602.04621.pdf", "metadata": {"source": "META", "title": "Deep Exploration via Bootstrapped DQN", "authors": ["Ian Osband", "Charles Blundell", "Alexander Pritzel", "Benjamin Van Roy"], "emails": ["IOSBAND@GOOGLE.COM", "CBLUNDELL@GOOGLE.COM", "APRITZEL@GOOGLE.COM", "BVR@STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "We study the reinforcement learning (RL) problem where an agent interacts with an unknown environment. The agent takes a sequence of actions and learns from observations and rewards. The goal is to maximize cumulative rewards. Unlike standard planning problems, an RL agent does not begin with perfect knowledge of the environment, but learns through experience. This leads to a fundamental trade-off of exploration versus exploitation; the agent may improve its future rewards by exploring poorly understood states and actions, but this may require sacrificing immediate rewards.\nTo learn efficiently an agent should explore only when there are valuable learning opportunities. Common heuristics such as -greedy and Boltzmann exploration leave enormous room for improvement. Since such algorithms explore at random times to balance effort between exploration and exploitation, we call these dithering strategies. Further, as the consequences of an action may extend over multiple timesteps, the agent should reason about the informational value of possible observation sequences. Without this sort of temporally extended (deep) exploration, learning times can worsen by an exponential factor.\nThe theoretical RL literature offers a variety of provablyefficient approaches to deep exploration (e.g., Jaksch et al. (2010); Guez et al. (2012)). However, most of these are designed for Markov decision processes (MDPs) with small finite state spaces, while others require solving computationally intractable planning tasks. These algorithms are not practical in complex environments where an agent must generalize to operate effectively.\nFor this reason, large-scale applications of RL have relied upon dithering exploration strategies which are statistically inefficient. These methods have lead to several high-profile applications with groundbreaking results (Tesauro, 1995; Mnih, 2015). Nevertheless, we might hope to improve upon these exploration heuristics. We review related literature in more detail in Section 4.\nIn this paper, we consider an alternative approach to exploration based on randomized value functions. Previous work demonstrated that randomized value functions can efficiently explore in tandem with linearly-parameterized value function generalization (Osband et al., 2014). We present a natural and principled extension of this approach that enables use of complex non-linear generalization methods such as deep neural networks.\nOur paper begins with a review of how to represent uncertainty with neural networks. We show that the bootstrap with random initialization can produce reasonable uncertainty estimates for neural networks at low computational cost. In Section 3 we present our algorithm, bootstrapped deep Q networks (DQN), which leverages these uncertainty estimates for efficient (and deep) exploration. We review related work in Section 4. Section 5 presents a series of didactic experiments that are designed to highlight the importance of deep exploration and that are intractable for any shallow exploration strategy. Bootstrapped DQN succeeds in tasks that require deep exploration extending over 100 time periods and even in difficult stochastic MDPs. Finally we demonstrate that these benefits can extend to large scale problems that are not designed to highlight deep exploration. Section 6 presents our results from application to ar X\niv :1\n60 2.\n04 62\n1v 1\n[ cs\n.L G\n] 1\n5 Fe\nb 20\nAtari 2600. Bootstrapped DQN substantially reduces learning times and improves performance across most games. This algorithm is computationally efficient and amenable to parallelization; on a single machine our implementation runs roughly 20% slower than DQN."}, {"heading": "2. Uncertainty for neural networks", "text": "Deep neural networks represent the state of the art in many supervised learning domains (Krizhevsky et al., 2012). This is largely due to their flexibility, scalability and inductive bias, which allows them to learn effective feature representations. The exploration method we study in this paper is designed to be statistically and computationally efficient when used in conjunction with neural network representations of the value function.\nTo explore efficiently, it is important to quantify uncertainty in value estimates so that the agent can judge potential benefits of exploratory actions. The neural network literature presents a sizable body of work on uncertainty quantification founded on parametric Bayesian inference. These include variational Bayes (Graves, 2011; Blundell et al., 2015), assumed density filtering (Herna\u0301ndez-Lobato & Adams, 2015), dropout-based variational inference (Gal & Ghahramani, 2015; Kingma et al., 2015), and stochastic gradient Langevin dynamics (Teh et al., 2015). Instead of appealing to parametric assumptions about the uncertainty present in RL problems, we use the non-parametric bootstrap (Efron, 1982) to obtain a distribution over functions represented by the neural network.\nThere are several variants of the bootstrap (Efron & Tibshirani, 1994), each of which relies upon data-based simulation. The key idea is to approximate a population distribution by a sample distribution (Efron & Tibshirani, 1994). In its most common form, the bootstrap takes as input a data set D and an estimator \u03c8. To generate a sample from the bootstrapped distribution, a data set D\u0303 of cardinality equal to that of D is sampled uniformly with replacement from D. The bootstrap sample estimate is then taken to be \u03c8(D\u0303).\nThe bootstrap is widely hailed as a great advance of 20th century applied statistics and comes with theoretical guarantees (Bickel & Freedman, 1981). Although the bootstrap was introduced as a frequentist method, several variants have natural interpretations under Bayesian and empirical Bayes analysis (Rubin et al., 1981). In some settings, when the original data set is augmented with appropriate synthetic samples, bootstrap samples are distributed according to the posterior distribution for an appropriate prior (Osband & Van Roy, 2015). The bootstrap is highly parallelizable and, as such, amenable to distributed computation. The approach can even scale to massive data with sub-linear computational cost (Kleiner et al., 2014).\nIn Figure 1 we present an efficient and scalable method for generating bootstrap samples from a large and deep neural network. The network consists of a shared architecture with K bootstrapped \u201cheads\u201d branching off independently. Each head is trained only on its bootstrapped sub-sample of the data, which can be generated online (Owen et al., 2012). Thus each head represents \u03c8(D\u0303) in the classical view of bootstrap. The shared network learns a joint feature representation, this can provide significant computational advantages at the cost of lower diversity between heads. This type of bootstrap can even be implemented efficiently by a single forward/backward pass of backpropagation; it can be thought of as a data-dependent dropout, where the dropout mask for each head is fixed for each data point (Srivastava et al., 2014).\nFigure 2 presents an example of uncertainty estimates from bootstrapped neural networks on a regression task with noisy data. We trained a fully-connected 2-layer neural networks with 50 rectified linear units (ReLU) in each layer on 50 bootstrapped samples from the data. As is standard practice, we initialize these networks with random parameter values, this induces an important initial diversity in the models. We were unable to generate effective uncertainty estimates for this problem using the dropout approach in prior literature (Gal & Ghahramani, 2015). Further details are provided in Appendix A."}, {"heading": "3. Bootstrapped DQN", "text": "For a policy \u03c0 we define the value of an action a in state s\nQ\u03c0(s,a) :=Es,a,\u03c0 [ \u221e\u2211 t=1 \u03b3trt ] ,\nwhere \u03b3 \u2208 (0, 1) is a discount factor that balances immediate versus future rewards rt. This expectation indicates that the initial state is s, the initial action is a, and thereafter actions are selected by the policy \u03c0. The optimal value is Q\u2217(s, a) := max\u03c0 Q\n\u03c0(s, a). To scale to large problems, we learn a parameterized estimate of the Q-value function Q(s, a; \u03b8) rather than a tabular encoding. We use a neural network to estimate this value.\nThe Q-learning update after taking action at in state st and observing reward rt and transitioning to st+1 is given by\n\u03b8t+1\u2190 \u03b8t+\u03b1(yQt \u2212Q(st,at;\u03b8t))\u2207\u03b8Q(st,at;\u03b8t) (1)\nwhere \u03b1 is the scalar learning rate and yQt is the target value rt+\u03b3maxaQ(st+1,a;\u03b8\n\u2212). \u03b8\u2212 are target network parameters fixed \u03b8\u2212=\u03b8t.\nSeveral important modifications to the Q-learning update improve stability for DQN (Mnih, 2015). First the algorithm learns from sampled transitions from an experience buffer, rather than learning fully online. Second the algorithm uses a target network with parameters \u03b8\u2212 that are copied from the learning network \u03b8\u2212 \u2190 \u03b8t only every \u03c4 time steps and then kept fixed in between updates. Double DQN (DDQN) modifies the target yQt :\nyQt \u2190 rt + \u03b3max a\nQ ( st+1, arg max\na Q(st+1, a; \u03b8t); \u03b8 \u2212). DDQN can demonstrate improved performance and stability (Van Hasselt et al., 2015). In this paper we use the DDQN update for all DQN variants unless explicitly stated.\nOur algorithm, bootstrapped DQN in Algorithm 1, modifies DQN to produce distribution over Q-values via the bootstrap. At the start of each episode, bootstrapped DQN samples a single Q-value function from its approximate posterior. The agent then follows the policy which is optimal for that sample for the duration of the episode. This is a natural extension of the Thompson sampling heuristic to RL that allows for temporally extended (or deep) exploration (Strens, 2000; Osband et al., 2013).\nIn order to implement this algorithm efficiently online we build up K \u2208 N bootstrapped estimates of the Q-value function in parallel as in Figure 1. Importantly, each one of these value function function heads Qk(s, a; \u03b8) is trained against its own target network Qk(s, a; \u03b8\u2212). This means that each Q1, .., QK provide a temporally extended (and consistent) estimate of the value uncertainty via TD estimates. We approximate a bootstrap sample by selecting k \u2208 {1, ..,K} uniformly at random and following Qk for\nthe duration of that episode. Bootstrapped DQN exhibits deep exploration unlike the naive application of Thompson sampling to RL which resample every timestep1.\nAlgorithm 1 Bootstrapped DQN\n1: Input: neural network (Qk)Kk=1, sampling distribution P 2: for each episode do 3: Update network parameters via minibatches 4: Sample k\u223cUniform{1,...,K} 5: while not end of episode do 6: Choose at\u2208argmaxaQk(st,a) 7: Receive state st+1 and reward rt from environment 8: Sample bootstrap mask mkt\u223cP for all k 9: Add (at,rt,st+1,mt) to replay buffer\n10: end while 11: end for"}, {"heading": "4. Related work", "text": "The observation that temporally extended exploration is necessary for efficient reinforcement learning is not new. In fact, for any prior distribution over MDPs, the optimal exploration strategy is available through dynamic programming in the Bayesian belief state space. However, the exact solution is intractable even for very simple systems (Burnetas & Katehakis, 1997). RL algorithms provide tractable approximations to this problem.\nMost common RL methods focus on generalization and planning, but address exploration via dithering strategies. Many successful applications employ only -greedy exploration (Tesauro, 1995; Mnih, 2015). However, such exploration strategies can be highly inefficient, as we demonstrate in Section 5.\nPolicy gradient methods offer another approach to RL and induce random exploration since they search over the space of stochastic policies. Although there are several successful applications of policy gradient methods (e.g., Levine et al. (2015)), the form of exploration induced by such algorithms can also be highly inefficient (Kakade, 2003).\nOther approaches aim to approximate Bayes-optimal exploration though tree-based search (Wang et al., 2005; Guez et al., 2014). These approaches are well-founded and, with unlimited computation will converge to the optimal policy. However, practical implementations with finite computation may fail dramatically (Munos, 2014).\nMany exploration strategies are guided by the principle of \u201coptimism in the face of uncertainty\u201d (OFU). These algorithms add an exploration bonus to values of state-action pairs that may lead to useful learning and select actions to maximize these adjusted values. This approach was first\n1We call bootstrapped DQN which resamples a head every timestep Thompson DQN. This is similar to to the \u201cThompson sampling\u201d algorithm of (Gal & Ghahramani, 2015) but uses the bootstrap in place of dropout as an approximate posterior.\nproposed for finite-armed bandits (Lai & Robbins, 1985), but the principle has been extended successfully across bandits with generalization (Rusmevichientong & Tsitsiklis, 2010) and tabular RL (Jaksch et al., 2010). Except for particular deterministic contexts (Wen & Van Roy, 2013), OFU methods that lead to efficient RL in complex domains have been computationally intractable. The work of (Stadie et al., 2015) aims to add an effective bonus through a variation of DQN. The resulting algorithm relies on a large number of hand-tuned parameters and is only suitable for application to deterministic problems. We compare our results on Atari to theirs in Appendix C.\nPerhaps the oldest heuristic for balancing exploration with exploitation is given by Thompson sampling (Thompson, 1933). This bandit algorithm takes a single sample from the posterior at every time step and chooses the action which is optimal for that time step. Thompson sampling offers good performance in bandits and several state of the art guarantees (Russo & Van Roy, 2014).\nTo apply the Thompson sampling principle to RL, an agent should sample a value function from its posterior. The agent must also commit to this sample for several time steps in order to achieve deep exploration (Strens, 2000; Guez et al., 2012). The algorithm PSRL does exactly this, with state of the art guarantees (Osband et al., 2013; Osband & Van Roy, 2014b;a; Abbasi-Yadkori & Szepesva\u0301ri, 2015; Gopalan & Mannor, 2015). However, this algorithm still requires solving a single known MDP, which will usually be intractable for large systems.\nOur new algorithm, bootstrapped DQN, approximates this approach to exploration via randomized value functions sampled from an approximate posterior. Recently, authors have proposed the RLSVI algorithm which accomplishes this for linearly parameterized value functions. Surprisingly, this algorithm recovers state of the art guarantees in the setting with tabular basis functions, but its performance is crucially dependent upon a suitable linear representation of the value function (Osband et al., 2014). We extend these ideas to produce an algorithm that can simultaneously perform generalization and exploration with a flexible nonlinear value function representation. Our method is simple, general and compatible with almost all advances in deep RL at low computational cost and with few tuning parameters."}, {"heading": "5. Deep Exploration", "text": "Uncertainty estimates allow an agent to direct its exploration at potentially informative states and actions. In bandits, this choice of directed exploration rather than dithering generally categorizes efficient algorithms. The story in RL is not as simple, directed exploration is not enough to guarantee efficiency; the exploration must also be deep.\nDeep exploration means exploration which is directed over multiple time steps; it can also be called \u201cplanning to learn\u201d or \u201cfar-sighted\u201d exploration. Unlike bandit problems, which balance actions which are immediately rewarding or immediately informative, RL settings require planning over several time steps (Kakade, 2003). For exploitation, this means that an efficient agent must consider the future rewards over several time steps and not simply the myopic rewards. In exactly the same way, efficient exploration may require taking actions which are neither immediately rewarding, nor immediately informative.\nTo illustrate this distinction, consider a simple deterministic chain {s\u22123, .., s+3} with three step horizon starting from state s0. This MDP is known to the agent a priori, with deterministic actions \u201cleft\u201d and \u201cright\u201d. All states have zero reward, except for the leftmost state s\u22123 which has known reward > 0 and the rightmost state s3 which is unknown. In order to reach either a rewarding state or an informative state within three steps from s0 the agent must plan a consistent strategy over several time steps.\nFigure 3 depicts the planning and look ahead trees for several algorithmic approaches in this example MDP. The action \u201cleft\u201d is gray, the action \u201cright\u201d is black. Rewarding states are depicted as red, informative states as blue. Dashed lines indicate that the agent can plan ahead for either rewards or information. Unlike bandit algorithms, an RL agent can plan to exploit future rewards. Only an RL agent with deep exploration can plan to learn."}, {"heading": "5.1. Testing for deep exploration", "text": "We now present a series of didactic computational experiments designed to highlight the need for deep exploration in RL. The agents will be placed in an environment made\nup of a long chain of states right next to a small reward. All other states have zero reward except at the far end of the chain where they can find a state with much higher reward.\nThe experiments in this section are toy problems intended to be expository rather than entirely realistic. However, they clearly test whether an agent can efficiently balance the potential benefits of delayed information from deep exploration. Balancing a well known and mildly successful strategy versus an unknown, but potentially more rewarding, approach can emerge in many practical applications."}, {"heading": "5.2. Learning from pixels", "text": "The environments in this section may be concisely described by a finite tabular MDP, similar to RiverSwim (Strehl & Littman, 2005). However, we will require our algorithm to interact with the MDP only through raw pixel features. For a chain of length N we will define features of the state \u03c6 : {1, .., N} \u2192 {0, 1}N . We consider two feature mappings, \u201cone-hot\u201d \u03c6oh(st) := (1{x = st})x=1,..,N and \u201cthermometer\u201d \u03c6therm(st) := (1{x \u2264 st})x=1,..,N .\nWe find that bootstrapped DQN is able to explore efficiently with either set of features. We focus upon the thermometer encoding, since this better captures generalization between states. Full details for these experiments are given in Appendix B."}, {"heading": "5.3. Scaling deeper", "text": "We consider a family of deterministic chains of lengthN > 3 as in Figure 4. Each episode of interaction lasts N + 9 steps after which point the agent resets to the initial state s2. We choose this so that the optimal policy is to move right at every step and receive a return of 10 in each episode. However, any shallow exploration strategy will take \u2126(2N ) episodes to learn the optimal policy (Osband et al., 2014).\nWe say that the algorithm has successfully learned the optimal policy when it has successfully completed one hundred episodes with optimal reward of 10. For each chain length, we ran each learning algorithm for 2000 episodes across three seeds. We plot the median time to learn in Figure 5, together with a conservative lower bound of 99 + 2N\u221211 on the expected time to learn for any shallow exploration strategy. Only bootstrapped DQN demonstrates a graceful scaling to long chains which require deep exploration."}, {"heading": "5.4. A difficult stochastic MDP", "text": "Figure 5 shows that bootstrapped DQN can implement effective (and deep) exploration where similar deep RL architectures fail. However, since the underlying system is a small and finite MDP there may be several other simpler strategies which would also solve this problem. We will now consider a difficult variant of this chain system with significant stochastic noise in transitions as depicted in Figure 6. Action \u201cleft\u201d deterministically moves the agent left, but action \u201cright\u201d is only successful 50% of the time and otherwise also moves left. The agent interacts with the MDP in episodes of length 15 and begins each episode at s1. Once again the optimal policy is to head right.\nBootstrapped DQN is unique amongst scalable approaches to efficient exploration with deep RL in stochastic domains. For benchmark performance we implement three algorithms which, unlike bootstrapped DQN, will receive the true tabular representation for the MDP. These algorithms are based on three state of the art approaches to exploration via dithering ( -greedy), optimism (UCRL2; Jaksch et al., 2010) and posterior sampling (PSRL; Osband et al., 2013). We discuss the choice of these benchmarks in Appendix B.\nIn Figure 7 we present the empirical regret of each algorithm averaged over 10 seeds over the first two thousand episodes. The empirical regret is the cumulative difference between the expected rewards of the optimal policy and the realized rewards of each algorithm. We find that bootstrapped DQN achieves similar performance to state of the art efficient exploration schemes such as PSRL even without prior knowledge of the tabular MDP structure and in noisy environments.\nMost telling is how much better bootstrapped DQN does than the state of the art optimistic algorithm UCRL2. Although Figure 7 seems to suggest UCRL2 incurs linear regret, actually it follows its bounds O\u0303(S \u221a AT ) (Jaksch et al., 2010) where S is the number of states and A is the number of actions. We demonstrate this through simulation in Appendix B, together with extended discussion."}, {"heading": "6. Arcade Learning Environment", "text": "We now evaluate our algorithm across 49 Atari games on the Arcade Learning Environment (Bellemare et al., 2012). Importantly, and unlike the experiments in Section 5, these domains are not specifically designed to showcase our algorithm. In fact, many Atari games are structured so that small rewards always indicate part of an optimal policy. This may be crucial for the strong performance observed by dithering strategies2. We find that exploration via bootstrapped DQN produces significant gains versus -greedy in this setting. Bootstrapped DQN reaches peak performance roughly similar to DQN. However, our improved exploration mean we reach human performance on average 30% faster across all games. This translates to significantly improved cumulative rewards through learning.\nWe follow the setup of (Van Hasselt et al., 2015) for our network architecture and benchmark our performance against their algorithm. Our network structure is identical to the convolutional structure of DQN (Mnih, 2015) except we split 10 separate bootstrap heads after the convolutional layer as per Figure 1. Recently, several authors have provided architectural and algorithmic improvements\n2By contrast, imagine the agent received a small reward for dying; dithering strategies would be hopeless, just like Section 5.\nto DDQN (Wang et al., 2015; Schaul et al., 2015). We do not compare our results to these since their advances are orthogonal to our concern and could easily be incorporated to our bootstrapped DQN design. Full details of our experimental set up are available in Appendix C."}, {"heading": "6.1. Implementing bootstrapped DQN at scale", "text": "We now examine how to generate online bootstrap samples for DQN in a computationally efficient manner. We use a shared convolutional network architecture with K bootstrap heads as per Figure 1. This leaves three key questions: how many heads do we need, how should we pass gradients to the shared network and how should we bootstrap data online? Each of these questions involves balancing computational and statistical considerations.\nWe found that even a small number of bootstrap heads was sufficient to incentivize efficient exploration. Figure 8 presents the cumulative reward of bootstrapped DQN on the game Breakout, for different number of heads K. More heads leads to faster learning, but even a small number of heads captures most of the benefits of bootstrapped DQN, we choose K=10. Due to the shared convolutional network, we find this overall network retains similar computate speed to DQN.\nThis shared network architecture allows us to train this combined network via backpropagation. One question is whether to normalize gradients flowing from the network heads. Feeding K network heads to the shared convolutional network effectively increases the learning rate for this portion of the network. In some games, this leads to premature and sub-optimal convergence. We found the best final scores by normalizing the gradients by 1/K, but this also leads to slower early learning. Since much of the literature focusses best policy after 200m frames, rather than cumulative rewards during learning, we choose 1/10 rescaling for our shared gradients3.\nFinally we have the question of how to implement an online bootstrap for the K network heads. In order to avoid a significant increase in memory footprint we use an independent Bernoulli mask w1, .., wK \u223c Ber(p) for each head\n3For more details on gradient rescaling see Appendix C.\nin each episode4. These flags are stored in the memory replay buffer and identify which heads are trained on which data. A smaller p means that fewer data is shared, which will maintain a higher diversity of bootstrap samples.\nHowever, when trained using a shared minibatch the algorithm will also require an effective 1/p more iterations. This is undesirable since DQN is already computationally taxing. Surprisingly, we found the algorithm performed similarly irrespective of p and all outperformed DQN, as shown in Figure 9. This is strange, but we present a more detailed investigation of this phenomenon in Appendix C. In light of this empirical observation for Atari, we chose p = 1 to save on minibatch passes. As a result bootstrapped DQN runs at similar computational speed to vanilla DQN on identical hardware5."}, {"heading": "6.2. Efficient exploration in Atari", "text": "We find that Bootstrapped DQN drives efficient exploration in several Atari games. This means that, for the same amount of game experience, bootstrapped DQN generally outperforms DQN with -greedy exploration. Figure 10 demonstrates this effect for a diverse selection of games.\nTo summarize this improvement in learning time we consider the number of frames required to reach human performance. We say algorithm A has a human speedup of x relative to algorithm B if it reaches human performance in\n4p = 0.5 is double-or-nothing bootstrap (Owen et al., 2012). 5Our implementation K=10, p=1 ran with less than a 20% increase on wall-time versus DQN for the same amount of frames.\n1/x as many frames as DQN. We find that, for the majority of games where DQN reaches human performance, Bootstrapped DQN reaches human performance significantly faster. We present these results in Figure 11."}, {"heading": "6.3. Understanding bootstrapped DQN", "text": "We will now present some insight into how bootstrapped DQN uses deep exploration to improve upon DQN. First, we will examine the exploration policies for the game Hero. In this game the agent controls a rescue mission which travels room to room, blows up obstacles and rescues hostages. In Figure 12 we show a screenshot for nine different heads of bootstrapped DQN run from the initial state for 3,200 steps. Although each individual head has learned a good representation for the value function that leads to a highscoring policy, the policies they find are quite distinct.\nIn fact, this screenshot shows the nine different heads in five different game rooms. This shows that even after more than 100m training frames with the data sharing p = 1, we still maintain significant diversity. By contrast, -greedy strategies are almost indistinguishable for small values of and totally ineffectual for larger values. Our heads explore a diverse range of policies, but still manage to each perform well individually. We believe that this deep exploration is key to the improved learning visible in Figure 10b, since diverse experiences allow for better generalization. We present videos to highlight this behavior at https://youtu.be/Zm2KoT82O_M.\nDisregarding exploration, bootstrapped DQN may be beneficial as a purely exploitative policy. We can combine all the heads into a single ensemble policy, for example by choosing the action with the most votes across heads. This approach might have several benefits. First, we find that the ensemble policy can often outperform any individual policy. Second, the distribution of votes across heads to give a measure of the uncertainty in the optimal policy.\nIn Figure 13 we present two screenshots of bootstrapped DQN playing Breakout according to an ensemble policy. In this game the agent can choose to move left, move right or stay. Beneath each screen we depict the number of heads that pick each action by the length of each arrow. We draw the action chosen by the ensemble policy in green.\nWhen the ball is approaching the bottom of the screen and all ten heads recognize the optimal policy is to move right to catch it (Figure 13a). However when the ball is far from the paddle the we can see the ensemble policy is uncertain on the best action (Figure 13b). We present videos of this effect at https://youtu.be/0jvEcC5JvGY. Unlike vanilla DQN, bootstrapped DQN can know what it doesn\u2019t know. In an application where executing a poorlyunderstood action is dangerous this could be crucial."}, {"heading": "6.4. Overall performance", "text": "We have seen that bootstrapped DQN is able to learn much faster than DQN. In Figure 14 we see that best performance reached by both algorithms is similar across most games. However, the benefits of efficient exploration mean that bootstrapped DQN greatly outperforms DQN when measured in terms of cumulative rewards through learning. We present these results in Figure 15. In Appendix\nC we present full results for our algorithm across all 49 games. Bootstrapped DQN significantly outperforms several recent heuristic approaches for improved exploration on Atari (Stadie et al., 2015), particularly in terms of cumulative rewards."}, {"heading": "7. Closing remarks", "text": "In this paper we present bootstrapped DQN as an algorithm for efficient reinforcement learning in complex environments. We demonstrate that the bootstrap can produce useful uncertainty estimates for deep neural networks. Bootstrapped DQN can leverage these uncertainty estimates for deep exploration even in difficult stochastic systems; it also produces several state of the art results in Atari 2600.\nBootstrapped DQN is computationally tractable and also naturally scalable to massive parallel systems as per (Nair et al., 2015). We believe that, beyond our specific implementation, randomized value functions represent a promising alternative to dithering for exploration. Bootstrapped DQN practically combines efficient generalization with exploration for complex nonlinear value functions."}, {"heading": "A. Uncertainty for neural networks", "text": "In this appendix we discuss some of the experimental setup to qualitatively evaluate uncertainty methods for deep neural networks. To do this, we generated twenty noisy regression pairs xi, yi with:\nyi = xi + sin(\u03b1(xi + wi)) + sin(\u03b2(xi + wi)) + wi\nwhere xi are drawn uniformly from (0, 0.6) \u222a (0.8, 1) and wi \u223c N(\u00b5 = 0, \u03c32 = 0.032). We set \u03b1 = 4 and \u03b2 = 13. None of these numerical choices were important except to represent a highly nonlinear function with lots of noise and several clear regions where we should be uncertain. We present the regression data together with an indication of the generating distribution in Figure 16.\nInterestingly, we did not find that using dropout produced satisfying confidence intervals for this task. We present one example of this dropout posterior estimate in Figure 17.\nThese results are unsatisfactory for several reasons. First, the network extrapolates the mean posterior far outside the range of any actual data for x = 0.75. We believe this is because dropout only perturbs locally from a single neural network fit, unlike bootstrap. Second, the posterior samples from the dropout approximation are very spiky and do not\nlook like any sensible posterior sample. Third, the network collapses to almost zero uncertainty in regions with data.\nWe spent some time altering our dropout scheme to fix this effect, which might be undesirable for stochastic domains and we believed might be an artefact of our implementation. However, after further thought we believe this to be an effect which you would expect for dropout posterior approximations. In Figure 18 we present a didactic example taken from the author\u2019s website (Gal & Ghahramani, 2015).\nOn the right hand side of the plot we generate noisy data with wildly different values. Training a neural network using MSE criterion means that the network will surely converge to the mean of the noisy data. Any dropout samples remain highly concentrated around this mean. By contrast, bootstrapped neural networks may include different subsets of this noisy data and so may produce a more intuitive uncertainty estimates for our settings.\nIn this paper we focus on the bootstrap approach to uncertainty for neural networks. We like its simplicity, connections to established statistical methodology and empirical good performance. However, the key insights of this paper is the use of deep exploration via randomized value functions. This is compatible with any approximate posterior estimator for deep neural networks. We believe that this area of uncertainty estimates for neural networks remains an important area of research in its own right.\nBootstrapped uncertainty estimates for the Q-value functions have another crucial advantage over dropout which does not appear in the supervised problem. Unlike random dropout masks trained against random target networks, our implementation of bootstrap DQN trains against its own temporally consistent target network. This means that our bootstrap estimates (in the sense of (Efron, 1982)), are able to \u201cbootstrap\u201d (in the TD sense of (Sutton & Barto, 1998)) on their own estimates of the long run value. This is important to quantify the long run uncertainty over Q and drive deep exploration."}, {"heading": "B. Experiments for deep exploration", "text": "B.1. Bootstrap methodology\nA naive implementation of bootstrapped DQN builds up K complete networks with K distinct memory buffers. This method is parallelizable up to many machines, however we wanted to produce an algorithm that was efficient even on a single machine. To do this, we implemented the bootstrap heads in a single larger network, like Figure 1 but without any shared network. We implement bootstrap by masking each episode of data according to w1, .., wK \u223c Ber(p).\nIn Figure 19 we demonstrate that bootstrapped DQN can implement deep exploration even with relatively small values of K. However, the results are more robust and scalable with largerK. We run our experiments on the example from Figure 4. Surprisingly, this method is even effective with p = 1 and complete data sharing between heads. This degenerate full sharing of information turns out to be remarkably efficient for training large and deep neural networks. We discuss this phenomenon more in Appendix C.\nGenerating good estimates for uncertainty is not enough for efficient exploration. In Figure 20 we see that other\nmethods trained with the same network architecture are totally ineffective at implementing deep exploration. The - greedy policy follows just one Q-value estimate. We allow this policy to be evaluated without dithering. The ensemble policy is trained exactly as per bootstrapped DQN except at each stage the algorithm follows the policy which is majority vote of the bootstrap heads. Thompson sampling is the same as bootstrapped DQN except a new head is sampled every timestep, rather than every episode.\nWe can see that only bootstrapped DQN demonstrates efficient and deep exploration in this domain.\nB.2. Comparison to efficient tabular methods\nFor the example in Figure 6 we attempted to display our performance compared to several benchmark tabula rasa approaches to exploration. There are many other algorithms we could have considered, but for a short paper we chose to focus against the most common approach ( - greedy) the pre-eminent optimistic approach (UCRL2) and posterior sampling (PSRL).\nOther common heuristic approaches, such as optimistic initialization for Q-learning can be tuned to work well on this domain, however the precise parameters are sensitive to the underlying MDP6. To make a general-purpose version of this heuristic essentially leads to optimistic algorithms. Famous optimistic algorithms like Rmax (Braf-\n6Further, it is difficult to extend the idea of optimistic initialization with function generalization, especially for deep neural networks.\nman & Tennenholtz, 2002), E3 (Kearns & Singh, 2002) or MBIE (Strehl & Littman, 2005) can be thought of as earlier approaches to optimistic exploration that are generally superseded by UCRL2. Since UCRL2 is originally designed for infinite-horizon MDPs, we use the natural adaptation of this algorithm, which has state of the art guarantees in finite horizon MDPs as well (Dann & Brunskill, 2015).\nFigure 7 displays the empirical regret of these algorithms together with bootstrapped DQN on the example from Figure 6. It is somewhat disconcerting that UCRL2 appears to incur linear regret, but it is proven to satisfy near-optimal regret bounds. Actually, as we show in Figure 21, the algorithm produces regret which scales very similarly to its established bounds (Jaksch et al., 2010). Similarly, even for this tiny problem size, the recent analysis that proves a near optimal sample complexity in fixed horizon problems (Dann & Brunskill, 2015) only guarantees that we will have fewer than 1010 = 1 suboptimal episodes. While these bounds may be acceptable in worst case O\u0303(\u00b7) scaling, they are not of much practical use.\nB.3. How/why does this give deep exploration?\nIn some ways bootstrapped DQN, with K fixed heads that generate alternative policies, resembles evolutionary or randomized policy algorithms. However, there are several key differences. Bootstrapped DQN exhibits deep exploration which allows it to learn exponentially faster. Purely policy-based methods typically require learning time that scales with the number of policies. For an MDP with S states, A actions and H timesteps this is O(ASH), or 2N 2 in the example from Figure 4 and potentially even worse than dithering strategies O(2N ). If we modified Figure 4 to remove the small reward then, prior to observing the reward there would be zero signal for any policy gradient algorithm. This can be formalized to a proof that policy gradients may take O(2N 2\n) episodes to learn (Osband et al., 2014). In practice, the small reward makes these algorithms even worse, since it draws the policy away from the optimal. Bootstrapped DQN is a very different algorithm, we now present some more intuition for why.\nImagine that after L episodes the agent has explored all actions in states 1 \u2264 s \u2264 n\u2212 1 < N and has only once taken\nan action from s = n < N . Since the agent has never been to s = n+ 1 then we know that it has never taken action right in s = n. For n,L large we can imagine that all K bootstrap heads will estimate Q\u2217(s, a) based upon the observed optimal policy to head left a = 1. However, if the networks generalize in a diverse way to unknown states (due to random initialization, different target networks and different datasets) then they should produce different estimates for Qk(n, 2).\nAs long as one head k imagines Qk(n, 2) > Qk(n, 1) then, through the TD bootstrapping in head k this incentive for deep exploration will propagate throughout the entire chain. If this estimate of Qk(n, 2) is high enough, then when head k is chosen it will produce a policy that leads the agent to state n+ 1. The expected time for these estimates at n to propagate to at least one head grows gracefully in n, even for relatively small K. This is very different from policy-based or evolutionary algorithms.\nIt is important to note that for our implementations we rely on the random initialization of deep neural networks as some kind of prior to induce diversity. Unlike supervised learning where all networks fit the same data, the target networks mean that small differences at initialization are prone to become bigger as they refit to unique TD errors. This is effective for our experimental setting, but this will not work in all situations. In stochastic environments or with rescaled rewards it may be necessary to augment the bootstrap heads with artificial prior data to maintain diversity in generalization (Osband & Van Roy, 2015).\nB.4. One-hot features\nIn Figure 22 we include the mean performance of bootstrapped DQN with one-hot feature encodings. We found that, using these features, bootstrapped DQN learned the optimal policy for most seeds, but was somewhat less robust than the thermometer encoding. Two out of ten seeds failed to learn the optimal policy within 2000 episodes, this is presented in Figure 22."}, {"heading": "C. Experiments for Atari", "text": "C.1. Experimental setup\nWe use the same 49 Atari games as (Mnih, 2015) for our experiments. Each step of the agent corresponds to four steps of the emulator, where the same action is repeated, the reward values of the agents are clipped between -1 and 1 for stability. We evaluate our agents and report performance based upon the raw scores.\nThe convolutional part of the network used is identical to the one used in (Mnih, 2015). The input to the network is 4x84x84 tensor with a rescaled, grayscale version of the last four observations. The first convolutional (conv) layer has 32 filters of size 8 with a stride of 4. The second conv layer has 64 filters of size 4 with stride 2. The last conv layer has 64 filters of size 3. We split the network beyond the final layer into K = 10 distinct heads, each one is fully connected and identical to the single head of DQN (Mnih, 2015). This consists of a fully connected layer to 512 units followed by another fully connected layer to the Q-Values for each action. The fully connected layers all use Rectified Linear Units(ReLU) as a non-linearity. We normalize gradients 1/K that flow from each head.\nWe trained the networks with RMSProp with a momentum of 0.95 and a learning rate of 0.00025 as in (Mnih, 2015). The discount was set to \u03b3 = 0.99, the number of steps between target updates was set to \u03c4 = 10000 steps. We trained the agents for a total of 50m steps per game, which corresponds to 200m frames. The agents were every 1m frames, for evaluation in bootstrapped DQN we use an ensemble voting policy. The experience replay contains the 1m most recent transitions. We update the network every 4 steps by randomly sampling a minibatch of 32 transitions from the replay buffer to use the exact same minibatch schedule as DQN. For training we used an -greedy policy with being annealed linearly from 1 to 0.01 over the first 1m timesteps.\nC.2. Gradient normalization in bootstrap heads\nMost literature in deep RL for Atari focuses on learning the best single evaluation policy, with particular attention to whether this above or below human performance (Mnih, 2015). This is unusual for the RL literature, which typically focuses upon cumulative or final performance.\nBootstrapped DQN makes significant improvements to the cumulative rewards of DQN on Atari, as we display in Figure 15, while the peak performance is much more We found that using bootstrapped DQN without gradient normalization on each head typically learned even faster than our implementation with rescaling 1/K, but it was somewhat prone to premature and suboptimal convergence. We present an example of this phenomenon in Figure 23.\nWe found that, in order to better the benchmark \u201cbest\u201d policies reported by DQN, it was very helpful for us to use the gradient normalization. However, it is not entirely clear whether this represents an improvement for all settings. In Figures 24 and 25 we present the cumulative rewards of the same algorithms on Beam Rider.\nWhere an RL system is deployed to learn with real interactions, cumulative rewards present a better measure for performance. In these settings the benefits of gradient normalization are less clear. However, even with normalization 1/K bootstrapped DQN significantly outperforms DQN in terms of cumulative rewards. This is reflected most clearly in Figure 15 and Table 2.\nC.3. Sharing data in bootstrap heads\nAs we report in Section 6 our results on Atari are reported for a degenerate masking of data p = 17. In this setting all network heads share all the data, so they are not actually a traditional bootstrap at all. This is different from the regression task in Section 2, where bootstrapped data was essential to obtain meaningful uncertainty estimates. We have several theories for why the networks maintain significant diversity even without data bootstrapping in this setting. We build upon the intuition of Appendix B.3.\nFirst, they all train on different target networks. This means that even when facing the same (s, a, r, s\u2032) datapoint this can still lead to drastically different Q-value updates. Second, Atari is a deterministic environment, any transition observation is the unique correct datapoint for this setting. Third, the networks are deep and initialized from different random values so they will likely find quite diverse generalization even when they agree on given data. Finally, since all variants of DQN take many many frames to update their policy, it is likely that even using p = 0.5 they would still populate their replay memory with identical datapoints. This means using p = 1 to save on minibatch passes seems like a reasonable compromise and it doesn\u2019t seem to negatively affect performance too much in this setting. More research is needed to examine exactly where/when this data sharing is important.\nC.4. A note on Montezuma\u2019s revenge\nThe game Montezuma\u2019s revenge has posed a major challenge for most RL algorithms. This environment is conceptually similar to Figure 4 for a large N and without the small reward. This algorithm requires deep exploration since rewards are extremely sparse. Based on the arguments in this paper we would hope that bootstrap DQN provides significant benefits here. Figure 26 shows the learning curves on this game.\nImportantly, bootstrapped DQN successfully finds a reward after fewer than 25M frames. DQN with -greedy exploration fails to find any reward after 200M frames. However, bootstrapped DQN does not successfully learn from this\n7We do this for computational reasons, however in a distributed system we can implement bootstrapped DQN in a naturally parallel manner (Nair et al., 2015).\nsignal to reach sustainably good performance. This is not entirely surprising, since training on random minibatches from 1M previous transitions the actual amount of training signal from this reward will be close to zero. However, if we paired this exploration with an aggressive prioritized replay (Schaul et al., 2015) this could provide an avenue towards efficient learning in Montezuma\u2019s revenge.\nC.5. Results tables\nIn Table 1 the average score achieved by the agents during the most successful evaluation period, compared to human performance and a uniformly random policy. DQN is our implementation of DQN with the hyperparameters specified above, using the double Q-Learning update.(Van Hasselt et al., 2015). We find that peak final performance is similar under bootstrapped DQN to previous benchmarks.\nTo compare the benefits of exploration via bootstrapped DQN we benchmark our performance against the most similar prior work on incentivizing exploration in Atari (Stadie et al., 2015). To do this, we compute the AUC-100 measure specified in this work. We present these results in Table 2 compare to their best performing strategy as well as their implementation of DQN. Importantly, bootstrapped DQN outperforms this prior work significantly.\nWe now compare our method against the results in (Stadie et al., 2015). In this paper they introduce a new measure of performance called AUC-100, which is something similar to normalized cumulative rewards up to 20 million frames. Table 2 displays the results for our reference DQN and bootstrapped DQN as Boot-DQN. We reproduce their reference results for DQN as DQN* and their best performing algorithm, Dynamic AE. We also present bootstrapped DQN without head rescaling as Boot-DQN+.\nWe see that, on average, both bootstrapped DQN implementations outperform Dynamic AE, the best algorithm from previous work. The only game in which Dynamic AE produces best results is Bowling, but this difference in Bowling is dominated by the implementation of DQN* vs DQN. Bootstrapped DQN still gives over 100% improvement over its relevant DQN baseline. Overall it is clear that Boot-DQN+ (bootstrapped DQN without rescaling) performs best in terms of AUC-100 metric. Averaged across the 14 games it is over 50% better than the next best competitor, which is bootstrapped DQN with gradient normalization.\nHowever, in terms of peak performance over 200m frames Boot-DQN generally reached higher scores. Boot-DQN+ sometimes plateaud early as in Figure 23. This highlights an important distinction between evaluation based on best learned policy versus cumulative rewards, as we discuss in Appendix C.2. Bootstrapped DQN displays the biggest improvements over DQN when doing well during learning is important."}], "references": [{"title": "Bayesian optimal control of smoothly parameterized systems", "author": ["Abbasi-Yadkori", "Yasin", "Szepesv\u00e1ri", "Csaba"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2015}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "arXiv preprint arXiv:1207.4708,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Some asymptotic theory for the bootstrap", "author": ["Bickel", "Peter J", "Freedman", "David A"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 1981}, {"title": "Weight uncertainty in neural networks", "author": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Blundell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blundell et al\\.", "year": 2015}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2002}, {"title": "Optimal adaptive policies for markov decision processes", "author": ["Burnetas", "Apostolos N", "Katehakis", "Michael N"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Burnetas et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Burnetas et al\\.", "year": 1997}, {"title": "Sample complexity of episodic fixed-horizon reinforcement learning", "author": ["Dann", "Christoph", "Brunskill", "Emma"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dann et al\\.", "year": 2015}, {"title": "The jackknife, the bootstrap and other resampling plans, volume 38", "author": ["Efron", "Bradley"], "venue": null, "citeRegEx": "Efron and Bradley.,? \\Q1982\\E", "shortCiteRegEx": "Efron and Bradley.", "year": 1982}, {"title": "An introduction to the bootstrap", "author": ["Efron", "Bradley", "Tibshirani", "Robert J"], "venue": "CRC press,", "citeRegEx": "Efron et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Efron et al\\.", "year": 1994}, {"title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "arXiv preprint arXiv:1506.02142,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Thompson sampling for learning parameterized markov decision processes", "author": ["Gopalan", "Aditya", "Mannor", "Shie"], "venue": "In Proceedings of the 28th Conference on Learning Theory (COLT),", "citeRegEx": "Gopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2015}, {"title": "Practical variational inference for neural networks", "author": ["Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves and Alex.,? \\Q2011\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2011}, {"title": "Efficient bayesadaptive reinforcement learning using sample-based search", "author": ["Guez", "Arthur", "Silver", "David", "Dayan", "Peter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guez et al\\.", "year": 2012}, {"title": "Bayes-adaptive simulation-based search with value function approximation", "author": ["Guez", "Arthur", "Heess", "Nicolas", "Silver", "David", "Dayan", "Peter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guez et al\\.", "year": 2014}, {"title": "Probabilistic backpropagation for scalable learning of bayesian neural networks", "author": ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", "Adams", "Ryan P"], "venue": null, "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Kakade", "Sham"], "venue": "PhD thesis,", "citeRegEx": "Kakade and Sham.,? \\Q2003\\E", "shortCiteRegEx": "Kakade and Sham.", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Kearns", "Michael J", "Singh", "Satinder P"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Kingma", "Diederik P", "Salimans", "Tim", "Welling", "Max"], "venue": "arXiv preprint arXiv:1506.02557,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "A scalable bootstrap for massive data", "author": ["Kleiner", "Ariel", "Talwalkar", "Ameet", "Sarkar", "Purnamrita", "Jordan", "Michael I"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Kleiner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kleiner et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Lai", "Tze Leung", "Robbins", "Herbert"], "venue": "Advances in applied mathematics,", "citeRegEx": "Lai et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1985}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih and Volodymyr,? \\Q2015\\E", "shortCiteRegEx": "Mnih and Volodymyr", "year": 2015}, {"title": "From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning", "author": ["Munos", "R\u00e9mi"], "venue": null, "citeRegEx": "Munos and R\u00e9mi.,? \\Q2014\\E", "shortCiteRegEx": "Munos and R\u00e9mi.", "year": 2014}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Nair", "Arun", "Srinivasan", "Praveen", "Blackwell", "Sam", "Alcicek", "Cagdas", "Fearon", "Rory", "De Maria", "Alessandro", "Panneershelvam", "Vedavyas", "Suleyman", "Mustafa", "Beattie", "Charles", "Petersen", "Stig"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Model-based reinforcement learning and the eluder dimension", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Near-optimal reinforcement learning in factored MDPs", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Bootstrapped thompson sampling and deep exploration", "author": ["Osband", "Ian", "Van Roy", "Benjamin"], "venue": "arXiv preprint arXiv:1507.00300,", "citeRegEx": "Osband et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2015}, {"title": "More) efficient reinforcement learning via posterior sampling", "author": ["Osband", "Ian", "Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "In NIPS,", "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "Generalization and exploration via randomized value functions", "author": ["Osband", "Ian", "Van Roy", "Benjamin", "Wen", "Zheng"], "venue": "arXiv preprint arXiv:1402.0635,", "citeRegEx": "Osband et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2014}, {"title": "Bootstrapping data arrays of arbitrary order", "author": ["Owen", "Art B", "Eckles", "Dean"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Owen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Owen et al\\.", "year": 2012}, {"title": "The bayesian bootstrap", "author": ["Rubin", "Donald B"], "venue": "The annals of statistics,", "citeRegEx": "Rubin and B,? \\Q1981\\E", "shortCiteRegEx": "Rubin and B", "year": 1981}, {"title": "Learning to optimize via posterior sampling", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2014}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Stadie", "Bradly C", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Strehl", "Alexander L", "Littman", "Michael L"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Strehl et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2005}, {"title": "A bayesian framework for reinforcement learning", "author": ["Strens", "Malcolm J. A"], "venue": "In ICML, pp", "citeRegEx": "Strens and A.,? \\Q2000\\E", "shortCiteRegEx": "Strens and A.", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard", "Barto", "Andrew"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Distributed bayesian learning with stochastic natural-gradient expectation propagation and the posterior server", "author": ["Teh", "Yee Whye", "Hasenclever", "Leonard", "Lienart", "Thibaut", "Vollmer", "Sebastian", "Webb", "Stefan", "Lakshminarayanan", "Balaji", "Blundell", "Charles"], "venue": "arXiv preprint arXiv:1512.09327,", "citeRegEx": "Teh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2015}, {"title": "Temporal difference learning and td-gammon", "author": ["Tesauro", "Gerald"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "arXiv preprint arXiv:1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["Wang", "Tao", "Lizotte", "Daniel", "Bowling", "Michael", "Schuurmans", "Dale"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "de Freitas", "Nando", "Lanctot", "Marc"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Efficient exploration and value function generalization in deterministic systems", "author": ["Wen", "Zheng", "Van Roy", "Benjamin"], "venue": "In NIPS, pp", "citeRegEx": "Wen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2013}, {"title": "Dropout converges with high certainty to the mean value. On the right hand side of the plot we generate noisy data with wildly different values. Training a neural network using MSE criterion means that the network", "author": ["Ghahramani"], "venue": null, "citeRegEx": "Ghahramani and 2015..,? \\Q2015\\E", "shortCiteRegEx": "Ghahramani and 2015..", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": ", Jaksch et al. (2010); Guez et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 12, "context": "(2010); Guez et al. (2012)).", "startOffset": 8, "endOffset": 27}, {"referenceID": 26, "context": "Previous work demonstrated that randomized value functions can efficiently explore in tandem with linearly-parameterized value function generalization (Osband et al., 2014).", "startOffset": 151, "endOffset": 172}, {"referenceID": 20, "context": "Deep neural networks represent the state of the art in many supervised learning domains (Krizhevsky et al., 2012).", "startOffset": 88, "endOffset": 113}, {"referenceID": 3, "context": "These include variational Bayes (Graves, 2011; Blundell et al., 2015), assumed density filtering (Hern\u00e1ndez-Lobato & Adams, 2015), dropout-based variational inference (Gal & Ghahramani, 2015; Kingma et al.", "startOffset": 32, "endOffset": 69}, {"referenceID": 18, "context": ", 2015), assumed density filtering (Hern\u00e1ndez-Lobato & Adams, 2015), dropout-based variational inference (Gal & Ghahramani, 2015; Kingma et al., 2015), and stochastic gradient Langevin dynamics (Teh et al.", "startOffset": 105, "endOffset": 150}, {"referenceID": 40, "context": ", 2015), and stochastic gradient Langevin dynamics (Teh et al., 2015).", "startOffset": 51, "endOffset": 69}, {"referenceID": 19, "context": "The approach can even scale to massive data with sub-linear computational cost (Kleiner et al., 2014).", "startOffset": 79, "endOffset": 101}, {"referenceID": 31, "context": "Each head is trained only on its bootstrapped sub-sample of the data, which can be generated online (Owen et al., 2012).", "startOffset": 100, "endOffset": 119}, {"referenceID": 29, "context": "This is a natural extension of the Thompson sampling heuristic to RL that allows for temporally extended (or deep) exploration (Strens, 2000; Osband et al., 2013).", "startOffset": 127, "endOffset": 162}, {"referenceID": 22, "context": ", Levine et al. (2015)), the form of exploration induced by such algorithms can also be highly inefficient (Kakade, 2003).", "startOffset": 2, "endOffset": 23}, {"referenceID": 44, "context": "Other approaches aim to approximate Bayes-optimal exploration though tree-based search (Wang et al., 2005; Guez et al., 2014).", "startOffset": 87, "endOffset": 125}, {"referenceID": 13, "context": "Other approaches aim to approximate Bayes-optimal exploration though tree-based search (Wang et al., 2005; Guez et al., 2014).", "startOffset": 87, "endOffset": 125}, {"referenceID": 15, "context": "proposed for finite-armed bandits (Lai & Robbins, 1985), but the principle has been extended successfully across bandits with generalization (Rusmevichientong & Tsitsiklis, 2010) and tabular RL (Jaksch et al., 2010).", "startOffset": 194, "endOffset": 215}, {"referenceID": 36, "context": "The work of (Stadie et al., 2015) aims to add an effective bonus through a variation of DQN.", "startOffset": 12, "endOffset": 33}, {"referenceID": 42, "context": "Perhaps the oldest heuristic for balancing exploration with exploitation is given by Thompson sampling (Thompson, 1933).", "startOffset": 103, "endOffset": 119}, {"referenceID": 12, "context": "The agent must also commit to this sample for several time steps in order to achieve deep exploration (Strens, 2000; Guez et al., 2012).", "startOffset": 102, "endOffset": 135}, {"referenceID": 29, "context": "The algorithm PSRL does exactly this, with state of the art guarantees (Osband et al., 2013; Osband & Van Roy, 2014b;a; Abbasi-Yadkori & Szepesv\u00e1ri, 2015; Gopalan & Mannor, 2015).", "startOffset": 71, "endOffset": 178}, {"referenceID": 26, "context": "Surprisingly, this algorithm recovers state of the art guarantees in the setting with tabular basis functions, but its performance is crucially dependent upon a suitable linear representation of the value function (Osband et al., 2014).", "startOffset": 214, "endOffset": 235}, {"referenceID": 26, "context": "However, any shallow exploration strategy will take \u03a9(2 ) episodes to learn the optimal policy (Osband et al., 2014).", "startOffset": 95, "endOffset": 116}, {"referenceID": 15, "context": "These algorithms are based on three state of the art approaches to exploration via dithering ( -greedy), optimism (UCRL2; Jaksch et al., 2010) and posterior sampling (PSRL; Osband et al.", "startOffset": 114, "endOffset": 142}, {"referenceID": 29, "context": ", 2010) and posterior sampling (PSRL; Osband et al., 2013).", "startOffset": 31, "endOffset": 58}, {"referenceID": 15, "context": "Although Figure 7 seems to suggest UCRL2 incurs linear regret, actually it follows its bounds \u00d5(S \u221a AT ) (Jaksch et al., 2010) where S is the number of states and A is the number of actions.", "startOffset": 105, "endOffset": 126}, {"referenceID": 1, "context": "We now evaluate our algorithm across 49 Atari games on the Arcade Learning Environment (Bellemare et al., 2012).", "startOffset": 87, "endOffset": 111}, {"referenceID": 45, "context": "to DDQN (Wang et al., 2015; Schaul et al., 2015).", "startOffset": 8, "endOffset": 48}, {"referenceID": 34, "context": "to DDQN (Wang et al., 2015; Schaul et al., 2015).", "startOffset": 8, "endOffset": 48}, {"referenceID": 31, "context": "5 is double-or-nothing bootstrap (Owen et al., 2012).", "startOffset": 33, "endOffset": 52}, {"referenceID": 36, "context": "Bootstrapped DQN significantly outperforms several recent heuristic approaches for improved exploration on Atari (Stadie et al., 2015), particularly in terms of cumulative rewards.", "startOffset": 113, "endOffset": 134}, {"referenceID": 25, "context": "Bootstrapped DQN is computationally tractable and also naturally scalable to massive parallel systems as per (Nair et al., 2015).", "startOffset": 109, "endOffset": 128}], "year": 2016, "abstractText": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as -greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.", "creator": "LaTeX with hyperref package"}}}