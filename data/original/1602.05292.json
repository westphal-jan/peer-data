{"id": "1602.05292", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2016", "title": "Authorship Attribution Using a Neural Network Language Model", "abstract": "In practice, training language models for individual authors is often expensive because of limited data resources. In such cases, Neural Network Language Models (NNLMs), generally outperform the traditional non-parametric N-gram models. Here we investigate the performance of a feed-forward NNLM on an authorship attribution problem, with moderate author set size and relatively limited data. We also consider how the text topics impact performance. Compared with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the proposed method achieves nearly 2:5% reduction in perplexity and increases author classification accuracy by 3:43% on average, given as few as 5 test sentences. The performance is very competitive with the state of the art in terms of accuracy and demand on test data. The source code, preprocessed datasets, a detailed description of the methodology and results are available at", "histories": [["v1", "Wed, 17 Feb 2016 04:06:28 GMT  (103kb,D)", "http://arxiv.org/abs/1602.05292v1", "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI'16)"]], "COMMENTS": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI'16)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["zhenhao ge", "yufang sun", "mark j t smith"], "accepted": true, "id": "1602.05292"}, "pdf": {"name": "1602.05292.pdf", "metadata": {"source": "CRF", "title": "Authorship Attribution Using a Neural Network Language Model", "authors": ["Zhenhao Ge", "Yufang Sun", "Mark J.T. Smith"], "emails": ["mjts}@purdue.edu,"], "sections": [{"heading": "Introduction", "text": "Authorship attribution refers to identifying authors from given texts by their unique textual features. It is challenging since the author\u2019s style may vary from time to time by topics, mood and environment. Many methods have been explored to address this problem, such as Latent Dirichlet Allocation for topic modeling (Seroussi, Zukerman, and Bohnert 2011) and Naive Bayes for text classification (Coyotl-Morales et al. 2006). Regarding language modeling methods, there is mixed advocacy for the conventional N-gram methods (Kes\u030celj et al. 2003) and methods using more compact and distributed representations, like Neural Network Language Models (NNLMs), which was claimed to capture semantics better with limited training data (Bengio et al. 2003).\nMost NNLM toolkits available (Mikolov et al. 2010) are designed for recurrent NNLMs which are better for capturing complex and longer text patterns and require more training data. In contrast, the feed-forward NNLM framework we proposed is less computationally expensive and more suitable for language modeling with limited data. It is developed in MATLAB with full network tuning functionalities.\nThe database we use is composed of transcripts of 16 video courses taken from Coursera, collected one sentence per line into a text file for each course. To reduce the influence of \u201ctopic\u201d on author/instructor classification, courses were all selected from science and engineering fields, such\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nas Algorithm, DSP, Data Mining, IT, Machine Learning, NLP, etc. There are 8000+ sentences/course and about 20 words/sentence on average. The vocabulary size of each author varies from 3000 to 9000. After stemming with Porter\u2019s algorithm and pruning words with frequency less than 1/100, 000, author vocabulary size is reduced to a range from 1800 to 2700, with average size around 2000. Fig. 1 shows the vocabulary size for each course, under various conditions and the database coverage with the most frequent k words (k = 500, 1000, 2000) after stemming and pruning."}, {"heading": "Neural Network Language Model (NNLM)", "text": "Similar to N-gram methods, the NNLM is also used to answer one of the fundamental questions in language modeling: predicting the best target wordW\u2217, given a context of N \u2212 1 words. The target word is typically the last word within context size N . However, it theoretically can be in any position. Fig. 2 demonstrates the structure of the proposed NNLM with multinomial classification cost function:\nC = \u2212 \u2211 V tj log yj , j \u2208 V, (1)\nwhere V is the vocabulary size, yj and tj are the final output and the target label. This NNLM setup contains 4 types of layers. The word layer contains N \u2212 1 input words represented by V -dimensional index vectors with V \u2212 1 \u201c0\u201ds and one \u201c1\u201d positioned in a different location to differentiate it from all other words. Words are then transformed to their distributed representation and concatenated in the embedding layer. Outputs from this layer forward propagate to the hidden sigmoid layer, then softmax layer to predict the probabilities of the possible target words. Weights/biases between layers are initiated randomly and with zeros respectively, and their error derivatives are computed through backward propagation. The network is iteratively updated with parameters, such as learning rate and momentum.\nar X\niv :1\n60 2.\n05 29\n2v 1\n[ cs\n.C L\n] 1\n7 Fe\nb 20\n16\nIn implementation, the processed text data for each course are randomly split into training, validation, and test sets with ratio 8:1:1. This segmentation is performed 10 times with different randomization seeds, so the mean/variance of performance of NNLMs can be measured later. We optimized a 4-gram NNLM (predicting the 4th word using the previous 3) with mini-batch training through 10 to 20 epochs for each course. The model parameters, such as number of nodes in each layer, learning rate, and momentum are customized for obtaining the best individual models."}, {"heading": "Classification with Perplexity Measurement", "text": "Denote Wn1 as a word sequence (W1,W2, . . . ,WN ) and P (Wn1 ) as the probability of Wn1 given a LM, perplexity is an intrinsic measurement of the LM fitness defined by:\nPP (Wn1 ) = P (Wn1 )\u2212 1 n (2)\nUsing Markov chain theory, P (Wn1 ) can be approximated by the probability of the closest N words P (Wnn\u2212N+1), so PP (Wn1 ) can be approximated by\nPP (Wnn\u2212N+1) = ( n\u220f\nk=1\nP (Wk|Wk\u22121k\u2212N+1)) \u22121/n (3)\nThe mean perplexity of applying trained 4-gram NNLMs to their corresponding test sets are 67.3\u00b1 2.4. This is lower (better) than the traditional N-gram method (69.0\u00b12.4 with 4-gram SRILM). The classification is performed by finding the author with his/her NNLM that maximizes the accumulative perplexity of the test sentences. By randomly selecting 1 to 20 test sentences from the test set, Fig. 3 shows the 16-way classification accuracy using 3 methods, for one particular course/instructor and for all courses on average. There are 2 courses taught by the same instructor, intentionally added for investigating the topic impact on accuracy. They are excluded when computing the average accuracy in Fig. 3. Similarly, the accuracies for courses using two methods with differing text lengths are compared in Fig. 4. Both figures show the NNLM method is slightly better than the SRI baselines at the 4-gram level. A classification confusion matrix (not included due to space limits) was also computed to show the similarity between authors. The results show higher confusion on similar courses, which indicates the topic does impact accuracy. The NNLM has higher confusion values than the SRI baseline on the two different courses from the same instructor, so it is more biased toward the author rather than the topic in that sense.\n."}, {"heading": "Conclusion and Future Work", "text": "The NNLM-based work achieves promising results compared with the N-gram baseline. The nearly perfect accuracies given 10+ test sentences are competitive with the stateof-the-art, which achieved 95%+ accuracy on a similar author size (Coyotl-Morales et al. 2006), or 80%+ with tens of authors and limited training data (Seroussi, Zukerman, and Bohnert 2011). However, it may also indicate this dataset is not sufficiently challenging, probably due to the training and test data consistency and the topic distinction. In the future, datasets with more authors can be used, for example, taken from collections of books or transcribed speeches. We also plan to integrate a nonlinear function optimization scheme using the conjugate gradient (Rasmussen 2006), which automatically selects the best training parameters and saves time in model customization. To compensate for the relatively small size of the training set, LMs may also be trained with a group of authors and then adapted to the individuals."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio"], "venue": "The Journal of Machine Learning Research 3:1137\u20131155", "citeRegEx": "Bengio,? \\Q2003\\E", "shortCiteRegEx": "Bengio", "year": 2003}, {"title": "R", "author": ["Coyotl-Morales"], "venue": "M.; Villase\u00f1orPineda, L.; Montes-y G\u00f3mez, M.; and Rosso, P.", "citeRegEx": "Coyotl.Morales et al. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "N-gram-based author profiles for authorship attribution", "author": ["Ke\u0161elj"], "venue": "PACLING", "citeRegEx": "Ke\u0161elj,? \\Q2003\\E", "shortCiteRegEx": "Ke\u0161elj", "year": 2003}, {"title": "S", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "Khudanpur"], "venue": "2010. Recurrent neural network based language model. In INTERSPEECH", "citeRegEx": "Mikolov et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "C", "author": ["Rasmussen"], "venue": "E.", "citeRegEx": "Rasmussen 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Authorship attribution with latent dirichlet allocation", "author": ["Zukerman Seroussi", "Y. Bohnert 2011] Seroussi", "I. Zukerman", "F. Bohnert"], "venue": "In CoNLL", "citeRegEx": "Seroussi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Seroussi et al\\.", "year": 2011}], "referenceMentions": [], "year": 2016, "abstractText": "In practice, training language models for individual authors is often expensive because of limited data resources. In such cases, Neural Network Language Models (NNLMs), generally outperform the traditional non-parametric N-gram models. Here we investigate the performance of a feed-forward NNLM on an authorship attribution problem, with moderate author set size and relatively limited data. We also consider how the text topics impact performance. Compared with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the proposed method achieves nearly 2.5% reduction in perplexity and increases author classification accuracy by 3.43% on average, given as few as 5 test sentences. The performance is very competitive with the state of the art in terms of accuracy and demand on test data. The source code, preprocessed datasets, a detailed description of the methodology and results are available at https: //github.com/zge/authorship-attribution. Introduction Authorship attribution refers to identifying authors from given texts by their unique textual features. It is challenging since the author\u2019s style may vary from time to time by topics, mood and environment. Many methods have been explored to address this problem, such as Latent Dirichlet Allocation for topic modeling (Seroussi, Zukerman, and Bohnert 2011) and Naive Bayes for text classification (Coyotl-Morales et al. 2006). Regarding language modeling methods, there is mixed advocacy for the conventional N-gram methods (Ke\u0161elj et al. 2003) and methods using more compact and distributed representations, like Neural Network Language Models (NNLMs), which was claimed to capture semantics better with limited training data (Bengio et al. 2003). Most NNLM toolkits available (Mikolov et al. 2010) are designed for recurrent NNLMs which are better for capturing complex and longer text patterns and require more training data. In contrast, the feed-forward NNLM framework we proposed is less computationally expensive and more suitable for language modeling with limited data. It is developed in MATLAB with full network tuning functionalities. The database we use is composed of transcripts of 16 video courses taken from Coursera, collected one sentence per line into a text file for each course. To reduce the influence of \u201ctopic\u201d on author/instructor classification, courses were all selected from science and engineering fields, such Copyright c \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. as Algorithm, DSP, Data Mining, IT, Machine Learning, NLP, etc. There are 8000+ sentences/course and about 20 words/sentence on average. The vocabulary size of each author varies from 3000 to 9000. After stemming with Porter\u2019s algorithm and pruning words with frequency less than 1/100, 000, author vocabulary size is reduced to a range from 1800 to 2700, with average size around 2000. Fig. 1 shows the vocabulary size for each course, under various conditions and the database coverage with the most frequent k words (k = 500, 1000, 2000) after stemming and pruning. 0 2 4 6 8 10 12 14 16 0 500", "creator": "LaTeX with hyperref package"}}}