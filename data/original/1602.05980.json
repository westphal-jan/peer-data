{"id": "1602.05980", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Revise Saturated Activation Functions", "abstract": "It has been generally believed that training deep neural networks is hard with saturated activation functions, including Sigmoid and Tanh. Recent works shows that deep Tanh networks are able to converge with careful model initialization while deep Sigmoid networks still fail. In this paper, we propose a re-scaled Sigmoid function which is able to maintain the gradient in a stable scale. In addition, we break the symmetry of Tanh by penalizing the negative part. Our preliminary results on deep convolution networks shown that, even without stabilization technologies such as batch normalization and sophisticated initialization, the \"re-scaled Sigmoid\" converges to local optimality robustly. Furthermore the \"leaky Tanh\" is comparable or even outperforms the state-of-the-art non-saturated activation functions such as ReLU and leaky ReLU.", "histories": [["v1", "Thu, 18 Feb 2016 21:26:53 GMT  (30kb)", "http://arxiv.org/abs/1602.05980v1", null], ["v2", "Mon, 2 May 2016 08:56:19 GMT  (56kb,D)", "http://arxiv.org/abs/1602.05980v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bing xu", "ruitong huang", "mu li"], "accepted": false, "id": "1602.05980"}, "pdf": {"name": "1602.05980.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["antinucleon@gmail.com", "rtonghuang@gmail.com", "muli@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n05 98\n0v 1\n[ cs\n.L G\n] 1\n8 Fe\nb 20\n16 Underreview for Workshop track - ICLR 2016"}, {"heading": "1 INTRODUCTION", "text": "Training a deep network with saturated activation functions has been experimentally proved to be hard. In the literature of neural networks, Sigmoid and Tanh, arguably, are among the most notable saturated activation functions. With careful Layer-sequential unit-variance (LSUV) initialization (Mishkin & Matas, 2015), deep Tanh network is able to converge to a local optimality from random initialization, while deep Sigmoid network fails using the LSUV initialization.\nOne publicly accepted reason of this failure is the gradient vanishing (and/or explosion) that is happening with saturated activation functions. Based on the explanation, Layer-wise pretrain (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) or Batch Normalization (Ioffe & Szegedy, 2015) can be used as an efficient way of tackling this saturation problem. Another possible way is to use a non-saturated activation function, like ReLU. This non-saturation property is also as an explanation of the better performance of ReLU compared to other saturated functions.\nIn this paper, we re-investigate the above claims: 1. Gradient vanishing (and/or explosion) causes the failure when using the saturated activation functions; 2. The non-saturation property is the reason that ReLU outperforms other saturated functions. In particular, we start with verifying the assumptions that are required in Xavier initialization (Glorot & Bengio, 2010), then based on which two methods are proposed to overcome the training problem of the deep Sigmoid networks. To verify the second claim, we test the performance of a newly proposed saturated activation function, called leaky Tanh. Our results provides more insights about the effect of different activation functions on the performance of the neural networks, and suggest that further investigation is still needed for better understanding.\nAll the networks in the paper are trained by using MXNet (Chen et al., 2015)."}, {"heading": "2 UNDERSTANDING DIFFICULTY OF TRAINING DEEP SIGMOID NETWORK", "text": "In this section we analyze the behavior of deep Sigmoid network based on the idea of popular initialization (Glorot & Bengio, 2010; He et al., 2015; Mishkin & Matas, 2015) that the variance of the gradient and the output of each layer is better to be maintained in a stable scale (for at least the first few iterations). Then we propose our method to fix the detected problem in deep Sigmoid network.\nAssume that for the l-th layer in neural network, the input dimension and the output one are the same, nl. Then with activation function f , in a forward pass we have\nx(l) = f(y(l\u22121)) (1)\ny(l) = W (l)x(l) + b(l) (2)\nwhere y(l) is the output, x(l) is the current input, W (l) is weight matrix, and b(l) is bias term. Now assume that all the y(l\u22121) are around 0, thus x(l) can be linearly approximated by diag(f \u2032(y(l\u22121)))y(l\u22121). Therefore, the variance of the output of the l-th layer is\nVar[y(l)] = nlVar[w (l)]Var[x(l)] (3)\n= nlVar[w(l)]diag(f \u2032(y(l\u22121)))Var[y(l\u22121)]diag(f \u2032(y(l\u22121))), (4)\nwhere we assume all elements of W (l) are mean 0, variance V ar[w(l)], and independent to each other, and V ar[x(l)] = \u03c32xInl for some \u03c3. Also, the variance of the gradient is\nVar[ \u2202\u03b5\n\u2202y(l\u22121) ] = Var[\n\u2202\u03b5 \u2202y(l) \u2202y(l) \u2202x(l) \u2202x(l) \u2202y(l\u22121) ] (5)\n= nlVar[w(l)]diag(f \u2032(y(l\u22121)))Var[ \u2202\u03b5\n\u2202y(l) ]diag(f \u2032(y(l\u22121))) (6)\nNow given that Var[y(l)] = \u03c32yI , and Var[ \u2202\u03b5 \u2202y(l\u22121) ] = \u03c32gI for some fixed \u03c3y , \u03c3g and any l, one can recover the initialization method in the paper (Glorot & Bengio, 2010) when f \u2032(y(l\u22121)) = 1nl where 1nl is a dimension nl vector with all its elements being 1. However, the following Taylor expansions of different activation functions suggest that Sigmoid heavily violate the condition that f \u2032(y(l\u22121)) = 1nl :\nrelu(x) = 0 + x (7)\ntanh(x) = 0 + x\u2212 x3\n3 +O(x5) (8)\nsigmoid(x) = 1\n2 +\nx 4 \u2212\nx3 48 +O(x5) (9)\nClearly, when x is around 0 Sigmoid will make gradient vanishing if we use same learning rate in each layer.\nOne way to fix this problem would be use different learning rate for different layers (the lower the larger) and also initialize W on f \u2032(y(l\u22121)) for different layer (the higher the larger). To simplify the implementation, we propose to use the re-scaled Sigmoid function that is roughly equivalent to the above method, as follows.\nsigmoid\u2217(x) = 4 \u00b7 sigmoid(x) \u2212 2 (10)\nNote that by using Equ.10, it is equivalent to scale original learning rate and the initialized W by factor of 4 for each layer."}, {"heading": "3 SATURATED ACTIVATION FUNCTION WITH LEAKY", "text": "In the case of using non-saturated activation functions, leaky in negative part of activation has been reported as a way to improve the network performance (Xu et al., 2015; Clevert et al., 2015). In this section we test the same idea on the Tanh function, as follows:\nleaky tanh(x) =\n{\ntanh(x) if x > 0 a \u00b7 tanh(x) if x < 0 , where a \u2208 [0, 12 ]\n(11)\nWe experiment ReLU, Leaky ReLU (with a = 0.25), Sigmoid, Sigmoid*, Tanh and Leaky Tanh on CIFAR-100 with 33 layer Inception Network (Ioffe & Szegedy, 2015) but removed all Batch Normalization layer. Experimental results are reported in Tab.1 and learning curve are shown in Fig.1, 2, 3, 4. Interestingly, a simple leaky change in Tanh makes 13.6% improvement on test set, and achieve similar performance as Leaky ReLU. This result suggests that saturation is no longer a problem when using the trick of \u2019Leaky\u2019.\nFigure 2: Tanh and ReLU\n20 40 60\n0.2\n0.4\n0.6\n0.8\n1\nepoch\nerror Leaky ReLU train Leaky ReLU test\nReLU Train\nReLU test\nFigure 3: Leaky ReLU and ReLU 20 40 60\n0.2\n0.4\n0.6\n0.8\n1\nepoch\nerror Leaky Tanh train Leaky Tanh test\nReLU Train\nReLU test\nFigure 4: Leaky Tanh and ReLU"}, {"heading": "4 CONCLUSION & FUTURE WORK", "text": "The result of this paper is two-fold. We first attempt to explain and fix the failure of training a deep Sigmoid network, based on the idea of the work (Glorot & Bengio, 2010). A re-scaled Sigmoid activation is proposed in the paper to make deep Sigmoid network trainable. The other result of this paper is to investigate the differences in network performances between using saturated activation function and using non-saturated ones. Our result suggests that when using the leaky trick, saturation of the activation function is comparable to ReLU and Leaky ReLU. There are still many open questions requiring further investigation: 1. How to efficiently determine different learning rates for different layers in a very deep neural network? 2. How does the positive part (on [0,+\u221e)) and the negative part (on (\u2212\u221e, 0]) of the activation function affect the performance of the network?"}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank NVIDIA\u2019s GPU donation."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["Tianqi Chen", "Mu Li", "Yutian Li", "Min Lin", "Naiyan Wang", "Minjie Wang", "Tianjun Xiao", "Bing Xu", "Chiyuan Zhang", "Zheng Zhang"], "venue": "arXiv preprint arXiv:1512.01274,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "All you need is a good init", "author": ["Dmytro Mishkin", "Jiri Matas"], "venue": "arXiv preprint arXiv:1511.06422,", "citeRegEx": "Mishkin and Matas.,? \\Q2015\\E", "shortCiteRegEx": "Mishkin and Matas.", "year": 2015}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li"], "venue": "arXiv preprint arXiv:1505.00853,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Based on the explanation, Layer-wise pretrain (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) or Batch Normalization (Ioffe & Szegedy, 2015) can be used as an efficient way of tackling this saturation problem.", "startOffset": 46, "endOffset": 97}, {"referenceID": 1, "context": "All the networks in the paper are trained by using MXNet (Chen et al., 2015).", "startOffset": 57, "endOffset": 76}, {"referenceID": 4, "context": "In this section we analyze the behavior of deep Sigmoid network based on the idea of popular initialization (Glorot & Bengio, 2010; He et al., 2015; Mishkin & Matas, 2015) that the variance of the gradient and the output of each layer is better to be maintained in a stable scale (for at least the first few iterations).", "startOffset": 108, "endOffset": 171}, {"referenceID": 8, "context": "In the case of using non-saturated activation functions, leaky in negative part of activation has been reported as a way to improve the network performance (Xu et al., 2015; Clevert et al., 2015).", "startOffset": 156, "endOffset": 195}, {"referenceID": 2, "context": "In the case of using non-saturated activation functions, leaky in negative part of activation has been reported as a way to improve the network performance (Xu et al., 2015; Clevert et al., 2015).", "startOffset": 156, "endOffset": 195}], "year": 2017, "abstractText": "It has been generally believed that training deep neural networks is hard with saturated activation functions, including Sigmoid and Tanh. Recent works (Mishkin & Matas, 2015) shown that deep Tanh networks are able to converge with careful model initialization while deep Sigmoid networks still fail. In this paper, we propose a re-scaled Sigmoid function which is able to maintain the gradient in a stable scale. In addition, we break the symmetry of Tanh by penalizing the negative part. Our preliminary results on deep convolution networks shown that, even without stabilization technologies such as batch normalization and sophisticated initialization, the \u201cre-scaled Sigmoid\u201d converges to local optimality robustly. Furthermore the \u201cleaky Tanh\u201d is comparable or even outperforms the state-of-the-art non-saturated activation functions such as ReLU and leaky ReLU.", "creator": "LaTeX with hyperref package"}}}