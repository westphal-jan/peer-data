{"id": "1602.07566", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Time and Activity Sequence Prediction of Business Process Instances", "abstract": "The ability to know in advance the trend of running process instances, with respect to different features, such as the expected completion time, would allow business managers to timely counteract to undesired situations, in order to prevent losses. Therefore, the ability to accurately predict future features of running business process instances would be a very helpful aid when managing processes, especially under service level agreement constraints. However, making such accurate forecasts is not easy: many factors may influence the predicted features.", "histories": [["v1", "Wed, 24 Feb 2016 15:42:06 GMT  (984kb,D)", "http://arxiv.org/abs/1602.07566v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mirko polato", "alessandro sperduti", "rea burattin", "massimiliano de leoni"], "accepted": false, "id": "1602.07566"}, "pdf": {"name": "1602.07566.pdf", "metadata": {"source": "CRF", "title": "Time and Activity Sequence Prediction of Business Process Instances", "authors": ["M. Polatoa", "A. Sperduti", "A. Burattin", "M. de Leoni"], "emails": ["mpolato@math.unipd.it", "sperduti@math.unipd.it", "andrea.burattin@uibk.ac.at"], "sections": [{"heading": null, "text": "The ability to know in advance the trend of running process instances, with respect to different features, such as the expected completion time, would allow business managers to timely counteract to undesired situations, in order to prevent losses. Therefore, the ability to accurately predict future features of running business process instances would be a very helpful aid when managing processes, especially under service level agreement constraints. However, making such accurate forecasts is not easy: many factors may influence the predicted features.\nMany approaches have been proposed to cope with this problem but all of them assume that the underling process is stationary. However, in real cases this assumption is not always true. In this work we present new methods for predicting the remaining time of running cases. In particular we propose a method, assuming process stationarity, which outperforms the state-of-the-art and two other methods which are able to make predictions even with nonstationary processes. We also describe an approach able to predict the full sequence of activities that a running case is going to take. All these methods are extensively evaluated on two real case studies.\nKeywords: process mining, prediction, remaining time, machine learning"}, {"heading": "1. Introduction", "text": "An increasing number of companies are using Process Aware Information Systems (PAIS) to support their business. All these systems record execution traces, called event logs, with information regarding the executed activities. In typical scenarios, these traces do not only contain which activities have been performed, but also additional attributes.\nThe extraction of useful information out of large amount of data is the main challenge of the data mining field. However, recently, a new topic branched\nIPart of the work was conducted while affiliated with University of Padua. \u2217Corresponding author Email addresses: mpolato@math.unipd.it (M. Polato), sperduti@math.unipd.it\n(A. Sperduti), andrea.burattin@uibk.ac.at (A. Burattin), mdeleoni@m.d.leoni@tue.nl (M. de LeoniI)\nPreprint submitted to Elsevier February 25, 2016\nar X\niv :1\n60 2.\n07 56\n6v 1\n[ cs\n.A I]\n2 4\nFe b\n20 16\noff data mining: process mining [1, 2]. This latter case expects that the analyzed data are specifically referring to executions of business processes, and this assumption is made throughout the analysis phases. Frequently, under the umbrella of the term \u201cprocess mining\u201d, three main activities are distinguished: the discovery of models, such as control-flow, describing the actual process undergoing (namely, process discovery); the assessment of the conformance of a given event log with respect to a given process model (namely, conformance checking); and the extension of existing process models with additional information (namely, process enhancement). However, apart from these classical activities, it is also possible to exploit the event log in order to create predictive models, i.e., models that are useful to predict future characteristics of incomplete process instances.\nIn general, it is useful to distinguish two types of process mining techniques, according to when the analysis takes place: (i) a-posteriori; and (ii) at runtime. A-posteriori, or \u201coff-line\u201d, techniques use a finite portion of historical data to extract knowledge out of it. Runtime, or \u201con-line\u201d, approaches, on the other hand, give information as long as the business process is running. It is more difficult to define approaches belonging to the latter category: the process generating the data or the frequency of events emission, may be subject to modification or drifts (which may also be seasonable). To tackle this on-line problems, it is therefore necessary to use tools able to adapt to new or unobserved scenarios.\nOne of the most challenging task in process mining is prediction. Obviously, predictions are useful only if the object of such predictions have not been observed yet. For this reason, prediction per se is, inherently, a task performed on incomplete traces, at runtime. Moreover, the ability to predict the evolution of a running case is difficult also due to the peculiarities of each process instance and, because of the \u201chuman factor\u201d, which might introduce strong flexibility.\nThe literature proposes plenty of works aiming at improving business processes and providing support for their execution. One of the first work that analyzes the execution duration problem is described in [4]. This particular work concentrates on cross-trained resources, but no detailed prediction algorithm is reported. van Dongen et al., in [5, 6], describe a prediction model which uses all the data recorded in an event log. This approach uses non-parametric regression in order to predict the \u201ccycle time\u201d of running process instances. The recommendation system, described by van der Aalst et al. in [7], is built using historical information, and is able to predict the most likely activity that a running case is going to perform. The TIBCO Staffware iProcess Suite [8] is one of the first commercial tools that predicts the cycle time of running process instances. This tool simulates the complete process instance without analyzing historical data. The main building blocks of the prediction are parameters, provided by the users at \u201cbuild time\u201d, such as the process routing or the expected duration of activities.\nRecently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14]. We will discuss these methods in more details in the next section.\nIn this work, we focus on predicting the remaining time of running cases. We decided to ground our approach not only on the control flow, but also on additional data that we could observe. Moreover, the system we present is also capable of dealing with unexpected scenarios and evolving conditions.\nWith respect to our seminal work [16], here we propose an improved version of that method and we also propose two novel approaches able to overcome its limitations. In particular, we are going to define two different scenarios: in the first one we assume the process has a well defined static workflow, the same assumption made in [16], while in the latter we remove such assumption and the process is considered dynamic, e.g., the process has seasonal drift. We will show that all the proposed approaches improve state-of-the art performances in the first scenario and one of them are also able to deal with dynamic processes. We also leverage one of these models to predict the future sequence of activity of a running case. We assess the prediction accuracy of our methods against the state-of-the art ones using two real-life case studies concerning a process of an Italian software company and the management of road-traffic fines by a local police office of an Italian municipality.\nThe remainder of this paper is structured as follows: Section 2 reviews recent works concerning prediction tasks in the framework of process mining. Section 3 gives some essential background on process mining and machine learning concepts used throughout the paper, while Section 5 describes the prediction approaches. Section 6 shows the implementation and the experimental results and finally Section 8 briefly summarizes the presented content and concludes the paper."}, {"heading": "2. Related Work", "text": "The first framework focused on the time perspective have been proposed by Song et al., in [9]. They describe a prediction approach which extracts a transition system from the event log and decorates it with time information extracted from historical cases. The transition system consists in a finite state machine built with respect to a given abstraction of the events inside the events log. The time prediction is made using the time information e.g., mean duration of specific class of cases) inside the state of the transition system corresponding to the current running instance. The work presented in [10] considers the data perspective in order to identify SLAs (Service Level Agreement) violations. In this work, authors try to estimate the amount of unknown data, in order to improve the quality of the final prediction. The actual forecast is built using a multilayer perceptron, trained with the Backpropagation algorithm. Two works by Folino et al. [11, 12] report an extended version of the technique described in [9]. In particular, they cluster the log traces according to the corresponding \u201ccontext features\u201d and then, for each cluster, they create a predictive model using the method described in [9]. The clustering phase is performed using predictive clustering trees. In order to propose a forecast, the approach clusters the new running instance and then uses the model belonging to the specific cluster. One of the weaknesses of these methods based on [9] is that they assume a static process, where the event log used for the training phase contains all the possible process behaviours. Unfortunately, in real life cases this assumption is usually not valid. [13] reports an approach which uses the Instance-specific Probabilistic Process Models (PPM) and is able to predict the likelihood of future activities. Even though the method does not provide a time prediction, it gives to the business managers useful information regarding the progress of the process. This work also shows that the PPM built is actually Markovian.\nGhattas et al., in a recent work [14], exploit Generic Process Model and decision trees, based on the process context, to provide decision criteria defined according to the actual process goals. In [15], de Leoni et al. propose a general framework able to find correlation between business process characteristics. In this work they manipulate the event log in order to enrich it with derived information and then generate a decision tree in order to discover correlations. In particular, one of the correlation problem suggested here is the forecast of the remaining time of running case. Being based on decision trees, numeric values need to be discretized and this lower the accuracy of the method. For this reason, they do not provide any prediction example. Finally, in [16], Polato et al. show an approach based on [9] in which the additional attributes of the events are taken into account in order to refine the prediction quality. This method exploits the idea of annotating a transition system (presented in [9]) adding machine learning models, such as Na\u0308\u0131ve Bayes and Support Vector Regressor. The experimental results show how the additional attributes can influence positively the prediction results.\nIn this paper we propose two new approaches based on Support Vector Regression and we discuss their strengths and their weaknesses comparing with the approaches presented in [9]. In particular we emphasize in which scenario an approach is better then the others and why.\nApproaches coming from different areas can also be used to achieve similar results. For example, queue theory [17, 18] and queue mining can be seen as a very specific type of process mining, and recent works are starting to aim for similar prediction purposes [19]. In this particular case, authors decided to focus on the delay prediction problem (i.e., providing information to user waiting in lines, in order to improve customer satisfactions). The method presented here is based on the construction of an annotated transition system. Such model is then used to make delay predictions using simple averages or non-linear regression algorithms. They also show other methods based on queue mining techniques. Even if this approach concerns making predictions on the time perspective, the slant is totally different and the goal is more narrow with respect to the aim of the approach we propose in this work. Another example of prediction-focused paper has recently been published by Rogge-Solti and Weske [20]. Authors, in this case, focused on the prediction of the remaining time, in order to avoid missing deadlines. In this case, however, a Petri net representation of the process model is needed. In the work we present, we are going to relax this assumption since we will solely rely on our log (and a transition system, built starting from it)."}, {"heading": "3. Background", "text": "This section describes the basic notations and definitions necessary to understand our approach."}, {"heading": "3.1. Preliminaries", "text": "A multiset M (also known as bag or m-set) [21] is a generalization of set in which the elements may occurs multiple times, but these are not treated as repeated elements. It is formally defined as a function M : A \u2192 N+ such that for each a \u2208 A,M(a) > 0. The set A is called the root set because an element a is contained into the m-set M , a \u2208m M , if a \u2208 A.\nThe cardinality of a multiset M : A\u2192 N+, denoted by #M , is equal to the sum of the multiplicity of its elements, #M = \u2211 a\u2208AM(a). We call B(A) : A\u2192 N the set of multiset over a finite set A, i.e., X \u2208 B(A) is a m-set. Given two multisets M \u2208 A\u2192 N+,M \u2032 \u2208 B \u2192 N+ the notions of intersection and disjoint union are the following (to ease the readability we assume that X(c) = 0 if c /\u2208 Dom(X)):\n\u2022 Intersection: M = X CX \u2032 = {(A \u2229 B)\u2192 N+ | \u2200 c \u2208 A \u2229 B, M(c) = min(X(c), X \u2032(c))}\n\u2022 Disjoint Union: M = X ]X \u2032 = {(A \u222a B)\u2192 N+ | \u2200 c \u2208 A \u222a B, M(c) = X(c) +X \u2032(c))}\nLet us now define the concept of sequence. Given a set A, a finite sequence over A of length n is a mapping s \u2208 S : ([1, n] \u2282 N)\u2192 A, and it is represented by a string, i.e., s = \u3008s1, s2, . . . , sn\u3009. Over a sequence s we define the following functions:\n\u2022 selection operator (\u00b7): s(i) = si, \u2200 1 \u2264 i \u2264 n;\n\u2022 hdk(s) = \u3008s1, s2, . . . , smin(k,n)\u3009;\n\u2022 tlk(s) = \u3008sw, sw+1, . . . , sn\u3009 where w = max(n\u2212 k + 1, 1);\n\u2022 |s| = n;\n\u2022 s \u2191 A, is the projection of s onto some set A, e.g., \u3008a, b, b, c, d, d, d, e\u3009 \u2191 {a, d} = \u3008a, d, d, d\u3009;\n\u2022 set(s) = {si | si \u2208 s}, e.g., set(\u3008a, b, b, c, d, d, d, e\u3009) = {a, b, c, d, e}."}, {"heading": "3.2. Event Logs", "text": "Process mining techniques can extract information from event logs (Table 1). Usually, these techniques assume that event logs are well structured, in particular, they assume that each event of a business process is recorded and it refers to an actual activity of a particular case. Even other additional information may be required by a process mining algorithm, such as the originator of an activity or the timestamp. Nowadays, many companies are using softwares which keep tracks of the business process execution in the form of event logs (e.g., transaction logs, databases, spreadsheets, etc).\nIn this section we define some useful process mining concepts which we will use throughout the paper. First of all, we give the basic definition of event, trace and event log.\nDefinition 3.1 (Event). An event is a tuple e = (a, c, t, d1, . . . , dm), where a \u2208 A is the process activity associated to the event, c \u2208 C is the case id, t \u2208 N is the event timestamp (seconds since 1/1/19701) and d1, . . . , dm is a list of additional attributes, where \u2200 1 \u2264 i \u2264 m, di \u2208 Di. We call E = A\u00d7 C \u00d7 T \u00d7 D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Dm the event universe.\n1We assume this representation according to the Unix epoch time."}, {"heading": "65923 20-02-2002:11.11 Jack A - 1000", "text": ""}, {"heading": "65923 20-02-2002:13.31 Jack B Gold 1000", "text": ""}, {"heading": "65923 21-02-2002:08.40 John C Gold 900", "text": ""}, {"heading": "65923 22-02-2002:15.51 Joe F Gold 900", "text": ""}, {"heading": "65924 19-02-2002:09.10 Jack A - 200", "text": ""}, {"heading": "65924 19-02-2002:13.22 John B Standard 200", "text": ""}, {"heading": "65924 20-02-2002:17.17 John D Standard 200", "text": ""}, {"heading": "65924 21-02-2002:10.38 Joe F Standard 200", "text": ""}, {"heading": "65925 25-02-2002:10.50 Jack A - 850", "text": ""}, {"heading": "65925 25-02-2002:13.01 John B Gold 850", "text": ""}, {"heading": "65925 25-02-2002:16.42 Joe E Gold 500", "text": ""}, {"heading": "65925 26-02-2002:09.30 Joe F Gold 500", "text": "Over an event e we define the following projection functions: \u03c0A(e) = a, \u03c0C(e) = c, \u03c0T (e) = t and \u03c0Di(e) = di,\u2200 1 \u2264 i \u2264 m. If e does not contain the attribute value di for some i \u2208 [1,m] \u2282 N, \u03c0Di(e) =\u22a5. In the remainder of this paper we will call the number of additional attribute m as |D|.\nDefinition 3.2 (Trace, Partial Trace). A trace is a finite sequence of events \u03c3c = \u3008e1, e2, . . . , e|\u03c3c|\u3009 \u2208 E\u2217 such that \u2200 1 \u2264 i \u2264 |\u03c3c|, \u03c0C(ei) = c \u2227 \u2200 1 \u2264 j \u2264 |\u03c3c|, \u03c0T (\u03c3c(j)) \u2264 \u03c0T (\u03c3c(j + 1)). We define a partial trace of length k as \u03c3kc = hd\nk(\u03c3c), for some k \u2208 [1, |\u03c3c|] \u2282 N. We call \u03a3 the set of all possible (partial) traces.\nWhile a trace corresponds to a complete process instance, i.e., an instance which is both started and completed; a partial trace represents a process instance which is still in execution and hence it has not completed yet. Over a trace \u03c3c = \u3008e1, e2, . . . , e|\u03c3c|\u3009 we define the following projection functions: \u03a0A(\u03c3c) = \u3008\u03c0A(e1), \u03c0A(e2), . . . , \u03c0A(e|\u03c3c|)\u3009, \u03a0T (\u03c3c) = \u3008\u03c0T (e1), \u03c0T (e2), . . . , \u03c0T (e|\u03c3c|)\u3009 and \u03a0Di(\u03c3c) = \u3008\u03c0Di(e1), \u03c0Di(e2), . . . , \u03c0Di(e|\u03c3c|)\u3009 for all 1 \u2264 i \u2264 |D|.\nLet us now define the concept of event log as in [9].\nDefinition 3.3 (Event log). An event log L is a set of traces, L = {\u03c3c | c \u2208 C} such that each event appears at most once in the entire log, i.e., \u2200\u03c31, \u03c32 \u2208 L, \u03c31 6= \u03c32 : set(\u03c31) \u2229 set(\u03c31) = \u2205."}, {"heading": "3.3. Transition System", "text": "With the definitions given in the previous subsections we can now characterize the concept of transition system and how to construct it starting from an event log.\nA transition system is one of the simplest process modeling notations, it consists of states and transitions where each transition connect two states (not necessarily different). A transition system is also referred as a Finite-State Machine (FSM). From a mathematical point of view, it can be seen as a directed\ngraph in which every possible path from the initial state to the accepting ones represents a possible behavior of the underlying process.\nFormally, it is defined as follows:\nDefinition 3.4 (Transition System (TS)). A transition system is a triplet TS = (S,A, T ), where S is the set of states, A \u2286 A is the set of activities and T \u2286 S \u00d7 A\u00d7 S is the set of transitions. Sstart \u2286 S is the set of initial states, and Send \u2286 S is the set of final (accepting) states.\nA walk, in a transition system, is a sequence of transitions \u3008t1, t2, . . . , tn\u3009 such that t1 = (s1 \u2208 Sstart, e, s\u20321), tn = (sn, e, s\u2032n \u2208 Send) and \u2200 1 < h < n, th = (sh, e, sh+1). Given a state s \u2208 S, it is possible to define the set of reachable states from s as: s\u2022 = {s\u2032 \u2208 S | \u2203t \u2208 T, \u2203e \u2208 E s.t. t = (s, e, s\u2032)}.\nAccording to van der Aalst et al. [9], to construct a transition system which maps each partial trace in the log to a state, we need the so called state and event representation functions.\nDefinition 3.5 (State representation function). Let Rs be the set of possible state representations, a state representation function f state \u2208 \u03a3\u2192 Rs is a function that, given a (partial) trace \u03c3 returns some representation of it (e.g., sequences, sets, multiset over some event properties).\nDefinition 3.6 (Event representation function). Let Re be the set of possible event representations, an event representation function fevent \u2208 E \u2192 Re is a function that, given an event e produces some representation of it (e.g., projection functions over e \u2208 E: \u03c0A(e), \u03c0T (e)).\nChoosing the right functions f state and fevent, also referred to as abstractions, is not a trivial task [22, 9]. A conservative choice (e.g., no abstraction: f state(\u03c3c) = \u03c3c, f\nevent(e) = e) can lead to a transition system which does overfit the log L, because the state space becomes too large and specific. An aggressive choice (e.g., f state(\u03c3c) = {\u03c3c(|\u03c3c|)}), instead, can lead to a transition system that overgeneralizes the log L, allowing too much behaviors. In this latter case the transition system is underfitting L. Some possible good choices for f state and fevent are described and discussed in [22] and [9]. A common event abstraction is fevent(e) = \u03c0A(e), which maps an event onto the name of the activity, while commons state abstractions are: the set abstraction, i.e., f state(\u03c3c) = {\u03c0A(e) | e \u2208 \u03c3c}, the multiset abstraction, i.e., f state(\u03c3c) = {(a,m) | a = \u03c0A(e)\u2227m = |\u03a0A(\u03c3c) \u2191 {a}|} and the list abstraction, i.e., f state(\u03c3c) = \u3008\u03c0A(\u03c3c(1)), . . . , \u03c0A(\u03c3c(|\u03c3c|))\u3009.\nUsing these two functions fevent and f state, it is possible to define a (labeled) transition system where states correspond to prefixes of traces in the log mapped to some representations using f state, and transitions correspond to representation of events through fevent.\nDefinition 3.7 (Labeled Transition System (LTS)). Given a state representation function f state, an event representation function fevent and an event log L, we define a Transition System as LTS = (S,E, T ), where:\n\u2022 S = {f state(hdk(\u03c3)) | \u03c3 \u2208 L \u2227 0 \u2264 k \u2264 |\u03c3|} is the state space;\n\u2022 E = {fevent(\u03c3(k)) | \u03c3 \u2208 L \u2227 1 \u2264 k \u2264 |\u03c3|} is the set of event labels;\n\u2022 T (\u2286 S \u00d7 E \u00d7 S) = {f state(hdk(\u03c3)), fevent(\u03c3(k + 1)), f state(hdk+1(\u03c3))) | \u03c3 \u2208 L \u2227 0 \u2264 k < |\u03c3|} is the transition relation.\nSstart = {f state(\u3008\u3009)} is the set with the unique initial state, and Send = {f state(\u03c3) | \u03c3 \u2208 L} is the set of final (accepting) states.\nWe say that a trace is compliant with the transition system if it corresponds to a walk from si \u2208 Sstart to se \u2208 Send. We also call a trace \u03c3 non-fitting with respect to a transition system if f state(\u03c3) /\u2208 S.\nA straightforward method for constructing a transition system, given a log L, is the following: for each trace \u03c3 \u2208 L, and for each 1 \u2264 k \u2264 |\u03c3|, we create, if does not exist yet, a new state f state(hdk(\u03c3)). Then, through a second iteration over k, 1 \u2264 k < |\u03c3|, we create, if does not exist yet, a new transition fevent(\u03c3(k+1)) | f state(hdk(\u03c3)) fevent(\u03c3(k+1))\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 f state(hdk+1(\u03c3)). Algorithm 1 shows a pseudocode of the method just described, while Figure 1 depicts an example of a transition system extracted from the log fragment reported in Table 1.\nAlgorithm 1: Construction of a Transition System\nInput: L: event log; f state: state representation function; fevent: event representation function Output: TS: transition system\n1 E, T \u2190 \u2205 2 S \u2190 {f state(\u3008\u3009)} . initialize with the start state 3 foreach \u03c3 \u2208 L do 4 for k \u2190 1 to |\u03c3| do 5 if s = f state(hdk(\u03c3)) /\u2208 S then 6 S \u2190 S \u222a {s} . add a new state if necessary 7 end\n8 end\n9 end"}, {"heading": "10 foreach \u03c3 \u2208 L do", "text": ""}, {"heading": "11 for k \u2190 0 to |\u03c3| \u2212 1 do", "text": "12 s\u2190 f state(hdk(\u03c3)) 13 e\u2190 fevent(\u03c3(k + 1)) 14 s\u2032 \u2190 f state(hdk+1(\u03c3)))"}, {"heading": "15 if e /\u2208 E then", "text": "16 E \u2190 E \u222a {e} 17 end\n18 if t = (s, e, s\u2032) /\u2208 T then 19 T \u2190 T \u222a {t} . add a new transition if necessary 20 end"}, {"heading": "21 end", "text": ""}, {"heading": "22 end", "text": "23 TS\u2190 (S,E, T ) 24 return TS"}, {"heading": "4. Machine Learning Background", "text": "This section provides the fundamental machine learning concepts required throughout the paper."}, {"heading": "4.1. Na\u0308\u0131ve Bayes Classifier", "text": "Classification is a machine learning task that consists in predicting category membership of data instances. More formally, given a \u201cconcept\u201d F : X \u2192 Y which maps elements of the domainX into a range Y = {y1, y2, . . . , ym} (i.e., the possible categorizations), the classification task consists in learning a function F\u0303 which constitutes a good approximation of F .\nNa\u0308\u0131ve Bayes (NB) [23] is a probabilistic classifier which is based on the application of Bayes\u2019 theorem. This classifier belongs to the family of the so called supervised algorithms. These algorithms need a set of pre-classified instances in order to learn how to classify new, unseen, instances.\nLet ~x = (x1, x2, . . . , xn) \u2208 X be an n-dimensional vector. From a probabilistic point of view, the probability that ~x belongs to a category yi \u2208 Y is given by the Bayes\u2019 theorem: P (yi | ~x) = P (yi)P (~x | yi)/P (~x), where P (yi) is the a-priori probability of yi and P (~x) is the a-priori probability of ~x. The estimation of the conditional probability P (~x | yi) can be very hard to compute, because of the number of possible ~x may be very large. In order to simplify this problem, we can assume that the components xi of the vector ~x (viewed as random variables) are conditionally independent each other given the target information yi. With this assumption, P (~x | yi) can be easily computed by this product \u220fn k=1 P (xk |yi). However, the independence assumption is quite strong and in many cases it does not hold. This is why this method is usually named na\u0308\u0131ve.\nTo get the classification of the vector ~x, we have to find out the maximum a posteriori (MAP) class yMAP :\nyMAP = arg max y\u2208Y\nP (y | x1, x2, . . . , xn) (1)\n= arg max y\u2208Y P (y) n\u220f k=1 P (xk | y). (2)\nIt is worth to notice that in (2), if one (or more) probability P (xk | y) is zero the whole product is equal to zero. This undesirable situation can be avoid applying the Laplacian (or additive) smoothing [23] to the conditional probabilities P (xk | y). Sometimes, it is necessary to get not only the MAP class, but the probability distribution of the whole category. This can be compute by P (y) \u220fn k=1 P (xk | y)/P (x1, . . . , xn) for all y \u2208 Y .\nThe training phase of this method consists in the collection of the statistics, from the training set, necessary to calculate yMAP. The classification is simply the application of yMAP (or the class distribution) over the input vectors."}, {"heading": "4.2. Support Vector Regression", "text": "Regression analysis is a statistical process for estimating the relationships among variables. This approach is widely used for prediction and forecasting. One of the most recently proposed approaches is the Support Vector Regression (SVR) [24, 25, 26], which is, as the name suggests, based on Support Vector Machines (SVM).\nLet Tr = {(~x1, y1), (~x2, y2), . . . , (~xl, yl)} \u2208 X\u00d7R be the training data, where X \u2261 Rn, for some n \u2208 N+, denotes the space of the input vectors. In -SVR [25], the goal is to find a function f(~x) that deviates from the target yi by at most , for all the training instances. In addition to that, the function f has to be as \u201cflat\u201d as possible. Let\u2019s start considering the linear case, in which f has the following form:\nf(~x) = \u3008~w, ~x\u3009+ b with ~w \u2208 X, b \u2208 R (3)\nwhere \u3008~w, ~x\u3009 is the dot product between ~w and ~x in X. In Eq. 3, flatness means that the norm of ~w has to be as small as possible. A possible way to cope with this problem is to minimize the Euclidean norm, i.e., \u2016~w\u20162. Formally, this can be written as a quadratic constrained optimization problem:\nminimize 1\n2 \u2016~w\u20162 + C l\u2211 i=1 (\u03bei + \u03be \u2217 i )\nsubject to  yi \u2212 \u3008~w, ~xi\u3009 \u2212 b \u2264 + \u03bei\u3008~w, ~xi\u3009+ b\u2212 yi \u2264 + \u03be\u2217i \u03bei, \u03be \u2217 i \u2265 0\nwhere \u03bei, \u03be \u2217 i are slack variables which allow the violation of some constraints and the constant C > 0 represents a trade-off between the amount of allowed deviations, greater than , and the flatness of f . The dual formulation of this\nconvex optimization problem [25, 24, 26] provides the key for extending SVR to nonlinear functions:\nL = 1\n2 \u2016~w\u2016+ C l\u2211 i=1 (\u03bei + \u03be \u2217 i )\u2212 l\u2211 i=1 \u03b1i( + \u03bei \u2212 yi + \u3008~w, ~xi\u3009+ b)+\n\u2212 l\u2211 i=1 \u03b1\u2217i ( + \u03bei \u2212 yi + \u3008~w, ~xi\u3009+ b)\u2212 l\u2211 i=1 (\u03b7i\u03bei + \u03b7 \u2217 i \u03be \u2217 i )\n(4)\nwhere the dual variables have to satisfy positivity constraints (\u03b1i, \u03b1 \u2217 i , \u03b7i, \u03b7 \u2217 i \u2265 0).\nThe optimal solution of this problem is guaranteed to be in the saddle point, hence the solution can be found setting the partial derivatives of L, with respect to the primal variables ~w, b, \u03bei, \u03be \u2217 i , to zero, and then substituting back on Eq. 4 obtaining:\nmaximize l\u2211 i,j=1 (\u03b1i \u2212 \u03b1\u2217i )(\u03b1j \u2212 \u03b1\u2217j )\u3008~xi, ~xj\u3009 \u2212 l\u2211 i=1 (\u03b1i + \u03b1 \u2217 i ) + l\u2211 i=1 yi(\u03b1i \u2212 \u03b1\u2217i )\nsubject to\n{ \u2211l i=1(\u03b1i + \u03b1 \u2217 i ) = 0\n\u03b1i, \u03b1 \u2217 i \u2208 [0, C].\n(5)\nThe resulting hyperplane equation is ~w = \u2211l i=1(\u03b1 \u2217 i\u2212\u03b1i)~xi, and consequently\nwe can rewrite Eq. 3 as f(~x) = \u2211l i=1(\u03b1i\u2212\u03b1\u2217i )\u3008~xi, ~x\u3009+ b, which is called support vector expansion, because ~w is described as a linear combination of the training data ~xi. Exploiting the Karush-Kuhn-Tucker (KKT) conditions [26], which states that at the optimum the product between constraints and dual variables is zero, we can compute b and we can also notice that only for |f(~xi \u2212 yi)| = the coefficients \u03b1i, \u03b1 \u2217 i are non-zero. This particular set of training examples are called support vectors. The nonlinearity of this model can be achieved mapping (\u03a6 : X \u2192 S) the input vector ~x to a highly dimensional space S (i.e. feature space) and then applying the standard SVR method. Cover\u2019s Theorem [27] proves that given a set of training data that is not linearly separable in the input space, it is possible to transform the data into a training set that is linearly separable, with high probability, by projecting it into a higher-dimensional space (feature space) via some non-linear transformation.\nUnfortunately a direct approach is, in most cases, infeasible from a computational point of view. Anyhow, it is worthwhile to note that the dual form depends only on the dot product between the input vectors. Hence, to get the nonlinearity, it is sufficient to know the dot product of the input vectors in the feature space k(~x, ~x\u2032) = \u3008\u03a6(~x),\u03a6(~x\u2032)\u3009. Then, it is easy to rewrite the dual form with k(~x, ~x\u2032) instead of \u3008~xi, ~xj\u3009. Exploiting the Mercer\u2019s Theorem [26] it is possible to characterize these type of functions k, called kernel functions. Using \u03a6 and the kernel function k we can rewrite the optimization problem and the support vector expansion substituting the products \u3008\u00b7, \u00b7\u3009 with k(\u00b7, \u00b7) and ~xi with \u03a6(~xi). Solving this new optimization problem, which coincides with the algorithm\u2019s training phase, means finding the flattest function in the feature space. Once defined a suitable kernel function k, the standard SVR function which\nsolve the approximation problem has this form:\nf\u2217(~x) = l\u2211 i=1 (\u03b1\u2217i \u2212 \u03b1i)k(~xi, ~x) + b (6)\nwhere \u03b1i and \u03b1 \u2217 i are Lagrange multipliers obtained by solving Eq. 5."}, {"heading": "5. Remaining Time Prediction", "text": "In this section we are going to show a spectrum of different approaches able to predict the remaining time of running business process instances. In particular, we will emphasize the pros and cons of each approach and which are the situations in which an approach should be preferred among the others.\nThe problem of predicting the remaining time of running process instances can be summarized as follows: given an event log, containing historical traces about the execution of a business process, we want to predict, for an ongoing process instance, how much time remains until its completion.\nAll the approaches described in this work are based on the idea of making prediction using a model constructed (i.e., learned) using the information collected so far (i.e., the event log). The obtained model takes the partial trace, which represents the running process instance, as input and returns the remaining time forecast.\nThe remaining time of a case consists of the time that will be spent from now up to the end of the execution of the last activity of the process. This amount of time, given a complete trace \u03c3c, is easily computable for each event ei \u2208 \u03c3c. We define the function rem : \u03a3\u00d7 N\u2192 N as follow:\nrem(\u03c3c, i) = \u03c0T (\u03c3c(|\u03c3c|))\u2212 \u03c0T (\u03c3c(i))\nwhere i is an event index. If \u03c3c = \u3008\u3009 or i /\u2208 {1, 2, . . . , |\u03c3c|} then rem(\u03c3c, i) = 0. This function calculates the time difference between the last event in the trace and the i-th event.\nIn the reminder of this section we are going to present our new time prediction approaches based on the application of machine learning models. Moreover, we discuss how to exploit one of the proposed model to predict also the future sequence of activities.\n5.1. Approach 1: Simple Regression\nThis prediction task has all the characteristics to be faced as a regression problem. (Partial) Traces in the event log are the input vectors and the remaining time at each event are the target values. In this section we are going to present a direct application of -SVR algorithm. Despite the simplicity of this method, there are some aspects which need particular attention. First of all we describe how to switch from the event log to a representation suitable for the -SVR algorithm."}, {"heading": "5.1.1. Features Representation", "text": "In our setting, the input consists of traces and, in particular, the attributes of the corresponding events. Whereas SVR takes as input vectors ~x \u2208 Rl, for some l \u2208 N+, we have to convert sequences of events into some representation in Rl.\nLet us consider a (partial) trace \u03c3c = \u3008e1, e2, . . . , en\u3009 of length n \u2208 N+, for each event ei = (a i, c, ti, di1, . . . , d i m) \u2208 \u03c3kc , the attributes di1, . . . , dim may have different values, because they can change as the process instance evolves. We consider as additional attributes values (i.e., di1, . . . , d i m) the last values chronologically observed. Formally, we define the function last : \u03a3\u00d7N\u2192 D\u222a{\u22a5} as:\nlast(\u03c3c, i) = { \u03c0Di(ej) | j = arg max\n1\u2264h\u2264|\u03c3c| \u03c0Di(eh) 6=\u22a5\n} .\nwhere i is the index of the attribute. If there is no index j such that \u03c0Di(\u03c3c(j)) 6=\u22a5 then last(\u03c3c, i) =\u22a5. What we need to do next is to transform the domains Di into a numerical representation that can be given as input to the SVR. In order to do that, we use the one-hot encoding. This encoding converts the nominal data value di into a binary vector v \u2208 {0, 1}|Di|, with |Di| components and with all values set to 0 except for the component referring to di, which is set to 1. All the other attributes values di such that Di \u2286 R are simply put in a single component vector, e.g., let Di \u2261 N \u2282 R and \u03c0D(e) = 17, the output vector is ~u = [17] \u2208 R. In the remainder of this paper we will call 1 : A \u2192 Rj , for some j \u2208 N+, the function which maps an attribute value to its one-hot encoded vector.\nAfter the conversion just described, all the vectors are concatenated (keeping the same fixed order) together, e.g., recalling v \u2208 R4 and u \u2208 R from the previous examples, their concatenation is equal to ~z = ~v || ~u = [0, 1, 0, 0]||[17] = [0, 1, 0, 0, 17] \u2208 R5. Note that if \u03c0Di(e) =\u22a5 and Di is nominal, then \u22a5 is projected to a vector (\u2208 {0, 1}|Di|) with all components set to zero. Otherwise, if Di \u2286 R the value \u22a5 is simply interpreted as a zero. Eventually, the concatenation of all vectors constitutes an input vector ~x \u2208 Rl for the -SVR.\nWe summarize all of these steps with the function \u03b3\u2217 : \u03a3 \u2192 Rl, i.e., \u2200i, \u03b3\u2217(\u03c3ic) = ~xi, such that \u03b3\u2217(\u03c3) = f j 1(last(\u03c3, j)).\nWith respect to the target value, it is calculated using the function rem, e.g., y = rem(\u03c3c, i) for some 1 \u2264 i \u2264 |\u03c3c|. Hence, starting from a trace \u03c3c = \u3008e1, e2, . . . , en\u3009, the corresponding set of n training examples (~x, y) will be (\u03b3\u2217(\u03c3ic), rem(\u03c3c, i)),\u2200i \u2208 {1, . . . , n}."}, {"heading": "5.1.2. Training", "text": "As discussed in Section 4.2, a training data set for a -SVR algorithm is defined as Tr = {(~x1, y1), (~x2, y2), . . . , (~xl, yl)} \u2208 Rn \u00d7 R, for some n \u2208 N+. In order to map the data contained in an event log L, we exploit the transformations described in the previous section. In particular, the training set is created by the Algorithm 2.\nThe value returned by the function rem depends on the time granularity (e.g., hours, minutes, seconds). It is important to keep the same granularity for all the instances. Once constructed the training set Tr, the training phase consists in solving the optimization problem (Eq. 5) with input Tr.\nAlgorithm 2: Training set construction\nInput: L: event log Output: Tr: training set\n1 Tr\u2190 \u2205 2 foreach \u03c3 \u2208 L do 3 for k \u2190 1 to |\u03c3| do 4 ~x\u2190 1(\u03c0A(\u03c3(k))) 5 for i\u2190 1 to |D| do . for each attribute in L 6 v = last(\u03c3k, i) 7 ~x\u2190 ~x || 1(v) 8 end 9 y = rem(\u03c3, k)\n10 Tr\u2190 Tr \u222a (~x, y)"}, {"heading": "11 end", "text": ""}, {"heading": "12 end", "text": ""}, {"heading": "13 return TS", "text": ""}, {"heading": "5.1.3. Prediction", "text": "After the training phase, the -SVR model is created (i.e., function f\u2217, Eq. 6) and can be directly used to predict the remaining time of partial traces. First of all the trace is converted to a vector ~x suitable for the SVR, applying the same approach illustrated in Algorithm 2, in particular from line 4 to line 8. Then this vector ~x is given as input to f\u2217 which produces the time prediction. This prediction value has to be interpreted with the same granularity used in the training instances creation. Algorithm 3 shows the prediction algorithm.\nAlgorithm 3: Prediction\nInput: \u03c3p: (partial) trace, f \u2217: -SVR model Output: P: time prediction\n1 ~x\u2190 1(\u03c0A(\u03c3p(|\u03c3p|))) 2 for i\u2190 1 to |D| do 3 v = last(\u03c3kp , i) 4 ~x\u2190 ~x || 1(v) 5 end\n6 P \u2190 f\u2217(~x) 7 return P\n5.2. Approach 2: Regression with Contextual Information\nThis approach differs from the previous one since it makes use of control-flow information in order to add contextual knowledge. The basic idea consists of adding a limited set of features able to encapsulate the control-flow path followed by a partial trace. We chose as control-flow model a transition system because it generally represents a good trade-off between expressivity and compactness. In Section 3.3, we showed how to construct a labeled transition system TS =\n(S,E, T ) starting from an event log L. Now, we have to transform the TS into a series of features and encoding it into a proper form applicable to the -SVR algorithm. As for the literal attributes, we use the one-hot encoding: the set S \\Sstart is treated as a literal domain, where the possible values are the states s \u2208 S excluding the initial state because a non-empty trace always maps onto a state not included in Sstart. So, we enumerate the states, s1, s2, . . . , sn \u2208 S \\ Sstart, and we map a state si into a vector ~v \u2208 {0, 1}n by setting to 1 the i-th component in the n-dimensional vector, and to 0 all the others. For example, given the states set S \\ Sstart = {s1, s2, s3, s4} we encode the state s3 onto ~v \u2208 {0, 1}4 such that ~v = [0, 0, 1, 0]."}, {"heading": "5.2.1. Non-fitting Traces", "text": "As for the previous approach, even this one is able to handle non-fitting traces. However, without any adjustment, the prediction is calculated overlooking the control-flow. Indeed, using the encoding described above, if f state(\u03c3) /\u2208 S then the corresponding vector would be null (i.e., ~v = [0, 0, . . . , 0]). We cope this problem by mapping the non-compliant state s = f state(\u03c3) /\u2208 S, onto a set of lawful states si \u2208 S. The idea is to associate the non-fitting trace with states that are, within some degree, similar. Then the vector ~v will contain for each state the normalized similarity value. It is very important to define a thoughtful similarity function: we assume that two states are similar if their representations are similar. In particular, since we are focusing on control-flow, we use as event representation function something like fevent(e) = \u03c0A(e). This implies that abstractions are aggregate representations of a set of activities.\nLet us define a similarity function for each abstraction considered in Section 3 (i.e., set, bag and sequence):\nDefinition 5.1 (Set Similarity Function). Given two sets x1, x2 \u2286 X , with X the set of all possible values, we define the similarity function f simset \u2208 2X \u00d7 2X \u2192 [0, 1] as the Jaccard similarity [28]. Formally:\nf simset (x1, x2) = |x1 \u2229 x2| |x1 \u222a x2|\nDefinition 5.2 (Bag Similarity Function). Given two multi-sets over a root set X , x1, x2 \u2208 B(X ), we define the similarity function f simbag \u2208 B(X )\u00d7 B(X )\u2192 [0, 1] as the Jaccard similarity [28]. Formally:\nf simbag (x1, x2) = # (x1 C x2) # (x1 \u222a x2)\nDefinition 5.3 (List Similarity Function). Given two finite sequences over X , x1, x2 \u2208 S(X ), we define the similarity function f simlist \u2208 S(X ) \u00d7 S(X ) \u2192 [0, 1] \u2282 R as the Damerau-Levenhstein similarity [29]. Formally:\nf simlist (x1, x2) = 1\u2212 fdistD-L(x1, x2)\nmax(|x1|, |x2|)\nThe Damerau-Levenshtein distance (fdistD-L) is a distance between two string. It is calculated by counting the minimum number of edit operations needed to transform a string into the other. The set of possible edit operations takes into account by this metric are:\n\u2022 insertion of one character, e.g., ac becomes abc with the insertion of b;\n\u2022 deletion of one character, e.g., abc becomes ac with the deletion of b;\n\u2022 substitution of a character, e.g., ab becomes ac with the substitution of b with c;\n\u2022 transposition of two characters, e.g., abc becomes acb with the transposition of b and c.\nSince this metric works over strings, we need to convert a sequence of event representations into a string. To do this, we simply map each event onto a character.\nOn the basis of the abstraction used for constructing the transition system, the corresponding similarity function is chosen. Every time a non-fitting trace comes into play, its representation is compared with all the state representations of the TS (excluding the initial state). So, given a transition system TS = (S,E, T ) (created using fevent and f state), a similarity function f sim and a trace \u03c3 (such that s\u2032 = f state(\u03c3) /\u2208 S) f sim(s, s\u2032) is computed for each state s \u2208 S \\Sstart. After that each similarity value is normalized and finally put into the vector ~v. We will call this kind of TS similarity-based transition system.\nFormally, given s\u2032 /\u2208 S and a si \u2208 S\\Sstart the corresponding i-th component of the resulting vector ~v will contain the following value:\nf sim(s\u2032, si)\u2211 s\u2208S\\Sstart f sim(s\u2032, s) .\nLet us show a non-fitting trace management example: consider the transition system (i.e., TS = (S,E, T )) in Fig. 2 constructed using as state representation function the set abstraction (i.e., f state(\u03c3) = {fevent(e) | e \u2208 \u03c3}). In this example the state representation functions are: s0 = {}, s1 = {A}, s2 = {A,B}, s3 = {A,B,C}, s4 = {A,B,D}, s5 = {A,B,E}, s6 = {A,B,C, F}, s7 = {A,B,D, F} and s8 = {A,B,E, F}. Given the non-fitting trace \u03c3\u2032 = \u3008A,D\u3009 we calculate the similarity of s\u2032 = f state(\u03c3\u2032) = {A,D} with every s \u2208 S \\ {s0} using f simset :\nf simset (s \u2032, s1) =\n1 2 = 0.5, f simset (s \u2032, s2) = 1 3 = 0.3\u0304,\nf simset (s \u2032, s3) =\n1 4 = 0.25, f simset (s \u2032, s4) = 2 3 = 0.6\u0304,\nf simset (s \u2032, s5) =\n1 4 = 0.25, f simset (s \u2032, s6) = 1 5 = 0.2,\nf simset (s \u2032, s7) =\n2 4 = 0.5, f simset (s \u2032, s8) = 1 5 = 0.2.\nThen we normalize each value with the summation:\u2211 si\u2208S\\{s0} f simset (s \u2032, si) = 2.4\nobtaining the final vector:\n~v =\n[ 0.5\n2.4 ,\n0.3\u0304 2.4 , 0.25 2.4 , 0.6\u0304 2.4 , 0.25 2.4 , 0.2 2.4 , 0.5 2.4 , 0.2 2.4 ] = [0.2083\u0304, 0.138\u0304, 0.10416\u0304, 0.27\u0304, 0.10416\u0304, 0.083\u0304, 0.2083\u0304, 0.083\u0304]"}, {"heading": "5.2.2. Training", "text": "The training phase of this method is almost the same of the preceding one. The main difference lays on the introduction of a new derived feature to the training set. This can be done by making some minor changes to the Algorithm 2. Since we assume the construction of TS, we need it as input along with the state representation function f state and the similarity function f sim. We calculate the state associated to each partial traces and we encode it into a one-hot vector or into the normalized similarity vector if it is a non-fitting trace. Finally, we construct the rest of the training instances as in Alg. 2. Algorithm 4 show the non-fitting trace management routine.\nAlgorithm 4: Non-fitting trace encoder\nInput: \u03c3: non-fitting trace, S: states set, f state: state representation function, f sim: similarity function Output: Tr: training set\n1 s\u2190 f state(\u03c3) 2 ~v \u2190 [] 3 den\u2190 0 4 for si \u2208 S do . calculate the normalization factor 5 den\u2190 den + f sim(si, s) 6 end 7 for si \u2208 S do . append the normalized vector 8 c\u2190 f\nsim(si,s) den\n9 ~v \u2190 ~v || [c]"}, {"heading": "10 end", "text": ""}, {"heading": "11 return ~v", "text": "First loop (line 4) calculates the normalization factor, while the second loop (line 7) creates the vector."}, {"heading": "5.2.3. Prediction", "text": "In this phase, as for the Simple Regression approach, the -SVR model created in the previous step is used to forecast the remaining time of running process instances. The novelty of the method just described consists of the resulting model, which is obtained from the training phase. The introduction of contextual information, generally, leads to a different optimization problem and consequently to a different final model. The only changes to make in Alg. 3 are adding f state as input, and substitute the right side of line 1 with the one-hot (or the non-fitting) encoding of f state(\u03c3p).\n5.3. Approach 3: Data-aware Transition System (DATS)\nThe approach presented in this section is a refinement of [16] which exploits the same idea described in [9]. Let us recall the main characteristics of the latter method. In their work van der Aalst et al. introduced the concept of annotated transition system: each state of the transition system is \u201cdecorated\u201d with predictive information, called measurements. Since we want to predict the remaining time, a possible set of measurements collected in a state might be the remaining time starting from the state itself. Formally, in [9] a measurement is defined as:\nDefinition 5.4 (Measurement). A measurement function fmeasure is a function that, given a trace \u03c3 and en event index i \u2208 [1, 2, . . . , |\u03c3|] produces some measurement. Formally, fmeasure \u2208 \u03a3\u00d7N\u2192M, where M is the set of possible measurement values (e.g., remaining time).\nIn [9] different kinds of measurements are proposed. Once the suitable measurement is chosen, an annotated transition system is created according to the event log:\nDefinition 5.5 (Annotated transition system). Let L be an event log and TS = (S,E, T ) a labeled transition system constructed over L using the representation functions fevent and f state. Given a particular measurement function fmeasure : \u03a3 \u00d7 N \u2192 M, we define an annotation A \u2208 S \u2192 B(M), such that \u2200s \u2208 S:\nA(s) = \u228e \u03c3\u2208L \u228e 1\u2264k\u2264|\u03c3|\ns=f state(\u03c3k)\nfmeasure(\u03c3, k)\nAn annotated transition system is the tuple (S,E, T,A).\nSince we are facing the remaining time prediction problem, our fmeasure function coincides with the function rem defined in Section 5. The last step consists of the definition of a prediction function \u2208 B(M) \u2192 M, such that given a multiset of measurements produces some prediction, e.g., the average or the median value. So, in the operative setting we have an annotated transition system (S,E, T,A) constructed over L and a prediction function P : B(M)\u2192M. Using these tools a prediction is made in a straightforward way: given a partial trace \u03c3p observed so far, such that f\nstate(\u03c3p) = s, the prediction value is P(A(s)). It is worth to notice that the prediction is calculated using merely control-flow (i.e., transition system) and temporal (i.e., remaining time) information.\nThe main difference introduced by our approach is the addition of classifiers and regressors, which take advantage of additional attributes, as annotations. Let us give a brief overview of this approach. As in [9], we start with the transition system construction and then we enrich each state with a Na\u0308\u0131ve Bayes classifier (see Section 4.1) and each transitions with a Support Vector Regressor (see Section 4.2) trained with historical data considering all attributes. The introduction of these two machine learning models is based on the intuition that in a state s Na\u0308\u0131ve Bayes estimates the probability of transition from s to s\u2032 \u2208 s\u2022, while SVR predicts the remaining time if the next state will be s\u2032.\nFigure 3 proposes an example of state (s2) annotated with a Na\u0308\u0131ve Bayes classifier. In this state, the Na\u0308\u0131ve Bayes classifier gets the probabilities to reach each exiting state: probabilities to reach states s3, s4 and s5 are respectively 0.6, 0.1 and 0.3. Such values are used to weigh the remaining time values obtained from the support vector regressors. In Figure 3 the state s2 corresponds to the current partial trace \u03c3\u2032. From each outgoing state (i.e., s3, s4, s5) the remaining time is estimated using the SVR associated to the incoming transition (i.e., s2 \u2192 s3, s2 \u2192 s4 and s2 \u2192 s5).\nThese estimations are multiplied by the probability values obtained from the NB classifiers, and finally summed together in order to compute a weighted average over all the possible continuation from s. Figure 4 presents an example of remaining time estimation for each outgoing state. In this example the estimated remaining time, starting from the state s3, is 2 hours, while the average duration of the transition s2 \u2192 s3 is 20 minutes.\nFormally, let p\u0302s\u2032 be the Na\u0308\u0131ve Bayes estimated probability to reach state s\u2032 \u2208 s\u2022 from state s, and \u03c4\u0302s\u2192s\u2032 the estimated remaining time returned by the SVR associated to transition s\u2192 s\u2032. Then, given the state s reached after observing a (partial) trace \u03c3, the prediction returned by the annotated transition system is \u2211 s\u2032\u2208s\u2022(p\u0302s\u2032 \u00b7 \u03c4\u0302s\u2192s\u2032).\nLet us now define the annotations used to decorate the transition system.\nDefinition 5.6 (Na\u0308\u0131ve Bayes Annotation). Let TS be a labeled transition system, obtained from an event log L, based on an event representation function\nfevent and a state representation function f state. Let\u2019s call k the size of the \u03b3\u2217(\u03c3) vector calculated for traces \u03c3 \u2208 L. A Na\u0308\u0131ve Bayes Annotation is a function NB : S \u00d7 Rk \u00d7 S \u2192 [0, 1] \u2282 R, which, given two states si, sj \u2208 S and a data attribute vector ~x \u2208 Rk, returns the probability to reach the state sj starting from si through a single transition.\nDefinition 5.7 (SVR Annotation). Let TS be a labeled transition system, obtained from an event log L, based on an event representation function fevent and a state representation function f state. An SVR Annotation is a function R : T \u00d7Rk \u2192 R, such that, given a transition t \u2208 T and a data attribute vector ~x \u2208 Rk, it applies Support Vector Regression to produce an estimation of the remaining time.\nUsing these annotations, we define a predictor transition system as follows:\nDefinition 5.8 (Predictor TS). Let TS = (S,E, T ) be a labeled transition system, obtained from an event log L, based on an event representation function fevent and a state representation function f state. A predictor transition system is a tuple PTS = (S,E, T,NB, R) where NB, R are respectively a Na\u0308\u0131ve Bayes and a SVR annotation, based on the event log L and the transition system TS."}, {"heading": "5.3.1. Training", "text": "In this section, we are going to describe how to construct a predictor transition system. Algorithm 5 shows the construction procedure.\nAlgorithm 5: Construction of a Predictor Transition System\nInput: L: event log; TS = (S,E, T ): labeled transition system Output: T \u2032: predictor transition system\n1 foreach t \u2208 T do 2 svr[t] = \u2205 . training set for t 3 end\n4 foreach \u03c3c \u2208 L do 5 for i\u2190 1 to |\u03c3c| \u2212 1 do 6 s\u2190 f state(\u03c3ic) 7 s\u2032 \u2190 f state(\u03c3i+1c ) 8 e\u2190 fevent(\u03c3c(i+ 1)) 9 t\u2190 (s, e, s\u2032)\n10 ~x\u2190 \u03b3\u2217(\u03c3ic) 11 y \u2190 rem(\u03c3c, i) 12 svr[t]\u2190 svr[t] \u222a (~x, y)"}, {"heading": "13 if |s \u2022 | \u2265 2 then", "text": "14 Update NB for state s with instance (~x, s\u2032)"}, {"heading": "15 end", "text": ""}, {"heading": "16 end", "text": ""}, {"heading": "17 end", "text": ""}, {"heading": "18 foreach t \u2208 T do", "text": "19 Train SVR (R) for transition t with training set svr[t]"}, {"heading": "20 end", "text": "21 T \u2032 \u2190 (S,E, T,NB , R) 22 return T \u2032\nThe first loop (line 1) initializes all the training sets for the -SVR model to the empty set, while the second loop (lines 4-17) creates the training sets and updates the NB classifiers. In particular, lines 6-12 construct the training instances by extracting the additional data from the partial traces and calculating the remaining time. Being possible to build a NB model incrementally, this is done in line 14. Last loop (lines 18-20) trains the -SVR models using the training sets built previously."}, {"heading": "5.3.2. Prediction", "text": "In this section, we describe how to predict the remaining time for a running case using a predictor transition system constructed with Alg. 5. Algorithm 6 shows the prediction procedure.\nThe algorithm simply applies the formula, seen at the beginning of this section, \u2211 s\u2032\u2208s\u2022(p\u0302s\u2032 \u00b7 \u03c4\u0302s\u2192s\u2032). Each p\u0302s\u2032 is a value produced by the application of the NB classifiers and \u03c4\u0302s\u2192s\u2032 by the -SVRs. Specifically, let s = f state(\u03c3p), then p\u0302s\u2032 = NB(s, \u03b3 \u2217(\u03c3), s\u2032) and \u03c4\u0302s\u2192s\u2032 = R(s\u2192 s\u2032, \u03b3\u2217(\u03c3)), for all s \u2208 s\u2022. A core difference w.r.t. [16] is the absence of the expected sojourn time (on the current state): in this revised version this information is implicitly embedded inside the\nAlgorithm 6: Remaining time prediction for a running case\nInput: \u03c3p: (partial) trace; PTS = (S,E, T,NB, R): predictor transition system Output: P : remaining time prediction\n1 P \u2190 0 2 s\u2190 f state(\u03c3p) 3 ~x\u2190 \u03b3\u2217(\u03c3p) 4 if |Ts| \u2265 2 then 5 foreach t = (s, e, s\u2032) \u2208 Ts do 6 P \u2190 P + NB(s, ~x, s\u2032) \u00b7R(t, ~x) 7 end\n8 else\n9 s\u2032 \u2190 f state(\u03c3|\u03c3p|\u22121p ) 10 t\u2032 \u2190 (s\u2032, e, s) \u2208 Ts\u2032 11 P \u2190 R(t\u2032, ~x)"}, {"heading": "12 end", "text": ""}, {"heading": "13 return P", "text": "-SVR and hence can be removed from the formulation.\nFigure 5 puts together the two representations depicted in Section 5.3: it shows an example of NB and SVR application for a partial trace \u03c3 = \u3008A,B\u3009 (see event log in Table 1). The remaining time prediction, in hours, for this example is:\nP (s2) = \u2211\ns\u2032\u2208{s3,s4,s5}\n(p\u0302s\u2032 \u00b7 \u03c4\u0302s2\u2192s\u2032) = 0.6 \u00b7 2 + 0.1 \u00b7 3 + 0.3 \u00b7 1 = 1.8.\nPlease note that, given a predictor transition system (result of the learning procedure), the computation of the prediction requires constant number of operations which, in the worst case corresponds to the size of the largest set s\u2022\nof the transition system. This property allows the application of the approach in on-line settings [30], where each event is allowed to trigger only a constant number of operations."}, {"heading": "5.3.3. Future Path Prediction", "text": "The model we use in this last approach can also be exploit in order to predict, for a running case, which is the most likely sequence of activities until the end of the case. Let us take for example the situation depicted in Fig. 3 which is a fragment of the TS in Fig. 1: the most likely sequence of states starting from s2{B} is s2{B} \u2192 s3{C} \u2192 s6{F}, s6{F} is an accepting node and so the process instance is complete, with a probability equals to 0.6 \u2217 1 = 0.6. Note that the transition between s3{C} and s6{F} has a probability equals to 1 because is the only way the process can proceed. In general, the sequence can go through many split states in which the transition probabilities are < 1 and find the full sequence with the higher probability is not a trivial task.\nWe face this problem as a shortest path problem in which the goal is to find a path between to vertices of a graph such that the sum of weights of its constituent edges is minimized. Specifically, let us consider the transition system as a directed graph: we would like to find the shortest path between the current node and an accepting node. In order to leverage this idea, we need to define a suitable distance measure (i.e., cost) between nodes.\nGiven a possible sequence of activities (in a Predictor TS) between the node s1 to sn, i.e., s1, s2, . . . , sn, with the corresponding transition probabilities (obtained using the NB annotations), i.e., pi\u2200si \u2192 si+1, 1 \u2264 i < n, then the likelihood of such sequence is defined by \u220f 1\u2264i<n pi. Since probabilities have to be multiplied to get the likelihood of a sequence we cannot use it directly as edge cost because we need to follow the definition of the shortest path problem (i.e., edge cost have to be summed). However, we can exploit properties of the logarithm function to transform probabilities into distance like values, in particular:\nlog(pq) = log(p) + log(q),\nand since p are probabilities:\n\u22000 \u2264 p \u2264 1 \u2208 R, log(p) \u2264 0,\nwhere low values of log(p) mean that the transition probability is low, or, from a graph view point, the distance between the nodes is high. Using this idea, we can use as edge cost the opposite of the logarithm of the transition probability. Note that this logarithm transformation works as we desire: probabilities close to 0 (i.e., rare occurrences) correspond to high costs while probabilities close to 1 (i.e., very likely) correspond to costs almost null. Using the just mentioned transformation we can construct a graph corresponding to the TS where the shortest path problem can be solved applying a best first search. So, from a computational point of view the prediction of the activity sequence has a cost which is quadratic in the number of nodes (i.e., states of the TS)."}, {"heading": "6. Implementation", "text": "These techniques have been implemented for the ProM framework [31]. To mine the transition systems, we rely on the miner\u2019s implementation available\ninside the framework. Na\u0308\u0131ve Bayes Classification and the Support Vector Regression are performed using the implementation in the Weka framework [32]. In particular, for SVR we used the SMO (Sequential Minimal Optimization) implementation provided by the framework.\nThe main ProM plugin we developed is capable of building a prediction model according to the methods described earlier on this paper. Once the prediction model has been created, another ProM plugin exposes an on-line service which can be queried for predictions. This online querying layer uses JSON2 as communication languages and, therefore, in principle, any information system, implementing the query protocol, can embed the prediction features that we provide. Moreover, apart from these prediction functionalities, we also built another plugin which can be used to manually query the model. A screenshot of this plugin is reported in Figure 6. The plugin allows users to ask for prediction on a specific running instance, and the user can also specify a possible deadline for it. In case the prediction does not fulfill the deadline, an alarm is raised.\nThe figure shows how a prediction is visualized to the client-side plugin. On the left, there are all the information regarding the remaining time prediction (upper left corner) and also the activity sequence prediction (bottom left). The predicted time is highlighted in green when the provided deadline seems to be fulfilled in red otherwise. On the right side, there is the history of the queries, while the middle section shows two historical traces which have been the best performances. Specifically, these traces have all the same initial activities as the running case (shown as the first trace), but they are the ones which have the shortest remaining time. What differ is that one represent the fastest trace in the entire log regardless of the sequence of activities, while the second one (third in figure) is the fastest one which follows the predicted sequence of future activities. All these aim to support as much as possible the user in her decision making process.\nDue to the simplicity of the prediction procedures, the time for computing the forecast is negligible, thus paving the way to the adoption of our approach into intensive and on-line scenarios."}, {"heading": "7. Results", "text": "The experiments reported in this section aim to assess how the techniques leveraging on the similarity-based transition system give more accurate predictions for variants of process executions that have not been observed in the training set. The comparison is made with respect to the specialization of the technique reported in van der Aalst et al. [9] that has been discussed in Section 2. We also aimed to compare our approach versus the techniques proposed in [11, 12]. Unfortunately, up to this date, neither an implementation of these approaches nor the event logs used in the papers are publicly available. Of course, it is not possible to compare results obtained through different event logs, as the quality of a prediction heavily depends on the information available in the event log. Work [16] shows that no much difference can be appreciated\n2JSON stands for \u201cJavaScript Object Notation\u201d. More information can be retrieved from http://www.json.org/.\nwhen polynomial or RBF is used as kernel type. Therefore, the experiments only makes use of the latter.\nThe comparison is made using two real-life case studies: the first concerns with a ticketing management process of the help desk of an Italian software company, and the second with the execution of process instances in an information system for the management of road-traffic fines by a local police office of an Italian municipality.\nTo measure and compare the accuracy, we used two indicators: the Mean Absolute Percentage Error (MAPE) and the Root Mean Square Percentage Error (RMSPE). Let n be the number of samples and let Ai and Fi be respectively the actual value and the predicted value for the i-th sample. MAPE usually expresses the accuracy as a percentage:\nMAPE = 100%\nn n\u2211 i=1 \u2223\u2223\u2223\u2223Ai \u2212 FiAi \u2223\u2223\u2223\u2223 ,\nRMSPE is also defined as a percentage:\nRMSPE = 100%\n\u221a\u2211n i=1( Ai\u2212Fi Ai )2\nn ."}, {"heading": "7.1. Case Study 1: Road Fines Log", "text": "The first case study log concerns the execution of process instances in an information system for the management of road-traffic fines by a local police office of an Italian municipality. The management of road-traffic fines has to comply with Italian laws, which detail the precise work-flow. Usually, when a driver\ncommits a violation, a policeman opens a new fine management and leaves a ticket on the car glass. The actual fine amount depends on the violation performed. Within 180 days, the fine notification must be sent to the offender. The payment can occur in any moment, i.e. before or after that the fine notification is sent by post. If the offender does not pay within 60 days since the reception of the fine notification, the fine doubles. If the offender never pays, eventually the fine is sent to a special agency for credit collection. In a number of circumstances, the fine can be dismissed. For instance, the policeman could make a typo when the fine form is filled in (e.g., the license plate number does not match the type of car/truck/motorbike). More importantly, the offender may think to have come under injustice and, hence, can appeal to a judge and/or the local prefecture. If the appeal is in favour to the offender, the fine is dismissed. Otherwise, the management is carried on as if he/she had never appealed.\nWe extracted from the information systems an event log that refers to executions that end with sending for credit collection, i.e. the offenders have not paid the fine in full. These event logs refer to non-overlapping periods in time and contains about 7300 log traces. With this set of experiments we want to show that all the methods presented here perform well under the assumption that in the training set there are all the possible behaviours of the process (the same assumption is done by van der Aalst et al. in [9]). The experiments were performed using 5-fold cross validation. The SVR hyper-parameters have been tuned automatically using a grid search strategy. In particular we sought for the best combination of C, as penalty in the optimization problem, and \u03b3 for the RBF kernel. The transition system has been mined using different abstractions, namely set, multi-set and sequence with no limit. Since, in the event log, for 99% of traces, every activity was performed at most once, and the process is almost linear, the multi-set and the sequence abstraction was not considered to perform experiments.\nTable 2 reports the results of the experiments. The acronym used in the tables mean: VDA is the approach presented in [9], DATS is the data-aware transition system, SVR is the approach which use a simple support vector regression machine, and SVR+TS is the method which incorporates contextual information (via TS) in the training instances.\nResults show that every proposed approach outperform the baseline with similar results. However, it is worth to notice that the approaches based on the transition system, i.e., DATS and SVR+TS, achieve best performances on RMSPE and MAPE respectively. Readers can observe that VDA achieves better performance than in [16], this is due to the fact that the log used in these experiments is much bigger and hence the statistics of the methods have a stronger support.\nFrom these results, we can argue that the improvements with respect to the baseline are due to the introduction of the additional data in our models. In this log, we can also notice that the effect of considering the workflow (i.e., the transition system) is cramped in comparison to the data perspective."}, {"heading": "7.2. Case Study 2: Help Desk Log", "text": "The second case study log concerns the ticketing management process of the help desk of an Italian software company. In particular, this process consists of 14 activities: it starts with the insertion of a new ticket and then a seriousness level is applied. Then the ticket is managed by a resource and it is processed. When the problem is resolved it is closed and the process instance ends. This log has almost 4 500 instances with over 21 000 events. In this case the process is not linear, but it is well structured. As in the previous case study, the experiments were performed using 5-fold cross validation and the SVR\u2019s hyper-parameters have been tuned automatically using a grid search strategy. Since the process it is more complex, we performed two kinds of experiments:\n\u2022 we compared the performance of our approaches against the baseline assuming that in the training phase all the possible behaviours of the process are present at least once;\n\u2022 we removed some variants from the training set in order to have completely new activity sequences in the test set, so the assumption is not valid anymore.\nThe second type of experiments aim to show that the approaches based on single SVR performed well even with noisy instances or with new events.\nTables 3, 4 and 5 show the results using the entire log with 5-fold cross validation. We performed tests using all the three abstractions (i.e., set, multiset and sequence). With this dataset the usage of different set abstractions did not provide any remarkable difference. As for the previous case study, the best performing methods are DATS and SVR+TS, however the lowest MAPE and RMSPE are achieved by DATS using the set abstraction. Since the SVR approach had not good results (it is in line with VDA), the transition system information used by SVR+TS played a decisive role. The improvements with respect to the baseline are in average 14% for the MAPE and 4% for RMSPE.\nFrom a workflow point of view, this case study has a more complex structure and we can see how the workflow information, via the TS, had different\nimpact on the results. In this experiment the simple SVR failed in comparison with SVR+TS and the DATS methods, emphasizing the importance of the information brought by the TS.\nTable 6 shows the results with 5-fold cross validation on the log without some variants. In particular, column \u201cVariants\u201d shows results using training instances without half of the variants present in the starting event log. Column \u201cActivity\u201d instead, shows results removing from the training set all the traces with a specific activity, i.e., \u201cWaiting\u201d, which were present in almost 25% of the instances. Then the test phase uses the remaining part of the log, so inside the test there are completely new process behaviours which cause problems to VDA and DATS approaches.\nResults show that in both cases, SVR+TS outperforms all the other approaches with a MAPE around 34% and a RMSPE of 55%. The introduction of the similarity mechanism in SVR+TS makes this approach less sensible to the noise or change in the workflow, because it is able to mitigate the lack of the correct state using information from correlated ones.\nIt is worth highlighting, that both VDA and DATS would not return any prediction in case of unseen activity\u2019 sequences. However, in order to be able to\ncompare the approaches, we implemented a safety mechanism: if a trace does not map into a valid state of the transition system, the last event is removed and the mapping is redone. This process is repeated (see Alg. 7) until obtaining a prefix that can be mapped or, viceversa, is empty (and is discarded), and in this case the average remaining time of the entire training is returned. This explains why SVR has not better performance than VDA and DATS.\nThese experiments show the effectiveness of the SVR+TS approach and the importance of the similarity-based transition system.\nAlgorithm 7: Safety mechanism\nInput: \u03c3p: (partial) trace; TS = (S,E, T ): transition system Output: s: a valid state\n1 i\u2190 |\u03c3p| 2 s\u2190 f state(\u03c3ip) 3 while (s /\u2208 S) \u2227 (i > 0) do 4 i\u2190 i\u2212 1 5 s\u2190 f state(\u03c3ip) 6 end\n7 return s"}, {"heading": "7.2.1. Future Sequence of Activities Prediction", "text": "Using this event log we also test the future path prediction (FPP) method described in Section 5.3.3. A similar task is accomplished in [13] using Markov chain, however they predict the likelihood of executing a specific activity in the future regardless the sequence of steps taken to get there. For this reason we decide to evaluates our method against a random predictor which chooses randomly the next activity according to the possible continuation seen in the event log. If an activity is also a possible termination the method randomly decides whether to stop or not. As for the previous experiments, we used 5-fold cross validation. To evaluate the methods, which means assessing how much the predicted path respect the actual one, we used two metrics: the DamerauLevenshtein similarity (DAM) and the common prefix (PRE). Tables 7, 8 and 9 shows the results regarding the plots from Fig. 7 to Fig. 12.\nEach row in the tables represents the similarity considering sequences of exactly n activities where n is indicated in the # column. Rows with the E# symbol are the average similarities considering every sequence length. Readers can notice that with every abstraction the proposed method outperforms the random one. In particular, FPP it is able to identify the next activity almost 94% of the times and the similarity of the next two is almost 0.86 in average with an hit rate of 71% in average. We achieve good results, with respect to Damerau-Levenshtein similarity, even with 3, 4 and 5 activity sequences.\nThe trend of the plots look similar and we can notice how the difference in the prediction accuracy are evident starting on the left side (few activities in the future) and narrow on the right side. This fact is due to the stockpile of the uncertainty of each step in the future, which makes the prediction of the FPP closer to the random one.\nHowever, in average, our approach obtained for DAM and PRE 0.81 and 0.63 respectively, which are far better than the 0.36 and 0.09 obtained by the random method."}, {"heading": "8. Conclusions", "text": "In this work we presented some new methods that can be employed to tackle the problem of predicting the time-to-completion of running business process instances. The contributions we presented in this work can be summarized as the following:\n\u2022 we proposed three new prediction methods, which take advantage not only of the control-flow information but also of the additional data presented in the event log;\n\u2022 we leveraged on solid and well-studied machine learning techniques in order to build models able to manage the additional information;\n\u2022 we constructed our approach in order to deal with unexpected behaviours or noisy data, by looking at the closeness between the new trace and the most similar process flows already observed;\n\u2022 we have extensively evaluated our algorithms on real life data and show that our methods outperform state-of-the-art.\nThe proposed set of methods aims to face different scenarios and to overcome the limitations of the state-of-the-art approaches. Moreover, we distinguished the application of the prediction problem into two main scenarios:\n1. the process is stable and consequently the event log used to train the model contains all the possible process behaviours;\n2. the process is dynamic (i.e., might contain drifts) and, consequently, the event log used for training does not contain all the possible process behaviours, e.g., seasonability of the process.\nExperiments in [16] and those reported in Section 6 have shown how the DATS approach overcome the state-of-the-art in the first scenario. Assuming that the training event log contains all the possible process\u2019 behaviours, it is possible to take advantage from the static nature of the process and rely on the transition system structure. A central role in this method is played by the Na\u0308\u0131ve Bayes classifiers, which are also involved in the prediction of future sequence of activities in Section 5.3.3. These two tools together can be very useful for business managers because, in addition to the remaining time estimation, they have also some hints about the sequence of activities that the instance is going to take. Using these information business managers can act preventively and they can try to avoid uncomfortable situations. However, we also obtained good result with the single SVR-based methods which are well suited for the second scenario in which not all the process\u2019 behaviours are present in the training phase. Here, the SVR+TS method is not affected by the lack of information in the training set because is able to generalize the workflow thanks to the similarity-based transition system and the nature itself of the single-SVR approach. Experimental results point out that methods which are strongly dependent on the TS\nstructure have problem with new process\u2019 variants, while SVR method with the similarity-based TS, outperforms all the other approaches.\nAs future work, we plan to improve the parameters calibration of our approaches, in order to improve the overall results of our approach. We would like to investigate whether taking into account only work-hours in the prediction is valuable or not. Moreover, we would like to deploy our approach on real scenario, in order to stress the whole approach under production-level constraints."}, {"heading": "Acknowledgment", "text": "The work reported in this paper is supported by the Eurostars-Eureka project PROMPT (E!6696)."}], "references": [{"title": "Process Mining - Discovery, Conformance and Enhancement of Business Processes, 1st Edition", "author": ["W.M.P. van der Aalst"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Beyond Data Warehousing : What s Next in Business Intelligence ", "author": ["M. Golfarelli", "S. Rizzi", "I. Cella"], "venue": "in: 7th ACM international workshop on Data warehousing and OLAP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Case prediction in BPM systems: a research challenge", "author": ["H. Reijers"], "venue": "Journal of the Korean Institute of Industrial Engineers 33 (1) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Cycle Time Prediction: When Will This Case Finally Be Finished", "author": ["B.F. van Dongen", "R. Crooy", "W.M.P. van der Aalst"], "venue": "in: Proceedings of the 16th International Conference of Cooperative Information Systems, OTM 2008,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Predictions in Information Systems - A process mining perspective", "author": ["R. Crooy"], "venue": "Master thesis, Technische Universiteit Eindhoven ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "B", "author": ["H. Schonenberg", "B. Weber"], "venue": "F. van Dongen, W. M. P. van der Aalst, Supporting flexible processes through recommendations based on history, in: Proceedings of 6th International Conference BPM, Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Cycle time prediction in Staffware", "author": ["B. Schellekens"], "venue": "Master thesis, Technische Universiteit Eindhoven ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Time prediction based on process mining, Information Systems", "author": ["W.M.P. van der Aalst", "H. Schonenberg", "M. Song"], "venue": "doi:10. 1016/j.is.2010.09.001", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Runtime prediction of service level agreement violations for composite services", "author": ["P. Leitner", "B. Wetzstein", "F. Rosenberg", "A. Michlmayr", "S. Dustdar", "F. Leymann"], "venue": "in: International Workshops, ICSOC/ServiceWave, Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Discovering context-aware models for predicting business process performances", "author": ["F. Folino", "M. Guarascio", "L. Pontieri"], "venue": "in: Proceedings of On the Move to Meaningful Internet Systems Conference: OTM, Vol. 7565, Springer Berlin Heidelberg", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Discovering High-Level Performance Models for Ticket Resolution Processes", "author": ["F. Folino", "M. Guarascio", "L. Pontieri"], "venue": "in: Proceedings of On the Move to Meaningful Internet Systems Conference: OTM, Vol. 8185, Springer Berlin Heidelberg", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving business process decision making based on past experience", "author": ["J. Ghattas", "P. Soffer", "M. Peleg"], "venue": "Decision Support Systems 59 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A General Framework for Correlating Business Process Characteristics", "author": ["M.D. Leoni", "W.M.P.V.D. Aalst", "M. Dees"], "venue": "in: Business Process Management", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "M", "author": ["M. Polato", "A. Sperduti", "A. Burattin"], "venue": "de Leoni, Data-Aware Remaining Time Prediction of Business Process Instances, in: International Joint Conference on Neural Networks (WCCI)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Queueing methods for services and manufacturing", "author": ["R.W. Hall"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "H", "author": ["G. Bolch", "S. Greiner"], "venue": "de Meer, K. S. Trivedi, Queueing networks and Markov chains: modeling and performance evaluation with computer science applications, John Wiley & Sons", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Queue mining for delay prediction in multi-class service processes", "author": ["A. Senderovich", "M. Weidlich", "A. Gal", "A. Mandelbaum"], "venue": "Information Systems 53 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Prediction of business process durations using non-markovian stochastic petri nets", "author": ["A. Rogge-Solti", "M. Weske"], "venue": "Information Systems 54 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A note on the concept of multiset", "author": ["J. Hickman"], "venue": "Bulletin of the Australian Mathematical Society 22 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1980}, {"title": "Process mining: a two-step approach to balance between underfitting and overfitting, Software & Systems Modeling", "author": ["W.M.P. van der Aalst", "V. Rubin", "E. Verbeek", "B.F. van Dongen", "E. Kindler", "C.W. G\u00fcnther"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Machine Learning", "author": ["T.M. Mitchell"], "venue": "1st Edition, McGraw-Hill", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Support Vector Regression", "author": ["D. Basak", "S. Pal", "D.C. Patranabis"], "venue": "Neural Information Processing - Letters and Reviews 10 (10) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Support Vector Regression Machines", "author": ["H. Drucker", "C. Burges", "L. Kaufman", "A.J. Smola", "V. Vapnik"], "venue": "Neural Information Processing Systems 1 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "A Tutorial on Support Vector Regression", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "Statistics and Computing 14 (3) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition", "author": ["T.M. Cover"], "venue": "IEEE Transactions on Electronic Computers EC-14 (3) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1965}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "1st Edition, Cambrige University Press", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "A Technique for Computer Detection and Correction of Spelling Errors", "author": ["F. Damerau"], "venue": "Communications of the ACM (CACM) 7 (3) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1964}, {"title": "Process Mining Techniques in Business Environments", "author": ["A. Burattin"], "venue": "Springer International Publishing", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "B", "author": ["E. Verbeek", "J.C.A.M. Buijs"], "venue": "F. van Dongen, W. M. P. van der Aalst, ProM 6 : The Process Mining Toolkit, in: BPM 2010 Demos, Springer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "The WEKA data mining software", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD Explorations Newsletter 11 (1) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "off data mining: process mining [1, 2].", "startOffset": 32, "endOffset": 38}, {"referenceID": 2, "context": "One of the first work that analyzes the execution duration problem is described in [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": ", in [5, 6], describe a prediction model which uses all the data recorded in an event log.", "startOffset": 5, "endOffset": 11}, {"referenceID": 4, "context": ", in [5, 6], describe a prediction model which uses all the data recorded in an event log.", "startOffset": 5, "endOffset": 11}, {"referenceID": 5, "context": "in [7], is built using historical information, and is able to predict the most likely activity that a running case is going to perform.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "The TIBCO Staffware iProcess Suite [8] is one of the first commercial tools that predicts the cycle time of running process instances.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "Recently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14].", "startOffset": 114, "endOffset": 129}, {"referenceID": 10, "context": "Recently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14].", "startOffset": 114, "endOffset": 129}, {"referenceID": 16, "context": "Recently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14].", "startOffset": 114, "endOffset": 129}, {"referenceID": 13, "context": "Recently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14].", "startOffset": 114, "endOffset": 129}, {"referenceID": 17, "context": "Recently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 14, "context": "Recently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14].", "startOffset": 181, "endOffset": 189}, {"referenceID": 15, "context": "Recently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14].", "startOffset": 181, "endOffset": 189}, {"referenceID": 12, "context": "Recently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14].", "startOffset": 225, "endOffset": 233}, {"referenceID": 11, "context": "Recently, more sophisticated methods have been proposed, to mention some of these: transition system-based models [9, 12, 19, 16], probabilistic models [13, 20], queue theory based [17, 18] and decision tree based approaches [15, 14].", "startOffset": 225, "endOffset": 233}, {"referenceID": 13, "context": "With respect to our seminal work [16], here we propose an improved version of that method and we also propose two novel approaches able to overcome its limitations.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "In particular, we are going to define two different scenarios: in the first one we assume the process has a well defined static workflow, the same assumption made in [16], while in the latter we remove such assumption and the process is considered dynamic, e.", "startOffset": 166, "endOffset": 170}, {"referenceID": 7, "context": ", in [9].", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "The work presented in [10] considers the data perspective in order to identify SLAs (Service Level Agreement) violations.", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "[11, 12] report an extended version of the technique described in [9].", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[11, 12] report an extended version of the technique described in [9].", "startOffset": 0, "endOffset": 8}, {"referenceID": 7, "context": "[11, 12] report an extended version of the technique described in [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "In particular, they cluster the log traces according to the corresponding \u201ccontext features\u201d and then, for each cluster, they create a predictive model using the method described in [9].", "startOffset": 182, "endOffset": 185}, {"referenceID": 7, "context": "One of the weaknesses of these methods based on [9] is that they assume a static process, where the event log used for the training phase contains all the possible process behaviours.", "startOffset": 48, "endOffset": 51}, {"referenceID": 11, "context": ", in a recent work [14], exploit Generic Process Model and decision trees, based on the process context, to provide decision criteria defined according to the actual process goals.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "In [15], de Leoni et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Finally, in [16], Polato et al.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "show an approach based on [9] in which the additional attributes of the events are taken into account in order to refine the prediction quality.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "This method exploits the idea of annotating a transition system (presented in [9]) adding machine learning models, such as N\u00e4\u0131ve Bayes and Support Vector Regressor.", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "In this paper we propose two new approaches based on Support Vector Regression and we discuss their strengths and their weaknesses comparing with the approaches presented in [9].", "startOffset": 174, "endOffset": 177}, {"referenceID": 14, "context": "For example, queue theory [17, 18] and queue mining can be seen as a very specific type of process mining, and recent works are starting to aim for similar prediction purposes [19].", "startOffset": 26, "endOffset": 34}, {"referenceID": 15, "context": "For example, queue theory [17, 18] and queue mining can be seen as a very specific type of process mining, and recent works are starting to aim for similar prediction purposes [19].", "startOffset": 26, "endOffset": 34}, {"referenceID": 16, "context": "For example, queue theory [17, 18] and queue mining can be seen as a very specific type of process mining, and recent works are starting to aim for similar prediction purposes [19].", "startOffset": 176, "endOffset": 180}, {"referenceID": 17, "context": "Another example of prediction-focused paper has recently been published by Rogge-Solti and Weske [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "Preliminaries A multiset M (also known as bag or m-set) [21] is a generalization of set in which the elements may occurs multiple times, but these are not treated as repeated elements.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Let us now define the concept of event log as in [9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "[9], to construct a transition system which maps each partial trace in the log to a state, we need the so called state and event representation functions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Choosing the right functions f state and f, also referred to as abstractions, is not a trivial task [22, 9].", "startOffset": 100, "endOffset": 107}, {"referenceID": 7, "context": "Choosing the right functions f state and f, also referred to as abstractions, is not a trivial task [22, 9].", "startOffset": 100, "endOffset": 107}, {"referenceID": 19, "context": "Some possible good choices for f state and f are described and discussed in [22] and [9].", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "Some possible good choices for f state and f are described and discussed in [22] and [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 20, "context": "N\u00e4\u0131ve Bayes (NB) [23] is a probabilistic classifier which is based on the application of Bayes\u2019 theorem.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "This undesirable situation can be avoid applying the Laplacian (or additive) smoothing [23] to the conditional probabilities P (xk | y).", "startOffset": 87, "endOffset": 91}, {"referenceID": 21, "context": "One of the most recently proposed approaches is the Support Vector Regression (SVR) [24, 25, 26], which is, as the name suggests, based on Support Vector Machines (SVM).", "startOffset": 84, "endOffset": 96}, {"referenceID": 22, "context": "One of the most recently proposed approaches is the Support Vector Regression (SVR) [24, 25, 26], which is, as the name suggests, based on Support Vector Machines (SVM).", "startOffset": 84, "endOffset": 96}, {"referenceID": 23, "context": "One of the most recently proposed approaches is the Support Vector Regression (SVR) [24, 25, 26], which is, as the name suggests, based on Support Vector Machines (SVM).", "startOffset": 84, "endOffset": 96}, {"referenceID": 22, "context": "In -SVR [25], the goal is to find a function f(~x) that deviates from the target yi by at most , for all the training instances.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "convex optimization problem [25, 24, 26] provides the key for extending SVR to nonlinear functions:", "startOffset": 28, "endOffset": 40}, {"referenceID": 21, "context": "convex optimization problem [25, 24, 26] provides the key for extending SVR to nonlinear functions:", "startOffset": 28, "endOffset": 40}, {"referenceID": 23, "context": "convex optimization problem [25, 24, 26] provides the key for extending SVR to nonlinear functions:", "startOffset": 28, "endOffset": 40}, {"referenceID": 23, "context": "Exploiting the Karush-Kuhn-Tucker (KKT) conditions [26], which states that at the optimum the product between constraints and dual variables is zero, we can compute b and we can also notice that only for |f(~xi \u2212 yi)| = the coefficients \u03b1i, \u03b1 \u2217 i are non-zero.", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "Cover\u2019s Theorem [27] proves that given a set of training data that is not linearly separable in the input space, it is possible to transform the data into a training set that is linearly separable, with high probability, by projecting it into a higher-dimensional space (feature space) via some non-linear transformation.", "startOffset": 16, "endOffset": 20}, {"referenceID": 23, "context": "Exploiting the Mercer\u2019s Theorem [26] it is possible to characterize these type of functions k, called kernel functions.", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": ", let Di \u2261 N \u2282 R and \u03c0D(e) = 17, the output vector is ~u = [17] \u2208 R.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": ", recalling v \u2208 R and u \u2208 R from the previous examples, their concatenation is equal to ~z = ~v || ~u = [0, 1, 0, 0]||[17] = [0, 1, 0, 0, 17] \u2208 R.", "startOffset": 104, "endOffset": 116}, {"referenceID": 14, "context": ", recalling v \u2208 R and u \u2208 R from the previous examples, their concatenation is equal to ~z = ~v || ~u = [0, 1, 0, 0]||[17] = [0, 1, 0, 0, 17] \u2208 R.", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": ", recalling v \u2208 R and u \u2208 R from the previous examples, their concatenation is equal to ~z = ~v || ~u = [0, 1, 0, 0]||[17] = [0, 1, 0, 0, 17] \u2208 R.", "startOffset": 125, "endOffset": 141}, {"referenceID": 14, "context": ", recalling v \u2208 R and u \u2208 R from the previous examples, their concatenation is equal to ~z = ~v || ~u = [0, 1, 0, 0]||[17] = [0, 1, 0, 0, 17] \u2208 R.", "startOffset": 125, "endOffset": 141}, {"referenceID": 0, "context": "For example, given the states set S \\ S = {s1, s2, s3, s4} we encode the state s3 onto ~v \u2208 {0, 1} such that ~v = [0, 0, 1, 0].", "startOffset": 114, "endOffset": 126}, {"referenceID": 0, "context": "Given two sets x1, x2 \u2286 X , with X the set of all possible values, we define the similarity function f sim set \u2208 2X \u00d7 2X \u2192 [0, 1] as the Jaccard similarity [28].", "startOffset": 123, "endOffset": 129}, {"referenceID": 25, "context": "Given two sets x1, x2 \u2286 X , with X the set of all possible values, we define the similarity function f sim set \u2208 2X \u00d7 2X \u2192 [0, 1] as the Jaccard similarity [28].", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": "Given two multi-sets over a root set X , x1, x2 \u2208 B(X ), we define the similarity function f sim bag \u2208 B(X )\u00d7 B(X )\u2192 [0, 1] as the Jaccard similarity [28].", "startOffset": 117, "endOffset": 123}, {"referenceID": 25, "context": "Given two multi-sets over a root set X , x1, x2 \u2208 B(X ), we define the similarity function f sim bag \u2208 B(X )\u00d7 B(X )\u2192 [0, 1] as the Jaccard similarity [28].", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "Given two finite sequences over X , x1, x2 \u2208 S(X ), we define the similarity function f sim list \u2208 S(X ) \u00d7 S(X ) \u2192 [0, 1] \u2282 R as the Damerau-Levenhstein similarity [29].", "startOffset": 115, "endOffset": 121}, {"referenceID": 26, "context": "Given two finite sequences over X , x1, x2 \u2208 S(X ), we define the similarity function f sim list \u2208 S(X ) \u00d7 S(X ) \u2192 [0, 1] \u2282 R as the Damerau-Levenhstein similarity [29].", "startOffset": 164, "endOffset": 168}, {"referenceID": 13, "context": "Approach 3: Data-aware Transition System (DATS) The approach presented in this section is a refinement of [16] which exploits the same idea described in [9].", "startOffset": 106, "endOffset": 110}, {"referenceID": 7, "context": "Approach 3: Data-aware Transition System (DATS) The approach presented in this section is a refinement of [16] which exploits the same idea described in [9].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Formally, in [9] a measurement is defined as:", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "In [9] different kinds of measurements are proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "As in [9], we start with the transition system construction and then we enrich each state with a N\u00e4\u0131ve Bayes classifier (see Section 4.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "A N\u00e4\u0131ve Bayes Annotation is a function NB : S \u00d7 R \u00d7 S \u2192 [0, 1] \u2282 R, which, given two states si, sj \u2208 S and a data attribute vector ~x \u2208 R, returns the probability to reach the state sj starting from si through a single transition.", "startOffset": 56, "endOffset": 62}, {"referenceID": 13, "context": "[16] is the absence of the expected sojourn time (on the current state): in this revised version this information is implicitly embedded inside the", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "This property allows the application of the approach in on-line settings [30], where each event is allowed to trigger only a constant number of operations.", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "These techniques have been implemented for the ProM framework [31].", "startOffset": 62, "endOffset": 66}, {"referenceID": 29, "context": "N\u00e4\u0131ve Bayes Classification and the Support Vector Regression are performed using the implementation in the Weka framework [32].", "startOffset": 122, "endOffset": 126}, {"referenceID": 7, "context": "[9] that has been discussed in Section 2.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "We also aimed to compare our approach versus the techniques proposed in [11, 12].", "startOffset": 72, "endOffset": 80}, {"referenceID": 10, "context": "We also aimed to compare our approach versus the techniques proposed in [11, 12].", "startOffset": 72, "endOffset": 80}, {"referenceID": 13, "context": "Work [16] shows that no much difference can be appreciated", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "in [9]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "The acronym used in the tables mean: VDA is the approach presented in [9], DATS is the data-aware transition system, SVR is the approach which use a simple support vector regression machine, and SVR+TS is the method which incorporates contextual information (via TS) in the training instances.", "startOffset": 70, "endOffset": 73}, {"referenceID": 13, "context": "Readers can observe that VDA achieves better performance than in [16], this is due to the fact that the log used in these experiments is much bigger and hence the statistics of the methods have a stronger support.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Experiments in [16] and those reported in Section 6 have shown how the DATS approach overcome the state-of-the-art in the first scenario.", "startOffset": 15, "endOffset": 19}], "year": 2016, "abstractText": "The ability to know in advance the trend of running process instances, with respect to different features, such as the expected completion time, would allow business managers to timely counteract to undesired situations, in order to prevent losses. Therefore, the ability to accurately predict future features of running business process instances would be a very helpful aid when managing processes, especially under service level agreement constraints. However, making such accurate forecasts is not easy: many factors may influence the predicted features. Many approaches have been proposed to cope with this problem but all of them assume that the underling process is stationary. However, in real cases this assumption is not always true. In this work we present new methods for predicting the remaining time of running cases. In particular we propose a method, assuming process stationarity, which outperforms the state-of-the-art and two other methods which are able to make predictions even with nonstationary processes. We also describe an approach able to predict the full sequence of activities that a running case is going to take. All these methods are extensively evaluated on two real case studies.", "creator": "LaTeX with hyperref package"}}}