{"id": "1602.07868", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks", "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.", "histories": [["v1", "Thu, 25 Feb 2016 10:13:45 GMT  (91kb,D)", "http://arxiv.org/abs/1602.07868v1", null], ["v2", "Fri, 26 Feb 2016 08:53:23 GMT  (91kb,D)", "http://arxiv.org/abs/1602.07868v2", null], ["v3", "Sat, 4 Jun 2016 01:21:52 GMT  (61kb,D)", "http://arxiv.org/abs/1602.07868v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["tim salimans", "diederik p kingma"], "accepted": true, "id": "1602.07868"}, "pdf": {"name": "1602.07868.pdf", "metadata": {"source": "META", "title": "Weight Normalization: A Simple Reparameterization  to Accelerate Training of Deep Neural Networks", "authors": ["Tim Salimans", "Diederik P. Kingma"], "emails": ["SALIMANSTIM@GMAIL.COM", "D.P.KINGMA@UVA.NL"], "sections": [{"heading": "1. Introduction", "text": "Deep learning (Goodfellow et al., 2016) is a subfield of machine learning that consists in learning models that are wholly or partially specified by a class of flexible differentiable functions called neural networks. Learning usually entails the minimization or maximization of a scalar objective function, given the model and data, w.r.t. its parameters. In the case of deep neural networks, the set of\nparameters is composed of a large number of weights and biases. Optimization is typically performed by computing first-order gradients of the parameters w.r.t. the objective, and feeding them into a gradient-based optimization algorithm, iteratively performing small parameter updates from some initial point to a local optimum of the objective.\nIt is well known that the practical success of first-order gradient based optimization is highly dependent on the curvature of the objective that is optimized. If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature, and first-order gradient descent will have trouble making progress (Martens, 2010; Sutskever et al., 2013). The amount of curvature, and thus the success of our optimization, is not invariant to reparameterization (Amari, 1997): There may be multiple equivalent ways of parameterizing the same model, some of which are much easier to optimize than others. Finding good ways of parameterizing neural networks is thus an important problem in deep learning.\nWhile the architectures of neural networks differ widely across applications, they are typically mostly composed of conceptually simple computational building blocks sometimes called neurons: each such neuron computes a weighted sum over its real-valued inputs and adds a bias term, followed by the application of an elementwise nonlinear transformation. The weights and biases can be shared across neurons, and are (part of) the parameters of the model to be optimized over. Improving the general optimizability of deep networks is a challenging task (Glorot & Bengio, 2010), but since many neural architectures share these basic building blocks, improvements to the building blocks can improve the performance of a very wide range of model architectures.\nIn this paper, we propose a simple but general method, called weight normalization, for improving the optimizability of the weights of neural network models. The\nar X\niv :1\n60 2.\n07 86\n8v 1\n[ cs\n.L G\n] 2\n5 Fe\nb 20\nmethod is inspired by batch normalization (Ioffe & Szegedy, 2015), another recently proposed reparameterization, but it is a deterministic method, and does not share batch normalization\u2019s property of increasing the stochasticity of the gradients.\nWe present weight normalization, our main contribution, in section 2. We then propose a data-dependent initialization strategy for the model parameters in section 3. In section 4 we discuss a hybrid between weight normalization and batch normalization, where we normalize the minibatch mean but not the variance, a method we call meanonly batch normalization. We empirically demonstrate the usefulness of our methods in section 5, with experiments in supervised image recognition, generative modelling, and deep reinforcement learning. In section 6 we discuss two possible extensions to our method that did not make it into our final experiments, and we discuss some of the properties of weight normalization. Finally, section 7 concludes."}, {"heading": "2. Weight Normalization", "text": "We consider standard artificial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity:\ny = \u03c6(w \u00b7 x + b), (1)\nwhere w is a k-dimensional weight vector, b is a scalar bias term, x is a k-dimensional vector of input features, \u03c6(.) denotes an elementwise nonlinearity such as the rectifier max(., 0), and y denotes the scalar output of the neuron.\nAfter associating a loss function to one or more neuron outputs, such a neural network is commonly trained by stochastic gradient descent in the parameters w, b of each neuron. In an effort to speed up the convergence of this optimization procedure, we propose to reparameterize each weight vector w in terms of a parameter vector v and a scalar parameter g and to perform stochastic gradient descent with respect to those parameters instead. We do so by expressing the weight vectors in terms of the new parameters using\nw = g\n||v|| v (2)\nwhere v is a k-dimensional vector, g is a scalar, and ||v|| denotes the Euclidean norm of v. This reparameterization has the effect of normalizing the norm of the weight vector: we now have ||w|| = g, independent of the parameters v. We therefore call this reparameterizaton weight normalization. By decoupling the norm of the weight vector (g) from the direction of the weight vector (v/||v||), we speed up convergence of our stochastic gradient descent optimization, as we show experimentally in section 5."}, {"heading": "2.1. Gradients", "text": "Training a neural network in the new parameterization is done using standard stochastic gradient descent methods. Here we differentiate through (2) to obtain the gradient of a loss function L with respect to the new parameters v, g. Doing so gives\n\u2207gL = \u2207wL \u00b7 v ||v||\n\u2207vL = g\n||v|| \u2207wL\u2212 g\u2207gL ||v||2 v, (3)\nwhere \u2207wL is the gradient with respect to the weights w as used normally. Backpropagation using weight normalization thus only requires a minor modification on top of standard neural network software. The expressions above are independent of the minibatch size and cause only minimal computational overhead."}, {"heading": "2.2. Relation to batch normalization", "text": "An important source of inspiration for this reparameterization is batch normalization (Ioffe & Szegedy, 2015), which is equivalent to our method in the special case where our network only has a single layer, and the input features for that layer are whitened (independently distributed with zero mean and unit variance). Although exact equivalence does not usually hold for deeper architectures, we still find that our weight normalization method provides much of the speed-up of full batch normalization, while enjoying several advantages: The computational overhead of weight normalization is much lower, and it does not add stochasticity through the noisy estimates of minibatch statistics. This means that our method can also be applied to models like RNNs and LSTMs, as well as applications like Reinforcement Learning, for which batch normalization is less well suited."}, {"heading": "3. Data-Dependent Initialization of Parameters", "text": "Besides a reparameterization effect, batch normalization also has the benefit of fixing the scale of the features generated by each layer of the neural network. This makes the optimization robust against parameter initializations for which these scales vary across layers. Since weight normalization lacks this property, we find it is important to properly initialize our parameters. We propose to sample the elements of v from a simple distribution with a fixed scale, which is in our experiments a normal distribution with mean zero and standard deviation 0.05. Before starting training, we then initialize the b and g parameters by performing an initial feedforward pass through our network with a minibatch of data, using the following computation\nat each neuron:\nt = v \u00b7 x ||v|| ,\ny = \u03c6 ( t\u2212 \u00b5[t] \u03c3[t] ) , (4)\nwhere \u00b5[t] and \u03c3[t] are the mean and standard deviation of the pre-activation t over the examples in the minibatch. We can then initialize bias b and scale g as\ng \u2190 1 \u03c3[t]\nb \u2190 \u2212\u00b5[t] \u03c3[t] , (5)\nso that y = \u03c6(w \u00b7 x + b). Like batch normalization, this method ensures that all features initially have zero mean and unit variance before application of the nonlinearity. With our method this only holds for the minibatch we use for initialization, and subsequent minibatches may have slightly different statistics, but experimentally we find this initialization method to work well. The method can also be applied to networks without weight normalization, simply by doing stochastic gradient optimization on the parameters w directly, after initialization in terms of v and g: this is what we compare to in section 5. Independently from our work, this type of initialization was recently proposed by different authors (Mishkin & Matas, 2015; Kra\u0308henbu\u0308hl et al., 2015) who found such data-based initialization to work well for use with the normal parameterization.\nThe downside of this initialization method is that it can only be applied in similar cases as where batch normalization is applicable. For models with recursion, such as RNNs and LSTMs, we will have to resort to standard initialization methods."}, {"heading": "4. Mean-only Batch Normalization", "text": "Weight normalization, as introduced in section 2, makes the scale of neuron activations approximately independent of the parameters v. Unlike with batch normalization, however, the means of the neuron activations still depend on v. We therefore also explore the idea of combining weight normalization with a special version of batch normalization, which we call mean-only batch normalization: With this normalization method, we subtract out the minibatch means like with full batch normalization, but we do not divide by the minibatch standard deviations. That is, we compute neuron activations using\nt = w \u00b7 x, t\u0303 = t\u2212 \u00b5[t] + b, y = \u03c6(t\u0303) (6)\nwhere w is the weight vector, parameterized using weight normalization, and \u00b5[t] is the minibatch mean of the preactivation t. During training, we keep a running average of the minibatch mean which we substitute in for \u00b5[t] at test time.\nThe gradient of the loss with respect to the pre-activation t is calculated as\n\u2207tL = \u2207t\u0303L\u2212 \u00b5[\u2207t\u0303L], (7)\nwhere \u00b5[.] denotes once again the operation of taking the minibatch mean. Mean-only batch normalization thus has the effect of centering the gradients that are backpropagated. This is a comparatively cheap operation, and the computational overhead of mean-only batch normalization is thus lower than for full batch normalization. In addition, this method causes less noise during training, and the noise that is caused is more gentle as the law of large numbers ensures that \u00b5[t] and \u00b5[\u2207t\u0303] are approximately normally distributed. Thus, the added noise has much lighter tails than the highly kurtotic noise caused by the minibatch estimate of the variance used in full batch normalization. As we show in section 5.1, this leads to improved accuracy at test time."}, {"heading": "5. Experiments", "text": "We experimentally validate the usefulness of our method using four different models for varied applications in supervised image recognition, generative modelling, and deep reinforcement learning."}, {"heading": "5.1. Supervised Classification: CIFAR-10", "text": "To test our reparameterization method for the application of supervised classification, we consider the CIFAR-10 data set of natural images (Krizhevsky & Hinton, 2009). The model we are using is based on the ConvPool-CNN-C architecture of (Springenberg et al., 2015), with some small modifications: we replace the first dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 2\u00d7 2 max-pooling, rather than 3\u00d73. The only hyperparameter that we actively optimized (the standard deviation of the Gaussian noise) was chosen to maximize the performance of the network on a holdout set of 10000 examples, using the standard parameterization (no weight normalization or batch normalization). A full description of the resulting architecture is given in table 5.1.\nWe train our network for CIFAR-10 using Adam (Kingma & Ba, 2014) for 200 epochs, with a fixed learning rate and momentum of 0.9 for the first 100 epochs. For the last 100 epochs we set the momentum to 0.5 and linearly decay the learning rate to zero. We use a minibatch size of 100.\nWe evaluate 4 different parameterizations of the network in table 5.1: 1) the standard parameterization, 2) using batch normalization, 3) using weight normalization, 4) using weight normalization combined with mean-only batch normalization. The network parameters are initialized using the scheme of section 3 such that all four cases have identical parameters starting out. For each case we pick the optimal learning rate in {0.0003, 0.001, 0.003, 0.01}. The resulting error curves during training can be found in figure 1: both weight normalization and batch normalization provide a significant speed-up over the standard parameterization. Batch normalization makes slightly more progress per epoch than weight normalization early on, although this is partly offset by the higher computational cost: with our implementation, training with batch normalization was about 16% slower compared to the standard parameterization. In contrast, weight normalization was not noticeably slower. During the later stage of training, weight normalization, batch normalization, and mean-only batch normalization all seem to optimize at about the same speed, with the normal parameterization still lagging behind.\nAfter optimizing the network for 200 epochs using the different parameterizations, we evaluate their performance on the CIFAR-10 test set. The results are summarized in table 5.1: weight normalization and the normal parameterization have almost identical test accuracy (\u2248 8.45% error). Batch normalization does significantly better at 8.05% error: estimating the minibatch statistics adds noise during the training process, which has a beneficial regulariz-\ning effect. Mean-only batch normalization (combined with weight normalization) has the best performance at 7.31% test error. We hypothesize that the substantial improvement over regular batch normalization is due to the distribution of the noise caused by the normalization method during training: for mean-only batch normalization the minibatch mean has a distribution that is approximately Gaussian, while the noise added by full batch normalization during training has much higher kurtosis. As far as we are aware, the result with mean-only batch normalization represents the state-of-the-art for CIFAR-10 among methods that do not use data augmentation."}, {"heading": "5.2. Generative Modelling: Convolutional VAE", "text": "Next, we test the effect of weight normalization applied to deep convolutional variational auto-encoders (CVAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Salimans et al., 2015), trained on the MNIST dataset of images of handwritten digits and the CIFAR-10 dataset of small natural images.\nVariational auto-encoders are generative models that explain the data vector x as arising from a set of latent variables z, through a joint distribution of the form p(z,x) = p(z)p(x|z), where the decoder p(x|z) is specified using a neural network. A lower bound on the log marginal likelihood log p(x) can be obtained by approximately inferring the latent variables z from the observed data x using an encoder distribution q(z|x) that is also specified as a neu-\nral network. This lower bound is then optimized to fit the model to the data.\nWe follow a similar implementation of the CVAE as in (Salimans et al., 2015) with some modifications, mainly that the encoder and decoder are parameterized with ResNet (He et al., 2015) blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1. For MNIST, the encoder consists of 3 sequences of two ResNet blocks each, the first sequence acting on 16 feature maps, the others on 32 feature maps. The first two sequences are followed by a 2-times subsampling operation implemented using 2\u00d7 2 stride, while the third sequence is followed by a fully connected layer with 450 units. The decoder has a similar architecture, but with reversed direction. For CIFAR-10, we used a neural architecture with ResNet units and multiple intermediate stochastic layers1. We used Adamax (Kingma & Ba, 2014) with \u03b1 = 0.002 for optimization, in combination with Polyak averaging (Polyak & Juditsky, 1992) in the form of an exponential moving average that averages parameters over approximately 10 epochs.\nIn figure 2, we plot the test-set lower bound as a function of number of training epochs, including error bars based on multiple different random seeds for initializing parameters. As can be seen, the parameterization with weight normalization has lower variance and converges to a better optimum. We observe similar results across different hyper-parameter settings."}, {"heading": "5.3. Generative Modelling: DRAW", "text": "Next, we consider DRAW, a recurrent generative model by (Gregor et al., 2015). DRAW is a variational auto-encoder with generative model p(z)p(x|z) and encoder q(z|x), similar to the model in section 5.2, but with both the encoder and decoder consisting of a recurrent neural network comprised of Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) units. LSTM units consist of a memory cell with additive dynamics, combined with input, forget, and output gates that determine which information flows in and out of the memory. The additive dynamics enables learning of long-range dependencies in the data.\nAt each time step of the model, DRAW uses the same set of weight vectors to update the cell states of the LSTM units in its encoder and decoder. Because of the recurrent nature of this process it is not clear how batch normalization could be applied to this model: Normalizing the cell states diminishes their ability to pass through information. Fortunately, weight normalization can be applied trivially to the weight vectors of each LSTM unit, and we find this to work well empirically.\n1Manuscript in preparation\nWe take the Theano implementation of DRAW provided at https://github.com/jbornschein/draw and use it to model the MNIST data set of handwritten digits. We then make a single modification to the model: we apply weight normalization to all weight vectors. As can be seen in figure 3, this significantly speeds up convergence of the optimization procedure, even without modifying the initialization method and learning rate that were tuned for use with the normal parameterization."}, {"heading": "5.4. Reinforcement Learning: DQN", "text": "Next we apply weight normalization to the problem of Reinforcement Learning for playing games on the Atari Learning Environment (Bellemare et al., 2013). The approach we use is the Deep Q-Network (DQN) proposed by (Mnih et al., 2015). This is an application for which batch normalization is not well suited: the noise introduced by estimating the minibatch statistics destabilizes the learning process. We were not able to get batch normalization to work for DQN without using an impractically large minibatch size. In contrast, weight normalization is easy to apply in this context, as is the initialization method of section 3. Stochastic gradient learning is performed using Adamax (Kingma & Ba, 2014) with momentum of 0.5. We search for optimal learning rates in {0.0001, 0.0003, 0.001, 0.003}, generally finding 0.0003 to work well with weight normalization and 0.0001 to work well for the normal parameterization. We also use a larger minibatch size (64) which we found to be more efficient on our hardware (Amazon Elastic Compute Cloud g2.2xlarge GPU instance). Apart from these changes\nwe follow (Mnih et al., 2015) as closely as possible in terms of parameter settings and evaluation methods. However, we use a Python/Theano/Lasagne reimplementation of their work, adapted from the implementation available at https://github.com/spragunr/deep_q_rl, so there may be small additional differences in implementation.\nFigure 4 shows the training curves obtained using DQN with the standard parameterization and with weight normalization on Space Invaders. Using weight normalization the algorithm progresses more quickly and reaches a better final result. Table 5.4 shows the final evaluation scores obtained by DQN with weight normalization for four games: on average weight normalization improves the performance of DQN."}, {"heading": "6. Extensions", "text": "We investigated multiple extensions to the weight normalization method as presented above. Here we discuss two extensions that we did not end up using in our final experiments, but that may be interesting to investigate further in future work."}, {"heading": "6.1. Exponential parameterization of the scale parameter g", "text": "An alternative parameterization of the weight-scale is g = ecs, where s is a log-scale parameter to learn by stochastic gradient descent, and c is a constant that throttles the effective learning rate of s. Parameterizing the g parameter in the log-scale is more natural, and more easily allows it to span a wide range of different magnitudes. We performed various experiments with this parameterization with various values of c. We found that a value of c between 1 and 5 worked well, where a value of 3 seemed about optimal for our experiments. However, the eventual test-set performance was not significantly better or worse than the results with directly learning g in its original parameterization, so we did not use the log-scale parameterization in our final experiments."}, {"heading": "6.2. Fixing the norm of v", "text": "When learning a neural network with weight normalization using standard gradient descent, the norm of v grows monotonically with the number of weight updates, which decreases the effective learning rate on the weights in our network. This effect arises because the unnormalized weight vector v only enters our model through its normalized version v/||v||, which ensures that the gradient \u2207vL of any loss function L is necessarily orthogonal to v. That is, \u2207vL \u00b7 v = 0, which can be verified by plugging in the expression for the gradient given in (3). Suppose now that an optimizer updates the weight vector v to v\u2032 = v + \u2206v, where \u2206v \u221d \u2207vL (steepest ascent/descent), and that the relative norm of the update w.r.t. the old weight vector is ||\u2206v||/||v|| = c. The new weight vector then has norm ||v\u2032|| = \u221a ||v||2 + c2||v||2, as dictated by the Pythagorean theorem. Therefore, the norm of the unnormalized weights is increased by the factor ||v\u2032||/||v|| = \u221a ||v||2 + c2||v||2/||v|| = \u221a 1 + c2 > 1.\nThis property of monotonically increasing norm ||v|| does not strictly hold for optimizers that use separate learning rates for individual parameters, like Adam (Kingma & Ba, 2014) which we use in experiments, where ||v|| can either grow or shrink during optimization; still, we found that the norm would generally grow during optimization, resulting in decreasing effective learning rate. To eliminate this property, we experimented with imposing a hard\nconstraint in the form of ||v|| \u2264 a, for some constant a, to prevent the unnormalized weights from growing overly large. However, for a reasonable initialization of v (e.g. as in section 3), we did not find that imposing this upper bound resulted in noticeable improvements of convergence speed.\nInstead, we found that the ability to grow the norm ||v|| makes optimization of neural networks with weight normalization very robust to the value of the learning rate: If the learning rate is too large, the norm of the unnormalized weights grows quickly until an appropriate effective learning rate is reached. Once the norm of the weights has grown large with respect to the norm of the updates, the effective learning rate stabilizes as \u221a 1 + ||\u2206v||2/||v||2 \u2248 1 for ||\u2206v|| ||v||. Experimentally, we found that neural networks with weight normalization therefore work well with a much wider range of learning rates than when using the normal parameterization. It has been observed that neural networks with batch normalization also have this property (Ioffe & Szegedy, 2015), which can be explained with a similar analysis.\nAlthough bounding the norm of the unnormalized weights was not found to speed up convergence, it can be used to obtain a regularizing effect during training: For a small choice of upper bound, e.g. ||v|| \u2264 0.25, the parameter updates remain relatively large, which adds noise due to the random sampling of training examples. This can lead to good test-set performance when the parameters are averaged over many epochs (i.e. Polyak averaging, see (Polyak & Juditsky, 1992)). For variational auto-encoders (section 5.2) we found that this effect was often beneficial, but it was not helpful with our experiments on supervised classification and reinforcement learning. For consistency, all results reported in this paper are therefore without any upper bound on ||v||."}, {"heading": "7. Conclusion", "text": "We have presented weight normalization, a simple reparameterization of the weight vectors in a neural network that accelerates the convergence of stochastic gradient descent optimization. Weight normalization was applied to four different models in supervised image recognition, generative modelling, and deep reinforcement learning, showing a consistent advantage across applications. The reparameterization method is easy to apply, has low computational overhead, and does not introduce dependencies between the examples in a minibatch, making it our default choice in the development of new deep learning architectures."}], "references": [{"title": "Neural learning in structured parameter spaces natural Riemannian gradient", "author": ["S. Amari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Amari,? \\Q1997\\E", "shortCiteRegEx": "Amari", "year": 1997}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Deep learning. Book in preparation for", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "Proceedings of the 2nd International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Data-dependent initializations of convolutional neural networks", "author": ["Kr\u00e4henb\u00fchl", "Philipp", "Doersch", "Carl", "Donahue", "Jeff", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1511.06856,", "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Network in network", "author": ["Lin", "Min", "Qiang", "Chen", "Yan", "Shuicheng"], "venue": "In ICLR: Conference Track,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "All you need is a good init", "author": ["Mishkin", "Dmytro", "Matas", "Jiri"], "venue": "arXiv preprint arXiv:1511.06422,", "citeRegEx": "Mishkin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mishkin et al\\.", "year": 2015}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Polyak", "Boris T", "Juditsky", "Anatoli B"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Polyak et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Polyak et al\\.", "year": 1992}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Markov chain Monte Carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICML,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": "In ICLR Workshop Track,", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Deep learning (Goodfellow et al., 2016) is a subfield of machine learning that consists in learning models that are wholly or partially specified by a class of flexible differentiable functions called neural networks.", "startOffset": 14, "endOffset": 39}, {"referenceID": 20, "context": "If the condition number of the Hessian matrix of the objective at the optimum is low, the problem is said to exhibit pathological curvature, and first-order gradient descent will have trouble making progress (Martens, 2010; Sutskever et al., 2013).", "startOffset": 208, "endOffset": 247}, {"referenceID": 0, "context": "The amount of curvature, and thus the success of our optimization, is not invariant to reparameterization (Amari, 1997): There may be multiple equivalent ways of parameterizing the same model, some of which are much easier to optimize than others.", "startOffset": 106, "endOffset": 119}, {"referenceID": 11, "context": "Independently from our work, this type of initialization was recently proposed by different authors (Mishkin & Matas, 2015; Kr\u00e4henb\u00fchl et al., 2015) who found such data-based initialization to work well for use with the normal parameterization.", "startOffset": 100, "endOffset": 148}, {"referenceID": 19, "context": "The model we are using is based on the ConvPool-CNN-C architecture of (Springenberg et al., 2015), with some small modifications: we replace the first dropout layer by a layer that adds Gaussian noise, we expand the last hidden layer from 10 units to 192 units, and we use 2\u00d7 2 max-pooling, rather than 3\u00d73.", "startOffset": 70, "endOffset": 97}, {"referenceID": 13, "context": "68% Network in Network (Lin et al., 2014) 10.", "startOffset": 23, "endOffset": 41}, {"referenceID": 19, "context": "6% ConvPool-CNN-C (Springenberg et al., 2015) 9.", "startOffset": 18, "endOffset": 45}, {"referenceID": 19, "context": "31% ALL-CNN-C (Springenberg et al., 2015) 9.", "startOffset": 14, "endOffset": 41}, {"referenceID": 17, "context": "Next, we test the effect of weight normalization applied to deep convolutional variational auto-encoders (CVAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Salimans et al., 2015), trained on the MNIST dataset of images of handwritten digits and the CIFAR-10 dataset of small natural images.", "startOffset": 113, "endOffset": 182}, {"referenceID": 18, "context": "Next, we test the effect of weight normalization applied to deep convolutional variational auto-encoders (CVAEs) (Kingma & Welling, 2013; Rezende et al., 2014; Salimans et al., 2015), trained on the MNIST dataset of images of handwritten digits and the CIFAR-10 dataset of small natural images.", "startOffset": 113, "endOffset": 182}, {"referenceID": 18, "context": "We follow a similar implementation of the CVAE as in (Salimans et al., 2015) with some modifications, mainly that the encoder and decoder are parameterized with ResNet (He et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 6, "context": ", 2015) with some modifications, mainly that the encoder and decoder are parameterized with ResNet (He et al., 2015) blocks, and that the diagonal posterior is replaced with auto-regressive variational inference1.", "startOffset": 99, "endOffset": 116}, {"referenceID": 5, "context": "Next, we consider DRAW, a recurrent generative model by (Gregor et al., 2015).", "startOffset": 56, "endOffset": 77}, {"referenceID": 2, "context": "Next we apply weight normalization to the problem of Reinforcement Learning for playing games on the Atari Learning Environment (Bellemare et al., 2013).", "startOffset": 128, "endOffset": 152}], "year": 2016, "abstractText": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.", "creator": "LaTeX with hyperref package"}}}