{"id": "1603.00892", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Counter-fitting Word Vectors to Linguistic Constraints", "abstract": "In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors' capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.", "histories": [["v1", "Wed, 2 Mar 2016 21:19:36 GMT  (1103kb,D)", "http://arxiv.org/abs/1603.00892v1", "Paper accepted for the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)"]], "COMMENTS": "Paper accepted for the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["nikola mrksic", "diarmuid \u00f3 s\u00e9aghdha", "blaise thomson", "milica gasic", "lina maria rojas-barahona", "pei-hao su", "david vandyke", "tsung-hsien wen", "steve j young"], "accepted": true, "id": "1603.00892"}, "pdf": {"name": "1603.00892.pdf", "metadata": {"source": "CRF", "title": "Counter-fitting Word Vectors to Linguistic Constraints", "authors": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "emails": ["nm480@cam.ac.uk", "mg436@cam.ac.uk", "phs26@cam.ac.uk", "djv27@cam.ac.uk", "thw28@cam.ac.uk", "sjy@cam.ac.uk", "blaisethom}@apple.com"], "sections": [{"heading": "1 Introduction", "text": "Many popular methods that induce representations for words rely on the distributional hypothesis \u2013 the assumption that semantically similar or related words appear in similar contexts. This hypothesis supports unsupervised learning of meaningful word representations from large corpora (Curran, 2003; O\u0301 Se\u0301aghdha and Korhonen, 2014; Mikolov et al., 2013; Pennington et al., 2014). Word vectors trained using these methods have proven useful for many downstream tasks including machine translation (Zou et al., 2013) and dependency parsing (Bansal et al., 2014).\nOne drawback of learning word embeddings from co-occurrence information in corpora is that it tends to coalesce the notions of semantic similarity and conceptual association (Hill et al., 2014b). Furthermore, even methods that can distinguish similarity from association (e.g., based on syntactic co-occurrences) will generally fail to tell synonyms from antonyms (Mohammad et al., 2008). For example, words such\nas east and west or expensive and inexpensive appear in near-identical contexts, which means that distributional models produce very similar word vectors for such words. Examples of such anomalies in GloVe vectors can be seen in Table 1, where words such as cheaper and inexpensive are deemed similar to (their antonym) expensive.\nA second drawback is that similarity and antonymy can be application- or domain-specific. In our case, we are interested in exploiting distributional knowledge for the dialogue state tracking task (DST). The DST component of a dialogue system is responsible for interpreting users\u2019 utterances and updating the system\u2019s belief state \u2013 a probability distribution over all possible states of the dialogue. For example, a DST for the restaurant domain needs to detect whether the user wants a cheap or expensive restaurant. Being able to generalise using distributional information while still distinguishing between semantically different yet conceptually related words\nar X\niv :1\n60 3.\n00 89\n2v 1\n[ cs\n.C L\n] 2\nM ar\n(e.g. cheaper and pricey) is critical for the performance of dialogue systems. In particular, a dialogue system can be led seriously astray by false synonyms.\nWe propose a method that addresses these two drawbacks by using synonymy and antonymy relations drawn from either a general lexical resource or an application-specific ontology to fine-tune distributional word vectors. Our method, which we term counter-fitting, is a lightweight post-processing procedure in the spirit of retrofitting (Faruqui et al., 2015). The second row of Table 1 illustrates the results of counter-fitting: the nearest neighbours capture true similarity much more intuitively than the original GloVe vectors. The procedure improves word vector quality regardless of the initial word vectors provided as input.1 By applying counter-fitting to the Paragram-SL999 word vectors provided by Wieting et al. (2015), we achieve new state-of-the-art performance on SimLex-999, a dataset designed to measure how well different models judge semantic similarity between words (Hill et al., 2014b). We also show that the counter-fitting method can inject knowledge of dialogue domain ontologies into word vector space representations to facilitate the construction of semantic dictionaries which improve DST performance across two different dialogue domains. Our tool and word vectors are available at github.com/nmrksic/counter-fitting."}, {"heading": "2 Related Work", "text": "Most work on improving word vector representations using lexical resources has focused on bringing words which are known to be semantically related closer together in the vector space. Some methods modify the prior or the regularization of the original training procedure (Yu and Dredze, 2014; Bian et al., 2014; Kiela et al., 2015). Wieting et al. (2015) use the Paraphrase Database (Ganitkevitch et al., 2013) to train word vectors which emphasise word similarity over word relatedness. These word vectors achieve the current state-of-the-art performance on the SimLex-999 dataset and are used as input for counter-fitting in our experiments.\n1When we write \u201cimprove\u201d, we refer to improving the vector space for a specific purpose. We do not expect that a vector space fine-tuned for semantic similarity will give better results on semantic relatedness. As Mohammad et al. (2008) observe, antonymous concepts are related but not similar.\nRecently, there has been interest in lightweight post-processing procedures that use lexical knowledge to refine off-the-shelf word vectors without requiring large corpora for (re-)training as the aforementioned \u201cheavyweight\u201d procedures do. Faruqui et al.\u2019s (2015) retrofitting approach uses similarity constraints from WordNet and other resources to pull similar words closer together.\nThe complications caused by antonymy for distributional methods are well-known in the semantics community. Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013). The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009). As far as we are aware, there is no previous work on exploiting antonymy in dialogue systems. The modelling work closest to ours are Liu et al. (2015), who use antonymy and WordNet hierarchy information to modify the heavyweight Word2Vec training objective; Yih et al. (2012), who use a Siamese neural network to improve the quality of Latent Semantic Analysis vectors; Schwartz et al. (2015), who build a standard distributional model from co-occurrences based on symmetric patterns, with specified antonymy patterns counted as negative co-occurrences; and Ono et al. (2015), who use thesauri and distributional data to train word embeddings specialised for capturing antonymy."}, {"heading": "3 Counter-fitting Word Vectors to Linguistic Constraints", "text": "Our starting point is an indexed set of word vectors V = {v1,v2, . . . ,vN} with one vector for each word in the vocabulary. We will inject semantic relations into this vector space to produce new word vectors V \u2032 = {v\u20321,v\u20322, . . . ,v\u2032N}. For antonymy and synonymy we have a set of constraints A and S, respectively. The elements of each set are pairs of word indices; for example, each pair (i, j) in S is such that the i-th and j-th words in the vocabulary are synonyms. The objective function used to counter-fit the pre-trained word vectors V to the sets of linguistic constraints A and S contains three different terms:\n1. Antonym Repel (AR): This term serves to push antonymous words\u2019 vectors away from each other in the transformed vector space V \u2032:\nAR(V \u2032) = \u2211\n(u,w)\u2208A\n\u03c4 ( \u03b4 \u2212 d(v\u2032u,v\u2032w) ) where d(vi, vj) = 1\u2212cos(vi, vj) is a distance derived from cosine similarity and \u03c4(x) , max(0, x) imposes a margin on the cost. Intuitively, \u03b4 is the \u201cideal\u201d minimum distance between antonymous words; in our experiments we set \u03b4 = 1.0 as it corresponds to vector orthogonality.\n2. Synonym Attract (SA): The counter-fitting procedure should seek to bring the word vectors of known synonymous word pairs closer together:\nSA(V \u2032) = \u2211\n(u,w)\u2208S\n\u03c4 ( d(v\u2032u,v \u2032 w)\u2212 \u03b3 ) where \u03b3 is the \u201cideal\u201d maximum distance between synonymous words; we use \u03b3 = 0.\n3. Vector Space Preservation (VSP): the topology of the original vector space describes relationships between words in the vocabulary captured using distributional information from very large textual corpora. The VSP term bends the transformed vector space towards the original one as much as possible in order to preserve the semantic information contained in the original vectors: VSP(V, V \u2032) = N\u2211 i=1 \u2211 j\u2208N(i) \u03c4 ( d(v\u2032i,v \u2032 j)\u2212 d(vi,vj)\n) For computational efficiency, we do not calculate distances for every pair of words in the vocabulary. Instead, we focus on the (pre-computed) neighbourhood N(i), which denotes the set of words within a certain radius \u03c1 around the i-th word\u2019s vector in the original vector space V . Our experiments indicate that counter-fitting is relatively insensitive to the choice of \u03c1, with values between 0.2 and 0.4 showing little difference in quality; here we use \u03c1 = 0.2.\nThe objective function for the training procedure is given by a weighted sum of the three terms:\nC(V, V \u2032) = k1AR(V \u2032)+k2SA(V \u2032)+k3VSP(V, V \u2032)\nwhere k1, k2, k3 \u2265 0 are hyperparameters that control the relative importance of each term. In our experiments we set them to be equal: k1 = k2 = k3. To minimise the cost function for a set of starting vectors V and produce counter-fitted vectors V \u2032, we run stochastic gradient descent (SGD) for 20 epochs. An end-to-end run of counter-fitting takes less than two minutes on a laptop with four CPUs."}, {"heading": "3.1 Injecting Dialogue Domain Ontologies into Vector Space Representations", "text": "Dialogue state tracking (DST) models capture users\u2019 goals given their utterances. Goals are represented as sets of constraints expressed by slot-value pairs such as [food: Indian] or [parking: allowed]. The set of slots S and the set of values Vs for each slot make up the ontology of a dialogue domain.\nIn this paper we adopt the recurrent neural network (RNN) framework for tracking suggested in (Henderson et al., 2014d; Henderson et al., 2014c; Mrks\u030cic\u0301 et al., 2015). Rather than using a spoken language understanding (SLU) decoder to convert user utterances into meaning representations, this model operates directly on the n-gram features extracted from the automated speech recognition (ASR) hypotheses. A drawback of this approach is that the RNN model can only perform exact string matching to detect the slot names and values mentioned by the user. It cannot recognise synonymous words such as pricey and expensive, or even subtle morphological variations such as moderate and moderately. A simple way to mitigate this problem is to use semantic dictionaries: lists of rephrasings for the values in the ontology. Manual construction of dictionaries is highly labourintensive; however, if one could automatically detect high-quality rephrasings, then this capability would come at no extra cost to the system designer.\nTo obtain a set of word vectors which can be used for creating a semantic dictionary, we need to inject the domain ontology into the vector space. This can be achieved by introducing antonymy constraints between all the possible values of each slot (i.e. Chinese and Indian, expensive and cheap, etc.). The remaining linguistic constraints can come from semantic lexicons: the richer the sets of injected synonyms and antonyms are, the better the resulting word representations will become."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Word Vectors and Semantic Lexicons", "text": "Two different collections of pre-trained word vectors were used as input to the counter-fitting procedure:\n1. Glove Common Crawl 300-dimensional vectors made available by Pennington et al. (2014).\n2. Paragram-SL999 300-dimensional vectors made available by Wieting et al. (2015).\nThe synonymy and antonymy constraints were obtained from two semantic lexicons:\n1. PPDB 2.0 (Pavlick et al., 2015): the latest release of the Paraphrase Database. A new feature of this version is that it assigns relation types to its word pairs. We identify the Equivalence relation with synonymy and Exclusion with antonymy. We used the largest available (XXXL) version of the database and only considered single-token terms.\n2. WordNet (Miller, 1995): a well known semantic lexicon which contains vast amounts of high quality human-annotated synonym and antonym pairs. Any two words in our vocabulary which had antonymous word senses were considered antonyms; WordNet synonyms were not used.\nIn total, the lexicons yielded 12,802 antonymy and 31,828 synonymy pairs for our vocabulary, which consisted of 76,427 most frequent words in OpenSubtitles, obtained from invokeit.wordpress. com/frequency-word-lists/."}, {"heading": "4.2 Improving Lexical Similarity Predictions", "text": "In this section, we show that counter-fitting pretrained word vectors with linguistic constraints improves their usefulness for judging semantic similarity. We use Spearman\u2019s rank correlation coefficient with the SimLex-999 dataset, which contains word pairs ranked by a large number of annotators instructed to consider only semantic similarity.\nTable 2 contains a summary of recently reported competitive scores for SimLex-999, as well as the performance of the unaltered, retrofitted and counterfitted GloVe and Paragram-SL999 word vectors. To the best of our knowledge, the 0.685 figure reported for the latter represents the current high score. This figure is above the average inter-annotator agreement of 0.67, which has been referred to as the ceiling performance in most work up to now.\nIn our opinion, the average inter-annotator agreement is not the only meaningful measure of ceiling performance. We believe it also makes sense to compare: a) the model ranking\u2019s correlation with the gold standard ranking to: b) the average rank correlation that individual human annotators\u2019 rankings achieved with the gold standard ranking. The SimLex-999 authors have informed us that the average annotator agreement with the gold standard is 0.78.2 As shown in Table 2, the reported performance of all the models and word vectors falls well below this figure.\nRetrofitting pre-trained word vectors improves GloVe vectors, but not the already semantically specialised Paragram-SL999 vectors. Counter-fitting substantially improves both sets of vectors, showing that injecting antonymy relations goes a long way\n2This figure is now reported as a potentially fairer ceiling performance on the SimLex-999 website: http://www.cl. cam.ac.uk/\u02dcfh295/simlex.html.\ntowards improving word vectors for the purpose of making semantic similarity judgements.\nTable 3 shows the effect of injecting different categories of linguistic constraints. GloVe vectors benefit from all three sets of constraints, whereas the quality of Paragram vectors, already exposed to PPDB, only improves with the injection of WordNet antonyms. Table 4 illustrates how incorrect similarity predictions based on the original (Paragram) vectors can be fixed through counter-fitting. The table presents eight false synonyms and nine false antonyms: word pairs with predicted rank in the top (bottom) 200 word pairs and gold standard rank 500 or more positions lower (higher). Eight of these errors are fixed by counter-fitting: the difference between predicted and gold-standard ranks is now 100 or less. Interestingly, five of the eight corrected word pairs do not appear in the sets of linguistic constraints; these are indicated by double ticks in the table. This shows that secondary (i.e. indirect) interactions through the three terms of the cost function do contribute to the semantic content of the transformed vector space."}, {"heading": "4.3 Improving Dialogue State Tracking", "text": "Table 5 shows the dialogue state tracking datasets used for evaluation. These datasets come from the Dialogue State Tracking Challenges 2 and 3 (Henderson et al., 2014a; Henderson et al., 2014b).\nWe used four different sets of word vectors to construct semantic dictionaries: the original GloVe and Paragram-SL999 vectors, as well as versions counterfitted to each domain ontology. The constraints used for counter-fitting were all those from the previous section as well as antonymy constraints among the set of values for each slot. We treated all vocabulary words within some radius t of a slot value as\nrephrasings of that value. The optimal value of t was determined using a grid search: we generated a dictionary and trained a model for each potential t, then evaluated on the development set. Table 6 shows the performance of RNN models which used the constructed dictionaries. The dictionaries induced from the pre-trained vectors substantially improved tracking performance over the baselines (which used no semantic dictionaries). The dictionaries created using the counter-fitted vectors improved performance even further. Contrary to the SimLex-999 experiments, starting from the Paragram vectors did not lead to superior performance, which shows that injecting the application-specific ontology is at least as important as the quality of the initial word vectors."}, {"heading": "5 Conclusion", "text": "We have presented a novel counter-fitting method for injecting linguistic constraints into word vector space representations. The method efficiently postprocesses word vectors to improve their usefulness for tasks which involve making semantic similarity judgements. Its focus on separating vector representations of antonymous word pairs lead to substantial improvements on genuine similarity estimation tasks. We have also shown that counter-fitting can tailor word vectors for downstream tasks by using it to inject domain ontologies into word vectors used to construct semantic dictionaries for dialogue systems."}, {"heading": "Acknowledgements", "text": "We would like to thank Felix Hill for help with the SimLex-999 evaluation. We also thank the anonymous reviewers for their helpful suggestions."}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of ACL", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Knowledge-powered deep learning for word embedding. In Machine Learning and Knowledge Discovery in Databases", "author": ["Bian et al.2014] Jiang Bian", "Bin Gao", "Tie-Yan Liu"], "venue": null, "citeRegEx": "Bian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "From Distributional to Semantic Similarity", "author": ["James Curran"], "venue": "Ph.D. thesis, School of Informatics,", "citeRegEx": "Curran.,? \\Q2003\\E", "shortCiteRegEx": "Curran.", "year": 2003}, {"title": "Finding contradictions in text", "author": ["Anna N. Rafferty", "Christopher D. Manning"], "venue": "In Proceedings of ACL", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Non-distributional word vector representations", "author": ["Faruqui", "Dyer2015] Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of ACL", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Retrofitting Word Vectors to Semantic Lexicons", "author": ["Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "PPDB: The Paraphrase Database", "author": ["Benjamin Van Durme", "Chris Callison-burch"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Excitatory or inhibitory: A new semantic orientation extracts contradiction and causality from the Web", "author": ["Kentaro Torisawa", "Stijn De Saeger", "Jong-Hoon Oh", "Junichi Kazama"], "venue": "Proceedings of EMNLP-CoNLL", "citeRegEx": "Hashimoto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2012}, {"title": "The Second Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "The Third Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Robust Dialog State Tracking using Delexicalised Recurrent Neural Networks and Unsupervised Adaptation", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-Based Dialog State Tracking with Recurrent Neural Networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Embedding word similarity with neural machine translation", "author": ["Hill et al.2014a] Felix Hill", "Kyunghyun Cho", "Sbastien Jean", "Coline Devin", "Yoshua Bengio"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Specializing word embeddings for similarity or relatedness", "author": ["Kiela et al.2015] Douwe Kiela", "Felix Hill", "Stephen Clark"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Identifying synonyms among distributionally similar words", "author": ["Lin et al.2003] Dekang Lin", "Shaojun Zhao", "Lijuan Qin", "Ming Zhou"], "venue": "In Proceedings of IJCAI", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "author": ["Liu et al.2015] Quan Liu", "Hui Jiang", "Si Wei", "Zhen-Hua Ling", "Yu Hu"], "venue": "Proceedings of ACL", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "An unsupervised approach to recognizing discourse relations", "author": ["Marcu", "Echihabi2002] Daniel Marcu", "Abdsemmad Echihabi"], "venue": "In Proceedings of ACL", "citeRegEx": "Marcu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Marcu et al\\.", "year": 2002}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Proceedings of NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A Lexical Database for English. Communications of the ACM", "author": ["George A. Miller"], "venue": null, "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Computing word-pair antonymy", "author": ["Bonnie Dorr", "Graeme Hirst"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Mohammad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2008}, {"title": "Computing lexical contrast", "author": ["Bonnie J. Dorr", "Graeme Hirst", "Peter D. Turney"], "venue": null, "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "author": ["Mrk\u0161i\u0107 et al.2015] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "Proceedings of ACL", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Word Embedding-based Antonym Detection using Thesauri and Distributional Information", "author": ["Ono et al.2015] Masataka Ono", "Makoto Miwa", "Yutaka Sasaki"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "Ono et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ono et al\\.", "year": 2015}, {"title": "Probabilistic distributional semantics", "author": ["\u00d3 S\u00e9aghdha", "Anna Korhonen"], "venue": "Computational Linguistics,", "citeRegEx": "S\u00e9aghdha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "S\u00e9aghdha et al\\.", "year": 2014}, {"title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Pushpendre Rastogi", "Juri Ganitkevich", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Symmetric pattern based word embeddings for improved word similarity prediction", "author": ["Roi Reichart", "Ari Rappoport"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Schwartz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2015}, {"title": "A uniform approach to analogies, synonyms, antonyms, and associations", "author": ["Peter D. Turney"], "venue": "In Proceedings of COLING", "citeRegEx": "Turney.,? \\Q2008\\E", "shortCiteRegEx": "Turney.", "year": 2008}, {"title": "From paraphrase database to compositional paraphrase model and back. Transactions of the Association for Computational Linguistics", "author": ["Wieting et al.2015] John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Polarity inducing Latent Semantic Analysis", "author": ["Yih et al.2012] Wen-Tau Yih", "Geoffrey Zweig", "John C. Platt"], "venue": "In Proceedings of ACL", "citeRegEx": "Yih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2012}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Proceedings of ACL", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "A machine learning approach to textual entailment recognition", "author": ["Marco Pennachiotti", "Alessandro Moschitti"], "venue": "Journal of Natural Language Engineering,", "citeRegEx": "Zanzotto et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2009}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel M. Cer", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "This hypothesis supports unsupervised learning of meaningful word representations from large corpora (Curran, 2003; \u00d3 S\u00e9aghdha and Korhonen, 2014; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 17, "context": "This hypothesis supports unsupervised learning of meaningful word representations from large corpora (Curran, 2003; \u00d3 S\u00e9aghdha and Korhonen, 2014; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 25, "context": "This hypothesis supports unsupervised learning of meaningful word representations from large corpora (Curran, 2003; \u00d3 S\u00e9aghdha and Korhonen, 2014; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 101, "endOffset": 193}, {"referenceID": 32, "context": "Word vectors trained using these methods have proven useful for many downstream tasks including machine translation (Zou et al., 2013) and dependency parsing (Bansal et al.", "startOffset": 116, "endOffset": 134}, {"referenceID": 0, "context": ", 2013) and dependency parsing (Bansal et al., 2014).", "startOffset": 31, "endOffset": 52}, {"referenceID": 19, "context": ", based on syntactic co-occurrences) will generally fail to tell synonyms from antonyms (Mohammad et al., 2008).", "startOffset": 88, "endOffset": 111}, {"referenceID": 27, "context": "1 By applying counter-fitting to the Paragram-SL999 word vectors provided by Wieting et al. (2015), we achieve new state-of-the-art performance on SimLex-999, a dataset designed to measure how well different models judge semantic similarity between words (Hill et al.", "startOffset": 77, "endOffset": 99}, {"referenceID": 1, "context": "Some methods modify the prior or the regularization of the original training procedure (Yu and Dredze, 2014; Bian et al., 2014; Kiela et al., 2015).", "startOffset": 87, "endOffset": 147}, {"referenceID": 13, "context": "Some methods modify the prior or the regularization of the original training procedure (Yu and Dredze, 2014; Bian et al., 2014; Kiela et al., 2015).", "startOffset": 87, "endOffset": 147}, {"referenceID": 6, "context": "(2015) use the Paraphrase Database (Ganitkevitch et al., 2013) to train word vectors which emphasise word similarity over word relatedness.", "startOffset": 35, "endOffset": 62}, {"referenceID": 1, "context": "Some methods modify the prior or the regularization of the original training procedure (Yu and Dredze, 2014; Bian et al., 2014; Kiela et al., 2015). Wieting et al. (2015) use the Paraphrase Database (Ganitkevitch et al.", "startOffset": 109, "endOffset": 171}, {"referenceID": 17, "context": "As Mohammad et al. (2008) observe, antonymous concepts are related but not similar.", "startOffset": 3, "endOffset": 26}, {"referenceID": 4, "context": "Faruqui et al.\u2019s (2015) retrofitting approach uses similarity constraints from WordNet and other resources to pull similar words closer together.", "startOffset": 0, "endOffset": 24}, {"referenceID": 14, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 19, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 27, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 7, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 20, "context": "Most prior work focuses on extracting antonym pairs from text rather than exploiting them (Lin et al., 2003; Mohammad et al., 2008; Turney, 2008; Hashimoto et al., 2012; Mohammad et al., 2013).", "startOffset": 90, "endOffset": 192}, {"referenceID": 31, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009).", "startOffset": 128, "endOffset": 203}, {"referenceID": 3, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009). As far as we are aware, there is no previous work on exploiting antonymy in dialogue systems. The modelling work closest to ours are Liu et al. (2015), who use antonymy and WordNet hierarchy information to modify the heavyweight Word2Vec training objective; Yih et al.", "startOffset": 158, "endOffset": 356}, {"referenceID": 3, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009). As far as we are aware, there is no previous work on exploiting antonymy in dialogue systems. The modelling work closest to ours are Liu et al. (2015), who use antonymy and WordNet hierarchy information to modify the heavyweight Word2Vec training objective; Yih et al. (2012), who use a Siamese neural network to improve the quality of Latent Semantic Analysis vectors; Schwartz et al.", "startOffset": 158, "endOffset": 481}, {"referenceID": 3, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009). As far as we are aware, there is no previous work on exploiting antonymy in dialogue systems. The modelling work closest to ours are Liu et al. (2015), who use antonymy and WordNet hierarchy information to modify the heavyweight Word2Vec training objective; Yih et al. (2012), who use a Siamese neural network to improve the quality of Latent Semantic Analysis vectors; Schwartz et al. (2015), who build a standard distributional model from co-occurrences based on symmetric patterns, with specified antonymy patterns counted as negative co-occurrences; and Ono et al.", "startOffset": 158, "endOffset": 598}, {"referenceID": 3, "context": "The most common use of antonymy information is to provide features for systems that detect contradictions or logical entailment (Marcu and Echihabi, 2002; de Marneffe et al., 2008; Zanzotto et al., 2009). As far as we are aware, there is no previous work on exploiting antonymy in dialogue systems. The modelling work closest to ours are Liu et al. (2015), who use antonymy and WordNet hierarchy information to modify the heavyweight Word2Vec training objective; Yih et al. (2012), who use a Siamese neural network to improve the quality of Latent Semantic Analysis vectors; Schwartz et al. (2015), who build a standard distributional model from co-occurrences based on symmetric patterns, with specified antonymy patterns counted as negative co-occurrences; and Ono et al. (2015), who use thesauri and distributional data to train word embeddings specialised for capturing antonymy.", "startOffset": 158, "endOffset": 781}, {"referenceID": 26, "context": "52 Symmetric Patterns (Schwartz et al., 2015) 0.", "startOffset": 22, "endOffset": 45}, {"referenceID": 25, "context": "58 GloVe vectors (Pennington et al., 2014) 0.", "startOffset": 17, "endOffset": 42}, {"referenceID": 28, "context": "58 Paragram-SL999 (Wieting et al., 2015) 0.", "startOffset": 18, "endOffset": 40}, {"referenceID": 25, "context": "Glove Common Crawl 300-dimensional vectors made available by Pennington et al. (2014).", "startOffset": 61, "endOffset": 86}, {"referenceID": 28, "context": "Paragram-SL999 300-dimensional vectors made available by Wieting et al. (2015).", "startOffset": 57, "endOffset": 79}, {"referenceID": 24, "context": "0 (Pavlick et al., 2015): the latest release of the Paraphrase Database.", "startOffset": 2, "endOffset": 24}, {"referenceID": 18, "context": "WordNet (Miller, 1995): a well known semantic lexicon which contains vast amounts of high quality human-annotated synonym and antonym pairs.", "startOffset": 8, "endOffset": 22}], "year": 2016, "abstractText": "In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors\u2019 capability for judging semantic similarity. Applying this method to publicly available pre-trained word vectors leads to a new state of the art performance on the SimLex-999 dataset. We also show how the method can be used to tailor the word vector space for the downstream task of dialogue state tracking, resulting in robust improvements across different dialogue domains.", "creator": "LaTeX with hyperref package"}}}