{"id": "1603.01581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Causal inference for cloud computing", "abstract": "Two challenges in the theory and practice of cloud computing are: (1) smart control (allocation) of resources under exploration constraints, on time-varying systems, and (2) understanding and debugging of the performance of complex systems that involve e.g. virtualization. In this paper, we examine how these challenges can be approached using causal models. For challenge (1) we investigate how to infer and use causal models and selection diagrams to design and integrate experiments in a principled way, as well as to cope with partially varying systems. For challenge (2) we examine how to formalize performance attribution and debugging questions by counterfactual probabilities, and how to approximately answer them based on inferred (non-deterministic) graphical causal models.", "histories": [["v1", "Fri, 4 Mar 2016 19:28:13 GMT  (47kb)", "http://arxiv.org/abs/1603.01581v1", null], ["v2", "Wed, 25 May 2016 12:09:23 GMT  (111kb)", "http://arxiv.org/abs/1603.01581v2", null], ["v3", "Fri, 27 May 2016 09:14:54 GMT  (96kb)", "http://arxiv.org/abs/1603.01581v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["philipp geiger", "lucian carata", "bernhard schoelkopf"], "accepted": false, "id": "1603.01581"}, "pdf": {"name": "1603.01581.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["pgeiger@tuebingen.mpg.de", "lc525@cam.ac.uk", "bs@tuebingen.mpg.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n01 58\n1v 1\n[ cs\n.A I]\n4 M\nar 2\nTwo challenges in the theory and practice of cloud computing are: (1) smart control (allocation) of resources under exploration constraints, on time-varying systems, and (2) understanding and debugging of the performance of complex systems that involve e.g. virtualization. In this paper, we examine how these challenges can be approached using causal models. For challenge (1) we investigate how to infer and use causal models and selection diagrams to design and integrate experiments in a principled way, as well as to cope with partially varying systems. For challenge (2) we examine how to formalize performance attribution and debugging questions by counterfactual probabilities, and how to approximately answer them based on inferred (non-deterministic) graphical causal models."}, {"heading": "1 Introduction", "text": "In recent years, the paradigm and business model of cloud computing [Armbrust et al., 2010] has become increasingly popular. It allows to rent computing resources on-demand, and to use them efficiently by sharing them in a smart way. However, it also raises new challenges in terms of understanding, debugging and managing the complex systems involved. In the present paper, we investigate how causal models [Pearl, 2000, Spirtes et al., 2000] can help address two of these challenges. Note that we do not aim to present mature methodology here. We assay, for various aspects of cloud computing, how causal models can be applied to them, and draw inspiration from the kind of problems cloud computing brings about. We find that a number of concepts commonly used in\nthe field of cloud computing directly map to concepts from the theory of causal models (in particular counterfactuals and selection diagrams), and explain these connections in our paper.\nOur main contributions are conceptual rather than empirical:\n\u2022 We translate the problem of inferring controllers for cloud servers from experimental and timevarying systems into causal models, and, based on this, present an approach for designing experiments and integrating their outcome as well as past experience of partially time-varying systems (Sections 4.2 and 4.3). \u2022 We translate observation-level performance attribution and debugging questions into queries for counterfactual probabilities defined in causal models (Section 5.1), and present a new approach to approximately answer them from a (non-deterministic) graphical causal model only (Section 5.2). \u2022 Furthermore, we investigate how causal models for computer systems can be inferred in general (Sections 4.1 and 6).\nThe remainder of this paper is structured as follows: in Section 2 we introduce basic concepts; in Section 3 we formulate two problems from cloud computing and outline our approaches; in Sections 4 and 5 we elaborate our approach for the first and second problem, respectively; in Section 6, we briefly discuss some implementation aspects; in Section 7, we discuss related work; and we conclude the paper with Section 8."}, {"heading": "2 Preliminaries", "text": "In what follows, we introduce concepts that are used throughout the paper in a succinct way. More detailed expositions of these concepts can be found in the ref-\nerences provided.\nCausal models and interventions. We first define causal models on a mathematical level, following Pearl [2000], Spirtes et al. [2000]. We consider variables with discrete as well as continuous domains, and by a (probability) density we either refer to a density w.r.t. the Lebesgue measure, in the continuous case, or w.r.t. the counting measure, in the discrete case, respectively. If a formula that contains a sum over densities is not explicitly restricted to discrete variables, then usually the formula also holds with the sum replaced by the Lebesgue integral.\nLet V be a set of variables. A functional causal model (FCM), or structural equation model (SEM), over V (we call the elements of V the (regular) variables of the model) consists of a background variable UX with density pUX for each X \u2208 V , and a structural equation X := fX(PAX , UX) for each X \u2208 V , where PAX \u2282 V are called the parents of X . A graphical causal model (GCM), or causal Bayesian network (CBN), over V consists of a directed acyclic graph (DAG) G with V as node set, called causal diagram or causal DAG, and a conditional probability density pX|PAX=paX (for all paX in the domain of PAX) for each X \u2208 V , where PAX are the parents of X in G.\n1 Each FCM induces a GCM in a natural way: the parents in the structural equations define the parents in the causal diagram, and pX|PAX=paX is defined as the density of fX(paX , UX), for all variablesX . By causal models we refer to FCMs as well as GCMs.\nGiven a causal model M and a tuple of variables Z of M , the post-interventional causal model MdoZ=z is defined as follows: (1) if M is a FCM, drop the structural equations for all variables in Z and replace, in all remaining structural equations, variables of Z by the corresponding constant entries of z; (2) if M is a GCM, drop the variables in Z and all incoming arrows from the causal diagram, and fix the value of variables in Z to the corresponding entry of z in all remaining conditional densities. Based on this, we define the post-interventional density of Y after setting Z to z, denoted by pY |doZ=z, by the the density of Y in MdoZ=z. For a succinct terminology, we will consider the observational density pZ , for any set of variables Z in M , as a special case of a post-interventional density.\nOn a non-mathematical level, we consider M to be a correct causal model of some part of reality, if it correctly predicts the outcomes of interventions in that\n1The conditionals induce a joint density pV . If pV has support everywhere, then it, together with G, already determines the GCM.\npart of reality (clearly there are other reasonable definitions of causation). Note that we will use expressions like p(x|y) as shorthand for pX|Y (x|y).\nSelection diagrams. The idea behind selection diagrams [Bareinboim and Pearl, 2012] is to allow to graphically encode how the causal structure of two or more populations (or systems) relate to each other by expressing which mechanisms vary and which are invariant. Formally, a selection diagram is a DAG (with additional two-sided arrows to encode hidden confounders) which has two types of nodes: one for regular variables (the elements of V in the definition of causal models above) and one for so-called selection variables (we indicate the latter type of node by a rectangle). The selection diagram induced by a family (\nM(s1,...,sn) )\n(s1,...,sn)\u2208I of causal models over the same\nset of variables V and with the same causal diagram G is defined as follows (slightly adapting the definition to our needs): we start with the DAG G and for each variable X and for each i = 1, . . . , n, for which the generating mechanism of X depends on the parameter si, we add a selection variable node Si and an arrow from that node to X to the selection diagram.\nStructural counterfactuals. Let M be a FCM over a set V of variables, with a set U of background variables. Let E,X, Y be (sets of) variables in V . The structural counterfactual probability of Y being y, had X been x, given evidence E = e, can be defined [Pearl, 2000] based on M as2\np(YdoX=x = y|e) := \u2211\nu\np(y|do (x), u)p(u|e). (1)\nReinforcement learning and control. Reinforcement learning (RL) [Sutton and Barto, 1998] is concerned with designing automated agents which perform actions (or decisions) that optimize some given long-term reward or utility. It is usually assumed that the environment of the agent, also called the dynamics, is (partially) unknown and stochastic but timeinvariant. An agent whose actions only depend on a fixed (small) number of previous time steps is also called a policy or controller. In RL, one usually assumes the environment to be unknown, and so an agent first has to explore it, and later has to trade off exploration with exploitation of the gained experience (based on the time-invariance and additional assumption). In principle, given a perfect model of the environment, including utility function, the RL problem is reduced to finding an optimum w.r.t. the policy, of the utility under the model. In this paper, for simplic-\n2Note that while [Pearl, 2000] in his definition uses functions, we, for the sake of a concise definition, use (deterministic) conditional distributions.\nity, we often neglect inter-temporal dependences, and thus focus on what is usually referred to as contextual bandits [Langford and Zhang, 2008].\nCloud computing. Traditionally, both businesses and individuals have used dedicated local computers, or computer networks, for storing, managing and processing data. However, this can be inefficient in several ways: the overhead of maintaining such an infrastructure is high, and one needs to buy enough computers to handle peak loads, while during normal times most will remain unutilised. Cloud computing significantly changes this, by allowing computing resources to be rented on demand. A company, the \u201ccloud provider\u201d, is now responsible for running all the hardware, keeping it upgraded and sharing it amongst multiple clients. Such an infrastructure can be run in a highly efficient manner: tens or hundreds of virtual machines (VMs), i.e., emulations of computer systems, rented by different clients, run on a single physical server and share its resources such as central processing units (CPUs), memory and network. Note that by production, we will refer to the operation of the system that does actual work for clients and visitors and w.r.t. which contracts have to be met."}, {"heading": "3 Two problems in cloud computing and outline of our approach", "text": "In this section we introduce the two problems we are addressing in this paper and outline our approaches, which we then elaborate in the two subsequent sections. Besides addressing the two specific problems described below, in this paper we also follow the more general side goals of (1) understanding how causal and probabilistic models and methods can help for computer systems problems, and (2) better understanding the concept of experimentation and causation."}, {"heading": "3.1 Problem 1: inferring controllers from experimental and time-varying systems", "text": "Problem formulation. During the operation of a cloud server many \u201cdecisions\u201d automatically have to be made regarding how resources, such as CPU time or bandwidth of input/output (IO) devices, are allocated among the various applications or VMs running on that sever. Here we consider the goal of optimizing this automatic decision making, based on some given utility function, encoding e.g. energy consumption, guarantees given to customers, or simply profit. We regard two specific scenarios: In the first scenario (described in more detail in Section 4.2), we assume that an ex-\nperimental system, separate from the system in production, is available, and we can execute applications and VMs there and transfer the experiences gained there to the system in production. The motivation for considering such a setup is the following: when exploration (in the sense of RL) is performed during production, then tentative decisions are exposed to the clients and visitors. Several papers have examined such a scenario, assuming that experiments are performed either in advance [Chiang et al., 2014, Vasic\u0301 et al., 2012] or in parallel [Zheng et al., 2009] to production. In the second scenario (described in more detail in Section 4.3), we assume that there is only a single system, the production system, but this system is time-varying, e.g. due to varying applications (or VMs) running on it. A version of such a setup has been considered and addressed e.g. in [Padala et al., 2009], using adaptive control methods. A common limitation of the mentioned approaches (to both scenarios) can be seen in the lack of a framework to treat things such as experimentation or knowledge transfer between different environments in a principled way, potentially leading to all sorts of prediction errors and subsequent suboptimal decisions. For instance, decisions about which properties to vary and to regress on during an experiment are made ad-hoc. The structure of the problem and our approach is summarized in Figure 1.\nOutline of our approach. Generally, we propose to use a selection diagram to encode the invariant causal diagram of the system in production as well as the varying mechanisms, and furthermore write down what information becomes available at which point. The inference of such a diagrams is discussed in Section 4.1. Then, to address the first scenario described in the problem formulation above, we propose to use the selection diagram to design the experiment and integrate the outcome for the sake of decision making on the production system (Section 4, based on a \u201cquintessential\u201d scenario). For the second scenario, we suggest to use the information on invariant mechanisms encoded in the selection diagram to harness experience from an earlier stage of the production\nsystem, which we refer to as \u201cinitial setup\u201d, for a later one, \u201ctarget setup\u201d (Section 4.3, again based on a \u201cquintessential\u201d scenario). We argue that such a principled approach, compared to the ones mentioned above, is less prone to making errors, e.g. in the design and knowledge transfer from experiments, and allows to harness available information from \u201cdifferent\u201d systems (Section 4.4)."}, {"heading": "3.2 Problem 2: observation-level performance attribution and debugging", "text": "Problem formulation. Performance attribution in computing systems means to understand which component of a system contributes to what extent to the measured performance. (Part of the problem is that there are currently no rigorous and general definitions of many concepts. We hope that with this paper we also contribute a bit to making definitions more rigorous, to the extent this is reasonable.) Such attribution often forms an important step for debugging the performance, i.e., modifying certain components of the system, to achieve the goal of improving the performance. In cloud computing, performance attribution and debugging is, due to the complexity of the system, non-trivial but crucial: for instance, in case of a poor performance of some application running in a VM on a cloud server, one wants to understand whether this problem has to be attributed to the configuration of the VM (which one could change directly) or the configuration of the server or other VMs on the server (in which case one may have to buy a more expensive cloud product) [Snee et al., 2015]. Note that we presently focus on attribution and debugging for individual observations. In previous approaches, the lack of an appropriate formal framework leads to difficulties w.r.t., for instance, encoding valuable prior knowledge of the system, or rigorously distinguishing between correlation and causation.\nOutline of our approach. We first show how to translate performance attribution and debugging questions into queries for counterfactual distributions in causal models (Section 5.1). Although generally a FCM is necessary to answer such queries, we show that they can still approximately be answered based only on a GCM (Section 5.2). Last, we discuss how we propose to combine the above modules and discuss some advantages of our approach (Section 5.3)."}, {"heading": "4 Addressing problem 1: inferring controllers from experimental and time-varying systems", "text": "Keep in mind the problem formulation in Section 3.1. Here we elaborate the approach which we already outlined in that section, structuring it into \u201cmodules\u201d, which we then combine and discuss in Section 4.4."}, {"heading": "4.1 Module 1A: inference of causal models and selection diagram", "text": "Clearly, as with all models, inference and application of causal models is a back-and-forth, trial-and-error process. Based on their definition (Section 2), an important ingredient to the inference of causal models are (randomized) interventions on variables and the subsequent observation of their outcome, to the extent such interventions are feasible.3 Important additional input to causal model inference often comes from domain knowledge of experts [Pearl, 2009]. Additional insight can come from observational causal discovery methods [Spirtes et al., 2000, Mooij et al., 2014]. Generally, as many sources as possible should be used, and potentially redundant information can always be very valuable to make the inference more robust. We briefly discuss some aspects of implementation of causal model inference on real system in Section 6.\nRegarding selection diagrams, we first note that their mathematical definition (Section 2, based on [Bareinboim and Pearl, 2012]) takes complete causal models of various domains as input. In practice however, it is rather the goal to \u201coutput\u201d the complete (or relevant part of the) causal model of some \u201ctarget domain\u201d, based on (1) the joint causal diagram of all domains, (2) knowledge about mechanisms in \u201csource domains\u201d, and (3) knowledge on which mechanisms vary between target and various source domains. The role of the selection diagram is to allow to simultaneously encode (1) and (3). It seems that generally, for this method to work, a lot of expert knowledge and a good intuition for how to use causal modeling language are necessary.\nWe want to point out how the inference of causal models and selection diagrams can be facilitated for the case of computer systems, based on several appealing properties of this domain: First, many aspects of computer systems (hardware and software)\n3Clearly, the theory of causal model inference faces the classical problem of induction [Hume and Hendel, 1955]. We simply postulate here that humans can infer sensible models from \u201cfinite\u201d data or supervise machines to do so.\nare - by design - modular, i.e., separable into individually manipulable input-output mechanisms and the same (or similar) mechanisms occur in different systems. And modularity is a central assumption in causal models [Pearl, 2000]. Second, there is a potential source of information which is specific to computer systems: a lot of knowledge about non-causal associations, such as which program calls which other program during execution, is available, often times even in a well-formatted way (e.g. program code or system architecture specifications). Such information could be translated into hypotheses on causal association (or be used for measurement selection), in a (semi-)automatic way. Generally, keep in mind that the inference of a causal model for some system or component may be non-trivial; however, a good causal model, which only needs to be inferred once, can subsequently save a lot of exploration (in the sense of RL) for all instances of the system or component, and thus significantly lower the overall costs."}, {"heading": "4.2 Module 1B: integration of experiments", "text": "Here we describe in more detail a quintessential version of the first scenario from the problem formulation (Section 3.1) and show how to address it.\nA quintessential scenario. We consider the following scenario (which will be formalized by the selection diagram D in Figure 2). There is some target stage, where some cloud server, which we also refer to as \u201ctarget platform\u201d, will run a specific VM Vm and some concurrent workload (e.g., concurrent VMs) Con. We call the target platform, together with specific Vm and Con, \u201ctarget setup\u201d. 4 We assume that we have an experimental cloud server, which we refer to as \u201cexperimentation platform\u201d (or \u201csystem identification\u201d stage), equal but not identical to the target platform. The approach we will describe below is applicable to any type of resource, but for the sake of instructiveness we consider the specific resource CPU time, and denote by Cpu the fraction of that resource which is allocated for the execution of Vm at some fixed time point. The (subsequent) performance of Vm is measured by some variable Perf . We assume that we are given some utility function which depends on the distribution of Perf , among others.\nBased on this, we formulate the overall goal of this scenario as follows: find a policy \u03a0, which, based on some fixed input Feat, makes \u201coptimal\u201d allocation de-\n4Work such as [Chiang et al., 2014] do not assume to know the exact VM in advance but only a \u201csimilar\u201d one. However, such details are beyond the scope of this paper.\ncisions w.r.t. Cpu, under the given utility function, in the target setup, by making use of the target setup as well as the experimentation platform in a smart way. Note that, ignoring the availability of the experimental platform, this could be seen as a contextual bandit problem. Below, we will focus on an important ingredient towards the overall goal, which we will simply refer to as \u201cgoal\u201d: predict (the distribution) of Perf from Feat, for all possible decision Cpu, in the target setup, by making use of the target setup as well as the experimentation platform in a smart way. The goal relates to the overall goal as follows: given we achieved the former, the latter is reduced to an optimization problem (in theory, obviously), as mentioned in Section 2.\nWe introduce the following additional variables: Oth denotes the \u201cstate\u201d of the execution of the concurrent workload Con at some fixed time point, and In denotes some factor (different from Cpu) via which Perf of Vm is influenced by concurrent activities, such as the available bandwidth of some IO device, at a fixed time point. (One could also think of different scenarios where Oth is some observed or hidden confounder, such as time of the day or some class of users that interact with both the execution of Vm and the concurrent VMs.)\nSelection diagram and revelation structure. We assume that the target platform is correctly described by the selection diagram D depicted in Figure 2.5 That is: there is a (unknown) family of causal models (Mvm,con,\u03c0)(vm,con,\u03c0)\u2208I =: F such that the target platform under the configuration Vm = vm,Con = con,\u03a0 = \u03c0 is correctly described by Mvm,con,\u03c0 and D is the selection diagram induced by this family. We assume that the target setup under the policy \u03a0 = \u03c0 to be described by M\u03c0 := Mv,c,\u03c0, with v, c denoting constants which are not known from the beginning.\n5Note that a more accurate model would need to be temporal since clearly the execution of Vm also influences the concurrent VMs but this would lead to cycles in our scenario which we aim to keep simple for the sake of instructiveness. Furthermore, the \u201ccomplete\u201d policy would also allocate the resources for the concurrent workload.\nBy revelation structure we mean the information about which constants v, c, of M\u03c0 = Mv,c,\u03c0 become revealed at which stage (note that we assume D to be known from the beginning). For the scenario at hand, we assume the following revelation structure: at the beginning of the experimental stage we get to know v, i.e., the Vm that will be executed in the target setup; the concurrent workload Con in the target setup, i.e. c, become visible only during the target stage.\nNow we can formally express the above mentioned goal as follows: infer p(perf |v, c, \u03c0) for all \u03c0 in a smart way, using the experimental platform as well as the target setup.\nOur approach. We assume the following as given: the selection diagramD (e.g. based on Section 4.1) and the revelation structure. Keep in mind that, based on the selection diagram D, we have\np(perf |v, c, \u03c0) = (2) \u2211\nin,cpu,feat\np(perf |in, cpu, v) p(cpu|feat , \u03c0) p(in, feat|c).\nWe now describe an approach for the quintessential scenario described above, which we believe can be generalized to other relevant scenarios as well:\n1. Design and conduct of experiment: We know Vm of the target setup, i.e. v, when setting up the experiment and, additionally looking at D or equation (5), we conclude that the information which is relevant and can at most be inferred during the experiment is exactly p(perf |in, cpu, v). Therefore, we should conduct the experiment as follows: we should randomly vary In,Cpu during the experiment, record In,Cpu,Perf , which means sampling from p(perf |in, cpu, v), and based on this estimate the latter quantity.6 2. Inference in the target setup: We know Con of the target setup, i.e. c, only during the target stage and based on the above, we conclude that we have to estimate p(in, feat|c) on-line during that stage. 3. Calculating the desired quantity: p(perf |sIn , sCpu) can now be calculated based on equation (5)."}, {"heading": "4.3 Module 1C: addressing time-varying systems", "text": "Here we address a quintessential version of the second scenario described in Section 3.1: one has some \u201cinitial\n6More formally, one could also design the experiment based on writing down requirements regarding the selection diagram induced by target together with experimental system. We leave such investigations for future work.\nsetup\u201d of a system and some different \u201ctarget setup\u201d, and the goal is to harness information from the former for \u201coptimal\u201d control on the latter. Due to space constraints, here we only give a summary of our approach, while a more detailed version can be found in Section A. Again, we assume that the variation of the system is encoded by the selection diagramD (Figure 2) and this D is given (e.g. based on Section 4.1). The intuition is that the concurrent workload Con changes between these setups in an unpredictable way, while Vm is invariant. Let v be the invariant state of VM , and c be the state of Con in the target setup. Then, similarly as in Section 4.2, the idea is to infer p(perf |in, cpu, v) in the initial setup, then infer only p(in, feat|c) in the target setup, and based on this, calculate p(perf |v, c, \u03c0) based on equation (5)."}, {"heading": "4.4 Combination of modules and discussion", "text": "Combining the modules. One first has to infer a selection diagram based on Section 4.1. This then serves as input for Section 4.2, if one wants to design and integrate experiments; or as an input to Section 4.3, if one wants to integrate past experience of a time-varying system. In principle, for more complex structures, it is also possible to combine both.\nAdvantages of module 1B. First note that our approach in fact solves the problem of expensive or impossible exploration during production (mentioned as motivation for the first scenario in Section 3.1): the performance properties of the VM do not have to be explored during production. Second, without a principled and explicit modeling approach e.g. based on causal models, which provide terms such as \u201ccausal sufficiency\u201d or \u201ccauses of a variable\u201d, it is neither clear how to design the experiment (i.e., what variables to randomly intervened upon), nor what to measure and regress on. Therefore previous approaches which are not based on causal models [Chiang et al., 2014, Zheng et al., 2009] are prone to errors. For instance one may forget to vary In during the experiment, or forget to regress on it, which would lead to wrong predictions for the target setup: for instance a high concurrent workload Con slows down the performance of the VM, Perf , via In as well as Cpu, but if we do not vary and regress on In in the experiment, we will only predict that part of the slowdown that is produced via Cpu. Also regressing on children of Perf (w.r.t. D) can lead to wrong results. Note that this complies with previous research [Scho\u0308lkopf et al., 2012] that suggests that predictions from causes to effects are better transferable than vice versa. Keep in mind that our approach is also applicable to scenarios where Con would stand for a hidden confounder with\nan unknown distribution parameter Oth. Generally, one could try to learn (in the sense of machine learning [Bishop, 2006]) things such as how to perform and integrate the experiment, but one would always have to rely on prior assumptions.\nAdvantages of of module 1C. The main advantage of our method, compared to e.g. [Padala et al., 2009] which is based on adaptive control, is that one can encode and harness prior knowledge about which mechanisms vary and which stay invariant. A general advantage of causal models seems to be that they allow to express and reason about why distributions change, and not only observe that they change. Note that there might be other reasonable approaches to the scenario described above, e.g. based on treating the evolution of Con as a stochastic process.\nRemark. Recall that approaches such as RL often rely on the assumption of invariant dynamics (Section 2). In principle, the one-step dynamics of a fixed \u201csingle\u201d computer can in fact be seen as invariant (if one observes input, memory state, etc.). For instance, in the above scenario, p(perf |do (cpu), vm, con) as a function of all variables cpu, vm, con is invariant. Neglecting the difficulties this would bring along in terms of sampling and interpolation, one could in principle try to infer this invariant dynamics and then causal models would be superfluous. However, the long-term dynamics of a large-scale system, e.g. a complete data center, is rarely invariant (unless one would include time itself into the observation but this would make interpolation difficult), due to its interaction with varying components in the environment, e.g. the Internet, and changing hardware components. And usually cost functions depend on the long-term, large-scale dynamics. It seems that in contrast to usual RL, causal models are applicable to this continuum between \u201cinvariant\u201d and \u201ccompletely varying\u201d."}, {"heading": "5 Addressing problem 2: observation-level performance attribution and debugging", "text": "Keep in mind the problem formulation in Section 3.2. Here we elaborate the approach which we already outlined in that section, structuring it into \u201cmodules\u201d which we then combine and discuss in Section 5.3."}, {"heading": "5.1 Module 2A: formalizing attribution and debugging statements", "text": "Here we examine how the concepts \u201cperformance attribution\u201d7 and \u201cperformance debugging\u201d can be formalized based on causal modeling language. We deem both concepts closely related: attribution can be a crucial step towards debugging. Note that we do not claim that our proposed formalization is the only sensible one.\nWe assume the following form of a performance attribution statement: \u201cPerformance Y being y instead of y0 can be attributed to X having value x instead of the base case x0.\u201d We translate this into the following counterfactual statement: \u201cPerformance Y would have been y0 if X had been x0 given in fact we observed, among others, X = x, Y = y.\u201d This in turn we translate into the structural counterfactual probability p(Ydo X=x0 = y0|x, y, f), where F denotes some potential additional information about the situation in which the above statements are done. For a performance debugging statement we assume the following form: \u201cIt would improve performance Y from the current y to y\u2032, if instead of its current state x, we would setX to x\u2032.\u201d Stated this way, it seems natural to translate this statement into the structural counterfactual probability p(YdoX=x\u2032 = y\n\u2032|x, y, f), where F is meant as above.\nGenerally, attribution statements do not have to have a counterfactual form (e.g., one can also attribute hypothetical performance), and also debugging statements may have different forms. However, here we focus on scenarios where one observes a specific performance of a system at a specific time point, and would like to understand the current situation and possibly improve it soon. Also, keep in mind that we assume a pragmatic interpretation of counterfactual statements: an idea of what would have happened in some past situation can also include predictions about what will happen in similar situations in the future."}, {"heading": "5.2 Module 2B: approximate structural counterfactuals", "text": "Even though computing systems are \u201cmore deterministic\u201d than many other systems, due to interactions with the environment and missing information, one usually can only infer a GCM, and not a FCM, of a computer system. Without an FCM though, counter-\n7Clearly, \u201cattribution\u201d may have various meanings and, in particular, a correct causal model may help for attribution in various ways.\nfactual probabilities (equation (5.1)) are generally not uniquely determined, i.e., they cannot be derived from a GCM. Here we show that nonetheless counterfactual probabilities can be calculated approximately, and one can know, from only the GCM, how wrong the approximation is at most (on average).\nLet M be a GCM over variable set V , and W \u2282 V its root nodes. We define the approximate structural counterfactual or approximate counterfactual, i.e., the approximate probability of Y being y, had X been x, given evidence E = e, relative to W as\npW (Ydo X=x = y|e) := \u2211\nw\np(y|do (x), w)p(w|e). (3)\nLet D(\u00b7\u2016\u00b7) denote the Kullback-Leibler divergence, and H(\u00b7|\u00b7) the conditional Shannon entropy [Cover and Thomas, 1991]. Now we bound the average deviation between the structural and the approximate counterfactual by H(E|W ), which is calculable from only the GCM M . In particular, we show that our approximation gets arbitrarily close to the structural counterfactual when H(E|W ) goes to 0.\nProposition 1. Let M0 be a FCM over a set of discrete variables V , that induces a GCM M , and let W \u2282 V denote the root nodes in M . For all (sets of) variables E,X, Y we have\nD(p(Ydo X=x|E)\u2016p W (YdoX=x|E)) \u2264 H(E|W ) (4)\n(where p(YdoX=x|E) is defined w.r.t. M0 and pW (Ydo X=x|E) w.r.t. M).\nTo give some intuition about the approximate counterfactual and the proposition, we consider two special cases: If M is already a \u201cFCM\u201d in the sense that all its variables are completely determined by the root nodes, then we have H(E|W ) = 0, and thus, based on equation (7), both quantities coincide, which seems natural. If the evidence comprises the root nodes, W \u2282 E, then the approximation amounts to the simple conditional p(y|do (x), w) (where w is the part of e the corresponds to W ), similar as if we had evidence on all background variables in a FCM. We prove (using monotonicity of the Kullback-Leibler divergence) a generalization of Proposition 1 in Section B. Note that the idea of a counterfactual definition based on only the GCM has been mentioned in [Pearl, 2000, Section 7.2.2], but not been further investigated. In principle, it seems, one could also apply a similar approach as in [Balke and Pearl, 1994], i.e., considering all possible FCMs that explain given facts and then looking at their implications, to the problem of counterfactual inference from only a GCM. Note that, depending on the specific setting and the available information, there\nmay be more suitable quantities as the approximate counterfactual to encode counterfactual-like probabilities."}, {"heading": "5.3 Combination of modules and discussion", "text": "Combining the modules. To summarize, we propose the following approach, given a performance attribution or debugging question Q:\n1. Infer a GCM M , based on Section 4.1. Let W denote its root nodes. 2. Translate Q into a counterfactual probability query p(Ydo X=x\u2032 = y\n\u2032|x, y, f), as described in Section 5.1. 3. Calculate the approximate answer pW (YdoX=x\u2032 = y\u2032|x, y, f) from the GCMM , based on Section 5.2, if H(E|W ) is small.\nAdvantages. Previous approaches [Ostrowski et al., 2011, Snee et al., 2015] which do not use a rigorous and explicit modeling and reasoning framework such as causal models may be more prone to making errors, such as not distinguishing between correlation and causation, in certain cases. Furthermore, formalization can be an important step towards more automated treatment of attribution and debugging. Note that generally, if there is a known, invariant distribution over the root nodes of a causal model, then one can optimize the performance on the population level, similarly as suggested in Section 4. However, if one is uncertain about the distribution of root nodes, or this distribution are time-varying in an unpredictable way, but one still believes in the invariance of the other mechanisms, then the proposed observation-level attribution and debugging can be seen as a suitable tool, since the distributions on the root nodes are either updated or completely determined, by the observation."}, {"heading": "6 Some implementation aspects", "text": "While the main focus of the paper is on the conceptual level, here we want to discuss two implementationrelated points: the general problem of measurements on a computer system, and an example of how the detailed structure and data of a real system looks like.\nNotes on measurements. Measurements of computer systems were discussed e.g. by Carata et al. [2014], Snee et al. [2015], and we build on this work for our example system below. Two additional points need to be mentioned: First, care needs to be taken that the measurement process itself does not signif-\nLat\nUcKc\nUsoKso\nLocal\nKxo Uxo\nXb Xy\nCon vm\nFigure 3: Preliminary causal diagram GE .\n0 50 100 150 200 250 300 350 0\n1\n2\n3 \u00b7108\nFigure 4: X-axis: Xb; y-axis: Uxo. Test points in gray, GP posterior mean in blue.\nicantly incure the performance of the system. Second, the process of deciding which variables to measure and which not is difficult but central, in particular for sccessful debugging. In the theory of causal models [Pearl, 2000], this process is usually completely ignored and it is simply assumed that a sensible set of variables is given. In the practice of computer systems, this process has been studied, but it has not been investigated how causal considerations can guide measurement selection (see Section 4.4).\nExample: system, diagram and data. To illustrate the structure and data of a real system, we concisder a physical server with a Xen hypervisor [Barham et al., 2003]. We run a web server within one specific VM and measure its latency, Lat, (an important performance measure) upon requests, while also measuring the number of concurrent VMs running on the physical server, Con vm, workload within the VM, parallel to the web server, Local, and several other properties within the VM (Kso,Uso,Kc,Uc) as well as properties of the Xen hypervisor (variables in Figure 3 that contain an \u201cx\u201d). A preliminary causal diagram GE of the experimental system is depicted in Figure 3. For illustration purposes, we pick one variable, Uxo (\u201ccycles the VM is scheduled out while\nexecution was in user mode\u201d), and plot a sample of it against one of its parents, Xb (\u201cnumber of times the target VM has been scheduled out by Xen due to a block\u201d) in Figure 4. To show that, in principle, inference of these kind of mechanisms is possible, we also plot the posterior mean of a Gaussian process (GP), which was trained on a separate training set."}, {"heading": "7 Related work", "text": "To our knowledge, this is the first work that uses causal models as introduced by Pearl [2000], Spirtes et al. [2000] to address control and performance debugging problems in cloud computing, or in complex computer systems in general. Ostrowski et al. [2011], Snee et al. [2015] study the problem of performance attribution and debugging of cloud systems, but without causal models, which, as briefly discussed in Section 5.3, can lead to errors. Baah et al. [2010] apply causal models for debugging of programs, but not of entire computer systems. Bottou et al. [2013], Bareinboim et al. [2015] apply causal models to RL problems, as we do. However, their models are not applicable to the scenario considered in Section 4. For instance, they do not address the problem of designing, and transferring knowledge from a separate experimental (computer) system for shaping controllers. Previous work that does consider this problem [Chiang et al., 2014, Zheng et al., 2009], or the problem of time-dependence [Padala et al., 2009], is not based on causal models and we explained in Section 4.4 why this can be problematic."}, {"heading": "8 Conclusions", "text": "The present work translates several important problems from the field of cloud computing into causal inference terminology such as selection diagrams and counterfactuals. Based on this, we propose methods for inferring controllers from experimental and time-varying systems; and for observation-level performance attribution and debugging. Our work forms only a start of what we consider a fruitful direction, and leaves us with several open problems as well as suggestions for future work. It would be highly desirable to develop tests, based on causal and/or probabilistic methods, to check how accurate a model is, or whether and what kind of additional measurements should be taken. Moreover, due to strong intertemporal influences in real systems, temporal models should have the potential of being more accurate.\nAppendix"}, {"heading": "A Elaboration of Section 4.3 (addressing time-varying systems)", "text": "This is an elaborated version of Section 3.1. Recall that we address a quintessential version of the second scenario described in Section 3.1: one has some \u201cinitial setup\u201d of a system and some different \u201ctarget setup\u201d, and the goal is to harness information from the former for \u201coptimal\u201d control on the latter. (Note that this can be extended to more than two stages.) Recall the following equation:\np(perf |v, c, \u03c0) = \u2211\nin,cpu,feat\np(perf |in, cpu, v) p(cpu|feat, \u03c0) p(in, feat|c). (5)\nScenario, selection diagram and revelation structure. We consider a similar scenario as in Section 4.2, in particular with the same selection diagram D, except for the following differences: there is no experimental platform, but there are two episodes during production, i.e., the target platform is run with two different setups, which we refer to as \u201cinitial setup\u201d and \u201ctarget setup\u201d. The intuition is simply that the concurrent workload (applications) change between these setups. The goal, semi-formally, is to predict (the distribution) of Perf from Feat, for all possible decision Cpu, in the target setup, by making use of the target as well as the initial setup (in a smart way). We assume that Vm is invariant between both setups and we denote its state by v. The concurrent applications Con in the target setup we denote by c. That is, we assume the following revelation structure: v is given from the beginning; c is revealed at the beginning of the target episode. Now, formally expressed, the goal is to infer p(perf |v, c, \u03c0) (for all \u03c0).\nOur approach. We assume the following as given: the selection diagram D and the revelation structure. We now describe an approach for the scenario described above, which we belief can be generalized to other relevant scenarios as well.\n1. Design of, and inference in, initial setup: For the initial policy, chose a \u03c0 such that p(cpu|feat, \u03c0) has support everywhere, for any feat. Based on this, estimate p(perf |in, cpu, v). (Note that we assume here that all other mechanisms have support every as well, to keep things simple, but the approach can be extended to other cases.)\n2. Inference in target setup: Infer p(in, feat|c).\n3. Calculating the desired quantity: Now calculate p(perf |v, c, \u03c0) based on equation (5)."}, {"heading": "B Generalized version and proof of Proposition 1", "text": "Here we state and prove a generalization of Proposition 1.\nProposition 2 (Generalization of Proposition 1). Let M0 be a FCM over discrete variables that induces a GCM M . Let the triple (X,Y, Z) of (sets of) variables in M be such that (Y \u22a5 An(Z)|Z)M (i.e., are d-separated [Pearl, 2000]) and X does not influence W := Z \\X. Let E be an arbitrary set of variables in M . Let (slightly generalizing the definition from the main text):\npW (Ydo X=x = y|e) := \u2211\nw\np(y|doX = x,w)p(w|e). (6)\nThen\nD(p(Ydo X=x|E)\u2016p W (YdoX=x|E)) \u2264 H(E|W ) (7)\n(where p(Ydo X=x|E) is defined w.r.t. M0 and p W (YdoX=x|E) w.r.t. M).\nThis is a generalization of Proposition 1 since, whenever Z is a set of root nodes in M , the conditions (Y \u22a5 An(Z)|Z)M and X does not influence W := Z \\X are necessarily met.\nProof. Let U1 be the set (tuple) of background variables that influence W and U0 = U \\ U1. Then\npW (YdoX=x = y|e) (8)\n= \u2211\nw\np(y|doX = x,w)p(w|e) (9)\n= \u2211\nw,u0\np(y|doX = x,w, u0)p(w|e)p(u0|doX = x,w) (10)\n= \u2211\nw,u0\np(y|doX = x,w, u0)p(w|e)p(u0|w) (11)\n= \u2211\nw,u0\np(y|doX = x,w, u0)p(w|e)p(u0), (12)\nwhere equation (11) is due to the fact that X neither influences U0 nor W .\nOn the other hand, we have\np(YdoX=x = y|e) (13)\n= \u2211\nu:p(u,e)>0\np(y|doX = x, u)p(u|e) (14)\n= \u2211\nu0,u1:p(u0,u1,e)>0\np(y|doX = x, u0, u1)p(u0, u1|e) (15)\n= \u2211\nu0,u1,w:p(u0,u1,e)>0\np(y, w|doX = x, u0, u1)p(u0, u1|e) (16)\n= \u2211\nu0,u1,w:p(u0,u1,e),p(u0,u1,w)>0\np(y, w|doX = x, u0, u1)p(u0, u1|e) (17)\n= \u2211\nu0,u1,w:p(u0,u1,e),p(u0,u1,w)>0\np(y|doX = x, u0, u1, w)p(w|doX = x, u1, u0)p(u0, u1|e) (18)\n= \u2211\nu0,u1,w:p(u0,u1,e),p(u0,u1,w)>0\np(y|doX = x, u0, w)p(w|doX = x, u1, u0)p(u0, u1|e) (19)\n= \u2211\nu0,u1,w:p(u0,u1,e),p(u0,u1,w)>0\np(y|doX = x, u0, w)p(w|u1, u0)p(u0, u1|e) (20)\n= \u2211\nu0,u1,w:p(u0,u1,e),p(u0,u1,w)>0\np(y|doX = x, u0, w)p(w|u0, u1, e)p(u0, u1|e) (21)\n= \u2211\nu0,u1,w:p(u0,u1,e),p(u0,u1,w)>0\np(y|doX = x, u0, w)p(w, u0, u1|e) (22)\n= \u2211\nu0,u1,w:p(u0,u1,e,w)>0\np(y|doX = x, u0, w)p(w, u0, u1|e) (23)\n= \u2211\nu0,w:p(u0,e,w)>0\np(y|doX = x, u0, w) \u2211\nu1 :p(u0,u1,e,w)>0\np(w, u0, u1|e) (24)\n= \u2211\nu0,w:p(u0,e,w)>0\np(y|doX = x, u0, w)p(w, u0|e) (25)\n= \u2211\nu0,w\np(y|doX = x, u0, w)p(w, u0|e), (26)\nwhere equation (19) is due to Markovianity and (Y \u22a5 An(Z)|Z)M , which implies (Y \u22a5 U1|Z)M , and thus (Y \u22a5 U1|W )Mdo X=x , equation (20) follows from the fact that X does not influence W , equation (21) follows from the fact that U1 already determines W .\nNote that p(w|e)p(u0) = 0 implies p(w, u0|e) = 0 and therefore D[p(Ydo X=x|E)\u2016p W (YdoX=x|D)] is defined .\nNow we can calculate\nD[p(Ydo X=x|E)\u2016p W (YdoX=x|E)] (27)\n= \u2211\ne\np(e)D[p(Ydo X=x|e)\u2016p W (Ydo X=x|e)] (28)\n\u2264 \u2211\ne\np(e)D[p(W,U0|e)\u2016p(W |e)p(U0)] (29)\n= \u2211\ne,w,u0\np(w, u0, e) log p(w, u0|e)\np(w|e)p(u0) (30)\n= \u2211\ne,w,u0\np(w, u0, e) log p(w, u0, e)\np(w, e)p(u0) (31)\n= I(W,E : U0) (32)\n= I(W : U0) + I(E : U0|W ) (33)\n= 0 + H(E|W ) \u2212H(E|U0,W ), (34)\nwhere inequality (29) follows from the monotonicity (which follows from the chain rule) of the Kullback-Leibler divergence [Cover and Thomas, 1991] together with equations (26) and (12), equation (33) is the chain rule for mutual information, and I(W : U0) = 0 is due to U0 not influencing W and Markovianity."}], "references": [{"title": "A view of cloud computing", "author": ["M. Armbrust", "A. Fox", "R. Griffith", "A.D. Joseph", "R. Katz", "A. Konwinski", "G. Lee", "D. Patterson", "A. Rabkin", "I. Stoica"], "venue": "Communications of the ACM,", "citeRegEx": "Armbrust et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Armbrust et al\\.", "year": 2010}, {"title": "Causal inference for statistical fault localization", "author": ["G.K. Baah", "A. Podgurski", "M.J. Harrold"], "venue": "In Proceedings of the 19th international symposium on Software testing and analysis,", "citeRegEx": "Baah et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baah et al\\.", "year": 2010}, {"title": "Counterfactual probabilities: Computational methods, bounds and applications", "author": ["A. Balke", "J. Pearl"], "venue": "In Proceedings of the Tenth international conference on Uncertainty in artificial intelligence,", "citeRegEx": "Balke and Pearl.,? \\Q1994\\E", "shortCiteRegEx": "Balke and Pearl.", "year": 1994}, {"title": "Transportability of causal effects: Completeness results", "author": ["E. Bareinboim", "J. Pearl"], "venue": "In Proceedings of the 26th National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Bareinboim and Pearl.,? \\Q2012\\E", "shortCiteRegEx": "Bareinboim and Pearl.", "year": 2012}, {"title": "Bandits with unobserved confounders: A causal approach", "author": ["E. Bareinboim", "A. Forney", "J. Pearl"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bareinboim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bareinboim et al\\.", "year": 2015}, {"title": "Xen and the art of virtualization", "author": ["P. Barham", "B. Dragovic", "K. Fraser", "S. Hand", "T. Harris", "A. Ho", "R. Neugebauer", "I. Pratt", "A. Warfield"], "venue": "In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles,", "citeRegEx": "Barham et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barham et al\\.", "year": 2003}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L. Bottou", "J. Peters", "J. Quinonero-Candela", "D.X. Charles", "D.M. Chickering", "E. Portugaly", "D. Ray", "P. Simard", "E. Snelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Resourceful: fine-grained resource accounting for explaining service variability", "author": ["L. Carata", "O. Chick", "J. Snee", "R. Sohan", "A. Rice", "A. Hopper"], "venue": "Technical report,", "citeRegEx": "Carata et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Carata et al\\.", "year": 2014}, {"title": "Matrix: Achieving predictable virtual machine performance in the clouds", "author": ["R.C. Chiang", "J. Hwang", "H.H. Huang", "T. Wood"], "venue": "In 11th International Conference on Autonomic Computing (ICAC", "citeRegEx": "Chiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chiang et al\\.", "year": 2014}, {"title": "Elements of Information Theory", "author": ["T. Cover", "J. Thomas"], "venue": "Wileys Series in Telecommunications,", "citeRegEx": "Cover and Thomas.,? \\Q1991\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 1991}, {"title": "An inquiry concerning human understanding, volume 49", "author": ["D. Hume", "C.W. Hendel"], "venue": "Bobbs-Merrill Indianapolis,", "citeRegEx": "Hume and Hendel.,? \\Q1955\\E", "shortCiteRegEx": "Hume and Hendel.", "year": 1955}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Langford and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "Distinguishing cause from effect using observational data: methods and benchmarks", "author": ["J.M. Mooij", "J. Peters", "D. Janzing", "J. Zscheischler", "B. Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1412.3773,", "citeRegEx": "Mooij et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mooij et al\\.", "year": 2014}, {"title": "Diagnosing latency in multi-tier black-box services", "author": ["K. Ostrowski", "G. Mann", "M. Sandler"], "venue": "In 5th Workshop on Large Scale Distributed Systems and Middleware (LADIS 2011),", "citeRegEx": "Ostrowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ostrowski et al\\.", "year": 2011}, {"title": "Automated control of multiple virtualized resources", "author": ["P. Padala", "K.-Y. Hou", "K.G. Shin", "X. Zhu", "M. Uysal", "Z. Wang", "S. Singhal", "A. Merchant"], "venue": "In Proceedings of the 4th ACM European conference on Computer systems,", "citeRegEx": "Padala et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Padala et al\\.", "year": 2009}, {"title": "Causal inference in statistics: An overview", "author": ["J. Pearl"], "venue": "Statistics Surveys,", "citeRegEx": "Pearl.,? \\Q2009\\E", "shortCiteRegEx": "Pearl.", "year": 2009}, {"title": "On causal and anticausal learning", "author": ["B. Sch\u00f6lkopf", "D. Janzing", "J. Peters", "E. Sgouritsa", "K. Zhang", "J. Mooij"], "venue": "arXiv preprint arXiv:1206.6471,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2012}, {"title": "Soroban: attributing latency in virtualized environments", "author": ["J. Snee", "L. Carata", "O.R. Chick", "R. Sohan", "R.M. Faragher", "A. Rice", "A. Hopper"], "venue": "In Proceedings of the 7th USENIX Conference on Hot Topics in Cloud Computing,", "citeRegEx": "Snee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Snee et al\\.", "year": 2015}, {"title": "Causation, prediction, and search", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": null, "citeRegEx": "Spirtes et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Spirtes et al\\.", "year": 2000}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Dejavu: accelerating resource allocation in virtualized environments", "author": ["N. Vasi\u0107", "D. Novakovi\u0107", "S. Miu\u010din", "D. Kosti\u0107", "R. Bianchini"], "venue": "In ACM SIGARCH computer architecture news,", "citeRegEx": "Vasi\u0107 et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vasi\u0107 et al\\.", "year": 2012}, {"title": "Justrunit: Experiment-based management of virtualized data centers", "author": ["W. Zheng", "R. Bianchini", "G.J. Janakiraman", "J.R. Santos", "Y. Turner"], "venue": "In Proc. USENIX Annual technical conference,", "citeRegEx": "Zheng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, the paradigm and business model of cloud computing [Armbrust et al., 2010] has become increasingly popular.", "startOffset": 68, "endOffset": 91}, {"referenceID": 16, "context": "We first define causal models on a mathematical level, following Pearl [2000], Spirtes et al.", "startOffset": 65, "endOffset": 78}, {"referenceID": 16, "context": "We first define causal models on a mathematical level, following Pearl [2000], Spirtes et al. [2000]. We consider variables with discrete as well as continuous domains, and by a (probability) density we either refer to a density w.", "startOffset": 65, "endOffset": 101}, {"referenceID": 3, "context": "The idea behind selection diagrams [Bareinboim and Pearl, 2012] is to allow to graphically encode how the causal structure of two or more populations (or systems) relate to each other by expressing which mechanisms vary and which are invariant.", "startOffset": 35, "endOffset": 63}, {"referenceID": 20, "context": "Reinforcement learning (RL) [Sutton and Barto, 1998] is concerned with designing automated agents which perform actions (or decisions) that optimize some given long-term reward or utility.", "startOffset": 28, "endOffset": 52}, {"referenceID": 12, "context": "ity, we often neglect inter-temporal dependences, and thus focus on what is usually referred to as contextual bandits [Langford and Zhang, 2008].", "startOffset": 118, "endOffset": 144}, {"referenceID": 22, "context": ", 2012] or in parallel [Zheng et al., 2009] to production.", "startOffset": 23, "endOffset": 43}, {"referenceID": 15, "context": "in [Padala et al., 2009], using adaptive control methods.", "startOffset": 3, "endOffset": 24}, {"referenceID": 18, "context": "In cloud computing, performance attribution and debugging is, due to the complexity of the system, non-trivial but crucial: for instance, in case of a poor performance of some application running in a VM on a cloud server, one wants to understand whether this problem has to be attributed to the configuration of the VM (which one could change directly) or the configuration of the server or other VMs on the server (in which case one may have to buy a more expensive cloud product) [Snee et al., 2015].", "startOffset": 483, "endOffset": 502}, {"referenceID": 16, "context": "Important additional input to causal model inference often comes from domain knowledge of experts [Pearl, 2009].", "startOffset": 98, "endOffset": 111}, {"referenceID": 3, "context": "Regarding selection diagrams, we first note that their mathematical definition (Section 2, based on [Bareinboim and Pearl, 2012]) takes complete causal models of various domains as input.", "startOffset": 100, "endOffset": 128}, {"referenceID": 11, "context": "Clearly, the theory of causal model inference faces the classical problem of induction [Hume and Hendel, 1955].", "startOffset": 87, "endOffset": 110}, {"referenceID": 9, "context": "Work such as [Chiang et al., 2014] do not assume to know the exact VM in advance but only a \u201csimilar\u201d one.", "startOffset": 13, "endOffset": 34}, {"referenceID": 17, "context": "Note that this complies with previous research [Sch\u00f6lkopf et al., 2012] that suggests that predictions from causes to effects are better transferable than vice versa.", "startOffset": 47, "endOffset": 71}, {"referenceID": 6, "context": "Generally, one could try to learn (in the sense of machine learning [Bishop, 2006]) things such as how to perform and integrate the experiment, but one would always have to rely on prior assumptions.", "startOffset": 68, "endOffset": 82}, {"referenceID": 15, "context": "[Padala et al., 2009] which is based on adaptive control, is that one can encode and harness prior knowledge about which mechanisms vary and which stay invariant.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "Let D(\u00b7\u2016\u00b7) denote the Kullback-Leibler divergence, and H(\u00b7|\u00b7) the conditional Shannon entropy [Cover and Thomas, 1991].", "startOffset": 94, "endOffset": 118}, {"referenceID": 2, "context": "In principle, it seems, one could also apply a similar approach as in [Balke and Pearl, 1994], i.", "startOffset": 70, "endOffset": 93}, {"referenceID": 8, "context": "by Carata et al. [2014], Snee et al.", "startOffset": 3, "endOffset": 24}, {"referenceID": 8, "context": "by Carata et al. [2014], Snee et al. [2015], and we build on this work for our example system below.", "startOffset": 3, "endOffset": 44}, {"referenceID": 5, "context": "To illustrate the structure and data of a real system, we concisder a physical server with a Xen hypervisor [Barham et al., 2003].", "startOffset": 108, "endOffset": 129}, {"referenceID": 15, "context": ", 2009], or the problem of time-dependence [Padala et al., 2009], is not based on causal models and we explained in Section 4.", "startOffset": 43, "endOffset": 64}, {"referenceID": 10, "context": "To our knowledge, this is the first work that uses causal models as introduced by Pearl [2000], Spirtes et al.", "startOffset": 82, "endOffset": 95}, {"referenceID": 10, "context": "To our knowledge, this is the first work that uses causal models as introduced by Pearl [2000], Spirtes et al. [2000] to address control and performance debugging problems in cloud computing, or in complex computer systems in general.", "startOffset": 82, "endOffset": 118}, {"referenceID": 10, "context": "Ostrowski et al. [2011], Snee et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Ostrowski et al. [2011], Snee et al. [2015] study the problem of performance attribution and debugging of cloud systems, but without causal models, which, as briefly discussed in Section 5.", "startOffset": 0, "endOffset": 44}, {"referenceID": 1, "context": "Baah et al. [2010] apply causal models for debugging of programs, but not of entire computer systems.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Baah et al. [2010] apply causal models for debugging of programs, but not of entire computer systems. Bottou et al. [2013], Bareinboim et al.", "startOffset": 0, "endOffset": 123}, {"referenceID": 1, "context": "Baah et al. [2010] apply causal models for debugging of programs, but not of entire computer systems. Bottou et al. [2013], Bareinboim et al. [2015] apply causal models to RL problems, as we do.", "startOffset": 0, "endOffset": 149}, {"referenceID": 10, "context": "where inequality (29) follows from the monotonicity (which follows from the chain rule) of the Kullback-Leibler divergence [Cover and Thomas, 1991] together with equations (26) and (12), equation (33) is the chain rule for mutual information, and I(W : U0) = 0 is due to U0 not influencing W and Markovianity.", "startOffset": 123, "endOffset": 147}], "year": 2017, "abstractText": "Two challenges in the theory and practice of cloud computing are: (1) smart control (allocation) of resources under exploration constraints, on time-varying systems, and (2) understanding and debugging of the performance of complex systems that involve e.g. virtualization. In this paper, we examine how these challenges can be approached using causal models. For challenge (1) we investigate how to infer and use causal models and selection diagrams to design and integrate experiments in a principled way, as well as to cope with partially varying systems. For challenge (2) we examine how to formalize performance attribution and debugging questions by counterfactual probabilities, and how to approximately answer them based on inferred (non-deterministic) graphical causal models.", "creator": "LaTeX with hyperref package"}}}