{"id": "1603.03144", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2016", "title": "Part-of-Speech Tagging for Historical English", "abstract": "With the rise of digital humanities research, natural language processing for historical texts is of increasing interest. However, directly applying standard language processing tools to historical texts often yields unsatisfactory performance, due to language change and genre differences. Spelling normalization is the dominant solution, but it fails to account for changes in usage and vocabulary. In this empirical paper, we assess the capability of do- main adaptation techniques to cope with historical texts, focusing on the classic bench- mark task of part-of-speech tagging. We empirically evaluate several domain adaptation methods on the task of tagging two million- word treebanks of the Penn Corpora of Historical English. We demonstrate that domain adaptation significantly outperforms spelling normalization when adapting modern taggers to older texts, and that domain adaptation is complementary with spelling normalization, yielding better results in combination.", "histories": [["v1", "Thu, 10 Mar 2016 04:27:15 GMT  (29kb)", "http://arxiv.org/abs/1603.03144v1", "Accepted to NAACL 2016"], ["v2", "Mon, 4 Apr 2016 16:59:38 GMT  (31kb)", "http://arxiv.org/abs/1603.03144v2", "Accepted to NAACL 2016"]], "COMMENTS": "Accepted to NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.DL", "authors": ["yi yang", "jacob eisenstein"], "accepted": true, "id": "1603.03144"}, "pdf": {"name": "1603.03144.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yiyang+jacobe@gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n03 14\n4v 1\n[ cs\n.C L\n] 1\n0 M"}, {"heading": "1 Introduction", "text": "There is growing interest in applying natural language processing (NLP) techniques to historical texts (Piotrowski, 2012), with applications in information retrieval (Dougherty, 2010; Jurish, 2011), linguistics (Baron et al., 2009; Rayson et al., 2007), and the digital humanities (Hendrickx et al., 2011; Pettersson and Nivre, 2011). However, these texts differ from contemporary training corpora in a number of linguistic respects, including the lexicon (Giusti et al., 2007), morphology (Borin\nand Forsberg, 2008), and syntax (Eumeridou et al., 2004). This imposes significant challenges for modern NLP tools: for example, the accuracy of the CLAWS part-of-speech Tagger (Garside and Smith, 1997) drops from 97% on the British National Corpus to 82% on Early Modern English texts (Rayson et al., 2007). There are two main approaches that could improve the accuracy of NLP systems on historical texts: normalization and domain adaptation.\nNormalization Spelling normalization (also called canonicalization) involves mapping historical spellings to their canonical forms in modern languages, thus bridging the gap between contemporary training corpora and target historical texts. Figure 1 shows one historical sentence and its normalization by VARD (Baron and Rayson, 2008), a variant detector tool. Rayson et al. (2007) report an increase of about 3% accuracy on adaptation of POS tagging from Modern English texts to Early Modern English texts if the target texts were automatically normalized by the VARD system. However, normalization is not always a well-defined problem (Eisenstein, 2013), and it does not address the full range of linguistic changes over time, such as unknown words, morphological differences, and changes in the meanings of words (Kulkarni et al., 2015). In the example above, the word \u2018ryottours\u2019\nis not successfully normalized to \u2018rioters\u2019; the syntax is comprehensible to contemporary English speakers, but usages such as \u2018wild disposed\u2019 and \u2018drew unto\u2019 are sufficiently unusual as to pose problems for NLP systems trained on contemporary texts.\nDomain adaptation A more generic machine learning approach is to apply unsupervised domain adaptation techniques, which transform the representations of the training and target texts to be more similar, typically using feature co-occurrence statistics (Blitzer et al., 2006; Ben-David et al., 2010). It is natural to think of historical texts as a distinct domain from contemporary training corpora, and Yang and Eisenstein (2014, 2015) show that the accuracy of historical Portuguese POS tagging can be significantly improved by domain adaption; however, we are unaware of prior work that empirically evaluates the efficacy of this approach on Early Modern English texts. Furthermore, historical texts are often associated with multiple metadata attributes (e.g., author, genre, and epoch), each of which may influence the text\u2019s linguistic properties. Multi-domain adaptation (Mansour et al., 2009) and multi-attribute domain adaptation (Joshi et al., 2013; Yang and Eisenstein, 2015) can potentially exploit these metadata attributes to obtain further improvements.\nThis paper presents the first comprehensive empirical comparison of effectiveness of these approaches for part-of-speech tagging on historical texts. We focus on the two historical treebanks of the Penn Corpora of Historical English \u2014 the Penn Parsed Corpus of Modern British English (Kroch et al., 2010, PPCMBE) and the Penn-Helsinki Parsed Corpus of Early Modern English (Kroch et al., 2004, PPCEME). These datasets enable a range of analyses, which isolate the key issues in dealing with historical corpora:\n\u2022 In one set of analyses, we focus on the PPCMBE and the PPCEME corpora, training on more recent texts and testing on earlier texts. This isolates the impact of language change on tagging performance. \u2022 In another set of analyses, we train on the Penn Treebank (Marcus et al., 1993, PTB), and test on the historical corpora, using the tag map-\npings from Moon and Baldridge (2007). Our evaluation includes the well-known Stanford CoreNLP tagger (Manning et al., 2014), thus replicating the most typical situation for users of existing language technology. \u2022 We show that FEMA, a domain adaptation algorithm that is specifically designed for sequence labeling problems (Yang and Eisenstein, 2015), achieves an increase of nearly 4% accuracy when adapting from the PTB to the PPCEME. \u2022 We compare the impact of normalization with domain adaptation, and demonstrate that they are largely complementary. \u2022 Error analysis shows that the improvements obtained by domain adaptation are largely due to better handling of out-of-vocabulary (OOV) tokens. Many of the most frequent errors on in-vocabulary (IV) tokens are caused by mismatches in the tagsets or annotation guidelines, and may be difficult to address without labeled data in the target domain."}, {"heading": "2 Data", "text": "The Penn Corpora of Historical English consist of the Penn-Helsinki Parsed Corpus of Middle English, second edition (Kroch and Taylor, 2000, PPCME2), the Penn-Helsinki Parsed Corpus of Early Modern English (PPCEME), and the Penn Parsed Corpus of Modern British English (PPCMBE). The corpora are annotated with part-of-speech tags and syntactic parsing trees in an annotation style similar to that of the PTB. In this work, we focus on POS tagging the PPCMBE and the PPCEME.1\nThe Penn Parsed Corpus of Modern British English The PPCMBE is a syntactically annotated corpus of text, containing 948,895 words sampled from the period 1700-1914. It is divided into three 70-year time periods according to the composition date of the works. In contrast to the PTB, the PPCMBE contains text from a variety of genres, such as Bible, Drama, Fiction, and Letters.\n1Middle English is outside the scope of this paper, because it is sufficiently unintelligible to modern English speakers that texts such as Canterbury Tales are published in translation (e.g., Chaucer and Hopper, 2012). In tagging Middle English texts, Moon and Baldridge (2007) apply bitext projection techniques from multilingual learning, rather than domain adaptation.\nThe Penn-Helsinki Parsed Corpus of Early Modern English The PPCEME is a collection of text samples from the Helsinki Corpus as well as two supplements to the Helsinki Corpus, which consist of text material by the same authors and from the same editions as the material in the Helsinki Corpus. The corpus contains 1,737,853 words spanning from 1500 until 1710, and it is divided into three 70- year time periods similar to the PPCMBE corpus. The PPCEME consists of text from the same eighteen genres as the PPCMBE.\nPenn Treebank Release 3 The Penn Treebank (Marcus et al., 1993) is the de facto standard syntactically annotated corpus for English, which is used to train software such as Stanford CoreNLP (Manning et al., 2014). When using this dataset for supervised training, we follow Toutanova et al. (2003) and use WSJ sections 0-18 for training, and sections 19-21 for tuning. When applying unsupervised domain adaptation, we use all WSJ sections, together with texts from the PPCMBE and the PPCEME.\nTagsets The Penn Corpora of Historical English (PCHE) use a tagset that differs from the Penn Treebank, mainly in the direction of greater specificity. Auxiliary verbs \u2018do\u2019, \u2018have\u2019, and \u2018be\u2019 all have their own tags, as do words like \u2018one\u2019 and \u2018else\u2019, due to their changing syntactic function over time. Overall, there are 83 tags in the PPCEME, and 81 in the PPCMBE, as compared with 45 in the PTB. Furthermore, the tags in the PCHE tagset are allowed to join constituent morphemes in compounds, yielding complex tags such as PRO+N (e.g., \u2018himself\u2019) and ADJ+NS (e.g., \u2018gentlemen\u2019).\nIn order to measure the tagging accuracy of PTBtrained taggers on the historical texts, we follow Moon and Baldridge (2007), who define a set of deterministic mappings from the PCHE tags to the PTB tagset. For simplicity, we first convert each complex tag to the simple form by only considering the first simple tag component (e.g., PRO+N to PRO and ADJ+NS to ADJ). The conversion has little effect on the tagging performance, as the complex tags only cover slightly more than 1% of the tokens in the PCHE treebanks. Among the 83 tags, 74 mappings to the corresponding PTB tags are obtained from Moon and Baldridge (2007). We did our best\nto convert the other tags according to the tag description. For example, $ is mapped to PRP$, WARD is mapped to VB, and each punctuation tag is mapped to its identical tag. The complete list of mappings will be published in an appendix."}, {"heading": "3 Unsupervised Domain Adaptation", "text": "In typical usage scenarios, the user wants to tag some historical text but has no labeled data in the target domain (e.g., Muralidharan and Hearst, 2013). This best fits the paradigm of unsupervised domain adaptation, when labeled data from the source domain (e.g., the PTB) is combined with unlabeled data from the target domain. Representational differences between source and target domains can be a major source of errors in domain adaptation (BenDavid et al., 2010), and so several representation learning approaches have been proposed.\nThe most straightforward approach is to replace lexical features with word representations, such as Brown clusters (Brown et al., 1992; Lin et al., 2012) or word embeddings (Turian et al., 2010). This can assist in domain adaptation by linking out-ofvocabulary words to in-vocabulary words with similar distributional properties.\nA more general solution is Structural Correspondence Learning (Blitzer et al., 2006, SCL), in which we create artificial binary classification problems for thousands of cross-domain \u201cpivot\u201d features, and then use the weights from the resulting classifiers to project the instances into a new dense representation. This is similar to the (later) idea of skipgram word embeddings (Mikolov et al., 2013, word2vec), but can be applied to arbitrary features.\nWe also consider a recently-published approach called Feature Embedding (FEMA), which achieves the state-of-the-art results on several POS tagging adaptation tasks (Yang and Eisenstein, 2015). The intuition of FEMA is similar to prior work: it relies on co-occurrence statistics to link features across domains. Specifically, FEMA exploits the tendency of many NLP tasks to divide features into templates, and induces feature embeddings by using the features in each template to predict the active features in all other templates \u2014 just as the skipgram model learns word embeddings to predict neighboring words.\nA further advantage of FEMA is that it can perform multi-attribute domain adaptation, enabling it to exploit the many metadata attributes (e.g., year, genre, and author) that are often associated with historical texts. This is done by accounting for the specific impact of each domain attribute on the feature predictors, and then building a domain-neutral representation from the common substructure that is shared across all domain attributes. In the experiments that follow, we use genre and epoch as domain attributes."}, {"heading": "4 Experiments", "text": "We evaluate these unsupervised domain adaptation approaches on part-of-speech tagging for historical English (the PPCMBE and the PPCEME), in two settings: (1) temporal adaptation within each individual corpus, where we train POS taggers on the most modern data in the corpus and test on increasingly distant datasets; (2) adaptation of English POS tagging from modern news text to historical texts. The first setting focuses on temporal differences, and eliminates other factors that may impair tagging performance, such as different annotation schemes and text genres. The second setting is the standard and well-studied evaluation scenario for POS tagging, where we train on the Wall Street Journal (WSJ) text from the PTB and test on historical texts. In addition, we evaluate the effectiveness of the VARD normalization tool (Baron and Rayson, 2008) for improving POS tagging performance on the PPCEME corpus."}, {"heading": "4.1 Experimental Settings", "text": "The datasets used in the experiments are described in \u00a7 2. All the hyperparameters are tuned on development data in the source domain. In the case where there is no specific development dataset (adaptation within the historical corpora), we randomly sample 10% sentences from the training datasets for model selection."}, {"heading": "4.1.1 Baseline systems", "text": "We include two baseline systems for POS tagging: a classification-based support vector machine (SVM) tagger and a bidirectional maximum entropy Markov model (MEMM) tagger. Specifically, we use the L2-regularized L2-loss SVM\nimplementation in the scikit-learn package (Pedregosa et al., 2011) and L2-regularized bidirectional MEMM implementation provided by Stanford CoreNLP (Toutanova et al., 2003; Manning et al., 2014).\nFollowing Yang and Eisenstein (2015), we apply the feature templates defined by Ratnaparkhi (1996) to extract the basic features for all taggers. There are three broad types of templates: five lexical feature templates, eight affix feature templates, and three orthographic feature templates."}, {"heading": "4.1.2 Domain adaptation systems", "text": "We consider the unsupervised domain adaptation methods described in \u00a7 3: SCL, Brown clustering, word2vec2, and FEMA, which we train in both the single embedding mode (FEMA-single), where metadata attributes are ignored, and in multiattribute mode (FEMA-attribute), where metadata attributes are used. The domain adaptation models are trained on the union of the source and target datasets. Following Yang and Eisenstein (2015), we do not learn feature embeddings for the three orthographic feature templates: as each orthographic feature template represents only a binary value, it is unnecessary to replace it with a much longer numerical vector. The learned representations are then concatenated with the basic surface features to form the augmented representations. For computational reasons, the domain adaptation systems are all based on the SVM tagger; pilot studies showed that Viterbi tagging offers only minimal improvements."}, {"heading": "4.1.3 Parameter tuning", "text": "We choose the SVM regularization parameter by sweeping the range {0.1, 0.3, 0.5, 0.8, 1.0}. Following Blitzer et al. (2006), we consider pivot features that appear more than 50 times in all the domains for SCL. We empirically fix the number of singular vectors of the projection matrix K to 25, and also employ feature normalization and rescaling, as these settings yield best performance in prior work. The number of Brown clusters is chosen from {50, 100, 200, 400}. For FEMA and word2vec, we choose embedding sizes from {50, 100, 200, 300} and fix the numbers of negative samples to 15. The window size for training word embeddings is set as\n2https://code.google.com/p/word2vec/\n5. Finally, we adopt the same regularization penalty for all the attribute-specific embeddings of FEMA, which is selected from {0.01, 0.1, 1.0, 10.0}. All parameters were tuned on development data in the source domain. We train the Stanford MEMM tagger using the default configuration file."}, {"heading": "4.2 Temporal Adaptation", "text": "In the temporal adaptation setting, we work within each corpus, training on the most recent section, and evaluating on the two earlier sections. For PPCMBE, the source domain is the period from 1840 to 1914; for PPCEME, the source domain is the period from 1640 to 1710. All earlier texts are treated as target domains. We transform the tags to the PTB tagset for evaluation, so that results can be compared with the next experiment, in which the PTB is used for supervision.\nSettings We randomly sample 10% sentences from the training data as the development data for optimizing hyperparameters, and then retrain the models on the full training data using the best parameters. For FEMA, we consider domain attributes for 70-year temporal periods and genres, resulting in a total of 21 attributes for each corpus. The numbers of pivot features used in SCL are 4400 and 5048 for the PPCMBE and the PPCEME respectively. The best number of Brown clusters is 200, and the best embedding sizes are 200 and 100 for word2vec and FEMA.\nResults As shown in Table 1, accuracies are significantly improved by domain adaptation, es-\npecially for the PPCEME. English spelling had become mostly uniform and stable since around 1700 (Baron et al., 2009), which may explain why improvements on the PPCMBE are relatively modest, especially in the 1770-1839 epoch. Among the two baseline systems, MEMM performs slightly better than SVM, showing a small benefit to structured prediction. Among the domain adaptation algorithms, FEMA clearly outperforms SCL, Brown clustering and word2vec, with an averaged increase of about 0.5% and 1.5% accuracies on the PPCMBE and the PPCEME test sets respectively. The metadata attribute information boosts performance by a small but consistent margin, 0.1-0.2% on average."}, {"heading": "4.3 Adaptation from the Penn Treebank", "text": "Newspaper text is the primary data source for training modern NLP systems. For example, most of \u201coff-the-shelf\u201d English POS taggers (e.g., Stanford Tagger (Toutanova et al., 2003), SVMTool (Gime\u0301nez and Marquez, 2004), and CRFTagger (Phan, 2006)) are trained on the WSJ portion of the Penn Treebank, which is composed of professionally-written news text from 1989. This motivates this evaluation scenario, in which we train the tagger on the Penn Treebank WSJ data and apply it to historical English texts, using all sentences of the PPCMBE and PPCEME for testing.\nSettings The domain attributes for FEMA are set to include the three corpora themselves (PTB, PPCMBE, and PPCEME), and the genre attributes in the historical corpora. Note that all sentences in the Penn Treebank WSJ data belong to the same\ngenre (news). For SCL, we use the same threshold of 50 occurrences for pivot features, and include 8089 features that pass this threshold. PTB WSJ sections 19-21 are used for parameter tuning: we find that the best number of Brown clusters is 200, and the optimum embedding sizes are 200 and 100 for word2vec and FEMA.\nSpelling normalization Spelling variants lead to a high percentage of out-of-vocabulary (OOV) tokens in historical texts, which poses problems for POS tagging. We normalize the PPCEME sentences using VARD (Baron and Rayson, 2008), a widely used spelling normalization tool that has been proven to improve performance on POS tagging (Rayson et al., 2007) and syntactic parsing (Schneider et al., 2014). VARD is designed specifically for Early Modern English spelling variation, and additional labeled data and training are required for other forms of spelling variation, which we do not consider here. Following Schneider et al. (2014), we utilize VARD\u2019s auto-normalization function with a 50% normalization threshold, achieving a balance between precision and recall. At this threshold, a total of 12% (236298/1961157) of the tokens in the PPCEME are normalized.3\nResults As shown in Table 2, this task is considerably more difficult, with even the best systems achieving accuracies that are nearly 15% worse than in-domain training. Nonetheless, domain adaptation can help: FEMA improves performance by 1.3% on the PPCMBE data, and by 3.8% on the unnormalized PPCEME data. Spelling normalization also helps, improving the baseline systems by more than 2.5%. The combination of spelling normalization and domain adaptation gives an overall improvement in accuracy from 74.2% to 79.1%.\n3We only consider 1 : 1 mappings, and ignore 328 normalizations corresponding to 1 : n mappings."}, {"heading": "5 Analysis", "text": "As expected, the Early Modern English dataset (PPCEME) is considerably more challenging than the Modern British English dataset (PPCMBE): the baseline accuracy is 7% worse on the PPCEME than the PPCMBE. However, the PPCEME is also more amenable to domain adaptation, with FEMA offering considerably larger improvements. One reason is that the PPCEME has many more out-of-vocabulary (OOV) tokens: 23%, versus 9.2% in the PPCMBE. Both domain adaptation and normalization help to address this specific issue, and they yield further improvements when used in combination. This section offers further insights on the sources of errors and possibilities for improvement on the PPCEME data."}, {"heading": "5.1 Feature Ablation", "text": "Table 3 offers a set of feature ablation experiments for the non-adapted SVM tagger. The results show that the word context features are important for obtaining good accuracies on both IV and OOV tokens. If we drop the affix features, the accuracy decreases significantly on OOV tokens. The suffix features are more crucial than the prefix features, as many of them can determine the POS tags of English words with high precisions. The orthographic features are shown to be nearly irrelevant. Overall, the high percentage of OOV tokens can be a major source of errors, as the tagging accuracy on OOV tokens is below 50% in our best baseline system. Note that these results are for a classification-based tagger; while the Viterbi-based MEMM tagger performs only marginally better overall (\u223c 0.2% improvement), it is possible that its error distribution might be different due to the advantages of structured prediction."}, {"heading": "5.2 Error Analysis", "text": "We believe the low accuracy on in-vocabulary (IV) tokens is largely caused by the different annotation schemes between the PTB and the PPCEME. Table 4 presents the SVM accuracy per tag, and the most common error correspondingly. Most of the errors shown in the table are owing to different annotations of the same token in the two corpora. For example, more than 99.9% of token \u2018to\u2019 are labeled as TO in the PTB, but only 54.6% of the token are labeled as TO in the PPCEME and 44.3% of them are labeled as IN (P in the PCHE tagset). The words \u2018all\u2019, \u2018any\u2019 and \u2018every\u2019 are annotated as quantifiers in the PPCEME, which are mapped to JJ, while they are all labeled as DT in the PTB. A simple remapping from Q to DT leads to an increase of 0.78% baseline accuracy; it is possible that other changes to the tag mappings of Moon and Baldridge (2007) might yield further improvements, but a more systematic approach would be outside the bounds of unsupervised domain adaptation.\nA major source of errors on OOV tokens is spelling variation. For instance, \u2018ye\u2019 and \u2018thy\u2019, the older forms of \u2018the\u2019 and \u2018your\u2019, are often incorrectly tagged as NN and JJ in the PPCEME. In general, the per-tag accuracies are roughly correlated with the percentages of OOV tokens. Some exceptions including VB, NNP and NNS, where the affix features can be very useful for tagging OOV tokens."}, {"heading": "5.3 Improvements from Normalization", "text": "As shown above, the tagging accuracy decreases from 81.7% on IV tokens to 49.0% on OOV tokens. Spelling normalization helps to increase the accuracy by transforming OOV tokens to IV tokens. After normalization, the OOV rate for the PPCEME\nfalls from 23.0% to 13.5%, corresponding to a reduction of 41.5% OOV tokens. Normalization is not perfectly accurate, and the tagging performance for IV tokens drops slightly to 81.2% on IV tokens. But due to the dramatic decrease in the number of OOV tokens, normalization improves the overall accuracy by more than 2.5%. We also observe performance drops on tagging OOV tokens after normalization (49.0% to 48.1%), which suggests that the remaining unnormalized OOV tokens are the tough cases for both normalization and POS tagging."}, {"heading": "5.4 Improvements from Domain Adaptation", "text": "As presented in Table 5, the tagging accuracies are increased on both IV and OOV tokens with the domain adaptation methods. Compared against the baseline tagger, FEMA-attribute achieves an absolute improvement of 14% in accuracy on OOV tokens. SCL performs slightly better than Brown clustering and word2vec on IV tokens, but worse on OOV tokens. By incorporating metadata attributes, FEMA-attribute performs better than FEMA-single on OOV tokens, though the accuracies on IV tokens are similar. Interestingly, the venerable method of Brown clustering (slightly) outperforms both word2vec and SCL.\nWe further study the relationship between domain adaptation and spelling normalization by look-\ning into the errors corrected by both approaches. Domain adaptation yields larger improvements than spelling normalization on both IV and OOV tokens, although as noted above, the approaches are somewhat complementary. The results show that among the 60,928 error tokens corrected by VARD, 60% are also corrected by FEMA-attribute, while the remaining 40% would be left uncorrected by the domain adaptation technique. Conversely, among the errors corrected by FEMA-attribute, 38% are also corrected by VARD. The overlap of reduced errors is because of both approaches exploit similar sources of information, including affixes and local word contexts."}, {"heading": "6 Related Work", "text": "Domain adaptation Early work on domain adaptation focuses on supervised setting, in which some amount of labeled instances are available in the target domain (Jiang and Zhai, 2007; Daume\u0301 III, 2007; Finkel and Manning, 2009). Unsupervised domain adaptation is more challenging but attractive in many applications, and several representation learning methods have been proposed for addressing this problem. Structural Correspondence Learning (Blitzer et al., 2006, SCL) and marginalized denoising autoencoders (Chen et al., 2012, mDA) seek cross-domain representations that are useful to predict a subset of features in the original instances, called pivot features. More recent work trains crossdomain representations with neural networks, with additional objectives such as minimizing errors in the source domain and maximizing domain confusion loss (Ganin and Lempitsky, 2015; Tzeng et al., 2015). We show the Feature Embedding model, which is specifically designed for NLP problems with feature templates (Yang and Eisenstein, 2015), achieves strong performance on historical adapta-\ntion tasks.\nHistorical texts Historical texts differ from modern texts in spellings, syntax and semantics, posing significant challenges for standard NLP systems, which are usually trained with modern news text. Numerous resources have been created for overcoming the difficulties, including syntactically annotated corpora (Kroch et al., 2004; Kroch et al., 2010; Galves and Faria, 2010) and spelling normalization tools (Giusti et al., 2007; Baron and Rayson, 2008). Most previous work focuses on normalization, which can significantly increase tagging accuracy on historical English (Rayson et al., 2007) and German (Scheible et al., 2011). Similar improvements have been obtained for syntactic parsing (Schneider et al., 2014). Domain adaptation offers an alternative approach which is more generic \u2014 for example, it can be applied to any corpus without requiring the design of a set of normalization rules. As shown above, when normalization is possible, it can be combined with domain adaptation to yield better performance than that obtained by either approach alone."}, {"heading": "7 Conclusion", "text": "Building NLP systems that are robust to language variation is crucial if natural language processing is to be successfully employed in historical texts. Domain adaptation techniques have been widely adopted in similar problems but seldom used in this application. We empirically evaluate several unsupervised domain adaptation approaches on POS tagging of historical English texts. The results demonstrate that domain adaptation methods significantly improve the tagger performance on two historical English treebanks. FEMA outperforms other domain adaptation approaches with large margins, and it also performs better than a spelling normalization system. Normalization and domain adaptation combine to yield even better performance, with a total of 5% raw accuracy improvement over a baseline classifier in the most difficult setting. We also analyze common errors and point out that more work need to be done for resolving the inconsistent annotations. We hope that our work encourages the research on domain adaptation for historical texts and provide useful baselines in these efforts."}], "references": [{"title": "Vard2: A tool for dealing with spelling variation in historical corpora", "author": ["Baron", "Rayson", "2008 Alistair Baron", "Paul Rayson"], "venue": "In Postgraduate conference in corpus linguistics", "citeRegEx": "Baron et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Baron et al\\.", "year": 2008}, {"title": "Word frequency and key word statistics in corpus linguistics", "author": ["Baron et al", "2009 Alistair Baron", "Paul Rayson", "Dawn Archer"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "A theory of learning from different domains", "author": ["Ben-David et al", "2010 Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan"], "venue": "Machine learning,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer et al", "2006 John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Something old, something new: A computational morphological description of old swedish", "author": ["Borin", "Forsberg", "2008 Lars Borin", "Markus Forsberg"], "venue": "In LREC 2008 workshop on language technology for cultural heritage data (LaTeCH", "citeRegEx": "Borin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Borin et al\\.", "year": 2008}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al", "1992 Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1992\\E", "shortCiteRegEx": "al. et al\\.", "year": 1992}, {"title": "Canterbury Tales. Barron\u2019s Educational Series, Haupage, New York, 3 edition", "author": ["Chaucer", "Hopper", "2012 Geoffrey Chaucer", "Vincent Foster Hopper"], "venue": null, "citeRegEx": "Chaucer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chaucer et al\\.", "year": 2012}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Chen et al", "2012 Minmin Chen", "Z. Xu", "Killian Weinberger", "Fei Sha"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Frustratingly easy domain adaptation", "author": ["III Daum\u00e9"], "venue": "Hal Daume\u0301 III", "citeRegEx": "Daum\u00e9,? \\Q2007\\E", "shortCiteRegEx": "Daum\u00e9", "year": 2007}, {"title": "The google books project: will it make libraries obsolete? The journal of academic librarianship", "author": ["Dougherty", "2010 William C Dougherty"], "venue": null, "citeRegEx": "Dougherty and Dougherty.,? \\Q2010\\E", "shortCiteRegEx": "Dougherty and Dougherty.", "year": 2010}, {"title": "What to do about bad language on the internet", "author": ["Eisenstein", "2013 Jacob Eisenstein"], "venue": null, "citeRegEx": "Eisenstein and Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Eisenstein and Eisenstein.", "year": 2013}, {"title": "An analysis of verb subcategorization frames in three special language corpora with a view towards automatic term recognition", "author": ["Eumeridou et al", "2004 Eugenia Eumeridou", "Blaise Nkwenti-Azeh", "John McNaught"], "venue": "Computers and the Humanities,", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Hierarchical bayesian domain adaptation", "author": ["Finkel", "Manning", "2009 Jenny R. Finkel", "Christopher Manning"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Finkel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2009}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Ganin", "Lempitsky", "2015 Yaroslav Ganin", "Victor Lempitsky"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "Ganin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ganin et al\\.", "year": 2015}, {"title": "A hybrid grammatical tagger: Claws4. Corpus annotation: Linguistic information from computer text", "author": ["Garside", "Smith", "1997 Roger Garside", "Nicholas Smith"], "venue": null, "citeRegEx": "Garside et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Garside et al\\.", "year": 1997}, {"title": "Svmtool: A general pos tagger generator based on support vector machines", "author": ["Gim\u00e9nez", "Marquez", "2004 Jes\u00fas Gim\u00e9nez", "Lluis Marquez"], "venue": "Proceedings of the 4th International Conference on Language Resources and Evaluation", "citeRegEx": "Gim\u00e9nez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gim\u00e9nez et al\\.", "year": 2004}, {"title": "Automatic detection of spelling variation in historical corpus", "author": ["Giusti et al", "2007 Rafael Giusti", "A Candido", "Marcelo Muniz", "L\u0131\u0301via Cucatto", "Sandra Alu\u0131\u0301sio"], "venue": "In Proceedings of the Corpus Linguistics Conference (CL)", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Automatic pragmatic text segmentation of historical letters", "author": ["Hendrickx et al", "2011 Iris Hendrickx", "Michel G\u00e9n\u00e9reux", "Rita Marquilhas"], "venue": "In Language Technology for Cultural Heritage,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Instance weighting for domain adaptation in nlp", "author": ["Jiang", "Zhai", "2007 Jing Jiang", "ChengXiang Zhai"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL), Prague", "citeRegEx": "Jiang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2007}, {"title": "What\u2019s in a domain? multi-domain learning for multiattribute data", "author": ["Joshi et al", "2013 Mahesh Joshi", "Mark Dredze", "William W. Cohen", "Carolyn P. Ros\u00e9"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "The penn-helsinki parsed corpus of middle english (ppcme2). department of linguistics, university of pennsylvania", "author": ["Kroch", "Taylor", "2000 Anthony Kroch", "Ann Taylor"], "venue": "cd-rom. Department of Linguistics,", "citeRegEx": "Kroch et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kroch et al\\.", "year": 2000}, {"title": "The penn-helsinki parsed", "author": ["Kroch et al", "2004 Anthony Kroch", "Beatrice Santorini", "Lauren Delfs"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "The penn-helsinki parsed corpus of modern british english (ppcmbe). Department of Linguistics, University of Pennsylvania, CDROM", "author": ["Kroch et al", "2010 Anthony Kroch", "Beatrice Santorini", "Ariel Diertani"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Statistically significant detection of linguistic change", "author": ["Kulkarni et al", "2015 Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proceedings of International Conference on World Wide Web (WWW),", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Syntactic annotations for the google books ngram corpus", "author": ["Lin et al", "2012 Yuri Lin", "Jean-Baptiste Michel", "Erez Lieberman Aiden", "Jon Orwant", "Will Brockman", "Slav Petrov"], "venue": "In Proceedings of the ACL 2012 system demonstrations,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Manning et al", "2014 Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Domain adaptation with multiple sources", "author": ["Mansour et al", "2009 Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Marcus et al", "1993 Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1993\\E", "shortCiteRegEx": "al. et al\\.", "year": 1993}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al", "2013 Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Part-of-speech tagging for middle english through alignment and projection of parallel diachronic texts", "author": ["Moon", "Baldridge", "2007 Taesun Moon", "Jason Baldridge"], "venue": "In Proceedings of Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Moon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2007}, {"title": "Supporting exploratory text analysis in literature study", "author": ["Muralidharan", "Hearst", "2013 Aditi Muralidharan", "Marti A Hearst"], "venue": "Literary and linguistic computing,", "citeRegEx": "Muralidharan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Muralidharan et al\\.", "year": 2013}, {"title": "Automatic verb extraction from historical swedish texts", "author": ["Pettersson", "Nivre", "2011 Eva Pettersson", "Joakim Nivre"], "venue": "In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities,", "citeRegEx": "Pettersson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pettersson et al\\.", "year": 2011}, {"title": "Crftagger: Crf english pos tagger", "author": ["Phan", "2006 Xuan-Hieu Phan"], "venue": "URL: http://crftagger. sourceforge. net", "citeRegEx": "Phan and Phan.,? \\Q2006\\E", "shortCiteRegEx": "Phan and Phan.", "year": 2006}, {"title": "Natural language processing for historical texts", "author": ["Piotrowski", "2012 Michael Piotrowski"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "Piotrowski and Piotrowski.,? \\Q2012\\E", "shortCiteRegEx": "Piotrowski and Piotrowski.", "year": 2012}, {"title": "Tagging the bard: Evaluating the accuracy of a modern pos tagger on early modern english corpora", "author": ["Rayson et al", "2007 Paul Rayson", "Dawn Archer", "Alistair Baron", "Jonathan Culpeper", "Nicholas Smith"], "venue": "In Corpus Linguistics Conference", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Evaluating an\u2019off-the-shelf\u2019pos-tagger on early modern german text", "author": ["Scheible et al", "2011 Silke Scheible", "Richard J Whitt", "Martin Durrell", "Paul Bennett"], "venue": "In Proceedings of the 5th ACL-HLT workshop on language technology for cultural heritage,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Parsing early and late modern english corpora", "author": ["Schneider et al", "2014 Gerold Schneider", "Hans Martin Lehmann", "Peter Schneider"], "venue": "Literary and Linguistic Computing", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Toutanova et al", "2003 Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In Proceedings of the North American Chapter of the Association", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Word Representation: A Simple and General Method for Semi-Supervised Learning", "author": ["Turian et al", "2010 Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["Tzeng et al", "2015 Eric Tzeng", "Judy Hoffman", "Trevor Darrell", "Kate Saenko"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Fast easy unsupervised domain adaptation with marginalized structured dropout", "author": ["Yang", "Eisenstein", "2014 Yi Yang", "Jacob Eisenstein"], "venue": "In Proceedings of the Association for Computational Linguistics (ACL),", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Unsupervised multi-domain adaptation with feature embeddings", "author": ["Yang", "Eisenstein", "2015 Yi Yang", "Jacob Eisenstein"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "With the rise of digital humanities research, natural language processing for historical texts is of increasing interest. However, directly applying standard language processing tools to historical texts often yields unsatisfactory performance, due to language change and genre differences. Spelling normalization is the dominant solution, but it fails to account for changes in usage and vocabulary. In this empirical paper, we assess the capability of domain adaptation techniques to cope with historical texts, focusing on the classic benchmark task of part-of-speech tagging. We empirically evaluate several domain adaptation methods on the task of tagging two millionword treebanks of the Penn Corpora of Historical English. We demonstrate that domain adaptation significantly outperforms spelling normalization when adapting modern taggers to older texts, and that domain adaptation is complementary with spelling normalization, yielding better results in combination.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}