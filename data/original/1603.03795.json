{"id": "1603.03795", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "Demonstrating the Feasibility of Automatic Game Balancing", "abstract": "Game balancing is an important part of the (computer) game design process, in which designers adapt a game prototype so that the resulting gameplay is as entertaining as possible. In industry, the evaluation of a game is often based on costly playtests with human players. It suggests itself to automate this process using surrogate models for the prediction of gameplay and outcome. In this paper, the feasibility of automatic balancing using simulation- and deck-based objectives is investigated for the card game top trumps. Additionally, the necessity of a multi-objective approach is asserted by a comparison with the only known (single-objective) method. We apply a multi-objective evolutionary algorithm to obtain decks that optimise objectives, e.g. win rate and average number of tricks, developed to express the fairness and the excitement of a game of top trumps. The results are compared with decks from published top trumps decks using simulation-based objectives. The possibility to generate decks better or at least as good as decks from published top trumps decks in terms of these objectives is demonstrated. Our results indicate that automatic balancing with the presented approach is feasible even for more complex games such as real-time strategy games.", "histories": [["v1", "Fri, 11 Mar 2016 21:36:27 GMT  (508kb,D)", "http://arxiv.org/abs/1603.03795v1", null]], "reviews": [], "SUBJECTS": "cs.HC cs.AI cs.NE", "authors": ["vanessa volz", "g\\\"unter rudolph", "boris naujoks"], "accepted": false, "id": "1603.03795"}, "pdf": {"name": "1603.03795.pdf", "metadata": {"source": "CRF", "title": "Demonstrating the Feasibility of Automatic Game Balancing", "authors": ["Vanessa Volz", "G\u00fcnter Rudolph", "Boris Naujoks"], "emails": [], "sections": [{"heading": null, "text": "I. Introduction\nThe increasing complexity and popularity of (computer) games result in numerous challenges for game designers. Especially finetuning game mechanics, which affects the feel and required skill profile of a game significantly, is a difficult task. For example, changing the time between shots for the sniper rifle in Halo 3 from 0.5 to 0.7 seconds impacted the gameplay significantly according to designer Jaime Griesemer1.\nIt is important to draw attention to the fact that the game designer\u2019s vision of a game can rarely be condensed into just one intended game characteristic. In competitive games, for example, it is certainly important to consider fairness, meaning that the game outcome depends on skill rather than luck (skill-based) and that the win rate of two equally matched players is approx. 50% (unbiased). But additionally, the outcome should not be deterministic and entail exciting gameplay, possibly favouring tight outcomes.\nIt therefore suggests itself to support the\nbalancing process with tools that can automatically evaluate and suggest different game parameter configurations, which fulfil a set of predefined goals (cf. [14]). However, since the effects of certain goals tend to be obscure at the time of design, we suggest to use a multiobjective approach which allows to postpone the decision on a configuration until the tradeoffs can be observed. In this paper, we introduce game balancing as a multi-objective optimisation problem and demonstrate the feasibility of automating the process in a case study.\nFor the purpose of this paper, we define game balancing as the modification of parameters of the constitutive and operational rules of a game (i.e. the underlying physics and the induced consequences / feedback) in order to achieve optimal configurations in terms of a set of goals. To this end, we analyse the card game top trumps and different approaches to balance it automatically, demonstrating the feasibility and advantages of a multi-objective approach as well as possibilities to introduce surrogate models.\n1http://www.gdcvault.com/play/1012211/Design-in-Detail-Changing-the\nar X\niv :1\n60 3.\n03 79\n5v 1\n[ cs\n.H C\n] 1\n1 M\nar 2\n01 6\nIn the following section, we present related work on top trumps, balancing for multiplayer competitive games and gameplay evaluations. The subsequent section highlights some important concepts specific to the game top trumps and multi-objective optimisation, before the following section details our research approach including the research questions posed. Afterwards, the results of our analysis are presented and discussed, before we finish with a conclusion and outlook on future work.\nII. Related Work\nCardona et al. use an evolutionary algorithm to select cards for top trumps games from open data [4]. The focus of their research, however, is the potential to teach players about data and learn about it using games. The authors develop and use a single-objective dominancerelated measure to evaluate the balance of a given deck. This measure is used as a reference in this paper (cf. fD in Sec. IV).\nJaffe introduces a technique called restricted play that is supposed to enable designer to express balancing goals in terms of the win rate of a suitably restricted agent [11]. However, this approach necessitates expert knowledge about the game as well as an AI and several potentially computationally expensive simulations. In contrast, we explore other possibilities to express design goals and utilise non-simulation based metrics.\nChen et al. intend to solve \u201cthe balance problem of massively multiplayer online roleplaying games using co-evolutionary programming\u201d [5]. However, they focus on level progression and ignore any balancing concerns apart from equalising the win-rates of different in-game characters.\nYet, most work involving the evaluation of a game configuration is related to procedural content generation, specifically map or level generation. Several papers focus on issuing guarantees, e.g. with regards to playability [18], solvability [17], or diversity [15, 10]. Other research areas include dynamic difficulty adaptation for single-player games [9], the generation\nof rules [2, 16], and more interactive versions of game design, e.g. mixed-initiative [13].\nIII. Basics\nIn the following, the game top trumps is introduced and theoretical background for the applied methods from multi-objective optimisation and performance evaluation is summarised.\nI. Top Trumps\nTop trumps is a themed card game originally published in the 1970s and relaunched in 1999. Popular themes include cars, motorcycles, and aircrafts. Each card in the deck corresponds to a specific member of the theme (such as a car model for cars) and displays several of its characteristics, such as acceleration, cubic capacity, performance, or top speed. An example can be found in Fig. 1.\nAt the start of a game, the deck is shuffled and distributed evenly among players. The starting player chooses a characteristic whose value is then compared to the corresponding values on the cards of the remaining players. The player with the highest value receives all\ncards in the trick and then continues the game by selecting a new attribute from their next card. The game usually ends when at least one player has lost all their cards. However, for the purpose of this paper, we end the game after all cards have been played once in order to avoid possible issues of non-ending games.\nII. Multi-objective optimisation\nSwitching from single- to multi-objective optimisation has advantages and disadvantages [6]. While it is often better to consider multiple objectives for technical optimisation tasks, the complete order of individuals is lost in this case. Instead, one has to handle incomparable solutions, e.g. two solutions that are better than the other one in at least one objective. This objective or component wise approach goes back to the definition of Pareto dominance. A solution or individual x is said to strictly (Pareto) dominate another solution y (denoted x \u227a y) iff x is better than y in all objectives. Considering minimisation this reads\nx \u227a y iff \u2200i \u2208 {1, . . . , m} : f (xi) < f (yi)\nunder fitness function f : X \u2282 Rn \u2192 Rm, f (x) = ( f1(x), . . . , fm(x) ) .\nBased on this, the set of (Pareto) nondominated solutions (Pareto set) is defined as the set of incomparable solutions as defined above and the Pareto front to be the image of the Pareto set under fitness function f .\nNevertheless, even incomparable solutions need to be distinguished when it comes to selection in an evolutionary algorithm. In the evolutionary multi-objective optimiser considered, SMS-EMOA [1], this is done based on the contribution to the hypervolume (i.e the amount of objective space covered by a Pareto front w.r.t. a predefined reference point). The contribution of a single solution to the overall hypervolume of the front is used as the secondary ranking criterion for the (\u00b5 + 1)-approach to selection. The first one is the non-dominated sorting rank assigned to each solution.\nFor measuring the performance of different SMS-EMOA runs, we consider two other performance indicators next to the hypervolume\nof the resulting Pareto fronts. These are the additive \u03b5 indicator as well as the R2 indicator, all presented by Knowles et al. [12]. These indicators are also considered for the termination of EMOA runs using online convergence detection as introduced by Trautmann et al. [19].\nFor variation in SMS-EMOA, the most widely used operators in the field are considered, namely simulated binary crossover and polynomial mutation. These are parametrised using pc = 1.0, pm = 1/n, \u03b7c = 20.0, and \u03b7m = 15.0, respectively, cf. Deb [6].\nIII. Performance Measurement for Stochastic Multi-objective Optimisers\nThe unary performance indicators introduced above only express the performance of a single optimisation run. However, to evaluate and compare the relative performance of stochastic optimisers with potentially significantly different outcomes (such as evolutionary multiobjective algorithms), measurements for the statistical performance are needed.\nFor this purpose, (empirical) attainment functions that describe the sets of goals achieved with different approaches were proposed, expressing both the quality of the achieved solutions as well as their spread over multiple optimisation runs [7]. Based on these functions, the set of goals that are reached in 50% (or other quantiles) of the runs of the optimisers can be computed (also known as 50%- attainment surface). Comparing the attainment surfaces of different optimisers is already a much better indicator for their performances than comparing the best solutions achieved, as they are subject to stochastic influences.\nAdditionally, Fonseca et al. detail a statistical testing procedure (akin to a two-sample two-sided Kolmogorov-Smirnov hypothesis test) based on the first- and second-order attainment-functions of two optimisers [7]. If the null hypotheses of these tests are rejected, it can be assumed that the differences in performance of the considered optimisers are statistically significant.\nIV. Approach\nFor the remainder of this paper, we denote the number of cards in a deck as K (even number) and the number of characteristics (categories) displayed on a card L. Two representations are used for a deck, (1) as a vector x \u2208 RKL for the evolutionary algorithm and (2) as a K\u00d7 L matrix V for easier understanding.\nAccordingly, the value of the k-th card in the l-th category is vk,l with k \u2208 {1, . . . , K}, l \u2208 {1, . . . , L}. The k-th card in a deck is vk,\u00b7 = (vk,1, . . . , vk,L). A partial order for the cards can be expressed with vk1,\u00b7 vk2,\u00b7 meaning that card vk2,\u00b7 beats vk1,\u00b7 in all categories\nIn this paper, we only consider decks that fulfil two basic requirements we deem existential for entertaining gameplay: \u2022 all cards in the deck are unique: @(k1, k2) \u2208 {1, . . . , K}2, k1 6= k2 with vk1,\u00b7 = vk2,\u00b7\n\u2022 there is no strictly dominant card in the deck: @k1 \u2208 {1, . . . , K} with vk2,\u00b7 \u227a vk1,\u00b7\u2200k2 \u2208 {1, . . . , K} (in this case, dominant cards have larger values, since higher values win according to the game rules)\nWe consider two agents p4, p0 with different knowledge about the played deck: \u2022 p4 knows the exact values of all cards in\nthe deck \u2022 p0 only knows the valid value range for\nall values vk,l Both agents are able to perfectly remember which cards have been played already. Player p4 is expected to perform better than p0 on average on a balanced deck. In order to reduce the number of simulations needed to verify this, only games of a player p4 against p0 will be considered here.\nIn our simulation, both of these agents estimate the probabilities to win with each category on a given card with consideration of their respective knowledge about the deck as well as the cards already played. p0 therefore has to assume a uniform distribution and will only take the values of their current card into\naccount. p4, in contrast, is able to model the probability more precisely by accounting for the number of cards with a higher value in each category that are still in play.\nLet RG be the number of simulation runs. The number of tricks that p4 received at the end of the r-th game (r \u2208 {1, . . . , RG}) with deck V will be called t(r,V)4 henceforth, and thus iff t(r,V)4 > K 2 , p4 won the game, iff t (r,V) 4 = K 2 the game was a draw, and else, p4 lost. t (r,V) c is the number of times the player choosing the category did not win the trick in round r of the game with deck V, i.e. the number of times the player announcing the categories changed.\nSince optimisation tasks are generally assumed to be minimisation problems (without loss of generality), this convention is satisfied here as well for the sake of consistency. Therefore, maximisation problems are transformed into minimisation task by multiplication with \u22121. In the course of this paper, we compare 8 card sets from purchased decks with decks generated using three different approaches and corresponding fitness functions detailed in the following: \u2022 Single-objective optimisation according\nto the dominance-related (D) measure proposed in [4] which describes the distance of the cards in a deck V to the Pareto front:\nfD(V) = \u2212 1 K\nK\n\u2211 k=1\nK\n\u2211 i=1 (1\u2212 1(vk \u227a vi))\n\u2022 Multi-objective optimisation with simulation-based measures developed with expert knowledge that are supposed to express the decks V\u2019s fairness, excitement, and resulting balance (B):\nfB(V) =\n( \u2212 1\nRG\nRG \u2211 r=1 1\n( t(r,V)4 > K 2 ) ,\n\u2212 1 RG\nRG \u2211 r=1 t(r,V)c , 1 RG RG \u2211 r=1 \u2223\u2223\u2223\u22232t(r,V)4 \u2212 K2 \u2223\u2223\u2223\u2223 ) .\n\u2022 Multi-objective optimisation with simulation-independent measures developed in the pre-experimental planning phase (cf.\nSec. II) as surrogate (S) for (the simulation-based) fitness fB of different decks:\nfS(V) =(\u2212hv(V), \u2212 sd({avg(v\u00b7,l)|l \u2208 {1, . . . , L}})),\nwith the dominated hypervolume hv of a deck V, sd the empirical standard deviation and avg the average.\nIn order to compare the aforementioned approaches, an SMS-EMOA is used to approximate the Pareto front for the fitness functions fS and fB. Online convergence detection, variation operators, and parameters as described in Sec. II are used. For the single-objective fitness fD, the algorithm was modified as little as possible to enable comparisons.Thus, a (\u00b5 + 1)-EA was used with the same variation operators and equivalent selection. The convergence was tested based on the variations of some singleobjective performance indicators, namely the min, mean and max fitness values of the active population. The experiments were conducted using R with the help of the emoa package2 and a related SMS-EMOA implementation3,4.\nI. Research questions\nThe different approaches to finding a balanced top trumps deck are evaluated and compared in Sec. V. We focus on the following topics: I Problems of manual balancing and the solutions offered by automation II Feasibility of automatic balancing in terms of required quality III Performance of multi- and single-objective approaches IV Feasibility of automatic balancing in terms\nof computational costs\nII. Preexperimental planning\nBefore any experiments can be executed, the test case has to be defined more accurately. The following assumptions are made:\n\u2022 The number of cards and categories are set to K = 32, L = 4 in accordance with these values for the purchased decks. \u2022 The valid range of all values vl,k is set\nto [1, 10] \u2208 R, which all decks can be transformed to. This results in an infinite number of possible cards, but other options entail the necessity to construct a genotype-phenotype mapping.\nDue to the large number of possible card distributions among the players, the order of the cards in a deck, and different starting players, a single deck could potentially result in a large number of different games (4K!). As a consequence, all simulation-based metrics to evaluate the deck have to be approximated. The values of the metrics in fB, which all express an average, should be as close to the true mean of the respective distributions as possible. To ensure the quality of the approximation, a statistical t-test is conducted to compute the size of the confidence interval for each metric for RG between 100 and 10 000 at a confidence level of 0.95. This test is repeated 500 times for each possible sample size and each metric and. Assuming a normal distribution, the .95-quantile is stored as the result. RG = 2 000 games are found to be a good tradeoff between computational time and fitness approximation accuracy for all metrics.\nAn equivalent test is conducted to decide on the number of optimisation runs necessary to approximate the performance of the corresponding approach (to a suitable confidence interval). Here, the HV-, e- and R2-indicators are considered with the Pareto front resulting from all of these runs as a reference set. After considering the results, the number of runs RO was set to RO = 100.\nFor the simulation-independent approach, metrics that do not require a simulation are developed. The hypervolume is chosen as a measure to achieve as many non-dominated cards in each deck as possible. This is expected to improve the fairness of a deck (also cf. fD). The\n2https://cran.r-project.org/web/packages/emoa/ 3https://github.com/olafmersmann/emoa 4Additional code written for the simulation and experiments will be made accessible after publication\nstandard deviation of category means is used to increase the significance of player p0\u2019s disadvantage, thereby resulting in a higher win-rate for p4.\nDifferent population sizes are tested and the resulting Pareto fronts are compared for a quick estimate of their performance. Based on the results received, approaches are evaluated on runs with population sizes of 10 and 100 individuals. This accounts for both small populations with a high selection pressure and also bigger populations with a larger spread.\nV. Results\nWe evaluate all approaches according to the fitness function fB which is based on expert knowledge and therefore assumed to characterise a balanced deck best. This assumption is supported by the fact that most of the purchased decks are located on the approximated Pareto front according to these metrics.\nIn the remainder of the paper, we use the letter corresponding to the fitness function and the population size to refer to the union of the Pareto fronts of RO = 100 runs with the respective fitness function and population size for the multi-objective approaches. The introduced acronym with an added index p refers to the Pareto front of the respective set with regards to fB. A numerical index stands for the attainment surfaces to the indicated level. For example, S10 is the union of all Pareto fronts from optimiser runs with population size 10 and fitness function fS, S10p is the Pareto front of this set and S1050 is its 50% attainment surface. For the single-objective approach, the union of the best individuals achieved in RO = 100 runs are considered instead, because the populations converge to one deck. The set of purchased decks will henceforth be denoted PD.\nTo facilitate the discussion of the experiments, the results of the three different approaches are plotted in terms of their performance on the fitness function fB. Figure 2 depicts the sets listed in Tab. 1. Figure 3 visualises the 50%-attainment surfaces as well as the Pareto fronts resulting from the Pareto\nfront union for each approach. The legend for all plots can be found in Tab. 1, where the same colour scheme is used to refer to the Pareto fronts and attainment surfaces of the respective approaches.\nI. Automatic Balancing Advantages\nTo evaluate the advantages of automatic balancing, the following hypotheses are proposed. I-C1 The number of tests needed to approxi-\nmate some simulation-based metrics for a single deck to an appropriate accuracy is very high and possibly exceeds the number of playtests that could reasonably be done with human players.\nI-C2 Many of the purchased decks are unfair in the sense that the game\u2019s outcome depends strongly on luck and less on the players\u2019 skill levels.\nWith these hypotheses, the effort needed for manual balancing is considered and the performance of PD (i.e. likely manually balanced decks) is compared to that of automatically balanced decks.\nThe t-test described in Sec. II already determined that the best tradeoff between the accuracy of the approximation of simulationbased metrics and the number of simulations RG was \u2248 2 000. Considering the large effort playtests with humans necessitate as well as the bias induced by having different players play, testing 2 000 rounds of a game with humans is tedious and potentially not even possible on smaller budgets. For example, the standard deviation on fB for the decks in B10 is \u2248 (0.0427, 0.3191, 0.3576). If we assume we had 100 players play 10 games each, the resulting confidence interval for \u03b1 = 0.05 is\n\u2248 (0.0442, 0.4298, 0.15), which would not allow the designers to distinguish between different solutions and is therefore not accurate enough. The standard deviation as well as the number of games needed would likely increase with the complexity of the game as well. Therefore, a definitive advantage of automatic balancing over manual playtests is the possibility of a quantitative analysis of simulation-based metrics (cf. [11]).\nExcept for a single deck (motor cycles), the purchased decks are all on the edge of the estimated Pareto front with the worst performances in terms of the win-rate of p4. This is\nobvious in Fig. 3. The low win-rates for p4 are probably due to the fact that the number of non-dominated cards in those decks in PD is relatively low. The fD average is \u2248 \u221224.12 compared to optimum \u221231 (only non-dominated cards). This means that the resulting gameplay depends heavily on luck because there are card combinations with which a player simply cannot win regardless of their skill. The only exception is the motor cycle deck with a value for fD of \u221230.4375 and a much better p4 win-rate of approx. 0.8.\nThus, we demonstrated that the effort needed to evaluate one deck is beyond a rea-\nsonable number of playthroughs. Additionally, the manually balanced decks are located on the extreme edges of the Pareto front, implying that it is difficult to find less extreme solutions manually. This also suggests that the approximation of the Pareto front could help a designer by giving them a more sophisticated idea about the characteristics of their game and potential alternatives. The findings by Nelson and Mateas also connote that designers see potential in automatic balancing tools to support the balancing process [14].\nII. Automatic Balancing Quality\nNext, we demonstrate the feasibility of automatic balancing, i.e. that at least some of the automatically balanced decks perform on par with the purchased decks. II-C1 Automatically balanced decks are on the Pareto front. II-C2 The results for II-C1 are statistically significant. As is obvious from the plots (especially the right plot in Fig. 3), automatically balanced decks (S10 and B10) make up a large part of the Pareto front and are thus not dominated by the purchased decks. Moreover, most purchased decks are concentrated at the extreme edges of the front.\nThe same is true for individuals in S1050 and B1050, which ensures that, despite the stochasticity of the approach, decks on the Pareto front can be achieved in at least 50% of all optimisation runs, making the result statistically relevant.\nIII. Single- and Multi-objective performance\nWe analyse whether the multi-objectification of the approach used by Cardona et al. [4] could result in better performing individuals. Therefore we extend fD to fS. The bigger the dominated hypervolume (the first part of fS), the more non-dominated cards are in a deck, which is expressed by fD. The hypervol-\nume additionally favours cards with a larger spread, which does not affect the dominancerelationship (or the outcome of a playthrough or any simulation-based fitness values). The second part of fS is the standard deviation of the category means. The higher the deviation, the more problematic is the strategy of player p0 to assume uniform distributions for the categories to make up for their lack of knowledge.\nIII-C1 There is a significant difference between the (empirical) attainment functions of the considered multi-objective (S10, S100) and single-objective (D10, D100) optimisation approaches. III-C2 The results of the single-objective approach (D10, D100) perform significantly worse than the multi-objective ones (S10,S100) in terms of fB.\nIn order to test hypothesis III-C1, the statistical testing procedure for the comparison of empirical attainment surfaces described by Fonseca et al. [7] is conducted using software published by C. Fonseca5. With 10 000 random mutations and \u03b1 = 0.05, the null hypothesis (the attainment function of two approaches are equally distributed) is rejected with a p-value of 0 (critical value 0.23, test statistic 1) for all comparisons in {D10, D100} \u00d7 {S10, S100}. This result was expected as, judging from the visualisations (e.g. in Fig. 2), the individuals found by the analysed approaches are in very different areas. Additionally, the results found by the single-objective approach have a very low spread, which is probably owed to the character of the fitness measure fD.\nThe sets of solutions found for the singleobjective approaches are both strictly dominated by both surrogate approaches according to the definition by Knowles et al. [12]. Formally, it holds that\n(D1050 \u222a D10050) \u227a (D10\u222a D100) \u227a S1050 \u227a S10 (D1050 \u222a D10050) \u227a (D10\u222a D100) \u2016 S10050 \u227a S100.\n5https://eden.dei.uc.pt/~cmfonsec/software.html\nThe test for hypothesis III-C1 has shown that the attainment functions of the approaches are not the same. This indicates that using fitness function fS instead of fD has improved the results in terms of fB, thus confirming III-C2. This is suggests that the multi-objectification of fD can indeed improve the achieved results in this case.\nIV. Computational Costs and Surrogate Objectives\nWe now address the feasibility of automatic balancing in terms of computational costs. In Sec. II, we have already analysed and verified its feasibility on function fB. Therefore, the computational costs needed with RG = 2 000 simulations per game and RO = 100 optimisation runs are obviously manageable for the considered application.\nHowever, a simulation-based approach to balancing might prove too costly for more complex games with computationally expensive simulations or a large game-state space. We approach this problem by investigating the possibility of using simulation independent measures (e.g fS) instead of fB. Naturally, in practice these measures would need to be developed in accordance with the intended balancing goals and observations of the optimisers\u2019 behaviour, similar to what is described in Sec. II.\nThe following hypotheses are put forward in order to investigate the computational costs of automatic balancing and the feasibility of simulation-independent objectives: IV-C1 Some results optimised based on fitness\nfunction fS (S10, S100) are not dominated by B10 and B100.\nIV-C2 The best individuals in S10 and S100 perform at least equally well as the ones in B10 and B100 in terms of performance indicators. IV-C3 There is no significant difference between the attainment functions of S10, S100 and B10, B100. IV-C4 The results for IV-C1 and IV-C2 are statistically significant.\nAs visualised in Fig. 2 and Fig. 3, there are individuals in S10 on the shared Pareto front and which are therefore not dominated by any individual in B10 \u222a B100. In fact, B100 \u227a S10 and B10\u2016S10. For S100, it can only be said that S100\u2016B100, making IV-C1 only true for S10.\nIn order to compare the performances of the Pareto fronts of the approaches considered here, the performance indicators for HV, e and R2 are computed for S10p, S100p, B10p, B100p. To facilitate the interpretation of these values, the aforementioned sets are normalised (resulting in values between 1 and 2, cf. [19]) with regards to all values achieved (cf. Fig. 2) before computing the indicators. The normalised Pareto front of the union of all achieved fronts is used as a reference set (cf. Fig. 3 (right)). The resulting indicator values can be found in the upper half of Tab. 2. The non-dominated sorting ranks in Tab. 2 (top half) clearly show that hypothesis II-C2 is true for the computed values and that the approaches with the same population size perform equally well.\nIn order to test the statical significance of this statement, the width w of the confidence interval for \u03b1 = 0.05 for each set and each indicator is computed. This is done using a t-test to estimate the true indicator means on the separately achieved performance indicators for RO = 100 runs for each approach, normalised\nas before. When accounting for the uncertainty expressed in the confidence intervals, all differences in performance indicators in Tab. 2 (top half) are statistically significant except for the difference in R2 for B10 and S10, as well as B100 and S100. This means that in the true ranking, B100 could be ranked 3 instead of 2.\nThe tests used to compare empirical attainment functions for hypothesis III-C1 described in Sec. III are applied again here to compare the attainment functions for all combinations of B10, B100 and S10, S100. Contrary to hypothesis IV-C3, the tests all reject the null hypothesis of equal attainment functions with a p-value of 0, although the decisions are a bit tighter than in Sec. III. Thus, hypothesis IV-C3 can not be confirmed. The differences in attainment functions are likely due to the fact that the compared sets occupy different areas in the objective space (cf. Fig. 2). This can be explained by failing to express the excitement of a playthrough in fS, which was constructed to better express a deck\u2019s fairness starting from fD (cf. Sec. III). Therefore, if the goal was to approximate the solutions obtained from fB with simulation-independent fitness measures, different ones should be selected, possibly using the p-value of the aforementioned test as an indicator for their quality.\nFrom Fig. 3 (left) it is obvious that both B1050 and S1050 contain individuals on the shared Pareto front, thus proving that IV-C1 is true in at least 50% of optimisation runs. The performance indicators for the respective 50%-attainment surfaces for the respective approaches are listed in Tab. 2 (bottom half), along with their ranks and the possible true rank when uncertainty is accounted for. In this case, S10050 performs significantly worse than all the other approaches considered. However, S1050 definitely performs better than B10050 and there is no clear ranking of the performances of S1050 and B1050. This implies that hypothesis IV-C4 is true as well.\nSince the values in Tab. 2 are all based on normalised outcomes, the absolute values can be compared. As expected, the 50%-attainment surfaces all perform worse than their Pareto\nfront counterpart and the differences are significant. Interestingly, the differences per indicator are smaller in the bottom half of Tab. 2. This reflects the fact that the distances of the 50%- attainment surfaces of the different approaches in objective space are visibly smaller in Fig. 3 (left) than the Pareto fronts in Fig. 3 (right). There are also more individuals in S10050 and B10050 when compared to S100p and B100p, respectively, which explains their smaller loss in performance indicators. This is because both approaches experience less spread in the direction of the optimum.\nV. Additional Observations\nNext to the results discussed previously, some interesting observations were made during the experiments.\nThe single-objective optimisation approach converges to one deck for both population sizes tested, even though all decks with exclusively non-dominated cards perform equally well. The optimal fitness value for fD, 31, is achieved in almost all runs. This suggests that the algorithm used for single-objective optimisation including the convergence detection worked for this application. Furthermore, we can conclude that fD is not suited for deck generation because it does not distinguish decks well. This might be entirely different for data selection as done by Cardona et al. [4].\nThe optimiser runs were stopped by the convergence detection mechanism after very different numbers of function evaluations neval , even for the same approach. For example, the first 30 runs for S10 executed between 3 727 and 23 737 fitness function evaluations. There is no apparent correlation between neval and the quality of the achieved solutions, with neval between 3 993 and 20 243 for runs with solutions on the Pareto front of this subset of S10. This point to a high complexity of the fitness function landscapes and validates the use of online convergence detection in this experiment.\nVI. Conclusion and Outlook\nIn this paper, we present our approach to automatic game balancing (as defined in Sec. I) and apply it to the card game top trumps. Our approach includes the formalisation and interpretation of the task as a multi-objective minimisation problem which is solved using a stateof-the-art EMOA with online convergence detection. The performances of the resulting and purchased decks next to a single-objective approach [4] are evaluated using statistical analyses.\nWe conclusively show the feasibility of automatic game balancing in terms of the quality of the achieved solutions for the game top trumps under the assumptions detailed in Sec. IV. Being aware that computational concerns could render a simulation-based approach infeasible for complex applications, an approach to avoid simulation was outlined in section IV. The presented work, therefore, is a necessary step to proving the feasibility of automatic balancing in general. Additionally, the apparent advantages of an automated balancing approach and multi-objective balancing are discussed as well (cf. Sec. V). These discussions and the additional observations in Sec. V strongly indicate that the presented approach was suitable and successful.\nA possible way to proceed with this work is to further optimise the different parts of the approach. For example, the considered optimisers could be improved by better parameters, e.g. determined by tuning methods like sequential parameter optimisation, thereby potentially sharpening our results. In addition, several other modules should be tested for possible (parameter) improvements like the online convergence detection mechanism.\nWith respect to the implemented player AI, it seems reasonable to extend our research by testing different improvements of the probabilistic AI used in our study. This could provide interesting results if the restriction of allowing exactly K2 rounds of play is removed. In this case, the agent is required to plan ahead\nand making more complex strategies profitable. A viable AI extension is inference based reasoning about the opponent\u2019s cards as demonstrated by Buro et al. in their work on improving state evaluation in trick-based card games. Monte Carlo Search is common used for card games as well, as they commonly feature imperfect information (cf. [8, 20]). Another route would be the implementation of AIs that imitate human players.\nFurther work on the analysis of the presented measures and the discovery of new ones is intended. As a first step in this direction, we propose to use our approach for different applications, possibly after developing applicationspecific methods. In that regard, we aim to test our approach on more complex computer games. A first attempt will be made incorporating The Open Racing Car Simulator (TORCS)6, but further tests on real-time-strategy games and platformers are intended as well. Based on the analysis of different well-performing fitness measures, a next step could be the investigation of generalisable ones.\nMore importantly, we plan to evaluate our vision of a balanced deck, our fitness measures and the results of our methods with surveys for human players. In our opinion, incorporating human perception of balancing is the only acceptable way to achieve the eventual goal, i.e. accurately expressing and maximising human players\u2019 enjoyment of a game.\nReferences\n[1] N. Beume, B. Naujoks, and M. Emmerich. SMS-EMOA: Multiobjective Selection Based on Dominated Hypervolume. European Journal of Operational Research, 181(3):1653\u20131669, 2007.\n[2] C. B. Browne. Automatic generation and evaluation of recombination games. Phd thesis, Queensland University of Technology, 2008.\n[3] M. Buro, J. R. Long, T. Furtak, and N. R. Sturtevant. Improving state evaluation,\n6http://torcs.sourceforge.net/\ninference, and search in trick-based card games. In Conference on Artificial Intelligence (IJCAI), pages 1407\u20131413. Morgan Kaufmann, San Francisco, CA, 2009.\n[4] A. B. Cardona, A. W. Hansen, J. Togelius, and M. G. Friberger. Open Trumps, a Data Game. In Foundations of Digital Games (FDG 2014). Society for the Advancement of the Science of Digital Games, Santa Cruz, CA, 2014.\n[5] H. Chen, Y. Mori, and I. Matsuba. Solving the balance problem of massively multiplayer online role-playing games using coevolutionary programming. Applied Soft Computing, 18:1\u201311, 2014.\n[6] K. Deb. Multi-Objective Optimization Using Evolutionary Algorithms. Wiley, Chichester, UK, 2001.\n[7] C. Fonseca, V. D. Fonseca, and L. Paquete. Exploring the performance of stochastic multiobjective optimisers with the second-order attainment function. In C. A. C. Coello et al., editors, Evolutionary Multi-Criterion Optimization (EMO 2005), pages 250\u2013264. Springer, Berlin, 2005. doi: 10.1007/b106458.\n[8] T. Furtak and M. Buro. Recursive Monte Carlo search for imperfect information games. In Computational Intelligence and Games (CIG 2013), pages 225\u2013232. IEEE Press, Piscataway, NJ, 2013.\n[9] G. Hawkins, K. V. Nesbitt, and S. Brown. Dynamic Difficulty Balancing for Cautious Players and Risk Takers. International Journal of Computer Games Technology, 2012: 1\u201310, 2012.\n[10] A. Isaksen, D. Gopstein, J. Togelius, and A. Nealen. Discovering Unique Game Variants. In H. Toivonen et al., editors, Computational Creativity (ICCC 2015). Brigham Young University, Provo, Utah, 2015.\n[11] A. Jaffe. Understanding Game Balance with Quantitative Methods. Phd thesis, University of Washington, 2013.\n[12] J. Knowles, L. Thiele, and E. Zitzler. A Tutorial on the Performance Assessment of Stochastic Multiobjective Optimizers. TIK Report 214, Computer Engineering and Networks Laboratory (TIK), ETH Zurich, 2006.\n[13] A. Liapis, G. N. Yannakakis, and J. Togelius. Sentient sketchbook: Computeraided game level authoring. In Foundations of Digital Games (FDG 2013), pages 213\u2013220. Society for the Advancement of the Science of Digital Games, Santa Cruz, CA, 2013.\n[14] M. J. Nelson and M. Mateas. A requirements analysis for videogame design support tools. In Foundations of Digital Games (FDG 2009), pages 137\u2013144. ACM Press, New York, 2009.\n[15] M. Preuss, J. Togelius, and A. Liapis. Searching for Good and Diverse Game Levels. In Computational Intelligence and Games (CIG 2014), pages 381\u2013388. IEEE Press, Piscataway, NJ, 2014.\n[16] A. M. Smith and M. Mateas. Variations Forever: Flexibly generating rulesets from a sculptable design space of mini-games. In Computational Intelligence and Games (CIG 2010), pages 273\u2013280. IEEE Press, Piscataway, NJ, 2010. doi: 10.1109/ITW.2010. 5593343.\n[17] A. M. Smith, E. Butler, and Z. Popovic\u0301. Quantifying over Play : Constraining Undesirable Solutions in Puzzle Design. In Foundations of Digital Games (FDG 2013), pages 221\u2013228. Society for the Advancement of the Science of Digital Games, Santa Cruz, CA, 2013.\n[18] J. Togelius, M. Preuss, N. Beume, S. Wessing, J. Hagelb\u00e4ck, G. N. Yannakakis, and C. Grappiolo. Controllable procedural\nmap generation via multiobjective evolution. Genetic Programming and Evolvable Machines, 14(2):245\u2013277, 2013.\n[19] H. Trautmann, T. Wagner, B. Naujoks, M. Preuss, and J. Mehnen. Statistical Methods for Convergence Detection of MultiObjective Evolutionary Algorithms. Evo-\nlutionary Computation, 17(4):493\u2013509, 2009. doi: 10.1162/evco.2009.17.4.17403.\n[20] C. D. Ward and P. I. Cowling. Monte Carlo search applied to card selection in Magic: The Gathering. In Computational Intelligence and Games (CIG 2009), pages 9\u201316. IEEE Press, Piscataway, NJ, 2009."}], "references": [{"title": "SMS-EMOA: Multiobjective Selection Based on Dominated Hypervolume", "author": ["N. Beume", "B. Naujoks", "M. Emmerich"], "venue": "European Journal of Operational Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Automatic generation and evaluation of recombination games", "author": ["C.B. Browne"], "venue": "Phd thesis, Queensland University of Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Improving state evaluation, 6http://torcs.sourceforge.net/ 11  inference, and search in trick-based card games", "author": ["M. Buro", "J.R. Long", "T. Furtak", "N.R. Sturtevant"], "venue": "In Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Open Trumps, a Data Game", "author": ["A.B. Cardona", "A.W. Hansen", "J. Togelius", "M.G. Friberger"], "venue": "In Foundations of Digital Games (FDG", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Solving the balance problem of massively multiplayer online role-playing games using coevolutionary programming", "author": ["H. Chen", "Y. Mori", "I. Matsuba"], "venue": "Applied Soft Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Multi-Objective Optimization Using Evolutionary Algorithms", "author": ["K. Deb"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Exploring the performance of stochastic multiobjective optimisers with the second-order attainment function", "author": ["C. Fonseca", "V.D. Fonseca", "L. Paquete"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Recursive Monte Carlo search for imperfect information games", "author": ["T. Furtak", "M. Buro"], "venue": "In Computational Intelligence and Games (CIG", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Dynamic Difficulty Balancing for Cautious Players and Risk Takers", "author": ["G. Hawkins", "K.V. Nesbitt", "S. Brown"], "venue": "International Journal of Computer Games Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Discovering Unique Game Variants", "author": ["A. Isaksen", "D. Gopstein", "J. Togelius", "A. Nealen"], "venue": "Computational Creativity (ICCC", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Understanding Game Balance with Quantitative Methods", "author": ["A. Jaffe"], "venue": "Phd thesis, University of Washington,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "A Tutorial on the Performance Assessment of Stochastic Multiobjective Optimizers", "author": ["J. Knowles", "L. Thiele", "E. Zitzler"], "venue": "TIK Report 214,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Sentient sketchbook: Computeraided game level authoring", "author": ["A. Liapis", "G.N. Yannakakis", "J. Togelius"], "venue": "In Foundations of Digital Games (FDG", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "A requirements analysis for videogame design support tools", "author": ["M.J. Nelson", "M. Mateas"], "venue": "In Foundations of Digital Games (FDG", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Searching for Good and Diverse Game Levels", "author": ["M. Preuss", "J. Togelius", "A. Liapis"], "venue": "In Computational Intelligence and Games (CIG", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Variations Forever: Flexibly generating rulesets from a sculptable design space of mini-games", "author": ["A.M. Smith", "M. Mateas"], "venue": "In Computational Intelligence and Games (CIG", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Quantifying over Play : Constraining Undesirable Solutions in Puzzle Design", "author": ["A.M. Smith", "E. Butler", "Z. Popovi\u0107"], "venue": "In Foundations of Digital Games (FDG", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Controllable procedural 12  map generation via multiobjective evolution", "author": ["J. Togelius", "M. Preuss", "N. Beume", "S. Wessing", "J. Hagelb\u00e4ck", "G.N. Yannakakis", "C. Grappiolo"], "venue": "Genetic Programming and Evolvable Machines,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Statistical Methods for Convergence Detection of Multi- Objective Evolutionary Algorithms", "author": ["H. Trautmann", "T. Wagner", "B. Naujoks", "M. Preuss", "J. Mehnen"], "venue": "Evo-  lutionary Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Monte Carlo search applied to card selection in Magic: The Gathering", "author": ["C.D. Ward", "P.I. Cowling"], "venue": "In Computational Intelligence and Games (CIG", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}], "referenceMentions": [{"referenceID": 13, "context": "[14]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "use an evolutionary algorithm to select cards for top trumps games from open data [4].", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "Jaffe introduces a technique called restricted play that is supposed to enable designer to express balancing goals in terms of the win rate of a suitably restricted agent [11].", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": "ming\u201d [5].", "startOffset": 6, "endOffset": 9}, {"referenceID": 17, "context": "with regards to playability [18],", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "solvability [17], or diversity [15, 10].", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "solvability [17], or diversity [15, 10].", "startOffset": 31, "endOffset": 39}, {"referenceID": 9, "context": "solvability [17], or diversity [15, 10].", "startOffset": 31, "endOffset": 39}, {"referenceID": 8, "context": "Other research areas include dynamic difficulty adaptation for single-player games [9], the generation of rules [2, 16], and more interactive versions of game design, e.", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "Other research areas include dynamic difficulty adaptation for single-player games [9], the generation of rules [2, 16], and more interactive versions of game design, e.", "startOffset": 112, "endOffset": 119}, {"referenceID": 15, "context": "Other research areas include dynamic difficulty adaptation for single-player games [9], the generation of rules [2, 16], and more interactive versions of game design, e.", "startOffset": 112, "endOffset": 119}, {"referenceID": 12, "context": "mixed-initiative [13].", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "Switching from single- to multi-objective optimisation has advantages and disadvantages [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "In the evolutionary multi-objective optimiser considered, SMS-EMOA [1], this is done based on the con-", "startOffset": 67, "endOffset": 70}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Deb [6].", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "For this purpose, (empirical) attainment functions that describe the sets of goals achieved with different approaches were proposed, expressing both the quality of the achieved solutions as well as their spread over multiple optimisation runs [7].", "startOffset": 243, "endOffset": 246}, {"referenceID": 6, "context": "ment-functions of two optimisers [7].", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "In the course of this paper, we compare 8 card sets from purchased decks with decks generated using three different approaches and corresponding fitness functions detailed in the following: \u2022 Single-objective optimisation according to the dominance-related (D) measure proposed in [4] which describes the distance of the cards in a deck V to the Pareto front:", "startOffset": 281, "endOffset": 284}, {"referenceID": 0, "context": "\u2022 The valid range of all values vl,k is set to [1, 10] \u2208 R, which all decks can be transformed to.", "startOffset": 47, "endOffset": 54}, {"referenceID": 9, "context": "\u2022 The valid range of all values vl,k is set to [1, 10] \u2208 R, which all decks can be transformed to.", "startOffset": 47, "endOffset": 54}, {"referenceID": 10, "context": "[11]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The findings by Nelson and Mateas also connote that designers see potential in automatic balancing tools to support the balancing process [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 3, "context": "[4] could result in better performing individuals.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] is conducted using software published by C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19]) with regards to all values achieved (cf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The performances of the resulting and purchased decks next to a single-objective approach [4] are evaluated using statistical analyses.", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "[8, 20]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 19, "context": "[8, 20]).", "startOffset": 0, "endOffset": 7}], "year": 2016, "abstractText": "Game balancing is an important part of the (computer) game design process, in which designers adapt a game prototype so that the resulting gameplay is as entertaining as possible. In industry, the evaluation of a game is often based on costly playtests with human players. It suggests itself to automate this process using surrogate models for the prediction of gameplay and outcome. In this paper, the feasibility of automatic balancing using simulationand deck-based objectives is investigated for the card game top trumps. Additionally, the necessity of a multi-objective approach is asserted by a comparison with the only known (single-objective) method. We apply a multi-objective evolutionary algorithm to obtain decks that optimise objectives, e.g. win rate and average number of tricks, developed to express the fairness and the excitement of a game of top trumps. The results are compared with decks from published top trumps decks using simulation-based objectives. The possibility to generate decks better or at least as good as decks from published top trumps decks in terms of these objectives is demonstrated. Our results indicate that automatic balancing with the presented approach is feasible even for more complex games such as real-time", "creator": "LaTeX with hyperref package"}}}