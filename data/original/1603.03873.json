{"id": "1603.03873", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2016", "title": "Neural Discourse Relation Recognition with Semantic Memory", "abstract": "Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts. Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion. We refer to this recognizer as SeMDER. Starting from word embeddings of discourse arguments, SeMDER employs a shallow encoder to generate a distributed surface representation for a discourse. A semantic encoder with attention to the semantic memory matrix is further established over surface representations. It is able to retrieve a deep semantic meaning representation for the discourse from the memory. Using the surface and semantic representations as input, SeMDER finally predicts implicit discourse relations via a neural recognizer. Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56\\% on average over current state-of-the-art baselines in terms of F1-score.", "histories": [["v1", "Sat, 12 Mar 2016 08:54:16 GMT  (191kb,D)", "http://arxiv.org/abs/1603.03873v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biao zhang", "deyi xiong", "jinsong su"], "accepted": false, "id": "1603.03873"}, "pdf": {"name": "1603.03873.pdf", "metadata": {"source": "CRF", "title": "Neural Discourse Relation Recognition with Semantic Memory", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "emails": ["zb@stu.xmu.edu.cn,", "jssu@xmu.edu.cn", "dyxiong@suda.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Discourse relation recognition (DRR) that automatically identifies the logical relation of a coherent text is very important for discourse-level comprehension. It is relevant to a variety of nature language processing tasks such as summarization [Yoshida et al., 2014], machine translation [Guzma\u0301n et al., 2014], question answering [Jansen et al., 2014] and information extraction [Cimiano et al., 2005]. Although explicit DRR has recently achieved remarkable success [Miltsakaki et al., 2005; Pitler et al., 2008], implicit DRR still remains a serious challenge due to the absence of discourse connectives.\nHowever, even if discourse connectives are not provided, humans can still easily succeed in recognizing the relations of discourse arguments. One reason for this, according to cognitive psychology, would be that humans have a semantic memory in mind, which helps them comprehend word senses and further argument meanings via composition. After understanding what two arguments of a discourse convey, humans can easily interpret the discourse relation of the two\narguments. This semantic memory, as discussed by Tulving [1972], refers to general knowledge including \u201cwords and other verbal symbols, their meaning and referents, about relations among them, and about rules, formulas, and algorithms for manipulating them\u201d. It can be retrieved to help disambiguation and comprehension whenever the barrier of cognition occurs.\nConsider the implicit discourse relation between the following two sentences:\n(1) I was prepared to be in a very bad mood tonight. Now, I feel maybe there\u2019s a little bit of euphoria.\nIt is difficult for conventional discourse relation recognizers to identify the relation between the two sentences as there is little significant surface information for use. However, if the recognizer obtains the knowledge of the antonymous relationship between the meaning of \u201cbad mood\u201d and that of \u201ceuphoria\u201d, it will be easy to infer the COMPARISON relation between the two sentences. This semantic knowledge can be stored in an external memory for a discourse recognizer just like the semantic memory for humans.\nInspired by the semantic memory in cognitive neuroscience [Yee et al., 2014] as well as memory network [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] and attentional mechanisms [Mnih et al., 2014; Bahdanau et al., 2014; Xu et al., 2015], we propose a neural network with se-\nar X\niv :1\n60 3.\n03 87\n3v 1\n[ cs\n.C L\n] 1\n2 M\nar 2\n01 6\nmantic memory for implicit DRR, which refers to SeMDER. The philosophy of SeMDER includes: (1) the external semantic memory should be distributed as this allows easy computation; (2) the semantic memory should be easily accessed and retrieved; and (3) the retrieved content should be integrated into the comprehension of meanings of discourse arguments and their relations. In order to meet these requirements, we use a distributed matrix that encodes semantic knowledge of words as our external memory. The distributed memory is retrieved via an attentive reader. The retrieved distributed knowledge is incorporated into semantic representations of discourse arguments. Practically, we build a neural network that is composed of three essential components: a shallow encoder, a semantic encoder and a neural recognizer. The neural network is visualized in Figure 1. In particular,\n\u2022 Shallow encoder: we feed word embeddings of discourse arguments into a shallow encoder [Zhang et al., 2015] to obtain shallow representations of arguments. Due to their shallow property, we refer to them as surface representations (see Section 3.1);\n\u2022 Semantic encoder: we retrieve the semantic memory via an attention model. The retrieved content, together with surface representations, are incorporated into the semantic encoder to obtain deep semantic representations (see Section 3.2);\n\u2022 Neural recognizer: both surface and semantic representations are feed into a neural recognizer to predict the corresponding discourse relations (see Section 3.3).\nOur contributions are twofold. First, we propose a neural network architecture for implicit DRR with an encoded semantic memory that enhances representations of arguments. To the best of our knowledge, we are the first to explore semantic memory for DRR via attentional mechanisms. Second, we conduct a series of experiments for English implicit DRR on the PDTB-style corpus to evaluate the effectiveness of our proposed neural network and semantic memory. Experiment results show that our network achieves substantial improvements against several strong baselines in term of F1 score. Extensive analysis on the attention further indicates that our model can recognize some important relationrelevant words, which we conjecture is the main reason for our success."}, {"heading": "2 Related Work", "text": "The release of Penn Discourse Treebank (PDTB) [Prasad et al., 2008] opens the door to machine learning based implicit DRR. A variety of machine learning strategies have been presented previously, including feature engineering, connective predicting, data selection and discourse representation via neural networks.\nResearch on feature engineering exploits powerful and discriminative features for implicit DRR. In this respect, Pilter et al. [2009] investigate several linguistically informed features, such as polarity tags, verb classes, modality, context and lexical features. Lin et al. [2009] further consider contextual words, word pairs and parse trees for feature engineering. Later, several more powerful features have been developed:\naggregated word pairs [McKeown and Biran, 2013], Brown clusters and coreference patterns [Rutherford and Xue, 2014]. With these features, Park and Cardie [2012] perform feature set optimization for better feature combination.\nThe major difference between explicit and implicit DRR is the presence of discourse connectives, the most salient features for DRR. Therefore, if we find a way to predict connectives for implicit discourses, we can transform implicit DRR into explicit DRR. Along this line, Zhou et al. [2010] use a language model to automatically insert discourse connectives, while Patterson and Kehler [2013] use a classifier to predict the presence or omission of a lexical connective. Different from this prediction strategy, Hong et al. [2012] leverage discourse connectives as a bridge between explicit and implicit relations and adopt an unsupervised cross-argument inference mechanism.\nYet another strategy is data selection, where explicit discourse instances that are similar to the implicit ones are found and added to training corpus. Different data selection methods for implicit DRR can be classified into the following categories: instance typicality [Wang et al., 2012], multi-task learning [Lan et al., 2013], domain adaptation [Braud and Denis, 2014; Ji et al., 2015], semi-supervised learning [Hernault et al., 2010; Fisher and Simmons, 2015] and explicit discourse connective classification [Rutherford and Xue, 2015].\nThe third strategy is to learn representations of disourse arguments using neural networks for relation recognition, following remarkable success of neural networks in various natural language processing tasks. In this respect, Braud and Denis [2015] investigates the usefulness of word representations. Specifically, two different neural network models have been developed for implicit DRR: recursive neural network for entity-augmented distributed semantics [Ji and Eisenstein, 2015] and shallow convolutional neural network for discourse representation [Zhang et al., 2015]. The former incorporates coreferent entity mentions into compositional distributed representations, while the latter develops a pure neural network model for discourse representations in implicit DRR. Normally, entities utilized in the former heavily depend on the availability and robustness of an upstream coreference system, and the latter only learns shallow representations for discourse arguments. Instead, our proposed model does not rely on any linguistic resources and incorporates a semantic memory to obtain deep semantic representations over shallow representations in [Zhang et al., 2015]. Additionally, since the semantic memory is represented as a distributed matrix, our model is more robust and adaptable.\nThe exploration of semantic memory for implicit DRR is inspired by recent developments in cognitive neuroscience. Yee et al. [2014] show how this memory is organized and retrieved in brain. In order to explore semantic memory in neural networks, we borrow ideas from recently introduced memory networks [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] to organize semantic memory as a distributed matrix and use an attention model to retrieve this distributed memory. The adaptation and utilization of semantic memory into implicit DRR, to the best of our knowledge, has never been investigated before."}, {"heading": "3 The SeMDER Model", "text": "This section elaborates the proposed SeMDER model. We will first present the shallow encoder which converts a discourse into a distributed embedding. The semantic encoder, where the semantic memory is incorporated via an attention model is then described. After that, we explain how the neural recognizer classifies discourse relations. We also discuss the objective function and the procedure for parameter learning in this section."}, {"heading": "3.1 Shallow Encoder", "text": "To obtain surface representations for discourses, we employ a shallow convolutional neural network (SCNN) [Zhang et al., 2015] as our shallow encoder. SCNN is specifically designed on the PDTB corpus, where implicit discourse relations are annotated between two neighboring arguments, namely Arg1 and Arg2 (see the example in Figure 1). Given an argument which consists of n words, SCNN represents each word as a d-dimensional dense, real-valued vector xi \u2208 Rd1 and concatenates them into a word embedding matrix:\nX = (x1, x2, . . . , xn) (1)\nwhere X \u2208 Rd1\u00d7n forms the input layer of SCNN. All word vectors in vocabulary V are stacked into a parameter matrix L \u2208 Rd1\u00d7|V | (|V | is the vocabulary size), which will be tuned in the training phase.\nTo represent a discourse argument c, SCNN extracts major information inside X through three convolution operations avg, min and max defined as follows:\ncavgr = 1\nn n\u2211 i Xr,i (2)\ncminr = min (Xr,1, Xr,2, . . . , Xr,n) (3)\ncmaxr = max (Xr,1, Xr,2, . . . , Xr,n) (4)\nwhere r indicates the row of X. The argument c is thereby represented as the concatenation of these convolutional features:\nc = [ cavg; cmax; cmin ] (5)\nSCNN further obtains the representation p \u2208 R6d1 of a discourse by performing nonlinear transformations on the concatenation of two argument embeddings cArg1 and cArg2 generated in Eq. 5 as follows:\np = g([cArg1; cArg2]), g(x) = tanh(x)\n||tanh(x)|| (6)\nDespite its simplicity, SCNN outperforms several featurebased systems. This is the reason that we choose it as our shallow encoder to obtain surface representations of discourses. However, the lack of deep knowledge in SCNN limits its further development. We therefore introduce a deep semantic encoder over the shallow encoder, which will be elaborated in the next section."}, {"heading": "3.2 Semantic Encoder", "text": "Upon the surface representations, we further build a semantic encoder to incorporate a semantic memory to strengthen discourse comprehension. The semantic memory in SeMDER is represented as a distributed matrix M \u2208 Rm\u00d7d2 , where d2 is the dimension of word embedding in the memory. Each row in the matrix indicates one word in discourse arguments (thus typically m \u2264 n). We assume that the semantic and syntactic attributes of words have already been encoded into this matrix. Therefore, incorporating this memory information into discourse representations will be beneficial for implicit DRR task.\nFigure 2 gives an illustration of the procedure for incorporating the semantic memory. Specifically, given the surface representation p for a discourse and the semantic memory matrix M , we stack an attention layer to project them onto the same space, which we call attention space. The projection is done as follows:\npa = f(Wpp+ ba) (7)\nMa = f(WmM T + ba) (8)\nwhere the subscript a denotes the attention space, pa and Ma are the attentional representations for p and M respectively. Wp \u2208 Rda\u00d76d1 ,Wm \u2208 Rda\u00d7d2 are transformation matrices, ba \u2208 Rda is the bias term, da is the dimensionality of the attention space, and f(\u00b7) is an element-wise activation function such as tanh(\u00b7), which is used throughout our model. The arrows marked by \u201c 1\u00a9\u201d in Figure 2 show this projection process.\nNote that we differentiate the transformation matrix Wp in Eq. 7 to the Wm in Eq. 8, since the surface representation and semantic memory are from different semantic spaces. However, we share the same bias term for them. This will force our model to learn to encode attention semantics into the transformation matrices, rather than the biases.\nAfter obtaining the attentional representations for the discourse and semantic memory, we further estimate how useful each word memory cell i in the semantic memory (i.e., the ith row in M ) is to the corresponding discourse. This can be\ncalculated by a match score:\nsi = g(pa,Ma,i) (9)\nwhere g(\u00b7) is the scoring function. Since we are only interested in words occurred in the corresponding discourse, our attention schema is somewhat like a local attention. As discussed in [Luong et al., 2015], a general scoring function is much better for the local attention. Thus, we use a variant of the general function as our scoring function (see the red box in Figure 2):\ng(pa,Ma,i) = paWsMa,i (10)\nwhere Ws \u2208 Rda\u00d7da is the bilinear scoring matrix, in which each element (see the red node in Figure 2) represents an interaction between the corresponding dimension of pa and Ma,i.\nWe further normalize the match score vector in Eq. 9 to generate a probabilistic attention distribution over words in the semantic memory:\n\u03b1i = exp(si)\u2211m j=1 exp(sj)\n(11)\nIntuitively, the probability \u03b1i (a.k.a attention weight) reflects the importance of the word Mi in representing the whole discourse with respect to the final discourse relation recognition. Recall the above-mentioned example (1), if the importance of words \u201cbad mood\u201d and \u201ceuphoria\u201d is recognized, there would be more chance that the final recognizer succeeds.\nBased on this attention distribution, we can compute the semantic representation for a discourse as a weighted sum of words in the semantic memory according to \u03b1 (see the arrows marked by \u201c 3\u00a9\u201d in Figure 2):\npk = m\u2211 j=1 \u03b1jMj (12)\nAs shown in Eq. 12, the semantic representation is directly retrieved from the semantic memory. It encodes semantic knowledge of words in discourse arguments that can help discourse relation recognition."}, {"heading": "3.3 Neural Recognizer", "text": "Up to now, we have inferred both the surface and semantic representation for a discourse. To recognize the discourse relation, we further stack a Softmax layer upon these two representations:\nyp = h(Wr,pp+Wr,kpk + br) (13)\nwhere h(\u00b7) is the softmax function, Wr,p \u2208 Rl\u00d76d1 ,Wr,k \u2208 Rl\u00d7d2 and br \u2208 Rl are the parameter matrices and bias term respectively, and l indicates the number of discourse relations."}, {"heading": "3.4 Objective Function and Parameter Learning", "text": "Given a training corpus which contains T instances {(x, y)}Tt=1, we employ the following cross-entropy error to\naccess how well the predicted relation yp represents the gold relation y,\nE(yp, y) = \u2212 l\u2211 j yj \u00d7 log (yp,j) (14)\nTherefore, the joint training objective of SeMDER is defined as follows:\nJ(\u03b8) = 1\nT T\u2211 t=1 E(y(t)p , y (t)) +R(\u03b8) (15)\nwhere R(\u03b8) is the regularization term with respect to \u03b8. Towards the parameters \u03b8, we divide them into three different sets: \u2022 \u03b8L : word embedding matrix L; \u2022 \u03b8R : discourse relation recognition parameters Wr,p,Wr,k and br; \u2022 \u03b8M : memory-related parameters Wp,Wm,Ws and ba;\nAll these parameters are regularized with corresponding weights1:\nR(\u03b8) = \u03bbL 2 \u2016\u03b8L\u20162 + \u03bbR 2 \u2016\u03b8R\u20162 + \u03bbM 2 \u2016\u03b8M\u20162 (16)\nNotice that although we can fine-tune the semantic memory in an end-to-end manner, we do not do that in our model. This is because we hope that the semantic and syntactic attributes encoded in the semantic memory can be preserved throughout our neural network.\nWe apply Limited-memory Broyden-Fletcher-GoldfarbShanno (L-BFGS) algorithm to optimize each parameter. In order to run the L-BFGS algorithm, we need to solve two problems: parameter initialization and partial gradient calculation.\nIn the phase of parameter initialization, \u03b8R and \u03b8M are randomly set according to a normal distribution (\u00b5 = 0, \u03c3 = 0.01). For the word embedding \u03b8L, we use the toolkit Word2Vec2 to perform pretraining on a large-scale unlabeled data. This word embedding will be further fine-tuned in our SeMDER model to capture much more semantics related to discourse relations.\nThe partial gradient for parameter \u03b8j is computed as follows:\n\u2202J \u2202\u03b8j = 1 T T\u2211 t=1 \u2202E(y (t) p , y(t)) \u2202\u03b8j + \u03bbj\u03b8j (17)\nThis gradient will be feed into the toolkit libLBFGS3 for parameter updating in our practical implementation."}, {"heading": "4 Experiments", "text": "In this section, we conducted a series of experiments on English implicit DRR task. We begin with a brief review of the PDTB dataset. Then, we describe our experiment setup. Finally, we present experiment results and give an in-depth analysis on the attention.\n1The bias terms b is not regularized in practice. 2https://code.google.com/p/word2vec/ 3http://www.chokkan.org/software/liblbfgs/"}, {"heading": "4.1 Dataset", "text": "We used PDTB 2.0 corpus4 [Prasad et al., 2008] (PDTB thereafter), which is the largest hand-annotated discourse corpus. Discourse relations are annotated in a predicateargument view in PDTB, where each discourse connective is treated as a predicate that takes two text spans as its arguments. The relation tags in PDTB are arranged in a threelevel hierarchy, where the top level consists of four major semantic classes: TEMPORAL (TEM), CONTINGENCY (CON), EXPANSION (EXP) and COMPARISON (COM). Because the top-level relations are general enough to be annotated with a high inter-annotator agreement and are common to most theories of discourse, in our experiments we only use this level of annotations.\nPDTB contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used sections 2-20 as our training set, sections 21-22 as the test set. Sections 0-1 were used as the development set for hyperparameter optimization. We formulated the task as four separate one-against-all binary classification problems: each top level class vs. the other three discourse relation classes. We also balanced the training set by resampling training instances in each class until the number of positive and negative instances are equal. In contrast, all instances in the test and development set are kept in nature. The statistics of various data sets is listed in Table 1."}, {"heading": "4.2 Setup", "text": "We selected the GoogleNews-vectors-negative3005 as our external semantic memory. This data contains 300-dimensional vectors (thus, d2 = 300) for 3 million words and phrases. It is trained on part of Google News dataset (about 100 billion words). The wide coverage and newswire domain of its training corpus as well as the syntactic property of word2vec models make this vector a good choice for the semantic memory.\nWe tokenized all datasets using Stanford NLP Toolkit6, and employed a large-scale unlabeled data7 including 1.02M\n4http://www.seas.upenn.edu/ pdtb/ 5https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS 21pQmM/edit?pref=2&pli=1 6http://nlp.stanford.edu/software/corenlp.shtml 7This data contains the training and development set for implicit DRR, as well as the English sentences in the FBIS corpus and the English sentences in Hansards part of LDC2004T07 corpus.\nsentences (33.5M words) for word embedding \u03b8L initialization. We optimized the hyperparameters d1, \u03bbL, \u03bbR, \u03bbM according to previous work [Zhang et al., 2015] and preliminary experiments on the development set. Finally, we set d1 = 128, \u03bbL = 1e \u22125, \u03bbR = \u03bbM = 1e \u22124 for all the experiments. With respect to da, we tried three different settings da = 32, 64, 128.\nTo validate the effectiveness of SeMDER model, we compared against the following baseline methods:\n\u2022 SVM: a support vector machine (SVM) classifier trained with the labeled data in the training set. We used the toolkit SVM-light8 to train the classifier in our experiments.\n\u2022 SCNN: a shallow convolutional neural model proposed by Zhang et al. [2015].\nFeatures used in SVM experiments are taken from the stateof-the-art implicit discourse relation recognition model, including Bag of Words, Cross-Argument Word Pairs, Polarity, First-Last, First3, Production Rules, Dependency Rules and Brown cluster pair [Rutherford and Xue, 2014]. Additionally, in order to collect bag of words, production rules, dependency rules, and cross-argument word pairs, we used a frequency cutoff of 5 to remove rare features, following Lin et al. [2009]."}, {"heading": "4.3 Classification Results", "text": "Because of the imbalance nature in our test set, we choose F1 score as our major evaluation metric. The performance of different models is presented in Table 2, which, overall, shows that SeMDER outperforms the two baselines, achieving improvements in F1 score of 1.14% on COM, 1.66% on CON, 1.36% on EXP and 5.62% on TEM over the best baseline results. We further observe that the improvements mainly result from high precision for COM, CON and TEM, while high recall for EXP. This is reasonable since the EXP relation owns the largest number of instances in our data.\nAs the neural baseline, SCNN outperforms SVM on CON, EXP and TEM, but fails on COM. The SeMDER with semantic memory, however, consistently surpasses SVM and SCNN in all discourse relations. This suggests that the incorporated semantic memory is helpful for recognizing correct discourse relations. Additionally, for SeMDER, increasing the attention space dimensionality da from 32 to 128 improves the performance in most cases.\nYet another interesting observation from Table 2 is that the improvement of SeMDER over the two baselines for relation TEM is the biggest. The gain over SVM is 11.4% and 5.6% over SCNN. This improvement is largely due to high precisions. As the number of instances in relation TEM is the smallest (see Table 1), we argue that the traditional neural network models may suffer from overfitting in this case. However, our SeMDER enhanced with the semantic memory is capable of generalization that alleviates this overfitting issue.\n8http://svmlight.joachims.org/"}, {"heading": "4.4 Attention Analysis", "text": "We would like to know more about what role the semantic memory plays in our model, especially what the model learns from this semantic memory. Analyzing semantic representations is relatively meaningless. Therefore we turn to look into words with high attention weights for the answer.\nWe present one example per discourse relation from the test set in Table 3, where words assigned with the top-5 attention weights are listed separately. Consider the example for COM, our model retrieves the words \u201cwrong, people, dead, think, smokestack\u201d, which roughly reflect the discourse meaning that people think smokestack, dead wrong. Obviously, these words are crucial for discourse comprehension. These examples display that SeMDER prefers to retrieve from the semantic memory relation-relevant words that strongly indicate the corresponding relations, which we think is the main reason for the success of SeMDER."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we have presented a neural discourse relation recognizer with a distributed semantic memory for implicit DRR. The semantic memory encodes semantic knowledge of words in discourse arguments and helps disambiguation and comprehension. We employ an attention model to retrieve\ndiscourse relation-relevant information into semantic representations of discourses, which, to some extend, simulates the cognition process of humans. Experiment results show that our model outperforms several strong baselines, and further analysis reveals that our model can indeed detect some relation-relevant words.\nIn the future, we would like to exploit different types of semantic memory, e.g., a distributed memory on ontology concepts and relations. We also want to explore different attention architectures, e.g. the concat and dot in [Luong et al., 2015]. Furthermore, we are interested in adapting our model to other similar classification tasks, such as sentiment classification, movie review classification and nature language inference."}], "references": [{"title": "and Yoshua Bengio", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho"], "venue": "Neural machine translation by jointly learning to align and translate.", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 1694\u20131705", "author": ["Chlo\u00e9 Braud", "Pascal Denis. Combining natural", "artificial examples to improve implicit discourse relation identification. In Proc. of COLING"], "venue": "August", "citeRegEx": "Braud and Denis. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Data & Knowledge Engineering", "author": ["Philipp Cimiano", "Uwe Reyle", "Jasmin \u0160ari\u0107. Ontology-driven discourse analysis for information extraction"], "venue": "55:59\u201383,", "citeRegEx": "Cimiano et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "pages 89\u201393", "author": ["Robert Fisher", "Reid Simmons. Spectral semi-supervised discourse relation classification. In Proc. of ACL-IJCNLP"], "venue": "July", "citeRegEx": "Fisher and Simmons. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Llu\u0131\u0301s M\u00e0rquez", "author": ["Francisco Guzm\u00e1n", "Shafiq Joty"], "venue": "and Preslav Nakov. Using discourse structure improves machine translation evaluation. In Proc. of ACL, pages 687\u2013698, June", "citeRegEx": "Guzm\u00e1n et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proc", "author": ["Hugo Hernault", "Danushka Bollegala", "Mitsuru Ishizuka. A Semi-Supervised Approach to Improve Classification of Infrequent Discourse Relations Using Feature Vector Extension"], "venue": "of EMNLP,", "citeRegEx": "Hernault et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "of CIKM", "author": ["Yu Hong", "Xiaopei Zhou", "Tingting Che", "Jianmin Yao", "Qiaoming Zhu", "Guodong Zhou. Cross-argument inference for implicit discourse relation recognition. In Proc"], "venue": "pages 295\u2013304,", "citeRegEx": "Hong et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "pages 977\u2013986", "author": ["Peter Jansen", "Mihai Surdeanu", "Peter Clark. Discourse complements lexical semantics for non-factoid answer reranking. In Proc. of ACL"], "venue": "June", "citeRegEx": "Jansen et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "One vector is not enough: Entity-augmented distributed semantics for discourse relations", "author": ["Yangfeng Ji", "Jacob Eisenstein"], "venue": "TACL, pages 329\u2013344,", "citeRegEx": "Ji and Eisenstein. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Closing the gap: Domain adaptation from explicit to implicit discourse relations", "author": ["Yangfeng Ji", "Gongbo Zhang", "Jacob Eisenstein"], "venue": "Proc. of EMNLP, pages 2219\u20132224, September", "citeRegEx": "Ji et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "CoRR,", "citeRegEx": "Kumar et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 476\u2013 485", "author": ["Man Lan", "Yu Xu", "Zhengyu Niu. Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition. In Proc. of ACL"], "venue": "Sofia, Bulgaria, August", "citeRegEx": "Lan et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "of EMNLP", "author": ["Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proc"], "venue": "pages 343\u2013351,", "citeRegEx": "Lin et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Proc", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning. Effective approaches to attention-based neural machine translation"], "venue": "of EMNLP,", "citeRegEx": "Luong et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "of ACL", "author": ["Kathleen McKeown", "Or Biran. Aggregated word pair features for implicit discourse relation disambiguation. In Proc"], "venue": "pages 69\u201373,", "citeRegEx": "McKeown and Biran. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proc", "author": ["Eleni Miltsakaki", "Nikhil Dinesh", "Rashmi Prasad", "Aravind Joshi", "Bonnie Webber. Experiments on sense annotations", "sense disambiguation of discourse connectives"], "venue": "of TLT2005,", "citeRegEx": "Miltsakaki et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and koray kavukcuoglu", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "Recurrent models of visual attention. In Proc. of NIPS, pages 2204\u20132212.", "citeRegEx": "Mnih et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 108\u2013112", "author": ["Joonsuk Park", "Claire Cardie. Improving Implicit Discourse Relation Recognition Through Feature Set Optimization. In Proc. of SIGDIAL"], "venue": "Seoul, South Korea, July", "citeRegEx": "Park and Cardie. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "of EMNLP", "author": ["Gary Patterson", "Andrew Kehler. Predicting the presence of discourse connectives. In Proc"], "venue": "pages 914\u2013923,", "citeRegEx": "Patterson and Kehler. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Technical Reports (CIS)", "author": ["Emily Pitler", "Mridhula Raghupathy", "Hena Mehta", "Ani Nenkova", "Alan Lee", "Aravind K Joshi. Easily identifiable discourse relations"], "venue": "page 884,", "citeRegEx": "Pitler et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "pages 683\u2013691", "author": ["Emily Pitler", "Annie Louis", "Ani Nenkova. Automatic sense prediction for implicit discourse relations in text. In Proc. of ACL-AFNLP"], "venue": "August", "citeRegEx": "Pitler et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Bonnie L Webber", "author": ["Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K Joshi"], "venue": "The penn discourse treebank 2.0. In LREC. Citeseer,", "citeRegEx": "Prasad et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "pages 645\u2013654", "author": ["Attapol Rutherford", "Nianwen Xue. Discovering implicit discourse relations through brown cluster pair representation", "coreference patterns. In Proc. of EACL"], "venue": "April", "citeRegEx": "Rutherford and Xue. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 799\u2013808", "author": ["Attapol Rutherford", "Nianwen Xue. Improving the inference of implicit discourse relations via classifying explicit discourse connectives. In Proc. of NAACL-HLT"], "venue": "May\u2013June", "citeRegEx": "Rutherford and Xue. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "arthur szlam", "author": ["Sainbayar Sukhbaatar"], "venue": "Jason Weston, and Rob Fergus. End-to-end memory networks. In Proc. of NIPS, pages 2431\u20132439.", "citeRegEx": "Sukhbaatar et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "editors", "author": ["Endel Tulving. Episodic", "semantic memory. In Endel Tulving", "W. Donaldson"], "venue": "Organization of Memory, pages 381\u2013403. Academic Press, New York,", "citeRegEx": "Tulving. 1972", "shortCiteRegEx": null, "year": 1972}, {"title": "of COLING", "author": ["Xun Wang", "Sujian Li", "Jiwei Li", "Wenjie Li. Implicit discourse relation recognition by selecting typical training examples. In Proc"], "venue": "pages 2757\u20132772,", "citeRegEx": "Wang et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "CoRR", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes. Memory networks"], "venue": "abs/1410.3916,", "citeRegEx": "Weston et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio. Show"], "venue": "Proc. of ICML, pages 2048\u20132057,", "citeRegEx": "Xu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Sharon L Thompson-Schill", "author": ["Eiling Yee", "Evangelia G Chrysikou"], "venue": "The cognitive neuroscience of semantic memory,", "citeRegEx": "Yee et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 1834\u20131839", "author": ["Yasuhisa Yoshida", "Jun Suzuki", "Tsutomu Hirao", "Masaaki Nagata. Dependency-based discourse parser for single-document summarization. In Proc. of EMNLP"], "venue": "October", "citeRegEx": "Yoshida et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "of EMNLP", "author": ["Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao. Shallow convolutional neural network for implicit discourse relation recognition. In Proc"], "venue": "September", "citeRegEx": "Zhang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "of COLING", "author": ["Zhi-Min Zhou", "Yu Xu", "Zheng-Yu Niu", "Man Lan", "Jian Su", "Chew Lim Tan. Predicting discourse connectives for implicit discourse relation recognition. In Proc"], "venue": "pages 1507\u20131514,", "citeRegEx": "Zhou et al.. 2010", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 30, "context": "It is relevant to a variety of nature language processing tasks such as summarization [Yoshida et al., 2014], machine translation [Guzm\u00e1n et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 4, "context": ", 2014], machine translation [Guzm\u00e1n et al., 2014], question answering [Jansen et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 7, "context": ", 2014], question answering [Jansen et al., 2014] and information extraction [Cimiano et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 2, "context": ", 2014] and information extraction [Cimiano et al., 2005].", "startOffset": 35, "endOffset": 57}, {"referenceID": 15, "context": "Although explicit DRR has recently achieved remarkable success [Miltsakaki et al., 2005; Pitler et al., 2008], implicit DRR still remains a serious challenge due to the absence of discourse connectives.", "startOffset": 63, "endOffset": 109}, {"referenceID": 19, "context": "Although explicit DRR has recently achieved remarkable success [Miltsakaki et al., 2005; Pitler et al., 2008], implicit DRR still remains a serious challenge due to the absence of discourse connectives.", "startOffset": 63, "endOffset": 109}, {"referenceID": 29, "context": "Inspired by the semantic memory in cognitive neuroscience [Yee et al., 2014] as well as memory network [Weston et al.", "startOffset": 58, "endOffset": 76}, {"referenceID": 27, "context": ", 2014] as well as memory network [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] and attentional mechanisms [Mnih et al.", "startOffset": 34, "endOffset": 100}, {"referenceID": 24, "context": ", 2014] as well as memory network [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] and attentional mechanisms [Mnih et al.", "startOffset": 34, "endOffset": 100}, {"referenceID": 10, "context": ", 2014] as well as memory network [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] and attentional mechanisms [Mnih et al.", "startOffset": 34, "endOffset": 100}, {"referenceID": 16, "context": ", 2015] and attentional mechanisms [Mnih et al., 2014; Bahdanau et al., 2014; Xu et al., 2015], we propose a neural network with sear X iv :1 60 3.", "startOffset": 35, "endOffset": 94}, {"referenceID": 0, "context": ", 2015] and attentional mechanisms [Mnih et al., 2014; Bahdanau et al., 2014; Xu et al., 2015], we propose a neural network with sear X iv :1 60 3.", "startOffset": 35, "endOffset": 94}, {"referenceID": 28, "context": ", 2015] and attentional mechanisms [Mnih et al., 2014; Bahdanau et al., 2014; Xu et al., 2015], we propose a neural network with sear X iv :1 60 3.", "startOffset": 35, "endOffset": 94}, {"referenceID": 31, "context": "\u2022 Shallow encoder: we feed word embeddings of discourse arguments into a shallow encoder [Zhang et al., 2015] to obtain shallow representations of arguments.", "startOffset": 89, "endOffset": 109}, {"referenceID": 21, "context": "The release of Penn Discourse Treebank (PDTB) [Prasad et al., 2008] opens the door to machine learning based implicit DRR.", "startOffset": 46, "endOffset": 67}, {"referenceID": 14, "context": "Later, several more powerful features have been developed: aggregated word pairs [McKeown and Biran, 2013], Brown clusters and coreference patterns [Rutherford and Xue, 2014].", "startOffset": 81, "endOffset": 106}, {"referenceID": 22, "context": "Later, several more powerful features have been developed: aggregated word pairs [McKeown and Biran, 2013], Brown clusters and coreference patterns [Rutherford and Xue, 2014].", "startOffset": 148, "endOffset": 174}, {"referenceID": 26, "context": "Different data selection methods for implicit DRR can be classified into the following categories: instance typicality [Wang et al., 2012], multi-task learning [Lan et al.", "startOffset": 119, "endOffset": 138}, {"referenceID": 11, "context": ", 2012], multi-task learning [Lan et al., 2013], domain adaptation [Braud and Denis, 2014; Ji et al.", "startOffset": 29, "endOffset": 47}, {"referenceID": 1, "context": ", 2013], domain adaptation [Braud and Denis, 2014; Ji et al., 2015], semi-supervised learning [Hernault et al.", "startOffset": 27, "endOffset": 67}, {"referenceID": 9, "context": ", 2013], domain adaptation [Braud and Denis, 2014; Ji et al., 2015], semi-supervised learning [Hernault et al.", "startOffset": 27, "endOffset": 67}, {"referenceID": 5, "context": ", 2015], semi-supervised learning [Hernault et al., 2010; Fisher and Simmons, 2015] and explicit discourse connective classification [Rutherford and Xue, 2015].", "startOffset": 34, "endOffset": 83}, {"referenceID": 3, "context": ", 2015], semi-supervised learning [Hernault et al., 2010; Fisher and Simmons, 2015] and explicit discourse connective classification [Rutherford and Xue, 2015].", "startOffset": 34, "endOffset": 83}, {"referenceID": 23, "context": ", 2010; Fisher and Simmons, 2015] and explicit discourse connective classification [Rutherford and Xue, 2015].", "startOffset": 83, "endOffset": 109}, {"referenceID": 8, "context": "Specifically, two different neural network models have been developed for implicit DRR: recursive neural network for entity-augmented distributed semantics [Ji and Eisenstein, 2015] and shallow convolutional neural network for discourse representation [Zhang et al.", "startOffset": 156, "endOffset": 181}, {"referenceID": 31, "context": "Specifically, two different neural network models have been developed for implicit DRR: recursive neural network for entity-augmented distributed semantics [Ji and Eisenstein, 2015] and shallow convolutional neural network for discourse representation [Zhang et al., 2015].", "startOffset": 252, "endOffset": 272}, {"referenceID": 31, "context": "Instead, our proposed model does not rely on any linguistic resources and incorporates a semantic memory to obtain deep semantic representations over shallow representations in [Zhang et al., 2015].", "startOffset": 177, "endOffset": 197}, {"referenceID": 27, "context": "In order to explore semantic memory in neural networks, we borrow ideas from recently introduced memory networks [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] to organize semantic memory as a distributed matrix and use an attention model to retrieve this distributed memory.", "startOffset": 113, "endOffset": 179}, {"referenceID": 24, "context": "In order to explore semantic memory in neural networks, we borrow ideas from recently introduced memory networks [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] to organize semantic memory as a distributed matrix and use an attention model to retrieve this distributed memory.", "startOffset": 113, "endOffset": 179}, {"referenceID": 10, "context": "In order to explore semantic memory in neural networks, we borrow ideas from recently introduced memory networks [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015] to organize semantic memory as a distributed matrix and use an attention model to retrieve this distributed memory.", "startOffset": 113, "endOffset": 179}, {"referenceID": 31, "context": "To obtain surface representations for discourses, we employ a shallow convolutional neural network (SCNN) [Zhang et al., 2015] as our shallow encoder.", "startOffset": 106, "endOffset": 126}, {"referenceID": 13, "context": "As discussed in [Luong et al., 2015], a general scoring function is much better for the local attention.", "startOffset": 16, "endOffset": 36}, {"referenceID": 21, "context": "0 corpus4 [Prasad et al., 2008] (PDTB thereafter), which is the largest hand-annotated discourse corpus.", "startOffset": 10, "endOffset": 31}, {"referenceID": 20, "context": "Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used sections 2-20 as our training set, sections 21-22 as the test set.", "startOffset": 24, "endOffset": 102}, {"referenceID": 32, "context": "Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used sections 2-20 as our training set, sections 21-22 as the test set.", "startOffset": 24, "endOffset": 102}, {"referenceID": 11, "context": "Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used sections 2-20 as our training set, sections 21-22 as the test set.", "startOffset": 24, "endOffset": 102}, {"referenceID": 31, "context": "Following previous work [Pitler et al., 2009; Zhou et al., 2010; Lan et al., 2013; Zhang et al., 2015], we used sections 2-20 as our training set, sections 21-22 as the test set.", "startOffset": 24, "endOffset": 102}, {"referenceID": 31, "context": "We optimized the hyperparameters d1, \u03bbL, \u03bbR, \u03bbM according to previous work [Zhang et al., 2015] and preliminary experiments on the development set.", "startOffset": 75, "endOffset": 95}, {"referenceID": 22, "context": "Features used in SVM experiments are taken from the stateof-the-art implicit discourse relation recognition model, including Bag of Words, Cross-Argument Word Pairs, Polarity, First-Last, First3, Production Rules, Dependency Rules and Brown cluster pair [Rutherford and Xue, 2014].", "startOffset": 254, "endOffset": 280}, {"referenceID": 13, "context": "the concat and dot in [Luong et al., 2015].", "startOffset": 22, "endOffset": 42}], "year": 2016, "abstractText": "Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts. Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion. We refer to this recognizer as SeMDER. Starting from word embeddings of discourse arguments, SeMDER employs a shallow encoder to generate a distributed surface representation for a discourse. A semantic encoder with attention to the semantic memory matrix is further established over surface representations. It is able to retrieve a deep semantic meaning representation for the discourse from the memory. Using the surface and semantic representations as input, SeMDER finally predicts implicit discourse relations via a neural recognizer. Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56% on average over current state-of-the-art baselines in terms of F1-score.", "creator": "LaTeX with hyperref package"}}}