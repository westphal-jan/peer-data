{"id": "1603.04513", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Multichannel Variable-Size Convolution for Sentence Classification", "abstract": "We propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification.", "histories": [["v1", "Tue, 15 Mar 2016 00:25:02 GMT  (516kb,D)", "http://arxiv.org/abs/1603.04513v1", "in Proceeding of CoNLL2015"]], "COMMENTS": "in Proceeding of CoNLL2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "hinrich sch\\\"utze"], "accepted": false, "id": "1603.04513"}, "pdf": {"name": "1603.04513.pdf", "metadata": {"source": "CRF", "title": "Multichannel Variable-Size Convolution for Sentence Classification", "authors": ["Wenpeng Yin"], "emails": ["wenpeng@cis.uni-muenchen.de"], "sections": [{"heading": "1 Introduction", "text": "Different sentence classification tasks are crucial for many Natural Language Processing (NLP) applications. Natural language sentences have complicated structures, both sequential and hierarchical, that are essential for understanding them. In addition, how to decode and compose the features of component units, including single words and variable-size phrases, is central to the sentence classification problem.\nIn recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008). A problem largely specific to NLP is how to detect features of linguistic units, how to conduct composition over variable-size sequences and how to use them for NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees. This approach depends on the availability of a well performing parser; for many languages and domains, especially noisy domains, reliable parsing is difficult. Hence, convolution neural networks\n(CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014). Current CNN systems usually implement a convolution layer with fixed-size filters (i.e., feature detectors), in which the concrete filter size is a hyperparameter. They essentially split a sentence into multiple sub-sentences by a sliding window, then determine the sentence label by using the dominant label across all sub-sentences. The underlying assumption is that the sub-sentence with that granularity is potentially good enough to represent the whole sentence. However, it is hard to find the granularity of a \u201cgood sub-sentence\u201d that works well across sentences. This motivates us to implement variable-size filters in a convolution layer in order to extract features of multigranular phrases.\nBreakthroughs of deep learning in NLP are also based on learning distributed word representations \u2013 also called \u201cword embeddings\u201d \u2013 by neural language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a). Word embeddings are derived by projecting words from a sparse, 1-of-V encoding (V : vocabulary size) onto a lower dimensional and dense vector space via hidden layers and can be interpreted as feature extractors that encode semantic and syntactic features of words.\nMany papers study the comparative performance of different versions of word embeddings, usually learned by different neural network (NN) architectures. For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three repre-\nar X\niv :1\n60 3.\n04 51\n3v 1\n[ cs\n.C L\n] 1\n5 M\nar 2\n01 6\nsentative monolingual embedding versions: skipgram (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and C&W (Collobert et al., 2011) in some cases. These prior studies motivate us to explore combining multiple versions of word embeddings, treating each of them as a distinct description of words. Our expectation is that the combination of these embedding versions, trained by different NNs on different corpora, should contain more information than each version individually. We want to leverage this diversity of different embedding versions to extract higher quality sentence features and thereby improve sentence classification performance.\nThe letters \u201cM\u201d and \u201cV\u201d in the name \u201cMVCNN\u201d of our architecture denote the multichannel and variable-size convolution filters, respectively. \u201cMultichannel\u201d employs language from computer vision where a color image has red, green and blue channels. Here, a channel is a description by an embedding version.\nFor many sentence classification tasks, only relatively small training sets are available. MVCNN has a large number of parameters, so that overfitting is a danger when they are trained on small training sets. We address this problem by pretraining MVCNN on unlabeled data. These pretrained weights can then be fine-tuned for the specific classification task.\nIn sum, we attribute the success of MVCNN to: (i) designing variable-size convolution filters to extract variable-range features of sentences and (ii) exploring the combination of multiple public embedding versions to initialize words in sentences. We also employ two \u201ctricks\u201d to further enhance system performance: mutual learning and pretraining.\nIn remaining parts, Section 2 presents related work. Section 3 gives details of our classification model. Section 4 introduces two tricks that enhance system performance: mutual-learning and pretraining. Section 5 reports experimental results. Section 6 concludes this work."}, {"heading": "2 Related Work", "text": "Much prior work has exploited deep neural networks to model sentences.\nBlacoe and Lapata (2012) represented a sentence by element-wise addition, multiplication, or recursive autoencoder over embeddings of component single words. Yin and Schu\u0308tze (2014) ex-\ntended this approach by composing on words and phrases instead of only single words.\nCollobert and Weston (2008) and Yu et al. (2014) used one layer of convolution over phrases detected by a sliding window on a target sentence, then used max- or average-pooling to form a sentence representation.\nKalchbrenner et al. (2014) stacked multiple layers of one-dimensional convolution by dynamic kmax pooling to model sentences. We also adopt dynamic k-max pooling while our convolution layer has variable-size filters.\nKim (2014) also studied multichannel representation and variable-size filters. Differently, their multichannel relies on a single version of pretrained embeddings (i.e., pretrained Word2Vec embeddings) with two copies: one is kept stable and the other one is fine-tuned by backpropagation. We develop this insight by incorporating diverse embedding versions. Additionally, their idea of variable-size filters is further developed.\nLe and Mikolov (2014) initialized the representation of a sentence as a parameter vector, treating it as a global feature and combining this vector with the representations of context words to do word prediction. Finally, this fine-tuned vector is used as representation of this sentence. Apparently, this method can only produce generic sentence representations which encode no taskspecific features.\nOur work is also inspired by studies that compared the performance of different word embedding versions or investigated the combination of them. For example, Turian et al. (2010) compared Brown clusters, C&W embeddings and HLBL embeddings in NER and chunking tasks. They found that Brown clusters and word embeddings both can improve the accuracy of supervised NLP systems; and demonstrated empirically that combining different word representations is beneficial. Luo et al. (2014) adapted CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets: free text documents from Wikipedia, search click-through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarity task. However, these two papers either learned word representations on the same corpus (Turian et al., 2010) or enhanced the embedding quality by extending training corpora, not learning algorithms (Luo et\nal., 2014). In our work, there is no limit to the type of embedding versions we can use and they leverage not only the diversity of corpora, but also the different principles of learning algorithms."}, {"heading": "3 Model Description", "text": "We now describe the architecture of our model MVCNN, illustrated in Figure 1.\nMultichannel Input. The input of MVCNN includes multichannel feature maps of a considered sentence, each is a matrix initialized by a different embedding version. Let s be sentence length, d dimension of word embeddings and c the total number of different embedding versions (i.e., channels). Hence, the whole initialized input is a three-dimensional array of size c\u00d7d\u00d7 s. Figure 1 depicts a sentence with s = 12 words. Each word is initialized by c = 5 embeddings, each coming from a different channel. In implementation, sentences in a mini-batch will be padded to the same length, and unknown words for corresponding channel are randomly initialized or can acquire good initialization from the mutual-learning phase described in next section.\nMultichannel initialization brings two advantages: 1) a frequent word can have c representations in the beginning (instead of only one), which means it has more available information to leverage; 2) a rare word missed in some embedding versions can be \u201cmade up\u201d by others (we call it \u201cpartially known word\u201d). Therefore, this kind of initialization is able to make use of information about partially known words, without having to employ full random initialization or removal of unknown words. The vocabulary of the binary sentiment prediction task described in experimental part contains 5232 words unknown in HLBL embeddings, 4273 in Huang embeddings, 3299 in GloVe embeddings, 4136 in SENNA embeddings and 2257 in Word2Vec embeddings. But only 1824 words find no embedding from any channel! Hence, multichannel initialization can considerably reduce the number of unknown words.\nConvolution Layer (Conv). For convenience, we first introduce how this work uses a convolution layer on one input feature map to generate one higher-level feature map. Given a sentence of length s: w1, w2, . . . , ws; wi \u2208 Rd denotes the embedding of word wi; a convolution layer uses sliding filters to extract local features of that sentence. The filter width l is a param-\neter. We first concatenate the initialized embeddings of l consecutive words (wi\u2212l+1, . . . ,wi) as ci \u2208 Rld (1 \u2264 i < s + l), then generate the feature value of this phrase as pi (the whole vector p \u2208 Rs+l\u22121 contains all the local features) using a tanh activation function and a linear projection vector v \u2208 Rld as:\npi = tanh(v Tci + b) (1)\nMore generally, convolution operation can deal with multiple input feature maps and can be stacked to yield feature maps of increasing layers. In each layer, there are usually multiple filters of the same size, but with different weights (Kalchbrenner et al., 2014). We refer to a filter with a specific set of weights as a kernel. The goal is often to train a model in which different kernels detect different kinds of features of a local region. However, this traditional way can not detect the features of regions of different granularity. Hence\nwe keep the property of multi-kernel while extending it to variable-size in the same layer.\nAs in CNN for object recognition, to increase the number of kernels of a certain layer, multiple feature maps may be computed in parallel at the same layer. Further, to increase the size diversity of kernels in the same layer, more feature maps containing various-range dependency features can be learned. We denote a feature map of the ith layer by Fi, and assume totally n feature maps exist in layer i \u2212 1: F1i\u22121, . . . ,Fni\u22121. Considering a specific filter size l in layer i, each feature map Fji,l is computed by convolving a distinct set of filters of size l, arranged in a matrix Vj,ki,l , with each feature map Fki\u22121 and summing the results:\nFji,l = n\u2211\nk=1\nVj,ki,l \u2217 F k i\u22121 (2)\nwhere \u2217 indicates the convolution operation and j is the index of a feature map in layer i. The weights in V form a rank 4 tensor.\nNote that we use wide convolution in this work: it means word representations wg for g \u2264 0 or g \u2265 s+1 are actually zero embeddings. Wide convolution enables that each word can be detected by all filter weights in V.\nIn Figure 1, the first convolution layer deals with an input with n = 5 feature maps.1 Its filters have sizes 3 and 5 respectively (i.e., l = 3, 5), and each filter has j = 3 kernels. This means this convolution layer can detect three kinds of features of phrases with length 3 and 5, respectively.\nDCNN in (Kalchbrenner et al., 2014) used onedimensional convolution: each higher-order feature is produced from values of a single dimension in the lower-layer feature map. Even though that work proposed folding operation to model the dependencies between adjacent dimensions, this type of dependency modeling is still limited. Differently, convolution in present work is able to model dependency across dimensions as well as adjacent words, which obviates the need for a folding step. This change also means our model has substantially fewer parameters than the DCNN since the output of each convolution layer is smaller by a factor of d.\n1A reviewer expresses surprise at such a small number of maps. However, we will use four variable sizes (see below), so that the overall number of maps is 20. We use a small number of maps partly because training times for a network are on the order of days, so limiting the number of parameters is important.\nDynamic k-max Pooling. Kalchbrenner et al. (2014) pool the k most active features compared with simple max (1-max) pooling (Collobert and Weston, 2008). This property enables it to connect multiple convolution layers to form a deep architecture to extract high-level abstract features. In this work, we directly use it to extract features for variable-size feature maps. For a given feature map in layer i, dynamic k-max pooling extracts ki top values from each dimension and ktop top values in the top layer. We set\nki = max(ktop, d L\u2212 i L se) (3)\nwhere i \u2208 {1, 2, . . . L} is the order of convolution layer from bottom to top in Figure 1; L is the total numbers of convolution layers; ktop is a constant determined empirically, we set it to 4 as (Kalchbrenner et al., 2014).\nAs a result, the second convolution layer in Figure 1 has an input with two same-size feature maps, one results from filter size 3, one from filter size 5. The values in the two feature maps are for phrases with different granularity. The motivation of this convolution layer lies in that a feature reflected by a short phrase may be not trustworthy while the longer phrase containing the short one is trustworthy, or the long phrase has no trustworthy feature while its component short phrase is more reliable. This and even higher-order convolution layers therefore can make a trade-off between the features of different granularity.\nHidden Layer. On the top of the final kmax pooling, we stack a fully connected layer to learn sentence representation with given dimension (e.g., d).\nLogistic Regression Layer. Finally, sentence representation is forwarded into logistic regression layer for classification.\nIn brief, our MVCNN model learns from (Kalchbrenner et al., 2014) to use dynamic kmax pooling to stack multiple convolution layers, and gets insight from (Kim, 2014) to investigate variable-size filters in a convolution layer. Compared to (Kalchbrenner et al., 2014), MVCNN has rich feature maps as input and as output of each convolution layer. Its convolution operation is not only more flexible to extract features of variable-range phrases, but also able to model dependency among all dimensions of representations. MVCNN extends the network in (Kim, 2014) by hierarchical convolution architecture and\nfurther exploration of multichannel and variablesize feature detectors."}, {"heading": "4 Model Enhancements", "text": "This part introduces two training tricks that enhance the performance of MVCNN in practice.\nMutual-Learning of Embedding Versions. One observation in using multiple embedding versions is that they have different vocabulary coverage. An unknown word in an embedding version may be a known word in another version. Thus, there exists a proportion of words that can only be partially initialized by certain versions of word embeddings, which means these words lack the description from other versions.\nTo alleviate this problem, we design a mutuallearning regime to predict representations of unknown words for each embedding version by learning projections between versions. As a result, all embedding versions have the same vocabulary. This processing ensures that more words in each embedding version receive a good representation, and is expected to give most words occurring in a classification dataset more comprehensive initialization (as opposed to just being randomly initialized).\nLet c be the number of embedding versions in consideration, V1, V2, . . . , Vi, . . . , Vc their vocabularies, V \u2217 = \u222aci=1Vi their union, and V \u2212 i = V \u2217\\Vi (i = 1, . . . , c) the vocabulary of unknown words for embedding version i. Our goal is to learn embeddings for the words in V \u2212i by knowledge from the other c\u2212 1 embedding versions.\nWe use the overlapping vocabulary between Vi and Vj , denoted as Vij , as training set, formalizing a projection fij from space Vi to space Vj (i 6= j; i, j \u2208 {1, 2, . . . , c}) as follows:\nw\u0302j = Mijwi (4)\nwhere Mij \u2208 Rd\u00d7d, wi \u2208 Rd denotes the representation of word w in space Vi and w\u0302j is the projected (or learned) representation of word w in space Vj . Squared error between wj and w\u0302j is the training loss to minimize. We use w\u0302j = fij(wi) to reformat Equation 4. Totally c(c\u2212 1)/2 projections fij are trained, each on the vocabulary intersection Vij .\nLet w be a word that is unknown in Vi, but is known in V1, V2, . . . , Vk. To compute an embedding for w in Vi, we first compute the k projections f1i(w1), f2i(w2), . . ., fki(wk) from the source\nspaces V1, V2, . . . , Vk to the target space Vi. Then, the element-wise average of f1i(w1), f2i(w2), . . ., fki(wk) is treated as the representation of w in Vi. Our motivation is that \u2013 assuming there is a true representation of w in Vi (e.g., the one we would have obtained by training embeddings on a much larger corpus) and assuming the projections were learned well \u2013 we would expect all the projected vectors to be close to the true representation. Also, each source space contributes potentially complementary information. Hence averaging them is a balance of knowledge from all source spaces.\nAs discussed in Section 3, we found that for the binary sentiment classification dataset, many words were unknown in at least one embedding version. But of these words, a total of 5022 words did have coverage in another embedding version and so will benefit from mutual-learning. In the experiments, we will show that this is a very effective method to learn representations for unknown words that increases system performance if learned representations are used for initialization.\nPretraining. Sentence classification systems are usually implemented as supervised training regimes where training loss is between true label distribution and predicted label distribution. In this work, we use pretraining on the unlabeled data of each task and show that it can increase the performance of classification systems.\nFigure 1 shows our pretraining setup. The \u201csentence representation\u201d \u2013 the output of \u201cFully connected\u201d hidden layer \u2013 is used to predict the component words (\u201con\u201d in the figure) in the sentence (instead of predicting the sentence label Y/N as in supervised learning). Concretely, the sentence representation is averaged with representations of some surrounding words (\u201cthe\u201d, \u201ccat\u201d, \u201csat\u201d, \u201cthe\u201d, \u201cmat\u201d, \u201c,\u201d in the figure) to predict the middle word (\u201con\u201d).\nGiven sentence representation s \u2208 Rd and initialized representations of 2t context words (t left words and t right words): wi\u2212t, . . ., wi\u22121, wi+1, . . ., wi+t; wi \u2208 Rd, we average the total 2t + 1 vectors element-wise, depicted as \u201cAverage\u201d operation in Figure 1. Then, this resulting vector is treated as a predicted representation of the middle word and is used to find the true middle word by means of noise-contrastive estimation (NCE) (Mnih and Teh, 2012). For each true example, 10 noise words are sampled.\nNote that in pretraining, there are three places\nwhere each word needs initialization. (i) Each word in the sentence is initialized in the \u201cMultichannel input\u201d layer to the whole network. (ii) Each context word is initialized as input to the average layer (\u201cAverage\u201d in the figure). (iii) Each target word is initialized as the output of the \u201cNCE\u201d layer (\u201con\u201d in the figure). In this work, we use multichannel initialization for case (i) and random initialization for cases (ii) and (iii). Only finetuned multichannel representations (case (i)) are kept for subsequent supervised training.\nThe rationale for this pretraining is similar to auto-encoder: for an object composed of smaller-granular elements, the representations of the whole object and its components can learn each other. The CNN architecture learns sentence features layer by layer, then those features are justified by all constituent words.\nDuring pretraining, all the model parameters, including mutichannel input, convolution parameters and fully connected layer, will be updated until they are mature to extract the sentence features. Subsequently, the same sets of parameters will be fine-tuned for supervised classification tasks.\nIn sum, this pretraining is designed to produce good initial values for both model parameters and word embeddings. It is especially helpful for pretraining the embeddings of unknown words."}, {"heading": "5 Experiments", "text": "We test the network on four classification tasks. We begin by specifying aspects of the implementation and the training of the network. We then report the results of the experiments."}, {"heading": "5.1 Hyperparameters and Training", "text": "In each of the experiments, the top of the network is a logistic regression that predicts the probability distribution over classes given the input sentence. The network is trained to minimize cross-entropy of predicted and true distributions; the objective includes an L2 regularization term over the parameters. The set of parameters comprises the word embeddings, all filter weights and the weights in fully connected layers. A dropout operation (Hinton et al., 2012) is put before the logistic regression layer. The network is trained by back-propagation in mini-batches and the gradient-based optimization is performed using the AdaGrad update rule (Duchi et al., 2011)\nIn all data sets, the initial learning rate is 0.01,\ndropout probability is 0.8, L2 weight is 5 \u00b7 10\u22123, batch size is 50. In each convolution layer, filter sizes are {3, 5, 7, 9} and each filter has five kernels (independent of filter size)."}, {"heading": "5.2 Datasets and Experimental Setup", "text": "Standard Sentiment Treebank (Socher et al., 2013). This small-scale dataset includes two tasks predicting the sentiment of movie reviews. The output variable is binary in one experiment and can have five possible outcomes in the other: {negative, somewhat negative, neutral, somewhat positive, positive}. In the binary case, we use the given split of 6920 training, 872 development and 1821 test sentences. Likewise, in the finegrained case, we use the standard 8544/1101/2210 split. Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases. The subphrases were then labeled by human annotators in the same way as the sentences were labeled. Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014).\nSentiment1402 (Go et al., 2009). This is a large-scale dataset of tweets about sentiment classification, where a tweet is automatically labeled as positive or negative depending on the emoticon that occurs in it. The training set consists of 1.6 million tweets with emoticon-based labels and the test set of about 400 hand-annotated tweets. We preprocess the tweets minimally as follows. 1) The equivalence class symbol \u201curl\u201d (resp. \u201cusername\u201d) replaces all URLs (resp. all words that start with the @ symbol, e.g., @thomasss). 2) A sequence of k > 2 repetitions of a letter c (e.g., \u201ccooooooool\u201d) is replaced by two occurrences of c (e.g., \u201ccool\u201d). 3) All tokens are lowercased.\nSubj. Subjectivity classification dataset3 released by (Pang and Lee, 2004) has 5000 subjective sentences and 5000 objective sentences. We report the result of 10-fold cross validation as baseline systems did."}, {"heading": "5.2.1 Pretrained Word Vectors", "text": "In this work, we use five embedding versions, as shown in Table 1, to initialize words. Four of them are directly downloaded from the Internet.\n2http://help.sentiment140.com/for-students 3http://www.cs.cornell.edu/people/pabo/movie-review-\ndata/\n(i) HLBL. Hierarchical log-bilinear model presented by Mnih and Hinton (2009) and released by Turian et al. (2010);4 size: 246,122 word embeddings; training corpus: RCV1 corpus, one year of Reuters English newswire from August 1996 to August 1997. (ii) Huang.5 Huang et al. (2012) incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia. (iii) GloVe.6 Size: 1,193,514 word embeddings; training corpus: a Twitter corpus of 2B tweets with 27B tokens. (iv) SENNA.7 Size: 130,000 word embeddings; training corpus: Wikipedia. Note that we use their 50- dimensional embeddings. (v) Word2Vec. It has no 50-dimensional embeddings available online. We use released code8 to train skip-gram on English Gigaword Corpus (Parker et al., 2009) with\n4http://metaoptimize.com/projects/wordreprs/ 5http://ai.stanford.edu/ ehhuang/ 6http://nlp.stanford.edu/projects/glove/ 7http://ml.nec-labs.com/senna/ 8http://code.google.com/p/word2vec/\nsetup: window size 5, negative sampling, sampling rate 10\u22123, threads 12. It is worth emphasizing that above embeddings sets are derived on different corpora with different algorithms. This is the very property that we want to make use of to promote the system performance.\nTable 2 shows the number of unknown words in each task when using corresponding embedding version to initialize (rows \u201cHLBL\u201d, \u201cHuang\u201d, \u201cGlove\u201d, \u201cSENNA\u201d, \u201cW2V\u201d) and the number of words fully initialized by five embedding versions (\u201cFull hit\u201d row), the number of words partially initialized (\u201cPartial hit\u201d row) and the number of words that cannot be initialized by any of the embedding versions (\u201cNo hit\u201d row).\nAbout 30% of words in each task have partially initialized embeddings and our mutual-learning is able to initialize the missing embeddings through projections. Pretraining is expected to learn good representations for all words, but pretraining is especially important for words without initialization (\u201cno hit\u201d); a particularly clear example for this is the Senti140 task: 236,484 of 387,877 words or 61% are in the \u201cno hit\u201d category."}, {"heading": "5.2.2 Results and Analysis", "text": "Table 3 compares results on test of MVCNN and its variants with other baselines in the four sentence classification tasks. Row 34, \u201cMVCNN (overall)\u201d, shows performance of the best configuration of MVCNN, optimized on dev. This version uses five versions of word embeddings, four filter sizes (3, 5, 7, 9), both mutual-learning and pretraining, three convolution layers for Senti140 task and two convolution layers for the other tasks. Overall, our system gets the best results, beating all baselines.\nThe table contains five blocks from top to bottom. Each block investigates one specific configurational aspect of the system. All results in the five blocks are with respect to row 34, \u201cMVCNN (overall)\u201d; e.g., row 19 shows what happens when\nHLBL is removed from row 34, row 28 shows what happens when mutual learning is removed from row 34 etc.\nThe block \u201cbaselines\u201d (1\u201318) lists some systems representative of previous work on the corresponding datasets, including the state-of-the-art systems (marked as italic). The block \u201cversions\u201d (19\u201323) shows the results of our system when one of the embedding versions was not used during training. We want to explore to what extend different embedding versions contribute to performance. The block \u201cfilters\u201d (24\u201327) gives the results when individual filter width is discarded. It also tells us how much a filter with specific size influences. The block \u201ctricks\u201d (28\u201329) shows the system performance when no mutual-learning or no pretraining is used. The block \u201clayers\u201d (30\u201333) demonstrates how the system performs when it has different numbers of convolution layers.\nFrom the \u201clayers\u201d block, we can see that our system performs best with two layers of convolution in Standard Sentiment Treebank and Subjectivity Classification tasks (row 31), but with three layers of convolution in Sentiment140 (row 32). This is probably due to Sentiment140 being a much larger dataset; in such a case deeper neural networks are beneficial.\nThe block \u201ctricks\u201d demonstrates the effect of mutual-learning and pretraining. Apparently, pretraining has a bigger impact on performance than mutual-learning. We speculate that it is because pretraining can influence more words and all learned word embeddings are tuned on the dataset after pretraining.\nThe block \u201cfilters\u201d indicates the contribution of each filter size. The system benefits from filters of each size. Sizes 5 and 7 are most important for high performance, especially 7 (rows 25 and 26).\nIn the block \u201cversions\u201d, we see that each embedding version is crucial for good performance: performance drops in every single case. Though it is not easy to compare fairly different embedding versions in NLP tasks, especially when those embeddings were trained on different corpora of different sizes using different algorithms, our results are potentially instructive for researchers making decision on which embeddings to use for their own tasks."}, {"heading": "6 Conclusion", "text": "This work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization \u2013 diverse versions of pretrained word embeddings are used \u2013 and variable-size filters \u2013 features of multigranular phrases are extracted with variable-size convolution filters. We demonstrated that multichannel initialization and variable-size filters enhance system performance on sentiment classification and subjectivity classification tasks."}, {"heading": "7 Future Work", "text": "As pointed out by the reviewers the success of the multichannel approach is likely due to a combination of several quite different effects.\nFirst, there is the effect of the embedding learning algorithm. These algorithms differ in many aspects, including in sensitivity to word order (e.g., SENNA: yes, word2vec: no), in objective function and in their treatment of ambiguity (explicitly modeled only by Huang et al. (2012).\nSecond, there is the effect of the corpus. We would expect the size and genre of the corpus to have a big effect even though we did not analyze this effect in this paper.\nThird, complementarity of word embeddings is likely to be more useful for some tasks than for others. Sentiment is a good application for complementary word embeddings because solving this task requires drawing on heterogeneous sources of information, including syntax, semantics and genre as well as the core polarity of a word. Other tasks like part of speech (POS) tagging may benefit less from heterogeneity since the benefit of embeddings in POS often comes down to making a correct choice between two alternatives \u2013 a single embedding version may be sufficient for this.\nWe plan to pursue these questions in future work."}, {"heading": "Acknowledgments", "text": "Thanks to CIS members and anonymous reviewers for constructive comments. This work was supported by Baidu (through a Baidu scholarship awarded to Wenpeng Yin) and by Deutsche Forschungsgemeinschaft (grant DFG SCHU 2246/8-2, SPP 1335)."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "The expressive power of word embeddings", "author": ["Yanqing Chen", "Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena."], "venue": "ICML Workshop on Deep Learning for Audio, Speech, and Language Processing.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Modelling, visualising and summarising documents with a single convolutional neural network", "author": ["Misha Denil", "Alban Demiraj", "Nal Kalchbrenner", "Phil Blunsom", "Nando de Freitas."], "venue": "arXiv preprint arXiv:1406.3830.", "citeRegEx": "Denil et al\\.,? 2014", "shortCiteRegEx": "Denil et al\\.", "year": 2014}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C\u0131cero Nogueira Dos Santos", "Ma\u0131ra Gatti."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics.", "citeRegEx": "Santos and Gatti.,? 2014", "shortCiteRegEx": "Santos and Gatti.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang."], "venue": "CS224N Project Report, Stanford, pages 1\u201312.", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "A-R Mohamed", "Geoffrey Hinton."], "venue": "Acoustics, Speech and Signal Processing, 2013 IEEE International Conference on, pages 6645\u20136649. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Not all neural embeddings are born equal", "author": ["Felix Hill", "KyungHyun Cho", "Sebastien Jean", "Coline Devin", "Yoshua Bengio."], "venue": "NIPS Workshop on Learning Semantics.", "citeRegEx": "Hill et al\\.,? 2014", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Computational", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, October.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423\u2013430. Association for Computational Linguistics.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "Proceedings of the 31st international conference on Machine learning.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Pre-trained multi-view word embedding using two-side neural network", "author": ["Yong Luo", "Jian Tang", "Jun Yan", "Chao Xu", "Zheng Chen."], "venue": "TwentyEighth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Luo et al\\.,? 2014", "shortCiteRegEx": "Luo et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of Workshop at ICLR.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Statistical language models based on neural networks", "author": ["Tomas Mikolov."], "venue": "Presentation at Google, Mountain View, 2nd April.", "citeRegEx": "Mikolov.,? 2012", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1081\u20131088.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "Proceedings of the 29th International Conference on Machine Learning, pages 1751\u20131758.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Com-", "citeRegEx": "Pang and Lee.,? 2004", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing, 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng."], "venue": "Advances in Neural Information Processing Systems, pages 801\u2013809.", "citeRegEx": "Socher et al\\.,? 2011a", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Socher et al\\.,? 2011b", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on em-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90\u201394. As-", "citeRegEx": "Wang and Manning.,? 2012", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Fast dropout training", "author": ["Sida Wang", "Christopher Manning."], "venue": "Proceedings of the 30th International Conference on Machine Learning, pages 118\u2013126.", "citeRegEx": "Wang and Manning.,? 2013", "shortCiteRegEx": "Wang and Manning.", "year": 2013}, {"title": "An exploration of embeddings for generalized phrases", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 52nd annual meeting of the association for computational linguistics, student research workshop, pages 41\u201347.", "citeRegEx": "Yin and Sch\u00fctze.,? 2014", "shortCiteRegEx": "Yin and Sch\u00fctze.", "year": 2014}, {"title": "Deep learning for answer sentence selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman."], "venue": "NIPS deep learning workshop.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "In recent years, deep learning models have achieved remarkable results in computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al.", "startOffset": 90, "endOffset": 115}, {"referenceID": 9, "context": ", 2012), speech recognition (Graves et al., 2013) and NLP (Collobert and Weston, 2008).", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": ", 2013) and NLP (Collobert and Weston, 2008).", "startOffset": 16, "endOffset": 44}, {"referenceID": 13, "context": "Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014).", "startOffset": 167, "endOffset": 226}, {"referenceID": 5, "context": "Hence, convolution neural networks (CNN) are getting increasing attention, for they are able to model long-range dependencies in sentences via hierarchical structures (Dos Santos and Gatti, 2014; Kim, 2014; Denil et al., 2014).", "startOffset": 167, "endOffset": 226}, {"referenceID": 10, "context": ", 2011; Kalchbrenner et al., 2014; Kim, 2014). Socher et al. (2011a) proposed recursive neural networks to form phrases based on parsing trees.", "startOffset": 8, "endOffset": 69}, {"referenceID": 0, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 22, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 18, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 21, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 19, "context": "guage models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov, 2012; Mikolov et al., 2013a).", "startOffset": 13, "endOffset": 117}, {"referenceID": 22, "context": "(2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 3, "context": "(2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al.", "startOffset": 52, "endOffset": 80}, {"referenceID": 30, "context": "(2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al.", "startOffset": 89, "endOffset": 110}, {"referenceID": 11, "context": ", 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions.", "startOffset": 18, "endOffset": 38}, {"referenceID": 2, "context": "For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": "For example, Chen et al. (2013) compared HLBL (Mnih and Hinton, 2009), SENNA (Collobert and Weston, 2008), Turian (Turian et al., 2010) and Huang (Huang et al., 2012), showing great variance in quality and characteristics of the semantics captured by the tested embedding versions. Hill et al. (2014) showed that embeddings learned by neural machine translation models outperform three reprear X iv :1 60 3.", "startOffset": 13, "endOffset": 301}, {"referenceID": 20, "context": "gram (Mikolov et al., 2013b), GloVe (Pennington et al.", "startOffset": 5, "endOffset": 28}, {"referenceID": 25, "context": ", 2013b), GloVe (Pennington et al., 2014) and C&W (Collobert et al.", "startOffset": 16, "endOffset": 41}, {"referenceID": 4, "context": ", 2014) and C&W (Collobert et al., 2011) in some cases.", "startOffset": 16, "endOffset": 40}, {"referenceID": 33, "context": "Yin and Sch\u00fctze (2014) extended this approach by composing on words and phrases instead of only single words.", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "(2014) adapted CBOW (Mikolov et al., 2013a) to train word embeddings on different datasets: free text documents from Wikipedia, search click-through data and user query data, showing that combining them gets stronger results than using individual word embeddings in web search ranking and word similarity task.", "startOffset": 20, "endOffset": 43}, {"referenceID": 30, "context": "However, these two papers either learned word representations on the same corpus (Turian et al., 2010) or enhanced the embedding quality by extending training corpora, not learning algorithms (Luo et", "startOffset": 81, "endOffset": 102}, {"referenceID": 25, "context": "For example, Turian et al. (2010) compared Brown clusters, C&W embeddings and HLBL embeddings in NER and chunking tasks.", "startOffset": 13, "endOffset": 34}, {"referenceID": 17, "context": "Luo et al. (2014) adapted CBOW (Mikolov et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "In each layer, there are usually multiple filters of the same size, but with different weights (Kalchbrenner et al., 2014).", "startOffset": 95, "endOffset": 122}, {"referenceID": 12, "context": "DCNN in (Kalchbrenner et al., 2014) used onedimensional convolution: each higher-order feature is produced from values of a single dimension in the lower-layer feature map.", "startOffset": 8, "endOffset": 35}, {"referenceID": 3, "context": "(2014) pool the k most active features compared with simple max (1-max) pooling (Collobert and Weston, 2008).", "startOffset": 80, "endOffset": 108}, {"referenceID": 12, "context": "L} is the order of convolution layer from bottom to top in Figure 1; L is the total numbers of convolution layers; ktop is a constant determined empirically, we set it to 4 as (Kalchbrenner et al., 2014).", "startOffset": 176, "endOffset": 203}, {"referenceID": 12, "context": "(Kalchbrenner et al., 2014) to use dynamic kmax pooling to stack multiple convolution layers, and gets insight from (Kim, 2014) to investigate variable-size filters in a convolution layer.", "startOffset": 0, "endOffset": 27}, {"referenceID": 13, "context": ", 2014) to use dynamic kmax pooling to stack multiple convolution layers, and gets insight from (Kim, 2014) to investigate variable-size filters in a convolution layer.", "startOffset": 96, "endOffset": 107}, {"referenceID": 12, "context": "Compared to (Kalchbrenner et al., 2014), MVCNN has rich feature maps as input and as output of each convolution layer.", "startOffset": 12, "endOffset": 39}, {"referenceID": 13, "context": "MVCNN extends the network in (Kim, 2014) by hierarchical convolution architecture and", "startOffset": 29, "endOffset": 40}, {"referenceID": 23, "context": "Then, this resulting vector is treated as a predicted representation of the middle word and is used to find the true middle word by means of noise-contrastive estimation (NCE) (Mnih and Teh, 2012).", "startOffset": 176, "endOffset": 196}, {"referenceID": 7, "context": "ing the AdaGrad update rule (Duchi et al., 2011)", "startOffset": 28, "endOffset": 48}, {"referenceID": 29, "context": "Standard Sentiment Treebank (Socher et al., 2013).", "startOffset": 28, "endOffset": 49}, {"referenceID": 14, "context": "(2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases.", "startOffset": 32, "endOffset": 57}, {"referenceID": 25, "context": "Socher et al. (2013) used the Stanford Parser (Klein and Manning, 2003) to parse each sentence into subphrases.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014).", "startOffset": 117, "endOffset": 166}, {"referenceID": 12, "context": "Labeled phrases that occur as subparts of the training sentences are treated as independent training instances as in (Le and Mikolov, 2014; Kalchbrenner et al., 2014).", "startOffset": 117, "endOffset": 166}, {"referenceID": 8, "context": "Sentiment1402 (Go et al., 2009).", "startOffset": 14, "endOffset": 31}, {"referenceID": 24, "context": "Subjectivity classification dataset3 released by (Pang and Lee, 2004) has 5000 subjective sentences and 5000 objective sentences.", "startOffset": 49, "endOffset": 69}, {"referenceID": 22, "context": "Hierarchical log-bilinear model presented by Mnih and Hinton (2009) and released by Turian et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 22, "context": "Hierarchical log-bilinear model presented by Mnih and Hinton (2009) and released by Turian et al. (2010);4 size: 246,122 word em-", "startOffset": 45, "endOffset": 105}, {"referenceID": 11, "context": "5 Huang et al. (2012) incorporated global context to deal with challenges raised by words with multiple meanings; size: 100,232 word embeddings; training corpus: April 2010 snapshot of Wikipedia.", "startOffset": 2, "endOffset": 22}, {"referenceID": 27, "context": "baselines 1 RAE (Socher et al., 2011b) 82.", "startOffset": 16, "endOffset": 38}, {"referenceID": 28, "context": "2 \u2013 \u2013 2 MV-RNN (Socher et al., 2012) 82.", "startOffset": 15, "endOffset": 36}, {"referenceID": 29, "context": "4 \u2013 \u2013 3 RNTN (Socher et al., 2013) 85.", "startOffset": 13, "endOffset": 34}, {"referenceID": 12, "context": "7 \u2013 \u2013 4 DCNN (Kalchbrenner et al., 2014) 86.", "startOffset": 13, "endOffset": 40}, {"referenceID": 16, "context": "4 \u2013 5 Paragraph-Vec (Le and Mikolov, 2014) 87.", "startOffset": 20, "endOffset": 42}, {"referenceID": 13, "context": "7 \u2013 \u2013 6 CNN-rand (Kim, 2014) 82.", "startOffset": 17, "endOffset": 28}, {"referenceID": 13, "context": "6 7 CNN-static (Kim, 2014) 86.", "startOffset": 15, "endOffset": 26}, {"referenceID": 13, "context": "0 8 CNN-non-static (Kim, 2014) 87.", "startOffset": 19, "endOffset": 30}, {"referenceID": 13, "context": "4 9 CNN-multichannel (Kim, 2014) 88.", "startOffset": 21, "endOffset": 32}, {"referenceID": 31, "context": "2 10 NBSVM (Wang and Manning, 2012) \u2013 \u2013 \u2013 93.", "startOffset": 11, "endOffset": 35}, {"referenceID": 31, "context": "2 11 MNB (Wang and Manning, 2012) \u2013 \u2013 \u2013 93.", "startOffset": 9, "endOffset": 33}, {"referenceID": 32, "context": "6 12 G-Dropout (Wang and Manning, 2013) \u2013 \u2013 \u2013 93.", "startOffset": 15, "endOffset": 39}, {"referenceID": 32, "context": "4 13 F-Dropout (Wang and Manning, 2013) \u2013 \u2013 \u2013 93.", "startOffset": 15, "endOffset": 39}, {"referenceID": 8, "context": "6 14 SVM (Go et al., 2009) \u2013 \u2013 81.", "startOffset": 9, "endOffset": 26}, {"referenceID": 8, "context": "6 \u2013 15 BINB (Go et al., 2009) \u2013 \u2013 82.", "startOffset": 12, "endOffset": 29}, {"referenceID": 12, "context": "7 \u2013 16 MAX-TDNN (Kalchbrenner et al., 2014) \u2013 \u2013 78.", "startOffset": 16, "endOffset": 43}, {"referenceID": 12, "context": "8 \u2013 17 NBOW (Kalchbrenner et al., 2014) \u2013 \u2013 80.", "startOffset": 12, "endOffset": 39}, {"referenceID": 8, "context": "18 MAXENT (Go et al., 2009) \u2013 \u2013 83.", "startOffset": 10, "endOffset": 27}, {"referenceID": 27, "context": "RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b).", "startOffset": 75, "endOffset": 97}, {"referenceID": 28, "context": "MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012).", "startOffset": 64, "endOffset": 85}, {"referenceID": 29, "context": "RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013).", "startOffset": 89, "endOffset": 110}, {"referenceID": 3, "context": "DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al.", "startOffset": 121, "endOffset": 149}, {"referenceID": 12, "context": "DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014).", "startOffset": 178, "endOffset": 205}, {"referenceID": 16, "context": "Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014).", "startOffset": 63, "endOffset": 85}, {"referenceID": 8, "context": "SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009).", "startOffset": 115, "endOffset": 132}, {"referenceID": 13, "context": "CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a \u201cchannel\u201d) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014).", "startOffset": 305, "endOffset": 316}, {"referenceID": 3, "context": "DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a \u201cchannel\u201d) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014).", "startOffset": 122, "endOffset": 530}, {"referenceID": 3, "context": "DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a \u201cchannel\u201d) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). Minus sign \u201c-\u201d in MVCNN (-Huang) etc.", "startOffset": 122, "endOffset": 934}, {"referenceID": 11, "context": "SENNA: yes, word2vec: no), in objective function and in their treatment of ambiguity (explicitly modeled only by Huang et al. (2012).", "startOffset": 113, "endOffset": 133}], "year": 2016, "abstractText": "We propose MVCNN, a convolution neural network (CNN) architecture for sentence classification. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution filters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classification.", "creator": "TeX"}}}