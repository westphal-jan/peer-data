{"id": "1603.04947", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "On the Complexity of One-class SVM for Multiple Instance Learning", "abstract": "In traditional multiple instance learning (MIL), both positive and negative bags are required to learn a prediction function. However, a high human cost is needed to know the label of each bag---positive or negative. Only positive bags contain our focus (positive instances) while negative bags consist of noise or background (negative instances). So we do not expect to spend too much to label the negative bags. Contrary to our expectation, nearly all existing MIL methods require enough negative bags besides positive ones. In this paper we propose an algorithm called \"Positive Multiple Instance\" (PMI), which learns a classifier given only a set of positive bags. So the annotation of negative bags becomes unnecessary in our method. PMI is constructed based on the assumption that the unknown positive instances in positive bags be similar each other and constitute one compact cluster in feature space and the negative instances locate outside this cluster. The experimental results demonstrate that PMI achieves the performances close to or a little worse than those of the traditional MIL algorithms on benchmark and real data sets. However, the number of training bags in PMI is reduced significantly compared with traditional MIL algorithms.", "histories": [["v1", "Wed, 16 Mar 2016 03:30:59 GMT  (236kb,D)", "http://arxiv.org/abs/1603.04947v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhen hu", "zhuyin xue"], "accepted": false, "id": "1603.04947"}, "pdf": {"name": "1603.04947.pdf", "metadata": {"source": "CRF", "title": "On the Complexity of One-class SVM for Multiple Instance Learning", "authors": ["Zhen Hu", "Zhuyin Xue"], "emails": ["49859211@qq.com"], "sections": [{"heading": null, "text": "In traditional multiple instance learning (MIL), both positive and negative bags are required to learn a prediction function. However, a high human cost is needed to know the label of each bag\u2014positive or negative. Only positive bags contain our focus (positive instances) while negative bags consist of noise or background (negative instances). So we do not expect to spend too much to label the negative bags. Contrary to our expectation, nearly all existing MIL methods require enough negative bags besides positive ones. In this paper we propose an algorithm called \u201cPositive Multiple Instance\u201d (PMI), which learns a classifier given only a set of positive bags. So the annotation of negative bags becomes unnecessary in our method. PMI is constructed based on the assumption that the unknown positive instances in positive bags be similar each other and constitute one compact cluster in feature space and the negative instances locate outside this cluster. The experimental results demonstrate that PMI achieves the performances close to or a little worse than those of the traditional MIL algorithms on benchmark and real data sets. However, the number of training bags in PMI is reduced significantly compared with traditional MIL algorithms.\nKeywords: Multiple Instance Learning; One-class"}, {"heading": "1 Introduction", "text": "Multiple instance learning (MIL) is introduced by [9] to solve the drug activity prediction problem. During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10]. In traditional supervised learning, each instance(feature vector) of training set corresponds to one given label. By contrast, every set of instances (not one instance) is associated with a given label in MIL. Each instance set is called a \u201cbag\u201d. If a bag contains at least one positive instance, it is labeled as positive and negative otherwise. It is unknown which instance is positive in each bag. In other words, only bag level label is available while instance level label is not in positive bags. The task of MIL is to learn a concept to predict the label of an unseen bag.\n?Project supported by the National Nature Science Foundation of China (No. ***). \u2217Corresponding author. Email address: 49859211@qq.com (Zhen Hu).\nar X\niv :1\n60 3.\n04 94\n7v 1\n[ cs\n.L G\n] 1\n6 M\nar 2\n01 6\nIn nearly all existing MIL algorithms, both positive and negative bags are required during the training phase. But only the positive instances are our focus generally and the negative instances are unrelated to our interest. For example, if we attempts to learn a concept of face, the face patches in an image are positive instances and other non-face images are labeled as negative bags. According to this preference, we are willing to concentrate on labeling face images and ignore non-face images as much as possible. Non-face images are not our interest. However, negative bags are not less and even much more than positive in many real data sets such as Corel [7]. The availability of negative bags requires a high cost. So labeling on negative bags brings great inconvenience in real applications. In addition, it takes more time to label one negative bag than positive in that every instance must be confirmed to be negative. In contrast, remaining instances in one positive bag can be ignored if only one positive instance is found. The labeling on negative bags increases label cost significantly. So it is necessary to design a method using only positive bags to learn.\nOne related work is [22]. His method is simply to solve a query problem, in which an extra positive instance must be provided by user. The most similar instance in every positive bag to this provided positive instance is considered positive. This solution has the following disadvantages. Firstly, the positive instance is difficult to provide or unavailable in some applications. So [22] can be applied to a limited range of settings. Secondly, some bags contain more than one positive instances. [22] cannot select multiple positive instances accurately in one bag. Thirdly, the performance of this algorithm depends mostly upon the positive instance given by user. It causes the prediction accuracy of [22] is too sensitive to the provided positive instance. In contrast, our proposed algorithm in this paper requires no additional positive instance in most cases. Therefore, our proposed algorithm can solve a wider range of problems.\nIn this paper, a new algorithm called PMI(Positive Multiple Instance) is proposed, which learns a concept from only positive training bags. PMI is designed based on the assumption that the positive instances constitute one compact cluster in feature space and most negative instances locate outside this cluster. PMI works with two major steps\u2014training and query. The training step is to convert the problem of learning with only positive bags into a one-class classification problem [18] to get a classifier f . If the instance level label is not available, PMI terminates and outputs f as the concept. Otherwise, go to the query step. The query step is to select an instance to query its label from instances positively labeled by the classifier f in the previous training step. If the queried instance is negative, remove the instances positively labeled by the classifier f from the training bags and return to the training step. Otherwise, PMI outputs f as the desired concept and terminates. We provide the maximum number of queried instances theoretically. The queried instance number in real applications usually is far smaller than the theoretical result. The experimental results demonstrate that PMI achieves close performances to those of the traditional MIL algorithms on most data sets. Our contribution is that the training bags number can be reduced greatly at a little or no worse on accuracy. Negative bags are unnecessary for training in our method.\nThe remainder of this paper is organized as follows. We introduce PMI algorithm in Section 2 in detail. Section 3 illustrates PMI with experiments. We conclude on our work in Section 4 finally."}, {"heading": "2 Our Proposed Algorithm", "text": "In this section, we propose PMI algorithm. One-class SVM [18] is involved in PMI, so a brief review on one-class SVM is presented here."}, {"heading": "2.1 Review on One-Class SVM", "text": "The formal definition of one-class SVM is described as following. N d-dimensional instances x1, x2, ..., xN in feature space Rd are given, where R denotes real number field. The task is to learn a prediction function takes value +1 while capturing most given instances in a small region of feature space and -1 otherwise. One-class SVM maximizes the margin between the training instances and origin in feature space to obtain a decision hyperplane by the following formulation:\nmin w,\u03c1,\u03be,\u03bd\n1 2 \u2016w\u20162 + 1 \u03bdN \u2211 i \u03bei \u2212 \u03c1\ns. t. (w \u00b7 \u03a6(xi)) \u2265 \u03c1\u2212 \u03bei,\n\u03bei \u2265 0, w \u2208 Rd, \u03c1 \u2208 R,\n(1)\nwhere \u03bd \u2208 (0, 1) is a parameter to balance the regularization \u2016w\u20162 and the hinge loss \u2211 i \u03bei, w and \u03c1 are the weight coefficient and the bias in linear decision function respectively, \u03be, \u03beT = [\u03be1, ..., \u03ben] denotes a slack variable vector, \u03bei is the i-th element of \u03be, and \u03a6(xi) is a nonlinear map function to xi to deal with nonlinear boundary of training instances. A kernel function ker(x, y) is defined to replace \u03a6(x) \u00b7\u03a6(y). Model 1 is a quadratic programming problem, which can be solved through its dual form:\nmin \u03b1\n1\n2 \u2211 i,j \u03b1i\u03b1jker(xi, xj)\ns. t. 0 \u2264 \u03b1i \u2264 1 \u03bdN , \u2211 i \u03b1i = 1, (2)\nwhere \u03b1i is a Lagrange multiplying factor of xi. The value of \u03b1i is summarized into three categories according to where xi locates:\n1. \u03b1i = 0 \u2192 xi locates inside positive instance region;\n2. 1 \u03bdN > \u03b1i > 0\u2192 xi lies on the prediction function boundary;\n3. \u03b1i = 1 \u03bdN \u2192 xi falls outside the positive instance region.\nThe label of an unseen instance x is predicted by the following function: f(x) = sign( \u2211 i \u03b1iker(x, xi)\u2212 \u03c1), (3)\nwhere the function sign(y) outputs +1 if y \u2265 0 and -1 otherwise. If f(x) \u2265 0, we assume that x should be similar to the training instances with a high likelihood. Otherwise, x is an outlier. \u03c1 in Eq. (3) is computed:\n\u03c1 = \u2211 i \u03b1iker(xi, xj), (4)\nwhere xj is a support vector, which implies that xj locates on the boundary of decision function f(x) and f(x) = 0.\nVarious complex separation hyperplanes can be described by different types of kernel functions, such as polynomial, RBF, sigmod or self-defined ones. RBF kernel is given by:\nker(x, y) = e\u03b3\u2016x\u2212y\u2016 2 , (5)\nwhere \u03b3 denotes a parameter to control how similar x is to y. In general, we assume that samples of interest locate inside closed regions in the feature space. RBF function is one of the most widely used kernel for its flexibility. We will use RBF as the default kernel function in the rest of this paper."}, {"heading": "2.2 Our Proposed Algorithm", "text": "Our proposed algorithm PMI is introduced in detail in this subsection. At the beginning, we provide the formal definition of the problem PMI solves. A collection of bags {B1, B2, ..., BN} are given as the training set. The i-th bag in the training set Bi contains Ni d-dimensional instances Bi = [Bi1, ..., BiNi ] \u2208 Rd\u00d7Ni and Bij \u2208 Rd is a d-dimensional instance, j = 1, ..., Ni. We define an d\u00d7 n matrix stacking all instances together B = [B1, ..., BN ], where n denotes the total number of all instances in N bags. At least one positive instance resides in each bag of training set, but it is unknown which instance is positive. The same as the label rule in previous MIL approaches, a bag is positive if it contains at least one positive instance and is negative otherwise. Our goal is to learn a function to predict the label of an unseen bag. PMI works in two steps: training and query. If instance level label is available, PMI algorithm alternates between training and query step until the desired concept is obtained. Otherwise, PMI runs training step for once. The remainder describes the two steps explicitly."}, {"heading": "2.2.1 Training Step", "text": "As is mentioned in Section Introduction, PMI assumes that positive instances be similar to each other in feature space. We explain this assumption in Figure 1. Take face identification as an example, the task is to learn a function to predict whether an image contains face or not. So the patches containing face are positive instances we are focus on in Figure 1 and the other non-face patches are negative instances. It can be easily found that face patches (red rectangles in Figure 1) look similar to each other while non-face patches are dissimilar. In summary, positive instances refer to one concept leading to the high similarity between positive and negative instances usually locate outside the region occupied by positive instances in feature space.\nAccording to the previous explanation and illustration in Figure 1, we assume that most positive instances gather in a compact cluster and negative instances locate outside this cluster. At least one instance in most training bags resides in this positive cluster (the cluster composed of positive instances). Now we needs a function to describe the positive cluster. This function takes the value +1 in the positive cluster region and takes -1 elsewhere. We can apply this function on every instance in one bag to identify whether this bag is positive or not. But since we actually do not which instance is positive, the positive concept cannot directly be learned from positive instances.\nAs is stated in [2, 10], the selection of instances will be formulated as a combinatorial optimization problem, which is non-convex and difficult to solve especially when a global optimal solution\nis required. We tend to approximate one virtual instance bi with a high likelihood to be positive for each training bag Bi. The desired concept can be learned from these approximated positive instances. To make our method efficient and easy to solve, a linear combination coefficient vector \u03bbi is used to convert each bag Bi to a virtual positive instance bi in feature space:\nbi = Bi\u03bbi = Ni\u2211 j=1 Bij\u03bbij,\ns.t. \u2211 j \u03bbij = 1, \u03bbij \u2265 0, i = 1, ..., N, (6)\nwhere \u03bbi = [\u03bbi1, ..., \u03bbiNi ] >. Let \u03bb denote the vector concatenating all \u03bbi, \u03bb = [\u03bb1;\u03bb2; ...;\u03bbN ]. Assume that bi is positive, bi should be highly close to the positive instances in Bi. Furthermore, if Bij is positive, \u03bbij will be assigned a larger value and small otherwise. According to the assumption that the positive instances get similar to each other, all bi should get close to each other in feature space. To keep the distances between bi as small as possible, the variance between all bi is minimized to obtain \u03bb:\nmin N\u2211 i=1 (Bi\u03bbi \u2212mi)T (Bi\u03bbi \u2212mi)\ns. t. Ni\u2211 j=1 \u03bbij = 1, \u03bbij \u2265 0, (7)\nwhere mi = 1\nN N\u2211 i=1 Bi\u03bbi.\nWe define matrix Zi with n\u00d7n size, i = 1, ..., N . The diagnose elements in Zi are 1 at location (j, j), j = Ni\u22121 + 1, ..., Ni in Zi and 0 elsewhere. It satisfies that N\u2211 i=1 Zi = I, where I is an identity matrix with n\u00d7 n size.\nEq. (7) is rewritten as a standard quadratic programming formulation:\nmin \u03bbT ( N\u2211 i=1 [BZi \u2212 1 N B]T [BZi \u2212 1 N B])\u03bb\ns. t.\nNi\u2211 j=1 \u03bbij = 1, \u03bbij \u2265 0, (8)\nwhere [BZi \u2212 1\nN B]T [BZi \u2212\n1\nN B] is really symmetric and semi-positively definite, so ( N\u2211 i=1 [BZi \u2212\n1\nN B]T [BZi\u2212\n1\nN B]) is also real symmetric and semi-positive definite. The non-zero part of BZi\u03bb\nis equal to Bi\u03bbi. The formulation Eq. (8) is a convex optimization problem.\nTo tackle nonlinear separable data, the kernel trick is applied to Eq. (8). We reformulate the B>B as the kernel matrix K, whose element Kij at i-th row and j-th column is the kernel function value ker(xi, xj), i, j = 1, ..., n. Eq. (8) is reformulated:\nmin \u03bbT ( N\u2211 i=1 [KZi \u2212 2 N KZi + 1 N2 K])\u03bb\ns. t.\nNi\u2211 j=1 \u03bbij = 1, \u03bbij \u2265 0. (9)\nAs \u03bb is known by Eq. (9), we can learn the concept of positive instance from {bi, ..., bN} using one-class SVM method. The kernel function value between two virtual positive instances is computed:\nker(bi, bj) = Ni\u2211 k=1 Nj\u2211 r=1 \u03bbik\u03bbjrker(BikBjr). (10)\nEq. (2) is reformulated as the following:\nmin \u03b1\n1\n2 \u2211 i,j \u03b1i\u03b1j \u2211 k,r \u03bbik\u03bbjrker(Bik, Bjr)\ns. t. 0 \u2264 \u03b1i \u2264 1 \u03bdN , \u2211 i \u03b1i = 1. (11)\nAs is described previously, \u03b1i in three different ranges implies where bi locates in feature space. According to the value of \u03b1i, bag Bi can be summarized as two kinds:\nDefinition 1 (Support Bag) For a bag Bi, if 0 < \u03b1i < 1 \u03bdN , then Bi is a \u201cSupport Bag\u201d.\nDefinition 2 (Outlier Bag) For a bag Bi, if \u03b1i = 1 \u03bdN , then Bi is an \u201cOutlier Bag\u201d.\nBy solving Eq. (11), the prediction function is: f(Bi) = sign (\nmax r=1,...,Ni\nl(Bir) ) ,\nl(Bir) = N\u2211 j=1 \u03b1j Ni\u2211 k=1 \u03bbjkker(Bir, Bjk)\u2212 \u03c1, (12)\nwhere Bi is the predicted bag. \u03c1 is computed:\n\u03c1 = N\u2211 i=1 \u03b1i Ni\u2211 k=1 Nj\u2211 r=1 \u03bbik\u03bbjrker(Bik, Bjr),\nBj satisfies: 0 < \u03b1j < 1\n\u03bdN .\n(13)\nIf it holds that max j f(Bij) = f(bi),\u2200i = 1, ..., N , \u03bd satisfies the following theorem:\nTheorem 1 If \u03c1(6= 0) is the solution of Eq. (11) and max j f(Bij) = f(bi)(\u2200i = 1, ..., N), the following holds: \u03bd is the upper bound on the fraction of outlier bags.\nProof Each bag is converted to one virtual instance by Eq. (6). Theorem 1 is easy to be validated from proposition 4 in [18][17]. 2\nIf f(Bi) = f(bi) = 1 , at least one instance in Bi must locate on or inside the prediction function boundary(Eq. (12)). However, f(Bi) = f(bi) = 1 cannot be guaranteed completely. In other words, when the virtual instance bi = Bi\u03bbi falls inside or on the function boundary, all instances in Bi may fall outside. It can be expressed as follows:\nf(bi) = +1 6= max j=1,...,Ni f(Bij) = \u22121. (14)\nSo the assumption max j f(Bij) = f(bi) in Eq. (1) does not always hold in the proof of Theorem 1. It causes the upper bound on outlier bags fraction is larger than \u03bd sometimes. To keep the upper bound of outlier bags fraction on \u03bd, our solution is to select the instance Bisi that is closest to function boundary in each bag Bi and add it to the training set. We learn a new prediction function with the updated training set. Now the training set consists of both the selected instances {Bisi} and the virtual instances {bi}. The parameter to control the number of outlier bag becomes \u03bd\n2 since the number of training instances is 2N here. Bisi is determined by\nthe following criterion:\nsi = arg max j=1,...,Ni l(Bij). (15)\nThe new decision function will replace the result of Eq. (11) by solving the following quadratic programming problem:\nmin \u03b1\n1\n2 \u2211 p,q \u03b1p\u03b1q \u2211 p,q ker(xp, xq)\ns. t. 0 \u2264 \u03b1p \u2264 1 \u03bd2N , \u2211 p \u03b1p = 1\nxp, xq \u2208 {bi} \u222a {Bisi},\ni = 1, ..., N, p, q = 1, ..., 2N.\n(16)\nThe prediction function from Eq. (16) is:\nf(Bi) = sign (\nmax r=1,...,Ni\nl(Bir) )\nl(Bir) = 2N\u2211 p=1 \u03b1pker(Bir, xp)\u2212 \u03c1,\n\u03c1 = 2N\u2211 p=1 \u03b1pker(xp, xq), xq satisfies: 0 < \u03b1q < 1 \u03bd2N .\n(17)\nNow Eq. (17) is the desired prediction function instead of Eq. (12) if \u2203i, f(Bi) 6= f(bi). In Eq. (16), the upper bound on outlier bags fraction satisfies the following theorem:\nTheorem 2 If \u03c1(6= 0) is the solution of Eq. (16), the following holds: \u03bd is the upper bound on the fraction of outlier bags.\nProof If bi falls outside andBisi locates inside the prediction function boundary, it holds max j f(Bij = f(Bisi) = 1). In the worst case, there are \u03bd 2 \u00d7 2N = \u03bdN instances in the set {Bisi} falls outside the prediction function boundary. This implies that the largest number of outlier bags is \u03bdN . So in the worst case the upper bound on outlier bag fraction reaches \u03bd in Eq. (17). 2"}, {"heading": "2.2.2 Query Step", "text": "As is described previously, the positive instances should gather compactly in feature space. On the other hand, the distribution of negative instances is unknown. If the negative instances are scattered and dissimilar to each other greatly (Figure 2), we can get the prediction function to enclose most positive instances in feature space easily. However, negative instances are also\nclustered sometimes (Figure 3) and even share a higher similarity than positive. Take face identification as an example, some face images are used to learn a prediction function and each image contains the same background (for example, tree, sky and so on). If there is a higher similarity between background patches than face ones, the variance of negative instances get smaller than that of positive. The result of training step is to enclose the negative instances with a higher similarity instead of the desired positive cluster. To avoid this result, our method requires to confirm whether the cluster enclosed by prediction function boundary is composed of positive or negative instances. We select an instance Bqr characterizes this cluster best to query its label. The queried instance shares the same label with the most member of this cluster. If the queried instance is positive, this cluster is positive with a high likelihood and negative otherwise.\nThe function l(x) is a good measure on the membership of x to the resulted cluster. When x locates outside the cluster, l(x) < 0 and l(x) \u2265 0 otherwise. The larger l(x) is, the closer x gets to the center of the cluster. So the instance Bqr with maximum l(x) characterizes this cluster best and is selected to query its label:\nBqr = arg max i,j l(Bij)\ns.t. l(Bij) \u2265 0 (18)\nIf prediction function boundary is like a circle, the queried instance usually locates closest to the circle center than others (Figure 3). If Bqr is negative, this cluster should be also negative. Otherwise, this cluster should be positive. Figure 3 illustrates the queried instance location when there are more than one cluster in feature space.\nIf the queried instance is negative, we update the current training bags by removing the instances satisfying {Bij|l(Bij) \u2265 0} and go back to the training step for a second instance label query. PMI works in such a recycle between training and query steps. PMI terminates when the queried instance is positive or there is one empty bag in training set. PMI is summarized in Algorithm 1.\nIn Algorithm 1, the number of queried instances satisfies the following:\nTheorem 3 In Eq. (11), if the parameter \u03bd < 1\nN , the maximum number of queried instances is\nmin i=1,...,N\nNi \u2212 1 . Otherwise, the maximum number of queried instance is d n\n(1\u2212 \u03bd)N e \u2212 1.\nProof According to Theorem 1, \u03bd < 1 N\nimplies there is no outlier bag in the current training set. The step 4 in Algorithm 1 guarantees that there is at least one instance Bij satisfying f(Bij) = 1 for every bag Bi, i = 1, ..., N . Thus, At least one instance in each bag must be removed in every instance removal operation. If one training bag gets empty after removal, PMI terminates at the step 6 in Algorithm 1. Therefore, the maximum number of queried instances is mini=1,...,N Ni\u22121.\nIf \u03bd \u2265 1 N , at least (1 \u2212 \u03bd)N instances will be removed at the step 8 according to Theorem 1. So the maximum number of queried instances is d n\n(1\u2212\u03bd)N e \u2212 1, where dxe denotes the minimum integer number not smaller than x. 2\nThe step 6 in Algorithm 1 needs some further explanations. If all members of one bag are labeled positive, these instances belongs to the positive class in that each training bag contains at least one positive instance.\nHowever, the instance label information is not always available. When it is difficult to query instance label, our solution is to assume that the dissimilarities between negative instances are larger than positive ones. Thus, Algorithm 1 terminates at the step 4 and outputs the prediction function f(x) without running the query step. In most real applications, negative instances cover a diverse range of backgrounds or noise. Therefore, it is almost impossible that negative instances share a higher similarity than positive. It is high likely to achieve the concept on positive instance after only one training step. The experimental results in the next section illustrate that our hypothesis is applicable for most of data sets. It also implies that the maximum query number in Theorem 3 is not a tight bound. The number of queried instance label usually is far smaller than the result in Theorem 3 and close to 1 mostly. It suggests that the cost of instance label query is very limited, which is much lower than that of labeling a large number of negative bags.\nAlgorithm 1 PMI Input: the training bags B = {Bi, i = 1, ..., N}; kernel function parameters; \u03bd;\n1: Solve Eq. (9) to get \u03bb; 2: Solve Eq. (11) to get a prediction function f0(Eq. (12)). Set f = f0; 3: If there exists one bag Bi satisfying both \u03b1i < 1 \u03bdN\nand maxj\u22081,...,Ni f(Bij) = \u22121, select an instance Bisi by Eq. (18) from every training bag Bi, i = 1, ..., N ; 4: Solve Eq. (16) to get a prediction function f1(Eq. (17)). Set f = f1; 5: If no instance label information is available, PMI terminates; 6: If there is one bag Bi, i = 1, ..., N satisfying that \u2200j \u2208 1, ..., Ni f(Bij) = 1, PMI terminates; 7: Select the most certain instance by Eq. (18) to query its label; 8: If the queried label is positive, PMI ends. Otherwise, update the training set B by removing\nthe instances \u2200i, j Bij satisfying f(Bij) = 1. Go back to the step 1; Output: prediction function f ;"}, {"heading": "2.3 Time Complexity Analysis", "text": "The time complexity analysis of PMI involves Eq. (9), Eq. (11), Eq. (16) and the number of the queried instances. These three objective functions Eq. (9), Eq. (11) and Eq. (16) are three quadratic programming problems with the time complexities O(n3), O(N3), O(N3) respectively. Since n > N in general, the term O(n3) dominates the time complexity of PMI. According to Theorem 3, the number of queried instances is related to n\nN . So the time complexity is O(\nn4 N ).\nBut in real applications, the queried instance number is much smaller than the theoretical result in Theorem 3 and can be approximated to a constant. The time complexity of PMI is close to O(n3)."}, {"heading": "3 Experiments", "text": "This section presents the experimental results on five benchmark and one face image data sets. We evaluate PMI compared the traditional MIL algorithms. To solve the quadratic programming problem in Eq. (2), the optimization toolbox \u201cMosek\u201d [16] is used."}, {"heading": "3.1 Benchmark Data Sets", "text": "Five benchmark data sets are used in our experiment. They are Musk1, Musk2, Elephant, Fox and Tiger, which were used frequently to test new MIL algorithms in previous studies[9, 10, 2]. Both Musk1 and Musk2 come from UCI data set web site [20]. And the other three derive from Corel image set [7]. The details of five data sets are described in Table 1.\nThe accuracies of PMI on five benchmark data sets are recorded in Table 3. To compare with PMI, we also provide the performances [10, 2] of several other traditional MIL algorithms . These traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].\nThe same with previous MIL studies, 10-fold cross validation strategy is used to get the accuracies of PMI. Both positive and negative bags are divided into 10 folds randomly and one fold positive and negative bags are selected as testing set. The remaining 9 fold positive bags (without negative ones) are the training set. The difference between that of PMI and 10-fold cross validation used in traditional MIL studies is that the training set of PMI does not contain 9 fold negative bags.\nTable 2 shows the numbers of positive bags used for training in PMI and other traditional MIL algorithms. Table 3 reports the accuracies of five approaches on five benchmark data sets. These accuracies are the average of 10 runs. PMI terminates at the step 5 of Algorithm 1 and returns f(x) due to no available instance label of these five data sets. PMI achieves close results to those of the other traditional MIL methods mostly. The result on Musk2 is better than miSVM, MI-SVM, EM-DD, but worse than MICA. Musk2 has only 39 positive bags, which are obviously smaller than the negative bag number 63. So it is exciting that PMI achieves the similar performance using much fewer training bags (only positive bags) to those of other MIL approaches. PMI achieves a little lower accuracy on Musk1 than mi-SVM, MICA and EM-DD, but a little larger than MI-SVM. The results of PMI are close to those of the other MIL methods on Elephant and Fox.\nAccording to the performance comparison in Table 3, if the traditional MIL methods achieve a high accuracy on one data set, PMI can also obtain similar result. But when the performance of traditional MIL method becomes very low(only about 60% accuracy on Fox data set), PMI will get a worse result than those of traditional MIL methods. We explain the phenomenon as follows. If there is a enough large margin and no overlap between positive and negative instances, the traditional MIL methods with positive and negative training bags usually get a perfect accuracy. Due to no overlap between positive and negative instances, PMI with only positive training bags can describe the positive instance distribution accurately and also achieves satisfying performance. However, if there exists a big overlap between positive and negative, we will get a poor result though both positive and negative bags are used as the training set. So the result will degrade furthermore with only positive training bags."}, {"heading": "3.2 Face Identification", "text": "This experiment shows result of PMI on face identification. The image set [1] is used to evaluate PMI. The task of face identification is predict whether an image contains face(s) or not. Each of these face images contains one person\u2019s face and the similar green background. This image set collects 37 persons\u2019 faces from various views under different light conditions. Three face samples for each person are recorded. So this face image set includes totally 111 face images. Every person has various expressions. These 37 persons include various types: male and female, young and old. Each image is 640\u00d7 480 size with JPEG file format. No non-face image is provided in this image set. Figure 4 illustrates some examples of this image set. Much more non-face images than face are needed in previous MIL studies. But no non-face image is provided in this data set.\nThe feature extraction method in [5] is used in our experiments. Only the brief introduction is provided. Please read [6] for more details. The first is to split each image into 4 \u00d7 4 blocks and compute L, U, V mean values of each block in LUV color space. Secondly, 4-order wavelet transformation is applied to L value on each block to compute mean values of LH, HL, HH bands. Each block is transformed into a 6-dimension (L,U,V,LH,HL,HH) feature vector and scaled into [0, 1]. We use the toolkit \u201cJSEG\u201d[8] to segment each image into regions. Each region is represented by the mean feature vector of the blocks in this region. So one region corresponds to an instance. An image is associated with a bag. The average instance number of positive bags is 7.8.\nWe select 100 background images(for the balance between positive and negative bags) randomly from [4, 14] as negative bags for testing and training in other traditional MIL methods. These\n100 negative images contain rich contents and backgrounds. The average instance number of negative bags is about 19. The feature extraction method is the same as that of the face images. The numbers of training bags in PMI and other MIL algorithms are reported in Table 4. The 5-fold cross validation accuracies of both the traditional MIL algorithms and PMI are reported in Table 5. Each accuracy is the average of 10 runs. PMI achieves close performances to the other MIL algorithms. The accuracy of mi-SVM keeps very close to that of PMI. In contrast, MI-SVM and DD fall a little behind the other three methods.\nTo validate Theorem 3, we use all face images as the training set. Table 6 reports the number of queried instances under different parameter combinations of \u03b3 and \u03bd. According to Theorem 3, the maximum number of queried instances is larger than d7.81e \u2212 1 = 7 when \u03bd \u2265 1\nN = 1 111 . 7.81\nis the average instance number of positive bags. The largest number in Table 6 is much smaller than 7, which demonstrates Theorem 3 is correct."}, {"heading": "4 Conclusion", "text": "This paper proposes a new algorithm PMI, which learns a prediction function with only positive bags. Our proposed algorithm is to reduce the label cost significantly compared with previous MIL algorithms without much accuracy loss. A detailed theoretic analysis is also provided. In most real applications, the queried instance number is much smaller than the theoretical result in Theorem 3 and can be approximated to a constant. The time complexity of PMI is approximated as O(n3). Experimental results illustrate that PMI usually achieves close performance to those of traditional MIL methods mostly especially when traditional MIL algorithms can predict bag label with a high accuracy. In the future work, we try to apply some dimensional reduction approaches [12, 13, 11] on our proposed method to improve the efficiency."}, {"heading": "Acknowledgement", "text": "Acknowledge here.\nAppendix\nAppendix here."}], "references": [{"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Robust object tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Miles: Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "J.Z. Wang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1931}, {"title": "Image categorization by learning and reasoning with regions", "author": ["Y. Chen", "J.Z. Wang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Unsupervised segmentation of color-texture regions in images and video", "author": ["Y. Deng", "B.S. Manjunath"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on (PAMI),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T. Dietterich", "R. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Multiple instance learning for computer aided diagnosis", "author": ["G. Fung", "M. Dundar", "B. Krishnapuram", "R.B. Rao"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Multiple rank multi-linear svm for matrix data classification", "author": ["C. Hou", "F. Nie", "C. Zhang", "D. Yi", "Y. Wu"], "venue": "Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Stable local dimensionality reduction approaches", "author": ["C. Hou", "C. Zhang", "Y. Wu", "Y. Jiao"], "venue": "Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Multiple view semi-supervised dimensionality reduction", "author": ["C. Hou", "C. Zhang", "Y. Wu", "F. Nie"], "venue": "Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["R.F.L. Fei-Fei", "P. Perona"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Drosophila gene expression pattern annotation through multi-instance multi-label learning", "author": ["Y.-X. Li", "S. Ji", "S. Kumar", "J. Ye", "Z.-H. Zhou"], "venue": "Computational Biology and Bioinformatics, IEEE/ACM Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J. Platt", "J. Shawe-Taylor", "A. Smola", "R. Williamson"], "venue": "Neural Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R. Williamson", "A. Smola", "J. Shawe-Taylor", "J. Platt"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Image annotation by graph-based inference with integrated multiple/single instance representations", "author": ["J. Tang", "H. Li", "G.-J. Qi", "T.-S. Chua"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Multiple instance learning with multiple objective genetic programming for web mining", "author": ["A. Zafra", "E.L. Gibaja", "S. Ventura"], "venue": "Applied Soft Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "A multiple instance learning approach for content based image retrieval using one-class support vector machine", "author": ["C. Zhang", "X. Chen", "M. Chen", "S. Chen", "M. Shyu"], "venue": "In Multimedia and Expo,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "EM-DD: An improved multiple-instance learning technique", "author": ["Q. Zhang", "S. Goldman"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Content-Based Image Retrieval Using Multiple- Instance Learning", "author": ["Q. Zhang", "S.A. Goldman", "W. Yu", "J.E. Fritts"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": "Multiple instance learning (MIL) is introduced by [9] to solve the drug activity prediction problem.", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 124, "endOffset": 134}, {"referenceID": 3, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 124, "endOffset": 134}, {"referenceID": 14, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 124, "endOffset": 134}, {"referenceID": 18, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 152, "endOffset": 160}, {"referenceID": 16, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 152, "endOffset": 160}, {"referenceID": 1, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 171, "endOffset": 174}, {"referenceID": 15, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 209, "endOffset": 213}, {"referenceID": 6, "context": "During the past years, MIL approaches have been applied successfully to many real applications such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining [21], gene expression [15] and medical diagnosis [10].", "startOffset": 236, "endOffset": 240}, {"referenceID": 16, "context": "One related work is [22].", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "So [22] can be applied to a limited range of settings.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "[22] cannot select multiple positive instances accurately in one bag.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "It causes the prediction accuracy of [22] is too sensitive to the provided positive instance.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "The training step is to convert the problem of learning with only positive bags into a one-class classification problem [18] to get a classifier f .", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "One-class SVM [18] is involved in PMI, so a brief review on one-class SVM is presented here.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "As is stated in [2, 10], the selection of instances will be formulated as a combinatorial optimization problem, which is non-convex and difficult to solve especially when a global optimal solution", "startOffset": 16, "endOffset": 23}, {"referenceID": 6, "context": "As is stated in [2, 10], the selection of instances will be formulated as a combinatorial optimization problem, which is non-convex and difficult to solve especially when a global optimal solution", "startOffset": 16, "endOffset": 23}, {"referenceID": 13, "context": "Theorem 1 is easy to be validated from proposition 4 in [18][17].", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "Theorem 1 is easy to be validated from proposition 4 in [18][17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "They are Musk1, Musk2, Elephant, Fox and Tiger, which were used frequently to test new MIL algorithms in previous studies[9, 10, 2].", "startOffset": 121, "endOffset": 131}, {"referenceID": 6, "context": "They are Musk1, Musk2, Elephant, Fox and Tiger, which were used frequently to test new MIL algorithms in previous studies[9, 10, 2].", "startOffset": 121, "endOffset": 131}, {"referenceID": 0, "context": "They are Musk1, Musk2, Elephant, Fox and Tiger, which were used frequently to test new MIL algorithms in previous studies[9, 10, 2].", "startOffset": 121, "endOffset": 131}, {"referenceID": 6, "context": "To compare with PMI, we also provide the performances [10, 2] of several other traditional MIL algorithms .", "startOffset": 54, "endOffset": 61}, {"referenceID": 0, "context": "To compare with PMI, we also provide the performances [10, 2] of several other traditional MIL algorithms .", "startOffset": 54, "endOffset": 61}, {"referenceID": 0, "context": "These traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "These traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].", "startOffset": 55, "endOffset": 58}, {"referenceID": 17, "context": "These traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "These traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "The feature extraction method in [5] is used in our experiments.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "Please read [6] for more details.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "We use the toolkit \u201cJSEG\u201d[8] to segment each image into regions.", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "We select 100 background images(for the balance between positive and negative bags) randomly from [4, 14] as negative bags for testing and training in other traditional MIL methods.", "startOffset": 98, "endOffset": 105}, {"referenceID": 8, "context": "In the future work, we try to apply some dimensional reduction approaches [12, 13, 11] on our proposed method to improve the efficiency.", "startOffset": 74, "endOffset": 86}, {"referenceID": 9, "context": "In the future work, we try to apply some dimensional reduction approaches [12, 13, 11] on our proposed method to improve the efficiency.", "startOffset": 74, "endOffset": 86}, {"referenceID": 7, "context": "In the future work, we try to apply some dimensional reduction approaches [12, 13, 11] on our proposed method to improve the efficiency.", "startOffset": 74, "endOffset": 86}], "year": 2016, "abstractText": "In traditional multiple instance learning (MIL), both positive and negative bags are required to learn a prediction function. However, a high human cost is needed to know the label of each bag\u2014positive or negative. Only positive bags contain our focus (positive instances) while negative bags consist of noise or background (negative instances). So we do not expect to spend too much to label the negative bags. Contrary to our expectation, nearly all existing MIL methods require enough negative bags besides positive ones. In this paper we propose an algorithm called \u201cPositive Multiple Instance\u201d (PMI), which learns a classifier given only a set of positive bags. So the annotation of negative bags becomes unnecessary in our method. PMI is constructed based on the assumption that the unknown positive instances in positive bags be similar each other and constitute one compact cluster in feature space and the negative instances locate outside this cluster. The experimental results demonstrate that PMI achieves the performances close to or a little worse than those of the traditional MIL algorithms on benchmark and real data sets. However, the number of training bags in PMI is reduced significantly compared with traditional MIL algorithms.", "creator": "LaTeX with hyperref package"}}}