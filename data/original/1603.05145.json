{"id": "1603.05145", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions", "abstract": "Many deep Convolutional Neural Networks (CNN) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples. We hypothesize that this is caused by a failure to suppress unusual signals within network layers. As remedy we propose the use of Symmetric Activation Functions (SAF) in non-linear signal transducer units. These units suppress signals of exceptional magnitude. We prove that SAF networks can perform classification tasks to arbitrary precision in a simplified situation. In practice, rather than use SAFs alone, we add them into CNNs to improve their robustness. The modified CNNs can be easily trained using popular strategies with the moderate training load. Our experiments on MNIST and CIFAR-10 show that the modified CNNs perform similarly to plain ones on clean samples, and are remarkably more robust against adversarial and nonsense samples.", "histories": [["v1", "Wed, 16 Mar 2016 15:35:07 GMT  (1440kb,D)", "http://arxiv.org/abs/1603.05145v1", "11 pages, 12 figures"]], "COMMENTS": "11 pages, 12 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["qiyang zhao", "lewis d griffin"], "accepted": false, "id": "1603.05145"}, "pdf": {"name": "1603.05145.pdf", "metadata": {"source": "META", "title": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions", "authors": ["Qiyang Zhao", "Lewis D Griffin"], "emails": ["ZHAOQY@BUAA.EDU.CN", "L.GRIFFIN@CS.UCL.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "Although deep CNNs have delivered state-of-the-art performance on several major computer vision challenges (Krizhevsky et al., 2012)(He et al., 2015), they have some pooly understood aspects. In particular, it was shown in (Szegedy et al., 2014) that one can easily construct an adversarial sample by imperceptible perturbation of any clean sample using the box-constrained limited-memory BFGS method (Andrew & Gao, 2007). These samples are perceptually similar to the original ones, but are predicted to be of different categories, and even generalize well across different CNN models and train sets (Szegedy et al.,\nCopyright 2016 by the author(s).\n2014). In (Goodfellow et al., 2015), a simpler method, fast gradient sign (FGS), is proposed to compute adversarial samples effectively. Recently, a more complicated method is proposed to construct an adversarial sample with both the prediction and internal CNN features being similar to another arbitrary sample (Sabour et al., 2016). Nonsense samples (called rubbish samples in (Goodfellow et al., 2015)) are another challenge to the robustness of CNNs. These are generated from noise or arbitrary images using FGS-like methods (Goodfellow et al., 2015)(Nguyen et al., 2015). Nonsense samples are totally unrecognizable but popular CNNs typically predict with high confidence that they belong to some category. All these references argue that adversarial and nonsense samples arise from some unknown flaw of popular deep CNNs, either in their structure or training methods (Szegedy et al., 2014)(Goodfellow et al., 2015)(Sabour et al., 2016).\nA proposed solution to the problem is adversarial training (Szegedy et al., 2014)(Goodfellow et al., 2015): ad-\nar X\niv :1\n60 3.\n05 14\n5v 1\n[ cs\n.C V\n] 1\n6 M\nar 2\nversarial samples are constructed according to the current model, then utilized in subsequent training epochs. CNNs so trained will be vulnerable to new adversarial samples; but repeated rounds of adversarial training eventually lead to improved robustness, at the price of an increased training load. Unfortunately, the final error rates on the newest adversarial samples, after this scheme, remain large (Goodfellow et al., 2015). It is interesting that adversarial training can be understood as a new regularization method for training CNNs; for example in (Miyato et al., 2016), virtual adversarial samples are constructed to force the model distributions of a supervised learning process to be smooth. Adversarial samples can also be used to establish new unsupervised and/or semi-supervised representation learning methods (Radford et al., 2016) (Springenberg, 2016) or autoencoders (Makhzani et al., 2015).\nA different approach to the adversary problem is to modify the structure of the CNN. Autoencoders can be added to denoise input samples and improve the local stability of feature spaces in individual layers (Gu & Rigazio, 2015), where the contractive penalty terms are designed to punish unexpected feature changes caused by small perturbations. The idea is similar to (Miyato et al., 2016). However this method cannot achieve good robustness on moderately large perturbations while keeping high accuracies on clean samples (Gu & Rigazio, 2015). Another interesting idea, proposed in (Chalupka et al., 2015), is to identify features that are causally related, rather than merely correlated, with the categories. It is thus possible to predict the semantic categories according to causal features, so to improve the robustness. However it would be a large burden to integrate this idea with popular CNN models.\nIn this paper we propose a solution to the adversary problem that does not increase much training load, nor increase the complexity of the network. The solution is to use symmetric activation function (SAF) units. These are nonlinear transduction units that suppress exceptional signals. We incorporate SAFs into CNNs by addition or replacement of layers of standard models. The resulting CNNs are easily trained using standard approaches and give low error rates on clean and perturbed samples."}, {"heading": "2. Motivation", "text": "Unlike CNNs and linear classification models, local models (Moody & Darken, 1989)(Jacobs et al., 1991) based on radial basis functions (RBF) (Broomhead & Lowe, 1988) are intrinsically immune to abnormal samples which are far from high-density regions: they will always output low confidence scores for such. Furthermore, the RBFs are robust against tiny perturbations of the input. This suggests the possibility of improving the robustness of CNNs by using RBF units.\nRBFs have been used in several ways within networks, e.g. as a hidden layer in mixture of experts (Jacobs et al., 1991), and as the top layer units in LeNet-5 (LeCun et al., 1998). Only a few implementations attempt multiple RBF layers (Craddock & Warwick, 1996)(Bohte et al., 2002)(Mra\u0301zova\u0301 & Kukac\u030cka, 1996). The reason for avoiding multiple layers is the high-dimensionality of the parameter space of each RBF unit, together with the need for many RBF units. This high-dimensionality makes it hard to train deep networks involving RBF units to achieve acceptable accuracies (Goodfellow et al., 2015). Some have argued that more powerful optimization methods are needed to make this work (Goodfellow et al., 2015).\nWe have considered whether it is feasible to use 1-D RBFs instead of high-dimension ones. To assess this, we run a LeNet-5 model (LeCun et al., 1998) (see Sec. 5.1 for its details) on the MNIST test set, and show the histograms and joint distributions of features from the second convolutional layer in Fig. 3. The empirical distributions of these features are approximately Gaussian. In our experiments, we find it is effective to distinguish abnormal samples from normal ones by exploiting this observation. In the second row of Fig. 3, we show the feature pair values of nonsense samples of pure Gaussian noise in red squares. Although the LeNet-5 model incorrectly assigns nonsense samples to meaningful categories with high confidence (\u223c 99%), as the figure shows, their features usually deviate from the high-density regions of clean samples. The higher the layer the more distinct the deviation.\nWe also observe that features are not strongly correlated, as shown in Fig. 4. Therefore it is viable to use 1-D\nGaussian distributions instead of high-dimension ones to model the feature statistics of clean samples. Such 1-D RBFs can be used within networks, as multi-dimensional RBFs have been. The potential advantage is obvious: it may be possible to use much fewer 1-D RBF units than high-dimensional ones, and therefore with fewer parameters to train. It should be pointed out that we do not aim to approximate a traditional RBF network of only a single hidden layer using 1-D RBFs. Instead, we propose to integrate 1-D RBF units with popular CNN models. The integration is neither a pure \u2018local\u2019 model such as RBF networks nor a pure \u2018distributed\u2019 model such as CNNs. It models diverse features \u2018locally\u2019 with 1-D RBFs but combines them in a \u2018distributed\u2019 way to give the final representation."}, {"heading": "3. Symmetric Activation Functions", "text": "Compared with commonly-used activation functions such as Rectified Linear Units (ReLU) (Nair & Hinton, 1996) and sigmoid, it is well known the RBFs have a localizing property. 1-D RBFs suppress signals that deviate from the normal in either direction while their counterparts suppress signals only unilaterally. By suppressing the signals of abnormal samples, we ensure that no strong prediction\nconfidence will be produced at the top layer. Inspired by the ReLU, we propose the mirrored rectified linear unit (mReLU). In the paper, we call both the 1-D RBF and the mReLU symmetric activation functions (SAF) since they have the reflectional symmetry."}, {"heading": "3.1. 1-D Radial Basis Function", "text": "The 1-D RBF adopted in the paper is \u03c3(x) = e\u2212x 2\n, and its derivative is \u22122x\u03c3(x) (Fig. 1.a-b). Although it is parameter-free, any needed flexibility can be achieved by argument rescaling and shifting in the preceding convolutional layer."}, {"heading": "3.2. Mirrored Rectified Linear Unit (mReLU)", "text": "The 1-D RBF requires exponential and square calculations. These will add considerable load in both the training and test stages. Inspired by the simplicity of the ReLU, we propose the mReLU composed of only basic arithmetic and logic calculations (Fig. 1.c-d):\nmReLU(x) =  1 + x, 0 > x > \u22121,1\u2212 x, +1 > x > 0, 0, otherwise,\n(1)\nor equivalently\nmReLU(x) = min (ReLU(1\u2212 x),ReLU(1 + x)) . (2)\nIts derivative is\nd\ndx mReLU =  +1, 0 > x > \u22121;\u22121, +1 > x > 0; 0, otherwise.\n(3)"}, {"heading": "3.3. The Capacity of SAF Networks", "text": "In the limit of using an infinite number of RBF units, a network of high-dimension RBFs can approximate any function with arbitrary precision (Park & Sandberg, 1991). We can use the products of 1-D RBFs to exactly mimic any high-dimensional RBF, and therefore construct a 1- D RBF network for function approximation. However, the product-of-units scheme is not compatible with popular CNNs, and it does not work for the mReLUs. Below we establish a property which is weaker than function approximation but with closer ties to our problem: sample classification.\nWe express the problem in geometric form. Suppose we have N template samples Z1, Z2, ..., ZN in a uniform n-D data space, and a fidelity threshold r. If an input sample X has \u2016X \u2212 Zi\u2016 \u2264 r for a certain i \u2208 {1, 2, ..., N}, we classify X into the same category as Zi; otherwise we regard it as a nonsense sample. We assume the problem is selfconsistent, i.e., there are no template samples from different categories with overlapping radius-r hyperspheres. It is sufficient to find a method to judge whether a sample is in the radius-r hypersphere centered at a given sample Zi. First, we have Lemma 1. For a given hypersphere, we can build a network of two SAF layers to judge whether a given sample is in the hypersphere with arbitrary precision.\nProof. We present the proof for 1-D RBFs, as it is straightforward to be extended to the mReLU. We build a basic block of five layers as shown in Fig. 5.a. Let X = [x1, x2, ..., xn] T , Zi = [zi,1, zi,2, ..., zi,n] T . We calculate\nrk(X) = \u03bb(xk \u2212 zi,k), sk(X) = e\u2212r 2 k , t(X) = \u2211 k sk \u2212 n, y(X) = e\u2212t 2 (4)\nwhere \u03bb is a carefully chosen parameter, the sk\u2019s are the output of the first SAF layer and the single y is the output of the second SAF layer. To achieve a given error rate , it is sufficient to find a threshold \u03c4 which holds V (\u2126\u2229\u2126\n\u2032) V (\u2126\u222a\u2126\u2032) \u2265\n1 \u2212 . Here V (\u00b7) is the volume of a closed subspace, \u2126 = {X : \u2016X \u2212 Zi\u2016 \u2264 r} and \u2126\u2032 = {X : y(X) \u2265 \u03c4}. We let\nf1(r) = y ( [zi,1 + r, zi,2, ..., zi,n] T ) ,\nf2(r) = y ([ zi,1 +\nr\u221a n , zi,2 + r\u221a n , ..., zi,n + r\u221a n\n]T) .\n(5)\nIt is easy to prove that f1(r) \u2264 y(X) \u2264 f2(r) for any X which holds \u2016X \u2212 Zi\u2016 = r.\nWe set \u03c4 = f1(r) and r0 = f\u221212 (\u03c4). It is easy to see that r0 \u2264 r. So we have \u2126\u2032\u2032 \u2282 \u2126\u2032 \u2282 \u2126 where \u2126\u2032\u2032 = {X :\n\u2016X \u2212 Zi\u2016 \u2264 r0}. Since we have V (\u2126) = c \u00b7 rn and V (\u2126\u2032\u2032) = c \u00b7 rn0 for a certain constant c, it is required that( r0 r )n \u2265 1\u2212 , or equivalently f\u221212 (\u03c4)\nf\u221211 (\u03c4) \u2265 (1\u2212 )\n1 n . (6)\nSince lim \u03c4\u21921\nf\u221212 (\u03c4) f\u221211 (\u03c4) = 1, there must exist a \u03c40 satisfying eq.\n6, and so does \u03bb. In other words, we can judge whether a sample X is in the hypersphere of Zi with a error rate less than by comparing y(X) and \u03c40.\nFor example, we can approximate the unit disk with three 1-D RBF units in Fig. 6.a. Then we use the basic block in the proof of Lemma 1 to build the classification network. As shown in Fig. 5.b, we lay a basic block for each template sample, then choose the maximum ymax across all basic blocks. It ymax is larger than the threshold \u03c40 in the proof of Lemma 1, we output the category of the chosen basic block, otherwise we give a nonsense prediction. Interestingly, if we are allowed to use more 1-D RBF units in the basic block, the output y will be increasingly similar to a high-dimension RBF, as shown in Fig. 6.b-c.\nAlthough the network in Fig. 5.b is compatible with the popular CNNs in structure, the network is infeasible in practice as we would require a huge number of SAF units.\nInstead in Sec. 4, we exploit the advantages of deep CNNs, which generate representations distributedly, to build effective classifiers with moderate number of SAFs."}, {"heading": "4. Building Robust CNNs", "text": "In popular CNNs, convolutional layers are cascaded to learn increasingly abstract features (Zeiler & Fergus, 2014). We find the empirical distributions of these features are fairly compact and symmetric (Fig. 3). Thus it is feasible to suppress unusual signals using SAFs which are similarly shaped. We insert additional SAF layers immediately after the convolutional layers to suppress unusual signals, as shown in Fig. 2. Any ReLU activation layers can be discarded since the SAF outputs are non-negative. We do the same thing to sigmoid activation layers. We also add a 1- D RBF layer after the fully-connected layer as a new final layer, but do not insert SAF layers between fully-connected layers. In the remainder, we call the modified models robust CNNs and the original ones plain CNNs."}, {"heading": "4.1. The Hybrid Loss function", "text": "We design a special hybrid loss function for the robust CNN that combines the negative logarithm of the normalized value, and the weighted p-order errors (p > 1) according to:\nLh (X, `, \u03b8) = \u2212\u03b11\u00b7ln (\ny`\u2211 i yi\n) +\u03b12\u00b7(1\u2212 y`)p+\u03b13\u00b7 \u2211 i 6=` ypi\n(7) where ` is the correct label, Y = {yi} is the prediction on X by the robust CNN with the parameter \u03b8, L is the amount of categories, and \u03b11, \u03b12, \u03b13 are weighting parameters. Lh is designed to prefer y` to be large both relatively and absolutely. It is necessary to involve the p-order errors to get sufficiently large y`, thus to recognize nonsense samples which would be of small y`\u2019s. We set p = 2 in all our experiments."}, {"heading": "4.2. Robustness Analysis", "text": ""}, {"heading": "X hg gn Yhngf f g", "text": "In the simplified robust CNN in Fig. 7, fi\u2019s are convolutional or pooling modules, and gi\u2019s are the SAF modules following convolutional layers. We have\nY = gn(fn(gn\u22121(fn\u22121(...g1(f1(X))...)))) (8)\nwhere Y = [y1, y2, .., yL] T is the confidence vector of\nlength L, the number of categories. The partial derivative of y` with respect to xk is\n\u2202y` \u2202xk = dgn,` dfn,`\n\u00b7 \u2211\nin\u22121,...,i1\n( \u2202fn,`\n\u2202gn\u22121,in\u22121 \u00b7 ... \u00b7 \u2202f1,i1 \u2202xk\n) (9)\nwhere i1\u2192 i2\u2192, ...,\u2192 in\u22121\u2192 ` is a path from xk to y` crossing all layers, in other words, they are the indices of cells whose receptive field contain xk. For unusual signals which are away from the high-density regions, the g\u2019s in eq. 9 will be considerably small. Consequently, according to the derivatives of SAFs shown in Fig. 1, the \u2202g(.,.)\u2202f(.,.) \u2019s are also small or nearly zero, implying \u2202y`\u2202xk is unlikely to be large. If the norm of the gradient \u2207y` with respect to X is small, we have to make a large perturbation \u2206X to achieve a noticeable change of y`, as shown in Fig. 8. In conclusion, to achieve an adversarial sample successfully, the perturbation for the robust CNN would be larger than that for the plain CNN."}, {"heading": "4.3. Training Issues", "text": "An adversarial training requires repeated generations of adversarial samples and so increases the training load significantly. In this paper, we also use training data additional to the clean samples; but instead of adversarial samples, we use random perturbation of the mean of training data. We find the mean sample itself might be easily mis-classified (it seldom belongs to any meaningful category), and we can generate nonsense samples by perturbing it slightly. Therefore we use it as a typical hard instance, and train robust CNNs to produce high confidence for nonsense on it. However, there is only one mean sample and thus little effect on the training process. So we perturb it with the Gaussian noise and get a certain number of noisy copies. We call this trick mean training for short.\nIt is suggested in (Goodfellow et al., 2015) that randomly perturbed samples are of little significance. Interestingly, we do find them effective for our network, especially when adopted together with mean training. We generate per-\nturbed samples by adding perturbations randomly drawn from [\u2212\u03b4,+\u03b4]w\u00d7h\u00d7c (w: width, h: height, c: channel) to the original samples. We perturb a fraction of the train set and leave others untouched. We call this trick random training for short.\nWe use the standard stochastic gradient descent method to train the robust CNNs. It is unnecessary to use a pretraining stage to find RBF centers because 1-D RBFs are parameter-free. The batch normalization trick (Ioffe & Szegedy, 2015) is necessary to accelerate training robust CNNs and prevent the gradient from vanishing."}, {"heading": "5. Experiments", "text": "We use two well-studied datasets, MNIST (LeCun et al., 1998) and CIFAR-10 (CIFAR, accessed in Feb 2016), in our experiments. In the following figures and tables, we use plain, RBF and mReLU to refer respectively to plain CNNs, robust CNNs using 1-D RBF, and robust CNNs using mReLUs. We use the flags -a, -r and -m to indicate the use of adversarial, random and mean training. The plain CNNs are typical models implemented in (Vedaldi, accessed in Feb 2016). We use all default settings in (Vedaldi, accessed in Feb 2016) to train them, except for whitening and normalization. We plan to release our source code and data upon acceptance.\nIn related literature, CNNs are expected to be robust against three kinds of sample: adversarial, nonsense and noisy. If our proposals about SAFs are correct, then robust CNNs will be as accurate as plain CNNs; on adversarial/noisy samples with tiny perturbations, the classification accuracy should not drop remarkably; while on severely perturbed samples or pure noise images, they should be classified as nonsense. The nonsense case is a little different for models without an explicit nonsense category: we consider they predict a sample as nonsense if all confidence scores of meaningful categories are below a threshold, e.g. 0.5.\nWe follow (Goodfellow et al., 2015) to generate adversarial samples and nonsense samples by:\nXadv = X + 255 \u00b7 \u03b2 \u00b7 sign (\u2207Lh(X, `max, \u03b8)) (10)\nwhere \u2207Lh(X, `, \u03b8) is the gradient towards the smaller loss for a certain incorrect category `, \u03b2 is the strength parameter, and `max = arg max` \u2016\u2207Lh(X, `, \u03b8)\u20161. X and Xadv are the original and adversarial/nonsense samples respectively. Nonsense samples are generated from the stationary Gaussian noise, shifted and scaled to cover the range 0\u223c255. Noisy samples are generated by perturbing clean samples with stationary Gaussian noise. We collect classification accuracies on varying perturbation strength \u03b2, rather than at a single strength value, to better understand the robustness of CNNs. Meanwhile, we present the\nwell-adopted Peak-Signal-Noise-Ratios (PSNR) to assess the image quality easily."}, {"heading": "5.1. MNIST", "text": "We use LeNet-5 (LeCun et al., 1998) as the plain CNN for MNIST, please see Table 1 for its structure. There are 70,000 clean samples and we follow (Vedaldi, accessed in Feb 2016) to use 60,000 as the train set and 10,000 as the test set. There are 10,000 groups of nonsense samples. All CNNs are trained with 20 epochs. We set \u03b11 = 1, \u03b12 = 1, \u03b13 =0 for the hybrid loss functions of robust CNNs.\nWe present accuracies and error rates in Fig. 9 and Table 2. It is clear that CNNs using SAFs are much more robust than the plain CNN. The best one, mReLU-r-m, makes marginally more errors than the plain CNN for clean samples, adversarial samples with perturbation of up to strength \u03b2 = 0.01, and noisy samples of up to \u03b2 = 0.10; but has greatly superior performance for adversarial and noisy samples of large perturbation, and all nonsense samples. Particularly, the accuracy of mReLU-r-m under adversarial perturbations of the strength \u03b2 = 0.25 is only 0.073, which is much lower than 0.179 of Maxout (Goodfellow et al., 2013) trained using adversarial training (Goodfellow et al., 2015).\nRandom training is very effective in improving the robustness of SAF CNNs against both adversarial and noisy samples. In our opinion, random perturbations of the train samples make the large-output/non-zero widths of SAFs effectively broader, which improving robustness to perturbations of moderate strength. Using all training tricks, we can improve all CNNs to be considerably robust against nonsense and noisy samples, even the plain CNN. Adversarial training, which is recommended in previous literatures, is much less effective improving the robustness against both adversarial and nonsense samples. It is probably due to our insufficient number of adversarial training rounds. So in Sec. 5.2, we train all CNNs with only the combination of random and mean training.\nWe show some examples of mReLU-r-m classifications in Fig. 10. The robust CNN gives correct predictions on even severe distorted samples. It does make incorrect predictions for samples bounded by color boxes, but the images are hard even for a human being. The case of green boxes is a bit different: the predictions are wrong, but it is sensible to classify these samples into the nonsense category."}, {"heading": "5.2. CIFAR-10", "text": "The plain CNN for CIFAR-10 is a deeper LeNet model; see Table 3 for its structure. There are 60,000 clean samples and we follow (Vedaldi, accessed in Feb 2016) to use 50,000 as the train set and 10,000 as the test set. The\nTable 1. Structures of the plain and robust CNNs for MNIST. Parameters of convolutional layers: cv1-(5, 5, 20), cv2-(5, 5, 50) (in the height-width-channel order). The number of hidden units in fully-connected layers are 500 (fc1) and 10 (fc2). max-max pooling. sloss-softmaxloss. hloss-hrbridloss.\nModels Layers #Layers Plain cv1 - max cv2 - max fc1 ReLU fc2 - sloss 8 RBF cv1 1-D RBF max cv2 1-D RBF max fc1 ReLU fc2 1-D RBF hloss 11 mReLU cv1 mReLU max cv2 mReLU max fc1 ReLU fc2 1-D RBF hloss 11\n0 0.1 0.2 0.3 0.4 0.5 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ncl as\nsi fic\nat io\nn ac\ncu ra\ncy\n\u03b2 0 0.1 0.2 0.3 0.4 0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ncl as\nsi fic\nat io\nn ac\ncu ra\ncy\n\u03b2 0 0.1 0.2 0.3 0.4 0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ncl as\nsi fic\nat io\nn ac\ncu ra\ncy\n\u03b2\nplain plain\u2212a plain\u2212r plain\u2212r\u2212m RBF RBF\u2212r RBF\u2212r\u2212m mReLU mReLU\u2212r mReLU\u2212r\u2212m\n(a) (b) (c)\nFigure 9. Accuracies of the CNNs on the MNIST test set. (a), (b) and (c) are the accuracies on adversarial, nonsense and noisy samples respectively. The horizontal axes are the perturbation strength \u03b2\u2019s.\ntraining set is augmented by randomly reflecting samples about their vertical mid-line. There are 10,000 groups of nonsense samples. The robust CNNs are trained with 90 epochs, twice of that for the plain CNN. We set \u03b11 = 1, \u03b12 = 1, \u03b13 = 1 9 for the hybrid loss functions of robust CNNs.\nWe present the accuracies and error rates in Fig. 11 and Table 4. Since the CIFAR-10 images are colored, the perturbation is much easier to perceive. Therefore we use perturbations of smaller magnitude than those for the MNIST. As with the MNIST, the robust CNNs are much more robust than the plain CNN. The best one, mReLU-r-m, makes marginally more errors than the plain CNN for clean samples, and noisy samples of up to \u03b2 = 0.015; but has greatly superior performance for noisy samples of large perturbation, all adversarial samples and all nonsense samples. However, the improvement on CIFAR-10 is not as large as that on the MNIST. The reason is that there are not sufficient training samples in the CIFAR-10 to train a perfect classifier, thus lots of samples are close to the decision boundaries and easy to perturb to be an adversary. The robustness of plain-r-m against nonsense samples is not as good as with the MNIST, and some curves in Fig. 11.b seem a bit odd. We leave it to the future work.\nInterestingly, the accuracy of mReLU CNN (without random and mean training) on clean samples is 0.194 which is marginally better than that of the plain CNN, 0.217. Al-\nthough we believe the plain CNN can perform better with advanced training tricks, we hypothesize that there is a connection between the robustness and generalization capability. Thus the CNN using mReLUs can generalize slightly better on the test set.\nWe show some examples of mReLU-r-m classifications in Fig. 12. The model makes correct predictions on most samples, but makes incorrect predictions for the samples in the red squares which are of the severest perturbation."}, {"heading": "6. Conclusions and Discussion", "text": "Our experiments show it is effective to use SAFs to improve the robustness of CNNs. Without substantial changes to the accuracies on clean samples, we obtain remarkably better robustness against adversarial, nonsense and noisy samples. That supports our proposal that SAFs can improve the robustness of CNNs by suppressing unusual signals.\nSince we change only the activation functions of CNNs, while leaving their structure, training strategies and optimization methods untouched, it implies there are no severe weaknesses in existing CNN frameworks. Nevertheless, in contrast with popular activation functions including sigmoid and ReLU, the SAFs have less documented support from neuroscience research. Furthermore, it remains a question how to accommodate SAFs better by fine-tuning the CNN structure and training strategies to get the better\nTable 3. Structures of the plain and robust CNNs for CIFAR-10. Parameters of convolutional layers: cv1-(5, 5, 32), cv2-(5, 5, 32), cv3(5, 5, 64) (in the height-width-channel order). The number of hidden units in fully-connected layers are 64 (fc1) and 10 (fc2). max-max pooling. avg-average pooling. sloss-softmaxloss. hloss-hrbridloss.\nModels Layers #Layers Plain cv1 - max cv2 - avg cv3 - avg fc1 ReLU fc2 - sloss 10 RBF cv1 1-D RBF max cv2 1-D RBF avg cv3 1-D RBF avg fc1 ReLU fc2 1-D RBF hloss 14 mReLU cv1 mReLU max cv2 mReLU avg cv3 mReLU avg fc1 ReLU fc2 1-D RBF hloss 14\n0 0.05 0.1 0.15 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\ncl as\nsi fic\nat io\nn ac\ncu ra\ncy\n\u03b2 0 0.05 0.1 0.15\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ncl as\nsi fic\nat io\nn ac\ncu ra\ncy\n\u03b2 0 0.05 0.1 0.15\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\ncl as\nsi fic\nat io\nn ac\ncu ra\ncy\n\u03b2\nplain plain\u2212r\u2212m RBF RBF\u2212r\u2212m mReLU mReLU\u2212r\u2212m\n(a) (b) (c)\nFigure 11. Accuracies of the CNNs on the CIFAR-10 test set. (a), (b) and (c) are the accuracies on adversarial, nonsense and noisy samples respectively. The horizontal axes are the perturbation strength \u03b2\u2019s.\nperformance. In that sense, we believe the attempt in this paper is just a start and far from the end."}], "references": [{"title": "Scalable training of l1-regularized log-linear models", "author": ["G. Andrew", "J. Gao"], "venue": "In ICML,", "citeRegEx": "Andrew and Gao,? \\Q2007\\E", "shortCiteRegEx": "Andrew and Gao", "year": 2007}, {"title": "Unsupervised clustering with spiking neurons by sparse temporal coding and multilayer rbf networks", "author": ["S.M. Bohte", "H.L. Poutr\u00e9", "J.N. Kok"], "venue": "IEEE TNN,", "citeRegEx": "Bohte et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bohte et al\\.", "year": 2002}, {"title": "Multivariable fuctional interpolation and adaptive networks", "author": ["D.S. Broomhead", "D. Lowe"], "venue": "Complex systems,", "citeRegEx": "Broomhead and Lowe,? \\Q1988\\E", "shortCiteRegEx": "Broomhead and Lowe", "year": 1988}, {"title": "Visual causal feature learning", "author": ["K. Chalupka", "P. Perona", "F. Eberhardt"], "venue": "In UAI,", "citeRegEx": "Chalupka et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chalupka et al\\.", "year": 2015}, {"title": "Multi-layer radial basis function networks. an extension to the radial basis function", "author": ["R.J. Craddock", "K. Warwick"], "venue": "In ICNN,", "citeRegEx": "Craddock and Warwick,? \\Q1996\\E", "shortCiteRegEx": "Craddock and Warwick", "year": 1996}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "In ICLR,", "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": "In ICLR workshop,", "citeRegEx": "Gu and Rigazio,? \\Q2015\\E", "shortCiteRegEx": "Gu and Rigazio", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Batch normalization: accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Adaptive mixtures of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural computations,", "citeRegEx": "Jacobs et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 1991}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Distributional smoothing by virtual adversarial examples", "author": ["T. Miyato", "S.I. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "In ICLR,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Fast learning in networks of locally-tuned processing units", "author": ["J. Moody", "C. Darken"], "venue": "Neural computation,", "citeRegEx": "Moody and Darken,? \\Q1989\\E", "shortCiteRegEx": "Moody and Darken", "year": 1989}, {"title": "Multi-layer radial basis function networks. an extension to the radial basis function", "author": ["I. Mr\u00e1zov\u00e1", "M. Kuka\u010dka"], "venue": "In ICNN,", "citeRegEx": "Mr\u00e1zov\u00e1 and Kuka\u010dka,? \\Q1996\\E", "shortCiteRegEx": "Mr\u00e1zov\u00e1 and Kuka\u010dka", "year": 1996}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton,? \\Q1996\\E", "shortCiteRegEx": "Nair and Hinton", "year": 1996}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "In CVPR,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Universal approximation using radial-basis-function networks", "author": ["J. Park", "I.W. Sandberg"], "venue": "Neural computation,", "citeRegEx": "Park and Sandberg,? \\Q1991\\E", "shortCiteRegEx": "Park and Sandberg", "year": 1991}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "In ICLR,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Adversarial manipulation of deep representations", "author": ["S. Sabour", "Y. Cao", "F. Faghri", "D.J. Fleet"], "venue": "In ICLR,", "citeRegEx": "Sabour et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sabour et al\\.", "year": 2016}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["J.T. Springenberg"], "venue": "In ICLR,", "citeRegEx": "Springenberg,? \\Q2016\\E", "shortCiteRegEx": "Springenberg", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In ICLR,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Matconvnet. http://www.vlfeat.org/matconvnet, accessed in Feb 2016", "author": ["A. Vedaldi"], "venue": null, "citeRegEx": "Vedaldi,? \\Q2016\\E", "shortCiteRegEx": "Vedaldi", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "Zeiler and Fergus,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Although deep CNNs have delivered state-of-the-art performance on several major computer vision challenges (Krizhevsky et al., 2012)(He et al.", "startOffset": 107, "endOffset": 132}, {"referenceID": 7, "context": ", 2012)(He et al., 2015), they have some pooly understood aspects.", "startOffset": 7, "endOffset": 24}, {"referenceID": 21, "context": "In particular, it was shown in (Szegedy et al., 2014) that one can easily construct an adversarial sample by imperceptible perturbation of any clean sample using the box-constrained limited-memory BFGS method (Andrew & Gao, 2007).", "startOffset": 31, "endOffset": 53}, {"referenceID": 5, "context": "In (Goodfellow et al., 2015), a simpler method, fast gradient sign (FGS), is proposed to compute adversarial samples effectively.", "startOffset": 3, "endOffset": 28}, {"referenceID": 19, "context": "Recently, a more complicated method is proposed to construct an adversarial sample with both the prediction and internal CNN features being similar to another arbitrary sample (Sabour et al., 2016).", "startOffset": 176, "endOffset": 197}, {"referenceID": 5, "context": "Nonsense samples (called rubbish samples in (Goodfellow et al., 2015)) are another challenge to the robustness of CNNs.", "startOffset": 44, "endOffset": 69}, {"referenceID": 5, "context": "These are generated from noise or arbitrary images using FGS-like methods (Goodfellow et al., 2015)(Nguyen et al.", "startOffset": 74, "endOffset": 99}, {"referenceID": 16, "context": ", 2015)(Nguyen et al., 2015).", "startOffset": 7, "endOffset": 28}, {"referenceID": 21, "context": "All these references argue that adversarial and nonsense samples arise from some unknown flaw of popular deep CNNs, either in their structure or training methods (Szegedy et al., 2014)(Goodfellow et al.", "startOffset": 162, "endOffset": 184}, {"referenceID": 5, "context": ", 2014)(Goodfellow et al., 2015)(Sabour et al.", "startOffset": 7, "endOffset": 32}, {"referenceID": 19, "context": ", 2015)(Sabour et al., 2016).", "startOffset": 7, "endOffset": 28}, {"referenceID": 21, "context": "A proposed solution to the problem is adversarial training (Szegedy et al., 2014)(Goodfellow et al.", "startOffset": 59, "endOffset": 81}, {"referenceID": 5, "context": ", 2014)(Goodfellow et al., 2015): adar X iv :1 60 3.", "startOffset": 7, "endOffset": 32}, {"referenceID": 5, "context": "Unfortunately, the final error rates on the newest adversarial samples, after this scheme, remain large (Goodfellow et al., 2015).", "startOffset": 104, "endOffset": 129}, {"referenceID": 12, "context": "It is interesting that adversarial training can be understood as a new regularization method for training CNNs; for example in (Miyato et al., 2016), virtual adversarial samples are constructed to force the model distributions of a supervised learning process to be smooth.", "startOffset": 127, "endOffset": 148}, {"referenceID": 18, "context": "Adversarial samples can also be used to establish new unsupervised and/or semi-supervised representation learning methods (Radford et al., 2016) (Springenberg, 2016) or autoencoders (Makhzani et al.", "startOffset": 122, "endOffset": 144}, {"referenceID": 20, "context": ", 2016) (Springenberg, 2016) or autoencoders (Makhzani et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 12, "context": "The idea is similar to (Miyato et al., 2016).", "startOffset": 23, "endOffset": 44}, {"referenceID": 3, "context": "Another interesting idea, proposed in (Chalupka et al., 2015), is to identify features that are causally related, rather than merely correlated, with the categories.", "startOffset": 38, "endOffset": 61}, {"referenceID": 9, "context": "Unlike CNNs and linear classification models, local models (Moody & Darken, 1989)(Jacobs et al., 1991) based on radial basis functions (RBF) (Broomhead & Lowe, 1988) are intrinsically immune to abnormal samples which are far from high-density regions: they will always output low confidence scores for such.", "startOffset": 81, "endOffset": 102}, {"referenceID": 9, "context": "as a hidden layer in mixture of experts (Jacobs et al., 1991), and as the top layer units in LeNet-5 (LeCun et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 11, "context": ", 1991), and as the top layer units in LeNet-5 (LeCun et al., 1998).", "startOffset": 47, "endOffset": 67}, {"referenceID": 1, "context": "Only a few implementations attempt multiple RBF layers (Craddock & Warwick, 1996)(Bohte et al., 2002)(Mr\u00e1zov\u00e1 & Kuka\u010dka, 1996).", "startOffset": 81, "endOffset": 101}, {"referenceID": 5, "context": "This high-dimensionality makes it hard to train deep networks involving RBF units to achieve acceptable accuracies (Goodfellow et al., 2015).", "startOffset": 115, "endOffset": 140}, {"referenceID": 5, "context": "Some have argued that more powerful optimization methods are needed to make this work (Goodfellow et al., 2015).", "startOffset": 86, "endOffset": 111}, {"referenceID": 11, "context": "To assess this, we run a LeNet-5 model (LeCun et al., 1998) (see Sec.", "startOffset": 39, "endOffset": 59}, {"referenceID": 5, "context": "It is suggested in (Goodfellow et al., 2015) that randomly perturbed samples are of little significance.", "startOffset": 19, "endOffset": 44}, {"referenceID": 11, "context": "We use two well-studied datasets, MNIST (LeCun et al., 1998) and CIFAR-10 (CIFAR, accessed in Feb 2016), in our experiments.", "startOffset": 40, "endOffset": 60}, {"referenceID": 5, "context": "We follow (Goodfellow et al., 2015) to generate adversarial samples and nonsense samples by:", "startOffset": 10, "endOffset": 35}, {"referenceID": 11, "context": "MNIST We use LeNet-5 (LeCun et al., 1998) as the plain CNN for MNIST, please see Table 1 for its structure.", "startOffset": 21, "endOffset": 41}, {"referenceID": 5, "context": ", 2013) trained using adversarial training (Goodfellow et al., 2015).", "startOffset": 43, "endOffset": 68}], "year": 2016, "abstractText": "Many deep Convolutional Neural Networks (CNN) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples. We hypothesize that this is caused by a failure to suppress unusual signals within network layers. As remedy we propose the use of Symmetric Activation Functions (SAF) in non-linear signal transducer units. These units suppress signals of exceptional magnitude. We prove that SAF networks can perform classification tasks to arbitrary precision in a simplified situation. In practice, rather than use SAFs alone, we add them into CNNs to improve their robustness. The modified CNNs can be easily trained using popular strategies with the moderate training load. Our experiments on MNIST and CIFAR-10 show that the modified CNNs perform similarly to plain ones on clean samples, and are remarkably more robust against adversarial and nonsense samples.", "creator": "LaTeX with hyperref package"}}}