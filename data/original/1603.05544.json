{"id": "1603.05544", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent", "abstract": "SGD is the widely adopted method to train CNN. Conceptually it approximates the population with a randomly sampled batch; then it evenly trains batches by conducting a gradient update on every batch in an epoch. In this paper, we demonstrate Sampling Bias, Intrinsic Image Difference and Fixed Cycle Pseudo Random Sampling differentiate batches in training, which then affect learning speeds on them. Because of this, the unbiased treatment of batches involved in SGD creates improper load balancing. To address this issue, we present Inconsistent Stochastic Gradient Descent (ISGD) to dynamically vary training effort according to learning statuses on batches. Specifically ISGD leverages techniques in Statistical Process Control to identify a undertrained batch. Once a batch is undertrained, ISGD solves a new subproblem, a chasing logic plus a conservative constraint, to accelerate the training on the batch while avoid drastic parameter changes. Extensive experiments on a variety of datasets demonstrate ISGD converges faster than SGD. In training AlexNet, ISGD is 21.05\\% faster than SGD to reach 56\\% top1 accuracy under the exactly same experiment setup. We also extend ISGD to work on multiGPU or heterogeneous distributed system based on data parallelism, enabling the batch size to be the key to scalability. Then we present the study of ISGD batch size to the learning rate, parallelism, synchronization cost, system saturation and scalability. We conclude the optimal ISGD batch size is machine dependent. Various experiments on a multiGPU system validate our claim. In particular, ISGD trains AlexNet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4 NVIDIA TITAN X at the batch size of 1536.", "histories": [["v1", "Thu, 17 Mar 2016 15:49:48 GMT  (733kb,D)", "https://arxiv.org/abs/1603.05544v1", "The patent of ISGD belongs to NEC Labs"], ["v2", "Fri, 18 Mar 2016 05:35:22 GMT  (593kb,D)", "http://arxiv.org/abs/1603.05544v2", "The patent of ISGD belongs to NEC Labs"], ["v3", "Tue, 28 Mar 2017 13:56:03 GMT  (1052kb,D)", "http://arxiv.org/abs/1603.05544v3", "The patent of ISGD belongs to NEC Labs"]], "COMMENTS": "The patent of ISGD belongs to NEC Labs", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["linnan wang", "yi yang", "martin renqiang min", "srimat chakradhar"], "accepted": false, "id": "1603.05544"}, "pdf": {"name": "1603.05544.pdf", "metadata": {"source": "CRF", "title": "Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent", "authors": ["Linnan Wanga", "Yi Yangb", "Renqiang Minb", "Srimat Chakradharb"], "emails": ["linnan.wang@gatech.edu"], "sections": [{"heading": null, "text": "Stochastic Gradient Descent (SGD) updates network parameters with a noisy gradient computed from a random batch, and each batch evenly updates the network once in an epoch. This model applies the same training effort to each batch, but it overlooks the fact that the gradient variance, induced by Sampling Bias and Intrinsic Image Difference, renders different training dynamics on batches. In this paper, we develop a new training strategy for SGD, referred to as Inconsistent Stochastic Gradient Descent (ISGD) to address this problem. The core concept of ISGD is the inconsistent training, which dynamically adjusts the training effort w.r.t the loss. ISGD models the training as a stochastic process that gradually reduces down the mean of batch\u2019s loss, and it utilizes a dynamic upper control limit to identify a large loss batch on the fly. Then, it solves a new subproblem on the identified batch to accelerate the training while avoiding drastic parameter changes. ISGD is straightforward, computationally efficient and without requiring auxiliary memories. A series of empirical evaluations on real world datasets and networks demonstrate the promising performance of inconsistent training.\nKeywords: Neural Networks, Stochastic Gradient Descent, Statistical Process Control"}, {"heading": "1. Introduction", "text": "The accessible TFLOPs brought forth by accelerator technologies bolster the booming development in Neural Networks. In particular, large scale neural networks have drastically improved various systems in natural language processing [1], video motion analysis [2], and recommender systems [3]. However, training a large neural network saturated with nonlinearity is notoriously difficult. For example, it takes 10000 CPU cores up to days to complete the training of a network with 1 billion parameters [4]. Such computational challenges have manifested the importance of improving the efficiency of gradient based training algorithm.\nThe network training is an optimization problem that searches for optimal parameters to approximate the intended function defined over a finite training set. A notable aspect of training is the vast solution hyperspace defined by abundant network parameters. The recent ImageNet contests have seen the parameter size of Convolutional Neural Networks (CNN) increase to n \u223c 109. Solving an optimization problem at this scale is prohibitive to the second order optimization methods, as the required Hessian matrix, of size 109 \u00d7 109, is too large to be tackled by modern computer architectures. Therefore, the first order gradient descent is widely used in training the large scale neural networks.\nThe standard first order full Gradient Descent (GD), which dates back to [5], calculates the gradient with the whole dataset. Despite the appealing linear convergence rate of full gradient\nEmail address: linnan.wang@gatech.edu (Linnan Wang)\ndescent (O(\u03c1k), \u03c1 < 1) [6], the computation in an iteration linearly increases with the size of dataset. This makes the method unsuitable for neural networks trained with the sheer volume of labelled data. To address this issue, Stochastic Gradient Descent [7, 8] was proposed by observing a large amount of redundancy among training examples. It approximates the dataset with a batch of random samples, and uses the stochastic gradient computed from the batch to update the model. Although the convergence rate of SGD, O(1/ \u221a bk + 1/k) [9] where b is the batch size, is slower than GD, SGD updates the model much faster than GD in a period, i.e. larger k. As a result, the faster convergence is observable on SGD compared to GD in practice. SGD hits a sweet spot between the good system utilization [10] and the fast gradient updates. Therefore, it soon becomes a popular and effective method to train large scale neural networks.\nThe key operation in SGD is to draw a random batch from the dataset. It is simple in math, while none-trivial to be implemented on a large-scale dataset such as ImageNet [11]. State of the art engineering approximation is the Fixed Cycle Pseudo Random (FCPR) sampling (defined in section 3.4), which retrieves batches from the pre-permuted dataset like a ring, e.g. d0 \u2192 d1 \u2192 d2 \u2192 d0 \u2192 d1 \u2192 ... , where di denotes a batch. In this case, each batch receives the same training iterations as a batch updates the network exactly once in an epoch. Please note this engineering simplification allows batches to repetitively flow into the network, which is different from the random sampling in Statistics. However, it is known that the gradient variances differentiate batches in the training [12], and gradient updates from the large loss batch contribute more than the small\nPreprint submitted to Neural Networks March 29, 2017\nar X\niv :1\n60 3.\n05 54\n4v 3\n[ cs\n.L G\n] 2\n8 M\nar 2\n01 7\nloss ones [13]. This suggests that rebalancing the training effort across batches is necessary. SGD fails to consider the issue, and we think this is a problem.\nIn this paper, we propose Inconsistent Stochastic Gradient Descent (ISGD) to rebalance the training effort among batches. The inconsistency is reflected by the uneven gradient updates on batches. ISGD measures the training status of a batch by the associated loss. At any iteration t, ISGD traces the losses in iterations [t \u2212 nb, t], where nb is the number of distinct batches in a dataset. These losses assist in constructing a dynamic upper threshold to identify a under-trained batch during the training. If a batch\u2019s loss exceeds the threshold, ISGD accelerates the training on the batch by solving a new subproblem that minimizes the discrepancy between the loss of current batch and the mean. The subproblem also contains a conservative constraint to avoid overshooting by bounding the parameter change. The key idea of the subproblem is to allow additional gradient updates on a under-trained batch while still remaining the proximity to the current network parameters. Empirical experiments demonstrate ISGD, especially at the final stage, performs much better than the baseline method SGD on various mainstream datasets and networks.\nFor practical considerations, we also delve into the effect of batch size toward the convergence rate with system factors considered. Enlarging the batch size expedites the convergence [14], but it linearly adds computations in an iteration. In the scenario of single node training, a small batch is favored to ensure frequent gradient updates. In the scenario of the multinode training, it entails heavy synchronizations among nodes per iteration. The more gradient updates, the higher synchronization cost is. In this case, a moderate large batch reduces overall communications [15], and it also improves the system saturation and the available parallelism.\nIn summary, the novelties of this work are: \u2022 we propose a new training model, referred to as the incon-\nsistent training, to improve the efficiency of SGD. \u2022 we apply the inconsistent training on SGD and its variants."}, {"heading": "2. Related Work", "text": "A variety of approaches have been proposed to improve vanilla SGD for the neural network training. In this section, we demonstrate the concept of inconsistent training is fundamentally different from the existing methods.\nThe stochastic sampling in SGD introduces the gradient variance, which slows down the convergence rate [9]. The problem motivates researchers to apply various variance reduction techniques on SGD to improve the convergence rate. Stochastic Variance Reduced Gradient (SVRG) [12] keeps network historical parameters and gradients to explicitly reduce the variance of update rule, but the authors indicate SVRG only works well for the fine-tuning of non-convex neural network. Chong et al. [16] explore the control variates on SGD, while Zhao and Tong [17] explore the importance sampling. These variance reduction techniques, however, are rarely used in the large scale neural networks, as they consume the huge RAM space to store\nthe intermediate variables. ISGD adjusts to the negative effect of gradient variances, and it does not construct auxiliary variables being much more memory efficient and practical than the variance reduction ones.\nMomentum [18] is a widely recognized heuristic to boost SGD. SGD oscillates across the narrow ravine as the gradient always points to the other side instead of along the ravine toward the optimal. As a result, it tends to bounce around leading to the slow convergence. Momentum damps oscillations in directions of high curvature by combining gradients with opposite signs, and it builds up speed toward a direction that is consistent with the previously accumulated gradients [19]. The update rule of Nesterov\u2019s accelerated gradient is similar to Momentum [20], but the minor different update mechanism for building the velocity results in important behavior differences. Momentum strikes in the direction of the accumulated gradient plus the current gradient. In contrast, Nesterov\u2019s accelerated gradient strikes along the previous accumulated gradient, then it measures the gradient before making a correction. This prevents the update from descending fast, thereby increases the responsiveness. ISGD is fundamentally different from these approaches by considering the training dynamics on batches. ISGD rebalances the training effort across batches, while Momentum and Nesterov\u2019s accelerated gradient leverage the curvature tricks. Therefore, the inconsistent training is expected to be compatible with both methods.\nAdagrad [21] adapts the learning rate to the parameters, performing larger updates for infrequent parameters, and smaller updates for frequent parameters. It accumulates the squared gradients in the denominator, which will drastically shrink the learning rate. Subsequently, RMSprop and Adadelta have been developed to resolve the issue. These adaptive learning rate approaches adjust the extent of parameter updates w.r.t the parameter\u2019s update frequency to increase the robustness of training, while ISGD adjusts the frequency of a batch\u2019s gradient updates w.r.t the loss to improve the training efficiency. From this perspective, ISGD is different from the adaptive learning rate approaches.\nThe core concept of inconsistent training is to spare more training effort on the large loss batches than the small loss ones. The rational behind the scene is that gradient updates from the small loss batches contribute less than the large loss ones. Simo-Serra et al. [13] adopt a similar idea in training the Siamese network to learn the deep descriptors by intentionally feeding the network with hard training pairs, i.e. pairs yield large losses, and the method is proven to be an effective way to improve the performance. They manually pick the hard pairs to feed the network, while ISGD automatically identifies the hard batch during the training. In addition, the mechanism of ISGD\u2019s hard batch acceleration is different from the SimoSerra\u2019s method. ISGD solves a sub-optimization problem on the hard batch to reduce the batch\u2019s loss and avoids drastic parameter changes, while the Simo-Serra\u2019s method simply feeds the batch more often. Please note it is important to bound the parameter changes, because overshooting a batch leads to the divergence on other batches. In summary, ISGD is the first neural network solver to consider the batch-wise training dy-\nnamics, and it has demonstrated promising performance on a variety of real world datasets and models."}, {"heading": "3. Problem Statement", "text": "This section demonstrates the non-uniform batch-wise training dynamics. Theoretically, we prove the contribution of gradient updates varies among batches based on the analysis of SGD\u2019s convergence rate. We also hypothesize that Intrinsic Image Differences and Sampling Bias are high level factors to the phenomenon, and the hypothesis is verified by two controlled experiments. Both theories and experiments support our conclusion that the contribution of a batch\u2019s gradient update is different.\nThen we demonstrate the Fixed Cycle Pseudo Random sampling employed by SGD is inefficient to handle this issue. In particular, the consistent gradient updates on all batches, regardless of their statuses, is wasteful especially at the end of training, and the gradient updates on the small loss batch could have been used to accelerate large loss batches."}, {"heading": "3.1. A Recap of CNN Training", "text": "We formulate the CNN training as the following optimization problem. Let \u03c8 be a loss function with weight vector w as function parameters, which takes a batch of images d as the input. The objective of CNN training is to find a solution to the following optimization problem:\nmin w \u03c8w(d) + 1 2 \u03bb \u2016 w\u201622 (1)\nThe second term is Weight Decay [22], and \u03bb is a parameter to adjust its contribution (normally around 10\u22124). The purpose of Weight Decay is to penalize the large parameters so that static noise and irrelevant components of weight vectors get suppressed. [22].\nA typical training iteration of CNN consists of a Forward and Backward pass. Forward pass yields a loss that measures the discrepancy between the current predictions and the truth. Backward pass calculates the gradient, the negative of which points to the steepest descent direction. Gradient Descent updates the w as follows:\nwt = wt\u22121 \u2212 \u03b7t\u2207\u03c8w(d) (2)\nWhereas evaluating the gradient over the entire dataset is extremely expensive especially for large datasets such as ImageNet. To resolve this issue, mini-batched SGD is proposed to approximate the entire dataset with a small randomly drawn sample dt. The upside of mini-batched SGD is the efficiency of evaluating a small sample in the gradient calculation, while the downside is the stochastic gradient slowing down the convergence. Let\u2019s define a sample space \u2126. If \u03c8w(dt) is a random variable defined on a probability space (\u2126,\u03a3, P), the new objective function is\nmin w\nE{\u03c8w(dt)} = \u222b\n\u2126\n\u03c8w(dt)dP + 1 2 \u03bb \u2016 w\u201622 (3)\nthe update rule changes to\nwt = wt\u22121 \u2212 \u03b7t\u2207\u03c8w(dt\u22121) (4)\nand the following holds,\nE{\u2207\u03c8w(dt)} = \u2207\u03c8w(d) (5)"}, {"heading": "3.2. Measure Training Status with Cross Entropy Error", "text": "We use the loss to reflect the training status of a batch. A convolutional neural network is a function of Rn \u2192 R, the last layer of which is a softmax loss function calculating the cross entropy between the true prediction probabilities p(x) and the estimated prediction probabilities p\u0302(x). The definition of softmax loss function of a batch at iteration t is\n\u03c8wt (dt) = \u2212 nb\u2211 i \u2211 x p(x) log p\u0302(x) + 1 2 \u03bb \u2016 w\u201622 (6)\nwhere nb is the number of images in a batch, and \u03bb regulates Weight Decay. Since Weight Decay is applied, the loss of a batch fluctuates around a small number after being fully trained.\nThe loss produced by the cross entropy is a reliable indicator of a batch\u2019s training status. Given a batch dt, the cross entropy \u03c8wt (dt) measures the discrepancy between the estimated probabilities and the truth. In the image classification task, the truth p(x) is a normalized possibility vector, containing most zeros with only one scalar set to 1. The index of the vector corresponds to an object category. For example, p(x) = [0, 0, 1, 0, 0] indicates the object belongs the category 2 (index starts from 0). The neural network produces an normalized estimate possibility p\u0302(x), and the loss function only captures the extent of making the correct prediction as the zeros in p(x) offset the incorrect predictions in p\u0302(x). If p\u0302(x) is close to p(x), the loss function yields a small value. If p\u0302(x) is far from p(x), the loss function yields a large value. Therefore, we use the loss of a batch to assess the model\u2019s training status on it. Intuitively a large loss indicates that most predictions made by the network on the batch are false, and the additional training on the batch is necessary."}, {"heading": "3.3. Motivation: Non-uniform Training Dynamics of Batches", "text": "The gradient variance is the source of batch-wise training variations. The benefit of using a random sample to approximate the population is the significantly less computations in an iteration, while the downside is the noisy gradient. Please note the convergence rate in this section is measured by iterations. To analyze the training dynamics per iteration, we need define the Lyapunov process:\nht =\u2016 wt \u2212 w\u2217 \u201622 (7)\nThe equation measures the distance between the current solution wt and the optimal solution w\u2217. ht is a random variable. Hence the convergence rate of SGD can be derived using Eq.4\nand Eq.7:\nht+1 \u2212 ht =\u2016 wt+1 \u2212 w\u2217 \u201622 \u2212 \u2016 wt \u2212 w\u2217 \u201622 = (wt+1 + wt \u2212 2w\u2217)(wt+1 \u2212 wt) = (2wt \u2212 2w\u2217 \u2212 \u03b7t\u2207\u03c8w(dt))(\u2212\u03b7t\u2207\u03c8w(dt)) = \u22122\u03b7t(wt \u2212 w\u2217)\u2207\u03c8w(dt) + \u03b72t (\u2207\u03c8w(dt))2 (8)\ndt is a random sample of d in the sample space \u2126, and ht+1 \u2212 ht is a random variable that depends on the drawn sample dt and learning rate \u03b7t. It suggests how far an iteration step toward w\u2217. This equation demonstrates two important insights:\n\u2022 Reducing VAR{\u2207\u03c8w(dt)} improves the convergence rate. The expectation of Eq.8 yields the average convergence rate at the precision of an iteration.\nE{ht+1 \u2212 ht} = \u22122\u03b7t(wt \u2212 w\u2217)E{\u2207\u03c8w(dt)} + \u03b72t E{(\u2207\u03c8w(dt))2} = \u22122\u03b7t(wt \u2212 w\u2217)E{\u2207\u03c8w(dt)} + \u03b72t (E{\u2207\u03c8w(dt)})2 + VAR{\u2207\u03c8w(dt)} (9)\nTo simplify the analysis of Eq.9, let\u2019s assume the convexity on \u03c8w(dt) implying that\nht+1 \u2212 ht < 0 (10)\n\u2212 (wt \u2212 w\u2217)E{\u2207\u03c8w(dt)} < 0. (11)\nE{\u2207\u03c8w(dt)} is the unbiased estimate of E{\u2207\u03c8w(d)}. Therefore, maximizing the contribution of an iteration is reduced to the minimization of VAR{\u2207\u03c8w(dt)}. This direction has been well addressed [23, 24].\n\u2022 The contribution of an iteration, ht+1 \u2212 ht, varies with respect to dt. According to Eq.8, the variance of ht+1 \u2212 ht is:\nVAR{ht+1 \u2212 ht} = 4\u03b72t (wt \u2212 w\u2217)2VAR{\u2207\u03c8w(dt)} +\u03b74t VAR{(\u2207\u03c8w(dt))2}\n\u22122\u03b73t (wt \u2212 w\u2217)COV{\u2207\u03c8w(dt),\u2207\u03c8w(dt)2} (12)\nThe equation demonstrates VAR{ht+1 \u2212 ht} , 0, which implies the contribution of gradient updates is non-uniform. It is interesting to notice that the determining factors in this equation, \u2207\u03c8w(dt)2 and \u2207\u03c8w(dt), is contingent upon dt, suggesting a correlation between ht+1 \u2212 ht and dt. This unique insight motivates us to understand what factors in dt affect the convergence rate ht+1 \u2212 ht, and how to address the load balancing problem in the training. Although there are extensive studies toward the variance reduction on \u2207\u03c8w(dt), few explores this direction. Let\u2019s use the loss of a batch to measure the model\u2019s training status on it (explained in section 3.2). Fig.1 demonstrates the loss traces of 10 separate batches during the training. It is observable that the losses of batches degenerate at different rates. Therefore, the empirical observations and Eq.12 prompt us to conclude that\nthe contribution of a batch\u2019s gradient update is non-uniform\nThis also explains the distinctive training dynamics of batches in Fig.1. Eq.12 suggests dt is critical for the claim. We conduct a set of empirical evaluations to understand how dt affect VAR{ht+1 \u2212 ht}, and we propose two high level factors, Sampling Bias and Intrinsic Image Difference, to explain the phenomenon. The definitions of these\ntwo terms are as follows: Sampling Bias: It is a bias in which a sample is collected in such a way that some members of the intended population are less likely to be included than others. Intrinsic Image Difference: Intrinsic Image Difference indicates images from the same subpopulation are also different at pixels. For example, the category \u2019cat\u2019 can contain some white cat pictures or black cat pictures. Though black cat and white cat belong to the cat subpopulation, they are still different at pixels.\nSampling Bias is the first factor to explain the training variations on batches. We consider two kinds of Sampling Bias. First, existing datasets, such as Places [25] or ImageNet, contain uneven number of images in each category. As a result, the dominate sub-population is more likely to be selected in a batch than others. Second, the insufficient shuffling on the dataset may lead to clusters of subpopulations. When SGD sequentially draws images from the insufficient permuted dataset to form a randomized batch (explained in section.3.4), one subpopulation is more likely to be included than others. In both cases, they conform to the definition of Sampling Bias. For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest. To support the claim, we synthesized 10 single-class batches randomly drawn from an exclusive image category in CIFAR-10 [26]. Please note CIFAR-10 contains 10 independent image categories. Each batch represents a unique CIFAR-10 category, and they are highly polluted with Sampling Bias as each batch only contains one subpopulation. Fig.1a demonstrates the loss traces of ten single-class batches. It is obvious to see the losses of ten batches degrade independently. In particular, gradient updates from the yellow batch is more effective than the purple batch. Therefore, these results justify our claim about Sampling Bias and the batch-wise training variation.\nIntrinsic Image Difference is the second factor to explain the training variations on batches. To substantiate this point, we conduct a controlled experiment on 10 independent identically distributed batches. A batch includes 1000 images, and each batch contains 100 randomly drawn images from category 0, 100 images from category 1, ... , 100 images from category 9. This sequence is fixed across batches to eliminate the potential ordering influence. In this case, each batch contains the same number of images from 10 subpopulations in CIFAR-10 and the only difference among them is the pixels. Hence, we consider these batches independent identically distributed. The network is same as the one used in Sampling Bias. Fig.1b demonstrates the loss traces of 10 i.i.d batches. Although a strong correlation persists through the training, it is still clear the losses of i.i.d batches degrade at separate rates. Particularly, the loss of batch 4 (green) is around 0.5 while batch 3 (purple) is around 1.3 at the epoch 400. Please note these batches are i.i.d, and they are supposed to be approximately identical to the original dataset. However, the training variations still exist indicating the nonuniform contribution of gradient updates from each batches."}, {"heading": "3.4. Problems of Consistent Training in SGD", "text": "SGD relies on a key operation, uniformly drawing a batch from the entire dataset. It is simple in math but nontrivial in the system implementation. ImageNet, ILSVRC2012 for example, contains 1431167 256\u00d7256 high resolution RGB images accounting for approximately 256 GB in total size. Uniformly drawing a random batch from the 256 GB binary file involves significant overhead such as TLB misses or random Disk I/O operations. In addition, the drastic speed gap between Processor and Disk further deteriorates the issue. Existing deep learning frameworks, such as Caffe [27] or Torch [28], alleviates the issue by pre-permuting the entire dataset before slicing into batches: Permute{d} \u2192 d = {d0,d1, ...,dn\u22121,dn} = \u2126. During the training, each iteration fetches a batch from the permuted dataset \u2126 in a sequential manner d0 \u2192 d1 \u2192 ... \u2192 dn; and restart fetching from the beginning d0 after dn, creating a fixed circle batch retrieval pattern. We refer to this sampling method as Fixed Circle Pseudo Random Sampling. The random reads are subsequently reduced to sequential reads on Disk. Therefore, FCPR Sampling is widely adopted by SGD. Let nd to be the size of a dataset and nb to be the batch size. The size of sample space is nd/ nb, and the batch being assigned to iteration j is dt, where\nt = j mod \u2217 nd nb\nAt any iteration, the model always anticipate a fixed batch, as the batch will flow into the model at iteration t+1 \u00b7epoch, ..., t+ n \u00b7 epcoh. If the training of a batch is dominated by the gradient update on itself, the loss of this batch is predominately reduced at iteration t, t + 1 \u2217 epoch, t + 2 \u2217 epoch, ..., t + n \u2217 epoch. Since the contribution from a batch\u2019s gradient update is different, the repetitive batch retrieval pattern fosters the batches\u2019 distinctive training speeds. However, the FCPR sampling in SGD treats batches identically.\nThe problem of FCPR sampling is the consistent gradient updates on batches regardless of the model\u2019s training status. It is inefficient to update a small loss batch as frequently as a large loss batch. Fig.1b demonstrates the yellow batch are fully trained after the epoch 1600, while the blue batch does not until the epoch 2100. During epochs [1600, 2100], the yellow batch stays fully trained most of time indicating unnecessary training iterations on it. Besides, we also verify that the contribution of a batch\u2019s gradient update is different. Therefore, regulating the training iterations w.r.t the model\u2019s training status on batches will improve the efficiency of SGD."}, {"heading": "4. Inconsistent Stochastic Gradient Descent", "text": "In this section, we present Inconsistent Stochastic Gradient Descent that rebalances the training effort w.r.t a batch\u2019s training status. The inconsistency is reflected by the non-uniform gradient updates on batches. The first question is how to dynamically identify a slow or under-trained batch during the training. We model the training as a stochastic process, and apply the upper control limit to dynamically identify a undertrained batch. The second question is how to accelerate a undertrained batch. We propose a new optimization to be solved on\nthe batch, the objective of which is to accelerate the training without drastic parameters changes. For practical considerations, we also study the effects of ISGD batch size on the convergence rate, system saturations and synchronization cost."}, {"heading": "4.1. Identifying Under-trained Batch", "text": "ISGD models the training as a stochastic process that slowly reduces down the average loss of batches. We assume the normal distribution on the batch\u2019s loss in an epoch. The reasons are that: 1) SGD demands a small learning rate (lr) [29] to converge, and lr is usually less than 10\u22121. lr determines the step length, while the normalized gradient determines the step direction. The small value of lr limits the contribution made by a gradient update, thus the training process is to gradually reduce down the loss toward a local optimal. 2) Each batch represents the original dataset, and there exists a strong correlation among batches in the training. This implies that the loss of a batch will not be drastically different from the average at any iteration t. Fig.2 demonstrates the loss distribution of training a network on CIFAR-10, in which the losses are arranged by epochs. From the figure, it is valid to assume the normal distribution on the loss in an epoch. Therefore, we conclude that\nthe training is a stochastic process that slowly decreases the mean of losses tile the network converges.\nThe 3\u03c3 control limit is an effective method to monitor the abnormalities in a statistical process. Since we treat the training as a process that decreases the average loss of batches, ISGD utilizes the upper control limit to dynamically identify abnormal large-loss batches on the fly. To get the limit, ISGD calculates two important descriptive statistics, the running average loss \u03c8\u0304 and the running standard deviation \u03c3\u03c8 during the training. ISGD keeps a queue to store the losses produced by iterations in [t \u2212 nb, t], where nb is the size of sample space (or the number of batches in an epoch). The queue functions as a moving window tracking the loss information in the previous epoch to yield \u03c8\u0304 and \u03c3\u03c8.\n\u03c8\u0304 = 1 nb nb\u2211 i=1 \u03c8wt\u2212i (dt\u2212i) (13)\n\u03c3\u03c8 = \u221a 1 nb nb\u2211 i=1 [\u03c8wt\u2212i (dt\u2212i) \u2212 \u03c8\u0304]2 (14)\nSince the queue length is fixed at nb and the loss of a batch is a float number, the calculations of \u03c8\u0304 and \u03c3\u03c8 and the memory cost for the queue are O(1) at any iteration t. Therefore, ISGD is much more memory efficient than the variance reduction approaches that require intermediate variables of the same size as the network parameters. With \u03c8\u0304 and \u03c3\u03c8, the upper control limit is\nlimit = \u03c8\u0304 + 3\u03c3\u03c8 (15)\nIn this case, we adopt the 3\u03c3 control limit. The multiplier before the \u03c3\u03c8 plays an important role in between the exploration of new batches and the exploitation of the current batch. Please refer to the discussion of Alg.1 in section 9 for more discussion. If the loss of current iteration t is\n\u03c8wt\u22121 (dt) > limit (16)\nwe consider dt as a under-trained batch. Fig.3 demonstrates an example of the proposed method to identify a under-trained batch on the fly. The blue line is the loss of batch, and the yellow line is the running average \u03c8\u0304. The green line is the upper control limit, and the red dots are outliers considered as under-trained. The experiment is conducted with AlexNet on ImageNet, and it is clear that ISGD successfully identifies the large-loss batches in the training with the proposed approach."}, {"heading": "4.2. Inconsistent Training", "text": "The core concept of our training model is to spend more iterations on the large-loss batches than the small-loss ones. The batch retrieval pattern in ISGD is similar to FCPR sampling but with the following important difference. Once a batch is identified as under-trained, ISGD stays on the batch to solve a new sub-optimization problem to accelerate the training, and the batch receives additional training iterations inside the subproblem. In this case, ISGD does not compromise the system efficiency of FCPR sampling, while it still regulates the training effort across the batches. The new subproblem is\nAlgorithm 1: Inconsistent Stochastic Gradient Descent Data: d , w Result: w\u2032\n1 begin 2 iter = 0 3 n = ntrainingnbatch 4 \u03c8 = 0 5 \u03c3\u03c8 = 0 6 limit = 0 7 lossqueue \u2190 \u2205 8 while not converge do 9 broadcast(w)\n10 [\u03c8,\u2207\u03c8] = ForwardBackward(dt) 11 reduce(\u03c8) 12 reduce(\u2207\u03c8) 13 if iter < n then 14 lossqueue.push(\u03c8) 15 \u03c8 = \u03c8\u00b7iter+\u03c8iter+1 16 else 17 l = lossqueue.dequeue() 18 \u03c3\u03c8 = S T D(lossqueue) 19 \u03c8 = \u03c8\u00b7n\u2212l+\u03c8n 20 limit = \u03c8 + 3 \u2217 \u03c3\u03c8 21 w = w \u2212 \u03b7 \u00b7 \u2207\u03c8 22 if \u03c8 > limit and iter > n then 23 minimize Eq.17 with Alg.2\n24 iter++\nmin w \u03c6w(dt) = 1 2 \u2016 \u03c8w(dt) \u2212 limit\u201622\n+ 2nw \u2016 w \u2212 wt\u22121\u201622\n(17)\nwhere nw is the number of weight parameters in the network and is a parameter for the second term. The first term minimizes the difference between the loss of current under-trained batch dt and the control limit. This is to achieve the acceleration effect. The second term is a conservative constraint to avoid drastic parameter changes. Please note the second term is critical because overshooting a batch negatively affects the training on other batches. The parameter adjusts the conservative constrain, and it is recommended to be 10\u22121. The derivative of Eq.17 is:\n\u2207\u03c6w(dt) =[\u03c8w(dt) \u2212 limit]\u2207\u03c8w(dt)\n+ (w \u2212 wt\u22121)\nnw\n(18)\nPlease note limit, wt\u22121 and dt are constants. Solving Eq.17 precisely incurs the significant computation and communication overhead, which offsets the benefit of it. In practice, we approximate the solution to the new subproblem, Eq.17, with the\nAlgorithm 2: Solving the conservative subproblem to accelerate a under-trained batch\nData: dt, w, wt\u22121, stop, limit Result: w\n1 begin 2 iter = 0 3 while iter < stop and \u03c8 > limit do 4 [\u03c8,\u2207\u03c8] = ForwardBackward(dt) 5 reduce(\u03c8) 6 reduce(\u2207\u03c8) 7 w = w \u2212 \u03b6{[\u03c8w(dt) \u2212 limit]\u2207\u03c8w(dt) + (w\u2212wt\u22121)nw } 8 broadcast(w) 9 iter++\nearly stopping. This avoids the huge searching time wasted on hovering around the optimal solution. A few iterations, 5 for example, are good enough to achieve the acceleration effects. Therefore, we recommend approximating the solution by the early stopping.\nAlg.1 demonstrates the basic procedures of ISGD. Since the training status of a batch is measured by the loss, ISGD identifies a batch as under-trained if the loss is larger than the control limit \u03c8+3\u2217\u03c3\u03c8 (Line 20). A stringent limit triggers Eq.17 more frequently. This increases exploitation of a batch, but it also decreases the exploration of batches to a network in the fixed time. Therefore, a tight limit is also not desired. A soft margin, 2 or 3 \u03c3\u03c8, is preferred in practice; and this margin is also widely applied in Statistical Process Control to detect abnormalities in a process. We recommend users adjusting the margin according to the specific problem. ISGD adopts a loss queue to dynamically track the losses in an epoch so that the average loss, \u03c8, is calculated in O(1) (line 17). The loss queue tracks iterations in the previous epoch; the length of it equals to the length of an epoch. Similiarly, calculating \u03c3\u03c8 is also in O(1) time (line 18). We do not initiate Alg.2 until the first epoch to build up a reliable limit (line 22 the condition of iter > n).\nAlg.2 outlines the procedures to solve the conservative subproblem on a under-trained batch. The conservative subproblem intends to accelerate the under-trained batch without drastic weight changes. The update equation in line 7 corresponds to Eq.18. Specifically, [\u03c8w(dt) \u2212 limit]\u2207\u03c8w(dt) is the gradient of 12 \u2016 \u03c8w(dt) \u2212 limit\u201622 to accelerate the training of a undertrained batch; the second term, nw (w \u2212 wt\u22121), is the gradient of 2nw \u2016 w \u2212 wt\u22121\u201622 that bounds significant weight changes. The limit is the same upper control threshold in Alg.1. The stop specifies the maximal approximate iterations to reflect the early stopping. \u03b6 is a constant learning rate.\nThe neural network training needs gradually decrease the learning rate to ensure the convergence [9]. It is a common tactic to decrease the learning rate w.r.t training iterations. The inconsistent iterations of ISGD requires a new way to guide the learning rate. Instead, ISGD decreases the learning rate w.r.t the average loss of a dataset. The average loss is better than iterations, as it directly reflects the training status of the\nmodel, while calculating the average loss of a dataset is expensive. Since the average loss in Eq.13 is from the latest scan of dataset (or losses in an epoch), it is approximate to the average loss of dataset. Hence, we use the average loss (Alg.1, line 19) to guide the learning rate."}, {"heading": "4.3. Extend to Other SGD Variants", "text": "It is straight forward to extend the inconsistent training to other SGD variants. For example, Momentum [30] updates the weight with the following equations\nvt+1 = \u00b5vt \u2212 \u03b1\u2207\u03c8(wt) wt+1 = wt + vt+1\n(19)\nand the Nesterov accelerated gradient follows the update rule of\nvt+1 = \u00b5vt \u2212 \u03b1\u2207\u03c8(wt + \u00b5vt) wt+1 = wt + vt+1\n(20)\nTo introduce the inconsistent training to these SGD variants, we only need change the line 21 of Alg.1 according to Eq.19 and Eq.20, respectively. The Alg.2 remains the same."}, {"heading": "4.4. Parallel ISGD", "text": "ISGD intends to scale over the distributed or multiGPU system using MPI-style collectives such as broadcast, reduce, and allreduce [31]. Alg.1 and Alg.2 are already the parallel version manifested by the collectives in them. Fig.4 demonstrates the data parallelization scheme inside ISGD. Let\u2019s assume there are n computing nodes, each of which is a GPU or a server in a cluster. Each node contains a model duplicate. A node fetches an independent segment of the original batch referred to as the sub-batch. Subsequently, all nodes simultaneously calculate sub-gradients and sub-losses with the assigned subbatches. Once the calculation is done, the algorithm reduce sub-gradients and sub-losses (Line 10-12 in Alg.1) to a master node so as to acquire a global gradient and loss. Then, the master node updates network weights (line 21 in Alg.1) and broadcast (line 9 in Alg.1) the latest weights. Please refer to [31] for more discussion regarding the MPI-style collectives and the SGD parallelization. Therefore, ISGD separates the algorithm from the system configurations by employing MPIstyle collectives [32]. Since MPI is an industrial and academia standard, ISGD is highly portable on various heterogeneous distributed system."}, {"heading": "4.5. Batch Size and Convergence Speed", "text": "Batch size is the key factor to the parallelism of ISGD. As operations on a batch are independent, scaling ISGD on systems with the massive computing power prefers a sufficiently large batch. An unwieldy large batch size, however, is detrimental to the convergence rate under the limited computing budget. Current convergence rate analysis utilizes iterations as the only performance metric, but it fails to consider the fact that an iteration faster algorithm may cost more time than the slower counterpart. Hence, it is practical to analyze the convergence rate in the time domain.\nLet\u2019s assume the maximal processing capability of a system is C1 images per second, and the time spent on synchronizations is C2 seconds. Network cost is a constant because it only depends on the size of network parameter. A gradient update essentially costs:\ntiter = tcomp + tcomm\n= nb C1 + C2 (21)\nwhere nb is the batch size. Given fixed time t, the number of gradient updates is\nT = t\ntiter (22)\nAfter T gradient updates, the loss is bounded by [33]\n\u03c8 6 1 \u221a\nnbT + 1 T\n(23)\nLet\u2019s assume equality in Eq.23 and substitute Eq.22. It yields Eq.24 that governs loss \u03c8, time t and system configurations C1 and C2:\n\u03c8t = \u221a t\n\u221a nb + C1C2\nnbC1 + nb C1 + C2 (24)\nFig.5 presents the predicted training time under two system configurations calculated by Eq.24 at different batch sizes nb \u2208 (0, 3000). By fixing \u03c8, the equation approximates the total training time under different batches. The figure demonstrates the optimal batch size of the first and second system are 500 and 1000 respectively. In this case, a faster system needs a larger batch. The performance of both systems deteriorates afterward. As a result, the optimal batch size is a tradeoff between system configurations and algorithmic convergences."}, {"heading": "5. Experiments", "text": "In this section, we demonstrate the performance of inconsistent training against SGD variants such as Momentum and Nesterov on a variety of widely recognized datasets including MNIST [34], CIFAR-10 and ImageNet. MNIST has 60000 handwritten digits ranging from 0 to 9. CIFAR-10 has 60000 32\u00d732 RGB images categorized in 10 classes. ILSVRC 2012 ImageNet has 1431167 256 \u00d7256 RGB images depicting 1000 object categories. We use LeNet, Caffe CIFAR-10 Quick, and AlexNet to train on MNIST, CIFAR-10, and ImageNet, respectively. The complexity of networks is proportional to the size of datasets. Therefore, our benchmarks cover the small, middle, and large scale CNN training.\nWe conduct the experiments on a multiGPU system with 4 NVIDIA Maxwell TITAN X. The CUDA version is 7.5, the compiler is GCC 4.8.4. The machine has 64 GB RAM and 1TB SSD. CPU is Xeon E5 4655 v3. Caffe is built with the cuDNN version 4. The GPU machine was exclusively owned by us during the benchmark."}, {"heading": "5.1. Qualitative Evaluation of Inconsistent Training", "text": "This section intends to qualitatively evaluate the impacts of inconsistent training. The purpose of inconsistent training is to rebalance the training effort across batches so that the largeloss batch receives more training than the small-loss one. To qualitatively evaluate the impacts of inconsistent training, we exam the progression of the loss distribution, the average loss, the standard deviation of batch\u2019s loss distribution, as well as the\nvalidation accuracy. We setup the training with Caffe CIFAR10 Quick network on CIFAR-10 dataset. The batch size is set at 2500 yielding 20 independent batches. Fig.6a and Fig.6b present the loss distribution of 20 batches in the training. We arrange losses in epochs as the solver explores a batch only once in an epoch,\nThe inconsistent training has the following merits. 1) ISGD converges faster than SGD due to the improvement of training model. We measure the convergence rate by the average loss of batches in a dataset, and the method conforms to the training definition in Eq.3. The average loss data in Fig.6d demonstrates that ISGD converges faster than SGD. In contrast with SGD, the lower average loss of ISGD after iter > 7000 (Fig.6d) explains the better accuracy of ISGD after testing 9 (Fig.6e). The validation accuracy of ISGD in Fig.6e is also above SGD, that is consistent with data in Fig.6d that the average loss of ISGD is below the average loss of SGD in the training. These justify the convergence advantage of inconsistent training. 2) ISGD dynamically accelerates the large-loss batch in the progress of training to reduce the training gap with small-loss batches. Therefore, the variation of batch\u2019s training status is less than the one trained by SGD. Please note we measure the training status of a batch with its loss, and the variation of batch\u2019s training status is measured by the standard deviation of batch\u2019s loss distribution. Fig.6c demonstrates the inconsistent training successfully attenuates the training variations among batches. When iter \u2208 [1000, 6000], the std of batch\u2019s loss distribution of ISGD is much lower than SGD. The result is also consistent with the loss distribution in Fig.6a and Fig.6b,\nin which the loss distribution of SGD is much wider than ISGD at epoch \u2208 [50, 300]."}, {"heading": "5.2. Performance Evaluations", "text": "The setup of each comparisons, ISGD V.S. SGD, has been carefully set to be the single factor experiment, i.e. the only difference is the inconsistent training. Some parameters of SGD greatly affect the training performance, setting different values on them jeopardizes the credibility of experiments. Therefore, we ensure the parameters of SGD and ISGD to be same in each comparisions. The first parameter considered is the learning rate. The MNIST tests adopt a constant learning rate of 0.01, and CIFAR tests adopt a constant learning rate of 0.001. Both cases are consistent with the solver defined in Caffe. Caffe fixes the learning rate for these two cases because networks yield the satisfactory accuracies , 75% on CIFAR and 99% on MNIST, without shrinking the learning rate. Since AlexNet has to shrink lr, the learning rate of it has 3 possibilities: lr = 0.015 if the average loss \u03c8 \u2208 [2.0,+\u221e], lr = 0.0015 if \u03c8 in [1.2, 2.0), and lr = 0.00015 if \u03c8 in [0, 1.2). The batch size is also same for each comparison in CIFAR, MNIST and ImageNet. We adopt a large batch to fully saturate 4 GPUs. For other parameters such as the weight decay and momentum, they are also same through all the tests.\nISGD consistently outperforms SGD in all tests manifesting the effectiveness of inconsistent training. Please note both methods incorporate the momentum term. Since an iteration of ISGD is inconsistent, we test every other 2, 6, 900 seconds (only count the training time with the test time excluded) for MNIST, CIFAR and ImageNet tests, respectively. The horizontal dashed line represents the target accuracy, and the total training time starts from 0 to the point that the validation accuracy is consistently above the dashed line. In the ImageNet test, ISGD demonstrates the 14.94% faster convergence than SGD. SGD takes 21.4 hours to reach the 81% top 5 accuracy, while ISGD takes 18.2 hours (Fig.7c) . In the CIFAR test, ISGD demonstrates 23.57% faster convergence than SGD. The top accuracy for CIFAR-Quick network reported on CIFAR-10 is 75%. After 306 seconds, the test accuracy of SGD is steadily above 75%, while ISGD only takes 234 seconds (Fig.7b). Finally, ISGD demonstrates 28.57% faster convergence than SGD on MNIST dataset. It takes SGD 56 seconds to reach the 99% top accuracy, while ISGD only takes 40 seconds. Since the training is essentially a stochastic process, the performance subjects to changes. We repeat each test cases 10 times, and we list the performance data in Table.1. The results also uphold the convergence advantages of inconsistent training.\nTo explain the performance advantages of ISGD, we also use the training dataset to test. Whereas, the training set of Im-\nageNet 256 GB is too large to be tested, we use \u03c8 in Alg.1 to approximate the training error. Fig.7d, Fig.7e and Fig.7f demonstrate the training error of ISGD is consistently below the SGD. The results demonstrate the benefit of inconsistent training, and they also explain the good validation accuracy of ISGD in Fig.7a, Fig.7b and Fig.7c.\nThe inconsistent training is also compatible with the Nesterov accelerated gradient. Fig.9 demonstrates the validation accuracy and the training loss progression on ImageNet trained with the Nesterov accelerated gradient. The inconsistent training beats the regular Nesterov method. If set 58% top 1 accuracy as the threshold, the inconsistent training takes 65 tests to exceed the threshold, while the regular one takes 75 tests. Please note the time interval of two consecutive tests is fixed. Therefore, the inconsistent training demonstrates the 13.4 % performance gain. The compatibility is under our expectation. The Nesterov method accelerates the convergence by considering the curvature information, while ISGD rebalances the training across batches."}, {"heading": "5.3. Time Domain Convergence Rate W.R.T Batch Size on MultiGPUs", "text": "Fig.8 demonstrates convergence speeds at different batch sizes on MNIST, CIFAR and ImageNet datasets. The figures reflect the following conclusions: 1) A sufficiently large batch is necessary to the multiGPU training. The single GPU only involves computations tcompt, while the multiGPU training entails an additional term tcomm for synchronizations. A small batch size for the single GPU training is favored to ensure the frequent gradient updates. In the multiGPU training, the cost of synchronizations linearly increases with the number of gradient updates. Increasing batch size improves the convergence rate, thereby fewer iterations and synchronizations. Besides, it also improves system utilizations and saturations. As a consequence, a moderate batch size is favored to the multiGPU training as indicated in Fig.8. 2) An unwieldy batch size slows down the convergence. Because computations linearly increase with the batch size, which reduces the number of gradient updates in a limited time. The declined convergence speed is observable in the Fig.8a, Fig.8b and Fig.8c when batch size is set at 3000, 10000, 3400, respectively."}, {"heading": "6. Summary", "text": "In this paper, we propose the inconsistent training to dynamically adjust the training effort w.r.t batch\u2019s training status. ISGD models the training as a stochastic process, and it utilizes techniques in Stochastic Process Control to identify a large-loss batch on the fly. Then, ISGD solves a new subproblem to accelerate the training on the under-trained batch. Extensive experiments on a variety of datasets and models demonstrate the promising performance of inconsistent training."}], "references": [{"title": "Distributed asynchronous online learning for natural language processing", "author": ["K. Gimpel", "D. Das", "N.A. Smith"], "venue": "in: Proceedings of the Fourteenth Conference on Computational Natural Language Learning, Association for Computational Linguistics", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel matrix factorization for recommender systems", "author": ["H.-F. Yu", "C.-J. Hsieh", "S. Si", "I.S. Dhillon"], "venue": "Knowledge and Information Systems 41 (3) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Q", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang"], "venue": "V. Le, et al., Large scale distributed deep networks, in: Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "M\u00e9thode g\u00e9n\u00e9rale pour la r\u00e9solution des systemes d\u00e9quations simultan\u00e9es", "author": ["A. Cauchy"], "venue": "Comp. Rend. Sci. Paris 25 (1847) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1847}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F.R. Bach"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1951}, {"title": "Large scale online learning", "author": ["L.B.Y. Le Cun", "L. Bottou"], "venue": "Advances in neural information processing systems 16 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Online learning and stochastic approximations", "author": ["L. Bottou"], "venue": "On-line learning in neural networks 17 (9) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Blasx: A high performance level-3 blas library for heterogeneous multi-gpu computing", "author": ["L. Wang", "W. Wu", "Z. Xu", "J. Xiao", "Y. Yang"], "venue": "in: Proceedings of the 2016 International Conference on Supercomputing, ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "in: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative learning of deep convolutional feature point descriptors", "author": ["E. Simo-Serra", "E. Trulls", "L. Ferraz", "I. Kokkinos", "P. Fua", "F. Moreno- Noguer"], "venue": "in: Proceedings of the IEEE International Conference on Computer Vision", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Sample size selection in optimization methods for machine learning", "author": ["R.H. Byrd", "G.M. Chin", "J. Nocedal", "Y. Wu"], "venue": "Mathematical programming 134 (1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["M. Li", "T. Zhang", "Y. Chen", "A.J. Smola"], "venue": "in: Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Variance reduction for stochastic gradient optimization", "author": ["C. Wang", "X. Chen", "A.J. Smola", "E.P. Xing"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "An incremental gradient (-projection) method with momentum term and adaptive stepsize rule", "author": ["P. Tseng"], "venue": "SIAM Journal on Optimization 8 (2) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "G", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl"], "venue": "E. Hinton, On the importance of initialization and momentum in deep learning., ICML (3) 28 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Training recurrent neural networks", "author": ["I. Sutskever"], "venue": "Ph.D. thesis, University of Toronto ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12 (Jul) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "A simple weight decay can improve generalization", "author": ["J. Moody", "S. Hanson", "A. Krogh", "J.A. Hertz"], "venue": "Advances in neural information processing systems 4 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A.J. Smola"], "venue": "in:  Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "in: Proceedings of the 22nd ACM international conference on Multimedia, ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "in: BigLearn, NIPS Workshop, no. EPFL- CONF-192376", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "in: Proceedings of COMPSTAT\u20192010, Springer", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "On the momentum term in gradient descent learning algorithms", "author": ["N. Qian"], "venue": "Neural networks 12 (1) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1999}, {"title": "A", "author": ["E. Gabriel", "G.E. Fagg", "G. Bosilca", "T. Angskun", "J.J. Dongarra", "J.M. Squyres", "V. Sahay", "P. Kambadur", "B. Barrett"], "venue": "Lumsdaine, et al., Open mpi: Goals, concept, and design of a next generation mpi implementation, in: European Parallel Virtual Machine/Message Passing Interface Users Group Meeting, Springer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Journal of Machine Learning Research 13 (Jan) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "C", "author": ["Y. LeCun", "C. Cortes"], "venue": "J. Burges, The mnist database of handwritten digits ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "In particular, large scale neural networks have drastically improved various systems in natural language processing [1], video motion analysis [2], and recommender systems [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "In particular, large scale neural networks have drastically improved various systems in natural language processing [1], video motion analysis [2], and recommender systems [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "In particular, large scale neural networks have drastically improved various systems in natural language processing [1], video motion analysis [2], and recommender systems [3].", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "For example, it takes 10000 CPU cores up to days to complete the training of a network with 1 billion parameters [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "The standard first order full Gradient Descent (GD), which dates back to [5], calculates the gradient with the whole dataset.", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "edu (Linnan Wang) descent (O(\u03c1k), \u03c1 < 1) [6], the computation in an iteration linearly increases with the size of dataset.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "To address this issue, Stochastic Gradient Descent [7, 8] was proposed by observing a large amount of redundancy among training examples.", "startOffset": 51, "endOffset": 57}, {"referenceID": 7, "context": "To address this issue, Stochastic Gradient Descent [7, 8] was proposed by observing a large amount of redundancy among training examples.", "startOffset": 51, "endOffset": 57}, {"referenceID": 8, "context": "Although the convergence rate of SGD, O(1/ \u221a bk + 1/k) [9] where b is the batch size, is slower than GD, SGD updates the model much faster than GD in a period, i.", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "SGD hits a sweet spot between the good system utilization [10] and the fast gradient updates.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "It is simple in math, while none-trivial to be implemented on a large-scale dataset such as ImageNet [11].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "However, it is known that the gradient variances differentiate batches in the training [12], and gradient updates from the large loss batch contribute more than the small", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "loss ones [13].", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "Enlarging the batch size expedites the convergence [14], but it linearly adds computations in an iteration.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "In this case, a moderate large batch reduces overall communications [15], and it also improves the system saturation and the available parallelism.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "The stochastic sampling in SGD introduces the gradient variance, which slows down the convergence rate [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "Stochastic Variance Reduced Gradient (SVRG) [12] keeps network historical parameters and gradients to explicitly reduce the variance of update rule, but the authors indicate SVRG only works well for the fine-tuning of non-convex neural network.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "[16] explore the control variates on SGD, while Zhao and Tong [17] explore the importance sampling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Momentum [18] is a widely recognized heuristic to boost SGD.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Momentum damps oscillations in directions of high curvature by combining gradients with opposite signs, and it builds up speed toward a direction that is consistent with the previously accumulated gradients [19].", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "The update rule of Nesterov\u2019s accelerated gradient is similar to Momentum [20], but the minor different update mechanism for building the velocity results in important behavior differences.", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "Adagrad [21] adapts the learning rate to the parameters, performing larger updates for infrequent parameters, and smaller updates for frequent parameters.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "[13] adopt a similar idea in training the Siamese network to learn the deep descriptors by intentionally feeding the network with hard training pairs, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The second term is Weight Decay [22], and \u03bb is a parameter to adjust its contribution (normally around 10\u22124).", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For example, p(x) = [0, 0, 1, 0, 0] indicates the object belongs the category 2 (index starts from 0).", "startOffset": 20, "endOffset": 35}, {"referenceID": 21, "context": "This direction has been well addressed [23, 24].", "startOffset": 39, "endOffset": 47}, {"referenceID": 22, "context": "First, existing datasets, such as Places [25] or ImageNet, contain uneven number of images in each category.", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 0, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 0, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 1, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 2, "context": "For example, the chance of sampling 1 from [1, 1, 1, 0, 2, 3] is higher than the rest.", "startOffset": 43, "endOffset": 61}, {"referenceID": 23, "context": "Existing deep learning frameworks, such as Caffe [27] or Torch [28], alleviates the issue by pre-permuting the entire dataset before slicing into batches: Permute{d} \u2192 d = {d0,d1, .", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "Existing deep learning frameworks, such as Caffe [27] or Torch [28], alleviates the issue by pre-permuting the entire dataset before slicing into batches: Permute{d} \u2192 d = {d0,d1, .", "startOffset": 63, "endOffset": 67}, {"referenceID": 25, "context": "The reasons are that: 1) SGD demands a small learning rate (lr) [29] to converge, and lr is usually less than 10\u22121.", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "The neural network training needs gradually decrease the learning rate to ensure the convergence [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 26, "context": "For example, Momentum [30] updates the weight with the following equations", "startOffset": 22, "endOffset": 26}, {"referenceID": 27, "context": "Therefore, ISGD separates the algorithm from the system configurations by employing MPIstyle collectives [32].", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": "After T gradient updates, the loss is bounded by [33]", "startOffset": 49, "endOffset": 53}, {"referenceID": 29, "context": "In this section, we demonstrate the performance of inconsistent training against SGD variants such as Momentum and Nesterov on a variety of widely recognized datasets including MNIST [34], CIFAR-10 and ImageNet.", "startOffset": 183, "endOffset": 187}], "year": 2017, "abstractText": "Stochastic Gradient Descent (SGD) updates network parameters with a noisy gradient computed from a random batch, and each batch evenly updates the network once in an epoch. This model applies the same training effort to each batch, but it overlooks the fact that the gradient variance, induced by Sampling Bias and Intrinsic Image Difference, renders different training dynamics on batches. In this paper, we develop a new training strategy for SGD, referred to as Inconsistent Stochastic Gradient Descent (ISGD) to address this problem. The core concept of ISGD is the inconsistent training, which dynamically adjusts the training effort w.r.t the loss. ISGD models the training as a stochastic process that gradually reduces down the mean of batch\u2019s loss, and it utilizes a dynamic upper control limit to identify a large loss batch on the fly. Then, it solves a new subproblem on the identified batch to accelerate the training while avoiding drastic parameter changes. ISGD is straightforward, computationally efficient and without requiring auxiliary memories. A series of empirical evaluations on real world datasets and networks demonstrate the promising performance of inconsistent training.", "creator": "LaTeX with hyperref package"}}}