{"id": "1603.06147", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation", "abstract": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.", "histories": [["v1", "Sat, 19 Mar 2016 21:35:04 GMT  (172kb,D)", "http://arxiv.org/abs/1603.06147v1", null], ["v2", "Wed, 23 Mar 2016 20:57:23 GMT  (172kb,D)", "http://arxiv.org/abs/1603.06147v2", null], ["v3", "Thu, 16 Jun 2016 04:06:01 GMT  (172kb,D)", "http://arxiv.org/abs/1603.06147v3", null], ["v4", "Tue, 21 Jun 2016 01:12:22 GMT  (174kb,D)", "http://arxiv.org/abs/1603.06147v4", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["junyoung chung", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1603.06147"}, "pdf": {"name": "1603.06147.pdf", "metadata": {"source": "CRF", "title": "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation", "authors": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "emails": ["junyoung.chung@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "The existing machine translation systems have relied almost exclusively on word-level modelling with explicit segmentation. This is mainly due to the issue of data sparsity which becomes much more severe, especially for n-grams, when a sentence is represented as a sequence of characters rather than words, as the length of the sequence grows significantly. In addition to data sparsity, we often have a priori belief that a word, or its segmented-out lexeme, is a basic unit of meaning, making it natural to approach translation as mapping from a sequence of source-language words to a sequence of target-language words.\nThis has continued with the more recently proposed paradigm of neural machine transla-\ntion, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary (Jean et al., 2015; Luong et al., 2015b). Therefore, in this paper, we address a question of whether neural machine translation can be done directly on a sequence of characters without any explicit word segmentation.\nTo answer this question, we focus on representing the target side as a character sequence. We evaluate neural machine translation models with a character-level decoder on four language pairs from WMT\u201915 to make our evaluation as convincing as possible. We represent the source side as a sequence of subwords extracted using byte-pair encoding from Sennrich et al. (2015), and vary the target side to be either a sequence of subwords or characters. On the target side, we further design a novel recurrent neural network (RNN), called biscale recurrent network, that better handles multiple timescales in a sequence, and test it in addition to a naive, stacked recurrent neural network.\nOn all of the four language pairs\u2013En-Cs, En-De, En-Ru and En-Fi\u2013, the models with a characterlevel decoder outperformed the ones with a subword-level decoder. We observed a similar trend with the ensemble of each of these configurations, outperforming both the previous best neural and non-neural translation systems on EnCs, En-De and En-Fi, while achieving a comparable result on En-Ru. We find these results to be a strong evidence that neural machine translation can indeed learn to translate at the character level and that in fact, it benefits from doing so."}, {"heading": "2 Neural Machine Translation", "text": "Neural machine translation refers to a recently proposed approach to machine translation (Cho et\nar X\niv :1\n60 3.\n06 14\n7v 1\n[ cs\n.C L\n] 1\n9 M\nar 2\n01 6\nal., 2014; Sutskever et al., 2014; Bahdanau et al., 2015). This approach aims at building an end-toend neural network that takes as input a source sentence X = (x1, . . . , xTx) and outputs its translation Y = (y1, . . . , yTy), where xt and yt\u2032 are respectively source and target symbols. This neural network is constructed as a composite of an encoder network and a decoder network.\nThe encoder network encodes the input sentence X into its continuous representation. In this paper, we closely follow the neural translation model proposed in Bahdanau et al. (2015) and use a bidirectional recurrent neural network, which consists of two recurrent neural networks. The forward network reads the input sentence in a forward direction: \u2212\u2192z t = \u2212\u2192 \u03c6 (ex(xt),\n\u2212\u2192z t\u22121), where ex(xt) is a continuous embedding of the t-th input symbol, and \u03c6 is a recurrent activation function. Similarly, the reverse network reads the sentence in a reverse direction (right to left): \u2190\u2212z t = \u2190\u2212 \u03c6 (ex(xt),\n\u2190\u2212z t+1). At each location in the input sentence, we concatenate the hidden states from the forward and reverse RNNs to form a context set C = {z1, . . . , zTx} , where zt =\n[\u2212\u2192z t;\u2190\u2212z t]. Then the decoder computes the conditional distribution over all possible translations based on this context set. This is done by first rewriting the conditional probability of a translation: log p(Y |X) = \u2211Ty t\u2032=1 log p(yt\u2032 |y<t\u2032 , X). For each conditional term in the summation, the decoder RNN updates its hidden state by\nht\u2032 = \u03c6(ey(yt\u2032\u22121),ht\u2032\u22121, ct\u2032), (1)\nwhere ey is the continuous embedding of a target symbol. ct\u2032 is a context vector computed by a softalignment mechanism:\nct\u2032 = falign(ey(yt\u2032\u22121),ht\u2032\u22121, C)). (2)\nThe soft-alignment mechanism falign weights each vector in the context set C according to its relevance given what has been translated. The weight of each vector zt is computed by\n\u03b1t,t\u2032 = 1\nZ efscore(ey(yt\u2032\u22121),ht\u2032\u22121,zt), (3)\nwhere fscore is a parametric function returning an unnormalized score for zt given ht\u2032\u22121 and yt\u2032\u22121. We use a feedforward network with a single hidden layer in this paper.1 Z is a normalization constant: Z = \u2211Tx k=1 e\nfscore(ey(yt\u2032\u22121),ht\u2032\u22121,zk). This 1For other possible implementations, see (Luong et al., 2015a).\nprocedure can be understood as computing the alignment probability between the t\u2032-th target symbol and t-th source symbol.\nThe hidden state ht\u2032 , together with the previous target symbol yt\u2032\u22121 and the context vector ct\u2032 , is fed into a feedforward neural network to result in the conditional distribution:\np(yt\u2032 | y<t\u2032 , X) \u221d ef yt\u2032 out(ey(yt\u2032\u22121),ht\u2032 ,ct\u2032 ). (4)\nThe whole model, consisting of the encoder, decoder and soft-alignment mechanism, is then tuned end-to-end to minimize the negative loglikelihood using stochastic gradient descent."}, {"heading": "3 Towards Character-Level Translation", "text": ""}, {"heading": "3.1 Motivation", "text": "Let us revisit how the source and target sentences (X and Y ) are represented in neural machine translation. For the source side of any given training corpus, we scan through the whole corpus to build a vocabulary Vx of unique tokens to which we assign integer indices. A source sentence X is then built as a sequence of the indices of such tokens belonging to the sentence, i.e., X = (x1, . . . , xTx), where xt \u2208 {1, 2, . . . , |Vx|}. The target sentence is similarly transformed into a target sequence of integer indices.\nEach token, or its index, is then transformed into a so-called one-hot vector of dimensionality |Vx|. All but one elements of this vector are set to 0. The only element whose index corresponds to the token\u2019s index is set to 1. This one-hot vector is the one which any neural machine translation model sees. The embedding function, ex or ey, is simply the result of applying a linear transformation (the embeddings matrix) to this one-hot vector.\nThe important property of this approach based on one-hot vectors is that the neural network is oblivious to the underlying semantics of the tokens. To the neural network, each and every token in the vocabulary is equal distance away from every other token. The semantics of those tokens are simply learned (into the embeddings) to maximize the translation quality, or the log-likelihood of the model.\nThis property allows us great freedom in the choice of tokens\u2019 unit. Neural networks have been shown to work well with word tokens (Bengio et al., 2001; Schwenk, 2007; Mikolov et al., 2010)\nbut also with finer units, such as subwords (Sennrich et al., 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015). Although there have been a number of previous research reporting the use of neural networks with characters (see, e.g., Mikolov et al. (2012)), the dominant approach has been to preprocess the text into a sequence of symbols, each associated with a sequence of characters, after which the neural network is presented with those symbols rather than with characters.\nMore recently in the context of neural machine translation, two research groups have proposed to directly use characters. Kim et al. (2015) proposed to represent each word not as a single integer index as before, but as a sequence of characters, and use a convolutional network followed by a highway network (Srivastava et al., 2015) to extract a continuous representation of the word. This approach, which effectively replaces the embedding function ex, was adopted by Costa-Jussa\u0300 and Fonollosa (2016) for neural machine translation. Similarly, Ling et al. (2015b) use a bidirectional recurrent neural network to replace the embedding functions ex and ey to respectively encode a character sequence to and from the corresponding continuous word representation. A similar, but slightly different approach was proposed by Lee et al. (2015), where they explicitly mark each character with its relative location in a word (e.g., \u201cB\u201deginning and \u201cI\u201dntermediate).\nDespite the fact that these recent approaches work at the level of characters, it is less satisfying that they all rely on knowing how to segment characters into words. Although it is generally easy for languages like English, this is not always the case. This word segmentation procedure can be as simple as tokenization followed by some punctuation normalization, but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance (Creutz and Lagus, 2005; Huang and Zhao, 2007). Furthermore, these segmentation2 steps are often tuned or designed separately from the ultimate objective of translation quality, potentially contributing to a suboptimal quality.\nBased on this observation and analysis, in this paper, we ask ourselves and the readers a question\n2From here on, the term segmentation broadly refers to any method that splits a given character sequence into a sequence of subword symbols.\nwhich should have been asked much earlier: Is it possible to do character-level translation without any explicit segmentation?"}, {"heading": "3.2 Why Word-Level Translation?", "text": "(1) Word as a Basic Unit of Meaning A word can be understood in two different senses. In the abstract sense, a word is a basic unit of meaning (lexeme), and in the other sense, can be understood as a \u201cconcrete word as used in a sentence.\u201d (Booij, 2012). A word in the former sense turns into that in the latter sense via a process of morphology, including inflection, compounding and derivation. These three processes do alter the meaning of the lexeme, but often it stays close to the original meaning. Because of this view of words as basic units of meaning (either in the form of lexemes or derived form) from linguistics, much of previous work in natural language processing has focused on using words as basic units of which a sentence is encoded as a sequence. Also, the potential difficulty in finding a mapping between a word\u2019s character sequence and meaning3 has likely contributed to this trend toward word-level modelling.\n(2) Data Sparsity There is a further technical reason why much of previous research on machine translation has considered words as a basic unit. This is mainly due to the fact that major components in the existing translation systems, such as language models and phrase tables, are a count-based estimator of probabilities. In other words, a probability of a subsequence of symbols, or pairs of symbols, is estimated by counting the number of its occurrences in a training corpus. This approach severely suffers from the issue of data sparsity, which is due to a large state space which grows exponentially w.r.t. the length of subsequences while growing only linearly w.r.t. the vocabulary size. This poses a great challenge to character-level modelling, as any subsequence will be on average 4\u20135 times longer when characters, instead of words, are used. Indeed, Vilar et al. (2007) reported worse performance when the character sequence was directly used by a phrasebased machine translation system.\n(3) Vanishing Gradient Specifically to neural machine translation, a major reason behind the\n3For instance, \u201cquit\u201d, \u201cquite\u201d and \u201cquiet\u201d are one editdistance away from each other but have distinct meanings.\nwide adoption of word-level modelling is due to the difficulty in modelling long-term dependencies with recurrent neural networks (Bengio et al., 1994; Hochreiter, 1998). As the lengths of the sentences on both sides grow when they are represented in characters, it is easy to believe that there will be more long-term dependencies that must be captured by the recurrent neural network for successful translation."}, {"heading": "3.3 Why Character-Level Translation?", "text": "Why not word-level translation? The most pressing issue with word-level processing is that we do not have a perfect word segmentation algorithm for any one language. A perfect segmentation algorithm needs to be able to segment any given sentence into a sequence of lexemes and morphemes. This problem is however a difficult problem on its own and often requires decades of research (see, e.g., Creutz and Lagus (2005) for Finnish and other morphologically rich languages and Huang and Zhao (2007) for Chinese). Therefore, many opt to using either a rule-based tokenization approach or a suboptimal, but still available, learning based segmentation algorithm.\nThe outcome of this naive, sub-optimal segmentation is that the vocabulary is often filled with many similar words that share a lexeme but have different morphology. For instance, if we apply a simple tokenization script to an English corpus, \u201crun\u201d, \u201cruns\u201d, \u201cran\u201d and \u201crunning\u201d are all separate entries in the vocabulary, while they clearly share the same lexeme \u201crun\u201d. This prevents any machine translation system, in particular neural machine translation, from modelling these morphological variants efficiently. More specifically in the case of neural machine translation, each of these morphological variants\u2013\u201crun\u201d, \u201cruns\u201d, \u201cran\u201d and \u201crunning\u201d\u2013 will be assigned a d-dimensional word vector, leading to five independent vectors, while it is clear that if we can segment those variants into a lexeme and other morphemes, we can model them more efficiently. For instance, we can have a d-dimensional vector for the lexeme \u201crun\u201d and much smaller vectors for \u201cs\u201d and\u201cing\u201d. Each of those variants will be then a composite of the lexeme vector (shared across these variants) and morpheme vectors (shared across words sharing the same suffix, for example) (Botha and Blunsom, 2014). This makes use of distributed representation, which generally yields better general-\nization, but seems to require an optimal segmentation, which is unfortunately almost never available.\nIn addition to inefficiency in modelling, there are two additional negative consequences from using (unsegmented) words. First, the translation system cannot generalize well to novel words, which are often mapped to a token reserved for an unknown word. This effectively ignores any meaning or structure of the word to be incorporated when translating. Second, even when a lexeme is common and frequently observed in the training corpus, its morphological variant may not be. This implies that the model sees this specific, rare morphological variant much less and will not be able to translate it well. However, if this rare morphological variant shares a large part of its spelling with other more common words, it is desirable for a machine translation system to exploit those common words when translating those rare variants.\nWhy Character-Level Translation? All of these issues can be addressed to certain extent by directly modelling characters. Although the issue of data sparsity arises in character-level translation, it is elegantly addressed by using a parametric approach based on recurrent neural networks instead of a non-parametric count-based approach. Furthermore, in recent years, we have learned how to build and train a recurrent neural network that can well capture long-term dependencies by using more sophisticated activation functions, such as long short-term memory units (Hochreiter and Schmidhuber, 1997) and gated recurrent units (Cho et al., 2014).\nKim et al. (2015) and Ling et al. (2015a) recently showed that by having a neural network that converts a character sequence into a word vector, we avoid the issues from having many morphological variants appearing as separate entities in a vocabulary. This is made possible by sharing the character-to-word neural network across all the unique tokens. A similar approach was applied to machine translation by Ling et al. (2015b).\nThese recent approaches, however, still rely on the availability of a good, if not optimal, segmentation algorithm. Ling et al. (2015b) indeed states that \u201c[m]uch of the prior information regarding morphology, cognates and rare word translation among others, should be incorporated.\u201d\nIt however becomes unnecessary to consider\nthese prior information, if we use a neural network, be it recurrent, convolution or their combination, directly on the unsegmented character sequence. The possibility of using a sequence of unsegmented characters has been studied over many years in the field of deep learning. For instance, Mikolov et al. (2012) and Sutskever et al. (2011) trained a recurrent neural network language model (RNN-LM) on character sequences. The latter showed that it is possible to generate sensible text sequences by simply sampling a character at a time from this model. More recently, Zhang et al. (2015) and Xiao and Cho (2016) successfully applied a convolutional net and a convolutionalrecurrent net respectively to character-level document classification without any explicit segmentation. These previous works suggest the possibility of applying neural networks for the task of machine translation, which is often considered a substantially more difficult problem compared to document classification and language modelling."}, {"heading": "3.4 Challenges and Questions", "text": "There are two overlapping sets of challenges for the source and target sides. On the source side, it is unclear how to build a neural network that learns a highly nonlinear mapping from a spelling to the meaning of a sentence.\nOn the target side, there are two challenges. The first challenge is the same one from the source side, as the decoder neural network needs to summarize what has been translated. In addition to this, the character-level modelling on the target side is more challenging, as the decoder network must be able to generate a long, coherent sequence of characters. This is a great challenge, as the size of the state space grows exponentially w.r.t. the number of symbols, and in the case of characters, it is often 300-1000 symbols long.\nAll these challenges should first be framed as questions; whether the current recurrent neural networks, which are already widely used in neural machine translation, are able to address these challenges as they are. In this paper, we aim at answering these questions empirically and focus on the challenges on the target side (as the target side shows both of the challenges)."}, {"heading": "4 Character-level Translation", "text": "In this paper, we try to answer the questions posed earlier by testing two different types of recurrent\nneural networks on the target side (decoder). First, we test an existing recurrent neural network with gated recurrent units (GRU). We call this decoder a base decoder.\nSecond, we build a novel two-layer recurrent neural network, inspired by the gated-feedback network from Chung et al. (2015), called a biscale recurrent neural network. We design this network to facilitate capturing two timescales, motivated by the fact that characters and words may work at two separate timescales.\nWe choose to test these two alternatives for the following purposes. Experiments with the base decoder will clearly answer whether the existing neural network is enough to handle character-level decoding, which has not been properly answered in the context of machine translation. The alternative, the bi-scale decoder, is tested in order to see whether it is possible to design a better decoder, if the answer to the first question is positive."}, {"heading": "4.1 Bi-Scale Recurrent Neural Network", "text": "In this proposed bi-scale recurrent neural network, there are two sets of hidden units, h1 and h2. They contain the same number of units, i.e., dim(h1) = dim(h2). The first set h1 models a fast-changing timescale (thereby, a faster layer), and h2 a slower timescale (thereby, a slower layer). For each hidden unit, there is an associated gating unit, to which we refer by g1 and g2. For the description below, we use yt\u2032\u22121 and ct\u2032 for the previous target symbol and the context vector (see Eq. (2)), respectively.\nLet us start with the faster layer. The faster layer outputs two sets of activations, a normal output h1t\u2032 and its gated version h\u030c1t\u2032 . The activation of the faster layer is computed by\nh1t\u2032 = tanh ( Wh 1 [ ey(yt\u2032\u22121); h\u030c 1 t\u2032\u22121; h\u0302 2 t\u2032\u22121; ct\u2032 ]) ,\nwhere h\u030c1t\u2032\u22121 and h\u0302 2 t\u2032\u22121 are the gated activations of\nthe faster and slower layers respectively. These gated activations are computed by\nh\u030c1t\u2032 = (1\u2212 g1t\u2032) h1t\u2032 , h\u03022t\u2032 = g1t\u2032 h2t\u2032 .\nIn other words, the faster layer\u2019s activation is based on the adaptive combination of the faster and slower layers\u2019 activations from the previous time step. Whenever the faster layer determines that it needs to reset, i.e., g1t\u2032\u22121 \u2248 1, the next activation will be determined based more on the slower layer\u2019s activation.\nThe faster layer\u2019s gating unit is computed by g1t\u2032 = \u03c3 ( Wg 1 [ ey(yt\u2032\u22121); h\u030c 1 t\u2032\u22121; h\u0302 2 t\u2032\u22121; ct\u2032 ]) ,\nwhere \u03c3 is a sigmoid function. The slower layer also outputs two sets of activations, a normal output h2t\u2032 and its gated version h\u030c2t\u2032 . These activations are computed as follows:\nh2t\u2032 = (1\u2212 g1t\u2032) h2t\u2032\u22121 + g1t\u2032 h\u03032t\u2032 , h\u030c2t\u2032 = (1\u2212 g2t\u2032) h2t\u2032 ,\nwhere h\u03032t\u2032 is a candidate activation. The slower layer\u2019s gating unit g2t\u2032 is computed by\ng2t\u2032 =\u03c3 ( Wg 2 [ (g1t\u2032 h1t\u2032); h\u030c2t\u2032\u22121; ct\u2032 ]) .\nThis adaptive leaky integration based on the gating unit from the faster layer has a consequence that the slower layer updates its activation only when the faster layer resets. This puts a soft constraint that the faster layer runs at a faster rate by preventing the slower layer from updating while the faster layer is processing a current chunk.\nThe candidate activation is then computed by h\u03032t\u2032 = tanh ( Wh 2 [ (g1t\u2032 h1t\u2032); h\u030c2t\u2032\u22121; ct\u2032 ]) . (5)\nh\u030c2t\u2032\u22121 indicates the reset activation from the previous time step, similarly to what happened in the faster layer, and ct\u2032 is the input from the context.\nAccording to g1t\u2032 h1t\u2032 in Eq. (5), the faster layer influences the slower layer, only when the faster layer has finished processing the current chunk and is about to reset itself (g1t\u2032 \u2248 1). In other words, the slower layer does not receive any input from the faster layer, until the faster layer has quickly processed the current chunk, thereby running at a slower rate than the faster layer does.\nAt each time step, the final output of the proposed bi-scale recurrent neural network is the concatenation of the output vectors of the faster and\nslower layers, i.e., [ h1;h2 ] . This concatenated vector is used to compute the probability distribution over all the symbols in the vocabulary, as in Eq. (4). See Fig. 1 for graphical illustration."}, {"heading": "5 Experiment Settings", "text": "For evaluation, we represent a source sentence as a sequence of subword symbols extracted by bytepair encoding (BPE, Sennrich et al. (2015)) and a target sentence either as a sequence of BPE-based symbols or as a sequence of characters.\nCorpora and Preprocessing We use all available parallel corpora for four language pairs from WMT\u201915: En-Cs, En-De, En-Ru and En-Fi. They consist of 12.1M, 4.5M, 2.3M and 2M sentence pairs, respectively. We tokenize each corpus using a tokenization script included in Moses.4 We only use the sentence pairs, when the source side is up to 50 subword symbols long and the target side is either up to 100 subword symbols or 500 characters. We do not use any monolingual corpus.\nFor all the pairs other than En-Fi, we use newstest-2013 as a development set, and newstest2014 (Test1) and newstest-2015 (Test2) as test sets. For En-Fi, we use newsdev-2015 and newstest2015 as development and test sets, respectively.\nModels and Training We test three models settings: (1) BPE\u2192BPE, (2) BPE\u2192Char (base) and (3) BPE\u2192Char (bi-scale). The latter two differ by the type of recurrent neural network we use. We use gated recurrent units (GRU) for the encoder in all the settings. We used GRU for the decoders in the first two settings, (1) and (2), while the proposed bi-scale recurrent network was used in the\n4Although tokenization is not necessary for characterlevel modelling, we tokenize the all target side corpora to make comparison against word-level modelling easier.\nlast setting, (3). The encoder has 512 hidden units for each direction (forward and reverse), and the decoder has 1024 hidden units per layer.\nWe train each model using stochastic gradient descent with Adam (Kingma and Ba, 2014). Each update is computed using a minibatch of 128 sentence pairs. The norm of the gradient is clipped with a threshold 1 (Pascanu et al., 2013).\nDecoding and Evaluation We use beamsearch to approximately find the most likely translation given a source sentence. The beam widths are 5 and 15 respectively for the subword-level and character-level decoders. They were chosen based on the translation quality on the development set. The translations are evaluated using BLEU.5\nMultilayer Decoder and Soft-Alignment Mechanism When the decoder is a multilayer recurrent neural network (including a stacked network as well as the proposed bi-scale network), the decoder outputs multiple hidden vectors\u2013{ h1, . . . ,hL } for L layers, at a time. This allows\n5We used the multi-bleu.perl script from Moses.\nan extra degree of freedom in the soft-alignment mechanism (fscore in Eq. (3)). We evaluate using alternatives, including (1) using only hL (slower layer) and (2) using all of them (concatenated).\nEnsembles We also evaluate an ensemble of neural machine translation models and compare its performance against the state-of-the-art phrasebased translation systems on all four language pairs. We decode from an ensemble by taking the average of the output probabilities at each step."}, {"heading": "6 Quantitative Analysis", "text": "Slower Layer for Alignment On En-De, we test which layer of the decoder should be used for computing soft-alignments. In the case of subword-level decoder, we observed no difference between choosing any of the two layers of the decoder against using the concatenation of all the layers (Table 1 (a\u2013b)) On the other hand, with the character-level decoder, we noticed an improvement when only the slower layer (h2) was used for the soft-alignment mechanism (Table 1 (c\u2013g)). This suggests that the soft-alignment mechanism\nbenefits by aligning a larger chunk in the target with a subword unit in the source, and we use only the slower layer for all the other language pairs.\nSingle Models In Table 1, we present a comprehensive report of the translation qualities of (1) subword-level decoder, (2) character-level base decoder and (3) character-level bi-scale decoder, for all the language pairs. We see that the both types of character-level decoder outperform the subword-level decoder for En-Cs and En-Fi quite significantly. On En-De, the character-level base decoder outperforms both the subword-level decoder and the character-level bi-scale decoder, validating the effectiveness of the character-level modelling. On En-Ru, among the single models, the character-level decoders outperform the subword-level decoder, but in general, we observe that all the three alternatives work comparable to each other.\nThese results clearly suggest that it is indeed possible to do character-level translation without explicit segmentation. In fact, what we observed is that character-level translation often surpasses the translation quality of word-level translation. Of course, we note once again that our experiment is restricted to using an unsegmented character sequence at the decoder only, and a further exploration toward replacing the source sentence with an unsegmented character sequence is needed.\nEnsembles Each ensemble was built using four (En-De) and eight (En-{Cs,Ru,Fi}) independent models. The first observation we make is that in all the language pairs, neural machine translation performs comparably to, or often better than, the state-of-the-art non-neural translation system. Furthermore, the character-level decoders outperform the subword-level decoder in all the cases."}, {"heading": "7 Qualitative Analysis", "text": "(1) Can the character-level decoder generate a long, coherent sentence? The translation in characters is dramatically longer than that in words, likely making it more difficult for a recurrent neural network to generate a coherent sentence in characters. This belief turned out to be false. As shown in Fig. 2 (left), there is no significant difference between the subword-level and character-level decoders, even though the lengths of the generated translations are generally 5\u201310 times longer in characters.\n(2) Does the character-level decoder help with rare words? One advantage of character-level modelling is that it can model the composition of any character sequence, thereby better modelling rare morphological variants. We empirically confirm this by observing the growing gap in the average negative log-probability of words between the subword-level and character-level decoders as the frequency of the words decreases. This is shown in Fig. 2 (right) and explains one potential cause behind the success of character-level decoding in our experiments (we define diff(x, y) = x\u2212 y).\n(3) Can the character-level decoder soft-align between a source word and a target character? In Fig. 3 (a), we show an example soft-alignment of a source sentence, \u201cTwo sets of light so close to one another.\u201d It is clear that the character-level translation model well captured the alignment between the source subwords and target characters. We observe that the character-level decoder correctly aligns to \u201clights\u201d and \u201csets of\u201d when generating a German compound word \u201cLichtersets\u201d (see Fig. 3 (b) for the zoomed-in version). This type of behaviour happens similarly between \u201cone another\u201d and \u201ceinander\u201d. Of course, this does not mean that there exists an alignment between a source word and a target character. Rather, this suggests that the internal state of the character-\nlevel decoder, the base or bi-scale, well captures the meaningful chunk of characters, allowing the model to map it to a larger chunk (subword) in the source."}, {"heading": "8 Conclusion", "text": "In this paper, we addressed a fundamental question on whether a recently proposed neural machine translation system can directly handle translation at the level of characters without any word segmentation. We focused on the target side, in which a decoder was asked to generate one character at a time, while soft-aligning between a target character and a source subword. Our extensive experiments, on four language pairs\u2013En-Cs, EnDe, En-Ru and En-Fi\u2013 strongly suggest that it is indeed possible for neural machine translation to translate at the level of characters, and that it actually benefits from doing so.\nOur result has one limitation that we used subword symbols in the source side. However, this has allowed us a more fine-grained analysis, but in the future, a setting where the source side is also represented as a character sequence must be investigated."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano (Bastien et al., 2012). We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs, CIFAR and Samsung. KC thanks the support by Facebook and Google (Google Faculty Award 2016)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1211.5590.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE Transactions on Neural Networks, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent."], "venue": "Advances in Neural Information Processing Systems, pages 932\u2013938.", "citeRegEx": "Bengio et al\\.,? 2001", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "The grammar of words: An introduction to linguistic morphology", "author": ["Geert Booij."], "venue": "Oxford University Press.", "citeRegEx": "Booij.,? 2012", "shortCiteRegEx": "Booij.", "year": 2012}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A Botha", "Phil Blunsom."], "venue": "ICML 2014.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Variablelength word encodings for neural translation models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2088\u20132093.", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Empiricial", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 32nd International Conference on Machine Learning.", "citeRegEx": "Chung et al\\.,? 2015", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Marta R Costa-Juss\u00e0", "Jos\u00e9 AR Fonollosa."], "venue": "arXiv preprint arXiv:1603.00810.", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.Juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0", "author": ["Mathias Creutz", "Krista Lagus"], "venue": "Helsinki University of Technology", "citeRegEx": "Creutz and Lagus.,? \\Q2005\\E", "shortCiteRegEx": "Creutz and Lagus.", "year": 2005}, {"title": "Edinburgh\u2019s phrase-based machine translation systems for wmt-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield."], "venue": "Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, Baltimore, MD, USA,", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "Eu-bridge mt: Combined machine translation", "author": ["Markus Freitag", "Stephan Peitz", "Joern Wuebker", "Hermann Ney", "Matthias Huck", "Rico Sennrich", "Nadir Durrani", "Maria Nadejde", "Philip Williams", "Philipp Koehn"], "venue": null, "citeRegEx": "Freitag et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Freitag et al\\.", "year": 2014}, {"title": "The edinburgh/jhu phrase-based machine translation systems for wmt 2015", "author": ["Barry Haddow", "Matthias Huck", "Alexandra Birch", "Nikolay Bogoychev", "Philipp Koehn."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 126\u2013", "citeRegEx": "Haddow et al\\.,? 2015", "shortCiteRegEx": "Haddow et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter."], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107\u2013116.", "citeRegEx": "Hochreiter.,? 1998", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Chinese word segmentation: A decade review", "author": ["Changning Huang", "Hai Zhao."], "venue": "Journal of Chinese Information Processing, 21(3):8\u201320.", "citeRegEx": "Huang and Zhao.,? 2007", "shortCiteRegEx": "Huang and Zhao.", "year": 2007}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics: Short", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Naver machine translation system for wat 2015", "author": ["Hyoung-Gyu Lee", "JaeSong Lee", "Jun-Seok Kim", "Chang-Ki Lee."], "venue": "Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 69\u201373.", "citeRegEx": "Lee et al\\.,? 2015", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015b", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL, pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "arXiv preprint arXiv:1410.8206.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Subword language modeling with neural networks", "author": ["Tomas Mikolov", "Ilya Sutskever", "Anoop Deoras", "HaiSon Le", "Stefan Kombrink", "J Cernocky."], "venue": "Preprint.", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1312.6026.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Abu-matran at wmt 2015 translation task: Morphological segmentation and web crawling", "author": ["Raphael Rubino", "Tommi Pirinen", "Miquel Espla-Gomis", "N Ljube\u0161ic", "Sergio Ortiz Rojas", "Vassilis Papavassiliou", "Prokopis Prokopidis", "Antonio Toral."], "venue": "Proceed-", "citeRegEx": "Rubino et al\\.,? 2015", "shortCiteRegEx": "Rubino et al\\.", "year": 2015}, {"title": "Continuous space language models", "author": ["Holger Schwenk."], "venue": "Computer Speech & Language, 21(3):492\u2013 518.", "citeRegEx": "Schwenk.,? 2007", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Advances in Neural Information Processing Systems, pages 2368\u20132376.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML\u201911), pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Can we translate letters? In Proceedings of the Second Workshop on Statistical Machine Translation, pages 33\u201339", "author": ["David Vilar", "Jan-T Peter", "Hermann Ney."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Vilar et al\\.,? 2007", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}, {"title": "Edinburgh\u2019s syntax-based systems at wmt 2015", "author": ["Philip Williams", "Rico Sennrich", "Maria Nadejde", "Matthias Huck", "Philipp Koehn."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 199\u2013209.", "citeRegEx": "Williams et al\\.,? 2015", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers", "author": ["Yijun Xiao", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1602.00367.", "citeRegEx": "Xiao and Cho.,? 2016", "shortCiteRegEx": "Xiao and Cho.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems, pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "This has continued with the more recently proposed paradigm of neural machine translation, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary (Jean et al., 2015; Luong et al., 2015b).", "startOffset": 312, "endOffset": 352}, {"referenceID": 25, "context": "This has continued with the more recently proposed paradigm of neural machine translation, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary (Jean et al., 2015; Luong et al., 2015b).", "startOffset": 312, "endOffset": 352}, {"referenceID": 31, "context": "We represent the source side as a sequence of subwords extracted using byte-pair encoding from Sennrich et al. (2015), and vary the target side to be either a sequence of subwords or characters.", "startOffset": 95, "endOffset": 118}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015). This approach aims at building an end-toend neural network that takes as input a source sentence X = (x1, . . . , xTx) and outputs its translation Y = (y1, . . . , yTy), where xt and yt\u2032 are respectively source and target symbols. This neural network is constructed as a composite of an encoder network and a decoder network. The encoder network encodes the input sentence X into its continuous representation. In this paper, we closely follow the neural translation model proposed in Bahdanau et al. (2015) and use a bidirectional recurrent neural network, which consists of two recurrent neural networks.", "startOffset": 8, "endOffset": 540}, {"referenceID": 24, "context": "For other possible implementations, see (Luong et al., 2015a).", "startOffset": 40, "endOffset": 61}, {"referenceID": 3, "context": "Neural networks have been shown to work well with word tokens (Bengio et al., 2001; Schwenk, 2007; Mikolov et al., 2010)", "startOffset": 62, "endOffset": 120}, {"referenceID": 30, "context": "Neural networks have been shown to work well with word tokens (Bengio et al., 2001; Schwenk, 2007; Mikolov et al., 2010)", "startOffset": 62, "endOffset": 120}, {"referenceID": 26, "context": "Neural networks have been shown to work well with word tokens (Bengio et al., 2001; Schwenk, 2007; Mikolov et al., 2010)", "startOffset": 62, "endOffset": 120}, {"referenceID": 31, "context": "but also with finer units, such as subwords (Sennrich et al., 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015).", "startOffset": 44, "endOffset": 112}, {"referenceID": 5, "context": "but also with finer units, such as subwords (Sennrich et al., 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015).", "startOffset": 44, "endOffset": 112}, {"referenceID": 23, "context": "but also with finer units, such as subwords (Sennrich et al., 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015).", "startOffset": 44, "endOffset": 112}, {"referenceID": 6, "context": ", 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015).", "startOffset": 63, "endOffset": 89}, {"referenceID": 5, "context": ", 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015). Although there have been a number of previous research reporting the use of neural networks with characters (see, e.g., Mikolov et al. (2012)), the dominant approach has been to preprocess the text into a sequence of symbols, each associated with a sequence of characters, after which the neural network is presented with those symbols rather than with characters.", "startOffset": 8, "endOffset": 278}, {"referenceID": 5, "context": ", 2015; Botha and Blunsom, 2014; Luong et al., 2013) as well as symbols resulting from compression/encoding (Chitnis and DeNero, 2015). Although there have been a number of previous research reporting the use of neural networks with characters (see, e.g., Mikolov et al. (2012)), the dominant approach has been to preprocess the text into a sequence of symbols, each associated with a sequence of characters, after which the neural network is presented with those symbols rather than with characters. More recently in the context of neural machine translation, two research groups have proposed to directly use characters. Kim et al. (2015) proposed to represent each word not as a single integer index as before, but as a sequence of characters, and use", "startOffset": 8, "endOffset": 641}, {"referenceID": 32, "context": "a convolutional network followed by a highway network (Srivastava et al., 2015) to extract a continuous representation of the word.", "startOffset": 54, "endOffset": 79}, {"referenceID": 9, "context": "This approach, which effectively replaces the embedding function ex, was adopted by Costa-Juss\u00e0 and Fonollosa (2016) for neural machine translation.", "startOffset": 84, "endOffset": 117}, {"referenceID": 9, "context": "This approach, which effectively replaces the embedding function ex, was adopted by Costa-Juss\u00e0 and Fonollosa (2016) for neural machine translation. Similarly, Ling et al. (2015b) use a bidirectional recurrent neural network to replace the embedding functions ex and ey to respectively encode a character sequence to and from the corresponding continuous word representation.", "startOffset": 84, "endOffset": 180}, {"referenceID": 10, "context": "This word segmentation procedure can be as simple as tokenization followed by some punctuation normalization, but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance (Creutz and Lagus, 2005; Huang and Zhao, 2007).", "startOffset": 218, "endOffset": 264}, {"referenceID": 16, "context": "This word segmentation procedure can be as simple as tokenization followed by some punctuation normalization, but also can be as complicated as morpheme segmentation requiring a separate model to be trained in advance (Creutz and Lagus, 2005; Huang and Zhao, 2007).", "startOffset": 218, "endOffset": 264}, {"referenceID": 18, "context": "ent approach was proposed by Lee et al. (2015), where they explicitly mark each character with its relative location in a word (e.", "startOffset": 29, "endOffset": 47}, {"referenceID": 4, "context": "\u201d (Booij, 2012).", "startOffset": 2, "endOffset": 15}, {"referenceID": 35, "context": "Indeed, Vilar et al. (2007) reported worse performance when the character sequence was directly used by a phrasebased machine translation system.", "startOffset": 8, "endOffset": 28}, {"referenceID": 2, "context": "wide adoption of word-level modelling is due to the difficulty in modelling long-term dependencies with recurrent neural networks (Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 130, "endOffset": 169}, {"referenceID": 15, "context": "wide adoption of word-level modelling is due to the difficulty in modelling long-term dependencies with recurrent neural networks (Bengio et al., 1994; Hochreiter, 1998).", "startOffset": 130, "endOffset": 169}, {"referenceID": 10, "context": ", Creutz and Lagus (2005) for Finnish and other morphologically rich languages and Huang and Zhao (2007) for Chinese).", "startOffset": 2, "endOffset": 26}, {"referenceID": 10, "context": ", Creutz and Lagus (2005) for Finnish and other morphologically rich languages and Huang and Zhao (2007) for Chinese).", "startOffset": 2, "endOffset": 105}, {"referenceID": 5, "context": "Each of those variants will be then a composite of the lexeme vector (shared across these variants) and morpheme vectors (shared across words sharing the same suffix, for example) (Botha and Blunsom, 2014).", "startOffset": 180, "endOffset": 205}, {"referenceID": 14, "context": "Furthermore, in recent years, we have learned how to build and train a recurrent neural network that can well capture long-term dependencies by using more sophisticated activation functions, such as long short-term memory units (Hochreiter and Schmidhuber, 1997) and gated recurrent units (Cho et al.", "startOffset": 228, "endOffset": 262}, {"referenceID": 7, "context": "Furthermore, in recent years, we have learned how to build and train a recurrent neural network that can well capture long-term dependencies by using more sophisticated activation functions, such as long short-term memory units (Hochreiter and Schmidhuber, 1997) and gated recurrent units (Cho et al., 2014).", "startOffset": 289, "endOffset": 307}, {"referenceID": 21, "context": "Ling et al. (2015b) indeed states that \u201c[m]uch of the prior information regarding morphology, cognates and rare word translation among others, should be incorporated.", "startOffset": 0, "endOffset": 20}, {"referenceID": 26, "context": "For instance, Mikolov et al. (2012) and Sutskever et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 26, "context": "For instance, Mikolov et al. (2012) and Sutskever et al. (2011) trained a recurrent neural network language model (RNN-LM) on character sequences.", "startOffset": 14, "endOffset": 64}, {"referenceID": 26, "context": "For instance, Mikolov et al. (2012) and Sutskever et al. (2011) trained a recurrent neural network language model (RNN-LM) on character sequences. The latter showed that it is possible to generate sensible text sequences by simply sampling a character at a time from this model. More recently, Zhang et al. (2015) and Xiao and Cho (2016) successfully applied a convolutional net and a convolutionalrecurrent net respectively to character-level document classification without any explicit segmentation.", "startOffset": 14, "endOffset": 314}, {"referenceID": 26, "context": "For instance, Mikolov et al. (2012) and Sutskever et al. (2011) trained a recurrent neural network language model (RNN-LM) on character sequences. The latter showed that it is possible to generate sensible text sequences by simply sampling a character at a time from this model. More recently, Zhang et al. (2015) and Xiao and Cho (2016) successfully applied a convolutional net and a convolutionalrecurrent net respectively to character-level document classification without any explicit segmentation.", "startOffset": 14, "endOffset": 338}, {"referenceID": 8, "context": "neural network, inspired by the gated-feedback network from Chung et al. (2015), called a biscale recurrent neural network.", "startOffset": 60, "endOffset": 80}, {"referenceID": 31, "context": "For evaluation, we represent a source sentence as a sequence of subword symbols extracted by bytepair encoding (BPE, Sennrich et al. (2015)) and a", "startOffset": 117, "endOffset": 140}, {"referenceID": 12, "context": "(1) Freitag et al. (2014). (2, 6) Williams et al.", "startOffset": 4, "endOffset": 26}, {"referenceID": 12, "context": "(1) Freitag et al. (2014). (2, 6) Williams et al. (2015).", "startOffset": 4, "endOffset": 57}, {"referenceID": 11, "context": "(3, 5) Durrani et al. (2014). (4) Haddow et al.", "startOffset": 7, "endOffset": 29}, {"referenceID": 11, "context": "(3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al.", "startOffset": 7, "endOffset": 55}, {"referenceID": 11, "context": "(3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al. (2015).", "startOffset": 7, "endOffset": 81}, {"referenceID": 19, "context": "descent with Adam (Kingma and Ba, 2014).", "startOffset": 18, "endOffset": 39}, {"referenceID": 28, "context": "The norm of the gradient is clipped with a threshold 1 (Pascanu et al., 2013).", "startOffset": 55, "endOffset": 77}, {"referenceID": 1, "context": "The authors would like to thank the developers of Theano (Bastien et al., 2012).", "startOffset": 57, "endOffset": 79}], "year": 2016, "abstractText": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder\u2013 decoder with a subword-level encoder and a character-level decoder on four language pairs\u2013En-Cs, En-De, En-Ru and En-Fi\u2013 using the parallel corpora from WMT\u201915. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.", "creator": "TeX"}}}