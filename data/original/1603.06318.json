{"id": "1603.06318", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Harnessing Deep Neural Networks with Logic Rules", "abstract": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.", "histories": [["v1", "Mon, 21 Mar 2016 03:33:20 GMT  (172kb,D)", "http://arxiv.org/abs/1603.06318v1", "18 pages, 3 figures. In submission"], ["v2", "Thu, 14 Apr 2016 05:28:21 GMT  (174kb,D)", "http://arxiv.org/abs/1603.06318v2", "Fix typos; add more references and disucssions (Sec.6 etc). 19 pages, 3 figures. In submission"], ["v3", "Tue, 19 Jul 2016 23:30:48 GMT  (174kb,D)", "http://arxiv.org/abs/1603.06318v3", "To appear in ACL2016"], ["v4", "Tue, 15 Nov 2016 21:41:21 GMT  (174kb,D)", "http://arxiv.org/abs/1603.06318v4", "Fix typos and experiment setting"]], "COMMENTS": "18 pages, 3 figures. In submission", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL stat.ML", "authors": ["zhiting hu", "xuezhe ma", "zhengzhong liu", "eduard h hovy", "eric p xing"], "accepted": true, "id": "1603.06318"}, "pdf": {"name": "1603.06318.pdf", "metadata": {"source": "CRF", "title": "Harnessing Deep Neural Networks with Logic Rules", "authors": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eric P. Xing"], "emails": ["zhitingh@cs.cmu.edu", "xuezhem@cs.cmu.edu", "liu@cs.cmu.edu", "hovy@cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks provide a powerful mechanism for learning patterns from massive data, achieving new levels of performance on image classification (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), machine translation (Bahdanau et al., 2014), playing strategic board games (Silver et al., 2016), and so forth.\nDespite the impressive advances, the widely-used DNN methods still have limitations. The high predictive accuracy has heavily relied on large amounts of labeled data; and the purely data-driven learning can lead to uninterpretable and sometimes counter-intuitive results (Szegedy et al., 2014; Nguyen et al., 2015). It is also difficult to encode human intention to guide the models to capture desired patterns, without expensive direct supervision or ad-hoc initialization.\nOn the other hand, the cognitive process of human beings have indicated that people learn not only from concrete examples (as DNNs do) but also from different forms of general knowledge and rich experiences (Minksy, 1980; Lake et al., 2015). Logic rules provide a flexible declarative language for communicating high-level cognition and expressing structured knowledge. It is therefore desirable to integrate logic rules into DNNs, to transfer human intention and domain knowledge to neural models, and regulate the learning process.\nar X\niv :1\n60 3.\n06 31\n8v 1\n[ cs\n.L G\n] 2\n1 M\nIn this paper, we present a framework capable of enhancing general types of neural networks, such as convolutional networks (CNNs) and recurrent networks (RNNs), on various tasks, with logic rule knowledge. Combining symbolic representations with neural methods have been considered in different contexts. Neural-symbolic systems (Garcez et al., 2012) construct a network from a given rule set to execute reasoning. To exploit a priori knowledge in general neural architectures, recent work augments each raw data instance with useful features (Collobert et al., 2011), while network training, however, is still limited to instance-label supervision and suffers from the same issues mentioned above. Besides, a large variety of structural knowledge cannot be naturally encoded in the feature-label form.\nOur framework enables a neural network to learn simultaneously from labeled instances as well as logic rules, through an iterative rule knowledge distillation procedure that transfers the structured information encoded in the logic rules into the network parameters. Since the general logic rules are complementary to the specific data labels, a natural \u201cside-product\u201d of the integration is the support for semi-supervised learning where unlabeled data can be used to better absorb the logical knowledge. Methodologically, our approach can be seen as a combination of the knowledge distillation (Hinton et al., 2015; Bucilu et al., 2006) and the posterior regularization (PR) method (Ganchev et al., 2010). In particular, at each iteration we adapt the posterior constraint principle from PR to construct a rule-regularized teacher, and train the student network of interest to imitate the predictions of the teacher network. We leverage soft logic to support flexible rule encoding.\nWe apply the proposed framework on both CNN and RNN, and deploy on the task of sentiment analysis (SA) and named entity recognition (NER), respectively. With only a few (one or two) very intuitive rules, the enhanced networks strongly improve over their basic forms (without rules), and achieve better or comparable performance to state-of-theart models which typically have more parameters and complicated architectures.\nTo the best of our knowledge, this is the first work to integrate logic rules with the general workhorse types of deep neural networks in a principled framework. The encouraging results indicate that our method can be potentially useful for other NLP tasks and application domains, as well as incorporating richer types of human knowledge."}, {"heading": "2 Related Work", "text": "Given the intuitive value of combining logic rules and neural networks, various distinct forms of such combination have been considered in different contexts. Neural-symbolic systems (Garcez et al., 2012), such as KBANN (Towell et al., 1990) and CILP++ (Franc\u0327a et al., 2014), construct network architectures from given rules to perform reasoning and knowledge acquisition. A related line of research, such as Markov logic networks (Richardson and Domingos, 2006), derives probabilistic graphical models (rather than neural networks) from the rule set.\nWith the recent success of deep neural networks in a vast variety of application domains, it is increasingly desirable to incorporate structured logic knowledge into general types of networks to harness flexibility and reduce unpredictability. Recent work that trains on extra\nfeatures from domain knowledge (Collobert et al., 2011), while producing improved results, does not go beyond the data-label paradigm. Kulkarni et al. (2015) uses a specialized training procedure with careful ordering of training instances to obtain an interpretable neural layer of an image network. Though there do exist general frameworks that allow encoding structured constraints on latent variable models (Ganchev et al., 2010; Zhu et al., 2014; Liang et al., 2009), they either are not directly applicable to the NN case, or could yield inferior performance as in our empirical study.\nOur proposed approach is distinct in that we use an iterative rule distillation process to effectively transfer rich structured knowledge, expressed in the declarative first-order logic language, into parameters of general neural networks. We show that the proposed approach strongly outperforms an extensive array of other either ad-hoc or general integration methods."}, {"heading": "3 Method", "text": "In this section we present our framework which encapsulates the logical structured knowledge into a neural network. This is achieved by forcing the network to emulate the predictions of a rule-regularized teacher, and evolving both models iteratively throughout training (section 3.2). The process is agnostic to the network architecture, and thus applicable to general types of neural models including CNNs and RNNs. We construct the teacher network in each iteration by adapting the posterior regularization principle in our logical constraint setting (section 3.3), where our formulation provides a closed-form solution. Figure 1 shows an overview of the proposed framework."}, {"heading": "3.1 Learning Resources: Instances and Rules", "text": "Our approach allows neural networks to learn from both specific examples and general rules. Here we give the settings of these \u201clearning resources\u201d.\nAssume we have input variable x \u2208 X and target variable y \u2208 Y. For clarity, we focus on K-way classification, where Y = \u2206K is the K-dimensional probability simplex and y \u2208 {0, 1}K \u2282 Y is a one-hot encoding of the class label. However, our method specification can straightforwardly be applied to other contexts such as regression and sequence learning (e.g., NER tagging, which is typically a sequence of classification decisions). The training data D = {(xn,yn)}Nn=1 is a set of instantiations of (x,y).\nFurther consider a set of first-order logic (FOL) rules with confidences R = {(Rl, \u03bbl)}Ll=1, where Rl is the lth rule over the above input-target space (X ,Y), and \u03bbl \u2208 [0,\u221e] is the confidence level with \u03bbl = \u221e indicating a hard rule, i.e., all groundings are required to be true (=1). Given a set of examples (X,Y) \u2282 (X ,Y) (e.g., a minibatch from D), the set of groundings of Rl are denoted as {rlg(X,Y)}Glg=1. In practice a rule grounding is typically relevant to only a single or subset of examples, though here we give the most general form on the entire set.\nWe encode the FOL rules using soft logic (Bach et al., 2015) for flexible encoding and stable optimization. Specifically, soft logic allows continuous truth values from the interval [0, 1] instead of {0, 1}, and the Boolean logic operators are reformulated as:\nA&B = max{A+B \u2212 1, 0} A \u2228B = min{A+B, 1} \u00acA = 1\u2212A\n(1)"}, {"heading": "3.2 Rule Knowledge Distillation", "text": "A neural network defines a conditional probability p\u03b8(y|x) by using a softmax output layer that produces a K-dimensional soft prediction vector denoted as \u03c3\u03b8(x). The network is parameterized by weights \u03b8. Previous neural network training has been to iteratively update \u03b8 to produce the correct labels of training instances. To integrate the information encoded in the rules, we propose to train the network to also imitate the outputs of a rule-regularized projection of p\u03b8(y|x), denoted as q(y|x), which explicitly includes rule constraints as regularization terms. In each iteration q is constructed by projecting p\u03b8 into a subspace constrained by the rules, and thus has desirable properties. We present the construction in the next section. The prediction behavior of q reveals the information of the regularized subspace and structured rules. Emulating the q outputs serves to transfer this knowledge into p\u03b8. The new objective is then formulated as a balancing between imitating the soft predictions of q and predicting the true hard labels:\n\u03b8(t+1) = arg min \u03b8\u2208\u0398\n1\nN N\u2211 n=1 (1\u2212 \u03c0)`(yn,\u03c3\u03b8(t)(xn))\n+ \u03c0`(s(t)n ,\u03c3\u03b8(t)(xn)),\n(2)\nwhere ` denotes the loss function selected according to specific applications (e.g., typically the cross entropy loss for classification); s (t) n is the soft prediction vector of q on xn at iteration t; and \u03c0 is the imitation parameter calibrating the relative importance of the two objectives.\nA similar imitation procedure has been used in other settings such as model compression (Bucilu et al., 2006; Hinton et al., 2015) where the process is termed distillation. Following them we call p\u03b8(y|x) the \u201cstudent\u201d and q(y|x) the \u201cteacher\u201d, which can be intuitively explained in analogous to human education where a teacher is aware of systematic general rules and she instructs students by providing her solutions to particular questions (i.e., the soft predictions). Another noticeable difference from previous distillation work, where teacher is obtained beforehand and student is trained thereafter, is that our teacher and student are learned simultaneously during training.\nThough it is possible to combine a network with rule constraints by projecting the network to the rule-regularized subspace after it is fully trained as before with only data-label instances, or by optimizing projected network directly, we found our iterative teacher-student distillation approach provides a much superior performance, as shown in the experiments. Moreover, since p\u03b8 distills the rule information into the weights \u03b8 instead of relying on explicit rule representations, we can use p\u03b8 for predicting new examples at test time when the rule assessment is expensive or even unavailable (i.e., the privileged information setting (Lopez-Paz et al., 2016)) while still enjoying the benefit of integration. Besides, the second loss term in Eq.(2) can be augmented with rich unlabeled data in addition to the labeled examples, which enables semi-supervised learning for better absorbing the rule knowledge."}, {"heading": "3.3 Teacher Network Construction", "text": "We now proceed to construct the teacher network q(y|x) at each iteration from p\u03b8(y|x). The iteration index t is omitted for clarity. We adapt the posterior regularization principle in our logic constraint setting. Our formulation ensures a closed-form solution for q and thus avoids any significant increases in computational overhead.\nRecall the set of FOL rules R = {(Rl, \u03bbl)}Ll=1. Our goal is to find the optimal q that fits the rules while at the same time staying close to p\u03b8. For the first property, we apply a commonlyused strategy that imposes the rule constraints on q through an expectation operator. That is, for each grounding (indexed by g) of each rule (indexed by l) on (X,Y), we expect Eq(Y|X)[rlg(X,Y)] = 1 (with confidence \u03bbl). The constraints define a rule-regularized space of all valid distributions. For the second property, we measure the closeness between q and p\u03b8 with KL-divergence, and wish to minimize it. Combining the two factors together and further allowing slackness for the constraints, we finally get the following optimization\nproblem:\nmin q,\u03be\u22650 KL(q(Y|X)\u2016p\u03b8(Y|X)) + C \u2211 l,gl \u03bel,gl\ns.t. \u03bbl(1\u2212 Eq[rl,gl(X,Y)]) \u2264 \u03bel,gl gl = 1, . . . , Gl, l = 1, . . . , L,\n(3)\nwhere \u03bel,gl \u2265 0 is the slack variable for respective logic constraint; and C is the regularization parameter. The problem can be seen as projecting p\u03b8 into the constrained subspace. The problem is convex and can be efficiently solved in its dual form with closed-form solutions. We provide the detailed derivation in the supplementary materials and directly give the solution here:\nq\u2217(Y|X) \u221d p\u03b8(Y|X) exp \u2212\u2211 l,gl C\u03bbl(1\u2212 rl,gl(X,Y))  . (4) Intuitively, a strong rule with large \u03bbl will lead to low probabilities of predictions that fail to meet the constraints.\nOur framework is related to the posterior regularization (PR) method (Ganchev et al., 2010) which places constraints over model posterior in unsupervised setting. In classification, our optimization procedure is analogous to the modified EM algorithm for PR, by using crossentropy loss in Eq.(2) and evaluating the second loss term on unlabeled data differing from D, so that Eq.(4) corresponds to the E-step and Eq.(2) is analogous to the M-step. This sheds light from another perspective on why our framework would work. However, we found in our experiments (section 5) that to produce strong performance it is crucial to use the same labeled data in the two losses of Eq.(2) so as to form a trade-off between imitating soft predictions and predicting correct hard labels.\nInference and Implementation The procedure of iterative distilling optimization of our framework is summarized in Algorithm 1. During training we need to compute the soft predictions of q at each iteration, which is straightforward if the rule constraints in Eq.(4) are factored in the same way as the base neural model p\u03b8. If the constraints introduce additional dependencies, e.g., bi-gram dependency as the transition rule in the NER task (section 4.2), we can use dynamic programming for efficient computation; for higher-order constraints (e.g., the listing rule in NER), we approximate using Gibbs sampling that iteratively samples from q(yi|y\u2212i,x) for each position i. If the constraints span multiple instances, we group the relevant instances in minibatches for joint inference (and randomly break some dependencies when a group is too large).\nAt test time we can use either the distilled student network p\u03b8, or the teacher network q after a final projection. Our empirical results show that both models substantially improve over the base network (trained using only data-label instances), while in general q performs better than p\u03b8. Besides, q is more suitable when the logic rules are over multiple examples, requiring joint inference. In contrast, as mentioned above, p\u03b8 is useful when rule evaluation is expensive or impossible at prediction time.\nAlgorithm 1 Harnessing NN with Rules Input: The training data D = {(xn,yn)}Nn=1, The rule set R = {(Rl, \u03bbl)}Ll=1, Parameters: \u03c0 \u2013 imitation parameter C \u2013 regularization strength 1: Initialize neural network parameter \u03b8 2: repeat 3: Sample a minibatch (X,Y) \u2282 D 4: Construct teacher network q with Eq.(4) 5: Distill knowledge into p\u03b8 by updating \u03b8 with Eq.(2) 6: until convergence Output: Distilled student network p\u03b8 and teacher network q"}, {"heading": "4 Applications", "text": "We have presented our framework that is general enough to improve various types of neural networks with rules, and easy to use in that users are allowed to impose their knowledge and intentions through the declarative first-order logic. In this section we illustrate the versatility of our approach by applying it on two workhorse network architectures, i.e., convolutional network and recurrent network, on two representative applications, i.e., sentence-level sentiment analysis which is a classification problem, and named entity recognition which is a sequence learning problem.\nFor each task, we first briefly describe the base neural network. Since we are not focusing on tuning network architectures, we largely use the same or similar networks to previous successful neural models. We then design the linguistically-motivated rules to be integrated."}, {"heading": "4.1 Sentiment Classification", "text": "Sentence-level sentiment analysis is to identify the sentiment (e.g., positive or negative) underlying an individual sentence. The task is crucial for many opinion mining applications. One challenging point of the task is to capture the contrastive sense (e.g., by conjunction \u201cbut\u201d) within a sentence.\nBase Network We use the single-channel convolutional network proposed in (Kim, 2014). The simple model has achieved compelling performance on various sentiment classification benchmarks. The network contains a convolutional layer on top of word vectors of a given sentence, followed by a max-over-time pooling layer and then a fully-connected layer with softmax output activation. A convolution operation is to apply a filter to word windows. Multiple filters with varying window sizes are used to obtain multiple features. Figure 2, left panel, shows the network architecture.\nLogic Rules One difficulty for the plain neural network is to identify contrastive sense in order to capture the dominant sentiment precisely. Conjunction \u201cbut\u201d is one of the strong\nindicators for such sentiment changes in a sentence, where the sentiment of clauses following \u201cbut\u201d generally dominates. We thus consider sentences S with an \u201cA-but-B\u201d structure, and expect the sentiment of the whole sentence to be consistent with the sentiment of clause B. The logic rule is written as:\nhas-\u2018A-but-B\u2019-structure(S)\u21d2 \u2016\u03c3\u03b8(S)\u2212 \u03c3\u03b8(B)\u20162, (5)\nwhere we use the `2 distance between the softmax outputs as a measure for the closeness of the sentiment predictions of S and B. Note that the distance takes value in [0, 1] which is a proper soft truth value."}, {"heading": "4.2 Named Entity Recognition", "text": "NER is to locate and classify elements in text into entity categories such as \u201cpersons\u201d and \u201corganizations\u201d. It is an essential first step for downstream language understanding applications. The task assigns to each word a named entity tag in an \u201cX-Y\u201d format where X is one of BIEOS (Beginning, Inside, End, Outside, and Singleton) and Y is the entity category. A valid tag sequence has to follow certain constraints by the definition of the tagging scheme. Besides, text with structures (e.g., lists) within or across sentences can usually expose some consistency patterns.\nBase Network The base network has a similar architecture with the bi-directional LSTM recurrent network (called BLSTM-CNN) proposed in (Chiu and Nichols, 2015) for NER which has outperformed most of previous neural models. The model uses a CNN and pretrained word vectors to capture character- and word-level information, respectively. These features are then fed into a bi-directional RNN with LSTM units for sequence tagging. Compared to (Chiu and Nichols, 2015) we omit the character type and capitalization features, as well as the additive transition matrix in the output layer. Figure 2, right panel, shows the network architecture.\nLogic Rules The base network largely makes independent tagging decisions at each position, ignoring the constraints on successive labels for a valid tag sequence (e.g., I-ORG cannot follow B-PER). In contrast to recent work (Lample et al., 2016) which adds a conditional random field (CRF) to capture bi-gram dependencies between outputs, we instead apply logic rules which does not introduce extra parameters to learn. An example rule is:\nEqual(yi\u22121, I-ORG)\u21d2 \u00ac Equal(yi,B-PER) (6)\nThe confidence levels are set to \u221e to prevent any violation.\nWe further leverage the list structures within and across sentences of the same documents. Specifically, named entities at corresponding positions in a list are likely to be in the same categories. For instance, in \u201c1. Juventus, 2. Barcelona, 3. ...\u201d we know \u201cBarcelona\u201d must be an organization rather than a location, since its counterpart entity \u201cJuventus\u201d is an organization. We describe our simple procedure for identifying lists and counterparts in the supplementary materials. The logic rule is encoded as:\nis-counterpart(A,B) = \u2016c(\u03c3\u03b8(A))\u2212 c(\u03c3\u03b8(B))\u20162, (7)\nwhere c collapses the probability mass on the labels with the same categories into a single probability, yielding a vector with length equaling to the number of categories. The list rule can span multiple sentences (within the same document), thus we use q at test time for joint prediction."}, {"heading": "5 Experiments", "text": "We validate the proposed framework by evaluating its applications of sentiment classification and named entity recognition on a variety of public benchmarks. By integrating the simple yet effective rules with the base networks, we obtain substantial improvements on both tasks and achieve state-of-the-art or comparable results to previous best-performing systems. Comparison with a diverse set of other rule integration methods demonstrates the unique effectiveness of our framework. Our approach also shows promising potentials in the semisupervised learning and sparse data context.\nThroughout the experiments we set the imitation parameter to \u03c0 = 0.1, the regularization parameter to C = 400. These values are chosen on the dev set of the SST2 sentiment data (see below). The confidence levels of rules are set to \u03bbl = 1, except for hard constraints whose confidence is \u221e. For neural network configuration, we largely followed the reference work as specified in the following respective sections. All experiments were performed on a Linux machine with eight 4.0GHz CPU cores, one Tesla K40c GPU, and 32GB RAM. We implemented neural networks using Theano 1, a popular deep learning platform.\n1http://deeplearning.net/software/theano"}, {"heading": "5.1 Sentiment Classification", "text": ""}, {"heading": "5.1.1 Setup", "text": "We test our method on a number of commonly used benchmarks, including 1) SST2, Stanford Sentiment Treebank (Socher et al., 2013) which contains 2 classes (negative and positive), and 6920/872/1821 sentences in the train/dev/test sets respectively. Following (Kim, 2014) we train models on both sentences and phrases since all labels are provided. 2) MR (Pang and Lee, 2005), a set of 10,662 one-sentence movie reviews with negative or positive sentiment. 3) CR (Hu and Liu, 2004), customer reviews of various products, containing 2 classes and 3,775 instances.\nFor the base neural network we use the \u201cnon-static\u201d version in (Kim, 2014) with the exact same configurations. Specifically, word vectors are initialized using word2vec (Mikolov et al., 2013) and fine-tuned throughout training, and the neural parameters are trained using SGD with the Adadelta update rule (Zeiler, 2012)."}, {"heading": "5.1.2 Results", "text": "Table 1 shows the sentiment classification performance. Row 1 and Row 2 compare the base network with the one enhanced by our framework with the \u201cbut\u201d-rule (Eq.(5)). We see that our method provides a strong boost on accuracy over all three datasets. For instance, on the SST2 dataset we obtain 2.1% absolute improvement. Rows 3-10 show the accuracy of recent top-performing methods. On the MR and CR datasets, our model achieves the state-of-theart performance. On SST2, MVCNN (Yin and Schutze, 2015) (Row 4) is the only system that shows a slightly better result than ours. Their neural network has combined diverse\nversions of pre-trained word embeddings (while we use only word2vec) and has contained more neural layers and parameters than our model.\nTo further investigate the effectiveness of our framework in integrating structured rule knowledge, we compare with an extensive array of other possible integration approaches. Table 2 lists these methods and their performance on the SST2 sentiment classification task. We see that: 1) Although all methods lead to different degrees of improvement, our framework outperforms all other competitors with a large margin. 2) The distilled student network \u201c-Rule-p\u201d achieves much superior accuracy compared to the base CNN, as well as \u201c-project\u201d and \u201c-opt-project\u201d which explicitly project the CNN model to the ruleconstrained subspace. We have observed similar results on the MR and CR datasets\u2014the student network achieves an accuracy of 81.59% and 85.22% on MR and CR, respectively, greatly improving over the base CNN as shown in Row 1 Table 1. This validates that our knowledge distillation procedure (section 3.2) can indeed transfer the structured knowledge into the neural parameters effectively. 3) The teacher network \u201c-Rule-q\u201d further improves over the student network \u201c-Rule-p\u201d, though the student network is more widely applicable in certain contexts as discussed in section 3.2.\nWe next explore the performance of our framework with varying numbers of labeled instances as well as the effect of exploiting unlabeled data. Intuitively, with less labeled examples we expect the general rules would contribute more to the performance, and unlabeled data should help better learn from the rules. This can be a useful property especially when data are sparse and labels are expensive to obtain. Table 3 shows the results. The subsampling is conducted on the sentence level. That is, for instance, in \u201c5%\u201d we first selected 5% training sentences uniformly at random, then trained the models on these sentences as well as their phrases. The results verify our expectations. 1) Rows 1-3 give the accuracy of only using data-label subsets for training. In every setting our methods consistently outperform the base CNN. 2) \u201c-Rule-q\u201d provides higher improvement on 5% data\n(with margin 2.6%) than on larger data (e.g., 2.3% on 10% data, and 2.0% on 30% data), showing promising potential in the sparse data context. 3) By adding unlabeled instances for semi-supervised learning, we get further improved accuracy. 4) \u201c-semi-PR\u201d is the posterior regularization (Ganchev et al., 2010) which imposes the rule constraint only through unlabeled data during training. Our iterative distillation framework consistently provides substantially better results."}, {"heading": "5.2 Named Entity Recognition", "text": ""}, {"heading": "5.2.1 Setup", "text": "We evaluate on the well-established CoNLL-2003 NER benchmark (Tjong Kim Sang and De Meulder, 2003), which contains 14,987/3,466/3,684 sentences and 204,567/51,578/46,666 tokens in train/dev/test sets, respectively. The dataset includes 4 categories, i.e., person, location, organization, and misc. BIOES tagging scheme is used.\nWe use the mostly same configurations for the BLSTM network as in (Chiu and Nichols, 2015), except that, besides the slight architecture difference (section 4.2), we apply Adadelta for parameter updating. GloVe (Pennington et al., 2014) word vectors are used to initialize word features."}, {"heading": "5.2.2 Results", "text": "Table 4 presents the performance on the NER task. By incorporating the bi-gram transition rules (Row 2), we obtain 1.56 improvement in F1 score that outperforms all previous neural based methods (Rows 4-8), including the BLSTM-CRF model (Lample et al., 2016) which applies a conditional random field (CRF) on top of a BLSTM model in order to capture the transition patterns and encourage valid sequences. In contrast, our method implements\nthe desired constraints in a more straightforward way by using the declarative logic rule language, and at the same time does not introduce extra model parameters to learn. Further integration of the list rule (Row 3) provides a second boost in performance, achieving an F1 score very close to the best-performing system Joint-NER-EL (Luo et al., 2015) (Row 9) which is a probabilistic graphical model based method optimizing NER and entity linking jointly and using large amount of external resources."}, {"heading": "6 Discussion", "text": "We have developed a framework which combines deep neural networks with first-order logic rules to allow integrating human knowledge and intentions into the neural models. In particular, we proposed an iterative distillation procedure that transfers the structured information of logic rules into the weights of neural networks. The transferring is done via a teacher network constructed using the posterior regularization principle. Our framework is general and applicable to various types of neural architectures. With a few intuitive rules, our framework significantly improves base networks on sentiment analysis and named entity recognition, demonstrating the practical significance of our approach.\nThe encouraging results indicate a strong potential of our approach on improving other NLP tasks and application domains. We plan to explore more applications and incorporate more structured knowledge in neural networks. We also would like to improve our framework to automatically learn the importance of different rules, and derive new rules from data."}, {"heading": "A Appendix", "text": "A.1 Solving Problem Eq.(3), Section 3.3\nWe provide the detailed derivation for solving the problem in Eq.(3), Section 3.3, which we repeat here:\nmin q,\u03be\u22650 KL(q(Y|X)\u2016p\u03b8(Y|X)) + C \u2211 l,gl \u03bel,gl\ns.t. \u03bbl(1\u2212 Eq[rl,gl(X,Y)]) \u2264 \u03bel,gl gl = 1, . . . , Gl, l = 1, . . . , L,\n(A.1)\nThe following derivation is largely adapted from (Ganchev et al., 2010) for the logic rule constraint setting, with some reformulation that produces closed-form solution.\nThe Lagrangian is\nmax \u00b5\u22650,\u03b7\u22650,\u03b1\u22650 min q(y,\u03bb),\u03be L, (A.2)\nwhere L = KL(q(Y|X)\u2016p\u03b8(Y|X)) + \u2211 l,gl (C + \u00b5l,gl)\u03bel,gl\n+ \u2211 l,gl \u03b7l,gl (Eq[\u03bbl(1\u2212 rl,gl(X,Y))]\u2212 \u03bel,gl) + \u03b1( \u2211 Y q(Y|X)\u2212 1) (A.3)\nSolving Eq.(A.2), we obtain \u2207qL = log q(Y|X) + 1\u2212 log p\u03b8(Y|X) + \u2211 l,gl \u03b7l,gl [\u03bbl(1\u2212 rl,gl(X,Y))] + \u03b1 = 0\n=\u21d2 q(Y|X) = p\u03b8(Y|X) exp {\u2212\n\u2211 l \u03b7l\u03bbl(1\u2212 rl,gl(X,Y))} e exp(\u03b1)\n(A.4)\n\u2207\u03bel,glL = C + \u00b5l,gl \u2212 \u03b7l,gl = 0 =\u21d2 \u00b5l,gl = C \u2212 \u03b7l,gl (A.5)\n\u2207\u03b1L = \u2211 Y\np\u03b8(Y|X) exp { \u2212 \u2211\nl,gl \u03b7l,gl\u03bbl(1\u2212 rl,gl(X,Y)) } e exp(\u03b1) \u2212 1 = 0\n=\u21d2 \u03b1 = log\n\u2211Y p(Y|X) exp { \u2212 \u2211 l,gl \u03b7l,gl\u03bbl(1\u2212 rl,gl(X,Y)) } e  (A.6)\nLet Z\u03b7 = \u2211 Y p(Y|X) exp { \u2212 \u2211 l,gl \u03b7l,gl\u03bbl(1\u2212 rl,gl(X,Y)) } . Plugging \u03b1 into L\nL = \u2212 logZ\u03b7 + \u2211 l,gl (C + \u00b5l,gl)\u03bel,gl \u2212 \u2211 l,gl \u03b7l,gl\u03bel,gl\n= \u2212 logZ\u03b7 (A.7)\nSince Z\u03b7 monotonically decreases as \u03b7 increases, and from Eq.(A.6) we have \u03b7l,gl \u2264 C, therefore:\nmax C\u2265\u03b7\u22650\n\u2212 logZ\u03b7\n=\u21d2 \u03b7\u2217l,gl = C (A.8)\nPlugging Eqs.(A.6) and (A.8) into Eq.(A.5) we obtain the solution of q as in Eq.(4).\nA.2 Identifying Lists for NER\nWe design a simple pattern-matching based method to identify lists and counterparts in the NER task. We ensure high precision and do not expect high recall. In particular, we only retrieve lists that with the pattern \u201c1. ... 2. ... 3. ...\u201d (i.e., indexed by numbers), and \u201c- ... - ... - ...\u201d (i.e., each item marked with \u201c-\u201d). We require at least 3 items to form a list.\nWe further require the text of each item follows certain patterns to ensure the text is highly likely to be named entities, and rule out those lists whose item text is largely free text. Specifically, we require 1) all words of the item text all start with capital letters; 2) referring the text between punctuations as \u201cblock\u201d, each block includes no more than 3 words.\nWe detect both intra-sentence lists and inter-sentence lists in documents. We found the above patterns are effective to identify true lists. A better list detection method is expected to further improve our NER results."}], "references": [{"title": "Hinge-loss markov random fields and probabilistic soft logic", "author": ["S.H. Bach", "M. Broecheler", "B. Huang", "L. Getoor"], "venue": "arXiv preprint arXiv:1505.04406.", "citeRegEx": "Bach et al\\.,? 2015", "shortCiteRegEx": "Bach et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Model compression", "author": ["C. Bucilu", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proc. of KDD, pages 535\u2013541. ACM.", "citeRegEx": "Bucilu et al\\.,? 2006", "shortCiteRegEx": "Bucilu et al\\.", "year": 2006}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["J.P. Chiu", "E. Nichols"], "venue": "arXiv preprint arXiv:1511.08308.", "citeRegEx": "Chiu and Nichols,? 2015", "shortCiteRegEx": "Chiu and Nichols", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Fast relational learning using bottom clause propositionalization with artificial neural networks", "author": ["M.V. Fran\u00e7a", "G. Zaverucha", "Garcez", "A.S. d."], "venue": "Machine learning, 94(1):81\u2013104.", "citeRegEx": "Fran\u00e7a et al\\.,? 2014", "shortCiteRegEx": "Fran\u00e7a et al\\.", "year": 2014}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Gra\u00e7a", "J. Gillenwater", "B. Taskar"], "venue": "JMLR, 11:2001\u20132049.", "citeRegEx": "Ganchev et al\\.,? 2010", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "Neural-symbolic learning systems: foundations and applications", "author": ["Garcez", "A.S. d.", "K. Broda", "D.M. Gabbay"], "venue": "Springer Science & Business Media.", "citeRegEx": "Garcez et al\\.,? 2012", "shortCiteRegEx": "Garcez et al\\.", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "Mohamed", "A.-r", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T. N"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531.", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proc. of KDD, pages 168\u2013177. ACM.", "citeRegEx": "Hu and Liu,? 2004", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Z. Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proc. of EMNLP.", "citeRegEx": "Kim,? 2014", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. of NIPS, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deep convolutional inverse graphics network", "author": ["T.D. Kulkarni", "W.F. Whitney", "P. Kohli", "J. Tenenbaum"], "venue": "Proc. of NIPS, pages 2530\u20132538.", "citeRegEx": "Kulkarni et al\\.,? 2015", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science, 350(6266):1332\u20131338. 14", "citeRegEx": "Lake et al\\.,? 2015", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "Proc. of NAACL.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "Proc. of ICML.", "citeRegEx": "Le and Mikolov,? 2014", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "Learning from measurements in exponential families", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Proc. of ICML, pages 641\u2013648. ACM.", "citeRegEx": "Liang et al\\.,? 2009", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "Unifying distillation and privileged information", "author": ["D. Lopez-Paz", "L. Bottou", "B. Sch\u00f6lkopf", "V. Vapnik"], "venue": "Prof. of ICLR.", "citeRegEx": "Lopez.Paz et al\\.,? 2016", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2016}, {"title": "Joint named entity recognition and disambiguation", "author": ["G. Luo", "X. Huang", "Lin", "C.-Y.", "Z. Nie"], "venue": "Proc. of EMNLP.", "citeRegEx": "Luo et al\\.,? 2015", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Proc. of NIPS, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning meaning", "author": ["M. Minksy"], "venue": "Technical Report AI Lab Memo. Project MAC. MIT.", "citeRegEx": "Minksy,? 1980", "shortCiteRegEx": "Minksy", "year": 1980}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proc. of CVPR, pages 427\u2013 436. IEEE.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "Proc. of ACL, pages 115\u2013124. Association for Computational Linguistics.", "citeRegEx": "Pang and Lee,? 2005", "shortCiteRegEx": "Pang and Lee", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proc. of EMNLP, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning, 62(12):107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proc. of EMNLP, volume 1631, page 1642. Citeseer.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "Proc. of ICLR.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "Proc. of CoNLL, pages 142\u2013 147. Association for Computational Linguistics. 15", "citeRegEx": "Sang and Meulder,? 2003", "shortCiteRegEx": "Sang and Meulder", "year": 2003}, {"title": "Refinement of approximate domain theories by knowledge-based neural networks", "author": ["G.G. Towell", "J.W. Shavlik", "M.O. Noordewier"], "venue": "Proceedings of the eighth National conference on Artificial intelligence, pages 861\u2013866. Boston, MA.", "citeRegEx": "Towell et al\\.,? 1990", "shortCiteRegEx": "Towell et al\\.", "year": 1990}, {"title": "Fast dropout training", "author": ["S. Wang", "C. Manning"], "venue": "Proc. of ICML, pages 118\u2013126.", "citeRegEx": "Wang and Manning,? 2013", "shortCiteRegEx": "Wang and Manning", "year": 2013}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S. Wang", "C.D. Manning"], "venue": "Proc. of ACL, pages 90\u201394. Association for Computational Linguistics.", "citeRegEx": "Wang and Manning,? 2012", "shortCiteRegEx": "Wang and Manning", "year": 2012}, {"title": "Context-aware learning for sentence-level sentiment analysis with posterior regularization", "author": ["B. Yang", "C. Cardie"], "venue": "Proc. of ACL, pages 325\u2013335.", "citeRegEx": "Yang and Cardie,? 2014", "shortCiteRegEx": "Yang and Cardie", "year": 2014}, {"title": "Multichannel variable-size convolution for sentence classification", "author": ["W. Yin", "H. Schutze"], "venue": "Proc. of CONLL.", "citeRegEx": "Yin and Schutze,? 2015", "shortCiteRegEx": "Yin and Schutze", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler,? 2012", "shortCiteRegEx": "Zeiler", "year": 2012}, {"title": "Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Y. Zhang", "S. Roller", "B. Wallace"], "venue": "Proc. of NAACL.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Bayesian inference with posterior regularization and applications to infinite latent svms", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "JMLR, 15(1):1799\u20131847. 16", "citeRegEx": "Zhu et al\\.,? 2014", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "Deep neural networks provide a powerful mechanism for learning patterns from massive data, achieving new levels of performance on image classification (Krizhevsky et al., 2012), speech recognition (Hinton et al.", "startOffset": 151, "endOffset": 176}, {"referenceID": 8, "context": ", 2012), speech recognition (Hinton et al., 2012), machine translation (Bahdanau et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 1, "context": ", 2012), machine translation (Bahdanau et al., 2014), playing strategic board games (Silver et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 27, "context": ", 2014), playing strategic board games (Silver et al., 2016), and so forth.", "startOffset": 39, "endOffset": 60}, {"referenceID": 29, "context": "The high predictive accuracy has heavily relied on large amounts of labeled data; and the purely data-driven learning can lead to uninterpretable and sometimes counter-intuitive results (Szegedy et al., 2014; Nguyen et al., 2015).", "startOffset": 186, "endOffset": 229}, {"referenceID": 23, "context": "The high predictive accuracy has heavily relied on large amounts of labeled data; and the purely data-driven learning can lead to uninterpretable and sometimes counter-intuitive results (Szegedy et al., 2014; Nguyen et al., 2015).", "startOffset": 186, "endOffset": 229}, {"referenceID": 22, "context": "On the other hand, the cognitive process of human beings have indicated that people learn not only from concrete examples (as DNNs do) but also from different forms of general knowledge and rich experiences (Minksy, 1980; Lake et al., 2015).", "startOffset": 207, "endOffset": 240}, {"referenceID": 15, "context": "On the other hand, the cognitive process of human beings have indicated that people learn not only from concrete examples (as DNNs do) but also from different forms of general knowledge and rich experiences (Minksy, 1980; Lake et al., 2015).", "startOffset": 207, "endOffset": 240}, {"referenceID": 7, "context": "Neural-symbolic systems (Garcez et al., 2012) construct a network from a given rule set to execute reasoning.", "startOffset": 24, "endOffset": 45}, {"referenceID": 4, "context": "To exploit a priori knowledge in general neural architectures, recent work augments each raw data instance with useful features (Collobert et al., 2011), while network training, however, is still limited to instance-label supervision and suffers from the same issues mentioned above.", "startOffset": 128, "endOffset": 152}, {"referenceID": 9, "context": "Methodologically, our approach can be seen as a combination of the knowledge distillation (Hinton et al., 2015; Bucilu et al., 2006) and the posterior regularization (PR) method (Ganchev et al.", "startOffset": 90, "endOffset": 132}, {"referenceID": 2, "context": "Methodologically, our approach can be seen as a combination of the knowledge distillation (Hinton et al., 2015; Bucilu et al., 2006) and the posterior regularization (PR) method (Ganchev et al.", "startOffset": 90, "endOffset": 132}, {"referenceID": 6, "context": ", 2006) and the posterior regularization (PR) method (Ganchev et al., 2010).", "startOffset": 53, "endOffset": 75}, {"referenceID": 7, "context": "Neural-symbolic systems (Garcez et al., 2012), such as KBANN (Towell et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 31, "context": ", 2012), such as KBANN (Towell et al., 1990) and CILP++ (Fran\u00e7a et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 5, "context": ", 1990) and CILP++ (Fran\u00e7a et al., 2014), construct network architectures from given rules to perform reasoning and knowledge acquisition.", "startOffset": 19, "endOffset": 40}, {"referenceID": 26, "context": "A related line of research, such as Markov logic networks (Richardson and Domingos, 2006), derives probabilistic graphical models (rather than neural networks) from the rule set.", "startOffset": 58, "endOffset": 89}, {"referenceID": 4, "context": "features from domain knowledge (Collobert et al., 2011), while producing improved results, does not go beyond the data-label paradigm.", "startOffset": 31, "endOffset": 55}, {"referenceID": 6, "context": "Though there do exist general frameworks that allow encoding structured constraints on latent variable models (Ganchev et al., 2010; Zhu et al., 2014; Liang et al., 2009), they either are not directly applicable to the NN case, or could yield inferior performance as in our empirical study.", "startOffset": 110, "endOffset": 170}, {"referenceID": 38, "context": "Though there do exist general frameworks that allow encoding structured constraints on latent variable models (Ganchev et al., 2010; Zhu et al., 2014; Liang et al., 2009), they either are not directly applicable to the NN case, or could yield inferior performance as in our empirical study.", "startOffset": 110, "endOffset": 170}, {"referenceID": 18, "context": "Though there do exist general frameworks that allow encoding structured constraints on latent variable models (Ganchev et al., 2010; Zhu et al., 2014; Liang et al., 2009), they either are not directly applicable to the NN case, or could yield inferior performance as in our empirical study.", "startOffset": 110, "endOffset": 170}, {"referenceID": 4, "context": "features from domain knowledge (Collobert et al., 2011), while producing improved results, does not go beyond the data-label paradigm. Kulkarni et al. (2015) uses a specialized training procedure with careful ordering of training instances to obtain an interpretable neural layer of an image network.", "startOffset": 32, "endOffset": 158}, {"referenceID": 0, "context": "We encode the FOL rules using soft logic (Bach et al., 2015) for flexible encoding and stable optimization.", "startOffset": 41, "endOffset": 60}, {"referenceID": 2, "context": "A similar imitation procedure has been used in other settings such as model compression (Bucilu et al., 2006; Hinton et al., 2015) where the process is termed distillation.", "startOffset": 88, "endOffset": 130}, {"referenceID": 9, "context": "A similar imitation procedure has been used in other settings such as model compression (Bucilu et al., 2006; Hinton et al., 2015) where the process is termed distillation.", "startOffset": 88, "endOffset": 130}, {"referenceID": 19, "context": ", the privileged information setting (Lopez-Paz et al., 2016)) while still enjoying the benefit of integration.", "startOffset": 37, "endOffset": 61}, {"referenceID": 6, "context": "Our framework is related to the posterior regularization (PR) method (Ganchev et al., 2010) which places constraints over model posterior in unsupervised setting.", "startOffset": 69, "endOffset": 91}, {"referenceID": 12, "context": "Base Network We use the single-channel convolutional network proposed in (Kim, 2014).", "startOffset": 73, "endOffset": 84}, {"referenceID": 3, "context": "Base Network The base network has a similar architecture with the bi-directional LSTM recurrent network (called BLSTM-CNN) proposed in (Chiu and Nichols, 2015) for NER which has outperformed most of previous neural models.", "startOffset": 135, "endOffset": 159}, {"referenceID": 3, "context": "Compared to (Chiu and Nichols, 2015) we omit the character type and capitalization features, as well as the additive transition matrix in the output layer.", "startOffset": 12, "endOffset": 36}, {"referenceID": 16, "context": "In contrast to recent work (Lample et al., 2016) which adds a conditional random field (CRF) to capture bi-gram dependencies between outputs, we instead apply logic rules which does not introduce extra parameters to learn.", "startOffset": 27, "endOffset": 48}, {"referenceID": 28, "context": "We test our method on a number of commonly used benchmarks, including 1) SST2, Stanford Sentiment Treebank (Socher et al., 2013) which contains 2 classes (negative and positive), and 6920/872/1821 sentences in the train/dev/test sets respectively.", "startOffset": 107, "endOffset": 128}, {"referenceID": 12, "context": "Following (Kim, 2014) we train models on both sentences and phrases since all labels are provided.", "startOffset": 10, "endOffset": 21}, {"referenceID": 24, "context": "2) MR (Pang and Lee, 2005), a set of 10,662 one-sentence movie reviews with negative or positive sentiment.", "startOffset": 6, "endOffset": 26}, {"referenceID": 10, "context": "3) CR (Hu and Liu, 2004), customer reviews of various products, containing 2 classes and 3,775 instances.", "startOffset": 6, "endOffset": 24}, {"referenceID": 12, "context": "For the base neural network we use the \u201cnon-static\u201d version in (Kim, 2014) with the exact same configurations.", "startOffset": 63, "endOffset": 74}, {"referenceID": 21, "context": "Specifically, word vectors are initialized using word2vec (Mikolov et al., 2013) and fine-tuned throughout training, and the neural parameters are trained using SGD with the Adadelta update rule (Zeiler, 2012).", "startOffset": 58, "endOffset": 80}, {"referenceID": 36, "context": ", 2013) and fine-tuned throughout training, and the neural parameters are trained using SGD with the Adadelta update rule (Zeiler, 2012).", "startOffset": 122, "endOffset": 136}, {"referenceID": 12, "context": "Model SST2 MR CR 1 CNN (Kim, 2014) 87.", "startOffset": 23, "endOffset": 34}, {"referenceID": 37, "context": "04 3 MGNC-CNN (Zhang et al., 2016) 88.", "startOffset": 14, "endOffset": 34}, {"referenceID": 35, "context": "4 \u2013 \u2013 4 MVCNN (Yin and Schutze, 2015) 89.", "startOffset": 14, "endOffset": 37}, {"referenceID": 12, "context": "4 \u2013 \u2013 5 CNN-multichannel (Kim, 2014) 88.", "startOffset": 25, "endOffset": 36}, {"referenceID": 17, "context": "0 6 Paragraph-Vec (Le and Mikolov, 2014) 87.", "startOffset": 18, "endOffset": 40}, {"referenceID": 34, "context": "8 \u2013 \u2013 7 CRF-PR (Yang and Cardie, 2014) \u2013 \u2013 82.", "startOffset": 15, "endOffset": 38}, {"referenceID": 28, "context": "7 8 RNTN (Socher et al., 2013) 85.", "startOffset": 9, "endOffset": 30}, {"referenceID": 32, "context": "4 \u2013 \u2013 9 G-Dropout (Wang and Manning, 2013) \u2013 79.", "startOffset": 18, "endOffset": 42}, {"referenceID": 33, "context": "1 10 NBSVM (Wang and Manning, 2012) \u2013 79.", "startOffset": 11, "endOffset": 35}, {"referenceID": 12, "context": "Row 1, CNN (Kim, 2014) corresponds to the \u201cCNN-non-static\u201d model in (Kim, 2014).", "startOffset": 11, "endOffset": 22}, {"referenceID": 12, "context": "Row 1, CNN (Kim, 2014) corresponds to the \u201cCNN-non-static\u201d model in (Kim, 2014).", "startOffset": 68, "endOffset": 79}, {"referenceID": 35, "context": "On SST2, MVCNN (Yin and Schutze, 2015) (Row 4) is the only system that shows a slightly better result than ours.", "startOffset": 15, "endOffset": 38}, {"referenceID": 6, "context": "4) \u201c-semi-PR\u201d is the posterior regularization (Ganchev et al., 2010) which imposes the rule constraint only through unlabeled data during training.", "startOffset": 46, "endOffset": 68}, {"referenceID": 3, "context": "We use the mostly same configurations for the BLSTM network as in (Chiu and Nichols, 2015), except that, besides the slight architecture difference (section 4.", "startOffset": 66, "endOffset": 90}, {"referenceID": 25, "context": "GloVe (Pennington et al., 2014) word vectors are used to initialize word features.", "startOffset": 6, "endOffset": 31}, {"referenceID": 16, "context": "56 improvement in F1 score that outperforms all previous neural based methods (Rows 4-8), including the BLSTM-CRF model (Lample et al., 2016) which applies a conditional random field (CRF) on top of a BLSTM model in order to capture the transition patterns and encourage valid sequences.", "startOffset": 120, "endOffset": 141}, {"referenceID": 16, "context": "18 4 BLSTM-CRF1 (Lample et al., 2016) 90.", "startOffset": 16, "endOffset": 37}, {"referenceID": 16, "context": "94 5 S-LSTM (Lample et al., 2016) 90.", "startOffset": 12, "endOffset": 33}, {"referenceID": 3, "context": "33 6 BLSTM-lex (Chiu and Nichols, 2015) 90.", "startOffset": 15, "endOffset": 39}, {"referenceID": 11, "context": "77 7 BLSTM-CRF2 (Huang et al., 2015) 90.", "startOffset": 16, "endOffset": 36}, {"referenceID": 4, "context": "10 8 NN-lex (Collobert et al., 2011) 89.", "startOffset": 12, "endOffset": 36}, {"referenceID": 20, "context": "59 9 Joint-NER-EL (Luo et al., 2015) 91.", "startOffset": 18, "endOffset": 36}, {"referenceID": 20, "context": "Further integration of the list rule (Row 3) provides a second boost in performance, achieving an F1 score very close to the best-performing system Joint-NER-EL (Luo et al., 2015) (Row 9) which is a probabilistic graphical model based method optimizing NER and entity linking jointly and using large amount of external resources.", "startOffset": 161, "endOffset": 179}], "year": 2016, "abstractText": "Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce unpredictability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.", "creator": "LaTeX with hyperref package"}}}