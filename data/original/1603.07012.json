{"id": "1603.07012", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "Semi-supervised Word Sense Disambiguation with Neural Models", "abstract": "Determining the intended sense of words in text -- word sense disambiguation (WSD) -- is a long-standing problem in natural language processing. In this paper, we present WSD algorithms which use neural network language models to achieve state-of-the-art precision. Each of these methods learns to disambiguate word senses using only a set of word senses, a few example sentences for each sense taken from a licensed lexicon, and a large unlabeled text corpus. We classify based on cosine similarity of vectors derived from the contexts in unlabeled query and labeled example sentences. We demonstrate state-of-the-art results when using the WordNet sense inventory, and significantly better than baseline performance using the New Oxford American Dictionary inventory. The best performance was achieved by combining an LSTM language model with graph label propagation.", "histories": [["v1", "Tue, 22 Mar 2016 22:15:10 GMT  (207kb,D)", "http://arxiv.org/abs/1603.07012v1", null], ["v2", "Sat, 5 Nov 2016 01:15:21 GMT  (153kb,D)", "http://arxiv.org/abs/1603.07012v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dayu yuan", "julian richardson", "ryan doherty", "colin evans", "eric altendorf"], "accepted": false, "id": "1603.07012"}, "pdf": {"name": "1603.07012.pdf", "metadata": {"source": "CRF", "title": "Word Sense Disambiguation with Neural Language Models", "authors": ["Dayu Yuan", "Ryan Doherty", "Julian Richardson", "Colin Evans", "Eric Altendorf"], "emails": ["dayuyuan@google.com", "portalfire@google.com", "jdcr@google.com", "colinhevans@google.com", "ealtendorf@google.com"], "sections": [{"heading": "1 Introduction", "text": "Word sense disambiguation (WSD) is a longstanding problem in natural language processing (NLP) with broad applications (Navigli, 2009). Supervised, unsupervised, and knowledge-based approaches have been studied for WSD (Navigli, 2009). However, for all-words WSD, where all words in a corpus need to be annotated with word senses, it has proven extremely challenging to beat the strong baseline, which always assigns the most frequent sense of a word without considering the context (Pradhan et al., 2007; Navigli et al., 2007; Navigli, 2009; Navigli et al., 2013; Moro and Navigli, 2015). Given the good\nperformance of published supervised WSD systems when provided with significant training data on specific words (Navigli, 2009; Zhong and Ng, 2010), it appears lack of sufficient labeled training data for large vocabularies is the central problem.\nTo address the difficulty of obtaining large amounts of labeled data, in this work we study semi-supervised approaches. We use vector representations of words and contexts created using a neural-network based language model (NNLM) trained over a large unlabeled text corpus. We define a context to be a sentence minus a held out word. The NNLM produces a context vector from each context. We then classify based on the cosine similarity of context vectors.\nWe study two NNLMs in this paper. The first model is the continuous bag of words model (CBOW) (Mikolov et al., 2013). We construct the context vector as a weighted sum of the CBOW word vectors for the words in the context. This model is simple and efficient, but does not account for word order. The second model is based on a Long Short Term Memory model (LSTM) (Hochreiter and Schmidhuber, 1997). We train the LSTM on a large unlabeled corpus of text to predict a held-out word given the context (as a sequence). We use a hidden layer in the network, during prediction, as a vector representation for the context.\nWe also present an algorithm for semisupervised learning, using label propagation (Talukdar and Crammer, 2009; Ravi and Diao, 2016) to label unlabeled sentences based on their similarity to labeled ones. This allows us to better estimate the distribution of word senses, obtaining more accurate decision boundaries and higher classification accuracy.\nOur experiments show that using the LSTM language model achieves significantly higher precision than the CBOW language model, especially\nar X\niv :1\n60 3.\n07 01\n2v 1\n[ cs\n.C L\n] 2\n2 M\nar 2\n01 6\non verbs and adverbs. This suggests that sequential order information is important to discriminating senses of verbs and adverbs. The best performance was achieved by using an LSTM language model with label propagation. Our algorithm outperforms the baseline by more than 10% (0.87 vs. 0.75).\nOrganization: We review related work in Section 2. We introduce our supervised WSD algorithms in Section 4, and the semi-supervised WSD algorithm in Section 5. Experimental results are discussed in Section 6. We provide further discussion and future work in Section 7."}, {"heading": "2 Related Work", "text": "The recent development of large lexical resources, such as WordNet (Fellbaum, 1998) and BabelNet (Navigli and Ponzetto, 2012), has enabled knowledge-based algorithms which show promising results on all-word prediction tasks (Ponzetto and Navigli, 2010; Navigli et al., 2013; Moro and Navigli, 2015). WSD algorithms based on supervised learning are generally believed to perform better than knowledge-based WSD algorithms, but they need large training sets to perform well (Pradhan et al., 2007; Navigli et al., 2007; Navigli, 2009; Zhong and Ng, 2010). Acquiring large training sets is costly. In this paper, we show that a supervised WSD algorithm can perform well with a small number (e.g., 20) of training example sentences per sense.\nIn the past few years, much progress has been made on using neural networks to learn word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014), to construct language models (Mikolov et al., 2011), perform sentiment analysis (Socher et al., 2013), machine translation (Sutskever et al., 2014) and many other NLP applications. For the WSD problem, Rothe and Schu\u0308tze (2015) extended word embeddings to word sense or synset embeddings, and trained a SVM classifier (Zhong and Ng, 2010) with word sense and synset embeddings as features. Taghipour et al. (2015) trained a feedforward neural network with automatically labeled training samples to learn word embeddings for WSD. However, the quality of the automatically labeled data is hard to control.\nIn this work, we take a different approach. Our neural networks are trained with large corpora of unlabeled data to predict a word from its sur-\nrounding context. The context vector, produced by the NNLM, captures the syntactic and semantic patterns of the context. Using a nonparametric nearest neighbor classifier with those vectors, our WSD classifier achieves high performance with little training data by taking advantage of the highlevel features learned from neural-network based language models."}, {"heading": "3 Classifying Using Example Sentences", "text": "The methods which we propose require one or more example sentences for each sense in the inventory to which we wish to classify. Our supervised WSD algorithms (Section 4) classify a word by finding the example sentences which are most similar to the sentence in which the word appears (Figure 3a). To overcome relative sparseness in the space of example sentences, we also present a semisupervised method (Section 5), which augments the example sentences with a large number of unlabeled sentences from the web. Sense labels are then propagated from the example sentences to the unlabeled sentences (Figure 3)."}, {"heading": "4 Supervised WSD", "text": ""}, {"heading": "4.1 Word Vector Similarity", "text": "Distributional methods, in particular using word vectors trained using neural networks, have been shown to be extremely effective in NLP tasks (Turian et al., 2010; Baroni et al., 2014). The CBOW model is trained to predict a target word with its context as input (Mikolov et al., 2013). CBOW first computes a context vector by adding the vectors of the context words, and then predicts the target with the context vector (Figure 1). We use the context vector from CBOW for WSD.\nWe used 1000-dimensional embeddings from a 100-billion-word news corpus, trained using word2vec (Mikolov et al., 2013). The vocabulary consists of the most frequent 1, 000, 000 words, without lemmatization or case normalization. To\nobtain a context vector, we compute the sum of the embedding vectors for the words in the context, weighted by the log of their reciprocal rank in the corpus. This is similar to (Gabrilovich and Markovitch, 2007), which computes an embedding for a context by weighting the constituent word embeddings using their TF-IDF in some corpus \u2013 the purpose is to down-weight frequent words.\nTo compute a vector for a given sense, we average the context vectors for the example sentences of that sense. To classify, we find the sense vector with maximal cosine similarity to the context vector of the target word. For example, the query in Table 1 should be annotated as \u2018sense#1\u2019 because of its similarity to the first sense, 0.44, is higher than the similarity to the other two senses, 0.31 and 0.21. (We also tried classifying to the sense which contains the single example sentence with maximal cosine similarity, but this performed no better.)\nMore formally, let \u03c6(w) denote the vector for a word w obtained from the trained embeddings (or 0 if w is not in the vocabulary), and r(w) denote the rank of w in the training corpus, sorted by frequency. Then the context w = w1, ..., wk\u22121 , wk, wk+1, ..., wn, where wk is the word we wish to classify, has the context vector\n\u03c6((w)) = \u2211 i 6=k log(1 + r(wi))\u00d7 \u03c6(wi).\nFor a word w with part of speech p, let sw,pi be the word senses which have the same lemma and part of speech as w, and sw,pij be the example sentences associated with sw,pi . If cos(., .) denotes the cosine similarity function, then the classification of a word, w, appearing with part of speech p in a\ncontext w is sw,pi , where\ni = argmax cos(\u03c6(w), \u2211 j \u03c6(swpij )) (1)\nThe evaluation results are shown in Table 5. This method is limited by the fact that it sees only a bag of context words. Next we investigate a stronger language model which considers word order.\n4.2 LSTM Language Model\nNeural networks with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) make good language models which do consider word order (Sundermeyer et al., 2012). We train an LSTM language model to predict the heldout word in a sentence (Figure 2). The LSTM was trained on a news corpus of about 100 billion tokens, with a vocabulary of almost 1, 000, 000 words. Words in the vocabulary are neither lemmatized nor case normalized.\nThe behavior of the LSTM can be intuited by its predictions. Table 2 shows the top 10 words predicted by an LSTM language model for the word \u2018stock\u2019 in example and query sentences from Table 1.\nid top 10 predictions from LSTM label 1 cash, stock, equity, shares, loans, bonus,benefits, awards, equivalents, deposits sense#1\n2 bonds, debt, notes, shares, stock, balance,securities, rest, Notes, debentures 3 inventory, goods, parts, sales, inventories,capacity, products, oil, items, fuel sense#2 4 foods, food, vegetables, meats, recipes,cheese, meat, chicken, pasta, milk sense#3\nIn our initial experiments, we computed similarity between two contexts by the overlap between their bags of predicted words. For example, the top predictions for the query overlap most with the LSTM predictions for \u2018sense#1\u2019, so we predict that \u2018sense#1\u2019 is the correct sense.\nThis bag of predictions, while easily interpretable, is just a discrete approximation to the internal state of the LSTM when predicting the held out word. We therefore directly use the LSTM\u2019s hidden layer from which the bag of predictions was computed as a representation of the context (see Figure 2). Given context vectors extracted from the LSTM, classification is performed by finding the sense with the most similar example sentences as in Equation 1.\nAs can be seen in Tables 5 and 6, this model has significantly better performance than the CBOW model described in Section 4.1.\n5 Semi-supervised WSD\nThe non-parametric nearest neighbor algorithm which we described in Section 4 has the following drawbacks: \u2022 It assumes a spherical shape for each sense\ncluster, being unable to accurately model the decision boundaries given the limited number of examples. \u2022 It has no training data for, and does not\nmodel, the sense prior, omitting an extremely\npowerful potential signal. We next study a semi-supervised algorithm and show that by selecting and labeling sentences randomly sampled from the web, we can better estimate the shape and size of the sense clusters.\nA label-propagation graph consists of (a) vertices with a number of labeled seed nodes and (b) undirected weighted edges. Label propagation (LP) (Talukdar and Crammer, 2009) iteratively computes a distribution of labels on the graph\u2019s vertices to minimize a weighted combination of: \u2022 The discrepancy between seed labels and and\ntheir computed labels distributions. \u2022 The disagreement between the label distribu-\ntions of connected vertices. \u2022 A regularization term which penalizes distri-\nbutions which differ from the prior (by default, a uniform distribution).\nWe construct a graph for each lemma where each vertex represents a sentence in which the lemma occurs. We first create and label vertices for labeled example sentences. In addition, we sample 1000 unlabeled sentences containing the lemma from a large text corpus. Vertices for sufficiently similar sentences (based on criteria discussed below) are connected by an edge whose weight is the cosine similarity between the respective context vectors, using either the CBOW or LSTM language model.\nTo classify an occurrence of the lemma, we create an additional vertex for the new sentence and run LP to propagate the sense labels from the seed vertices to the unlabeled vertices.\nFigure 3 (b) illustrates the graph configuration. Spatial proximity represents similarity of the sentences attached to each vertex and the shape of each node represents the word sense. Filled nodes represent seed nodes with known word senses. Unfilled nodes represent sentences with no word sense label, and the ? represents the word we want to classify.\nWith too many edges, sense labels propagate too far, giving low precision. With too few, sense labels do not propagate sufficiently, giving low recall. We found that the graph has about the right density for common senses when we ranked vertex pairs by similarity and connected the top 5%. This may still leave rare senses sparsely connected, so we additionally added edges to ensure that every vertex is connected to at least 10 other vertices.\nThis WSD algorithm produced the best preci-\nsion and recall (Table 5, 6). Since it requires running LP for every classification, the algorithm is slow compared to the algorithms we previously described."}, {"heading": "6 Experiments", "text": "We evaluated performance with several different experiments. We compared the CBOW and LSTM NNLMs with and without LP. We trained on several different corpora using two different inventories. We also compare the performance of our algorithms to a selection of baseline WSD algorithms and to SemEval 2013 and 2015 results."}, {"heading": "6.1 Word Sense Inventory", "text": "In this work we use the New Oxford American Dictionary (NOAD) (Stevenson and Lindberg, 2010). The NOAD focuses on American English and is based on the Oxford Dictionary of English (ODE) (Stevenson, 2010). It distinguishes between coarse (core) and fine-grained (sub) word senses in the same manner as ODE. Previous investigations (Navigli, 2006; Navigli et al., 2007) using the ODE have shown that coarse-grained word senses induced by the ODE inventory address problems with WordNet\u2019s fine-grained inventory, and that the inventory is useful for word sense disambiguation.\nFor our experiments, we use NOAD\u2019s core senses, and we also use lexicographer-curated example sentences from the Semantic English Language Database (SELD)1, provided by Oxford University Press. Table 3 shows the total number of polysemes (more than one core sense), average number of senses per polyseme and average number of example sentences per sense in NOAD/SELD (hereafter, \u201dNOAD\u201d). We manually annotated all words of the English language SemCor (Miller et al., 1993) corpus and MASC 2 corpora with NOAD word senses in order to evaluate performance.\nnoun verb adj. adv. polyseme count 8097 2124 2126 266\nsense count/polyseme 2.46 2.58 2.30 2.47 example count/sense 15.67 27.38 19.40 20.30\nas the inventory. To overcome the general lack of example sentences in WordNet, we developed a partially crowd-sourced mapping from NOAD to WordNet. This allows us to produce WordNet sense labels for NOAD example sentences, and therefore use those example sentences to classify directly into WordNet space."}, {"heading": "6.2 NOAD Experiment", "text": "Data: We use NOAD example sentences as labeled training data. We evaluate on SemCor and MASC. We evaluate all polysemous words in the evaluation corpus. Table 4 lists the number of polysemous words and average number of candidate senses per word in NOAD, SemCor and MASC.\nWe compare our algorithms with five baseline algorithms: \u2022 First sense: Label word w with w\u2019s first\nNOAD sense, which typically is the most popular sense. \u2022 Most frequent sense: Compute the sense fre-\nquency (from a labeled corpus) and label word w with w\u2019s most frequent sense. \u2022 Gloss overlap: Annotate a word in a given\ncontext with the sense whose definition overlaps the most with the context. \u2022 Fuzzy Gloss overlap: Similar to Gloss\noverlap, but measure the overlap of two words with the cosine similarity between two words\u2019 CBOW word vectors. \u2022 Lesk: Annotate a word in a given context\nwith the sense whose gloss shares the largest number of common words with the glosses of the other words in the context (Lesk, 1986).\nWe train the CBOW and LSTM language models from a 100 billion word news corpus. The CBOW word vectors are of dimension 1024. The LSTM model has 2048 hidden units, and inputs are 512-dimensional word vectors. The LSTM learning rate is 0.1. We experimented with other learning rates, and observed no significant performance difference under different learning rates after the training converges.\nTable 5 compares the F1 scores of the CBOW, LSTM and baseline algorithms. Both CBOW and LSTM beat the baselines by a wide margin. LSTM outperforms CBOW by more than 10% over all words, where most of the gains are from verbs and adverbs. The results suggest that syntactic information, which is well modeled by LSTM but ignored by CBOW, is key to distinguishing word senses of verbs and adverbs.\nWe also compute F1 score per sense and average the F1 scores across all word senses to compute macro F1 scores. Table 6 shows the macro scores. Both CBOW and LSTM beat the baseline algorithms. Also, LSTM outperforms CBOW by a wide margin. The first sense classifier and the most frequent sense classifier perform poorly on the macro scores since they never predict the less frequent senses."}, {"heading": "6.2.1 Training data", "text": "By default, the WSD classifier uses the NOAD example sentences as training data. We build a larger training dataset by adding labeled sentences from SemCor and MASC, and study the change of F1 scores in Table 5 and Table 6. Across most part of speech tags and datasets, both the micro and macro F1 scores increase after adding more training data. We further test our algorithm by using SemCor (or MASC) as training data (without NOAD exam-\nples). The SemCor (or MASC) trained classifier is on a par with the NOAD trained classifier on F1 score. However, the macro F1 score of the former is much lower than the latter, because of the limited coverage of rare senses and words in SemCor and MASC."}, {"heading": "6.2.2 Change of language model capacity", "text": "In this experiment, we change the LSTM model capacity by varying the number of hidden units h and the dimensions of the input embeddings p and measuring F1. Figure 4 shows strong correlation between F1 and the capacity of the language model. However, larger models are slower to train and use more memory. To balance the accuracy and resource usage, we use the second best LSTM model (h = 2048 and p = 512) by default."}, {"heading": "6.3 Semi-supervised WSD", "text": "We evaluate our semi-supervised wsd classifier in this subsection.\nFor each word, we collect three types of sentence:(1) seed sentences: labeled sentences from the training datasets, (2) 1000 unlabeled sentences randomly sampled from the web, and (3) evaluation sentences with hidden gold sense labels.\nWe construct the graph as described in Section 5 and run LP to propagate sense labels from the seed vertices to the unlabeled vertices. We eval-\nuate the performance of the algorithm by comparing the predicted labels and the gold labels on eval nodes. As can be observed from Table 5, LP did not yield clear benefits when using the CBOW language model. We hypothesize that this is because LP is sensitive to the quality of the graph distance metric.\nWe did see significant improvements using LP with the LSTM language model. As can be seen in Table 5, LP substantially improves classifier F1 when the training datasets are SemCor+NOAD or MASC+NOAD. Running LP with the LSTM language model achieves the highest F1 scores.\nAs discussed in Section 5, the system benefits from (1) explicitly modeling the sense prior and (2) using the unlabeled nodes to better model sense distributions and decision boundaries. The distribution of example sentences in NOAD does not give a good approximation to the sense prior in free text, so LP would not give much benefit without the unlabeled web data.\nThere is a trade-off between the macro F1 (an average of the per-sense F1) and micro F1 (computed per example, with more frequent senses carrying more weight). Table 6 shows the macro F1 scores of the LP classifiers. LP generally results in lower macro performance, although the decrease is not severe."}, {"heading": "6.4 SemEval Tasks", "text": "In this section, we study the performance of our classifiers on SemEval-2013 Task 12 (Navigli et al., 2013) and SemEval-2015 task 13 (Moro and Navigli, 2015). These two tasks use WordNet as the sense inventory for English WSD. We use a mapping to label NOAD example sentences with WordNet senses (see Section 6.1) for training. We also use SemCor (annotated with WordNet senses) for training. For a fair comparison with related works, the classifiers are evaluated on all words (both polysemous and monosemous).\nTable 7 shows the results of Sem-Eval 2013. Our proposed algorithms outperform UMCCDLSI, the best WSD algorithm reported in SemEval 2013. The LP classifier with an LSTM language model has the highest score.\nTable 8 shows the results of Sem-Eval 2015. The LP classifier with an LSTM language model\nachieves the highest scores on nouns and verbs as well as overall F1."}, {"heading": "7 Discussion and Future Work", "text": "Our approach to WSD does not rely on large labeled data sets, or on structured semantic resources, both of which are costly to curate. Instead, it leans on powerful language models learned from large unlabeled corpora, and on label propagation over unlabeled sentences using representations learned by the language model. This enables us to exploit small (e.g. 20 examples per sense) labeled data sets which may be licensed from existing lexicographic sources.\nSeveral unanswered questions suggest lines of future work. We have not yet studied the relationship between number of labeled training examples and performance. Since our general approach is amenable to incorporating any language model, further developments in NNLMs may permit increased performance. We would also like to better understand the limitations of language modeling for this task: we expect there are situations \u2013 e.g., in idiomatic phrases \u2013 where per-word predictions carry little information.\nWe believe our model should generalize to languages other than English, but have not yet explored this. Character-level LSTMs (Kim et al., 2015) may provide robustness to morphology and diacritics and may prove useful even in English for spelling errors and out of vocabulary words. We would also like to investigate how work on unsupervised sense induction (Navigli, 2009) could be integrated with our models to detect out of vocabulary senses \u2013 particularly a problem in web text where terminology evolves rapidly.\nFinally, many applications of WSD systems for nominal resolution require integration with resolution systems for named entities, since surface forms often overlap (Moro et al., 2014; Navigli and Ponzetto, 2012). This will require inventory alignment work and model reformulation, since we currently use no document-level, topical, or knowledge-base coherence features."}, {"heading": "8 Conclusion", "text": "In this paper, we have presented several WSD algorithms which combine (1) neural network language models trained on a large unlabeled text corpus, with (2) labeled data in the form of example sentences, and, optionally, (3) unlabeled data in the form of additional sentences. Each of our algorithms looks for the most similar example sentences to the sentence in which the word we wish to classify appears. Using an LSTM language model gave better performance than one based on CBOW word embeddings, although both surpass the current state of the art. The best performance was achieved by our semi-supervised WSD algorithm which builds a graph containing labeled example sentences augmented with a large number of unlabeled sentences from the web, and classifies by propagating sense labels through this graph."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis", "author": ["Gabrilovich", "Shaul Markovitch"], "venue": "In IJCAI,", "citeRegEx": "Gabrilovich et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2007}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone", "author": ["Michael Lesk"], "venue": "In Proceedings of the 5th Annual International Conference on Systems Documentation,", "citeRegEx": "Lesk.,? \\Q1986\\E", "shortCiteRegEx": "Lesk.", "year": 1986}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Extensions of recurrent neural network language model", "author": ["Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Honza \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In arXiv preprint arXiv:1301.3781", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A semantic concordance", "author": ["Claudia Leacock", "Randee Tengi", "Ross T Bunker"], "venue": "In Proceedings of the workshop on Human Language Technology,", "citeRegEx": "Miller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1993}, {"title": "Semeval-2015 task 13: multilingual all-words sense disambiguation and entity linking", "author": ["Moro", "Navigli2015] Andrea Moro", "Roberto Navigli"], "venue": "Proc. of SemEval,", "citeRegEx": "Moro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moro et al\\.", "year": 2015}, {"title": "Entity linking meets word sense disambiguation: a unified approach. Transactions of the Association for Computational Linguistics, 2:231\u2013244", "author": ["Moro et al.2014] Andrea Moro", "Alessandro Raganato", "Roberto Navigli"], "venue": null, "citeRegEx": "Moro et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Moro et al\\.", "year": 2014}, {"title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "author": ["Navigli", "Ponzetto2012] Roberto Navigli", "Simone Paolo Ponzetto"], "venue": "Artificial Intelligence,", "citeRegEx": "Navigli et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2012}, {"title": "Semeval2007 task 07: Coarse-grained English all-words task", "author": ["Kenneth C Litkowski", "Orin Hargraves"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Navigli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2007}, {"title": "Semeval-2013 task 12: Multilingual word sense disambiguation", "author": ["David Jurgens", "Daniele Vannella"], "venue": "In Second Joint Conference on Lexical and Computational Semantics (* SEM),", "citeRegEx": "Navigli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2013}, {"title": "Meaningful clustering of senses helps boost word sense disambiguation performance", "author": ["Roberto Navigli"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Associ-", "citeRegEx": "Navigli.,? \\Q2006\\E", "shortCiteRegEx": "Navigli.", "year": 2006}, {"title": "Word sense disambiguation: A survey", "author": ["Roberto Navigli"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "Navigli.,? \\Q2009\\E", "shortCiteRegEx": "Navigli.", "year": 2009}, {"title": "Knowledge-rich word sense disambiguation rivaling supervised systems", "author": ["Ponzetto", "Roberto Navigli"], "venue": "In Proceedings of the 48th Annual Meeting of the Association", "citeRegEx": "Ponzetto et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ponzetto et al\\.", "year": 2010}, {"title": "Semeval-2007 task 17: English lexical sample, SRL and all words", "author": ["Edward Loper", "Dmitriy Dligach", "Martha Palmer"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Pradhan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2007}, {"title": "Large scale distributed semi-supervised learning using streaming approximation", "author": ["Ravi", "Diao2016] Sujith Ravi", "Qiming Diao"], "venue": "In AISTATS", "citeRegEx": "Ravi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2016}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "LSTM neural networks for language modeling", "author": ["Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In INTERSPEECH,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Semi-supervised word sense disambiguation using word embeddings in general and specific domains", "author": ["Taghipour", "Ng2015] Kaveh Taghipour", "Hwee Tou Ng"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter", "citeRegEx": "Taghipour et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Taghipour et al\\.", "year": 2015}, {"title": "New regularized algorithms for transductive learning", "author": ["Talukdar", "Koby Crammer"], "venue": "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II,", "citeRegEx": "Talukdar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Talukdar et al\\.", "year": 2009}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th annual meeting of the association for computational linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhong", "Ng2010] Zhi Zhong", "Hwee Tou Ng"], "venue": "ACLDemos", "citeRegEx": "Zhong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Word sense disambiguation (WSD) is a longstanding problem in natural language processing (NLP) with broad applications (Navigli, 2009).", "startOffset": 119, "endOffset": 134}, {"referenceID": 15, "context": "Supervised, unsupervised, and knowledge-based approaches have been studied for WSD (Navigli, 2009).", "startOffset": 83, "endOffset": 98}, {"referenceID": 17, "context": "However, for all-words WSD, where all words in a corpus need to be annotated with word senses, it has proven extremely challenging to beat the strong baseline, which always assigns the most frequent sense of a word without considering the context (Pradhan et al., 2007; Navigli et al., 2007; Navigli, 2009; Navigli et al., 2013; Moro and Navigli, 2015).", "startOffset": 247, "endOffset": 352}, {"referenceID": 12, "context": "However, for all-words WSD, where all words in a corpus need to be annotated with word senses, it has proven extremely challenging to beat the strong baseline, which always assigns the most frequent sense of a word without considering the context (Pradhan et al., 2007; Navigli et al., 2007; Navigli, 2009; Navigli et al., 2013; Moro and Navigli, 2015).", "startOffset": 247, "endOffset": 352}, {"referenceID": 15, "context": "However, for all-words WSD, where all words in a corpus need to be annotated with word senses, it has proven extremely challenging to beat the strong baseline, which always assigns the most frequent sense of a word without considering the context (Pradhan et al., 2007; Navigli et al., 2007; Navigli, 2009; Navigli et al., 2013; Moro and Navigli, 2015).", "startOffset": 247, "endOffset": 352}, {"referenceID": 13, "context": "However, for all-words WSD, where all words in a corpus need to be annotated with word senses, it has proven extremely challenging to beat the strong baseline, which always assigns the most frequent sense of a word without considering the context (Pradhan et al., 2007; Navigli et al., 2007; Navigli, 2009; Navigli et al., 2013; Moro and Navigli, 2015).", "startOffset": 247, "endOffset": 352}, {"referenceID": 15, "context": "Given the good performance of published supervised WSD systems when provided with significant training data on specific words (Navigli, 2009; Zhong and Ng, 2010), it appears lack of sufficient labeled training data for large vocabularies is the central problem.", "startOffset": 126, "endOffset": 161}, {"referenceID": 7, "context": "The first model is the continuous bag of words model (CBOW) (Mikolov et al., 2013).", "startOffset": 60, "endOffset": 82}, {"referenceID": 13, "context": "The recent development of large lexical resources, such as WordNet (Fellbaum, 1998) and BabelNet (Navigli and Ponzetto, 2012), has enabled knowledge-based algorithms which show promising results on all-word prediction tasks (Ponzetto and Navigli, 2010; Navigli et al., 2013; Moro and Navigli, 2015).", "startOffset": 224, "endOffset": 298}, {"referenceID": 17, "context": "WSD algorithms based on supervised learning are generally believed to perform better than knowledge-based WSD algorithms, but they need large training sets to perform well (Pradhan et al., 2007; Navigli et al., 2007; Navigli, 2009; Zhong and Ng, 2010).", "startOffset": 172, "endOffset": 251}, {"referenceID": 12, "context": "WSD algorithms based on supervised learning are generally believed to perform better than knowledge-based WSD algorithms, but they need large training sets to perform well (Pradhan et al., 2007; Navigli et al., 2007; Navigli, 2009; Zhong and Ng, 2010).", "startOffset": 172, "endOffset": 251}, {"referenceID": 15, "context": "WSD algorithms based on supervised learning are generally believed to perform better than knowledge-based WSD algorithms, but they need large training sets to perform well (Pradhan et al., 2007; Navigli et al., 2007; Navigli, 2009; Zhong and Ng, 2010).", "startOffset": 172, "endOffset": 251}, {"referenceID": 7, "context": "In the past few years, much progress has been made on using neural networks to learn word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014), to construct language models (Mikolov et al.", "startOffset": 101, "endOffset": 148}, {"referenceID": 6, "context": ", 2013; Levy and Goldberg, 2014), to construct language models (Mikolov et al., 2011), perform sentiment analysis (Socher et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 20, "context": ", 2011), perform sentiment analysis (Socher et al., 2013), machine translation (Sutskever et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 22, "context": ", 2013), machine translation (Sutskever et al., 2014) and many other NLP applications.", "startOffset": 29, "endOffset": 53}, {"referenceID": 6, "context": "In the past few years, much progress has been made on using neural networks to learn word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014), to construct language models (Mikolov et al., 2011), perform sentiment analysis (Socher et al., 2013), machine translation (Sutskever et al., 2014) and many other NLP applications. For the WSD problem, Rothe and Sch\u00fctze (2015) extended word embeddings to word sense or synset embeddings, and trained a SVM classifier (Zhong and Ng, 2010) with word sense and synset embeddings as features.", "startOffset": 102, "endOffset": 377}, {"referenceID": 6, "context": "In the past few years, much progress has been made on using neural networks to learn word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014), to construct language models (Mikolov et al., 2011), perform sentiment analysis (Socher et al., 2013), machine translation (Sutskever et al., 2014) and many other NLP applications. For the WSD problem, Rothe and Sch\u00fctze (2015) extended word embeddings to word sense or synset embeddings, and trained a SVM classifier (Zhong and Ng, 2010) with word sense and synset embeddings as features. Taghipour et al. (2015) trained a feedforward neural network with automatically labeled training samples to learn word embeddings for WSD.", "startOffset": 102, "endOffset": 563}, {"referenceID": 25, "context": "Distributional methods, in particular using word vectors trained using neural networks, have been shown to be extremely effective in NLP tasks (Turian et al., 2010; Baroni et al., 2014).", "startOffset": 143, "endOffset": 185}, {"referenceID": 0, "context": "Distributional methods, in particular using word vectors trained using neural networks, have been shown to be extremely effective in NLP tasks (Turian et al., 2010; Baroni et al., 2014).", "startOffset": 143, "endOffset": 185}, {"referenceID": 7, "context": "The CBOW model is trained to predict a target word with its context as input (Mikolov et al., 2013).", "startOffset": 77, "endOffset": 99}, {"referenceID": 7, "context": "We used 1000-dimensional embeddings from a 100-billion-word news corpus, trained using word2vec (Mikolov et al., 2013).", "startOffset": 96, "endOffset": 118}, {"referenceID": 21, "context": "Neural networks with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) make good language models which do consider word order (Sundermeyer et al., 2012).", "startOffset": 147, "endOffset": 173}, {"referenceID": 14, "context": "Previous investigations (Navigli, 2006; Navigli et al., 2007) using the ODE have shown that coarse-grained word senses induced by the ODE inventory address problems with WordNet\u2019s fine-grained inventory, and that the inventory is useful for word sense disambiguation.", "startOffset": 24, "endOffset": 61}, {"referenceID": 12, "context": "Previous investigations (Navigli, 2006; Navigli et al., 2007) using the ODE have shown that coarse-grained word senses induced by the ODE inventory address problems with WordNet\u2019s fine-grained inventory, and that the inventory is useful for word sense disambiguation.", "startOffset": 24, "endOffset": 61}, {"referenceID": 8, "context": "We manually annotated all words of the English language SemCor (Miller et al., 1993) corpus and MASC 2 corpora with NOAD word senses in order to evaluate performance.", "startOffset": 63, "endOffset": 84}, {"referenceID": 4, "context": "\u2022 Lesk: Annotate a word in a given context with the sense whose gloss shares the largest number of common words with the glosses of the other words in the context (Lesk, 1986).", "startOffset": 163, "endOffset": 175}, {"referenceID": 13, "context": "In this section, we study the performance of our classifiers on SemEval-2013 Task 12 (Navigli et al., 2013) and SemEval-2015 task 13 (Moro and Navigli, 2015).", "startOffset": 85, "endOffset": 107}, {"referenceID": 3, "context": "Character-level LSTMs (Kim et al., 2015) may provide robustness to morphology and diacritics and may prove useful even in English for spelling errors and out of vocabulary words.", "startOffset": 22, "endOffset": 40}, {"referenceID": 15, "context": "We would also like to investigate how work on unsupervised sense induction (Navigli, 2009) could be integrated with our models to detect out of vocabulary senses \u2013 particularly a problem in web text where terminology evolves rapidly.", "startOffset": 75, "endOffset": 90}, {"referenceID": 10, "context": "Finally, many applications of WSD systems for nominal resolution require integration with resolution systems for named entities, since surface forms often overlap (Moro et al., 2014; Navigli and Ponzetto, 2012).", "startOffset": 163, "endOffset": 210}], "year": 2017, "abstractText": "Determining the intended sense of words in text \u2013 word sense disambiguation (WSD) \u2013 is a long-standing problem in natural language processing. In this paper, we present WSD algorithms which use neural network language models to achieve state-of-the-art precision. Each of these methods learns to disambiguate word senses using only a set of word senses, a few example sentences for each sense taken from a licensed lexicon, and a large unlabeled text corpus. We classify based on cosine similarity of vectors derived from the contexts in unlabeled query and labeled example sentences. We demonstrate state-of-the-art results when using the WordNet sense inventory, and significantly better than baseline performance using the New Oxford American Dictionary inventory. The best performance was achieved by combining an LSTM language model with graph label propagation.", "creator": "LaTeX with hyperref package"}}}