{"id": "1603.07235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2016", "title": "Global-Local Face Upsampling Network", "abstract": "Face hallucination, which is the task of generating a high-resolution face image from a low-resolution input image, is a well-studied problem that is useful in widespread application areas. Face hallucination is particularly challenging when the input face resolution is very low (e.g., 10 x 12 pixels) and/or the image is captured in an uncontrolled setting with large pose and illumination variations. In this paper, we revisit the algorithm introduced in [1] and present a deep interpretation of this framework that achieves state-of-the-art under such challenging scenarios. In our deep network architecture the global and local constraints that define a face can be efficiently modeled and learned end-to-end using training data. Conceptually our network design can be partitioned into two sub-networks: the first one implements the holistic face reconstruction according to global constraints, and the second one enhances face-specific details and enforces local patch statistics. We optimize the deep network using a new loss function for super-resolution that combines reconstruction error with a learned face quality measure in adversarial setting, producing improved visual results. We conduct extensive experiments in both controlled and uncontrolled setups and show that our algorithm improves the state of the art both numerically and visually.", "histories": [["v1", "Wed, 23 Mar 2016 15:29:09 GMT  (4928kb,D)", "http://arxiv.org/abs/1603.07235v1", null], ["v2", "Wed, 27 Apr 2016 15:31:01 GMT  (4927kb,D)", "http://arxiv.org/abs/1603.07235v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["oncel tuzel", "yuichi taguchi", "john r hershey"], "accepted": false, "id": "1603.07235"}, "pdf": {"name": "1603.07235.pdf", "metadata": {"source": "CRF", "title": "Global-Local Face Upsampling Network", "authors": ["Oncel Tuzel", "Yuichi Taguchi", "John R. Hershey"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Face has been one of the main targets for image enhancement tasks. In particular, upsampling a low-resolution face image to a high-resolution one has been an important problem, called by its own name of face hallucination [2].\nThe problem is formulated as follows. Given a low-resolution NL = n \u00d7m face image IL, our goal is to obtain a photorealistic high-resolution NH = (dn)\u00d7 (dm) face image IH whose down-sampled version is equal to IL, where d is the upsampling factor. The relation can be written as\nxL = K xH , (1)\nwhere xL and xH are the low and high-resolution images stacked into column vectors and K is an NL\u00d7NH sparse matrix implementing low-pass filtering and down-sampling. To invert this largely (d2-times) under-determined linear system and recover the high-resolution image, additional constraints are needed.\nWe approximate the solution of this linear inverse problem using a deep neural network where facial constraints are explicitly modeled and learned using training data. Our considerations for the proposed face upsampling network\nar X\niv :1\n60 3.\n07 23\n5v 1\n[ cs\n.C V\n] 2\nare inspired by the face hallucination work of Liu et al. [1]. Similar to [1], we utilize the following three constraints to regularize the under-determined problem. (1) Global constraint: The reconstructed high-resolution face image should satisfy holistic constraints such as shape, pose, and symmetry, and should include detailed characteristic facial features such as eyes and nose. (2) Local constraint: Statistics of the reconstructed local image regions should match that of high-resolution face image patches (e.g., smooth regions with sharp boundaries), and should include face-specific details. (3) Data constraint: The reconstruction should be consistent with the observed low-resolution image and satisfy Eq. (1).\nLiu et al. [1] used a two-step approach according to these constraints. First a global face reconstruction is acquired using an eigenface model, which is a linear projection operation. In the second step details of the reconstructed face are enhanced by non-parametric patch transfer from a training set where consistency across neighboring patches is enforced through a Markov random field. This method produces high-quality results when the face images are near frontal, well aligned, and lighting conditions are controlled. However, when these assumptions are violated, the simple linear eigenface model fails to produce satisfactory global reconstruction. In addition, the patch transfer does not scale well with large datasets due to the nearest-neighbor (NN) search.\nIn this paper, we present a deep network architecture that resembles Liu et al.\u2019s framework [1] but solves the aforementioned problems for accurate and efficient face hallucination. Our network consists of the two sub-networks: the first one implements holistic face reconstruction according to the global constraints, and the second one enhances face-specific details and enforces local patch statistics. However, they are learned jointly using a large amount of training data, providing an optimized structure for upsampling. Moreover, the feed-forward operation provides computational efficiency in the test time. In extensive experiments using two benchmark datasets captured under controlled and uncontrolled setups, we show that our algorithm outperforms the state-of-the-art algorithms."}, {"heading": "1.1 Contributions", "text": "Our main contributions can be summarized as follows: (1) We present a deep interpretation of the global-local face hallucination framework [1]. (2) We design a deep network architecture that replaces the original two-step approach with an end-to-end learning and feed-forward operation, improving both the accuracy and speed. (3) We learn the deep network by minimizing a combination of reconstruction error and a learned face quality loss in adversarial setting, which produces high resolution images with improved visual quality. (4) We conduct extensive comparisons with the state-of-the-art algorithms and demonstrate that our algorithm outperforms them both qualitatively and quantitatively."}, {"heading": "1.2 Related Work", "text": "Face hallucination is the single-image super-resolution (SR) problem specific to face images. Single-image SR algorithms developed for generic images share\nthe same formulation in (1). To invert the under-determined system, local constraints are enforced as priors based on image statistics [3,4,5] and exemplar patches [6,7,8]. Global constraints are typically not available for the generic SR problem, which limits the plausible upsampling factor. Yang et al.\u2019s recent study [9] showed that 4\u00d7 upsampling results in the lower bound of the human perceptual scores.\nLiu et al. [1] used a global constraint for face hallucination based on eigenfaces [10], and proposed a two-step approach where the initial global reconstruction is improved by local non-parametric patch transfer [6]. As described above, the simple eigenface model has a difficulty when the datasets include large pose and illumination variations, and their local refinement process is computationally expensive due to the NN patch search. Ma et al. [11] assumed that training and test images are precisely aligned and searched the NN patches of a target pixel in the test image only at the specific pixel location in the training images. Using the location-specific patches provides global constraints implicitly, as long as the images are well-aligned. Yang et al. [12] partitioned a face image into three groups of facial components, contours, and smooth regions based on a facial landmark detection [13]. They used the NN search for each of the facial components with the training images, while for contours and smooth regions edge-based statistics and NN patch search were used. The result was generated by integrating gradient maps from the three groups and imposing them on the high-resolution image. Their method relies on the facial landmark detection and thus the result degrades for low-resolution input images where the landmark localization is typically inaccurate.\nIn the past several years, the success of deep learning methods has revolutionized the computer vision field from image classification [14,15] and object detection [16] to face recognition [17], segmentation [18], and video event detection [19]. These methods have also been replacing highly optimized handdesigned algorithms in low-level vision tasks such as image denoising [20,21,22], image enhancement [23,24], and SR [25,26,27]. Dong et al. [25] proposed superresolution convolutional neural network (SRCNN) for generic SR. They interpreted it as a deep network version of the conventional sparse coding methods [8]. SRCNN provides the state-of-the-art performance for generic SR, but not for face-specific SR as we compare in experiments. More recently, Wang et al. [26] proposed an improved deep model for generic SR that also takes into account self similarities. Zhou et al. [27] presented bi-channel convolutional neural network (BCCNN) for face-specific SR. They used a convolutional neural network architecture whose output was blended with the bicubic upsampled image using a weighting factor which is also predicted from the network. The last layer of this network linearly combines high-resolution basis images, which corresponds to a global face reconstruction and smooths out the person-specific details.\nBasic building blocks of our algorithm are well known neural network architectures such as encoder [28,29,30], convolutional [31], and deconvolutional [32,33] neural nets. Our architectural design enables effective learning of global and local\nconstraints that are important for face upsampling task using these well-known building blocks.\nRecently, generative adversarial networks (GANs) [34] have been proposed as an alternative to learn deep generative models. In GAN framework, a generative network learns to generate samples from a given data distribution, while simultaneously a discriminative network learns to identify the samples that are generated from this network. Since then, GANs have been successfully used for image [34,35], scene [36], and sequence synthesis [37] tasks. In this paper, we use GAN framework to learn a discriminative network which evaluates face quality, while at the same time optimizing the face super-resolution network according to the learned quality measure."}, {"heading": "2 Method Overview", "text": "Figure 1 shows an overview of our global-local face upsampling network (GLN). Our network consists of two sub-networks, referred to as Global Upsampling Network (GN) and Local Enhancement Network (LN), which model the global and local constraints for face hallucination. The operations performed by the two subnetworks are conceptually similar to Liu et al.\u2019s global reconstruction followed by local enhancement [1]. However, by jointly modeling and learning global and local constraints using a deep architecture, our method provides higher accuracy while being more efficient in the test time due to the feed-forward processing, as detailed below.\nHolistic face reconstruction according to global constraints is achieved using GN, which is a two stream neural network running in parallel. The first stream implements a simple interpolation-based upsampling of the low-resolution face using a deconvolutional network, producing a smooth image without details. The second stream produces the high frequency characteristic facial details, such as eyes and nose, using a fully connected neural network. Hidden layers of this encoder network [28,29] build a global representation of high-resolution face\nimages that can be inferred from the low-resolution input. Compared to the linear eigenface model of [1], the multi-layer nonlinear embedding and reconstruction used in our framework enables more effective encoding of characteristic facial features, in addition to variations such as alignment, face pose, and illumination. We concatenate the two streams generated by GN to be processed by LN.\nThe local constraints are modeled in LN using a fully convolutional neural network, which implements a shift-invariant nonlinear filter. This network enhances the face-specific local details by fusing the smooth and detail layers produced by the GN. Even though the convolutional filters are relatively small (5\u00d7 5 or 7\u00d7 7), by stacking many filters, the receptive field of the network becomes quite large (43 pixels in 8 layer network). The large receptive field enables resolving the ambiguity (e.g., eye region vs. mouth region) and the deep architecture has enough capacity to apply necessary filtering operation to a given region. Compared to the non-parametric detail transfer of [1], our structure is very efficient and produces higher quality enhancement.\nAlthough we do not explicitly model the data constraint within the network, by training the network using a large amount of training data, the network learns to produce high-resolution face images that are consistent with the lowresolution images according to Eq. (1). One could enforce the data constraint by using the back-projection (BP) algorithm [38] in a postprocessing step, which is a common approach used in many upsampling schemes. However, in our experiments, we found that the results directly obtained from our network and those obtained after the BP postprocessing were indistinguishable both qualitatively and quantitatively. Thus we did not use such a postprocessing."}, {"heading": "3 Global-Local Upsampling Network (GLN)", "text": "This section presents the details of our deep network architecture that is used to upsample the low-resolution face images. Our network structure is designed for very low-resolution input face images. We consider two upsampling factors: (1) 4\u00d7 upsampling where 32 \u00d7 32 input face image is mapped to 128 \u00d7 128 resolution; (2) 8\u00d7 upsampling where 16 \u00d7 16 input face image is mapped to 128 \u00d7 128 resolution. We have two different network configurations for the 4\u00d7 and 8\u00d7 cases. We assume that the low-resolution face images are roughly aligned."}, {"heading": "3.1 Global Upsampling Network (GN)", "text": "The structure of the GN is summarized in Table 1. GN is a two stream network running in parallel. The image upsampling stream maps the input face image to a high-resolution face image using linear interpolation. In our network, we implemented the image upsampling stream using a deconvolution layer [32,33]. We initialize the interpolation weights using bilinear matrix but allow the weights to change during training.\nThe global detail generation stream is implemented as a fully connected encoder network with 3 hidden layers. We use rectified linear unit (ReLU) af-\nter every linear map except for the last layer which generates the 128 \u00d7 128- dimensional upsampled global detail. In our encoder network the code layer is 256-dimensional, both for 4\u00d7 and 8\u00d7 upsampling networks. This is mainly dictated by the limited amount of training data where larger latent spaces for the global detail tend to overfit. Finally we concatenate the outputs of the image upsampling stream and the global detail generation stream, and form a 2\u00d7128\u00d7128 tensor to be processed by the LN.\nFigure 2 shows a typical output of the 8\u00d7 GN. Even though we allow the image upsampling stream\u2019s weights to change during training, the weights tend not to change much and the network implements a smooth upsampling (Figure 2(a)). The output of the global detail generation stream (Figure 2(b)) encodes high frequency details and more difficult to interpret. The pattern is more visible around the characteristic facial features such as eyes, nose and mouth."}, {"heading": "3.2 Local Refinement Network (LN)", "text": "The structure of the LN is summarized in Table 2. These structures are identical for 4\u00d7 and 8\u00d7 upsampling tasks. We analyzed three fully convolutional neural network architectures with different numbers of layers (LN4, LN6, and LN8). Before each convolution operation the image is padded with the ceiling of the half filter size so that the output image dimension is same as the input dimension. After every convolutional layer we apply ReLU except the last layer which\nconstructs the final upsampled image. We do not perform pooling and the stride is 1, therefore this network learns a very large shift-invariant nonlinear filter. As shown in Figure 2(c), the LN enhances the face specific local details by fusing the smooth and detail layers produced by the GN (see eyes and nose). In addition, the reconstructed image\u2019s local statistics match that of high-resolution face image patch statistics (e.g., smooth cheek region and sharp face boundaries)."}, {"heading": "3.3 Training", "text": "We conducted two stage training procedure. In the first stage, we train the network by minimizing a reconstruction error criteria. During the (optional) second stage, we fine-tune the network by minimizing a weighted combination of reconstruction error and a learned face quality loss function.\nTraining for reconstruction: We minimize the mean-squared loss between the ground truth high-resolution images and the reconstructed images to learn the network parameters. Let {(xiL, xiH)}i=1,...,n be the set of n low-resolution and high-resolution training image pairs. The loss is given by\nLMS = 1\nn n\u2211 i=1 \u2016G(xiL)\u2212 xiH\u20162, (2)\nwhere G(.) is the GLN function. The network is trained using stochastic gradient descent (SGD) on mini-batches of 5 images, with fixed learning rate schedule 10\u22128, and momentum 0.9.\nAdversarial fine-tuning: Mean-squared loss function prefers blurry reconstructions at the regions with high ambiguity, such as edges. Here we complement this loss function with a learned loss function, which is tuned to measure the quality of reconstruction by discriminating reconstructed images from the true high resolution images. We use a variant of the generative adversarial network framework proposed by Goodfellow et al. [34] to learn the discriminative loss function in conjunction with the GLN parameters.\nIn our framework, the discriminative network D(.) detects the images reconstructed by the GLN. It is trained to maximize the output probability when the input is reconstructed by the GLN and minimize the output probability when\nthe input is a true high resolution face image using the loss function:\nLD = \u2212 1\n2n n\u2211 i=1 ( log(1\u2212D(xiH)) + log(D(G(xiL))) ) . (3)\nThe GLN is trained both to minimize the reconstruction error LMS , and to confuse the discriminative network by minimizing the output probability of the discriminative network on the input reconstructed images. This is achieved by using the combined mean-squared and adversarial loss function:\nLG = LMS \u2212 \u03bb 1\nn n\u2211 i=1 log(1\u2212D(G(xiL))), (4)\nwhere \u03bb is a weighting factor between the two loss terms. Similar to [34], we alternate between minimizing LD with respect to parameters of the discriminative network, while keeping the GLN parameters fixed, and minimizing LG with respect to parameters of the GLN, while keeping discriminative network parameters fixed. We used 10 SGD iterations for discriminative network and 50 SGD iterations for the GLN during alternations. We switched 10000 times between optimizing discriminative network and the GLN.\nThe discriminative network was implemented as a convolutional neural network with four layers: (1) conv5-16, ReLU, MaxPool 2x2, (2) conv5-16, ReLU, MaxPool 2x2, (3) fc-50, ReLU, (4) fc-2. We started adversarial fine-tuning using the network trained for reconstruction only. The weighting factor \u03bb was set such that the initial adversarial loss was equal to 1/10 of the mean-squared loss. We used open source CAFFE library [39] to implement the networks."}, {"heading": "4 Experiments", "text": "We designed two sets of experiments under controlled and uncontrolled (in the wild) settings. The controlled setting was conducted using Face Recognition Grand Challenge (FRGC) dataset [40], where frontal face images were taken in a studio setting under two lighting conditions with only two facial expressions (smiling and neutral). We used a total of 22, 149 images, where 20, 000 images were used for training and 2, 149 for testing. We used a variant of the supervised descent face alignment algorithm [41] to detect facial landmarks. We then applied similarity transformation to the input face images (translation, rotation and uniform scaling) to approximately align detected eye and mouth center locations to a set of fixed points.\nThe uncontrolled setting was conducted using an aligned version of the Labeled Faced in the Wild dataset [42], called Labeled Faces in the Wild-a (LFWa) [43]. This dataset is intended for studying unconstrained face recognition problem and includes face images with various illumination, poses, and expressions. It contains 13, 233 face images from 1, 680 people, where we used 12, 000 for training and 1, 233 for testing. In both settings we kept the identities of the\npeople in the training and testing sets disjoint. Note that the alignment is quite noisy for both settings, particularly for the LFW-a dataset.\nWe evaluated two upsampling factors of 4\u00d7 and 8\u00d7. Faces occupied approximately 20 \u00d7 24 pixel area in the 4\u00d7 upsampling case and 10 \u00d7 12 pixel area in the 8\u00d7 upsampling case. The low-resolution images were generated using the procedure described in [9], filtering the high-resolution images with a Gaussian blur kernel \u03c3 followed by down-sampling. We used \u03c3 = 1.2 for 4\u00d7 down-sampling as suggested, and \u03c3 = 2.4 for 8\u00d7 down-sampling."}, {"heading": "4.1 Comparisons", "text": "We compared our method with the state-of-the-art generic and face-specific SR algorithms. As the generic SR algorithms, we used Kim and Kwon\u2019s algorithm (KK) [5] and the SRCNN algorithm [25], which are among the top performers in the evaluations reported in [9,25]. We used the implementations and pre-trained models (the 9-5-5 network for SRCNN) available on the authors\u2019 websites1. Since their algorithms were trained only up to 4\u00d7 upsampling factor, we performed 4\u00d7 upsampling followed by 2\u00d7 upsampling to generate 8\u00d7 upsampling results. As the face-specific SR algorithms, we used (1) Liu et al.\u2019s algorithm (LSF) [1]; (2) Ma et al.\u2019s (MZQ) [11]; (3) Yang et al.\u2019s (YLY) [12]; and (4) Zhou et al.\u2019s BCCNN [27]. We used Yang et al.\u2019s implementations [12] for LSF, MZQ, and YLY, and our own implementation for BCCNN. MZQ and BCCNN were trained using the same sets of training images as ours, while for LSF and YLY, we used 1/10 of the training images, because their run time increases linearly with the number of training images due to the NN patch and facial component search and it was impractical to run the algorithms using all the training images2. Our results presented in this section, both qualitatively and quantitatively, were obtained by minimizing the reconstruction error only. Note that after adversarial fine-tuning, the visual quality improves while quantitative results change marginally.\nQualitative Results: Figures 3 and 4 respectively show 4\u00d7 and 8\u00d7 upsampling results. Note that our input resolution is low (32\u00d7 32 and 16\u00d7 16 pixels), as it can be observed in the NN interpolation results. The bicubic interpolation results are blurry as expected. The generic SR algorithms (KK and SRCNN) sharpen the images by recovering some high-frequency components, but they do not reproduce face-specific features. The face-specific SR algorithms recover such facial features. In particular, we observed that MZQ and BCCNN produce visually pleasing results in our settings where the input resolution is low. However, since MZQ assumes precise alignments between the test and training images, the results degrade for the LFW-a dataset including larger alignment errors. Moreover, finding similar patches is hard for the case of 8\u00d7 upsampling, leading to inaccurate hallucination results for MZQ. BCCNN\u2019s network structure mainly performs global reconstruction by computing a weighted average of several highresolution basis images and the bicubic upsampled image in the last layer of\n1 We also retrained the SRCNN using our datasets. These models had almost identical PSNR (0.02dB better for both 4\u00d7 and 8\u00d7 upsampling) to the pre-trained models. 2 The global eigenface model for LSF was computed using all the training images.\nthe network. This structure is similar to the GN-Only stream of our network, which is detailed in Section 4.2, and provides high resolution images with global details (e.g., symmetry and characteristic facial features) while suffering from loss of person-specific details. YLY, which relies on the facial landmark detection [13], cannot produce results when the landmark detection fails; even if the detection is successful, the landmark localization accuracy is typically low for the low-resolution input, resulting in the facial features recovered at incorrect locations. We note that the resolution of our input images are lower than the images used in their original work. Our GLN produces globally consistent and locally sharp images, which are the closest to the ground truth (GT).\nQuantitative Results: In addition to the standard peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), we computed the information fidelity criterion (IFC) [44], weighted PSNR (WPSNR), and noise quality measure (NQM) [45] for quantitative evaluations as suggested in [9]. Tables 3 and\n4 show the results for different datasets and upsampling factors, demonstrating that our method provides the best performance in all the different metrics.\nRun Time: Table 5 compares the average run time for processing a single test image for FRGC 4\u00d7 upsampling. LSF and YLY are computationally expensive because of the NN search for each of the training images, requiring the run time linear to the number of training images. As described above, we used 1/10 of the training images for LSF and YLY; using all the training images took 10\u00d7 more run time. MZQ\u2019s run time also increases linearly with the number of training images, but it is faster than LSF and YLY since the NN patches are searched only at the specific pixel location. The other algorithms have the run time independent of the number of training images. The algorithms based on feed-forward neural networks, including ours, achieve efficient processing in the test time3. The run time of our algorithm was measured using Intel i7 CPU (single core implementation) and NVIDIA 780 GTX GPU.\n3 The run time of SRCNN was measured using the Matlab implementation available on the authors\u2019 website [25]. It would be the fastest with a C++/GPU implementation since their network has fewer arithmetic operations than BCCNN\u2019s and ours."}, {"heading": "4.2 Analysis of the Network Architecture", "text": "Here we analyze the sub-modules of our network. Figure 5 shows 4\u00d7 (top row) and 8\u00d7 (bottom row) upsampling results using the sub-modules\u2014global upsampling network and local refinement network. These networks are slightly different from the original networks such that (1) we train global detail generation stream (fully connected) of GN to directly produce the high-resolution image, which we call GN-Only; (2) we train LN8 network using only bilinear upsampled lowresolution image as the input, which we call LN-Only. As shown in Figure 5(a), GN-Only produces high quality global details such as symmetry and characteristic facial features, while smoothing out uncommon features (e.g., details on the cheeks and highlights) and producing high frequency artifacts that are not consistent with face patch statistics. Figure 5(b) shows an example of LN-Only result where global details such as characteristic features and symmetry are not preserved (especially for 8\u00d7), but the local patch statistics are consistent with face patch statistics (e.g., sharp edges) and local details are preserved. The results of 4\u00d7 upsampling is significantly better that 8\u00d7 upsampling using LN-Only where resolving patch level ambiguities is easier. The GLN successfully utilizes both global and local cues and produces significantly higher quality results than both sub-modules (Figure 5(c)).\nTable 6 shows quantitative comparisons using the sub-modules of our network. These results are consistent with the qualitative comparisons and show that LN-Only produces high quality 4\u00d7 upsampling and the results degrade at 8\u00d7. We also compare the variants of GLN with different numbers of convolutional layers as shown in Table 2. The results show that the improvement is significant from 4 to 6 convolutional layers but it starts saturating afterwards."}, {"heading": "4.3 Adversarial Fine-Tuning", "text": "Figure 6 compares the results of training the GLN using reconstruction cost only (top row) with fine-tuning the GLN using the combined adversarial loss function (bottom row), as explained in Section 3.3. The adversarial fine-tuning further improves the visual quality of the generated high-resolution face images where the images are sharper and have more characteristic details. However, this step marginally reduces the PSNR score\u20140.01dB and 0.25dB for 4\u00d7 and 8\u00d7 respectively. This is expected since the additional adversarial loss does not use the identity of the faces but only evaluates the quality of the generated face images.\nWe analyze the effect of the weighting factor, \u03bb, between the mean-squared loss and the adversarial loss, equation (4), on the super-resolution result. The 4\u00d7 upsampling results are shown in Figure 7, and 8\u00d7 upsampling results are shown in Figure 8. The weighting factors \u03bb = 103 (for 4\u00d7 upsampling) and \u03bb = 4 \u2217 103 (for 8\u00d7 upsampling) correspond to the results presented in Figure 6. We also present results for \u03bb = 2 \u2217 103 and \u03bb = 8 \u2217 103 for 4\u00d7 and 8\u00d7 upsampling respectively. The weighting factor \u03bb = 0 corresponds to training the network using only the mean-squared loss.\nThe more we weight the adversarial loss (larger \u03bb values), the reconstructed images become sharper and they include more facial details. However, with larger \u03bb we also observe some high frequency artifacts."}, {"heading": "4.4 Color Face Upsampling", "text": "Human vision is not sensitive to chrominance channels (u, v). Therefore, a common procedure for handling color images is to process only the luminance channel (Y) and add bicubic-upsampled chrominance channels to the result. We used the same procedure for obtaining color upsampling results shown in Figures 9 and 10."}, {"heading": "4.5 Failure Cases", "text": "Our method does not have major failure modes since it does not rely on very precise alignment. The algorithm produces less satisfactory results when there are big variations in pose and facial expression, and/or occlusion. Several less satisfactory results of our algorithm are shown in Figures 11 and 12."}, {"heading": "5 Conclusion", "text": "We proposed a face hallucination algorithm that produces high quality images even when the input face resolution is very low and the image is captured in an uncontrolled setting. The key element of our algorithm is a deep learning architecture that jointly learns global and local constraints of the high resolution faces. We conducted extensive comparisons with the state-of-the-art algorithms and showed improved performance."}], "references": [{"title": "Face hallucination: Theory and practice", "author": ["C. Liu", "H.Y. Shum", "W.T. Freeman"], "venue": "Int\u2019l J. Computer Vision 75(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Hallucinating faces", "author": ["S. Baker", "T. Kanade"], "venue": "Proc. IEEE Int\u2019l Conf. Automatic Face and Gesture Recognition.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Image upsampling via imposed edge statistics", "author": ["R. Fattal"], "venue": "ACM Trans. Graphics 26(3)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Image super-resolution using gradient profile prior", "author": ["J. Sun", "J. Sun", "Z. Xu", "H.Y. Shum"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Single-image super-resolution using sparse regression and natural image prior", "author": ["K.I. Kim", "Y. Kwon"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 32(6)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Example-based super-resolution", "author": ["W.T. Freeman", "T.R. Jones", "E.C. Pasztor"], "venue": "IEEE Computer Graphics and Applications 22(2)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Super-resolution from a single image", "author": ["D. Glasner", "S. Bagon", "M. Irani"], "venue": "Proc. IEEE Int\u2019l Conf. Computer Vision (ICCV).", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Trans. Image Proc. 19(11)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Single-image super-resolution: A benchmark", "author": ["C.Y. Yang", "C. Ma", "M.H. Yang"], "venue": "Proc. European Conf. Computer Vision (ECCV).", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "Journal of Cognitive Neuroscience 3(1)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1991}, {"title": "Hallucinating face by position-patch", "author": ["X. Ma", "J. Zhang", "C. Qi"], "venue": "Pattern Recognition 43(6)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Structured face hallucination", "author": ["C.Y. Yang", "S. Liu", "M.H. Yang"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Face detection, pose estimation, and landmark localization in the wild", "author": ["X. Zhu", "D. Ramanan"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 35(8)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Image denoising: Can plain neural networks compete with BM3D? In: Proc", "author": ["H.C. Burger", "C.J. Schuler", "S. Harmeling"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive multi-column deep neural networks with application to robust image denoising", "author": ["F. Agostinelli", "M.R. Anderson", "H. Lee"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural image denoising with convolutional networks", "author": ["V. Jain", "S. Seung"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep convolutional neural network for image deconvolution", "author": ["L. Xu", "J.S. Ren", "C. Liu", "J. Jia"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Image super-resolution using deep convolutional networks", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "arXiv preprint arXiv:1501.00092", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Selftuned deep super resolution", "author": ["Z. Wang", "Y. Yang", "Z. Wang", "S. Chang", "W. Han", "J. Yang", "T.S. Huang"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning face hallucination in the wild", "author": ["E. Zhou", "H. Fan", "Z. Cao", "Y. Jiang", "Q. Yin"], "venue": "Proc. AAAI Conf. Artificial Intelligence.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science 313(5786)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "Proc. Int\u2019l Conf. Machine Learning (ICML).", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proc. Int\u2019l Conf. Machine Learning (ICML).", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Proc. Neural Information Processing Systems (NIPS).", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R Fergus"], "venue": "Proc. Neural Information Processing Systems (NIPS).", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual structure using predictive generative networks", "author": ["W. Lotter", "G. Kreiman", "D. Cox"], "venue": "arXiv preprint arXiv:1511.06380", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving resolution by image registration", "author": ["M. Irani", "S. Peleg"], "venue": "CVGIP: Graphical models and image processing 53(3)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1991}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Overview of the face recognition grand challenge", "author": ["P.J. Phillips", "P.J. Flynn", "T. Scruggs", "K.W. Bowyer", "J. Chang", "K. Hoffman", "J. Marques", "J. Min", "W. Worek"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR). Volume 1.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "Supervised descent method and its applications to face alignment", "author": ["X. Xiong", "F. De la Torre"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Technical report, ECCV Workshop on Faces in Real-life Images", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Effective unconstrained face recognition by combining multiple descriptors and learned background statistics", "author": ["L. Wolf", "T. Hassner", "Y. Taigman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 33(10)", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "An information fidelity criterion for image quality assessment using natural scene statistics", "author": ["H.R. Sheikh", "A.C. Bovik", "G. De Veciana"], "venue": "IEEE Trans. Image Proc. 14(12)", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "Image quality assessment based on a degradation model", "author": ["N. Damera-Venkata", "T.D. Kite", "W.S. Geisler", "B.L. Evans", "A.C. Bovik"], "venue": "IEEE Trans. Image Proc. 9(4)", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "In this paper, we revisit the algorithm introduced in [1] and present a deep interpretation of this framework that achieves state-of-the-art under such challenging scenarios.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "In particular, upsampling a low-resolution face image to a high-resolution one has been an important problem, called by its own name of face hallucination [2].", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Similar to [1], we utilize the following three constraints to regularize the under-determined problem.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "[1] used a two-step approach according to these constraints.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "\u2019s framework [1] but solves the aforementioned problems for accurate and efficient face hallucination.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "Our main contributions can be summarized as follows: (1) We present a deep interpretation of the global-local face hallucination framework [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 2, "context": "To invert the under-determined system, local constraints are enforced as priors based on image statistics [3,4,5] and exemplar patches [6,7,8].", "startOffset": 106, "endOffset": 113}, {"referenceID": 3, "context": "To invert the under-determined system, local constraints are enforced as priors based on image statistics [3,4,5] and exemplar patches [6,7,8].", "startOffset": 106, "endOffset": 113}, {"referenceID": 4, "context": "To invert the under-determined system, local constraints are enforced as priors based on image statistics [3,4,5] and exemplar patches [6,7,8].", "startOffset": 106, "endOffset": 113}, {"referenceID": 5, "context": "To invert the under-determined system, local constraints are enforced as priors based on image statistics [3,4,5] and exemplar patches [6,7,8].", "startOffset": 135, "endOffset": 142}, {"referenceID": 6, "context": "To invert the under-determined system, local constraints are enforced as priors based on image statistics [3,4,5] and exemplar patches [6,7,8].", "startOffset": 135, "endOffset": 142}, {"referenceID": 7, "context": "To invert the under-determined system, local constraints are enforced as priors based on image statistics [3,4,5] and exemplar patches [6,7,8].", "startOffset": 135, "endOffset": 142}, {"referenceID": 8, "context": "\u2019s recent study [9] showed that 4\u00d7 upsampling results in the lower bound of the human perceptual scores.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "[1] used a global constraint for face hallucination based on eigenfaces [10], and proposed a two-step approach where the initial global reconstruction is improved by local non-parametric patch transfer [6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[1] used a global constraint for face hallucination based on eigenfaces [10], and proposed a two-step approach where the initial global reconstruction is improved by local non-parametric patch transfer [6].", "startOffset": 72, "endOffset": 76}, {"referenceID": 5, "context": "[1] used a global constraint for face hallucination based on eigenfaces [10], and proposed a two-step approach where the initial global reconstruction is improved by local non-parametric patch transfer [6].", "startOffset": 202, "endOffset": 205}, {"referenceID": 10, "context": "[11] assumed that training and test images are precisely aligned and searched the NN patches of a target pixel in the test image only at the specific pixel location in the training images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] partitioned a face image into three groups of facial components, contours, and smooth regions based on a facial landmark detection [13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] partitioned a face image into three groups of facial components, contours, and smooth regions based on a facial landmark detection [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "In the past several years, the success of deep learning methods has revolutionized the computer vision field from image classification [14,15] and object detection [16] to face recognition [17], segmentation [18], and video event detection [19].", "startOffset": 135, "endOffset": 142}, {"referenceID": 14, "context": "In the past several years, the success of deep learning methods has revolutionized the computer vision field from image classification [14,15] and object detection [16] to face recognition [17], segmentation [18], and video event detection [19].", "startOffset": 135, "endOffset": 142}, {"referenceID": 15, "context": "In the past several years, the success of deep learning methods has revolutionized the computer vision field from image classification [14,15] and object detection [16] to face recognition [17], segmentation [18], and video event detection [19].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "In the past several years, the success of deep learning methods has revolutionized the computer vision field from image classification [14,15] and object detection [16] to face recognition [17], segmentation [18], and video event detection [19].", "startOffset": 189, "endOffset": 193}, {"referenceID": 17, "context": "In the past several years, the success of deep learning methods has revolutionized the computer vision field from image classification [14,15] and object detection [16] to face recognition [17], segmentation [18], and video event detection [19].", "startOffset": 208, "endOffset": 212}, {"referenceID": 18, "context": "In the past several years, the success of deep learning methods has revolutionized the computer vision field from image classification [14,15] and object detection [16] to face recognition [17], segmentation [18], and video event detection [19].", "startOffset": 240, "endOffset": 244}, {"referenceID": 19, "context": "These methods have also been replacing highly optimized handdesigned algorithms in low-level vision tasks such as image denoising [20,21,22], image enhancement [23,24], and SR [25,26,27].", "startOffset": 130, "endOffset": 140}, {"referenceID": 20, "context": "These methods have also been replacing highly optimized handdesigned algorithms in low-level vision tasks such as image denoising [20,21,22], image enhancement [23,24], and SR [25,26,27].", "startOffset": 130, "endOffset": 140}, {"referenceID": 21, "context": "These methods have also been replacing highly optimized handdesigned algorithms in low-level vision tasks such as image denoising [20,21,22], image enhancement [23,24], and SR [25,26,27].", "startOffset": 130, "endOffset": 140}, {"referenceID": 22, "context": "These methods have also been replacing highly optimized handdesigned algorithms in low-level vision tasks such as image denoising [20,21,22], image enhancement [23,24], and SR [25,26,27].", "startOffset": 160, "endOffset": 167}, {"referenceID": 23, "context": "These methods have also been replacing highly optimized handdesigned algorithms in low-level vision tasks such as image denoising [20,21,22], image enhancement [23,24], and SR [25,26,27].", "startOffset": 160, "endOffset": 167}, {"referenceID": 24, "context": "These methods have also been replacing highly optimized handdesigned algorithms in low-level vision tasks such as image denoising [20,21,22], image enhancement [23,24], and SR [25,26,27].", "startOffset": 176, "endOffset": 186}, {"referenceID": 25, "context": "These methods have also been replacing highly optimized handdesigned algorithms in low-level vision tasks such as image denoising [20,21,22], image enhancement [23,24], and SR [25,26,27].", "startOffset": 176, "endOffset": 186}, {"referenceID": 26, "context": "These methods have also been replacing highly optimized handdesigned algorithms in low-level vision tasks such as image denoising [20,21,22], image enhancement [23,24], and SR [25,26,27].", "startOffset": 176, "endOffset": 186}, {"referenceID": 24, "context": "[25] proposed superresolution convolutional neural network (SRCNN) for generic SR.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "They interpreted it as a deep network version of the conventional sparse coding methods [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 25, "context": "[26] proposed an improved deep model for generic SR that also takes into account self similarities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] presented bi-channel convolutional neural network (BCCNN) for face-specific SR.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Basic building blocks of our algorithm are well known neural network architectures such as encoder [28,29,30], convolutional [31], and deconvolutional [32,33] neural nets.", "startOffset": 99, "endOffset": 109}, {"referenceID": 28, "context": "Basic building blocks of our algorithm are well known neural network architectures such as encoder [28,29,30], convolutional [31], and deconvolutional [32,33] neural nets.", "startOffset": 99, "endOffset": 109}, {"referenceID": 29, "context": "Basic building blocks of our algorithm are well known neural network architectures such as encoder [28,29,30], convolutional [31], and deconvolutional [32,33] neural nets.", "startOffset": 99, "endOffset": 109}, {"referenceID": 30, "context": "Basic building blocks of our algorithm are well known neural network architectures such as encoder [28,29,30], convolutional [31], and deconvolutional [32,33] neural nets.", "startOffset": 125, "endOffset": 129}, {"referenceID": 31, "context": "Basic building blocks of our algorithm are well known neural network architectures such as encoder [28,29,30], convolutional [31], and deconvolutional [32,33] neural nets.", "startOffset": 151, "endOffset": 158}, {"referenceID": 32, "context": "Basic building blocks of our algorithm are well known neural network architectures such as encoder [28,29,30], convolutional [31], and deconvolutional [32,33] neural nets.", "startOffset": 151, "endOffset": 158}, {"referenceID": 33, "context": "Recently, generative adversarial networks (GANs) [34] have been proposed as an alternative to learn deep generative models.", "startOffset": 49, "endOffset": 53}, {"referenceID": 33, "context": "Since then, GANs have been successfully used for image [34,35], scene [36], and sequence synthesis [37] tasks.", "startOffset": 55, "endOffset": 62}, {"referenceID": 34, "context": "Since then, GANs have been successfully used for image [34,35], scene [36], and sequence synthesis [37] tasks.", "startOffset": 55, "endOffset": 62}, {"referenceID": 35, "context": "Since then, GANs have been successfully used for image [34,35], scene [36], and sequence synthesis [37] tasks.", "startOffset": 70, "endOffset": 74}, {"referenceID": 36, "context": "Since then, GANs have been successfully used for image [34,35], scene [36], and sequence synthesis [37] tasks.", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "\u2019s global reconstruction followed by local enhancement [1].", "startOffset": 55, "endOffset": 58}, {"referenceID": 27, "context": "Hidden layers of this encoder network [28,29] build a global representation of high-resolution face", "startOffset": 38, "endOffset": 45}, {"referenceID": 28, "context": "Hidden layers of this encoder network [28,29] build a global representation of high-resolution face", "startOffset": 38, "endOffset": 45}, {"referenceID": 0, "context": "Compared to the linear eigenface model of [1], the multi-layer nonlinear embedding and reconstruction used in our framework enables more effective encoding of characteristic facial features, in addition to variations such as alignment, face pose, and illumination.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "Compared to the non-parametric detail transfer of [1], our structure is very efficient and produces higher quality enhancement.", "startOffset": 50, "endOffset": 53}, {"referenceID": 37, "context": "One could enforce the data constraint by using the back-projection (BP) algorithm [38] in a postprocessing step, which is a common approach used in many upsampling schemes.", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "In our network, we implemented the image upsampling stream using a deconvolution layer [32,33].", "startOffset": 87, "endOffset": 94}, {"referenceID": 32, "context": "In our network, we implemented the image upsampling stream using a deconvolution layer [32,33].", "startOffset": 87, "endOffset": 94}, {"referenceID": 33, "context": "[34] to learn the discriminative loss function in conjunction with the GLN parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Similar to [34], we alternate between minimizing LD with respect to parameters of the discriminative network, while keeping the GLN parameters fixed, and minimizing LG with respect to parameters of the GLN, while keeping discriminative network parameters fixed.", "startOffset": 11, "endOffset": 15}, {"referenceID": 38, "context": "We used open source CAFFE library [39] to implement the networks.", "startOffset": 34, "endOffset": 38}, {"referenceID": 39, "context": "The controlled setting was conducted using Face Recognition Grand Challenge (FRGC) dataset [40], where frontal face images were taken in a studio setting under two lighting conditions with only two facial expressions (smiling and neutral).", "startOffset": 91, "endOffset": 95}, {"referenceID": 40, "context": "We used a variant of the supervised descent face alignment algorithm [41] to detect facial landmarks.", "startOffset": 69, "endOffset": 73}, {"referenceID": 41, "context": "The uncontrolled setting was conducted using an aligned version of the Labeled Faced in the Wild dataset [42], called Labeled Faces in the Wild-a (LFWa) [43].", "startOffset": 105, "endOffset": 109}, {"referenceID": 42, "context": "The uncontrolled setting was conducted using an aligned version of the Labeled Faced in the Wild dataset [42], called Labeled Faces in the Wild-a (LFWa) [43].", "startOffset": 153, "endOffset": 157}, {"referenceID": 8, "context": "The low-resolution images were generated using the procedure described in [9], filtering the high-resolution images with a Gaussian blur kernel \u03c3 followed by down-sampling.", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "As the generic SR algorithms, we used Kim and Kwon\u2019s algorithm (KK) [5] and the SRCNN algorithm [25], which are among the top performers in the evaluations reported in [9,25].", "startOffset": 68, "endOffset": 71}, {"referenceID": 24, "context": "As the generic SR algorithms, we used Kim and Kwon\u2019s algorithm (KK) [5] and the SRCNN algorithm [25], which are among the top performers in the evaluations reported in [9,25].", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "As the generic SR algorithms, we used Kim and Kwon\u2019s algorithm (KK) [5] and the SRCNN algorithm [25], which are among the top performers in the evaluations reported in [9,25].", "startOffset": 168, "endOffset": 174}, {"referenceID": 24, "context": "As the generic SR algorithms, we used Kim and Kwon\u2019s algorithm (KK) [5] and the SRCNN algorithm [25], which are among the top performers in the evaluations reported in [9,25].", "startOffset": 168, "endOffset": 174}, {"referenceID": 0, "context": "\u2019s algorithm (LSF) [1]; (2) Ma et al.", "startOffset": 19, "endOffset": 22}, {"referenceID": 10, "context": "\u2019s (MZQ) [11]; (3) Yang et al.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "\u2019s (YLY) [12]; and (4) Zhou et al.", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "\u2019s BCCNN [27].", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "\u2019s implementations [12] for LSF, MZQ, and YLY, and our own implementation for BCCNN.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "YLY, which relies on the facial landmark detection [13], cannot produce results when the landmark detection fails; even if the detection is successful, the landmark localization accuracy is typically low for the low-resolution input, resulting in the facial features recovered at incorrect locations.", "startOffset": 51, "endOffset": 55}, {"referenceID": 43, "context": "Quantitative Results: In addition to the standard peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), we computed the information fidelity criterion (IFC) [44], weighted PSNR (WPSNR), and noise quality measure (NQM) [45] for quantitative evaluations as suggested in [9].", "startOffset": 171, "endOffset": 175}, {"referenceID": 44, "context": "Quantitative Results: In addition to the standard peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), we computed the information fidelity criterion (IFC) [44], weighted PSNR (WPSNR), and noise quality measure (NQM) [45] for quantitative evaluations as suggested in [9].", "startOffset": 232, "endOffset": 236}, {"referenceID": 8, "context": "Quantitative Results: In addition to the standard peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), we computed the information fidelity criterion (IFC) [44], weighted PSNR (WPSNR), and noise quality measure (NQM) [45] for quantitative evaluations as suggested in [9].", "startOffset": 282, "endOffset": 285}, {"referenceID": 4, "context": "KK [5] 28.", "startOffset": 3, "endOffset": 6}, {"referenceID": 24, "context": "SRCNN [25] 28.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "LSF [1] 25.", "startOffset": 4, "endOffset": 7}, {"referenceID": 10, "context": "MZQ [11] 29.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "YLY [12] 26.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "BCCNN [27] 28.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "KK [5] 27.", "startOffset": 3, "endOffset": 6}, {"referenceID": 24, "context": "SRCNN [25] 27.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "LSF [1] 22.", "startOffset": 4, "endOffset": 7}, {"referenceID": 10, "context": "MZQ [11] 26.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "YLY [12] 25.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "BCCNN [27] 26.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "3 The run time of SRCNN was measured using the Matlab implementation available on the authors\u2019 website [25].", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "Face hallucination, which is the task of generating a highresolution face image from a low-resolution input image, is a well-studied problem that is useful in widespread application areas. Face hallucination is particularly challenging when the input face resolution is very low (e.g., 10\u00d712 pixels) and/or the image is captured in an uncontrolled setting with large pose and illumination variations. In this paper, we revisit the algorithm introduced in [1] and present a deep interpretation of this framework that achieves state-of-the-art under such challenging scenarios. In our deep network architecture the global and local constraints that define a face can be efficiently modeled and learned end-to-end using training data. Conceptually our network design can be partitioned into two sub-networks: the first one implements the holistic face reconstruction according to global constraints, and the second one enhances face-specific details and enforces local patch statistics. We optimize the deep network using a new loss function for super-resolution that combines reconstruction error with a learned face quality measure in adversarial setting, producing improved visual results. We conduct extensive experiments in both controlled and uncontrolled setups and show that our algorithm improves the state of the art both numerically and visually.", "creator": "LaTeX with hyperref package"}}}