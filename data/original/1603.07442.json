{"id": "1603.07442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Pixel-Level Domain Transfer", "abstract": "We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.", "histories": [["v1", "Thu, 24 Mar 2016 05:20:59 GMT  (4963kb,D)", "http://arxiv.org/abs/1603.07442v1", "The code and the dataset will be available soon"], ["v2", "Mon, 29 Aug 2016 01:20:33 GMT  (4974kb,D)", "http://arxiv.org/abs/1603.07442v2", "The code and the dataset will be available soon"], ["v3", "Mon, 28 Nov 2016 13:17:40 GMT  (5118kb,D)", "http://arxiv.org/abs/1603.07442v3", "Published in ECCV 2016. Code and dataset available at dgyoo.github.io"]], "COMMENTS": "The code and the dataset will be available soon", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["donggeun yoo", "namil kim", "sunggyun park", "anthony s paek", "in so kweon"], "accepted": false, "id": "1603.07442"}, "pdf": {"name": "1603.07442.pdf", "metadata": {"source": "CRF", "title": "Pixel-Level Domain Transfer", "authors": ["Donggeun Yoo", "Namil Kim", "Sunggyun Park", "Anthony S. Paek", "In So Kweon"], "emails": ["dgyoo@rcv.kaist.ac.kr", "nikim@rcv.kaist.ac.kr", "sunggyun@kaist.ac.kr", "apaek@lunit.io", "iskweon@kaist.ac.kr"], "sections": null, "references": [{"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Fundamentals of cognition", "author": ["M.W. Eysenck"], "venue": "Psychology Press East Sussex, UK, USA and Canada", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science 313(5786)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International conference on artificial intelligence and statistics.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning, ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Rezende", "D. Wierstra"], "venue": "Proceedings of The 32nd International Conference on Machine Learning.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["J. Sohl-Dickstein", "E. Weiss", "N. Maheswaranathan", "S. Ganguli"], "venue": "Proceedings of The 32nd International Conference on Machine Learning.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative image modeling using spatial lstms", "author": ["L. Theis", "M. Bethge"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "Computer Vision\u2013ECCV 2010. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep domain adaptation for describing people based on fine-grained clothing attributes", "author": ["Q. Chen", "J. Huang", "R. Feris", "L.M. Brown", "J. Dong", "S. Yan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-domain image retrieval with a dual attribute-aware ranking network", "author": ["J. Huang", "R.S. Feris", "Q. Chen", "S. Yan"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["X. Yan", "J. Yang", "K. Sohn", "H. Lee"], "venue": "arXiv preprint arXiv:1512.00570", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R Fergus"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["A. Dosovitskiy", "J. Tobias Springenberg", "T. Brox"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": "arXiv preprint arXiv:1511.05440", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation 1(4)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M Bernstein"], "venue": "International Journal of Computer Vision 115(3)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-scale pyramid pooling for deep convolutional representation", "author": ["D. Yoo", "S. Park", "J.Y. Lee", "I. Kweon"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "To generate realistic target images, we employ the real/fake-discriminator in Generative Adversarial Nets [1], but also introduce a novel domain-discriminator to make the generated image relevant to the input image.", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "To generate mental images [2] of ourselves wearing clothes on a hanger is an effortless work for our brain.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Image generation has been attempted by a long line of works [3,4,5] but generating realistic images has been challenging since an image itself is high dimensional and has complex relations between pixels.", "startOffset": 60, "endOffset": 67}, {"referenceID": 3, "context": "Image generation has been attempted by a long line of works [3,4,5] but generating realistic images has been challenging since an image itself is high dimensional and has complex relations between pixels.", "startOffset": 60, "endOffset": 67}, {"referenceID": 4, "context": "Image generation has been attempted by a long line of works [3,4,5] but generating realistic images has been challenging since an image itself is high dimensional and has complex relations between pixels.", "startOffset": 60, "endOffset": 67}, {"referenceID": 0, "context": "However, recently a few machines have succeeded in generating realistic images [1,6,7,8], with the drastic advances of deep learning.", "startOffset": 79, "endOffset": 88}, {"referenceID": 5, "context": "However, recently a few machines have succeeded in generating realistic images [1,6,7,8], with the drastic advances of deep learning.", "startOffset": 79, "endOffset": 88}, {"referenceID": 6, "context": "However, recently a few machines have succeeded in generating realistic images [1,6,7,8], with the drastic advances of deep learning.", "startOffset": 79, "endOffset": 88}, {"referenceID": 7, "context": "However, recently a few machines have succeeded in generating realistic images [1,6,7,8], with the drastic advances of deep learning.", "startOffset": 79, "endOffset": 88}, {"referenceID": 8, "context": "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.", "startOffset": 60, "endOffset": 78}, {"referenceID": 9, "context": "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.", "startOffset": 60, "endOffset": 78}, {"referenceID": 10, "context": "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.", "startOffset": 60, "endOffset": 78}, {"referenceID": 11, "context": "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.", "startOffset": 60, "endOffset": 78}, {"referenceID": 12, "context": "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.", "startOffset": 60, "endOffset": 78}, {"referenceID": 13, "context": "different image domain has been proposed in computer vision [9,10,11,12,13,14], but all these adaptations take place in the feature space, i.", "startOffset": 60, "endOffset": 78}, {"referenceID": 14, "context": "However, training the converter is not straightforward because the target is not deterministic [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "[1] propose for generating realistic images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Secondly, in addition to the domain discriminator, we also employ the discriminator of [1], which is supervised by the labels of \u201creal\u201d or \u201cfake\u201d, to produce realistic images.", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].", "startOffset": 95, "endOffset": 102}, {"referenceID": 3, "context": "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].", "startOffset": 95, "endOffset": 102}, {"referenceID": 4, "context": "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].", "startOffset": 95, "endOffset": 102}, {"referenceID": 0, "context": "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].", "startOffset": 130, "endOffset": 142}, {"referenceID": 15, "context": "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].", "startOffset": 130, "endOffset": 142}, {"referenceID": 16, "context": "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].", "startOffset": 130, "endOffset": 142}, {"referenceID": 17, "context": "The image-generative models can be grouped into two families; generative parametric approaches [3,4,5] and adversarial approaches [1,16,17,18].", "startOffset": 130, "endOffset": 142}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Mirza and Osindero [16] extend GAN to a class conditional version, and Denton et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "[17] improve the image resolution in a coarse-to-fine fashion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] have proposed architectures named Deep Convolutional GANs, which is stable to be trained, and have succeeded in generating high quality images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Besides the two families, a recurrent network based model [6] and a deconvolutional network based model [19] have also been proposed.", "startOffset": 58, "endOffset": 61}, {"referenceID": 18, "context": "Besides the two families, a recurrent network based model [6] and a deconvolutional network based model [19] have also been proposed.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "We replace the generator with our converter which is an image-conditioned model, while [16] is class-conditional and [15] is attribute-conditional.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "We replace the generator with our converter which is an image-conditioned model, while [16] is class-conditional and [15] is attribute-conditional.", "startOffset": 117, "endOffset": 121}, {"referenceID": 19, "context": "[20] is similar to ours in that it is conditioned with video frames to produce next frames.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "For visual recognition, many methods to adapt domains [9,10,11] have been proposed.", "startOffset": 54, "endOffset": 63}, {"referenceID": 9, "context": "For visual recognition, many methods to adapt domains [9,10,11] have been proposed.", "startOffset": 54, "endOffset": 63}, {"referenceID": 10, "context": "For visual recognition, many methods to adapt domains [9,10,11] have been proposed.", "startOffset": 54, "endOffset": 63}, {"referenceID": 20, "context": "Especially for the recent use of the deep convolutional neural network [21], it has been common to", "startOffset": 71, "endOffset": 75}, {"referenceID": 21, "context": "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].", "startOffset": 97, "endOffset": 107}, {"referenceID": 23, "context": "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].", "startOffset": 97, "endOffset": 107}, {"referenceID": 24, "context": "pre-train a large network [22] over ImageNet [23] and transfer the parameters to a target domain [12,24,25].", "startOffset": 97, "endOffset": 107}, {"referenceID": 12, "context": "[13] and Huang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] address a gap between fashion shopping mall images and unconstrained human images for the clothing attribute recognition [13] and the product retrieval [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] address a gap between fashion shopping mall images and unconstrained human images for the clothing attribute recognition [13] and the product retrieval [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "[14] address a gap between fashion shopping mall images and unconstrained human images for the clothing attribute recognition [13] and the product retrieval [14].", "startOffset": 157, "endOffset": 161}, {"referenceID": 0, "context": "Generative Adversarial Nets (GAN) [1] is a generalized framework for generative models which [17,18,20] and we utilize for visual data.", "startOffset": 34, "endOffset": 37}, {"referenceID": 16, "context": "Generative Adversarial Nets (GAN) [1] is a generalized framework for generative models which [17,18,20] and we utilize for visual data.", "startOffset": 93, "endOffset": 103}, {"referenceID": 17, "context": "Generative Adversarial Nets (GAN) [1] is a generalized framework for generative models which [17,18,20] and we utilize for visual data.", "startOffset": 93, "endOffset": 103}, {"referenceID": 19, "context": "Generative Adversarial Nets (GAN) [1] is a generalized framework for generative models which [17,18,20] and we utilize for visual data.", "startOffset": 93, "endOffset": 103}, {"referenceID": 19, "context": "It has been well known that MSE is prone to produce blurry images because it inherently assumes that the pixels are drawn from Gaussian distribution [20].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "As in [1,17,18], the discriminator network guides the converter to produce realistic target under the supervision of real/fake.", "startOffset": 6, "endOffset": 15}, {"referenceID": 16, "context": "As in [1,17,18], the discriminator network guides the converter to produce realistic target under the supervision of real/fake.", "startOffset": 6, "endOffset": 15}, {"referenceID": 17, "context": "As in [1,17,18], the discriminator network guides the converter to produce realistic target under the supervision of real/fake.", "startOffset": 6, "endOffset": 15}, {"referenceID": 19, "context": "One supervision candidate to let the converter C meet the condition is the combined use of MSE with the real/fake loss, just as what [20] does for the video prediction.", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "The architecture of the real/fake discriminator is identical to that of [18] as illustrated in Fig.", "startOffset": 72, "endOffset": 76}, {"referenceID": 25, "context": "Several architecture families have been proposed to feed a pair of images to compare them but a simple stack across the channel axis has shown the best performance as studied in [26].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "With these two losses, we basically follow the adversarial training procedure of [1], as explained in Sec.", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "[13] also has presented a similar fashion dataset dealing with two domains.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] to stabilize the training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "To compensate the effect, as suggested in [15], we also measure the root mean square error in another setting where the \u201cConverter+RF-Discrim.", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "The user-study scores are normalized to a range of [0, 1].", "startOffset": 51, "endOffset": 57}, {"referenceID": 0, "context": "The scores are averaged and normalized to a range of [0, 1].", "startOffset": 53, "endOffset": 59}], "year": 2017, "abstractText": "We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator in Generative Adversarial Nets [1], but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.", "creator": "LaTeX with hyperref package"}}}