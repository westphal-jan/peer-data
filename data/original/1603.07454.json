{"id": "1603.07454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Deep Extreme Feature Extraction: New MVA Method for Searching Particles in High Energy Physics", "abstract": "In this paper, we present Deep Extreme Feature Extraction (DEFE), a new ensemble MVA method for searching $\\tau^{+}\\tau^{-}$ channel of Higgs bosons in high energy physics. DEFE can be viewed as a deep ensemble learning scheme that trains a strongly diverse set of neural feature learners without explicitly encouraging diversity and penalizing correlations. This is achieved by adopting an implicit neural controller (not involved in feedforward compuation) that directly controls and distributes gradient flows from higher level deep prediction network. Such model-independent controller results in that every single local feature learned are used in the feature-to-output mapping stage, avoiding the blind averaging of features. DEFE makes the ensembles 'deep' in the sense that it allows deep post-process of these features that tries to learn to select and abstract the ensemble of neural feature learners. With the application of this model, a selection regions full of signal process can be obtained through the training of a miniature collision events set. In comparison of the Classic Deep Neural Network, DEFE shows a state-of-the-art performance: the error rate has decreased by about 37\\%, the accuracy has broken through 90\\% for the first time, along with the discovery significance has reached a standard deviation of 6.0 $\\sigma$. Experimental data shows that, DEFE is able to train an ensemble of discriminative feature learners that boosts the overperformance of final prediction.", "histories": [["v1", "Thu, 24 Mar 2016 07:12:20 GMT  (1818kb)", "http://arxiv.org/abs/1603.07454v1", "20 pages, 9 figures"]], "COMMENTS": "20 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["chao ma", "tianchenghou", "bin lan", "jinhui xu", "zhenhua zhang"], "accepted": false, "id": "1603.07454"}, "pdf": {"name": "1603.07454.pdf", "metadata": {"source": "CRF", "title": "Deep Extreme Feature Extraction: New MVA Method for Searching Particles in High Energy Physics", "authors": ["Chao Ma"], "emails": ["20111003715@gdufs.edu.cn", "20130200814@gdufs.edu.cn", "20120200798@gdufs.edu.cn", "xujinh@indiana.edu", "zhangzhenhua@gdufs.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n07 45\n4v 1\n[ cs"}, {"heading": "1 Introduction", "text": "Particle accelerators are among of the most important tools in high energy physics research. The collision of proton creates a great amount of particles as well as a large number of data resource, which lays a foundation for the application of statistics as well as MVA techniques. The discovery of new particles is closely related to optimization of selection zone as well as the classification of\nsignal events and background events. Hence, an effective model of statistics and Machine Learning is playing an increasingly significant role in high energy physics [20,21,29,30]. Likewise, the challenging data from HEP would facilitate the invention and application of the new model of Machine Learning. The research to be conducted by is an aspect of this two-side promotion.\nHiggs boson, whose existence was temporarily confirmed in 2013, is an elementary particle in the Standard Model of particle physics [12]. In order to affirm the coupling effect between Higgs and Fermion and finally to verify the Standard Model, the study of decay channel\u03c4+\u03c4\u2212 through the large hadron collider (LHC) is of great significance [23]. However, Higgs boson is often buried by a large number of background events, which makes it hard to be detected. Recently, ATLAS has detected the evidence of the decay from Higgs boson to \u03c4+\u03c4\u2212 channel by BDT(Boosted Decision Tree, one of the state-of-the-art machine learning techniques). Since the signals are relatively weak and are buried in background noises. Hence, the significance of the observed deviation from BOH (short for Background-Only Hypothesis) is only 4.1 sigma. Hence, it is demonstrating to develop more sophisticated MVA methods which are expected to have higher sensitivity to signal events.\nOur research is based on several kinematic features(both low-level and high-level features) of final state productions of MC simulated events. Low-level features are physical quantities of decay production can be observed by detectors of LHC such as CMS. High-level features are derivatives of low-level features calculated by physicists. Identifying signal events (short for collision events created by the \u03c4+\u03c4\u2212 decay of Higgs boson) as well as selection region (short for the corresponding region of the decision areas of signal events in feature space) with a relatively high statistics significance and accuracy rate from a large amount of background events(short for non-Higgs-boson events), is a difficult issue due to the high dimensionality and imbalanced nature of the data. Therefore, relevant analysis is often based on sophisticate MVA methods based on machine Learning, such as Boosted Decision Tree and neural networks. In fact, the requirements of classifiers are becoming stricter in order to improve the searching efficiency of LHC searching for new particles as well as confidence level. The result of a growing number of researches suggest that, even with the help of experienced physicists, traditional classifiers such as SVM, NN, Decision Tree, Ensemble Learning and so forth, fail to detect all the significant structures hidden in data. Extracting high-level features automatically, Deep Learning is regarded as one of the new approaches to break through this limitation and promote the development high energy physics."}, {"heading": "2 Deep Learning and Related Works", "text": "As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28]. Deep Learning can not only automatically design more complicated, distinct and nonlinear features (called feature learning), but also mitigate the local extremum problem of classical training algorithm.\nHowever, the application of deep learning to high energy physics hasn\u2019t been studied until recently. Baldi.P et al.,2014 [3] initially applies the classical Deep Learning approach to the identification of the Higgs boson(the counter channel of bottom quark-anti bottom quark). The experiment result expresses that the nonlinear features designed by Deep Learning algorithm possess good prediction capability. Compared with the features designed by physicists(later referred to as \u2018high-level features\u2019), these nonlinear features increase the performance index by eight percent and reach the expected discovery significance(EDS) with five sigma. The result shows that deep neural network unearths some important features ignored by physicists without drawing support from physical expertise, which indicates that the superiority of Deep Learning approach can be fully applied to in the data analysis of Large Hadron Collider.\nIt is worth noting that, though the performance of deep learning approach outperforms the handdesigned features of physicists when using deep neural network of low level feature training, further experiment result clarifies that the addition of high-level features does not improve the classification performance of deep neural network. This phenomenon is explained as \u2018the algorithms are automatically discovering the insight contained in the high-level features\u2019 in the original paper of Baldi.P et al.,2014 [3].\nHowever, in our research, we found it is not the case. In the following part we would come up with a new model to give a different explanation to above-mentioned phenomenon, that deep neural\nnetworks actually fails to fully discover the insight contained in the high-level features or neither completely excavate low level features, hence resulted in the equivalent performances with or without high-level features. Thus, there is still a long way to go in the aspect of feature extraction.\nIn addition, classical deep learning algorithm needs a large number of training samples, thus resulted in a considerable amount of training time(it often takes days to train). In spite of using millions of training samples, the final accuracy index of the research is still less than 0.9, which also reflects the inefficiency of classical deep learning algorithms. Therefore, in conclusion, the research of applying deep learning to the discovery of new particle is still in the beginning stage, it still has certain one-sidedness in the extraction of high-level features and the optimizing of selection field."}, {"heading": "3 DEFE: the proposed method", "text": ""}, {"heading": "3.1 Introduction", "text": "Based on the analysis above, our research is focused on \u03c4+\u03c4\u2212 of the Higgs boson, and we propose a new MVA method\u2013 the Deep Extreme Feature Extraction(DEFE for short) model. The idea of the model is, instead of directly approximating the ideological selection region, we divide the sample-variable space supervisedly and train multiple SDAENN [27] as well as the so-called extreme selection region, using which as a bridge finally to approximate globally and optimize the selection region of the hadron signal events.\nMore specifically, we supervisedly operate the space partition of of product space between feature space and sample space by a weak classifier and divide it into a number of overlapping subspaces(this process is called the discriminative partition ), maintaining at the same time the ratio balance between the background events and signal events on each subspace. Based on this, we build a SDAENN for each subspace to process partial feature extraction. The resulting selection region is called extreme selection region. Finally, we take the union of the features over all subspaces and approximate extreme selection region globally by only a single terminal classifier, in order to achieve the goal of multi-perspective feature extraction and feature appreciation as well as covering the Higgs boson\u2019s selection region as much as possible. The resulting selection region is called the approximated extreme selection region. In some cases, the extracted features are further reduced by PCA to obtain linearly independent features. In a macro context, this model embeds several unsupervised feature extraction in a large-scale framework of supervised feature extraction, avoiding the blindness and locality of single unsupervised pre-training. Therefore, DEFE can be regarded as a new ensemble learning method, and it is a thorough ensemble learning rather than a voting based ensemble learning."}, {"heading": "3.2 Problem Formulation", "text": "Let the set of simulated event to be D = {(x1, y1, w1), ..., (xn, yn, wn)}, where xi \u2208 Rd, d is the dimensionality of the input feature, yi \u2208 {s, b}is the label of each event, meaning signal and background respectively. wi \u2208 R+ is weight associated with each event, which is intended to adjust the bias derived from the fact that the proportion of signal event in simulation may not be identical to the real prior class probability. Let S to be the set containing all signal events, B be the set containing background event, ns to be the number of signal events, and nb to be the number background events.The weight of each event should satisfy:\n\u2211\ni\u2208S\nwi = ns, \u2211\ni\u2208B\nwi = nb, (1)\nGiven a classifier g : Rd \u2192 {s, b}, we call G\u0302 = {x \u2208 Rd, g(x) = s} the approximate selection region of classifier g. Let G = {xi, yi = s}, GT = G \u22c2 G\u0302. Then n\u0302s = \u2211\ni\u2208GT wi is an unbiased\nestimator of the expected number of signal events which is selected by the classifier.\nThen, objective of the problem is now to maximize the approximate median significance (AMS) [1], which defined as:\nAMS =\n\u221a\n2((ns + nb + bregular) ln(1 + ns\nnb + bregular )\u2212 ns) (2)\nTo simplify the problem,in this paper weights are normalized to be uniformly distributed in S and B respectively, i.e.:\nwi =\n{\nns |S| i \u2208 S, nb |B| i \u2208 B"}, {"heading": "3.3 Extreme Feature Extraction As Ensemble Learning With Diversity", "text": "Before introducing the idea of Extreme Feature Extraction (EFE), we first briefly recap the formulation of ensemble learning that is closely related our proposed model here. Ensemble learning is an important strategy of improving the performance and accuracy of machine learning algorithms.Ensemble learning tries to learn a linear combination of base models of the following form:\nf(y|x; \u0398) = 1\n|M|\n\u2211\nm\u2208M\nf(y|x,\u0398m)\nThe key of the success of ensemble learning relies on the diversity of each base model f(y|x; \u0398m). If these base models are trained with decorrelated errors, their predictions can be averaged to improve performance. Thus, a set of classifiers (or experts) are trained to solve the same task under slightly modified settings (e.g., different batch of training examples, different set of variables, or different random initializations). During the test period, predictions from multiple classifiers are then averaged to a final prediction that is expected to be more accurate and robust.\nIt\u2019s natural to improve the performance of deep learning by training an ensemble of neural networks with different initializations. Ensemble deep learning forms many state-of-the-art solutions of different large scale tasks [22, 26]. However, in such vanilla ensemble learning, sub-neural networks are not trained with respect to an unified loss function (i.e., not ensemble awared), and no efforts are made to improve diversity [17]. To overcome this, different schemes of explicitly encouraging diversity or penalizing correlations [2,13,18] are proposed. It\u2019s then trivial to generalize these models to the task of feature learning by training auto-encoders as base models. Nevertheless, these frameworks are not well-suited for feature learning tasks, since model averaging are often taken over final output rather than features learned by base models. Direct averaging over learned features might be unstable. Also, vanilla ensembles of feature learners are generally \u2019shallow\u2019 in the sense that base models are ensembled linearly, which might have an impact of pushing each base models towards the target too aggresively, resulting in a potential reduction of diversity.\nNow we introduce an alternative scheme of performing ensemble feature learning, i.e. Extreme Feature Extraction (EFE). Let\nHm(x; \u0398 f m),m = 1, ..., |M|\nBe the set of neural feature mappings (which can be initialized by excatly the same initial parameters), where \u0398fm is the parameters of the m\nth feature map. Assume H be the matrix concatenating every sub feature matrix Hm. Thus, In EFE, the model can be described by the following feature extraction - output model:\nf(y|x; \u0398) = F(H(x; \u0398f ); \u0398o)\nWhere F is the deep neural predictor that defines the feature-to-output mapping and \u0398o the corresponding parameters. So far the structure of EFE bears no difference from classical deep neural nets. The discriminating feature of EFE that forces each neural feature extractor to be diverse is the implicit neural controller (gating function) that is not involved in the feedforward compuation with |M| dimensional output defined by g := g(x; \u0398g), which controls the gradient flow during learning:\n\u2202L0(y,x; \u0398)\n\u2202\u0398fm = gm(x; \u0398\ng) \u2202L0(y,x; \u0398)\n\u2202Hm\n\u2202Hm(x; \u0398 f m)\n\u2202\u0398m\nWhen these feature mappings are parameterized by deep neural networks, EFE model becomes Deep EFE (DEFE) model. The proposed EFE model has a number of desired properties. Firstly, the neural\ncontroller g directly distributes the gradient flows toward different feature learners, forcing thee learned features to be strongly diversed. Thus, EFE can be viewed as a ensemble learning scheme that only updates a small set of base feature learners by modifying the information of gradients, thus resulting in an diversed set of feature learners. Secondly, since EFE trains ensembles of feature learners without explicitly getting involved in the final averaging function, every single local feature learned are used in the feature-to-output mapping stage, avoiding the blind averaging of features. Thus, DEFE makes the ensemble \u2019deep\u2019 in the sense that it allows deep post-process of these features that tries to learn to select and abstract the ensemble of neural feature learners. Thirdly, even the feature-to-output mapping F is set to be an averaging error, diversity is still not eliminated due to the implicit controller g.\nHowever, these advantages come with the difficulty of training the gating function g due to the fact that g itself is not incoorporated into the loss function and network structure. In the following of the paper, we incoorporate the gating function into the loss function by simple linear combination:\nLEFE(y,x; \u0398) = \u03bbmin(Lg(y,x; \u0398 g), \u03b4) + L0(y,x; \u0398)\nWhere Lg(y,x; \u0398) is the loss function of training g toward target y. Through such incoorporation of gating function into the total loss function, discriminative information from output targets are allowed to train the gating function. We restrict |M| to be even: when the dimension of y is not equal to |M|, a binary tree of g (i.e., the discriminative partition to be introduced in the following of the paper) is trained to match the dimension of target and minimize min(Lg(y,x; \u0398), \u03b4). The reason that we employ min(\u00b7, \u03b4) on Lg(y,x; \u0398) is to restric the discriminative information from the targerts y, so that each feature learner are trained with approximatedly equal emphasis. Since training the model by an unified manner may be numerically stable and computationally expensive, in this paper, we introduce an algorithm in which g, H, and F are trained sequentially and greedily to obtain a good enough estimation of DEFE\u2019s parameters."}, {"heading": "3.4 Constructing and Learning of the Extreme Selection Region", "text": "In this section, we introduce the formal description of the pratical algorithm that trains an DEFE ensemble. We first give a few definitions needed to describe the DEFE algorithm:\nDefinition 1. Given a classifier g : Rd \u2192 {s, b}, we call G\u0302 = {x \u2208 Rd, g(x) = s}the approximate selection region of classifierg. Let G = {xi, yi = s}, then GT = G \u22c2\nG\u0302 is called the hit selection region.\nDefinition 2. Given a classifier g, the approximate rejection region is defined as H\u0302 = {xi, g(xi) = b}. Let H = {xi, yi = b}, then HT = H \u22c2 H\u0302 is the hit rejection region.\nDefinition 3. Given the classifier g, we call T = GT \u22c3\nHT the hit region, and F = X \\ T the anomalous region. Then, we can define the discriminative partition of the training example space as the tuples {T, F, G\u0302, H\u0302}.\nFrom the definition above, it\u2019s easy to see that the hit region and anomalous region is exactly the correctly classified and miss-classified samples, respectively. The reason that separate treatment of samples that counts for the fictious knowledge (i.e., {G\u0302, H\u0302}) of the weak classifier is that we want to further characterize the decision boundary trained by a first and quick \u2018glance\u2019 at the data. We can further perform discriminative partition over the resulting regions {T, F, G\u0302, H\u0302} respectively. By doing this procedure recursively for n iterations, we can obtain 4n partition of the sample space. In this paper, we consider the case that n is sufficiently small.\nHit region and anomalous region characterize the two different region of the sample space that exhibit potentially different patterns and distributions of high-level features, therefore a single classifier might fail to capture such information. To balance the number of samples of the partition, we normally set classifier to be either a weak classifier (e.g. Decision trees) or a neural network that is not fully trained. Furthermore, \u2018weak\u2019 discriminative partition obtained via such weak classifier is in fact the decision boundary trained by a first and quick \u2018glance\u2019 at the data, thus information containing the partition of {G\u0302, H\u0302} represents the subspace with principal different the structures hidden in the data. In contradiction to cluster analysis, discriminative partition tries to make use the information of the labels. The problems of overfitting might exist both due to the partition itself and the random errors from the weak classifier. To avoid this, we propose an additional procedure of\nrandom interchange, i.e. randomly select the samples from both hit region and anomalous region according to a preset ratio and switch these selected samples. This additional procedure will not only balance the partition, but also enhance the robustness.\nNow, we consider the partition against the feature space, i.e. the set containing every input attributes. In our work, we partition the feature set according to its physical interpretations. Note that overlapping of the partition is allowed. Given the partition S = \u22c3\nSi, we are now able to define the following procedures.\nDefinition 4. Let X = 4 n \u22c3\ni=1\nXi be a discriminative partition of the sample space, and S = m \u22c3\ni=1\nSi\na given partition of the feature space; Then we call X \u2297 F = ( \u22c3 Xi) \u2297 ( \u22c3\nSi) a partition of the sample-feature space. Every resulting subsets forms a new set of U = {Xi} \u2297 {Sj} = {(Xi, Sj)}, where \u2297 is the direct product.\nDefinition 5. From very subset Dh \u2208 U = {Xi} \u2297 {Sj}, h = 1, ..., 4n \u00d7m of the sample-feature space, we choose/train the corresponding classifier gh and its approximate selection region G\u0302h. Then, we define G\u0302E = \u22c3\nh G\u0302h, as the extreme selection region. Similarly, we can define as the extreme hit region GET = G \u22c2\nG\u0302E . The process of generating and constructing the extreme hit region based on the classifier chosen is called the expansion of the selection region. Similarly we can define the process of the expansion of H .\nIt\u2019s trivial to see that the process of expanding selection region always increases the number of samples that can be possibly covered by a set of multiple classifiers, i.e. GT \u2282 GET . However, one primal concern might be that since discriminative partition and expansion of selection region closely rely on the label of the data, how can one guarantee that the selection region is still expanded without the prior knowledge of labels of the testing data? The key fact to solve this question lies in the fact that apart from the training data (including labels), the definition of selection region only depends of the resulting decision boundaries that can be well described and parameterized by classifiers g and gh(even with simple rules in the case of decision tree based discriminative partitions). As a result, information regarding these regions are compressed by a limited number of classifiers rather than the raw sample-feature space X \u2297 S = ( \u22c3 Xi) \u2297 ( \u22c3\nSi). Thus, although the previously described expansion of selection region technique cannot be directly used for deriving a divideand-conquer mixture of classifier model, with the help of the resulting selection regions as stepping stones, \u2018extreme\u2019 information can then be unfolded and approximated by a single strong classifier.\nIn conclusion, the problem of improving the performance of deep learning can now be converted to the problem of approximating the expanded the selection region by merely a single classifier. In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier. As stated above, nevertheless, in the task of recognition of Higgs Bosons, this class of ensemble algorithms (Boosted Decision Tree for example) failed to significantly improve the performance of classification. The reason might be two folds: firstly, when applying divide-andconquer principle to the sample-feature space, only the shallow and presentational are exploited, thus missing local high-level information; secondly, only the weak classifiers\u2019 final output is considered, therefore in intrinsic structures and learned representational features are ignored. Also, it\u2019s too computational expensive to apply directly ensemble learning algorithms to deep learning algorithms."}, {"heading": "3.5 Greedy Training Algorithm for DEFE", "text": "To contribute to overcoming these difficulties, we have introduced the idea of feature learning from Deep Learning framework, and propose a new algorithm, the Deep Extreme Feature Extraction (DEFE). Now, we introduce a greedy training algorithm for DEFE. As depicted in Figure 1, in our prototype DEFE algorithm, the initial controller g is chosen to be a neural network or decision tree, and Hh to be the Stacked Denoising Autoencoder Neural Networks (SDANN). In this setting, DEFE is not allowed to utilize the output of each classifier; instead, the union of all the high level features (the output of the final hidden layer) learned by each SDANN (i.e., the feature set of G\u0302E ). Based on this feature set, a final deep neural predictor is employed to reorganize the extreme feature set, and learn to approximate the extreme selection region G\u0302E . By establishing this framework, both advantages of prior experiences of extreme selection region and the feature extraction power of deep learning techniques are combined. The local features on G\u0302E is thus reorganized into high-\nlevel features learned by the final deep classifier. With the existing mature training algorithms of deep learning to train the final deep classifier, the expensive computational cost of apply ensemble learning directly to learning the gating weights of each classifier gh can be also avoided. The DEFE algorithm applied to the optimization of recognizing Higgs Bosons are described as follows:\nInput: the sample-feature space X \u2297 S, labels {yi}, and interchange rate \u03b1. We assume n = 1 and m = 1.\nStep 1. (Discriminative Partition): Train a neural controller on X , and obtain a partition of {T, F, G\u0302, H\u0302}.\nStep 2. (Random Interchange) According to an interchange rate \u03b1, randomly exchange the elements between F and T , G\u0302 and H\u0302, respectively.\nStep 3. (Partition of feature set): Given the feature set S, we deploy an overlapping partition. In the task of LHC hadron collisions, feature sets are partitioned as S = S1 \u22c3 S2 \u22c3\nS3, where S1 is the momentum features, S2 is the derivative of physical attributes, and S3 = S is the entire feature set.\nStep 4. (The construction of extreme selection region): So far we obtained a partition U of the sample-feature space X \u2297 S. For every Uh,h = 1, 2, ..., 4n \u00d7m, we train an SDAENN, denoted as Hh. Note that the number of units in the first layer far outnumbers the length of input vector, and the number of hidden units at each layer decreases gradually to a fix number K as depth increases to compress the information. In order to make every SDAENN equally important,K is fix as 50. All SDAENNs are trained unsupervised in order to learn non-trivial features (or optionally followed by supervised finetuning step with very few epochs). By training these 4n \u00d7m SDAENNs, we obtained implicitly the extreme selection region G\u0302E .\nStep 5. (Combining extreme feature set): For every Hh, we take their output Sh = {Sh1, Sh2, ..., ShK , } of the last hidden layers. Then, the extreme feature set can be constructed as SE = \u22c3 h Sh, and the new sample-feature space becomes X \u2297 SE .\nStep 6. (Learning and approximating G\u0302E): Finally, we train an deep neural network F on X \u2297SE as a final classifier with stochastic gradient descent. The resulting decision boundary will be a improved estimation of G\u0302E ."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Methodology", "text": "Based on the simulated data, the proposed Deep Extreme Feature Extraction (DEFE) is used to learn the selection region (or extreme selection region G\u0302E ). Goodness of such approximation is usually measured by various metric. In this paper, The metric used for goodness of fit comparison is the total area under the Receiver Operating Characteristic curve (ROC), i.e. The AUC metric. In general, a higher value of AUC represents higher classification accuracy averaged across a wide range of different choices of threshold. Expected significance of a discovery (in units of sigmas) is also calculated for 100 signal events and 1,000 background events. It denotes the significance of null selection region hypothesis (or the discovery significance) [8]. If the resulting P-value of null selection hypothesis is below certain value (normally required to be one millionth or lower, corresponding to discovery significance greater than 5-sigma), then the declaration of a new physics can be made. Once the selection region been trained, the model is ready for analyzing real experimental data."}, {"heading": "4.2 Data", "text": "The data we use in our experiment is obtained from the Higgs Boson Machine Learning Challenge(data can be downloaded at http://www.kaggle.com/c/higgs-boson). Data is generated by an official simulator of ATLAS, with Higgs to Tau-Tau events mixed with different backgrounds. Based on current knowledge of particle physics, random collisions are simulated, tracked and detected by simulated detector. The mass of the Higgs Boson is fixed at 125GeV, considering the following collision event:\n1. Signal Event: The Higgs boson decays into \u03c4+\u03c4\u2212.\n2. Background Event 1: The Z bosons (91.2 GeV) decay into \u03c4+\u03c4\u2212, which is similar to the signal event and becoming the difficult point in classification.\n3. Background Event 2: A pair of top quarks is involved, accompany with lepton and hadronic decayed \u03c4 .\n4. Background Event 3: W bosons decay into an electron or an muon and a hadronic decayed tau.\nThe total number of events is 250,000. For any given collision event, the following 30 input attributes are obtained, with 17 low-level features measured by the detector and 13 additional high-level features calculated from low-level features,see Table 1."}, {"heading": "4.3 Parameters and Training Strategy", "text": "We use a hundred thousand samples to train the DEFE model, and use about eighty thousand samples to test the DEFE model. ROC (Receiver Operating Characteristic Curve) is used to visualize the performance and. The AUC (Area under the Curve of ROC) and Expected Discovery Significance are used to quantify the performance.\nAll data are normalized. Afterwards, we do an n = 1 discriminative partition, on each subset, with random swap ratio \u03b1=0.05. In other words, we partition the original dataset into four overlapped subsets. Finally, we employ SDAENN to gain the high level feature on an m = 3 partitioned feature space, on each of the data subset, gaining altogether twelve high-level feature sets. The SDAENNs\nare chosen to have fifty output unit, so by complying the steps above, we can ultimately obtain the so called \u201cextreme features\u201d with 12\u00d750=600 dimensions. And then, before inputting into the DNN classifier, we reduce the dimension to 300 by PCA.\nIn our model, each of the SDAENN on their corresponding sample-feature partition is set to have the following parameters: For all SDAENN: Totally five hidden layers, the output layer has fifty neural units. For each feature space, the structure of each hidden layer is given as S1 : {250,200,150,100,50};S2: {200,200,150,100,50};S3: {300,250,200,200,50}.\nThe activation function is set to be sigmoid function, and the training algorithm is plain stochastic gradient descent with batch training and momentum (batch size is one hundred, and momentum is 0.5), and learning ratio is 0.1 in the beginning and decrease in the training process, the descending ratio is 0.997. Under the fine-tuning phase, we adopt the following early-stop strategy: Stop training if cross-validation error of SDAENN increase to 0.002 above the minimum, or the change of cost is lower than 0.0001 after 10 iteration. Under this strategy, the fine-tuning normally stop after 70\u223c120 iteration. This effectively deterred over-fitting. For each neural feature learner, we adopt an additional supervised fine tuning step with only 10 epochs. The parameter stated above is also used in the terminal classifier (DNN). Drop-out training technique is not used because of the deterioration on accuracy."}, {"heading": "5 Results", "text": "Table 1 demonstrates the collection of the thirty-dimensional feature used in our model. In Table 2 we observe the comparison of AUC accuracy rate between DEFE model and other baseline models. Among which, the training sets contain 140,000 samples, and if not specially addressed, low-level features and high-level features are all adopted( if not adopt high-level features, then the performance of DEFE and DNN are much equivalent ). The expected significance of a discovery (in units of Gaussians) for 100 signal events and 1,000 background events. The calculation of expected statistical significance is referred to the method presented in document [3]. In [3], a slightly different task that the case of a pair of leptonic decay of Taus is considered. Due to the similarities of both events and features, their results are also listed for comparison.\nCompared with classic Deep Neural Network (DNN) under the restriction of 90% background rejection rate, the error rate of DEFE drops by approximately 37%, and the precision indicator of AUC breaks through 90% for the first time, with statistical significance reaching as high as 6.0 \u03c3. It is also worth noting that, unlike DNN, the additional high-level features promote the accuracy of DEFE significantly. In other word, DEFE can learn essential features more effectively from additional high-level features.\nFinally, it\u2019s worth mentioning that DEFE does capture some important features of Higgs \u2192 \u03c4+\u03c4\u2212 channel. Appendix I illustrates 20% of the features extracted by DEFE. Obviously, automatically learned features by DEFE exploit to the full the discriminative power hidden under raw input features. Note the great diversity among different feature learners trained by DEFE algorithm. Among high-level features, there are still some important patterns that are unidentified by DNN and are independent from low-level features, therefore the DNN\u2019s treatment of high-level and low-level features are insufficient, while DEFE is able to identify these significant patterns more efficiently. With\nthe state-of-the-art performances of the proposed method, we hope to improve the analyzing quality of HEP data and the statistical significance of confirming the physical facts."}, {"heading": "6 Conclusion", "text": "In this paper we propsed a novel ensemble deep learning technique, Deep Extreme Feature Extraction (DEFE), to the task of identifying Higgs Bosons(Tau-Tau channel) from background signal. Based the construction and approximation of the so-called extreme selection region, the model is able to efficiently extract discriminative features from multiple angles and dimensions and therefore boost the overall performance. The result is improved in approximately one \u03c3 compared to DNN. In comparison with traditional deep learning algorithm, we discover that performance of DEFE is significantly boosted with high-level feature inputs, avoiding the equivalent performances with or without high-level features. This results indicates that unlike vanilla deep neural network, DEFE successfully trains a diverse set of neural feature learners, and discover the excess discriminative information contained in the high-level features. In the future, it\u2019s still an open question to propose further training algorithms to train an EFE model universally and efficiently"}], "references": [{"title": "Learning to discover: the higgs boson machine learning challenge", "author": ["Claire Adam-Bourdarios", "Glen Cowan", "Cecile Germain", "Isabelle Guyon", "Balzs Kgl", "David Rousseau"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Fast decorrelated neural network ensembles with random weights", "author": ["Monther Alhamdoosh", "Dianhui Wang"], "venue": "Information Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Searching for exotic particles in high-energy physics with deep learning", "author": ["P. Baldi", "P. Sadowski", "D. Whiteson"], "venue": "Nature Communications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations & Trends in Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Bagging predictors", "author": ["Leo Breiman"], "venue": "In Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Asymptotic formulae for likelihood-based tests of new physics", "author": ["Glen Cowan", "Kyle Cranmer", "Eilam Gross", "Ofer Vitells"], "venue": "European Physical Journal C,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Generalization bounds for averaged classifiers", "author": ["Yoav Freund", "Yishay Mansour", "Robert E. Schapire"], "venue": "Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "Journal of Computer & System Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1986}, {"title": "Postuse review: Introduction to elementary particles", "author": ["David Griffiths", "Gerald W. Intemann"], "venue": "American Journal of Physics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Multiple choice learning: Learning to produce multiple structured outputs", "author": ["Abner Guzman-Rivera", "Dhruv Batra", "Pushmeet Kohli"], "venue": "Nips, pages 1799\u20131807,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "Eprint Arxiv,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V. Le", "Rajat Monga", "Matthieu Devin", "Greg Corrado", "Kai Chen", "Marc\u2019Aurelio Ranzato", "Jeffrey Dean", "Andrew Y. Ng"], "venue": "CoRR, abs/1112.6209,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Why M heads are better than one: Training a diverse ensemble of deep networks", "author": ["Stefan Lee", "Senthil Purushwalkam", "Michael Cogswell", "David J. Crandall", "Dhruv Batra"], "venue": "CoRR, abs/1511.06314,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Ensemble learning via negative correlation", "author": ["Y. Liu", "X. Yao"], "venue": "Neural Networks the Official Journal of the International Neural Network Society,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio Speech & Language Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Tau identification using multivariate techniques in atlas", "author": ["D.C. O\u2019Neil", "Atlas Collaboration"], "venue": "In Journal of Physics Conference Series,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Boosted decision trees as an alternative to artificial neural networks for particle identification", "author": ["Byron P. Roe", "Hai Jun Yang", "Ji Zhu", "Yong Liu", "Ion Stancu", "Gordon Mcgregor"], "venue": "Nuclear Instruments & Methods in Physics Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Evidence for higgs boson decays to the \u03c4\u03c4 final state with the atlas detector", "author": ["Nils Ruthmann"], "venue": "GeV-cms,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Theoretical Views of Boosting and Applications", "author": ["Robert E. Schapire"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "Eprint Arxiv,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "Wei Liu", "Yangqing Jia", "P. Sermanet"], "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih Volodymyr", "Kavukcuoglu Koray", "Silver David", "Andrei A Rusu", "Veness Joel", "Marc G Bellemare", "Graves Alex", "Riedmiller Martin", "Andreas K Fidjeland", "Ostrovski Georg"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Machine learning for event selection in high energy physics", "author": ["Shimon Whiteson", "Daniel Whiteson"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}], "referenceMentions": [{"referenceID": 18, "context": "Hence, an effective model of statistics and Machine Learning is playing an increasingly significant role in high energy physics [20,21,29,30].", "startOffset": 128, "endOffset": 141}, {"referenceID": 19, "context": "Hence, an effective model of statistics and Machine Learning is playing an increasingly significant role in high energy physics [20,21,29,30].", "startOffset": 128, "endOffset": 141}, {"referenceID": 27, "context": "Hence, an effective model of statistics and Machine Learning is playing an increasingly significant role in high energy physics [20,21,29,30].", "startOffset": 128, "endOffset": 141}, {"referenceID": 10, "context": "Higgs boson, whose existence was temporarily confirmed in 2013, is an elementary particle in the Standard Model of particle physics [12].", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "In order to affirm the coupling effect between Higgs and Fermion and finally to verify the Standard Model, the study of decay channel\u03c4\u03c4 through the large hadron collider (LHC) is of great significance [23].", "startOffset": 201, "endOffset": 205}, {"referenceID": 3, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 7, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 12, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 13, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 14, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 17, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 23, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 26, "context": "2 Deep Learning and Related Works As a new learning algorithm of Multilayer neural network, Deep Learning [4], has become a great interest in the field of machine learning research, and achieved great success in various of tasks [7,9,14\u201316,19,25,28].", "startOffset": 229, "endOffset": 249}, {"referenceID": 2, "context": ",2014 [3] initially applies the classical Deep Learning approach to the identification of the Higgs boson(the counter channel of bottom quark-anti bottom quark).", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": ",2014 [3].", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "The idea of the model is, instead of directly approximating the ideological selection region, we divide the sample-variable space supervisedly and train multiple SDAENN [27] as well as the so-called extreme selection region, using which as a bridge finally to approximate globally and optimize the selection region of the hadron signal events.", "startOffset": 169, "endOffset": 173}, {"referenceID": 0, "context": "Then, objective of the problem is now to maximize the approximate median significance (AMS) [1], which defined as: 3", "startOffset": 92, "endOffset": 95}, {"referenceID": 20, "context": "Ensemble deep learning forms many state-of-the-art solutions of different large scale tasks [22, 26].", "startOffset": 92, "endOffset": 100}, {"referenceID": 24, "context": "Ensemble deep learning forms many state-of-the-art solutions of different large scale tasks [22, 26].", "startOffset": 92, "endOffset": 100}, {"referenceID": 15, "context": ", not ensemble awared), and no efforts are made to improve diversity [17].", "startOffset": 69, "endOffset": 73}, {"referenceID": 1, "context": "To overcome this, different schemes of explicitly encouraging diversity or penalizing correlations [2,13,18] are proposed.", "startOffset": 99, "endOffset": 108}, {"referenceID": 11, "context": "To overcome this, different schemes of explicitly encouraging diversity or penalizing correlations [2,13,18] are proposed.", "startOffset": 99, "endOffset": 108}, {"referenceID": 16, "context": "To overcome this, different schemes of explicitly encouraging diversity or penalizing correlations [2,13,18] are proposed.", "startOffset": 99, "endOffset": 108}, {"referenceID": 4, "context": "In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier.", "startOffset": 38, "endOffset": 52}, {"referenceID": 8, "context": "In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier.", "startOffset": 38, "endOffset": 52}, {"referenceID": 9, "context": "In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier.", "startOffset": 38, "endOffset": 52}, {"referenceID": 22, "context": "In previous work of ensemble learning [5,6,10,11,24] tries to unify every sub-classifier gh by an ensemble procedure of linear weighting, voting or winner-take-all, and achieves a fairly good result compared to single classifier.", "startOffset": 38, "endOffset": 52}, {"referenceID": 6, "context": "It denotes the significance of null selection region hypothesis (or the discovery significance) [8].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "The calculation of expected statistical significance is referred to the method presented in document [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "In [3], a slightly different task that the case of a pair of leptonic decay of Taus is considered.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "In the future, it\u2019s still an open question to propose further training algorithms to train an EFE model universally and efficiently References [1] Claire Adam-Bourdarios, Glen Cowan, Cecile Germain, Isabelle Guyon, Balzs Kgl, and David Rousseau.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "[2] Monther Alhamdoosh and Dianhui Wang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Yoshua Bengio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Leo Breiman and Leo Breiman.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Ronan Collobert and Jason Weston.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Glen Cowan, Kyle Cranmer, Eilam Gross, and Ofer Vitells.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Yoav Freund, Yishay Mansour, and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Yoav Freund and Robert E Schapire.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] David Griffiths and Gerald W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, and Adam Coates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Quoc V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Byron P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, and Michael Bernstein.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Nils Ruthmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Karen Simonyan and Andrew Zisserman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre Antoine Manzagol.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Mnih Volodymyr, Kavukcuoglu Koray, Silver David, Andrei A Rusu, Veness Joel, Marc G Bellemare, Graves Alex, Riedmiller Martin, Andreas K Fidjeland, and Ostrovski Georg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Shimon Whiteson and Daniel Whiteson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Appendix II: Definition of Input variables [1]: 1.", "startOffset": 43, "endOffset": 46}], "year": 2016, "abstractText": "In this paper, we present Deep Extreme Feature Extraction (DEFE), a new ensemble MVA method for searching \u03c4\u03c4 channel of Higgs bosons in high energy physics. DEFE can be viewed as a deep ensemble learning scheme that trains a strongly diverse set of neural feature learners without explicitly encouraging diversity and penalizing correlations. This is achieved by adopting an implicit neural controller (not involved in feedforward compuation) that directly controls and distributes gradient flows from higher level deep prediction network. Such modelindependent controller results in that every single local feature learned are used in the feature-to-output mapping stage, avoiding the blind averaging of features. DEFE makes the ensembles \u2019deep\u2019 in the sense that it allows deep post-process of these features that tries to learn to select and abstract the ensemble of neural feature learners. Based the construction and approximation of the so-called extreme selection region, the DEFE model is able to be trained efficiently, and extract discriminative features from multiple angles and dimensions, hence the improvement of the selection region of searching new particles in HEP can be achieved. With the application of this model, a selection regions full of signal process can be obtained through the training of a miniature collision events set. In comparison of the Classic Deep Neural Network, DEFE shows a state-of-the-art performance: the error rate has decreased by about 37%, the accuracy has broken through 90% for the first time, along with the discovery significance has reached a standard deviation of 6.0 \u03c3. Experimental data shows that, DEFE is able to train an ensemble of discriminative feature learners that boosts the overperformance of final prediction. Furthermore, among high-level features, there are still some important patterns that are unidentified by DNN and are independent from low-level features, while DEFE is able to identify these significant patterns more efficiently.", "creator": "LaTeX with hyperref package"}}}