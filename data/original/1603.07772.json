{"id": "1603.07772", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks", "abstract": "Skeleton based action recognition distinguishes human actions using the trajectories of skeleton joints, which provide a very good representation for describing actions. Considering that recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) can learn feature representations and model long-term temporal dependencies automatically, we propose an end-to-end fully connected deep LSTM network for skeleton based action recognition. Inspired by the observation that the co-occurrences of the joints intrinsically characterize human actions, we take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. To train the deep LSTM network effectively, we propose a new dropout algorithm which simultaneously operates on the gates, cells, and output responses of the LSTM neurons. Experimental results on three human action recognition datasets consistently demonstrate the effectiveness of the proposed model.", "histories": [["v1", "Thu, 24 Mar 2016 22:43:55 GMT  (874kb,D)", "http://arxiv.org/abs/1603.07772v1", "AAAI 2016 conference"]], "COMMENTS": "AAAI 2016 conference", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["wentao zhu", "cuiling lan", "junliang xing", "wenjun zeng", "yanghao li", "li shen", "xiaohui xie"], "accepted": true, "id": "1603.07772"}, "pdf": {"name": "1603.07772.pdf", "metadata": {"source": "META", "title": "Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks", "authors": ["Wentao Zhu", "Cuiling Lan", "Junliang Xing", "Wenjun Zeng", "Yanghao Li", "Li Shen", "Xiaohui Xie"], "emails": ["wentaoz1@uci.edu,", "culan@microsoft.com,", "wezeng@microsoft.com,", "jlxing@nlpr.ia.ac.cn,", "lyttonhao@pku.edu.cn,", "li.shen@vipl.ict.ac.cn,", "xhx@ics.uci.edu"], "sections": [{"heading": "1 Introduction", "text": "Recognizing human actions has remained one of the most important and challenging tasks in computer vision. It facilitates a wide range of applications such as intelligent video surveillance, human-computer interaction, and video understanding (Poppe 2010; Weinland, Ronfard, and Boyerc 2011).\nTraditional studies on action recognition mainly focus on recognizing actions from RGB videos recorded by 2D cameras (Weinland, Ronfard, and Boyerc 2011). However, capturing human actions in the full 3D space in which they actually occur can provide more comprehensive information. Biological observations suggest that humans can recognize actions from just the motion of a few light displays attached to the human body (Johansson 1973). Motion capture systems (CMU 2003) extract 3D joint positions using markers and high precision camera arrays. Although slightly higher in price, such systems provide highly accurate joint positions for skeletons. Recently, the Kinect device has gained much\n\u2217This work was done when W. Zhu was an intern at Microsoft Research Asia. Copyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\npopularity thanks to its excellent accuracy in human body modeling and affordable price. The bundled SDK for Kinect v2 can directly generate accurate skeletons in real-time. Due to the prevalence of these devices, skeleton based representations of the human body and its temporal evolution has become an attractive option for action recognition.\nIn this paper, we focus on the problem of skeleton based action recognition. The key to this problem lies mainly in two aspects. One is to design robust and discriminative features from the skeleton (and the corresponding RGBD images) for intra-frame content representation (Mu\u0308ller, Ro\u0308der, and Clausen 2005; Wang, Liu, and Junsong 2012; Sung et al. 2012; Yang and Tian 2014; Ji, Ye, and Cheng 2014). The other is to explore temporal dependencies of the interframe content for action dynamics modeling, using hierarchical maximum entropy Markov model (Sung et al. 2011), hidden Markov model (Xia, Chen, and Aggarwal 2012) or Conditional Random Fields (Sminchisescu et al. 2005). Inspired by the success of deep recurrent neural networks (RNNs) using the Long Short-Term Memory (LSTM) architecture for speech feature learning and time series modeling (Graves, Mohamed, and Hinton 2013; Graves and Schmidhuber 2005), we intend to build an effective action recognition model based on deep LSTM network.\nTo this end, we propose an end-to-end fully connected deep LSTM network to perform automatic feature learning and motion modeling (Fig. 1). The proposed network is constructed by inheriting many insights from recent successful networks (Graves 2012; Krizhevsky, Sutskever, and Hinton 2012; Szegedy et al. 2015; Du, Wang, and Wang 2015) and is designed to robustly model complex relationships among different joints. The LSTM layers and feedforward layers are alternately deployed to construct a deep network to capture the motion information. To ensure the model learns effective features and motion dynamics, we enforce different types of strong regularization in different parts of the model, which effectively mitigates over-fitting.\nSpecifically, two types of regularizations are proposed. (i) For the fully connected layers, we introduce regularization to drive the model to learn co-occurrence features of the joints at different layers. (ii) For the LSTM neurons, we derive a new dropout and apply it to the LSTM neurons in the last LSTM layer, which helps the network to learn complex motion dynamics. With these forms of regularization, we\nar X\niv :1\n60 3.\n07 77\n2v 1\n[ cs\n.C V\n] 2\n4 M\nar 2\n01 6\nvalidate our deep LSTM networks on three public datasets for human action recognition. The proposed model has been shown to consistently outperform other state-of-the-art algorithms for skeleton based human action recognition."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Activity Recognition with Neural Networks", "text": "In contrast to the handcrafted features, there is a growing trend of learning robust feature representations from raw data with deep neural networks, and excellent performance has been reported in image classification (Krizhevsky, Sutskever, and Hinton 2012) and speech recognition (Graves, Mohamed, and Hinton 2013). However, there are only few works which leverage neural networks for skeleton based action recognition. A multi-layer perceptron network is trained to classify each frame (Cho and Chen 2014); however, such a network cannot explore temporal dependencies very well. In contrast, a gesture recognition system (Lefebvre et al. 2013) employs a shallow bidirectional LSTM with only one forward hidden layer and one backward hidden layer to explore long-range temporal dependencies. A deep recurrent neural network architecture with handcrafted subnets is utilized for skeleton based action recognition (Du, Wang, and Wang 2015). However, the handcrafted hierarchical subnets and their fusion ignore the inherent cooccurrences of joints. This motivates us to design a deep fully connected neural network which is capable of fully exploiting the inherent correlations among skeleton joints in various actions."}, {"heading": "2.2 Co-occurrence Exploration", "text": "An action is usually only associated with and characterized by the interactions and combinations of a subset of the skeleton joints. For example, the joints \u201chand\u201d, \u201carm\u201d and \u201chead\u201d are associated with the action \u201cmaking telephone call\u201d. An actionlet ensemble model exploits this trait by mining some particular conjunctions of the features corresponded to some subsets of the joints (Wang, Liu, and Junsong 2012). Similarly, actions involving two people can be characterized by the interactions of a subset of the two persons\u2019 joints (Yun et al. 2012; Ji, Ye, and Cheng 2014). Inspired by the actionlet ensemble model, we introduce a new exploration mechanism in the deep LSTM architecture to achieve automatic co-occurrence mining as opposed to pre-specifying in advance which joints should be grouped."}, {"heading": "2.3 Dropout for Recurrent Neural Networks", "text": "Dropout has been demonstrated to be quite effective in deep convolutional neural networks (Krizhevsky, Sutskever, and Hinton 2012), but there has been relatively little research on applying it to RNNs. In order to preserve the ability of RNNs to model sequences, dropout applied only to the feedforward (along layers) connections but not to the recurrent (along time) connections is proposed (Pham et al. 2014). This is to avoid erasing all the information from the units (due to dropout). Note that the previous work only considers dropout at the output response for an LSTM neuron\n(Zaremba, Sutskever, and Vinyals 2014). However, considering that an LSTM neuron consists of internal cell and gate units, we believe one should not only look at the output of the neuron but also into its internal structure to design effective dropout schemes. In this paper, we design an in-depth dropout for LSTM to address this problem."}, {"heading": "3 Deep LSTM with Co-occurrence Exploration and In-depth Dropout", "text": "Leveraging the insights from recent successful networks, we design a fully connected deep LSTM network for skeleton based action recognition. Fig. 1 shows the architecture of the proposed network, which has three bidirectional LSTM layers, two feedforward layers, and a softmax layer that gives the predictions. The proposed full connection architecture enables one to fully exploit the inherent correlations among skeleton joints. In the network, the co-occurrence exploration is applied to the connections prior to the second LSTM layer to learn the co-occurrences of joints/features. LSTM dropout is applied to the last LSTM layer to enable more effective learning. Note that each LSTM layer uses bidirectional LSTM and we do not explicitly distinguish the forward and backward LSTM neurons in Fig. 1. At each time step, the input to the network is a vector denoting the 3D positions of the skeleton joints in a frame.\nIn the following, we first review LSTM briefly to make the paper self-contained. Then we introduce our method for cooccurrence exploration in the deep LSTM network. Lastly we describe our dropout algorithm which is designed for the LSTM neurons and enables effective learning of the model."}, {"heading": "3.1 Overview of LSTM", "text": "The RNN is a successful model for sequential learning (Graves 2012). For the recurrent neurons at some layer, the output responses ht are calculated based on the inputs xt to this layer and the responses ht\u22121 from the previous time slot\nht = \u03b8 (Wxhxt + Whhht\u22121 + bh) , (1)\nwhere \u03b8 (\u00b7) denotes the activation function, bh denotes the bias vector, Wxh is the matrix of weights between the input and hidden layer and Whh is the matrix of recurrent weights from the hidden layer to itself at adjacent time steps which is used for exploring temporal dependency.\nLSTM is an advanced RNN architecture which can learn long-range dependencies (Graves, Mohamed, and Hinton 2013). Fig. 2 shows a typical LSTM neuron, which contains an input gate it, a forget gate ft, a cell ct, an output gate ot and an output response ht. The input gate and forget gate govern the information flow into and out of the cell. The output gate controls how much information from the cell is passed to the output ht. The memory cell has a selfconnected recurrent edge of weight 1, ensuring that the gradient can pass across many time steps without vanishing or exploding. Therefore, it overcomes the difficulties in training the RNN model caused by the \u201cvanishing gradient\u201d effect. For all the LSTM neurons in some layer, at time t, the recursive computation of activations of the units is\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi) ,\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf ) ,\nct = ft ct\u22121+ it tanh(Wxcxt+Whcht\u22121+bc) , ot = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo) ,\nht = ot tanh (ct) ,\n(2)\nwhere denotes element-wise product, \u03c3 (x) is the sigmoid function defined as \u03c3 (x) = 1/(1+e\u2212x), W\u03b1\u03b2 is the weight matrix between \u03b1 and \u03b2 (e.g., Wxi is the weight matrix from the inputs xt to the input gates it), and b\u03b2 denotes the bias term of \u03b2 with \u03b2 \u2208 {i, f, c, o}. Four weight matrixes are associated with input xt. To allow the information from both the future and the past to determine the output, bidirectional LSTM can be utilized (Graves 2012)."}, {"heading": "3.2 Co-occurrence Exploration", "text": "The fully connected deep LSTM network in Fig. 1 has very powerful learning capability. However, it is difficult to learn directly due to the huge parameter space. To overcome this problem, we introduce a co-occurrence exploration process to ensure the deep model learns effective features.\nThe co-occurrence of some joints can intrinsically characterize a human action. Fig. 3 shows two examples. For \u201cwalking\u201d, the joints from hands and feet have high correlations but they all have low correlations with the joint of root. The sets of correlated joints for \u201cwalking\u201d and \u201cdrinking\u201d are very different, indicating the discriminative subset of joints varies for different types of actions. Two main aspects have been considered in our design of the network and the specialized regularization we propose. (i) We expect the network can automatically explore the conjunctions of discriminative joints. (ii) We expect the network can explore different co-occurrences for different types of actions.\nTherefore, we design the fully connected network to allow each neuron being connected to any joints (for the first layer) or responses of the previous layer (for the second or higher layer) to automatically explore the co-occurrences. Note that the output responses are also referred to as features which are the input of the next layer. We divide the neurons in the same layer into K groups to allow different groups to focus on exploration of different conjunctions of discriminative joints. Taking the kth group of neurons as an example (see Fig. 4 (a)), the neurons will automatically connect the discriminative joints/features. In our design, we incorporate the co-occurrence regularization into the loss function\nmin Wx\u03b2 L+ \u03bb1 \u2211 \u03b2\u2208S \u2016Wx\u03b2\u20161+\u03bb2 \u2211 \u03b2\u2208S K\u2211 k=1 \u2225\u2225\u2225Wx\u03b2,kT\u2225\u2225\u2225 2,1 , (3)\nwhereL is the maximum likelihood loss function of the deep LSTM network (Graves 2012). The other two terms are used for the co-occurrence regularization which can be applied to each layer. Wx\u03b2 = [Wx\u03b2,1; \u00b7 \u00b7 \u00b7 ;Wx\u03b2,K ] \u2208 RN\u00d7J is the connection weight matrix from inputs to the units associated with \u03b2 \u2208 S, with N indicating the number of neurons and J the dimension of inputs (e.g., for the first layer, J is the number of joints multiplied by 3). The N neurons are partitioned into K groups and the number of neurons in a group\nis L=dN/Ke, with d\u00b7e representing the rounding up operation. Wx\u03b2,k is the matrix composed of the (L(k\u22121)+1)th to (Lk)th rows of Wx\u03b2 . Wx\u03b2,kT denotes its transpose. S denotes the set of internal units which are directly connected to the input of a neuron. For the LSTM layer, S={i, f, c, o} denotes the gates and cell in LSTM neurons. For the feedforward layer, S={h} denotes the neuron itself.\nIn the third term, for each group of units, a structural `21 norm, which is defined as \u2016W\u20162,1= \u2211 i \u221a\u2211 jw 2 i,j (Cotter et\nal. 2005), is used to drive the units to select a conjunction of descriptive inputs (joints/features) since the `21 norm can encourage the matrix Wx\u03b2,k to be column sparse. Different groups explore different connection (co-occurrence) patterns in order to acquire the capability of recognizing multiple categories of actions. The `1 norm constraint (the second term) helps to learn discriminative joints/features.\nThe stochastic gradient descent method is then employed to solve (3). The advantage of the co-occurrence learning is that the model can automatically learn the discriminative joint/feature connections, avoiding the fixed a priori blocking of joint co-occurrences across human parts (Du, Wang, and Wang 2015) as illustrated in Fig. 4 (b)."}, {"heading": "3.3 In-depth Dropout for LSTM", "text": "Dropout tries to combine the predictions of many \u201cthinned\u201d networks to boost the performance. During training, the network randomly drops some neurons to force the remaining sub-network to compensate. During testing, the network uses all the neurons together to make predictions.\nTo extend this idea to LSTM, we propose a new dropout algorithm to allow the dropping of the internal gates, cell and output response for an LSTM neuron, encouraging each unit to learn better parameters. For clarity, an LSTM neuron is shown in Fig. 5 (a) in the unfolded form, where the units are explicitly connected. For recurrent neural networks, the erasing of all the information from a unit is not expected, especially when the unit remembers events that occurred many timesteps back in the past (Pham et al. 2014). Therefore, we allow the influence of dropout in LSTM to flow along layers (marked by dashed arrows) but prohibit it to flow along the time axis (marked by solid arrows) as illustrated in Fig. 5\n(b). To control the influence flows, in the feedforward process, the network calculates and records two types of activations as follows. The responses of units to be transmitted along the time without dropout are\ni\u0303t = \u03c3 ( Wxixt + Whih\u0303t\u22121 + Wcic\u0303t\u22121 + bi ) ,\nf\u0303t = \u03c3 ( Wxfxt + Whf h\u0303t\u22121 + Wcf c\u0303t\u22121 + bf ) ,\nc\u0303t= f\u0303t c\u0303t\u22121 + i\u0303t tanh ( Wxcxt+Whch\u0303t\u22121+bc ) ,\no\u0303t = \u03c3 ( Wxoxt + Whoh\u0303t\u22121 + Wcoc\u0303t + bo ) ,\nh\u0303t = o\u0303t tanh (c\u0303t) .\n(4)\nThe responses of units to be transmitted across layers with dropout applied are\ni\u030at = \u03c3 ( Wxixt + Whih\u0303t\u22121 + Wcic\u0303t\u22121 + bi ) mi,\nf\u030at = \u03c3 ( Wxfxt+Whf h\u0303t\u22121+Wcf c\u0303t\u22121 + bf ) mf ,\nc\u030at= (\u030a ft c\u0303t\u22121+\u030ait tanh ( Wxcxt+Whch\u0303t\u22121+bc )) mc,\no\u030at = \u03c3 ( Wxoxt + Whoh\u0303t\u22121 + Wco\u030act + bo ) mo,\nh\u030at = o\u030at tanh (\u030act) mh,\n(5)\nwhere mi,mf ,mc,mo, and mh are dropout binary mask vectors for input gates, forget gates, cells, output gates and output responses, respectively, with an element value of 0 indicating that dropout happens. Note that for the first LSTM layer, the inputs xt are the skeleton joints of a frame; for the higher LSTM layer, the inputs xt are the response outputs of the previous layer.\nDuring the training process, the errors back-propagated to the output responses ht are\nth = \u030a t h + \u0303 t h,\n\u030ath = hier h mh, \u0303th = recurh ,\n(6)\nwhere hierh denotes the vector of errors back-propagated from the upper layer at the same time slot, recurh denotes the vector of errors from the next time slot in the same layer, \u030at and \u0303t denote the dropout errors from the upper layer and recurrent errors from the next time slot, respectively.\nBy taking derivative of h\u030at with respect to o\u030at based on (5), we get the errors from h\u030at to o\u030at which represent the errors from upper layer with dropout involved\n\u030ato = (\u030a t h \u2202h\u030at \u2202o\u030at ) mo = \u030ath tanh (\u030act) mo. (7)\nSimilarly, based on (4), we get the errors back-propagated from h\u0303t to o\u0303t which represent the errors from the next time slot in the same layer without dropout\n\u0303to = \u0303 t h \u2202h\u0303t \u2202o\u0303t = \u0303th tanh (c\u0303t) . (8)\nThen, the errors back-propagated to the output gates are the summation of the two types of errors\nto = \u030a t o + \u0303 t o. (9)\nIn the same way, we derive the errors propagated to the cells, forget gates, and input gates, based on (4) and (5).\nDuring the testing process, we use all the neurons but multiplying the units of LSTM neurons (in the LSTM layer where dropout is applied) by the probability values of 1\u2212 p, where p is the dropout probability of that unit. Note that the simple dropout which only drops the output responses ht (Zaremba, Sutskever, and Vinyals 2014) is a special case of our proposed dropout."}, {"heading": "3.4 Action Recognition using the Learned Model", "text": "With the learned deep LSTM network, the probability that a sequence X belongs to the class Ck is\np(Ck|X) = eok\u2211C i=1 e oi , k = 1, \u00b7 \u00b7 \u00b7 , C,\no = T\u2211 t=1 ( W\u2212\u2192 h o \u2212\u2192 h t + W\u2190\u2212h o \u2190\u2212 h t + bo ) ,\n(10)\nwhere C denotes the number of classes, T represents the length of the test sequence, o = [o1, o2, \u00b7 \u00b7 \u00b7 , oC ],\n\u2212\u2192 h t and\u2190\u2212 h t denote the output responses of the last bidirectional LSTM layer. Then, the class with the highest probability is chosen as action class."}, {"heading": "4 Experiments", "text": "We validate the proposed model on the SBU kinect interaction dataset (Yun et al. 2012), HDM05 dataset (Mu\u0308ller et al. 2007), and CMU dataset (CMU 2003) whose groundtruth was labeled by ourselves. We have also tested our model on the Berkeley MHAD action recognition dataset (Ofli et al. 2013) and achieved 100% accuracy. To investigate the impact of each component in our model, we conduct experiments under different configurations represented as follows:\n\u2022 Deep LSTM is our basic scheme without regularizations; \u2022 Deep LSTM + Co-occurrence is the scheme with our pro-\nposed co-occurrence regularization applied;\n\u2022 Deep LSTM + Simple Dropout is the scheme with the dropout algorithm proposed by Zaremba et al. (Zaremba, Sutskever, and Vinyals 2014) applied to our basic scheme;\n\u2022 Deep LSTM + In-depth Dropout is the scheme with our proposed in-depth dropout applied;\n\u2022 Deep LSTM + Co-occurrence + In-depth Dropout is our final scheme with both co-occurrence regularization and in-depth dropout applied.\nDown-sampling the skeleton sequences in temporal is performed to have the frame rate of 30fps on the HDM05 dataset and CMU dataset. To reduce the influence of noise in the captured skeleton data, we smooth each joint\u2019s position of the raw skeleton using the filter (\u22123, 12, 17, 12,\u22123) /35 in the temporal domain (Savitzky and Golay 1964; Du, Wang, and Wang 2015). The number of groups (K) in our model is set to 5, 10, and 10 for the first three layers experimentally. We set the dropout probability p to 0.2 for each unit in an LSTM neuron, which makes the overall dropout\nprobability of an LSTM neuron approach 0.5 (this can be derived based on (5)). Note that when dropout is applied, the number of neurons in the corresponding layer is doubled as suggested by previous work (Srivastava et al. 2014). We set the parameters \u03bb1 and \u03bb2 in (3) experimentally."}, {"heading": "4.1 SBU Kinect Interaction Dataset", "text": "The SBU kinect interaction dataset is a Kinect captured human activity recognition dataset depicting two person interaction, which contains 230 sequences of 8 classes (6,614 frames) with subject independent 5-fold cross validation. The smoothed positions of joints are used as the input of the deep LSTM network for recognition. The number of neurons is set to 100\u00d72, 100, 110\u00d72, 110, 200\u00d72 for the first to fifth layers respectively, where 2 indicates bidirectional LSTM is used and thus the number of neurons is doubled.\nWe have compared our schemes with other skeleton based methods (Yun et al. 2012; Ji, Ye, and Cheng 2014; Du, Wang, and Wang 2015). Note that we add an additional layer to fuse the two subnets corresponding to the two persons when extending Hierarchical RNN scheme for use in the two person interaction scenario (Du, Wang, and Wang 2015). We summarize the results in terms of the average recognition accuracy (5-fold cross validation) in Table 1.\nTable 1 shows that our basic scheme of Deep LSTM achieves comparable performance to the method using handcrafted complex features (Ji, Ye, and Cheng 2014). The proposed schemes of Deep LSTM + Co-occurrence and Deep LSTM + In-depth Dropout can improve the recognition accuracy by 3.4% and 4.1% respectively over Deep LSTM, indicating that the co-occurrence exploration boosts the discrimination of features and the proposed LSTM dropout is capable of learning a more effective model. Deep LSTM + In-depth Dropout is superior to Deep LSTM + Simple Dropout. Note that the deep LSTM network achieves remarkable (5.6%) performance improvement in comparison with the hierarchical RNN network (Du, Wang, and Wang 2015). That is because allowing full connection of joints/features with neurons rather than imposing a priori subnet constraints facilitates the interaction among joints especially when the joints do not belong to the same part, or same person. Our scheme with combined regularizations achieves the best performance."}, {"heading": "4.2 HDM05 Dataset", "text": "The HDM05 dataset contains 2,337 skeleton sequences performed by 5 actors (184,046 frames after down-sampling). For fair comparison, we use the same protocol (65 classes, 10-fold cross validation) as used by Cho and Chen (Cho and Chen 2014). The pre-processing is the same as that done in the hierarchical RNN scheme (Du, Wang, and Wang 2015) (centralize the joints\u2019 positions to human center for each frame and smooth the positions). The number of neurons is 100\u00d72, 110, 120\u00d72, 120, 200\u00d72 for the five layers respectively. Table 2 shows the results in terms of average accuracy. Our basic deep LSTM achieves better results than the Multi-layer Perception model, which suggests that LSTM exhibits better motion modeling ability than the MLP. With the proposed co-occurrence learning and in-depth dropout regularization, our full model also performs better than the manually designed hierarchical RNN approach."}, {"heading": "4.3 CMU Dataset", "text": "We have categorized the CMU motion capture dataset into 45 classes for the purpose of skeleton based action recognition1. The categorized dataset contains 2,235 sequences (987,341 frames after down-sampling) and is the largest skeleton based human action dataset so far. This dataset is much more challenging because: (i) the lengths of sequences vary greatly; (ii) the within-class diversity is large, e.g., for \u201cwalking\u201d, different people walk at different speeds and along different paths; (iii) the dataset contains complex actions such as dance, doing yoga.\nWe have evaluated the performance on both the entire dataset (CMU) and a subset of the dataset (CMU subset). For this subset, we have chosen eight representative action categories containing 664 sequences (125,667 frames after down-sampling), with actions of jump, walk back, run, sit, getup, pickup, basketball, cartwheel. The same preprocessing as used for the HDM05 dataset is performed. The number of neurons is set to 100\u00d72, 100, 120\u00d72, 120, 100\u00d72 for the five layers. Three-fold cross validation is conducted and the results in terms of average accuracy are shown in Table 3. We can see that the proposed model achieves significant performance improvement, indicating that it can better learn the discriminative features and model long-range temporal dynamics even for this challenging dataset.\n1http://www.escience.cn/people/zhuwentao/29634.html"}, {"heading": "4.4 Discussions", "text": "To further understand our deep LSTM network, we visualize the weights learned in the first LSTM layer on the SBU kinect interaction dataset in Fig. 6 (a). Each element represents the absolute value of the weight between the corresponding skeleton joint and input gate of that LSTM neuron. It is observed that the weights in the diagonal positions marked by the red ellipse have high values, which means the co-occurrence regularization helps learn the human parts automatically. In contrast to the part based subnet fusion model (Du, Wang, and Wang 2015), the learned co-occurrences of joints by our model do not limit the connections to be in the same part, as there are many large weights outside the diagonal regions, e.g., in the regions marked by white circles, making the network more powerful for action recognition. This also signifies the importance of the proposed full connection architecture. By averaging the energy of the weights in the same group of neurons for each joint, we obtain Fig. 6 (b) which has five groups of LSTM neurons. It is observed that different groups have different weight patterns, preferring different conjunctions of joints."}, {"heading": "5 Conclusion", "text": "In this paper, we propose an end-to-end fully connected deep LSTM network for skeleton based action recognition. The proposed model facilitates the automatic learning of feature co-occurrences from the skeleton joints through our designed regularization. To ensure effective learning of the deep model, we design an in-depth dropout algorithm for the LSTM neurons, which performs dropout for the internal gates, cell, and output response of the LSTM neuron. Experimental results demonstrate the state-of-the-art performance of our model on several datasets."}, {"heading": "Acknowledgment", "text": "We would like to thank David Wipf from Microsoft Research Asia for the valuable discussions, and thank Yong Du from Institute of Automation, Chinese Academy of Sciences for providing Hierarchical RNN code for the comparison."}], "references": [{"title": "Classifying and visualizing motion capture sequences using deep neural networks", "author": ["K. Cho", "X. Chen"], "venue": "International Conference on Computer Vision Theory and Applications, 122\u2013130.", "citeRegEx": "Cho and Chen,? 2014", "shortCiteRegEx": "Cho and Chen", "year": 2014}, {"title": "CMU graphics lab motion capture database", "author": ["CMU."], "venue": "http://mocap.cs.cmu.edu/.", "citeRegEx": "CMU.,? 2003", "shortCiteRegEx": "CMU.", "year": 2003}, {"title": "Sparse solutions to linear inverse problems with multiple measurement vectors", "author": ["S.F. Cotter", "B.D. Rao", "K. Engan", "K. Kreutz-Delgado"], "venue": "IEEE Transactions on Signal Processing 53(7):2477\u20132488.", "citeRegEx": "Cotter et al\\.,? 2005", "shortCiteRegEx": "Cotter et al\\.", "year": 2005}, {"title": "Hierarchical recurrent neural network for skeleton based action recognition", "author": ["Y. Du", "W. Wang", "L. Wang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 1110\u20131118.", "citeRegEx": "Du et al\\.,? 2015", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber,? 2005", "shortCiteRegEx": "Graves and Schmidhuber", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 6645\u2013 6649.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Supervised sequence labelling with recurrent neural networks, volume 385", "author": ["A. Graves"], "venue": "Springer.", "citeRegEx": "Graves,? 2012", "shortCiteRegEx": "Graves", "year": 2012}, {"title": "Interactive body part contrast mining for human interaction recognition", "author": ["Y. Ji", "G. Ye", "H. Cheng"], "venue": "IEEE International Conference on Multimedia and Expo Workshops, 1\u20136.", "citeRegEx": "Ji et al\\.,? 2014", "shortCiteRegEx": "Ji et al\\.", "year": 2014}, {"title": "Visual perception of biological motion and a model for it is analysis", "author": ["G. Johansson"], "venue": "Perception and Psychophysics 14(2):201\u2013 211.", "citeRegEx": "Johansson,? 1973", "shortCiteRegEx": "Johansson", "year": 1973}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "BLSTM-RNN based 3D gesture classification", "author": ["G. Lefebvre", "S. Berlemont", "F. Mamalet", "C. Garcia"], "venue": "Proceedings of the International Conference on Artificial Neural Networks and Machine Learning, 381\u2013388.", "citeRegEx": "Lefebvre et al\\.,? 2013", "shortCiteRegEx": "Lefebvre et al\\.", "year": 2013}, {"title": "Documentation mocap database HDM05", "author": ["M. M\u00fcller", "T. R\u00f6der", "M. Clausen", "B. Eberhardt", "B. Kr\u00fcger", "A. Weber"], "venue": "Technical Report CG-2007-2, Universit\u00e4t Bonn. M\u00fcller, M.; R\u00f6der, T.; and Clausen, M. 2005. Efficient contentbased retrieval of motion capture data. ACM Transactions on", "citeRegEx": "M\u00fcller et al\\.,? 2007", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2007}, {"title": "Berkeley MHAD: A comprehensive multimodal human action database", "author": ["F. Ofli", "R. Chaudhry", "G. Kurillo", "R. Vidal", "R. Bajcsy"], "venue": "Proceedings of the IEEE Workshop on Applications on Computer Vision, 53\u201360.", "citeRegEx": "Ofli et al\\.,? 2013", "shortCiteRegEx": "Ofli et al\\.", "year": 2013}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "International Conference on Frontiers in Handwriting Recognition, 285\u2013290.", "citeRegEx": "Pham et al\\.,? 2014", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "A survey on vision-based human action recognition", "author": ["R. Poppe"], "venue": "Image and Vision Computing 28(6):976\u2013990.", "citeRegEx": "Poppe,? 2010", "shortCiteRegEx": "Poppe", "year": 2010}, {"title": "Smoothing and differentiation of data by simplified least squares procedures", "author": ["A. Savitzky", "M.J. Golay"], "venue": "Analytical chemistry 36(8):1627\u20131639.", "citeRegEx": "Savitzky and Golay,? 1964", "shortCiteRegEx": "Savitzky and Golay", "year": 1964}, {"title": "Conditional models for contextual human motion recognition", "author": ["C. Sminchisescu", "A. Kanaujia", "Z. Li", "D. Metaxas"], "venue": "IEEE Conference on Computer Vision, volume 2, 1808\u20131815.", "citeRegEx": "Sminchisescu et al\\.,? 2005", "shortCiteRegEx": "Sminchisescu et al\\.", "year": 2005}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Human activity detection from RGBD images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "AAAI workshop on Pattern, Activity and Intent Recognition, 47\u201355.", "citeRegEx": "Sung et al\\.,? 2011", "shortCiteRegEx": "Sung et al\\.", "year": 2011}, {"title": "Unstructured human activity detection from RGBD images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "IEEE International Conference on Robotics and Automation, 842\u2013849.", "citeRegEx": "Sung et al\\.,? 2012", "shortCiteRegEx": "Sung et al\\.", "year": 2012}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 1\u20139.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Mining actionlet ensemble for action recognition with depth cameras", "author": ["J. Wang", "Z. Liu", "Y. Junsong"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 1290\u20131297.", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "A survey of vision-based methods for action representation, segmentation and recognition", "author": ["D. Weinland", "R. Ronfard", "E. Boyerc"], "venue": "Computer Vision and Image Understanding 115(2):224\u2013241.", "citeRegEx": "Weinland et al\\.,? 2011", "shortCiteRegEx": "Weinland et al\\.", "year": 2011}, {"title": "View invariant human action recognition using histograms of 3D joints", "author": ["L. Xia", "C.-C. Chen", "J.K. Aggarwal"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops, 20\u201327.", "citeRegEx": "Xia et al\\.,? 2012", "shortCiteRegEx": "Xia et al\\.", "year": 2012}, {"title": "Effective 3D action recognition using EigenJoints", "author": ["X. Yang", "Y. Tian"], "venue": "Journal of Visual Communication and Image Representation 25(1):2\u201311.", "citeRegEx": "Yang and Tian,? 2014", "shortCiteRegEx": "Yang and Tian", "year": 2014}, {"title": "Two-person interaction detection using body pose features and multiple instance learning", "author": ["K. Yun", "J. Honorio", "D. Chattopadhyay", "T.L. Berg", "D. Samaras"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops, 28\u201335.", "citeRegEx": "Yun et al\\.,? 2012", "shortCiteRegEx": "Yun et al\\.", "year": 2012}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "It facilitates a wide range of applications such as intelligent video surveillance, human-computer interaction, and video understanding (Poppe 2010; Weinland, Ronfard, and Boyerc 2011).", "startOffset": 136, "endOffset": 184}, {"referenceID": 8, "context": "Biological observations suggest that humans can recognize actions from just the motion of a few light displays attached to the human body (Johansson 1973).", "startOffset": 138, "endOffset": 154}, {"referenceID": 19, "context": "One is to design robust and discriminative features from the skeleton (and the corresponding RGBD images) for intra-frame content representation (M\u00fcller, R\u00f6der, and Clausen 2005; Wang, Liu, and Junsong 2012; Sung et al. 2012; Yang and Tian 2014; Ji, Ye, and Cheng 2014).", "startOffset": 145, "endOffset": 269}, {"referenceID": 24, "context": "One is to design robust and discriminative features from the skeleton (and the corresponding RGBD images) for intra-frame content representation (M\u00fcller, R\u00f6der, and Clausen 2005; Wang, Liu, and Junsong 2012; Sung et al. 2012; Yang and Tian 2014; Ji, Ye, and Cheng 2014).", "startOffset": 145, "endOffset": 269}, {"referenceID": 18, "context": "The other is to explore temporal dependencies of the interframe content for action dynamics modeling, using hierarchical maximum entropy Markov model (Sung et al. 2011), hidden Markov model (Xia, Chen, and Aggarwal 2012) or Conditional Random Fields (Sminchisescu et al.", "startOffset": 150, "endOffset": 168}, {"referenceID": 16, "context": "2011), hidden Markov model (Xia, Chen, and Aggarwal 2012) or Conditional Random Fields (Sminchisescu et al. 2005).", "startOffset": 87, "endOffset": 113}, {"referenceID": 4, "context": "Inspired by the success of deep recurrent neural networks (RNNs) using the Long Short-Term Memory (LSTM) architecture for speech feature learning and time series modeling (Graves, Mohamed, and Hinton 2013; Graves and Schmidhuber 2005), we intend to build an effective action recognition model based on deep LSTM network.", "startOffset": 171, "endOffset": 234}, {"referenceID": 6, "context": "The proposed network is constructed by inheriting many insights from recent successful networks (Graves 2012; Krizhevsky, Sutskever, and Hinton 2012; Szegedy et al. 2015; Du, Wang, and Wang 2015) and is designed to robustly model complex relationships among different joints.", "startOffset": 96, "endOffset": 195}, {"referenceID": 20, "context": "The proposed network is constructed by inheriting many insights from recent successful networks (Graves 2012; Krizhevsky, Sutskever, and Hinton 2012; Szegedy et al. 2015; Du, Wang, and Wang 2015) and is designed to robustly model complex relationships among different joints.", "startOffset": 96, "endOffset": 195}, {"referenceID": 0, "context": "A multi-layer perceptron network is trained to classify each frame (Cho and Chen 2014); however, such a network cannot explore temporal dependencies very well.", "startOffset": 67, "endOffset": 86}, {"referenceID": 10, "context": "In contrast, a gesture recognition system (Lefebvre et al. 2013) employs a shallow bidirectional LSTM with only one forward hidden layer and one backward hidden layer to explore long-range temporal dependencies.", "startOffset": 42, "endOffset": 64}, {"referenceID": 25, "context": "Similarly, actions involving two people can be characterized by the interactions of a subset of the two persons\u2019 joints (Yun et al. 2012; Ji, Ye, and Cheng 2014).", "startOffset": 120, "endOffset": 161}, {"referenceID": 13, "context": "In order to preserve the ability of RNNs to model sequences, dropout applied only to the feedforward (along layers) connections but not to the recurrent (along time) connections is proposed (Pham et al. 2014).", "startOffset": 190, "endOffset": 208}, {"referenceID": 6, "context": "1 Overview of LSTM The RNN is a successful model for sequential learning (Graves 2012).", "startOffset": 73, "endOffset": 86}, {"referenceID": 6, "context": "To allow the information from both the future and the past to determine the output, bidirectional LSTM can be utilized (Graves 2012).", "startOffset": 119, "endOffset": 132}, {"referenceID": 6, "context": "whereL is the maximum likelihood loss function of the deep LSTM network (Graves 2012).", "startOffset": 72, "endOffset": 85}, {"referenceID": 2, "context": "i \u221a\u2211 jw 2 i,j (Cotter et al. 2005), is used to drive the units to select a conjunction of descriptive inputs (joints/features) since the `21 norm can encourage the matrix Wx\u03b2,k to be column sparse.", "startOffset": 14, "endOffset": 34}, {"referenceID": 13, "context": "For recurrent neural networks, the erasing of all the information from a unit is not expected, especially when the unit remembers events that occurred many timesteps back in the past (Pham et al. 2014).", "startOffset": 183, "endOffset": 201}, {"referenceID": 25, "context": "We validate the proposed model on the SBU kinect interaction dataset (Yun et al. 2012), HDM05 dataset (M\u00fcller et al.", "startOffset": 69, "endOffset": 86}, {"referenceID": 11, "context": "2012), HDM05 dataset (M\u00fcller et al. 2007), and CMU dataset (CMU 2003) whose groundtruth was labeled by ourselves.", "startOffset": 21, "endOffset": 41}, {"referenceID": 12, "context": "We have also tested our model on the Berkeley MHAD action recognition dataset (Ofli et al. 2013) and achieved 100% accuracy.", "startOffset": 78, "endOffset": 96}, {"referenceID": 15, "context": "To reduce the influence of noise in the captured skeleton data, we smooth each joint\u2019s position of the raw skeleton using the filter (\u22123, 12, 17, 12,\u22123) /35 in the temporal domain (Savitzky and Golay 1964; Du, Wang, and Wang 2015).", "startOffset": 180, "endOffset": 230}, {"referenceID": 17, "context": "Note that when dropout is applied, the number of neurons in the corresponding layer is doubled as suggested by previous work (Srivastava et al. 2014).", "startOffset": 125, "endOffset": 149}, {"referenceID": 25, "context": "We have compared our schemes with other skeleton based methods (Yun et al. 2012; Ji, Ye, and Cheng 2014; Du, Wang, and Wang 2015).", "startOffset": 63, "endOffset": 129}, {"referenceID": 25, "context": "(%) Raw skeleton (Yun et al. 2012) 49.", "startOffset": 17, "endOffset": 34}, {"referenceID": 25, "context": "7 Joint feature (Yun et al. 2012) 80.", "startOffset": 16, "endOffset": 33}, {"referenceID": 0, "context": "For fair comparison, we use the same protocol (65 classes, 10-fold cross validation) as used by Cho and Chen (Cho and Chen 2014).", "startOffset": 109, "endOffset": 128}, {"referenceID": 0, "context": "(%) Multi-layer Perceptron (Cho and Chen 2014) 95.", "startOffset": 27, "endOffset": 46}], "year": 2016, "abstractText": "Skeleton based action recognition distinguishes human actions using the trajectories of skeleton joints, which provide a very good representation for describing actions. Considering that recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) can learn feature representations and model long-term temporal dependencies automatically, we propose an endto-end fully connected deep LSTM network for skeleton based action recognition. Inspired by the observation that the co-occurrences of the joints intrinsically characterize human actions, we take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. To train the deep LSTM network effectively, we propose a new dropout algorithm which simultaneously operates on the gates, cells, and output responses of the LSTM neurons. Experimental results on three human action recognition datasets consistently demonstrate the effectiveness of the proposed model.", "creator": "TeX"}}}