{"id": "1603.08042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "On the Compression of Recurrent Neural Networks with an Application to LVCSR acoustic modeling for Embedded Speech Recognition", "abstract": "We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.", "histories": [["v1", "Fri, 25 Mar 2016 21:43:28 GMT  (171kb,D)", "http://arxiv.org/abs/1603.08042v1", "Accepted in ICASSP 2016"], ["v2", "Mon, 2 May 2016 15:19:30 GMT  (261kb,D)", "http://arxiv.org/abs/1603.08042v2", "Accepted in ICASSP 2016"]], "COMMENTS": "Accepted in ICASSP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["rohit prabhavalkar", "ouais alsharif", "antoine bruguier", "ian mcgraw"], "accepted": false, "id": "1603.08042"}, "pdf": {"name": "1603.08042.pdf", "metadata": {"source": "CRF", "title": "ON THE COMPRESSION OF RECURRENT NEURAL NETWORKS WITH AN APPLICATION TO LVCSR ACOUSTIC MODELING FOR EMBEDDED SPEECH RECOGNITION", "authors": ["Rohit Prabhavalkar", "Ouais Alsharif", "Antoine Bruguier", "Ian McGraw"], "emails": ["prabhavalkar@google.com", "oalsha@google.com", "tonybruguier@google.com", "imcgraw@google.com"], "sections": [{"heading": null, "text": "Index Terms\u2014 model compression, LSTM, RNN, SVD, embedded speech recognition"}, {"heading": "1. INTRODUCTION", "text": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurrent hidden layers [3, 4] have emerged as state-of-theart acoustic models (AMs) for automatic speech recognition (ASR) tasks. Advances in computational capabilities coupled with the availability of large annotated speech corpora have made it possible to train NN-based AMs with a large number of parameters [5] with great success.\nAs speech recognition technologies continue to improve, they are becoming increasingly ubiquitous on mobile devices: voice assistants such as Apple\u2019s Siri, Microsoft\u2019s Cortana, Amazon\u2019s Alexa and Google Now [6] enable users to search for information using their voice. Although the traditional model for these applications has been to recognize speech remotely on large servers, there has been growing interest in developing ASR technologies that can recognize the input speech directly \u201con-device\u201d [7]. This has the promise to reduce latency while enabling user interaction even in cases where a mobile data connection is either unavailable, slow or unreliable. Some of the main challenges in this regard are the disk, memory and computational constraints imposed by these devices. Since the number of operations in neural\n\u2020Equal contribution. The authors would like to thank Has\u0327im Sak and Raziel Alvarez for helpful comments and suggestions on this work, and Chris Thornton and Yu-hsin Chen for comments on an earlier draft.\nnetworks is proportional to the number of model parameters, compressing the model is desirable from the point of view of reducing memory usage and power consumption.\nIn this paper, we study techniques for compressing recurrent neural networks (RNNs), specifically RNN acoustic models. We demonstrate how a generalization of conventional inter-layer matrix factorization techniques (e.g., [9, 10]), where we jointly compress both recurrent and inter-layer weight matrices, allows us to compress acoustic models up to a third of their original size with negligible loss in accuracy. While we focus on acoustic modeling, the techniques presented can be applied to RNNs in other domains, e.g., handwriting recognition [11] and machine translation [12] inter alia. The technique presented in this paper encompasses both traditional recurrent neural networks (RNNs) as well as Long Short-Term Memory (LSTM) neural networks.\nIn Section 2, we review previous work that has focussed on techniques for compressing neural networks. Our proposed compression technique is presented in Section 3. We examine the effectiveness of proposed techniques in Sections 4 and 5. Finally, we conclude with a discussion of our findings in Section 6."}, {"heading": "2. RELATED WORK", "text": "There have been a number of previous proposals to compress neural networks, both in the context of ASR as well as in the broader field of machine learning. We summarize a number of proposed approaches in this section.\nIt has been noted in previous work that there is a large amount of redundancy in the parameters of a neural network. For example, Denil et al. [13] show that the entire neural network can be reconstructed given the values of a small number of parameters. Caruana and colleagues show that the output distribution learned by a larger neural network can be approximated by a neural network with fewer parameters by training the smaller network to directly predict the outputs of the larger network [14, 15]. This approach, termed \u201cmodel compression\u201d [14] is closely related to the recent \u201cdistillation\u201d approach proposed by Hinton et al. [16]. The redundancy in a neural network has also been exploited in the HashNet approach of Chen et al. [17], which imposes parameter tying in\nar X\niv :1\n60 3.\n08 04\n2v 1\n[ cs\n.C L\n] 2\n5 M\nar 2\n01 6\nnetwork based on a set of hash functions. In the context of ASR, previous approaches to acoustic model compression have focused mainly on the case of feedforward DNNs. One popular technique is based on sparsifying the weight matrices in the neural network, for example, by setting weights whose magnitude falls below a certain threshold to zero [1] or based on the second-derivative of the loss function in the \u201coptimal brain damage\u201d procedure [18]. In fact, Seide et al. [1] demonstrate that up to two-thirds of the weights of the feed-forward network can be set to zero without incurring any loss in performance. Although techniques based on sparsification do decrease the number of effective weights, encoding the subset of weights which can be \u2018zeroed out\u2019 requires additional memory. Further, if the weight matrices are represented as dense matrices for efficient computation, then the parameter savings on disk will not translate in to savings of runtime memory. Other techniques to reduce the number of model parameters is based on changing the neural network architecture, e.g., by introducing bottleneck layers [19] or through a low-rank matrix factorization layer [20]. We also note recent work by Wang et al. [21] which uses a combination of singular value decomposition (SVD) and vector quantization to compress acoustic models.\nThe methods investigated in our work are most similar to previous work that has examined using SVD to reduce the number of parameters in the network in the context of feedforward DNNs [9, 10, 22]. As we describe in Section 3, our methods can be thought of as an extension of the techniques proposed by Xue et al. [9], wherein we jointly factorize both recurrent and (non-recurrent) inter-layer weight matrices in the network."}, {"heading": "3. MODEL COMPRESSION", "text": "In this section, we present a general technique for compressing individual recurrent layers in a recurrent neural network, thus generalizing the methods proposed by Xue et al. [9].\nWe describe our approach in the most general setting of a standard RNN. We denote the activations of the l-th hidden layer, consisting of N l nodes, at time t by hlt \u2208 RN l\n. The inputs to this layer at time t \u2013 which are in turn the activations from the previous layer or the input features \u2013 are denoted by hl\u22121t \u2208 RN l\u22121 . We can then write the following equations which define the output activations of the l-th and (l + 1)-th layers in a standard RNN:\nhlt = \u03c3(W l\u22121 x h l\u22121 t + W l hh l t\u22121 + b l) (1)\nhl+1t = \u03c3(W l xh l t + W l+1 h h l+1 t\u22121 + b l+1) (2)\nwhere, bl \u2208 RN l and bl+1 \u2208 RN l+1 represent bias vectors, \u03c3(\u00b7) denotes a non-linear activation function, and W lx \u2208 RN l+1\u00d7N l and W lh \u2208 RN\nl\u00d7N l denote weight matrices that we refer to respectively as the inter-layer and the recurrent\nweight matrices, respectively1. Since our proposed approach can be applied independently for each recurrent hidden layer, we only describe the compression operations for a particular layer l. We jointly compress the recurrent and inter-layer matrices corresponding to a specific layer l by determining a suitable recurrent projection matrix [3], denoted by P l \u2208 Rrl\u00d7N l , of rank rl < N l such that, W lh = ZlhP l and W lx = ZlxP l, thus allowing us to re-write (1) and (2) as,\nhlt = \u03c3(W l\u22121 x h l\u22121 t + Z l hP lhlt\u22121 + b l) (3)\nhl+1t = \u03c3(Z l xP lhlt + W l+1 h h l+1 t\u22121 + b l+1) (4)\nwhere, Zlh \u2208 RN l\u00d7rl and Zlx \u2208 RN l+1\u00d7rl . This compression process is depicted graphically in Figure 1.\nWe note that sharing P l across the recurrent and interlayer matrices allows for more efficient parameterization of the weight matrices; as shown in Section 5, this does not result in a significant loss of performance. Thus, the degree of compression in the model can be controlled by setting the ranks rl of the projection matrices in each of the layers of the network.\nWe determine the recurrent projection matrix P l, by first computing an SVD of the recurrent weight matrix, which we then truncate, retaining only the top rl singular values (denoted by \u03a3\u0303lh) and the corresponding singular vectors from U l h and V lh (denoted by U\u0303 l h and V\u0303 l h, respectively):\nW lh = U l h\u03a3 l hV l h T \u2248 ( U\u0303 lh\u03a3\u0303 l h ) V\u0303 lh T = ZlhP l (5)\nwhere Zlh = U\u0303 l h\u03a3\u0303 l h and P l = V\u0303 lh T\n. Finally, we determine Zlx, as the solution to the following least-squares problem:\nZlx = arg min Y \u2016Y P l \u2212W lx\u20162F (6)\n1The equations are slightly more complicated when using LSTM cells in the recurrent layer, but the basic form remains the same. See Section 3.1.\nwhere, \u2016X\u2016F denotes the Frobenius norm of the matrix. In pilot experiments we found that the proposed SVD-based initialization performed better than training a model with recurrent projection matrices (i.e., same model architecture) but with random initialization of the network weights."}, {"heading": "3.1. Applying our technique to LSTM RNNs", "text": "Generalizing the procedure described above in the context of standard RNNs to the case of LSTM RNNs [3, 23, 24] is straightforward. Using the notation in [3], note that the recurrent-weight matrix W lh in the case of the LSTM is the concatenation of the four gate weight matrices, obtained by stacking them vertically:\n[Wim,Wom,Wfm,Wcm] T\nwhich represent respectively, recurrent connections to the input gate, the output gate, the forget gate and the cell state. Similarly, the inter-layer matrix W lx is the concatenation of the matrices:\n[Wix,Wfx,Wox,Wcx] T\nwhich correspond to the input gate, the forget gate, the output gate and the cell state (of the next layer). With these definitions, compression can be applied as described in Section 3. Note that we do not compress the \u201cpeep-hole\u201d weights, since they are already narrow, single column matrices and do not contribute significantly to the total number of parameters in the network."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "In order to determine the effectiveness of the proposed RNN compression technique, we conduct experiments on a openended large-vocabulary dictation task.\nAs we mentioned in Section 1, one of our primary motivations behind investigating acoustic model compression is to build compact acoustic models that can be deployed on mobile devices. In recent work, Sak et al. have demonstrated that deep LSTM-based AMs trained to predict either contextindependent (CI) phoneme targets [23] or context-dependent (CD) phoneme targets [24] approach state-of-the-art performance on speech tasks. These systems have two important characteristics: in addition to the CI or CD phoneme labels, the system can also hypothesize a \u201cblank\u201d label if it is unsure of the identity of the current phoneme, and the systems are trained to optimize the connectionist temporal classification (CTC) criterion [25] which maximizes the total probability of correct label sequence conditioned on the input sequence. More details can be found in [23, 24].\nFollowing [23], our baseline model is thus a CTC model: a five hidden layer RNN with 500 LSTM cells in each layer, which predicts 41 CI phonemes (plus \u201cblank\u201d). As a point of comparison, we also present results obtained using a much\nlarger state-of-the-art \u2018server-sized\u2019 model which is too large to deploy on embedded devices but nonethless serves as an upper-bound performance for our models on this dataset. This model consists of five hidden layers with 600 LSTM cells per layer, and is trained to predict one of 9287 context-dependent phonemes (plus \u201cblank\u201d).\nOur systems are trained using distributed asynchronous stochastic gradient descent with a parameter server [26]. The systems are first trained to convergence to optimize the CTC criterion, following which these are discriminatively sequence trained to optimize the state-level minimum Bayes risk (sMBR) criterion [?, 27]. As discussed in Section 5, after applying the proposed compression scheme, we further fine-tune the network: first with the CTC criterion, followed by sequence discriminative training with the sMBR criterion. This additional fine-tuning step was found to be necessary to achieve good performance, particularly as the amount of compression was increased.\nThe language model used in this work is a 5-gram model trained on\u223c100M sentences of in-domain data, with entropybased pruning applied to reduce the size of the LM down to roughly 1.5M n-grams (mainly bigrams) with a 64K vocabulary. Since our goal is to build a recognizer to run efficiently on mobile devices, we minimize the size of the decoder graph used for recognition, following the approach outlined in [7]: we perform an additional pruning step to generate a much smaller first-pass language model (69.5K n-grams; mainly unigrams), which is composed with the lexicon transducer to construct the decoder graph. We then perform on-the-fly rescoring with the larger LM. The resulting models, when compressed for use on-device, total about 20.3 MB, thus enabling them to be run many times faster than real-time on recent mobile devices [?].\nWe parameterize the input acoustics by computing 40- dimensional log mel-filterbank energies over the 8Khz range, which are computed every 10ms over 25ms windowed speech segments. The server-sized system uses 80-dimensional features computed over the same range since this resulted in slightly improved performance. Following [24], we stabilize CTC training by stacking together 8 consecutive speech frames (7 right context frames); only every third stacked frame is presented as an input to the network."}, {"heading": "4.1. Training and Evaluation Data", "text": "Our systems are trained on\u223c3M hand-transcribed anonymized utterances extracted from Google voice search traffic (\u223c2000 hours). We create \u201cmulti-style\u201d training data by synthetically distorting utterances to simulate background noise and reverberation using a room simulator with noise samples extracted from YouTube videos and environmental recordings of everyday events; 20 distorted examples are created for each utterance in the training set. Systems are additionally adapted using the sMBR criterion [?, 27] on a set\nof \u223c1M anonymized hand-transcribed (in-domain) dictation utterances extracted from Google traffic, processed to generate \u201cmulti-style\u201d training data as described above, which improves performance on our dictation task. All results are reported on a set of 13.3K hand-transcribed anonymized utterances extracted from Google traffic from an open-ended dictation domain."}, {"heading": "5. RESULTS", "text": "In our experiments, we seek to determine the impact of the proposed joint SVD-based compression technique on system performance. In particular, we are interested in determining how system performance varies as a function of the degree of compression, which is controlled by setting the ranks of the recurrent projection matrices rl as described in Section 3.\nNotice that since the proposed compression scheme is applied to all hidden layers of the baseline system, there are numerous settings of the ranks rl for the projection matrices in each layer which result in the same number of total parameters in the compressed network. In order to avoid this ambiguity, we set the various projection ranks using the following criterion: Given a threshold \u03c4 , for each layer l, we set the rank rl of the corresponding projection matrix such that it corresponds to retaining a fraction of at most \u03c4 of the explained variance after the truncated SVD of W lh. More specifically, if the singular values in \u03a3lh in (5) are sorted in non-increasing order as \u03c3l1 \u2265 \u03c3l2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3lN , we set each rl as:\nrl = arg max 1\u2264k\u2264N\n{\u2211k j=1 \u03c3\nl j 2\u2211N\nj=1 \u03c3 l j 2 \u2264 \u03c4\n} (7)\nChoosing the projection ranks using (7) allows us to control the degree of compression, and thus compressed model size by varying a single parameter, \u03c4 . In pilot experiments we found that this scheme performed better than setting ranks to be equal for all layers (given the same total parameter budget). Once the projection ranks rl have been determined for the various projection matrices we fine-tune the compressed models by first optimizing the CTC criterion, followed by sequence training with the sMBR criterion and adaptation on in-domain data as described in Section 4.1. The results of our experiments are presented in Table 1.\nAs can be seen in Table 1, the baseline system which predicts CI phoneme targets is only \u223c10% relative worse than the larger server-sized system, although it has half as many parameters. Since the ranks rl are all chosen to retain a given fraction of the explained variance in the SVD operation, we also note that earlier hidden layers in the network appear to have lower ranks than later layers, since most of the variance is accounted for by a smaller number of singular values. It can be seen from Table 1 that word error rates increase as the amount of compression is increased, although performance of\nthe compressed systems are close to the baseline for moderate compression (\u03c4 \u2265 0.7). Using a value of \u03c4 = 0.6, enables the model to be compressed to a third of its original size, with only a small degradation in accuracy. However, performance begins to degrade significantly for \u03c4 \u2264 0.5. Future work will consider alternative techniques for setting the projection ranks rl in order to examine their impact on system performance."}, {"heading": "6. CONCLUSIONS", "text": "We presented a technique to compress RNNs using a joint factorization of recurrent and inter-layer weight matrices, generalizing previous work [9]. The proposed technique was applied to the task of compressing LSTM RNN acoustic models for embedded speech recognition, where we found that we could compress our baseline acoustic model to a third of its original size with negligible loss in accuracy. The proposed techniques, in combination with weight quantization, allow us to build a small and efficient speech recognizer that run many times faster than real-time on recent mobile devices [?]."}, {"heading": "7. REFERENCES", "text": "[1] Frank Seide, Gang Li, and Dong Yu, \u201cConversational speech transcription using context-dependent deep neural networks,\u201d in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2011, pp. 437\u2013440.\n[2] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[3] Has\u0327im Sak, Andrew Senior, and Franc\u0327oise Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2014, pp. 338\u2013342.\n[4] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Has\u0327im Sak, \u201cConvolutional, long short-term memory, fully connected deep neural networks,\u201d in Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4580\u20134584.\n[5] Li Deng and Dong Yu, \u201cDeep learning: methods and applications,\u201d Foundations and Trends in Signal Processing, vol. 7, no. 3\u20134, pp. 197\u2013387, 2014.\n[6] Johan Schalkwyk, Doug Beeferman, Franc\u0327oise Beaufays, Bill Byrne, Ciprian Chelba, Mike Cohen, Maryam Kamvar, and Brian Strope, \u201c\u201cyour word is my command\u201d: Google search by voice: A case study,\u201d in Advances in Speech Recognition, pp. 61\u201390. Springer US, 2010.\n[7] Xin Lei, Andrew Senior, Alexander Gruenstein, and Jeffrey Sorensen, \u201cAccurate and compact large vocabulary speech recognition on mobile devices,\u201d in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2013, pp. 662\u2013665.\n[8] Ian McGraw et al., \u201cPersonalized speech recognition on mobile devices,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP), (submitted).\n[9] Jian Xue, Jinyu Li, and Yifan Gong, \u201cRestructuring of deep neural network acoustic models with singular value decomposition,\u201d in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2013, pp. 2365\u20132369.\n[10] Jian Xue, Jinyu Li, Dong Yu, Mike Seltzer, and Yifan Gong, \u201cSingular value decomposition based lowfootprint speaker adaptation and personalization for deep neural network,\u201d in Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 6359\u20136363.\n[11] Alex Graves, Marcus Liwicki, Santiago Ferna\u0301ndez, Roman Bertolami, Horst Bunke, and Ju\u0308rgen Schmidhuber, \u201cA novel connectionist system for unconstrained handwriting recognition,\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, no. 5, pp. 855\u2013868, 2009.\n[12] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le, \u201cSequence to sequence learning with neural networks,\u201d in Advances in neural information processing systems, 2014, pp. 3104\u20133112.\n[13] Misha Denil, Babak Shakibi, Laurent Dinh, Marc\u2019Aurelio Ranzato, and Nando de Freitas, \u201cPredicting parameters in deep learning,\u201d in Proceedings of Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2148\u20132156.\n[14] Cristian Bucilua\u0306, Rich Caruana, and Alexandru Niculescu-Mizil, \u201cModel compression,\u201d in Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 535\u2013541.\n[15] Lei Jimmy Ba and Rich Caruana, \u201cDo deep nets really need to be deep?,\u201d in Proceedings of Advances in Neural Information Processing Systems (NIPS), 2014, pp. 2654\u20132662.\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, \u201cDistilling the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015.\n[17] Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen, \u201cCompressing neural networks with the hashing trick,\u201d in Proceedings of the International Conference on Machine Learning (ICML), 2015.\n[18] Yann LeCun, John S. Denker, and Sara A. Solla, \u201cOptimal brain damage,\u201d in Proceedings of Advances in Neural Information Processing Systems (NIPS), 1989, pp. 598\u2013605.\n[19] Frantis\u030cek Gre\u0301zl and Petr Fousek, \u201cOptimizing bottleneck features for LVCSR,\u201d in Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2008, pp. 4729\u20134732.\n[20] Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran, \u201cLow-rank matrix factorization for deep neural network training with high-dimensional output targets,\u201d in Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6655\u20136659.\n[21] Yongqiang Wang, Jinyu Li, and Yifan Gong, \u201cSmallfootprint high-performance deep neural network-based speech recognition using split-VQ,\u201d in Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[22] Preetum Nakkiran, Raziel Alvarez, Rohit Prabhavalkar, and Carolina Parada, \u201cCompressing deep neural networks using a rank-constrained topology,\u201d in\nProceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2015, pp. 1473\u20131477.\n[23] Has\u0327im Sak, Andrew Senior, Kanishka Rao, Ozan I\u0307rsoy, Alex Graves, Franc\u0327oise Beaufays, and Johan Schalkwyk, \u201cLearning acoustic frame labeling for speech recognition with recurrent neural networks,\u201d in Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4280\u20134284.\n[24] Has\u0327im Sak, Andrew Senior, Kanishka Rao, and Franc\u0327oise Beaufays, \u201cFast and accurate recurrent neural network acoustic models for speech recognition,\u201d in Proceedings of the Annual Conference of the Internation Speech Communication Association (Interspeech), 2015, pp. 1468\u20131472.\n[25] Alex Graves, Santiago Ferna\u0301ndez, Faustino Gomez, and Ju\u0308rgen Schmidhuber, \u201cConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the International Conference on Machine Learning (ICML). ACM, 2006, pp. 369\u2013376.\n[26] Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc\u2019Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng, \u201cLarge scale distributed deep networks,\u201d in Proceedings of Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1223\u20131231.\n[27] Has\u0327im Sak, Oriol Vinyals, Georg Heigold, Andrew Senior, Erik McDermott, Rajat Monga, and Mark Mao, \u201cSequence discriminative distributed training of long short-term memory recurrent neural networks,\u201d in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2014, pp. 1209\u20131213."}], "references": [{"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Frank Seide", "Gang Li", "Dong Yu"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2011, pp. 437\u2013440.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E. Dahl", "Abdel rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Ha\u015fim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2014, pp. 338\u2013342.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["Tara N Sainath", "Oriol Vinyals", "Andrew Senior", "Ha\u015fim Sak"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4580\u20134584.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning: methods and applications", "author": ["Li Deng", "Dong Yu"], "venue": "Foundations and Trends in Signal Processing, vol. 7, no. 3\u20134, pp. 197\u2013387, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "your word is my command\u201d: Google search by voice: A case study", "author": ["Johan Schalkwyk", "Doug Beeferman", "Fran\u00e7oise Beaufays", "Bill Byrne", "Ciprian Chelba", "Mike Cohen", "Maryam Kamvar", "Brian Strope"], "venue": "Advances in Speech Recognition, pp. 61\u201390. Springer US, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Accurate and compact large vocabulary speech recognition on mobile devices", "author": ["Xin Lei", "Andrew Senior", "Alexander Gruenstein", "Jeffrey Sorensen"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2013, pp. 662\u2013665.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Personalized speech recognition on mobile devices", "author": ["Ian McGraw"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), (submitted).", "citeRegEx": "8", "shortCiteRegEx": null, "year": 0}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech). ISCA, 2013, pp. 2365\u20132369.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Singular value decomposition based lowfootprint speaker adaptation and personalization for deep neural network", "author": ["Jian Xue", "Jinyu Li", "Dong Yu", "Mike Seltzer", "Yifan Gong"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 6359\u20136363.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, no. 5, pp. 855\u2013868, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2148\u20132156.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Model compression", "author": ["Cristian Bucilu\u0103", "Rich Caruana", "Alexandru Niculescu-Mizil"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 535\u2013541.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Do deep nets really need to be deep", "author": ["Lei Jimmy Ba", "Rich Caruana"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), 2014, pp. 2654\u20132662.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S. Denker", "Sara A. Solla"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), 1989, pp. 598\u2013605.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1989}, {"title": "Optimizing bottleneck features for LVCSR", "author": ["Franti\u0161ek Gr\u00e9zl", "Petr Fousek"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), March 2008, pp. 4729\u20134732.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N. Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6655\u20136659.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Smallfootprint high-performance deep neural network-based speech recognition using split-VQ", "author": ["Yongqiang Wang", "Jinyu Li", "Yifan Gong"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing deep neural networks using a rank-constrained topology", "author": ["Preetum Nakkiran", "Raziel Alvarez", "Rohit Prabhavalkar", "Carolina Parada"], "venue": " Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2015, pp. 1473\u20131477.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Ozan \u0130rsoy", "Alex Graves", "Fran\u00e7oise Beaufays", "Johan Schalkwyk"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "Proceedings of the Annual Conference of the Internation Speech Communication Association (Interspeech), 2015, pp. 1468\u20131472.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the International Conference on Machine Learning (ICML). ACM, 2006, pp. 369\u2013376.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg S. Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc\u2019Aurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1223\u20131231.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence discriminative distributed training of long short-term memory recurrent neural networks", "author": ["Ha\u015fim Sak", "Oriol Vinyals", "Georg Heigold", "Andrew Senior", "Erik McDermott", "Rajat Monga", "Mark Mao"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2014, pp. 1209\u20131213.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurrent hidden layers [3, 4] have emerged as state-of-theart acoustic models (AMs) for automatic speech recognition (ASR) tasks.", "startOffset": 49, "endOffset": 55}, {"referenceID": 1, "context": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurrent hidden layers [3, 4] have emerged as state-of-theart acoustic models (AMs) for automatic speech recognition (ASR) tasks.", "startOffset": 49, "endOffset": 55}, {"referenceID": 2, "context": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurrent hidden layers [3, 4] have emerged as state-of-theart acoustic models (AMs) for automatic speech recognition (ASR) tasks.", "startOffset": 83, "endOffset": 89}, {"referenceID": 3, "context": "Neural networks (NNs) with multiple feed-forward [1, 2] or recurrent hidden layers [3, 4] have emerged as state-of-theart acoustic models (AMs) for automatic speech recognition (ASR) tasks.", "startOffset": 83, "endOffset": 89}, {"referenceID": 4, "context": "Advances in computational capabilities coupled with the availability of large annotated speech corpora have made it possible to train NN-based AMs with a large number of parameters [5] with great success.", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "As speech recognition technologies continue to improve, they are becoming increasingly ubiquitous on mobile devices: voice assistants such as Apple\u2019s Siri, Microsoft\u2019s Cortana, Amazon\u2019s Alexa and Google Now [6] enable users to search for information using their voice.", "startOffset": 207, "endOffset": 210}, {"referenceID": 6, "context": "Although the traditional model for these applications has been to recognize speech remotely on large servers, there has been growing interest in developing ASR technologies that can recognize the input speech directly \u201con-device\u201d [7].", "startOffset": 230, "endOffset": 233}, {"referenceID": 8, "context": ", [9, 10]), where we jointly compress both recurrent and inter-layer weight matrices, allows us to compress acoustic models up to a third of their original size with negligible loss in accuracy.", "startOffset": 2, "endOffset": 9}, {"referenceID": 9, "context": ", [9, 10]), where we jointly compress both recurrent and inter-layer weight matrices, allows us to compress acoustic models up to a third of their original size with negligible loss in accuracy.", "startOffset": 2, "endOffset": 9}, {"referenceID": 10, "context": ", handwriting recognition [11] and machine translation [12] inter alia.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": ", handwriting recognition [11] and machine translation [12] inter alia.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "[13] show that the entire neural network can be reconstructed given the values of a small number of parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Caruana and colleagues show that the output distribution learned by a larger neural network can be approximated by a neural network with fewer parameters by training the smaller network to directly predict the outputs of the larger network [14, 15].", "startOffset": 240, "endOffset": 248}, {"referenceID": 14, "context": "Caruana and colleagues show that the output distribution learned by a larger neural network can be approximated by a neural network with fewer parameters by training the smaller network to directly predict the outputs of the larger network [14, 15].", "startOffset": 240, "endOffset": 248}, {"referenceID": 13, "context": "This approach, termed \u201cmodel compression\u201d [14] is closely related to the recent \u201cdistillation\u201d approach proposed by Hinton et al.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17], which imposes parameter tying in ar X iv :1 60 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "One popular technique is based on sparsifying the weight matrices in the neural network, for example, by setting weights whose magnitude falls below a certain threshold to zero [1] or based on the second-derivative of the loss function in the \u201coptimal brain damage\u201d procedure [18].", "startOffset": 177, "endOffset": 180}, {"referenceID": 17, "context": "One popular technique is based on sparsifying the weight matrices in the neural network, for example, by setting weights whose magnitude falls below a certain threshold to zero [1] or based on the second-derivative of the loss function in the \u201coptimal brain damage\u201d procedure [18].", "startOffset": 276, "endOffset": 280}, {"referenceID": 0, "context": "[1] demonstrate that up to two-thirds of the weights of the feed-forward network can be set to zero without incurring any loss in performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": ", by introducing bottleneck layers [19] or through a low-rank matrix factorization layer [20].", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": ", by introducing bottleneck layers [19] or through a low-rank matrix factorization layer [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "[21] which uses a combination of singular value decomposition (SVD) and vector quantization to compress acoustic models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The methods investigated in our work are most similar to previous work that has examined using SVD to reduce the number of parameters in the network in the context of feedforward DNNs [9, 10, 22].", "startOffset": 184, "endOffset": 195}, {"referenceID": 9, "context": "The methods investigated in our work are most similar to previous work that has examined using SVD to reduce the number of parameters in the network in the context of feedforward DNNs [9, 10, 22].", "startOffset": 184, "endOffset": 195}, {"referenceID": 21, "context": "The methods investigated in our work are most similar to previous work that has examined using SVD to reduce the number of parameters in the network in the context of feedforward DNNs [9, 10, 22].", "startOffset": 184, "endOffset": 195}, {"referenceID": 8, "context": "[9], wherein we jointly factorize both recurrent and (non-recurrent) inter-layer weight matrices in the network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "The initial model (Figure (a)) is compressed by jointly factorizing recurrent (W l h) and inter-layer (W l x) matrices, using a shared recurrent projection matrix (P ) [3] (Figure (b)).", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "We jointly compress the recurrent and inter-layer matrices corresponding to a specific layer l by determining a suitable recurrent projection matrix [3], denoted by P l \u2208 Rrl\u00d7N l , of rank r < N l such that, W l h = Z hP l and W l x = Z xP , thus allowing us to re-write (1) and (2) as, ht = \u03c3(W l\u22121 x h l\u22121 t + Z l hP ht\u22121 + b ) (3)", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Generalizing the procedure described above in the context of standard RNNs to the case of LSTM RNNs [3, 23, 24] is straightforward.", "startOffset": 100, "endOffset": 111}, {"referenceID": 22, "context": "Generalizing the procedure described above in the context of standard RNNs to the case of LSTM RNNs [3, 23, 24] is straightforward.", "startOffset": 100, "endOffset": 111}, {"referenceID": 23, "context": "Generalizing the procedure described above in the context of standard RNNs to the case of LSTM RNNs [3, 23, 24] is straightforward.", "startOffset": 100, "endOffset": 111}, {"referenceID": 2, "context": "Using the notation in [3], note that the recurrent-weight matrix W l h in the case of the LSTM is the concatenation of the four gate weight matrices, obtained by stacking them vertically:", "startOffset": 22, "endOffset": 25}, {"referenceID": 22, "context": "have demonstrated that deep LSTM-based AMs trained to predict either contextindependent (CI) phoneme targets [23] or context-dependent (CD) phoneme targets [24] approach state-of-the-art performance on speech tasks.", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "have demonstrated that deep LSTM-based AMs trained to predict either contextindependent (CI) phoneme targets [23] or context-dependent (CD) phoneme targets [24] approach state-of-the-art performance on speech tasks.", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "These systems have two important characteristics: in addition to the CI or CD phoneme labels, the system can also hypothesize a \u201cblank\u201d label if it is unsure of the identity of the current phoneme, and the systems are trained to optimize the connectionist temporal classification (CTC) criterion [25] which maximizes the total probability of correct label sequence conditioned on the input sequence.", "startOffset": 296, "endOffset": 300}, {"referenceID": 22, "context": "More details can be found in [23, 24].", "startOffset": 29, "endOffset": 37}, {"referenceID": 23, "context": "More details can be found in [23, 24].", "startOffset": 29, "endOffset": 37}, {"referenceID": 22, "context": "Following [23], our baseline model is thus a CTC model: a five hidden layer RNN with 500 LSTM cells in each layer, which predicts 41 CI phonemes (plus \u201cblank\u201d).", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "Our systems are trained using distributed asynchronous stochastic gradient descent with a parameter server [26].", "startOffset": 107, "endOffset": 111}, {"referenceID": 6, "context": "Since our goal is to build a recognizer to run efficiently on mobile devices, we minimize the size of the decoder graph used for recognition, following the approach outlined in [7]: we perform an additional pruning step to generate a much smaller first-pass language model (69.", "startOffset": 177, "endOffset": 180}, {"referenceID": 23, "context": "Following [24], we stabilize CTC training by stacking together 8 consecutive speech frames (7 right context frames); only every third stacked frame is presented as an input to the network.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "We presented a technique to compress RNNs using a joint factorization of recurrent and inter-layer weight matrices, generalizing previous work [9].", "startOffset": 143, "endOffset": 146}], "year": 2017, "abstractText": "We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.", "creator": "LaTeX with hyperref package"}}}