{"id": "1603.08701", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets", "abstract": "In this paper, we claim that Vector Cosine, which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models, can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that, independently of the adopted parameters, outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50%) and several state-of-the-art approaches.", "histories": [["v1", "Tue, 29 Mar 2016 10:00:27 GMT  (1016kb)", "http://arxiv.org/abs/1603.08701v1", "in LREC 2016"]], "COMMENTS": "in LREC 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["enrico santus", "tin-shing chiu", "qin lu", "alessandro lenci", "chu-ren huang"], "accepted": false, "id": "1603.08701"}, "pdf": {"name": "1603.08701.pdf", "metadata": {"source": "CRF", "title": "What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets", "authors": ["Enrico Santus", "Tin-Shing Chiu", "Qin Lu", "Alessandro Lenci", "Chu-Ren Huang"], "emails": ["esantus@gmail.com,", "cstschiu@comp.polyu.edu.hk,", "churen.huang}@polyu.edu.hk", "alessandro.lenci@unipi.it"], "sections": [{"heading": null, "text": "identifying word similarity in Vector Space Models \u2013 can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that \u2013 independently of the adopted parameters \u2013 outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50%) and several state-of-the-art approaches.\nKeywords: Vector Space Models, VSMs, Distributional Semantic Models, DSMs, Semantic Relations, Words Similarity"}, {"heading": "1. Introduction", "text": "Word similarity detection plays an important role in Natural Language Processing (NLP), as it is the backbone of several applications, such as Paraphrasing, Query Expansion, Word Sense Disambiguation, Automatic Thesauri Creation, and so on (Terra and Clarke, 2003). Several approaches have been proposed to measure word similarity (Terra and Clarke, 2003; Jarmasz and Szpakowicz, 2003; Mikolov et al., 2013; Levy et al., 2015; Santus et al. 2016a). Some of them rely on knowledge resources (such as lexicons or semantic networks), while others are corpus-based. The latter approaches generally exploit the Distributional Hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954). Although these approaches extract statistics from large corpora, they vary in the way they define what has to be considered context (i.e. lexical context, syntactic context, documents, etc.), how the association with such context is measured (e.g. frequency of co-occurrence, association measures like Pointwise Mutual Information, etc.), and how the association with the contexts is used to identify the similarity (Terra and Clarke, 2003; Hearst, 1992; Santus et al., 2014a; Santus et al., 2014b; Santus et al., 2016a). A common way to represent word meaning in NLP is by using vectors to encode the association between the target words and their contexts. The resulting vector space is generally referred as Vector Space Model (VSM) or, more specifically, as Distributional Semantic Model (DSM). In such vector space, word similarity can be calculated by using the Vector Cosine, which measures the angle\nbetween the vectors (Turney and Pantel, 2010). Other measures \u2013 such as Manhattan Distance, Dice\u2019s Coefficient, Euclidean Distance, Jaccard Similarity and Matching Coefficient \u2013 can be used to calculate the distance between the vectors (Gomaa and Fahmy, 2013), but the Vector Cosine is generally considered to be the optimal choice (Bullinaria and Levy 2007). Another common way to represent word meaning is using word embeddings, which are vector-space word representations that are implicitly learned by the input-layer weights of neural networks. These models have shown a strong ability to capture synonymy and analogies (such as in the famous \u201cKing - Man + Woman = Queen\u201d example, where Mikolov et al. (2013) subtract the vector of \u201cMan\u201d from the one of \u201cKing\u201d, and then add the vector of \u201cWoman\u201d, obtaining a very similar vector to the one of \u201cQueen\u201d), even though Levy et al. (2015) have claimed that traditional count-based DSMs can achieve the same results if their hyperparameters are properly optimized. A well-known problem with the distributional approaches is that they rely on a very loose definition of similarity. In fact, vectors have as nearest neighbours not only synonyms, but also hypernyms, co-hyponyms, antonyms, as well as a wide range of other semantically related items (Santus et al., 2015). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures. Among the most common ones, there are the English as a Second Language 1 dataset (ESL; Turney,\n1\nFor the state-of-the-art on the ESL, see:\n2001) and the Test of English as Foreign Language 2 (TOEFL; Landauer and Dumais, 1997). The former consists of 50 multiple-choice synonym questions, with 4 choices each, while the latter consists of 80 multiple-choice synonym questions, with 4 choices each. In this paper, we describe and evaluate APSyn, a completely unsupervised measure that calculates the extent of the intersection among the N most related contexts of two target words, weighting such intersection according to the rank of the shared contexts in a mutual dependency ranked list. In our experiments, APSyn outperforms the Vector Cosine and the co-occurrence frequency, reaching 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50%) and several state-of-the-art approaches."}, {"heading": "2. Background", "text": "Word similarity measures play a fundamental role in tasks such as Information Retrieval (IR), Text Classification (TC), Text Summarization (TS), Question Answering (QA), Sentiment Analysis (SA), and so on (Terra and Clarke, 2003; Tungthamthiti et al., 2015). They can be either knowledge-based or corpus-based (Gomaa and Fahmy, 2013). The former rely on lexicons or semantic networks, such as WordNet (Fellbaum, 1998), measuring the distance between the nodes in the network. The latter, instead, compute the similarity between words relying on statistical information about their distributions in large corpora (Church and Hanks, 1990). Knowledge based approaches generally exploit hand-crafted resources. While being hand-crafted ensures high quality, it also entails arbitrariness and high development and update costs. This is the main reason why these resources are known for their limited coverage (Santus et al., 2015b). Such limitation has often prompted researchers to pursue hybrid approaches (Turney, 2001). A key assumption of corpus-based approaches is that similarity between words can be measured by looking at words co-occurrences. In particular, following the Distributional Hypothesis (Harris, 1954; Firth, 1957), these methods assume that words occurring in similar contexts are also similar. These methods mainly vary according to two dimensions: i) how they define the contexts (e.g. document, paragraph, sentence, fixed-size window, etc.); and ii) how they measure whether the targets occur in similar contexts (e.g. weighted occurrence frequency, extent of the intersection, etc.). These models are generally referred as traditional count-based DSMs. A well-known traditional count-based DSM applied to synonymy identification is the Latent Semantic Analysis\nhttp://aclweb.org/aclwiki/index.php?title=TOEFL_Synon ym_Questions_(State_of_the_art) 2\nFor the state-of-the-art on the TOEFL, see: http://aclweb.org/aclwiki/index.php?title=ESL_Synonym _Questions_(State_of_the_art)\n(LSA; Landauer and Dumais, 1997). This system was tested on the 80 multiple-choice synonym questions of the TOEFL, achieving an accuracy of 64.38%, which is very close to the reported average of non-English US college applicant (i.e. 64.50%). Another interesting way to learn words statistics is generally referred as word embeddings and is discussed in Mikolov et al. (2013). The authors report that when a neural network language model is trained, it is not only the model that is obtained, but also distributed words representations, which can be eventually used for other goals, such as in Collobert and Weston (2008). In their paper, Mikolov et al. (2013) show that these words representations capture both syntactic and semantic regularities, performing particularly well in word similarity identification and analogies. While such models have obtained an enthusiastic reception, with a consequent boost of papers using word embeddings, Levy et al. (2015) have demonstrated that similar results can be also obtained with optimized traditional count-based DSMs."}, {"heading": "2.1 Distance Measures", "text": "Independently from the approach that is used to learn words statistics, corpus-based approaches represent word meanings as vectors in vector spaces, generally called semantic spaces. In such semantic spaces, words similarity can be measured as the proximity between vectors. Several measures have been adopted to this scope. In the following rows, we briefly describe some of them, while defining the Vector Cosine, which is generally considered the most efficient one. Manhattan Distance (L1) can be defined as the sum of the differences of the dimensions. Euclidean Distance (L2) is the square root of the sum of the squared differences of the dimensions. Dice\u2019s Coefficient is instead twice the number of common dimensions, divided by the total number of dimensions in the two vectors. Jaccard Similarity is defined as the number of shared dimensions divided by the number of unique dimensions in both the vectors. Matching Coefficient is the number of dimensions different from zero in both the vectors. Vector Cosine looks instead at the normalized correlation between the dimensions of two words, \ud835\udc641 and \ud835\udc642, and is described by the following equation:\ncos(\ud835\udc641, \ud835\udc642) = \u2211 \ud835\udc531\ud835\udc56 \u00d7\n\ud835\udc5b \ud835\udc56=1 \ud835\udc532\ud835\udc56\n\u221a\u2211(\ud835\udc531\ud835\udc56) 2 \u00d7 \u221a\u2211(\ud835\udc532\ud835\udc56) 2\nwhere \ud835\udc53\ud835\udc65\ud835\udc56 is the i-th dimension in the vector x. This measure has been extensively used to identify word similarity in vector spaces becoming a sort of de facto standard in distributional semantics (Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2003; Mikolov et al., 2013; Levy et al., 2015)."}, {"heading": "2.2 State-of-the-art in the ESL and TOEFL", "text": "After its first use in Landauer and Dumais (1997), the TOEFL dataset became one of the most common benchmarks for vector space models testing: Karlgren and Sahlgren (2001), Pado and Lapata (2007), Turney (2001), Turney (2008), Terra and Clarke (2003), Bullinaria and Levy (2007), Matveeva et al. (2005), Dob\u00f3 and Csirik (2013) and Rapp (2003). Bullinaria and Levy (2012) even achieved 100% accuracy on this dataset. In their paper, the authors extensively analyze numerous parameters, including the influence of corpus size, window size, stop-lists, stemming and Singular Values Decomposition (SVD), until they find a perfectly optimized model. After achieving perfect precision on the TOEFL, the authors acknowledge that while these results are impressive for the benchmark, they can hardly be generalized to new tasks. Few years after the introduction of TOEFL as a benchmark, Turney (2001) proposed the ESL. These 50 multiple-choice synonym questions are provided in a sentence context, to facilitate sense disambiguation. ESL has soon become a very popular benchmark on which several models have been evaluated. The best reported corpus-based approaches in this benchmark were those of Turney (2001), Terra and Clarke (2003) and Jarmasz and Szpakowicz (2003). The latter achieving the best result of 82% accuracy."}, {"heading": "3. Method", "text": "Given a traditional count-based DSM, where every word is represented as a vector of weighted associations between such word and contexts, we can re-think the Distributional Hypothesis (Harris, 1954) by hypothesizing that similar words not only occur in similar contexts, but \u2013 more specifically \u2013 they share a larger number of their most associated contexts, compared to less similar ones. A way to test this hypothesis is by: i) measuring the extent of the intersection among the N most related contexts of two target words, and ii) weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This can be done in several steps. First of all, for every target word we rank the contexts according to the Positive Pointwise Mutual Information (PPMI; Levy et al., 2015), which is defined as follows:\n\ud835\udc43\ud835\udc40\ud835\udc3c(\ud835\udc64, \ud835\udc50) = log ( \ud835\udc43(\ud835\udc64, \ud835\udc50)\n\ud835\udc43(\ud835\udc64) \u00d7 \ud835\udc43(\ud835\udc50) ) = log (\n|\ud835\udc64, \ud835\udc50| \u00d7 \ud835\udc37\n|\ud835\udc64| \u00d7 |\ud835\udc50| )\n\ud835\udc43\ud835\udc43\ud835\udc40\ud835\udc3c(\ud835\udc64, \ud835\udc50) = max (\ud835\udc43\ud835\udc40\ud835\udc3c(\ud835\udc64, \ud835\udc50), 0)\nwere \ud835\udc64 is the target word, \ud835\udc50 is the given context, \ud835\udc43(\ud835\udc64, \ud835\udc50) is the probability of co-occurrence and \ud835\udc37 is the collection of observed word-context pairs. Once the contexts are ranked according to their PPMI, for every target word we pick the top N contexts and we\nintersect them. At this point, for each shared context, we add one divided by the average rank of the shared context in the PPMI-ranked contexts lists. We formalize this as the APSyn similarity measure:\n\ud835\udc34\ud835\udc43\ud835\udc46\ud835\udc66\ud835\udc5b(\ud835\udc641, \ud835\udc642) = \u2211 1\n(\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc581(\ud835\udc53) + \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc582(\ud835\udc53))/2 \ud835\udc53\u2208\ud835\udc41(\ud835\udc391)\u2229\ud835\udc41(\ud835\udc392)\nFor every feature \ud835\udc53 included in the intersection between the top N features of \ud835\udc641, \ud835\udc41(\ud835\udc391), and \ud835\udc642, \ud835\udc41(\ud835\udc392), APSyn will add 1 divided by the average rank of the feature, among the top PPMI ranked features of \ud835\udc641, \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc581(\ud835\udc531), and \ud835\udc642, \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc582(\ud835\udc532). The choice of the weighting function is a parameter of APsyn. In previous experiments, published in Santus et al. (2016a), we used Local Mutual Information (LMI; Evert, 2005) to rank the contexts, instead of using PPMI. However, the LMI-ranked APSyn obtained worse results than those reported in the current paper. Such results were nonetheless still outperforming the Vector Cosine and the co-occurrence frequency. In section 9, we will comment on the differences."}, {"heading": "4. Evaluation", "text": "In the following paragraphs we describe our DSMs, the test sets and the task."}, {"heading": "4.1 Distributional Semantic Model", "text": "We use several window-based DSMs, recording word co-occurrences within the K nearest content words to the left and right of each target, where K has the following values: 2, 3, 5 and 10. Co-occurrences are extracted from a combination of ukWaC and WaCkypedia corpora (around 2.7 billion words) for content words \u2013 namely adjectives, nouns and verbs \u2013 occurring over 1,000 times, and are weighted with PPMI. The model consists of 28,870 word vectors, each of which with 28.870 dimensions."}, {"heading": "4.2 Test Sets", "text": "In order to evaluate the proposed measure, we use both the ESL (Turney, 2001) and TOEFL (Landauer and Dumais, 1997) datasets. The former consists of 50 questions, while the latter of 80 questions. The ESL sentences were not used in our experiments. An example of ESL question is the following:\n \u201cAn underground [passage] connected the house\nto the garage.\u201d\na. Hallway b. Ticket c. Entrance d. Room\nFor both datasets, we have turned each question in four pairs, each of which containing the problem word and\napossible answer. Unfortunately, we do not have a full coverage of the datasets, because our model was built for content words with frequency over 1000, Parts-Of-Speech-tagged either as adjectives, nouns or verbs. In the ESL test set, 4 out of 50 questions were excluded because the correct answers were not present in the DSM. In the TOEFL test set, 20 out of 80 questions were excluded for the same reason. Few questions, moreover, have one missing choice. In order to keep them for the evaluation, in case of correct answer, the score is increased of 0.25 \u2217 |\ud835\udc50\u210e\ud835\udc5c\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60 \ud835\udc56\ud835\udc5b \ud835\udc37\ud835\udc46\ud835\udc40| (where, 1 is added only if all four choices are in the DSM)."}, {"heading": "4.3 Task", "text": "We have assigned APSyn scores to all the pairs, and then \u2013 for every problem word \u2013 we have sorted the possible choices in a decreasing order. We considered positive every problem word having the correct answer on top, negative all the others."}, {"heading": "5. Results", "text": "In Table 1, we report the results of APSyn and the baselines in the ESL test.\nAs it can be seen from Table 1 and from Figure 1, APSyn always outperforms the baselines. The window size and N have a certain impact on the performance of APSyn. The former parameter has an impact also on the baselines (Vector Cosine seems to perform slightly better for larger windows, while the co-occurrence frequency seems to prefer smaller ones). Our measure, in particular, seems to perform better on smaller windows and for N close to 100, while its performance slightly drops for N close to 1000. A possible reason for such drop may be that if too many contexts are considered, some rumor is added. This happens because with larger values of N, APSyn is forced to consider less important contexts of the targets. In Table 2, we report the scores for APSyn and the baselines in the evaluation on the TOEFL test set (see also\nFigure 2). APSyn outperforms the baselines, especially when the window size and N are small. Interestingly, the Vector Cosine prefers a smaller window in this dataset (this is actually what we would have expected also for the ESL, as smaller windows are known to better capture paradigmatic similarity).\nWhile considering the results, it must be kept in mind that those that may look as high variances, could be actually\nbe very small ones, given the small size of the test sets. Guessing one more question, for example, would have a large impact on the accuracy."}, {"heading": "6. Error Analysis", "text": "In this section we briefly analyse the best and worst performance of APSyn in the ESL dataset. From the previous section, we have seen that APSyn performs best with a window 2 DSM and N=100. In Table 3, we report the non-identified synonyms (with their Parts of Speech: n=noun, v=verb and j=adjective), with their actual rank in the question pairs. As it can be seen, most of the non-identified synonyms are ranked second according to APSyn, and they are generally placed after a word that \u2013 at least in certain contexts \u2013 has a very similar meaning to the problem word (e.g. grind-slice, limb-trunk, passage-entrance, refer-call, scrape-slice, steep-rugged, twist-curl). Unfortunately, we have not used the contextual sentences to disambiguate the problem words.\nIn Table 4, we report the non-identified synonyms in the worst setting of APSyn, namely window 10 and N=1000. As it can be seen, most of those questions for which the synonym was not identified in the best setting (reported in italics in Table 4) are kept as errors also in the worst one. It is also interesting to notice that not only the number of errors increased, but also that the positions of the true synonyms in the error are lower than in the best setting. Even for questions that were already wrong in the best setting, the correct synonym was further penalized, losing a position (e.g. refer-direct and yield-submit). Finally, it is evident in Table 4 that several new non-identified synonyms are introduced, affecting negatively the overall performance. To summarize, window 2 and N close to 100 are certainly the best parameters. Not only because they improve the overall accuracy, but also because, when making a mistake, they have a closer approximation to the correct\nanswers. Namely the real synonym is typically ranked second rather than first, and the error is mostly due to sense ambiguity.\nThe worst performance of APSyn is obtained for large window and large N (window 10 and N=1000). In the error analysis, we have seen that the worst model has generally bigger difficulty in classifying the synonym as somehow similar to the question word, often ranking it fourth.\n7. Comparison with LMI-based APSyn\nAPSyn was introduced in Santus et al. (2016a) and tested on the ESL. In this paper, the top features were selected after ranking them by LMI instead of PPMI. LMI is less biased towards infrequent events than PPMI and it is defined as follows:\n\ud835\udc3f\ud835\udc40\ud835\udc3c(\ud835\udc64, \ud835\udc50) = \ud835\udc43(\ud835\udc64, \ud835\udc50) \u00d7 \ud835\udc43\ud835\udc43\ud835\udc40\ud835\udc3c(\ud835\udc64, \ud835\udc50)\nwhere \ud835\udc64 is the target word, \ud835\udc50 is the given context, \ud835\udc43(\ud835\udc64, \ud835\udc50) is the probability of co-occurrence, as shown in the PPMI formula, above.\nAs mentioned above, the performance of the LMI-based APSyn on a window 5 DSM was worse than what reported with PPMI. However, its 58.33% accuracy was much above the Vector Cosine, which was instead blocked at 49.44%. In Table 5 we show all the scores, recalculated with the LMI-based APSyn. Note that the recall in the models used for this paper is slightly higher, reaching 46 questions rather than 45, so the scores can be slightly different. Despite results are worse than those obtained with PPMI, they are however relatively stable with reference to N, and almost always above the baseline. Only with window 10 and N close to 100 the performance is the equal to the Vector Cosine."}, {"heading": "8. Conclusions", "text": "In this paper, we have described APSyn, a completely unsupervised measure based on the evaluation of the extent and the relevance of the intersection among the top ranked distributional features of target words. APSyn was tested on the ESL and TOEFL questions, outperforming the Vector Cosine and the co-occurrence, plus several lexicon-based and hybrid models. In particular, our results are above those reported in the literature for non-English US college applicants on the TOEFL test (64.50%). Our experiments show that the intersection among the N most related contexts of the target words is in fact a reliable index of similarity. In our evaluations we have also mentioned the role of both the window size and N. APSyn performs better on smaller windows and with N\n3\nNot differently from Santus et al. (2016a), the LMI-based APSyn guessed 26.25 questions (24 full and 3 partial), but being the recall higher in the current DSM, this number has been divided by 46. 4\nNot differently from Santus et al. (2016a), the LMI-based APSyn guessed 24.25 questions (22 full and 3 partial), but being the recall higher in the current DSM, this number has been divided by 46.\nclose to 100. In fact, the larger the amount of considered contexts, the lower the ability of identifying similarity (exceptions to this consideration are minimal and can be appreciated only because of the limited size of the test sets). This also confirms our hypothesis that similar words share a significantly larger number of top mutually dependent contexts, but such intersection becomes less significant when not only the top contexts are considered, as rumor is introduced. Given that, it is important to notice that APSyn performance is quite stable, in respect to N variances. APSyn has been recently used as one of the thirteen features of ROOT13, a random-forest based supervised system for the identification of hypernyms, co-hyponyms and unrelated words. In a 10-fold evaluation on 9600 pairs extracted from EVALuation (Santus et al., 2015a), ROOT13 achieved 88.3% accuracy when the three classes were present, 93.4% for hypernyms-co-hyponyms discrimination, 92.3% for hypernyms-random discrimination, 97.3% for co-hyponyms-random (Santus et al., 2016b). Possible improvements to the measure include changing the numerator to a more significant value, rather than simply using the constant 1. Moreover, it would be important to test the measure on optimized DSMs, where more parameters are investigated (e.g. stemming, dependency, SVD, etc.). Moreover, since ESL and TOEFL are small test sets, APSyn performance should be further explored on larger datasets, such as the Lenci/Benotto (Benotto, 2015), SimLex-999 (Hill et al., 2014) and EVALuation (Santus et al., 2015a)."}, {"heading": "9. Acknowledgements", "text": "This work is partially supported by HK PhD Fellowship Scheme under PF12-13656\n10. Main References\nBaroni, M. and Lenci, A. (2011). How we BLESSed\ndistributional semantic evaluation. Proceedings of the EMNLP 2011 Geometrical Models for Natural Language Semantics (GEMS 2011) Workshop. Edinburg, UK. 1-10. Benotto, Giulia. (2015). Distributional Models for\nSemantic Relations: A Sudy on Hyponymy and Antonymy. PhD Thesis, University of Pisa. Bullinaria, J.A., and Levy, J.P. (2007). Extracting\nsemantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39(3), 510-526. Bullinaria, J.A., and Levy, J.P. (2012). Extracting\nsemantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD. Behavior Research Methods, 44(3):890-907. Church, K. Ward, and Patrick Hanks. (1990). Word\nassociation norms, mutual information, and lexicography. In Journal Computational Linguistics, Vol. 16 (1).\nCollobert, Robert, and Jason Weston. (2008). A Unified\nArchitecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML. Dob\u00f3, A., and Csirik, J. (2013). \u201cComputing semantic\nsimilarity using large static corpora\u201d. In: van Emde Boas, P. et al. (eds.) SOFSEM 2013: Theory and Practice of Computer Science. LNCS, Vol. 7741. Springer-Verlag, Berlin Heidelberg, pp. 491-502 Evert, S. (2005). The Statistics of Word Cooccurrences:\nWord Pairs and Collocations. Dissertation, University of Stuttgart. Fellbaum C. (1998). WordNet: An electronic lexical\ndatabase. Cambridge, MA: MIT Press.\nFirth, J. R. (1957). Papers in Linguistics 1934\u20131951.\nLondon: Oxford University Press.\nGomaa, Wael H. and Aly A. Fahmy. (2013). A survey of\ntext similarity approaches. In International Journal of Computer Applications, Vol. 68 (13). Harris, Z. (1954). Distributional structure. Word, Vol. 10\n(23). 146-162.\nHearst, M. A. (1992). Automatic Acquisition of\nHyponyms from Large Text Corpora. Proceedings of the 14th International Conference on Computational Linguistics. Nantes, France. 539-545. Hill, Felix, Roi Reichart and Anna Korhonen. (2014).\nSimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation. In Computational Linguistics. Jarmasz, M. and Szpakowicz, S. (2003). Roget\u2019s\nthesaurus and semantic similarity, Proceedings of RANLP 2003, Borovets, Bulgaria. 212-219. Landauer, T. K. and Dumais, S.T. (1997). A solution to\nPlato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211-240. Levy, O., Goldberg, Y. and Dagan I. (2015). Improving\nDistributional Similarity with Lessons Learned from Word Embeddings. TACL 2015. Matveeva, I., Levow, G., Farahat, A., and Royer, C.\n(2005). Generalized latent semantic analysis for term representation. Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-05), Borovets, Bulgaria. Mikolov, T., Yih, W. and Zweig G. (2013). Linguistic\nRegularities in Continuous Space Word Representations. Proceedings of HLT-NAACL, 746-751. Karlgren, J., and Sahlgren, M. (2001). \u201cFrom Words to\nUnderstanding\u201d. In Uesaka, Y., Kanerva, P., & Asoh, H. (Eds.), Foundations of Real-World Intelligence, Stanford: CSLI Publications, pp. 294\u2013308. Pado, S., and Lapata, M. (2007). Dependency-based\nconstruction of semantic space models. Computational Linguistics, Vol. 33 (2), pp. 161-199. Rapp, R. (2003). Word sense discovery based on sense\ndescriptor dissimilarity. Proceedings of the Ninth Machine Translation Summit, pp. 315-322.\nSantus, E., Chiu, T.-S., Lu, Q., Lenci, A., and Huang,\nC.-R. (2016a). Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs. In Proceedings of AAAI 2016, Phoenix, Arizona (USA) Santus, E., Chiu, T.-S., Lu, Q., Lenci, A., and Huang,\nC.-R. (2016b). ROOT 13: Spotting Hypernyms, Co-Hyponyms and Randoms. In Proceedings of AAAI 2016, Phoenix, Arizona (USA) Santus, E., Yung, F., Lenci, A. and Huang C-R. (2015).\nEVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models. Proceedings of the 4th Workshop on Linked Data in Linguistics, ACL-IJCNLP 2015, 64 Santus, E., Lenci, A., Lu, Q., and Huang C-R.\n(2015). When Similarity Becomes Opposition: Synonyms and Antonyms Discrimination in DSMs. Italian Journal on Computational Linguistics, aAccademia University Press Santus, E., Lenci, A., Lu, Q. and Schulte im Walde, S.\n(2014a). Chasing Hypernyms in Vector Spaces with Entropy. Proceedings of EACL 2014, 2:38\u201342, Gothenburg, Sweden. Santus, E., Lu, Q. Lenci, A. and Huang, C-R. (2014b).\nTaking Antonymy Mask off in Vector Space. Proceedings of PACLIC 2014, Phuket, Thailand. Terra, E. and Clarke, C.L.A. (2003). Frequency estimates\nfor statistical word similarity measures. Proceedings of HLT/NAACL 2003. 244\u2013251. Tungthamthiti, Piyoros, Enrico Santus, Hongzhi Xu,\nChu-Ren Huang and Kiyoaki Shirai. 2015. Sentiment Analyzer with Rich Features for Ironic and Sarcastic Tweets. In Proceedings of PACLIC 2015, 178-187. Turney, P.D. and Pantel, P. (2010). From Frequency to\nMeaning: Vector Space Models of Semantics. Journal of Articial Intelligence Research, Vol. 37. 141-188. Turney, P.D. (2008). A uniform approach to analogies,\nsynonyms, antonyms, and associations. Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), Manchester, UK, pp. 905-912. Turney, P.D. (2001). Mining the Web for synonyms:\nPMI-IR versus LSA on TOEFL. Proceedings of ECML-2001, Freiburg, Germany. 491-502.\nLanguage Resource References\nESL. In: Turney, P.D. (2001). Mining the Web for\nsynonyms: PMI-IR versus LSA on TOEFL. Proceedings of ECML-2001, Freiburg, Germany. 491-502. EVALution. In: Santus, E., Yung, F., Lenci, A. and\nHuang C-R. (2015). EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models. Proceedings of the 4th Workshop on Linked Data in Linguistics, ACL-IJCNLP 2015, 64. Lenci/Benotto. In: Benotto, Giulia. (2015).\nDistributional Models for Semantic Relations: A Sudy on Hyponymy and Antonymy. PhD Thesis, University of Pisa. SimLex-999. In: Hill, Felix, Roi Reichart and Anna\nKorhonen. (2014). SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation.\nIn Computational Linguistics.\nTOEFL. In: Landauer, T. K. and Dumais, S.T. (1997). A\nsolution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211-240."}], "references": [{"title": "Mining the Web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["P.D. ESL. In: Turney"], "venue": "Proceedings of ECML-2001,", "citeRegEx": "Turney,? \\Q2001\\E", "shortCiteRegEx": "Turney", "year": 2001}, {"title": "EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models", "author": ["E. EVALution. In: Santus", "F. Yung", "A. Lenci", "Huang C-R"], "venue": "Proceedings of the 4th Workshop on Linked Data in Linguistics,", "citeRegEx": "Santus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2015}, {"title": "SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation", "author": ["SimLex-999. In: Hill", "Felix", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "A solution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge", "author": ["T.K. TOEFL. In: Landauer", "S.T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "In fact, vectors have as nearest neighbours not only synonyms, but also hypernyms, co-hyponyms, antonyms, as well as a wide range of other semantically related items (Santus et al., 2015).", "startOffset": 166, "endOffset": 187}, {"referenceID": 3, "context": "(TOEFL; Landauer and Dumais, 1997).", "startOffset": 0, "endOffset": 34}, {"referenceID": 0, "context": "Such limitation has often prompted researchers to pursue hybrid approaches (Turney, 2001).", "startOffset": 75, "endOffset": 89}, {"referenceID": 3, "context": "php?title=ESL_Synonym _Questions_(State_of_the_art) (LSA; Landauer and Dumais, 1997).", "startOffset": 52, "endOffset": 84}, {"referenceID": 3, "context": "standard in distributional semantics (Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2003; Mikolov et al., 2013; Levy et al., 2015).", "startOffset": 37, "endOffset": 135}, {"referenceID": 3, "context": "2 State-of-the-art in the ESL and TOEFL After its first use in Landauer and Dumais (1997), the", "startOffset": 63, "endOffset": 90}, {"referenceID": 0, "context": "TOEFL dataset became one of the most common benchmarks for vector space models testing: Karlgren and Sahlgren (2001), Pado and Lapata (2007), Turney (2001), Turney (2008), Terra and Clarke (2003), Bullinaria and Levy (2007), Matveeva et al.", "startOffset": 142, "endOffset": 156}, {"referenceID": 0, "context": "TOEFL dataset became one of the most common benchmarks for vector space models testing: Karlgren and Sahlgren (2001), Pado and Lapata (2007), Turney (2001), Turney (2008), Terra and Clarke (2003), Bullinaria and Levy (2007), Matveeva et al.", "startOffset": 142, "endOffset": 171}, {"referenceID": 0, "context": "TOEFL dataset became one of the most common benchmarks for vector space models testing: Karlgren and Sahlgren (2001), Pado and Lapata (2007), Turney (2001), Turney (2008), Terra and Clarke (2003), Bullinaria and Levy (2007), Matveeva et al.", "startOffset": 142, "endOffset": 196}, {"referenceID": 0, "context": "TOEFL dataset became one of the most common benchmarks for vector space models testing: Karlgren and Sahlgren (2001), Pado and Lapata (2007), Turney (2001), Turney (2008), Terra and Clarke (2003), Bullinaria and Levy (2007), Matveeva et al.", "startOffset": 142, "endOffset": 224}, {"referenceID": 0, "context": "TOEFL dataset became one of the most common benchmarks for vector space models testing: Karlgren and Sahlgren (2001), Pado and Lapata (2007), Turney (2001), Turney (2008), Terra and Clarke (2003), Bullinaria and Levy (2007), Matveeva et al. (2005), Dob\u00f3 and Csirik (2013) and Rapp (2003).", "startOffset": 142, "endOffset": 248}, {"referenceID": 0, "context": "TOEFL dataset became one of the most common benchmarks for vector space models testing: Karlgren and Sahlgren (2001), Pado and Lapata (2007), Turney (2001), Turney (2008), Terra and Clarke (2003), Bullinaria and Levy (2007), Matveeva et al. (2005), Dob\u00f3 and Csirik (2013) and Rapp (2003).", "startOffset": 142, "endOffset": 272}, {"referenceID": 0, "context": "TOEFL dataset became one of the most common benchmarks for vector space models testing: Karlgren and Sahlgren (2001), Pado and Lapata (2007), Turney (2001), Turney (2008), Terra and Clarke (2003), Bullinaria and Levy (2007), Matveeva et al. (2005), Dob\u00f3 and Csirik (2013) and Rapp (2003). Bullinaria and Levy (2012) even achieved 100% accuracy on this dataset.", "startOffset": 142, "endOffset": 288}, {"referenceID": 0, "context": "TOEFL dataset became one of the most common benchmarks for vector space models testing: Karlgren and Sahlgren (2001), Pado and Lapata (2007), Turney (2001), Turney (2008), Terra and Clarke (2003), Bullinaria and Levy (2007), Matveeva et al. (2005), Dob\u00f3 and Csirik (2013) and Rapp (2003). Bullinaria and Levy (2012) even achieved 100% accuracy on this dataset.", "startOffset": 142, "endOffset": 316}, {"referenceID": 0, "context": "Few years after the introduction of TOEFL as a benchmark, Turney (2001) proposed the ESL.", "startOffset": 58, "endOffset": 72}, {"referenceID": 0, "context": "Few years after the introduction of TOEFL as a benchmark, Turney (2001) proposed the ESL. These 50 multiple-choice synonym questions are provided in a sentence context, to facilitate sense disambiguation. ESL has soon become a very popular benchmark on which several models have been evaluated. The best reported corpus-based approaches in this benchmark were those of Turney (2001), Terra and Clarke (2003) and Jarmasz and Szpakowicz (2003).", "startOffset": 58, "endOffset": 383}, {"referenceID": 0, "context": "Few years after the introduction of TOEFL as a benchmark, Turney (2001) proposed the ESL. These 50 multiple-choice synonym questions are provided in a sentence context, to facilitate sense disambiguation. ESL has soon become a very popular benchmark on which several models have been evaluated. The best reported corpus-based approaches in this benchmark were those of Turney (2001), Terra and Clarke (2003) and Jarmasz and Szpakowicz (2003).", "startOffset": 58, "endOffset": 408}, {"referenceID": 0, "context": "Few years after the introduction of TOEFL as a benchmark, Turney (2001) proposed the ESL. These 50 multiple-choice synonym questions are provided in a sentence context, to facilitate sense disambiguation. ESL has soon become a very popular benchmark on which several models have been evaluated. The best reported corpus-based approaches in this benchmark were those of Turney (2001), Terra and Clarke (2003) and Jarmasz and Szpakowicz (2003). The latter achieving the best result of 82% accuracy.", "startOffset": 58, "endOffset": 442}, {"referenceID": 1, "context": "In previous experiments, published in Santus et al. (2016a), we used Local Mutual Information (LMI; Evert,", "startOffset": 38, "endOffset": 60}, {"referenceID": 0, "context": "2 Test Sets In order to evaluate the proposed measure, we use both the ESL (Turney, 2001) and TOEFL (Landauer and Dumais, 1997) datasets.", "startOffset": 75, "endOffset": 89}, {"referenceID": 3, "context": "2 Test Sets In order to evaluate the proposed measure, we use both the ESL (Turney, 2001) and TOEFL (Landauer and Dumais, 1997) datasets.", "startOffset": 100, "endOffset": 127}, {"referenceID": 1, "context": "APSyn was introduced in Santus et al. (2016a) and tested on the ESL.", "startOffset": 24, "endOffset": 46}, {"referenceID": 1, "context": "3 Not differently from Santus et al. (2016a), the LMI-based APSyn guessed 26.", "startOffset": 23, "endOffset": 45}, {"referenceID": 1, "context": "3 Not differently from Santus et al. (2016a), the LMI-based APSyn guessed 26.25 questions (24 full and 3 partial), but being the recall higher in the current DSM, this number has been divided by 46. 4 Not differently from Santus et al. (2016a), the LMI-based APSyn guessed 24.", "startOffset": 23, "endOffset": 244}, {"referenceID": 2, "context": "Moreover, since ESL and TOEFL are small test sets, APSyn performance should be further explored on larger datasets, such as the Lenci/Benotto (Benotto, 2015), SimLex-999 (Hill et al., 2014) and EVALuation (Santus et al.", "startOffset": 170, "endOffset": 189}], "year": 2016, "abstractText": "In this paper, we claim that Vector Cosine \u2013 which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models \u2013 can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that \u2013 independently of the adopted parameters \u2013 outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50%) and several state-of-the-art approaches.", "creator": "Microsoft\u00ae Word 2010"}}}