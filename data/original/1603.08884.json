{"id": "1603.08884", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "abstract": "Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\\it MCTest} benchmark. Partly because of its limited size, prior work on {\\it MCTest} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for {\\it MCTest}, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15\\% absolute).", "histories": [["v1", "Tue, 29 Mar 2016 18:52:46 GMT  (58kb,D)", "http://arxiv.org/abs/1603.08884v1", "9 pages, submitted to ACL"]], "COMMENTS": "9 pages, submitted to ACL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adam trischler", "zheng ye", "xingdi yuan", "jing he", "philip bachman"], "accepted": true, "id": "1603.08884"}, "pdf": {"name": "1603.08884.pdf", "metadata": {"source": "CRF", "title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "authors": ["Adam Trischler", "Zheng Ye jeff.ye", "Xingdi Yuan eric.yuan", "Jing He", "Phillip Bachman"], "emails": ["k.suleman@maluuba.com"], "sections": [{"heading": "1 Introduction", "text": "Humans learn in a variety of ways\u2014by communication with each other, and by study, the reading of text. Comprehension of unstructured text by machines, at a near-human level, is a major goal for natural language processing. It has garnered significant attention from the machine learning research community in recent years.\nMachine comprehension (MC) is evaluated by posing a set of questions based on a text passage (akin to the reading tests we all took in school). Such tests are objectively gradable and can be used to assess a range of abilities, from basic understanding to causal reasoning to inference (Richardson et al., 2013). Given a text passage and a question about its content, a system is tested on its ability to determine the correct answer (Sachan et al., 2015). In this work, we focus on MCTest, a complex but data-limited comprehension benchmark, whose multiple-choice questions require not only extraction but also inference and limited reasoning (Richardson et al., 2013). Inference and reasoning are important human skills that apply broadly, beyond language.\nWe present a parallel-hierarchical approach to machine comprehension designed to work well in a data-limited setting. There are many use-cases in which comprehension over limited data would be handy: for example, user manuals, internal documentation, legal contracts, and so on. Moreover, work towards more efficient learning from any quantity of data is important in its own right, for bringing machines more in line with the way humans learn. Typically, artificial neural networks require numerous parameters to capture complex patterns, and the more parameters, the more training data is required to tune them. Likewise, deep models learn to extract their own features, but this is a data-intensive process. Our model learns to comprehend at a high level even when data is sparse.\nThe key to our model is that it compares the question and answer candidates to the text using several distinct perspectives. We refer to a question combined with one of its answer candidates as a hypothesis (to be detailed below). The semantic perspective compares the hypothesis to sentences in the text viewed as single, self-contained\nar X\niv :1\n60 3.\n08 88\n4v 1\n[ cs\n.C L\n] 2\n9 M\nar 2\n01 6\nthoughts; these are represented using a sum and transformation of word embedding vectors, similarly to in Weston et al. (2014). The word-by-word perspective focuses on similarity matches between individual words from hypothesis and text, at various scales. As in the semantic perspective, we consider matches over complete sentences. We also use a sliding window acting on a subsentential scale (inspired by the work of Hill et al. (2015)), which implicitly considers the linear distance between matched words. Finally, this word-level sliding window operates on two different views of text sentences: the sequential view, where words appear in their natural order, and the dependency view, where words are reordered based on a linearization of the sentence\u2019s dependency graph. Words are represented throughout by embedding vectors (Mikolov et al., 2013). These distinct perspectives naturally form a hierarchy that we depict in Figure 1. Language is hierarchical, so it makes sense that comprehension relies on hierarchical levels of understanding.\nThe perspectives of our model can be considered a type of feature. However, they are implemented by parametric differentiable functions. This is in contrast to most previous efforts on MCTest, whose numerous hand-engineered features cannot be trained. Our model, significantly, can be trained end-to-end with backpropagation. To facilitate learning with limited data, we also develop a unique training scheme. We initialize the model\u2019s neural networks to perform specific heuristic functions that yield decent (thought not impressive) performance on the dataset. Thus, the training scheme gives the model a safe, reasonable baseline from which to start learning. We call this technique training wheels.\nComputational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Hermann et al., 2015; Kumar et al., 2015). Models designed specifically for MCTest include those of Richardson et al. (2013), and more recently Sachan et al. (2015), Wang and McAllester (2015), and Yin et al. (2016). In experiments, our Parallel-Hierarchical model achieves state-of-the-art accuracy on MCTest, outperforming these existing methods.\nBelow we describe related work, the mathematical details of our model, and our experiments,\nthen analyze our results."}, {"heading": "2 The Problem", "text": "In this section we borrow from Sachan et al. (2015), who laid out the MC problem nicely. Machine comprehension requires machines to answer questions based on unstructured text. This can be viewed as selecting the best answer from a set of candidates. In the multiple-choice case, candidate answers are predefined, but candidate answers may also be undefined yet restricted (e.g., to yes, no, or any noun phrase in the text) (Sachan et al., 2015).\nFor each question q, let T be the unstructured text and A = {ai} the set of candidate answers to q. The machine comprehension task reduces to selecting the answer that has the highest evidence given T . As in Sachan et al. (2015), we combine an answer and a question into a hypothesis, hi = f(q, ai). To facilitate comparisons of the text with the hypotheses, we also break down the passage into sentences tj , T = {tj}. In our setting, q, ai, and tj each represent a sequence of embedding vectors, one for each word and punctuation mark in the respective item."}, {"heading": "3 Related Work", "text": "Machine comprehension is currently a hot topic within the machine learning community. In this section we will focus on the best-performing models applied specifically to MCTest, since it is somewhat unique among MC datasets (see Section 5). Generally, models can be divided into two categories: those that use fixed, engineered features, and neural models. The bulk of the work on MCTest falls into the former category.\nManually engineered features often require significant effort on the part of a designer, and/or various auxiliary tools to extract them, and they cannot be modified by training. On the other hand, neural models can be trained end-to-end and typically harness only a single feature: vectorrepresentations of words. Word embeddings are fed into a complex and possibly deep neural network which processes and compares text to question and answer. Among deep models, mechanisms of attention and working memory are common, as in Weston et al. (2014) and Hermann et al. (2015)."}, {"heading": "3.1 Feature-engineering models", "text": "Sachan et al. (2015) treated MCTest as a structured prediction problem, searching for a latent answerentailing structure connecting question, answer, and text. This structure corresponds to the best latent alignment of a hypothesis with appropriate snippets of the text. The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015). The model uses event and entity coreference links across sentences along with a host of other features. These include specifically trained word vectors for synonymy; antonymy and class-inclusion relations from external database sources; dependencies and semantic role labels. The model is trained using a latent structural SVM extended to a multitask setting, so that questions are first classified using a pretrained top-level classifier. This enables the system to use different processing strategies for different question categories. The model also combines question and answer into a well-formed statement using the rules of Cucerzan and Agichtein (2005).\nOur model is simpler than that of Sachan et al. (2015) in terms of the features it takes in, the training procedure (stochastic gradient descent vs. alternating minimization), question classification (we use none), and question-answer combination (simple concatenation or mean vs. a set of rules).\nWang and McAllester (2015) augmented the baseline feature set from Richardson et al. (2013) with features for syntax, frame semantics, coreference chains, and word embeddings. They combined features using a linear latent-variable classifier trained to minimize a max-margin loss function. As in Sachan et al. (2015), questions and answers are combined using a set of manually written rules. The method of Wang and McAllester (2015) achieved the previous state of the art, but has significant complexity in terms of the feature set.\nSpace does not permit a full description of all models in this category, but see also Smith et al. (2015) and Narasimhan and Barzilay (2015).\nDespite its relative lack of features, the ParallelHierarchical model improves upon the featureengineered state of the art for MCTest by a small amount (about 1% absolute) as detailed in Section 5."}, {"heading": "3.2 Neural models", "text": "Neural models have, to date, performed relatively poorly on MCTest. This is because the dataset is sparse and complex.\nYin et al. (2016) investigated deep-learning approaches concurrently with the present work. They measured the performance of the Attentive Reader (Hermann et al., 2015) and the Neural Reasoner (Peng et al., 2015), both deep, end-to-end recurrent models with attention mechanisms, and also developed an attention-based convolutional network, the HABCNN. Their network operates on a hierarchy similar to our own, providing further evidence of the promise of hierarchical perspectives. Specifically, the HABCNN processes text at the sentence level and the snippet level, where the latter combines adjacent sentences (as we do through an n-gram input). Embedding vectors for the question and the answer candidates are combined and encoded by a convolutional network. This encoding modulates attention over sentence and snippet encodings, followed by maxpooling to determine the best matches between question, answer, and text. As in the present work, matching scores are given by cosine similarity. The HABCNN also makes use of a question classifier.\nDespite the shared concepts between the HABCNN and our approach, the ParallelHierarchical model performs significantly better on MCTest (more than 15% absolute) as detailed in Section 5. Other neural models tested in Yin et al. (2016) fare even worse."}, {"heading": "4 The Parallel-Hierarchical Model", "text": "Let us now define our machine comprehension model in full. We first describe each of the perspectives separately, then describe how they are combined. Below, we use subscripts to index elements of sequences, like word vectors, and superscripts to indicate whether elements come from the text, question, or answer. In particular, we use the subscripts k,m, n, p to index sequences from the text, question, answer, and hypothesis, respectively, and superscripts t, q, a, h. We depict the model schematically in Figure 1."}, {"heading": "4.1 Semantic Perspective", "text": "The semantic perspective is similar to the Memory Networks approach for embedding inputs into memory space (Weston et al., 2014). Each sen-\ntence of the text is a sequence of d-dimensional word vectors: tj = {tk}, tk \u2208 Rd. The semantic vector st is computed by embedding the word vectors into a D-dimensional space using a two-layer network that implements weighted sum followed by an affine tranformation and a nonlinearity; i.e.,\nst = f ( At \u2211 k \u03c9ktk + btA ) . (1)\nThe matrix At \u2208 RD\u00d7d, the bias vector btA \u2208 RD, and for f we use the leaky ReLU function. The scalar \u03c9k is a trainable weight associated to each word in the vocabulary. These scalar weights implement a kind of exogenous or bottomup attention that depends only on the input stimulus (Mayer et al., 2004). They can, for example, learn to perform the function of stopword lists in a soft, trainable way, to nullify the contribution of unimportant filler words.\nThe semantic representation of a hypothesis is formed analogously, except that we combine the question word vectors qm and answer word vectors an as a single sequence {hp} = {qm, an}. For semantic vector sh of the hypothesis, we use a unique transformation matrix Ah \u2208 RD\u00d7d and bias vector bhA \u2208 RD.\nThese transformations map a text sentence and a hypothesis into a common space where they can be compared. We compute the semantic match be-\ntween text sentence and hypothesis using the cosine similarity,\nM sem = cos(st, sh). (2)"}, {"heading": "4.2 Word-by-Word Perspective", "text": "The first step in building the word-by-word perspective is to transform word vectors from a text sentence, question, and answer through respective neural functions. For the text, t\u0303k = f ( Bttk + btB ) , where Bt \u2208 RD\u00d7d, btB \u2208 RD and f is again the leaky ReLU. We transform the question and the answer to q\u0303m and a\u0303n analogously using distinct matrices and bias vectors. In contrast with the semantic perspective, we keep the question and answer candidates separate in the wordby-word perspective. This is because matches to answer words are inherently more important than matches to question words, and we want our model to learn to use this property."}, {"heading": "4.2.1 Sentential", "text": "Inspired by the work of Wang and Jiang (2015) in paraphrase detection, we compute matches between hypotheses and text sentences at the word level. This computation uses the cosine similarity as before:\ncqkm = cos(\u0303tk, q\u0303m), (3) cakn = cos(\u0303tk, a\u0303n). (4)\nThe word-by-word match between a text sentence and question is determined by taking the maximum over k (finding the text word that best matches each question word) and then taking a weighted mean over m (finding the average match over the full question):\nM q = 1\nZ \u2211 m \u03c9mmax k cqkm. (5)\nHere, \u03c9m is the word weight for the question word andZ normalizes these weights to sum to one over the question. We define the match between a sentence and answer candidate, Ma, analogously. Finally, we combine the matches to question and answer according to\nMword = \u03b11M q + \u03b12M a + \u03b13M qMa. (6)\nHere the \u03b1 are trainable parameters that control the relative importance of the terms."}, {"heading": "4.2.2 Sequential Sliding Window", "text": "The sequential sliding window is related to the original MCTest baseline by Richardson et al. (2013). Our sliding window decays from its focus word according to a Gaussian distribution, which we extend by assigning a trainable weight to each location in the window. This modification enables the window to use information about the distance between word matches; the original baseline used distance information through a predefined function.\nThe sliding window scans over the words of the text as one continuous sequence, without sentence breaks. Each window is treated like a sentence in the previous subsection, but we include a location-based weight \u03bb(k). This weight is based on a word\u2019s position in the window, which, given a window, depends on its global position k. The cosine similarity is adapted as\nsqkm = \u03bb(k) cos(\u0303tk, q\u0303m), (7)\nfor the question and analogously for the answer. We initialize the location weights with a Gaussian and fine-tune them during training. The final matching score, denoted as M sws, is computed as in (5) and (6) with sqkm replacing c q km."}, {"heading": "4.2.3 Dependency Sliding Window", "text": "The dependency sliding window operates identically to the linear sliding window, but on a different view of the text passage. The output of this component is M swd and is formed analogously to M sws.\nThe dependency perspective uses the Stanford Dependency Parser (Chen and Manning, 2014) as an auxiliary tool. Thus, the dependency graph can be considered a fixed feature. Moreover, linearization of the dependency graph, because it relies on an eigendecomposition, is not differentiable. However, we handle the linearization in data preprocessing so that the model sees only reordered word-vector inputs.\nSpecifically, we run the Stanford Dependency Parser on each text sentence to build a dependency graph. This graph has nw vertices, one for each word in the sentence. From the dependency graph we form the Laplacian matrix L \u2208 Rnw\u00d7nw and determine its eigenvectors. The second eigenvector u2 of the Laplacian is known as the Fiedler\nvector. It is the solution to the minimization\nminimize g N\u2211 i,j=1 \u03b7ij(g(vi)\u2212 g(vj))2, (8)\nwhere vi are the vertices of the graph, and \u03b7ij is the weight of the edge from vertex i to vertex j (Golub and Van Loan, 2012). The Fiedler vector maps a weighted graph onto a line such that connected nodes stay close, modulated by the connection weights.1 This enables us to reorder the words of a sentence based on their proximity in the dependency graph. The reordering of the words is given by the ordered index set\nI = arg sort(u2). (9)\nTo give an example of how this works, consider the following sentence from MCTest and its dependency-based reordering:\nJenny, Mrs. Mustard \u2019s helper, called the police. the police, called Jenny helper, Mrs. \u2019s Mustard.\nSliding-window-based matching on the original sentence will answer the question Who called the police? with Mrs. Mustard. The dependency reordering enables the window to determine the correct answer, Jenny."}, {"heading": "4.3 Combining Distributed Evidence", "text": "It is important in comprehension to synthesize information found throughout a document. MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone, but would instead require some form of inference or limited reasoning (Richardson et al., 2013). It therefore includes questions where the evidence for an answer spans several sentences.\nTo perform synthesis, our model also takes in ngrams of sentences, i.e., sentence pairs and triples strung together. The model treats these exactly as it does single sentences, applying all functions detailed above. A later pooling operation combines scores across all n-grams (including the singlesentence input). This is described in the next subsection.\n1We experimented with assigning unique edge weights to unique relation types in the dependency graph. However, this had negligible effect. We hypothesize that this is because dependency graphs are trees, without cycles.\nWith n-grams, the model can combine information distributed across contiguous sentences. In some cases, however, the required evidence is spread across distant sentences. To give our model some capacity to deal with this scenario, we take the top N sentences as scored by all the preceding functions, and then repeat the scoring computations viewing these top N as a single sentence.\nThe reasoning behind these approaches can be explained well in a probabilistic setting. If we consider our similarity scores to model the likelihood of a text sentence given a hypothesis, p(tj |hi), then the n-gram and top N approaches model a joint probability p(tj1 , tj2 , . . . , tjk |hi). We cannot model the joint probability as a product of individual terms (score values) because distributed pieces of evidence are likely not independent."}, {"heading": "4.4 Combining Perspectives", "text": "We use a multilayer perceptron to combine M sem, Mword, M swd, and M sws as a final matching score Mi for each answer candidate. This network also pools and combines the separate n-gram scores, and uses a linear activation function.\nOur overall training objective is to minimize the ranking loss\nL(T, q, A) = max(0, \u00b5+max i Mi 6=i\u2217 \u2212Mi\u2217),\n(10) where \u00b5 is a constant margin, i\u2217 indexes the correct answer, and we take the maximum over i so that we are ranking the correct answer over the best-ranked incorrect answer (of which there are three). This approach worked better than comparing the correct answer to the incorrect answers individually as in Wang and McAllester (2015).\nOur implementation of the Parallel-Hierarchical model, using the Keras framework, is available on Github.2"}, {"heading": "4.5 Training Wheels", "text": "Before training, we initialized the neural-network components of our model to perform sensible heuristic functions. Training did not converge on the small MCTest without this vital approach.\nEmpirically, we found that we could achieve above 50% accuracy on MCTest using a simple sum of word vectors followed by a dot product between the question sum and the hypothesis sum.\n2http://www.hiddenwebsite.com\nTherefore, we initialized the network for the semantic perspective to perform this sum, by initializing Ax as the identity matrix and bxA as the zero vector, x \u2208 {t, h}. Recall that the activation function is aReLU so that positive outputs are unchanged.\nWe also found basic word-matching scores to be helpful, so we initialized the word-by-word networks likewise. The network for perspectivecombination was initialized to perform a sum of individual scores, using a zero bias-vector and a weight matrix of ones, since we found that each perspective contributed positively to the overall result.\nThis training wheels approach is related to other techniques from the literature. For instance, Le et al. (2015) proposed the identity-matrix initialization in the context of recurrent neural networks in order to preserve the error signal through backpropagation. In residual networks (He et al., 2015), shortcut connections bypass certain layers in the network so that a simpler function can be trained in conjunction with the full model."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 The Dataset", "text": "MCTest is a collection of 660 elementary-level children\u2019s stories and associated questions, written by human subjects. The stories are fictional, ensuring that the answer must be found in the text itself, and carefully limited to what a young child can understand (Richardson et al., 2013).\nThe more challenging variant consists of 500 stories with four multiple-choice questions each. Despite the elementary level, stories and questions are more natural and more complex than those found in synthetic MC datasets like bAbI (Weston et al., 2014) and CNN (Hermann et al., 2015).\nMCTest is challenging because it is both complicated and small. As per Hill et al. (2015), \u201cit is very difficult to train statistical models only on MCTest.\u201d Its size limits the number of parameters that can be trained, and prevents learning any complex language modeling simultaneously with the capacity to answer questions."}, {"heading": "5.2 Training and Model Details", "text": "In this section we describe important details of the training procedure and model setup. For a complete list of hyperparameter settings, our stopword\nlist, and other minutiae, we refer interested readers to our Github repository.\nFor word vectors we use Google\u2019s publicly available embeddings, trained with word2vec on the 100-billion-word News corpus (Mikolov et al., 2013). These vectors are kept fixed throughout training, since we found that training them was not helpful (likely because of MCTest\u2019s size). The vectors are 300-dimensional (d = 300).\nWe do not use a stopword list for the text passage, instead relying on the trainable word weights to ascribe global importance ratings to words. These weights are initialized with the inverse document frequency (IDF) statistic computed over the MCTest corpus.3 However, we do use a short stopword list for questions. This list nullifies query words such as {Who, what, when, where, how}, along with conjugations of the verbs to do and to be.\nFollowing earlier methods, we use a heuristic to improve performance on negation questions (Sachan et al., 2015; Wang and McAllester, 2015). When a question contains the words which and not, we negate the hypothesis ranking scores so that the minimum becomes the maximum.\nThe most important technique for training the model was the training wheels approach. Without this, training was not effective at all. The identity initialization requires that the network weight matrices are square (d = D).\nWe found dropout (Srivastava et al., 2014) to be particularly effective at improving generalization from the training to the test set, and used 0.5 as the dropout probability. Dropout occurs after all neural-network transformations, if those transformations are allowed to change with training. Our best performing model held networks at the wordby-word level fixed.\nFor combining distributed evidence, we used up to trigrams over sentences and our bestperforming model reiterated over the top two sentences (N = 2).\nWe used the Adam optimizer with the standard settings (Kingma and Ba, 2014) and a learning rate of 0.003. To determine the best hyperparameters we performed a grid search over 150 settings based on validation-set accuracy. MCTest\u2019s original validation set is too small for reliable hyperparameter tuning, so, following Wang and\n3We override the IDF initialization for words like not, which are frequent but highly informative.\nMcAllester (2015), we merged the training and validation sets of MCTest-160 and MCTest-500, then split them randomly into a 250-story training set and a 200-story validation set."}, {"heading": "5.3 Results", "text": "Table 1 presents the performance of featureengineered and neural methods on the MCTest test set. Accuracy scores are divided among questions whose evidence lies in a single sentence (single) and across multiple sentences (multi), and among the two variants. Clearly, MCTest-160 is easier.\nThe first three rows represent featureengineered methods. Richardson et al. (2013) + RTE is the best-performing variant of the original baseline published along with MCTest. It uses a lexical sliding window and distance-based measure, augmented with rules for recognizing textual entailment. We described the methods of Sachan et al. (2015) and Wang and McAllester (2015) in Section 3. On MCTest-500, the Parallel Hierarchical model significantly outperforms these methods on single questions (> 2%) and slightly outperforms the latter two on multi questions (\u2248 0.3%) and overall (\u2248 1%). The method of Wang and McAllester (2015) achieves the best overall result on MCTest-160. We suspect this is because our neural method suffered from the relative lack of training data.\nThe last four rows in Table 1 are neural methods that we discussed in Section 3. Performance measures are taken from Yin et al. (2016). Here we see our model outperforming the alternatives by a large margin across the board (> 15%). The Neural Reasoner and the Attentive Reader are large, deep models with hundreds of thousands of parameters, so it is unsurprising that they performed poorly on MCTest. The specificallydesigned HABCNN fared better, its convolutional architecture cutting down on the parameter count. Because there are similarities between our model and the HABCNN, we hypothesize that much of the performance difference is attributable to our training wheels methodology."}, {"heading": "6 Analysis and Discussion", "text": "We measure the contribution of each component of the model by ablating it. Results are given in Table 2. Not surprisingly, the n-gram functionality is important, contributing almost 5% accuracy improvement. Without this, the model has almost no\nmeans for synthesizing distributed evidence. The top N function contributes very little to the overall performance, suggesting that most multi questions have their evidence distributed across contiguous sentences. Ablating the sentential component made the most significant difference, reducing performance by more than 5%. Simple word-by-word matching is obviously useful on MCTest. The sequential sliding window makes a 3% contribution, highlighting the importance of word-distance measures. On the other hand, the dependency-based sliding window makes only a minor contribution. We found this surprising. It may be that linearization of the dependency graph removes too much of its information. Finally, the exogenous word weights make a significant contribution of almost 5%.\nAnalysis reveals that most of our system\u2019s test failures occur on questions about quantity (e.g., How many...? ) and temporal order (e.g., Who was invited last? ). Quantity questions make up 9.5% of our errors on the validation set, while order questions make up 10.3%. This weakness is not unexpected, since our architecture lacks any capacity for counting or tracking temporal order. Incorporating mechanisms for these forms of reasoning is a priority for future work (in contrast, the Memory Network model is quite good at temporal reasoning (Weston et al., 2014)).\nThe Parallel-Hierarchical model is simple. It does no complex language or sequence modeling. Its simplicity is a response to the limited data of\nMCTest. Nevertheless, the model achieves stateof-the-art results on the multi questions, which (putatively) require some limited reasoning. Our model is able to handle them reasonably well just by stringing important sentences together. Thus, the model imitates reasoning with a heuristic. This suggests that, to learn true reasoning abilities, MCTest is too simple a dataset\u2014and it is almost certainly too small for this goal.\nHowever, it may be that human language processing can be factored into separate processes of comprehension and reasoning. If so, the ParallelHierarchical model is a good start on the former. Indeed, if we train the method exclusively on single questions then its results become even more impressive: we can achieve a test accuracy of 79.1% on MCTest-500."}, {"heading": "7 Conclusion", "text": "We have presented the novel Parallel-Hierarchical model for machine comprehension, and evaluated it on the small but complex MCTest. Our model achieves state-of-the-art results, outperforming several feature-engineered and neural approaches.\nWorking with our model has emphasized to us the following (not necessarily novel) concepts, which we record here to promote further empirical validation.\n\u2022 Good comprehension of language is supported by hierarchical levels of understanding (Cf. Hill et al. (2015)).\n\u2022 Exogenous attention (the trainable word weights) may be broadly helpful for NLP.\n\u2022 The training wheels approach, that is, initializing neural networks to perform sensible heuristics, appears helpful for small datasets.\n\u2022 Reasoning over language is challenging, but easily simulated in some cases."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Factoid question answering over unstructured and structured web content", "author": ["Cucerzan", "Agichtein2005] Silviu Cucerzan", "Eugene Agichtein"], "venue": "In TREC,", "citeRegEx": "Cucerzan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cucerzan et al\\.", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2015] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations. arXiv preprint arXiv:1511.02301", "author": ["Hill et al.2015] Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Kumar et al.2015] Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941", "author": ["Le et al.2015] Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Neural networks underlying endogenous and exogenous visual\u2013spatial orienting", "author": ["Mayer et al.2004] Andrew R Mayer", "Jill M Dorflinger", "Stephen M Rao", "Michael Seidenberg"], "venue": null, "citeRegEx": "Mayer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mayer et al\\.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Machine comprehension with discourse relations", "author": ["Narasimhan", "Barzilay2015] Karthik Narasimhan", "Regina Barzilay"], "venue": "In 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Towards neural network-based reasoning", "author": ["Peng et al.2015] Baolin Peng", "Zhengdong Lu", "Hang Li", "Kam-Fai Wong"], "venue": "arXiv preprint arXiv:1508.05508", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Christopher JC Burges", "Erin Renshaw"], "venue": "In EMNLP,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Learning answerentailing structures for machine comprehension", "author": ["Avinava Dubey", "Eric P Xing", "Matthew Richardson"], "venue": "In Proceedings of ACL", "citeRegEx": "Sachan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sachan et al\\.", "year": 2015}, {"title": "A strong lexical matching method for the machine comprehension test", "author": ["Smith et al.2015] Ellery Smith", "Nicola Greco", "Matko Bosnjak", "Andreas Vlachos"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Smith et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Learning natural language inference with lstm", "author": ["Wang", "Jiang2015] Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1512.08849", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Machine comprehension with syntax, frames, and semantics", "author": ["Wang", "McAllester2015] Hai Wang"], "venue": "Volume 2: Short Papers,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Attention-based convolutional neural network for machine comprehension", "author": ["Yin et al.2016] Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "venue": "arXiv preprint arXiv:1602.04341", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Such tests are objectively gradable and can be used to assess a range of abilities, from basic understanding to causal reasoning to inference (Richardson et al., 2013).", "startOffset": 142, "endOffset": 167}, {"referenceID": 14, "context": "Given a text passage and a question about its content, a system is tested on its ability to determine the correct answer (Sachan et al., 2015).", "startOffset": 121, "endOffset": 142}, {"referenceID": 13, "context": "In this work, we focus on MCTest, a complex but data-limited comprehension benchmark, whose multiple-choice questions require not only extraction but also inference and limited reasoning (Richardson et al., 2013).", "startOffset": 187, "endOffset": 212}, {"referenceID": 10, "context": "Words are represented throughout by embedding vectors (Mikolov et al., 2013).", "startOffset": 54, "endOffset": 76}, {"referenceID": 5, "context": "We also use a sliding window acting on a subsentential scale (inspired by the work of Hill et al. (2015)), which implicitly considers the linear distance between matched words.", "startOffset": 86, "endOffset": 105}, {"referenceID": 17, "context": "Computational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Hermann et al., 2015; Kumar et al., 2015).", "startOffset": 143, "endOffset": 250}, {"referenceID": 5, "context": "Computational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Hermann et al., 2015; Kumar et al., 2015).", "startOffset": 143, "endOffset": 250}, {"referenceID": 4, "context": "Computational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Hermann et al., 2015; Kumar et al., 2015).", "startOffset": 143, "endOffset": 250}, {"referenceID": 7, "context": "Computational models that comprehend (insofar as they perform well on MC datasets) have developed contemporaneously in several research groups (Weston et al., 2014; Sukhbaatar et al., 2015; Hill et al., 2015; Hermann et al., 2015; Kumar et al., 2015).", "startOffset": 143, "endOffset": 250}, {"referenceID": 4, "context": ", 2015; Hermann et al., 2015; Kumar et al., 2015). Models designed specifically for MCTest include those of Richardson et al. (2013), and more recently Sachan et al.", "startOffset": 8, "endOffset": 133}, {"referenceID": 4, "context": ", 2015; Hermann et al., 2015; Kumar et al., 2015). Models designed specifically for MCTest include those of Richardson et al. (2013), and more recently Sachan et al. (2015), Wang and McAllester (2015), and Yin et al.", "startOffset": 8, "endOffset": 173}, {"referenceID": 4, "context": ", 2015; Hermann et al., 2015; Kumar et al., 2015). Models designed specifically for MCTest include those of Richardson et al. (2013), and more recently Sachan et al. (2015), Wang and McAllester (2015), and Yin et al.", "startOffset": 8, "endOffset": 201}, {"referenceID": 4, "context": ", 2015; Hermann et al., 2015; Kumar et al., 2015). Models designed specifically for MCTest include those of Richardson et al. (2013), and more recently Sachan et al. (2015), Wang and McAllester (2015), and Yin et al. (2016). In experiments, our Parallel-Hierarchical model achieves state-of-the-art accuracy on MCTest, outperforming these existing methods.", "startOffset": 8, "endOffset": 224}, {"referenceID": 14, "context": ", to yes, no, or any noun phrase in the text) (Sachan et al., 2015).", "startOffset": 46, "endOffset": 67}, {"referenceID": 14, "context": "In this section we borrow from Sachan et al. (2015), who laid out the MC problem nicely.", "startOffset": 31, "endOffset": 52}, {"referenceID": 14, "context": "As in Sachan et al. (2015), we combine an answer and a question into a hypothesis, hi =", "startOffset": 6, "endOffset": 27}, {"referenceID": 4, "context": "(2014) and Hermann et al. (2015).", "startOffset": 11, "endOffset": 33}, {"referenceID": 0, "context": "The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015).", "startOffset": 161, "endOffset": 246}, {"referenceID": 5, "context": "The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015).", "startOffset": 161, "endOffset": 246}, {"referenceID": 4, "context": "The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015).", "startOffset": 161, "endOffset": 246}, {"referenceID": 0, "context": "The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation (Bahdanau et al., 2014; Weston et al., 2014; Hill et al., 2015; Hermann et al., 2015). The model uses event and entity coreference links across sentences along with a host of other features. These include specifically trained word vectors for synonymy; antonymy and class-inclusion relations from external database sources; dependencies and semantic role labels. The model is trained using a latent structural SVM extended to a multitask setting, so that questions are first classified using a pretrained top-level classifier. This enables the system to use different processing strategies for different question categories. The model also combines question and answer into a well-formed statement using the rules of Cucerzan and Agichtein (2005).", "startOffset": 162, "endOffset": 908}, {"referenceID": 14, "context": "Our model is simpler than that of Sachan et al. (2015) in terms of the features it takes in, the training procedure (stochastic gradient descent vs.", "startOffset": 34, "endOffset": 55}, {"referenceID": 13, "context": "Wang and McAllester (2015) augmented the baseline feature set from Richardson et al. (2013) with features for syntax, frame semantics, coreference chains, and word embeddings.", "startOffset": 67, "endOffset": 92}, {"referenceID": 13, "context": "Wang and McAllester (2015) augmented the baseline feature set from Richardson et al. (2013) with features for syntax, frame semantics, coreference chains, and word embeddings. They combined features using a linear latent-variable classifier trained to minimize a max-margin loss function. As in Sachan et al. (2015), questions and answers are combined using a set of manually written rules.", "startOffset": 67, "endOffset": 316}, {"referenceID": 13, "context": "Wang and McAllester (2015) augmented the baseline feature set from Richardson et al. (2013) with features for syntax, frame semantics, coreference chains, and word embeddings. They combined features using a linear latent-variable classifier trained to minimize a max-margin loss function. As in Sachan et al. (2015), questions and answers are combined using a set of manually written rules. The method of Wang and McAllester (2015) achieved the previous state of the art, but has significant complexity in terms of the feature set.", "startOffset": 67, "endOffset": 432}, {"referenceID": 15, "context": "Space does not permit a full description of all models in this category, but see also Smith et al. (2015) and Narasimhan and Barzilay (2015).", "startOffset": 86, "endOffset": 106}, {"referenceID": 15, "context": "Space does not permit a full description of all models in this category, but see also Smith et al. (2015) and Narasimhan and Barzilay (2015).", "startOffset": 86, "endOffset": 141}, {"referenceID": 4, "context": "They measured the performance of the Attentive Reader (Hermann et al., 2015) and the Neural Reasoner (Peng et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 12, "context": ", 2015) and the Neural Reasoner (Peng et al., 2015), both deep, end-to-end recurrent models with attention mechanisms, and also developed an attention-based convolutional network, the HABCNN.", "startOffset": 32, "endOffset": 51}, {"referenceID": 20, "context": "Other neural models tested in Yin et al. (2016) fare even worse.", "startOffset": 30, "endOffset": 48}, {"referenceID": 9, "context": "These scalar weights implement a kind of exogenous or bottomup attention that depends only on the input stimulus (Mayer et al., 2004).", "startOffset": 113, "endOffset": 133}, {"referenceID": 13, "context": "The sequential sliding window is related to the original MCTest baseline by Richardson et al. (2013). Our sliding window decays from its focus word according to a Gaussian distribution, which we extend by assigning a trainable weight to each location in the window.", "startOffset": 76, "endOffset": 101}, {"referenceID": 13, "context": "MCTest was explicitly designed to ensure that it could not be solved by lexical techniques alone, but would instead require some form of inference or limited reasoning (Richardson et al., 2013).", "startOffset": 168, "endOffset": 193}, {"referenceID": 3, "context": "In residual networks (He et al., 2015), shortcut connections bypass certain layers in the network so that a simpler function can be trained in conjunction with the full model.", "startOffset": 21, "endOffset": 38}, {"referenceID": 7, "context": "For instance, Le et al. (2015) proposed the identity-matrix initialization in the context of recurrent neural networks in order to preserve the error signal through backpropagation.", "startOffset": 14, "endOffset": 31}, {"referenceID": 13, "context": "The stories are fictional, ensuring that the answer must be found in the text itself, and carefully limited to what a young child can understand (Richardson et al., 2013).", "startOffset": 145, "endOffset": 170}, {"referenceID": 4, "context": ", 2014) and CNN (Hermann et al., 2015).", "startOffset": 16, "endOffset": 38}, {"referenceID": 5, "context": "As per Hill et al. (2015), \u201cit is very difficult to train statistical models only on MCTest.", "startOffset": 7, "endOffset": 26}, {"referenceID": 10, "context": "For word vectors we use Google\u2019s publicly available embeddings, trained with word2vec on the 100-billion-word News corpus (Mikolov et al., 2013).", "startOffset": 122, "endOffset": 144}, {"referenceID": 14, "context": "Following earlier methods, we use a heuristic to improve performance on negation questions (Sachan et al., 2015; Wang and McAllester, 2015).", "startOffset": 91, "endOffset": 139}, {"referenceID": 16, "context": "We found dropout (Srivastava et al., 2014) to be particularly effective at improving generalization from the training to the test set, and used 0.", "startOffset": 17, "endOffset": 42}, {"referenceID": 13, "context": "Richardson et al. (2013) + RTE is the best-performing variant of the original baseline published along with MCTest.", "startOffset": 0, "endOffset": 25}, {"referenceID": 13, "context": "Richardson et al. (2013) + RTE is the best-performing variant of the original baseline published along with MCTest. It uses a lexical sliding window and distance-based measure, augmented with rules for recognizing textual entailment. We described the methods of Sachan et al. (2015) and Wang and McAllester (2015) in Section 3.", "startOffset": 0, "endOffset": 283}, {"referenceID": 13, "context": "Richardson et al. (2013) + RTE is the best-performing variant of the original baseline published along with MCTest. It uses a lexical sliding window and distance-based measure, augmented with rules for recognizing textual entailment. We described the methods of Sachan et al. (2015) and Wang and McAllester (2015) in Section 3.", "startOffset": 0, "endOffset": 314}, {"referenceID": 13, "context": "Richardson et al. (2013) + RTE is the best-performing variant of the original baseline published along with MCTest. It uses a lexical sliding window and distance-based measure, augmented with rules for recognizing textual entailment. We described the methods of Sachan et al. (2015) and Wang and McAllester (2015) in Section 3. On MCTest-500, the Parallel Hierarchical model significantly outperforms these methods on single questions (> 2%) and slightly outperforms the latter two on multi questions (\u2248 0.3%) and overall (\u2248 1%). The method of Wang and McAllester (2015) achieves the best overall result on MCTest-160.", "startOffset": 0, "endOffset": 571}, {"referenceID": 20, "context": "Performance measures are taken from Yin et al. (2016). Here we see our model outperforming the alternatives by a large margin across the board (> 15%).", "startOffset": 36, "endOffset": 54}, {"referenceID": 5, "context": "Hill et al. (2015)).", "startOffset": 0, "endOffset": 19}], "year": 2016, "abstractText": "Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging MCTest benchmark. Partly because of its limited size, prior work on MCTest has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for MCTest, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15% absolute).", "creator": "LaTeX with hyperref package"}}}