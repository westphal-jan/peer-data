{"id": "1603.09054", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs", "abstract": "In this paper, we claim that vector cosine, which is generally considered among the most efficient unsupervised measures for identifying word similarity in Vector Space Models, can be outperformed by an unsupervised measure that calculates the extent of the intersection among the most mutually dependent contexts of the target words. To prove it, we describe and evaluate APSyn, a variant of the Average Precision that, without any optimization, outperforms the vector cosine and the co-occurrence on the standard ESL test set, with an improvement ranging between +9.00% and +17.98%, depending on the number of chosen top contexts.", "histories": [["v1", "Wed, 30 Mar 2016 07:05:45 GMT  (790kb)", "http://arxiv.org/abs/1603.09054v1", "in AAAI 2016. arXiv admin note: substantial text overlap witharXiv:1603.08701"]], "COMMENTS": "in AAAI 2016. arXiv admin note: substantial text overlap witharXiv:1603.08701", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["enrico santus", "tin-shing chiu", "qin lu", "alessandro lenci", "chu-ren huang"], "accepted": true, "id": "1603.09054"}, "pdf": {"name": "1603.09054.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs", "authors": ["Enrico Santus", "Tin-Shing Chiu", "Qin Lu", "Alessandro Lenci", "Chu-Ren Huang"], "emails": ["esantus@gmail.com,", "cstschiu@comp.polyu.edu.hk,{qin.lu,", "churen.huang}@polyu.edu.hk", "alessandro.lenci@ling.unipi.it"], "sections": [{"heading": "Introduction and Related Work", "text": "Word similarity detection plays an important role in Natural Language Processing (NLP), as it is the backbone of several applications, such as paraphrasing, query expansion, word sense disambiguation, automatic thesauri creation, and so on (Terra and Clarke, 2003). Several approaches have been proposed to measure word similarity (Jarmasz and Szpakowicz, 2003; Levy et al., 2015). Some of them are lexicon-based, while others are corpora-based. The latter generally rely on the distributional hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954). Although all of them extract statistics from large corpora, they vary in the way they define what has to be considered context (i.e. lexemes, syntax, etc.) and in which way such context is used (Santus et al., 2014a; Hearst, 1992). A common way to represent word meaning in NLP is by using vectors to encode the Strength of Association (SoA) between the target word and its contexts. In the resulting Vector Space Model (VSM), vector cosine is then general-\nCopyright \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nly used to calculate word similarity by measuring the distance between these vectors (Turney and Pantel, 2010). A well-known problem with the statistical approaches is that they rely on a very loose definition of similarity. Indeed, according to the distributional hypothesis, similarity does not only include synonyms, but also other semantic relations, such as hypernymy, co-hyponymy and even antonymy (Santus et al., 2014b-c). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures (Santus et al., 2015). One of the most used is the English as a Second Language dataset (ESL), introduced in Turney (2001). It consists of 50 multiple-choice synonym questions, with 4 choices each. In this paper, we describe and evaluate APSyn, a completely unsupervised measure that calculates the extent of the intersection among the N most related contexts of two target words, weighting such intersection according to the rank of the contexts in a mutual dependency ranked list. In our experiments, APSyn outperforms the cosine, with an improvement ranging between +9.00% and +17.98% in the ESL test set, depending on the chosen N."}, {"heading": "Method and Evaluation", "text": "The vector cosine, described in the following equation (where \ud835\udc53!\" is the i-th feature in the vector x), calculates word similarity by looking at the normalized correlation between the contexts distribution of two words, \ud835\udc64! and \ud835\udc64!:\ncos \ud835\udc64!,\ud835\udc64! = \ud835\udc53!!\u00d7!!!! \ud835\udc53!!\n\ud835\udc53!! !\u00d7 \ud835\udc53!! !\nOur hypothesis is that similar words share more mutually dependent contexts than less similar ones. A way to test it is: i) measuring the intersection among the N most relat-\ned contexts of two target words, and ii) weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. For every target word, in fact, we rank all contexts according to the Local Mutual Information values (LMI; Evert, 2005) and pick the top N:\n\ud835\udc34\ud835\udc43\ud835\udc46\ud835\udc66\ud835\udc5b(\ud835\udc64!,\ud835\udc64!) = 1\n(\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58! \ud835\udc53! + \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58! \ud835\udc53! )/2 !\u2208!(!!)\u2229!(!!)\nThat is, for every feature \ud835\udc53 included in the intersection between the top N features of \ud835\udc64!, \ud835\udc41(\ud835\udc39!), and \ud835\udc64!, \ud835\udc41(\ud835\udc39!), APSyn will add 1 divided by the average rank of the feature, among the top LMI ranked features of \ud835\udc64!, \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58! \ud835\udc53 , and \ud835\udc64!, \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58! \ud835\udc53 ."}, {"heading": "Evaluation", "text": "VSM. We use a window-based VSM recording word cooccurrences within the 5 nearest content words to the left and right of each target. Co-occurrences are extracted from a combination of ukWaC and WaCkypedia corpora (around 2.7 billion words) and weighted with LMI. TEST SET. For evaluation, we use the ESL dataset, introduced in Turney (2001) as a way of evaluating algorithms for measuring the degree of word similarity. The test set consists of 50 multiple-choice synonym questions, with 4 choices. Each question was turned into four pairs, having as first word the problem word and as second word one of the possible choices. Some words have been lemmatized, in order to have a correspondent form in the VSM. TASK. We have assigned APSyn scores to all the pairs, and then sorted them in a decreasing order. We considered positive every problem word having the correct answer on top, negative the others. 5 out of 50 questions were excluded, because the correct answers were not present in the VSM. 6 out of 45 questions had one wrong choice missing. In this case, only 0.75 was added if the answer was correct. RESULTS. In Table 1, we report the results in the test. Measure N Full + 75% + Correct APSyn 100 24 3 26.25/45 = 58.33% APSyn 1000 22 3 24.25/45 = 53.89% Cosine --- 20 3 22.25/45 = 49.44% Co-occ. --- 14 4 17.00/45 = 37.78%\nTable 1. Correct answers in the ESL test set for APSyn (N=1000, 100), vector cosine and co-occurrence."}, {"heading": "Discussion and Conclusions", "text": "In this paper, we have described APSyn, a measure based on the calculation of the shared top ranked features, and its performance on the ESL questions. The measure outperforms the vector cosine and the co-occurrence, plus several\nlexicon-based and hybrid models1. Even though the performance needs to be improved by optimizing the model (i.e. learning the value of N from a training set), our experiments show that the intersection among the N most related contexts of the target words is in fact an index of similarity. It is relevant to mention here also the role of N: the larger the amount of considered contexts, the lower the ability of identifying similarity. This also confirms our hypothesis that similar words share a significant number of top mutually dependent contexts, but such intersection becomes less significant when not only the top contexts are considered. Since ESL is small, the reported results should be further investigated on a larger dataset (Santus et al., 2015).2"}], "references": [{"title": "The Statistics of Word Cooccurrences: Word Pairs and Collocations", "author": ["S. Evert"], "venue": "Dissertation, University of Stuttgart.", "citeRegEx": "Evert,? 2005", "shortCiteRegEx": "Evert", "year": 2005}, {"title": "Distributional structure", "author": ["Z. Harris"], "venue": "Word, Vol. 10 (23). 146162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Automatic Acquisition of Hyponyms from Large Text Corpora", "author": ["M.A. Hearst"], "venue": "Proceedings of the 14th International Conference on Computational Linguistics. 539-545.", "citeRegEx": "Hearst,? 1992", "shortCiteRegEx": "Hearst", "year": 1992}, {"title": "Roget\u2019s thesaurus and semantic similarity, Proceedings of RANLP 2003", "author": ["M. Jarmasz", "S. Szpakowicz"], "venue": "212-219.", "citeRegEx": "Jarmasz and Szpakowicz,? 2003", "shortCiteRegEx": "Jarmasz and Szpakowicz", "year": 2003}, {"title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "author": ["O. Levy", "Y. Goldberg", "Dagan I."], "venue": "TACL 2015.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Chasing Hypernyms in Vector Spaces with Entropy", "author": ["E. Santus", "A. Lenci", "Q. Lu", "Schulte im Walde", "S.."], "venue": "Proceedings of EACL 2014, 2:38\u201342.", "citeRegEx": "Santus et al\\.,? 2014a", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Unsupervised Antonym-Synonym Discrimination in Vector Space", "author": ["E. Santus", "Q. Lu", "A. Lenci", "Huang", "C-R."], "venue": "Proceedings of CLIC-IT 2014.", "citeRegEx": "Santus et al\\.,? 2014b", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Taking Antonymy Mask off in Vector Space", "author": ["E. Santus", "Q. Lu", "A. Lenci", "Huang", "C-R."], "venue": "Proceedings of PACLIC 2014.", "citeRegEx": "Santus et al\\.,? 2014c", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models", "author": ["E. Santus", "F. Yung", "A. Lenci", "Huang", "C-R"], "venue": "Proceedings of ACLIJCNLP", "citeRegEx": "Santus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2015}, {"title": "Frequency estimates for statistical word similarity measures", "author": ["E. Terra", "C.L.A. Clarke"], "venue": "Proceedings of HLT/NAACL 2003. 244\u2013251.", "citeRegEx": "Terra and Clarke,? 2003", "shortCiteRegEx": "Terra and Clarke", "year": 2003}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Articial Intelligence Research, Vol. 37. 141-188.", "citeRegEx": "Turney and Pantel,? 2010", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Mining the Web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["P.D. Turney"], "venue": "Proceedings of ECML-2001. 491-502.", "citeRegEx": "Turney,? 2001", "shortCiteRegEx": "Turney", "year": 2001}], "referenceMentions": [{"referenceID": 9, "context": "Word similarity detection plays an important role in Natural Language Processing (NLP), as it is the backbone of several applications, such as paraphrasing, query expansion, word sense disambiguation, automatic thesauri creation, and so on (Terra and Clarke, 2003).", "startOffset": 240, "endOffset": 264}, {"referenceID": 3, "context": "Several approaches have been proposed to measure word similarity (Jarmasz and Szpakowicz, 2003; Levy et al., 2015).", "startOffset": 65, "endOffset": 114}, {"referenceID": 4, "context": "Several approaches have been proposed to measure word similarity (Jarmasz and Szpakowicz, 2003; Levy et al., 2015).", "startOffset": 65, "endOffset": 114}, {"referenceID": 1, "context": "The latter generally rely on the distributional hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954).", "startOffset": 143, "endOffset": 157}, {"referenceID": 5, "context": ") and in which way such context is used (Santus et al., 2014a; Hearst, 1992).", "startOffset": 40, "endOffset": 76}, {"referenceID": 2, "context": ") and in which way such context is used (Santus et al., 2014a; Hearst, 1992).", "startOffset": 40, "endOffset": 76}, {"referenceID": 10, "context": "ly used to calculate word similarity by measuring the distance between these vectors (Turney and Pantel, 2010).", "startOffset": 85, "endOffset": 110}, {"referenceID": 8, "context": "For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures (Santus et al., 2015).", "startOffset": 117, "endOffset": 138}, {"referenceID": 5, "context": "Indeed, according to the distributional hypothesis, similarity does not only include synonyms, but also other semantic relations, such as hypernymy, co-hyponymy and even antonymy (Santus et al., 2014b-c). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures (Santus et al., 2015). One of the most used is the English as a Second Language dataset (ESL), introduced in Turney (2001). It consists of 50 multiple-choice synonym questions, with 4 choices each.", "startOffset": 180, "endOffset": 445}, {"referenceID": 0, "context": "For every target word, in fact, we rank all contexts according to the Local Mutual Information values (LMI; Evert, 2005) and pick the top N:", "startOffset": 102, "endOffset": 120}, {"referenceID": 11, "context": "For evaluation, we use the ESL dataset, introduced in Turney (2001) as a way of evaluating algorithms for measuring the degree of word similarity.", "startOffset": 54, "endOffset": 68}, {"referenceID": 8, "context": "Since ESL is small, the reported results should be further investigated on a larger dataset (Santus et al., 2015).", "startOffset": 92, "endOffset": 113}], "year": 2015, "abstractText": "In this paper, we claim that vector cosine \u2013 which is generally considered among the most efficient unsupervised measures for identifying word similarity in Vector Space Models \u2013 can be outperformed by an unsupervised measure that calculates the extent of the intersection among the most mutually dependent contexts of the target words. To prove it, we describe and evaluate APSyn, a variant of the Average Precision that, without any optimization, outperforms the vector cosine and the co-occurrence on the standard ESL test set, with an improvement ranging between +9.00% and +17.98%, depending on the number of chosen top contexts. Introduction and Related Work Word similarity detection plays an important role in Natural Language Processing (NLP), as it is the backbone of several applications, such as paraphrasing, query expansion, word sense disambiguation, automatic thesauri creation, and so on (Terra and Clarke, 2003). Several approaches have been proposed to measure word similarity (Jarmasz and Szpakowicz, 2003; Levy et al., 2015). Some of them are lexicon-based, while others are corpora-based. The latter generally rely on the distributional hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954). Although all of them extract statistics from large corpora, they vary in the way they define what has to be considered context (i.e. lexemes, syntax, etc.) and in which way such context is used (Santus et al., 2014a; Hearst, 1992). A common way to represent word meaning in NLP is by using vectors to encode the Strength of Association (SoA) between the target word and its contexts. In the resulting Vector Space Model (VSM), vector cosine is then generalCopyright \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. ly used to calculate word similarity by measuring the distance between these vectors (Turney and Pantel, 2010). A well-known problem with the statistical approaches is that they rely on a very loose definition of similarity. Indeed, according to the distributional hypothesis, similarity does not only include synonyms, but also other semantic relations, such as hypernymy, co-hyponymy and even antonymy (Santus et al., 2014b-c). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures (Santus et al., 2015). One of the most used is the English as a Second Language dataset (ESL), introduced in Turney (2001). It consists of 50 multiple-choice synonym questions, with 4 choices each. In this paper, we describe and evaluate APSyn, a completely unsupervised measure that calculates the extent of the intersection among the N most related contexts of two target words, weighting such intersection according to the rank of the contexts in a mutual dependency ranked list. In our experiments, APSyn outperforms the cosine, with an improvement ranging between +9.00% and +17.98% in the ESL test set, depending on the chosen N. Method and Evaluation The vector cosine, described in the following equation (where f!\" is the i-th feature in the vector x), calculates word similarity by looking at the normalized correlation between the contexts distribution of two words, w! and w!: cos w!,w! = f!!\u00d7 !!! f!! f!! !\u00d7 f!! ! Our hypothesis is that similar words share more mutually dependent contexts than less similar ones. A way to test it is: i) measuring the intersection among the N most related contexts of two target words, and ii) weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. For every target word, in fact, we rank all contexts according to the Local Mutual Information values (LMI; Evert, 2005) and pick the top N: APSyn(w!,w!) = 1 (rank! f! + rank! f! )/2 !\u2208!(!!)\u2229!(!!) That is, for every feature f included in the intersection between the top N features of w!, N(F!), and w!, N(F!), APSyn will add 1 divided by the average rank of the feature, among the top LMI ranked features of w!, rank! f , and w!, rank! f .", "creator": "Word"}}}