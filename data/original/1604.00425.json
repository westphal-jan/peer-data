{"id": "1604.00425", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "abstract": "Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.", "histories": [["v1", "Fri, 1 Apr 2016 22:18:51 GMT  (225kb,D)", "http://arxiv.org/abs/1604.00425v1", null], ["v2", "Wed, 8 Jun 2016 03:14:08 GMT  (127kb,D)", "http://arxiv.org/abs/1604.00425v2", "To appear at ACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shyam upadhyay", "manaal faruqui", "chris dyer", "dan roth"], "accepted": true, "id": "1604.00425"}, "pdf": {"name": "1604.00425.pdf", "metadata": {"source": "CRF", "title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "authors": ["Shyam Upadhyay", "Manaal Faruqui", "Chris Dyer", "Dan Roth"], "emails": ["upadhya3@illinois.edu,", "mfaruqui@cs.cmu.edu", "cdyer@cs.cmu.edu,", "danr@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "Learning word vector representations using monolingual distributional information is now a ubiquitous technique in NLP. The quality of these word vectors can be significantly improved by incorporating cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Vulic\u0301 and Moens, 2013b; Mikolov et al., 2013b; Faruqui and Dyer, 2014; Hermann and Blunsom, 2014; Lauly et al., 2014, inter alia), with improvements observed both on mono-lingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and crosslingual tasks (Guo et al., 2015; S\u00f8gaard et al., 2015; Guo et al., 2016).\nSeveral models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision \u2013 some can use document-level alignments (Vulic\u0301 and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al.,\n2015) or word (Faruqui and Dyer, 2014; Gouws and S\u00f8gaard, 2015) level, while some require both sentence and word alignments (Luong et al., 2015). However, a systematic and extensive comparison of these models is missing from the literature, making it difficult to analyse which approach is suitable for a particular NLP task. In this paper, we fill this void by empirically comparing four cross-lingual word embedding models each of which require different form of alignment(s) as supervision, across several dimensions. To this end, we train these models on four different language pairs, and evaluate them on both mono-lingual and cross-lingual tasks. We will release our comparison setup and trained embeddings upon publication.\nFirst, we show that different models can be viewed as instances of a more general framework for inducing bilingual word embeddings. Then, we evaluate these models on both extrinsic and intrinsic tasks. Our intrinsic evaluation assesses the quality of the vectors on mono-lingual (\u00a74.2) and cross-lingual (\u00a74.3) word similarity tasks, while our extrinsic evaluation spans semantic (crosslingual document classification (\u00a74.4)) and syntactic (cross-lingual dependency parsing (\u00a74.5)) tasks.\nOur experiments show that word vectors trained using expensive cross-lingual supervision (word alignments or sentence alignments) perform the best on semantic tasks. On the other hand, for syntactic tasks like cross-lingual dependency parsing, models requiring weaker form of cross-lingual supervision (such as context agnostic translation dictionary) are competitive to models requiring expensive supervision. We also analyze the embeddings qualitatively with the aim of revealing latent regularities in the vector space which explain why some models do better than others.\nar X\niv :1\n60 4.\n00 42\n5v 1\n[ cs\n.C L\n] 1\nA pr\n2 01\n6\nAlgorithm 1 General Algorithm 1: Initialize W\u2190W0,V\u2190 V0 2: (W\u2217,V\u2217)\u2190 argmin\u03b1A(W) + \u03b2B(V) +C(W,V)\nFigure 1: (Above) A general schema for induction of crosslingual word vector representations, when provided with bilingual supervision for a language pair. The word vector model generates embeddings which incorporates distributional information cross-lingually. (Below) A general algorithm for inducing bilingual word embeddings, where \u03b1, \u03b2,W0,V0 are parameters and A,B,C are suitably defined losses."}, {"heading": "2 Bilingual Embeddings", "text": "A general schema for inducing bilingual embeddings is shown in Figure 1. Our comparison focuses on dense, fixed-length distributed embeddings which are obtained using some form of cross-lingual supervision.1 We briefly describe the embedding induction procedure for each of the selected bilingual word vector models, with the aim to provide a unified algorithmic perspective for all methods, to facilitate better understanding and comparison. Our choice of models spans different forms of supervision required for inducing the embeddings, illustrated in Figure 2.\nNotation. Let W = {w1, w2, . . . , w|W |} be the vocabulary of a language l1 with |W | words, and W \u2208 R|W |\u00d7l be the corresponding word embeddings of length l. Let V = {v1, v2, . . . , v|V |} be the vocabulary of another language l2 with |V | words, and V \u2208 R|V |\u00d7m the corresponding word embeddings of lengthm. We sometimes abuse notation and denote a word and its vector with boldface v (or w)."}, {"heading": "2.1 Bilingual Skip-Gram Model (BiSkip)", "text": "Luong et al. (2015) proposed Bilingual SkipGram, a simple extension of the mono-lingual skip-gram model, which learns bilingual embeddings by using a parallel corpus along with word\n1We compare different cross-lingual word embeddings, which are not to be confused with a collection of monolingual word embeddings trained for different languages individually (Al-Rfou et al., 2013).\nalignments (both sentence and word level alignments).\nThe learning objective is a simple extension of the skip-gram model, where the context of a word is expanded to include bilingual links obtained from word alignments, so that the model is trained to predict words cross-lingually. In particular, given a word alignment link from word v \u2208 V in language l1 to w \u2208 W in language l2, the model predicts the context words of w using v and vice-versa. Formally, the cross lingual part of the objective is,\nD12(W,V) = \u2212 \u2211\n(v,w)\u2208Q \u2211 wc\u2208NBR2(w) logP (wc | v)\n(1) Where NBR2(w) is the context of w in language l2, Q is the set of word alignments, and P (wc | v) \u221d exp(wTc v). Another similar term D21 models the objective for v and NBR1(v). The objective can be cast into Algorithm 1 as,\nC(W,V) = D12(W,V) +D21(W,V) (2) A(W) = \u2212 \u2211 w\u2208W \u2211 wc\u2208NBR2(w) logP (wc | w) (3) B(V) = \u2212\n\u2211 v\u2208V \u2211 vc\u2208NBR1(v) logP (vc | v) (4)\nWhere A(W) and B(V) are the familiar skipgram formulation of the mono-lingual part of the objective."}, {"heading": "2.2 Bilingual Compositional Model (BiCVM)", "text": "Hermann and Blunsom (2014) present a method that learns bilingual word vectors from a sentence aligned corpus. Their model leverages the fact that aligned sentences have equivalent meaning, thus their sentence representations should be similar.\nWe denote two aligned sentences, ~v = \u3008x1, . . . , \u3009 and ~w = \u3008y1, . . .\u3009 , where xi \u2208 V, yi \u2208 W, are vectors corresponding to the words in the sentences. Let functions f : ~v \u2192 Rn and g : ~w \u2192 Rn, map sentences to their semantic representations in Rn. BiCVM generates word vectors by minimizing the squared `2 norm between the sentence representations of aligned sentences. In order to prevent the degeneracy arising from directly minimizing the `2 norm, they use a noise-contrastive large-margin update, with randomly drawn sentence pairs (~v, ~wn) as negative samples. The loss for the sentence pairs (~v, ~w) and (~v, ~wn) can be written as,\nE(~v, ~w, ~wn) = max (\u03b4 + \u2206E(~v, ~w, ~wn), 0) (5)\nwhere,\nE(~v, ~w) = \u2016f(~v)\u2212 g(~w)\u20162 (6)\nand,\n\u2206E(~v, ~w, ~wn) = E(~v, ~w)\u2212 E(~v, ~wn) (7)\nThis can be cast into Algorithm 1 by, C(W,V) = \u2211\naligned (~v,~w) random ~wn\nE(~v, ~w, ~wn) (8)\nA(W) = \u2016W\u20162 B(V) = \u2016V\u20162 (9)\nwith A(W) and B(V) being regularizers, with \u03b1 = \u03b2."}, {"heading": "2.3 Bilingual Correlation Based Embeddings (BiCCA)", "text": "The BiCCA model, proposed by Faruqui and Dyer (2014), showed that when (independently trained) mono-lingual vector matrices W,V are projected using CCA (Hotelling, 1936) to respect a translation lexicon, their performance improves on word similarity and word analogy tasks. They first construct W\u2032 \u2286 W,V\u2032 \u2286 V such that |W\u2032|= |V\u2032| and the corresponding words (wi, vi) in the matrices are translations of each other. The projection is then computed as:\nPW ,PV = CCA(W \u2032,V\u2032) (10)\nW\u2217 = WPW V \u2217 = VPV (11)\nwhere, PV \u2208 Rl\u00d7d,PW \u2208 Rm\u00d7d are the projection matrices with d = min(l,m) and the V\u2217 \u2208 R|V |\u00d7d,W\u2217 \u2208 R|W |\u00d7d are the word vectors that have been \u201cenriched\u201d using bilingual knowledge.\nThe BiCCA objective can be viewed2 as the following instantiation of Algorithm 1\nW0 = W\u2032,V0 = V\u2032 (12) 2Canonical correlation versus distance, Section\n6.5 (Hardoon et al., 2004)\nC(W,V) = \u2016W \u2212V\u20162+\u03b3 ( VTW ) (13)\nA(W) = \u2016W\u20162\u22121 B(V) = \u2016V\u20162\u22121 (14)\nwhere W = W0PW and V = V0PV , where we set \u03b1 = \u03b2 = \u03b3 =\u221e to set hard constraints."}, {"heading": "2.4 Bilingual Vectors from Comparable Data (BiVCD)", "text": "Another approach of inducing bilingual word vectors, which we refer to as BiVCD, was proposed by Vulic\u0301 and Moens (2015). Their approach is designed to use comparable corpus between the source and target language pair to induce crosslingual vectors.\nLet de and df denote a pair of comparable documents with length p and q respectively (assume p > q). BiVCD first merges these two comparable documents into a single pseudo-bilingual document using a deterministic strategy based on length ratio of two documents R = bpq c. Every Rth word of the merged pseudo-bilingual document is picked sequentially from df . Finally, a skip-gram model is trained on the corpus of pseudo-bilingual documents, to generate vectors for all words in W\u2217 \u222a V\u2217. The vectors constituting W\u2217 and V\u2217 can then be easily identified.\nInstantiation of BiVCD in the general algorithm is obvious: C(W,V) assumes the familiar word2vec skip-gram objective over the pseudobilingual document,\nC(W,V) = \u2212 \u2211\ns\u2208W\u222aV \u2211 t\u2208NBR(s) logP (t | s)\n(15) where NBR(s) is defined by the pseudo-bilingual document and P (t | s) \u221d exp(tT s).\nAlthough BiVCD is designed to use comparable corpus, we provide it with parallel data in our experiments (to ensure comparability), and treat two aligned sentences as comparable."}, {"heading": "3 Data", "text": "We train cross-lingual embeddings for 4 language pairs: English-German (en-de), English-French (en-fr), English-Swedish (en-sv) and EnglishChinese (en-zh). For en-de and en-sv we use the Europarl v7 parallel corpus3 (Koehn, 2005). For en-fr, we use Europarl combined with the newscommentary and UN-corpus dataset from WMT 2015.4 For en-zh, we use the FBIS parallel corpus from the news domain (LDC2003E14). We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh parallel corpus. Corpus statistics for all languages is shown in Table 1."}, {"heading": "4 Evaluation", "text": "We measure the quality of induced cross-lingual word embeddings in terms of their performance, when used as features in the following tasks:\n\u2022 Mono-lingual word similarity for English\n\u2022 Cross-lingual dictionary induction\n\u2022 Cross-lingual document classification\n\u2022 Cross-lingual syntactic dependency parsing\nThe first two tasks intrinsically measure how much mono-lingual and cross-lingual similarity benefit from cross-lingual training. The last two tasks measure the ability of bilingually trained vectors to extrinsically facilitate model transfer across languages, for semantic and syntactic applications respectively. These tasks have been used in previous works (Klementiev et al., 2012; Luong et al., 2015; Vulic\u0301 and Moens, 2013a; Guo et al., 2015) for evaluating bilingual embeddings, but no comparison exists which uses them in conjunction.\nTo ensure fair comparison, all models are trained with embeddings of size 200. We provide\n3www.statmt.org/europarl/v7/{de,_sv}-_ en.tgz\n4www.statmt.org/wmt15/translation-_ task.html\nall models with parallel corpora, irrespective of their requirements. Whenever possible, we also report statistical significance of our results."}, {"heading": "4.1 Parameter Selection", "text": "We follow the BestAvg parameter selection strategy from Lu et al. (2015). We selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks.\nBiSkip. All models were trained using a window size of 10 (tuned over {5, 10, 20}), and 30 negative samples (tuned over {10, 20, 30}). The cross-lingual weight was set to 4 (tuned over {1, 2, 4, 8}). The word alignments for training the model (available at github.com/lmthang/ bivec) were generated using cdec (Dyer et al., 2010). The number of training iterations was set to 5 (no tuning).\nBiCVM. We use the tool (available at github. com/karlmoritz/bicvm) released by Hermann and Blunsom (2014) to train all embeddings. We train an additive model (that is, f(~x) = g(~x) = \u2211 i xi) with hinge loss margin set to 200 (no tuning), batch size of 50 (tuned over 50, 100, 1000) and noise parameter of 10 (tuned over {10, 20, 30}). All models are trained for 100 iterations (no tuning).\nBiCCA. First, mono-lingual word vectors are trained using the skip-gram model5 with negative sampling (Mikolov et al., 2013a) with window of size 5 (tuned over {5, 10, 20}). To generate a bilingual dictionary, word alignments are generated using cdec from the parallel corpus. Then, word pairs (a, b), a \u2208 l1, b \u2208 l2 are selected such that a is aligned to b the most number of times and vice versa. This way, we obtained dictionaries of approximately 36k, 35k, 30k and 28k word pairs for en-de, en-fr, en-sv and en-zh respectively.\nThe mono-lingual vectors are aligned using the above dictionaries with the tool (available at github.com/mfaruqui/eacl14-_cca) released by Faruqui and Dyer (2014) to generate the bilingual word embeddings. We use k = 0.5 as the number of canonical components (tuned over {0.2, 0.3, 0.5, 1.0}). Note that this results in a embedding of size 100 after performing CCA.\nBiVCD. We use word2vec\u2019s skip gram model for training our embeddings, with a window size\n5code.google.com/p/word2vec\nof 5 (tuned on {5, 10, 20, 30}) and negative sampling parameter set to 5 (tuned on {5, 10, 25}). Every pair of parallel sentences is treated as a pair of comparable documents, and merging is performed using the sentence length ratio strategy described earlier. We implemented the code for performing the merging as we could not find a tool provided by the authors."}, {"heading": "4.2 Mono-lingual Evaluation", "text": "We first evaluate if the inclusion of cross-lingual knowledge using bilingual models improves the quality of English embeddings.\nWord Similarity. Word similarity datasets contain word pairs which are assigned similarity ratings by humans. The task evaluates how well the notion of word similarity according to humans is emulated in the vector space. Evaluation is based on the Spearman\u2019s rank correlation coefficient (Myers and Well, 1995) between human rankings and rankings produced by computing cosine similarity between the vectors of two words.\nWe use SimLex dataset for English (Hill et al., 2014) which contains 999 pairs of English words, with a balanced set of noun, adjective and verb pairs. SimLex is claimed to capture word similarity exclusively instead of WordSim-353 (Finkelstein et al., 2001) which captures both word similairty and relatedness. We declare significant improvement if p < 0.1 according to Steiger\u2019s method (Steiger, 1980) for calculating the statistical significant differences between two dependent correlation coefficients.\nTable 2 shows performance of English embeddings induced by all the models by training on different language pairs on the SimLex word similarity task. Overall, across all language pairs, BiCVM is the best performing model in terms of spearman\u2019s correlation, but its improvement over BiSkip and BiVCD is often insignificant. It is notable that 2 of the 3 top performing models, BiCVM and BiVCD, need sentence aligned and document-aligned corpus only, which are easier to obtain than parallel data with word alignments required by BiSkip.\nQVEC. Tsvetkov et al. (2015) proposed an intrinsic evaluation metric for estimating the quality of English word vectors. The score produced by QVEC measures how well a given set of word vectors is able to quantify linguistic properties of words, with higher being better. The metric is shown to have strong correlation with per-\nformance on downstream semantic applications. As it can be only used for English, we use it to evaluate the English vectors obtained using crosslingual training of different models. Table 3 shows that on average across language pairs, BiSkip achieves the best score, followed by BiVCD and BiCCA. Interestingly, BiCVM which was the best model according to SimLex, ranks last overall according to QVEC. The fact that the best models according to QVEC and word similarities are different reinforces the observation of Tsvetkov et al. (2015) that performance on word similarity tasks alone does not reflect quantification of linguistic properties of words."}, {"heading": "4.3 Cross-lingual Dictionary Induction", "text": "The task of cross-lingual dictionary induction (Vulic\u0301 and Moens, 2013a; Gouws et al., 2015; Mikolov et al., 2013b) judges how good bilingual embeddings are at detecting word pairs that are semantically similar across languages. We follow the setup of Vulic\u0301 and Moens (2013a), but instead of manually creating a gold bilingual dictionary, we derived our gold dictionaries using the Open Multilingual Wordnet data released by Bond and Foster (2013). The data includes synset alignments across 26 languages with over 90% accuracy. First, we prune out words from each synset whose frequency count is less than 1000 in the vocabulary of the training data from \u00a73. Then, for\neach pair of aligned synsets s1 = {k1, k2, \u00b7 \u00b7 \u00b7} s2 = {g1, g2, \u00b7 \u00b7 \u00b7}, we include all elements from the set {(k, g) | k \u2208 s1, g \u2208 s2} into the gold dictionary, where k and g are the lemmas. Using this approach we generated dictionaries of sizes 1.5k, 1.4k, 1.0k and 1.6k pairs for en-fr, en-de, en-sv and en-zh respectively.6\nWe report top-10 accuracy, which is the fraction of the entries (e, f) in the gold dictionary, for which f belongs to the list of top-10 neighbours of the word vector of e, according to the induced bilingual embeddings. From the results (Table 4), it can be seen that for dictionary induction, the performance improves with the quality of supervision. As we move from cheaply supervised methods (eg. BiVCD) to more expensive supervision (eg. BiSkip), the accuracy improves. This suggests that for cross lingual similarity tasks, the more expensive the cross-lingual knowledge available, the better. Models using weak supervision like BiVCD perform poorly in comparison to models like BiSkip and BiCVM, with performance gaps upwards of 10 pts on an average."}, {"heading": "4.4 Cross-lingual Document Classification", "text": "We follow the cross-lingual document classification (CLDC) setup of Klementiev et al. (2012), but extend it to cover all of our language pairs. We use the RCV2 Reuters multilingual corpus7 for our experiments. In this task, for a language pair (l1, l2), a document classifier is trained using the document representations derived from word embeddings in language l1, and then the trained model is tested on documents from language l2 (and vice-versa). By using supervised training data in one language and evaluating without further supervision in another, CLDC assesses whether the learned multilingual representations are semantically coherent across multiple languages.\n6We will release these dictionaries. 7http://trec.nist.gov/data/reuters/\nreuters.html\nAll embeddings are learned on the data described in \u00a73, and we only use the RCV2 data to learn document classification models. Following previous work, we compute document representation by taking the tf-idf8 weighted average of vectors of the words present in it. A multi-class classifier is trained using an averaged perceptron (Freund and Schapire, 1999) for 10 iterations, using the document vectors of language l1 as features9. Majority baselines for en \u2192 l2 and l1 \u2192 en are 49.7% and 46.7% respectively, for all languages. Table 5 shows the performance of different models across different language pairs. We computed confidence values using the McNemar test (McNemar, 1947) and declare significant improvement if p < 0.05.\nFrom Table 5, it is evident that in almost all the cases, BiSkip performs significantly better than the remaining models. For transferring semantic knowledge across languages via embeddings, sentence and word level alignment proves superior to sentence or word level alignment alone. This observation is consistent with the trend in crosslingual dictionary induction, where too the most expensive form of supervision performed the best."}, {"heading": "4.5 Cross-lingual Dependency Parsing", "text": "The use of cross lingual similarity measures for direct-transfer of dependency parsers was first shown in Ta\u0308ckstro\u0308m et al. (2012). The idea behind direct-transfer is to train a dependency parsing model using embeddings for language l1 and\n8tf-idf was computed using all documents for that language in RCV2\n9We use the implementation of Klementiev et al. (2012)\nthen test the trained model on language l2, replacing embeddings for language l1 with those of l2. The transfer relies on coherence of the embeddings across languages arising from the cross lingual training. For our experiments, we use the cross lingual transfer setup of Guo et al. (2015).10 Their framework trains a transition-based dependency parser using nonlinear activation function, with the source-side embeddings as lexical features. These embeddings can be replaced by target-side embeddings at test time.\nAll models are trained for 5000 iterations with fixed word embeddings during training. Since our goal is to determine the utility of word embeddings in dependency parsing, we turn off other features that can capture distributional information like brown clusters, which were originally used in Guo et al. (2015). We use the universal dependency treebank (McDonald et al., 2013) version2.0 for our evaluation. For Chinese, we use the treebank released as part of the CoNLL-X shared task (Buchholz and Marsi, 2006).\nWe first evaluate how useful the word embeddings are in cross-lingual model transfer of dependency parsers (Table 6). On an average, BiCCA does better than other models. BiSkip is a close second, with an average performance gap of less than 1 point. BiSkip outperforms BiCVM on German and French (over 2 point improvement), owing to word alignment information BiSkip\u2019s model uses during training. It is not surprising that English-Chinese transfer scores are low, due to the significant difference in syntactic structure of the two languages. Surprisingly, unlike the semantic tasks considered earlier, the models with expensive supervision requirements like BiSkip and BiCVM could not outperform a cheaply supervised BiCCA.\nWe also evaluate whether using bilingually trained vectors for learning dependency parsers is better than using mono-lingually trained vectors in Table 7. We compare against parsing models trained using mono-lingually trained word vectors (column marked Mono in Table 7). These vectors are the same mono-lingual used as input to the BiCCA model. All other settings remain the same. On an average across language pairs, improvement over the mono-lingual embeddings was obtained with the BiSkip and BiCCA models, while BiCVM and BiVCD consistently performed worse. A possible reason for this is that BiCVM\n10 github.com/jiangfeng1124/acl15-_ clnndep\nand BiVCD operate on sentence level contexts to learn the embeddings, which only captures the semantic meaning of the sentences and ignores the internal syntactic structure. As a result, embedding trained using BiCVM and BiVCD are not informative for syntactic tasks. On the other hand, BiSkip and BiCCA both utilize the word alignment information to train their embeddings and thus do better in capturing some notion of syntax."}, {"heading": "5 Qualitative Analysis", "text": "Figure 3 visualizes the word embeddings in a twodimensional space using the t-SNE tool (Van der Maaten and Hinton, 2008). The plot shows projected embeddings of some randomly selected words from the most frequent words in the English segment of the parallel en-fr corpus, along with their French translations. We plot the BiCVM, BiSkip, and BiVCD vectors which are 200 dimensional.11\nWe can explain qualitatively why some models do better on the cross lingual similarity task using Figure 3. BiVCD embeddings moves similar pairs like (children, enfants) and\n11BiCCA could not be included as its 100 dimensional and is incompatible for the tool.\n(war, guerre) apart, while BiCVM does the same with the pair (world,monde), but overall is better than BiVCD. BiSkip does an almost perfect crosslingual alignment, matching every word to its corresponding translation."}, {"heading": "6 Discussion and Conclusion", "text": "We presented the first systematic comparative evaluation of cross-lingual embedding methods on several downstream NLP tasks, both intrinsic and extrinsic. We provided a unified representation for all approaches, showing them as instances of a general algorithm. Our choice of methods spans a diverse range of approaches, in that each requires a different form of alignment as supervision. It will be interesting to see how these embedding methods fare on other such tasks like frame semantic parsing (Johannsen et al., 2015), super-sense tagging (Gouws and S\u00f8gaard, 2015) etc.\nThe paper did not cover all approaches that generate cross-lingual word embeddings. Some methods do not have publicly available code (Coulmance et al., 2015; Zou et al., 2013); for others, like BilBOWA (Gouws et al., 2015), we identified problems in the available code, which caused it to consistently produced results that are inferior even to mono-lingually trained vectors.12 However, the\n12We contacted the authors of the papers and were unable to resolve the issues in the toolkit.\nmodels that we included for comparison in our survey are representative of other cross-lingual models in terms of the form of cross-lingual supervision required by them. For example, BilBOWA (Gouws et al., 2015) and Bilingual Autoencoder (Lauly et al., 2014) are similar to BiCVM in this respect and Multi-view CCA (Rastogi et al., 2015) and deep CCA (Lu et al., 2015) can be viewed as extensions of BiCCA. Our choice of models was motivated to compare different forms of supervision, and therefore, adding these models, would not provide additional insight.\nOur experiments reveal several interesting trends. When evaluating on intrinsic tasks such as mono-lingual word similarity, models (such as BiVCD) relying on cheaper forms of supervision perform almost on par (no statistically significant difference) with models requiring expensive supervision. On the other hand, for cross-lingual semantic tasks, like CLDC and dictionary induction, the model with the most informative supervision performs best overall. In contrast, for the syntactic task of dependency parsing, cheaply supervised models perform slightly better. It appears that for cross-lingual transfer of syntactic knowledge, word level alignment alone can suffice."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proc. of CoNLL", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Linking and extending an open multilingual wordnet", "author": ["Bond", "Foster2013] Francis Bond", "Ryan Foster"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Bond et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bond et al\\.", "year": 2013}, {"title": "Conll-x shared task on multilingual dependency parsing", "author": ["Buchholz", "Marsi2006] Sabine Buchholz", "Erwin Marsi"], "venue": "In Proc. of CoNLL", "citeRegEx": "Buchholz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 2006}, {"title": "Trans-gram, fast cross-lingual wordembeddings", "author": ["Jean-Marc Marty", "Guillaume Wenzek", "Amine Benhalloum"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Coulmance et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Coulmance et al\\.", "year": 2015}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context", "author": ["Dyer et al.2010] Chris Dyer", "Adam Lopez", "Juri Ganitkevitch", "Jonathan Weese", "Hendra Setiawan", "Ferhan Ture", "Vladimir Eidelman", "Phil Blunsom", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "In Proc. of EACL", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Placing search in context: the concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proc. of WWW", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Freund", "Schapire1999] Yoav Freund", "Robert E Schapire"], "venue": "Machine learning,", "citeRegEx": "Freund et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1999}, {"title": "Simple task-specific bilingual word embeddings", "author": ["Gouws", "S\u00f8gaard2015] Stephan Gouws", "Anders S\u00f8gaard"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "In Proc. of ICML", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of ACL", "citeRegEx": "Guo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A representation learning framework for multi-source transfer parsing", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu"], "venue": "In Proc. of AAAI", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Multilingual Models for Compositional Distributional Semantics", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proc. of ACL", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "arXiv preprint arXiv:1408.3456", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Anylanguage frame-semantic parsing", "author": ["H\u00e9ctor Mart\u0131\u0301nez Alonso", "Anders S\u00f8gaard"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Johannsen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johannsen et al\\.", "year": 2015}, {"title": "Inducing crosslingual distributed representations of words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": "In Proc. of COLING", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In Proc. of MT summit", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas C Raykar", "Amrita Saha"], "venue": "In Proc. of NIPS", "citeRegEx": "Lauly et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lauly et al\\.", "year": 2014}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Lu et al.2015] Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of NAACL", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proc. of the Workshop on Vector Space Modeling for NLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Multiview lsa: Representation learning via generalized cca", "author": ["Benjamin Van Durme", "Raman Arora"], "venue": "In Proc. of NAACL", "citeRegEx": "Rastogi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rastogi et al\\.", "year": 2015}, {"title": "Inverted indexing for cross-lingual nlp", "author": ["\u017deljko Agi\u0107", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Barbara Plank", "Bernd Bohnet", "Anders Johannsen"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Compu-", "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2015}, {"title": "Tests for comparing elements of a correlation matrix", "author": ["James H Steiger"], "venue": "Psychological bulletin,", "citeRegEx": "Steiger.,? \\Q1980\\E", "shortCiteRegEx": "Steiger.", "year": 1980}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["R. McDonald", "J. Uszkoreit"], "venue": null, "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "A conditional random field word segmenter for sighan bakeoff", "author": ["Tseng et al.2005] Huihsin Tseng", "Pichuan Chang", "Galen Andrew", "Daniel Jurafsky", "Christopher Manning"], "venue": "In Proceedings of the Fourth SIGHAN Workshop on Chinese Lan-", "citeRegEx": "Tseng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proc. of EMNLP", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Cross-lingual semantic similarity of words as the similarity of their semantic word responses", "author": ["Vuli\u0107", "Moens2013a] Ivan Vuli\u0107", "MarieFrancine Moens"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2013}, {"title": "Bilingual word embeddings from non-parallel document-aligned data applied to bilingual lexicon induction", "author": ["Vuli\u0107", "Moens2015] Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": "In Proc. of ACL", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proc. of EMNLP", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": ", 2014, inter alia), with improvements observed both on mono-lingual (Faruqui and Dyer, 2014; Rastogi et al., 2015) and crosslingual tasks (Guo et al.", "startOffset": 69, "endOffset": 115}, {"referenceID": 10, "context": ", 2015) and crosslingual tasks (Guo et al., 2015; S\u00f8gaard et al., 2015; Guo et al., 2016).", "startOffset": 31, "endOffset": 89}, {"referenceID": 24, "context": ", 2015) and crosslingual tasks (Guo et al., 2015; S\u00f8gaard et al., 2015; Guo et al., 2016).", "startOffset": 31, "endOffset": 89}, {"referenceID": 11, "context": ", 2015) and crosslingual tasks (Guo et al., 2015; S\u00f8gaard et al., 2015; Guo et al., 2016).", "startOffset": 31, "endOffset": 89}, {"referenceID": 8, "context": "Several models for inducing cross-lingual embeddings have been proposed, each requiring a different form of cross-lingual supervision \u2013 some can use document-level alignments (Vuli\u0107 and Moens, 2015), others need alignments at the sentence (Hermann and Blunsom, 2014; Gouws et al., 2015) or word (Faruqui and Dyer, 2014; Gouws and S\u00f8gaard, 2015) level, while some require both sentence and word alignments (Luong et al.", "startOffset": 239, "endOffset": 286}, {"referenceID": 20, "context": ", 2015) or word (Faruqui and Dyer, 2014; Gouws and S\u00f8gaard, 2015) level, while some require both sentence and word alignments (Luong et al., 2015).", "startOffset": 126, "endOffset": 146}, {"referenceID": 0, "context": "We compare different cross-lingual word embeddings, which are not to be confused with a collection of monolingual word embeddings trained for different languages individually (Al-Rfou et al., 2013).", "startOffset": 175, "endOffset": 197}, {"referenceID": 12, "context": "5 (Hardoon et al., 2004) C(W,V) = \u2016W \u2212V\u2016+\u03b3 ( VW ) (13)", "startOffset": 2, "endOffset": 24}, {"referenceID": 17, "context": "For en-de and en-sv we use the Europarl v7 parallel corpus3 (Koehn, 2005).", "startOffset": 60, "endOffset": 73}, {"referenceID": 27, "context": "We use the Stanford Chinese Segmenter (Tseng et al., 2005) to preprocess the en-zh parallel corpus.", "startOffset": 38, "endOffset": 58}, {"referenceID": 16, "context": "These tasks have been used in previous works (Klementiev et al., 2012; Luong et al., 2015; Vuli\u0107 and Moens, 2013a; Guo et al., 2015) for evaluating bilingual embeddings, but no comparison exists which uses them in conjunction.", "startOffset": 45, "endOffset": 132}, {"referenceID": 20, "context": "These tasks have been used in previous works (Klementiev et al., 2012; Luong et al., 2015; Vuli\u0107 and Moens, 2013a; Guo et al., 2015) for evaluating bilingual embeddings, but no comparison exists which uses them in conjunction.", "startOffset": 45, "endOffset": 132}, {"referenceID": 10, "context": "These tasks have been used in previous works (Klementiev et al., 2012; Luong et al., 2015; Vuli\u0107 and Moens, 2013a; Guo et al., 2015) for evaluating bilingual embeddings, but no comparison exists which uses them in conjunction.", "startOffset": 45, "endOffset": 132}, {"referenceID": 19, "context": "We follow the BestAvg parameter selection strategy from Lu et al. (2015). We selected the parameters for all models by tuning on a set of values (described below) and picking the parameter setting which did best on an average across all tasks.", "startOffset": 56, "endOffset": 73}, {"referenceID": 4, "context": "com/lmthang/ bivec) were generated using cdec (Dyer et al., 2010).", "startOffset": 46, "endOffset": 65}, {"referenceID": 14, "context": "We use SimLex dataset for English (Hill et al., 2014) which contains 999 pairs of English words, with a balanced set of noun, adjective and verb pairs.", "startOffset": 34, "endOffset": 53}, {"referenceID": 6, "context": "SimLex is claimed to capture word similarity exclusively instead of WordSim-353 (Finkelstein et al., 2001) which captures both word similairty and relatedness.", "startOffset": 80, "endOffset": 106}, {"referenceID": 25, "context": "1 according to Steiger\u2019s method (Steiger, 1980) for calculating the statistical significant differences between two dependent correlation coefficients.", "startOffset": 32, "endOffset": 47}, {"referenceID": 28, "context": "Tsvetkov et al. (2015) proposed an intrinsic evaluation metric for estimating the quality of English word vectors.", "startOffset": 0, "endOffset": 23}, {"referenceID": 28, "context": "ferent reinforces the observation of Tsvetkov et al. (2015) that performance on word similarity tasks alone does not reflect quantification of linguistic properties of words.", "startOffset": 37, "endOffset": 60}, {"referenceID": 8, "context": "The task of cross-lingual dictionary induction (Vuli\u0107 and Moens, 2013a; Gouws et al., 2015; Mikolov et al., 2013b) judges how good bilingual embeddings are at detecting word pairs that are semantically similar across languages.", "startOffset": 47, "endOffset": 114}, {"referenceID": 8, "context": "The task of cross-lingual dictionary induction (Vuli\u0107 and Moens, 2013a; Gouws et al., 2015; Mikolov et al., 2013b) judges how good bilingual embeddings are at detecting word pairs that are semantically similar across languages. We follow the setup of Vuli\u0107 and Moens (2013a), but instead of manually creating a gold bilingual dictionary, we derived our gold dictionaries using the Open Multilingual Wordnet data released by Bond and Foster (2013).", "startOffset": 72, "endOffset": 275}, {"referenceID": 8, "context": "The task of cross-lingual dictionary induction (Vuli\u0107 and Moens, 2013a; Gouws et al., 2015; Mikolov et al., 2013b) judges how good bilingual embeddings are at detecting word pairs that are semantically similar across languages. We follow the setup of Vuli\u0107 and Moens (2013a), but instead of manually creating a gold bilingual dictionary, we derived our gold dictionaries using the Open Multilingual Wordnet data released by Bond and Foster (2013). The data includes synset alignments across 26 languages with over 90% accuracy.", "startOffset": 72, "endOffset": 447}, {"referenceID": 16, "context": "We follow the cross-lingual document classification (CLDC) setup of Klementiev et al. (2012), but extend it to cover all of our language pairs.", "startOffset": 68, "endOffset": 93}, {"referenceID": 26, "context": "The use of cross lingual similarity measures for direct-transfer of dependency parsers was first shown in T\u00e4ckstr\u00f6m et al. (2012). The idea behind direct-transfer is to train a dependency parsing model using embeddings for language l1 and", "startOffset": 106, "endOffset": 130}, {"referenceID": 16, "context": "tf-idf was computed using all documents for that language in RCV2 We use the implementation of Klementiev et al. (2012)", "startOffset": 95, "endOffset": 120}, {"referenceID": 10, "context": "For our experiments, we use the cross lingual transfer setup of Guo et al. (2015).10 Their framework trains a transition-based dependency parser using nonlinear activation function, with the source-side embeddings as lexical features.", "startOffset": 64, "endOffset": 82}, {"referenceID": 10, "context": "Since our goal is to determine the utility of word embeddings in dependency parsing, we turn off other features that can capture distributional information like brown clusters, which were originally used in Guo et al. (2015). We use the universal dependency treebank (McDonald et al.", "startOffset": 207, "endOffset": 225}, {"referenceID": 15, "context": "It will be interesting to see how these embedding methods fare on other such tasks like frame semantic parsing (Johannsen et al., 2015), super-sense tagging (Gouws and S\u00f8gaard, 2015) etc.", "startOffset": 111, "endOffset": 135}, {"referenceID": 3, "context": "Some methods do not have publicly available code (Coulmance et al., 2015; Zou et al., 2013); for others, like BilBOWA (Gouws et al.", "startOffset": 49, "endOffset": 91}, {"referenceID": 32, "context": "Some methods do not have publicly available code (Coulmance et al., 2015; Zou et al., 2013); for others, like BilBOWA (Gouws et al.", "startOffset": 49, "endOffset": 91}, {"referenceID": 8, "context": ", 2013); for others, like BilBOWA (Gouws et al., 2015), we identified problems in the available code, which caused it to consistently produced results that are inferior even to mono-lingually trained vectors.", "startOffset": 34, "endOffset": 54}, {"referenceID": 8, "context": "For example, BilBOWA (Gouws et al., 2015) and Bilingual Autoencoder (Lauly et al.", "startOffset": 21, "endOffset": 41}, {"referenceID": 18, "context": ", 2015) and Bilingual Autoencoder (Lauly et al., 2014) are similar to BiCVM in this respect and Multi-view CCA (Rastogi et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 23, "context": ", 2014) are similar to BiCVM in this respect and Multi-view CCA (Rastogi et al., 2015) and deep CCA (Lu et al.", "startOffset": 64, "endOffset": 86}, {"referenceID": 19, "context": ", 2015) and deep CCA (Lu et al., 2015) can be viewed as extensions of BiCCA.", "startOffset": 21, "endOffset": 38}], "year": 2017, "abstractText": "Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.", "creator": "LaTeX with hyperref package"}}}