{"id": "1604.00503", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Discriminative Phrase Embedding for Paraphrase Identification", "abstract": "This work, concerning paraphrase identification task, on one hand contributes to expanding deep learning embeddings to include continuous and discontinuous linguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN to learn the discriminative weights of words and phrases specific to paraphrase task, so that a weighted sum of embeddings can represent sentences more effectively. Based on these two innovations we get competitive state-of-the-art performance on paraphrase identification.", "histories": [["v1", "Sat, 2 Apr 2016 13:57:02 GMT  (29kb)", "http://arxiv.org/abs/1604.00503v1", "NAACL'2015"]], "COMMENTS": "NAACL'2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "hinrich sch\\\"utze"], "accepted": true, "id": "1604.00503"}, "pdf": {"name": "1604.00503.pdf", "metadata": {"source": "CRF", "title": "Discriminative Phrase Embedding for Paraphrase Identification", "authors": ["Wenpeng Yin"], "emails": ["wenpeng@cis.lmu.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n00 50\n3v 1\n[ cs\n.C L\n] 2\nA pr"}, {"heading": "1 Introduction", "text": "This work investigates representation learning via deep learning in paraphrase identification task, which aims to determine whether two sentences have the same meaning. One main innovation of deep learning is that it learns distributed word representations (also called \u201cword embeddings\u201d) to deal with various Natural Language Processing (NLP) tasks. Our goal is to use and refine embeddings to get competitive performance.\nWe adopt a supervised classification approach to paraphrase identification like most top performing systems. Our focus is representation learning of sentences. Following prior work (e.g., Blacoe and Lapata (2012)), we compute the vector of a sentence as the sum of the vectors of its components. But unlike prior work we use single words, continuous phrases and discontinuous phrases as the components, not just single words. Our rationale is that many semantic units are formed by multiple words \u2013 e.g., the\ncontinuous phrase \u201cside effects\u201d and the discontinuous phrase \u201cpick . . . off\u201d. The better we can discover and represent such components, the better the compositional sentence vector should be. We use the term unit to refer to single words, continuous phrases and discontinuous phrases.\nJi and Eisenstein (2013) show that not all words are equally important for paraphrase identification. They propose TF-KLD, a discriminative weighting scheme to address this problem. While they do not represent sentences as vectors composed of other vectors, TF-KLD is promising for a vector-based approach as well since the insight that units are of different importance still applies. A shortcoming of TF-KLD is its failure to define weights for words that do not occur in the training set. We propose TF-KLD-KNN, an extension of TF-KLD that computes the weight of an unknown unit as the average of the weights of its k nearest neighbors. We determine nearest neighbors by cosine measure over embedding space. We then represent a sentence as the sum of the vectors of its units, weighted by TFKLD-KNN.\nWe use (Madnani et al., 2012) as our baseline system. They used simple features \u2013 eight different machine translation metrics \u2013 yet got good performance. Based on above new sentence representations, we compute three kinds of features to describe a pair of sentences \u2013 cosine similarity, element-wise sum and absolute element-wise difference \u2013 and show that combining them with the features from Madnani et al. (2012) gets state-of-the-art performance on the Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004).\nIn summary, our first contribution lies in embedding learning of continuous and discontinuous phrases. Our second contribution is the weighting scheme TF-KLD-KNN.\nThis paper is structured as follows. Section 2 reviews related work. Section 3 describes our method for learning embeddings of units. Section 4 introduces a measure of unit discriminativity that can be used for differential weighting of units. Section 5 presents experimental setup and results. Section 6 concludes."}, {"heading": "2 Related work", "text": "The key for good performance in paraphrase identification is the design of good features. We now discuss relevant prior work based on the linguistic granularity of feature learning.\nThe first line is compositional semantics, which learns representations for words and then composes them to representations of sentences. Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al., 2011)). They showed that addition over word embeddings is competitive, despite its simplicity.\nThe second category directly seeks sentence-level features. Ji and Eisenstein (2013) explored unigrams, bigrams and dependency pairs as sentence features. They proposed TF-KLD to weight features and used non-negative factorization to learn latent sentence representations. Our method TF-KLDKNN is an extension of their work.\nThe third line directly computes features for sentence pairs. Wan et al. (2006) used N-gram overlap, dependency relation overlap, dependency tree-edit distance and difference of sentence lengths. Finch et al. (2005) and Madnani et al. (2012) combined several machine translation metrics. Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given\nsentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences. Other work representative of this line is by Kozareva and Montoyo (2006), Qiu et al. (2006), Ul-Qayyum and Altaf (2012).\nOur work, first learning unit embeddings, then adding them to form sentence representations, finally calculating pair features (cosine similarity, absolute difference and MT metrics) actually is a combination of above three lines."}, {"heading": "3 Embedding learning for units", "text": "As explained in Section 1, \u201cunits\u201d in this work include single words, continuous phrases and discontinuous phrases. Phrases have a larger linguistic granularity than words and thus will in general contain more meaning aspects for a sentence. For example, successful detection of continuous phrase \u201cside effects\u201d and discontinuous phrase \u201cpick \u00b7 \u00b7 \u00b7 off\u201d is helpful to understand the sentence meaning correctly. This section focuses on how to detect phrases and how to represent them."}, {"heading": "3.1 Phrase collection", "text": "Phrases defined by a lexicon have not been investigated extensively before in deep learning. To collect canonical phrase set, we extract two-word phrases defined in Wiktionary1 and Wordnet (Miller and Fellbaum, 1998) to form a collection of size 95,218. This collection contains continuous phrases \u2013 phrases whose parts always occur next to each other (e.g., \u201cside effects\u201d) \u2013 and discontinuous phrases \u2013 phrases whose parts more often occur separated from each other (e.g., \u201cpick . . . off\u201d)."}, {"heading": "3.2 Identification of phrase continuity", "text": "Wiktionary and WordNet do not categorize phrases as continuous or discontinuous. So we need a heuristic to determine this automatically.\nFor each phrase \u201cA B\u201d, we compute [c1, c2, c3, c4, c5] where ci, 1 \u2264 i \u2264 5, indicates there are ci occurrences of A and B in that order with a distance\n1http://en.wiktionary.org\nof i. We compute these statistics for a corpus consisting of English Gigaword (Graff et al., 2003) and Wikipedia. We set the maximal distance to 5 because discontinuous phrases are rarely separated by more than 5 tokens.\nIf c1 is 10 times higher than (c2+c3+c4+c5)/4, we classify \u201cA B\u201d as continuous, otherwise as discontinuous. For example, [c1, . . . , c5] is [1121, 632, 337, 348, 4052] for \u201cpick off\u201d, so c1 is smaller than the average 1342.25 and \u201cpick off\u201d is set as \u201cdiscontinuous\u201d; [c1, . . . , c5] is [14831, 16, 177, 331, 3471] for \u201cCornell University\u201d, c1 is 10 times larger than the average and this phrase is set to \u201ccontinuous\u201d.\nWe found that that this heuristic for distinguishing between continuous and discontinuous phrases works well and leave the development of a more principled method for future work."}, {"heading": "3.3 Sentence reformatting", "text": "Sentence \u201c. . . A . . . B . . . \u201d is\n\u2022 reformatted as \u201c. . . A B . . . \u201d if A and B form a continuous phrase and no word intervenes between them and\n\u2022 reformatted as \u201c. . . A B . . . A B . . . \u201d if A and B form a discontinuous phrase and are separated by 1 to 4 words. We replace each of the two component words with A B to make the context of both constituents available to the phrase in learning.\nThis method of phrase detection will generate some false positives, e.g., if \u201cpick\u201d and \u201coff\u201d occur in a context like \u201cshe picked an island off the coast of Maine\u201d. However, our experimental results indicate that it is robust enough for our purposes.\nWe run word2vec (Mikolov et al., 2013) on the reformatted Wikipedia corpus to learn embeddings for all units. Embedding size is set to 200."}, {"heading": "4 Measure of unit discriminativity", "text": "We will represent a sentence as the sum of the embeddings of its units. Building on Ji and Eisenstein (2013)\u2019s TF-KLD, we want to weight units according to their ability to discriminate two sentences specific to the paraphrase task.\nTF-KLD assumes a training set of sentence pairs in the form \u3008ui, vi, ti\u3009, where ui and vi denote the\nbinary unit occurrence vectors for the sentences in the ith pair and ti \u2208 {0, 1} is the gold tag. Then, we define pk and qk as follows.\n\u2022 pk = P (uik|vik = 1, ti = 1). This is the probability that unit wk occurs in sentence ui given that wk occurs in its counterpart vi and they are paraphrases.\n\u2022 qk = P (uik|vik = 1, ti = 0). This is the probability that unit wk occurs in sentence ui given that wk occurs in its counterpart vi and they are not paraphrases.\nTF-KLD computes the discriminativity of unit wk as the Kullback-Leibler divergence of the Bernoulli distributions (pk, 1-pk) and (qk, 1-qk)\nTF-KLD has a serious shortcoming for unknown units. Unfortunately, the test data of the commonly used MSPR corpus in paraphrase task has about 6% unknown words and 62.5% of its sentences contain unknown words. It motivates us to design an improved scheme TF-KLD-KNN to reweight the features.\nTF-KLD-KNN weights are the same as TF-KLD weights for known units. For a unit that did not occur in training, TF-KLD-KNN computes its weight as the average of the weights of its k nearest neighbors in embedding space, where unit similarity is calculated by cosine measure.2\nWord2vec learns word embeddings based on the word context. The intuition of TF-KLD-KNN is that words with similar context have similar discriminativities. This enables us to transfer the weights of features in training data to the unknown features in test data, greatly helping to address problems of sparseness."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Data and baselines", "text": "We use the MSRP corpus (Dolan et al., 2004) for evaluation. It consists of a training set of 2753 true paraphrase pairs and 1323 false paraphrase pairs and a test set of 1147 true and 578 false pairs.\n2Unknown words without embeddings (only seven cases in our experiments) are ignored. This problem can be effectively relieved by training embedding on larger corpora.\nFor our new method, it is interesting to measure the improvement on the subset of those MSRP sentences that contain at least one phrase. In the standard MSRP corpus, 3027 training pairs (2123 true, 904 false) and 1273 test pairs (871 true, 402 false) contain phrases; we denote this subset as subset. We carry out experiments on overall (all MSRP sentences) as well as subset cases.\nWe compare six methods for paraphrase identification.\n\u2022 NOWEIGHT. Following Blacoe and Lapata (2012), we simply represent a sentence as the unweighted sum of the embeddings of all its units.\n\u2022 MT is the method proposed by Madnani et al. (2012): the sentence pair is represented as a vector of eight different machine translation metrics.\n\u2022 Ji and Eisenstein (2013). We reimplemented their \u201cinductive\u201d setup which is based on matrix factorization and is the top-performing system in paraphrasing task.3\nThe following three methods not only use this vector of eight MT metrics, but use three kinds of additional features given two sentence representations s1 and s2: cosine similarity, element-wise sum s1+s2 and element-wise absolute difference |s1 \u2212 s2|. We now describe how each of the three methods computes the sentence vectors.\n\u2022 WORD. The sentence is represented as the sum of all single-word embeddings, weighted by TF-KLD-KNN.\n\u2022 WORD+PHRASE. The sentence is represented as the sum of the embeddings of all its units (including phrases), weighted by TFKLD-KNN.\n\u2022 WORD+GOOGLE. Mikolov et al. (2013) use a data-driven method to detect statistical phrases which are mostly continuous bigrams.\n3They report even better performance in a \u201ctransductive\u201d setup that makes use of test data. We only address paraphrase identification for the case that the test data are not available for training the model in this paper.\nWe implement their system by first exploiting word2phrase4 to reformat Wikipedia, then using word2vec skip-gram model to train phrase embeddings.\nWe use the same weighting scheme TF-KLDKNN for the three weighted sum approaches: WORD, WORD+PHRASE and WORD+GOOGLE. Note however that there is an interaction between representation space and nearest neighbor search. We limit the neighbor range of unknown words for WORD to single words; in contrast, we search the space of all single words and linguistic (resp. Google) phrases for WORD+PHRASE (resp. WORD+GOOGLE).\nWe use LIBLINEAR (Fan et al., 2008) as our linear SVM implementation. 20% training data is used as development data. Parameter k is fine-tuned on development set and the best value 3 is finally used in following reported results."}, {"heading": "5.2 Experimental results", "text": "Table 1 shows performance for the six methods as well as for the majority baseline. In the overall (resp. subset) setup, WORD+PHRASE performs best and outperforms (Ji and Eisenstein, 2013) by .009 (resp. .052) on accuracy. Interestingly, Ji and Eisenstein (2013)\u2019s method obtains worse performance on subset. This can be explained by the effect of matrix factorization in their work: it works less well for smaller datasets like subset. This is a shortcoming of their approach. WORD+GOOGLE has a slightly worse performance than WORD+PHRASE; this suggests that linguistic phrases might be more effective than statistical phrases in identifying paraphrases.\nCases overall and subset both suggest that phrase embeddings improve sentence representations. The accuracy of WORD+PHRASE is lower on overall than on subset because WORD+PHRASE has no advantage over WORD for sentences without phrases."}, {"heading": "5.3 Effectiveness of TF-KLD-KNN", "text": "The key contribution of TF-KLD-KNN is that it achieves full coverage of feature weights in the face of data sparseness. We now compare four weighting methods on overall corpus and with the combi-\n4https://code.google.com/p/word2vec/\nnation of MT features: NOWEIGHT, TF-IDF, TFKLD, TF-KLD\nTable 2 suggests that task-specific reweighting approaches (including TF-KLD and TF-KLD-KNN) are superior to unspecific schemes (NOWEIGHT and TF-IDF). Also, it demonstrates the effectiveness of our weight learning solution for unknown units in paraphrase task."}, {"heading": "5.4 Reweighting schemes for unseen units", "text": "We compare our reweighting scheme KNN (i.e., TFKLD-KNN) with three other reweighting schemes. Zero: zero weight, i.e., ignore unseen units; Typeaverage: take the average of weights of all known unit types in test set; Context-average: average of the weights of the adjacent known units of the unknown unit (two, one or defaulting to Zero, depending on how many there are). Figure 1 shows that KNN performs best."}, {"heading": "6 Conclusion", "text": "This work introduced TF-KLD-KNN, a new reweighting scheme that learns the discriminativities of known as well as unknown units effectively. We further improved paraphrase identification per-\nformance by the utilization of continuous and discontinuous phrase embeddings.\nIn future, we plan to do experiments in a crossdomain setup and enhance our algorithm for domain adaptation paraphrase identification."}, {"heading": "Acknowledgments", "text": "We are grateful to members of CIS for comments on earlier versions of this paper. This work was supported by Baidu (through a Baidu scholarship awarded to Wenpeng Yin) and by Deutsche Forschungsgemeinschaft (grant DFG SCHU 2246/8-2, SPP 1335)."}], "references": [{"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Computational Linguistics, 36(4):673\u2013 721.", "citeRegEx": "Baroni and Lenci.,? 2010", "shortCiteRegEx": "Baroni and Lenci.", "year": 2010}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Paraphrase identification as probabilistic quasi-synchronous recognition", "author": ["Dipanjan Das", "Noah A Smith."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Pro-", "citeRegEx": "Das and Smith.,? 2009", "shortCiteRegEx": "Das and Smith.", "year": 2009}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Bill Dolan", "Chris Quirk", "Chris Brockett."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, pages 350\u2013356. Association for", "citeRegEx": "Dolan et al\\.,? 2004", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin."], "venue": "The Journal of Machine Learning Research, 9:1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Using machine translation evaluation techniques to determine sentence-level semantic equivalence", "author": ["Andrew Finch", "Young-Sook Hwang", "Eiichiro Sumita."], "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005), pages 17\u201324.", "citeRegEx": "Finch et al\\.,? 2005", "shortCiteRegEx": "Finch et al\\.", "year": 2005}, {"title": "English gigaword", "author": ["David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda."], "venue": "Linguistic Data Consortium, Philadelphia.", "citeRegEx": "Graff et al\\.,? 2003", "shortCiteRegEx": "Graff et al\\.", "year": 2003}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Ji and Eisenstein.,? 2013", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2013}, {"title": "Paraphrase identification on the basis of supervised machine learning techniques", "author": ["Zornitsa Kozareva", "Andr\u00e9s Montoyo."], "venue": "Advances in natural language processing, pages 524\u2013533. Springer.", "citeRegEx": "Kozareva and Montoyo.,? 2006", "shortCiteRegEx": "Kozareva and Montoyo.", "year": 2006}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["Nitin Madnani", "Joel Tetreault", "Martin Chodorow."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Madnani et al\\.,? 2012", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: An electronic lexical database", "author": ["George Miller", "Christiane Fellbaum"], "venue": null, "citeRegEx": "Miller and Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Miller and Fellbaum.", "year": 1998}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "User\u2019s guide to sigf: Significance testing by approximate randomisation", "author": ["Sebastian Pad\u00f3"], "venue": null, "citeRegEx": "Pad\u00f3,? \\Q2006\\E", "shortCiteRegEx": "Pad\u00f3", "year": 2006}, {"title": "Paraphrase recognition via dissimilarity significance classification", "author": ["Long Qiu", "Min-Yen Kan", "Tat-Seng Chua."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 18\u201326. Association for Computational Lin-", "citeRegEx": "Qiu et al\\.,? 2006", "shortCiteRegEx": "Qiu et al\\.", "year": 2006}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems, volume 24, pages 801\u2013", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Paraphrase identification using semantic heuristic features", "author": ["Zia Ul-Qayyum", "Wasif Altaf."], "venue": "Research", "citeRegEx": "Ul.Qayyum and Altaf.,? 2012", "shortCiteRegEx": "Ul.Qayyum and Altaf.", "year": 2012}, {"title": "Using dependency-based features to take the para-farce out of paraphrase", "author": ["Stephen Wan", "Mark Dras", "Robert Dale", "C\u00e9cile Paris."], "venue": "Proceedings of the Australasian Language Technology Workshop, volume 2006, pages 131\u2013138.", "citeRegEx": "Wan et al\\.,? 2006", "shortCiteRegEx": "Wan et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": ", Blacoe and Lapata (2012)), we compute the vector of a sentence as the sum of the vectors of its components.", "startOffset": 2, "endOffset": 27}, {"referenceID": 10, "context": "We use (Madnani et al., 2012) as our baseline", "startOffset": 7, "endOffset": 29}, {"referenceID": 4, "context": "(2012) gets state-of-the-art performance on the Microsoft Research Paraphrase (MSRP) corpus (Dolan et al., 2004).", "startOffset": 92, "endOffset": 112}, {"referenceID": 9, "context": "resentations, we compute three kinds of features to describe a pair of sentences \u2013 cosine similarity, element-wise sum and absolute element-wise difference \u2013 and show that combining them with the features from Madnani et al. (2012) gets state-of-the-art performance on the Microsoft Research Paraphrase (MSRP) corpus (Dolan et al.", "startOffset": 210, "endOffset": 232}, {"referenceID": 13, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al.", "startOffset": 136, "endOffset": 163}, {"referenceID": 0, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al.", "startOffset": 194, "endOffset": 218}, {"referenceID": 2, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al.", "startOffset": 238, "endOffset": 266}, {"referenceID": 16, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al., 2011)).", "startOffset": 370, "endOffset": 391}, {"referenceID": 0, "context": "Blacoe and Lapata (2012) carried out a comparative study of three word representation methods (the simple distributional semantic space (Mitchell and Lapata, 2010), distributional memory tensor (Baroni and Lenci, 2010) and word embedding (Collobert and Weston, 2008)), along with three composition methods (addition, point-wise multiplication, and recursive autoencoder (Socher et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "Ji and Eisenstein (2013) explored unigrams, bigrams and dependency pairs as sentence features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 18, "context": "Wan et al. (2006) used N-gram overlap,", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "Finch et al. (2005) and Madnani et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Finch et al. (2005) and Madnani et al. (2012) combined several machine translation metrics.", "startOffset": 0, "endOffset": 46}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences.", "startOffset": 0, "endOffset": 250}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences. Other work representative of this line is by Kozareva and Montoyo (2006), Qiu et al.", "startOffset": 0, "endOffset": 561}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences. Other work representative of this line is by Kozareva and Montoyo (2006), Qiu et al. (2006), Ul-Qayyum and Altaf (2012).", "startOffset": 0, "endOffset": 580}, {"referenceID": 3, "context": "Das and Smith (2009) presented a generative model over two sentences\u2019 dependency trees, incorporating syntax, lexical semantics, and hidden loose alignments between the trees to model generating a paraphrase of a given sentence. Socher et al. (2011) used recursive autoencoders to learn representations for words and word sequences on each layer of the sentence parsing tree, and then proposed dynamic pooling layer to form a fixed-size matrix as the representation of the two sentences. Other work representative of this line is by Kozareva and Montoyo (2006), Qiu et al. (2006), Ul-Qayyum and Altaf (2012).", "startOffset": 0, "endOffset": 608}, {"referenceID": 12, "context": "To collect canonical phrase set, we extract two-word phrases defined in Wiktionary1 and Wordnet (Miller and Fellbaum, 1998) to form a collection of size 95,218.", "startOffset": 96, "endOffset": 123}, {"referenceID": 7, "context": "We compute these statistics for a corpus consisting of English Gigaword (Graff et al., 2003) and Wikipedia.", "startOffset": 72, "endOffset": 92}, {"referenceID": 11, "context": "We run word2vec (Mikolov et al., 2013) on the reformatted Wikipedia corpus to learn embeddings for all units.", "startOffset": 16, "endOffset": 38}, {"referenceID": 8, "context": "Building on Ji and Eisenstein (2013)\u2019s TF-KLD, we want to weight units according to their ability to discriminate two sentences specific to the paraphrase task.", "startOffset": 12, "endOffset": 37}, {"referenceID": 4, "context": "We use the MSRP corpus (Dolan et al., 2004) for evaluation.", "startOffset": 23, "endOffset": 43}, {"referenceID": 1, "context": "Following Blacoe and Lapata (2012), we simply represent a sentence as the unweighted sum of the embeddings of all its units.", "startOffset": 10, "endOffset": 35}, {"referenceID": 10, "context": "\u2022 MT is the method proposed by Madnani et al. (2012): the sentence pair is represented as a vector of eight different machine translation metrics.", "startOffset": 31, "endOffset": 53}, {"referenceID": 8, "context": "\u2022 Ji and Eisenstein (2013). We reimplemented their \u201cinductive\u201d setup which is based on matrix factorization and is the top-performing system in paraphrasing task.", "startOffset": 2, "endOffset": 27}, {"referenceID": 11, "context": "Mikolov et al. (2013) use a data-driven method to detect statistical phrases which are mostly continuous bigrams.", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "We use LIBLINEAR (Fan et al., 2008) as our linear SVM implementation.", "startOffset": 17, "endOffset": 35}, {"referenceID": 8, "context": "subset) setup, WORD+PHRASE performs best and outperforms (Ji and Eisenstein, 2013) by .", "startOffset": 57, "endOffset": 82}, {"referenceID": 8, "context": "subset) setup, WORD+PHRASE performs best and outperforms (Ji and Eisenstein, 2013) by .009 (resp. .052) on accuracy. Interestingly, Ji and Eisenstein (2013)\u2019s method obtains worse performance on subset.", "startOffset": 58, "endOffset": 157}, {"referenceID": 8, "context": "839 Ji and Eisenstein (2013) .", "startOffset": 4, "endOffset": 29}, {"referenceID": 14, "context": "Significant improvements over MT are marked with \u2217 (approximate randomization test, Pad\u00f3 (2006), p < .", "startOffset": 84, "endOffset": 96}], "year": 2016, "abstractText": "This work, concerning paraphrase identification task, on one hand contributes to expanding deep learning embeddings to include continuous and discontinuous linguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN to learn the discriminative weights of words and phrases specific to paraphrase task, so that a weighted sum of embeddings can represent sentences more effectively. Based on these two innovations we get competitive state-of-the-art performance on paraphrase identification.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}