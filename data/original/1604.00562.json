{"id": "1604.00562", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Reasoning about Pragmatics with Neural Listeners and Speakers", "abstract": "We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural \"listener\" and \"speaker\" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 64% success rate using existing techniques.", "histories": [["v1", "Sat, 2 Apr 2016 21:52:03 GMT  (1172kb,D)", "http://arxiv.org/abs/1604.00562v1", null], ["v2", "Mon, 26 Sep 2016 13:48:20 GMT  (932kb,D)", "http://arxiv.org/abs/1604.00562v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jacob andreas", "dan klein"], "accepted": true, "id": "1604.00562"}, "pdf": {"name": "1604.00562.pdf", "metadata": {"source": "CRF", "title": "Reasoning About Pragmatics with Neural Listeners and Speakers", "authors": ["Jacob Andreas"], "emails": ["jda@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "We present a model for describing scenes and objects by reasoning about context and listener behavior. By incorporating standard neural modules for image retrieval and language modeling into a probabilistic framework for pragmatics, our model generates rich, contextually appropriate descriptions of structured world representations.\nFigure 1 shows a reference game RG played between a listener L and a speaker S:\n 1. Reference candidates r1 and r2 are revealed to both players. 2. S is secretly assigned a random target t \u2208 {1, 2}. 3. S produces a description d = S(t, r1, r2), which is shown to L. 4. L chooses c = L(d, r1, r2). 5. Both players win if c = t.\n(RG)\nIn order for the players to win, S\u2019s description d must be pragmatic: it must (implicitly or explicitly) encode an understanding of L\u2019s behavior. In Figure 1, for example, the owl is wearing a hat and the owl is sitting in the tree are both accurate descriptions of the target image, but only the second allows a human listener to succeed with high probability. RG can be used to elicit a broad class of such pragmatic phenomena, which ultimately concern behavior rather than truth conditions.\nExisting computational models of pragmatics can be divided into two essentially independent lines of work, which we term the direct and derived approaches.\nBroadly, direct models (see Section 2 for examples) are based on a representation of S. They learn pragmatic behavior by example. Beginning with datasets annotated for the specific task they are trying to solve (e.g. examples of humans playing RG), direct models use feature-based architectures to predict appropriate behavior without a listener representation. While quite general in principle, such models require training data annotated\nar X\niv :1\n60 4.\n00 56\n2v 1\n[ cs\n.C L\n] 2\nA pr\n2 01\n6\nspecifically with pragmatics in mind; such data is scarce in practice.\nDerived models, by contrast, are based on a representation of L. They first instantiate a base listener L0, intended to simulate the real listener L. They then form a reasoning speaker S1, which chooses a description that causes L0 to behave correctly. Existing derived models couple handwritten grammars and hand-engineered listener models with sophisticated inference procedures. They exhibit complex behavior, but are restricted to small domains where grammar engineering is practical.\nThe approach we present in this paper aims to capture the best aspects of both lines of work. Like direct approaches, we use machine learning to acquire a complete grounded generation model from data, without domain knowledge in the form of a hand-written grammar or hand-engineered listener model. But like derived approaches, we use this learning to construct a base model, and embed it within a higher-order model that reasons about listener responses. As will be seen, this reasoning step allows the model to make use of weaker supervision than previous data-driven approaches, while exhibiting robust behavior in a variety of contexts.\nOur approach is, in a sense, a straightforward derived model, but with the underlying generation and interpretation behavior learned rather than engineered. Independent of the application to RG, our model also resembles the suite of neural image captioning models that have been a popular subject of recent study (Xu et al., 2015). Nevertheless, our approach appears to be:\n\u2022 the first such captioning model to reason explicitly about listeners\n\u2022 more generally, the first neural captioning model that can generate different captions for the same target in different contexts\n\u2022 the first learned approach to pragmatics to require no pragmatic training data\nFollowing previous work, we evaluate our model on RG, though the general architecture could be applied to other tasks where pragmatics plays a core role. Using a large database of abstract scenes like the one shown in Figure 1, we run a series of games with humans in the role of L and our system in the role of S. We find that the descriptions generated by our model result in\ncorrect interpretation 17% more often than a recent learned baseline system. We use these experiments to explore various other aspects of computational pragmatics, including tradeoffs between adequacy and fluency, and between computational efficiency and expressive power.1"}, {"heading": "2 Related Work", "text": "Direct pragmatics As an example of the direct approach mentioned in the introduction, FitzGerald et al. (2013) collect a set of human-generated referring expressions about abstract representations of sets of colored blocks. Given a set of blocks to describe, their model directly learns a maximum-entropy distribution over the set of logical expressions whose denotation is the target set. Other research, focused on direct referring expression generation from a computer vision perspective, includes that of Mao et al. (2015) and Kazemzadeh et al. (2014).\nDerived pragmatics The derived approach is exemplified by the work of Smith et al. (2013). This work describes a series of nested Bayesian models, in which intelligent listeners reason about the behavior of reflexive speakers, and even higher-order speakers reason about these listeners. Experiments (Frank et al., 2009) show that this model explains human behavior well, but both computational and representational issues restrict its application to very simple reference games.\nOther work in this family includes that of Vogel et al. (2013), Golland et al. (2010), and Monroe and Potts (2015). These approaches couple template-driven language generation with gametheoretic reasoning frameworks to produce contextually appropriate language. While somewhat more expressive than the models of Smith et al. (2013), they still require both domain-specific engineering, controlled world representations, and pragmatically annotated training data.\nRepresenting language and the world In addition to the pragmatics literature, the approach proposed in this paper relies extensively on recently developed tools for multimodal processing of language and unstructured representations like images. These includes both image retrieval models, which select an image from a collection given a\n1Models, human annotations, and code to generate all tables and figures in this paper can be found at http://github.com/jacobandreas/pragma.\ntextual description (Socher et al., 2014), and neural conditional language models, which take a content representation and emit a sequence of tokens (Donahue et al., 2015).\nReasoning with neural networks A general framework for constructing task-specific neural networks and composing them to produce novel behavior was explored in the context of visual question answering by Andreas et al. (2015). The current work can be thought of as a generalization of that approach, in which the high-level model actively reasons about the output of low-level modules, rather than directly composing them. While not trained end-to-end, our approach can also be viewed as a cooperative analog of the \u201cgenerative adversarial networks\u201d used for image generation (Goodfellow et al., 2014a)."}, {"heading": "3 Approach", "text": "Our goal is to produce a model that can play the role of the speaker S in RG. Specifically, given a target referent (e.g. scene or object) r and a distractor r\u2032, the model must produce a description d that uniquely identifies r. For training, we have access to a set of non-contrastively captioned referents {(ri, di)}: each training description di is generated for its associated referent ri in isolation. There is no guarantee that di would actually serve as a good referring expression for ri in any particular context. We must thus use the training data to ground language in referent representations, but rely on reasoning to produce pragmatics.\nOur model architecture is compositional and hierarchical. We begin in Section 3.2 by describing a collection of \u201cmodules\u201d: basic computational primitives for mapping between referents, descriptions, and reference judgments, here implemented as linear operators or small neural networks. While these modules appear as substructures in neural architectures for a variety of tasks, we put them to novel use in constructing a reasoning pragmatic speaker model.\nSection 3.3 describes how to assemble two base models: a literal speaker, which maps from referents to strings, and a literal listener, which maps from strings to reference judgments. Section 3.4 describes how these base models are used to implement a top-level reasoning speaker: a learned, probabilistic, derived model of pragmatics.\nFCFC\nngram features desc ref features referent\n(a) referent encoder Er (b) description encoder Ed"}, {"heading": "3.1 Preliminaries", "text": "Formally, we\u2019ll take a description d to consist of a sequence of words d1, d2, . . . , dn, drawn from a vocabulary of known size. For encoding, we\u2019ll also assume access to a feature representation f(d) of the sentence (here a vector of indicator features on n-grams for the rest of this paper). These two views\u2014as a sequence of words di and a feature vector f(d)\u2014form the basis of module interactions with language.\nReferent representations are similarly simple. Because the model never generates referents\u2014 only conditions on them and scores them\u2014a vector-valued feature representation of referents suffices. Our approach is completely indifferent to the nature of this representation. While the experiments in this paper use a vector of indicator features on objects and actions present in abstract scenes (Figure 1), it would be easy to instead use pre-trained convolutional representations for referring to natural images. As with descriptions, we denote this feature representation f(r) for referents."}, {"heading": "3.2 Modules", "text": "All listener and speaker models are built from a kit of simple building blocks for working with multimodal representations of images and text:\n1. a referent encoder Er 2. a description encoder Ed 3. a choice ranker R 4. a referent describer D\nThese are depicted in Figure 2, and specified more formally below. All modules are parameterized by weight matrices, written with capital letters W1, W2, . . .; we refer to the collection of weights for all modules together as W .\nEncoders The referent and description encoders produce a linear embedding of referents and descriptions in a common vector space.\nReferent encoder: Er(r) =W1f(r) (1)\nDescription encoder: Ed(d) =W2f(d) (2)\nChoice ranker The choice ranker takes a string encoding and a collection of referent encodings, assigns a score to each (string, referent) pair, and then transforms these scores into a distribution over referents. We write R(ei|e\u2212i, ed) for the probability of choosing i in contrast to the alternative; for example, R(e2|e1, ed) is the probability of answering \u201c2\u201d when presented with encodings e1 and e2.\ns1 = w > 3 \u03c1(W4e1 +W5ed) s2 = w > 3 \u03c1(W4e2 +W5ed)\nR(ei|e\u2212i, ed) = esi\nes1 + es2 (3)\n(Here \u03c1 is a rectified linear activation function.)\nReferent describer The referent describer takes an image encoding and outputs a description using a (feedforward) conditional neural language model. We express this model as a distribution p(dn+1|dn, d<n, er), where dn is an indicator feature on the last description word generated, d<n is a vector of indicator features on all other words previously generated, and er is a referent embedding. This is a \u201c2-plus-skip-gram\u201d model, with local positional history features, global positionindependent history features, and features on the referent being described. To implement this probability distribution, we first use a multilayer perceptron to compute a vector of scores s (one si for each vocabulary item):\ns =W6\u03c1(W7[dn, d<n, ei])\nand then normalize them to obtain probabilities:\npi = esi\u2211 j e sj\nFinally,\np(dn+1|dn, d<n, er) = pdn+1 (4)"}, {"heading": "3.3 Base models", "text": "From these building blocks, we construct a pair of base models. The first of these is a literal listener L0, which takes a description and a set of referents, and chooses the referent most likely to be described. This serves the same purpose as the base listener in the general derived approach described in the introduction. We additionally construct a literal speaker S0, which takes a referent in isolation and outputs a description. The literal speaker is used for efficient inference over the space of possible descriptions, as described in Section 3.4. L0 is, in essence, a retrieval model, and S0 is neural captioning model.\nBoth of the base models are probabilistic: L0 produces a distribution over referent choices, and S0 produces a distribution over strings. They are depicted with gray backgrounds in Figure 3.\nLiteral listener Given a description d and a pair of candidate referents r1 and r2, the literal listener embeds both referents and passes them to the ranking module, which produces a distribution over choices i.\ned = Ed(d)\ne1 = Er(r1)\ne2 = Er(r2)\npL0(i|d, r1, r2) = R(ei|e\u2212i, ed) (5)\nThat is,\npL0(1|d, r1, r2) = R(e1|e2, ed) and vice-versa.\nThis model is trained contrastively, by solving\nthe following optimization problem:\nmax W \u2211 j log pL0(1|dj , rj , r\u2032) (6)\nHere r\u2032 is a random distractor chosen uniformly from the training set. For each training example (ri, di), this objective attempts to maximize the probability that the model chooses ri as the referent of di over a random distractor.\nThis uniform random choice is important: it ensures that our approach is applicable even when there is not a naturally-occurring source of target\u2013 distractor pairs, as previous work (Golland et al., 2010; Monroe and Potts, 2015) has required. Note that this objective can also be viewed as the contrastive loss described by Smith and Eisner (2005), where it serves as an approximation to the likelihood objective that encourages L0 to prefer ri to every other possible referent simultaneously.\nLiteral speaker As in the figure, the literal speaker is obtained by composing a referent encoder with a describer, as follows:\ne = Er(f(r))\npS0(d|r) = Dd(d|e) As with the listener, the literal speaker should be understood as producing a distribution over strings. It is trained by maximizing the conditional likelihood of captions in the training data:\nmax W \u2211 i log pS0(di|ri) (7)\nThese base models are intended to be the minimal learned equivalents of the hand-engineered speakers and hand-written grammars employed in previous derived approaches (Golland et al., 2010). The neural encoding/decoding framework implemented by the modules in the previous subsection provides a simple way to map from referents to descriptions and descriptions to judgments without worrying too much about the details of syntax or semantics. Past work amply demonstrates that neural conditional language models are powerful enough to generate fluent and accurate (though not necessarily pragmatic) descriptions of images or structured representations (Donahue et al., 2015)."}, {"heading": "3.4 Reasoning model", "text": "As described in the introduction, the general derived approach to pragmatics constructs a base listener and then selects a description that makes it\nbehave correctly. Since the assumption that listeners will behave deterministically is often a poor one, it is common for such derived approaches to implement probabilistic base listeners, and maximize the probability of correct behavior.\nThe neural literal listener L0 described in the preceding section is such a probabilistic listener. Given a target i and a pair of candidate referents r1 and r2, it is natural to specify the behavior of a reasoning speaker as simply:\nmax d pL0(i|d, r1, r2) (8)\nAt a first glance, the only thing necessary to implement this model is the representation of the literal listener itself. When the set of possible utterances comes from a fixed vocabulary (Vogel et al., 2013) or a grammar small enough to exhaustively enumerate (Smith et al., 2013) the operation maxd in Equation 8 is practical.\nIn this case, however, we would like our model to be capable of producing arbitrary strings. Because the score pL0 is produced by a discriminative listener model, and does not factor along the words of the description, there is no dynamic program that enables efficient inference over the space of all strings.\nWe instead use a sampling-based optimization procedure. The key ingredient here is a good proposal distribution from which to sample sentences likely to be assigned high weight by the model listener. For this we turn to the literal speaker S0 described in the previous section. Recall that this speaker produces a distribution over plausible descriptions of isolated images, while ignoring pragmatic context. We can use it as a source of candidate descriptions, to be reweighted according to the expected behavior of L0.\nThe full specification of a sampling neural reasoning speaker is as follows:\n1. Draw samples d1, . . . dn \u223c pS0(\u00b7|ri). 2. Score samples: pk = pL0(i|dk, r1, r2). 3. Select dk with k = argmax pk.\nWhile primarily to enable efficient inference, we can also use the literal speaker to serve a different purpose: \u201cregularizing\u201d model behavior towards choices that are adequate and fluent, rather than exploiting strange model behavior. Past work has restricted the set of utterances in a way that guarantees fluency. But with an imperfect learned listener model, and a procedure that optimizes this\nlistener\u2019s judgments directly, the speaker model might accidentally discover the kinds of pathological optima that neural classification models are known to exhibit (Goodfellow et al., 2014b)\u2014in this case, sentences that cause exactly the right response from L0, but no longer bear any resemblance to human language use. To correct this, we allow the model to consider two questions: as before, \u201chow likely is it that a listener would interpret this sentence correctly?\u201d, but additionally \u201chow likely is it that a speaker would produce it?\u201d\nFormally, we introduce a parameter \u03bb that trades off between L0 and S0, and take the reasoning model score in step 2 above to be:\npk = pS0(dk|ri)\u03bb \u00b7 pL0(i|dk, r1, r2)1\u2212\u03bb (9) This can be viewed as a weighted joint probability that a sentence is both uttered by the literal speaker and correctly interpreted by the literal listener, or alternatively in terms of Grice\u2019s conversational maxims (Grice, 1970): L0 encodes the maxims of quality and relation, ensuring that the description contains enough information for L to make the right choice, while S0 encodes the maxims of quantity and manner, ensuring that the description conforms with human language users with respect to sentence length and word choice."}, {"heading": "4 Evaluation", "text": "We evaluate our model on the reference game RG described in the introduction. In particular, we construct instances of RG using the Abstract Scenes Dataset introduced by Zitnick and Parikh (2013). Example abstract scenes are shown in Figure 1 and Figure 6. The dataset contains simple pictures constructed by humans and described in natural language. Scene representations are available both as rendered images and as feature representations containing the identity and location of each object; as noted in Section 3.1, we use this feature set to produce our referent representation f(r). This dataset was previously used for a variety of language and vision tasks (e.g. Ortiz et al. (2015), Zitnick et al. (2014)). It consists of 10,020 scenes, each annotated with up to 6 captions.\nAbstract scenes are appealing for several reasons. Because high-quality features are readily available, we can avoid complications from visual model training in our evaluation. It is additionally straightforward to measure the similarity of pairs of scenes, facilitating model evaluation at varying\nlevels of difficulty. Some past work has avoided human evaluation for this task, instead reporting success from a synthetic listener model. In the case of derived methods, this amounts to reporting posterior likelihood. Here we instead rely on human evaluation (via Amazon Mechanical Turk). We begin by holding out a development set and a test set; each held-out set contains 1000 scenes and their accompanying descriptions. For each held-out set, we construct two sets of 200 paired (target, distractor) scenes: All, with up to four differences between paired scenes, and Hard, with exactly one difference between paired scenes. (We take the number of differences between scenes to be the number of objects that appear in one scene but not the other.)\nWe report two evaluation metrics. Fluency is determined by showing human raters isolated sentences, and asking them to rate linguistic quality on a scale from 1\u20135. Accuracy is success rate at RG: as in Figure 1, humans are shown two images and a model-generated description, and asked to select the image matching the description.\nIn the remainder of this section, we measure the tradeoff between fluency and accuracy that results from different mixtures of the base models (Section 4.1), measure the number of samples needed to obtain good performance from the reasoning listener (Section 4.2), and attempt to approximate the reasoning listener with a monolithic \u201ccompiled\u201d listener (Section 4.3). In Section 4.4 we report final accuracies for our approach and baselines."}, {"heading": "4.1 How good are the base models?", "text": "To measure the performance of the base models, we draw 10 samples djk for a subset of 100 pairs (r1,j , r2,j) in the Dev-All set. We collect human fluency and accuracy judgments for each of the 1000 total samples. This allows us to conduct a post-hoc search over possible values of the mixing parameter \u03bb: for a range of \u03bb, we compute the average accuracy and fluency of the highest scoring sample. By varying \u03bb, we can view the tradeoff between accuracy and fluency that results from interpolating between the listener and speaker model\u2014setting \u03bb = 0 gives samples from pL0, and \u03bb = 1 gives samples from pS0.\nFigure 4 shows the resulting accuracy and fluency for various values of \u03bb. It can be seen that relying entirely on the listener gives the highest accuracy but substantially degraded fluency.\nHowever, by adding only a very small weight to the speaker model, it is possible to achieve nearperfect fluency without a substantial decrease in accuracy. Example sentences for an individual reference game are shown in Figure 5; increasing \u03bb causes captions to become more generic. For the remaining experiments in this paper, we take \u03bb = 0.02, finding that this gives excellent performance on both metrics.\nOn the dev set, \u03bb = 0.02 results in an average fluency of 4.8 (compared to 4.8 for the literal speaker \u03bb = 1). This high fluency can be confirmed by inspection of model samples (Figure 6). We thus focus on accuracy or the remainder of the evaluation."}, {"heading": "4.2 How many samples are needed?", "text": "Next we turn to the computational efficiency of the reasoning model. As in all sampling-based inference, the number of samples that must be drawn from the proposal is of critical interest\u2014if too\nmany samples are needed, the model will be too slow to use in practice. Having fixed \u03bb = 0.02 in the preceding section, we measure accuracy for versions of the reasoning model that draw 1, 10, 100, and 1000 samples. Results are shown in Table 1. We find that substantial gains continue up to 100 samples, then level off."}, {"heading": "4.3 Is reasoning necessary?", "text": "Because they do not require complicated inference procedures, direct approaches to pragmatics typically enjoy better computational efficiency than derived approaches. Having built an accurate derived speaker, can we use it to bootstrap a more efficient direct speaker?\nTo explore this, we constructed a \u201ccompiled\u201d speaker model as follows: Given reference candidates r1 and r2 and target t, this model produces embeddings e1 and e2, concatenates them together into a \u201ccontrast embedding\u201d [et, e\u2212t], and then feeds this whole embedding into a string decoder module. Like S0, this model generates captions without the need for discriminative rescoring; unlike S0, the contrast embedding means this model can in principle learn to produce pragmatic captions, if given access to pragmatic training data. Since no such training data exists, we train the compiled model on captions sampled from the reasoning speaker itself.\nThis model is evaluated in Table 2. While the distribution of scores is quite different from that of the base model (it improves noticeably over S0 on scenes with 2\u20133 differences), the overall gain is very small (the difference in mean scores is not significant). The compiled model significantly underperforms the reasoning model. While this merits further exploration, the results at least suggest either that the reasoning procedure is not easily approximated by a shallow neural network, or that example descriptions of randomly-sampled training pairs (which are usually easy to discriminate) do not provide a strong enough signal for a reflex learner to recover pragmatic behavior."}, {"heading": "4.4 Final evaluation", "text": "Based on the following sections, we keep \u03bb = 0.02 and use 100 samples to generate predictions. We evaluate on the test set, comparing this Reasoning model S1 to two baselines: Literal, an image captioning model trained normally on the abstract scene captions (corresponding to our L0), and Contrastive, a model trained with a soft contrastive objective, and previously used for visual referring expression generation (Mao et al., 2015).\nResults are shown in Table 3. Our reasoning model outperforms both the literal baseline and\nprevious work by a substantial margin, achieving an improvement of 17% on all pairs set and 15% on hard pairs. Figures 6 and 7 show various representative descriptions from the model."}, {"heading": "5 Conclusion", "text": "We have presented an approach for learning to generate pragmatic descriptions about general referents, even without training data collected in a pragmatic context. Our approach is built from a pair of simple neural base models, a listener and a speaker, and a high-level model that reasons about their outputs in order to produce pragmatic descriptions. In an evaluation on a standard referring expression game, our model\u2019s descriptions produced correct behavior in human listeners significantly more often than existing baselines.\nIt is generally true of existing derived approaches to pragmatics that much of the system\u2019s behavior required hand-engineering, and generally true of direct approaches (and neural networks\nin particular) that training is only possible when supervision was available for the precise target task. By synthesizing these two approaches, we address both problems, obtaining pragmatic behavior without domain knowledge and without targeted training data. We believe that this general strategy of using reasoning at inference time to obtain novel contextual behavior from neural decoding models might be much more broadly applied."}, {"heading": "Acknowledgments", "text": "We thank Lisa Anne Hendricks and Marcus Rohrbach for useful discussions about this work, and Nvidia for a hardware grant. JA is supported by a National Science Foundation Graduate Fellowship."}], "references": [{"title": "Deep compositional question answering with neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "arXiv:1511.02799.", "citeRegEx": "Andreas et al\\.,? 2015", "shortCiteRegEx": "Andreas et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "Proceedings of the Confer-", "citeRegEx": "Donahue et al\\.,? 2015", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Learning distributions over logical forms for referring expression generation", "author": ["Nicholas FitzGerald", "Yoav Artzi", "Luke Zettlemoyer."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "FitzGerald et al\\.,? 2013", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2013}, {"title": "Informative communication in word production and word learning", "author": ["Michael C Frank", "Noah D Goodman", "Peter Lai", "Joshua B Tenenbaum."], "venue": "Proceedings of the 31st annual conference of the cognitive science society, pages 1228\u20131233.", "citeRegEx": "Frank et al\\.,? 2009", "shortCiteRegEx": "Frank et al\\.", "year": 2009}, {"title": "A game-theoretic approach to generating spatial descriptions", "author": ["Dave Golland", "Percy Liang", "Dan Klein."], "venue": "Proceedings of the 2010 conference on Empirical Methods in Natural Language Processing, pages 410\u2013419. Association for Computational", "citeRegEx": "Golland et al\\.,? 2010", "shortCiteRegEx": "Golland et al\\.", "year": 2010}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems, pages 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014a", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian Goodfellow", "Jonathon Shlens", "Christian Szegedy."], "venue": "arXiv preprint arXiv:1412.6572.", "citeRegEx": "Goodfellow et al\\.,? 2014b", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Referitgame: Referring to objects in photographs of natural scenes", "author": ["Sahar Kazemzadeh", "Vicente Ordonez", "Mark Matten", "Tamara L Berg."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 787\u2013798.", "citeRegEx": "Kazemzadeh et al\\.,? 2014", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Generation and comprehension of unambiguous object descriptions", "author": ["Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana Camburu", "Alan Yuille", "Kevin Murphy."], "venue": "arXiv preprint arXiv:1511.02283.", "citeRegEx": "Mao et al\\.,? 2015", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Learning in the rational speech acts model", "author": ["Will Monroe", "Christopher Potts."], "venue": "Proceedings of 20th Amsterdam Colloquium, Amsterdam, December. ILLC.", "citeRegEx": "Monroe and Potts.,? 2015", "shortCiteRegEx": "Monroe and Potts.", "year": 2015}, {"title": "Learning to interpret and describe abstract scenes", "author": ["Luis Gilberto Mateos Ortiz", "Clemens Wolff", "Mirella Lapata."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Ortiz et al\\.,? 2015", "shortCiteRegEx": "Ortiz et al\\.", "year": 2015}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Noah A. Smith", "Jason Eisner."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Smith and Eisner.,? 2005", "shortCiteRegEx": "Smith and Eisner.", "year": 2005}, {"title": "Learning and using language via recursive pragmatic reasoning about other agents", "author": ["Nathaniel J Smith", "Noah Goodman", "Michael Frank."], "venue": "Advances in Neural Information Processing Systems, pages 3039\u20133047.", "citeRegEx": "Smith et al\\.,? 2013", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng."], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Emergence of Gricean maxims from multi-agent decision theory", "author": ["Adam Vogel", "Max Bodoia", "Christopher Potts", "Daniel Jurafsky."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Asso-", "citeRegEx": "Vogel et al\\.,? 2013", "shortCiteRegEx": "Vogel et al\\.", "year": 2013}, {"title": "Show, attend and tell: neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C Zitnick", "Devi Parikh."], "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition, pages 3009\u20133016.", "citeRegEx": "Zitnick and Parikh.,? 2013", "shortCiteRegEx": "Zitnick and Parikh.", "year": 2013}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C Lawrence Zitnick", "Ramakrishna Vedantam", "Devi Parikh"], "venue": null, "citeRegEx": "Zitnick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Independent of the application to RG, our model also resembles the suite of neural image captioning models that have been a popular subject of recent study (Xu et al., 2015).", "startOffset": 156, "endOffset": 173}, {"referenceID": 2, "context": "Direct pragmatics As an example of the direct approach mentioned in the introduction, FitzGerald et al. (2013) collect a set of human-generated referring expressions about abstract representations of sets of colored blocks.", "startOffset": 86, "endOffset": 111}, {"referenceID": 2, "context": "Direct pragmatics As an example of the direct approach mentioned in the introduction, FitzGerald et al. (2013) collect a set of human-generated referring expressions about abstract representations of sets of colored blocks. Given a set of blocks to describe, their model directly learns a maximum-entropy distribution over the set of logical expressions whose denotation is the target set. Other research, focused on direct referring expression generation from a computer vision perspective, includes that of Mao et al. (2015) and Kazemzadeh et al.", "startOffset": 86, "endOffset": 527}, {"referenceID": 2, "context": "Direct pragmatics As an example of the direct approach mentioned in the introduction, FitzGerald et al. (2013) collect a set of human-generated referring expressions about abstract representations of sets of colored blocks. Given a set of blocks to describe, their model directly learns a maximum-entropy distribution over the set of logical expressions whose denotation is the target set. Other research, focused on direct referring expression generation from a computer vision perspective, includes that of Mao et al. (2015) and Kazemzadeh et al. (2014).", "startOffset": 86, "endOffset": 556}, {"referenceID": 3, "context": "Experiments (Frank et al., 2009) show that this model explains human behavior well, but both computational and representational issues restrict its application to very simple reference games.", "startOffset": 12, "endOffset": 32}, {"referenceID": 11, "context": "Derived pragmatics The derived approach is exemplified by the work of Smith et al. (2013). This work describes a series of nested Bayesian models, in which intelligent listeners reason about the behavior of reflexive speakers, and even higher-order speakers reason about these listeners.", "startOffset": 70, "endOffset": 90}, {"referenceID": 11, "context": "Other work in this family includes that of Vogel et al. (2013), Golland et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 4, "context": "(2013), Golland et al. (2010), and Monroe and Potts (2015).", "startOffset": 8, "endOffset": 30}, {"referenceID": 4, "context": "(2013), Golland et al. (2010), and Monroe and Potts (2015). These approaches couple template-driven language generation with gametheoretic reasoning frameworks to produce contextually appropriate language.", "startOffset": 8, "endOffset": 59}, {"referenceID": 4, "context": "(2013), Golland et al. (2010), and Monroe and Potts (2015). These approaches couple template-driven language generation with gametheoretic reasoning frameworks to produce contextually appropriate language. While somewhat more expressive than the models of Smith et al. (2013), they still require both domain-specific engineering, controlled world representations, and pragmatically annotated training data.", "startOffset": 8, "endOffset": 276}, {"referenceID": 13, "context": "textual description (Socher et al., 2014), and neural conditional language models, which take a content representation and emit a sequence of tokens (Donahue et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 1, "context": ", 2014), and neural conditional language models, which take a content representation and emit a sequence of tokens (Donahue et al., 2015).", "startOffset": 115, "endOffset": 137}, {"referenceID": 5, "context": "While not trained end-to-end, our approach can also be viewed as a cooperative analog of the \u201cgenerative adversarial networks\u201d used for image generation (Goodfellow et al., 2014a).", "startOffset": 153, "endOffset": 179}, {"referenceID": 0, "context": "Reasoning with neural networks A general framework for constructing task-specific neural networks and composing them to produce novel behavior was explored in the context of visual question answering by Andreas et al. (2015). The current work can be thought of as a generalization of that approach, in which the high-level model actively reasons about the output of low-level modules, rather than directly composing them.", "startOffset": 203, "endOffset": 225}, {"referenceID": 4, "context": "This uniform random choice is important: it ensures that our approach is applicable even when there is not a naturally-occurring source of target\u2013 distractor pairs, as previous work (Golland et al., 2010; Monroe and Potts, 2015) has required.", "startOffset": 182, "endOffset": 228}, {"referenceID": 9, "context": "This uniform random choice is important: it ensures that our approach is applicable even when there is not a naturally-occurring source of target\u2013 distractor pairs, as previous work (Golland et al., 2010; Monroe and Potts, 2015) has required.", "startOffset": 182, "endOffset": 228}, {"referenceID": 4, "context": "This uniform random choice is important: it ensures that our approach is applicable even when there is not a naturally-occurring source of target\u2013 distractor pairs, as previous work (Golland et al., 2010; Monroe and Potts, 2015) has required. Note that this objective can also be viewed as the contrastive loss described by Smith and Eisner (2005), where it serves as an approximation to the likelihood objective that encourages L0 to prefer ri to every other possible referent simultaneously.", "startOffset": 183, "endOffset": 348}, {"referenceID": 4, "context": "These base models are intended to be the minimal learned equivalents of the hand-engineered speakers and hand-written grammars employed in previous derived approaches (Golland et al., 2010).", "startOffset": 167, "endOffset": 189}, {"referenceID": 1, "context": "Past work amply demonstrates that neural conditional language models are powerful enough to generate fluent and accurate (though not necessarily pragmatic) descriptions of images or structured representations (Donahue et al., 2015).", "startOffset": 209, "endOffset": 231}, {"referenceID": 14, "context": "When the set of possible utterances comes from a fixed vocabulary (Vogel et al., 2013) or a grammar small enough to exhaustively enumerate (Smith et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 12, "context": ", 2013) or a grammar small enough to exhaustively enumerate (Smith et al., 2013) the operation maxd in Equation 8 is practical.", "startOffset": 60, "endOffset": 80}, {"referenceID": 6, "context": "listener\u2019s judgments directly, the speaker model might accidentally discover the kinds of pathological optima that neural classification models are known to exhibit (Goodfellow et al., 2014b)\u2014in this case, sentences that cause exactly the right response from L0, but no longer bear any resemblance to human language use.", "startOffset": 165, "endOffset": 191}, {"referenceID": 15, "context": "In particular, we construct instances of RG using the Abstract Scenes Dataset introduced by Zitnick and Parikh (2013). Example abstract scenes are shown in Figure 1 and Figure 6.", "startOffset": 92, "endOffset": 118}, {"referenceID": 10, "context": "Ortiz et al. (2015), Zitnick et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "Ortiz et al. (2015), Zitnick et al. (2014)).", "startOffset": 0, "endOffset": 43}, {"referenceID": 8, "context": "\u201cContrastive\u201d is a reimplementation of the approach of Mao et al. (2015). \u201cReasoning\u201d is the reasoning model S1 from this paper.", "startOffset": 55, "endOffset": 73}, {"referenceID": 8, "context": "We evaluate on the test set, comparing this Reasoning model S1 to two baselines: Literal, an image captioning model trained normally on the abstract scene captions (corresponding to our L0), and Contrastive, a model trained with a soft contrastive objective, and previously used for visual referring expression generation (Mao et al., 2015).", "startOffset": 322, "endOffset": 340}], "year": 2017, "abstractText": "We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural \u201clistener\u201d and \u201cspeaker\u201d models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated without demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 64% success rate using existing techniques.", "creator": "LaTeX with hyperref package"}}}