{"id": "1604.02125", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes", "abstract": "We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. The motivation for this work comes from the fact that some ambiguities in language simply cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence \"I shot an elephant in my pajamas\", looking at the language alone (and not reasoning about common sense), it is unclear if it is the person or the elephant that is wearing the pajamas or both. Our approach involves producing a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly re-ranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. We also show that multiple hypotheses are crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms a state-of-the-art NLP system (Stanford Parser [16,27]) by 17.91% (28.69% relative) in one experiment, and by 12.83% (25.28% relative) in another. We also make small improvements over a state-of-the-art vision system (DeepLab-CRF [13]).", "histories": [["v1", "Thu, 7 Apr 2016 19:26:56 GMT  (7574kb,D)", "https://arxiv.org/abs/1604.02125v1", "*The first two authors contributed equally"], ["v2", "Thu, 5 May 2016 19:05:27 GMT  (7575kb,D)", "http://arxiv.org/abs/1604.02125v2", "*The first two authors contributed equally"], ["v3", "Fri, 2 Sep 2016 01:47:35 GMT  (5865kb,D)", "http://arxiv.org/abs/1604.02125v3", "*The first two authors contributed equally"], ["v4", "Mon, 26 Sep 2016 04:08:19 GMT  (6799kb,D)", "http://arxiv.org/abs/1604.02125v4", "*The first two authors contributed equally. Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016"]], "COMMENTS": "*The first two authors contributed equally", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["gordon christie", "ankit laddha", "aishwarya agrawal", "stanislaw antol", "yash goyal", "kevin kochersberger", "dhruv batra"], "accepted": true, "id": "1604.02125"}, "pdf": {"name": "1604.02125.pdf", "metadata": {"source": "CRF", "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes", "authors": ["Gordon Christie", "Ankit Laddha", "Aishwarya Agrawal", "Stanislaw Antol", "Yash Goyal", "Kevin Kochersberger", "Dhruv Batra"], "emails": ["ankit1991laddha@gmail.com", "gordonac@vt.edu", "aish@vt.edu", "santol@vt.edu", "ygoyal@vt.edu", "kbk@vt.edu", "dbatra@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "Perception and intelligence problems are hard. Whether we are interested in understanding an im-\n* Denotes equal contribution\nage or a sentence, our algorithms must operate under tremendous levels of ambiguity. When a human reads the sentence \u201cI eat sushi with tuna\u201d, it is clear that the preposition phrase \u201cwith tuna\u201d modifies \u201csushi\u201d and not the act of eating, but this may be ambiguous to a machine. This problem of determining whether a prepositional phrase (\u201cwith tuna\u201d) modifies a noun phrase (\u201csushi\u201d) or verb phrase (\u201ceating\u201d) is formally known as Prepositional Phrase Attachment Resolution (PPAR) (Ratnaparkhi et al., 1994). Consider the captioned scene shown in Fig-\nar X\niv :1\n60 4.\n02 12\n5v 4\n[ cs\n.C V\n] 2\n6 Se\np 20\nure 1. The caption \u201cA dog is standing next to a woman on a couch\u201d exhibits a PP attachment ambiguity \u2013 \u201c(dog next to woman) on couch\u201d vs \u201cdog next to (woman on couch)\u201d. It is clear that having access to image segmentations can help resolve this ambiguity, and having access to the correct PP attachment can help image segmentation.\nThere are two main roadblocks that keep us from writing a single unified model (say a graphical model) to perform both tasks: (1) Inaccurate Models \u2013 empirical studies (Meltzer et al., 2005, Szeliski et al., 2008, Kappes et al., 2013) have repeatedly found that models are often inaccurate and miscalibrated \u2013 their \u201cmost-likely\u201d beliefs are placed on solutions far from the ground-truth. (2) Search Space Explosion \u2013 jointly reasoning about multiple modalities is difficult due to the combinatorial explosion of search space ({exponentially-many segmentations} \u00d7 {exponentially-many sentence-parses}).\nProposed Approach and Contributions. In this paper, we address the problem of simultaneous object segmentation (also called semantic segmentation) and PPAR in captioned scenes. To the best of our knowledge this is the first paper to do so.\nOur main thesis is that a set of diverse plausible hypotheses can serve as a concise interpretable summary of uncertainty in vision and language \u2018modules\u2019 (What does the semantic segmentation module see in the world? What does the PPAR module describe?) and form the basis for tractable joint reasoning (How do we reconcile what the semantic segmentation module sees in the world with how the PPAR module describes it?).\nGiven our two modules with M hypotheses each, how can we integrate beliefs across the segmentation and sentence parse modules to pick the best pair of hypotheses? Our key focus is consistency \u2013 correct hypotheses from different modules will be correct in a consistent way, but incorrect hypotheses will be incorrect in incompatible ways. Specifically, we develop a MEDIATOR model that scores pairs for consistency and searches over all M2 pairs to pick the highest scoring one. We demonstrate our approach on three datasets \u2013 ABSTRACT-50S (Vedantam et al., 2014), PASCAL-50S, and PASCALContext-50S (Mottaghi et al., 2014). We show that our vision+language approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006)\nby 20.66% (36.42% relative) for ABSTRACT-50S, 17.91% (28.69% relative) for PASCAL-50S, and by 12.83% (25.28% relative) for PASCAL-Context50S. We also make small but consistent improvements over DeepLab-CRF (Chen et al., 2015)."}, {"heading": "2 Related Work", "text": "Most works at the intersection of vision and NLP tend to be \u2018pipeline\u2019 systems, where vision tasks take 1-best inputs from NLP (e.g., sentence parsings) without trying to improve NLP performance and vice-versa. For instance, Fidler et al. (2013) use prepositions to improve object segmentation and scene classification, but only consider the mostlikely parse of the sentence and do not resolve ambiguities in text. Analogously, Yatskar et al. (2014) investigate the role of object, attribute, and action classification annotations for generating human-like descriptions. While they achieve impressive results at generating descriptions, they assume perfect vision modules to generate sentences. Our work uses current (still imperfect) vision and NLP modules to reason about images and provided captions, and simultaneously improve both vision and language modules. Similar to our philosophy, an earlier work by Barnard and Johnson (2005) used images to help disambiguate word senses (e.g. piggy banks vs snow banks). In a more recent work, Gella et al. (2016) studied the problem of reasoning about an image and a verb, where they attempt to pick the correct sense of the verb that describes the action depicted in the image. Berzak et al. (2015) resolve linguistic ambiguities in sentences coupled with videos that represent different interpretations of the sentences. Perhaps the work closest to us is Kong et al. (2014), who leverage information from an RGBD image and its sentential description to improve 3D semantic parsing and resolve ambiguities related to coreference resolution in the sentences (e.g., what \u201cit\u201d refers to). We focus on a different kind of ambiguity \u2013 the Prepositional Phrase (PP) attachment resolution. In the classification of parsing ambiguities, coreference resolution is considered a discourse ambiguity (Poesio and Artstein, 2005) (arising out of two different words across sentences for the same object), while PP attachment is considered a syntactic ambiguity (arising out of multiple valid sentence\nstructures) and is typically considered much more difficult to resolve (Bach, 2016, Davis, 2016).\nA number of recent works have studied problems at the intersection of vision and language, such as Visual Question Answering (Antol et al., 2015, Geman et al., 2014, Malinowski et al., 2015), Visual Madlibs (Yu et al., 2015), and image captioning (Vinyals et al., 2015, Fang et al., 2015). Our work falls in this domain with a key difference that we produce both vision and NLP outputs.\nOur work also has similarities with works on \u2018spatial relation learning\u2019 (Malinowski and Fritz, 2014, Lan et al., 2012), i.e. learning a visual representation for noun-preposition-noun triplets (\u201ccar on road\u201d). While our approach can certainly utilize such spatial relation classifiers if available, the focus of our work is different. Our goal is to improve semantic segmentation and PPAR by jointly reranking segmentation-parsing solution pairs. Our approach implicitly learns spatial relationships for prepositions (\u201con\u201d, \u201cabove\u201d) but these are simply emergent latent representations that help our reranker pick out the most consistent pair of solutions.\nOur work utilizes a line of work (Batra et al., 2012, Batra, 2012, Prasad et al., 2014) on producing diverse plausible solutions from probabilistic models, which has been successfully applied to a number of problem domains (Guzman-Rivera et al., 2013, Yadollahpour et al., 2013, Gimpel et al., 2013, Premachandran et al., 2014, Sun et al., 2015, Ahmed et al., 2015)."}, {"heading": "3 Approach", "text": "In order to emphasize the generality of our approach, and to show that our approach is compatible with a wide class of implementations of semantic segmentation and PPAR modules, we present our approach with the modules abstracted as \u201cblack boxes\u201d that satisfy a few general requirements and minimal assumptions. In Section 4, we describe each of the modules in detail, making concrete their respective features, and other details."}, {"heading": "3.1 What is a Module?", "text": "The goal of a module is to take input variables x \u2208 X (images or sentences), and predict output variables y \u2208 Y (semantic segmentation) and\nz \u2208 Z (prepositional attachment expressed in sentence parse). The two requirements on a module are that it needs to be able to produce scores S(y|x) for potential solutions and a list of plausible hypotheses Y = {y1,y2, . . . ,yM}.\nMultiple Hypotheses. In order to be useful, the set Y of hypotheses must provide an accurate summary of the score landscape. Thus, the hypotheses should be plausible (i.e., high-scoring) and mutually non-redundant (i.e., diverse). Our approach (described next) is applicable to any choice of diverse hypothesis generators. In our experiments, we use the k-best algorithm of Huang and Chiang (2005) for the sentence parsing module and the DivMBest algorithm (Batra et al., 2012) for the semantic segmentation module. Once we instantiate the modules in Section 4, we describe the diverse solution generation in more detail."}, {"heading": "3.2 Joint Reasoning Across Multiple Modules", "text": "We now show how to intergrate information from both segmentation and PPAR modules. Recall that our key focus is consistency \u2013 correct hypotheses from different modules will be correct in a consistent way, but incorrect hypotheses will be incorrect in incompatible ways. Thus, our goal is to search for a pair (semantic segmentation, sentence parsing) that is mutually consistent.\nLet Y = {y1, . . . ,yM} denote the M semantic segmentation hypotheses and Z = {z1, . . . , zM} denote the M PPAR hypotheses.\nMEDIATOR Model. We develop a \u201cmediator\u201d model that identifies high-scoring hypotheses across modules in agreement with each other. Concretely, we can express the MEDIATOR model as a factor graph where each node corresponds to a module (semantic segmentation and PPAR). Working with such a factor graph is typically completely intractable because each node y, z has exponentiallymany states (image segmentations, sentence parsing). As illustrated in Figure 2, in this factor-graph view, the hypothesis sets Y,Z can be considered \u2018delta-approximations\u2019 for reducing the size of the output spaces.\nUnary factors S(\u00b7) capture the score/likelihood of each hypothesis provided by the corresponding module for the image/sentence at hand. Pairwise factors C(\u00b7, \u00b7) represent consistency factors. Impor-\ntantly, since we have restricted each module variables to just M states, we are free to capture arbitrary domain-specific high-order relationships for consistency, without any optimization concerns. In fact, as we describe in our experiments, these consistency factors may be designed to exploit domain knowledge in fairly sophisticated ways.\nConsistency Inference. We perform exhaustive inference over all possible tuples.\nargmax i,j\u2208{1,...,M}\n{ M(yi, zj) = S(yi) + S(zj) + C(yi, zj) } .\n(1)\nNotice that the search space with M hypotheses each isM2. In our experiments, we allow each module to take a different value for M , and typically use around 10 solutions for each module, leading to a mere 100 pairs, which is easily enumerable. We found that even with such a small set, at least one of the solutions in the set tends to be highly accurate, meaning that the hypothesis sets have relatively high recall. This shows the power of using a small set of diverse hypotheses. For a large M , we can exploit a number of standard ideas from the graphical models literature (e.g. dual decomposition or belief propagation). In fact, this is one reason we show the factor in Figure 2; there is a natural decomposition of the problem into modules.\nTraining MEDIATOR. We can express the MEDIATOR score as M(yi, zj) = w\u1d40\u03c6(x,yi, zj), as a linear function of score and consistency features \u03c6(x,yi, zj) = [\u03c6S(y i);\u03c6S(z j);\u03c6C(y\ni, zj)] , where \u03c6S(\u00b7) are the single-module (semantic segmentation and PPAR module) score features, and \u03c6C(\u00b7, \u00b7) are the inter-module consistency features. We describe these features in detail in the experiments. We learn these consistency weights w from a dataset annotated with ground-truth for the two modules y, z. Let {y\u2217, z\u2217} denote the oracle pair, composed of\nthe most accurate solutions in the hypothesis sets. We learn the MEDIATOR parameters in a discriminative learning fashion by solving the following Structured SVM problem:\nmin w,\u03beij\n1 2 w\u1d40w + C \u2211 ij \u03beij (2a)\ns.t. w\u1d40\u03c6(x,y\u2217, z\u2217)\ufe38 \ufe37\ufe37 \ufe38 Score of oracle tuple \u2212 w\u1d40\u03c6(x,yi, zj)\ufe38 \ufe37\ufe37 \ufe38 Score of any other tuple\n\u2265 1\ufe38\ufe37\ufe37\ufe38 Margin \u2212 \u03beij L(yi, zj)\ufe38 \ufe37\ufe37 \ufe38\nSlack scaled by loss\n\u2200i, j \u2208 {1, . . . ,M}.\n(2b)\nIntuitively, we can see that the constraint (2b) tries to maximize the (soft) margin between the score of the oracle pair and all other pairs in the hypothesis sets. Importantly, the slack (or violation in the margin) is scaled by the loss of the tuple. Thus, if there are other good pairs not too much worse than the oracle, the margin for such tuples will not be tightly enforced. On the other hand, the margin between the oracle and bad tuples will be very strictly enforced.\nThis learning procedure requires us to define the loss function L(yi, zj) , i.e., the cost of predicting a tuple (semantic segmentation, sentence parsing). We use a weighted average of individual losses:\nL(yi, zj) = \u03b1`(ygt,yi) + (1\u2212 \u03b1)`(zgt, zj) (3)\nThe standard measure for evaluating semantic segmentation is average Jaccard Index (or Intersectionover-Union) (Everingham et al., 2010), while for evaluating sentence parses w.r.t. their prepositional phrase attachment, we use the fraction of prepositions correctly attached. In our experiments, we report results with such a convex combination of module loss functions (for different values of \u03b1)."}, {"heading": "4 Experiments", "text": "We now describe the setup of our experiments, provide implementation details of the modules, and describe the consistency features.\nDatasets. Access to rich annotated image + caption datasets is crucial for performing quantitative evaluations. Since this is the first paper to study the problem of joint segmentation and PPAR, no standard datasets for this task exist so we had to curate our own annotations for PPAR on three image caption datasets \u2013 ABSTRACT50S (Vedantam et al., 2014), PASCAL-50S (Vedantam et al., 2014) (expands the UIUC PASCAL sentence dataset (Rashtchian et al., 2010) from 5 captions per image to 50), and PASCAL-Context50S (Mottaghi et al., 2014) (which uses the PASCAL Context image annotations and the same sentences as PASCAL-50S). Our annotations are publicly available on the authors\u2019 webpages. To curate the PASCAL-Context-50S PPAR annotations, we first select all sentences that have preposition phrase attachment ambiguities. We then plotted the distribution of prepositions in these sentences. The top 7 prepositions are used, as there is a large drop in the frequencies beyond these. The 7 prepositions are: \u201con\u201d, \u201cwith\u201d, \u201cnext to\u201d, \u201cin front of\u201d, \u201cby\u201d, \u201cnear\u201d, and \u201cdown\u201d. We then further sampled sentences to ensure uniform distribution across prepositions. We perform a similar filtering for PASCAL-50S and ABSTRACT-50S (using the top-6 prepositions for ABSTRACT-50S). Details are in the appendix. We consider a preposition ambiguous if there are at least two parsings where one of the two objects in the preposition dependency is the same across the two parsings while the other object is different (e.g. (dog on couch) and (woman on couch)). To summarize the statistics of all three datasets:\n1. ABSTRACT-50S (Vedantam et al., 2014): 25,000 sentences (50 per image) with 500 images from abstract scenes made from clipart. Filtering for captions containing the top-6 prepositions resulted in 399 sentences describing 201 unique images. These 6 prepositions are: \u201cwith\u201d, \u2018next to\u201d, \u201con top of\u201d, \u201cin front of\u201d, \u201cbehind\u201d, and \u201cunder\u201d. Overall, there are 502 total prepositions, 406 ambiguous prepositions, 80.88% ambiguity rate and 60 sentences\nwith multiple ambiguous prepositions. 2. PASCAL-50S (Vedantam et al., 2014): 50,000\nsentences (50 per image) for the images in the UIUC PASCAL sentence dataset (Rashtchian et al., 2010). Filtering for the top-7 prepositions resulted in a total of 30 unique images, and 100 image-caption pairs, where groundtruth PPAR were carefully annotated by two vision + NLP graduate students. Overall, there are 213 total prepositions, 147 ambiguous prepositions, 69.01% ambiguity rate and 35 sentences with multiple ambiguous prepositions. 3. PASCAL-Context-50S (Mottaghi et al., 2014): We use images and captions from PASCAL-50S, but with PASCAL Context segmentation annotations (60 categories instead of 21). This makes the vision task more challenging. Filtering this dataset for the top-7 prepositions resulted in a total of 966 unique images and 1,822 image-caption pairs. Ground truth annotations for the PPAR were collected using Amazon Mechanical Turk. Workers were shown an image and a prepositional attachment (extracted from the corresponding parsing of the caption) as a phrase (\u201cwoman on couch\u201d), and asked if it was correct. A screenshot of our interface is available in the appendix. Overall, there are 2,540 total prepositions, 2,147 ambiguous prepositions, 84.53% ambiguity rate and 283 sentences with multiple ambiguous prepositions.\nSetup. Single Module: We first show that visual features help PPAR by using the ABSTRACT-50S dataset, which contains clipart scenes where the extent and position of all the objects in the scene is known. This allows us to consider a scenario with a perfect vision system.\nMultiple Modules: In this experiment we use imperfect language and vision modules, and show improvements on the PASCAL-50S and PASCALContext-50S datasets.\nModule 1: Semantic Segmentation (SS) y. We use DeepLab-CRF (Chen et al., 2015) and DivMBest (Batra et al., 2012) to produce M diverse segmentations of the images. To evaluate we use image-level class-averaged Jaccard Index.\nModule 2: PP Attachment Resolution (PPAR)\nz. We use a recent version (v3.3.1; released 2014) of the PCFG Stanford parser module (De Marneffe et al., 2006, Huang and Chiang, 2005) to produce M parsings of the sentence. In addition to the parse trees, the module can also output dependencies, which make syntactical relationships more explicit. Dependencies come in the form dependency type(word1, word2), such as the preposition dependency prep on(woman-8, couch-11) (the number indicates the word position in sentence). To evaluate, we count the percentage of preposition attachments that the parse gets correct.\nBaselines: \u2022 INDEP. In our experiments, we compare our\nproposed approach (MEDIATOR) to the highest scoring solution predicted independently from each module. For semantic segmentation this is the output of DeepLab-CRF (Chen et al., 2015) and for the PPAR module this is the 1-best output of the Stanford Parser (De Marneffe et al., 2006, Huang and Chiang, 2005). Since our hypothesis lists are generated by greedy M-Best algorithms, this corresponds to predicting the (y1, z1) tuple. This comparison establishes the importance of joint reasoning. To the best of our knowledge, there is no existing (or even natural) joint model to compare to. \u2022 DOMAIN ADAPTATION. We learn a reranker on the parses. Note that domain adaptation is only needed for PPAR since the Stanford parser is trained on Penn Treebank (Wall Street Journal text) and not on text about images (such as image captions). Such domain adaptation is not necessary for semantic segmentation. This is a competitive single-module baseline. Specifically, we use the same parse-based features as our approach, and learn a reranker over the the Mz parse trees (Mz = 10).\nOur approach (MEDIATOR) significantly outperforms both baselines. The improvements over INDEP show that joint reasoning produces more accurate results than any module (vision or language) operating in isolation. The improvements over DOMAIN ADAPTATION establish the source of improvements is indeed vision, and not the reranking step. Simply adapting the parse from its original training domain (Wall Street Journal) to our domain (image captions) is not enough.\nAblative Study. Ours-CASCADE: This ablation studies the importance of multiple hypothesis. For each module (say y), we feed the single-best output of the other module z1 as input. Each module learns its own weight w using exactly the same consistency features and learning algorithm as MEDIATOR and predicts one of the plausible hypotheses y\u0302CASCADE = argmaxy\u2208Y w\n\u1d40\u03c6(x,y, z1). This ablation of our system is similar to (Heitz et al., 2008) and helps us in disentangling the benefits of multiple hypothesis and joint reasoning.\nFinally, we note that Ours-CASCADE can be viewed as special cases of MEDIATOR. Let MEDIATOR-(My,Mz) denote our approach run with My hypotheses for the first module and Mz for the second. Then INDEP corresponds to MEDIATOR-(1, 1) and CASCADE corresponds to predicting the y solution from MEDIATOR-(My, 1) and the z solution from MEDIATOR-(1,Mz). To get an upper-bound on our approach, we report oracle, the accuracy of the most accurate tuple in 10\u00d7 10 tuples.\nOur results are presented where MEDIATOR was trained with equally weighted loss (\u03b1 = 0.5), but we provide additional results for varying values of \u03b1 in the appendix.\nMEDIATOR and Consistency Features. Recall that we have two types of features \u2013 (1) score features \u03c6S(yi) and \u03c6S(zj), which try to capture how likely solutions yi and zj are respectively, and (2) consistency features \u03c6C(yi, zj), which capture how consistent the PP attachments in zj are with the segmentation in yi. For each (object1, preposition, object2) in zj , we compute 6 features between object1 and object2 segmentations in yi. Since the humans writing the captions may use multiple synonymous words (e.g. dog, puppy) for the same visual entity, we use word2vec (Mikolov et al., 2013) similarities to map the nouns in the sentences to the corresponding dataset categories.\n\u2022 Semantic Segmentation Score Features (\u03c6S(yi)) (2-dim): We use ranks and solution scores from DeepLab-CRF (Chen et al., 2015). \u2022 PPAR Score Features (\u03c6S(zi)) (9-dim): We use ranks and the log probability of parses from (De Marneffe et al., 2006), and 7 binary indicators for PASCAL (6 for ABSTRACT50S) denoting which prepositions are present in the parse.\n\u2022 Inter-Module Consistency Features (56- dim): For each of the 7 prepositions, 8 features are calculated:\n\u2013 One feature is the Euclidean distance between the center of the segmentation masks of the two objects connected by the preposition. These two objects in the segmentation correspond to the categories with which the soft similarity of the two objects in the sentence is highest among all PASCAL categories. \u2013 Four features capture max{0, (normalized -directional-distance)}, where directionaldistance measures above/below/left/right displacements between the two objects in the segmentation, and normalization involves dividing by height/width. \u2013 One feature is the ratio of sizes between object1 and object2 in the segmentation. \u2013 Two features capture the word2vec similarity between the two objects in PPAR (say \u2018puppy\u2019 and \u2018kitty\u2019) with their most similar PASCAL category (say \u2018dog\u2019 and \u2018cat\u2019), where these features are 0 if the categories are not present in segmentation. A visual illustration for some of these features for PASCAL can be seen in Figure 3. In the case where an object parsed from zj is not\npresent in the segmentation yi, the distance features are set to 0. The ratio of areas features (area of smaller object / area of larger object) are also set to 0 assuming that the smaller object is missing. In the case where an object has two or more connected components in the segmentation, the distances are computed w.r.t. the centroid of the segmentation and the area is computed as the number of pixels in the union of the instance segmentation masks. We also calculate 20 features for PASCAL-50S and 59 features for PASCAL-Context-50S that capture that consistency between yi and zj , in terms of presence/absence of PASCAL categories. For each noun in PPAR we compute its word2vec similarity with all PASCAL categories. For each of the PASCAL categories, the feature is the sum of similarities (with the PASCAL category) over all nouns if the category is present in segmentation, and is -1 times the sum of similarities over all nouns otherwise. This feature set was not used for ABSTRACT50S, since these features were intended to help improve the accuracy of the semantic segmentation module. For ABSTRACT-50S, we only use the 5 distance features, resulting in a 30- dim feature vector."}, {"heading": "4.1 Single-Module Results", "text": "We performed a 10-fold cross-validation on the ABSTRACT-50S dataset to pick M (=10) and the weight on the hinge-loss for MEDIATOR (C). The results are presented in Table 1. Our approach significantly outpeforms 1-best outputs of the Stanford Parser (De Marneffe et al., 2006) by 20.66% (36.42% relative). This shows a need for diverse hypotheses and reasoning about visual features when picking a sentence parse. oracle denotes the best achievable performance using these 10 hypotheses."}, {"heading": "4.2 Multiple-Module Results", "text": "We performed 10-fold cross-val for our results of PASCAL-50S and PASCAL-Context-50S, with 8\ntrain folds, 1 val fold, and 1 test fold, where the val fold was used to pick My, Mz, and C. Figure 4 shows the average combined accuracy on val, which was found to be maximal atMy = 5,Mz = 3 for PASCAL-50S, and My = 1,Mz = 10 for PASCAL-Context-50S, which are used at test time.\nWe present our results in Table 2. Our approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) for PASCAL-50S, and 12.83% (25.28% relative) for PASCAL-Context-50S. We also make small improvements over DeepLabCRF (Chen et al., 2015) in the case of PASCAL-50S. To measure statistical significance of our results, we performed paired t-tests between MEDIATOR and INDEP. For both modules (and average), the null hypothesis (that the accuracies of our approach and baseline come from the same distribution) can be successfully rejected at p-value 0.05. For sake of completeness, we also compared MEDIATOR with our ablated system (CASCADE) and found statistically significant differences only in PPAR. These results demonstrate a need for each module to produce a diverse set of plausible hypotheses for our MEDIATOR model to reason about. In the case of PASCAL-Context-50S, MEDIATOR performs identical to CASCADE since My is chosen as 1 (which is the CASCADE setting) in crossvalidation. Recall that MEDIATOR is a larger model class than CASCADE (in fact, CASCADE is a special case of MEDIATOR with My = 1). It is interesting to see that the large model class does not hurt, and MEDIATOR gracefully reduces to a smaller capacity model (CASCADE) if the amount of data is not enough to warrant the extra capacity. We hypothesize that in the presence of more training data, crossvalidation may pick a different setting of My and Mz, resulting in full utilization of the model capacity. Also note that our domain adaptation baseline achieved an accuracy higher than MAP/StanfordParser, but significantly lower than our approach for both PASCAL-50S and PASCAL-Context-50S. We also performed this for our single-module experiment and picked Mz (=10) with cross-validation,\nwhich resulted in an accuracy of 57.23%. Again, this is higher than MAP/Stanford-Parser (56.73%), but significantly lower than our approach (77.39%). Clearly, domain adaptation alone is not sufficient. We also see that oracle performance is fairly high, suggesting that when there is ambiguity and room for improvement, MEDIATOR is able to rerank effectively.\nAblation Study for Features. Table 3 displays results of an ablation study on PASCAL-50S and PASCAL-Context-50S to show the importance of the different features. In each row, we retain the module score features and drop a single set of consistency features. We can see all consistency features contribute to the performance of MEDIATOR.\nVisualizing Prepositions. Figure 5 shows a visualization for what our MEDIATOR model has implicitly learned about 3 prepositions (\u201con\u201d, \u201cby\u201d, \u201cwith\u201d). These visualizations show the score obtained by taking the dot product of distance features (Euclidean and directional) between object1 and object2 connected by the preposition with the corresponding learned weights of the model, considering object2 to be at the center of the visualization. Notice that these were learned without explicit training for spatial learning as in spatial relation learning (SRL) works (Malinowski and Fritz, 2014, Lan et al., 2012). These were simply recovered as an intermediate step towards reranking SS + PPAR hypotheses. Also note that SRL cannot handle multiple segmentation hypotheses, which our work shows are important (Table 2 CASCADE). In addition, our approach is more general."}, {"heading": "5 Discussions and Conclusion", "text": "We presented an approach to the simultaneous reasoning about prepositional phrase attachment res-\nolution of captions and semantic segmentation in images that integrates beliefs across the modules to pick the best pair of a diverse set of hypotheses. Our full model (MEDIATOR) significantly improves the accuracy of PPAR over the Stanford Parser by 17.91% for PASCAL-50S and by 12.83% for PASCAL-Context-50S, and achieves a small improvement on semantic segmentation over DeepLab-CRF for PASCAL-50S. These results demonstrate a need for information exchange between the modules, as well as a need for a diverse set of hypotheses to concisely capture the uncertainties of each module. Large gains in PPAR validate our intuition that vision is very helpful for dealing with ambiguity in language. Furthermore, we see even larger gains are possible from the oracle accuracies.\nWhile we have demonstrated our approach on a task involving simultaneous reasoning about language and vision, our approach is general and can be used for other applications. Overall, we hope our approach will be useful in a number of settings.\nAcknowledgements. We thank Larry Zitnick, Mohit Bansal, Kevin Gimpel, and Devi Parikh for helpful discussions, suggestions, and feedback included in this work. A majority of this work was done while AL was an intern at Virginia Tech. This work was partially supported by a National Science Foundation CAREER award, an Army Research Office YIP Award, an Office of Naval Research grant N00014-14-1-0679, and GPU donations by NVIDIA, all awarded to DB. GC was supported by a DTRA Basic Research grant HDTRA113-1-0015 provided by KK. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor."}, {"heading": "Appendix Overview", "text": "In this appendix, we provide the following:\nAppendix I: Additional motivation for our MEDIATOR model. Appendix II: Background on ABSTRACT-50S. Appendix III: Details of the dataset curation process for the ABSTRACT-50S, PASCAL50S, and PASCAL-Context-50S datasets. Appendix IV: Results where we study the effect of varying the weighting of each module in our approach. Appendix V: Performances and gains over the independent baseline on PASCAL-Context50S for each preposition. Appendix VI: Qualitative examples from our approach.\nAppendix I Additional Motivation for MEDIATOR\nAn example providing additional motivation for our approach is shown in Figure 6, where the ambiguous sentence that describes the image is \u201cA dog is standing next to a woman on a couch\u201d. The ambiguity is \u201c(dog next to woman) on couch\u201d vs \u201cdog next to (woman on couch)\u201d, which is reflected in parse trees\u2019 uncertainty. Parse tree #1 (Figure 6g) shows \u201cstanding\u201d (the verb phrase of the noun \u201cdog\u201d) connected with \u201ccouch\u201d via the \u201con\u201d preposition, whereas parse trees #2 (Figure 6h) and #3 (Figure 6i) show \u201cwoman\u201d connected with \u201ccouch\u201d via the \u201con\u201d preposition. This ambiguity can be resolved if we look at an accurate semantic segmentation such as Hypothesis #3 (Figure 6f) of the associated image (Figure 6b). Likewise, we might be able to do better at semantic segmentation if we choose a segmentation that is consistent with the sentence, such as Segmentation Hypothesis #3 (Figure 6f), which contains a person on a couch with a dog next to them, unlike the other two hypotheses (Figure 6d and Figure 6e).\nAppendix II Background About ABSTRACT-50S\nThe Abstract Scenes dataset (Zitnick and Parikh, 2013) contains synthetic images generated by human subjects via a drag-and-drop clipart interface. The subjects are given access to a (random) subset of 56 clipart objects that can be found in park scenes,\nas well as two characters, Mike and Jenny, with a variety of poses and expressions. Example scenes can be found in Figure 7. The motivation is to allow researchers to focus on higher-level semantic understanding without having to deal with noisy information extraction from real images, since the entire contents of the scene are known exactly, while also providing a dense semantic space to study (due to the heavily constrained world). We used the dataset in precisely this way to first test out the PPAR module in isolation to demonstrate that this problem can be helped by a sentence\u2019s corresponding image.\nAppendix III Dataset Curation and Annotation\nThe subsets of the PASCAL-50S and ABSTRACT50S datasets used in our experiments were carefully curated by two vision + NLP graduate students. The subset of the PASCAL-Context-50S dataset was curated by Amazon Mechanical Turk (AMT) workers. The following describes the dataset curation process for each datset.\nPASCAL-50S: For PASCAL-50S we first obtained sentences that contain one or more of 7 prepositions (i.e., \u201cwith\u201d, \u201cnext to\u201d, \u201con top of\u201d, \u201cin front of\u201d, \u201cbehind\u201d, \u201cby\u201d, and \u201con\u201d) that intuitively would typically depend on the relative distance between objects. Then we look for sentences that have preposition phrase attachment ambiguities, i.e., sentences where the parser output has different sets of prepositions for different parsings. Due to our focus on PP attachment, we do not pay attention to other parts of the sentence parse, so the parses can\nchange while the PP attachments remain the same, as in Figure 6h and Figure 6i. The sentences thus obtained are further filtered to obtain sentences in which the objects that are connected by the preposition belonging to one of the 20 PASCAL object categories. Since our vision module is semantic segmentation and not instance-level segmentation, we restrict the dataset to sentences involving prepositions connecting two different PASCAL categories. Thus, our final dataset contains 100 sentences describing 30 unique images and contains 16 of the 20 PASCAL categories as described in the paper. We then manually annotated the ground truth PP attachments. Such manual labeling by student annotators with expertise in NLP takes a lot of time, but results in annotations that are linguistically highquality, with any inter-human disagreement resolved by strict adherence to rules of grammar. ABSTRACT-50S: We first obtained sentences that contain one or more of 6 prepositions (i.e.,\n\u201cwith\u201d, \u201cnext to\u201d, \u201con top of\u201d, \u201cin front of\u201d, \u201cbehind\u201d, \u201cunder\u201d). Due to the semantic differences between the datasets, not all prepositions found in one were present in the other. Further filtering on sentences was done to ensure that the sentences contain at least one preposition phrase attachment ambiguity that is between the clipart noun categories (i.e., each clipart piece has a name, like \u201csnake\u201d, that we search the sentence parsing for). This filtering reduced the original dataset of 25,000 sentences and 500 scenes to our final experiment dataset of 399 sentences and 201 scenes. We then manually annotated the ground truth PP attachments.\nPASCAL-Context-50S: For PASCAL-Context50S, we first selected all sentences that have preposition phrase attachment ambiguities. We then plotted the distribution of prepositions in these sentences (see Figure 8). We found that there was a drop in the percentage of sentences for prepositions that appear in the sorted list after \u201cdown\u201d. Therefore, we only kept sentences that have one or more 2-D visual prepositions in the list of prepositions up to \u201cdown\u201d. Thus we ended up with the following 7 prepositions: \u201con\u201d, \u201cwith\u201d, \u201cnext to\u201d, \u201cin front of\u201d, \u201cby\u201d, \u201cnear\u201d, and \u201cdown\u201d. We then further sampled sentences to ensure uniform distribution across prepositions. Unlike PASCAL-50S, we did not filter sentences based on whether the objects connected by the prepositions belong to one of 60 PASCAL Context categories or not. Instead, we used the word2vec (Mikolov et al., 2013) similarity between the objects in the sentence and the PASCAL Context categories as one of the features. Thus, our final dataset contains 1,822 sentences describing 966 unique images.\nThe ground truth PP attachments for these 1,822 sentences were annotated by AMT workers. For each unique prepositional relation in a sentence, we showed the workers the prepositional relation of the form primary object preposition secondary object and its associated image and sentence and asked them to specify whether the prepositional relation is correct or not correct. We also asked them to choose the third option - \u201cPrimary object/ secondary object is not a noun in the caption\u201d in case that happened. The user interface used to collect these annotations is shown in Figure 9. We collected five answers for each prepositional relation. For evaluation, we used the majority response. We found that 87.11% of hu-\nman responses agree with the majority response, indicating that even though AMT workers were not explicitly trained in rules of grammar by us, there is relatively high inter-human agreement.\nAppendix IV Effect of Different Weighting of Modules\nSo far we have used the \u201cnatural\u201d setting of \u03b1 = 0.5, which gives equal weight to both modules. Note that \u03b1 is not a parameter of our approach; it is a design choice that the user/experiment-designer makes. To see the effect of weighting the modules differently, we tested our approach for various values of \u03b1. Figure 10 shows how the accuracies of each module vary depending on \u03b1 for the MEDIATOR model for PASCAL-50S and PASCAL-Context-50S. Recall that \u03b1 is the coefficient for the semantic segmentation module and 1-\u03b1 is the coefficient for the PPAR resolution module in the loss function. We see that as expected, putting no or little weight on the PPAR module drastically hurts performance for that module. Our approach is fairly robust to the setting of \u03b1, with a peak lying but any weight on it performs fairly similar with the peak lying somewhere between the extremes. The segmentation module has similar behavior, though it is not as sensitive to the choice of \u03b1. We believe this is because of small \u201cdynamic range\u201d of this module \u2013 the gap between the 1-best and oracle segmentation is smaller\nand thus the MEDIATOR can always default to the 1-best as a safe choice.\nAppendix V Performances for Each Preposition\nWe provide performances and gains over the independent baseline on PASCAL-Context-50S for each preposition in Table 4. We see that vision helps all prepositions.\nAppendix VI Qualitative Examples\nFigure 11 - Figure 16 show qualitative examples for our experiments. Figure 11 - Figure 13 show examples for the multiple modules examples (semantic segmentation and PPAR), and Figure 14 - Figure 16 show examples for the single module experiment. In each figure, the top row shows the image and the associated sentence. For the multiple modules figures, the second and third row show the diverse segmentations of the image, and the bottom two rows show different parsings of the sentence (last two rows for single module examples, as well). In these examples our approach uses 10 diverse solutions for the semantic segmentation module and 10 different solutions for the PPAR module. The highlighted pairs of solutions show the solutions picked by the MEDIATOR model. Examining the results can give you a sense of how the parsings can help the semantic segmentation module pick the best solution and viceversa."}], "references": [{"title": "Optimizing Expected Intersection-over-Union with Candidate-Constrained CRFs", "author": ["Ahmed et al.2015] Faruk Ahmed", "Dany Tarlow", "Dhruv Batra"], "venue": null, "citeRegEx": "Ahmed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2015}, {"title": "VQA: Visual Question Answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": null, "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "2016. Routledge encyclopedia of philosophy entry. http://online.sfsu.edu/ kbach/ambguity.html", "author": ["Kent Bach"], "venue": null, "citeRegEx": "Bach.,? \\Q2016\\E", "shortCiteRegEx": "Bach.", "year": 2016}, {"title": "Word Sense Disambiguation with Pictures", "author": ["Barnard", "Johnson2005] Kobus Barnard", "Matthew Johnson"], "venue": "Artificial Intelligence,", "citeRegEx": "Barnard et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Barnard et al\\.", "year": 2005}, {"title": "Diverse M-Best Solutions in Markov Random Fields", "author": ["Batra et al.2012] Dhruv Batra", "Payman Yadollahpour", "Abner Guzman-Rivera", "Gregory Shakhnarovich"], "venue": null, "citeRegEx": "Batra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Batra et al\\.", "year": 2012}, {"title": "An Efficient MessagePassing Algorithm for the M-Best MAP Problem", "author": ["Dhruv Batra"], "venue": "In UAI", "citeRegEx": "Batra.,? \\Q2012\\E", "shortCiteRegEx": "Batra.", "year": 2012}, {"title": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities", "author": ["Andrei Barbu", "Daniel Harari", "Boris Katz", "Shimon Ullman"], "venue": null, "citeRegEx": "Berzak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Berzak et al\\.", "year": 2015}, {"title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs", "author": ["George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille"], "venue": "In ICLR", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Generating Typed Dependency Parses from Phrase Structure Parses", "author": ["Bill MacCartney", "Christopher D Manning"], "venue": null, "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "The Pascal Visual Object Classes (VOC) Challenge", "author": ["L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "A Sentence is Worth a Thousand Pixels", "author": ["Fidler et al.2013] Sanja Fidler", "Abhishek Sharma", "Raquel Urtasun"], "venue": null, "citeRegEx": "Fidler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fidler et al\\.", "year": 2013}, {"title": "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings", "author": ["Gella et al.2016] Spandana Gella", "Mirella Lapata", "Frank Keller"], "venue": "In NAACL HLT", "citeRegEx": "Gella et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gella et al\\.", "year": 2016}, {"title": "A Visual Turing Test for Computer Vision Systems", "author": ["Geman et al.2014] Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "In PNAS", "citeRegEx": "Geman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2014}, {"title": "A Systematic Exploration of Diversity in Machine Translation", "author": ["Gimpel et al.2013] K. Gimpel", "D. Batra", "C. Dyer", "G. Shakhnarovich"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "DivMCuts: Faster Training of Structural SVMs with Diverse M-Best Cutting-Planes", "author": ["Pushmeet Kohli", "Dhruv Batra"], "venue": "In AISTATS", "citeRegEx": "Guzman.Rivera et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guzman.Rivera et al\\.", "year": 2013}, {"title": "Cascaded Classification Models: Combining Models for Holistic Scene Understanding", "author": ["Heitz et al.2008] Geremy Heitz", "Stephen Gould", "Ashutosh Saxena", "Daphne Koller"], "venue": null, "citeRegEx": "Heitz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Heitz et al\\.", "year": 2008}, {"title": "What are you talking about? Text-to-Image Coreference", "author": ["Kong et al.2014] Chen Kong", "Dahua Lin", "Mohit Bansal", "Raquel Urtasun", "Sanja Fidler"], "venue": null, "citeRegEx": "Kong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Image Retrieval with Structured Object Queries Using Latent Ranking SVM", "author": ["Lan et al.2012] Tian Lan", "Weilong Yang", "Yang Wang", "Greg Mori"], "venue": null, "citeRegEx": "Lan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2012}, {"title": "A pooling approach to modelling spatial relations for image retrieval and annotation", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": "arXiv preprint arXiv:1411.5190", "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images", "author": ["Marcus Rohrbach", "Mario Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Globally Optimal Solutions for Energy Minimization in Stereo Vision Using Reweighted Belief Propagation", "author": ["Chen Yanover", "Yair Weiss"], "venue": null, "citeRegEx": "Meltzer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Meltzer et al\\.", "year": 2005}, {"title": "Efficient Estimation of Word Representations in Vector Space. In ICLR", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Annotating (Anaphoric) ambiguity", "author": ["Poesio", "Artstein2005] Massimo Poesio", "Ron Artstein"], "venue": "In Corpus Linguistics Conference", "citeRegEx": "Poesio et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Poesio et al\\.", "year": 2005}, {"title": "Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets", "author": ["Prasad et al.2014] Adarsh Prasad", "Stefanie Jegelka", "Dhruv Batra"], "venue": null, "citeRegEx": "Prasad et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2014}, {"title": "Empirical Minimum Bayes Risk Prediction: How to extract an extra few% performance from vision models with just three more parameters", "author": ["Daniel Tarlow", "Dhruv Batra"], "venue": null, "citeRegEx": "Premachandran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Premachandran et al\\.", "year": 2014}, {"title": "Collecting Image Annotations Using Amazon\u2019s Mechanical Turk", "author": ["Peter Young", "Micah Hodosh", "Julia Hockenmaier"], "venue": "In NAACL HLT Workshop on Creating Speech and Language Data with Amazon\u2019s Mechan-", "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "A Maximum Entropy Model for Prepositional Phrase Attachment", "author": ["Jeff Reynar", "Salim Roukos"], "venue": "In Proceedings of the workshop on Human Language Technology. ACL", "citeRegEx": "Ratnaparkhi et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ratnaparkhi et al\\.", "year": 1994}, {"title": "Active Learning for Structured Probabilistic Models With Histogram Approximation", "author": ["Sun et al.2015] Qing Sun", "Ankit Laddha", "Dhruv Batra"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "A Comparative Study of Energy Minimization Methods for Markov Random", "author": ["Ramin Zabih", "Daniel Scharstein", "Olga Veksler", "Vladimir Kolmogorov", "Aseem Agarwala", "Marshall Tappen", "Carsten Rother"], "venue": null, "citeRegEx": "Szeliski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Szeliski et al\\.", "year": 2008}, {"title": "CIDEr: Consensus-based Image Description Evaluation", "author": ["Vedantam", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "CVPR.", "citeRegEx": "Vedantam et al\\.,? 2014", "shortCiteRegEx": "Vedantam et al\\.", "year": 2014}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Discriminative Re-ranking of Diverse Segmentations", "author": ["Dhruv Batra", "Greg Shakhnarovich"], "venue": null, "citeRegEx": "Yadollahpour et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yadollahpour et al\\.", "year": 2013}, {"title": "See No Evil, Say No Evil: Description Generation from Densely Labeled Images", "author": ["Yatskar et al.2014] Mark Yatskar", "Michel Galley", "Lucy Vanderwende", "Luke Zettlemoyer"], "venue": "In Lexical and Computational Semantics", "citeRegEx": "Yatskar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yatskar et al\\.", "year": 2014}, {"title": "Visual Madlibs: Fill in the Blank Description Generation and Question Answering", "author": ["Yu et al.2015] Licheng Yu", "Eunbyung Park", "Alexander C. Berg", "Tamara L. Berg"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Bringing Semantics Into Focus Using Visual Abstraction", "author": ["Zitnick", "Parikh2013] C. Lawrence Zitnick", "Devi Parikh"], "venue": null, "citeRegEx": "Zitnick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "We also make small improvements over DeepLab-CRF (Chen et al., 2015).", "startOffset": 49, "endOffset": 68}, {"referenceID": 26, "context": "This problem of determining whether a prepositional phrase (\u201cwith tuna\u201d) modifies a noun phrase (\u201csushi\u201d) or verb phrase (\u201ceating\u201d) is formally known as Prepositional Phrase Attachment Resolution (PPAR) (Ratnaparkhi et al., 1994).", "startOffset": 203, "endOffset": 229}, {"referenceID": 29, "context": "We demonstrate our approach on three datasets \u2013 ABSTRACT-50S (Vedantam et al., 2014), PASCAL-50S, and PASCAL-", "startOffset": 61, "endOffset": 84}, {"referenceID": 7, "context": "We also make small but consistent improvements over DeepLab-CRF (Chen et al., 2015).", "startOffset": 64, "endOffset": 83}, {"referenceID": 10, "context": "For instance, Fidler et al. (2013) use prepositions to improve object segmentation and scene classification, but only consider the mostlikely parse of the sentence and do not resolve ambi-", "startOffset": 14, "endOffset": 35}, {"referenceID": 32, "context": "Analogously, Yatskar et al. (2014) investigate the role of object, attribute, and action classification annotations for generating human-like descriptions.", "startOffset": 13, "endOffset": 35}, {"referenceID": 11, "context": "In a more recent work, Gella et al. (2016) studied the problem of reasoning about an image and a verb, where they attempt to pick the correct sense", "startOffset": 23, "endOffset": 43}, {"referenceID": 6, "context": "Berzak et al. (2015) resolve linguistic ambiguities in sentences coupled with videos that represent different interpretations of the sentences.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Berzak et al. (2015) resolve linguistic ambiguities in sentences coupled with videos that represent different interpretations of the sentences. Perhaps the work closest to us is Kong et al. (2014), who leverage information from an RGBD image and its sentential description to improve 3D semantic parsing and resolve ambiguities related to coreference resolution in the sentences (e.", "startOffset": 0, "endOffset": 197}, {"referenceID": 33, "context": ", 2015), Visual Madlibs (Yu et al., 2015), and image captioning (Vinyals et al.", "startOffset": 24, "endOffset": 41}, {"referenceID": 4, "context": "In our experiments, we use the k-best algorithm of Huang and Chiang (2005) for the sentence parsing module and the DivMBest algorithm (Batra et al., 2012) for the semantic seg-", "startOffset": 134, "endOffset": 154}, {"referenceID": 9, "context": "The standard measure for evaluating semantic segmentation is average Jaccard Index (or Intersectionover-Union) (Everingham et al., 2010), while for evaluating sentence parses w.", "startOffset": 111, "endOffset": 136}, {"referenceID": 29, "context": "we had to curate our own annotations for PPAR on three image caption datasets \u2013 ABSTRACT50S (Vedantam et al., 2014), PASCAL-50S (Vedantam et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 29, "context": ", 2014), PASCAL-50S (Vedantam et al., 2014) (expands the UIUC PASCAL sentence dataset (Rashtchian et al.", "startOffset": 20, "endOffset": 43}, {"referenceID": 25, "context": ", 2014) (expands the UIUC PASCAL sentence dataset (Rashtchian et al., 2010) from 5", "startOffset": 50, "endOffset": 75}, {"referenceID": 29, "context": "ABSTRACT-50S (Vedantam et al., 2014): 25,000 sentences (50 per image) with 500 images from abstract scenes made from clipart.", "startOffset": 13, "endOffset": 36}, {"referenceID": 29, "context": "PASCAL-50S (Vedantam et al., 2014): 50,000 sentences (50 per image) for the images in the UIUC PASCAL sentence dataset (Rashtchian et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 25, "context": ", 2014): 50,000 sentences (50 per image) for the images in the UIUC PASCAL sentence dataset (Rashtchian et al., 2010).", "startOffset": 92, "endOffset": 117}, {"referenceID": 7, "context": "We use DeepLab-CRF (Chen et al., 2015) and DivMBest (Batra et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 4, "context": ", 2015) and DivMBest (Batra et al., 2012) to produce M diverse segmentations of the images.", "startOffset": 21, "endOffset": 41}, {"referenceID": 7, "context": "the output of DeepLab-CRF (Chen et al., 2015) and for the PPAR module this is the 1-best output of the Stanford Parser (De Marneffe et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 15, "context": "This ablation of our system is similar to (Heitz et al., 2008) and helps us in disentangling the benefits of multiple", "startOffset": 42, "endOffset": 62}, {"referenceID": 21, "context": "sual entity, we use word2vec (Mikolov et al., 2013) similarities to map the nouns in the sentences to the corresponding dataset categories.", "startOffset": 29, "endOffset": 51}, {"referenceID": 7, "context": "\u2022 Semantic Segmentation Score Features (\u03c6S(y)) (2-dim): We use ranks and solution scores from DeepLab-CRF (Chen et al., 2015).", "startOffset": 106, "endOffset": 125}, {"referenceID": 7, "context": "We also make small improvements over DeepLabCRF (Chen et al., 2015) in the case of PASCAL-50S.", "startOffset": 48, "endOffset": 67}], "year": 2016, "abstractText": "We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence \u201cI shot an elephant in my pajamas\u201d, looking at language alone (and not using common sense), it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms the Stanford Parser (De Marneffe et al., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two different experiments. We also make small improvements over DeepLab-CRF (Chen et al., 2015).", "creator": "LaTeX with hyperref package"}}}