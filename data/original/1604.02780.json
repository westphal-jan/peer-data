{"id": "1604.02780", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Knowledge Extraction and Knowledge Integration governed by {\\L}ukasiewicz Logics", "abstract": "The development of machine learning in particular and artificial intelligent in general has been strongly conditioned by the lack of an appropriate interface layer between deduction, abduction and induction. In this work we extend traditional algebraic specification methods in this direction. Here we assume that such interface for AI emerges from an adequate Neural-Symbolic integration. This integration is made for universe of discourse described on a Topos governed by a many-valued {\\L}ukasiewicz logic. Sentences are integrated in a symbolic knowledge base describing the problem domain, codified using a graphic-based language, wherein every logic connective is defined by a neuron in an artificial network. This allows the integration of first-order formulas into a network architecture as background knowledge, and simplifies symbolic rule extraction from trained networks. For the train of such neural networks we changed the Levenderg-Marquardt algorithm, restricting the knowledge dissemination in the network structure using soft crystallization. This procedure reduces neural network plasticity without drastically damaging the learning performance, allowing the emergence of symbolic patterns. This makes the descriptive power of produced neural networks similar to the descriptive power of {\\L}ukasiewicz logic language, reducing the information lost on translation between symbolic and connectionist structures. We tested this method on the extraction of knowledge from specified structures. For it, we present the notion of fuzzy state automata, and we use automata behaviour to infer its structure. We use this type of automata on the generation of models for relations specified as symbolic background knowledge.", "histories": [["v1", "Mon, 11 Apr 2016 03:23:21 GMT  (35kb)", "http://arxiv.org/abs/1604.02780v1", "38 pages"]], "COMMENTS": "38 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["carlos leandro"], "accepted": false, "id": "1604.02780"}, "pdf": {"name": "1604.02780.pdf", "metadata": {"source": "CRF", "title": "KNOWLEDGE EXTRACTION AND KNOWLEDGE INTEGRATION GOVERNED BY LUKASIEWICZ LOGICS", "authors": ["Carlos Leandro"], "emails": ["miguel.melro.leandro@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n02 78\n0v 1\n[ cs\n.A I]\n1 1\nA pr\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013"}, {"heading": "1 INTRODUCTION", "text": "Category Theory generalized the use of graphic language to specify structures and properties through diagrams. These categorical techniques provide powerful\n2 tools for formal specification, structuring, model construction, and formal verification for a wide range of systems, presented on a grate variety of papers. The data specification requires finite, effective and comprehensive presentation of complete structures, this type of methodology was explored on Category Theory for algebraic specification by Ehresmann[6]. He developed sketches as a specification methodology of mathematical structures and presented it as an alternative to the string-based specification employed in mathematical logic. The functional semantic of sketches is sound in the informal sense that it preserves by definition the structure given in the sketch. Sketch specification enjoy a unique combination of rigour, expressiveness and comprehensibility. They can be used for data modelling, process modelling and meta-data modelling as well thus providing a unified specification framework for system modelling. For our goal we extend the syntax of sketch to multi-graphs and define its models on the Topos (see e.g. for definition [7]), defined by relation evaluated in a many-valued logic. We named specification system too our version of Ehresmanns sketch, and on its definition we developed a conservative extension to the notions of commutative diagram, limit and colimit for many-valued logic.\nIn this work, we use background knowledge about a problem to specify its domain structures. This type of information is assumed to be vague or uncertain, and described using multi-diagrams. We simplify the exposition and presentation of this notions using a string-based codification, for this type of multi-diagrams, named relational specification. We use this description for presenting structures extracted from data and on its integration.\nThere are essentially two representation paradigms to represent the extracted information, usually taken very differently. On one hand, symbolic-based descriptions are specified through a grammar that has fairly clear semantics. On the other hand, the usual way to see information presented using a connectionist description is its codification on a neural network (NN). Artificial NNs, in principle, combine the ability to learn and robustness or insensitivity to perturbations of input data. NNs are usually taken as black boxes, thereby providing little insight into how the information is codified. It is natural to seek a synergy integrating the white-box character of symbolic base representation and the learning power of artificial neuronal networks. Such neuro-symbolic models are currently a very active area of research. In the context of classic logic see [8] [9] [10], for the extraction of logic programs from trained networks. For the extraction of modal and temporal logic programs see [11] and [3]. In [12] we can find processes to generate connectionist representation of multi-valued logic programs and for Lukasiewicz logic programs ( LL) [?].\nOur approach to the generation of neuro-symbolic models uses Lukasiewicz logic. This type of many-valued logic has a very useful property motivated by the \u201dlinearity\u201d of logic connectives. Every logic connective can be defined by a neuron in an artificial network having, by activation function, the identity truncated to zero and one [13]. This allows the direct codification of formulas into network architecture, and simplifies the extraction of rules. Multilayer feedforward NN, having this type of activation function, can be trained efficiently\n3 using the Levenderg-Marquardt (LM) algorithm [5], and the generated network can be simplified using the \u201dOptimal Brain Surgeon\u201d algorithm proposed by B. Hassibi, D. G. Stork and G.J. Stork [14].\nWe combine specification system and the injection of information extracted, on the specification, in the context of structures generated using a fuzzy automata. This type of automata are presented as simple process to generate uncertain structures. They are used to describe an example: where the generated data is stored in a specified structure and where we apply the extraction methodology, using different views of the data, to find new insights about the data. This symbolic knowledge is inject in the specification improving the available description about the data. In this sense we see the specification system as a knowledge base about the problem domain ."}, {"heading": "2 PRELIMINARIES", "text": "In this section, we present some concepts that will be used throughout the paper."}, {"heading": "2.1 Lukasiewicz logics", "text": "Classical propositional logic is one of the earliest formal systems of logic. The algebraic semantics of this logic are given by Boolean algebra. Both, the logic and the algebraic semantics have been generalized in many directions [15]. The generalization of Boolean algebra can be based in the relationship between conjunction and implication given by (x \u2227 y) \u2264 z \u21d4 x \u2264 (y \u2192 z). These equivalences, called residuation equivalences, imply the properties of logic operators in Boolean algebras.\nIn applications of fuzzy logic, the properties of Boolean conjunction are too rigid, hence it is extended a new binary connective, \u2297, which is usually called fusion, and the residuation equivalence (x \u2297 y) \u2264 z \u21d4 x \u2264 (y \u21d2 z) defines implication.\nThese two operators induce a structure of residuated poset on a partially ordered set of truth values P [15]. This structure has been used in the definition of many types of logics. If P has more than two values, the associated logics are called a many-valued logics.\nWe focused our attention on many-valued logics having a subset of interval P = [0, 1] as set of truth values. In this type of logics the fusion operator \u2297 is known as a t -norm. In [16], it is described as a binary operator defined in [0, 1] commutative and associative, non-decreasing in both arguments, 1\u2297 x = x and 0\u2297 x = 0.\nAn example of a continuous t-norms is x \u2297 y = max(0, x + y \u2212 1), named Lukasiewicz t-norm, used on definition of Lukasiewicz logic ( LL)[17].\nSentences in LL are, as usually, built from a (countable) set of propositional variables, a conjunction \u2297 (the fusion operator), an implication \u21d2, and the truth constant 0. Further connectives are defined as: \u00ac\u03d51 is \u03d51 \u21d2 0, 1 is 0 \u21d2 0 and \u03d51 \u2295 \u03d52 is \u00ac\u03d51 \u21d2 \u03d52. The interpretation for a well-formed formula \u03d5 in\n4 Llogic is defined inductively, as usual, assigning a truth value to each propositional variable.\nThe Lukasiewicz fusion operator x\u2297y = max(0, x+y\u22121), its residue x\u2297y = min(1, 1\u2212x+y), and the lattice operators x\u2228y = max{x, y} and x\u2227y = minx, y, defined in \u2126 = [0, 1] a structure of resituated lattice [15] since:\n1. (\u2126,\u2297, 1) is a commutative monoid 2. (\u2126,\u2228,\u2227, 0, 1) is a bounded lattice, and 3. the residuation property holds,\nfor all x, y, z \u2208 \u2126, x \u2264 y \u21d2 z iff x\u2297 y \u2264 z.\nThis structure is divisible, x \u2227 y = x \u2297 (x \u21d2 y), and \u00ac\u00acx = x. Structures with this characteristics are usually called MV-algebras [?].\nHowever truth table f\u03d5 is a continuous structure, for our computational goal, it must be discretized, ensuring sufficient information to describe the original formula. A truth table f\u03d5 for a formula \u03d5, in LL, is a map f\u03d5 : [0, 1]\nm \u2192 [0, 1], where m is the number of propositional variables used in \u03d5. For each integer n > 0, let Sn be the set {0, 1 n , . . . , n\u22121 n , 1}. Each n > 0, defines a sub-table for f\u03d5 defined by f (n) \u03d5 : (Sn) m \u2192 Sm, given by f (n) \u03d5 (v\u0304) = f\u03d5(v\u0304), and called the \u03d5 (n+1)-valued truth sub-table. Since Sn is closed for the logic connectives defined in LL, we define a (n+1)-valued Lukasiewicz logic (n- LL), as the fragment of LL having by truth values \u2126 = Sn. On the following we generic call them \u201da LL\u201d.\nFuzzy logics, like LL, deals with degree of truth and its logic connectives are functional, whereas probability theory (or any probabilistic logic) deals with degrees of degrees of uncertainty and its connectives aren\u2019t functional. If we take two sentence from L the language of LL, \u03d5 and \u03c8, for any probability defined in L we have P (\u03c6\u2295 \u03d5) = P (\u03c6)\u2295 P (\u03d5) if \u00ac(\u03c6\u2297 \u03d5) is a boolean tautology, however for a valuation v on L we have v(\u03c6 \u2295 \u03d5) = v(\u03c6) \u2295 v(\u03d5). The divisibility in \u2126, is usually taken as a fuzzy modus ponens of LL, \u03d5, \u03d5 \u2192 \u03c8 \u22a2 \u03c8, where v(\u03c8) = v(\u03d5) \u2297 v(\u03c8). This inference is known to preserve lower formals of probability, P (\u03c6) \u2265 x and P (\u03d5 \u2192 \u03c8) \u2265 y then P (\u03c8) \u2265 x \u2297 y. Petr Ha\u0301jek presented in [18] extends this principle by embedding probabilistic logic in LL, for this we associated to each boolean formula \u03d5 a fuzzy proposition \u201d\u03d5 is provable\u201d. This is a new propositional variable on LL, where P (\u03d5) is now taken to be its degree of truth.\nWe assume in our work what the involved entities or concepts on a UoD can be described, or characterize, through fuzzy relations and the information associated to them can be presented or approximated using sentences on LL. In next section we describe this type of relations in the context of allegory theory [19]."}, {"heading": "2.2 Relations", "text": "A vague relation R defined between a family of sets (Ai)i\u2208Att, and evaluated on \u2126, is a map R : \u220f\ni\u2208Att Ai \u2192 \u2126. Here we assume that \u2126 is the set of truth\n5 values for a LL. In this case we named Att the set of attributes, where each index \u03b1 \u2208 Att, is called an attribute and the set indexed by i, Ai, represents the set of possible values for that attribute on the relation or its domain. In relation R every instance x\u0304 \u2208 \u220f\ni\u2208Att Ai, have associated a level of uncertainty, given by R(x\u0304), and interpreted as the truth value of proposition x\u0304 \u2208 R, in \u2126.\nEvery partition Atti \u222a Atta \u222a Atto = Att, where the sets of attributes Atti, Atta and Atto are disjoint, define a relation\nG : \u220f\ni\u2208Atti\nAi \u00d7 \u220f\ni\u2208Atto\nAi \u2192 \u2126,\nby\nG(x\u0304, z\u0304) = \u2295\ny\u0304\u2208 \u220f\ni\u2208Atta Ai\nR(x\u0304, y\u0304, z\u0304),\nand denoted by G : \u220f\ni\u2208Atti Ai \u21c0\n\u220f\ni\u2208Atto Ai, this type of relation we call a\nview for R. For each partition Atti \u222a Atta \u222a Atto = Att define a view G for R, where Atti and Atto are called, respectively, the set of G inputs and the set of its outputs. This sets are denoted, on the following, by I(G) and O(G). Graphically a view\nG:A0\u00d7A1\u00d7A2\u21c0A3\u00d7A4\u00d7A5 ,\ncan be presented by multi-arrow on figure 1.\nA view S : A \u21c0 A is called a similarity relation is\n1. S(x\u0304, x\u0304) = 1 (reflexivity),\n2. S(x\u0304, y\u0304) = S(y\u0304, x\u0304) (symmetry), and\n3. S(x\u0304, y\u0304)\u2297 S(y\u0304, z\u0304) \u2264 S(x\u0304, z\u0304) (transitivity).\nWe use Greek lets for similarity relation relations, and if \u03b1 : A \u21c0 A is a similarity relation we write \u03b1 : A, and call to A the support for similarity \u03b1. The similarity using \u03b1 between to elements x\u0304 and y\u0304 is denoted by [x\u0304 = y\u0304]\u03b1 to mean \u03b1(x\u0304, y\u0304).\nWe see a similarity relation \u03b1 : A as a way to encoded fuzzy sets in LL. We do this interpreting, for x\u0304 \u2208 A, its diagonal [x\u0304, x\u0304]\u03b1 as the degree of true for proposition x\u0304 \u2208 \u03b1. Given two elements in the support set x\u0304, y\u0304 \u2208 A, we interpret [x\u0304, y\u0304]\u03b1 as the degree of true for proposition x\u0304 = y\u0304 in \u03b1. This offer us a way to evaluate the equality and de membership relation on the LL.\n6 Let \u2126-Set be the class of views defined by relations evaluated in \u2126. We define a monoidal structure in \u2126-Set for every pair of views\nG : \u220f\ni\u2208I(G)\nAi \u21c0 \u220f\ni\u2208O(G)\nAi and R : \u220f\ni\u2208I(R)\nAi \u21c0 \u220f\ni\u2208O(R)\nAi\nwe define,\nR\u2297G : \u220f\ni\u2208I(G)\u222aI(R)\\O(G)\nAi \u21c0 \u220f\ni\u2208O(R)\u222aO(G)\\I(R)\nAi,\ngiven, for every\n(x\u0304, z\u0304) \u2208 \u220f\ni\u2208I(G)\u222aI(R)\\O(G)\nAi \u00d7 \u220f\ni\u2208O(R)\u222aO(G)\\I(R)\nAi,\nby\n(R\u2297G)(x\u0304, z\u0304) = \u2295\ny\u0304\u2208O(R)\u2229I(G)\n(R(x\u0304, y\u0304)\u2297 S(y\u0304, z\u0304)).\nWe call to this tensor product composition of view. This operation extends composition of functions: if relation G is a function between sets A and B, and if R is a function between sets B and A, then for this two views in \u2126-Set, G\u2297R is the function R \u25e6G.\nWhile composition between maps is a partial operator, it is defined only for componible maps, the tensor product \u2297 is total, it is defined for every pair of relations. In figure 2 for two multi-arrow R and G representing views such that I(R) = {A0, A1}, O(R) = {A2, A3, A4}, I(G) = {A2, A3}, and O(G) = {A5}, for the resulting view R\u2297G we have I(R\u2297G) = {A0, A1, A2} and O(R\u2297G) = {A4, A5}.\nIn \u2126-Set we denote by I : \u2217 \u2192 \u2126 the relation defined on a singleton set by I(\u2217) = 1. This relation is the identity for \u2297; R\u2297 I \u2248 I \u2297R \u2248 R.\nThe class \u2126-Set have a natural structure of category, having by objects \u2126sets and by morphisms view, such that R : \u03b1 \u2192 \u03b2 is a morphism from \u2126-set \u03b1 : A to \u03b2 : B if R : A \u21c0 B and\n7 1. \u03b1\u2297R \u2264 R, and 2. R\u2297 \u03b2 \u2264 R.\nNote that every object \u03b1 : A have by identity the relation \u03b1\u0304 : A \u21c0 A defined by reflexive the close of \u03b1, define making \u03b1\u0304(x\u0304, x\u0304) = 1.\nThe category \u2126-Set is a symmetric monoidal closed category [20], where the tensor product of \u2126-sets is given for \u03b1 : A and \u03b2 : B by\n\u03b1\u2297 \u03b2 : A\u00d7B\ndefined\n(\u03b1\u2297 \u03b2)(a, b) = \u03b1(a) \u2297 \u03b2(b).\nThis can be used to describe a functor\n\u03b1\u2297 : \u2126 \u2212 Set \u2192 \u2126 \u2212 Set,\ngiven for morphisms R : \u03b3 \u21c0 \u03b2, with support f : A \u21c0 B, by\n\u03b1\u2297 f : \u03b1\u2297 \u03b3 \u21c0 \u03b1\u2297 \u03b2\nhaving by support \u03b1\u2297 f : A\u00d7 C \u21c0 A\u00d7B, described by\n(\u03b1\u2297 f)(a, c, a\u2032, b) = \u03b1(a, a\u2032)\u2297 f(c, b).\nFunctor \u03b1\u2297 have by left adjunct a functor\n\u03b1 \u22b8 : \u2126-Set \u2192 \u2126-Set,\ndefined for \u2126-sets \u03b2 : B by\n\u03b1 \u22b8 \u03b2 : [A,B],\nconstruct as the internalization for \u2126-set Hom [21]\n(\u03b1 \u22b8 \u03b2)(t, h) = \u2228\nb0,b1\n\u2295\na\n(\u03b1(a, a) \u2297 t(a, b0)\u2297 h(a, b1)\u2297 \u03b2(b0, b1)),\nfor relations f : \u03b3 \u21c0 \u03b2, with support f : C \u21c0 D, we have\n(\u03b1 \u22b8 f) : (\u03b1 \u22b8 \u03b3) \u21c0 (\u03b1 \u22b8 \u03b2),\na relation with support \u03b1 \u22b8 f : [A,C] \u21c0 [A,B], described by\n(\u03b1 \u22b8 f)(h, g)(a, c, a\u2032, b) = h(a, c)\u2297 g(a\u2032, b)\u2297 \u03b1(a, a\u2032).\nThis adjunction \u03b1\u2297 \u22a2 \u03b1 \u22b8 have by unit [20] the natural transformation, \u03bb defined for each \u2126-set \u03b3 : C, by a multi-morphism\n\u03bb\u03b3 : (\u03b1 \u22b8 \u03b3)\u2297 \u03b1 \u21c0 \u03b3,\n8 with support \u03bb\u03b3 : [A,C]\u00d7A \u21c0 C, by\n\u03bb\u03b3(h, a, b) = h(a, b),\nthe relation h evaluation evaluated in (a, b) \u2208 A\u00d7B. The \u03b1 \u22b8 \u03b2 : [A,B] reflexive closure defines a similarity relation in [A,B], we use this relation in the following to quantify the similarity between relation form S and B, and we call them power similarity relation. In the follow we use this relation to compare models or on the quantification of model quality.\nTwo views R and G, in \u2126-Set, are called independents if R \u2297 G = G \u2297 R. By this we mean what the R output not depend on G inputs and the G output not depend on R input. Given a view R : A \u21c0 B, we define projections RA : B \u2192 \u2126 and RB : B \u2192 \u2126, respectively, by RA(b\u0304) = \u2295\na\u0304\u2208A R(a\u0304, b\u0304) and RB(a\u0304) = \u2295\nb\u0304\u2208B R(a\u0304, b\u0304). In the following we used R(a\u0304, ) to denote the relation defined from R by fixing a input vector a\u0304 \u2208 A, R(a\u0304, )(b\u0304) = R(a\u0304, b\u0304)."}, {"heading": "2.3 Inference", "text": "Generically inference is a process used to generate now facts based on known facts. On the context of multi-valued logic, the inference allows fining the degree of two for a new proposition based on the known degree of truth for propositions [?]. This inference can be described using the composition operator defined in \u2126-Set [?].The syllogism describe by the rule:\nR: If a \u2208 \u03b1 then b \u2208 \u03b2 S: If b \u2208 \u03b2 then c \u2208 \u03b3"}, {"heading": "R\u2297 S: If a \u2208 \u03b1 then c \u2208 \u03b3", "text": "This rule is interpreted saying that: If\n1. R(a, b) \u2265 ([a]\u03b1 \u21d2 [b]\u03b2), and 2. S(b, c) \u2265 ([b]\u03b2 \u21d2 [c]\u03b3)\nthen\n(R \u2297 S)(a, c) \u2265 [a]\u03b1 \u21d2 [c]\u03b3 .\nThis gives us a lower bond for degree of truth. However this strategy works better on the version of Modus Ponens :\nR: a \u2208 \u03b1 S: If a \u2208 \u03b1 then b \u2208 \u03b2 R\u2297 S: a \u2208 \u03b1 \u2227 b \u2208 \u03b2\nSince LL is a divisible logic we can write:\n(R\u2297 S)(a, b) = R(a, a)\u2297 S(a, b) = [a]\u03b1 \u2297 ([a]\u03b1 \u21d2 [b]\u03b2) = [a]\u03b1 \u2227 [b]\u03b2\n9 Applying this rule to a simple relation H : \u03b1 \u2192 \u03b2, we have\nR: a \u2208 \u03b1 S: If a \u2208 \u03b1 then (a, b) \u2208 H R\u2297 S: a \u2208 \u03b1 \u2227 (a, b) \u2208 H\nsince [a]\u03b1 = \u2295\nb H(a, b), the degree of truth of a \u2208 \u03b1 \u2227 (a, b) \u2208 H is the degree of truth for (a, b) \u2208 H , then:\n[a]\u03b1 \u2297 ([a]\u03b1 \u21d2 H(a, b)) = H(a, b). (1)\nWe simplified this excretion defining\nH(\u03b2|a)(b) = [a]\u03b1 \u21d2 H(a, b), (2)\nand we write [a]\u03b1 \u2297H(\u03b2|a)(b) = H(a, b). (3)\nIn this contextH(\u03b2|a)(b) is interpreted as the degree of truth for the proposition:\n\u201dA class associated by relation H is b, if its input is a\u201d,\ngiven by the result for the evaluation of (a, b) by H , conditionated to the degree belonging of a on f input domain. In classic logic, when A is finite, this is express by \u2200a \u2208 \u03b1 : H(a, b).\nProposition 1 (Bayes Rule on LL) Given a faithful view R : A \u21c0 B, and a\u0304 \u2208 A and b\u0304 \u2208 B from \u2126-Set. The equations\nR(a\u0304)B \u2297 x = R(a\u0304, ) and R(b\u0304)A \u2297 x = R( , b\u0304),\nhave by solution, relation R( |a\u0304) and R( |b\u0304), respectively, defined by R( |a\u0304) = R(a\u0304)B \u21d2 R(a\u0304, ) and R( |b\u0304) = R(b\u0304)A \u21d2 R( , b\u0304).\nWe use this rule to solve inference problems in \u2126-Set. Given two compatible views R : A \u21c0 B and G : B \u21c0 C, i.e. such that the output attributes for view R are the input attributes for G. For observable descriptions a\u0304 \u2208 A and c\u0304 \u2208 C, we have\n10\nR(a\u0304)B \u2297 (R \u2297G)( |a\u0304)(c\u0304) = (R \u2297G)(a\u0304, c\u0304) = \u2295\nb\u0304 R(a\u0304, b\u0304)\u2297 S(b\u0304, c\u0304) = \u2295\nb\u0304 R(a\u0304)B \u2297R( |a\u0304)(b\u0304)\u2297G(b\u0304, c\u0304),\nthen\n(R \u2297G)( |a\u0304)(c\u0304) = R(a\u0304)B \u21d2 (R(a\u0304)B \u2297 \u2295\nb\u0304\nR( |a)(b)\u2297 S(b, c)),\ni.e.\n(R\u2297G)( |a\u0304) = \u2295\nb\u0304\nR( |a\u0304)(b\u0304)\u2297 S(b\u0304, ).\nWhen views R : A \u21c0 C and G : B \u21c0 D are independent we have\n(R \u2297G)( |a\u0304, b\u0304)(c\u0304, d\u0304) = R( |a\u0304)(c)\u2297 S( |b\u0304)(d\u0304).\nNaturally, if C = D we write (R \u2297G)( |a\u0304, b\u0304)(d\u0304) for (R\u2297G)( |a\u0304, b\u0304)(d\u0304, d\u0304)."}, {"heading": "2.4 Limit sentences and colimit sentences", "text": "A multi-arrow defines a link between a set of input nodes and a set of output nodes, we can see an example of this on figure 1. We can use multi-arrows to generalize the notion of arrow in a graph. This allows the definition of a multigraph as a set of nodes linked together using multi-arrows. Examples of multigraphs can be seen on figures 2 and 4. A multi-diagram in \u2126-Set, defined having by support a multi-graph G, is a multi-graph homomorphism D : G \u2192 \u2126-Set, where each node in G is mapped to a \u2126-set \u03b1 : A, and each multi-arrows in G is mapped to a relation view. In this sense, every set of views in \u2126-Set defines a multi-diagram, having by support the multi-graph where the selected views are multi-arrows, and the \u2126-set used on this views as nodes.\nThe classically definition of limit for a diagram, in the category of sets, can been as a way to internalize the structure of a diagram in form of a table [20]. Given a diagram D : G \u2192 Set with vertices V = {ai}i\u2208I and arrows A = {fj}j\u2208J , its limit is a table or a subset of the cartesian product \u220f\ni\u2208I D(ai) given by\nLim D = {(. . . , xi, . . . , xj , . . .) \u2208 \u220f\ni\nD(ai) : \u2200f :ai\u2192ajD(f)(xi) = xj}. (4)\nwere the relation is evaluated on classic logic. We present as limit for a multi-diagram D : G \u2192 \u2126-Set a conservative extension from the classical limit definition. Let D : G \u2192 \u2126-Set be a multidiagram with vertices (vi)i\u2208L. The limit of diagram D is a relation denoted by Lim D, and defined as\nLim D : \u220f\ni\u2208L\nD(vi) \u2192 \u2126,\n11\nsuch that\n(Lim D)(. . . , xi, . . . , xj , . . .) = \u2297\nf :vi\u21c0vj\u2208G\nD(f)(xi, xj).\nThe limit for multi-diagram on figure 4 is the relation Lim D : A0\u00d7. . .\u00d7A5 \u2192 \u2126 given for every (a0, . . . , a5) \u2208 A0 \u00d7 . . .\u00d7A5 by\nIn this sense for parallel views R,S : X \u21c0 Y , they define a multi-diagram, and its limit is the relation\nLim(R = S) : X \u00d7 Y \u2192 \u2126,\ngiven by\nLim(R = S)(x, y) = R(x, y)\u2297 S(x, y).\nThis relation is denoted by [R = S] and usually called, on Classic logic, R and S equalizer. If R : X \u21c0 U and S : Y \u21c0 U are views its pullback, denoted by R\u2297U S is defined by the limit\nLim(R\u2297U S) : X \u00d7 U \u00d7 Y \u2192 \u2126,\ngiven by Lim(R\u2297U S)(x, u, y) = R(x, u)\u2297 S(y, u).\nGiven a family of views, having the some output, (Ri : Xi \u21c0 U)i\u2208L, its widepullback is the relation Lim(\u2297URi) : \u2297i\u2208LRi.\nDefinition 1 (\u03bb-Limit) A relation R described in LL, is the \u03bb-limit for a multi-diagram D if R : A \u2192 \u2126 is \u03bb-similar to Lim D : A \u2192 \u2126, i.e if\n(R \u22b8 Lim D) \u2265 \u03bb, (5)\nwhen this is the case we write\nR = Lim\u03bb D. (6)\n12\nWe used the definition of limit to extend the notion of commutative diagram. The idea was to characterize a commutative diagrams using its internalization on a table.\nDefinition 2 (Commutativity ) If D : G \u2192 \u2126-Set is a multi-diagram with vertices in V , and associated \u2126-sets (\u03b1i)i\u2208V , where we selected a set s(D) of input vertices. Assuming that the sub-graph of G defined by vertices s(D) is acyclic and that P is the Cartesian product defined by each vertices on D with not belong to s(D).\nThe multi-diagram D is commutative with inputs if s(D) if\n\u2228\nn\u0304\u2208P\n(Lim D)(s\u0304, n\u0304) = \u2228\nn\u0304\u2208V\n( \u2297\ni\n\u03b1i)(s\u0304, n\u0304), (7)\nfor every s\u0304 \u2208 \u220f\ni\u2208s(D) D(i). A diagram is \u03bb-commutative if\n(\n\u2228\nn\u0304\u2208V\n(Lim D)( , n\u0304) \u22b8 \u2228\nn\u0304\u2208V\n( \u220f\ni\nD(i))( , n\u0304)\n)\n\u2265 \u03bb, (8)\nLimits, colimits and commutativity can be used on the specification of structures [2]. We use the conservative extensions to this notions for the detrition of fuzzy structures. However the notion of colimit is more difficult to present generically. The construction of a colimit reduces to that of two coproducts and a coequalizer, siting [20], in the category of sets governed by classic logic the explicit description of a coequalizer is generically very technical since it involves the description of the equivalence relation generated by a family of pairs. This complexity is incased when we extend this notion to relations evaluated on multi-valued logics. We present bellow two examples.\nThe coproduct of \u2126-sets \u03b1 : A and \u03b2 : B is a relation R having by support set A \u2210 B given by\nR(a, a\u2032) = [a = a\u2032]\u03b1 \u2295 [a = a \u2032]\u03b2 . (9)\nWhere, for simplicity, we assume what relations \u03b1 and \u03b2 assume the value 0 when are evaluating pairs outside its support sets. We denote the coproduct for \u03b1 : A and \u03b2 : B by \u03b1\u2295 \u03b2 : A \u2210\nB. The diagram defined by a parallel pair of multi-morphisms f : \u03b1 : A \u2192 \u03b2 : B\nand g : \u03b1 : A \u2192 \u03b2 : B have by colimite a \u2126-set, with support A \u2210 B, given by\nR(a, a\u2032) =\n\u2295\nb,b\u2032\u2208B f(a, b)\u2295 f(a \u2032, b\u2032)\u2295 [b = b\u2032]B\n\u2295 \u2295 b,b\u2032\u2208B g(a, b)\u2295 g(a \u2032, b\u2032)\u2295 [b = b\u2032]B \u2295 \u2295\nb,b\u2032\u2208A f(b, a)\u2295 g(b \u2032, a\u2032)\u2295 [b = b\u2032]A\n\u2295 [a = a\u2032]A \u2295 [a = a\u2032]B\n,\nwhere a, a\u2032 \u2208 A \u2210\nB, for simplicity, we assume what relations \u03b1 and \u03b2 assume the value 0 when are evaluated on pairs outside its support sets.\n13"}, {"heading": "2.5 Concepts", "text": "We describe a table or a concept using relation views. A table or a concept description using values in the family (A\u03b1)\u03b1\u2208Att, for attributes Att, is a view R : O \u21c0 \u2210\n\u03b1\u2208Att A\u03b1, where O is a set of keys identifying concept instances. We use R(o, \u03b1 = x) = \u03bb to denote that, in instance o \u2208 O, the uncertainty of an attribute \u03b1 to be equal to value x \u2208 A\u03b1 is \u03bb. This mean that, in an instance, an attribute may assume different values, associated with different uncertain levels expressed by truth values. When we have R(o, \u03b1 = x) \u2265 \u03bb, for every entity o \u2208 O, we write R(\u03b1 = x) \u2265 \u03bb or just \u03b1 \u223c\u03bb x in R.\nA concept description have different presentations, corresponding to each of the perspectives taken to data. Each partition Att = V \u222aU , defines a perspective through the view RV,U :O\u00d7 \u220f \u03b1\u2208V A\u03b1\u21c0 \u2210 \u03b1\u2208U A\u03b1 , given by\nRV,U (o,\u03b1=x,y)= \u2297 \u03b1\u2208V,x\u2208A\u03b1 R(o,\u03b1=x)\u2297R(o,y),\nwhere \u03b1 = x abbreviates the tuple defined using family (\u03b1 = x)\u03b1\u2208V,x\u2208A\u03b1 . Relation between information on a data set can be defined as a diagram D, from a multi-graph G to Set(\u2126), where each multi-arrow is mapped to a view of a concept description. Every multi-graph homomorphism I : I \u2192 G defines a query in the structure D, having by answer the concept description defined by Lim D\u25e6I. Where D\u25e6I denotes the composition between graph homomorphisms.\nIf we assume that \u2126 = [0, 1], given a pair of concept presentations defined using a finite set of keys O,\nR0,R1:O\u00d7 \u220f \u03b1\u2208V A\u03b1\u21c0 \u2210 \u03b1\u2208U A\u03b1 ,\nwe measure the similarity between this two views using relation\n\u0393 (R0, R1) = e \u2212 1 |O|\n\u2211\nx\u0304\u2208 \u220f \u03b1\u2208Att A\u03b1 \u00ac(R0(x\u0304)\u21d4R1(x\u0304)),\nwhere |O| is the number of keys. Relation \u0393 is a similarity relation between pairs of concept described using a tuple in \u220f\n\u03b1\u2208Att A\u03b1 since:\n1. \u0393 (R0, R0) = 1 (reflexivity), 2. \u0393 (R0, R1) = \u0393 (R1, R0) (symmetry), and 3. \u0393 (R0, R1)\u2297 \u0393 (R1, R2) \u2264 \u0393 (R0, R2) (transitivity).\nThe transitivity is a consequence of, in any ML-algebra \u2126, for all \u03bb0, \u03bb1, \u03bb2 \u2208 \u2126,(\u03bb0 \u21d2 \u03bb1)\u2297 (\u03bb1 \u21d2 \u03bb2) \u2264 \u03bb0 \u21d2 \u03bb2. When \u0393 (R0, R1) = \u03bb we write R0 \u223c\u03bb R1 and we say that, R0 is \u03bb-similar to R1, for R0 \u223c1 R1 we write R0 = R1.\nWe named this similarity measure of exponential similarity. In the literature we can find other measures for relation similarity measurement. Like the infsimilarity, used in Possibilistic logic,\n\u0393 (R0,R1)= \u2227\nx\u0304\u2208 \u220f \u03b1\u2208Att A\u03b1 (R0(x\u0304)\u21d4R1(x\u0304)),\nor the and-similarity, used in Boolean logic,\n\u0393 (R0,R1)= \u2297\nx\u0304\u2208 \u220f \u03b1\u2208Att A\u03b1 (R0(x\u0304)\u21d4R1(x\u0304)),\n14\nhowever these relations are to crispy for model evaluation. We need to be able to quantify who models are similar to a concept description.\nThis notion is fundamental to make fuzzy some key concepts of relational algebra, useful on data structure specification. Example of this is the description of a \u201dis a\u201d relation evaluated on multi-valued logic. For that, let R : A \u21c0 B be a concept description, here we assume the existence of a similarity \u0393A defined in A. The concept description R defines a \u201dis a\u201d relation, for similarity \u0393A, if sentence\nR(a0,b)\u2297R(a1,b)\u21d2\u0393A(a0,a1),\nhave by truth-value 1, for every a0, a1 \u2208 A and every b \u2208 B, i.e.\n\u2297\na0,a1\u2208A\n\u2297\nb\u2208B(R(a0,b)\u2297R(a1,b)\u21d2\u0393A(a0,a1))=1.\nIn this sense we call to view R a mono-view or a clustering.\nViews R : A \u21c0 B such that \u2297 b\u2208B \u2295 a\u2208A R(a,b) = 1, are called epi-views."}, {"heading": "3 SPECIFICATION SYSTEM", "text": "All the widely used data specification mechanisms (like Entity Relationship Model [22], the Fundamental Data Model [23], the Generic Semantic Model [24]), OOA&D-schemas in a million of versions and UML which itself comprises a host of various notations, have a strong graphical component. They are essentially graphs with special markers in them. Usually the semantics of these markers is defined in an ad-hoc and sometimes non-formal way. An important component of the mathematical structure that will be used to formalized knowledge, are multi-graph homomorphisms into the class Set(\u2126) of relations views. When specifying an information system, it will be necessary to formulate constrains on such graph homomorphisms. A identical notion of a specification whose models are graphs homomorphisms into a category theory are known on the category community under the name of sketch. Sketches where invented by Charles Ehresmann and can be perceived as a graphic based logic, which formalizes in a precise and uniform way the semantic of graph with marks [25]. For our propose we extended Ehresmann\u2019s sketch to formalize a graphic based fuzzy logic. We named, this mathematical structure, specification system. On information specification, specification system will be used to specify finite fuzzy structures. Semantic data specification have been used for may years in the early stages of database design, and they have become key ingredients of object-oriented software. The goal of a semantic data specification is to build a mathematical abstraction of a small part of the real word. This small part of the word is usually called the universe of discourse (UoD) in the database literature. The models of the data specification are possible states of the UoD, and will be the structures stored in an information system. The mathematical structure that will be used to describe the UoD are finite models of specification systems together with a labeling of all the elements of these models.\n15\nHence a data specification will consist of two parts. The first part will be a specification system, and it describes the fuzzy structure and the interdependencies of the various entities about which we want to store information. The second part indicates what kind of information we want to store about each type of entity: for each type of entity, we give its set of possible attribute values, i.e. the set of all possible labels that an entity of the given type can have. The structure defined, by specification system S and model M , we named the semiotic (S,M).\nBy a specification system S we understood as a structure S = (G, C, L, coL), where G is a multi-graph, C is a set of pairs (G, \u03bb), and L and coL are sets of tuples (f,G, i(G), o(G), \u03bb), such that f is a multi-arrow in G, D \u2282 G is a multi-graph, \u03bb \u2208 \u2126 and i(G) and o(G) are sets of nodes from G.\nGiven a specification system S = (G, C, L, coL) a model for S is a diagram M : G \u2192 Set(\u2126), mapping multi-arrows to concept description, such that:\n1. for every (G, \u03bb) \u2208 C, M(G) is \u03bb-commutative, 2. for every (f,G, i(G), o(G), \u03bb) \u2208 L, M(f) have by input M(i(G)) and by\noutput M(o(G)) and is \u03bb-equivalent to relation Lim M(G), and 3. for every (f,G, i(G), o(G), \u03bb) \u2208 L, M(f) have by input M(i(G)) and by\noutput M(o(G)) and is \u03bb-equivalent to relation coLim M(G).\nThe pair (S,M) defined by a model M : G \u2192 Set(\u2126) for a specification system S is called a semiotic, where the multi-graph G describes a library of components.\nA specification system S, if consistent, describes the fuzzy structure for a class of UoD. If the set of all models for specification system S is denoted by Mod(S), everyM \u2208 Mod(S) can be seen as a system state. Two statesM0,M1 \u2208 Mod(S) have similar structures and the specification S can be enriched with now knowledge to increase the dissimilarity between the states. The knowledge need to distinguish between M0 and M1 can be extracted querying the state M0, trying to find its particularities.\nFor that, every multi-graph homomorphism I : I \u2192 G defines a query to a model M : G \u2192 Set(\u2126) \u2208 Mod(S). And this query I have by answer the relation given by\nLim M \u25e6 I,\nand each of its views can be used as a data set, usable to feed a data mining processes, to extracted insights about the model."}, {"heading": "3.1 Knowledge integration via specification systems", "text": "Specification systems are graphic specifications formalisms (its components are multi-graphs and markers in these graphs) and can be taken as repositories of knowledge about the UoD. They are described using a rigorous graphic language with a precise semantic, where we have a methodology to querying its models. Your goal is the enrichment of this structure with knowledge extracted from one of its models. We simplify this process by expressing constraints of first-order logic formulae into a graphical marker.\n16\nTo be able to do that, we consider the multi-graph used on the definition of the specification system component library as a presentation of a first-order many-sorted signature: nodes of the graph are interpreted as data sorts, and multi-arrows are interpreted as relations evaluated in \u2126. A structure for this signature is exactly a multi-graph homomorphism from G to Set(\u2126). Every formula for this signature are particularly simple, since there are only relations. A formula R(x1, x2, . . . , xn) is defined through a multi-graph homomorphism R : I \u2192 G and its interpretation in model M , is the relation Lim M \u25e6 R. Where the interpretation of each initial and terminal node of the multi-diagram R defines the sorts for variables x1, x2, . . . , xn.\nEvery atomic formula is a relation R(x1, x2, . . . , xn). The finite conjunction of atomic formulas on variables x1, x2, . . . , xn, using relations R1, R2, . . . , Rm is denoted by: (R1 \u2297 R2 \u2297 . . . \u2297 Rm)(x1, x2, . . . , xn) and it is interpreted as the relation Lim M \u25e6 (R1 \u2297R2 \u2297 . . .\u2297Rm). In this sense, every disjunctive formula defined using relation can be interpreted as the limit of a finite diagram.\nThe translation in the other direction is also simple. Given a limit mark (f,G, i(G), o(G), \u03bb) in S, the meaning for sign f , is the interpretation of a disjunctive formula defined trough the interpretation of each multi-arrows used on the definition of multi-graph G having its interdependencies (gluing order in G) defined using variable repetition.\nSimilarly, every conjunctive formula defined using relation on semiotic (S,M) can be interpreted as the colimit of a finite diagram. For colimit makes (f,G, i(G), o(G), \u03bb) is S, the interpretation for f , can be defined as the interpretation for a conjunctive formula defined using each multi-arrow in G .\nGiven a semiotic (S,M) every formula \u03d5, extracted from model M using a query I, can be described by a set of limit marks and colimit marks. This allows the enrichment of specification system S, defining a new system S\u2032, such that Mod(S\u2032) \u2286 Mod(S) and M \u2208 Mod(S\u2032).\nHowever, it is known from [2] what where are structures specifiable using sketches but not in first-order logic. Then first-order logic have less expressive power as specification systems. This result allows to use a mixture of limits, colimits and formulas in the UoD specification and be sure that the resulting specification can always be translated to a specification system [25]."}, {"heading": "3.2 A specification system description", "text": "A specification system is by nature a graphic specification described, however, some times like in this exposition, it is preferable a description for the specification system using a string-based presentation. For this we used a string-based codification for specification systems named relational specification, generalizing the notion of essentially algebraic specification: the original idea goes back to Freyd [26] and having the same expressive power as finite limit sketches [27]. The essentially algebraic fragment, are interesting computer sciences mainly because theories, of many kinds of specification formalisms, are in fact initial algebras for some essentially algebraic specification. A number of proof systems for essentially algebraic specification have been introduced [28].\n17\nRecall that an ordering on a set X is well-founded iff every strictly decreasing chain of elements of X must be finite. A relational specification consists of:\n1. A set of sorts. 2. A set of relational signs, with a well-founded ordering (called a library of\ncomponents). 3. A set of diagrams defined using relational views, with a well-founded ordering\n(called a a set of diagrams). 4. A set of condition build on relational signs and diagrams.\nEvery relation sign or view sign \u03c9 have an arity and a set Def(\u03c9) of relations with the same arity as w, called the set of domain condition.\nFor our propose sorts are nominal and finite, and we list its possible values by writing A : {a1, a2, . . . , at},. These are the basic structures for UoD specification, used on the description of relations and views. Every relation have an arity described by a list of data sorts. We denote a relation symbol R, with arity the list of sorts A1, A2, . . . , An, as R : A1, A2, . . . , An. Interpreted as a concept description M(R) : O \u21c0 \u2210\ni Ai. A view of R having by source arity the list A1, A2, . . . , An and by target arity\nB1, B2, . . . , Bm is denoted by:\nR : { A1, A2, . . . , An \u21c0 B1, B2, . . . , Bm;\n}\nThese are interpreted as concept descriptions M(R) : O \u00d7 \u220f iAi \u21c0 \u2210\nj Bj . The relationship between relations and views may be defined using marked diagrams. A diagram is described by a set of views where we marked some of its sorts as input or output sort. We write\nD : { A1, A2, . . . , An \u21c0 B1, B2, . . . , Bm; D : D1 \u2297 D2 \u2297 . . . \u2297 Dk;\n}\nwhere (Di)i\u2208I is a list of diagram signs, what are smaller than D, describing the proper sequence of gluing to form D. Note what, every view can be seen as a diagram with only one multi-arrow. The commutativity or \u03bb-commutativity for a diagram D, for view G, in the model, is denote, respectively, by G : [D] or G : [D]\u03bb. When the interpretation for a relation or view G must satisfy a limit or a colimit, we writing G:lim D, G:\u03bb\u2212lim D, G:colim D, or G:\u03bb\u2212colim D. In specification:\nG : { A1, A2, . . . , An \u21c0 B1, B2, . . . , Bm; D : { A1, A2, . . . , An \u21c0 B1, B2, . . . , Bm;\nD : D1 \u2297 D2 \u2297 . . . \u2297 Dk; }\nG : \u03bb0-lim D; D\u2032 : { A1, A2, . . . , An \u21c0 B1, B2, . . . , Bm;\nD\u2032 : D\u20321 \u2297 D \u2032 2 \u2297 . . . \u2297 D \u2032 k;\n}\nG:[D\u2032]\u03bb1 ;\n}\nevery interpretation for view G must be \u03bb0-similar to Lim D, there D is diagram defined gluing diagrams or multi-arrows (Di)i\u2208I , and must transform D\n\u2032 is a \u03bb0commutative diagram. However, this type of properties are very generic and some\n18\ntimes difficult to understand. When we want to be more specific, we describe some of the relations or views properties using first-order formulas. We will express internal properties on a relation or view G using formulas, with the same arity as G, and defined using only signs that are smaller than G in the library of component. For it we write,\nG := { A1, A2, . . . , An; G(x1, x2, . . . , xn) : P(R1(x1, x2, . . . , xn), R1(x1, x2, . . . , xn), . . . , Rm(x1, x2, . . . , xn));\n}\nif view or relation G interpretation satisfies formula P having the some arity as R and, dependente from relations R1, R2, . . . , Rn that are smaller than R in the library of componentes.\nWe simplify the specification using some special meta-signs, following the spirit of M. Makkai [28] and Z. Diskin [29]. If a view D is a is a relation or a clustering relation using the similarity relation \u0393 , we write:\nD : { A1, A2, . . . , An \u21c0 B; \u0393 : { A1, A2, . . . , An;\n\u0393 : similarity; }\nD : is a(\u0393 ); }\nIn the next section we describe a process useful for knowledge extraction. We are particularly interest in symbolic representation to simplify knowledge integration via relational specifications. There are many methodologists available for this task, however our method uses the same logic assumed to govern the UoD."}, {"heading": "4 EXTRACTING KNOWLEDGE FROM CONCEPT DESCRIPTIONS", "text": "Given a concept description R : O \u21c0 \u2210\n\u03b1\u2208Att A\u03b1, our goal is the extraction of knowledge from one of its views RV,U : O \u00d7 \u220f \u03b1\u2208V A\u03b1 \u21c0 \u2210\n\u03b1\u2208U A\u03b1. For that the information structure is crystallized in a neural network and codified in stringbased notation as a formula. Different concept can be seen as answers to different queries to a UoD models. In this sense different concepts represent different perspective for the available data, allowing the enrichment of a knowledge base or specification systems with new insights. In this section, we present a methodology to extract first-order formulas using neural networks describing available information in a Llogic.\nAs mentioned in [30] there is a lack of a deep investigation of the relationships between logics and NNs. In [13] it is shown how, by taking as activation function, \u03c8, the identity truncated to zero and one,\n\u03c8(x)=min(1,max(x,0)),\nit is possible to represent the corresponding NN as a combination of propositions of Lukasiewicz calculus and vice-versa [30].\nFor used NNs to learn Lukasiewicz sentences, we define the first-order language as a set of circuits generated from the plugging of atomic components.\n19\nFor this, we used the library of components presented in table 1, interpreted as neural units and we gluing them together, to form NNs having only one output, without loops. This task of construct complex structures based on simplest ones can be formalized using generalized programming [31]. The neurons\nof these types of networks, which have two inputs and one output, can be interpreted as a function (see figure 5) and are generically denoted, in the following, by \u03c8b(w1x1, w2x2), where b represent the node bias, w1 and w3 are the weights and, x1 and x2 input values. We simplify exposition by calling to w1x1 and w2x2 variables in \u03c8b. In this context a network is the functional interpretation of a formula in the string-based notation when the relation, defined by network execution, corresponds to the formula truth table. The use of NNs as interpretation\nof formulas simplifies the transformation between string-based representations and the network representation, allowing one to write:\nProposition 2 Every well-formed formula in the Llogic language can be codified using a NN, and the network defines the formula interpretation, when the activation function is the identity truncated to zero and one.\nFor instance, the semantic for sentence \u03d5=(x\u2297y\u21d2z)\u2295(z\u21d2w), can be described using the bellow network or can be codified by the presented set of matrices. From this matrices we must note that the local interpretation of each unit is a simple exercise of pattern checking, where we take by reference the existent relation between formulas and configuration described in table 1.\n20\nx\n1 ##\u25cf \u25cf\u25cf \u22121'&%$ !\"#\u2297 \u22121\n##\u25cf\u25cf \u25cf\u25cf 1\ny 1 <<\u2460\u2460\u2460 = 1 // '&%$ !\"#\u21d2 1 ##\u274b \u274b\u274b 0 z \u22121 ##\u25cf \u25cf\u25cf 1 <<\u2460\u2460\u2460 1 0 0\n'&%$ !\"#\u2295 //'&%$ !\"#\u21d2 1 // = 1 ;;\u2707\u2707\u2707 w 1 ::\u2708\u2708\u2708\nx y z w b\u2019s partial interpretation i1 i2 i3   1 1 0 0 0 0 1 0 0 0 \u22121 1     \u22121 0 1   x \u2297 y z z \u21d2 w\ni1 i2 i3 j1 j2 [ \u22121 1 0 0 0 1 ] [ 1 0 ] i1 \u21d2 i2 i3\nj1 j2 [\n1 1 ] [ 0 ]\nj1 \u2295 j2\nINTERPRETATION: j1 \u2295 j2 = (i1 \u21d2 i2) \u2295 (i3) = ((x \u2297 y) \u21d2 z) \u2295 (z \u21d2 w)\nIn this sense this NN can be seen as an interpretation for sentence \u03d5; it codifies f\u03d5, the proposition truth table.\nf\u03d5(x,y,z,w)=\u03c80(\u03c80(\u03c81(\u2212z,w)),\u03c81(\u03c80(z),\u2212\u03c8\u22121(x,y)))\nHowever, truth table f\u03d5 is a continuous structure, for our goal, it must be discretized and represented using a finite structure, ensuring sufficient information to describe the original formula. A truth table f\u03d5 for a formula \u03d5, in a fuzzy logic, is a map f\u03d5 : [0, 1]\nm \u2192 [0, 1], where m is the number of propositional variables used in \u03d5. For each integer n > 0, let Sn be the set {0, 1 n , . . . , n\u22121 n , 1}. Each n > 0, defines a sub-table for f\u03d5 defined by f (n) \u03d5 : (Sn) m \u2192 [0, 1], given by f (n) \u03d5 (v\u0304) = f\u03d5(v\u0304), and called the \u03d5 (n+1)-valued truth sub-table."}, {"heading": "4.1 Similarity between a configuration and a formula", "text": "We call Castro neural network (CNN) a type of NN having as activation function \u03c8(x) = min(1,max(0, x)), where its weights are -1, 0 or 1 and having by bias an integer. A CNN is called Lukasiewicz neural network ( LNN) if it can be codified as a binary NN: i.e. a CNN where each neuron has one or two inputs. A CNN is representable when can be codified using an equivalent LNN.\nEach neuron, with n inputs, in a CNN can be described using configuration\n\u03b1=\u03c8b(x1,x2,...,xn\u22121,xn)\nand it is representable when can be describes by a LNN\n\u03b1=\u03c8b1y1,\u03c8b2(y2,\u03c8b3 (...,\u03c8bn\u22121(yn\u22121,yn))).\nA CNN is representable if each of its neurons is representable. Note that, a representable CNN can be translated directly into Lukasiewicz first-order language, using the correspondences between configurations and formulas described on table 1.\nGiven the configuration \u03b1=\u03c8b(x1,x2,...,xn), in a CNN, with 0\u2264x1+x2+...+xn+b\u22641, we have 0\u2264x1+(x2+...+xn+b2)+b1\u22641, where b=b1+b2 for integers b0 and b1. And we have\n\u03c8b(x1,x2,...,xn)=\u03c8b1 (x1,\u03c8b2 (x2,...,xn)) .\n21\nNaturally, a neuron configuration - when representable - can by codified by different LNN. Particularly, we have:\nProposition 3 If the neuron configuration \u03b1=\u03c8b(x1,x2,...,xn\u22121,xn) is representable, but not constant, it can be codified in a LNN with the following structure:\n\u03b1=\u03c8b1 (x1,\u03c8b2(x2,...,\u03c8bn\u22121(xn\u22121,xn)...)),\nwhere b1,b2,...,bn\u22121 are integers, and b=b1+b2+...+bn\u22121.\nAnd, since the n-nary operator \u03c8b is commutative, variables x1,x2,...,xn\u22121,xn) could interchange its position in function \u03b1=\u03c8b(x1,x2,...,xn\u22121,xn) without changing the operator output. By this we mean that, for a three input configuration, when we permutate variables, we generate equivalent configurations:\n\u03c8b(x1,x2,x3)=\u03c8b(x2,x3,x1)=\u03c8b(x3,x2,x1)=...\nWhen these are representable, they can be codified in string-based notation using logic connectives. But these different configuration only generate equivalent formulas if these formulas are disjunctive or conjunctive. A disjunctive formulas is formula written using the disjunction of propositional variables or negation of propositional variable. Similarly, a conjunctive formulas are formulas written using only the conjunction of propositional variables or its negation.\nProposition 4 If \u03b1=\u03c8b(x1,x2,...,xn\u22121,xn) is representable, it is the interpretation of a disjunctive formula or a conjunctive formula.\nThis leave us with the task of classifying a neuron configuration according to its representation. For that, we must note what, if\n\u03b1=\u03c8b(\u2212x1,\u2212x2,...,\u2212xn,xn+1,...,xm)\nis representable:\n1. When b = n is the number of negative inputs, in \u03b1, we have\n\u03b1=\u03c81(\u2212x1,\u03c81(\u2212x2,...\u03c81(\u2212xn,\u03c80(xn+1,...\u03c80(xm\u22121,xm))...)...)),\nusing Table 1, the configuration \u03b1 is the interpretation for\n\u00acx1\u2295...\u2295\u00acxn\u2295xn+1\u2295...\u2295xm.\n2. When b = \u2212p+ 1 is the number of negative inputs, in \u03b1, we have\n\u03b1=\u03c81(\u2212x1,\u03c81(\u2212x2,...\u03c80(\u2212xn,\u03c8\u22121(xn+1,...\u03c8\u22121(xm\u22121,xm))...)...)),\nan interpretation for formula\n\u00acx1\u2297...\u2297\u00acxn\u2297xn+1\u2297...\u2297xm.\n22\nThis establishes a relationship between the formula structure and the configuration bias, the number of negative and positive weights.\nProposition 5 Given the neuron configuration \u03b1=\u03c8b(\u2212x1,\u2212x2,...,\u2212xn,xn+1,...,xm) with m = n + p inputs and where n and p are, respectively, the number of negative and the number of positive weights, on the neuron configuration:\n1. If b = \u2212p+ 1 the neuron is called a conjunction and it is an interpretation for \u00acx1\u2297...\u2297\u00acxn\u2297xn+1\u2297...\u2297xm. 2. When b = n the neuron is called a disjunction and it is an interpretation of\n\u00acx1\u2295...\u2295\u00acxn\u2295xn+1\u2295...\u2295xm.\nImposing some structural order on the neural network transformation:\nProposition 6 Every conjunctive or disjunctive configuration\n\u03b1=\u03c8b(x1,x2,...,xn\u22121,xn),\ncan be codified by a LNN\n\u03b2=\u03c8b1(x1,\u03c8b2 (x2,...,\u03c8bn\u22121(xn\u22121,xn)...)),\nwhere\nb1,b2,...,bn\u22121 are integers, b=b1+b2+\u00b7\u00b7\u00b7+bn\u22121 and b1\u2264b2\u2264\u00b7\u00b7\u00b7\u2264bn\u22121.\nThis property can be translated in the following rewriting rule,\nw1 \u2744 \u2744\u2744 \u2744 b. .\n. '&%$ !\"#\u03c8 // R //\nwn ??\u2467\u2467\u2467\u2467\nw1 \u273f \u273f\u273f \u273f b0.\n. . '&%$ !\"#\u03c8\n1 \u273f \u273f\u273f \u273f b1 wn\u22121 CC\u271d\u271d\u271d\u271d\u271d '&%$ !\"#\u03c8 //wn 66\u2767\u2767\u2767\u2767\u2767\u2767\u2767\nlinking equivalent networks, when the integers b0 and b1 satisfy b = b0 + b1 and b1 \u2264 b0, and are such that neither of the involved neurons have constant output. Note that, a representable CNN can be transformed by the application of rule R in a set of equivalent LNN with simplest neuron configuration:\nProposition 7 Un-representable neuron configurations are those transformed by rule R in, at least, two non-equivalent NNs.\nFor instance, the un-representable configuration \u03c80(\u2212x1, x2, x3), is transformed by rule R in three non-equivalent configurations:\n\u03c80(x3, \u03c80(\u2212x1, x2)) = fx3\u2295(\u00acx1\u2297x2) \u03c8\u22121(x3, \u03c81(\u2212x, x2)) = fx3\u2297(\u00acx1\u2297x2) \u03c80(\u2212x1, \u03c80(x2, x3)) = f\u00acx1\u2297(x2\u2295x3)\nThe representable configuration \u03c82(\u2212x1,\u2212x2, x3) is transformed by rule R on only two distinct but equivalent configurations:\n23\n\u03c80(x3, \u03c82(\u2212x1,\u2212x2)) = fx3\u2295\u00ac(x1\u2297x2) \u03c81(\u2212x2, \u03c81(\u2212x1, x3)) = f\u00acx2\u2295(\u00acx1\u2295x3)\nFor the extraction of knowledge from trained NNs, we translate neuron configuration in propositional connectives to form formulas. However, not all neuron configurations can be translated in formulas, but they can be approximate by one. To quantify the approximation quality we used the exponential-similarity.\nTwo neuron configurations \u03b1 = \u03c8b(x1, x2, . . . , xn) and \u03b2 = \u03c8b\u2032(y1, y2, . . . , yn),\nare called \u03bb-similar, in a (m + 1)-valued Llogic, if \u03bb = e\u2212 1 |O|\n\u2211\nx\u0304\u2208T \u00ac(\u03b1(x\u0304)\u21d4\u03b2(x\u0304)), we write \u03b1 \u223c\u03bb \u03b2. If \u03b1 is un-representable and \u03b2 is representable, the second configuration is called a representable approximation to the first.\nOn the 2-valued Llogic (the Boolean logic case), we have for the un-representable configuration \u03b1 = \u03c80(\u2212x1, x2, x3):\n\u03c80(\u2212x1, x2, x3) \u223c0.883 \u03c80(x3, \u03c80(\u2212x1, x2)) \u03c80(\u2212x1, x2, x3) \u223c0.883 \u03c8\u22121(x3, \u03c81(\u2212x1, x2)) \u03c80(\u2212x1, x2, x3) \u223c0.883 \u03c80(\u2212x1, \u03c80(x2, x3))\nIn this case, the truth sub-tables of, formulas \u03b11 = x3 \u2295 (\u00acx1 \u2297 x2), \u03b11 = x3 \u2297 (\u00acx1 \u2297 x2) and \u03b11 = \u00acx1 \u2297 (x2 \u2295 x3) are both \u03bb-similar to \u03c80(\u2212x1, x2, x3), where \u03bb = 0.883, since they differ in one position on 8 possible positions. This means that both formulas are 92% accurate.\nFor an un-representable configuration, \u03b1, we can generate the finite set S(\u03b1), of representable networks similar to \u03b1, using rule R. Given a (n+1)-valued logic, from that set of formulas we select to approximate \u03b1 the formula having the interpretation more similar to \u03b1. This identification of un-representable configuration, using representable approximations, is used to transform networks with un-representable neurons into representable structures. The stress associated with this transformation characterizes the translation quality.\nBellow we present an example of a un-representable CNN:\n\n \u22121 1 \u22121 1 0 \u22121 0 0 0 0 1 1 0 \u22121 1 1 0 0 0 0 \u22121\n\n\n\n 0 1 0\n\n i1 un-representable A4 \u2295 A5 \u2295 \u00acA7 i3 un-representable\n[ 1 \u22121 1 ] [ 0 ]\nj1un-representable\nFor each local un-representable configuration \u03b1, we selected the most similar representable configuration on S(\u03b1), after applying rule R, we have in this case:\n1. i1 \u223c0.9387 ((\u00acA1 \u2297 A4) \u2295 A2) \u2297 \u00acA3 \u2297 \u00acA6, 2. i3 \u223c0.8781 (A1 \u2295 \u00acA7) \u2297 A2, and 3. j1 \u223c0.8781 (i1 \u2297 \u00aci2) \u2295 i3.\nUsing this substitutions we reconstructed the formula:\n\u03b1 = (((((\u00acA1 \u2297 A4) \u2295 A2) \u2297 \u00acA3 \u2297 \u00acA6) \u2297 \u00ac(A4 \u2295 A5 \u2295 \u00acA7)) \u2295 ((A1 \u2295 \u00acA7) \u2297 A2),\n\u03bb-similar to the original CNN, with \u03bb = 0.7323, in a 5-valued Llogic."}, {"heading": "4.2 Crystallizing trained neural networks", "text": "Standard error back-propagation algorithm (EBP) is a gradient descent algorithm, in which the network weights are moved along the negative of the gradient of the performance function. EBP algorithm has been a significant improvement\n24\nin NN research, but it has a weak convergence rate. Many efforts have been made to speed up the EBP algorithm. The Levenberg-Marquardt (LM) algorithm [5] [32] ensued from the development of EBP algorithm-dependent methods. It gives a good exchange between the speed of the Newton algorithm and the stability of the steepest descent method [33].\nThe basic EBP algorithm adjusts the weights in the steepest descent direction. When training with the EBP method, an iteration of the algorithm defines the change of weights and has the form wk+1=wk\u2212\u03b1Gk, where Gk is the gradient of performance index F on wk, and \u03b1 is the learning rate.\nNote that, the basic step of Newton\u2019s method can be derived from Taylor formula and is wk+1=wk\u2212H\u22121k Gk, where Hk is the Hessian matrix of the performance index at the current values of the weights. Since Newton\u2019s method implicitly uses quadratic assumptions, the Hessian matrix dos not need be evaluated exactly. Rather, an approximation can be used, such as Hk\u2248JTk Jk, where Jk is the Jacobian matrix that contains first derivatives of the network errors with respect to the weights wk.\nThe simple gradient descent and Newtonian iteration are complementary in the advantages they provide. Levenberg proposed an algorithm based on this observation, whose update rule blends aforementioned algorithms and is given as\nwk+1=wk\u2212[JTk Jk+\u00b5I] \u22121JTk ek\n,\nwhere ek is a vector of current network errors and \u00b5 is the learning rate. This update rule is used as follows. If the error goes down following an update, it implies that our quadratic assumption on the function is working and we reduce \u00b5 (usually by a factor of 10) to reduce the influence of gradient descent. In this way, the performance function is always reduced at each iteration of the algorithm [34]. On the other hand, if the error goes up, we would like to follow the gradient more and so \u00b5 is increased by the same factor.\nWe can obtain some advantage out of the second derivative, by scaling each component of the gradient according to the curvature. This should result in larger movements along the direction where the gradient is smaller so the classic \u201derror valley\u201d problem does not occur any more. This crucial insight was provided by Marquardt. He replaced the identity matrix in the Levenberg update rule with the diagonal of Hessian matrix approximation resulting in the LM update rule.\nIn each LM iteration we restricted the NN representation bias, making its structure similar to a CNN. For that, we used a smooth crystallization procedure resulting from function,\n\u03a5n(w)=sign(w).((cos(1\u2212abs(w)\u2212\u230aabs(w)\u230b). \u03c0 2 ) n+\u230aabs(w)\u230b),\niteration, where sign(w) is the sign of w and abs(w) its absolute value. Denoting by \u03a5n(N) the function having by input and output a NN, where the weights on the output network results of applying \u03a5 to all the input network weights and neurons biases. Each interactive application of \u03a5 produce a networks progressively more similar to a CNNs. For show that, we define by representation error,\n25\nfor a network N , with weights w1, . . . , wn,\n\u2206(N) =\nn \u2211\ni=1\n(wi \u2212 \u230awi\u230b).\nWhen N is a CNNs we have \u2206(N) = 0. Since, for every network N and n > 0, \u2206(N) \u2265 \u2206(\u03a5n(N)), we have\nProposition 8 Given a neural networks N with weights in the interval [0, 1]. For every n > 0 the function \u03a5n(N) have by fixed points Lukasiewicz neural networks N \u2032.\nWe changed the LM algorithm by applying a soft crystallization step after the LM update rule:\nwk+1=\u03a52(wk\u2212[JTk Jk+\u00b5.diag(J T k Jk)] \u22121JTk ek )\nThis can be seen as a process to regularize the network and improves drastically the convergence to a CNN preserving its ability to learn. On our method network regularization is made using three different strategies:\n1. using the described soft crystallization process, where we restricted the knowledge dissemination on the network structure, information is concentrated on some weights; 2. after the training we use crisp crystallization, where links between neurons with weights near 0 are removed and weights near -1 or 1 are consolidated; 3. the resulting crisp network is pruned using \u201dOptimal Brain Surgeon\u201d method.\nThe first regularization technic avoids knowledge dissemination on the NN. The last regularization technic avoid redundancies, in the sense that the same or redundant information can be codified at different locations. We minimized this by selecting weights to eliminate. The \u201dOptimal Brain Surgeon\u201d method uses the criterion of minimal increase in training error. It uses information from all second-order derivatives of the error function to perform network pruning.\nThe Optimal Brain Surgeon method is derived from Taylor series of the error with respect to weights,\n\u2206E = JTw .\u2206w + 1\n2 \u2206wTHw\u2206w +O(\u2016\u2206w\u2016 3).\nFor a network trained to a local minimum in error, \u2206w \u2248 0, the first linear term vanishes, third and all higher order terms are ignored. The method finds a weight to be set to zero (which we call wq) to minimize \u2206E the increase in error. Making wq zero correspond to change its value using \u2206wq making \u2206wq+wq = 0 or more generally:\neTq \u2206w + wq = 0,\nwhere eq is the unit vector in weight space corresponding to (scalar) weight wq. This reduces our goal to solve:\nmin q {min \u2206w\n1 2 \u2206wTHw\u2206w | e T q \u2206wq + wq = 0}\n26\nThe optimization problem is constrained, following [14], we form the Lagrangian operator\nL = 1\n2 \u2206wTHw\u2206w + \u03bb(e T q \u2206wq + wq),\nwhere \u03bb is a Lagrange undetermined multiplier. After take functional derivatives, employ the constraint eTq \u2206w + wq = 0, and use matrix inverse to find that the optimal weight change and resulting change in error are respectively\n\u2206w = \u2212 wq\n[H\u22121w ]qq H\u22121w eq and Lq =\n1\n2\nw2q\n[H\u22121w ]qq .\nThe method recalculates the magnitude of all the weights in the NN. Hassibi, Stork and Stork called Lq the \u201dsaliency\u201d of weight q, the increase in error that results when the weight is eliminated.\nAlgorithm 1 describes our methodology for training CNN and extraction of symbolic pattern descriptions.\nAlgorithm 1 Reverse Engineering algorithm 1: Given a concept description evaluated on a (n+1)-valued Lukasiewicz logic 2: Define an initial network complexity 3: Generate an initial neural network 4: Apply the LM algorithm with soft crystallization 5: if the generated network have bad performance then 6: If need increase network complexity 7: Try a new network. Go to 3 8: end if 9: Crisp crystallization on the trained NN. 10: if crystallized network have bad performance then 11: Try a new network. Go to 3 12: end if 13: Refine the crystallized NN using \u201dOptimal Brain Surgeon\u201d algorithm 14: Identify un-representable configurations 15: Replace each un-representable configurations, using a similar representable configuration, selected from the set\nof configurations generated using rule R.\n16: Evaluated the procured NN performance on the original concept description. 17: Translated the procured NN on string-based notation.\nGiven a view for a concept description, we try to find a CNN describing information. For that our implementation generates neural networks with a fixed number of hidden layers (in our implementation we used three). When a network have bad learning performance, training is stopped, and train is initiate for a new network, with random heights. After a fixed number of tries the network topology is changed. This number iterations depends on the number of network inputs. After trying configure a set of networks with a given complexity and bad learning performance, the system tries to apply LM algorithm, with soft crystallization, for a more complex set of networks. The process stops when a adequate description for data is find. And after the network be pruned, unrepresentable configurations are approximated using representable ones. This defines a description for the information using a LNN.\n27"}, {"heading": "5 GENERATING ARTIFICIAL FUZZY STRUCTURES", "text": "We tested our methodology in real data sets [35] and on artificially generated data structures. On this section we present our approach using artificially generated data sets. For that, we developed a simple way to generate complex fuzzy structures. Our method is based on a non-deterministic state machine. In this machine every states have associated a level of uncertainty quantified by a truth value, from a many-valued logic\u2126. These type of machines have its stats changed based on the reading a word and this change is described by a relation defining stat transition, from the actual state and the signs that are being read. This relation, for stat change, is a multi-morphism evaluated in \u2126 and the actual signs being read are described by a \u2126-set. We called to these type of state machines a \u2126-automata.\nIn this sections we describe an automata as a concept to be learned, and show how reverse engineering its structure using the data generated from its execution. We do this translating a CNN structure in a formula, representable on a specification system. Beginning with a structural description of a problem, in the from of a specification system, and a model, we enrich the specification structure using knowledge extracted from data generated querying the model and crystallized on neuronal networks."}, {"heading": "5.1 Lukasiewicz automatas", "text": "A \u2126-automata is an non-deterministic state machine, where state transition relation. This change is made after reading a sign from a word. Each symbol used to define automata input string is a sign having the form \u03b1 =\u03bb x, where \u03b1 \u2208 Att is an attribute, x a possible value for \u03b1, x \u2208 A\u03b1, and \u03bb quantifies the truth value of \u03b1 = x evaluated in \u2126. The input sting at position i defines the truth of \u03b1 =\u03bb x for each \u03b1 \u2208 Att. The automata is defined by a set of states E, and at each moment this states have associated a level of uncertainty in \u2126 of being the automata state. In this sense an \u2126-automata actual state is describes by a relation evaluated in \u2126, ei : E \u2192 \u2126, having by support the set of automata possible states. The states of an automata are classified in three classes: input states, auxiliary states and output states. The input states have its uncertainty directly assigned by the uncertainty on the input signs. Each of this state is associated directly with one sign \u03b1 = x, used on the input string, and its uncertainty is the truth value of equality \u03b1 = x on the actual reading position. After all input string have been read, the level of uncertainty on output states defines the \u2126-automata output. An \u2126-automata begins its activity in a initial state e0 : E \u2192 \u2126, reads a string s1s2 . . . sn, in each iteration a position si is read and on the n-iteration reach its final state en : E \u2192 \u2126. The final state is used to describe the automata output o : EO \u2192 \u2126. Formally\nDefinition 3 An \u2126-automata A is a structure described by\n( L, (Ai)i\u2208Att, E,EO, {M\u03bb,M\u00ac\u03bb}\u03bb\u2208\u03a3 , e0)\nwhere:\n28\n1. L is a finite multi-valued logic having truth values in \u2126; 2. (A\u03b1)\u03b1\u2208Att a family \u2126-sets used as domain for attributes in construction of\nsigns \u2032\u03b1 = x\u2032; 3. E is the set of states, where each input sign \u2032\u03b1 = x\u2032 have a state associated.\nThe states, with a sign associated, are called input states; 4. EO is a subset of E, called set output states; 5. two boolean matrices M0 and M1, describing state transition. M0 describes\nnegative uncertainty propagation and M1 positive uncertainty propagation; 6. e0 : E \u2192 \u2126 is a relation describing the initial automata state; 7. if the automata state on iteration k is ek : E \u2192 \u2126 and let Xk : \u2210\n\u03b1\u2208Att A\u03b1 \u2192 \u2126 describe the input sign on position k, the new automata state is given by\nek+1 = M1(ek)\u2295M0(\u00acek),\nwhere in ek : E we update, the input states, with the sign uncertainty on the reading position, described by vector Xk.\nA \u2126-automata is called a Lukasiewicz automata when the system is governed by a finite Lukasiewicz logic.\nWe named relational automata to an extension to the \u2126-automata defined using transition matrices M0 and M1 with values in \u2126. This type of fuzzy automata have its behaviour described using formulas of Relational Lukasiewicz logic, introduced in [36], outfitting the scope of this work.\nIn this sense we interpret a word as a string of begs, where the possibility of a sign is in the beg is described by a relation Xk : \u2210\n\u03b1\u2208Att A\u03b1 \u2192 \u2126. In each iteration the \u2126-automata reads a position on the string. If in the iteration k the position k is read, on the position k + 1 it reads the string position k + 1. Each position in the string can have more than a sign or can be empty. On iteration k the automata reads the position k, updating the uncertainty on each input states, using the sign uncertainties on the reading positions. This uncertainty is propagating to other states, applying the state transformation matrices. The update and the propagation of state uncertainty is done for each iteration. The input string length determines the number of automata iteration.\nIn this context a word w can be define as a sequence of relations\ns1, s2, . . . si . . . , sn\nhaving the type si : \u2210\n\u03b1\u2208Att A\u03b1 \u2192 \u2126. Using w as input to the automata, it generates a sequence of states described using a sequence of relation\ne1, e2, . . . ei . . . , en,\nhaving the type ei : E \u2192 \u2126, describing the change on automata state between iterations. The \u2126-set e1 is defined using the automata initial state e0, where the input states were update with the reading position s0. The state ei, when i > 1, depends on state ei\u22121 updated with the uncertainty on input sign, described by the reading position si. The automata output is defined by the uncertainty in each output state EO in the automata state en. The following example illustrates an automata execution.\n29\nExample 1 (A binary Lukasiewicz automata) The string of a binary Lukasiewicz automata is defined using only an attribute, having two possible values. Let this attribute be a and its possible values 0 or 1. Words interpreted using this automata are described using a sequence of \u2126-sets having by support the set of signs {\u2032a = 0\u2032,\u2032 a = 1\u2032}. An example, of this type of words is presented on table 2, where each column codifies the existence of each sign in that position. For this example, we fixed the finite Lukasiewicz logic having by truth values\n\u2126={0,1/4,1/2,3/4,1}. This table can be interpreted by saying what: first and second marks in the word are a \u20191\u2019, position 4 and 12 have a \u20190\u2019, and in position 11 we not know the symbol used, on position 6 we can not distinguished between a \u20190\u2019 or a \u20191\u2019.\nFor simplify the graphic presentation, we labelled each input state by its associates sign and to each not input state we labelled it with a number. Lets\n\u3008I(a=1),I(a=0),1,2,3,4,5,6\u3009\nbe the list of states, where I(a = 1) and I(a = 0) represent the input states, its uncertainty is indexed to the uncertainty on the reading of sign \u2032a = 1\u2032 and \u2032a = 0\u2032, and let 4, 5, and 6 be the automata output states. Let the state change boolean relations described using the graph presented on figure 6, where the arrows labelled with 0 represents the propagation of negative uncertainty and the arrows labelled with 1 represents the propagation of positive uncertainty. This graph can\nbe codified using two graph adjacency matrices: M0, for sub-graph having arrows labelled with 0, describing the state change when a state isn\u2019t active, and M1 the adjacency for sub-graphs having arrows labelled with 1, describing behaviour\n30\nwhen a state is active.\nM0 =\n\n   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n\n   M1 =\n\n   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n\n  \nExecuting this automata, for word described on table 2, and taking the automata initial state uncertain, i.e. making the initial state e1=[1/2,1/2,1/2,1/2,1/2,1/2,1/2,1/2]\u2032 . After reading the first position, we update e1 using the uncertain for each signs on first position, e1=[1,0,1/2,1/2,1/2,1/2,1/2,1/2]\u2032 , from it we compute e2 by:\ne2=M0\u2297(\u00ace1)\u2295M1\u2297e1=[0,0,0,1,1,1/2,1,1/2] \u2032.\nUpdating the resulting state, since the second mark is \u2019a=1\u2019, we have\ne2=[1,0,0,1,1,1/2,1,1/2]\u2032.\nUsing the same procedure e3=[1/2,1/2,0,0,1,1,1,1/2,1/2]\u2032, since we not know if in position 3 we have \u2019a=1\u2019 or \u2019a=0\u2019. Repeating the process for other word positions we have:\ne4 = [0, 1, 1/2, 1, 1, 1, 1/2, 1] \u2032 e8 = [1, 0, 0, 1, 3/4, 1/2, 3/4, 3/4] \u2032 e5 = [1/4, 3/4, 1, 1, 1/2, 1, 0, 1] \u2032 e9 = [1/2, 1/2, 0, 1, 1, 1, 1/2, 1/2] \u2032 e6 = [1/2, 1/2, 3/4, 3/4, 0, 1, 1/2, 1] \u2032 e10 = [1/4, 3/4, 1/2, 1, 1, 1, 1/2, 1] \u2032 e7 = [1, 0, 1/2, 1/2, 1/4, 3/4, 1, 1] \u2032 e11 = [0, 0, 2/3, 1, 1/2, 1, 0, 1] \u2032\ne12 = [0, 1, 1, 1/2, 1/4, 1, 1/2, 1] \u2032\nThe automata final state is defined using, e13=[0,0,1,1/4,0,1/2,3/4,1]\u2032, and it is A(w)=[1/2,3/4,1]\u2032 since these are the uncertainties in e13 for output states {4, 5, 6}. We will interpret A(w)=[1/2,3/4,1]\u2032 as the output for automata A when it is executed over the fuzzy string w.\nSince in this type of automata transitions are defined using only boolean relations, they can be described using CNNs. Each node depends on a positive or on a negative way from its neighbours, and the relation can be expressed through a sentence in disjunctive form. We can see this translation from a local configuration to an admissible configuration on figure 7. Note what, neuron bias is defined by the number of arrows labelled with zero. In string-base notation it could be write as ci=c1\u2295c2\u2295c3\u2295\u00acc4\u2295\u00acc5, which can also be seen as a colimit sentence.\n31"}, {"heading": "5.2 Reverse engineering Lukasiewicz automatas", "text": "The extraction of knowledge from data generated by automata execution offers a rich framework to present a simple knowledge integration methodology on Llogic. For this we used automata to generate complex artificial data sets.\nWe selected two binary automata described graphically on figure 8, and constructed data sets using information generated through automata execution. This was made selecting possible input configuration with length 6, in a 5-valued Llogic. Where we impose a consistence principle in the automata reading sensor: the uncertainty for sign \u2019s = 0\u2019, is the negation of sign \u2019s = 1\u2019 uncertainty. For both automata we generated a data set describing the dependence between states, relating state uncertainty on iteration t with the state in iteration t+ 1. With this data set we reverse engineering the automata structure, using Algorithm 1 presented in last section. Trying predict the uncertainty in each state on iteration t+ 1 using the information, about the automata state uncertainty, in iteration t and the input uncertainty. For each prediction task we selected a rule describing the relationships between states. Figure 9 describes the reverse engineering output for each state, generated using the data produced by the acyclic automata from figure 8.\nEach configuration can be expressed or approximate using a string-based presentation. Each of this formulas is interpreted as knowledge extracted from different views of the data set. The set of all extracted formulas (equalities)\n32\nwe call a theory, and can be codified as an specification system. For acyclic automata, on figure 8, and translating configurations presented in figure 9, we have the theory:\nT0={A2(t+1)=I(at=1), A3(t+1)=\u00ac(\u00acI(at=1)\u2295A4(t)), A4(t+1)=\u00ac(\u00acA2(t)\u2295A6(t)),\nA5(t+1)\u223c0.97\u00acA3(t)\u2295A4(t)\u2295A6(t)\u2295\u00acA7(t), A6(t+1)=\u00acA2(t), A7(t+1)=A6(t)}\nThis symbolic description allows forecast the automata behaviour. In this case theory T0 isn\u2019t a prefect automata description, since we only have a approximation to automata beaver on state A5. The consistence level of a equational theory T , with a model D, is describes by the disjunction of each similarity level associated to each equality in T . In the case of equational theory T0, we have a consistence level of 0.97. Given a generic word w = s0s1s2s3s4 in the interaction t = 5, the uncertainty on state A4 depends on signs from positions s2 and s3.\nA4(5) = A2(4) \u2297 \u00acA6(4) = I(s3 = 1) \u2297 \u00acA2(3) = I(s3 = 1) \u2297 I(s2 = 1)\nIn interaction t = 6, the uncertainty on state A5, is given by:\nA5(6) \u223c0.97 \u00acA3(5) \u2297 A4(5) \u2297 A6(5) \u2297 \u00acA7(5) \u223c \u00acI(s2 = 1) \u2295 (I(s3 = 1) \u2297 I(s2 = 1)) \u2295 \u00acA3(4) \u2295 \u00acA6(4) \u223c \u00acI(s2 = 1) \u2295 I(s3 = 1) \u2295 \u00acI(s3 = 1) \u2295 A4(3) \u2295 A3(3) \u223c 1\nFrom a functional point of view, knowledge integrated in theory T0, are interpreted using the first Lukasiewicz neural network on figure 10. This NN configuration results from integrating NNs presented on figure 9. The resulting NN have 6 outputs, one for each non input state.\nIntegrating configuration representing knowledge in an NN usually introduce redundancies. In the sense that the same or similar information is codified by different configurations in different locations. This can be minimized by regularizing the produced NN using \u201dOptimal Brain Surgeon\u201d method [14].\nIn the other direction, every formula in Lukasiewicz logic can be used to describe a state uncertainty. By this we mean that, for every formula we can define an automata with a state having by uncertainty level the result of formula\n33\nevaluation, after some interactions. For this the automata must have by input a string, having by first position the uncertainty on formula arguments, followed by marks with uncertainty zero for every possible sign. The number of positions in input sting must be equal to the formula parsing tree height.\nInjecting a formula in an automata structure is a simple process. For this we must note what every formula can be expressed in the disjunctive normal form, i.e. using only the connectives disjunction and negation. For instance the formula ((a\u2297 b)\u2295 c) \u2192 d, is equivalent to \u00ac(\u00ac(\u00aca \u2295 \u00acb) \u2295 c) \u2295 d, evaluated as the uncertainty on state E, after 4 interaction, using automata describe in figure 11. Note that, this defines a concept view R:O\u00d7{a,b,c,d}\u21c0{E}."}, {"heading": "6 KNOWLEDGE INTEGRATION VIA SPECIFICATION SYSTEMS", "text": "The integration of knowledge using connective models is very restrictive, and difficult to be used directory by domain experts. However, it can be useful to deploy analytic models, or on procedures for redundancy minimization in a knowledge base.\nWe want to present specification systems as the suitable framework for symbolic knowledge integration. Given a semiotic (S,M) our goal is to complete the semiotic by enriching specification S, such that the new specification S\u2032 is a better description for M . This is made by enriching structure S with knowledge extracted from different views on M .\nFor that, in this section, we extract knowledge from a structure construct using fuzzy relations. This relations are known to be defined using fuzzy automata, but this fact is hidden to the completion methodology.\nFigure 12 describe the UoD structure, where concept views Ga and Gb are known to be described, respectively, by the acyclic and cyclic automata on figure 8, where A and B are sets of input strings and E is the set of automata possible states. This relations are described by two data sets with 15625 cases, using 14 attributes. Relations Ta and Tb are defined by the state transformation relations, codified on two data sets with 15625 cases, using 16 attributes, each. In the\nspecification, views Ra and Bb denote the set of automata output states, given through the selection by F of automata final states in E, satisfying: Ra = Ga\u2297F and Ra = Gb \u2297 F .\nGenerically, we can formalize this structure using the following string-based relational specification:\n%Nodes: I : {0, 1};\nA,B,A\u2032, B\u2032 : I,I,I,I,I,I,I,I; F : I; E : {A0, A1, A2, A3, A4, A5, A6, A7};\n%Arrows: Ra : {A \u21c0 F}; Ga : {A \u21c0 E}; Rb : {B \u21c0 F}; Gb : {B \u21c0 F}; P : {E \u21c0 F};\nTa, Tb : {E \u21c0 E};\nIa : {A \u2032 \u21c0 A;\n\u0393a : { A \u2032;\n\u0393a : similarity; }\nIa : is a(\u0393a); };\nIb : {B \u2032 \u21c0 B;\n\u0393b : { B \u2032;\n\u0393b : similarity; }\nIb : is a(\u0393b); };\n%Commutative diagrams: D1 : {A \u21c0 F ;\nD1 : Ra \u2297 Ga \u2297 P ; }\n[D1]\nD2 : {B \u21c0 F ; D2 : Rb \u2297 Ga \u2297 P ; }\n[D2]\nD3 : {A \u2032 \u21c0 E;\nD3 : Ia \u2297 Ga \u2297 Ga\u2032 ;\n} [D3]\nD4 : {B \u2032 \u21c0 E;\nD4 : Ib \u2297 Gb \u2297 Gb\u2032 ;\n} [D4]\nThis specification have the model, described by a set of data sets generated using fuzzy automata. This data sets are interpretations for each of the arrows used on the specification. Using the extraction process, described in the last section, we can extract rules with insights about the UoD structure.\nThe knowledge extracted from crystallized rules from CNN, when trained over views Ga and Gb, describe the relation between sign uncertainty on input strings, with length 6, and the uncertainty for each state after the string have been read. Since extraction is made in a supervising learning context, the process is oriented for the perdition of each automata state, using input string. Every obtain configuration, in this cases was representable, simplifying the translation for the string-based rules presented bellow for the knowledge base enrichment.\n35\nGa : {A \u21c0 E; Ga(s1, s2, s3, s4, s5, s6) :\n: A0 = s6; : A1 = \u00acs6; : A2 = \u00acs5; : A3 = \u00ac(\u00acs6) \u2295 (s3 \u2297 s4); : A4 = \u00ac(s4 \u2295 s5); : A5 = \u00ac(s3 \u2295 s4) \u2295 (\u00acs2 \u2297 s3 \u2297 \u00acs5); : A6 = s5; : A7 = s4;\n}; Gb : {B \u21c0 E;\nGb(s1, s2, s3, s4, s5, s6) : : A0 = s6; : A1 = \u00acs6; : A2 \u223c0.9391 (s2 \u2297 \u00acs4) \u2295 \u00acs6; : A3 \u223c0.9961 \u00ac(\u00acs4 \u2297 \u00acs6); : A4 \u223c0.9785 \u00ac(s2 \u2297 s3 \u2297 \u00acs4); : A5 \u223c0.9947 \u00acs4 \u2295 (\u00acs3 \u2297 \u00acs5); : A6 \u223c0.9429 \u00ac(s3 \u2297 \u00acs5); : A7 \u223c0.9767 \u00acs2 \u2295 \u00acs3 \u2295 s4; };\nThe knowledge presented on figure 10 describes the relation between automata states. It was generated predicting the uncertainty in each state in interaction i+1 using automata state uncertainty in interaction i and input signs uncertainty. We present bellow the best rules generated by the extraction process for each task.\nTa : {E \u21c0 E; Ta(A0, A1, A2, A3, A4, A5, A6, A7) :\n: A2 = \u00acA0; : A3 = \u00ac(\u00acA0 \u2295 A4); : A4 = \u00ac(\u00acA2 \u2295 A6); : A5 \u223c0.9747 \u00acA3 \u2295 A4 \u2295 A6 \u2295 \u00acA7); : A6 = \u00acA3; : A7 = A6;\n} Tb : {E \u21c0 E;\nTb(A0, A1, A2, A3, A4, A5, A6, A7) : : A2 = A1 \u2295 \u00acA6; : A3 = \u00acA0 \u2297 A4; : A4 =; : A5 = \u00ac(\u00acA2 \u2297 A6); : A6 \u223c0.9993 \u00acA3 \u2295 A4 \u2295 \u00acA7); : A7 = A6; }\nWe can improve the description of our UoD, presenting constrains valid on queries to the specification model. For example, from query defined using equalizer Ga\u2032\u2297b\u2032 : A\n\u2032 \u00d7B\u2032 \u21c0 E for view Ga\u2032 and Gb\u2032 , we can generate new insights in the model structure, which can be used in knowledge base enrichment.\n%Limit sentences:\nD3 : {A \u2032, B\u2032 \u21c0 E;\nD3 := Ga\u2032 \u2297 G b\u2032 ;\n} Ga\u2297b : {A,B,E;\nGa\u2297b : Lim D3; Ga\u2297b(s1, . . . , s6, s \u2032 1, . . . , s \u2032 6) :\n: A0 = s6 \u2297 s \u2032 6; : A1 = \u00ac(s6 \u2295 s \u2032 6); : A2 \u223c0.9878 (s6 \u2295 \u00acs \u2032 6) \u2295 (\u00acs6 \u2297 s \u2032 2 \u2297 s \u2032 3 \u2297 \u00acs \u2032 4 \u2297 s \u2032 6); : A3 \u223c0.9873 \u00ac(\u00acs6 \u2295 \u00acs \u2032 4) \u2295 (s6 \u2297 s \u2032 6); : A4 \u223c0.9869 (\u00acs5 \u2295 \u00acs \u2032 2 \u2295 s4) \u2295 (s \u2032 1 \u2295 s \u2032 3 \u2295 \u00acs \u2032 5) \u2295 (s5 \u2295 s \u2032 5); : A5 \u223c0.9609 ((\u00acs \u2032 4 \u2295 \u00acs \u2032 5) \u2295 (s \u2032 3 \u2297 s \u2032 4) \u2295 \u00ac(s4 \u2295 \u00acs5))\u2297\n\u2297\u00ac((s\u20323 \u2297 s \u2032 4) \u2297 \u00ac(s4 \u2295 s5));\n: A6 = (s4 \u2295 s5) \u2297 \u00ac(s \u2032 3 \u2297 s \u2032 4 \u2297 \u00acs \u2032 5) \u2297 (s \u2032 2 \u2295 s \u2032 3 \u2295 s \u2032 5); : A7 \u223c0.9526 \u00acs \u2032 2 \u2295 \u00acs \u2032 3 \u2295 s4;\n}"}, {"heading": "7 CONCLUSIONS", "text": "This methodology to codify and extract symbolic knowledge from a NN is very simple and efficient for the extraction of comprehensible rules from medium-sized data sets. It is, moreover, very sensible to attribute relevance.\n36\nIn the theoretical point of view it is particularly interesting that restricting the values assumed by neurons weights restrict the information propagation in the network, thus allowing the emergence of patterns in the neuronal network structure. For the case of linear neuronal networks, having by activation function the identity truncate to 0 and 1, these structures are characterized by the occurrence of patterns in neuron configuration directly presentable as formulas in Llogic.\nThe application of procedures like the one above, on information systems, generates grates amount of information. We organize this information in a specification systems using a relational language. And we propose this language as an Interface Layer for AI. Here, classic graphical models like Bayesian and Markov networks have to some extent played the part of an interface layer, but one with a limited range having insufficiently expressive for general AI [37]."}], "references": [{"title": "Locally Presentable and Accessible Cateories", "author": ["J. Ad\u00e1mek", "J. Rosick\u00fd"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Neural-simbolic Cognitive Reasoning", "author": ["A.S. d\u2019Avila Garcez", "L.C. Lamb", "D.M. Gabbay"], "venue": "Cognitive Technologies,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Topoi: The Categorical Analysis of Logic", "author": ["R. Goldblatt"], "venue": "Dover Publications", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Training feed-forward networks with marquardt algorithm", "author": ["M. Hagan", "M. Menhaj"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Esquesses et types de structures algbriques", "author": ["C. Ehresmann"], "venue": "Bull. Instit. Polit., XIV,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1968}, {"title": "Sketches of an Elephant: A Topos Theory Compendium", "author": ["P. Johnstone"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Massively parallel reasoning. in: Automated Deduction - A Basis foe", "author": ["S. Bornscheuer", "S. H\u00f6lldobler", "Y. Kalinke", "A. Strohmaier"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Logic programs and connectionist networks", "author": ["P. Hitzler", "S. H\u00f6lldobler", "A. Seda"], "venue": "Journal of Applied Logic,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Challenge problems for the integration of logic and connectionist systems", "author": ["S. H\u00f6lldobler"], "venue": "Proceedings 14. Workshop Logische Programmierung, GMD Report", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Advances in neural-symbolic learning systems: Modal and temporal reasoning", "author": ["A.S. d\u2019Avila Garcez"], "venue": "Perspectives of Neural- Symbolic Integration, Studies in Computational Intelligence, Volume", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Connectionistic representation of multi-valued logic programs", "author": ["E. Komendantskaya", "M. Lane", "A.K. Seda"], "venue": "Studies in Computational Intelligence, Volume", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "The logic of neural networks", "author": ["J. Castro", "E. Trillas"], "venue": "Mathware and Soft Computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Optimal brain surgeon and general network pruning", "author": ["B. Hassibi", "D. Stork", "G. Wolf"], "venue": "IEEE International Conference on Neural Network,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "An overview of generalised basic logic algebra", "author": ["P. Jipsen"], "venue": "Neural Network World,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Functional representation of many-valued logics based on continuous t-norms", "author": ["B. Gerla"], "venue": "PhD thesis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Fuzzy logic and probability", "author": ["P. H\u00e1jek", "L. Godo", "Esteva"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}, {"title": "Categories, Allegories", "author": ["P. Freyd", "A. Scedrov"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1990}, {"title": "Handbook of Categorical Algebra 1: Basic Category Theory", "author": ["B. Borceux"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "One setting for all: Metric, topology, uniformity, approach structure", "author": ["M. Clementino", "D. Hofmann", "W. Tholen"], "venue": "Theory Appl. Categ., v.11 n.15,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "The entity-relationship model - towards a unified view of data", "author": ["P. Chen"], "venue": "ACM Translations on Database Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1976}, {"title": "The functional data model and the data language daplex", "author": ["D. Shipman"], "venue": "ACM Translations on Database Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1981}, {"title": "What vs. how of visual modeling: The arrow logic of graphic notation", "author": ["Z. Diskin", "B. Kadish", "F. Piessens"], "venue": "Behavioral Specifications in Businesses and Systems, Eds. H. Kilov et al, Kluwer Acad.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Aspects of topoi", "author": ["P. Freyd"], "venue": "Bulletin of the Australian Mathematical Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1972}, {"title": "Category Theory for Computing Science. Prentice-Hall International Series in Computer Science, Prentice-Hall", "author": ["M. Barr", "C. Wells"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1990}, {"title": "Generalized sketches as a framework for completeness theorems", "author": ["M. Makkai"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1993}, {"title": "Humans, computers, specifications: The arrow logic of information systems engineering", "author": ["Z. Diskin", "B. Kadish", "F. Pissen"], "venue": "Int. J. of Computing Anticipatory Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Neural networks and rational lukasiewicz logic", "author": ["P. Amato", "A. Nola", "B. Gerla"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "Semantics of architectural connectors", "author": ["J. Fiadeiro", "A. Lopes"], "venue": "TAPSOFT\u201997 LNCS, v.1214,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1997}, {"title": "A modified regression algorithm for fast one layer neural network training", "author": ["T. Andersen", "B. Wilamowski"], "venue": "World Congress of Neural Networks, Washington DC, USA,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1995}, {"title": "Frist- and second-order methods for learning between steepest descent and newton\u2019s method", "author": ["R. Battiti"], "venue": "Neural Computation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1992}, {"title": "Neural Network Design", "author": ["M. Hagan", "H. Demuth", "M. Beal"], "venue": "PWS Publishing Company, Boston", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1996}, {"title": "Reverse engineering and simbolic knowledge extraction on lukasiewicz logics using neural networks", "author": ["C. Leandro"], "venue": "International Conference on Fuzzy Computation, IJCCI2009 proceedings,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Rational lukasiewicz logic and divisible mv-algebras", "author": ["B. Gerla"], "venue": "Neural Networks World,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Unifying logical and statistical ai", "author": ["P. Domingos", "S. Kok", "H. Poon", "M. Richardson", "P. Singla"], "venue": "In Proceeding of the Twenty-First National Conference on Artificial Intelegence", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "In this work we extend traditional algebraic specification methods [2] in this direction.", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "Here we assume that such interface for AI emerges from an adequate Neural-Symbolic integration [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "This integration is made for universe of discourse described on a Topos[4] governed by a many-valued Lukasiewicz logic.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "For the train of such neural networks we changed the Levenderg-Marquardt algorithm [5], restricting the knowledge dissemination in the network structure using soft crystallization.", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "The data specification requires finite, effective and comprehensive presentation of complete structures, this type of methodology was explored on Category Theory for algebraic specification by Ehresmann[6].", "startOffset": 202, "endOffset": 205}, {"referenceID": 5, "context": "for definition [7]), defined by relation evaluated in a many-valued logic.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "In the context of classic logic see [8] [9] [10], for the extraction of logic programs from trained networks.", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "In the context of classic logic see [8] [9] [10], for the extraction of logic programs from trained networks.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "In the context of classic logic see [8] [9] [10], for the extraction of logic programs from trained networks.", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "For the extraction of modal and temporal logic programs see [11] and [3].", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "For the extraction of modal and temporal logic programs see [11] and [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "In [12] we can find processes to generate connectionist representation of multi-valued logic programs and for Lukasiewicz logic programs ( LL) [?].", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Every logic connective can be defined by a neuron in an artificial network having, by activation function, the identity truncated to zero and one [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 3, "context": "using the Levenderg-Marquardt (LM) algorithm [5], and the generated network can be simplified using the \u201dOptimal Brain Surgeon\u201d algorithm proposed by B.", "startOffset": 45, "endOffset": 48}, {"referenceID": 12, "context": "Stork [14].", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "Both, the logic and the algebraic semantics have been generalized in many directions [15].", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "These two operators induce a structure of residuated poset on a partially ordered set of truth values P [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "In [16], it is described as a binary operator defined in [0, 1] commutative and associative, non-decreasing in both arguments, 1\u2297 x = x and 0\u2297 x = 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "The Lukasiewicz fusion operator x\u2297y = max(0, x+y\u22121), its residue x\u2297y = min(1, 1\u2212x+y), and the lattice operators x\u2228y = max{x, y} and x\u2227y = minx, y, defined in \u03a9 = [0, 1] a structure of resituated lattice [15] since: 1.", "startOffset": 203, "endOffset": 207}, {"referenceID": 15, "context": "Petr H\u00e1jek presented in [18] extends this principle by embedding probabilistic logic in LL, for this we associated to each boolean formula \u03c6 a fuzzy proposition \u201d\u03c6 is provable\u201d.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "In next section we describe this type of relations in the context of allegory theory [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "The category \u03a9-Set is a symmetric monoidal closed category [20], where the tensor product of \u03a9-sets is given for \u03b1 : A and \u03b2 : B by \u03b1\u2297 \u03b2 : A\u00d7B defined (\u03b1\u2297 \u03b2)(a, b) = \u03b1(a) \u2297 \u03b2(b).", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "Functor \u03b1\u2297 have by left adjunct a functor \u03b1 \u22b8 : \u03a9-Set \u2192 \u03a9-Set, defined for \u03a9-sets \u03b2 : B by \u03b1 \u22b8 \u03b2 : [A,B], construct as the internalization for \u03a9-set Hom [21] (\u03b1 \u22b8 \u03b2)(t, h) = \u2228", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "This adjunction \u03b1\u2297 \u22a2 \u03b1 \u22b8 have by unit [20] the natural transformation, \u03bb defined for each \u03a9-set \u03b3 : C, by a multi-morphism \u03bb\u03b3 : (\u03b1 \u22b8 \u03b3)\u2297 \u03b1 \u21c0 \u03b3,", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "The classically definition of limit for a diagram, in the category of sets, can been as a way to internalize the structure of a diagram in form of a table [20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "Limits, colimits and commutativity can be used on the specification of structures [2].", "startOffset": 82, "endOffset": 85}, {"referenceID": 17, "context": "The construction of a colimit reduces to that of two coproducts and a coequalizer, siting [20], in the category of sets governed by classic logic the explicit description of a coequalizer is generically very technical since it involves the description of the equivalence relation generated by a family of pairs.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "All the widely used data specification mechanisms (like Entity Relationship Model [22], the Fundamental Data Model [23], the Generic Semantic Model [24]), OOA&D-schemas in a million of versions and UML which itself comprises a host of various notations, have a strong graphical component.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "All the widely used data specification mechanisms (like Entity Relationship Model [22], the Fundamental Data Model [23], the Generic Semantic Model [24]), OOA&D-schemas in a million of versions and UML which itself comprises a host of various notations, have a strong graphical component.", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "Sketches where invented by Charles Ehresmann and can be perceived as a graphic based logic, which formalizes in a precise and uniform way the semantic of graph with marks [25].", "startOffset": 171, "endOffset": 175}, {"referenceID": 0, "context": "However, it is known from [2] what where are structures specifiable using sketches but not in first-order logic.", "startOffset": 26, "endOffset": 29}, {"referenceID": 21, "context": "This result allows to use a mixture of limits, colimits and formulas in the UoD specification and be sure that the resulting specification can always be translated to a specification system [25].", "startOffset": 190, "endOffset": 194}, {"referenceID": 22, "context": "For this we used a string-based codification for specification systems named relational specification, generalizing the notion of essentially algebraic specification: the original idea goes back to Freyd [26] and having the same expressive power as finite limit sketches [27].", "startOffset": 204, "endOffset": 208}, {"referenceID": 23, "context": "For this we used a string-based codification for specification systems named relational specification, generalizing the notion of essentially algebraic specification: the original idea goes back to Freyd [26] and having the same expressive power as finite limit sketches [27].", "startOffset": 271, "endOffset": 275}, {"referenceID": 24, "context": "A number of proof systems for essentially algebraic specification have been introduced [28].", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "Makkai [28] and Z.", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "Diskin [29].", "startOffset": 7, "endOffset": 11}, {"referenceID": 26, "context": "As mentioned in [30] there is a lack of a deep investigation of the relationships between logics and NNs.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "In [13] it is shown how, by taking as activation function, \u03c8, the identity truncated to zero and one,", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "\u03c8(x)=min(1,max(x,0)), it is possible to represent the corresponding NN as a combination of propositions of Lukasiewicz calculus and vice-versa [30].", "startOffset": 143, "endOffset": 147}, {"referenceID": 27, "context": "This task of construct complex structures based on simplest ones can be formalized using generalized programming [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 3, "context": "The Levenberg-Marquardt (LM) algorithm [5] [32] ensued from the development of EBP algorithm-dependent methods.", "startOffset": 39, "endOffset": 42}, {"referenceID": 28, "context": "The Levenberg-Marquardt (LM) algorithm [5] [32] ensued from the development of EBP algorithm-dependent methods.", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "It gives a good exchange between the speed of the Newton algorithm and the stability of the steepest descent method [33].", "startOffset": 116, "endOffset": 120}, {"referenceID": 30, "context": "In this way, the performance function is always reduced at each iteration of the algorithm [34].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "The optimization problem is constrained, following [14], we form the Lagrangian operator L = 1 2 \u2206wHw\u2206w + \u03bb(e T q \u2206wq + wq), where \u03bb is a Lagrange undetermined multiplier.", "startOffset": 51, "endOffset": 55}, {"referenceID": 31, "context": "5 GENERATING ARTIFICIAL FUZZY STRUCTURES We tested our methodology in real data sets [35] and on artificially generated data structures.", "startOffset": 85, "endOffset": 89}, {"referenceID": 32, "context": "This type of fuzzy automata have its behaviour described using formulas of Relational Lukasiewicz logic, introduced in [36], outfitting the scope of this work.", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "This can be minimized by regularizing the produced NN using \u201dOptimal Brain Surgeon\u201d method [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 33, "context": "Here, classic graphical models like Bayesian and Markov networks have to some extent played the part of an interface layer, but one with a limited range having insufficiently expressive for general AI [37].", "startOffset": 201, "endOffset": 205}], "year": 2016, "abstractText": "The development of machine learning in particular and artificial intelligent in general has been strongly conditioned by the lack of an appropriate interface layer between deduction, abduction and induction [1]. In this work we extend traditional algebraic specification methods [2] in this direction. Here we assume that such interface for AI emerges from an adequate Neural-Symbolic integration [3]. This integration is made for universe of discourse described on a Topos[4] governed by a many-valued Lukasiewicz logic. Sentences are integrated in a symbolic knowledge base describing the problem domain, codified using a graphicbased language, wherein every logic connective is defined by a neuron in an artificial network. This allows the integration of first-order formulas into a network architecture as background knowledge, and simplifies symbolic rule extraction from trained networks. For the train of such neural networks we changed the Levenderg-Marquardt algorithm [5], restricting the knowledge dissemination in the network structure using soft crystallization. This procedure reduces neural network plasticity without drastically damaging the learning performance, allowing the emergence of symbolic patterns. This makes the descriptive power of produced neural networks similar to the descriptive power of Lukasiewicz logic language, reducing the information lost on translation between symbolic and connectionist structures. We tested this method on the extraction of knowledge from specified structures. For it, we present the notion of fuzzy state automata, and we use automata behaviour to infer its structure. We use this type of automata on the generation of models for relations specified as symbolic background knowledge. Using the involved automata behaviour as data sets, we used our learning methodology, to extract new insights about the models, and inject them into the specification. This allows the improvement about the problem domain knowledge. \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013", "creator": "LaTeX with hyperref package"}}}