{"id": "1604.03506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "An Unbiased Data Collection and Content Exploitation/Exploration Strategy for Personalization", "abstract": "One of missions for personalization systems and recommender systems is to show content items according to users' personal interests. In order to achieve such goal, these systems are learning user interests over time and trying to present content items tailoring to user profiles. Recommending items according to users' preferences has been investigated extensively in the past few years, mainly thanks for the popularity of Netflix competition. In a real setting, users may be attracted by a subset of those items and interact with them, only leaving partial feedbacks to the system to learn in the next cycle, which leads to significant biases into systems and hence results in a situation where user engagement metrics cannot be improved over time. The problem is not just for one component of the system. The data collected from users is usually used in many different tasks, including learning ranking functions, building user profiles and constructing content classifiers. Once the data is biased, all these downstream use cases would be impacted as well. Therefore, it would be beneficial to gather unbiased data through user interactions. Traditionally, unbiased data collection is done through showing items uniformly sampling from the content pool. However, this simple scheme is not feasible as it risks user engagement metrics and it takes long time to gather user feedbacks. In this paper, we introduce a user-friendly unbiased data collection framework, by utilizing methods developed in the exploitation and exploration literature. We discuss how the framework is different from normal multi-armed bandit problems and why such method is needed. We layout a novel Thompson sampling for Bernoulli ranked-list to effectively balance user experiences and data collection. The proposed method is validated from a real bucket test and we show strong results comparing to old algorithms", "histories": [["v1", "Tue, 12 Apr 2016 18:32:43 GMT  (38kb,D)", "http://arxiv.org/abs/1604.03506v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["liangjie hong", "adnan boz"], "accepted": false, "id": "1604.03506"}, "pdf": {"name": "1604.03506.pdf", "metadata": {"source": "CRF", "title": "An Unbiased Data Collection and Content Exploitation/Exploration Strategy for Personalization", "authors": ["Liangjie Hong", "Adnan Boz"], "emails": ["liangjie@yahoo-inc.com,", "adnanb@yahoo-inc.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "It is critical for online service providers to show content items according to users\u2019 personal interests. The process is sometimes denoted as \u201cPersonalization\u201d where user interests are learned over time and presentations of content items are tailored to user profiles. Recommending items according to users\u2019 preferences has been investigated extensively in the past few years, mainly thanks for the success of Netflix competition where the notion of \u201cRecommender Systems\u201d has been brought under the attention.\nA normal setting of a recommender system is that, when a user comes into the service, a few, ranging from a handful to hundreds, of items are shown to the user according to his/her profile. The user may be attracted by a subset\nof those items and interacts with them by clicking, commenting, re-posting, thumb-up or thumb-down those items. These user feedbacks are later incorporated into the next round of model training/testing/evaluation, in order to improve the effectiveness of personalization over time. In an ideal scenario, a user would go over all items prepared by the system and express his/her preferences. However, due to limited resource constraints like time budges and spaces, a user may only interact with very few, sometimes, even one or two items from the prepared list, leaving most items untouched. Therefore, systems are only learning from partial feedback data from users [6]. This cycle leads to significant biases into systems and hence results in a situation where user engagement metrics cannot be improved over time. The problem is not just for one component of the system. The data collected from users is usually used in many different tasks, including learning ranking functions, building user profiles and constructing content classifiers. Once the data is biased, all these downstream use cases would be impacted as well. Therefore, it would be beneficial to gather unbiased data through user interactions.\nA generic approach to reduce biases is to show those under-represented items, balancing the set of items that are already interacted with users and the remaining set of items that are not interacted by or not even shown to users, with the risk of jeopardizing user experiences. The problem of exploiting and exploring items from the content pool with the aim to optimize a particular user engagement metric like Click-Through-Rate (CTR) has been intensively studied in a wide range of scenarios (e.g., [1, 4, 8]). In general, algorithms are proposed with a certain metric to optimize in mind. Each item contributes to the overall metric with uncertainty. As the exact amount of contribution is not known a prior, algorithms need to balance between choosing items with existing good performances and items with potential contributions. Different algorithms would utilize different strategies to exploit and explore items, either in deterministic or randomized ways.\nAlthough existing exploit/explore frameworks are effectives tools to optimize a metric with uncertainty, they cannot be used directly to address the issue of biases and partial feedbacks from users. In particular, these methods are proposed to optimize a metric not to remove biases. Even if they are exploring items that are never shown before, once the uncertainty of these items with respect to contributing to the metric is exploited enough, algorithms would concentrate on the ones yielding largest rewards, achieving the optimal performance. On the contrary, any solution to re-\nar X\niv :1\n60 4.\n03 50\n6v 1\n[ cs\n.I R\n] 1\n2 A\npr 2\n01 6\nduce or remove biases in online services does not necessarily need to optimize any metrics. Indeed, for a mature online system, multiple, sometimes even competing, metrics might exist and thus, it is not wise for the solution to bias towards any one of them. Another extreme, which might mitigate the issue of biases, is to show items in uniformly random. Although this strategy will remove biases completely, it is not an efficient way to gather data from users. For instance, if a user faces a completely random items from a large content pool, it is very unlikely that these items are relevant to the user.\nIn this paper, we propose a novel unbiased data collection strategy and utilize Bayesian posterior sampling to balance exploitation and exploration to improve user experiences when gathering data. We demonstrate the effectiveness of the algorithm through a bucket test in France."}, {"heading": "2. EXPLOITATION AND EXPLORATION", "text": "In this section, we review a particular type of exploitation and exploration algorithms: contextual multi-armed bandit methods and we will develop them to suit unbiased data collection. In order to simplify the discussion, we use CTR as the metric to optimize."}, {"heading": "2.1 Contextual Multi-Armed Bandit Problems", "text": "A multi-armed bandit (MAB) problem is a sequential decision making process. Bandit problems involve making a decision in each round. Once a decision is made an observation is collected and the corresponding reward computed. The contextual-bandit formalism generalizes this classic setting by introducing contextual information in the interaction loop.\nFormally, we define by A = {1, 2, \u00b7 \u00b7 \u00b7 ,K}, a set of actions, a contextual vector xt \u2208 X , a reward vector rt = {rt,1, \u00b7 \u00b7 \u00b7 , rt,K}, where each rt,a \u2208 [0, 1] and a policy \u03c0 : X \u2192 A. A contextual bandit (cMAB) describes a round-byround interaction between a learner and the environment. At each round t, the problem can be decomposed into following steps:\n\u2022 The environment chooses (xt, rt) from some unknown distribution D. Only xt is revealed to the learner while the reward is not.\n\u2022 Upon observing xt, the learner uses some policy \u03c0 to choose an action a \u2208 A, and in return observes the corresponding reward rt,a.\n\u2022 (Optional) The policy \u03c0 is revised with the data collected for this round.\nHere, we define \u03c0 is parameterized by an unknown parameter \u03b8. Ideally, we would like to choose the policy maximizing the expected reward:\narg max \u03c0\nEx,r\u223cD [ r\u03c0(x;\u03b8) ] If we are just interested in maximizing the immediate reward, then one should choose the action that maximize:\u222b\nEx,r\u223cD [ r\u03c0(x;\u03b8) ] P (\u03b8 | Q) d\u03b8\nAlgorithm 1 Thompson Sampling for the Bernoulli Bandit\nRequire: \u03b1, \u03b2 prior parameters of a Beta distribution Si = 0 and Fi = 0, \u2200i {Success and failure counters} for t = 1, \u00b7 \u00b7 \u00b7 , T do\nfor i = 1, \u00b7 \u00b7 \u00b7 ,K do Draw \u03b8i according to Beta(Si + \u03b1, Fi + \u03b2). end for Draw arm i\u0302 = arg maxi \u03b8i and observe reward r if r = 1 then\nSi\u0302 = Si\u0302 + 1 else\nFi\u0302 = Fi\u0302 + 1 end if\nend for\nwhere Q is the posterior distribution of \u03b8, given contextual information and reward information. As we discussed before, cMAB would maximize expected reward, which may not be a good thing for reducing biases. However, cMAB provides a nice framework to our later proposed method.\n2.2 Thompson Sampling for K-armed Bernoulli Bandit\nThompson sampling [2] is an effective way to conduct exploitation and exploration through Bayesian posterior sampling, optimizing CTR in long-run. In an exploration/exploitation setting, we randomly selection an action according to its probability of being optimal. Therefore, we draw a set of random parameters \u03b8, which characterizes each arm in the current round, and pick the one that yields the maximum reward.\nIn the standard K-armed Bernoulli bandit, each action corresponds to the choice of an arm. The reward of the i-th arm follows a Bernoulli distribution with mean \u03b8\u2217i . It is standard to model the mean reward of each arm using a Beta distribution since it is the conjugate distribution of the binomial distribution. The instantiation of Thompson sampling for the Bernoulli bandit is given in Algorithm 1. Although it seems promising in the first place, K-armed Bernoulli bandit can be hardly applied to real-world personalization ranking problems. One major issue of the algorithm is that it is an item-based strategy. Namely, in each round, only a single item is selected.\nOne way to extend an item-wise Bernoulli bandit to a listwise Bernoulli bandit is to use a permutation of all items as an arm. Each permutation will be associated with a parameter and we could draw a sample from its posterior distribution to determine whether to choose this arm or not. However, it is in general difficult to estimate such parameters without any assumptions. With a strong independent assumption, one could sample the posterior distribution of a list-wise arm by jointly sampling posterior distributions from each individual item\u2019s posterior. With further assumptions, in a recently proposed work [3], the authors proposed such method to tackle the problem of playing subsets of bandit arms. The proposed method is to sample individual arms\u2019 parameters from their posterior distribution and then select the topN arms deterministically. However, the optimality of the algorithm is not proven for the Beta/Bernoulli case. We treat it as a heuristic for Bernoulli ranked-list case, shown in Algorithm 2.\nAlgorithm 2 Thompson Sampling for Bernoulli Rankedlist Bandit\nRequire: \u03b1, \u03b2 prior parameters of a Beta distribution Si = 0 and Fi = 0, \u2200i {Success and failure counters} for t = 1, \u00b7 \u00b7 \u00b7 , T do\nfor i = 1, \u00b7 \u00b7 \u00b7 ,K do Draw \u03b8i according to Beta(Si + \u03b1, Fi + \u03b2). end for Sort \u03b8i in the descending order Select the top N items and observe N rewards. Update S and F for those N items.\nend for"}, {"heading": "3. UNBIASED OFFLINE EVALUATION", "text": "In this section, we introduce the basic framework to conduct unbiased data collection. Before we dive into details, there are two basic design criterions any approach to such issue needs to meet:\n1. The dataset is collected in an unbiased fashion or with bias but the bias could be countered in later analysis.\n2. The proposed method needs to have a reasonable user engagement performance such that users do not need to suffer from any data collection strategies.\nThese two design criterions are usually not the focus in classic exploitation/exploration literature.\n3.1 Unbiased Data Collection Through cMAB cMAB is not only a powerful way to balance exploration and exploitation but also a method to construct unbiased offline evaluation process, suggested by [5]. The basic idea is that, we use a known policy to operate a cMAB problem for collecting data and record the action to be performed, the reward, as well as the probability to select the winning arm at each round.\nSimilar to cMAB, we have the following procedure:\n1. The environment chooses (xt, rt) from some unknown distribution D. Only xt is revealed to the learner while the reward is not.\n2. Upon observing xt, the learner computes a multinomial distribution p over the actions A. A random action a is drawn according to the distribution and in return observes the corresponding reward rt,a.\n3. The contexutal vector xt, the selected action a, reward rt,a and the probability mass pa are logged.\nComparing this to the standard cMAB procedure, one could observe that the only difference is that, the optimal action is not pursued every single round while a random action is selected. As [?], this is critical to perform causal inference in an offline environment and hence is important for unbiased data collection as well. Additionally, the probability to the winning action is logged where it can be used as propensity score for further analysis. In order evaluate the value of a policy \u03c0 offline, the following estimator is used:\nV\u0302 (\u03c0) = \u2211\n(x,a,ra,pa)\u2208D\nraI(\u03c0(x) == a) pa\n(1)\nAlgorithm 3 Thompson Sampling for Bernoulli Rankedlist Bandit\nRequire: \u03b1, \u03b2 prior parameters of a Beta distribution Si = 0 and Fi = 0, \u2200i {Success and failure counters} for t = 1, \u00b7 \u00b7 \u00b7 , T do\nfor i = 1, \u00b7 \u00b7 \u00b7 ,K do Draw \u03b8i according to Beta(Si + \u03b1, Fi + \u03b2). end for Compute p such that pk = \u03b8k\u2211 \u03b8k . Sample N items from Mult.(p). Observe N rewards rt. Update S and F for those N items according to rt. Logging N items, p and rt.\nend for\nThis framework (but not exactly same) stemmed from [7], also discussed in [?, 8]. The main difference is that, these related work still uses the framework to evaluate cMAB problems and therefore, requiring choosing the best possible arm in every round whereas in this framework, each arm is performed stochastically. Note that, if p is uniform over all arms, it is essentially uniformly random strategy that has been used frequently in the past, like [4, 2]."}, {"heading": "3.2 Unbiased Policy Evaluation", "text": "In this sub-section, we show that Equation 1 is an unbiased offline policy evaluation procedure. We define the value of a policy \u03c0 as:\nV (\u03c0) = E(x,r)\u2208D [ ra ]\n= \u222b (x,r)\u2208D raP (x, r) dxdr\nIn such case, we want to prove: ED\u2208D [ V\u0302offline(\u03c0) ] = V (\u03c0)\nLet us expand the left hand side as:\u222b D\u2208D [ \u2211 (x,a,ra,pa)\u2208D raI(\u03c0(x) == a) pa ] P (D) dD\nThe important step in the proof is that we need to make use of the following quantity:\nEa\u2208p [raI(\u03c0(x) == a)\npa\n] = raI(\u03c0(x) == a) = r\u03c0(x) (2)\nThus, on expectation, we have:\u222b D\u2208D [ \u2211 (x,a,ra,pa)\u2208D raI(\u03c0(x) == a) pa ] P (D) dD =\n\u222b D\u2208D [ \u2211 (x,r)\u2208D r\u03c0(x) ] P (D) dD = \u222b (x,r)\u2208D raP (x, r) dxdr\nThe last line comes from the fact that (x, r) from D are sampled from D i.i.d. and D is a random sample from D. The integral also mounts to D. Note that the key part in Equation 2 is that we need to select arms stochastically.\n3.3 Thompson Sampling for K-armed Unbiased Data Collection\nWe adapt Algorithm 2 to the unbiased data collection case, shown in Algorithm 3. The main difference between these two algorithms is that, in stead of deterministically choosing the best arm (ranked-list) in every round, Algorithm 3 chooses each arm probabilistically.\nAlgorithm 3 has several nice properties:\n\u2022 It uses CTR as an underlying metric to drive the data collection process. Therefore, the user engagement would be reasonable as high CTR items will have higher chances to be selected into the ranked-list.\n\u2022 Both \u03b1 and \u03b2 can be tweaked such that we control the variance of items to be sampled. Also, prior knowledge about items can be easily embedded into these parameters.\nIndeed, Beta distribution and Bernoulli bandits are not necessary for the algorithm. One can easily replace these settings with other underlying metrics and distributions.\nFor implementation details, we notice that two steps, shown in bold in Algorithm 3, are expensive for every request, give we could have millions of items. In particular, it is slow to sample N out of M items without replacement for every round. Here, we use a further approximation. Instead of sampling N from M items, with proportional to their probability to be clicked, we compute the following quantity:\n\u03b8\u2032i = \u03bb\u03b8i\nwhere \u03bb \u223c Unif.(0, 1). Then, we sort \u03b8\u2032i and select the top N items. This procedure is much more efficient and also supported as:\nE[P (c | i)] = E[\u03b8\u2032i] = 1\n2 E[\u03b8i]\nTherefore, the expectation to be clicked is within a constant factor of the expectation of \u03b8i. Note that, we do need to compute the normalization factor by using \u03b8\u2032i values."}, {"heading": "4. ONLINE EXPERIMENTS", "text": "In this section, we demonstrate how one version of Algorithm 3 is deployed in production and its effectiveness from a bucket test. We use Yahoo\u2019s France Homepage (Slingstone France) as a testbed. Before we deployed this algorithm, Slingstone France was running a deterministic ranking which is essentially to rank items chronologically. As we will see, this algorithm has a poor user engagement metric. For using our proposed algorithm, we use 6-hour average CTR for a bucket (5% of traffic) as the parameter for \u03b1 and \u03b2. Therefore, a new item would have a reasonable starting point, instead of zero CTR. We launched the algorithm in bucket 203 in France in June 20, 2014. For comparison purpose, we compare the bucket to a General-Available (GA) bucket (bucket 684), which has the same size of traffic. Here, we are interested in three user engagement metrics:\n\u2022 Dwell Time Per Depth: Post-click dwell time on article pages, divided by the total number of items viewed in the stream, denoted as S1.\n\u2022 Dwell Time Per User: Total amount of dwell time, divided by the total number of users.\n\u2022 Clicks per PageView: Total number of clicks per pageview.\nWe show metric changes before and after the launch of the new algorithm in Figure 1. We are interested in two things: 1) user engagement metrics should be improved by the new algorithm, 2) the new algorithm is not optimizing CTR as well. From the figure, we can obviously see that all three user engagement metrics has been improved since June 20th. Before that, bucket 203, which was running old deterministic chronological stream, suffered from all metrics. Right after the launch, all metrics jumped closer to the GA bucket. However, we do notice that both Dwell Time Per User and Clicks per PageView, two main indicators for CTR, do not perform as good as GA, meaning that the algorithm is optimizing CTR, which is a good thing for the data collection bucket. At the same time, S1 is always comparable as the GA bucket, implying that users engage with the stream in the data collection bucket as well.\nFor the data collection bucket, we are also interested in how it is effectively exploring the whole content pool. In other words, the data collection bucket should pay more attention to all kinds of items and the skewness of its view distribution, click distribution should be less than the previous algorithm. We show the comparison between the old algorithm and the new algorithm in Table 1 in a number of distributions. We can see that, from view distributions, click distributions and CTR distributions, the newly proposed algorithm significantly improves the skewness and the distributions are more balanced. Indeed, the results show that the view skewness improved 30%, click skewness improved 2%, CTR skewness improved 30% and the article cold-start has improved dramatically; the articles seen in the first 30 minutes went up from 66% to 81% and in the first 2 hours went up from 82% to 92%.\n5. CONCLUSION AND FUTURE WORK\nTable 1: Comparison between the old exploration algorithm and the new exploration algorithm on a number of key distributions.\nAlgorithm Metrics Skewness Mean Median\nNew Algorithm View Distribution 6.76 10, 868.46 2, 500.00 Old Algorithm 9.65 2, 328.70 441.50 New Algorithm\nClick Distribution 14.46 1, 059.25 64.00\nOld Algorithm 14.64 241.17 7.00 New Algorithm\nCTR Distribution 2.28 0.04 0.03\nOld Algorithm 3.87 0.03 0.02 New Algorithm\nItem Cold-Start Distribution 1.15 37.26 13.86\nOld Algorithm 3.47 100.02 13.05\nIn this paper, we introduce a new algorithm to gather unbiased data with reasonable user engagement metrics. We discussed how it differs from traditional exploitation and exploration work and why the proposed framework would gather unbiased data. We demonstrated the effectiveness of the proposed approach through a live bucket test and showed that the method significantly improved the user engagement metrics and the skewness of a number of distributions of items.For future work, we would like to compare more variants of the framework and train machine learning models (e.g., ranking models, user profiling models) from the data we gathered to demonstrate that model training process can benefit from the data."}, {"heading": "6. REFERENCES", "text": "[1] D. Agarwal, B. Chen, and P. Elango. Explore/exploit\nschemes for web content optimization. In Proceedings of the The Ninth IEEE International Conference on Data Mining, pages 1\u201310, 2009.\n[2] O. Chapelle and L. Li. An empirical evaluation of thompson sampling. In Advances in Neural Information Processing Systems 24, pages 2249\u20132257. 2011.\n[3] A. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex online problems. In Proceedings of the 31th International Conference on Machine Learning, pages 100\u2013108, 2014.\n[4] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web, pages 661\u2013670, 2010.\n[5] L. Li, J. Y. Kim, and I. Zitouni. Toward predicting the outcome of an a/b experiment for search relevance. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, pages 37\u201346, 2015.\n[6] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference Annual Conference on Uncertainty in Artificial Intelligence, pages 452\u2013461, 2009.\n[7] A. Strehl, J. Langford, L. Li, and S. M. Kakade. Learning from logged implicit exploration data. In Advances in Neural Information Processing Systems 23, pages 2217\u20132225. 2010.\n[8] L. Tang, R. Rosales, A. Singh, and D. Agarwal. Automatic ad format selection via contextual bandits.\nIn Proceedings of the 22nd ACM International Conference on Information and Knowledge Management, pages 1587\u20131594, 2013."}], "references": [{"title": "Explore/exploit schemes for web content optimization", "author": ["D. Agarwal", "B. Chen", "P. Elango"], "venue": "In Proceedings of the The Ninth IEEE International Conference on Data Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Thompson sampling for complex online problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "In Proceedings of the 19th International Conference on World Wide Web,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Toward predicting the outcome of an a/b experiment for search relevance", "author": ["L. Li", "J.Y. Kim", "I. Zitouni"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Learning from logged implicit exploration data", "author": ["A. Strehl", "J. Langford", "L. Li", "S.M. Kakade"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Automatic ad format selection via contextual bandits", "author": ["L. Tang", "R. Rosales", "A. Singh", "D. Agarwal"], "venue": "Proceedings of the 22nd ACM International Conference on Information and Knowledge Management,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Therefore, systems are only learning from partial feedback data from users [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": ", [1, 4, 8]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 3, "context": ", [1, 4, 8]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 7, "context": ", [1, 4, 8]).", "startOffset": 2, "endOffset": 11}, {"referenceID": 0, "context": "Formally, we define by A = {1, 2, \u00b7 \u00b7 \u00b7 ,K}, a set of actions, a contextual vector xt \u2208 X , a reward vector rt = {rt,1, \u00b7 \u00b7 \u00b7 , rt,K}, where each rt,a \u2208 [0, 1] and a policy \u03c0 : X \u2192 A.", "startOffset": 153, "endOffset": 159}, {"referenceID": 1, "context": "Thompson sampling [2] is an effective way to conduct exploitation and exploration through Bayesian posterior sampling, optimizing CTR in long-run.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "With further assumptions, in a recently proposed work [3], the authors proposed such method to tackle the problem of playing subsets of bandit arms.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "cMAB is not only a powerful way to balance exploration and exploitation but also a method to construct unbiased offline evaluation process, suggested by [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "This framework (but not exactly same) stemmed from [7], also discussed in [?, 8].", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "Note that, if p is uniform over all arms, it is essentially uniformly random strategy that has been used frequently in the past, like [4, 2].", "startOffset": 134, "endOffset": 140}, {"referenceID": 1, "context": "Note that, if p is uniform over all arms, it is essentially uniformly random strategy that has been used frequently in the past, like [4, 2].", "startOffset": 134, "endOffset": 140}], "year": 2016, "abstractText": "One of missions for personalization systems and recommender systems is to show content items according to users\u2019 personal interests. In order to achieve such goal, these systems are learning user interests over time and trying to present content items tailoring to user profiles. Recommending items according to users\u2019 preferences has been investigated extensively in the past few years, mainly thanks for the popularity of Netflix competition. In a real setting, users may be attracted by a subset of those items and interact with them, only leaving partial feedbacks to the system to learn in the next cycle, which leads to significant biases into systems and hence results in a situation where user engagement metrics cannot be improved over time. The problem is not just for one component of the system. The data collected from users is usually used in many different tasks, including learning ranking functions, building user profiles and constructing content classifiers. Once the data is biased, all these downstream use cases would be impacted as well. Therefore, it would be beneficial to gather unbiased data through user interactions. Traditionally, unbiased data collection is done through showing items uniformly sampling from the content pool. However, this simple scheme is not feasible as it risks user engagement metrics and it takes long time to gather user feedbacks. In this paper, we introduce a user-friendly unbiased data collection framework, by utilizing methods developed in the exploitation and exploration literature. We discuss how the framework is different from normal multiarmed bandit problems and why such method is needed. We layout a novel Thompson sampling for Bernoulli ranked-list to effectively balance user experiences and data collection. The proposed method is validated from a real bucket test and we show strong results comparing to old algorithms.", "creator": "LaTeX with hyperref package"}}}