{"id": "1604.03968", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Visual Storytelling", "abstract": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.", "histories": [["v1", "Wed, 13 Apr 2016 20:27:43 GMT  (3042kb,D)", "http://arxiv.org/abs/1604.03968v1", "to appear in NAACL 2016"]], "COMMENTS": "to appear in NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["ting-hao huang", "francis ferraro", "nasrin mostafazadeh", "ishan misra", "aishwarya agrawal", "jacob devlin", "ross b girshick", "xiaodong he", "pushmeet kohli", "dhruv batra", "c lawrence zitnick", "devi parikh", "lucy vanderwende", "michel galley", "margaret mitchell"], "accepted": true, "id": "1604.03968"}, "pdf": {"name": "1604.03968.pdf", "metadata": {"source": "CRF", "title": "Visual Storytelling", "authors": ["Ting-Hao (Kenneth) Huang", "Francis Ferraro", "Nasrin Mostafazadeh", "Ishan Misra", "Aishwarya Agrawal", "Jacob Devlin", "Ross Girshick", "Xiaodong He", "Pushmeet Kohli", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh", "Lucy Vanderwende", "Michel Galley", "Margaret Mitchell"], "emails": ["jdevlin@microsoft.com", "lucyv@microsoft.com", "mgalley@microsoft.com", "memitc@microsoft.com"], "sections": [{"heading": null, "text": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND1 v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression."}, {"heading": "1 Introduction", "text": "Beyond understanding simple objects and concrete scenes lies interpreting causal structure; making sense of visual input to tie disparate moments together as they give rise to a cohesive narrative of events through time. This requires moving from reasoning about single images \u2013 static moments, devoid of context \u2013 to sequences of images that depict events as they occur and change. On the vision side, progressing from single images to images in context allows us to begin to create an artificial intelligence (AI) that can reason about a visual moment given what it has already seen. On the language side, progressing from literal description to narrative helps to learn more evaluative, conversational, and abstract\n\u2217T.H. and F.F. contributed equally to this work. 1Sequential Images Narrative Dataset. This and future re-\nleases are made available on sind.ai.\nlanguage. This is the difference between, for example, \u201csitting next to each other\u201d versus \u201chaving a good time\u201d, or \u201csun is setting\u201d versus \u201csky illuminated with a brilliance...\u201d (see Figure 1). The first descriptions capture image content that is literal and concrete; the second requires further inference about what a good time may look like, or what is special and worth sharing about a particular sunset.\nWe introduce the first dataset of sequential images with corresponding descriptions, which captures some of these subtle but important differences, and advance the task of visual storytelling. We release the data in three tiers of language for the same images: (1) Descriptions of imagesin-isolation (DII); (2) Descriptions of images-insequence (DIS); and (3) Stories for images-insequence (SIS). This tiered approach reveals the effect of temporal context and the effect of narrative language. As all the tiers are aligned to the same images, the dataset facilitates directly modeling the relationship between literal and more abstract visual concepts, including the relationship between visual imagery and typical event patterns. We additionally propose an automatic evaluation metric which is best ar X\niv :1\n60 4.\n03 96\n8v 1\n[ cs\n.C L\n] 1\n3 A\npr 2\n01 6\ncorrelated with human judgments, and establish several strong baselines for the visual storytelling task."}, {"heading": "2 Motivation and Related Work", "text": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015).\nSuch work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between remarking that a visual scene shows \u201csitting in a room\u201d \u2013 typical of most image captioning work \u2013 and that the same visual scene shows \u201cbonding\u201d. The latter description is grounded in the visual signal, yet it brings to bear information about social relations and emotions that can be additionally inferred in context (Figure 1). Visually-grounded stories facilitate more evaluative and figurative language than has previously been seen in vision-to-language research: If a system can recognize that colleagues look bored, it can remark and act on this information directly.\nStorytelling itself is one of the oldest known human activities (Wiessner, 2014), providing a way to educate, preserve culture, instill morals, and share advice; focusing AI research towards this task therefore has the potential to bring about more humanlike intelligence and understanding."}, {"heading": "3 Dataset Construction", "text": "Extracting Photos We begin by generating a list of \u201cstoryable\u201d event types. We leverage the idea that\nStory 1\nRe-telling\n\u201cstoryable\u201d events tend to involve some form of possession, e.g., \u201cJohn\u2019s birthday party,\u201d or \u201cShabnam\u2019s visit.\u201d Using the Flickr data release (Thomee et al., 2015), we aggregate 5-grams of photo titles and descriptions, using Stanford CoreNLP (Manning et al., 2014) to extract possessive dependency patterns. We keep the heads of possessive phrases if they can be classified as an EVENT in WordNet3.0, relying on manual winnowing to target our collection efforts.2 These terms are then used to collect albums using the Flickr API.3 We only include albums with 10 to 50 photos where all album photos are taken within a 48-hour span and CC-licensed. See Table 1 for the query terms with the most albums returned.\nThe photos returned from this stage are then presented to crowd workers using Amazon\u2019s Mechanical Turk to collect the corresponding stories and descriptions. The crowdsourcing workflow of developing the complete dataset is shown in Figure 2.\nCrowdsourcing Stories In Sequence We develop a 2-stage crowdsourcing workflow to collect naturalistic stories with text aligned to images. The first stage is storytelling, where the crowd worker selects a subset of photos from a given album to form a photo sequence and writes a story about it (see Figure 3). The second stage is re-telling, in which the worker writes a story based on one photo sequence\n2 We simultaneously supplemented this data-driven effort by a small hand-constructed gazetteer.\n3https://www.flickr.com/services/api/\ngenerated by workers in the first stage.\nIn both stages, all album photos are displayed in the order of the time that the photos were taken, with a \u201cstoryboard\u201d underneath. In storytelling, by clicking a photo in the album, a \u201cstory card\u201d of the photo appears on the storyboard. The worker is instructed to pick at least five photos, arrange the order of selected photos, and then write a sentence or a phrase on each card to form a story; this appears as a full story underneath the text aligned to each image. Additionally, this interface captures the alignments between text and photos. Workers may skip an album if it does not seem storyable (e.g., a collection of coins). Albums skipped by two workers are discarded. The interface of re-telling is similar, but it displays the two photo sequences already created in the first stage, which the worker chooses from to write the story. For each album, 2 workers perform storytelling (at $0.3/HIT), and 3 workers perform re-telling (at $0.25/HIT), yielding a total of 1,907 workers. All HITs use quality controls to ensure varied text at least 15 words long.\nCrowdsourcing Descriptions of Images In Isolation & Images In Sequence We also use crowdsourcing to collect descriptions of imagesin-isolation (DII) and descriptions of images-insequence (DIS), for the photo sequences with stories from a majority of workers in the first task (as Figure 2). In both DII and DIS tasks, workers are asked to follow the instructions for image captioning proposed in MS COCO (Lin et al., 2014) such as describe all the important parts. In DII, we use\n5\nman woman standing holding wearing\nthe MS COCO image captioning interface.4 In DIS, we use the storyboard and story cards of our storytelling interface to display a photo sequence, with MS COCO instructions adapted for sequences. We recruit 3 workers for DII (at $0.05/HIT) and 3 workers for DIS (at $0.07/HIT).\nData Post-processing We tokenize all storylets and descriptions with the CoreNLP tokenizer, and replace all people names with generic MALE/FEMALE tokens,5 and all identified named entities with their entity type (e.g., location). The data is released as training, validation, and test following an 80%/10%/10% split on the stories-insequence albums. Example language from each tier is shown in Figure 4."}, {"heading": "4 Data Analysis", "text": "Our dataset includes 10,117 Flickr albums with 210,819 unique photos. Each album on average has 20.8 photos (\u03c3 = 9.0). The average time span of each album is 7.9 hours (\u03c3 = 11.4). Further details of each tier of the dataset are shown in Table 2.6\nWe use normalized pointwise mutual information to identify the words most closely associated with each tier (Table 3). Top words for descriptions-\n4https://github.com/tylin/coco-ui 5 We use those names occurring at least 10,000 times.\nhttps://ssa.gov/oact/babynames/names.zip 6We exclude words seen only once.\nin-isolation reflect an impoverished disambiguating context: References to people often lack social specificity, as people are referred to as simply \u201cman\u201d or \u201cwoman\u201d. Single images often do not convey much information about underlying events or actions, which leads to the abundant use of posture verbs (\u201cstanding\u201d, \u201csitting\u201d, etc.). As we turn to descriptions-in-sequence, these relatively uninformative words are much less represented. Finally, top story-in-sequence words include more storytelling elements, such as names ([male]), temporal references (today) and words that are more dynamic and abstract (went, decided)."}, {"heading": "5 Automatic Evaluation Metric", "text": "Given the nature of the complex storytelling task, the best and most reliable evaluation for assessing the quality of generated stories is human judgment. However, automatic evaluation metrics are useful to quickly benchmark progress. To better understand which metric could serve as a proxy for human evaluation, we compute pairwise correlation coefficients between automatic metrics and human judgments on 3,000 stories sampled from the SIS training set.\nFor the human judgements, we again use crowdsourcing on MTurk, asking five judges per story to rate how strongly they agreed with the statement \u201cIf these were my photos, I would like using a story like this to share my experience with my friends\u201d.7 We take the average of the five judgments as the final score for the story. For the automatic metrics, we use METEOR,8 smoothed-BLEU (Lin and Och, 2004), and Skip-Thoughts (Kiros et al., 2015) to compute similarity between each story for a given sequence. Skip-thoughts provide a Sentence2Vec embedding which models the semantic space of novels.\nAs Table 4 shows, METEOR correlates best with human judgment according to all the correlation co-\n7Scale presented ranged from \u201cStrongly disagree\u201d to \u201cStrongly agree\u201d, which we convert to a scale of 1 to 5.\n8We use METEOR version 1.5 with hter weights.\nefficients. This signals that a metric such as METEOR which incorporates paraphrasing correlates best with human judgement on this task. A more detailed study of automatic evaluation of stories is an area of interest for a future work."}, {"heading": "6 Baseline Experiments", "text": "We report baseline experiments on the storytelling task in Table 7, training on the SIS tier and testing on half the SIS validation set (valtest). Example output from each system is presented in Table 5. To highlight some differences between story and caption generation, we also train on the DII tier in isolation, and produce captions per-image, rather than in sequence. These results are shown in Table 7.\nTo train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al. (2014) to multiple images. Here, we encode an image sequence by running an RNN over the fc7 vectors of each image, in reverse order. This is used as the initial hidden state to the story decoder model, which learns to produce the story one word at a time using softmax loss over the training data vocabulary. We use Gated Recurrent Units (GRUs) (Cho et al., 2014) for both the image encoder and story decoder.\nIn the baseline system, we generate the story using a simple beam search (size=10), which has been successful in image captioning previously (Devlin et al., 2015). However, for story generation, the results of this model subjectively appear to be very poor \u2013 the system produces generic, repetitive, highlevel descriptions (e.g., \u201cThis is a picture of a dog\u201d). This is a predictable result given the label bias problem inherent in maximum likelihood training; recent work has looked at ways to address this issue directly (Li et al., 2016).\nTo establish a stronger baseline, we explore several decode-time heuristics to improve the quality of the generated story. The first heuristic is to lower the decoder beam size substantially. We find that using a beam size of 1 (greedy search) significantly increases the story quality, resulting in a 4.6 gain in METEOR score. However, the same effect is not seen for caption generation, with the greedy caption model obtaining worse quality than the beam search model. This highlights a key difference in generating stories versus generating captions.\nAlthough the stories produced using a greedy search result in significant gains, they include many repeated words and phrases, e.g., \u201cThe kids had a great time. And the kids had a great time.\u201d We introduce a very simple heuristic to avoid this, where the same content word cannot be produced more than once within a given story. This improves METEOR by another 2.3 points.\nAn advantage of comparing captioning to storytelling side-by-side is that the captioning output may be used to help inform the storytelling output. To this end, we include an additional baseline where \u201cvisually grounded\u201d words may only be produced if they are licensed by the caption model. We define the set of visually grounded words to be those which occurred at higher frequency in the caption training than the story training:\nP (w|Tcaption) P (w|Tstory) > 1.0 (1)\nWe train a separate model using the caption annotations, and produce an n-best list of captions for each image in the valtest set. Words seen in at\nleast 10 sentences in the 100-best list are marked as \u2018licensed\u2019 by the caption model. Greedy decoding without duplication proceeds with the additional constraint that if a word is visually grounded, it can only be generated by the story model if it is licensed by the caption model for the same photo set. This results in a further 1.3 METEOR improvement.\nIt is interesting to note what a strong effect relatively simple heuristics have on the generated stories. We do not intend to suggest that these heuristics are the right way to approach story generation. Instead, the main purpose is to provide clear baselines that demonstrate that story generation has fundamentally different challenges from caption generation; and the space is wide open to explore for training and decoding methods to generate fluent stories."}, {"heading": "7 Conclusion and Future Work", "text": "We have introduced the first dataset for sequential vision-to-language, which incrementally moves from images-in-isolation to stories-in-sequence. We argue that modelling the more figurative and social language captured in this dataset is essential for evolving AI towards more human-like understanding. We have established several strong baselines for the task of visual storytelling, and have motivated METEOR as an automatic metric to evaluate progress on this task moving forward."}], "references": [{"title": "Vqa: Visual question answering", "author": ["C. Lawrence Zitnick", "Devi Parikh."], "venue": "International Conference on Computer Vision (ICCV).", "citeRegEx": "Zitnick and Parikh.,? 2015", "shortCiteRegEx": "Zitnick and Parikh.", "year": 2015}, {"title": "D\u00e9j\u00e0 image-captions: A corpus of expressive descriptions in repetition", "author": ["Chen et al.2015] Jianfu Chen", "Polina Kuznetsova", "David Warren", "Yejin Choi"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Devlin et al.2015] Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell"], "venue": "In Proceedings of the 53rd Annual Meeting of the As-", "citeRegEx": "Devlin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Image description using visual dependency representations", "author": ["Elliott", "Keller2013] Desmond Elliott", "Frank Keller"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Elliott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2013}, {"title": "A survey of current datasets for vision and language research", "author": ["Nasrin Mostafazadeh", "Ting-Hao K. Huang", "Lucy Vanderwende", "Jacob Devlin", "Michel Galley", "Margaret Mitchell"], "venue": null, "citeRegEx": "Ferraro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["Gao et al.2015] Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li FeiFei"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Visual genome: Connecting language and vision", "author": ["Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalanditis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei"], "venue": null, "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "A diversitypromoting objective function for neural conversation models", "author": ["Li et al.2016] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "NAACL HLT 2016", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skipbigram statistics", "author": ["Lin", "Och2004] Chin-Yew Lin", "Franz Josef Och"], "venue": "In Proceedings of the 42Nd Annual Meeting on Association for Computational Linguis-", "citeRegEx": "Lin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Associa-", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Video event understanding using natural language descriptions", "author": ["Percy Liang", "Li Fei-Fei"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Ramanathan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2013}, {"title": "Exploring models and data for image question answering", "author": ["Ren et al.2015] Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "Advances in Neural Information Processing", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Recognition using visual phrases", "author": ["Sadeghi", "Ali Farhadi"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Sadeghi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sadeghi et al\\.", "year": 2011}, {"title": "The new data and new challenges in multimedia research", "author": ["Thomee et al.2015] Bart Thomee", "David A Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": "arXiv preprint arXiv:1503.01817", "citeRegEx": "Thomee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thomee et al\\.", "year": 2015}, {"title": "Show and tell: a neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Embers of society: Firelight talk among the ju/hoansi bushmen", "author": ["Polly W Wiessner"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Wiessner.,? \\Q2014\\E", "shortCiteRegEx": "Wiessner.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Young et al.2014] Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 19, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 21, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 1, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 22, "context": "Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al.", "startOffset": 85, "endOffset": 235}, {"referenceID": 15, "context": ", 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al.", "startOffset": 54, "endOffset": 138}, {"referenceID": 6, "context": ", 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al.", "startOffset": 54, "endOffset": 138}, {"referenceID": 14, "context": ", 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al.", "startOffset": 101, "endOffset": 126}, {"referenceID": 8, "context": ", 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015).", "startOffset": 29, "endOffset": 70}, {"referenceID": 20, "context": "Storytelling itself is one of the oldest known human activities (Wiessner, 2014), providing a way to educate, preserve culture, instill morals, and share advice; focusing AI research towards this task therefore has the potential to bring about more humanlike intelligence and understanding.", "startOffset": 64, "endOffset": 80}, {"referenceID": 17, "context": "\u201d Using the Flickr data release (Thomee et al., 2015), we aggregate 5-grams of photo titles and descriptions, using Stanford CoreNLP (Manning et al.", "startOffset": 32, "endOffset": 53}, {"referenceID": 13, "context": ", 2015), we aggregate 5-grams of photo titles and descriptions, using Stanford CoreNLP (Manning et al., 2014) to extract possessive dependency patterns.", "startOffset": 87, "endOffset": 109}, {"referenceID": 11, "context": "In both DII and DIS tasks, workers are asked to follow the instructions for image captioning proposed in MS COCO (Lin et al., 2014) such as describe all the important parts.", "startOffset": 113, "endOffset": 131}, {"referenceID": 5, "context": "5 Table 2: A summary of our dataset, following the proposed analyses of Ferraro et al. (2015), including the Frazier and Yngve measures of syntactic complexity.", "startOffset": 72, "endOffset": 94}, {"referenceID": 2, "context": "We use Gated Recurrent Units (GRUs) (Cho et al., 2014) for both the image encoder and story decoder.", "startOffset": 36, "endOffset": 54}, {"referenceID": 3, "context": "In the baseline system, we generate the story using a simple beam search (size=10), which has been successful in image captioning previously (Devlin et al., 2015).", "startOffset": 141, "endOffset": 162}, {"referenceID": 9, "context": "This is a predictable result given the label bias problem inherent in maximum likelihood training; recent work has looked at ways to address this issue directly (Li et al., 2016).", "startOffset": 161, "endOffset": 178}, {"referenceID": 2, "context": "To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al.", "startOffset": 169, "endOffset": 190}, {"referenceID": 2, "context": "To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al. (2014) to multiple images.", "startOffset": 169, "endOffset": 216}], "year": 2016, "abstractText": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND1 v.1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.", "creator": "LaTeX with hyperref package"}}}