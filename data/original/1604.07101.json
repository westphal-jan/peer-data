{"id": "1604.07101", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2016", "title": "Double Thompson Sampling for Dueling Bandits", "abstract": "In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As indicated by its name, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison by sampling twice from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as its special case. For general Copeland dueling bandits, we show that D-TS achieves $O(K^2 \\log T)$ regret. For Condorcet dueling bandits, we further simplify the D-TS algorithm and show that the simplified D-TS algorithm achieves $O(K \\log T + K^2 \\log \\log T)$ regret. Simulation results based on both synthetic and real-world data demonstrate the efficiency of the proposed D-TS algorithm.", "histories": [["v1", "Mon, 25 Apr 2016 00:38:16 GMT  (1289kb,D)", "http://arxiv.org/abs/1604.07101v1", "27 pages, 5 figures"], ["v2", "Thu, 27 Oct 2016 17:36:57 GMT  (1363kb,D)", "http://arxiv.org/abs/1604.07101v2", "27 pages, 5 figures, 9 tables; accepted by 30th Conference on Neural Information Processing Systems (NIPS), 2016"]], "COMMENTS": "27 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["huasen wu", "xin liu 0002"], "accepted": true, "id": "1604.07101"}, "pdf": {"name": "1604.07101.pdf", "metadata": {"source": "CRF", "title": "Double Thompson Sampling for Dueling Bandits", "authors": ["Huasen Wu", "Xin Liu", "R. Srikant"], "emails": ["hswu@ucdavis.edu", "liu@cs.ucdavis.edu", "rsrikant@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "The dueling bandit problem [3] is a variation of the classical multi-armed bandit (MAB) problem, where the feedback comes in the format of pairwise comparison. This model can be applied in systems such as information retrieval (IR) [5, 6], where user preferences are easier to obtain and typically more stable. Most existing work focuses on Condorcet dueling bandits, where there exists an arm, referred to as the Condorcet winner, that beats all other arms. Earlier dueling bandit algorithms such as Interleaved Filter (IF) [3] and Beat-The-Mean (BTM) [7] belong to the \u201cexplorethen-exploit\u201d family, which requires the time horizon as an input and only applies to finite-horizon settings. Algorithms that work for both finite and infinite horizon settings are studied recently [4, 8]. [4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits. The more recent work [8] proposes a Relative Minimum Empirical Divergence (RMED) algorithm, which is shown to be optimal in the sense of both asymptotic order and constant coefficient, under the Condorcet assumption. However, the existence of a Condorcet winner is not guaranteed in practice. A more general and practical definition of winner is the Copeland winner, which is the arm (or arms) that beats the most other arms. A recent work [2] studies the Copeland dueling bandits and proposes a Copeland Confidence Bound (CCB) algorithm, which can be viewed as an extension of RUCB in this general setting.\nAn alternative solution to the traditional MAB problem is pseudo-Bayesian decision, which is first proposed by William R. Thompson in [10] and is also referred to as Thompson Sampling. The basic idea of Thompson Sampling is very simple: assuming certain prior distribution for the system parameters, the algorithm maintains the posterior distribution for these parameters and takes the best action according to samples drawn from the posterior distribution; the feedback of the system under the taken action is then used to update the posterior distribution. Empirical studies [11] show that this simple framework achieves performance similar to or better than other types of algorithms in practice. However, theoretical analysis of Thompson Sampling is much more difficult than other algorithms such as UCB. By analyzing the concentration property of Thompson Sampling, [12]\nar X\niv :1\n60 4.\n07 10\n1v 1\n[ cs\n.L G\n] 2\n5 A\npr 2\n01 6\nproposes a logarithmic bound for its expected regret, whose constant factor is further improved in [13]. Recently, Thompson Sampling becomes a popular method that is widely applied in other more complex online learning problems, such as MAB with multiple plays [14], MAB with budget constraints [15], online problems with complex actions and feedbacks [16].\nInspired by the success of Thompson Sampling in traditional MAB and other online learning problems, a natural question is whether it can be applied to dueling bandits and how. Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties. In dueling bandits, not all comparisons provide information about the system statistics. Unlike the multi-play MAB [14], dueling bandits allow to compare one arm against itself and a good learning algorithm is expected to end up with comparing the winner against itself 1. However, comparing one arm against itself does not provide any system statistical information and cannot be used to update the posterior distribution. This is different from traditional MAB where pulling suboptimal arms accumulates knowledge of their distributions, which will help the algorithm gradually get out of the suboptimal actions. Therefore, we need to carefully select the arms for comparison so that: 1) comparing the best arms (Condorcet or Copeland winners) against themselves is allowed, but, 2) trapping in comparing suboptimal arm/arms against itself/themselves is avoided.\nIn fact, Thompson Sampling has been preliminarily studied for dueling bandits. A sketch of this idea was first discussed at a workshop [18], without a published paper. Recently, [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS). Under RCS, the first arm is selected by Thompson Sampling while the second arm is selected according to their RUCB. Empirical studies demonstrate the performance improvement of using RCS in practice, but no theoretical bounds on the regret are provided.\nIn this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandits, including both Condorcet dueling bandits and general Copeland dueling bandits. As indicated by its name, D-TS typically (special treatment will be discussed soon) selects both the first and the second candidates according to samples independently drawn from the posterior distributions. Compared to RCS, this is a more straightforward way of applying Thompson Sampling in dueling bandits. This straightforward manner enables Thompson Sampling to be more fully utilized in dueling bandits and thus brings significant performance improvements. Moreover, this simple framework also applies to general Copeland dueling bandits and achieves much better performance than the state-of-the-art algorithms such as Copeland Confidence Bounds (CCB) [2]. One may suggest treating each pair of arms as a complex action and apply the simple framework of Thompson Sampling in [16]. However, as discussed previously, not all comparisons provide information and it is unclear how to select the pair according to the samples, especially for general Copeland dueling bandits. In order to avoid getting stuck on suboptimal comparisons, we leverage the RUCB method [4] to eliminate the arms that are unlikely to be the winners. We resort to RUCB because the samples drawn from the posterior distribution are random and cannot be used to judge an arm of being a winner or not with a given confidence level, although they drop in the confidence intervals with high probability.\nWe obtain theoretical bounds for the proposed D-TS algorithm. Analyzing regret bounds for Thompson Sampling is much more difficult than UCB-type algorithms, as noted in literature for traditional MAB [12, 13, 19, 16]. In traditional MAB, the main difficulty comes from the fact that sufficient operations on the suboptimal arms may not guarantee the elimination of these arms, because they can still be pulled if the optimal arm has not been pulled sufficiently and the corresponding sample turns out to be small. The analysis in dueling bandits is more difficult because the selection of arms involves on many more factors and the two selected arms may be correlated. For example, an arm that is selected as the first candidate may also have a higher probability (than that in traditional MAB) to be chosen as the second candidate. Fortunately, because D-TS draws the two sets of samples independently, this correlation can be addressed because both of their distributions are captured by the historic observations. Thus, when fixing the first candidate at a specific arm, the comparison between it and all other arms is similar to a traditional MAB and hence the techniques of regret analysis in traditional MAB can be extended to our scenario. By investigating the properties of both RUCB and Thompson Sampling, we show that D-TS achieves an O(K2 log T ) regret for a general K-armed Copeland dueling bandit. Moreover, for the special Condorcet dueling bandits, we sim-\n1In practice, when comparing an arm against itself, one does not need to conduct an actual comparison but just implement this arm directly.\nplify the D-TS algorithm, and show that this simplified D-TS achieves O(K log T + K2 log log T ) regret. This is accomplished by a \u201cback substitution\u201d trick. Specifically, we have shown that we can bound the number of time-slots when a non-winner arm is chosen as the first candidate byO(log T ). Then by viewing the comparison between it and all other arms as a traditional MAB, we can bound the number of comparison between this arm and the suboptimal arms (that is beaten by it) is further reduced toO(log log T ) due to the concavity of the logarithmic function. We also discuss the refinement for the regret of D-TS in general Copeland dueling bandits. Our conjecture is that the original D-TS will achieve an O(KLC log T + K2 log log T ) regret, where LC is the number of arms that beats the Copeland winners, similar to that in [2]. We are still working on the rigorous proof for this conjecture.\nFinally, we evaluate the proposed D-TS algorithms through experiments based on both synthetic and real-world data. The simulation results show that, for both Condorcet dueling bandits and general Copeland dueling bandits, D-TS outperforms existing algorithms such as SAVAGE [1], RUCB [4], RMED [8], and CCB [2] in many practical settings. One exception is that in Condorcet dueling bandits, SAVAGE and RMED may perform slightly better than D-TS under certain extreme conditions where the Condorcet winner may not be an appropriate definition for \u201cwinner\u201d. Moreover, the simplified D-TS achieves similar performance as the original D-TS in Condorcet dueling bandits. These results demonstrate the effectiveness of D-TS in dueling bandits.\nIn summary, the main contributions of this paper are as follows:\n\u2022 We propose a D-TS algorithm for the general Copeland dueling bandit problem, including the Condorcet dueling bandit problem as its special case. Double sampling enables full utilization of Thompson Sampling in dueling bandits and brings significant performance improvement. Our study also demonstrates how to deal with the unique difficulties when using Thompson Sampling in dueling bandits, e.g., using RUCB-based elimination to avoid trapping in suboptimal comparisons.\n\u2022 We obtain the theoretical regret bounds for D-TS. For general Copeland dueling bandits, we show that the proposed D-TS algorithm achieves O(K2 log T ) regret. For the special case, i.e., Condorcet dueling bandits, we simplify the D-TS algorithm and show this simplified version achieves O(K log T +K2 log log T ) regret.\n\u2022 We evaluate the D-TS algorithm through experiments based on both synthetic and realworld data. The simulation results demonstrate the effectiveness of D-TS in practice. DTS significantly outperforms existing algorithms in both Condorcet dueling bandits and general Copeland dueling bandits. In Condorcet dueling bandits, the simplified D-TS algorithm achieves similar performance as the original D-TS algorithm."}, {"heading": "2 Related Work", "text": "The dueling bandit problem is first brought by [3]. Unlike traditional MAB, there are different definitions of best arms, depending on the preference matrix and the application requirements. Most existing work focuses on Condorcet dueling bandits where there exists an arm, referred to as the Condorcet winner, that beats all other arms. Under certain strict assumptions such as strong stochastic transitivity and stochastic triangle inequality, [3] proposes the first \u201cexplore-then-exploit\u201d algorithms called Interleaved Filter 1 (IF1) and Interleaved Filter 2 (IF2), followed-up by a Beat-TheMean (BTM) algorithm in [7], where the strong stochastic transitivity assumption is relaxed. IF1 is shown to achieve O(K logK log T ) regret, while IF2 and BTM are shown to achieve O(K log T ) regret, respectively, in K-armed dueling bandits. Another algorithm called SAVAGE (Sensitivity Analysis of VAriables for Generic Exploration) is proposed in [1] and shown to outperform the IF and BTM algorithms by simulations, although only O(K2 log T ) regret is obtained for SAVAGE. All these algorithms need to know the time horizon T beforehand, and thus only apply to finitehorizon settings. Algorithms that work for both finite and infinite horizon settings are studied in [4] and [8]. [4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits. The more recent work [8] proposes a Relative Minimum Empirical Divergence (RMED) algorithm, which is shown to be optimal in the sense of both the asymptotic order O(K log T ) and the constant coefficient, under the Condorcet assumption. However, the existence of a Condorcet winner is not guaranteed in practice. A more general and practical definition\nis the Copeland winner, which is the arm (or arms) that beats the most other arms. A recent work [2] studies the Copeland dueling bandits and proposes a Copeland Confidence Bound (CCB) algorithm, which can be viewed as an extension of RUCB in this general setting. Another definition of winner is the Borda winner, which is that arm that has the highest total probability of beating other arms. Because Borda dueling bandits can be reduced to the traditional MAB problem [1, 4], only a few researchers study this problem with special structures, such as sparseness [20]. By treating each pair as an action, the SAVAGE algorithm [1] applies to dueling bandits with all the above three definitions of winners, but it is an \u201cexplore-then-exploit\u201d algorithm and exploring all pairs of arms may result in a large regret.\nDating back to 1933, Thompson Sampling [10] is one of the earliest algorithms for \u201cexplorationand-exploitation\u201d tradeoff and has been widely applied in traditional MAB and other more complex problems recently. The Thompson Sampling algorithm makes decisions according to random samples drawn from the posterior distribution and updates the posterior distribution according to the system feedbacks under the taken action. Its effectiveness has been demonstrated by empirical studies [11], which shows that Thompson Sampling achieves performance better than or close to that of UCB-type algorithms in practice. However, theoretical analysis of Thompson Sampling is much more difficult than UCB-type algorithms and a lot of efforts have been made to bound the regret theoretically. [12] proposes a logarithmic bound for the standard frequentist expected regret, whose constant factor is further improved in [13]. On the other hand, through information-theoretic analysis, [21, 19] derives the bounds for its Bayesian expected regret. Recently, Thompson Sampling becomes a popular method and has been widely applied in many variations of the traditional MAB, such as MAB with multiple plays [14], MAB with budget constraints [15], and MAB with complex actions and feedbacks [16]. Moreover, Thompson Sampling is also used in an even more complex learning problem - parameterized Markov decision process [17], which proposes a TSMDP algorithm for this problem.\nThompson Sampling has been preliminarily studied for dueling bandits. A sketch of this idea was first discussed at a workshop [18], without a published paper. In this talk, the presenter suggests viewing the (normalized) preference matrix as a transition probability matrix for a Markov chain. Then the transition probabilities are sampled from the Beta priors and the arms are selected by sampling from the stationary distribution. This is only a rough idea - neither its formal description nor its theoretical analysis is available. A recent work [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS). Under RCS, the first arm is selected by Thompson Sampling while the second arm is selected according to their RUCB. Empirical studies demonstrate the performance improvement by using RCS in practice, but no theoretical bounds on the regret are provided."}, {"heading": "3 System Model", "text": "We consider a dueling bandit problem with K arms, denoted by A = {1, 2, . . . ,K}. At each timeslot t > 0, a pair of arms (a(1)t , a (2) t ) is displayed and a noisy comparison outcome wt is obtained, which is \u201c1\u201d if a(1)t beats a (2) t , and \u201c2\u201d otherwise. Given (a (1) t , a (2) t ), the noisy comparison outcomes wt\u2019s are independent random variables. We assume the probability that one arm beats another is stationary over time and the distribution of comparison outcomes is characterized by the preference matrix P = [pij ]K\u00d7K , where pij is the probability that arm i beats arm j, i.e.,\npij = P{i j}, i, j = 1, 2, . . . ,K.\nSimilar to existing work, we assume that the displaying order does not affect the preference, and hence, pij + pji = 1 and pii = 1/2. For the sake of simplicity, we assume there is no tie, i.e., for any i 6= j, we have pij 6= 1/2. This assumption is made in literature [2] and usually holds in practice, e.g., two rankers are unlikely to be indistinguishable in information retrieval systems. The results of this paper can be extended to scenarios where this no-tie assumption is replaced by a relaxed version, as will be discussed later.\nA dueling bandit algorithm decides which pair of arms to compare depending on the historic observations. Specifically, define a filtrationHt\u22121 as the history before t, i.e.,\nHt\u22121 = {a(1)\u03c4 , a(2)\u03c4 , w\u03c4 , \u03c4 = 1, 2, . . . , t\u2212 1}.\nThen a dueling bandit algorithm \u0393 is a function that mapsHt\u22121 to (a(1)t , a (2) t ), i.e.,\n(a (1) t , a (2) t ) = \u0393(Ht\u22121).\nThe task of a dueling bandit algorithm is to identify the best arm, which is then compared against itself. Unlike traditional MAB, there are different definitions for the best arm in dueling bandits, depending on the properties of the preference matrix and the application requirements. A widelyused definition is the Condorcet winner, which is the arm i\u2217 that beats all other arms, i.e., with pi\u2217j > 1/2 for all j 6= i\u2217 [3, 4, 8]. However, the existence of Condorcet winner is not guaranteed in practice [1, 2]. A more general and practical definition is the Copeland winner, which is the arm (or arms) that maximizes the number of other arms it beats [1, 2]. Specifically, the Copeland score is defined as \u2211 j 6=i 1(pij > 1/2), and the normalized Copeland score is defined as\n\u03b6i = 1 K \u2212 1 \u2211 j 6=i 1(pij > 1/2), (1)\nwhere 1(\u00b7) is the indicator function. Let \u03b6\u2217 be the highest normalized Copeland score, i.e.,\n\u03b6\u2217 = max 1\u2264i\u2264K \u03b6i.\nThen the Copeland winner is defined as the arm (or arms) with the highest normalized Copeland score, i.e., C\u2217 = {i : 1 \u2264 i \u2264 K, \u03b6i = \u03b6\u2217}. (2) Note that the Condorcet dueling bandit problem is a special case of the Copeland dueling bandit problem, with \u03b6\u2217 = 1. Our proposed D-TS algorithm applies to general Copeland dueling bandits, and of course to the special Condorcet dueling bandits. We will also refine the results for D-TS in Condorcet dueling bandits.\nTo measure the performance of a dueling bandit algorithm \u0393, we adopt the definition of cumulative regret for Copeland dueling bandits [2]. Specifically, the cumulative regret of a dueling bandit algorithm \u0393 is defined as\nR\u0393(T ) = \u03b6 \u2217T \u2212 1\n2 T\u2211 t=1 E [ \u03b6 a (1) t + \u03b6 a (2) t ] . (3)\nThe objective of \u0393 is then to minimize the cumulative regret. As pointed out in [2], the results will apply to other definitions of regret because the above definition bounds the number of queries to non-winner arms."}, {"heading": "4 Double Thompson Sampling", "text": "In this section, we present the Double Thompson Sampling (D-TS) algorithm and study its regret. We first propose the D-TS algorithm for general Copeland bandits, and show that it achieves O(K2 log T ) regret. Then for Condorcet dueling bandits, we refine the regret to O(K log T + K2 log log T ) with a simplified version of D-TS. Inspired by the insights from Condorcet dueling bandits, we also discuss how to refine the regret of D-TS in general Copeland dueling bandits."}, {"heading": "4.1 D-TS Algorithm", "text": "We present a D-TS algorithm, described in Algorithm 1, for general Copeland dueling bandits. As indicated by its name, the basic idea of D-TS is to select both the first and the second candidates for comparison by Thompson Sampling. In D-TS, we assume a Beta prior distribution for the preference probability pij of each pair (i, j) with i 6= j. These distributions are then updated according to the comparison results Bij(t\u2212 1) and Bji(t\u2212 1), where Bij(t\u2212 1) (resp. Bji(t\u2212 1)) is the number of time-slots when arm i (resp. j) beats arm j (resp. i) before t (time index t is omitted in the pseudo codes without causing confusion). At each time-slot t, before implementing Thompson Sampling, we first use the RUCB [4] of each pair to eliminate the arms that are unlikely to be the Copeland winner, resulting in a candidate set Ct (Lines 4 to 6). The algorithm then samples \u03b8(1)ij (t) from the\nposterior beta distribution, and the first candidate a(1)t is chosen by \u201cmajority voting\u201d, i.e., the arm within Ct that beats the most arms according to \u03b8(1)ij (t) will be selected (Lines 7 to 11). A similar idea is applied to select the second candidate a(2)t , where new samples \u03b8 (2)\nia (1) t\n(t) are generated and the\narm with the largest \u03b8(2) ia\n(1) t\n(t) among all arms with l ia (1) t \u2264 1/2 is selected as the second candidate\n(Lines 13 to 14).\nAlgorithm 1 D-TS for Copeland Dueling Bandits 1: Init: B \u2190 0K\u00d7K ; // Bij is the number of times arm i beats j. 2: for t = 1 to T do 3: // Phase 1: Choose the first candidate a(1)\n4: U := [uij ], L := [lij ], where uij = Bij Bij+Bji + \u221a \u03b1 log t Bij+Bji , lij = Bij Bij+Bji \u2212 \u221a \u03b1 log t Bij+Bji , if i 6= j, and uii = lii = 1/2, \u2200i; // x0 := 1 for any x. 5: \u03b6\u0302i \u2190 1K\u22121 \u2211 j 6=i 1(uij > 1/2); // Upper bound of the normalized Copeland score. 6: C \u2190 {i : \u03b6\u0302i = maxj \u03b6\u0302j}; 7: for i, j = 1, . . . ,K with i < j do 8: Sample \u03b8(1)ij \u223c Beta(Bij + 1, Bji + 1); 9: \u03b8(1)ji \u2190 1\u2212 \u03b8 (1) ij ;\n10: end for 11: a(1) \u2190 arg max\ni\u2208C\n\u2211 j 6=i 1(\u03b8 (1) ij > 1/2); // Break tie randomly.\n12: // Phase 2: Choose the second candidate a(2) 13: Sample \u03b8(2)\nia(1) \u223c Beta(Bia(1) + 1, Ba(1)i + 1) for all i 6= a(1), and let \u03b8\n(2) a(1)a(1) = 1/2;\n14: a(2) \u2190 arg max i:l ia(1) \u22641/2 \u03b8\n(2) ia(1) ;\n15: // Compare and Update 16: Compare pair (a(1), a(2)) and observe the result w; 17: Update B: Ba(1)a(2) \u2190 Ba(1)a(2) + 1 if w = 1, or Ba(2)a(1) \u2190 Ba(2)a(1) + 1 if w = 2; 18: end for\nWe can see that, the double sampling procedure of D-TS fits the nature of dueling bandits. When using Thompson Sampling in traditional MAB, samples are typically drawn only once at each timeslot and decisions are made based on these samples, e.g., [11, 12, 19, 16, 14]. However, we need to select two arms for comparison in dueling bandits. Sampling only once allows us to choose the best candidate, but we need to further select the second arm. The existing algorithm RCS [6] uses RUCB to choose the second arm. In contrast, D-TS launches an additional sampling procedure and selects the second candidate accordingly. As expected, this full utilization of Thompson Sampling will significantly reduce the regret compared to RCS. Moreover, different from RUCB, the distribution of the Thompson samples does not depend on time t, but only the comparison statistics Bij(t \u2212 1) and Bji(t \u2212 1). This enables us to use the back substitution trick to refine the regret for Condorcet dueling bandits from O(K2 log T ) to O(K log T +K2 log log T ), as we will see in Section 4.2.2.\nWe also note that the RUCB-based elimination step (Lines 4 to 6) is very important in D-TS. Without this elimination, we may face the dilemma of trapping in suboptimal comparisons. Consider the following extreme case in Condorcet dueling bandits: assume arm-1 is the Condorcet winner with p1j = 0.501 for all j > 1, and arm-2 is not the Condorcet winner, but with p2j = 1 for all j > 2 (in fact, arm-2 is the Borda winner for larger K [20]). Then for K > 4, without the RUCB-based elimination, the algorithm may get trapped in comparing arm-2 with arm-1. This is because at each time-slot t, arm-2 beats at least K \u2212 2 arms with probability 1, while arm-1 typically beats less 2. In fact, after sufficient comparison between arm-1 and arm-2, we know that with high confidence, arm-2 is not the Condorcet winner. This issue can be addressed by RUCB-based elimination as follows: when arm-2 has been compared with arm-1 sufficiently, we know that arm-1 likely beats\n2Without RUCB-based elimination, the D-TS algorithm may still be able to get out of this trap in this case because the optimal action will be taken with a positive probability [16], but its regret will be much larger and difficult to be bound theoretically.\narm-2 and the upper bound of arm-2\u2019s normalized Copeland score \u03b6\u03022(t) < 1 with high probability; while for arm-1, \u03b6\u03021(t) = 1 with high probability even when it has not been compared with other arms sufficiently; thus arm-2 will be eliminated. Similarly, RLCB (Relative Lower Confidence Bound)-based elimination (Line 14, where we restrict to the arms with l\nia (1) t (t) \u2264 1/2) is important especially for non-Condorcet dueling bandits. Specifically, l\nia (1) t\n(t) > 1/2 indicates that arm i beats\na (1) t with high probability. Thus, comparing a (1) t with an arm i with lia(1)t (t) > 1/2 brings little information gain and may result in loss if i is not a Copeland winner."}, {"heading": "4.2 Regret Analysis", "text": "In this section, we study the regret bounds for D-TS in dueling bandits. For general Copeland dueling bandits, we show that the regret of D-TS is bounded by O(K2 log T ). For Condorcet dueling bandits, we show that a simplified version of D-TS can reduce the regret to O(K log T +K2 log log T ). Finally, we discuss the refinement of bounds for D-TS in general Copeland dueling bandits.\nWe introduce certain notations that will be used in the analysis. First of all, 1/2 is an important benchmark for pij in dueling bandits, and we let \u2206ij be the gap between pij and 1/2, i.e.,\n\u2206ij = |pij \u2212 1/2|. (4)\nIn order to bound the regret of D-TS, we need to bound the number of comparisons between any pair (i, j) with i /\u2208 C\u2217 or j /\u2208 C\u2217. Under D-TS, (i, j) may be compared in the format of (a(1)t , a (2) t ) = (i, j) or (a(1)t , a (2) t ) = (j, i). We will consider these two cases respectively, and define the following counters: for i 6= j, we define\nN (1) ij (T ) = T\u2211 t=1 1(a (1) t = i, a (2) t = j), (5)\nN (2) ij (T ) = T\u2211 t=1 1(a (1) t = j, a (2) t = i), (6)\nand Nij(T ) = N (1) ij (T ) +N (2) ij (T ); (7)\nFor i = j, we define\nNii(T ) = T\u2211 t=1 1(a (1) t = a (2) t = i). (8)\n4.2.1 O(K2 log T ) Regret for D-TS in General Copeland Dueling Bandits\nWe present the first main result for D-TS in general Copeland dueling bandits: Theorem 1. When applying D-TS in a Copeland dueling bandit with a preference matrix P = [pij ]K\u00d7K , its regret is bounded as:\nRD-TS(T ) \u2264 \u2211\ni 6=j:pij<1/2\n[ 4\u03b1 log T\n\u22062ij + (1 + )2\nlog T\nD(pij ||1/2)\n] +O( K2\n2 ), (9)\nwhere \u03b1 > 0.5 is the control factor for RUCB/RLCB, > 0 is an arbitrary constant, and D(p||q) = p log pq + (1\u2212 p) log 1\u2212p 1\u2212q is the KL divergence.\nThe summation operation in Eq. (9) is conducted over all pairs (i, j) with pij < 1/2. Thus, Theorem 1 states that D-TS achieves an O(K2 log T ) regret in general Copeland dueling bandits, including Condorcet dueling bandits as a special case. To the best of our knowledge, this is the first theoretical bound for Thompson Sampling in dueling bandits. The scaling behavior of this bound with respect to the time horizon T is order optimal, since a lower bound \u2126(log T ) has been shown in [3, 8]. The refinement of the scaling behavior with respect to the number of arms K will be discussed later.\nWe prove Theorem 1 by bounding the number of comparisons for all pairs (i, j) with i /\u2208 C\u2217 or j /\u2208 C\u2217. When fixing the first candidate as a(1)t = i, the selection of the second candidate a (2) t is similar to a traditional K-armed bandit problem with expected utilities pji (j = 1, 2, . . . ,K). However, the analysis is more complex here since different arms are eliminated differently depending on the value of pji. The proof of Theorem 1 is accomplished through Lemmas 1 to 3, which bound the number of comparisons for all suboptimal pairs (i, j) under different scenarios, i.e., pji < 1/2, pji > 1/2, and pji = 1/2 (j = i /\u2208 C\u2217), respectively. Lemma 1. Under D-TS, for one pair (i, j) with pji < 1/2, we have\nE[N (1)ij (T )] \u2264 (1 + ) log T\nD(pji||1/2) +O(\n1\n2 ). (10)\nProof. The proof of this lemma is similar to the analysis of Thompson Sampling for traditional MAB. In fact, it may be even simpler since under D-TS, arm j will compete with arm i with pii = 1/2, which is fixed and known. We prove this lemma by borrowing the idea in [13]. The key idea is to study the distribution of the empirical probability that arm j beats arm i, and the sample drawn from the corresponding posterior distribution. Specifically, let xji be a number such that pji < xji < 1/2 and p\u0304ji(t) =\nBji(t\u22121) Bji(t\u22121)+Bij(t\u22121) be the empirical probability that arm j beats arm\ni. Define the following events: E p\u0304ji(t) = {p\u0304ji(t) < xji}, (11)\nand E\u03b8ji(t) = {\u03b8 (2) ji (t) < 1/2}. (12)\nHere, E p\u0304ji(t) and E\u03b8ji(t) represent the concentration of the empirical probability p\u0304ji(t) and the sample \u03b8\n(2) ji (t), respectively.\nThen, we divide the probability of (a(1)t , a (2) t ) = (i, j) into three cases:\nP {\n(a (1) t , a (2) t ) = (i, j)\n} = P { (a\n(1) t , a (2) t ) = (i, j), E p\u0304 ji(t), E \u03b8 ji(t) } +P {\n(a (1) t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t) } +P {\n(a (1) t , a (2) t ) = (i, j), qE p\u0304 ji(t)\n} .\nWe can bound N (1)ij (T ) by analyzing the above three cases:\n1) The first term P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), E\u03b8ji(t)} = 0 because a (2) t 6= j when \u03b8 (2) ji (t) < 1/2 = \u03b8 (2) ii (t).\n2) The sum of the second term over time can be bounded as\nT\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t)} \u2264\nlog T\nD(xji||1/2) + 1. (13)\n3) The sum of the third term can be bounded as\nT\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), qE p\u0304 ji(t)} \u2264 1 +\n1\nD(xji||pji) . (14)\nFor any 0 < \u2264 1, we choose xji \u2208 (pji, 1/2) such thatD(xji||1/2) = D(pji||1/2)/(1+ ), which also implies 1D(xji||pji) = O( 1 2 ) as shown in [13]. The conclusion then follows by combining all the above three terms. Please refer to Appendix B.1 for more details.\nLemma 2. Under D-TS, for one pair (i, j) with pji > 1/2, we have\nE[N (1)ij (T )] \u2264 4\u03b1 log T\n\u22062ji +O(1). (15)\nProof. This lemma can be easily proved using the concentration property of RLCB lji(t). Note that when a(1)t = i, arm j can be selected as a (2) t only when the RLCB lji(t) \u2264 1/2. However, the RLCB satisfies P{lji(t) \u2264 1/2|Nij(t \u2212 1) \u2265 4\u03b1 log T\u22062ji } \u2264 1 T 2\u03b1 . Thus, the expected total number of comparisons N (1)ij (T ) can be bounded similar to UCB-type algorithms for traditional MAB. The details of proof can be found in Appendix B.2.\nLemma 3. Under D-TS, for an arm i /\u2208 C\u2217, we have\nE[Nii(T )] \u2264 \u2211\nk:pki>1/2\n[ 24 \u22062ki + \u0398 ( 1 \u22062ki +\n1\n\u22062kiD(1/2||pki) +\n1\n\u22064ki\n)] = O(1). (16)\nBefore proving Lemma 3, we present an important property for \u03b6\u0302\u2217(t) = max1\u2264i\u2264K \u03b6\u0302i(t), which is the maximum value of the upper bound for the normalized Copeland score. Recall that \u03b6\u2217 is the maximum normalized Copeland score and \u03b6\u2217 = 1 in Condorcet dueling bandits. The following lemma shows that \u03b6\u0302\u2217(t) is equal to or greater than \u03b6\u2217 with high probability, i.e., \u03b6\u0302\u2217(t) is indeed a UCB of \u03b6\u2217. This lemma holds for both Condorcet dueling bandits and general Copeland dueling bandits.\nLemma 4. For any \u03b1 > 0 and t > 0,\nP{\u03b6\u0302\u2217(t) \u2265 \u03b6\u2217} \u2265 1\u2212 K t2\u03b1 . (17)\nProof. The proof of this lemma is accomplished by leveraging the concentration property of RUCB. Details can be found in Appendix B.3.\nSketch for the Proof of Lemma 3: We prove Lemma 3 similarly to [13], but we need to adjust it to address the correlation between the selection of two candidates under D-TS. When the upper bound of the normalized Copeland score \u03b6\u0302\u2217(t) \u2265 \u03b6\u2217, the event (a(1)t , a (2) t ) = (i, i) occurs only if: a) there exists at least one k \u2208 K with pki > 1/2, such that lki(t) \u2264 1/2; and b) \u03b8(2)ki (t) \u2264 1/2 for all k with lki(t) \u2264 1/2. In this case,we can bound the probability of a(2)t = i by the probability of a\n(2) t = k, given pki > 1/2 and lki(t) \u2264 1/2. However, we cannot expect that the coefficient will decay exponentially as in traditional MAB [13], because lki(t) \u2264 1/2 may indicate that arm k has not performed well when compared with i in the past. By considering all possible realizations of the history Ht\u22121, we can relax the condition of lki(t) \u2264 1/2 and obtain results similar to [13]. Details of proof can be found in Appendix B.4.\nTheorem 1 can be proved directly by combining Lemmas 1 to 3.\n4.2.2 Refining the Regret Bound to O(K log T ) in Condorcet Dueling Bandits\nAlthough the design and analysis of D-TS for Copeland dueling bandits is non-trivial, an O(K2 log T ) regret may be unsurprising. In particular, we expect to achieve lower regret as [3, 4, 8] for Condorcet dueling bandits. In this section, we show how to refine the regret to O(K log T +K2 log log T ) in Condorcet dueling bandits, with an even simpler version of D-TS:\nSimplified D-TS (D-TS(s)): Same as D-TS described in Algorithm 1, except that when selecting the second candidate, Line 14 is modified to:\na(2) \u2190 arg max 1\u2264i\u2264K \u03b8 (2) ia(1) .\nUnder D-TS(s), the RLCB-based elimination step is removed when choosing the second candidate and a(2)t is selected as the arm with the highest sample \u03b8 (2) ia(1) . Without loss of generality, we assume arm-1 is the Condorcet winner, i.e., i\u2217 = 1. Similar to D-TS, we can show that D-TS(s)achieves O(K2 log T ) regret in Condorcet dueling bandits.\nProposition 1. When applying D-TS(s)in a Condorcet dueling bandit with a preference matrix P = [pij ]K\u00d7K , its regret is bounded as:\nRD-TS(s)(T ) \u2264 (1+ ) 2 \u2211 j>1\nlog T D(pj1||1/2) + \u2211 i>1 4\u03b1 log T \u22062j\u2217i i +(1+ )2 \u2211 i>1 \u2211 j\u2208A\\{i,j\u2217i } log T D(pji||p\u2217i ) +O( K2 2 ),\n(18) where j\u2217i = arg maxj pji, p \u2217 i = maxj pji, \u03b1 > 0.5 is the control factor for RUCB/RLCB, > 0 is an arbitrary constant, and D(\u00b7||\u00b7) is the KL divergence.\nIn Eq. (18), the first term corresponds to the comparisons between the Condorcet winner and the other arms j with pj1 < 1/2, the second term corresponds to the comparisons between arm i 6= 1 and the corresponding \u201cbest\u201d arm j\u2217i , and the third term corresponds to the comparisons between arm i 6= 1 and other arms j 6= j\u2217i or i. The proof of Proposition 1 is omitted here since it is quite similar to Theorem 1.\nA more important objective of this section is to refine the regret of D-TS(s)toO(K log T+K2 log T ) in Condorcet dueling bandits. Specifically, we use a \u201cback substitution\u201d trick to refine the third term in Eq. (18) to O(K2 log log T ), and obtain Theorem 2.\nTheorem 2. When applying D-TS(s)in a Condorcet dueling bandit with a preference matrix P = [pij ]K\u00d7K , its regret is bounded as:\nRD-TS(s)(T ) \u2264 (1 + ) 2 \u2211 j>1\nlog T D(pj1||1/2) + \u2211 i>1 4\u03b1 log T \u22062j\u2217i i\n+(1 + 2\u03b1)(1 + )2 \u2211 i>1 \u2211 j\u2208A\\{i,j\u2217i } log log T D(pji||p\u2217i ) +O(K2 logK) +O( K2 2 ),\n(19)\nwhere \u03b1 > 0.5 is the control factor for RUCB, > 0 is a constant, and D(\u00b7||\u00b7) is the KL-divergence.\nSketch of Proof: We prove this theorem using a \u201cback substitution\u201d trick. The intuition behind this idea is that when fixing the first candidate as a(1)t = i, the comparison between a (1) t = i and the other arms is similar to a traditional MAB with expected utility pji (j = 1, 2, . . . ,K). Using Thompson Sampling, the number of comparisons with each suboptimal arm with pji < p\u2217i will be bounded as E[N (1)ij (T )|N (1) i (T )] = O(logN (1) i (T )), where N (1) i (T ) = \u2211T t=1 1(a (1) t = i) is the number of time-slots when i is chosen as the first candidate. Note that this conclusion holds uniformly for any N (1)i (T ). During the analysis of Proposition 1, we have shown that E[N (1) i (T )] is bounded by O(K log T ). Thus, substituting the bound of E[N (1)i (T )] back and using the concavity of the log(\u00b7) function, we have E[N (1)ij (T )] = E [ E[N (1)ij (T )|N (1) i (T )] ] \u2264 O(logE[N (1)i (T )]) = O(log log T + logK). For rigorous analysis, we use the total probability formula and consider all possible values n for N (1)i (T ). Then similar to the analysis of Lemma 1, we bound the value of N\n(1) ij (T ) by further considering the cases with Nij(t \u2212 1) \u2264 L\u03b1ji(n) and Nij(t \u2212 1) > L\u03b1ji(n),\nrespectively, where L\u03b1ji(n) = \u0398(logn). Details of the proof can be found in Appendix C."}, {"heading": "4.2.3 Discussion: Refining the Regret Bound in General Copeland Dueling Bandits", "text": "As will be demonstrated in Section 5, we can see that the original version of D-TS in fact outperforms the CCB algorithm [22], which has been shown to achieve O(K(LC + 1) log T regret, where LC is the number of arms that the Copeland winners lose to. Inspired by the insights from the Condorcet dueling bandits and the simulation results for general Copeland dueling bandits, we make the following conjecture: Conjecture 1. When applying D-TS in a Copeland dueling bandit with a preference matrix P = [pij ]K\u00d7K , its regret is bounded as:\nRD-TS(T ) = O(KLC log T +K 2 log log T ). (20)\nwhere LC is the number of arms that a Copeland winner loses to.\nThe intuition that we believe the conjecture holds is as follows. When fixing the first candidate as a\n(1) t = i, the comparison between a (1) t = i and the other arms is similar to a traditional MAB. Consider an arm i /\u2208 C\u2217. For the arm j\u2217i = arg maxj pji, its RLCB lj\u2217i i(t) \u2264 1/2 occurs typically when it has not been compared with i sufficiently. At this case, it will dominate the comparisons, i.e., it will be allocated much more opportunities to compare with i. As long as j\u2217i has been compared with i sufficiently, say Nij(t \u2212 1) \u2265 4\u03b1 log T\u22062\nj\u2217 i i\n, it will be eliminated with high probability; then the\nsecond best arm with pji > 1/2 will dominate, so on and so forth; as long as (LC + 1) arms have been explored sufficiently, then \u03b6\u0302i(t) < \u03b6\u0302\u2217(t) with high probability, and i will not be selected as the first candidate. During this process, the other arms with pji smaller than the (LC + 1)-th best arm will be explored by O(log log T ) times. Consider all arms i \u2208 {1, 2, . . . ,K}, the total cumulative regret will be bounded byO(KLC log T+K2 log log T ). The rigorous study of this conjecture need to incorporate the evolution of RUCB, RLCB, and the Thompson samples. This is a very complex process and we are still working on the rigorous proof for this conjecture."}, {"heading": "5 Experiments", "text": "To evaluate the proposed D-TS and D-TS(s)algorithms, we run experiments based on synthetic and real world data. Here we present the results for experiments based on the Microsoft Learning to Rank (MSLR) dataset [23], which provides the relevance for queries and ranked documents. Based on this dataset, [2] derives a preference matrix for 136 rankers, where each ranker is a function that maps a user\u2019s query to a document ranking and can be viewed as one arm in dueling bandits. We use the two 5-armed submatrices in [2], one for Condorcet dueling bandit and the other for non-Condorcet dueling bandit. More experimental results and discussions can be found in Appendix D.\nWe compare D-TS and D-TS(s)with the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2. For BTM, we set the relaxed factor \u03b3 = 1.3 as [7]. For algorithms using RUCB and RLCB, including our D-TS and D-TS(s), we set the scale factor \u03b1 = 0.51. For RMED, we use the same settings as [8], where f(K) = 0.3K1.01 for both RMED algorithms and \u03b1 = 3 for RMED2. For the \u201cexplore-thenexploit\u201d algorithms, BTM and SAVAGE, each point is obtained by resetting the time horizon as the corresponding value.\nWe evaluate algorithms according to the regret defined in Eq. (3), which is based on the normalized Copeland score. For Condorcet dueling bandits, Appendix D also provides the regret performance defined based on the preference probability gap. The results are averaged over 500 independent experiments, where we randomly shuffle the arms in each experiment to prevent algorithms from exploiting certain special structures of the preference matrix.\nIn Condorcet dueling bandits, as shown in Fig. 1(a), the earlier algorithms BTM and SAVAGE result in very large regret. RUCB, CCB, RMED1, and RMED2, significantly reduce the regret compared to BTM and SAVAGE. The SCB algorithm, a complement to CCB in many-armed dueling bandits, results in high regret in this small scale scenario. Using Thompson Sampling for selecting the first candidate, RCS slightly reduces the regret compared to RUCB and CCB, but the improvement is limited. This is because, without the RUCB-based elimination in our D-TS algorithm, RCS requires \u201c100% pass\u201d when choosing the first candidate (which is picked randomly if no such an arm exists) and could miss many opportunities of choosing the Condorcet winner as the first candidate. Moreover, RCS chooses the second candidate relying on RUCB and will take more efforts to identify the best arm compared to D-TS. Compared to the RMED algorithms, our D-TS algorithm also performs better. [8] has shown that RMED is optimal in Condorcet dueling bandits, not only in the sense of asymptotic order, but also the coefficients in the lower bound. However, the simulation results show that our D-TS algorithm can achieve the similar slope as RMED, and also reduce the constant term. This inspires us to further refine the theoretical bounds for the regret of D-TS in the future. Comparing D-TS and D-TS(s), we can see that their performance is very close. This even holds in dueling bandits without strong transitivity, where the Condorcet winner may not have the highest preference probability when comparing all arms with a fixed arm, as can be seen in Appendix D. This is because under D-TS in Condorcet dueling bandits, the suboptimal arms will quickly lose the opportunities of being chosen as the first candidate, and the RLCB-based elimination in choosing the second candidate seldomly occurs in practice.\nIn non-Condorcet dueling bandits, as shown in Fig. 1(b), the regrets of BTM, RUCB, RCS, RMED1, and RMED2 grow very fast as time increases, because they keep exploring to find the Condorcet winner, which does not exist in this setting. Being aware of this possible non-existence, the general Copeland SAVAGE, CCB, and SCB algorithms achieve logarithmic regrets, and again SCB performs worse than CCB or SAVAGE in this small scale scenario. Our D-TS algorithm performs best and significantly reduces the regret. In particular, D-TS approaches smoothly to the asymptotic regime quickly, where the curve tends to be a line for a logarithmic scaling t-axis."}, {"heading": "6 Conclusions and Future Work", "text": "In this paper, we study Thompson Sampling for dueling bandits. We propose a D-TS algorithm that fits better to the nature of dueling bandits, where at each time one pair of arms is selected for comparison. We introduce a second round sampling to address the dilemma in dueling bandits where we aim at comparing the best arm against itself, but comparing one arm against itself does not provide information. We also introduce RUCB/RLCB-based elimination to help D-TS get out from suboptimal comparisons. The proposed D-TS algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as the special case. We obtain theoretical regret bound for D-TS, which is O(K2 log T ) for general Copeland dueling bandits. The bound is further refined to O(K log T + K2 log log T ) with a simplified version of D-TS for the special Condorcet dueling bandits. Experimental results demonstrate that our proposed D-TS algorithm performs much better than the state-of-the-art algorithms.\nAlthough logarithmic regret bounds have been obtained for D-TS, these bounds are usually worse than that for other algorithms such as RUCB or RMED. These current bounds are likely loose because our D-TS algorithm performs much better than these algorithms in practice. Currently, our analysis relies heavily on the properties of RUCB/RLCB. In fact, however, we see from the singlestep tracking that the RUCB-based elimination seldomly occurs under most practical settings. We will further refine the regret bounds by investigating the properties of Thompson-Sampling-based majority-voting. Also, studying D-TS type algorithms for dueling bandits with other definition of winners or conditions/constraints (e.g., with side information [22] or budget constraints [24]) is another interesting direction for future research."}, {"heading": "Acknowledgement", "text": "We thank Masrour Zoghi from University of Amsterdam, Netherlands, and Dr. Junpei Komiyama from the University of Tokyo, for helpful discussions on the experiments."}, {"heading": "Appendices", "text": ""}, {"heading": "A Preliminaries", "text": "Fact 1. (Chernoff-Hoeffding Bound [25]) 1) Bernoulli random variables: Given Bernoulli random variables X1, X2, . . . , Xn with E[Xi] = pi. Let X\u0304n = 1n \u2211n i=1Xi and p = E[X\u0304n] = 1 n \u2211n i=1 pi. Then, for \u2208 [0, p],\nP{X\u0304n \u2264 p\u2212 } \u2264 e\u2212nD(p\u2212 ||p), (21)\nand for \u2208 [0, 1\u2212 p], P{X\u0304n \u2265 p+ } \u2264 e\u2212nD(p+ ||p), (22)\nwhere D(q||p) = q log qp + (1\u2212 q) log 1\u2212q 1\u2212p is the KL-divergence.\n2) General random variables over [0,1]: Let X1, X2, . . . , Xn be random variables over [0,1] and E[Xi|X1, X2, . . . , Xi\u22121] = p. Let X\u0304n = 1n \u2211n i=1Xi. Then for all \u2265 0, we have\nP{X\u0304n \u2265 p+ } \u2264 e\u22122n 2 , (23)\nP{X\u0304n \u2264 p\u2212 } \u2264 e\u22122n 2 . (24)\nUsing the above fact, we obtain the following results for RUCB/RLCB:\nLemma 5. (Concentration of RUCB/RLCB) 1) For any (i, j) and t > 0,\nP{pij \u2264 uij(t)} \u2265 1\u2212 1\nt2\u03b1 , (25)\nP{pij \u2265 lij(t)} \u2265 1\u2212 1\nt2\u03b1 . (26)\n2) For (i, j) with pij < 1/2,\nP{uij(t) \u2265 1/2|Nij(t\u2212 1) \u2265 4\u03b1 log T \u22062ij } \u2264 1 T 2\u03b1 . (27)\nProof. The conclusions follow by using the Chernoff-Hoeffding bound for general random variables over [0,1].\nThe next fact reveals the equality between the Beta distribution and the Binomial distribution, which is used to show the concentration property of Thompson samples.\nFact 2. (Beta-Binomial equality [12]) Let \u03b1 and \u03b2 be arbitrary positive integers. Let F beta\u03b1,\u03b2 (\u00b7) be the Cumulative Distribution Function (CDF) of the beta distribution Beta(\u03b1, \u03b2), and F Bn,p(\u00b7) be the CDF of the binomial distribution Binomial(n, p). Then,\nF beta\u03b1,\u03b2 (y) = 1\u2212 F B\u03b1+\u03b2\u22121,y(\u03b1\u2212 1). (28)"}, {"heading": "B Regret Analysis in General Copeland Dueling Bandits", "text": ""}, {"heading": "B.1 Proof of Lemma 1", "text": "Let p\u0304ji(t) = Bji(t\u22121)\nBji(t\u22121)+Bij(t\u22121) be the empirical estimation for the probability that arm j beats arm i. Let xji be a number satisfying pji < xji < 1/2. Define the following events:\nE p\u0304ji(t) = {p\u0304ji(t) < xji},\nand E\u03b8ji(t) = {\u03b8 (2) ji (t) < 1/2}.\nThen\nP{(a(1)t , a (2) t ) = (i, j)} = P{(a (1) t , a (2) t ) = (i, j), E p\u0304 ji(t), E \u03b8 ji(t)}\n+P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t)}\n+P{(a(1)t , a (2) t ) = (i, j), qE p\u0304 ji(t)}.\nThe first term P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), E\u03b8ji(t)} = 0 because a (2) t 6= j when \u03b8 (2) ji (t) < 1/2 = \u03b8 (2) ii (t).\nFor the second term, letting Lji(T ) = log TD(xji||1/2) , similar to the proof of Lemma 4 in [13], we can show that it is bounded as follows:\nT\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t)}\n= T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) \u2264 Lji(T )}\n+ T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) > Lji(T )}\n\u2264 Lji(T ) + T\u2211 t=1 1 T\n= log T\nD(xji||1/2) + 1, (29)\nwhere the term P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE\u03b8ji(t), Nij(t \u2212 1) > Lji(T )} \u2264 P{(a (1) t , a (2) t ) = (i, j), E p\u0304ji(t), qE\u03b8ji(t), Nij(t\u2212 1)|Lji(T )} \u2264 1T , as shown in [13]. The third term can be bounded similarly to Lemma 3 in [13], where the only difference is that i and j are compared when (a(1)t , a (2) t ) = (i, j) or (a (1) t , a (2) t ) = (j, i). Specifically, let \u03c4n be the slot index when i and j are compared for the n-th time, including both cases (a(1)t , a (2) t ) = (i, j) and (a(1)t , a (2) t ) = (j, i). Let \u03c40 = 0. Then, p\u0304ji(t) is fixed between \u03c4n + 1 and \u03c4n+1, and\u2211\u03c4n+1\nt=\u03c4n+1 1((a (1) t , a (2) t ) = (i, j)) \u2264 1 (it is 0 if the (n + 1)-th comparison is implemented in the\nformat of (j, i)). Then\nT\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), qE p\u0304 ji(t)}\n\u2264 T\u22121\u2211 n=0 E [ \u03c4n+1\u2211 t=\u03c4n+1 1((a (1) t , a (2) t ) = (i, j)) \u00b7 1(qE p\u0304 ji(t)) ]\n\u2264 T\u22121\u2211 n=0 E [ 1(qE p\u0304ji(\u03c4n + 1)) \u03c4n+1\u2211 t=\u03c4n+1 1((a (1) t , a (2) t ) = (i, j)) ]\n\u2264 T\u22121\u2211 n=0 P(qE p\u0304ji(\u03c4n + 1))\n\u2264 1 + T\u22121\u2211 n=1 e\u2212nD(xji||pji)\n\u2264 1 + 1 D(xji||pji) . (30)\nFor any 0 < \u2264 1, we choose xji \u2208 (pji, 1/2) such thatD(xji||1/2) = D(pji||1/2)/(1+ ), which also implies 1D(xji||pji) = O( 1 2 ) as shown in [13]. The conclusion then follows by combining all the above three terms."}, {"heading": "B.2 Proof of Lemma 2", "text": "This lemma can be easily proved using the concentration property of RLCB lji(t). Note that when a\n(1) t = i, arm j can be selected as a (2) t only when the lji(t) \u2264 1/2. However, the RLCB satisfies\nP{lji(t) \u2264 1/2|Nij(t\u2212 1) \u2265 4\u03b1 log T\u22062ji } \u2264 1 T 2\u03b1 . Thus,\nP{(a(1)t , a (2) t ) = (i, j)} = P{(a (1) t , a (2) t ) = (i, j), Nij(t\u2212 1) \u2265\n4\u03b1 log T\n\u22062ji }\n+P{(a(1)t , a (2) t ) = (i, j), Nij(t\u2212 1) <\n4\u03b1 log T\n\u22062ji }\n\u2264 P{lji(t) \u2264 1/2, Nij(t\u2212 1) \u2265 4\u03b1 log T\n\u22062ji }\n+P{(a(1)t , a (2) t ) = (i, j), Nij(t\u2212 1) <\n4\u03b1 log T\n\u22062ji }\n\u2264 1 T 2\u03b1 + P{(a(1)t , a (2) t ) = (i, j), Nij(t\u2212 1) < 4\u03b1 log T \u22062ji }. (31)\nNote that T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), Nij(t\u2212 1) < 4\u03b1 log T \u22062ji }\n= T\u2211 t=1 E[1((a(1)t , a (2) t ) = (i, j), Nij(t\u2212 1) < 4\u03b1 log T \u22062ji )]\n= E [ T\u2211 t=1 1((a (1) t , a (2) t ) = (i, j), Nij(t\u2212 1) < 4\u03b1 log T \u22062ji ) ] \u2264 4\u03b1 log T \u22062ji , (32)\nbecause \u2211T t=1 1 ( (a (1) t , a (2) t ) = (i, j), Nij(t \u2212 1) <\n4\u03b1 log T \u22062ji\n) \u2264 4\u03b1 log T\n\u22062ji due to the fact that: when\n(a (1) t , a (2) t ) = (i, j) at t, Nij(t\u2212 1) will be increased by 1, but 1\n( (a\n(1) t , a (2) t ) = (i, j), Nij(t\u2212 1) <\n4\u03b1 log T \u22062ji ) = 0 as long as Nij(t\u2212 1) \u2265 4\u03b1 log T\u22062ji . The conclusion then follows."}, {"heading": "B.3 Proof of Lemma 4", "text": "Let i\u2217 be the Copeland winner (or any one of the winners) in the dueling bandit. We prove Lemma 4 by analyzing the RUCB ui\u2217j(t) at t. Using the Chernoff-Hoeffding bound, we have that for any j 6= i\u2217,\nP{ui\u2217j(t) < pi\u2217j} \u2264 e\u22122\u03b1 log t = 1\nt2\u03b1 . (33) Note that \u03b6\u2217 = 1K\u22121 \u2211 j 6=i 1(pi\u2217j > 1/2). Let Li\u2217 = {j : 1 \u2264 j \u2264 K, pi\u2217j > 1/2} be the arms that lose to i\u2217. Thus,\n\u03b6\u0302\u2217(t) < \u03b6\u2217 \u21d2 \u2203j \u2208 Li\u2217 , such that ui\u2217j(t) < pi\u2217j . (34)\nConsider all elements in Li\u2217 , we have\nP{\u03b6\u0302\u2217(t) \u2265 \u03b6\u2217} = 1\u2212 P{\u03b6\u0302\u2217(t) < \u03b6\u2217} \u2265 1\u2212 P{\u2203j \u2208 Li\u2217 , s.t. ui\u2217j(t) < pi\u2217j}\n\u2265 1\u2212 |Li \u2217 | t2\u03b1 \u2265 1\u2212 K t2\u03b1 . (35)"}, {"heading": "B.4 Proof of Lemma 3", "text": "To bound the number of time-slots when we compare one non-winner arm against itself, we need to investigate the necessary conditions for this event to happen.\nWhen the upper bound of the Copeland score \u03b6\u0302\u2217(t) \u2265 \u03b6\u2217, the event (a(1)t , a (2) t ) = (i, i) occurs only if: a) there exists at least one k \u2208 K with pki > 1/2, such that lki(t) \u2264 1/2; and b) \u03b8(2)ki (t) \u2264 1/2 for all k with lki(t) \u2264 1/2. For k with pki > 1/2, we define the following probability\nqki(t) = P{\u03b8(2)ki (t) > 1/2|Ht\u22121}. (36)\nNote that all information about \u03b6\u0302\u2217(t), lki(t), and qki(t) is determined by Ht\u22121. Given k with pki > 1/2 andHt\u22121 such that \u03b6\u0302\u2217(t) \u2265 \u03b6\u2217 and lki(t) \u2264 1/2, similarly to [13], we can show that\nP {\n(a (1) t , a (2) t ) = (i, i)|Ht\u22121\n} \u2264 1\u2212 qki(t) qki(t) P { (a (1) t , a (2) t ) = (i, k)|Ht\u22121 } . (37)\nThus, E[Nii(T )]\n= T\u2211 t=1 P { (a (1) t , a (2) t ) = (i, i)}\n\u2264 T\u2211 t=1 P { (a (1) t , a (2) t ) = (i, i), \u03b6\u0302 \u2217(t) \u2265 \u03b6\u2217,\u2203k, pki > 1/2, lki(t) \u2264 1/2 } + T\u2211 t=1 P { \u03b6\u0302\u2217(t) < \u03b6\u2217 } \u2264\n\u2211 k:pki>1/2 T\u2211 t=1 P { (a (1) t , a (2) t ) = (i, i), \u03b6\u0302 \u2217(t) \u2265 \u03b6\u2217, lki(t) \u2264 1/2 } + T\u2211 t=1 K t2\u03b1\n\u2264 \u2211\nk:pki>1/2\nT\u2211 t=1 E [ P { (a (1) t , a (2) t ) = (i, i), \u03b6\u0302 \u2217(t) \u2265 \u03b6\u2217, lki(t) \u2264 1/2|Ht\u22121 }] +O(K). (38)\nLetting ht\u22121 be the realization of Ht\u22121 and let q(h)ki (t) be the value of qki(t) when Ht\u22121 = ht\u22121. Then for each k with pki > 1/2, we have\nE[Nii,k(T )]\n:= T\u2211 t=1 E [ P { (a (1) t , a (2) t ) = (i, i), \u03b6\u0302 \u2217(t) \u2265 \u03b6\u2217, lki(t) \u2264 1/2|Ht\u22121 }]\n= T\u2211 t=1 \u2211 ht\u22121:\u03b6\u0302\u2217(t)\u2265\u03b6\u2217,lki(t)\u22641/2 P{Ht\u22121 = ht\u22121}P { (a (1) t , a (2) t ) = (i, i)|Ht\u22121 = ht\u22121 }\n\u2264 T\u2211 t=1 \u2211 ht\u22121:\u03b6\u0302\u2217(t)\u2265\u03b6\u2217,lki(t)\u22641/2 1\u2212 q(h)ki (t) q (h) ki (t) P{Ht\u22121 = ht\u22121}P { (a (1) t , a (2) t ) = (i, k)|Ht\u22121 = ht\u22121 }\n\u2264 T\u2211 t=1 E [ 1\u2212 qki(t) qki(t) P { (a (1) t , a (2) t ) = (i, k),Ht\u22121 }]\n\u2264 T\u2211 t=1 E [ 1\u2212 qki(t) qki(t) 1 ( (a (1) t , a (2) t ) = (i, k) )]\n\u2264 T\u22121\u2211 n=0 E [ 1\u2212 qki(\u03c4n + 1) qki(\u03c4n + 1) \u03c4n+1\u2211 t=\u03c4n+1 1 ( (a (1) t , a (2) t ) = (i, k) )]\n\u2264 T\u22121\u2211 n=0 E [ 1 qki(\u03c4n + 1) \u2212 1 ] . (39)\nAccording to Lemma 2 in [13], E [\n1 qki(\u03c4n+1)\n] is bounded as follows:\nE [ 1 qki(\u03c4n + 1) ] \u2264 { 1 + 3\u2206ki , for n < 8 \u2206ki ,\n1 + \u0398 ( e\u2212n\u2206 2 ki/2 + 1\n(n+1)\u22062ki e\u2212nD(1/2||pki) + 1 en\u2206 2 ki /4\u22121 ) , for n \u2265 8\u2206ki .\n(40)\nThus,\nE[Nii,k(T )] \u2264 T\u22121\u2211 n=0 E [ 1 qki(\u03c4n + 1) \u2212 1 ]\n\u2264 24 \u22062ki + T\u22121\u2211 n=0 \u0398 ( e\u2212n\u2206 2 ki/2 +\n1\n(n+ 1)\u22062ki e\u2212nD(1/2||pki) +\n1\nen\u2206 2 ki/4 \u2212 1 ) \u2264 24\n\u22062ki + \u0398 ( 1 \u22062ki + 1 \u22062kiD(1/2||pki) + 1 \u22064ki + 1 \u22062ki ) . (41)\nThe conclusion then follows by summing over all k with pki > 1/2."}, {"heading": "C Regret Analysis in Condorcet Dueling Bandits", "text": "In this appendix, we discuss the proof of Theorem 2. We can show an O(K2 log T ) regret for D-TS(s)similarly to [13] and that for general Copeland dueling bandits, as stated in Proposition 1. The complete proof for Proposition 1 is omitted here due to the similarity, and we only discuss the analysis for the part we need to refine.\nTo refine the bound to O(K log T + K2 log T ), we only need to refine the third term in Eq. (18), which is the bound for the number of comparisons between i /\u2208 C\u2217 and j 6= j\u2217i . Note that under D-TS(s), when fixing the first candidate as a(1)t = i, we compare i with all j to find the best arm j\u2217i . Thus, each arm j is competing with j \u2217 i , rather than i. Thus, different from the analysis for the original D-TS, the proof should be modified accordingly, which becomes more similar to the analysis of Thompson Sampling for traditional MAB [13].\nSimilar to [13], for j 6= j\u2217i , we choose two numbers xji, yji, such that pji < xji < yji < p\u2217i = maxj pji. Redefine the events as follows:\nE p\u0304ji(t) = {p\u0304ji(t) < xji}, (42) and\nE\u03b8ji(t) = {\u03b8 (2) ji (t) < yji}. (43)\nTo further refine the bound to O(K log T +K2 log T ), we only need to refine the proof of Lemma 4 in [13], because other components are bounded byO(1). To show how to refine this proof, we recall the key step in the proof of Lemma 4 in [13] , which bounds the expected number of steps when (i, j) (j 6= j\u2217i ) are compared while p\u0304ji < xji and \u03b8 (2) ji (t) \u2265 yji:\nT\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t)}\n\u2264 T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) \u2264 Lji(T )}\n+ T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) > Lji(T )}\n\u2264 Lji(T ) + T\u2211 t=1 1 T\n= log T\nD(xji||yji) + 1, (44)\nwhere Lji(T ) is defined as Lji(T ) = log TD(xji||yji) (slightly different from that in the original D-TS, because in that case j is competing with i with pii = 1/2 and we can just let yji = 1/2.).\nTo further reduce the regret, we consider the distribution of N (1)i (T ), which is the number of steps that arm i is chosen as the first candidate, i.e., N (1)i (T ) = \u2211T t=1 1(a (1) t = i) . For \u03b1 > 0.5, let\nL\u03b1ji(n) = (1 + 2\u03b1) log n\nD(xji||yji) . (45)\nThen, T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t)}\n= T\u2211 n=0 T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) \u2264 L\u03b1ji(n), N (1) i (T ) = n}\n+ T\u2211 n=0 T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) > L\u03b1ji(n), N (1) i (T ) = n}.(46)\nWe can bound the first term as: T\u2211 n=0 T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) \u2264 L\u03b1ji(n), N (1) i (T ) = n}\n= T\u2211 n=0 T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) \u2264 L\u03b1ji(n)|N (1) i (T ) = n}P{N (1) i (T ) = n}\n(a) \u2264 T\u2211 n=0 L\u03b1ji(n)P{N (1) i (T ) = n}\n= E[(1 + 2\u03b1) logN (1)i (T )] D(xji||yji) (b) \u2264 (1 + 2\u03b1) logE[N (1) i (T )] D(xji||yji) , (47)\nwhere (b) holds due to the concavity of the log(\u00b7) function, and (a) holds because, similar to the discussion in Appendix B.2, we have\nT\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) \u2264 L\u03b1ji(n) \u2223\u2223N (1)i (T ) = n} =\nT\u2211 t=1 E [ 1 ( (a (1) t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) \u2264 L\u03b1ji(n) )\u2223\u2223N (1)i (T ) = n]\n\u2264 E [ T\u2211 t=1 1 ( (a (1) t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) \u2264 L\u03b1ji(n) )\u2223\u2223N (1)i (T ) = n] = L\u03b1ji(n).\nFor any > 0, choosing appropriate xji and yji such that D(xji||yji) = D(pji||p\u2217i )/(1 + )2, similar to [13], we have\nT\u2211 n=0 T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) \u2264 L\u03b1ji(n), N (1) i (T ) = n}\n\u2264 (1 + 2\u03b1)(1 + )2\n( log log T +O(logK) ) D(pji||p\u2217i ) . (48)\nFor the second term, different from the proof for the O(K2 log T ) regret, we analyze the probability at the time-slot where i is selected as the first candidate. Specifically, let \u03c4 (i)m be the time-slot index where i is chosen as the first candidate for the m-th time, and let \u03c4 (i)0 = 0. Then,\nT\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) > L\u03b1ji(n), N (1) i (T ) = n}\n= E [ T\u2211 t=1 1 ( (a (1) t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) > L\u03b1ji(n), N (1) i (T ) = n )] \u2264 n\u22121\u2211 m=0 E [ \u03c4(i)m+1\u2211 t=\u03c4 (i) m +1 1 ( (a (1) t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) > L\u03b1ji(n)\n)] (c)\n\u2264 n\u22121\u2211 m=0 E [ 1((a (1) \u03c4 (i) m+1 , a (2) \u03c4 (i) m+1 ) = (i, j), E p\u0304ji(\u03c4 (i) m+1), qE\u03b8ji(\u03c4 (i) m+1), Nij(\u03c4 (i) m+1 \u2212 1) > L\u03b1ji(n)) ] \u2264\nn\u22121\u2211 m=0 P{E p\u0304ji(\u03c4 (i) m+1), qE\u03b8ji(\u03c4 (i) m+1), Nij(\u03c4 (i) m+1 \u2212 1) > L\u03b1ji(n) } (d) \u2264 n \u00b7 1 n1+2\u03b1 = 1 n2\u03b1 ,\nwhere (c) holds because a(1)t = i could only happen at t = \u03c4 (i) m+1; (d) is true because, given Nij(\u03c4 (i) m+1 \u2212 1) > L\u03b1ji(n), the events E p\u0304 ji(t) and E\u03b8ji(t) are independent of t = \u03c4 (i) m+1, and thus the probability can be bounded according to the concentration property of Thompson samples (in the proof of Lemma 3 in [13]):\nP{E p\u0304ji(t), qE \u03b8 ji(t)|Nij(t\u2212 1) > L\u03b1ji(n)} \u2264 e\u2212L \u03b1 ji(n)D(xji||yji) =\n1\nn1+2\u03b1 .\nThus, T\u2211 n=0 T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t), Nij(t\u2212 1) > L\u03b1ji(n), N (1) i (T ) = n}\n\u2264 T\u2211 n=0 1 n2\u03b1 = O(1)\n(49)\nFor any i /\u2208 C\u2217, and j 6= j\u2217i , using theO(1) bounds for the {E p\u0304 ji(t), E\u03b8ji(t)} and E p\u0304 ji(t), qE\u03b8ji(t) cases, we have\nE[N (1)ij (T )] = T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t)}\n+ T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), E \u03b8 ji(t)}\n+ T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), qE p\u0304 ji(t)}\n= T\u2211 t=1 P{(a(1)t , a (2) t ) = (i, j), E p\u0304 ji(t), qE \u03b8 ji(t)}+O(1)\n\u2264 (1 + 2\u03b1)(1 + )2\n( log log T +O(logK) ) D(pji||p\u2217i ) +O(1). (50)\nThe conclusion of the theorem follows by replacing the third term in Eq. (18) according to the above inequation."}, {"heading": "D Additional Experimental Results", "text": "This appendix presents additional experimental results for more datasets, including the results for experiments based on synthetic data.\nBecause the simulation complexity is O(K2), we adjust the number of independent experiments to save time, where the number of independent experiments is set to 500, 50, 10, for K < 10, 10 \u2264 K \u2264 100, and K > 100, respectively. Most algorithms, except SCB, are stable and the results can reveal the correct trend and relative relationship."}, {"heading": "D.1 Condorcet Dueling Bandits", "text": ""}, {"heading": "D.1.1 Datasets", "text": "In additional to the MSLR dataset, we also run experiments based on the following datasets, including four synthetic datasets and the ArXiv.org dataset.\nLinearOrder: This is the Arithmetic dataset in [8], where there are eight arms with the preference probability given by pij = 0.5 + 0.05(j \u2212 i). In fact, a strict linear ordering exists among these arms.\nCyclic: A dataset adopted from [8], where the preference matrix is given by Table 1. In this dataset, Arm 1 is the Condorcet winner with p1j = 0.6, and the other arms have a cyclic preference relationship with one arm beating another with high probability. Strong transitivity does not hold in this data set, and the Condorcet winner is not necessary the best arm when comparing all other arms with a fixed arm, i.e., p1i < maxj pji for i 6= 1. StrongBorda: A 5-armed dueling bandit with a preference matrix in Table 2. In addition to a Condorcet winner, there is a strong Borda winner, which is not the Condorcet winner, but beats the other arms with high probability. We still treat this problem as a Condorcet dueling bandit problem and try to find the Condorcet winner, although a Borda winner may be more appropriate here [20].\nArXiv: A 6-armed dueling bandits with a preference matrix given in Table 3, which is derived [7] by conducting pairwise interleaving experiments based on the search engine of ArXiv.org.\nTable 1: Cyclic\n1 2 3 4 1 0.5 0.6 0.6 0.6 2 0.4 0.5 0.9 0.1 3 0.4 0.1 0.5 0.9 4 0.4 0.9 0.1 0.5\nTable 2: StrongBorda\n1 2 3 4 5 1 0.5 0.55 0.55 0.55 0.55 2 0.45 0.5 0.95 0.95 0.95 3 0.45 0.05 0.5 0.95 0.95 4 0.45 0.05 0.05 0.5 0.95 5 0.45 0.05 0.05 0.05 0.5\nTable 3: ArXiv\n1 2 3 4 5 6 1 0.50 0.55 0.55 0.54 0.61 0.61 2 0.45 0.50 0.55 0.55 0.58 0.60 3 0.45 0.45 0.50 0.54 0.51 0.56 4 0.46 0.45 0.46 0.50 0.54 0.50 5 0.39 0.42 0.49 0.46 0.50 0.51 6 0.39 0.40 0.44 0.50 0.49 0.50"}, {"heading": "D.1.2 Performance Comparisons", "text": "Because we aim at designing algorithms for general Copeland dueling bandits, we evaluate algorithms in the main body by the regret defined as (3), which is based on the normalized Copeland score (Cpld-based). As a special case, another definition of regret widely used in Condorcet dueling bandits [26, 8] is based on the preference-probability gap to the winner (G2W-based). Here we also present results for the G2W-based regret defined as [26] (the definition in [8] is the same except for an additional 1/2 factor):\nR\u2032\u0393(T ) = T\u2211 t=1 E [ \u2206 i\u2217a (1) t + \u2206 i\u2217a (2) t ] , (51)\nwhere i\u2217 is the Condorcet winner and \u2206i\u2217j is the preference probability gap between arm j and the winner. The probability gap \u2206ij represents the difference between i and j when comparing these two arms themselves, while the normalized Copeland scope difference \u03b6i \u2212 \u03b6j represents the difference between i and j when comparing them with other arms. As we can see from Figs. 2 and\n3, the relative performance based on Cpld-based and G2W-based regrets are usually similar, except in certain synthetic data where the strong transitivity is seriously violated, as will be discussed later.\nFrom Figs. 2 and 3, we can see that our D-TS algorithm outperforms BTM, RUCB, RCS, CCB, and SCB in all settings and both regret definitions.\nSome interesting results can be observed when the strong transitivity is seriously violated, e.g., in the Cyclic and StrongBorda datasets shown from Figs. 2(c) to 2(f). In these datasets, the Condorcet winner is not the best arm when comparing other arms with a fixed arm j, i.e., pi\u2217i < maxj pji. In these cases, if we want to make the relaxed transitivity hold for BTM, we need to choose a very large \u03b3, e.g., \u03b3 = 4 for the Cyclic dataset and \u03b3 = 9 for the StrongBorda dataset. These scenarios may seldomly happen in practice as we see from the MSLR and ArXiv datasets, or when they happen, the definition of \u201cwinner\u201d may need to be adjusted [20]. However, observations from these scenarios will help us understand the algorithms. The first observation is that, somewhat surprisingly, SAVAGE performs similarly to RUCB/CCB in the Cyclic dataset, and outperforms most other algorithms in the StrongBorda dataset especially when using the Cpld-based regret. The reason of this phenomenon is that SAVAGE can eliminate some suboptimal arms more quickly by leveraging the comparison results between a suboptimal arm i and its corresponding best arm j\u2217i = arg maxj pji. Specifically, take the Cyclic dataset as an example. Because 0.5 < p12 = 0.6 < p42 = 0.9, it is much easier to identify the fact of \u201carm 4 beating arm 2\u201d than that of \u201carm 1 beating arm 2\u201d. If one algorithm can \u201cwaste\u201d certain time-slots for comparing arm 2 and arm 4 at the beginning, arm 2 will be eliminated more quickly than eliminating it by comparing it with arm 1. SAVAGE is an \u201cexplore-then-exploit\u201d algorithm that sequentially explores each pair until it passes the independent test. Although this \u201cexplore-then-exploit\u201d algorithm only applies to scenarios with known time-horizon T , by leveraging this property and the problem structure, SAVAGE can perform well in certain settings such as the Cyclic and StrongBorda datasets. Similar trends can be seen for SCB, which performs better here than it does in other small scale scenarios. Second, the RMED algorithms, especially RMED2, can achieve performance close to or even better than D-TS. When one arm is beaten by the other arm with high probability, it can be eliminated quickly by RMED using the empirical divergence [8]. Relying on majority-voting results, our DTS and D-TS(s)algorithms require more efforts to get the right results when the probability that the Condorcet winner beats the other arms is relatively small. This issue is more serious for RCS that requires a \u201c100%-pass\u201d result. Thus, RCS performs even worse than RUCB/CCB in these scenarios. Third, even in these scenarios, D-TS(s)achieves similar performance as the original DTS. This is because under D-TS in Condorcet dueling bandits, the suboptimal arms will quickly lose the opportunities of being chosen as the first candidate, and the RLCB-based elimination in choosing the second candidate seldomly occurs in practice."}, {"heading": "D.2 General Copeland Dueling Bandits", "text": ""}, {"heading": "D.2.1 Datasets", "text": "Similar to Condorcet dueling bandits, we evaluate the algorithms for general Copeland dueling bandits by running experiments based on the MSLR dataset and the following synthetic datasets.\nNon-Condorcet Cyclic: This is a type of dueling bandits where cyclic preference relationships occurs among the certain arms. We consider two scenarios with K = 6 and K = 9, whose preference matrices are given by Table 4 and Table 6, respectively. For the case of K = 6, there are three Copeland winners with Copeland score 3. There is a cyclic preference relationship among the three winners and the three non-winner arms, respectively. Moreover, each Copeland winner is beaten by a non-winner arm with probability 0.9. For the case of K = 9, there are three group of arms, with Copeland score 6, 4, 2, respectively.\nNon-Condorcet StrongBorda: Similar to the Condorcet dueling bandits, we consider this case where there is a Borda winner that is different from the Copeland winner. In this non-Condorcet setting, we even assume that when comparing the Copeland winner and the Borda winner, the Copeland winner is beaten by the Borda winner with high probability.\n500-Armed Dueling Bandits: The 500-armed dueling bandit constructed in [2], where there are three Copeland winners that form a cycle and have Copeland score 498, and the other arms have Copeland scores from 0 to 496. For large scale system, we only compare CCB, SCB, and D-TS.\nTable 4: Non-Condorcet Cyclic (K = 6)\n1 2 3 4 5 6 1 0.5 0.4 0.6 0.1 0.6 0.6 2 0.6 0.5 0.4 0.6 0.1 0.6 3 0.4 0.6 0.5 0.6 0.6 0.1 4 0.9 0.4 0.4 0.5 0.1 0.9 5 0.4 0.9 0.4 0.9 0.5 0.1 6 0.4 0.4 0.9 0.1 0.9 0.5\nTable 5: Non-Condorcet StrongBorda\n1 2 3 4 5 6 1 0.5 0.05 0.55 0.55 0.55 0.55 2 0.95 0.5 0.95 0.95 0.45 0.45 3 0.45 0.05 0.5 0.95 0.95 0.95 4 0.45 0.05 0.05 0.5 0.95 0.95 5 0.45 0.55 0.05 0.05 0.5 0.95 6 0.45 0.55 0.05 0.05 0.05 0.5\nMoreover, for the MSLR data set, in addition to the 5-ranker case shown in the main body, we also study another two cases with K = 16 and K = 32. For the case of K = 16, there are two Copeland winners with Copeland score 14, and for the case of K = 32, there are two Copeland winners with Copeland score 30."}, {"heading": "D.2.2 Performance Comparison", "text": "For non-Condorcet dueling bandits, we only use the Cpld-based regret defined in (3) to evaluate the algorithm performance.\nFrom Figs. 4 and 5, we can see that the regrets of Condorcet dueling bandit algorithms, including BTM, RUCB, RCS, CCB, RMED1, and RMED2, grow rapidly, since they keep exploring for the Condorcet winner, which does not exist here. The SAVAGE algorithm for general Copeland dueling bandits achieves logarithmic regret, but is usually much larger than CCB and D-TS. In certain cases, SCB achieves comparable regret as CCB, but this algorithm can result in a very high regret if it fails to identify the Copeland winners.\nComparing CCB and D-TS, we can see that D-TS outperforms CCB in all cases. In particular, as shown in Fig. 5, D-TS significantly reduces the regret in experiments based on the real world datasets. However, from Fig. 4(a), the performance gain of D-TS seems to be small in the nonCondorcet Cyclic (K = 6) dataset, where the regret of D-TS has a similar trend as CCB. This is because in this case, the Copeland winners need to be compared with its corresponding best arm, e.g., arm 4 for the winner arm 1, until this best arm is eliminated according to RLCB. The loss before the RLCB-based elimination taking effect is similar to that of RUCB. When the number of arms is relatively small (LC is close to K), the performance gain of Thompson Sampling is relatively small. As the number of arms increases, e.g., K = 9, the performance gain becomes larger, as we can see from Fig. 4(b)."}], "references": [{"title": "Generic exploration and k-armed voting bandits", "author": ["T. Urvoy", "F. Clerot", "R. F\u00e9raud", "S. Naamane"], "venue": "International Conference on Machine Learning (ICML), pages 91\u201399,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Copeland dueling bandits", "author": ["M. Zoghi", "Z.S. Karnin", "S. Whiteson", "M. de Rijke"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The k-armed dueling bandits problem", "author": ["Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims"], "venue": "Journal of Computer and System Sciences, 78(5):1538\u20131556,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Relative upper confidence bound for the k-armed dueling bandit problem", "author": ["M. Zoghi", "S. Whiteson", "R. Munos", "M.D. Rijke"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 10\u201318,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Y. Yue", "T. Joachims"], "venue": "International Conference on Machine Learning (ICML), pages 1201\u2013 1208. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Relative confidence sampling for efficient on-line ranker evaluation", "author": ["M. Zoghi", "S.A. Whiteson", "M. De Rijke", "R. Munos"], "venue": "Proceedings of the 7th ACM international conference on Web search and data mining, pages 73\u201382. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Beat the mean bandit", "author": ["Y. Yue", "T. Joachims"], "venue": "International Conference on Machine Learning (ICML), pages 241\u2013248,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Regret lower bound and optimal algorithm in dueling bandit problem", "author": ["J. Komiyama", "J. Honda", "H. Kashima", "H. Nakagawa"], "venue": "Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, 47(2-3):235\u2013256,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, pages 285\u2013294,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1933}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in Neural Information Processing Systems, pages 2249\u20132257,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Further optimal regret bounds for thompson sampling", "author": ["S. Agrawal", "N. Goyal"], "venue": "Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pages 99\u2013107,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays", "author": ["J. Komiyama", "J. Honda", "H. Nakagawa"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Thompson sampling for budgeted multi-armed bandits", "author": ["Y. Xia", "H. Li", "T. Qin", "N. Yu", "T.-Y. Liu"], "venue": "International Joint Conference on Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Thompson sampling for complex online problems", "author": ["A. Gopalan", "S. Mannor", "Y. Mansour"], "venue": "International Conference on Machine Learning (ICML), pages 100\u2013108,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Thompson sampling for learning parameterized Markov decision processes", "author": ["A. Gopalan", "S. Mannor"], "venue": "Proceedings of The 28th Conference on Learning Theory, pages 861\u2013898,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "Mathematics of Operations Research, 39(4):1221\u20131243,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse dueling bandits", "author": ["K. Jamieson", "S. Katariya", "A. Deshpande", "R. Nowak"], "venue": "Conference on Learning Theory (COLT),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "An information-theoretic analysis of thompson sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "arXiv preprint arXiv:1403.5341,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual dueling bandits", "author": ["M. Dud\u0131\u0301k", "K. Hofmann", "R.E. Schapire", "A. Slivkins", "M. Zoghi"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Algorithms with logarithmic or sublinear regret for constrained contextual bandits", "author": ["H. Wu", "R. Srikant", "X. Liu", "C. Jiang"], "venue": "The 29th Annual Conference on Neural Information Processing Systems (NIPS), Montr\u00e9al, Canada, Dec.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Concentration of measure for the analysis of randomized algorithms", "author": ["D.P. Dubhashi", "A. Panconesi"], "venue": "Cambridge University Press,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "The k-armed dueling bandits problem", "author": ["Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims"], "venue": "Conference on Learning Theory (COLT),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case.", "startOffset": 66, "endOffset": 72}, {"referenceID": 1, "context": "This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case.", "startOffset": 66, "endOffset": 72}, {"referenceID": 2, "context": "This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case.", "startOffset": 110, "endOffset": 116}, {"referenceID": 3, "context": "This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case.", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "The dueling bandit problem [3] is a variation of the classical multi-armed bandit (MAB) problem, where the feedback comes in the format of pairwise comparison.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "This model can be applied in systems such as information retrieval (IR) [5, 6], where user preferences are easier to obtain and typically more stable.", "startOffset": 72, "endOffset": 78}, {"referenceID": 5, "context": "This model can be applied in systems such as information retrieval (IR) [5, 6], where user preferences are easier to obtain and typically more stable.", "startOffset": 72, "endOffset": 78}, {"referenceID": 2, "context": "Earlier dueling bandit algorithms such as Interleaved Filter (IF) [3] and Beat-The-Mean (BTM) [7] belong to the \u201cexplorethen-exploit\u201d family, which requires the time horizon as an input and only applies to finite-horizon settings.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Earlier dueling bandit algorithms such as Interleaved Filter (IF) [3] and Beat-The-Mean (BTM) [7] belong to the \u201cexplorethen-exploit\u201d family, which requires the time horizon as an input and only applies to finite-horizon settings.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Algorithms that work for both finite and infinite horizon settings are studied recently [4, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 7, "context": "Algorithms that work for both finite and infinite horizon settings are studied recently [4, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 3, "context": "[4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "The more recent work [8] proposes a Relative Minimum Empirical Divergence (RMED) algorithm, which is shown to be optimal in the sense of both asymptotic order and constant coefficient, under the Condorcet assumption.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "A recent work [2] studies the Copeland dueling bandits and proposes a Copeland Confidence Bound (CCB) algorithm, which can be viewed as an extension of RUCB in this general setting.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "Thompson in [10] and is also referred to as Thompson Sampling.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "Empirical studies [11] show that this simple framework achieves performance similar to or better than other types of algorithms in practice.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "proposes a logarithmic bound for its expected regret, whose constant factor is further improved in [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "Recently, Thompson Sampling becomes a popular method that is widely applied in other more complex online learning problems, such as MAB with multiple plays [14], MAB with budget constraints [15], online problems with complex actions and feedbacks [16].", "startOffset": 156, "endOffset": 160}, {"referenceID": 13, "context": "Recently, Thompson Sampling becomes a popular method that is widely applied in other more complex online learning problems, such as MAB with multiple plays [14], MAB with budget constraints [15], online problems with complex actions and feedbacks [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "Recently, Thompson Sampling becomes a popular method that is widely applied in other more complex online learning problems, such as MAB with multiple plays [14], MAB with budget constraints [15], online problems with complex actions and feedbacks [16].", "startOffset": 247, "endOffset": 251}, {"referenceID": 9, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 77, "endOffset": 93}, {"referenceID": 10, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 77, "endOffset": 93}, {"referenceID": 12, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 77, "endOffset": 93}, {"referenceID": 14, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 129, "endOffset": 137}, {"referenceID": 15, "context": "Unfortunately, the simple framework of Thompson Sampling for traditional MAB [10, 11, 12, 14] and other online learning problems [16, 17] may not directly apply to the dueling bandit problem, due to its unique properties.", "startOffset": 129, "endOffset": 137}, {"referenceID": 12, "context": "Unlike the multi-play MAB [14], dueling bandits allow to compare one arm against itself and a good learning algorithm is expected to end up with comparing the winner against itself 1.", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "Recently, [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS).", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Recently, [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS).", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Moreover, this simple framework also applies to general Copeland dueling bandits and achieves much better performance than the state-of-the-art algorithms such as Copeland Confidence Bounds (CCB) [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 14, "context": "One may suggest treating each pair of arms as a complex action and apply the simple framework of Thompson Sampling in [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 3, "context": "In order to avoid getting stuck on suboptimal comparisons, we leverage the RUCB method [4] to eliminate the arms that are unlikely to be the winners.", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "Analyzing regret bounds for Thompson Sampling is much more difficult than UCB-type algorithms, as noted in literature for traditional MAB [12, 13, 19, 16].", "startOffset": 138, "endOffset": 154}, {"referenceID": 16, "context": "Analyzing regret bounds for Thompson Sampling is much more difficult than UCB-type algorithms, as noted in literature for traditional MAB [12, 13, 19, 16].", "startOffset": 138, "endOffset": 154}, {"referenceID": 14, "context": "Analyzing regret bounds for Thompson Sampling is much more difficult than UCB-type algorithms, as noted in literature for traditional MAB [12, 13, 19, 16].", "startOffset": 138, "endOffset": 154}, {"referenceID": 1, "context": "Our conjecture is that the original D-TS will achieve an O(KLC log T + K log log T ) regret, where LC is the number of arms that beats the Copeland winners, similar to that in [2].", "startOffset": 176, "endOffset": 179}, {"referenceID": 0, "context": "The simulation results show that, for both Condorcet dueling bandits and general Copeland dueling bandits, D-TS outperforms existing algorithms such as SAVAGE [1], RUCB [4], RMED [8], and CCB [2] in many practical settings.", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "The simulation results show that, for both Condorcet dueling bandits and general Copeland dueling bandits, D-TS outperforms existing algorithms such as SAVAGE [1], RUCB [4], RMED [8], and CCB [2] in many practical settings.", "startOffset": 169, "endOffset": 172}, {"referenceID": 7, "context": "The simulation results show that, for both Condorcet dueling bandits and general Copeland dueling bandits, D-TS outperforms existing algorithms such as SAVAGE [1], RUCB [4], RMED [8], and CCB [2] in many practical settings.", "startOffset": 179, "endOffset": 182}, {"referenceID": 1, "context": "The simulation results show that, for both Condorcet dueling bandits and general Copeland dueling bandits, D-TS outperforms existing algorithms such as SAVAGE [1], RUCB [4], RMED [8], and CCB [2] in many practical settings.", "startOffset": 192, "endOffset": 195}, {"referenceID": 2, "context": "The dueling bandit problem is first brought by [3].", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "Under certain strict assumptions such as strong stochastic transitivity and stochastic triangle inequality, [3] proposes the first \u201cexplore-then-exploit\u201d algorithms called Interleaved Filter 1 (IF1) and Interleaved Filter 2 (IF2), followed-up by a Beat-TheMean (BTM) algorithm in [7], where the strong stochastic transitivity assumption is relaxed.", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "Under certain strict assumptions such as strong stochastic transitivity and stochastic triangle inequality, [3] proposes the first \u201cexplore-then-exploit\u201d algorithms called Interleaved Filter 1 (IF1) and Interleaved Filter 2 (IF2), followed-up by a Beat-TheMean (BTM) algorithm in [7], where the strong stochastic transitivity assumption is relaxed.", "startOffset": 280, "endOffset": 283}, {"referenceID": 0, "context": "Another algorithm called SAVAGE (Sensitivity Analysis of VAriables for Generic Exploration) is proposed in [1] and shown to outperform the IF and BTM algorithms by simulations, although only O(K log T ) regret is obtained for SAVAGE.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Algorithms that work for both finite and infinite horizon settings are studied in [4] and [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Algorithms that work for both finite and infinite horizon settings are studied in [4] and [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "[4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[4] extends the classic Upper Confidence Bound (UCB) method [9] to dueling bandits and proposes a Relative Upper Confidence Bound (RUCB) algorithm that achieves O(K log T ) regret in K-armed dueling bandits.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "The more recent work [8] proposes a Relative Minimum Empirical Divergence (RMED) algorithm, which is shown to be optimal in the sense of both the asymptotic order O(K log T ) and the constant coefficient, under the Condorcet assumption.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "A recent work [2] studies the Copeland dueling bandits and proposes a Copeland Confidence Bound (CCB) algorithm, which can be viewed as an extension of RUCB in this general setting.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "Because Borda dueling bandits can be reduced to the traditional MAB problem [1, 4], only a few researchers study this problem with special structures, such as sparseness [20].", "startOffset": 76, "endOffset": 82}, {"referenceID": 3, "context": "Because Borda dueling bandits can be reduced to the traditional MAB problem [1, 4], only a few researchers study this problem with special structures, such as sparseness [20].", "startOffset": 76, "endOffset": 82}, {"referenceID": 17, "context": "Because Borda dueling bandits can be reduced to the traditional MAB problem [1, 4], only a few researchers study this problem with special structures, such as sparseness [20].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "By treating each pair as an action, the SAVAGE algorithm [1] applies to dueling bandits with all the above three definitions of winners, but it is an \u201cexplore-then-exploit\u201d algorithm and exploring all pairs of arms may result in a large regret.", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "Dating back to 1933, Thompson Sampling [10] is one of the earliest algorithms for \u201cexplorationand-exploitation\u201d tradeoff and has been widely applied in traditional MAB and other more complex problems recently.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "Its effectiveness has been demonstrated by empirical studies [11], which shows that Thompson Sampling achieves performance better than or close to that of UCB-type algorithms in practice.", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "[12] proposes a logarithmic bound for the standard frequentist expected regret, whose constant factor is further improved in [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "On the other hand, through information-theoretic analysis, [21, 19] derives the bounds for its Bayesian expected regret.", "startOffset": 59, "endOffset": 67}, {"referenceID": 16, "context": "On the other hand, through information-theoretic analysis, [21, 19] derives the bounds for its Bayesian expected regret.", "startOffset": 59, "endOffset": 67}, {"referenceID": 12, "context": "Recently, Thompson Sampling becomes a popular method and has been widely applied in many variations of the traditional MAB, such as MAB with multiple plays [14], MAB with budget constraints [15], and MAB with complex actions and feedbacks [16].", "startOffset": 156, "endOffset": 160}, {"referenceID": 13, "context": "Recently, Thompson Sampling becomes a popular method and has been widely applied in many variations of the traditional MAB, such as MAB with multiple plays [14], MAB with budget constraints [15], and MAB with complex actions and feedbacks [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "Recently, Thompson Sampling becomes a popular method and has been widely applied in many variations of the traditional MAB, such as MAB with multiple plays [14], MAB with budget constraints [15], and MAB with complex actions and feedbacks [16].", "startOffset": 239, "endOffset": 243}, {"referenceID": 15, "context": "Moreover, Thompson Sampling is also used in an even more complex learning problem - parameterized Markov decision process [17], which proposes a TSMDP algorithm for this problem.", "startOffset": 122, "endOffset": 126}, {"referenceID": 5, "context": "A recent work [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS).", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "A recent work [6] combines Thompson Sampling with RUCB [4] for Condorcet dueling bandits and proposes an algorithm called Relative Confidence Sampling (RCS).", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "This assumption is made in literature [2] and usually holds in practice, e.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": ", with pi\u2217j > 1/2 for all j 6= i\u2217 [3, 4, 8].", "startOffset": 34, "endOffset": 43}, {"referenceID": 3, "context": ", with pi\u2217j > 1/2 for all j 6= i\u2217 [3, 4, 8].", "startOffset": 34, "endOffset": 43}, {"referenceID": 7, "context": ", with pi\u2217j > 1/2 for all j 6= i\u2217 [3, 4, 8].", "startOffset": 34, "endOffset": 43}, {"referenceID": 0, "context": "However, the existence of Condorcet winner is not guaranteed in practice [1, 2].", "startOffset": 73, "endOffset": 79}, {"referenceID": 1, "context": "However, the existence of Condorcet winner is not guaranteed in practice [1, 2].", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "A more general and practical definition is the Copeland winner, which is the arm (or arms) that maximizes the number of other arms it beats [1, 2].", "startOffset": 140, "endOffset": 146}, {"referenceID": 1, "context": "A more general and practical definition is the Copeland winner, which is the arm (or arms) that maximizes the number of other arms it beats [1, 2].", "startOffset": 140, "endOffset": 146}, {"referenceID": 1, "context": "To measure the performance of a dueling bandit algorithm \u0393, we adopt the definition of cumulative regret for Copeland dueling bandits [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "As pointed out in [2], the results will apply to other definitions of regret because the above definition bounds the number of queries to non-winner arms.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "At each time-slot t, before implementing Thompson Sampling, we first use the RUCB [4] of each pair to eliminate the arms that are unlikely to be the Copeland winner, resulting in a candidate set Ct (Lines 4 to 6).", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": ", [11, 12, 19, 16, 14].", "startOffset": 2, "endOffset": 22}, {"referenceID": 16, "context": ", [11, 12, 19, 16, 14].", "startOffset": 2, "endOffset": 22}, {"referenceID": 14, "context": ", [11, 12, 19, 16, 14].", "startOffset": 2, "endOffset": 22}, {"referenceID": 12, "context": ", [11, 12, 19, 16, 14].", "startOffset": 2, "endOffset": 22}, {"referenceID": 5, "context": "The existing algorithm RCS [6] uses RUCB to choose the second arm.", "startOffset": 27, "endOffset": 30}, {"referenceID": 17, "context": "501 for all j > 1, and arm-2 is not the Condorcet winner, but with p2j = 1 for all j > 2 (in fact, arm-2 is the Borda winner for larger K [20]).", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "This issue can be addressed by RUCB-based elimination as follows: when arm-2 has been compared with arm-1 sufficiently, we know that arm-1 likely beats Without RUCB-based elimination, the D-TS algorithm may still be able to get out of this trap in this case because the optimal action will be taken with a positive probability [16], but its regret will be much larger and difficult to be bound theoretically.", "startOffset": 327, "endOffset": 331}, {"referenceID": 2, "context": "The scaling behavior of this bound with respect to the time horizon T is order optimal, since a lower bound \u03a9(log T ) has been shown in [3, 8].", "startOffset": 136, "endOffset": 142}, {"referenceID": 7, "context": "The scaling behavior of this bound with respect to the time horizon T is order optimal, since a lower bound \u03a9(log T ) has been shown in [3, 8].", "startOffset": 136, "endOffset": 142}, {"referenceID": 11, "context": "We prove this lemma by borrowing the idea in [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "For any 0 < \u2264 1, we choose xji \u2208 (pji, 1/2) such thatD(xji||1/2) = D(pji||1/2)/(1+ ), which also implies 1 D(xji||pji) = O( 1 2 ) as shown in [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 11, "context": "Sketch for the Proof of Lemma 3: We prove Lemma 3 similarly to [13], but we need to adjust it to address the correlation between the selection of two candidates under D-TS.", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "However, we cannot expect that the coefficient will decay exponentially as in traditional MAB [13], because lki(t) \u2264 1/2 may indicate that arm k has not performed well when compared with i in the past.", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "By considering all possible realizations of the history Ht\u22121, we can relax the condition of lki(t) \u2264 1/2 and obtain results similar to [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "In particular, we expect to achieve lower regret as [3, 4, 8] for Condorcet dueling bandits.", "startOffset": 52, "endOffset": 61}, {"referenceID": 3, "context": "In particular, we expect to achieve lower regret as [3, 4, 8] for Condorcet dueling bandits.", "startOffset": 52, "endOffset": 61}, {"referenceID": 7, "context": "In particular, we expect to achieve lower regret as [3, 4, 8] for Condorcet dueling bandits.", "startOffset": 52, "endOffset": 61}, {"referenceID": 19, "context": "3 Discussion: Refining the Regret Bound in General Copeland Dueling Bandits As will be demonstrated in Section 5, we can see that the original version of D-TS in fact outperforms the CCB algorithm [22], which has been shown to achieve O(K(LC + 1) log T regret, where LC is the number of arms that the Copeland winners lose to.", "startOffset": 197, "endOffset": 201}, {"referenceID": 1, "context": "Based on this dataset, [2] derives a preference matrix for 136 rankers, where each ranker is a function that maps a user\u2019s query to a document ranking and can be viewed as one arm in dueling bandits.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "We use the two 5-armed submatrices in [2], one for Condorcet dueling bandit and the other for non-Condorcet dueling bandit.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "We compare D-TS and D-TSwith the following algorithms: BTM [7], SAVAGE [1], RUCB [4], RCS [6], CCB [2], SCB [2], and RMED [8], including RMED1 and RMED2.", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "3 as [7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "For RMED, we use the same settings as [8], where f(K) = 0.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "[8] has shown that RMED is optimal in Condorcet dueling bandits, not only in the sense of asymptotic order, but also the coefficients in the lower bound.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": ", with side information [22] or budget constraints [24]) is another interesting direction for future research.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": ", with side information [22] or budget constraints [24]) is another interesting direction for future research.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "[1] T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] O.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[25] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[26] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "(Chernoff-Hoeffding Bound [25]) 1) Bernoulli random variables: Given Bernoulli random variables X1, X2, .", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "2) General random variables over [0,1]: Let X1, X2, .", "startOffset": 33, "endOffset": 38}, {"referenceID": 0, "context": ", Xn be random variables over [0,1] and E[Xi|X1, X2, .", "startOffset": 30, "endOffset": 35}, {"referenceID": 0, "context": "The conclusions follow by using the Chernoff-Hoeffding bound for general random variables over [0,1].", "startOffset": 95, "endOffset": 100}, {"referenceID": 11, "context": "For the second term, letting Lji(T ) = log T D(xji||1/2) , similar to the proof of Lemma 4 in [13], we can show that it is bounded as follows:", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "where the term P{(a t , a (2) t ) = (i, j), E p\u0304 ji(t), qE ji(t), Nij(t \u2212 1) > Lji(T )} \u2264 P{(a (1) t , a (2) t ) = (i, j), E p\u0304 ji(t), qE ji(t), Nij(t\u2212 1)|Lji(T )} \u2264 1 T , as shown in [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "The third term can be bounded similarly to Lemma 3 in [13], where the only difference is that i and j are compared when (a t , a (2) t ) = (i, j) or (a (1) t , a (2) t ) = (j, i).", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "For any 0 < \u2264 1, we choose xji \u2208 (pji, 1/2) such thatD(xji||1/2) = D(pji||1/2)/(1+ ), which also implies 1 D(xji||pji) = O( 1 2 ) as shown in [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 11, "context": "Given k with pki > 1/2 andHt\u22121 such that \u03b6\u0302\u2217(t) \u2265 \u03b6\u2217 and lki(t) \u2264 1/2, similarly to [13], we can show that P { (a (1) t , a (2) t ) = (i, i)|Ht\u22121 } \u2264 1\u2212 qki(t) qki(t) P { (a (1) t , a (2) t ) = (i, k)|Ht\u22121 } .", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "According to Lemma 2 in [13], E [ 1 qki(\u03c4n+1) ] is bounded as follows:", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "We can show an O(K log T ) regret for D-TSsimilarly to [13] and that for general Copeland dueling bandits, as stated in Proposition 1.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "Thus, different from the analysis for the original D-TS, the proof should be modified accordingly, which becomes more similar to the analysis of Thompson Sampling for traditional MAB [13].", "startOffset": 183, "endOffset": 187}, {"referenceID": 11, "context": "Similar to [13], for j 6= j\u2217 i , we choose two numbers xji, yji, such that pji < xji < yji < pi = maxj pji.", "startOffset": 11, "endOffset": 15}, {"referenceID": 11, "context": "To further refine the bound to O(K log T +K log T ), we only need to refine the proof of Lemma 4 in [13], because other components are bounded byO(1).", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "To show how to refine this proof, we recall the key step in the proof of Lemma 4 in [13] , which bounds the expected number of steps when (i, j) (j 6= j\u2217 i ) are compared while p\u0304ji < xji and \u03b8 (2) ji (t) \u2265 yji:", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "For any > 0, choosing appropriate xji and yji such that D(xji||yji) = D(pji||pi )/(1 + ), similar to [13], we have", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "where (c) holds because a t = i could only happen at t = \u03c4 (i) m+1; (d) is true because, given Nij(\u03c4 (i) m+1 \u2212 1) > Lji(n), the events E p\u0304 ji(t) and E ji(t) are independent of t = \u03c4 (i) m+1, and thus the probability can be bounded according to the concentration property of Thompson samples (in the proof of Lemma 3 in [13]): P{E p\u0304 ji(t), qE \u03b8 ji(t)|Nij(t\u2212 1) > Lji(n)} \u2264 e\u2212L \u03b1 ji(n)D(xji||yji) = 1 n1+2\u03b1 .", "startOffset": 320, "endOffset": 324}, {"referenceID": 7, "context": "LinearOrder: This is the Arithmetic dataset in [8], where there are eight arms with the preference probability given by pij = 0.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Cyclic: A dataset adopted from [8], where the preference matrix is given by Table 1.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "We still treat this problem as a Condorcet dueling bandit problem and try to find the Condorcet winner, although a Borda winner may be more appropriate here [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 6, "context": "ArXiv: A 6-armed dueling bandits with a preference matrix given in Table 3, which is derived [7] by conducting pairwise interleaving experiments based on the search engine of ArXiv.", "startOffset": 93, "endOffset": 96}, {"referenceID": 22, "context": "As a special case, another definition of regret widely used in Condorcet dueling bandits [26, 8] is based on the preference-probability gap to the winner (G2W-based).", "startOffset": 89, "endOffset": 96}, {"referenceID": 7, "context": "As a special case, another definition of regret widely used in Condorcet dueling bandits [26, 8] is based on the preference-probability gap to the winner (G2W-based).", "startOffset": 89, "endOffset": 96}, {"referenceID": 22, "context": "Here we also present results for the G2W-based regret defined as [26] (the definition in [8] is the same except for an additional 1/2 factor):", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "Here we also present results for the G2W-based regret defined as [26] (the definition in [8] is the same except for an additional 1/2 factor):", "startOffset": 89, "endOffset": 92}, {"referenceID": 17, "context": "These scenarios may seldomly happen in practice as we see from the MSLR and ArXiv datasets, or when they happen, the definition of \u201cwinner\u201d may need to be adjusted [20].", "startOffset": 164, "endOffset": 168}, {"referenceID": 7, "context": "When one arm is beaten by the other arm with high probability, it can be eliminated quickly by RMED using the empirical divergence [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "500-Armed Dueling Bandits: The 500-armed dueling bandit constructed in [2], where there are three Copeland winners that form a cycle and have Copeland score 498, and the other arms have Copeland scores from 0 to 496.", "startOffset": 71, "endOffset": 74}], "year": 2016, "abstractText": "In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As indicated by its name, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison by sampling twice from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits [1, 2], including Condorcet dueling bandits [3, 4] as its special case. For general Copeland dueling bandits, we show that D-TS achieves O(K log T ) regret. For Condorcet dueling bandits, we further simplify the D-TS algorithm and show that the simplified D-TS algorithm achieves O(K log T + K log log T ) regret. Simulation results based on both synthetic and real-world data demonstrate the efficiency of the proposed D-TS algorithm.", "creator": "LaTeX with hyperref package"}}}