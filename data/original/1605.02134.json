{"id": "1605.02134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2016", "title": "Neural Recovery Machine for Chinese Dropped Pronoun", "abstract": "Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese, Japanese etc. Previous work mainly focused on painstakingly exploring the empirical features for DPs recovery. In this paper, we propose a neural recovery machine (NRM) to model and recover DPs in Chinese, so that to avoid the non-trivial feature engineering process. The experimental results show that the proposed NRM significantly outperforms the state-of-the-art approaches on both two heterogeneous datasets. Further experiment results of Chinese zero pronoun (ZP) resolution show that the performance of ZP resolution can also be improved by recovering the ZPs to DPs.", "histories": [["v1", "Sat, 7 May 2016 02:41:54 GMT  (1885kb,D)", "http://arxiv.org/abs/1605.02134v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wei-nan zhang", "ting liu", "qingyu yin", "yu zhang"], "accepted": false, "id": "1605.02134"}, "pdf": {"name": "1605.02134.pdf", "metadata": {"source": "CRF", "title": "Neural Recovery Machine for Chinese Dropped Pronoun", "authors": ["Wei-Nan Zhang", "Ting Liu", "Qingyu Yin", "Yu Zhang"], "emails": ["wnzhang@ir.hit.edu.cn"], "sections": [{"heading": null, "text": "Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese, Japanese etc. Previous work mainly focused on painstakingly exploring the empirical features for DPs recovery. In this paper, we propose a neural recovery machine (NRM) to model and recover DPs in Chinese, so that to avoid the non-trivial feature engineering process. The experimental results show that the proposed NRM significantly outperforms the state-of-the-art approaches on both two heterogeneous datasets. Further experiment results of Chinese zero pronoun (ZP) resolution show that the performance of ZP resolution can also be improved by recovering the ZPs to DPs.\nKeywords: neural network, dropped pronoun recovery, Chinese zero\npronoun resolution"}, {"heading": "1. Introduction", "text": "One of the key challenges in natural language understanding is to effectively model missing elements which are in some sense pragmatically inferable, such as dropped pronouns. A segment of Chinese sentences observed in the real human to human dialogue text is shown in Table 1. The pronouns in square brackets are dropped in the Chinese sentences. Humans can easily understand the meaning of the dialogue text due to the \u201ccoherent model\u201d in mind. However, it is nontrivial for the computer to understand the correct meaning of the incomplete\nEmail address: wnzhang@ir.hit.edu.cn (Wei-Nan Zhang)\nPreprint submitted to arXiv May 10, 2016\nar X\niv :1\n60 5.\n02 13\n4v 1\n[ cs\n.C L\n] 7\nM ay\n2 01\nsentences without recovering the dropped pronouns. Therefore, it is one of the key steps to recover the dropped pronouns for elliptical sentence completion and natural language understanding for machines. Recently, recovering the dropped pronouns has been verified to be effective for statistical machine translation [1].\nTo address the dropped pronoun recovery problem, [2] manually annotated dropped pronouns in Chinese newswire text. [3] utilized a maximum entropy (ME) classifier to recover dropped pronoun from Chinese text messages. However, the major drawbacks of the previous work are two-fold. First, manually annotating the dropped pronouns is a labor intensive work. Second, the applying of the ME classifier also dependents on empirically selecting features, which is also called the feature engineering.\nIn this study, we proposed a neural recovery machine (NRM) to recover the dropped pronouns on two heterogeneous datasets so that to avoid the nontrivial feature engineering process. The contributions of this paper include the following:\n\u2022 To our knowledge, we are the first to propose a neural network framework,\nwhich avoids the empirical feature engineering process, to recover Chinese dropped pronouns.\n\u2022 The proposed approach outperforms the state-of-the-art approach signifi-\ncantly on two heterogeneous datasets, which also demonstrate the domain adaption of the NRM.\n\u2022 By integrating the dropped pronouns into the anaphoric zero pronouns,\nwe verified that the proposed NRM can further improve the performance of zero pronoun resolution task."}, {"heading": "2. Preliminary", "text": ""}, {"heading": "2.1. Data Description", "text": "First, we utilized an interview dialogue from the \u201cbc\u201d section of the OntoNotes Release 4.01 which contains 2,501 sentences. Second, we collected a question answering dialogue2 from Baidu Zhidao3 which contains 11,160 sentences. We use both of the two heterogeneous datasets for the experiment of Chinese dropped pronoun recovery. Table 2 shows the statistics of the datasets for annotation.\n1http://catalog.ldc.upenn.edu/LDC2011T03, the files are: phoenix 0000-phoenix 0011, msnbc 0000, cnn 0000-cnn 0004, cctv 0000-cctv 0007, p2.5 cmn 0001-p2.5 cmn 0061 2A question answering dialogue is constructed by multiple turns of asking and answering between the asker and answerer. 3http://zhidao.baidu.com/"}, {"heading": "2.2. Dropped Pronoun Annotation", "text": ""}, {"heading": "2.2.1. Actual Pronouns", "text": "In this study, we follow the annotation scheme in [2]. The annotation framework which is proposed by [2] includes 14 types of pronouns. Of these 14 pronouns, 10 of them are actual pronouns that are commonly used in Chinese speech and writing. The details of these pronouns by following the description of [2] are listed below:\nTable 3: Actual pronouns and their descriptions.\nActual Pronoun Description \u6211(I) First person singular. \u6211\u4eec(we) First person plural. \u4f60(you) Second person singular. \u4f60\u4eec(you) Second person plural. \u4ed6(he) Third person masculine singular. \u4ed6\u4eec(they) Third person masculine plural. \u5979(she) Third person feminine singular. \u5979\u4eec(they) Third person feminine plural. \u5b83(it) Third person inanimate singular. \u5b83\u4eec(they) Third person inanimate plural.\nExample (1) demonstrates a context for the inanimate third person singular\npronoun. (1) CN \u90a3 \u6768\u6d0b \u636e \u4f60 \u77e5\u9053\nw2w Well Yang yang base on your know\nCN [\u5b83] \u9700\u8981 \u591a \u957f \u65f6\u95f4\uff1f\nw2w [it] need how long time?\nEN Well , Yang yang , based on your understanding ,\nhow long will it take? In this example, the one that is \u201ctake\u201d is dropped in the Chinese sentence (CN). To contrast the CN to its English translation (EN), we can see that the dropped pronoun should be referred to the pronoun \u201c \u5b83(it)\u201d."}, {"heading": "2.2.2. Abstract Pronouns", "text": "The rest 4 types of pronouns are called abstract pronouns that exist in Chinese but do not correspond to any specific Chinese words. However, they are expressible in non pro-drop languages such as English. We detail them as follows: existential: An existential subject usually appears in front of a small number of \u201cexistence\u201d verbs, such as \u201c \u6709\u201d(to have) and \u201c \u5b58\u5728\u201d(to exist) etc. Example (2) shows an existential subject preceding the verb \u201c\u6709\u201d(to have). (2) CN [existential] \u6709 \u8fd9\u4e48 \u4e8b\u513f \u55ef\nw2w [existential] have such thing um\nEN There was such an incident, um\nunspecified: An unspecified subject occurs when there is no one (or anyone) that should be interpreted. This type of subject can sometimes be translated to \u201cone\u201d or \u201csomeone\u201d. Example (3) shows an unspecified subject preceding the verb \u201c\u751f\u201d(to live). (3) CN [unspecified] \u751f \u5728 \u56fd\u9645\nw2w [Someone] live in international\nCN \u90fd\u4f1a \u7684 \u53f0\u5317\u5e02 w2w capital DE Taipei\nEN For people living in cosmopolitan Taipei\nevent: An event subject is usually the word, phrase or clause that has occurred in context, but has no need to be repetitively referred. Example (4) shows an event subject which is dropped in an interrogative sentence. (4) CN \u5988\u5988 \u8bf4\uff1a \u90a3 \u5976\u5976 \u4f4f \u6211\u4eec\nw2w Mom said: that grandma lives our\nCN \u5bb6\uff0c [event] \u597d\u5417\uff1f\nw2w home, [event] okay?\nEN Mom said: \u201cHow about grandma stays in\nour home , is that okay ?\u201d\npleonastic: A pleonastic subject is usually has no actual semantic meaning and its existence is only to satisfy the syntactic need for a subject. Example (5) shows a pleonastic subject preceding the verb \u201c\u4e0b\u96ea\u201d(snowfall).\n(5) CN \u56e0\u4e3a [pleonastic] \u4e0b\u96ea \u5bf9\nw2w Because [pleonastic] snowfall toward\nCN \u8fd9\u4e2a \u5317\u4eac \u7684 \u8def\u9762\nw2w this Beijing DE road surface\nCN \u5f71\u54cd \u4f1a \u6bd4\u8f83 \u5927\nw2w affect will relative great\nEN Since snowfall would affect road conditions in\nBeijing to a rather large extent."}, {"heading": "2.3. Annotation Statistics", "text": "For data annotation, we follow the annotation scheme proposed by [2]. To annotate the OntoNotes 4.0 sentences, two native Chinese speakers that do not participate in our experiment design are invited to annotate the DPs. To facilitate the annotation and avoid the ambiguity, we asked the two annotators\nto take the English translations of the Chinese corpus, which are also offered in the OntoNotes release 4.0 data, as reference. This is because English is a non pro-drop language, while the Chinese DPs usually exist in their corresponding English translations. To annotate the Baidu Zhidao sentences, three native Chinese speakers that do not participate in our experiment design are invited to annotate the DPs. The final labels are obtained by voting. The annotating agreement measured by Cohen\u2019s Kappa [4] equals to 0.71, which denotes a \u201cgood\u201d agreement. The distribution of these types of dropped pronouns in our data set is shown in Table 4."}, {"heading": "3. Our Approach", "text": "In this section, we detail the proposed NRM for dropped pronoun recovery. Figure 1 shows the general framework of the NRM for dropped pronoun recovery with an example.\nHere, the dropped position identification process focuses on choosing the position where a dropped pronoun should exist. The dropped pronoun generation process then generates a explicit pronoun on the dropped position."}, {"heading": "3.1. Dropped Position Identification", "text": "Figure 2 shows the framework of the dropped position identification. Here, given a raw input, we first carry out the preprocessing which is described in Section 4.1. To generate the dropped hypothesis (\u03c6), we assume that there may be a dropped position between each of the adjacent words as well as the beginning and the end of an input sentence as shown in Figure 2. Each of a possible dropped position is a dropped hypothesis. To represent the dropped hypothesis, we utilize the context embedding which is constructed by concatenating the word embedding in a specific window. We then utilize a neural network to identify the dropped position in a sentence. Here, we use a multi-layer perceptron to realize the dropped position identification neural network. Note that the dropped position identification neural network can be also realized by the convolutional neural network (CNN), the recurrent neural network (RNN), etc. We will leave the exploration of these neural networks to the future work."}, {"heading": "3.2. Dropped Pronoun Generation", "text": "After the dropped position is fixed, the next process is to generate the explicit pronoun in a sentence. Figure 3 shows the framework of the dropped\npronoun generation. As shown in Figure 3, we represent the dropped position in a sentence by using its context embedding which is constructed by concatenating the word embedding in a specific window. We then utilize a neural network to generate the explicit dropped pronoun for the dropped position. The dropped pronoun generate neural network is also realized by using a multi-layer perceptron.\nDropped Pronoun Generation"}, {"heading": "3.3. Multi-Layer Perceptron Model", "text": "[5] proposed a unified framework for natural language processing (NLP), such as part-of-speech tagging, chunking, named entity recognition, semantic role labeling, language modeling, semantically related words, etc. In this paper, we focus on another NLP task, named dropped pronoun recovery in the pro-drop language such as Chinese, Japanese, etc. Inspired by [5], to the specification of the dropped pronoun recovery task, we proposed a multi-layer perceptron model to realize the dropped position identification neural network (Section 3.1) and the dropped pronoun generation neural network (Section 3.2). We use x, Wi and bi to represent the continuous representation of the input, the weight matrix and bias of the i-th layer respectively. Here, the continuous representation of the\ninput is obtained by concatenating the word embedding, which is pre-trained as described in Section 4.1, in a specific window in a sentence. We use the ReLU as the activation function, which is described in Equation (1).\nfi = ReLU(Wix+ bi) (1)\nHere, we take rect() as the ReLU() and z = Wix+ bi. The activation function can be represented as Equation (2).\nrect(z) = max(z, 0) (2)\nAs shown in Figure 1, the dropped pronoun recovery includes 2 steps. The first step is to identify the dropped position. Hence, for each of the dropped hypothesis, the dropped position identification is transformed to a binary classification task. The second step is to generate the explicit pronouns for the fixed dropped position. It therefore can be seen as a multi-classification task. According to the statistics of the annotated data which is shown in Table 4, the dropped pronoun generation is transformed to a 14-category and a 10-category classification task on the OntoNote 4.0 and Baidu Zhidao datasets, respectively. We thus utilize a softmax function for the output layer. The softmax function is represented in Equation (3).\ny\u0302 = softmax(Woh+ bo) (3)\n= exp(h\u0302 Twi + bi)\u2211\ni\u2032 exp(h\u0302Twi\u2032 + bi\u2032)\nThe cross entropy is set to be the cost function as follows:\nC(y, y\u0302) = \u2212 \u2211\ni\nyi log(y\u0302i) (4)\nThe stochastic gradient descent (SGD) is used to update the variables.\nTo avoid overfitting in training step, we employ the dropout regularization\nscheme after each activation function."}, {"heading": "4. Experimental Results", "text": ""}, {"heading": "4.1. Data Preprocessing", "text": "To obtain the input of the NRM, there are two preprocessing steps. The first is Chinese word segmentation that segments the input raw sentences into Chinese word sequences. In this paper, we use the LTP cloud 4 service, which is a state-of-the-art Chinese word segmenter, to obtain the segmentation result. Note that the OntoNotes 4.0 data has the manually word segmentation result so that the LTP cloud is only applied on the Baidu Zhidao data. The second one is to transform the raw words into continuous word embedding. Here, we utilize the Word2Vec toolkit5 to obtain the Chinese word embedding. The learning theory of the Word2Vec is described in [6]. We train the Word2Vec with the SougouCS corpus (2008 version)6, which is a Chinese newswire corpus of which size is 1.65GB. LTP cloud is also used for the word segmentation of the corpus."}, {"heading": "4.2. Parameter Tuning", "text": "For the experiments, we separate the data in Table 2 to training data, development data and test data in the proportion of 3 : 1 : 1 in both of the OntoNotes 4.0 and Baidu Zhidao datasets. We use the development data to tune the parameters of the proposed NRM.\nFirst, we consider the accuracy variation over three parameters. They are 1) the dimension of word embedding, 2) the window size of the dropped hypothesis and 3) the number of layers of the multi-layer perceptron for the dropped position identification and dropped pronoun generation, respectively. Figure 4 and 5 shows the accuracy of dropped position identification over the dimension of word embedding, the window size of the dropped hypothesis and the number of layers of the multi-layer perceptron on the OntoNotes 4.0 and Baidu Zhidao datasets, respectively. Figure 6 and 7 show the accuracy of dropped pronoun\n4http://www.ltp-cloud.com/ 5https://code.google.com/p/word2vec/ 6http://www.sogou.com/labs/dl/cs.html\ngeneration over the dimension of word embedding, the window size of the dropped hypothesis and the number of layers of the multi-layer perceptron on the OntoNotes 4.0 and Baidu Zhidao datasets, respectively.\nSecond, Figure 8 and 9 show the accuracy variation of dropped position identification and dropped pronoun generation over 4) the value of dropout parameter on the two datasets, respectively.\nThird, Figure 10 and 11 show the accuracy variation of dropped position identification and dropped pronoun generation over 5) the number of epochs on the two datasets respectively.\nAfter the parameter tuning, we get the parameter setting, which is shown\nin Table 5, for the experiments."}, {"heading": "4.3. Dropped Pronoun Recovery Results", "text": "In our experiments, we use accuracy to evaluate the proposed approach. Four comparison systems are selected as baselines. They are SVM [7] with linear kernel and sparse vector input (SVMSL), SVM with linear kernel and\nDropout Tuning for Dropped Position Identification\nDropout Tuning for Dropped Pronoun Generation\nEpoch Tuning for Dropped Position Identification\nEpoch Tuning for Dropped Pronoun Generation\ndense vector input (SVMDL), SVM with sigmoid kernel and sparse vector input (SVMSS) and SVM with sigmoid kernel and dense vector input (SVMDS). Meanwhile, we also re-implement a state-of-the-art (SOTA) approach to recovering dropped pronoun (Yang et al. 2015 [3]) from Chinese text message for empirical comparison. Here, the sparse vector is constructed by transforming the context of a dropped hypothesis into a vocabulary size vector. When the words occur in the context, the elements in the corresponding positions of the vocabulary are set to 1 and the elements in other positions are set to 0. The dense vector is produced by concatenating the embedding of the words in the context of a dropped hypothesis.\nWe compare the performance of the proposed NRM and the baselines in both of the OntoNotes 4.0 and Baidu Zhidao datasets. The experimental results are shown in Tabel 6.\nWe can see from Table 6 that our approach significantly outperforms the baselines in statistics on both of the two datasets for DPI task and the Baidu Zhidao dataset for DPG task. To compare the results of the SVMSL and SVMDL, we can see that the dense representation of the dropped hypothesis\nhas better impact on the fine grained task, namely DPG rather than the DPI task. To compare the results of SVMSL and SVMSS as well as the SVMDL and SVMDS , we can see that the results of using linear kernel function are better than the use of sigmoid function7. To see the results on the two datasets, we can see that the results on Baidu Zhidao dataset are better than those on OntoNotes 4.0 dataset. It may be caused by the different scales of the two datasets and the different category numbers. We also note that the SOTA, which is a feature engineering approach, performs good when the data scale is small, such as the experiment results of DPG on OntoNotes 4.0 dataset. It may illustrates that the feature engineering approach can fit the data better when the data scale is small. However, the feature engineering is a highly empirical process and also a complicated task."}, {"heading": "4.4. Error Analysis on Dropped Pronoun Recovery", "text": "Further, one challenge on DPs recovery is the circumstance that the referents for DPs are clear, but the pronouns which represent them are not. This commonly occurs with nouns that represent organizations. An organization can be a referent that is referred by a singular or a plural. For example, \u201c\u516c\u53f8(company)\u201d can be the company itself as one unit or the people in the company, in which case the corresponding pronouns would be \u201c\u5b83(it)\u201d and \u201c\u4ed6\u4eec(they)\u201d, respectively. Hence, this can be viewed as an ambiguity which is intricate to model. The following two sentences are examples to show the above cases. (6) CN \u53bb\u5e74 \u5317\u4eac \u7684 \u4e00 \u5bb6 \u516c\u53f8 \u56e0\u4e3a [\u5b83]\nw2w Last year Beijing DE one company because [it]\nCN \u9500\u552e \u6708\u7403 \u571f\u5730 \u88ab \u5224\u5b9a \u4e3a \u6295\u673a \u5012\u628a\u3002\nw2w sells moon earth by judge to venture profiteer .\nEN Last year , a Beijing company was found guilty of buying and selling\non speculation because it sold land on the moon.\n7Here, we use the default setting of the parameters in LibSVM toolkit. The LibSVM\ntoolkit is available at http://www.csie.ntu.edu.tw/ cjlin/libsvm\n(7) CN \u793c\u6765 \u516c\u53f8 \u5728 \u65b0\u6cfd\u897f\u5dde \u5417\uff1f\nw2w Eli company at New Jersey MA ?\nEN Is Eli Lilly in New Jersey?\nCN \u6211 \u4e0d \u77e5\u9053 [\u4ed6\u4eec] \u516c\u53f8 \u5730\u5740 \u5728 \u54ea\u91cc\u3002\nw2w I don\u2019t know [they] company address at where .\nEN I don\u2019t know where they are based.\nIn example (6), \u201ca Beijing company\u201d is referred by the Chinese DP \u201c\u5b83(it)\u201d. While, in example (7), the \u201cEli Lilly\u201d is interpreted by the Chinese DP \u201c\u4ed6 \u4eec(they)\u201d.\nBesides that, the confusion caused by the gender of DPs is another challenge. For instance, when the antecedents are absent on context, it is non-trivial to determine the explicit pronouns for recovery. Example (8) shows the above case.\nIn example (8), the antecedent of the dropped pronoun \u201c\u5979(She)\u201d is absent on context. Other personal pronouns can also be placed in the position of the DP and are sensible both on syntactic and semantic. This problem can be alleviated by the anaphora resolution technique, which will be exploited by us in future work. (8) CN [\u5979] \u770b\u8d77\u6765 \u5c11\u5e74\u8001\u6210\uff0c \u662f\u5427\uff1f\nw2w [She] looks like young but mature , is it ?\nEN She seems young and competent, right?\nCN [\u5979] \u771f\u7684 \u662f \u8fd9\u6837\u3002\nw2w [She] really is this .\nEN She really does."}, {"heading": "4.5. Zero Pronoun Resolution Results", "text": "As the zero pronouns have no indicative information, such as the overt pronouns, we are thus motivated to recovering the dropped pronouns for the zero pronoun resolution task. For integration, we first run our NRM on the OntoNotes 5.0 data, which is a standard and authoritative dataset for the zero pronoun resolution task. For each of the anaphoric zero pronoun, which is\npredicted by the zero pronoun specific neural network (ZPSNN) [8], the NRM generates a pronoun, of which the representation is an embedding vector. The ZPSNN then extends its input with the recovered pronoun embedding and produces the zero pronoun resolution results.\nTo verify the effectiveness of the NRM for zero pronoun resolution, we compare the results of the ZPSNN and the ZPSNN+NRM with the results of the state-of-the-art (SOTA) approach [9] on zero pronoun resolution task. The experimental results are shown in Table 7. As we use the same dataset for training, development and test with [9], we directly show the original experimental results of [9] in Table 7.\nThe results on Table 7 show that by recovering the dropped pronouns for the anaphoric zero pronouns, the performance of the zero pronoun resolution can be further improved. Meanwhile, the ZPSNN+NRM significantly outperforms the SOTA approach in statistics. For further analysis, we will show the case study on the following section."}, {"heading": "4.6. Case Study on Zero Pronoun Resolution", "text": "We compare the different cases that are produced by the ZPSNN and ZPSNN+NRM. We find that the NRM can improve the performance of ZPSNN in the following kinds of cases.\n\u2022 The expected content for recovering a zero pronoun (antecedent) is an\novert pronoun.\n(9) CN \u6240\u4ee5 [\u2217AZP \u2217] \u6709 \u4e00 \u4e2a \u60f3\u6cd5\nw2w So [\u2217AZP \u2217] have a idea\nEN So, [we] have an idea.\nHere, in the case (9), the \u2217AZP\u2217 is an anaphoric zero pronoun which should be referred to \u201c\u6211\u4eec(We)\u201d. The NRM generates a dropped pronoun of \u201c\u6211\u4eec(We)\u201d for the \u2217AZP\u2217. Hence, the \u2217AZP\u2217 is correctly resolved. While, without integrating the NRM, the \u2217AZP\u2217 is recovered as \u201c\u591c\u666f(Night scene)\u201d by ZPSNN.\n\u2022 The recovered dropped pronoun is a type indicator for the antecedent.\n(10) CN [\u2217AZP \u2217] \u4ee5 \u4fe1\u606f\u5316 \u5e26\u52a8\nw2w [\u2217AZP \u2217] use informationization lead\nCN \u6559\u80b2 \u7684 \u73b0\u4ee3\u5316\nw2w education DE modernization\nEN [T heMinistryofEducation] simulates the modern-\nization of education with informationization.\nHere, the NRM first generates a pronoun of \u201c\u5b83(It)\u201d for the \u2217AZP\u2217. The system then chooses the candidate antecedents in the inanimate type. Therefore, the \u2217AZP\u2217 is recovered as \u201c\u6559\u80b2\u90e8(The Ministry of Education)\u201d by the ZPSNN+NRM system. Whereas, the ZPSNN takes a name from the context as the antecedent of the \u2217AZP\u2217."}, {"heading": "5. Related Work", "text": "The related studies with dropped pronoun recovery mainly focused on Empty Category (EC) recovery and zero anaphora resolution. The EC recovery can be\nviewed as a previous step for DP recovery. While zero anaphora resolution can be used either as features or an application for the DP recovery task."}, {"heading": "5.1. Empty Category Recovery", "text": "Motivated by the government-binding theory, empty categories (ECs) are artificially annotated to explain specific language phenomenon in Penn Treebanks.\n[10] proposed a log-linear based model to recover the ECs in Chinese Treebanks, by utilizing the lexical and syntactic features. For training and testing, they encoded the surface node into two categories, EC and NEC (non-EC). Hence, the recovery is actually a binary classification process. However, the major drawback of this study is the overlook of structure and position information. [11] presented a simple and highly effective EC recovery approach, which can fully integrate with state-of-the-art parser. The basic assumption is that the state-splitting of the parsing model will enable it to learn where to expect ECs to be inserted into the test sentences. Experimental results indicated their superiority. [12] described a novel approach to detecting ECs that represented in dependency parse trees. They first converted the phrase structure into dependency structure. The lexical and hierarchical features were then exploited for predicting both the position and type of ECs. [13] explored a clause-level hybrid approach, which integrated the linear tagging and structure parsing information, to recovering ECs in Chinese. They employed a higher level framework, semantic role labeling, to model the hybrid features. Meanwhile, a comma disambiguation approach is also utilized to improve the performance of syntactic parsing and further affect the final results on EC recovery. [14] proposed a structure learning based approach to recovering ECs for machine translation (MT) task. 3 major categories of features, lexical feature, syntactic feature and EC-specific feature, were taken into consideration. They further validated that the proposed EC recovery approach can seamlessly integrate into the two state-of-the-art MT models and enhance the performance of MT systems.\nDespite the similarity between the EC and DP recovery, the task of predict-\ning and recovering DPs is more intricate due to its large modeling space."}, {"heading": "5.2. Zero Pronoun Resolution", "text": "Zero anaphora is a pervasive phenomenon in Chinese, which is used to represent the co-reference between zero pronouns and their antecedents. A zero pronoun (ZP) is a gap in a sentence which refers to an entity or event that can supply the necessary information for interpreting the gap.\n[15] first performed the identification and resolution of Chinese anaphoric zero pronouns by using a machine learning approach. They exploited two categories of features which can be summarized to intra- and inter-sentence features, including the position of the gap, syntactic roles, comma, clause, and the distances of zero pronouns and their antecedents in sentences. Finally, a J48 decision tree model was employed to integrate these features for zero pronoun resolution. [16] proposed a unified tree kernel based framework for Chinese zero pronoun resolution. The main contribution of this study focused on measuring the similarity between zero pronouns and their antecedents on syntactic tree structure. The task, then converted to classify the pair of zero pronoun and antecedent to anaphoric zero pronoun (AZP) and non-AZP by using the SVM classifier. [17] further extended [15]\u2019s work by involving more features and exploiting the co-reference links between zero pronoun and antecedent into a SVMlight classifier. Experimental results indicated that the extended approach outperformed the two state-of-the-art approaches [16, 15] significantly.\nA zero pronoun resolution task was also carried out on other pro-drop languages like Japanese and Korean. [18] implemented the zero pronoun resolution task in a semantic role labeling framework. They first transformed the dependency parsing trees of sentences into syntactic patterns. A hybrid set of features, including lexical, grammatical, semantic and heuristic, as well as the syntactic patterns were uniquely integrated into the learning model. [19] exploited the generated lexicalized case frames from large scale Web sentences as external knowledge. They then proposed an example based log-linear model by inte-\ngrating intra- and inter-sentence features for Japanese zero pronoun resolution. [20] discussed the subjectobject drop phenomenon and the pattern found in the spoken and written text of child Korean. Further, they analyzed the similar pro-drop phenomenon among seven languages, such as English, Italian, Portuguese, Chinese, Cantonese, and Japanese. [21] presented a two step approach, which included the clause- and phrase-level antecedent detection, to resolving zero pronouns in Korean by reducing the number of candidate antecedents with syntactic structure information. [22] proposed an unsupervised approach for Chinese zero pronouns resolution with language independent features. They first predicted ten overt pronouns and then ranked the candidate antecedents. The integer linear programming approach was then employed to enhance the performance of the ranking model. They [9] further proposed an end-to-end unsupervised probabilistic model for Chinese zero pronoun resolution task. They used a salience model to capture discourse information. Experimental results showed that the proposed unsupervised model significantly outperformed the other approaches."}, {"heading": "6. Conclusion and Future Work", "text": "In this study, we present a novel neural network framework, namely NRM, for Chinese dropped pronoun recovery. Experimental results show that the proposed NRM significantly outperforms the state-of-the-art approach in statistics. Further, we integrate the recovered dropped pronoun into the zero pronoun resolution task. Experimental results show that the performance of the zero pronoun resolution can be improved by recovering the dropped pronouns.\nThe future work will be carried out on the following: First, we plan to explore more compatible neural networks to capture the context information and the compositional semantic of words. Second, as one of the obstacles of the supervised learning approach is the lack of annotated training data, we plan to automatically construct or generate training data for dropped pronoun recovery."}], "references": [{"title": "A novel approach for dropped pronoun translation", "author": ["W. Longyue", "T. Zhaopeng", "Z. Xiaojun", "L. Hang", "W. Andy", "L. Qun"], "venue": "in: Proceedings of NAACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Annotating dropped pronouns in chinese newswire text", "author": ["E. Baran", "Y. Yang", "N. Xue"], "venue": "in: LREC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Recovering dropped pronouns from chinese text messages, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)", "author": ["Y. Yang", "Y. Liu", "N. Xue"], "venue": "Association for Computational Linguistics, Beijing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A coefficient of agreement for nominal scales, Educational and psychological measurement", "author": ["J. Cohen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1960}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "in: Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Eprint Arxiv", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "A deep neural network for chinese zero pronoun resolution, in: arXiv:1604.05800v1, 2016", "author": ["Q. Yin", "W.-N. Zhang", "Y. Zhang", "T. Liu"], "venue": "URL http://arxiv.org/abs/1604.05800v1", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Chinese zero pronoun resolution: A joint unsupervised discourse-aware model rivaling state-of-the-art resolvers", "author": ["C. Chen", "V. Ng"], "venue": "in: Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Chasing the ghost: recovering empty categories in the chinese treebank", "author": ["Y. Yang", "N. Xue"], "venue": "in: Proceedings of the 23rd International Conference on Computational Linguistics: Posters, Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Language-independent parsing with empty elements", "author": ["S. Cai", "D. Chiang", "Y. Goldberg"], "venue": "in: ACL (Short Papers),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Dependency-based empty category detection via phrase structure trees", "author": ["N. Xue", "Y. Yang"], "venue": "in: Proceedings of NAACL-HLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A clause-level hybrid approach to chinese empty element recovery, in: Proceedings of the Twenty-Third international joint conference on Artificial Intelligence", "author": ["F. Kong", "G. Zhou"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Enlisting the ghost: Modeling empty categories for machine translation", "author": ["B. Xiang", "X. Luo", "B. Zhou"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Identification and resolution of chinese zero pronouns: A machine learning approach", "author": ["S. Zhao", "H.T. Ng"], "venue": "in: EMNLP-CoNLL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "A tree kernel-based unified framework for chinese zero anaphora resolution", "author": ["F. Kong", "G. Zhou"], "venue": "in: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Zero-anaphora resolution by learning rich syntactic pattern features, ACM Transactions on Asian Language Information Processing", "author": ["R. Iida", "K. Inui", "Y. Matsumoto"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames", "author": ["R. Sasano", "S. Kurohashi"], "venue": "in: IJCNLP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Subject/object drop in the acquisition of korean: A crosslinguistic comparison", "author": ["Y.-J. Kim"], "venue": "Journal of East Asian Linguistics", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming", "author": ["C. Chen", "V. Ng"], "venue": "in: Twenty- Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recently, recovering the dropped pronouns has been verified to be effective for statistical machine translation [1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "To address the dropped pronoun recovery problem, [2] manually annotated dropped pronouns in Chinese newswire text.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "[3] utilized a maximum entropy (ME) classifier to recover dropped pronoun from Chinese text messages.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Actual Pronouns In this study, we follow the annotation scheme in [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "The annotation framework which is proposed by [2] includes 14 types of pronouns.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "The details of these pronouns by following the description of [2] are listed below:", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "For data annotation, we follow the annotation scheme proposed by [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "The annotating agreement measured by Cohen\u2019s Kappa [4] equals to 0.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Multi-Layer Perceptron Model [5] proposed a unified framework for natural language processing (NLP), such as part-of-speech tagging, chunking, named entity recognition, semantic role labeling, language modeling, semantically related words, etc.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "Inspired by [5], to the specification of the dropped pronoun recovery task, we proposed a multi-layer perceptron model to realize the dropped position identification neural network (Section 3.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "The learning theory of the Word2Vec is described in [6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "They are SVM [7] with linear kernel and sparse vector input (SVMSL), SVM with linear kernel and", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "2015 [3]) from Chinese text message for empirical comparison.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "predicted by the zero pronoun specific neural network (ZPSNN) [8], the NRM generates a pronoun, of which the representation is an embedding vector.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "To verify the effectiveness of the NRM for zero pronoun resolution, we compare the results of the ZPSNN and the ZPSNN+NRM with the results of the state-of-the-art (SOTA) approach [9] on zero pronoun resolution task.", "startOffset": 179, "endOffset": 182}, {"referenceID": 8, "context": "As we use the same dataset for training, development and test with [9], we directly show the original experimental results of [9] in Table 7.", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "As we use the same dataset for training, development and test with [9], we directly show the original experimental results of [9] in Table 7.", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "[10] proposed a log-linear based model to recover the ECs in Chinese Treebanks, by utilizing the lexical and syntactic features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] presented a simple and highly effective EC recovery approach, which can fully integrate with state-of-the-art parser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] described a novel approach to detecting ECs that represented in dependency parse trees.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] explored a clause-level hybrid approach, which integrated the linear tagging and structure parsing information, to recovering ECs in Chinese.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed a structure learning based approach to recovering ECs for machine translation (MT) task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] first performed the identification and resolution of Chinese anaphoric zero pronouns by using a machine learning approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] proposed a unified tree kernel based framework for Chinese zero pronoun resolution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] further extended [15]\u2019s work by involving more features and exploiting the co-reference links between zero pronoun and antecedent into a SVM classifier.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "Experimental results indicated that the extended approach outperformed the two state-of-the-art approaches [16, 15] significantly.", "startOffset": 107, "endOffset": 115}, {"referenceID": 14, "context": "Experimental results indicated that the extended approach outperformed the two state-of-the-art approaches [16, 15] significantly.", "startOffset": 107, "endOffset": 115}, {"referenceID": 16, "context": "[18] implemented the zero pronoun resolution task in a semantic role labeling framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] exploited the generated lexicalized case frames from large scale Web sentences as external knowledge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] discussed the subjectobject drop phenomenon and the pattern found in the spoken and written text of child Korean.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] proposed an unsupervised approach for Chinese zero pronouns resolution with language independent features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "They [9] further proposed an end-to-end unsupervised probabilistic model for Chinese zero pronoun resolution task.", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese, Japanese etc. Previous work mainly focused on painstakingly exploring the empirical features for DPs recovery. In this paper, we propose a neural recovery machine (NRM) to model and recover DPs in Chinese, so that to avoid the non-trivial feature engineering process. The experimental results show that the proposed NRM significantly outperforms the state-of-the-art approaches on both two heterogeneous datasets. Further experiment results of Chinese zero pronoun (ZP) resolution show that the performance of ZP resolution can also be improved by recovering the ZPs to DPs.", "creator": "TeX"}}}