{"id": "1605.03529", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2016", "title": "On the Iteration Complexity of Oblivious First-Order Optimization Algorithms", "abstract": "We consider a broad class of first-order optimization algorithms which are \\emph{oblivious}, in the sense that their step sizes are scheduled regardless of the function under consideration, except for limited side-information such as smoothness or strong convexity parameters. With the knowledge of these two parameters, we show that any such algorithm attains an iteration complexity lower bound of $\\Omega(\\sqrt{L/\\epsilon})$ for $L$-smooth convex functions, and $\\tilde{\\Omega}(\\sqrt{L/\\mu}\\ln(1/\\epsilon))$ for $L$-smooth $\\mu$-strongly convex functions. These lower bounds are stronger than those in the traditional oracle model, as they hold independently of the dimension. To attain these, we abandon the oracle model in favor of a structure-based approach which builds upon a framework recently proposed in (Arjevani et al., 2015). We further show that without knowing the strong convexity parameter, it is impossible to attain an iteration complexity better than $\\tilde{\\Omega}\\left((L/\\mu)\\ln(1/\\epsilon)\\right)$. This result is then used to formalize an observation regarding $L$-smooth convex functions, namely, that the iteration complexity of algorithms employing time-invariant step sizes must be at least $\\Omega(L/\\epsilon)$.", "histories": [["v1", "Wed, 11 May 2016 17:30:08 GMT  (23kb)", "http://arxiv.org/abs/1605.03529v1", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG", "authors": ["yossi arjevani", "ohad shamir"], "accepted": true, "id": "1605.03529"}, "pdf": {"name": "1605.03529.pdf", "metadata": {"source": "CRF", "title": "On the Iteration Complexity of Oblivious First-Order Optimization Algorithms", "authors": ["Yossi Arjevani"], "emails": ["yossi.arjevani@weizmann.ac.il", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n03 52\n9v 1\n[ m\nat h.\nO C\n] 1\n1 M\n\u221a\nL/\u01eb) for L-smooth convex functions, and \u2126\u0303( \u221a\nL/\u00b5 ln(1/\u01eb)) for L-smooth \u00b5-strongly convex functions. These lower bounds are stronger than those in the traditional oracle model, as they hold independently of the dimension. To attain these, we abandon the oracle model in favor of a structure-based approach which builds upon a framework recently proposed in [1]. We further show that without knowing the strong convexity parameter, it is impossible to attain an iteration complexity better than \u2126\u0303 ((L/\u00b5) ln(1/\u01eb)). This result is then used to formalize an observation regarding L-smooth convex functions, namely, that the iteration complexity of algorithms employing time-invariant step sizes must be at least \u2126(L/\u01eb)."}, {"heading": "1 Introduction", "text": "The ever-increasing utility of mathematical optimization in machine learning and other fields has led to a great interest in understanding the computational boundaries of solving optimization problems. Of a particular interest is the class of unconstrained smooth, and possibly strongly convex, optimization problems. Formally, we consider the following problem,\nmin x\u2208Rd f(x)\nwhere f : Rd \u2192 R is convex and L-smooth, i.e.,\n\u2016\u2207f(x)\u2212\u2207f(y)\u2016 \u2264 L \u2016x\u2212 y\u2016\nfor some L > 0, and possibly \u00b5-strongly convex, that is,\nf(y) \u2265 f(x) + \u3008y \u2212 x,\u2207f(x)\u3009+ \u00b5 2 \u2016y \u2212 x\u20162\nfor some \u00b5 > 0. In this work, we address the question as to how fast can one expect to solve this sort of problems to a prescribed level of accuracy, using methods which are based on first-order information (gradients, or more generally sub-gradients) alone.\nThe standard approach to quantify the computational hardness of optimization problems is through the Oracle Model. In this approach, one models the interaction of a given optimization algorithm with some\ninstance from a class of functions as a sequence of queries, issued by the algorithm, to an external first-order oracle procedure. Upon receiving a query point x \u2208 Rd, the oracle reports the corresponding value f(x) and gradient \u2207f(x). In their seminal work, Nemirovsky and Yudin [11] showed that for any first-order optimization algorithm, there exists an L-smooth and \u00b5-strongly convex function f : Rd \u2192 R such that the number of queries required to obtain an \u01eb-optimal solution x\u0303 which satisfies\nf(x\u0303) < min x\u2208Rd f(x) + \u01eb,\nis at least1\n\u2126\u0303 ( min { d, \u221a \u03ba } ln(1/\u01eb) ) , \u00b5 > 0 (1)\n\u2126\u0303(min{d ln(1/\u01eb), \u221a L/\u01eb}), \u00b5 = 0\nwhere \u03ba := L/\u00b5 is the so-called condition number. This lower bound, although based on information considerations alone, is tight. Concretely, it is achieved by a combination of Nesterov\u2019s well-known accelerated gradient descent (AGD, [12]) with an iteration complexity of\nO\u0303 (\u221a \u03ba ln(1/\u01eb) ) , \u00b5 > 0 (2) O ( \u221a L/\u01eb ) , \u00b5 = 0,\nand the center of gravity method (MCG, [9, 14]) whose iteration complexity is\nO(d ln(1/\u01eb)).\nAlthough the combination of MCG and AGD appear to achieve optimal iteration complexity, this is not the case when focusing on computationally efficient algorithms. In particular, the per-iteration cost of MCG scales poorly with the problem dimension, rendering it impractical for high-dimensional problems. In other words, not taking into account the computational resources needed for processing first-order information limits the ability of the oracle model to give a faithful picture of the complexity of optimization.\nTo overcome this issue [1] recently proposed the framework of p-Stationary Canonical Linear Iterative (p-SCLI) in which, instead of modeling the way algorithms acquire information on the function at hand, one assumes certain dynamics which restricts the way new iterates are being generated. This framework includes a large family of computationally efficient first-order algorithms, whose update rule, when applied on quadratic functions, reduce to a recursive application of some fixed linear transformation on the most recent p points (in other words, p indicates the number of previous iterates stored by the algorithm in order to compute a new iterate). The paper showed that the iteration complexity of p-SCLIs over smooth and strongly convex functions is bounded from below by\n\u2126\u0303 ( p \u221a \u03ba ln(1/\u01eb) ) . (3)\nCrucially, as opposed to the classical lower bounds in (1), the lower bound in (3) holds for any dimension d > 1. This implies that even for fixed d, the iteration complexity of p-SCLI algorithms must scale with the condition number. That being said, the lower bound in (3) raises a few major issues which we wish to address in this work:\n1Following standard conventions, here, tilde notation hides logarithmic factors in the smoothness parameter, the strong convexity parameter and the distance of the initialization point from the minimizer.\n\u2022 Practical first-order algorithms in the literature only attain this bound for p = 1, 2 (by standard gradient descent and AGD, respectively), so the lower bound appears intuitively loose. Nevertheless, [1] showed that this bound is actually tight for all p. The reason for this discrepancy is that the bound for p > 2 was shown to be attained by p-SCLI algorithms whose updates require exact knowledge of spectral properties of the Hessian, which is computationally prohibitive to obtain in large-scale problems. In this work, we circumvent this issue by systematically considering the side-information available to the algorithm. In particular, we show that under the realistic assumption, that the algorithm may only utilize the strong convexity and smoothness of the objective function, the lower bound in (3) can be substantially improved.\n\u2022 The lower bound stated above is limited to stationary optimization algorithms whose coefficients \u03b1j , \u03b2j are not allowed to change in time (see Section 2.2).\n\u2022 The formulation suggested in [1] does not allow generating more than one iterate at a time. This requirement is not met by many popular optimization problems for finite sums minimization.\n\u2022 Lastly, whereas the proofs in [1] are elaborate and technically complex, the proofs we provide here are relatively short and simple.\nIn its simplest form, the framework we consider is concerned with algorithms which generate iterates by applying the following simple update rule repeatedly:\nx (k+1) =\np \u2211\nj=1\n\u03b1j\u2207f(x(k+1\u2212j)) + \u03b2jx(k+1\u2212j), (4)\nwhere \u03b1j , \u03b2j \u2208 R denote the corresponding coefficients. A clear advantage of this class of algorithms is that, given the corresponding gradients, the computational cost of executing each update rule scales linearly with the dimension of the problem and p.\nThis basic formulation already subsumes popular first-order optimization algorithms. For example, at each iteration the Gradient Descent (GD) method generates a new iterate by computing a linear combination of the current iterate and the gradient of the current iterate, i.e.,\nx (k+1) = x(k) + \u03b1\u2207f(x(k)) (5)\nfor some real scalar \u03b1. Another important example is a stationary variant of AGD [13] and the heavy-ball method (e.g., [16]) which generates iterates according to\nx (k+1) = \u03b21x (k) + \u03b11\u2207f(x(k)) + \u03b22x (k\u22121) + \u03b12\u2207f(x(k\u22121)). (6)\nIn this paper, we follow a generalized form of (4) which is exhibited by standard optimization algorithms: GD, conjugate gradient descent, sub-gradient descent, AGD, the heavy-ball method, coordinate descent, quasi-Newton methods, ellipsoid method, etc. The main difference being how much effort one is willing to put in computing the coefficients of the optimization process. We call these methods first-order p-Canonical Linear Iterative optimization algorithms (in this paper, abbr. p-CLI). We note that our framework (as a method to prove lower bounds) also applies to stochastic algorithms, as long as the expected update rule (conditioned on the history) follows a generalized form similar to (4).\nIn the context of machine learning, many algorithms for minimizing finite sums of functions with, possibly, a regularization term (also known as, Regularized Empirical Risk Minimization) also fall into our\nframework, e.g., Stochastic Average Gradient (SAG, [17]), Stochastic Variance Reduction Gradient (SVRG, [7]), Stochastic Dual Coordinate Ascent (SDCA, [19]), Stochastic Dual Coordinate Ascent without Duality (SDCA without duality, [18]) and SAGA [3], to name a few, and as such, are subject to the same lower bounds established through this framework.\nIn its full generality, the formulation of this framework is too rich to say much. In what follows, we shall focus on oblivious p-CLIs, which satisfy the realistic assumption that the coefficients \u03b1j, \u03b2j do not depend on the specific function under consideration. Instead, they can only depend on time and some limited sideinformation on the function (this term will be made more precise in Definition 1). In particular, we show that the iteration complexity of oblivious p-CLIs over L-smooth and \u00b5-strongly convex functions whose coefficients are allowed to depend on \u00b5 and L is\n\u2126\u0303 (\u221a \u03ba ln(1/\u01eb) ) , \u00b5 > 0 (7)\n\u2126\u0303( \u221a L/\u01eb), \u00b5 = 0.\nNote that, in addition to being dimension-independent (similarly to (3)), this lower bound holds regardless of p. We further stress that the algorithms discussed earlier which attain the lower bound stated in (3) are not oblivious and require more knowledge of the objective function.\nIn the paper, we also demonstrate other cases where the side-information available to the algorithm crucially affects its performance, such as knowing vs. not knowing the strong convexity parameter.\nFinally, we remark that this approach of modeling the structure of optimization algorithms, as opposed to the more traditional oracle model, can be also found in [16, 8, 5, 4]. However, whereas these works are concerned with upper bounds on the iteration complexity, in this paper we primarily focus on lower bounds.\nTo summarize, our main contributions are the following:\n\u2022 In Section 2.1, we propose a novel framework which substantially generalizes the framework introduced in [1], and includes a large part of modern first-order optimization algorithms.\n\u2022 In Section 2.2, we identify within this framework the class of oblivious optimization algorithms, whose step sizes are scheduled regardless of the function at hand, and provide an iteration complexity lower bound as given in (7). We improve upon [1] by establishing lower bounds which hold both for smooth functions and smooth and strongly convex functions, using simpler and shorter proofs. Moreover, in addition to being dimension-independent, the lower bounds we derive here are tight. In the context of machine learning optimization problems, the same lower bound is shown to hold on the bias of methods for finite sums with a regularization term, such as: SAG, SAGA, SDCA without duality and SVRG.\n\u2022 Some oblivious algorithms for L-smooth and \u00b5-strongly convex functions admit a linear convergence rate using step sizes which are scheduled regardless of the strong convexity parameter (e.g., standard GD with a step size of 1/L. See Section 3 in [17] and Section 5 in [3]). In Section 4.1, we show that adapting to \u2019hidden\u2019 strong convexity, without explicitly incorporating the strong convexity parameter, results in an inferior iteration complexity of\n\u2126\u0303 (\u03ba ln(1/\u01eb)) . (8)\nThis result sheds some light on a major issue regarding scheduling step sizes of optimization algorithms .\n\u2022 In Section 4.2, we discuss the class of stationary optimization algorithms, which use time-invariant step sizes, over L-smooth functions and show that they admit a tight iteration complexity of\n\u2126(L/\u01eb). (9)\nIn particular, this bound implies that in terms of dependency on the accuracy parameter \u01eb, SAG and SAGA admit an optimal iteration complexity w.r.t. the class of stochastic stationary p-CLIs. Acceleration schemes, such as [6, 10], are able to break this bound by re-scheduling these algorithms in a non-stationary (though oblivious) way."}, {"heading": "2 Framework", "text": ""}, {"heading": "2.1 Definitions", "text": "In the sequel we present our framework for analyzing first-order optimization algorithms. We begin by providing a precise definition of a class of optimization problems, accompanied by some side-information. We then formally define the framework of p-CLI algorithms and the corresponding iteration complexity.\nDefinition 1 (Class of Optimization Problems). A class of optimization problems C is an ordered pair of (F , I), where F is a family of functions which defined over the same domain, and I : F \u2192 I is a mapping which provides for each f \u2208 F the corresponding side-information element in some set I. The domain of the functions in F is denoted by dom(C).\nFor example, let us consider quadratic functions of the form\nx 7\u2192 1 2 x \u22a4Qx+ q\u22a4x,\nwhere Q \u2208 Rd\u00d7d is a positive semidefinite matrix whose spectrum lies in \u03a3 \u2286 R+, and q \u2208 Rd. Here, each instance may be accompanied with either a complete specification of \u03a3; lower and upper bounds for \u03a3; just an upper bound for \u03a3; a rough approximation of Q\u22121 (e.g., sketching techniques), etc. We will see that the exact nature of side-information strongly affects the iteration complexity, and that this differentiation between the family of functions under consideration and the type of side-information is not mere pedantry, but a crucial necessity.\nWe now turn to rigorously define first-order p-CLI optimization algorithms. The basic formulation shown in (4) does not allow generating more than one iterate at a time. The framework which we present below relaxes this restriction to allow a greater generality which is crucial for incorporating optimization algorithms for finite sums (see Stochastic p-CLIs in Section 2.2). We further extend (4) to allow nondifferentiable functions and constraints into this framework, by generalizing gradients to sub-gradients.\nDefinition 2. [First-order p-CLI] An optimization algorithm is called a first-order p-Canonical Linear Iterative (p-CLI) optimization algorithm over a class of optimization problems C = (F , I(\u00b7)), if given an instance f \u2208 F and an arbitrary set of p initialization points x01, . . . ,x0p \u2208 dom(C), it operates by iteratively generating points for which\nx (k+1) i \u2208\np \u2211\nj=1\n(\nA (k) ij \u2202f +B (k) ij\n)\n(x (k) j ), k = 0, 1, . . . (10)\nholds, where the coefficients Akij , B k ij are some linear operators which may depend on I(f).\nFormally, the expression A(k)ij \u2202f in (10) denotes the composition of A (k) ij and the sub-gradient operator. Likewise, the r.h.s. of (10) is to be understand as an evaluation of sum of two operators A(k)ij \u2202f and B (k) ij at x (k) j .\nIn this level of generality, this framework encompasses very different kinds of optimization algorithms. We shall see that various assumptions regarding the coefficients complexity and side-information yield different lower bound on the iteration complexity.\nWe note that although this framework concerns algorithms whose update rules are based on a fixed number of points, a large part of the results shown in this paper holds in the case where p grows indefinitely in accordance with the number of iterations.\nWe now turn to provide a formal definition of Iteration Complexity. We assume that the point returned after k iterations is x(k)p . This assumption merely serves as a convention and is not necessary for our bounds to hold.\nDefinition 3 (Iteration Complexity). The iteration complexity IC(\u01eb) of a given p-CLI w.r.t. a given problem class C = (F , I) is defined to be the minimal number of iterations K such that\nf(Ex(k)p )\u2212 min x\u2208domC f(x) < \u01eb, \u2200k \u2265 K\nuniformly over F , where the expectation is taken over all the randomness introduced into the optimization process (see Stochastic p-CLIs below).\nFor simplicity, when stating bounds in this paper, we shall omit the dependency of the iteration complexity on the initialization points. The precise dependency can be found in the corresponding proofs.\n2.2 Classification of First-order p-CLIs and Scope of Work\nAs mentioned before, we cannot say much about the framework in its full generality. In this paper, we restrict our attention to the following three (partially overlapping) classes of p-CLIs:\nStationary p-CLI where the coefficients are allowed to depend exclusively on side-information (see Definition 3). In particular, the coefficients are not allowed to change with time. Seemingly restrictive, this class of p-CLIs subsumes many efficient optimization methods, especially when coupled with stochasticity (see below). Notable stationary p-CLIs are: GD with fixed step size [13], stationary AGD [13] and the Heavy-Ball method [16].\nOblivious p-CLI where the coefficients are allowed to depend on side-information, as well as to change in time. Notable algorithms here are GD and AGD with step sizes which are scheduled irrespectively of the function under consideration [13] and the Sub-Gradient Descent method (e.g., [20]).\nStochastic p-CLI where (10) holds with respect to Ex(k)j , that is,\nEx (k+1) i \u2208\np \u2211\nj=1\n(\nA (k) ij \u2202f +B (k) ij\n)\n(Ex (k) j ). (11)\nStochasticity is an efficient machinery of tackling optimization problems where forming the gradient is prohibitive, but engineering an efficient unbiased estimator is possible. Such situations occur frequently in the context of machine learning, where one is interested in minimizing finite sums of large number of convex functions,\nmin x\u2208Rd F (x) :=\nm \u2211\ni=1\nfi(x),\nin which case, forming a sub-gradient of F at a given point may be too expensive. Notable optimization algorithms for variants of this problem are: SAG, SDCA without duality, SVRG and SAGA, all of which are stationary stochastic p-CLIs. Moreover, as opposed to algorithms which produce only one new point at each iteration (e.g., (4)), these algorithms sometimes update a few points at the same time. To illustrate this, let us express SAG as a stochastic stationary (m + 1)-CLI. In order to avoid the computationally demanding task of forming the exact gradient of F at each iteration, SAG uses the first m points to store estimates for the gradients of the individual functions\nyi \u2248 \u2207fi(x(k)m+1), i = 1 . . . m.\nAt each iteration, SAG sets yi = \u2207fi(x(k)m+1) for some randomly chosen i \u2208 [m], and then updates x(k)m+1 accordingly, by making a gradient step with a fixed step size using the new estimate for \u2207F (x(k)m+1). This implies that the expected update rule of SAG is stationary and satisfies (11).\nAs opposed to an oblivious schedule of step sizes, many optimization algorithms set the step sizes according to the first-order information which is accumulated during the optimization process. A wellknown example for such a non-oblivious schedule is conjugate gradient descent, whose update rule can be expressed as follows:\nx (k+1) 1 = x (k) 2 x (k+1) 2 = (\u03b1\u2202f + (1 + \u03b2)I)x (k) 2 \u2212 \u03b2x (k) 1 , (12)\nwhere the step sizes are chosen so as to minimize f(x(k+1)1 ) over \u03b1, \u03b2 \u2208 R. Other algorithms employ coefficients whose schedule does not depend directly on first-order information. For example, at each iteration coordinate descent updates one coordinate of the current iterate, by completely minimizing the function at hand along some direction. In our formulation, such update rules are expressed using coefficients which are diagonal matrices. In a sense, the most expensive coefficients used in practice are the one employed by Newton method, which in this framework, may be expressed as follows:\nx (k+1) 1 = (I \u2212\u22072(f)\u22121\u2207f)x (k) 1 (13)\nThe algorithms mentioned above: conjugate gradient descent, coordinate descent and Newton methods; as well as other non-oblivious p-CLI optimization algorithms, such as quasi-Newton methods (e.g., [15]) and the ellipsoid method (e.g., [2]), will not be further considered in this paper.\n3 Lower Bounds on the Iteration Complexity of Oblivious p-CLIs\nHaving formally defined the framework, we are now in position to state our first main result. Perhaps the most common side-information used by practical algorithms is the strong-convexity and smoothness parameters of the objective function. Oblivious p-CLIs with such side-information tend to have low periteration cost and a straightforward implementation. However, this lack of adaptivity to the function being optimized results in an inevitable lower bound on the iteration complexity:\nTheorem 1. Suppose the smoothness parameter L and the strong convexity parameter \u00b5 are known, i.e., I(\u00b7) = {L, \u00b5}. Then the iteration complexity of any oblivious, possibly stochastic, p-CLI optimization\nalgorithm is bounded from below by\n\u2126\u0303 (\u221a \u03ba ln(1/\u01eb) ) , \u00b5 > 0 (14)\n\u2126( \u221a L/\u01eb), \u00b5 = 0,\nwhere \u03ba := L/\u00b5\nAs discussed in the introduction, Theorem 1 significantly improves upon the lower obtained by [1] in 3 major aspects:\n\u2022 It holds for both smooth functions, as well as smooth and strongly convex functions.\n\u2022 In both the strongly-convex and non-strongly convex cases, the bounds we derive are tight for p > 1 (Note that if the coefficients are scalars and time-invariant, then for smooth and strongly convex functions a better lower bound of \u2126\u0303(\u03ba ln(1/\u01eb)) holds. See Theorem 8, [1]).\n\u2022 It considers a much wider class of algorithms, namely, methods which may use different step size at each iteration and may freely update each of the p points.\nWe stress again that, in contrast to (1), this lower bound does not scale with the dimension of the problem. The proof of Theorem 1, including logarithmic factors and constants which appear in the lower bound, is found in (A.1), and can be roughly sketched as follows. First, we consider L-smooth and \u00b5-strongly convex quadratic functions of the form\nx 7\u2192 1 2 x \u22a4diag (\u03b7)x+ \u03b71\u22a4x, \u03b7 \u2208 [\u00b5,L],\nover Rd, all of which share the same minimizer,\nx \u2217 = \u2212 diag\u22121(\u03b7)\u03b71 = \u22121.\nNext, we observe that each iteration of p-CLI involves application of A\u2202f +B, which is a linear expression in \u2202f whose coefficients are some linear operators, on the current points x(k)j , j = 1, . . . , p, which are then summed up to form the next iterate. Applying this argument inductively, and setting the initialization points to be zero, we see that the point returned by the algorithm at the k\u2019th iteration can be expressed as follows,\nx (k) p = (s1(\u03b7)\u03b7, . . . , sd(\u03b7)\u03b7) \u22a4,\nwhere si(\u03b7) are real polynomials of degree k \u2212 1. Here, the fact that the coefficients are scheduled obliviously, i.e., do not depend on the very choice of \u03b7, is crucial (when analyzing other types of p-CLIs, one may encounter cases where the coefficients of s(\u03b7) are not constants, in which case the resulting expression may not be a polynomial). Bearing in mind that our goal is to bound the distance to the minimizer \u22121 (which, in thic case, is equivalent to the iteration complexity up to logarithmic factors), we are thus led to ask how small can |s(\u03b7)\u03b7 + 1| be. Formally, we aim to bound\nmax \u03b7\u2208[\u00b5,L]\n|s(\u03b7)\u03b7 + 1|\nfrom below. To this end, we use the properties of the well-known Chebyshev polynomials, by which we derive the following lower bound:\nmin s(\u03b7)\u2208R[\u03b7],\u2202(s)=k\u22121 max \u03b7\u2208[\u00b5,L]\n|s(\u03b7)\u03b7 + 1| \u2265 (\u221a\n\u03ba\u2212 1\u221a \u03ba+ 1\n)k\n.\nThe proof of the smooth non-strongly convex case is also based on a reduction from a minimization problem to a polynomial approximation problem, only this time the resulting approximation problem is slightly different (see Equation (21) in Appendix A.2).\nThe idea of reducing optimization bounds to polynomial approximation problems is not new, and is also found for instance in [11], where lower bounds under the oracle model are derived. In particular, both approaches, the oracle model and p-CLI, exploit the idea that when applied on some strongly convex quadratic functions 12x\n\u22a4Qx + q\u22a4x over Rd, the k\u2019th iterate can be expressed as s(Q)q for some real polynomial s(\u03b7) \u2208 R[\u03b7] of degree at most k \u2212 1. Bounding the iteration complexity is then essentially reduced to the question of how well can we approximate Q\u22121 using such polynomials. However, the approach here uses a fundamentally different technique for achieving this, and whereas the oracle model does not impose any restrictions on the coefficients of s(\u03b7), the framework of p-CLIs allows us to effectively control the way these coefficients are being produced. The excessive freedom in choosing s(\u03b7) constitutes a major weakness in the oracle model and prevents obtaining iteration complexity bounds significantly larger than the dimension d. To see why, note that by the Cayley-Hamilton theorem, there exists a real polynomial s(\u03b7) of degree at most d \u2212 1 such that s(Q) = \u2212Q\u22121. Therefore, the d\u2019th iterate can potentially be s(Q)q = \u2212Q\u22121q, the exact minimizer. We avoid this limited applicability of the oracle model by adopting a more structural approach, which allows us to restrict the kind of polynomials which can be produced by practical optimization algorithms. Furthermore, our framework is more flexible in the sense that the coefficients of s(\u03b7) may be formed by optimization algorithms which do not necessarily fall into the category of first-order algorithms, e.g., coordinate descent.\nIt is instructive to contrast our approach with another structural approach for deriving lower bounds which was proposed by [13]. Nesterov [13] considerably simplifies the technique employed by Nemirovsky and Yudin [11] at the cost of introducing additional assumption regarding the way new iterates are generated. Specifically, it is assumed that each new iterate lies in the span of all the gradients acquired earlier. Similarly to [11], this approach also does not yield dimension-independent lower bounds. Moreover, such an approach may break in presence of conditioning mechanisms (which essentially, aim to handle poorly-conditioned functions by multiplying the corresponding gradients by some matrix). In our framework, such conditioning is handled through non-scalar coefficients. Thus, as long as the conditioning matrices depend solely on \u00b5,L our lower bounds remain valid."}, {"heading": "4 Side-Information in Oblivious Optimization", "text": ""}, {"heading": "4.1 No Strong Convexity Parameter, No Acceleration", "text": "Below we discuss the effect of not knowing exactly the strong convexity parameter on the iteration complexity of oblivious p-CLIs. In particular, we show that the ability of oblivious p-CLIs to obtain iteration complexity which scales like \u221a \u03ba crucially depends on the quality of the strong convexity estimate of the function under consideration. Moreover, we show that stationary p-CLIs are strictly weaker than general oblivious p-CLIs for smooth non-strongly convex functions, in the sense that stationary p-CLIs cannot obtain an iteration complexity of O( \u221a\nL/\u01eb). The fact that decreasing the amount of side-information increases the iteration complexity is best demon-\nstrated by a family of quadratic functions which we already discussed before, namely,\nx 7\u2192 1 2 x \u22a4Qx+ q\u22a4x,\nwhere Q \u2208 Rd\u00d7d is positive semidefinite whose spectrum lies in \u03a3 \u2286 R+ and q \u2208 Rd. In Theorem 8 in\n[1], it is shown that if Q is given in advance, but q is unknown, then the iteration complexity of stationary p-CLIs which follows (4) is\n\u2126\u0303( p \u221a \u03ba ln(1/\u01eb)).\nIt is further shown that this lower bound is tight (see Appendix A in [1]). In Theorem 1 we show that if both the smoothness and the strong convexity parameters {\u00b5,L} are known then the corresponding lower bound for this kind of algorithms is\n\u2126\u0303( \u221a \u03ba ln(1/\u01eb)).\nAs mentioned earlier, this lower bound is tight and is attained by a stationary version of AGD. However, what if only the smoothness parameter L is known a-priori? The following theorem shows that in this case the iteration complexity is substantially worse. For reasons which will become clear later, it will be convenient to denote the strong convexity parameter and the condition number of a given function f by \u00b5(f) and \u03ba(f), respectively.\nTheorem 2. Suppose that only L the smoothness parameter is known, i.e. I(\u00b7) = {L}. If the iteration complexity of a given oblivious, possibly stochastic, p-CLI optimization algorithm is\nO\u0303(\u03ba(f)\u03b1 ln(1/\u01eb)), (15)\nthen \u03b1 \u2265 1.\nTheorem 2 pertains to the important issue of optimal schedules for step sizes. Concretely, it implies that, in the absence of the strong convexity parameter, one is still able to schedule the step sizes according to the smoothness parameter so as to obtain exponential convergence rate, but only to the limited extent of linear dependency on the condition number (as mentioned before, this sub-optimality in terms of dependence on the condition number, can be also found in [17] and [3]). This bound is tight and is attained by standard gradient descent (GD).\nTheorem 2 also emphasizes the superiority of standard GD in cases where the true strong convexity parameter is poorly estimated. Such situations may occur when one underestimate the true strong convexity parameter by following the strong convexity parameter introduced by an explicit regularization term. Specifically, if \u00b5\u0302 denotes our estimate for the true strong convexity parameter \u00b5 (obviously, \u00b5\u0302 < \u00b5 to ensure convergence), then Theorem 1 already implies that, for a fixed accuracy level, the worst iteration complexity of our algorithm is on the order of \u221a\nL/\u00b5\u0302, whereas standard GD with 1/L step sizes has iteration complexity on the order of L/\u00b5. Thus, if our estimate is too conservative, i.e., \u00b5\u0302 < \u00b52/L, then the iteration complexity of GD is \u00b5/ \u221a L\u00b5\u0302 \u2265 1 times better. Theorem 2 further strengthen this statement, by indicating that if our estimate does not depend on the true strong convexity parameter, then the iteration complexity of GD is even more favorable with a factor of \u00b5/\u00b5\u0302 \u2265 1, compared to our algorithm.\nThe proof of Theorem 2, which appears in Appendix A.2, is again based on a reduction to an approximation problem via polynomials. In contrast to the proof of Theorem 1 which employs Chebyshev polynomials, here only elementary algebraic manipulations are needed.\nAnother implication of Theorem 2 is that the coefficients of optimal stationary p-CLIs for smooth and strongly convex functions must have an explicit dependence on the strong convexity parameter. In the next section we shall see that this fact is also responsible for the inability of stationary p-CLIs to obtain a rate of O( \u221a L/\u01eb) for L-smooth convex functions."}, {"heading": "4.2 No Acceleration for Stationary Algorithms over Smooth Convex Functions", "text": "Below, we prove that, as opposed to oblivious p-CLIs, stationary p-CLIs (namely, p-CLIs with time-invariant coefficients) over L-smooth convex functions can obtain an iteration complexity no better than O(L/\u01eb). An interesting implication of this is that some current methods for minimizing finite sums of functions, such as SAG and SAGA (which are in fact stationary p-CLIs) cannot be optimal in this setting, and that timechanging coefficients are essential to get optimal rates. This further motivates the use of current acceleration schemes (e.g., [6, 10]) which turn a given stationary algorithm into an non-stationary oblivious one.\nThe proof of this result is based on a reduction from the class of p-CLIs over L-smooth convex functions to p-CLIs over L-smooth and \u00b5-strongly convex, where the strong convexity parameter is given explicitly. This reduction allows us to apply the lower bound in Theorem 2 on p-CLIs designed for smooth non-strongly convex functions.\nWe now turn to describe the reduction in detail. In his seminal paper, Nesterov [12] presents the AGD algorithm and shows that it obtains a convergence rate of\nf(xk)\u2212 f(x\u2217) \u2264 4L \u2225 \u2225x 0 \u2212 x\u2217 \u2225 \u2225 2\n(k + 2)2 (16)\nfor L-smooth convex functions, which admits at least one minimizer (accordingly, throughout the rest of this section we shall assume that the functions under consideration admit at least one minimizer, i.e., argmin(f) 6= \u2205). In addition, Nesterov proposes a restarting scheme of this algorithm which, assuming the strong convexity parameter is known, allows one to obtain an iteration complexity of O\u0303(\u221a\u03ba ln(1/\u01eb)). Scheme 4.2 shown below forms a simple generalization of the scheme discussed in that paper, and allows one to explicitly introduce a strong convexity parameter into the dynamics of (not necessarily oblivious) p-CLIs over L-smooth convex functions.\nSCHEME 4.2 RESTARTING SCHEME PARAMETERS SMOOTHNESS PARAMETER L > 0\nSTRONG CONVEXITY PARAMETER \u00b5 > 0 CONVERGENCE PARAMETERS \u03b1 > 0, C > 0\nGIVEN A p-CLI OVER L-SMOOTH FUNCTIONS P WITH\nf(xk)\u2212 f\u2217 \u2264 CL\u2016x\u03040\u2212x\u2217\u20162\nk\u03b1\nFOR ANY INITIALIZATION VECTOR x\u03040\nITERATE FOR t = 1, 2, . . .\nRESTART THE STEP SIZE SCHEDULE OF P INITIALIZE P AT x\u03040 RUN P FOR \u03b1 \u221a 4CL/\u00b5 ITERATIONS SET x\u03040 TO BE THE LAST ITERATE OF THIS EXECUTION\nEND\nThe following lemma provides an upper bound on the iteration complexity of p-CLIs obtained through Scheme 4.2.\nLemma 1. The convergence rate of a p-CLI algorithm obtained by applying Scheme 4.2, using the corresponding set of parameters L, \u00b5,C, \u03b1, is\nO\u0303 ( \u03b1 \u221a \u03ba ln(1/\u01eb) ) ,\nwhere \u03ba = L/\u00b5 denotes the condition number.\nProof Suppose P is a p-CLI as stated in Scheme 4.2 and let f be a L-smooth and \u00b5-strongly convex function. Each external iteration in this scheme involves running P for k = \u03b1 \u221a\n4CL/\u00b5 iterations, Thus, for any arbitrary point x\u0304,\nf(x(k))\u2212 f\u2217 \u2264 CL \u2016x\u0304\u2212 x \u2217\u20162\n( \u03b1 \u221a 4CL/\u00b5)\u03b1 = \u2016x\u0304\u2212 x\u2217\u20162 4/\u00b5 .\nAlso, f is \u00b5-strongly convex, therefore\nf(x(k))\u2212 f\u2217 \u2264 2(f(x\u0304)\u2212 f(x \u2217))/\u00b5 4/\u00b5 \u2264 f(x\u0304)\u2212 f(x \u2217) 2 .\nThat is, after each external iteration the sub-optimality in the objective value is halved. Thus, after T external iterations, we get\nf(x(T \u03b1 \u221a\n4CL/\u00b5))\u2212 f\u2217 \u2264 f(x\u0304 0)\u2212 f(x\u2217) 2T ,\nwhere x\u03040 denotes some initialization point. Hence, the iteration complexity for obtaining an \u01eb-optimal solution is\n\u03b1 \u221a 4C\u03ba log2 ( f(x\u03040)\u2212 f(x\u2217) \u01eb ) .\nThe stage is now set to prove the statement made at the beginning of this section. Let P be a stationary p-CLI over L-smooth functions with a convergence rate of O(L/k\u03b1), and let \u00b5 \u2208 (0, L) be the strong convexity parameter of the function to be optimized. We apply Scheme 4.2 to obtain a new p-CLI, which according to Lemma 1, admits an iteration complexity of O( \u03b1\u221a\u03ba ln(1/\u01eb)). But, since P is stationary, the resulting p-CLI under Scheme 4.2 is again P (That is, stationary p-CLIs are invariant w.r.t. Scheme 4.2). Now, P is a p-CLI over smooth non-strongly convex, and as such, its coefficients do not depend on \u00b5. Therefore, by Theorem 2, we get that \u03b1 \u2264 1. Thus, we arrive at the following corollary:\nCorollary 1. If the iteration complexity of a given stationary p-CLI over L-smooth functions is\nO ( \u03b1 \u221a L/\u01eb ) ,\nthen \u03b1 \u2264 1.\nThe lower bound above is tight and is attained by standard Gradient Descent."}, {"heading": "5 Summary", "text": "In this work, we propose the framework of first-order p-CLIs and show that it can be efficiently utilized to derive bounds on the iteration complexity of a wide class of optimization algorithms, namely, oblivious, possibly stochastic, p-CLIs over smooth and strongly-convex functions.\nWe believe that these results are just the tip of the iceberg, and the generality offered by this framework can be successfully instantiated for many other classes of algorithms. For example, it is straightforward to\nderive a lower bound of \u2126(1/\u01eb) for 1-CLIs over 1-Lipschitz (possibly non-smooth) convex functions using the following set of functions\n{ \u2016x\u2212 c\u2016 \u2223 \u2223 \u2223c \u2208 Rd } .\nHow to derive a lower bound for other types of p-CLIs in the non-smooth setting is left to future work.\nAcknowledgments: This research is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel Science Foundation grant 425/13. We thank Nati Srebro for several helpful discussions and insights."}, {"heading": "A Proofs", "text": "A.1 Proof for Theorem 1\nLet us apply the given oblivious p-CLI algorithm on a quadratic function of the form\nf : Rd \u2192 R : x 7\u2192 1 2 x \u22a4Qx+ q\u22a4x,\nwhere Q = diag (\u03b7, . . . , \u03b7) and q = \u2212v\u03b7 for some \u03b7 \u2208 [\u00b5,L] and v 6= 0 \u2208 Rd. In particular, we have that the norm of the unique minimizer is \u2016x\u2217\u2016 = \u2225 \u2225\u2212Q\u22121q \u2225\n\u2225 = \u2016v\u2016. We set the initialization points to be zero, i.e., Exj = 0, j = 1, . . . , p, and denote the corresponding coefficients by A (k) ij , B (k) ij \u2208 Rd\u00d7d. The crux of proof is that, as long as \u03b7 lies in [\u00b5,L], the side-information {\u00b5,L} remains consistent, and therefore, the coefficients remain unchanged.\nFirst, we express Exk+1i in terms of Q,q and Ex (k) 1 , . . . ,Ex (k) p \u2208 Rd. By Definition 3 we have for any\ni \u2208 [p],\nEx k+1 i =\np \u2211\nj=1\n(\nA (k) ij \u2202f +B (k) ij\n)\n(Ex (k) j )\n=\np \u2211\nj=1\n(A (k) ij \u2202f(Ex (k) j ) +B (k) ij Ex (k) j )\n=\np \u2211\nj=1\n(A (k) ij (QEx (k) j + q) +BijEx (k) j )\n=\np \u2211\nj=1\n(A (k) ij Q+B (k) ij )Ex (k) j +\np \u2211\nj=1\nA (k) ij q.\nOur next step is to reduce the problem of minimizing f to a polynomial approximation problem. We claim that for any k \u2265 1 and i \u2208 [d] there exist d real polynomials sk,i,1(\u03b7), . . . , sk,i,d(\u03b7) of degree at most k \u2212 1, such that\nEx (k) i = (sk,i,\u2217(\u03b7))\u03b7, (17)\nwhere (sk,i,\u2217(\u03b7)) := (sk,i,1(\u03b7), . . . , sk,i,d(\u03b7)) \u22a4.\nLet us prove this claim using mathematical induction. For k = 1 we have\nEx (1) i =\np \u2211\nj=1\n(A (0) ij Q+B (0) ij )Ex (0) j +\np \u2211\nj=1\nA (0) ij q = \u2212\np \u2211\nj=1\nA (0) ij v\u03b7, (18)\nshowing that the base case holds. For the induction step, assume the statement holds for some k > 1 with\nsk,i,j(\u03b7) as above. Then,\nEx (k+1) i =\np \u2211\nj=1\n(A (k) ij Q+B (k) ij )Ex (k) j +\np \u2211\nj=1\nA (k) ij q\n=\np \u2211\nj=1\n(A (k) ij diag (\u03b7, . . . , \u03b7) +B (k) ij )(sk,j,\u2217(\u03b7))\u03b7 \u2212\np \u2211\nj=1\nA (k) ij v\u03b7\n=\n\n\np \u2211\nj=1\n(A (k) ij diag (\u03b7, . . . , \u03b7) +B (k) ij )(sk,j,\u2217(\u03b7)) \u2212\np \u2211\nj=1\nA (k) ij v\n\n \u03b7. (19)\nThe expression inside the last parenthesis is a vector with d entries, each of which contains a real polynomial of degree at most k. This concludes the induction step (note that the derivations of equalities (18) and (19) above are exactly where we use the fact that there is no functional dependency of A(k)ij and B (k) ij on \u03b7).\nWe are now ready to estimate the sub-optimality of Ex(k)p , the expected point returned by the algorithm at the k\u2019th iteration. Setting m \u2208 argmaxj |vj | we have\n\u2225 \u2225 \u2225 Ex (k) p \u2212 x\u2217 \u2225 \u2225 \u2225 = \u2016(sk,p,\u2217(\u03b7))\u03b7 + v\u2016 \u2265 |sk,p,m(\u03b7)\u03b7 + vm| = |vm||sk,p,m(\u03b7)\u03b7/vm + 1|. (20)\nBy Lemma 2 in appendix B, there exists \u03b7 \u2208 [\u00b5,L], such that\n|sk,p,m(\u03b7)\u03b7/vm + 1| \u2265 (\u221a\n\u03ba\u2212 1\u221a \u03ba+ 1\n)k\n,\nwhere \u03ba = L/\u00b5. Defining Q and q accordingly, and choosing, e.g., v = Re1 where R denotes a prescribed distance, yields\n\u2225 \u2225 \u2225Ex (k) p \u2212 x\u2217 \u2225 \u2225 \u2225 \u2265 R (\u221a\n\u03ba\u2212 1\u221a \u03ba+ 1\n)k\n.\nUsing the fact that f is \u00b5-strongly convex concludes the proof of the first part of the theorem. For the smooth case we need to estimate f(Ex(k)p )\u2212f\u2217. Let \u03b7 \u2208 (0, L] and define Q and q, accordingly. Inequality (20) yields\nf(Ex(k))\u2212 f(x\u2217) = 1 2 (Ex(k) \u2212 x\u2217)\u22a4Q(Ex(k) \u2212 x\u2217) (21)\n\u2265 (vm) 2\n2 \u03b7((sk,p,m)(\u03b7)\u03b7/vm + 1)\n2.\nNow, by Lemma 3 in appendix B,\nmin s(\u03b7), \u2202s\u2264k\u22121 max \u03b7\u2208(0,L] \u03b7(s(\u03b7)\u03b7 + 1)2 \u2265 L (2k + 1)2\nThus, choosing v = Re1 concludes the proof.\nA.2 Proof for Theorem 2\nThe proof of this theorem follows the exact reduction used in the proof of Theorem 1 (see Appendix A.1 above). The only difference is that here \u00b5 is allowed to be any real number in (0, L). This consideration reduces our problem into, yet another, polynomial approximation problem. For completeness, we provide here the full proof.\nLet us apply the given oblivious p-CLI algorithm on a quadratic function of the form\nf : Rd \u2192 R : x 7\u2192 1 2 x \u22a4Qx+ q\u22a4x,\nwhere Q = diag (\u03b7, . . . , \u03b7) and q = \u2212v\u03b7 for some \u03b7 \u2208 (0, L) and v 6= 0 \u2208 Rd. In particular, we have that the norm of the unique minimizer is \u2016x\u2217\u2016 = \u2225 \u2225\u2212Q\u22121q \u2225\n\u2225 = \u2016v\u2016. We set the initialization points to be zero, i.e., Exj = 0, j = 1, . . . , p, and denote the corresponding coefficients by A (k) ij , B (k) ij \u2208 Rd\u00d7d. The crux of proof is that, as long as \u03b7 lies in (0, L], the side-information {\u00b5,L} remains consistent, and therefore, the coefficients remain unchanged.\nFirst, we express Exk+1i in terms of Q,q and Ex (k) 1 , . . . ,Ex (k) p \u2208 Rd. By Definition 3 we have for any\ni \u2208 [p],\nEx k+1 i =\np \u2211\nj=1\n(\nA (k) ij \u2202f +B (k) ij\n)\n(Ex (k) j )\n=\np \u2211\nj=1\n(A (k) ij \u2202f(Ex (k) j ) +B (k) ij Ex (k) j )\n=\np \u2211\nj=1\n(A (k) ij (QEx (k) j + q) +BijEx (k) j )\n=\np \u2211\nj=1\n(A (k) ij Q+B (k) ij )Ex (k) j +\np \u2211\nj=1\nA (k) ij q\nOur next step is to reduce the problem of minimizing f to a polynomial approximation problem. We claim that for any k \u2265 1 and i \u2208 [d] there exist d real polynomials sk,i,1(\u03b7), . . . , sk,i,d(\u03b7) of degree at most k \u2212 1, such that\nEx (k) i = (sk,i,\u2217(\u03b7))\u03b7, (22)\nwhere (sk,i,\u2217(\u03b7)) := (sk,i,1(\u03b7), . . . , sk,i,d(\u03b7)) \u22a4.\nLet us prove this claim using mathematical induction. For k = 1 we have,\nEx (1) i =\np \u2211\nj=1\n(A (0) ij Q+B (0) ij )Ex (0) j +\np \u2211\nj=1\nA (0) ij q = \u2212\np \u2211\nj=1\nA (0) ij v\u03b7, (23)\nshowing that the base case holds. For the induction step, assume the statement holds for some k > 1 with sk,i,j(\u03b7) as above, then\nEx (k+1) i =\np \u2211\nj=1\n(A (k) ij Q+B (k) ij )Ex (k) j +\np \u2211\nj=1\nA (k) ij q\n=\np \u2211\nj=1\n(A (k) ij diag (\u03b7, . . . , \u03b7) +B (k) ij )(sk,j,\u2217(\u03b7))\u03b7\n\u2212 p \u2211\nj=1\nA (k) ij v\u03b7\n=\n\n\np \u2211\nj=1\n(A (k) ij diag (\u03b7, . . . , \u03b7) +B (k) ij )(sk,j,\u2217(\u03b7))\n\u2212 p \u2211\nj=1\nA (k) ij v\n\n \u03b7. (24)\nThe expression inside the last parenthesis is a vector with d entries, each of which contains a real polynomial of degree at most k. This concludes the induction step. (note that the derivations of equalities (23) and (24) above are exactly where we use the fact that there is no functional dependency of A(k)ij and B (k) ij on \u03b7).\nWe are now ready to estimate the sub-optimality of Ex(k)p , the point returned by the algorithm at the k\u2019th iteration. Let us set m \u2208 argmaxj |vj |, then\n\u2225 \u2225 \u2225Ex (k) p \u2212 x\u2217 \u2225 \u2225\n\u2225 = \u2016(sk,p,\u2217(\u03b7))\u03b7 + v\u2016 \u2265 |sk,p,m(\u03b7)\u03b7 + vm| = |vm||sk,p,m(\u03b7)\u03b7/vm + 1| (25)\nBy Lemma 4, there exists \u03b7 \u2208 (L/2, L), such that\n|sk,p,m(\u03b7)\u03b7/vm + 1| \u2265 (1\u2212 \u03b7/L)k+1. (26)\nDefining Q and q accordingly, and choosing, e.g., v = Re1 where R denotes a prescribed distance, yields \u2225\n\u2225 \u2225Ex (k) p \u2212 x\u2217\n\u2225 \u2225 \u2225 \u2265 (1\u2212 \u03b7/L)k+1.\nUsing the fact that f is L/2-strongly convex concludes the proof."}, {"heading": "B Technical Lemmas", "text": "Below, we provide 3 lemmas which are used to bound from below the quantity |s(\u03b7)\u03b7 + 1| over different domains of \u03b7, where s(\u03b7) is a real polynomial. For brevity, we denote the set of real polynomials of degree k by Pk.\nLemma 2. Let s(\u03b7) \u2208 Pk, and let 0 < \u00b5 < L. Then,\nmax \u03b7\u2208[\u00b5,L]\n|s(\u03b7)\u03b7 + 1| \u2265 (\u221a\n\u03ba\u2212 1\u221a \u03ba+ 1\n)k+1\nwhere \u03ba := L/\u00b5.\nProof Denote q(\u03b7) := T\u22121k+1 ( L+\u00b5 L\u2212\u00b5 ) Tk+1 ( 2\u03b7\u2212\u00b5\u2212L L\u2212\u00b5 ) , where Tk(\u03b7) denotes the Chebyshev polynomial of degree k,\nTk(\u03b7) =\n\n \n  cos(k arccos(\u03b7)) |\u03b7| \u2264 1 cosh(k arcosh(\u03b7)) \u03b7 \u2265 1 (\u22121)n cosh(k arcosh(\u2212\u03b7)) \u03b7 \u2264 \u22121.\n(27)\nIt follows that |Tk+1(\u03b7)| \u2264 1 for \u03b7 \u2208 [\u22121, 1] and\nTk+1(cos(j\u03c0/(k + 1))) = (\u22121)j , j = 0, . . . , k + 1.\nAccordingly, |q(\u03b7)| \u2264 T\u22121k+1 ( L+\u00b5 L\u2212\u00b5 ) , \u03b7 \u2208 [\u00b5,L] and\nq (\u03b8j) = (\u22121)jT\u22121k+1 ( L+ \u00b5\nL\u2212 \u00b5\n)\n, j = 0, . . . , k + 1,\nwhere\n\u03b8j = cos(j\u03c0/(k + 1))(L \u2212 \u00b5) + \u00b5+ L\n2 .\nSuppose, for the sake of contradiction, that\nmax \u03b7\u2208[\u00b5,L] |s(\u03b7)\u03b7 + 1| < max \u03b7\u2208[\u00b5,L] |q(\u03b7)|.\nThus, for r(\u03b7) = q(\u03b7)\u2212 (1 + s(\u03b7)\u03b7)), we have r(\u03b8j) > 0 for even j, and r(\u03b8j) < 0 for odd j. Hence, r(\u03b7) has k + 1 roots in [\u00b5,L]. But, since r(0) = 0 and \u00b5 > 0, it follows r(\u03b7) has at least k + 2 roots, which contradicts the fact that the degree of r(\u03b7) is at most k + 1. Therefore,\nmax \u03b7\u2208[\u00b5,L] |s(\u03b7)\u03b7 + 1| \u2265 max \u03b7\u2208[\u00b5,L]\n|q(\u03b7)| = T\u22121k+1 ( \u03ba+ 1\n\u03ba\u2212 1\n)\n,\nwhere \u03ba = L/\u00b5. Since (\u03ba+ 1)/(\u03ba \u2212 1) \u2265 1, we have by Equation (27),\nTk\n(\n\u03ba+ 1 \u03ba\u2212 1\n)\n= cosh\n(\nk arcosh\n(\n\u03ba+ 1 \u03ba\u2212 1\n))\n= cosh\n\nk ln\n\n\n\u03ba+ 1 \u03ba\u2212 1 + \u221a ( \u03ba+ 1 \u03ba\u2212 1 )2 \u2212 1\n\n\n\n\n= cosh\n(\nk ln\n( \u03ba+ 2 \u221a \u03ba+ 1\n\u03ba\u2212 1\n))\n= cosh\n(\nk ln (\u221a \u03ba+ 1\u221a \u03ba\u2212 1 ))\n= 1\n2\n( (\u221a \u03ba+ 1\u221a \u03ba\u2212 1 )k + (\u221a \u03ba\u2212 1\u221a \u03ba+ 1 )k )\n\u2264 (\u221a\n\u03ba+ 1\u221a \u03ba\u2212 1\n)k\n.\nHence,\nmax \u03b7\u2208[\u00b5,L]\n|s(\u03b7)\u03b7 + 1| \u2265 (\u221a\n\u03ba\u2212 1\u221a \u03ba+ 1\n)k+1\nLemma 3. Let s(\u03b7) \u2208 Pk, and let 0 < L. Then,\nmax \u03b7\u2208[0,L] \u03b7|s(\u03b7)\u03b7 + 1|2 \u2265 L (2k + 3)2\nProof First, we define\nq(\u03b7) =\n{\n(\u22121)k(2k + 3)\u22121 \u221a L/\u03b7T2k+3( \u221a \u03b7/L) \u03b7 6= 0 0 \u03b7 = 0\nwhere Tk(\u03b7) is the k\u2019th Chebyshev polynomial (see (27)). Let us show that q(\u03b7) is a polynomial of degree k + 1 and that q(0) = 1. The following trigonometric identity\ncos\u03b1+ cos\u03b2 = 2cos\n( \u03b1\u2212 \u03b2 2 ) cos ( \u03b1+ \u03b2 2 ) ,\ntogether with (27), yields the following recurrence formula\nTk(\u03b7) = 2\u03b7Tk\u22121(\u03b7)\u2212 Tk\u22122(\u03b7).\nNoticing that T0(\u03b7) = 1 and T1(\u03b7) = x (also by (27)), we can use mathematical induction to prove that Chebyshev polynomials of odd degree have only odd powers and that the corresponding coefficient for the\nfirst power \u03b7 in T2k+3(\u03b7) is indeed (\u22121)k(2k+3). Equivalently, we get that q(\u03b7) is a polynomial of degree k + 1 and that q(0) = 1. Next, note that for\n\u03b8j = L cos\n(\nj\u03c0\n2k + 3\n)2\n\u2208 [0, L], j = 0, . . . , k + 1\nwe have\nmax \u03b7\u2208[0,L]\n\u03b71/2|q(\u03b7)| = (\u22121)j\u03b81/2j q(\u03b8j) = \u221a L\n2k + 3 .\nNow, suppose, for the sake of contradiction, that\nmax \u03b7\u2208[0,L] \u03b7|s(\u03b7)\u03b7 + 1|2 < max \u03b7\u2208[0,L] \u03b7|q(\u03b7)|2.\nIn particular,\n\u03b8 1/2 j |s(\u03b8j)\u03b8 1/2 j + 1| < \u03b8 1/2 j |q(\u03b8j)|.\nSince \u03b8j > 0, we have\n|s(\u03b8j)\u03b81/2j + 1| < |q(\u03b8j)|.\nWe proceed in a similar way to the proof of Lemma 2. For r(\u03b7) = q(\u03b7)\u2212 (1 + s(\u03b7)\u03b7)), we have r(\u03b8j) > 0 for even j, and r(\u03b8j) < 0 for odd j. Hence, r(\u03b7) has k + 1 roots in [\u03b8k+1, L]. But, since r(0) = 0 and \u03b8k+1 > 0, it follows r(\u03b7) has at least k + 2 roots, which contradicts the fact that degree of r(\u03b7) is a at most k + 1. Therefore,\nmax \u03b7\u2208[0,L] \u03b7|s(\u03b7)\u03b7 + 1|2 \u2265 max \u03b7\u2208[0,L] \u03b7|q(\u03b7)|2 \u2265 L (2k + 3)2\nconcluding the proof.\nLemma 4. Let s(\u03b7) \u2208 Pk, and let 0 < L. Then exactly one of the two following holds:\n1. For any \u01eb > 0, there exists \u03b7 \u2208 (L\u2212 \u01eb, L) such that\n|s(\u03b7)\u03b7 + 1| > (1\u2212 \u03b7/L)k+1.\n2. s(\u03b7)\u03b7 + 1 = (1\u2212 \u03b7/L)k+1.\nProof It suffices to show that if (1) does not hold then s(\u03b7)\u03b7+1 = (1\u2212 \u03b7/L)k+1. Suppose that there exists \u01eb > 0 such that for all \u03b7 \u2208 (L\u2212 \u01eb, L) it holds that\n|s(\u03b7)\u03b7 + 1| \u2264 ( 1\u2212 \u03b7 L )k+1 .\nDefine\nq(\u03b7) := s (L(1\u2212 \u03b7))L(1\u2212 \u03b7) + 1 (28)\nand denote the corresponding coefficients by q(\u03b7) = \u2211k+1 j=0 qi\u03b7 j . We show by induction that qj = 0 for all j = 0, . . . , k. For j = 0 we have that since for any \u03b7 \u2208 (0, 1 \u2212 (L\u2212 \u01eb)/L)\n|q(\u03b7)| \u2264 ( 1\u2212 L(1\u2212 \u03b7) L\n)k+1\n= \u03b7k+1,\nit holds that\n|q0| = |q(0)| = \u2223 \u2223 \u2223\n\u2223 lim \u03b7\u21920+ q(\u03b7)\n\u2223 \u2223 \u2223 \u2223\n\u2264 lim \u03b7\u21920+ \u03b7k+1 = 0.\nNow, if q0 = \u00b7 \u00b7 \u00b7 = qm\u22121 = 0 for m < k + 1 then\n|qm| = \u2223 \u2223 \u2223\n\u2223\nq(0)\n\u03b7m\n\u2223 \u2223 \u2223 \u2223 = \u2223 \u2223 \u2223 \u2223\nlim \u03b7\u21920+\nq(\u03b7)\n\u03b7m\n\u2223 \u2223 \u2223 \u2223\n\u2264 lim t\u21920+ \u03b7k+1\u2212m = 0.\nThus, proving the induction claim. This, in turns, implies that q(\u03b7) = qk+1\u03b7k+1. Now, by Equation (28), it follows that qk+1 = q(1) = 1. Hence, q(\u03b7) = \u03b7k+1. Lastly, using Equation (28) again yields\ns(\u03b7)\u03b7 + 1 = q ( 1\u2212 \u03b7 L ) = ( 1\u2212 \u03b7 L )k+1 ,\nconcluding the proof."}], "references": [{"title": "On lower and upper bounds for smooth and strongly convex optimization problems", "author": ["Yossi Arjevani", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1503.06833,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Algorithms and theory of computation handbook", "author": ["Mikhail J Atallah"], "venue": "CRC press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Contributions to the Complexity Analysis of Optimization Algorithms", "author": ["Yoel Drori"], "venue": "PhD thesis, Tel-Aviv University,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "From averaging to acceleration, there is only a step-size", "author": ["Nicolas Flammarion", "Francis Bach"], "venue": "arXiv preprint arXiv:1504.01577,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford"], "venue": "arXiv preprint arXiv:1506.07512,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Analysis and design of optimization algorithms via integral quadratic constraints", "author": ["Laurent Lessard", "Benjamin Recht", "Andrew Packard"], "venue": "arXiv preprint arXiv:1408.3595,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "On an algorithm for the minimization of convex functions", "author": ["A Yu Levin"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1965}, {"title": "A universal catalyst for first-order optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Problem complexity and method efficiency in optimization", "author": ["AS Nemirovsky", "DB Yudin"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1983}, {"title": "A method of solving a convex programming problem with convergence rate O (1/k2)", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1983}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Yurii Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Location of the maximum on unimodal surfaces", "author": ["Donald J Newman"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1965}, {"title": "Introduction to optimization", "author": ["Boris T Polyak"], "venue": "Optimization Software New York,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1987}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Sdca without duality", "author": ["Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1502.06177,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "To attain these, we abandon the oracle model in favor of a structure-based approach which builds upon a framework recently proposed in [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 10, "context": "In their seminal work, Nemirovsky and Yudin [11] showed that for any first-order optimization algorithm, there exists an L-smooth and \u03bc-strongly convex function f : Rd \u2192 R such that the number of queries required to obtain an \u01eb-optimal solution x\u0303 which satisfies f(x\u0303) < min x\u2208Rd f(x) + \u01eb, is at least1 \u03a9\u0303 (", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Concretely, it is achieved by a combination of Nesterov\u2019s well-known accelerated gradient descent (AGD, [12]) with an iteration complexity of \u00d5 (\u221a \u03ba ln(1/\u01eb) )", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": ", \u03bc = 0, and the center of gravity method (MCG, [9, 14]) whose iteration complexity is O(d ln(1/\u01eb)).", "startOffset": 48, "endOffset": 55}, {"referenceID": 13, "context": ", \u03bc = 0, and the center of gravity method (MCG, [9, 14]) whose iteration complexity is O(d ln(1/\u01eb)).", "startOffset": 48, "endOffset": 55}, {"referenceID": 0, "context": "To overcome this issue [1] recently proposed the framework of p-Stationary Canonical Linear Iterative (p-SCLI) in which, instead of modeling the way algorithms acquire information on the function at hand, one assumes certain dynamics which restricts the way new iterates are being generated.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "Nevertheless, [1] showed that this bound is actually tight for all p.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "\u2022 The formulation suggested in [1] does not allow generating more than one iterate at a time.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "\u2022 Lastly, whereas the proofs in [1] are elaborate and technically complex, the proofs we provide here are relatively short and simple.", "startOffset": 32, "endOffset": 35}, {"referenceID": 12, "context": "Another important example is a stationary variant of AGD [13] and the heavy-ball method (e.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": ", [16]) which generates iterates according to", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", Stochastic Average Gradient (SAG, [17]), Stochastic Variance Reduction Gradient (SVRG, [7]), Stochastic Dual Coordinate Ascent (SDCA, [19]), Stochastic Dual Coordinate Ascent without Duality (SDCA without duality, [18]) and SAGA [3], to name a few, and as such, are subject to the same lower bounds established through this framework.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": ", Stochastic Average Gradient (SAG, [17]), Stochastic Variance Reduction Gradient (SVRG, [7]), Stochastic Dual Coordinate Ascent (SDCA, [19]), Stochastic Dual Coordinate Ascent without Duality (SDCA without duality, [18]) and SAGA [3], to name a few, and as such, are subject to the same lower bounds established through this framework.", "startOffset": 89, "endOffset": 92}, {"referenceID": 17, "context": ", Stochastic Average Gradient (SAG, [17]), Stochastic Variance Reduction Gradient (SVRG, [7]), Stochastic Dual Coordinate Ascent (SDCA, [19]), Stochastic Dual Coordinate Ascent without Duality (SDCA without duality, [18]) and SAGA [3], to name a few, and as such, are subject to the same lower bounds established through this framework.", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": ", Stochastic Average Gradient (SAG, [17]), Stochastic Variance Reduction Gradient (SVRG, [7]), Stochastic Dual Coordinate Ascent (SDCA, [19]), Stochastic Dual Coordinate Ascent without Duality (SDCA without duality, [18]) and SAGA [3], to name a few, and as such, are subject to the same lower bounds established through this framework.", "startOffset": 216, "endOffset": 220}, {"referenceID": 2, "context": ", Stochastic Average Gradient (SAG, [17]), Stochastic Variance Reduction Gradient (SVRG, [7]), Stochastic Dual Coordinate Ascent (SDCA, [19]), Stochastic Dual Coordinate Ascent without Duality (SDCA without duality, [18]) and SAGA [3], to name a few, and as such, are subject to the same lower bounds established through this framework.", "startOffset": 231, "endOffset": 234}, {"referenceID": 14, "context": "Finally, we remark that this approach of modeling the structure of optimization algorithms, as opposed to the more traditional oracle model, can be also found in [16, 8, 5, 4].", "startOffset": 162, "endOffset": 175}, {"referenceID": 7, "context": "Finally, we remark that this approach of modeling the structure of optimization algorithms, as opposed to the more traditional oracle model, can be also found in [16, 8, 5, 4].", "startOffset": 162, "endOffset": 175}, {"referenceID": 4, "context": "Finally, we remark that this approach of modeling the structure of optimization algorithms, as opposed to the more traditional oracle model, can be also found in [16, 8, 5, 4].", "startOffset": 162, "endOffset": 175}, {"referenceID": 3, "context": "Finally, we remark that this approach of modeling the structure of optimization algorithms, as opposed to the more traditional oracle model, can be also found in [16, 8, 5, 4].", "startOffset": 162, "endOffset": 175}, {"referenceID": 0, "context": "1, we propose a novel framework which substantially generalizes the framework introduced in [1], and includes a large part of modern first-order optimization algorithms.", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "We improve upon [1] by establishing lower bounds which hold both for smooth functions and smooth and strongly convex functions, using simpler and shorter proofs.", "startOffset": 16, "endOffset": 19}, {"referenceID": 15, "context": "See Section 3 in [17] and Section 5 in [3]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "See Section 3 in [17] and Section 5 in [3]).", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Acceleration schemes, such as [6, 10], are able to break this bound by re-scheduling these algorithms in a non-stationary (though oblivious) way.", "startOffset": 30, "endOffset": 37}, {"referenceID": 9, "context": "Acceleration schemes, such as [6, 10], are able to break this bound by re-scheduling these algorithms in a non-stationary (though oblivious) way.", "startOffset": 30, "endOffset": 37}, {"referenceID": 12, "context": "Notable stationary p-CLIs are: GD with fixed step size [13], stationary AGD [13] and the Heavy-Ball method [16].", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "Notable stationary p-CLIs are: GD with fixed step size [13], stationary AGD [13] and the Heavy-Ball method [16].", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "Notable stationary p-CLIs are: GD with fixed step size [13], stationary AGD [13] and the Heavy-Ball method [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "Notable algorithms here are GD and AGD with step sizes which are scheduled irrespectively of the function under consideration [13] and the Sub-Gradient Descent method (e.", "startOffset": 126, "endOffset": 130}, {"referenceID": 1, "context": ", [2]), will not be further considered in this paper.", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "L/\u01eb), \u03bc = 0, where \u03ba := L/\u03bc As discussed in the introduction, Theorem 1 significantly improves upon the lower obtained by [1] in 3 major aspects: \u2022 It holds for both smooth functions, as well as smooth and strongly convex functions.", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "See Theorem 8, [1]).", "startOffset": 15, "endOffset": 18}, {"referenceID": 10, "context": "The idea of reducing optimization bounds to polynomial approximation problems is not new, and is also found for instance in [11], where lower bounds under the oracle model are derived.", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "It is instructive to contrast our approach with another structural approach for deriving lower bounds which was proposed by [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "Nesterov [13] considerably simplifies the technique employed by Nemirovsky and Yudin [11] at the cost of introducing additional assumption regarding the way new iterates are generated.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "Nesterov [13] considerably simplifies the technique employed by Nemirovsky and Yudin [11] at the cost of introducing additional assumption regarding the way new iterates are generated.", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "Similarly to [11], this approach also does not yield dimension-independent lower bounds.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "[1], it is shown that if Q is given in advance, but q is unknown, then the iteration complexity of stationary p-CLIs which follows (4) is \u03a9\u0303( p \u221a \u03ba ln(1/\u01eb)).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "It is further shown that this lower bound is tight (see Appendix A in [1]).", "startOffset": 70, "endOffset": 73}, {"referenceID": 15, "context": "Concretely, it implies that, in the absence of the strong convexity parameter, one is still able to schedule the step sizes according to the smoothness parameter so as to obtain exponential convergence rate, but only to the limited extent of linear dependency on the condition number (as mentioned before, this sub-optimality in terms of dependence on the condition number, can be also found in [17] and [3]).", "startOffset": 395, "endOffset": 399}, {"referenceID": 2, "context": "Concretely, it implies that, in the absence of the strong convexity parameter, one is still able to schedule the step sizes according to the smoothness parameter so as to obtain exponential convergence rate, but only to the limited extent of linear dependency on the condition number (as mentioned before, this sub-optimality in terms of dependence on the condition number, can be also found in [17] and [3]).", "startOffset": 404, "endOffset": 407}, {"referenceID": 5, "context": ", [6, 10]) which turn a given stationary algorithm into an non-stationary oblivious one.", "startOffset": 2, "endOffset": 9}, {"referenceID": 9, "context": ", [6, 10]) which turn a given stationary algorithm into an non-stationary oblivious one.", "startOffset": 2, "endOffset": 9}, {"referenceID": 11, "context": "In his seminal paper, Nesterov [12] presents the AGD algorithm and shows that it obtains a convergence rate of f(x)\u2212 f(x) \u2264 4L \u2225", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "References [1] Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Mikhail J Atallah.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Yoel Drori.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Nicolas Flammarion and Francis Bach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Roy Frostig, Rong Ge, Sham M Kakade, and Aaron Sidford.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Rie Johnson and Tong Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Laurent Lessard, Benjamin Recht, and Andrew Packard.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] A Yu Levin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] AS Nemirovsky and DB Yudin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Yurii Nesterov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Yurii Nesterov.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Donald J Newman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Boris T Polyak.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Mark Schmidt, Nicolas Le Roux, and Francis Bach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We consider a broad class of first-order optimization algorithms which are oblivious, in the sense that their step sizes are scheduled regardless of the function under consideration, except for limited sideinformation such as smoothness or strong convexity parameters. With the knowledge of these two parameters, we show that any such algorithm attains an iteration complexity lower bound of \u03a9( \u221a L/\u01eb) for L-smooth convex functions, and \u03a9\u0303( \u221a L/\u03bc ln(1/\u01eb)) for L-smooth \u03bc-strongly convex functions. These lower bounds are stronger than those in the traditional oracle model, as they hold independently of the dimension. To attain these, we abandon the oracle model in favor of a structure-based approach which builds upon a framework recently proposed in [1]. We further show that without knowing the strong convexity parameter, it is impossible to attain an iteration complexity better than \u03a9\u0303 ((L/\u03bc) ln(1/\u01eb)). This result is then used to formalize an observation regarding L-smooth convex functions, namely, that the iteration complexity of algorithms employing time-invariant step sizes must be at least \u03a9(L/\u01eb).", "creator": "LaTeX with hyperref package"}}}