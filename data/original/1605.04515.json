{"id": "1605.04515", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2016", "title": "Machine Translation Evaluation: A Survey", "abstract": "This paper introduces the state-of-the-art MT evaluation survey that contains both manual and automatic evaluation methods. The traditional human evaluation criteria mainly include the intelligibility, fidelity, fluency, adequacy, comprehension, and informativeness. We classify the automatic evaluation methods into two categories, including lexical similarity and linguistic features application. The lexical similarity methods contain edit distance, precision, recall, and word order, etc. The linguistic features can be divided into syntactic features and semantic features. Subsequently, we also introduce the evaluation methods for MT evaluation and the recent quality estimation tasks for MT.", "histories": [["v1", "Sun, 15 May 2016 09:41:00 GMT  (565kb)", "https://arxiv.org/abs/1605.04515v1", "14 pages, 21 formula"], ["v2", "Wed, 18 May 2016 18:38:02 GMT  (567kb)", "http://arxiv.org/abs/1605.04515v2", "14 pages, 21 formula"], ["v3", "Thu, 19 May 2016 16:12:34 GMT  (570kb)", "http://arxiv.org/abs/1605.04515v3", "some revision of the mathematical symbols and recent literature work in the content, and edit of references"], ["v4", "Mon, 23 May 2016 15:48:19 GMT  (643kb)", "http://arxiv.org/abs/1605.04515v4", "We added some further revision of the content, with introducation of previous MT evaluation related survey works from some european MT and language technology projects; fixed some mising references in paraphrase section"], ["v5", "Wed, 25 May 2016 10:30:16 GMT  (649kb)", "http://arxiv.org/abs/1605.04515v5", "We gave more intro information in the abstract about the content structure, the content layout, to make it easier for researchers understand the paper in the first moment. and we add some literature about deep learning for MT and evaluation"], ["v6", "Sun, 19 Jun 2016 12:28:58 GMT  (656kb)", "http://arxiv.org/abs/1605.04515v6", "We gave more intro information in the abstract about the content structure, the content layout, to make it easier for researchers understand the paper in the first moment. and we add some literature about deep learning for MT and evaluation, some literature about task-based MT evaluation"], ["v7", "Tue, 10 Oct 2017 14:04:07 GMT  (1610kb,D)", "http://arxiv.org/abs/1605.04515v7", "We add two presentation figures about Human MT Evaluation, and Automatic MT Evaluation, to make it clear for readers to find the whole structure of the paper. Add some advanced works of MT Evaluation, e.g. Neural models"]], "COMMENTS": "14 pages, 21 formula", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aaron li-feng han", "derek fai wong"], "accepted": false, "id": "1605.04515"}, "pdf": {"name": "1605.04515.pdf", "metadata": {"source": "CRF", "title": "Machine Translation Evaluation Resources and Methods: A Survey", "authors": ["Lifeng Han", "Derek F. Wong", "Lidia S. Chao"], "emails": ["lifeng.han@adaptcentre.ie", "derekfw@umac.mo", "lidiasc@umac.mo"], "sections": [{"heading": null, "text": "This paper differs from the existing works (Dorr et al., 2009; EuroMatrix, 2007) from several aspects, by introducing some recent development of MT evaluation measures, the different classifications from manual to automatic evaluation measures, the introduction of recent QE tasks of MT, and the concise construction of the content.\nWe hope this work will be helpful for MT\nresearchers to easily pick up some metrics that are best suitable for their specific MT model development, and help MT evaluation researchers to get a general clue of how MT evaluation research developed. Furthermore, hopefully, this work can also shine some light on other evaluation tasks, except for translation, of NLP fields. 1"}, {"heading": "1 Introduction", "text": "Machine translation (MT) began as early as in the 1950s (Weaver, 1955) , and gained a rapid development since the 1990s (Marin\u0303o et al., 2006) due to the development of storage and computing power of computer and the widely available multilingual and bilingual corpora. There are many important works in MT areas, for some to mention by time, IBM Watson research group (Brown et al., 1993) designed five statistical MT models and the ways of how to estimate the parameters in the models given the bilingual translation corpora; (Koehn et al., 2003) proposed statistical phrase-based MT model; Och (Och, 2003) presented Minimum Error Rate Training (MERT) for log-linear statistical machine translation models; (Koehn and Monz, 2005) introduced a Shared task of building statistical machine translation (SMT) systems for four European language pairs; (Chiang, 2005) proposed a hierarchical phrase-based SMT model that is learned from a bitext without syntactic information; (Menezes et al., 2006) introduced a syntactically informed phrasal SMT system for English-to-Spanish translation using a phrase translation model, which was based on global reordering and dependency tree; (Koehn et al., 2007b) developed an open source SMT software toolkit Moses; (Hwang et al., 2007) utilized the shallow linguistic knowledge to improve word alignment and language model quality be-\n1Some work was done in NLP2CT-lab/Macau and SLPLlab/ILLC/UvA/Amsterdam\nar X\niv :1\n60 5.\n04 51\n5v 7\n[ cs\n.C L\n] 1\n0 O\nct 2\n01 7\ntween linguistically different languages; (Fraser and Marcu, 2007) made a discussion of the relationship between word alignment and the quality of machine translation; (Sa\u0301nchez-Mart\u0301inez and Forcada, 2009) described an unsupervised method for the automatic inference of structural transfer rules for a shallow-transfer machine translation system; (Khalilov and Fonollosa, 2011) designed an effective syntax-based reordering approach to address the word ordering problem.\nWith the fast development of Deep Learning (DL), MT research has evolved from rule-based models to example based models, statistical models, hybrid models, and recent years\u2019 Neural models (Nirenburg, 1989; Carl and Way, 2003; Koehn and Knight, 2009; Bahdanau et al., 2014), such as the attention mechanism models, coverage models, multi-modal and multilingual MT models.\nNeural MT (NMT) is a recently active topic that conduct the automatic translation workflow very differently with the traditional phrase-based SMT methods. Instead of training the different MT components separately, NMT model utilizes the artificial neural network (ANN) to learn the model jointly to maximize the translation performance through two steps recurrent neural network (RNN) of encoder and decoder (Cho et al., 2014; Bahdanau et al., 2014; Wolk and Marasek, 2015). There were far more representative MT works that we havent listed here.\nDue to the wide-spread development of MT systems, the MT evaluation became more and more important to tell us how well the MT systems perform and whether they make some progress. However, the MT evaluation is difficult because the natural languages are highly ambiguous and different languages do not always express the same content in the same way (Arnold, 2003).\nThere are several events that promote the development of MT evaluation research. One of them was the NIST open machine translation Evaluation series (OpenMT), which were very prestigious evaluation campaigns from 2001 to 2009 (LI, 2005).\nThe innovation of MT and the evaluation methods is also promoted by the annual Workshop on Statistical Machine Translation (WMT) (Koehn and Monz, 2006a; Callison-Burch et al., 2007a; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011; Callison-Burch et al., 2012; Bojar et\nal., 2013; Bojar et al., 2014; Bojar et al., 2015) organized by the special interest group in machine translation (SIGMT) since 2006. The evaluation campaigns focus on European languages. There are roughly two tracks in the annual WMT workshop including the translation task and evaluation task. The tested language pairs are clearly divided into two directions, i.e., English-to-other and other-to-English, covering French, German, Spanish, Czech, Hungarian, Haitian Creole and Russian.\nAnother promotion is the international workshop of spoken language translation (IWSLT) that has been organized annually from 2004 (Eck and Hori, 2005; Paul, 2009; Paul et al., 2010; Federico et al., 2011). This campaign has a stronger focus on speech translation including the English and Asian languages, e.g. Chinese, Japanese and Korean.\nThe better evaluation metrics will be surly helpful to the development of better MT systems (Liu et al., 2011). Due to all the above efforts, the MT evaluation research achieved a rapid development.\nThis paper is constructed as follow: Section 2 and 3 discuss the human assessment methods and automatic evaluation methods respectively, Section 4 introduces the evaluating methods of the MT evaluation, Section 5 is the advanced MT evaluation, Section 6 is the discussion and conclusion, and the perspective is presented in Section 7."}, {"heading": "2 Human Evaluation Methods", "text": "This section introduces the traditional human evaluation methods and the advanced methods as shown in Fig. 1."}, {"heading": "2.1 Traditional Human Assessment", "text": "2.1.1 Intelligibility and Fidelity The earliest human assessment methods for MT can be traced back to around 1966. They include the intelligibility and fidelity used by the automatic language processing advisory committee (ALPAC) (Carroll, 1966). The requirement that a translation be intelligible means that, as far as possible, the translation should read like normal, well-edited prose and be readily understandable in the same way that such a sentence would be understandable if originally composed in the translation language. The requirement that a translation is of high fidelity or accuracy includes that the translation should, as little as possible, twist, distort, or\ncontrovert the meaning intended by the original.\n2.1.2 Fluency, Adequacy and Comprehension In1990s, the Advanced Research Projects Agency (ARPA) created the methodology to evaluate machine translation systems using the adequacy, fluency and comprehension (Church and Hovy, 1991) in MT evaluation campaigns (White et al., 1994).\nComprehension = #Cottect\n6 , (1)\nFluency = Judgment point\u22121\nS\u22121 #Sentences in passage , (2)\nAdequacy = Judgment point\u22121\nS\u22121 #Fragments in passage . (3)\nThe evaluator is asked to look at each fragment, delimited by syntactic constituent and containing sufficient information, and judge the adequacy on a scale 1-to-5. The results are computed by averaging the judgments over all of the decisions in the translation set.\nThe fluency evaluation is compiled with the same manner as that for the adequacy except for that the evaluator is to make intuitive judgments on a sentence by sentence basis for each translation. The evaluators are asked to determine whether the translation is good English without reference to the correct translation. The fluency evaluation is to determine whether the sentence is well-formed and fluent in context.\nThe modified comprehension develops into the \u201cInformativeness\u201d, whose objective is to measure\na system\u2019s ability to produce a translation that conveys sufficient information, such that people can gain necessary information from it. Developed from the reference set of expert translations, six questions have six possible answers respectively including, \u201cnone of above\u201d and \u201ccannot be determined\u201d.\n2.1.3 Further Development (Bangalore et al., 2000) conduct a research developing accuracy into several kinds including simple string accuracy, generation string accuracy, and two corresponding tree-based accuracies.Reeder (2004) shows the correlation between fluency and the number of words it takes to distinguish between human translation and machine translation.\nThe \u201cLinguistics Data Consortium\u201d (LDC) develops two five-points scales representing fluency and adequacy for the annual NIST machine translation evaluation workshop. The developed scales become the widely used methodology when manually evaluating MT is to assign values. The five point scale for adequacy indicates how much of the meaning expressed in the reference translation is also expressed in a hypothesis translation; the second five point scale indicates how fluent the translation is, involving both grammatical correctness and idiomatic word choices.\n(Specia et al., 2011) conduct a study of the MT adequacy and design it into four levels, from score 4 to score 1: highly adequate, the translation faithfully conveys the content of the input sentence; fairly adequate, while the translation generally conveys the meaning of the input sentence, there are some problems with word order or tense/voice/number, or there are repeated, added or un-translated words; poorly adequate, the content of the input sentence is not adequately conveyed by the translation; and completely inadequate, the content of the input sentence is not conveyed at all by the translation.\n2.2 Advanced Human Assessment 2.2.1 Task-oriented (White and Taylor, 1998) develop a task-oriented evaluation methodology for Japanese-to-English translation to measure MT systems in light of the tasks for which their output might be used. They seek to associate the diagnostic scores assigned to the output used in the DARPA evaluation with a scale of language-dependent tasks, such as scanning, sorting, and topic identification. They de-\nvelop the MT proficiency metric with a corpus of multiple variants which are usable as a set of controlled samples for user judgments.The principal steps include identifying the user-performed text-handling tasks, discovering the order of texthandling task tolerance, analyzing the linguistic and non-linguistic translation problems in the corpus used in determining task tolerance, and developing a set of source language patterns which correspond to diagnostic target phenomena. A brief introduction of task-based MT evaluation work was shown in their later work (Doyon et al., 1999).\nVoss and Tate (Voss and Tate, 2006) introduced the tasked-based MT output evaluation by the extraction of who, when, where types elements. They extend the work later into event understanding in (Laoudi et al., 2006).\n2.2.2 Extended Criteria (King et al., 2003) extend a large range of manual evaluation methods for MT systems, which, in addition to the early talked accuracy, include suitability, whether even accurate results are suitable in the particular context in which the system is to be used; interoperability, whether with other software or with hardware platforms; reliability, i.e., don\u2019t break down all the time or take long time to get running again after breaking down; usability, easy to get the interfaces, easy to learn and operate, and looks pretty; efficiency, when needed, keep up with the flow of dealt documents; maintainability, being able to modify the system in order to adapt it to particular users; and portability, one version of a system can be replaced by a new version, because MT systems are rarely static and they tend to be improved over time as resources grow and bugs are fixed.\n2.2.3 Utilizing Post-editing A measure of quality is to compare translation from scratch and post-edited result of an automatic translation. This type of evaluation is however time consuming and depends on the skills of the translator and post-editor. One example of a metric that is designed in such a manner is the human translation error rate (HTER) (Snover et al., 2006), based on the number of editing steps, computing the editing steps between an automatic translation and a reference translation. Here, a human annotator has to find the minimum number of insertions, deletions, substitutions, and shifts to convert the system output into an acceptable translation.\nHTER is defined as the number of editing steps divided by the number of words in the acceptable translation.\n2.2.4 Segment Ranking In the WMT metrics task, the human assessment based on segment ranking is usually employed. Judgesare frequently asked to provide a complete ranking over all the candidate translations of the same source segment (Callison-Burch et al., 2011; Callison-Burch et al., 2012). In the recent WMT tasks (Bojar et al., 2013),five systems are randomly selected for the judges to rank. Each time, the source segment and the reference translation are presented to the judges together with the candidate translations of five systems. The judges will rank the systems from 1 to 5, allowing tie scores. For each ranking, there is the potential to provide as many as 10 pairwise results if no ties. The collected pairwise rankings can be used to assign a score to each participated system to reflect the quality of the automatic translations.The assigned score can also be utilized to reflect how frequently a system is judged to be better or worse than other systems when they are compared on the same source segment, according to the following formula:\n#better pairwise ranking #total pairwise comparison \u2212#ties comparisons . (4)"}, {"heading": "3 Automatic Evaluation Metric", "text": "Manual evaluation suffers some disadvantages such as time-consuming, expensive, not tunable, and not reproducible. Due to the weaknesses in human judgments, automatic evaluation metrics have been widely used for machine translation. Typically, they compare the output of machine translation systems against human translations, but there are also some metrics that do not use the reference translation. There are usually two ways to offer the human reference translation, either offering one single reference or offering multiple references for a single source sentence (Lin and Och, 2004; Han et al., 2012). Common metrics measure the overlap in words and word sequences, as well as word order and edit distance. We classify this kind of metrics as the \u201cLexical Similarity\u201d category. Further developed metrics also take linguistic features into account such as\nsyntax and semantics, e.g. POS, sentence structure, textual entailment, paraphrase, synonyms, named entities, semantic roles and language models, etc. We classify these metrics that utilize the linguistic features into \u201cLinguistic Features\u201d category. It is not easy to separate these two categories clearly since sometimes they merge with each other, for instance, some metrics from category one also use certain linguistic features. Further more, we will introduce some advanced researches that apply deep learning into MTE framework, as in Fig. 2.\n3.1 Lexical Similarity 3.1.1 Edit Distance By calculating the minimum number of editing steps to transform output to reference, (Su et al., 1992) introduce the word error rate (WER) metric into MT evaluation. This metric takes word order into account, and the operations include insertion (adding word), deletion (dropping word) and replacement (or substitution, replace one word with another), the minimum number of editing steps needed to match two sequences.\nWER = substitution+insertion+deletion\nreferencelength . (5)\nOne of the weak points of the WER is the fact that word ordering is not taken into account appropriately. The WER scores very low when the word order of system output translation is \u201cwrong\u201d according to the reference. In the Levenshtein distance, the mismatches in word order require the deletion and re-insertion of the misplaced words.\nHowever, due to the diversity of language expression, some so-called \u201cwrong\u201d order sentences by WER also prove to be good translations. To address this problem, the position-independent word error rate (PER) (Tillmann et al., 1997) is designed to ignore word order when matching output and reference. Without taking into account of the word order, PER counts the number of times that identical words appear in both sentences. Depending on whether the translated sentence is longer or shorter than the reference translation, the rest of the words are either insertion or deletion ones.\nPER = 1\u2212 correc \u2212max(0, outputlength \u2212 referencelength)\nreferencelength .\n(6)\nAnother way to overcome the unconscionable penalty on word order in the Levenshtein distance is adding a novel editing step that allows the movement of word sequences from one part of the output to another. This is something a human posteditor would do with the cut-and-paste function of a word processor. In this light, (Snover et al., 2006) design the translation edit rate (TER) metric that adds block movement (jumping action) as an editing step.The shift option performs on a contiguous sequence of words within the output sentence. The TER score is calculated as:\nTER = #of edit\n#of average reference words (7)\nFor the edits, the cost of the block movement, any number of continuous words and any distance, is equal to that of the single word operation, such as insertion, deletion and substitution.\n3.1.2 Precision and Recall The widely used evaluation metric BLEU (Papineni et al., 2002) is based on the degree of ngram overlapping between the strings of words produced by the machine and the human translation references at the corpus level. BLEU computes the precision for n-gram of size 1-to-4 with the coefficient of brevity penalty (BP).\nBLEU = BP\u00d7 exp N\u2211 n=1 \u03bbn log Precisionn, (8)\nBP = { 1 if c > r, e1\u2212 r c if c <= r.\n(9)\nwhere c is the total length of candidate translation corpus, and r refers to the sum of effective reference sentence length in the corpus. If there are multi-references for each candidate sentence, then the nearest length as compared to the candidate sentence is selected as the effective one. In the BLEU metric, the n-gram precision weight \u03bbn is usually selected as uniform weight. However, the 4-gram precision value is usually very low or even zero when the test corpus is small. To weight more heavily those n-grams that are more informative, (Doddington, 2002) proposes the NIST metric with the information weight added.\nInfo = log2 (#occurrence of w1, \u00b7 \u00b7 \u00b7 , wn\u22121 #occurrence of w1, \u00b7 \u00b7 \u00b7 , wn ) (10)\nFurthermore, he replaces the geometric mean of co-occurrences with the arithmetic average of n-gram counts, extend the n-gram into 5-gram (N = 5), and select the average length of reference translations instead of the nearest length.\nROUGE (Lin and Hovy, 2003) is a recalloriented automated evaluation metric, which is initially developed for summaries. Following the adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, Lin conducts a study of a similar idea for evaluating summaries. They also apply the ROUGE into automatic machine translation evaluation work (Lin and Och, 2004).\n(Turian et al., 2006) conducted experiments to examine how standard measures such as precision and recall and F-measure can be applied for evaluation of MT and showed the comparisons of these standard measures with some existing alternative evaluation measures. F-measure is the combination of precision (P ) and recall (R), which is firstly employed in the information retrieval and latterly has been adopted by the information extraction, MT evaluation and other tasks.\nF\u03b2 = (1 + \u03b2 2)\nPR\nR+ \u03b22P (11)\n(Banerjee and Lavie, 2005) design a novel evaluation metric METEOR. METEOR is based on general concept of flexible unigram matching, unigram precision and unigram recall, including the match of words that are simple morphological variants of each other by the identical stem and\nwords that are synonyms of each other. To measure how well-ordered the matched words in the candidate translation are in relation to the human reference, METEOR introduces a penalty coefficient by employing the number of matched chunks.\nPenalty = 0.5\u00d7 ( #chunks #matched unigrams )3,\n(12)\nMEREOR = 10PR\nR+ 9P \u00d7 (1\u2212 Penalty). (13)\n3.1.3 Word Order The right word order places an important role to ensure a high quality translation output. However, the language diversity also allows different appearances or structures of the sentence. How to successfully achieve the penalty on really wrong word order (wrongly structured sentence) instead of on the \u201ccorrectly\u201d different order, the candidate sentence that has different word order with the reference is well structured, attracts a lot of interests from researchers in the NLP literature. In fact, the Levenshtein distance and n-gram based measures also contain the word order information.\nFeaturing the explicit assessment of word order and word choice, (Wong and yu Kit, 2009) develop the evaluation metric ATEC, assessment of text essential characteristics. It is also based on precision and recall criteria but with the designed position difference penalty coefficient attached. The word choice is assessed by matching word forms at various linguistic levels, including surface form, stem, sound and sense, and further by weighing the informativeness of each word. Combining the precision, order, and recall information together, (Chen et al., 2012) develop an automatic evaluation metric PORT that is initially for the tuning of the MT systems to output higher quality translation. Another evaluation metric LEPOR (Han et al., 2012; Han et al., 2014) is proposed as the combination of many evaluation factors including n-gram based word order penalty in addition to precision, recall, and sentence-length penalty. The LEPOR metric yields the excellent performance on the Englishto-other (Spanish, German, French, Czech and Russian) language pairs in ACL-WMT13 metrics shared tasks at system level evaluation (Han et al., 2013b)."}, {"heading": "3.2 Linguistic Features", "text": "Although some of the previous mentioned metrics employ the linguistic information into consideration, e.g. the semantic information synonyms and stemming in METEOR, the lexical similarity methods mainly focus on the exact matches of the surface words in the output translation. The advantages of the metrics based on lexical similarity are that they perform well in capturing the translation fluency (Lo et al., 2012), and they are very fast and low cost. On the other hand, there are also some weaknesses, for instance, the syntactic information is rarely considered and the underlying assumption that a good translation is one that shares the same lexical choices as the reference translations is not justified semantically. Lexical similarity does not adequately reflect similarity in meaning. Translation evaluation metric that reflects meaning similarity needs to be based on similarity of semantic structure not merely flat lexical similarity.\n3.2.1 Syntactic Similarity\nSyntactic similarity methods usually employ the features of morphological part-of-speech information, phrase categories or sentence structure generated by the linguistic tools such as language parser or chunker.\nIn grammar, a part of speech (POS) is a linguistic category of words or lexical items, which is generally defined by the syntactic or morphological behavior of the lexical item. Common linguistic categories of lexical items include noun, verb, adjective, adverb, and preposition, etc. To reflect the syntactic quality of automatically translated sentences, some researchers employ the POS information into their evaluation. Using the IBM model one, (Popovic\u0301 et al., 2011) evaluate the translation quality by calculating the similarity scores of source and target (translated) sentence without using reference translation, based on the morphemes, 4-gram POS and lexicon probabilities. (Dahlmeier et al., 2011) develop the evaluation metrics TESLA, combining the synonyms of bilingual phrase tables and POS information in the matching task. Other similar works using POS information include (Gime\u0301nez and Ma\u0301rquez, 2007; Popovic and Ney, 2007; Han et al., 2014).\nIn linguistics, a phrase may refer to any group of words that form a constituent and so function as a single unit in the syntax of a sentence. To\nmeasure a MT system\u2019s performance in translating new text-types, such as in what ways the system itself could be extended to deal with new text-types, (Povlsen et al., 1998) perform a research work focusing on the study of Englishto-Danish machine-translation system. The syntactic constructions are explored with more complex linguistic knowledge, such as the identifying of fronted adverbial subordinate clauses and prepositional phrases. Assuming that the similar grammatical structures should occur on both source and translations, (Avramidis et al., 2011) perform the evaluation on source (German) and target (English) sentence employing the features of sentence length ratio, unknown words, phrase numbers including noun phrase, verb phrase and prepositional phrase. Other similar works using the phrase similarity include the (Li et al., 2012) that uses noun phrase and verb phrase from chunking and (Echizen-ya and Araki, 2010) that only uses the noun phrase chunking in automatic evaluation and (Han et al., 2013a) that designs a universal phrase tagset for French to English MT evaluation.\nSyntax is the study of the principles and processes by which sentences are constructed in particular languages. To address the overall goodness of the translated sentence\u2019s structure, (Liu and Gildea, 2005) employ constituent labels and head-modifier dependencies from language parser as syntactic features for MT evaluation. They compute the similarity of dependency trees. The overall experiments prove that adding syntactic information can improve the evaluation performance especially for predicting the fluency of hypothesis translations. Other works that using syntactic information into the evaluation include (Lo and Wu, 2011a) and (Lo et al., 2012) that use an automatic shallow parser and RED metric (Yu et al., 2014) that applies dependency tree, etc.\n3.2.2 Semantic Similarity As a contrast to the syntactic information, which captures the overall grammaticality or sentence structure similarity, the semantic similarity of the automatic translations and the source sentences (or references) can be measured by the employing of some semantic features.\nTo capture the semantic equivalence of sentences or text fragments, the named entity knowledge is brought from the literature of named-entity recognition, which is aiming to identify and clas-\nsify atomic elements in the text into different entity categories (Marsh and Perzanowski, 1998; Guo et al., 2009). The commonly used entity categories include the names of persons, locations, organizations and time. In the MEDAR2011 evaluation campaign,one baseline system based on Moses (Koehn et al., 2007a) utilizes Open NLP toolkit to perform named entity detection, in addition to other packages. The low performances from the perspective of named entities cause a drop in fluency and adequacy.In the quality estimation of machine translation task of WMT 2012, (Buck, 2012) introduces the features including named entity, in addition to discriminative word lexicon, neural networks, back off behavior (Raybaud et al., 2011) and edit distance, etc. The experiments on individual features show that, from the perspective of the increasing the correlation score with human judgments, the feature of named entity contributes nearly the most compared with the contributions of other features.\nSynonyms are words with the same or close meanings. One of the widely used synonym database in NLP literature is the WordNet (Miller et al., 1990), which is an English lexical database grouping English words into sets of synonyms. WordNet classifies the words mainly into four kinds of part-of-speech (POS) categories including Noun, Verb, Adjective, and Adverb without prepositions, determiners, etc. Synonymous words or phrases are organized using the unit of synset. Each synset is a hierarchical structure with the words in different levels according to their semantic relations.\nTextual entailment is usually used as a directive relation between text fragments. If the truth of one text fragment TA follows another text fragment TB, then there is a directional relation between TA and TB (TB\u21d2 TA). Instead of the pure logical or mathematical entailment, the textual entailment in natural language processing (NLP) is usually performed with a relaxed or loose definition (Dagan et al., 2006). For instance, according to text fragment TB, if it can be inferred that the text fragment TA is most likely to be true then the relationship TB \u21d2 TA also establishes. That the relation is directive also means that the inverse inference (TA\u21d2 TB) is not ensured to be true (Dagan and Glickman, 2004). Recently, Castillo and Estrella (2012) present a new approach for MT evaluation based on the task of \u201cSemantic Textual\nSimilarity\u201d. This problem is addressed using a textual entailment engine based on WordNet semantic features.\nParaphrase is to restatement the meaning of a passage or text utilizing other words, which can be seen as bidirectional textual entailment (Androutsopoulos and Malakasiotis, 2010). Instead of the literal translation, word by word and line by line used by metaphrase, paraphrase represents a dynamic equivalent. Further knowledge of paraphrase from the aspect of linguistics is introduced in the works of (McKeown, 1979; Meteer and Shaked, 1988; Barzilayand and Lee, 2003). (Snover et al., 2006) describe a new evaluation metric TER-Plus (TERp). Sequences of words in the reference are considered to be paraphrases of a sequence of words in the hypothesis if that phrase pair occurs in TERp phrase table.\nThe semantic roles are employed by some researchers as linguistic features in the MT evaluation. To utilize the semantic roles, the sentences are usually first shallow parsed and entity tagged. Then the semantic roles used to specify the arguments and adjuncts that occur in both the candidate translation and reference translation. For instance, the semantic roles introduced by (Gime\u0301nez and Ma\u0301rquez, 2007; Gime\u0301ne and Ma\u0301rquez, 2008) include causative agent, adverbial adjunct, directional adjunct, negation marker, and predication adjunct, etc.In the further development, (Lo and Wu, 2011a; Lo and Wu, 2011b) design the metric MEANT to capture the predicateargument relations as the structural relations in semantic frames, which is not reflected by the flat semantic role label features in the work of (Gime\u0301nez and Ma\u0301rquez, 2007). Furthermore, instead of using uniform weights, (Lo et al., 2012) weight the different types of semantic roles according to their relative importance to the adequate preservation of meaning, which is empirically determined.Generally, the semantic roles account for the semantic structure of a segment and have proved effective to assess adequacy in the above papers.\nThe language models are also utilized by the MT and MT evaluation researchers. A statistical language model usually assigns a probability to a sequence of words by means of a probability distribution. (Gamon et al., 2005) propose LMSVM, language-model, support vector machine, method investigating the possibility of evaluating\nMT quality and fluency in the absence of reference translations. They evaluate the performance of the system when used as a classifier for identifying highly dysfluent and illformed sentences.\n(Stanojevic\u0301 and Sima\u2019an, 2014a) designed a novel sentence level MT evaluation metric BEER, which has the advantage of incorporate large number of features in a linear model to maximize the correlation with human judgments. To make smoother sentence level scores, they explored two kinds of less sparse features including \u201ccharacter n-grams\u201d (e.g. stem checking) and \u201cabstract ordering patterns\u201d (permutation trees). They further investigated the model with more dense features such as adequacy features, fluency features and features based on permutation trees (Stanojevic\u0301 and Sima\u2019an, 2014c). In the latest version, they extended the permutation-tree (Gildea et al., 2006) into permutation-forests model (Stanojevic\u0301 and Sima\u2019an, 2014b), and showed stable good performance on different language pairs in WMT sentence level evaluation task.\nGenerally, the linguistic features mentioned above, including both syntactic and semantic features, are usually combined in two ways, either by following a machine learning approach (Albrecht and Hwa, 2007; Leusch and Ney, 2009), or trying to combine a wide variety of metrics in a more simple and straightforward way, such as (Gime\u0301ne and Ma\u0301rquez, 2008; Specia and Gime\u0301nez, 2010; Comelles et al., 2012),etc."}, {"heading": "3.3 DL for MTE", "text": "There are researchers applying DL and NNs models for MTE which are promising for further exploration.\nFor instances, (Guzma\u0301n et al., 2015; Guzmn et al., 2017) used neural networks for MTE for pair wise modeling to choose best hypothesis translation by comparing candidate translations with reference, integrating syntactic and semantic information into NNs.\n(Gupta et al., 2015b) proposed LSTM networks based on dense vectors to conduct MTE. While (Ma et al., 2016) designed a new metric based on bi-directional LSTM, which is similar with the work of (Guzma\u0301n et al., 2015) but with less complexity by allowing the evaluation of single hypothesis with reference, instead of pairwise situation."}, {"heading": "4 Evaluating the MT Evaluation", "text": ""}, {"heading": "4.1 Statistical Significance", "text": "If different MT systems produce translations with different qualities on a data set, how can we ensure that they indeed own different system quality? To explore this problem, (Koehn, 2004) performs a research work on the statistical significance test for machine translation evaluation. The bootstrap resampling method is used to compute the statistical significance intervals for evaluation metrics on small test sets. Statistical significance usually refers to two separate notions, of which one is the p-value, the probability that the observed data will occur by chance in a given single null hypothesis. The other one is the \u201cType I\u201d error rate of a statistical hypothesis test, which is also named as \u201cfalse positive\u201d and measured by the probability of incorrectly rejecting a given null hypothesis in favor of a second alternative hypothesis (Hald, 1998)."}, {"heading": "4.2 Evaluating Human Judgment", "text": "Since the human judgments are usually trusted as the golden standards that the automatic evaluation metrics should try to approach, the reliability and coherence of human judgments is very important. Cohen\u2019s kappa agreement coefficient is one of the commonly used evaluation methods (Cohen, 1960). For the problem in nominal scale agreement between two judges, there are two relevant quantities p0 and pc. The factor p0 is the proportion of units in which the judges agreed and pc is the proportion of units for which agreement is expected by chance. The coefficient k is simply the proportion of chance-expected disagreements which do not occur, or alternatively, it is the proportion of agreement after chance agreement is removed from consideration:\nk = p0 \u2212 pc 1\u2212 pc\n(14)\nwhere p0 \u2212 pc represents the proportion of the cases in which beyond-chance agreement occurs and is the numerator of the coefficient (Landis and Koch, 1977)."}, {"heading": "4.3 Correlating Manual and Automatic Score", "text": "In this section, we introduce three correlation coefficient algorithms that are commonly used by the recent WMT workshops to measure the closeness of the automatic evaluation and manual judgments. Choosing which correlation algorithm de-\npends on whether the scores or ranks schemes are utilized.\n4.3.1 Pearson Correlation Pearson\u2019s correlation coefficient (Pearson, 1900) is commonly represented by the Greek letter \u03c1. The correlation between random variables X and Y denoted as is measured as follow (Montgomery and Runger, 2003).\n\u03c1XY = cov(X,Y )\u221a V (X)V (Y ) = \u03c3XY \u03c3X\u03c3Y\n(15)\nBecause the standard deviations of variable X and Y are higher than 0 (\u03c3X > 0 and \u03c3Y > 0), if the covariance \u03c3XY between X and Y is positive, negative or zero, the correlation score between X and Y will correspondingly result in positive, negative or zero, respectively. Based on a sample of paired data (X,Y ) as (xi, yi), i = 1 to n , the Pearson correlation coefficient is calculated by:\n\u03c1XY = \u2211n i=1(xi \u2212 \u00b5x)(yi \u2212 \u00b5y)\u221a\u2211n\ni=1(xi \u2212 \u00b5x)2 \u221a\u2211n\ni=1(yi \u2212 \u00b5y)2 (16)\nwhere \u00b5x and \u00b5y specify the means of discrete random variable X and Y respectively.\n4.3.2 Spearman rank Correlation Spearman rank correlation coefficient, a simplified version of Pearson correlation coefficient , is another algorithm to measure the correlations of automatic evaluation and manual judges, especially in recent years (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011). When there are no ties, Spearman rank correlation coefficient, which is sometimes specified as (rs) is calculated as:\nrs\u03d5(XY ) = 1\u2212 6 \u2211n i=1 d 2 i\nn(n2 \u2212 1) (17)\nwhere di is the difference-value (D-value) between the two corresponding rank variables (xi \u2212 yi) in ~X = {x1, x2, ..., xn} and ~Y = {y1, y2, ..., yn} describing the system \u03d5."}, {"heading": "4.3.3 Kendall\u2019s \u03c4", "text": "Kendall\u2019s \u03c4 (Kendall, 1938) has been used in recent years for the correlation between automatic order and reference order (Callison-Burch et al.,\n2010; Callison-Burch et al., 2011; Callison-Burch et al., 2012). It is defined as:\n\u03c4 = num concordant pairs\u2212 num discordant pairs\ntotal pairs (18)\nThe latest version of Kendall\u2019s \u03c4 is introduced in (Kendall and Gibbons, 1990). (Lebanon and Lafferty, 2002) give an overview work for Kendall\u2019s \u03c4 showing its application in calculating how much the system orders differ from the reference order. More concretely, (Lapata, 2003) proposes the use of Kendall\u2019s \u03c4 , a measure of rank correlation, estimating the distance between a system-generated and a human-generated goldstandard order."}, {"heading": "4.4 Metrics Comparison Works", "text": "There are some researchers who did some work about the comparisons of different kinds of metrics. For example, (Callison-Burch et al., 2006b; Callison-Burch et al., 2007b; Lavie, 2013) mentioned that, through some qualitative analysis on some standard data set, BLEU can not reflect well of the MT systems\u2019 performance in many situations, i.e. higher BLEU score cannot ensure better translation outputs. Furthermore, there are some recently developed metrics that can perform much better than the traditional ones especially on the challenging sentence-level evaluation, though they are not popular yet such as nLEPOR and SentBLEU-Moses (Graham et al., 2015; Graham and Liu, 2016). Such kind of comparison works will help MT researchers to select proper metrics to use for their special tasks."}, {"heading": "5 Advanced Quality Estimation", "text": "In recent years, some MT evaluation methods that do not use the manually offered golden reference translations are proposed. They are usually called as \u201cQuality Estimation (QE)\u201d. Some of the related works have already been mentioned in previous sections. The latest quality estimation tasks of MT can be found from WMT12 to WMT15 (CallisonBurch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). They defined a novel evaluation metric that provides some advantages over the traditional ranking metrics. The designed criterion DeltaAvg assumes that the reference test set has a number associated with each entry that represents its extrinsic value. Given these values,\ntheir metric does not need an explicit reference ranking, the way the Spearman ranking correlation does. The goal of the DeltaAvg metric is to measure how valuable a proposed ranking is according to the extrinsic values associated with the test entries.\nDeltaAvgv[n] =\nn\u22121\u2211 k=1 V (S1,k)\nn\u2212 1 \u2212 V (S) (19)\nFor the scoring task, they use two task evaluation metrics that have been traditionally used for measuring performance for regression tasks: Mean Absolute Error (MAE) as a primary metric, and Root of Mean Squared Error (RMSE) as a secondary metric. For a given test set S with entries si, 1 6 i 6 |S| , they denote by H(si) the proposed score for entry si (hypothesis), and by V (si) the reference value for entry si (goldstandard value).\nMAE =\n\u2211N i\u22121 |H(si)\u2212 V (si)|\nN (20)\nRMSE =\n\u221a\u2211N i\u22121(H(si)\u2212 V (si))2\nN (21)\nwhere N = |S|. Both these metrics are nonparametric, automatic and deterministic (and therefore consistent), and extrinsically interpretable."}, {"heading": "6 Discussion and Conclusion", "text": "So far, the human judgment scores of MT results are usually considered as the golden standard that the automatic evaluation metrics should try to approach. However, some improper handlings in the process also yield problems. For instance, in the ACL WMT 2011 English-Czech task, the multiannotator agreement kappa value k is very low and even the exact same string produced by two systems is ranked differently each time by the same annotator. The evaluation results are highly affected by the manual reference translations. How to ensure the quality of reference translations and the agreement level of human judgments are two important problems.\nAutomatic evaluation metrics are indirect measures of translation quality, because that they are\nusually using the various string distance algorithms to measure the closeness between the machine translation system outputs and the manually offered reference translations and they are based on the calculating of correlation score with manual MT evaluation (Moran and Lewis, 2012). Furthermore, the automatic evaluation metrics tend to ignore the relevance of words (Koehn, 2010), for instance, the name entities and core concepts are more important than punctuations and determiners but most automatic evaluation metrics put the same weight on each word of the sentences. Third, automatic evaluation metrics usually yield meaningless score, which is very test set specific and the absolute value is not informative. For instance, what is the meaning of -16094 score by the MTeRater metric (Parton et al., 2011) or 1.98 score by ROSE (Song and Cohn, 2011)? And similar goes to 19.07 by BEER / 28.47 by BLEU / 33.03 by METEOR for a mostly good translation in the paper (see experiments section Table 4) (Maillette de Buy Wenniger and Sima\u2019an, 2015)? Instead, we find one interesting metric family LEPOR and hLEPOR (Han et al., 2012; Han, 2014) that can give a somehow meaningful score for a somehow recognized good translation, e.g. the score can be around 0.60 to 0.80.\nThe automatic evaluation metrics should try to achieve the goals of low cost, reduce time and money spent on carrying out evaluation; tunable, automatically optimize system performance towards metric;meaningful, score should give intuitive interpretation of translation quality;consistent, repeated use of metric should give same results;correct, metric must rank better systems higher as mentioned in (Koehn, 2010), of which the low cost, tunable and consistent characteristics are easily achieved by the metric developers, but the rest two goals (meaningful and correct) are usually the challenges in front of the NLP researchers.\nThere are some related works about MT evaluation survey or literature review before. For instance, in the DARPA GALE report (Dorr et al., 2009), researchers first introduced the automatic and semi-automatic MT evaluation measures, and the task and human in loop measures; then, they gave a description of the MT metrology in GALE program, which focus on the HTER metric as standard method used in GALE; finally, they compared some automatic metrics and explored some\nother usages of the metric, such as optimization in MT parameter training.\nIn another research project report EuroMatrix (EuroMatrix, 2007), researchers first gave an introduction of the MT history, then, they introduced human evaluation of MT and objective evaluation of MT as two main sections of the work; finally, they introduced a listed of popular evaluation measures at that time including WER, SER, CDER, X-Score, D-score, NIST, RED, IER and TER etc.\nMrquez (Mrquez, 2013) introduced the Asiya online interface developed by their institute for MT output error analysis, where they also briefly mentioned the MT evaluation developments of lexical measures and linguistically motivated measures, and pointed out the the chanllenges in the quality estimation task.\nOur work differs with the previous ones, by introducing some recent development of MT evaluation models, the different classifications from manual to automatic evaluation measures, the introduction of recent QE tasks, and the concise construction of the content."}, {"heading": "7 Perspective", "text": "In this section, we mention several aspects that are useful and will attract much attention for the further development of MT evaluation field.\nFirstly, it is about the lexical similarity and the linguistic features. Because the natural languages are expressive and ambiguous at different levels (Gime\u0301nez and Ma\u0301rquez, 2007), lexical similarity based metrics limit their scope to the lexical dimension and are not sufficient to ensure that two sentences convey the same meaning or not. For instance, the researches of (Callison-Burch et al., 2006a) and (Koehn and Monz, 2006b) report that lexical similarity metrics tend to favor the automatic statistical machine translation systems. If the evaluated systems belong to different types that include rule based, human aided, and statistical systems, then the lexical similarity metrics, such as BLEU, give a strong disagreement between ranking results provided by them and the human evaluators.So the linguistic features are very important in the MT evaluation procedure. However, in-appropriate utilization, or abundant or abused utilization, will result in difficulty in promotion.In the future, how to utilize the linguistic features more accurate, flexible, and simplified, will be one tendency in MT evalua-\ntion.Furthermore, the MT evaluation from the aspects of semantic similarity is more reasonable and reaches closer to the human judgments, so it should receive more attention.\nSecondly, the Quality Estimation tasks make some difference from the traditional evaluation, such as extracting reference-independent features from input sentences and the translation, obtaining quality score based on models produced from training data, predicting the quality of an unseen translated text at system run-time, filtering out sentences which are not good enough for post processing, and selecting the best translation among multiple systems, etc., so they will continuously attract many researchers.\nThirdly, some advanced or challenging technologies that can be tried for the MT evaluation include the deep learning (Gupta et al., 2015a; Zhang and Zong, 2015), semantic logic form, and decipherment model, etc."}, {"heading": "8 Aknowledgement", "text": "The author Han is supported by ADAPT Centre for Digital Content Technology funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund. Han thanks the funding by NWO VICI under Grant No. 277-89- 002 of Netherlands, and by the Research Committee of the University of Macau (Grant No. MYRG2015-00175-FST and MYRG2015-00188FST) and the Science and Technology Development Fund of Macau (Grant No. 057/2014/A).\nThe author Han thanks Prof. Dr. Qun Liu, Prof. Dr. Khalil Sima\u2019an from ILLC/UvA Amsterdam, and Dr. Ying Shi."}], "references": [{"title": "A re-examination of machine learning approaches for sentence-level mt evaluation", "author": ["Albrecht", "Hwa2007] J. Albrecht", "R. Hwa"], "venue": "In The Proceedings of the 45th Annual Meeting of the ACL,", "citeRegEx": "Albrecht et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Albrecht et al\\.", "year": 2007}, {"title": "A survey of paraphrasing and textual entailment methods", "author": ["Androutsopoulos", "Prodromos Malakasiotis"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Androutsopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Androutsopoulos et al\\.", "year": 2010}, {"title": "Computers and Translation: A translator\u2019s guide-Chap8 Why translation is difficult for computers", "author": ["D. Arnold"], "venue": "Benjamins Translation Library", "citeRegEx": "Arnold.,? \\Q2003\\E", "shortCiteRegEx": "Arnold.", "year": 2003}, {"title": "Evaluate with confidence estimation: Machine ranking of translation outputs using grammatical features", "author": ["Maja Popovic", "David Vilar", "Aljoscha Burchardt"], "venue": "Proceedings of WMT", "citeRegEx": "Avramidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Avramidis et al\\.", "year": 2011}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": "In Proceedings of the ACL", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Evaluation metrics for generation", "author": ["Owen Rambow", "Steven Whittaker"], "venue": "In Proceedings of INLG", "citeRegEx": "Bangalore et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bangalore et al\\.", "year": 2000}, {"title": "Learning to paraphrase: an unsupervised approach using multiple-sequence alignment", "author": ["Barzilayand", "Lee2003] Regina Barzilayand", "Lillian Lee"], "venue": "In Proceedings NAACL", "citeRegEx": "Barzilayand et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barzilayand et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Black box features for the wmt 2012 quality estimation shared task", "author": ["Christian Buck"], "venue": "In Proceedings of WMT", "citeRegEx": "Buck.,? \\Q2012\\E", "shortCiteRegEx": "Buck.", "year": 2012}, {"title": "Improved statistical machine translation using paraphrases", "author": ["Philipp Koehn", "Miles Osborne"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Callison.Burch et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2006}, {"title": "Reevaluating the role of bleu in machine translation research", "author": ["Miles Osborne", "Philipp Koehn"], "venue": "In Proceedings of EACL,", "citeRegEx": "Callison.Burch et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2006}, {"title": "meta-) evaluation of machine translation", "author": ["Cameron Fordyce", "Philipp Koehn", "Christof Monz", "Josh Schroeder"], "venue": "In Proceedings of WMT", "citeRegEx": "Callison.Burch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2007}, {"title": "meta-) evaluation of machine translation", "author": ["Cameron Fordyce", "Philipp Koehn", "Christof Monz", "Josh Schroeder"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Callison.Burch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2007}, {"title": "Further meta-evaluation of machine translation", "author": ["Cameron Fordyce", "Philipp Koehn", "Christof Monz", "Josh Schroeder"], "venue": "In Proceedings of WMT", "citeRegEx": "Callison.Burch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2008}, {"title": "Findings of the 2009 workshop on statistical machine translation", "author": ["Philipp Koehn", "Christof Monz", "Josh Schroeder"], "venue": "In Proceedings of the 4th WMT", "citeRegEx": "Callison.Burch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2009}, {"title": "Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation", "author": ["Philipp Koehn", "Christof Monz", "Kay Peterson", "Mark Przybocki", "Omar F. Zaridan"], "venue": null, "citeRegEx": "Callison.Burch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2010}, {"title": "Recent advances in example-based machine translation", "author": ["Carl", "Way2003] Michael Carl", "Andy Way"], "venue": null, "citeRegEx": "Carl et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Carl et al\\.", "year": 2003}, {"title": "An experiment in evaluating the quality of translation", "author": ["John B. Carroll"], "venue": "Mechanical Translation and Computational Linguistics,", "citeRegEx": "Carroll.,? \\Q1966\\E", "shortCiteRegEx": "Carroll.", "year": 1966}, {"title": "Port: a precision-order-recall mt evaluation metric for tuning", "author": ["Chen et al.2012] Boxing Chen", "Roland Kuhn", "Samuel Larkin"], "venue": "In Proceedings of the ACL", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259", "author": ["Cho et al.2014] KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Good applications for crummy machine translation", "author": ["Church", "Hovy1991] Kenneth Church", "Eduard Hovy"], "venue": "In Proceedings of the Natural Language Processing Systems Evaluation Workshop", "citeRegEx": "Church et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Church et al\\.", "year": 1991}, {"title": "A coefficient of agreement for nominal scales", "author": ["Jasob Cohen"], "venue": "Educational and Psychological Measurement,", "citeRegEx": "Cohen.,? \\Q1960\\E", "shortCiteRegEx": "Cohen.", "year": 1960}, {"title": "Verta: Linguistic features in mt evaluation", "author": ["Jordi Atserias", "Victoria Arranz", "Irene Castell\u00f3n"], "venue": "In LREC,", "citeRegEx": "Comelles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Comelles et al\\.", "year": 2012}, {"title": "Probabilistic textual entailment: Generic applied modeling of language variability. In Learning Methods for Text Understanding and Mining workshop", "author": ["Dagan", "Glickman2004] Ido Dagan", "Oren Glickman"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2004}, {"title": "The pascal recognising textual entailment challenge", "author": ["Dagan et al.2006] Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": "Machine Learning Challenges:LNCS,", "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Tesla at wmt2011: Translation evaluation and tunable metric", "author": ["Chang Liu", "Hwee Tou Ng"], "venue": "In Proceedings of WMT", "citeRegEx": "Dahlmeier et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2011}, {"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["George Doddington"], "venue": "In HLT Proceedings", "citeRegEx": "Doddington.,? \\Q2002\\E", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Part 5: Machine translation evaluation", "author": ["Dorr et al.2009] Bonnie Dorr", "Matt Snover", "etc. Nitin Madnani"], "venue": "In Bonnie Dorr edited DARPA GALE program report", "citeRegEx": "Dorr et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dorr et al\\.", "year": 2009}, {"title": "Task-based evaluation for machine translation", "author": ["John S. White", "Kathryn B. Taylor"], "venue": "In Proceedings of MT Summit", "citeRegEx": "Doyon et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Doyon et al\\.", "year": 1999}, {"title": "Automatic evaluation method for machine translation using noun-phrase chunking", "author": ["Echizen-ya", "Araki2010] H. Echizen-ya", "K. Araki"], "venue": "In Proceedings of the ACL", "citeRegEx": "Echizen.ya et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Echizen.ya et al\\.", "year": 2010}, {"title": "Overview of the iwslt 2005 evaluation campaign", "author": ["Eck", "Hori2005] Matthias Eck", "Chiori Hori"], "venue": "In In proceeding of International Workshop on Spoken Language Translation (IWSLT)", "citeRegEx": "Eck et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Eck et al\\.", "year": 2005}, {"title": "Overview of the iwslt 2011 evaluation campaign", "author": ["Luisa Bentivogli", "Michael Paul", "Sebastian St\u00fcker"], "venue": "In In proceeding of International Workshop on Spoken Language Translation (IWSLT)", "citeRegEx": "Federico et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Federico et al\\.", "year": 2011}, {"title": "Measuring word alignment quality for statistical machine translation", "author": ["Fraser", "Marcu2007] Alexander Fraser", "Daniel Marcu"], "venue": null, "citeRegEx": "Fraser et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fraser et al\\.", "year": 2007}, {"title": "Sentence-level mt evaluation without reference translations beyond language modelling", "author": ["Gamon et al.2005] Michael Gamon", "Anthony Aue", "Martine Smets"], "venue": "In Proceedings of EAMT,", "citeRegEx": "Gamon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gamon et al\\.", "year": 2005}, {"title": "Factoring synchronous grammars by sorting", "author": ["Gildea et al.2006] Daniel Gildea", "Giorgio Satta", "Hao Zhang"], "venue": "In Proceedings of ACL", "citeRegEx": "Gildea et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gildea et al\\.", "year": 2006}, {"title": "A smorgasbord of features for automatic mt evaluation", "author": ["Gim\u00e9ne", "M\u00e1rquez2008] Jes\u00fas Gim\u00e9ne", "Ll\u00fais M\u00e1rquez"], "venue": "In Proceedings of WMT,", "citeRegEx": "Gim\u00e9ne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gim\u00e9ne et al\\.", "year": 2008}, {"title": "Linguistic features for automatic evaluation of heterogenous mt systems", "author": ["Gim\u00e9nez", "M\u00e1rquez2007] Jes\u00fas Gim\u00e9nez", "Ll\u00fais M\u00e1rquez"], "venue": "In Proceedings of WMT", "citeRegEx": "Gim\u00e9nez et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gim\u00e9nez et al\\.", "year": 2007}, {"title": "Achieving accurate conclusions in evaluation of automatic machine translation metrics", "author": ["Graham", "Liu2016] Yvette Graham", "Qun Liu"], "venue": "In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Graham et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2016}, {"title": "Accurate evaluation of segment-level machine translation metrics", "author": ["Graham et al.2015] Yvette Graham", "Timothy Baldwin", "Nitika Mathur"], "venue": "In NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Named entity recognition in query", "author": ["Guo et al.2009] Jiafeng Guo", "Gu Xu", "Xueqi Cheng", "Hang Li"], "venue": "In Proceeding of SIGIR", "citeRegEx": "Guo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2009}, {"title": "2015a. Machine translation evaluation using recurrent neural networks", "author": ["Gupta et al.2015a] Rohit Gupta", "Constantin Orasan", "Josef van Genabith"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "2015b. Reval: A simple and effective machine translation evaluation metric based on recurrent neural networks", "author": ["Gupta et al.2015b] Rohit Gupta", "Constantin Orasan", "Josef van Genabith"], "venue": "In Proceedings of the 2015 Conference on Emperical Methods", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Pairwise neural machine translation evaluation", "author": ["Shafiq Joty", "Llu\u0131\u0301s M\u00e0rquez", "Preslav Nakov"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and The 7th Interna-", "citeRegEx": "Guzm\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guzm\u00e1n et al\\.", "year": 2015}, {"title": "Machine translation evaluation with neural networks. Comput. Speech Lang., 45(C):180\u2013200, September", "author": ["Shafiq Joty", "Llus Mrquez", "Preslav Nakov"], "venue": null, "citeRegEx": "Guzmn et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Guzmn et al\\.", "year": 2017}, {"title": "A History of Mathematical Statistics from 1750 to 1930", "author": ["Anders Hald"], "venue": null, "citeRegEx": "Hald.,? \\Q1998\\E", "shortCiteRegEx": "Hald.", "year": 1998}, {"title": "A robust evaluation metric for machine translation with augmented factors", "author": ["Derek Fai Wong", "Lidia Sam Chao"], "venue": "Proceedings of COLING", "citeRegEx": "Han et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Han et al\\.", "year": 2012}, {"title": "Phrase tagset mapping for french and english treebanks and its application in machine translation evaluation", "author": ["Derek Fai Wong", "Lidia Sam Chao", "Liangeye He", "Shuo Li", "Ling Zhu"], "venue": null, "citeRegEx": "Han et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "A description of tunable machine translation evaluation systems in wmt13 metrics task", "author": ["Derek Fai Wong", "Lidia Sam Chao", "Yi Lu", "Liangye He", "Yiming Wang", "Jiaji Zhou"], "venue": "Proceedings of WMT,", "citeRegEx": "Han et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Unsupervised quality estimation model for english to german translation and its application in extensive supervised evaluation", "author": ["Derek Fai Wong", "Lidia Sam Chao", "Liangeye He", "Yi Lu"], "venue": null, "citeRegEx": "Han et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Han et al\\.", "year": 2014}, {"title": "LEPOR: An Augmented Machine Translation Evaluation Metric", "author": ["Lifeng Han"], "venue": "Ph.D. thesis,", "citeRegEx": "Han.,? \\Q2014\\E", "shortCiteRegEx": "Han.", "year": 2014}, {"title": "Improving statistical machine translation using shallow linguistic knowledge", "author": ["Andrew Finch", "Yutaka Sasaki"], "venue": "Computer Speech and Language,", "citeRegEx": "Hwang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2007}, {"title": "Rank Correlation Methods", "author": ["Kendall", "Gibbons1990] Maurice G. Kendall", "Jean Dickinson Gibbons"], "venue": null, "citeRegEx": "Kendall et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kendall et al\\.", "year": 1990}, {"title": "A new measure of rank correlation", "author": ["Maurice G. Kendall"], "venue": null, "citeRegEx": "Kendall.,? \\Q1938\\E", "shortCiteRegEx": "Kendall.", "year": 1938}, {"title": "Syntax-based reordering for statistical machine translation", "author": ["Khalilov", "Fonollosa2011] Maxim Khalilov", "Jos\u00e9 A.R. Fonollosa"], "venue": "Computer Speech and Language,", "citeRegEx": "Khalilov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Khalilov et al\\.", "year": 2011}, {"title": "Femti: Creating and using a framework for mt evaluation", "author": ["King et al.2003] Marrgaret King", "Andrei PopescuBelis", "Eduard Hovy"], "venue": "In Proceedings of the Machine Translation Summit IX", "citeRegEx": "King et al\\.,? \\Q2003\\E", "shortCiteRegEx": "King et al\\.", "year": 2003}, {"title": "Statistical machine translation, November 24. US Patent 7,624,005", "author": ["Koehn", "Knight2009] Philipp Koehn", "Kevin Knight"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2009}, {"title": "Shared task: Statistical machine translation between european languages", "author": ["Koehn", "Monz2005] Philipp Koehn", "Christof Monz"], "venue": "In Proceedings of the ACL Workshop on Building and Using Parallel Texts", "citeRegEx": "Koehn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2005}, {"title": "Manual and automatic evaluation of machine translation between european languages", "author": ["Koehn", "Monz2006a] Philipp Koehn", "Christof Monz"], "venue": "In Proceedings on the Workshop on Statistical Machine Translation,", "citeRegEx": "Koehn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2006}, {"title": "Manual and automatic evaluation of machine translation between european languages", "author": ["Koehn", "Monz2006b] Philipp Koehn", "Christof Monz"], "venue": "In Proceedings of WMT", "citeRegEx": "Koehn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2006}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "Proceedings of ACL.", "citeRegEx": "Herbst.,? 2007a", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Moses: Open source toolkit", "author": ["Koehn et al.2007b] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Koehn.,? \\Q2004\\E", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "Koehn.,? \\Q2010\\E", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "The measurement of observer agreement for categorical data", "author": ["Landis", "Koch1977] J. Richard Landis", "Gary G. Koch"], "venue": null, "citeRegEx": "Landis et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Landis et al\\.", "year": 1977}, {"title": "Task-based mt evaluation: From who/when/where extraction to event understanding", "author": ["Laoudi et al.2006] Jamal Laoudi", "Ra R. Tate", "Clare R. Voss"], "venue": "In in Proceedings of LREC06,", "citeRegEx": "Laoudi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Laoudi et al\\.", "year": 2006}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "author": ["Mirella Lapata"], "venue": "In Proceedings of ACL", "citeRegEx": "Lapata.,? \\Q2003\\E", "shortCiteRegEx": "Lapata.", "year": 2003}, {"title": "Automated metrics for mt evaluation", "author": ["Alon Lavie"], "venue": "Machine Translation,", "citeRegEx": "Lavie.,? \\Q2013\\E", "shortCiteRegEx": "Lavie.", "year": 2013}, {"title": "Combining rankings using conditional probability models on permutations", "author": ["Lebanon", "Lafferty2002] Guy Lebanon", "John Lafferty"], "venue": "In Proceeding of the ICML", "citeRegEx": "Lebanon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lebanon et al\\.", "year": 2002}, {"title": "Edit distances with block movements and error rate confidence estimates", "author": ["Leusch", "Ney2009] Gregor Leusch", "Hermann Ney"], "venue": "Machine Translation,", "citeRegEx": "Leusch et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leusch et al\\.", "year": 2009}, {"title": "Phrase-based evaluation for machine translation", "author": ["Li et al.2012] Liang You Li", "Zheng Xian Gong", "Guo Dong Zhou"], "venue": "In Proceedings of COLING,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Results of the 2005 nist machine translation evaluation", "author": ["A. LI"], "venue": "In Proceedings of WMT", "citeRegEx": "LI.,? \\Q2005\\E", "shortCiteRegEx": "LI.", "year": 2005}, {"title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "E.H. Hovy"], "venue": "In Proceedings NAACL", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Lin", "Och2004] Chin-Yew Lin", "Franz Josef Och"], "venue": "In Proceedings of ACL", "citeRegEx": "Lin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2004}, {"title": "Syntactic features for evaluation of machine translation. In Proceedingsof theACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization", "author": ["Liu", "Gildea2005] Ding Liu", "Daniel Gildea"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2005}, {"title": "Better evaluation metrics lead to better machine translation", "author": ["Chang Liu", "Daniel Dahlmeier", "Hwee Tou Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Meant: An inexpensive, high- accuracy, semiautomatic metric for evaluating translation utility based on semantic roles", "author": ["Lo", "Wu2011a] Chi Kiu Lo", "Dekai Wu"], "venue": "Proceedings of ACL", "citeRegEx": "Lo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lo et al\\.", "year": 2011}, {"title": "Structured vs. flat semantic role representations for machine translation evaluation", "author": ["Lo", "Wu2011b] Chi Kiu Lo", "Dekai Wu"], "venue": "In Proceedings of the 5th Workshop on Syntax and Structure in StatisticalTranslation (SSST-5)", "citeRegEx": "Lo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lo et al\\.", "year": 2011}, {"title": "Fully automatic semantic mt evaluation", "author": ["Lo et al.2012] Chi Kiu Lo", "Anand Karthik Turmuluru", "Dekai Wu"], "venue": "In Proceedings of WMT", "citeRegEx": "Lo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lo et al\\.", "year": 2012}, {"title": "Maxsd: A neural machine translation evaluation metric optimized by maximizing similarity", "author": ["Ma et al.2016] Qingsong Ma", "Fandong Meng", "Daqi Zheng", "Mingxuan Wang", "Yvette Graham", "Wenbin Jiang", "Qun Liu"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "Muc-7 evaluation of ie technology: Overview of results", "author": ["Marsh", "Perzanowski1998] Elaine Marsh", "Dennis Perzanowski"], "venue": "In Proceedingsof Message Understanding Conference (MUC-7)", "citeRegEx": "Marsh et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Marsh et al\\.", "year": 1998}, {"title": "Paraphrasing using given and new information in a question-answer system", "author": ["Kathleen R. McKeown"], "venue": "In Proceedings of ACL", "citeRegEx": "McKeown.,? \\Q1979\\E", "shortCiteRegEx": "McKeown.", "year": 1979}, {"title": "Microsoft research treelet translation system: Naacl 2006 europarl evaluation", "author": ["Menezes et al.2006] Arul Menezes", "Kristina Toutanova", "Chris Quirk"], "venue": "In Proceedings of WMT", "citeRegEx": "Menezes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Menezes et al\\.", "year": 2006}, {"title": "Microsoft research treelet translation system: Naacl 2006 europarl evaluation", "author": ["Meteer", "Shaked1988] Marie Meteer", "Varda Shaked"], "venue": "In Proceedings of COLING", "citeRegEx": "Meteer et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Meteer et al\\.", "year": 1988}, {"title": "Wordnet: an on-line lexical database", "author": ["G.A. Miller", "R. Beckwith", "C. Fellbaum", "D. Gross", "K.J. Miller"], "venue": null, "citeRegEx": "Miller et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1990}, {"title": "Applied statistics and probability for engineers", "author": ["Montgomery", "George C. Runger"], "venue": null, "citeRegEx": "Montgomery et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Montgomery et al\\.", "year": 2003}, {"title": "Unobtrusive methods for low-cost manual assessment of machine translation", "author": ["Moran", "Lewis2012] John Moran", "David Lewis"], "venue": "Tralogy I [Online], Session", "citeRegEx": "Moran et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Moran et al\\.", "year": 2012}, {"title": "automatic evaluation of machine translation quality", "author": ["L. Mrquez"], "venue": "Dialogue 2013 invited talk,", "citeRegEx": "Mrquez.,? \\Q2013\\E", "shortCiteRegEx": "Mrquez.", "year": 2013}, {"title": "Minimum error rate training for statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of ACL", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei Jing Zhu"], "venue": "In Proceedings of ACL", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Erating machine translation", "author": ["Joel Tetreault ans Nitin Madnani", "Martin Chodorow"], "venue": "In Proceedings of WMT", "citeRegEx": "Parton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parton et al\\.", "year": 2011}, {"title": "Overview of the iwslt 2010 evaluation campaign", "author": ["Paul et al.2010] Michael Paul", "Marcello Federico", "Sebastian St\u00fcker"], "venue": "In Proceeding of IWSLT", "citeRegEx": "Paul et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2010}, {"title": "Overview of the iwslt 2009 evaluation campaign", "author": ["M. Paul"], "venue": "In Proceeding of IWSLT", "citeRegEx": "Paul.,? \\Q2009\\E", "shortCiteRegEx": "Paul.", "year": 2009}, {"title": "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling", "author": ["Karl Pearson"], "venue": null, "citeRegEx": "Pearson.,? \\Q1900\\E", "shortCiteRegEx": "Pearson.", "year": 1900}, {"title": "Evaluation without references: Ibm1 scores as evaluation metrics", "author": ["Popovi\u0107 et al.2011] Maja Popovi\u0107", "David Vilar", "Eleftherios Avramidis", "Aljoscha Burchardt"], "venue": "In Proceedings of WMT", "citeRegEx": "Popovi\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Popovi\u0107 et al\\.", "year": 2011}, {"title": "Word error rates: Decomposition over pos classes and applications for error analysis", "author": ["Popovic", "Ney2007] M. Popovic", "Hermann Ney"], "venue": "In Proceedings of WMT", "citeRegEx": "Popovic et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Popovic et al\\.", "year": 2007}, {"title": "Evaluating text-type suitability for machine translation a case study on an english-danish system", "author": ["Nancy Underwood", "Bradley Music", "Anne Neville"], "venue": "In Proceeding LREC", "citeRegEx": "Povlsen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Povlsen et al\\.", "year": 1998}, {"title": "this sentence is wrong.\u201d detecting errors in machine-translated sentences", "author": ["David Langlois", "Kamel Sm\u00e4ili"], "venue": "Machine Translation,", "citeRegEx": "Raybaud et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Raybaud et al\\.", "year": 2011}, {"title": "Inferring shallow-transfer machine translation rules from small parallel corpora", "author": ["S\u00e1nchez-Mart\u0301inez", "Forcada2009] F. S\u00e1nchezMart\u0301inez", "M.L. Forcada"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "S\u00e1nchez.Mart\u0301inez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "S\u00e1nchez.Mart\u0301inez et al\\.", "year": 2009}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Bonnie J. Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": "In Proceeding of AMTA", "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Regression and ranking based optimisation for sentence level mt evaluation", "author": ["Song", "Cohn2011] Xingyi Song", "Trevor Cohn"], "venue": "In Proceedings of WMT", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Combining confidence estimation and reference-based metrics for segment-level mt evaluation. In The Ninth Conference of the Association for Machine Translation in the Americas (AMTA)", "author": ["Specia", "Gim\u00e9nez2010] L. Specia", "J. Gim\u00e9nez"], "venue": null, "citeRegEx": "Specia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2010}, {"title": "Predicting machine translation adequacy", "author": ["Specia et al.2011] Lucia Specia", "Naheh Hajlaoui", "Catalina Hallett", "Wilker Aziz"], "venue": "In Machine Translation Summit XIII", "citeRegEx": "Specia et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2011}, {"title": "A new quantitative quality measure for machine translation systems", "author": ["Su et al.1992] Keh-Yih Su", "Wu Ming-Wen", "Chang Jing-Shin"], "venue": "In Proceeding of COLING", "citeRegEx": "Su et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Su et al\\.", "year": 1992}, {"title": "Accelerated dp based search for statistical translation", "author": ["Stephan Vogel", "Hermann Ney", "Arkaitz Zubiaga", "Hassan Sawaf"], "venue": "In Proceeding of EUROSPEECH", "citeRegEx": "Tillmann et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tillmann et al\\.", "year": 1997}, {"title": "Evaluation of machine translation and its evaluation", "author": ["Luke Shea", "I Dan Melamed"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2006}, {"title": "Task-based evaluation of machine translation (mt) engines: Measuring how well people extract who, when, where-type elements in mt output", "author": ["Voss", "Tate2006] Clare R. Voss", "Ra R. Tate"], "venue": "Proceedings of 11th Annual Conference of the Euro-", "citeRegEx": "Voss et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Voss et al\\.", "year": 2006}, {"title": "Translation. Machine Translation of Languages: Fourteen Essays", "author": ["Warren Weaver"], "venue": null, "citeRegEx": "Weaver.,? \\Q1955\\E", "shortCiteRegEx": "Weaver.", "year": 1955}, {"title": "A task-oriented evaluation metric for machine translation", "author": ["White", "Taylor1998] John S. White", "Kathryn B. Taylor"], "venue": "In Proceeding LREC", "citeRegEx": "White et al\\.,? \\Q1998\\E", "shortCiteRegEx": "White et al\\.", "year": 1998}, {"title": "The arpa mt evaluation methodologies: Evolution, lessons, and future approaches", "author": ["White et al.1994] John S. White", "Theresa O\u2019 Connell", "Francis O\u2019 Mara"], "venue": "In Proceeding of AMTA", "citeRegEx": "White et al\\.,? \\Q1994\\E", "shortCiteRegEx": "White et al\\.", "year": 1994}, {"title": "Neural-based machine translation for medical text domain. based on european medicines agency leaflet texts", "author": ["Wolk", "Marasek2015] Krzysztof Wolk", "Krzysztof Marasek"], "venue": "Procedia Computer Science,", "citeRegEx": "Wolk et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wolk et al\\.", "year": 2015}, {"title": "Atec: automatic evaluation of machine translation via word choice and word order", "author": ["Wong", "yu Kit2009] Billy Wong", "Chun yu Kit"], "venue": "Machine Translation,", "citeRegEx": "Wong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2009}, {"title": "RED: A reference dependency based MT evaluation metric", "author": ["Yu et al.2014] Hui Yu", "Xiaofeng Wu", "Jun Xie", "Wenbin Jiang", "Qun Liu", "Shouxun Lin"], "venue": "COLING", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Deep neural networks in machine translation: An overview", "author": ["Zhang", "Zong2015] Jiajun Zhang", "Chengqing Zong"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 29, "context": "This paper differs from the existing works (Dorr et al., 2009; EuroMatrix, 2007) from several aspects, by introducing some recent development of MT evaluation measures, the different classifications from manual to automatic evaluation measures, the introduction of recent QE tasks of MT, and the concise construction of the content.", "startOffset": 43, "endOffset": 80}, {"referenceID": 109, "context": "Machine translation (MT) began as early as in the 1950s (Weaver, 1955) , and gained a rapid development since the 1990s (Mari\u00f1o et al.", "startOffset": 56, "endOffset": 70}, {"referenceID": 8, "context": "There are many important works in MT areas, for some to mention by time, IBM Watson research group (Brown et al., 1993) designed five statistical MT models and the ways of how to estimate the parame-", "startOffset": 99, "endOffset": 119}, {"referenceID": 61, "context": "ters in the models given the bilingual translation corpora; (Koehn et al., 2003) proposed statistical phrase-based MT model; Och (Och, 2003) presented Minimum Error Rate Training (MERT) for log-linear statistical machine translation models; (Koehn and Monz, 2005) introduced a Shared task of building statistical machine translation (SMT) systems for four European language pairs; (Chiang, 2005) proposed a hierarchical phrase-based SMT model that is learned from a bitext with-", "startOffset": 60, "endOffset": 80}, {"referenceID": 90, "context": ", 2003) proposed statistical phrase-based MT model; Och (Och, 2003) presented Minimum Error Rate Training (MERT) for log-linear statistical machine translation models; (Koehn and Monz, 2005) introduced a Shared task of building statistical machine translation (SMT) systems for four European language pairs; (Chiang, 2005) proposed a hierarchical phrase-based SMT model that is learned from a bitext with-", "startOffset": 56, "endOffset": 67}, {"referenceID": 20, "context": ", 2003) proposed statistical phrase-based MT model; Och (Och, 2003) presented Minimum Error Rate Training (MERT) for log-linear statistical machine translation models; (Koehn and Monz, 2005) introduced a Shared task of building statistical machine translation (SMT) systems for four European language pairs; (Chiang, 2005) proposed a hierarchical phrase-based SMT model that is learned from a bitext with-", "startOffset": 308, "endOffset": 322}, {"referenceID": 84, "context": "out syntactic information; (Menezes et al., 2006) introduced a syntactically informed phrasal SMT system for English-to-Spanish translation using a phrase translation model, which was based on global reordering and dependency tree; (Koehn et al.", "startOffset": 27, "endOffset": 49}, {"referenceID": 52, "context": ", 2007b) developed an open source SMT software toolkit Moses; (Hwang et al., 2007) utilized the shallow linguistic knowledge to improve word alignment and language model quality be-", "startOffset": 62, "endOffset": 82}, {"referenceID": 4, "context": "With the fast development of Deep Learning (DL), MT research has evolved from rule-based models to example based models, statistical models, hybrid models, and recent years\u2019 Neural models (Nirenburg, 1989; Carl and Way, 2003; Koehn and Knight, 2009; Bahdanau et al., 2014), such as the attention mechanism models, coverage models, multi-modal and multilingual MT models.", "startOffset": 188, "endOffset": 272}, {"referenceID": 21, "context": "mance through two steps recurrent neural network (RNN) of encoder and decoder (Cho et al., 2014; Bahdanau et al., 2014; Wolk and Marasek, 2015).", "startOffset": 78, "endOffset": 143}, {"referenceID": 4, "context": "mance through two steps recurrent neural network (RNN) of encoder and decoder (Cho et al., 2014; Bahdanau et al., 2014; Wolk and Marasek, 2015).", "startOffset": 78, "endOffset": 143}, {"referenceID": 2, "context": "content in the same way (Arnold, 2003).", "startOffset": 24, "endOffset": 38}, {"referenceID": 73, "context": "One of them was the NIST open machine translation Evaluation series (OpenMT), which were very prestigious evaluation campaigns from 2001 to 2009 (LI, 2005).", "startOffset": 145, "endOffset": 155}, {"referenceID": 18, "context": "They include the intelligibility and fidelity used by the automatic language processing advisory committee (ALPAC) (Carroll, 1966).", "startOffset": 115, "endOffset": 130}, {"referenceID": 111, "context": "1991) in MT evaluation campaigns (White et al., 1994).", "startOffset": 33, "endOffset": 53}, {"referenceID": 6, "context": "(Bangalore et al., 2000) conduct a research developing accuracy into several kinds including simple string accuracy, generation string accuracy, and two corresponding tree-based accuracies.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "(Bangalore et al., 2000) conduct a research developing accuracy into several kinds including simple string accuracy, generation string accuracy, and two corresponding tree-based accuracies.Reeder (2004) shows the correlation between fluency and the number of words it takes to distinguish between human translation and machine translation.", "startOffset": 1, "endOffset": 203}, {"referenceID": 104, "context": "(Specia et al., 2011) conduct a study of the", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "A brief introduction of task-based MT evaluation work was shown in their later work (Doyon et al., 1999).", "startOffset": 84, "endOffset": 104}, {"referenceID": 67, "context": "They extend the work later into event understanding in (Laoudi et al., 2006).", "startOffset": 55, "endOffset": 76}, {"referenceID": 56, "context": "(King et al., 2003) extend a large range of manual evaluation methods for MT systems, which, in", "startOffset": 0, "endOffset": 19}, {"referenceID": 101, "context": "One example of a metric that is designed in such a manner is the human translation error rate (HTER) (Snover et al., 2006), based on the number of editing steps, computing the editing steps between an automatic translation and a reference translation.", "startOffset": 101, "endOffset": 122}, {"referenceID": 47, "context": "There are usually two ways to offer the human reference translation, either offering one single reference or offering multiple references for a single source sentence (Lin and Och, 2004; Han et al., 2012).", "startOffset": 167, "endOffset": 204}, {"referenceID": 105, "context": "By calculating the minimum number of editing steps to transform output to reference, (Su et al., 1992) introduce the word error rate (WER) metric into MT evaluation.", "startOffset": 85, "endOffset": 102}, {"referenceID": 106, "context": "To address this problem, the position-independent word error rate (PER) (Tillmann et al., 1997) is designed to ignore word order when matching output and reference.", "startOffset": 72, "endOffset": 95}, {"referenceID": 101, "context": "In this light, (Snover et al., 2006) design the translation edit rate (TER) metric that adds block movement (jumping action) as", "startOffset": 15, "endOffset": 36}, {"referenceID": 91, "context": "The widely used evaluation metric BLEU (Papineni et al., 2002) is based on the degree of ngram overlapping between the strings of words produced by the machine and the human translation references at the corpus level.", "startOffset": 39, "endOffset": 62}, {"referenceID": 28, "context": "To weight more heavily those n-grams that are more informative, (Doddington, 2002) proposes the NIST metric with the information weight added.", "startOffset": 64, "endOffset": 82}, {"referenceID": 107, "context": "(Turian et al., 2006) conducted experiments to examine how standard measures such as precision and recall and F-measure can be applied for evaluation of MT and showed the comparisons of these standard measures with some existing alternative evaluation measures.", "startOffset": 0, "endOffset": 21}, {"referenceID": 19, "context": "Combining the precision, order, and recall information together, (Chen et al., 2012) develop an automatic evaluation metric PORT that is initially for the tuning of the MT systems to output higher quality translation.", "startOffset": 65, "endOffset": 84}, {"referenceID": 47, "context": "Another evaluation metric LEPOR (Han et al., 2012; Han et al., 2014) is proposed as the combination of many evaluation factors including n-gram based word order penalty in addition to precision, recall, and sentence-length penalty.", "startOffset": 32, "endOffset": 68}, {"referenceID": 50, "context": "Another evaluation metric LEPOR (Han et al., 2012; Han et al., 2014) is proposed as the combination of many evaluation factors including n-gram based word order penalty in addition to precision, recall, and sentence-length penalty.", "startOffset": 32, "endOffset": 68}, {"referenceID": 80, "context": "The advantages of the metrics based on lexical similarity are that they perform well in capturing the translation fluency (Lo et al., 2012), and they are very fast and low cost.", "startOffset": 122, "endOffset": 139}, {"referenceID": 96, "context": "Using the IBM model one, (Popovi\u0107 et al., 2011) evaluate the translation quality by calculating the similarity scores of source and target (translated) sentence without using reference translation, based on the morphemes, 4-gram POS and lexicon probabilities.", "startOffset": 25, "endOffset": 47}, {"referenceID": 27, "context": "(Dahlmeier et al., 2011) develop the evaluation metrics TESLA, combining the synonyms of bilingual phrase tables and POS information in the matching task.", "startOffset": 0, "endOffset": 24}, {"referenceID": 50, "context": "Other similar works using POS information include (Gim\u00e9nez and M\u00e1rquez, 2007; Popovic and Ney, 2007; Han et al., 2014).", "startOffset": 50, "endOffset": 118}, {"referenceID": 98, "context": "To measure a MT system\u2019s performance in translating new text-types, such as in what ways the system itself could be extended to deal with new text-types, (Povlsen et al., 1998) perform a research work focusing on the study of Englishto-Danish machine-translation system.", "startOffset": 154, "endOffset": 176}, {"referenceID": 3, "context": "Assuming that the similar grammatical structures should occur on both source and translations, (Avramidis et al., 2011) perform the evaluation on source (German) and target (English) sentence employing the features of sentence length ratio, unknown words, phrase numbers including noun phrase, verb phrase and prepositional phrase.", "startOffset": 95, "endOffset": 119}, {"referenceID": 72, "context": "Other similar works using the phrase similarity include the (Li et al., 2012)", "startOffset": 60, "endOffset": 77}, {"referenceID": 80, "context": "Other works that using syntactic information into the evaluation include (Lo and Wu, 2011a) and (Lo et al., 2012) that use an automatic shallow parser and RED metric (Yu et al.", "startOffset": 96, "endOffset": 113}, {"referenceID": 114, "context": ", 2012) that use an automatic shallow parser and RED metric (Yu et al., 2014) that applies dependency tree, etc.", "startOffset": 60, "endOffset": 77}, {"referenceID": 41, "context": "sify atomic elements in the text into different entity categories (Marsh and Perzanowski, 1998; Guo et al., 2009).", "startOffset": 66, "endOffset": 113}, {"referenceID": 9, "context": "In the quality estimation of machine translation task of WMT 2012, (Buck, 2012) introduces the features including named entity, in addition to discriminative word lexicon, neural networks, back off behavior (Raybaud et al.", "startOffset": 67, "endOffset": 79}, {"referenceID": 99, "context": "In the quality estimation of machine translation task of WMT 2012, (Buck, 2012) introduces the features including named entity, in addition to discriminative word lexicon, neural networks, back off behavior (Raybaud et al., 2011) and edit distance, etc.", "startOffset": 207, "endOffset": 229}, {"referenceID": 86, "context": "One of the widely used synonym database in NLP literature is the WordNet (Miller et al., 1990), which is an English lexical database grouping English words into sets of synonyms.", "startOffset": 73, "endOffset": 94}, {"referenceID": 26, "context": "Instead of the pure logical or mathematical entailment, the textual entailment in natural language processing (NLP) is usually performed with a relaxed or loose definition (Dagan et al., 2006).", "startOffset": 172, "endOffset": 192}, {"referenceID": 25, "context": "Instead of the pure logical or mathematical entailment, the textual entailment in natural language processing (NLP) is usually performed with a relaxed or loose definition (Dagan et al., 2006). For instance, according to text fragment TB, if it can be inferred that the text fragment TA is most likely to be true then the relationship TB \u21d2 TA also establishes. That the relation is directive also means that the inverse inference (TA\u21d2 TB) is not ensured to be true (Dagan and Glickman, 2004). Recently, Castillo and Estrella (2012) present a new approach for MT evaluation based on the task of \u201cSemantic Textual Similarity\u201d.", "startOffset": 173, "endOffset": 532}, {"referenceID": 83, "context": "Further knowledge of paraphrase from the aspect of linguistics is introduced in the works of (McKeown, 1979; Meteer and Shaked, 1988; Barzilayand and Lee, 2003).", "startOffset": 93, "endOffset": 160}, {"referenceID": 101, "context": "(Snover et al., 2006) describe a new evaluation", "startOffset": 0, "endOffset": 21}, {"referenceID": 80, "context": "Furthermore, instead of using uniform weights, (Lo et al., 2012) weight the different types of semantic roles according to their relative importance to the adequate preservation of meaning, which is empiri-", "startOffset": 47, "endOffset": 64}, {"referenceID": 35, "context": "(Gamon et al., 2005) propose LMSVM, language-model, support vector machine, method investigating the possibility of evaluating", "startOffset": 0, "endOffset": 20}, {"referenceID": 36, "context": "In the latest version, they extended the permutation-tree (Gildea et al., 2006) into permutation-forests model (Stanojevi\u0107 and Sima\u2019an, 2014b), and showed stable good perfor-", "startOffset": 58, "endOffset": 79}, {"referenceID": 44, "context": "For instances, (Guzm\u00e1n et al., 2015; Guzmn et al., 2017) used neural networks for MTE for pair wise modeling to choose best hypothesis translation by comparing candidate translations with reference, integrating syntactic and semantic information into NNs.", "startOffset": 15, "endOffset": 56}, {"referenceID": 45, "context": "For instances, (Guzm\u00e1n et al., 2015; Guzmn et al., 2017) used neural networks for MTE for pair wise modeling to choose best hypothesis translation by comparing candidate translations with reference, integrating syntactic and semantic information into NNs.", "startOffset": 15, "endOffset": 56}, {"referenceID": 81, "context": "While (Ma et al., 2016) designed a new metric based on bi-directional LSTM, which is similar with the work of (Guzm\u00e1n et al.", "startOffset": 6, "endOffset": 23}, {"referenceID": 44, "context": ", 2016) designed a new metric based on bi-directional LSTM, which is similar with the work of (Guzm\u00e1n et al., 2015) but with less complexity by allowing the evaluation of single hypothesis with reference, instead of pairwise situation.", "startOffset": 94, "endOffset": 115}, {"referenceID": 64, "context": "If different MT systems produce translations with different qualities on a data set, how can we ensure that they indeed own different system quality? To explore this problem, (Koehn, 2004) performs a research work on the statistical significance test for machine translation evaluation.", "startOffset": 175, "endOffset": 188}, {"referenceID": 46, "context": "of a second alternative hypothesis (Hald, 1998).", "startOffset": 35, "endOffset": 47}, {"referenceID": 95, "context": "Pearson\u2019s correlation coefficient (Pearson, 1900) is commonly represented by the Greek letter \u03c1.", "startOffset": 34, "endOffset": 49}, {"referenceID": 14, "context": "is another algorithm to measure the correlations of automatic evaluation and manual judges, especially in recent years (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011).", "startOffset": 119, "endOffset": 235}, {"referenceID": 15, "context": "is another algorithm to measure the correlations of automatic evaluation and manual judges, especially in recent years (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011).", "startOffset": 119, "endOffset": 235}, {"referenceID": 16, "context": "is another algorithm to measure the correlations of automatic evaluation and manual judges, especially in recent years (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011).", "startOffset": 119, "endOffset": 235}, {"referenceID": 54, "context": "Kendall\u2019s \u03c4 (Kendall, 1938) has been used in recent years for the correlation between automatic order and reference order (Callison-Burch et al.", "startOffset": 12, "endOffset": 27}, {"referenceID": 16, "context": "Kendall\u2019s \u03c4 (Kendall, 1938) has been used in recent years for the correlation between automatic order and reference order (Callison-Burch et al., 2010; Callison-Burch et al., 2011; Callison-Burch et al., 2012).", "startOffset": 122, "endOffset": 209}, {"referenceID": 68, "context": "More concretely, (Lapata, 2003) proposes the use of Kendall\u2019s \u03c4 , a measure of rank correlation, estimating the distance between", "startOffset": 17, "endOffset": 31}, {"referenceID": 69, "context": "For example, (Callison-Burch et al., 2006b; Callison-Burch et al., 2007b; Lavie, 2013) mentioned that, through some qualitative analysis on", "startOffset": 13, "endOffset": 86}, {"referenceID": 40, "context": "much better than the traditional ones especially on the challenging sentence-level evaluation, though they are not popular yet such as nLEPOR and SentBLEU-Moses (Graham et al., 2015; Graham and Liu, 2016).", "startOffset": 161, "endOffset": 204}, {"referenceID": 65, "context": "Furthermore, the automatic evaluation metrics tend to ignore the relevance of words (Koehn, 2010), for instance, the name entities and core concepts are more important than punctuations and determiners but most automatic evaluation metrics put the same weight on each word of the sentences.", "startOffset": 84, "endOffset": 97}, {"referenceID": 92, "context": "For instance, what is the meaning of -16094 score by the MTeRater metric (Parton et al., 2011) or 1.", "startOffset": 73, "endOffset": 94}, {"referenceID": 47, "context": "find one interesting metric family LEPOR and hLEPOR (Han et al., 2012; Han, 2014) that can give a somehow meaningful score for a somehow recognized good translation, e.", "startOffset": 52, "endOffset": 81}, {"referenceID": 51, "context": "find one interesting metric family LEPOR and hLEPOR (Han et al., 2012; Han, 2014) that can give a somehow meaningful score for a somehow recognized good translation, e.", "startOffset": 52, "endOffset": 81}, {"referenceID": 65, "context": "mance towards metric;meaningful, score should give intuitive interpretation of translation quality;consistent, repeated use of metric should give same results;correct, metric must rank better systems higher as mentioned in (Koehn, 2010), of which the low cost, tunable and consistent characteristics are easily achieved by the metric developers, but the rest two goals (meaningful and correct) are usually the challenges in front of the NLP researchers.", "startOffset": 223, "endOffset": 236}, {"referenceID": 29, "context": "For instance, in the DARPA GALE report (Dorr et al., 2009), researchers first introduced the automatic", "startOffset": 39, "endOffset": 58}, {"referenceID": 89, "context": "Mrquez (Mrquez, 2013) introduced the Asiya online interface developed by their institute for MT output error analysis, where they also briefly mentioned the MT evaluation developments of", "startOffset": 7, "endOffset": 21}], "year": 2017, "abstractText": "We introduce the Machine Translation (MT) evaluation survey that contains both manual and automatic evaluation methods. The traditional human evaluation criteria mainly include the intelligibility, fidelity, fluency, adequacy, comprehension, and informativeness. The advanced human assessments include task-oriented measures, post-editing, segment ranking, and extended criteriea, etc. We classify the automatic evaluation methods into two categories, including lexical similarity scenario and linguistic features application. The lexical similarity methods contain edit distance, precision, recall, F-measure, and word order. The linguistic features can be divided into syntactic features and semantic features respectively. The syntactic features include part of speech tag, phrase types and sentence structures, and the semantic features include named entity, synonyms, textual entailment, paraphrase, semantic roles, and language models. The deep learning models for evaluation are very newly proposed. Subsequently, we also introduce the evaluation methods for MT evaluation including different correlation scores, and the recent quality estimation (QE) tasks for MT. This paper differs from the existing works (Dorr et al., 2009; EuroMatrix, 2007) from several aspects, by introducing some recent development of MT evaluation measures, the different classifications from manual to automatic evaluation measures, the introduction of recent QE tasks of MT, and the concise construction of the content. We hope this work will be helpful for MT researchers to easily pick up some metrics that are best suitable for their specific MT model development, and help MT evaluation researchers to get a general clue of how MT evaluation research developed. Furthermore, hopefully, this work can also shine some light on other evaluation tasks, except for translation, of NLP fields. 1", "creator": "LaTeX with hyperref package"}}}