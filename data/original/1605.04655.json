{"id": "1605.04655", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "Joint Learning of Sentence Embeddings for Relevance and Entailment", "abstract": "We consider the problem of Recognizing Textual Entailment within an Information Retrieval context, where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes/no answer to a binary natural language question.", "histories": [["v1", "Mon, 16 May 2016 05:50:54 GMT  (23kb)", "http://arxiv.org/abs/1605.04655v1", "submitted to repl4nlp workshop at ACL Berlin 2016"], ["v2", "Wed, 22 Jun 2016 22:41:26 GMT  (24kb)", "http://arxiv.org/abs/1605.04655v2", "repl4nlp workshop at ACL Berlin 2016"]], "COMMENTS": "submitted to repl4nlp workshop at ACL Berlin 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["petr baudis", "silvestr stanko", "jan sedivy"], "accepted": false, "id": "1605.04655"}, "pdf": {"name": "1605.04655.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["baudipet@fel.cvut.cz"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n04 65\n5v 1\n[ cs\n.C L\n] 1\n6 M\nay 2\nWe compare several variants of neural networks for sentence embeddings in a setting of decision-making based on evidence of varying relevance. We propose a basic model to integrate evidence for entailment, show that joint training of the sentence embeddings to model relevance and entailment is feasible even with no explicit perevidence supervision, and show the importance of evaluating strong baselines. We also demonstrate the benefit of carrying over text comprehension model trained on an unrelated task for our small datasets.\nOur research is motivated primarily by a new open dataset we introduce, consisting of binary questions and news-based evidence snippets. We also apply the proposed relevance-entailment model on a similar task of ranking multiple-choice test answers, evaluating it on a preliminary dataset of school test questions as well as the standard MCTest dataset, where we improve the neural model state-of-art."}, {"heading": "1 Introduction", "text": "Let us consider the goal of building machine reasoning systems based on knowledge from fulltext data like encyclopedic articles, scientific papers or news articles. Such machine reasoning systems, like humans researching a problem, must be\nable to recover evidence from large amounts of retrieved but likely irrelevant information and judge what answer the evidence entails to the question at hand.\nA typical approach, used implicitly in information retrieval (and its extensions, like IR-based Question Answering systems (Baudis\u030c, 2015)), is to determine evidence relevancy by a keyword overlap feature (like tf-idf or BM-25 (Robertson et al., 1995)) and prune the evidence by the relevancy score. On the other hand, textual entialment systems that seek to confirm hypotheses based on evidence (Dagan et al., 2006) (Marelli et al., 2014) (Bowman et al., 2015) are typically provided with only a single piece of evidence or only evidence pre-determined as relevant, and are often restricted to short and simple sentences without open-domain named entity occurences.\nIn this work, we seek to fuse information retrieval and textual entaiment recognition by defining the Argus Yes/No Question Answering task. The problem is, given a real-world event binary question like Did Donald Trump announce he is running for president? and numerous retrieved news article fragments as evidence, to determine the answer for the question. We further generalize this problem and propose a Hypothesis Evaluation where the questions and evidence can vary in form, size and vocabulary.\nOur research is motivated by the Argus automatic reporting system for the Augur prediction market paltform. (Baudis et al., 2016b) Therefore, we consider the question answering task within the constraints of a practical task that has limited available dataset and only minimum supervision. Hence, the evidence are authentic news sentences (with noise like segmentation errors, irrelevant participial phrases, etc.), and whereas we have gold standard for the correct answers, the model must do without explicit supervision on which in-\ndividual evidence snippets are relevant and what do they entail.\nTo this end, we introduce an open dataset of questions and newspaper evidence, and a neural model within the Sentence Pair Scoring framework (Baudis\u030c et al., 2016a) that (A) learns sentence embeddings for the question and evidence, (B) the embeddings represent both relevance and entailment characteristics as linear classifier inputs, and (C) the model aggregates all available evidence to produce a binary signal as the answer, which is the only training supervision.\nA related task concerns ranking answers of multiple-choice questions given a set of evidencing sentences. We evaluate our model on this task as well, considering the MCTest dataset and the AI2-8grade/CK12 dataset that we introduce below.\nThe paper is structured as follows. In Sec. 2, we formally outline the Argus question answering task, describe the question-evidence dataset, and describe the multiple-choice questions task and datasets. In Sec. 3, we briefly survey the related work on similar problems, whereas in Sec. 4 we propose our neural models for joint learning of sentence relevance and entailment. We present the results in Sec. 5 and conclude with a summary, model usage recommendations and future work directions in Sec. 6."}, {"heading": "2 The Hypothesis Evaluation Task", "text": "Formally, the Hypothesis Evaluation task is to build a function yi = fh(Hi), where yi \u2208 [0, 1] is a binary label (no towards yes) and Hi = (qi,Mi) is a hypothesis in the form of question text qi and a set of Ei = {eij} evidence texts eij as extracted from an evidence-carrying corpus."}, {"heading": "2.1 Argus Dataset", "text": "Our main aim is to propose a solution to the Argus Task, where the Argus system (Baudis, 2015) (Baudis et al., 2016b) is to automatically analyze and answer questions in the context of the Augur prediction market platform.1 In a prediction market, some users pose questions about the future whereas other users bet on the yes or no answer, with the assumption that the bet price reflects the real probability of the event. At a specified moment (e.g. after the date of a to-be-predicted sports\n1 https://augur.net/\nmatch), the correct answer is retroactively determined and the bets are paid off. At a larger volume of questions, determining the bet results may present a significant overhead for running of the market. This motivates the Argus system, which should partially automate this determination \u2014 deciding questions related to recent events based on open news sources.\nTo help develop a machine learning model for the fh function, we have created a dataset of questions with gold labels, and used this dataset with an IR information retrieval component of the Argus system to generate evidence texts from a variety of news papers. We release this dataset openly.2\nTo pose a reproducible task for the IR component, the time domain of questions was restricted from September 1, 2014 to September 1, 2015, and topic domain was focused to politics, sports and the stock market. To build the question dataset, we have used several sources:\n\u2022 We asked Amazon Mechanical Turk users to pose questions, together with a golden label and a news article reference. This seeded the dataset with initial, somewhat repetitive 250 questions.\n\u2022 We manually extended this dataset by derived questions with reversed polarity (to obtain an opposite answer).\n\u2022 We extended the data with questions autogenerated from templates, pertaining top sporting event winners and US senate or gubernatorial elections.\nTo build the evidence dataset, we used the Syphon component (Baudis et al., 2016b) of the stock Argus implementation3 to identify semantic roles of all question tokens and produce the search keywords if a role was assigned to each token. We then used the stock IR component to query a corpus of newspaper articles, and kept sentences that contained at least 2/3 of all the keywords. Our corpus of articles contained articles from The Guardian (all articles) and from the New York Times (Sports, Politics and Business sections). Furthermore, we scraped partial archive.org historical data out of 35 RSS feeds from CNN,\n2https://github.com/brmson/dataset-sts directory data/hypev/argus 3 https://github.com/AugurProject/argus\nReuters, BBC International, CBS News, ABC News, c\u2014net, Financial Times, Skynews and the Washington Post.\nFor the final dataset, we kept only questions where at least a single evidence was found (i.e. we successfuly assigned a role to each token, found some news stories and found at least one sentence with 2/3 of question keywords within). The final size of the dataset is outlined in Fig. 1."}, {"heading": "2.2 AI2-8grade/CK12 Dataset", "text": "The AI2 Elementary School Science Questions (no-diagrams variant)4 released by the Allen Institute cover 855 basic four-choice questions regarding high school science and follows up to the Allen AI Science Kaggle challenge.5 The vocabulary includes scientific jargon and named entities, and many questions are not factoid, requiring realworld reasoning or thought experiments.\nWe have combined each answer with the respective question (crudely by substituting wh-word in the question with the answer) and retrieved evidence sentences for each hypothesis using Solr search in a collection of CK-12 \u201cConcepts B\u201d textbooks.6 525 questions attained any supporting evidence.\nWe consider this dataset as preliminary, because it was not reviewed by a human and many hypotheses are apparently unprovable by the evidence we have gathered (i.e. the theoretical top accuracy is much lower than 1.0). However, we released it to the public7 and included it in the comparison as these qualities reflect many realistic datasets of unknown qualities, so we find relative performances of models on such datasets instructive.\n4 http://allenai.org/data.html 5https://www.kaggle.com/c/\nthe-allen-ai-science-challenge 6We have also tried English Wikipedia, but the dataset is much harder. 7 https://github.com/brmson/dataset-sts directory data/hypev/ai2-8grade"}, {"heading": "2.3 MCTest Dataset", "text": "The Machine Comprehension Test (Richardson et al., 2013) dataset has been introduced to provide a challenge for researchers to come up with models that approach human-level reading comprehension, and serve as a higher-level alternative to semantic parsing tasks that enforce a specific knowledge representation. The dataset consists of a set of 660 stories spanning multiple sentences, written in simple and clean language (but with less restricted vocabulary than e.g. the bAbI dataset (Weston et al., 2015)). Each story is accompanied by four questions, each with four possible answers; the questions are tagged as based on just a single in-story sentence, or requiring multiple sentence inference. We use an official extension of the dataset for RTE evaluation that replaces answers by full statements that combine each answer with the question.\nThe dataset is split in two parts, MC-160 and MC-500, based on provenance but similar in quality. We always train the model on a joined training set.\nThe practical setting differs from the Argus task as the MCTest dataset contains relatively restricted vocabulary and well-formed sentences; furthermore, the goal is to find the single key point in the story to focus on, while in the Argus setting we may have many pieces of evidence supporting an answer. Finally, a specific characteristics of MCTest is that it consists of stories where the ordering and proximity of evidence sentences matters."}, {"heading": "3 Related Work", "text": "Our primary concern when integrating natural language query and textual evidence is to build sentence-level representations that can be used both for relevance weighing and answer prediction.\nSentence-level representations in the retrieval + inference context have been popularly proposed within the Memory Network framework (Weston et al., 2014), but just averaged word embeddings are explored; the task includes only very simple sentences and a small vocabulary. Much more realistic setting is introduced in the Answer Sentence Selection context (Wang et al., 2007) (Baudis\u030c et al., 2016a), with state-of-art models using complex deep neural architectures with attention (dos Santos et al., 2016), but ths selection task\nconsists of only retrieval and no inference (answer prediction). A more indirect retrieval task regarding news summarization was investigated by (Cao et al., 2016).\nIn the entailment context, (Bowman et al., 2015) introduced a large dataset with single-evidence sentence pairs (Stanford Natural Language Inference, SNLI), but a larger vocabulary and slightly more complicated (but still conservatively formed) sentences They also proposed baseline recurrent neural model for modeling sentence representations, while word-level attention based models are studied more recently (Rockta\u0308schel et al., 2015) (Cheng et al., 2016).\nIn the MCTest text comprehension challenge (Richardson et al., 2013), the best models use complex feature engineering ensembling multiple traditional semantic NLP approaches (Wang and McAllester, 2015), while the best deep model so far of (Yin et al., 2016) uses convolutional neural networks to build sentence representations, and attention on multiple levels to select evidencing sentences."}, {"heading": "4 Neural Model", "text": "Our approach is to use a sequence of word embeddings to build sentence embeddings for each hypothesis and respective evidence, then use the sentence embeddings to estimate relevance and entailment of each evidence with regard to the respective hypothesis, and integrate the evidence by different strategies."}, {"heading": "4.1 Sentence Embeddings", "text": "To produce sentence embeddings, we investigated the usage of neural models of the dataset-sts framework for deep learning of sentence pair scoring functions. (Baudis\u030c et al., 2016a)\nWe refer the reader to (Baudis\u030c et al., 2016a) and its references for detailed model descriptions. We evaluate an RNN model which uses bidirectionally summed GRU memory cells (Cho et al., 2014) and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings (Kim, 2014); a RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings (Tan et al., 2015); and an attn1511 model inspired by (Tan et al., 2015) that integrates the RNN-CNN model with per-word attention to build hypothesis-specific evidence embed-\ndings. We also report the baseline results of avg mean of word embeddings in the sentence with projection matrix and DAN Deep Averaging Network model that employs word-level dropout and adds multiple nonlinear transformations on top of the averaged sentence emebddings (Iyyer et al., 2015).\nThe original attn1511 model (Baudis\u030c et al., 2016a) (as tuned for the Answer Sentence Selection task) used a softmax attention mechanism that effectively selects only a few key words of the sentence to focus on \u2014 for a hypothesis-evidence token t scalar attention score ah,e(t), the focus sh,e(t) is:\nsh,e(t) = exp(ah,e(t))/ \u2211\nt\u2032\nexp(ah,e(t \u2032))\nA different focus mechanism exhibited better performance in the Hypothesis Evaluation task that models per-token attention more independently:\nsh,e(t) = \u03c3(ah,e(t))/max t\u2032\n\u03c3(ah,e(t \u2032))\nWe also use relu instead of tanh as the CNN transfer function.\nAs model input, we use the standard GloVe embeddings (Pennington et al., 2014) extended with binary inputs denoting token type and overlap with token or bigram in the paired sentence, as described in (Baudis\u030c et al., 2016a). However, we introduce two changes to the word embedding model \u2014 we use 50-dimensional embeddings instead of 300-dimensional, and rather than building an adaptable embedding matrix only from the training set words preinitialized by GloVe, we use only the top 100 most frequent tokens in the adaptable embedding matrix and use fixed GloVe vectors for all other tokens (including tokens not found in the training set). This should improve behavior for highly vocabulary-rich tasks like Argus, while still allowing the high-frequency tokens (like interpunction or conjunctions) to learn semantic operator representations.\nAs an additional method for producing sentence embeddings, we consider the Ubu. RNN transfer learning method proposed by (Baudis\u030c et al., 2016a) where an RNN model (as described above) is trained on the Ubuntu Dialogue task (Lowe et al., 2015).8 The pretrained model weights are\n8The Ubuntu Dialogue dataset consists of one million chat dialog contexts, learning to rank candidates for the next utter-\nused to initialize an RNN model which is then fine-tuned on the Hypothesis Evaluation task. We use the same model as originally proposed (except the aforementioned vocabulary handling modification), with the dot-product scoring used for Ubuntu Dialogue training replaced by MLP pointscores described below."}, {"heading": "4.2 Evidence Integration", "text": "Our main proposed schema for evidence integration is Evidence Weighing. From each pair of hypothesis and evidence embeddings,9 we produce two [0, 1] predictions using a pair of MLP point-scorers of dataset-sts (Baudis\u030c et al., 2016a)10 with sigmoid activation function. The predictions are interpreted as Ci \u2208 [0, 1] entailment (0 to 1 as no to yes) and relevance Ri \u2208 [0, 1]. To integrate the predictions across multiple pieces of evidence, we propose a weighed average model:\ny = \u2211 iCiRi\u2211 iRi\nWe do not have access to any explicit labels for the evidence, but we train the model end-to-end with just y labels and the formula for y is differentiable, carrying over the gradient to the sentence embedding model. This can be thought of as a simple passage-wide attention model.\nAs a baseline strategy, we also consider Evidence Averaging, where we simply produce a single scalar prediction per hypothesis-evidence pair (using the same strategy as above) and decide the hypothesis simply based on the mean prediction across available evidence.\nFinally, following success reported in the Answer Sentence Selection task (Baudis\u030c et al., 2016a), we consider a BM25 Feature combined with Evidence Averaging, where the MLP scorer producing the pair scalar prediction as above has an additional BM25 word overlap score input (Robertson et al., 1995) besides the elementwise embedding comparisons.\nance in the dialog; the sentences are based on IRC chat logs of the Ubuntu community technical support channels and contain casually typed interactions regarding computer-related problems, resembling tweet data, but longer and with heavily technical jargon.\n9We employ Siamese training, sharing the weights between hypothesis and evidence embedding models.\n10From the elementwise product and sum of the embeddings, a linear classifier directly produces a prediction; contrary to typical setup, we use no hidden layer."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "We implement the differentiable model in the Keras framework (Chollet, 2015) and train the whole network from word embeddings to output evidence-integrated hypothesis label using the binary cross-entropy loss as an objective11 and the Adam optimization algorithm (Kingma and Ba, 2014). We apply L2 = 10\u22124 regularization and a p = 1/3 dropout.\nFollowing the recommendation of (Baudis\u030c et al., 2016a), we report expected test set question accuracy12 as determined by average accuracy in 16-way independent training runs and with 95% confidence intervals based on the Student\u2019s t-distribution."}, {"heading": "5.2 Evaluation", "text": "In Fig. 2, we report the model performance on the Argus task, showing that the Ubuntu Dialogue transfer RNN outperforms other proposed models by a large margin. However, a comparison of evidence integration approaches in Fig. 3 shows that contrary to our expectations, evidence integration is not the major deciding factor and there are no staticially meaningful differences between the evaluated approaches.\n11Unlike (Yin et al., 2016), we have found ranking-based loss functions ineffective for this task.\n12In the MCTest and AI2-8grade/CK12 datasets, we test and rank four hypotheses per question, whereas in the Argus dataset, each hypothesis is a single question.\nIn Fig. 4, we look at the model performance on the AI2-8grade/CK12 task, repeating the story of Ubuntu Dialogue transfer RNN dominating other models. However, on this task our proposed evidence weighing scheme improves over simpler approaches \u2014 but just on the best model, as shown in Fig. 5. On the other hand, the simplest averaging model benefits from at least BM25 information to select relevant evidence, apparently.\nIn Fig. 6, we compare our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features (Wang and McAllester, 2015), as well as past neural models from the literature, all using attention mechanisms \u2014 the Attentive Reader of (Hermann et al., 2015), Neural Reasoner of (Peng et al., 2015) and the HABCNN model family of (Yin et al., 2016).13 We see that averaging-based models are surprisingly effective on this task, and in particu-\n13(Yin et al., 2016) also reports the results on the former models.\nlar on the MC-500 dataset it can beat even the best so far reported model of HABCNN-TE. Our proposed transfer model is statistically equivalent to the best model on both datasets (furthermore, previous work did not include confidence intervals, even though their models should also be stochastically initialized).\nAs expected, our models did badly on the multiple-evidence class of questions \u2014 we made no attempt to model information flow across adjacent evidence in our models since this aspect is unique to the MCTest task in the context of this work.\nInterestingly, on the MCTest task evidence weighing does play an important role as shown in Fig. 7, significantly boosting model accuracy. This confirms that a way to allocate attention to different sentences is indeed crucial for this task."}, {"heading": "5.3 Analysis", "text": "While we can universally proclaim Ubu. RNN as the best model, we observe many aspects of the Hypothesis Evaluation problem that are shared by the AI2-8grade/CK12 and MCTest tasks, but not by the Argus task.\nOur largest surprise lies in the ineffectivity of evidence weighing on the Argus task, since observations of irrelevant passages initially led us to investigate this model. We may also see that nonpretrained RNN does very well on the Argus task while CNN is a better model otherwise.\nAn aspect that could explain this rift is that the latter two tasks are primarily retrieval based, where we seek to judge each evidence as irrelevant or essentially a paraphrase of the hypothesis. On the other hand, the Argus task is highly semantic and compositional, with the questions often differing just by a presence of negation \u2014 recurrent model that can capture long-term dependencies and alter sentence representations based on the presence of negation may represent an essential improvement over an n-gram-like convolutional scheme. The lack of success of evidence weighing in the Argus task might be also attributed to a more conservative scheme of passage retrieval employed in the IR pipeline that produced the dataset.\nWe see from training versus test accuracies that RNN-based models (including the word-level attention model) have a strong tendency to overfit on our small datasets, while CNN is much more resilient. While word-level attention seems appealing for such a task, we speculate that we simply might not have enough training data to properly\ntrain it.14 Investigating attention transfer is a point for future work \u2014 our preliminary experiments on multiple datasets indicate that attention models are more task specific than the recurrent text comprehension models of memory cell based RNNs.\nOne concrete limitation of our models in case of the Argus task is a problem of reconciling particular named entity instances. The more obvious form of this issue is Had Roger Federer beat Martin Cilic in US OPEN 2014? versus an opposite Had Martin Cilic beat Roger Federer in US OPEN 2014? \u2014 another form of this problem is reconciling a hypothesis like Will the Royals win the World Series? with evidence Giants Win World Series With Game 7 Victory Over Royals. An abstract embedding of the sentence will not carry over the required information \u2014 it is important to explicitly pass and reconcile the roles of multiple named entities which cannot be meaningfully embedded in a GloVe-like semantic vector space.\n14Just reducing the dimensionality of hidden representations did not yield an improvement."}, {"heading": "6 Conclusion", "text": "We have established a general Hypothesis Evaluation task with three datasets of various properties, and shown that neural models can exhibit strong performance (with less hand-crafting effort than non-neural classifiers). We propose an evidence weighing model that is never harmful and improves performance on some tasks. We also demonstrate that simple models can outperform or closely match performance of complex architectures; all the models we consider are taskindependent and were successfully used in different contexts than Hypothesis Evaluation (Baudis\u030c et al., 2016a). Our results empirically show that a basic RNN text comprehension model well trained on a large dataset (even if the task is unrelated and vocabulary characteristics are very different) outperforms or matches more complex architectures trained only on the dataset of the task at hand.15\nFinally, on the MCTest dataset, our best proposed model is better of statistically indistinguishable from the best neural network model reported so far (Yin et al., 2016), even though it has a simpler architecture and only a naive attention mechanism.\nWe would like to draw several recommendations for future research from our findings: (A) encourage usage of basic neural architectures as evaluation baselines; (B) suggest that future research includes models pretrained on large data as baselines; (C) validate complex architectures on tasks with large datasets if they cannot beat baselines on small datasets; and (D) for randomized machine comprehension models (e.g. neural networks with random weight initialization, batch shuffling or probabilistic dropout), report expected test set performance based on multiple independent training runs.\nAs a general advice for solving complex tasks with small datasets, besides the point (B) above our analysis suggests convolutional networks as the best models regarding the tendency to overfit, unless semantic composionality plays a crucial role in the task; in this scenario, simple averagingbased models are a great start as well. Preinitializing a model also helps against overfitting.\nWe release our implementation of the Argus task, evidence integration models and processing\n15Even if these use multi-task learning, which was employed in case of the HABCNN models that were trained to also predict question classes.\nof all the evaluated datasets as open source.16\nWe believe the next step towards machine comprehension NLP models that are based on deep learning but capable of dealing with real-world, large-vocabulary data will involve research into a better way to deal with entities without available embeddings, as mentioned in the analysis above. In the realm of embeddings, simple word-level attention mechanisms will not do. A promising approach could extend the flexibility of the final sentence representation, moving from attention mechanism to a memory mechanism17 by allowing the network to remember a set of \u201cfacts\u201d derived from each sentence; related work has been done for example on end-to-end differentiable shift-reduce parsers with LSTM as stack cells (Dyer et al., 2015)."}, {"heading": "Acknowledgments", "text": "This work was co-funded by the Augur Project of the Forecast Foundation and financially supported by the Grant Agency of the Czech Technical University in Prague, grant No. SGS16/ 084/OHK3/1T/13. Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085, provided under the programme \u201cProjects of Large Research, Development, and Innovations Infrastructures.\u201d\nWe\u2019d like to thank Peronet Despeignes of the Augur\nProject for his support. Carl Burke has provided instructions\nfor searching CK-12 ebooks within the Kaggle challenge."}], "references": [{"title": "Sentence pair scoring: Towards unified framework for text comprehension", "author": ["Petr Baudi\u0161", "Jan Pichl", "Tom\u00e1\u0161 Vysko\u010dil", "Jan Sediv\u00fd."], "venue": "CoRR, abs/1603.06127.", "citeRegEx": "Baudi\u0161 et al\\.,? 2016a", "shortCiteRegEx": "Baudi\u0161 et al\\.", "year": 2016}, {"title": "2016b. Argus: An artificial-intelligence assistant for augur\u2019s prediction market platform reporters", "author": ["Petr Baudis", "Silvestr Stanko", "Peronet Despeignes"], "venue": null, "citeRegEx": "Baudis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Baudis et al\\.", "year": 2016}, {"title": "Argus: Deciding questions about events", "author": ["Petr Baudis"], "venue": null, "citeRegEx": "Baudis.,? \\Q2015\\E", "shortCiteRegEx": "Baudis.", "year": 2015}, {"title": "YodaQA: A Modular Question Answering System Pipeline", "author": ["Petr Baudi\u0161."], "venue": "POSTER 2015 - 19th International Student Conference on Electrical Engineering.", "citeRegEx": "Baudi\u0161.,? 2015", "shortCiteRegEx": "Baudi\u0161.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on https://github.com/brmson/dataset-sts", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Attsum: Joint learning of focusing and summarization with neural attention", "author": ["Ziqiang Cao", "Wenjie Li", "Sujian Li", "Furu Wei."], "venue": "arXiv preprint arXiv:1604.00125.", "citeRegEx": "Cao et al\\.,? 2016", "shortCiteRegEx": "Cao et al\\.", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "CoRR, abs/1601.06733.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "CoRR, abs/1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177\u2013", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "CoRR, abs/1505.08075.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "CoRR, abs/1506.08909.", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Towards neural network-based reasoning", "author": ["Baolin Peng", "Zhengdong Lu", "Hang Li", "Kam-Fai Wong."], "venue": "arXiv preprint arXiv:1508.05508.", "citeRegEx": "Peng et al\\.,? 2015", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": null, "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Okapi at trec-3", "author": ["Stephen E Robertson", "Steve Walker", "Susan Jones"], "venue": null, "citeRegEx": "Robertson et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Robertson et al\\.", "year": 1995}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom."], "venue": "CoRR, abs/1509.06664.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2015", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Lstmbased deep learning models for non-factoid answer selection", "author": ["Ming Tan", "Bing Xiang", "Bowen Zhou."], "venue": "CoRR, abs/1511.04108.", "citeRegEx": "Tan et al\\.,? 2015", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "Machine comprehension with syntax, frames, and semantics", "author": ["Hai Wang", "Mohit Bansal Kevin Gimpel David McAllester."], "venue": "Proceedings of ACL, Volume 2: Short Papers:700.", "citeRegEx": "Wang and McAllester.,? 2015", "shortCiteRegEx": "Wang and McAllester.", "year": 2015}, {"title": "What is the jeopardy model? a quasisynchronous grammar for qa", "author": ["Mengqiu Wang", "Noah A Smith", "Teruko Mitamura."], "venue": "EMNLP-CoNLL, volume 7, pages 22\u201332.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "CoRR, abs/1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "CoRR, abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Attention-based convolutional neural network for machine comprehension", "author": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze."], "venue": "CoRR, abs/1602.04341.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "A typical approach, used implicitly in information retrieval (and its extensions, like IR-based Question Answering systems (Baudi\u0161, 2015)), is to determine evidence relevancy by a keyword overlap feature (like tf-idf or BM-25 (Robertson et al.", "startOffset": 123, "endOffset": 137}, {"referenceID": 20, "context": "A typical approach, used implicitly in information retrieval (and its extensions, like IR-based Question Answering systems (Baudi\u0161, 2015)), is to determine evidence relevancy by a keyword overlap feature (like tf-idf or BM-25 (Robertson et al., 1995)) and prune the evidence by the relevancy score.", "startOffset": 226, "endOffset": 250}, {"referenceID": 9, "context": "On the other hand, textual entialment systems that seek to confirm hypotheses based on evidence (Dagan et al., 2006) (Marelli et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 16, "context": ", 2006) (Marelli et al., 2014) (Bowman et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 4, "context": ", 2014) (Bowman et al., 2015) are typically provided with only a single piece of evidence or only evidence pre-determined as relevant, and are often restricted to short and simple sentences without open-domain named entity occurences.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "To this end, we introduce an open dataset of questions and newspaper evidence, and a neural model within the Sentence Pair Scoring framework (Baudi\u0161 et al., 2016a) that (A) learns sentence embeddings for the question and evidence, (B) the embeddings represent both relevance and entailment characteristics as linear classifier inputs, and (C) the model aggregates all available evidence to produce a binary signal as the answer, which is the only training supervision.", "startOffset": 141, "endOffset": 163}, {"referenceID": 2, "context": "Our main aim is to propose a solution to the Argus Task, where the Argus system (Baudis, 2015) (Baudis et al.", "startOffset": 80, "endOffset": 94}, {"referenceID": 19, "context": "The Machine Comprehension Test (Richardson et al., 2013) dataset has been introduced to provide a challenge for researchers to come up with models that approach human-level reading comprehension, and serve as a higher-level alternative to semantic parsing tasks that enforce a specific knowledge representation.", "startOffset": 31, "endOffset": 56}, {"referenceID": 26, "context": "the bAbI dataset (Weston et al., 2015)).", "startOffset": 17, "endOffset": 38}, {"referenceID": 25, "context": "Sentence-level representations in the retrieval + inference context have been popularly proposed within the Memory Network framework (Weston et al., 2014), but just averaged word embeddings are explored; the task includes only very simple sentences and a small vocabulary.", "startOffset": 133, "endOffset": 154}, {"referenceID": 24, "context": "Much more realistic setting is introduced in the Answer Sentence Selection context (Wang et al., 2007) (Baudi\u0161 et al.", "startOffset": 83, "endOffset": 102}, {"referenceID": 0, "context": ", 2007) (Baudi\u0161 et al., 2016a), with state-of-art models using complex deep neural architectures with attention (dos Santos et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 5, "context": "A more indirect retrieval task regarding news summarization was investigated by (Cao et al., 2016).", "startOffset": 80, "endOffset": 98}, {"referenceID": 4, "context": "In the entailment context, (Bowman et al., 2015) introduced a large dataset with single-evidence sentence pairs (Stanford Natural Language Inference, SNLI), but a larger vocabulary and slightly more complicated (but still conservatively formed) sentences They also proposed baseline recurrent neural model for modeling sentence representations, while word-level attention based models are studied more recently (Rockt\u00e4schel et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 21, "context": ", 2015) introduced a large dataset with single-evidence sentence pairs (Stanford Natural Language Inference, SNLI), but a larger vocabulary and slightly more complicated (but still conservatively formed) sentences They also proposed baseline recurrent neural model for modeling sentence representations, while word-level attention based models are studied more recently (Rockt\u00e4schel et al., 2015) (Cheng et al.", "startOffset": 370, "endOffset": 396}, {"referenceID": 6, "context": ", 2015) (Cheng et al., 2016).", "startOffset": 8, "endOffset": 28}, {"referenceID": 19, "context": "In the MCTest text comprehension challenge (Richardson et al., 2013), the best models use complex feature engineering ensembling multiple traditional semantic NLP approaches (Wang and McAllester, 2015), while the best deep model so far of (Yin et al.", "startOffset": 43, "endOffset": 68}, {"referenceID": 23, "context": ", 2013), the best models use complex feature engineering ensembling multiple traditional semantic NLP approaches (Wang and McAllester, 2015), while the best deep model so far of (Yin et al.", "startOffset": 113, "endOffset": 140}, {"referenceID": 27, "context": ", 2013), the best models use complex feature engineering ensembling multiple traditional semantic NLP approaches (Wang and McAllester, 2015), while the best deep model so far of (Yin et al., 2016) uses convolutional neural networks to build sentence representations, and attention on multiple levels to select evidencing sentences.", "startOffset": 178, "endOffset": 196}, {"referenceID": 0, "context": "(Baudi\u0161 et al., 2016a)", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "We refer the reader to (Baudi\u0161 et al., 2016a) and its references for detailed model descriptions.", "startOffset": 23, "endOffset": 45}, {"referenceID": 7, "context": "We evaluate an RNN model which uses bidirectionally summed GRU memory cells (Cho et al., 2014) and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings (Kim, 2014); a RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings (Tan et al.", "startOffset": 76, "endOffset": 94}, {"referenceID": 13, "context": ", 2014) and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings (Kim, 2014); a RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings (Tan et al.", "startOffset": 128, "endOffset": 139}, {"referenceID": 22, "context": ", 2014) and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings (Kim, 2014); a RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings (Tan et al., 2015); and an attn1511 model inspired by (Tan et al.", "startOffset": 240, "endOffset": 258}, {"referenceID": 22, "context": ", 2015); and an attn1511 model inspired by (Tan et al., 2015) that integrates the RNN-CNN model with per-word attention to build hypothesis-specific evidence embeddings.", "startOffset": 43, "endOffset": 61}, {"referenceID": 12, "context": "We also report the baseline results of avg mean of word embeddings in the sentence with projection matrix and DAN Deep Averaging Network model that employs word-level dropout and adds multiple nonlinear transformations on top of the averaged sentence emebddings (Iyyer et al., 2015).", "startOffset": 262, "endOffset": 282}, {"referenceID": 0, "context": "The original attn1511 model (Baudi\u0161 et al., 2016a) (as tuned for the Answer Sentence Selection task) used a softmax attention mechanism that effectively selects only a few key words of the sentence to focus on \u2014 for a hypothesis-evidence token t scalar attention score ah,e(t), the focus sh,e(t) is:", "startOffset": 28, "endOffset": 50}, {"referenceID": 18, "context": "As model input, we use the standard GloVe embeddings (Pennington et al., 2014) extended with binary inputs denoting token type and overlap with token or bigram in the paired sentence, as described in (Baudi\u0161 et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 0, "context": ", 2014) extended with binary inputs denoting token type and overlap with token or bigram in the paired sentence, as described in (Baudi\u0161 et al., 2016a).", "startOffset": 129, "endOffset": 151}, {"referenceID": 0, "context": "RNN transfer learning method proposed by (Baudi\u0161 et al., 2016a) where an RNN model (as described above) is trained on the Ubuntu Dialogue task (Lowe et al.", "startOffset": 41, "endOffset": 63}, {"referenceID": 15, "context": ", 2016a) where an RNN model (as described above) is trained on the Ubuntu Dialogue task (Lowe et al., 2015).", "startOffset": 88, "endOffset": 107}, {"referenceID": 0, "context": "From each pair of hypothesis and evidence embeddings,9 we produce two [0, 1] predictions using a pair of MLP point-scorers of dataset-sts (Baudi\u0161 et al., 2016a)10 with sigmoid activation function.", "startOffset": 138, "endOffset": 160}, {"referenceID": 0, "context": "Finally, following success reported in the Answer Sentence Selection task (Baudi\u0161 et al., 2016a), we consider a BM25 Feature combined with Evidence Averaging, where the MLP scorer producing the pair scalar prediction as above has an additional BM25 word overlap score input (Robertson et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 20, "context": ", 2016a), we consider a BM25 Feature combined with Evidence Averaging, where the MLP scorer producing the pair scalar prediction as above has an additional BM25 word overlap score input (Robertson et al., 1995) besides the elementwise embedding comparisons.", "startOffset": 186, "endOffset": 210}, {"referenceID": 8, "context": "We implement the differentiable model in the Keras framework (Chollet, 2015) and train the whole network from word embeddings to output evidence-integrated hypothesis label using the binary cross-entropy loss as an objective11 and the Adam optimization algorithm (Kingma and Ba, 2014).", "startOffset": 61, "endOffset": 76}, {"referenceID": 14, "context": "We implement the differentiable model in the Keras framework (Chollet, 2015) and train the whole network from word embeddings to output evidence-integrated hypothesis label using the binary cross-entropy loss as an objective11 and the Adam optimization algorithm (Kingma and Ba, 2014).", "startOffset": 263, "endOffset": 284}, {"referenceID": 0, "context": "Following the recommendation of (Baudi\u0161 et al., 2016a), we report expected test set question accuracy12 as determined by average accuracy in 16-way independent training runs and with 95% confidence intervals based on the Student\u2019s t-distribution.", "startOffset": 32, "endOffset": 54}, {"referenceID": 27, "context": "Unlike (Yin et al., 2016), we have found ranking-based loss functions ineffective for this task.", "startOffset": 7, "endOffset": 25}, {"referenceID": 23, "context": "6, we compare our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features (Wang and McAllester, 2015), as well as past neural models from the literature, all using attention mechanisms \u2014 the Attentive Reader of (Hermann et al.", "startOffset": 127, "endOffset": 154}, {"referenceID": 11, "context": "6, we compare our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features (Wang and McAllester, 2015), as well as past neural models from the literature, all using attention mechanisms \u2014 the Attentive Reader of (Hermann et al., 2015), Neural Reasoner of (Peng et al.", "startOffset": 264, "endOffset": 286}, {"referenceID": 17, "context": ", 2015), Neural Reasoner of (Peng et al., 2015) and the HABCNN model family of (Yin et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 27, "context": ", 2015) and the HABCNN model family of (Yin et al., 2016).", "startOffset": 39, "endOffset": 57}, {"referenceID": 27, "context": "(Yin et al., 2016) also reports the results on the former models.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "We also demonstrate that simple models can outperform or closely match performance of complex architectures; all the models we consider are taskindependent and were successfully used in different contexts than Hypothesis Evaluation (Baudi\u0161 et al., 2016a).", "startOffset": 232, "endOffset": 254}, {"referenceID": 27, "context": "Finally, on the MCTest dataset, our best proposed model is better of statistically indistinguishable from the best neural network model reported so far (Yin et al., 2016), even though it has a simpler architecture and only a naive attention mechanism.", "startOffset": 152, "endOffset": 170}, {"referenceID": 10, "context": "A promising approach could extend the flexibility of the final sentence representation, moving from attention mechanism to a memory mechanism17 by allowing the network to remember a set of \u201cfacts\u201d derived from each sentence; related work has been done for example on end-to-end differentiable shift-reduce parsers with LSTM as stack cells (Dyer et al., 2015).", "startOffset": 339, "endOffset": 358}], "year": 2017, "abstractText": "We consider the problem of Recognizing Textual Entailment within an Information Retrieval context, where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes/no answer to a binary natural language question. We compare several variants of neural networks for sentence embeddings in a setting of decision-making based on evidence of varying relevance. We propose a basic model to integrate evidence for entailment, show that joint training of the sentence embeddings to model relevance and entailment is feasible even with no explicit perevidence supervision, and show the importance of evaluating strong baselines. We also demonstrate the benefit of carrying over text comprehension model trained on an unrelated task for our small datasets. Our research is motivated primarily by a new open dataset we introduce, consisting of binary questions and news-based evidence snippets. We also apply the proposed relevance-entailment model on a similar task of ranking multiple-choice test answers, evaluating it on a preliminary dataset of school test questions as well as the standard MCTest dataset, where we improve the neural model state-of-art.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}