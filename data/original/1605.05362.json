{"id": "1605.05362", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Yelp Dataset Challenge: Review Rating Prediction", "abstract": "Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user's star rating for a product, given the user's text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) Naive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models.", "histories": [["v1", "Tue, 17 May 2016 20:52:33 GMT  (184kb,D)", "http://arxiv.org/abs/1605.05362v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["nabiha asghar"], "accepted": false, "id": "1605.05362"}, "pdf": {"name": "1605.05362.pdf", "metadata": {"source": "META", "title": "Yelp Dataset Challenge: Review Rating Prediction", "authors": ["Nabiha Asghar"], "emails": ["nasghar@uwaterloo.ca"], "sections": [{"heading": "1. Introduction", "text": "User reviews are an integral part of web services like TripAdvisor, Amazon, Epinions and Yelp, where users can post their opinions about businesses, products and services through reviews consisting of free-form text and a numeric star rating, usually out of 5. These online reviews function as the \u2018online word-of-mouth\u2019 (Dellarocas, 2003) and a criterion for consumers to choose between similar products. Studies (e.g. (Chen et al., 2003)) show that they have a significant impact\non consumer purchase decisions as well as on product sales and business revenues. A user review typically looks like this:\nOn famous websites like Amazon and Yelp, many products and businesses receive tens or hundreds of reviews, making it impossible for readers to read all of them. Generally, readers prefer to look at the star ratings only and ignore the text. However, the relationship between the text and the rating is not obvious, as illustrated in Figure 1. In particular, several questions may be asked: why exactly did this reviewer give the restaurant 3/5 stars? In addition to the quality of food, variety, size and service time, what other features of the restaurant did the user implicitly consider, and what was the relative importance given to each of them? How does this relationship change if we consider a different user\u2019s rating and text review?\nThe process of predicting this relationship for a generic user (but for a specific product/business) is called Review Rating Prediction. Concretely, given the set S = {(r1, s1), ..., (rN , sN )} for a product P , where ri is the i\u2019th user\u2019s text review of P and si is the i\u2019th user\u2019s numeric rating for P , the goal is to learn the best mapping from a word vector r to a numeric rating s. Review Rating Prediction is a useful problem to solve, because it can help us decide whether it is enough to look at the star ratings of a product and ignore its textual reviews. Moreover, some review websites allow users to write text reviews without specifying a star rating. In these cases, Review Rating Prediction comes in handy. However, it is a hard problem be-\nar X\niv :1\n60 5.\n05 36\n2v 1\n[ cs\n.C L\n] 1\n7 M\nay 2\n01 6\ncause two users who give a product the same rating, may have very different reasons for doing so. User A may give a restaurant 2/5 stars because it does not have free wifi and free parking, even though the food is good. User B may give the same restaurant a rating of 2/5 because he does not care about the wifi and parking, and thinks that the food is below average. Therefore, the main challenge in building a good predictor is to effectively extract useful features of the product from the text reviews and to then quantify their relative importance with respect to the rating.\nIn this paper, we treat Review Rating Prediction Problem as a multi-class classification problem in Machine Learning, where the class labels are the star ratings. We combine four feature extraction methods, unigrams, bigrams, trigrams and Latent Semantic Indexing, with four supervised learning algorithms, logistic regression, Na\u0308\u0131ve Bayes classification, perceptrons and linear support vector classification to build sixteen prediction models. We train and evaluate the performance of each of these models on the dataset provided by Yelp. The rest of the paper is organized as follows. Section 2 and 3 provide the details of the related work and the dataset. Section 4 describes all the feature extraction methods and supervised learning algorithms, and section 6 provides detailed results and analysis. We end with concluding remarks and future work."}, {"heading": "2. Related Work", "text": "Most of the recent work related to review rating prediction relies on sentiment analysis to extract features from the review text. Qu et al. (2010) tackle this problem for Amazon.com reviews, by proposing a novel feature extraction method called bag-of-opinions, which extracts opinions (consisting of a root word, a modifier and/or a negation word) from the review corpus, computes their sentiment score, and predicts a review\u2019s rating by aggregating the scores of opinions present in that review and combining it with a domain-dependent unigrams model.\nLeung et al. (2006) use a novel relative frequency method to create an opinion dictionary, in order to infer review ratings from the review text. This method estimates the strength of a word with respect to a certain sentiment class as the relative frequency of its occurence in that class. They integrate this inference technique with collaborative filtering algorithms and test their method on movie reviews from IMDb on a 2-point rating scale.\nFan and Khademi (2014) predict a restaurant\u2019s average star rating on Yelp from its reviews (note that this is\nbusiness rating prediction, and is different from review rating prediction). They combine the unigrams model with feature engineering methods such as Parts-ofSpeech tagging, and use linear regression, support vector regression and decision trees for prediction. Their dataset consists of 4243 restaurants and 35645 text reviews, and is much smaller than the one we use in this paper. Li et al. (2011) perform rating prediction for reviews on Epinions.com by extracting additional features of the reviewer and the product/business being reviewed. Ganu et al. (2009) propose a method to use the text of the reviews to improve recommendor systems, like the ones used by Netflix, which often rely solely on the structured metadata information of the product/business and the star ratings. Their method relies on machine learning, sentiment analysis techniques and natural language processing to classify sentences as positive, negative, neutral or conflict. It is shown, using restaurant reviews from Citysearch New York, that the review text is a better indicator of the sentiment of the review than the coarse star rating.\nIn this paper, we concern ourselves only with the semantic analysis of the review text and do not deal with the sentiment analysis."}, {"heading": "3. Data Description", "text": "We use the dataset provided by Yelp as part of their Dataset Challenge 2014 (Dataset, 2014) for training and testing the prediction models. The dataset includes data from Phoenix, Las Vegas, Madison, Waterloo and Edinburgh, and contains information about 42,153 businesses, 320,002 business attributes, 31,617 check-in sets, 403,210 tips and 1,125,458 text reviews.\nConcretely, the dataset consists of five files, one for each object type: business, review, user, check-in and tip. Each file consists of one json-object-per-line. Thus, a business is represented in the \u2018business.json\u2019 file as a json object which specifies the business ID, its name, location, stars, review count, opening hours, etc. A text review is a json object in the \u2018review.json\u2019 file, which specifies the business ID, user ID, stars (integer values between and including 1 and 5), review text, date and votes. The necessary data is contained in the business.json and review.json files, therefore we do not use the rest of the data.\nThe businesses described in the Yelp dataset belong to different categories, such as restaurants, shopping, hotels and travel, etc. The text reviews for different business categories may be very different. For example, a typical hotel review may contain the words/phrases \u2018fridge\u2019, \u2018television\u2019 and \u2018comfortable bed\u2019, but these\nwords would not occur in a restaurant review. Therefore, it is important to perform Review Rating Prediction for each business category independently. That is, the model training and testing for each category should be separate. Figure 2(a) shows the distribution of business categories in the dataset. Restaurants make up almost 34% of the 42,153 businesses. Moreover, 68.3% of the 1,125,458 text reviews are about restaurants, as shown by Figure 2(b). Therefore, in this paper, we restrict ourselves to Review Rating Prediction for restaurants only. Thus, the trimmed dataset that we use consists of 14,403 restaurants and 706,646 reviews. Figure 2(c) shows that the star ratings (out of 5) for the restaurant reviews are not uniformly distributed. About 66% of these reviews rate the corresponding restaurants very highly (at least 4 stars); the other classes are smaller."}, {"heading": "4. Experimental Setup", "text": "In this paper, we build sixteen different prediction models, by combining each of four different feature extraction methods with each of four distinct supervised learning algorithms. In this section, we describe the preprocessing phase, the four feature extraction methods, the four supervised learning algorithms, and two performance evaluation metrics."}, {"heading": "4.1. Preprocessing", "text": "We first write some basic Python scripts to separate the restaurants from the business.json file, and to separate the restaurant reviews from the review.json file. We then preprocess the text reviews as follows.\nYelp allows users to write text reviews in free form. This means that a user may excessively use capital letters and punctuation marks (to express his/her intense dislike, for example) and slang words within a review. Moreover, stop words, like \u2018the\u2019, \u2018that\u2019, \u2018is\u2019 etc, occur frequently across reviews and are not very useful. Therefore it is necessary to preprocess the reviews in order to extract meaningful content from each of them. To do this, we use standard Python libraries to remove capitalizations, stop words and punctuations."}, {"heading": "4.2. Feature Extraction", "text": "We use four methods to extract useful features from the review corpus and to build a feature vector for each review. Each of these methods relies on semantic analysis of the text."}, {"heading": "4.2.1. Unigrams", "text": "In the uni-grams model (also called the \u201cbag of words\u201d model), each unique word in the pre-processed review corpus is considered as a feature. Thus, building a feature vector for a review is straightforward. First, a dictionary of all the words occuring in the review corpus is created. Then a word-review matrix is constructed, where entry (i, j) is the frequency of occurence of word i in the j\u2019th review. Finally, we apply the TF-IDF (Term Frequency - Inverse Document Frequency) weighting technique to this matrix to obtain the final feature matrix. This weighting technique assigns less weight to words that occur more frequently across reviews (e.g. \u201cfood\u201d) because they are generally not good distinguishers between any pair of reviews,\nand a high weight to more rare words. Each column of this matrix is a feature vector of the corresponding review.\nThe size of the dictionary in this case, i.e. the total number of features, is 171,846."}, {"heading": "4.2.2. Unigrams & Bigrams", "text": "The unigrams model is a widely used feature extraction method in natural language processing. It is quite simple to implement and in many cases, it gives surprisingly good results. However, its inherent drawback is its inability to capture relationships between two words (e.g. a word and its modifier, a word and its negation, etc), because it treats each word in isolation. To capture the effect of phrases such as \u2018tasty burger\u2019 and \u2018not delicious\u2019, we add bigrams to the unigrams model. Now, the dictionary additionally consists of all the 2-tuples of words (i.e. all pairs of consecutive words) occuring in the corpus of reviews. The matrix is computed as before; it has more rows now. As before, we apply TF-IDF weighting to this matrix so that less importance is given to common words and more importance is given to rare words.\nThe size of the dictionary (total number of features) is 7,612,422."}, {"heading": "4.2.3. Unigrams, Bigrams & Trigrams", "text": "To capture the effect of phrases like \u2018tasty fish burger\u2019, we now add trigrams (i.e. all triples of consecutive words) to the unigrams+bigrams model. The rest of the computations for building the feature matrix are the same as before. Note, however, that the same trigram would rarely occur across different reviews, because two different people are unlikely to use the same 3-word phrase in their reviews. Therefore, the results of this model are not expected to be very different from the unigrams+bigrams model.\nThe total number of features in this case is 31,677,669."}, {"heading": "4.2.4. Latent Semantic Indexing (LSI)", "text": "Latent Semantic Indexing (LSI) (Hofmann, 1999) is a more sophisticated method of lexical matching, which goes beyond exact matching of words. It finds \u2018topics\u2019 in reviews, which are words having similar meanings or words occuring in a similar context. In LSI, we first construct a word-review matrix M , of size m\u00d7 t, using the unigrams model, and then do Singular Value Decomposition (SVD) of M .\nSV D(M) = U \u00b7 S \u00b7 V T\nThe SVD function outputs three matrices: the wordtopic matrix U of size m\u00d7m, the rectangular diagonal matrix S of size m\u00d7t containing t singular values, and the transpose of the topic-review matrix V of size t\u00d7t. We use V as the feature matrix.\nThe singular values matrix S has t non-zero diagonal entries that are the singular values in decreasing order of importance. The columns of S correspond to the topics in the reviews. The i\u2019th singular value is a measure of the importance of the i\u2019th topic. By default, t equals the size of vocabulary (i.e. 171,846). However, the first t\u2217 topics can be chosen as the most important ones, and thus the top t\u2217 rows of V can be used as the feature matrix. Determining the value of t\u2217 is crucial, and this can be done by examining a simple plot of the singular values against their importance, and looking for an \u2018elbow\u2019 in the plot."}, {"heading": "4.3. Supervised Learning", "text": "To train our prediction models, we use four supervised learning algorithms."}, {"heading": "4.3.1. Logistic Regression", "text": "In logistic regression (Freedman, 2009), the conditional probability function P (s|r) is modelled, where r is a feature vector for review r and s belongs to the set of class labels {1, 2, 3, 4, 5}. Then, given a new feature vector r\u2217 for a new review r\u2217, this probability function is computed for all values of s, and the s value corresponding to the highest probability is output as the final class label (star rating) for this review."}, {"heading": "4.3.2. Na\u0308\u0131ve Bayes Classification", "text": "A Na\u0308\u0131ve Bayes (Ng and Jordan, 2002) classifier makes the Na\u0308\u0131ve Bayes assumption (i.e. it assumes conditional independence between any pair of features given some class) to model the joint probability P (r, s) for any feature vector r and star rating s. Then, given a new feature vector r\u2217 for a new review r\u2217, the joint probability function is computed for all values of s, and the s value corresponding to the highest probability is output as the final class label for review r\u2217.\nIn this paper, we use multinomial Na\u0308\u0131ve Bayes classification, which assumes that P (ri|s) is a multinomial distribution for all i. This is a typical choice for document classification, because it works well for data that can be turned into counts, for example weighted word frequencies in the text."}, {"heading": "4.3.3. Perceptrons", "text": "A perceptron (Rosenblatt, 1957) is a linear classifier that outputs class labels instead of probabilities. It uses a gradient-descent-like rule to iterate over the training set multiple times, in order to re-classify any misclassified examples, until all of them have been classified correctly. For linearly separable data, a Perceptron Convergence Rule states that a solution will always be found after some finite number of iterations. For data that is not linearly separable, there will be oscillation, which can be detected automatically.\nPerceptron solutions may be non-unique, because the margin of the linear decision boundary is ignored. In our experiments, we set the number of iterations to be 50 (a typical choice), that is, the classifier loops over the entire training set 50 times."}, {"heading": "4.3.4. Linear Support Vector Classification (SVC)", "text": "Support Vector Machines (SVM) (Tsochantaridis et al., 2004) are enhanced versions of perceptrons, in that they eliminate the non-uniqueness of solutions by optimizing the margin around the decision boundary, and handle non-separable data by allowing misclassifications. A parameter C controls overfitting. When C is small, the algorithm focuses on maximizing the margin, even if this means more misclassifications, and for lage values of C, the margin is decreased if this helps to classify more examples correctly.\nIn our experiments, we use linear SVMs for multiclass classification. The tolerance of the convergence criterion is set to 0.001. For each feature extraction method, we do internal 3-fold cross validation to choose the value of C that gives the highest accuracy. It turns out that C = 1.0 works best every time."}, {"heading": "4.4. Performance Metrics & Implementation Details", "text": "We use 80% of the dataset for training, and 20% for testing. For each of the sixteen prediction systems, we perform 3-fold cross validation on the training set and compute two metrics, Root Mean Squared Error (RMSE) and accuracy, for the training fold as well as the validation fold.\nAll the implementation is done on an Intel Core i5 CPU with 4 cores (1.6 GHz each), 8 GB RAM and 64 bit Ubuntu 12.04 operating system. The programming language used is Python, and extensive use is made of its libraries numpy, scipy and scikit-learn."}, {"heading": "5. Results and Analysis", "text": "In this section, we present the results for the four feature extraction methods separately. For each method, we show an RMSE graph and an Accuracy graph; each graph contains plots for the four classifiers. We then analyze these results to choose the best of the sixteen systems, and finally we evaluate the chosen system on the test set."}, {"heading": "5.1. Unigrams", "text": "Figures 3(a) and 4(a) show the performance of the four classifiers on unigrams\u2019 features. The total number of available features is 171,846 and we do not know how many of these are useful, so we plot the RMSE and the accuracy against the top1 x number of features. We see that as the number of features increases, the training-fold RMSE for each classifier increases and the training-fold accuracy of each classifier decreases, but the validation-fold RMSE and the validation-fold accuracy level off at about 10,000 features. The only exception is the Na\u0308\u0131ve Bayes classifier, whose RMSE reaches a minimum value around 10,000 features, but then starts rising again.\nClearly, perceptrons perform the worst, achieving a lowest RMSE of 1.25 and the highest accuracy of 43%. The Na\u0308\u0131ve Bayes classifier is the second worst, with the best RMSE and accuracy values of 0.96 and 52%. The performances of Linear SVC and logistic regression are not significantly different on the validation fold. Linear SVC\u2019s best RMSE and accuracy scores are 0.87 and 57% , while those for logistic regression are 0.85 and 58%. So, logistic regression has the best performance for unigrams.\nNote that a random (coin-flip) classifier, that assumes no knowledge of the data distribution, would have an accuracy of 20%, because we have 5 classes. Logistic regression improves the random classifier accuracy by almost 300%."}, {"heading": "5.2. Unigrams & Bigrams", "text": "Figures 3(b) and 4(b) show the performance of the four classifiers on the top x number of features obtained from unigrams & bigrams. The total number of available features in this case is over 7 million. We see that the RMSE and accuracy improves for every classifier, compared to Figures 3(a) and 4(a). This is because bigrams occur frequently enough in the corpus, capture a lot of information that unigrams cannot, and give\n1For example, top 10 features would be the features with the top 10 TF-IDF weights.\nmuch more meaningful features. However, the overall trends are quite the same for the training and validation folds. Perceptrons are still the worst, followed by the Na\u0308\u0131ve Bayes classifier. Linear SVC and logistic regression are quite close again. Linear SVC\u2019s best RMSE and accuracy scores are 0.81 and 63%, while those for logistic regression are 0.78 and 64%. Logistic regression wins again."}, {"heading": "5.3. Unigrams, Bigrams & Trigrams", "text": "The results for this feature extraction method are almost exactly the same as those for the \u2018Unigrams & Bigrams\u2019 method (Figures 3(b) and 4(b)), and we omit the graphs due to brevity of space. The best RMSE and accuracy scores are 0.78 and 64%, achieved by logistic regression.\nAdding trigrams to the previous model does not help, because trigrams repeat rarely. It is unlikely that two different user would use the exact same 3-tuple to describe a restaurant. The TF-IDF weighting technique weights almost all the 3-tuples as very rare, therefore they are not very useful as features."}, {"heading": "5.4. Latent Semantic Indexing (LSI)", "text": "Figure 5 shows the results of the experiments with LSI. Figure 5(a) is a plot of the 1000 highest singular values. We see that the graph starts leveling out at 200. This means that the top 200 topics in the reviews are the most important ones for training the model. Next, we evaluate each classifer on feature vectors of length up to 200; the performance of each classifer is shown in\nFigure 5(b) and (c), for the validation fold2.\nFigures 5(b) and (c) show some interesting patterns. Perceptrons perform the worst as usual, however we see two spikes in the performance around 170 and 200 features, where the RMSE and accuracy suddenly improve. It is not clear why this happens, but it suggests that we should consider more than 200 features to see if more of these spikes occur and improve the best scores for perceptrons. The plots for logistic regression show another interesting pattern. As the number of features increases, the RMSE remains constant around 1.3, but the accuracy increases from 0.34 to 0.43. One explanation for this apparent anomaly is that, more examples get classified accurately, but the RMSE for the misclassified examples increases and overshadows the decrease in the RMSE due to better accuracy.\nA similar pattern is seen for Linear SVC, and could be explained as above. The upward trend in its accuracy suggests we should consider more than 200 features. Due to time constraints, we leave this experiment as future work."}, {"heading": "5.5. The Best Model: Performance on the Test Set", "text": "Based on the results in Figure 3, 4 and 5, we can see that Logistic Regression achieved the highest accuracy of 64% using the top 10,000 Unigrams & Bigrams as features, followed very closely by Linear SVC which achieved 63% accuracy using the top 10,000 Unigrams & Bigrams.\nNext, we evaluate these two systems on the test set.\n2We could not obtain the training-fold RMSE and accuracy results due to time constraints.\nFor Linear SVC, the RMSE and accuracy scores are 1.05 and 56%, and for logistic regression, the scores are 0.92 and 54%. These test-set scores are slightly worse than the validation-fold scores, and possibly indicate overfitting; to fix that, we would need to add/adjust the regularization parameters and re-run all the experiments. This is left as future work."}, {"heading": "6. Conclusions & Future Work", "text": "In this paper, we tackle the Review Rating Prediction problem for restaurant reviews on Yelp. We treat it as a 5-class classification problem, and examine various feature extraction and supervised learning methods to construct sixteen prediction systems. Experimentation and performance evaluation through k-fold cross validation yields one system, Logistic Regression on the set of top 10,000 features obtained from Unigrams & Bigrams, that exhibits better predictive powers than the others. Our system can be used to generate star ratings on review websites where users can write freeform text reviews without giving a star rating.\nThough the methods tested in this paper are extensive, they are by no means exhaustive. In fact, there are many avenues for improvements and future work. We list them below.\n1. We can try more sophisticated feature engineering methods, such as Parts-of-Speech (POS) tagging and spell-checkers, to obtain more useful n-grams. For example, instead of considering all possible bigrams, we can extract all the adjective-noun pairs or all the noun-noun pairs to get more meaningful 2-tuples. This would significantly help with the system\u2019s efficiency as well. Our current implementation in Python is quite slow (each plot\nin Figures 3, 4 and 5 took 36 to 48 hours) because we deal with up to 32 million features. It is also memory-intensive. Choosing a smaller number of features more carefully will help tremendously with this bottleneck.\n2. We can try more elaborate experiments for LSI that consider more than 200 features. Moreover, instead of performing singular value decomposition of unigrams only, we can add other n-grams. Another possibility is to perform SVD on specific text-constructs only, such as the nouns or the adjective-noun pairs. It would be interesting to see how the results change.\n3. A possibly better alternative for logistic regression that should be tried is ordered/ordinal logistic regression 3. It is an extension of logistic regression that specifically caters to classification problems where the class labels are ordered. So, in our case, this model takes into consideration the fact that the class labels 1 and 2 are closer to each other, than the labels 1 and 4.\n4. All the prediction models that we use in this paper are linear. That is, the hypothesis function is a linear function of the parameters. It would be interesting to experiment with non-linear models, e.g. polynomial regression, SVC with a non-linear kernel, etc. Alternatively, to get a non-linear decision boundary, we could find a linear decision boundary in an expanded feature space; for example, the feature vectors ri could be replaced with \u03c6(ri), where \u03c6 is called a feature mapping.\n5. As mentioned in Section 5.5, our test-set performance was worse than the validation-fold per-\n3http://www.norusis.com/pdf/ASPC v13.pdf.\nformance, and this could be due to over-fitting. To fix this, we can introduce/adjust the regularization parameters in our models using internal cross-validation to get improved performance.\n6. For feature extraction, we could try topic mod-\nelling techniques such as Latent Dirichlet Allocation (Blei et al., 2003), Non-negative Matrix Factorization (Tandon and Sra, 2010) and/or Independent Component Analysis (Stone, 2004). We could also use one or more of the sentiment analysis techniques mentioned in Section 2, and possibly combine them with the semantic analysis techniques we use in this paper to improve the results.\n7. To get a more detailed performance evaluation, we can try other metrics, such as precision, recall, Fscore and confusion matrix. Also, for probabilistic models, we can analyse the confidence scores.\n8. We can compare our classifiers\u2019 preformance with the performance of the traditional recommendation techniques such as collaborative filtering.\n9. We chose to do 3-fold cross validation in our experiments because 10-fold cross validation turned out to be very expensive in terms of time and memory. It would be worthwhile to try 10-fold cross validation and see if it yields different and/or better results. To deal with the time and RAM bottleneck, we could try parallel processing over clusters, such as those provided by Sharcnet4.\n10. Another avenue for future work is to test how our prediction models would perform on other business categories, such as shopping, travel, etc.\n4https://www.sharcnet.ca."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "The impact of online recommendations and consumer feedback on sales", "author": ["P.Y. Chen", "S.Y. Wu", "J. Yoon"], "venue": "In Proceedings of the International Conference on Information Systems,", "citeRegEx": "Chen et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2003}, {"title": "The digitization of word of mouth: promise and challenges of online feedback mechanisms", "author": ["C. Dellarocas"], "venue": "Management Science,", "citeRegEx": "Dellarocas.,? \\Q2003\\E", "shortCiteRegEx": "Dellarocas.", "year": 2003}, {"title": "Predicting a business star in yelp from its reviews text alone", "author": ["Mingming Fan", "Maryam Khademi"], "venue": "arXiv preprint arXiv:1401.0864,", "citeRegEx": "Fan and Khademi.,? \\Q2014\\E", "shortCiteRegEx": "Fan and Khademi.", "year": 2014}, {"title": "Statistical models: theory and practice", "author": ["David Freedman"], "venue": null, "citeRegEx": "Freedman.,? \\Q2009\\E", "shortCiteRegEx": "Freedman.", "year": 2009}, {"title": "Beyond the stars: Improving rating predictions using review text content", "author": ["Gayatree Ganu", "Noemie Elhadad", "Am\u00e9lie Marian"], "venue": "In WebDB,", "citeRegEx": "Ganu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ganu et al\\.", "year": 2009}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Integrating collaborative filtering and sentiment analysis: A rating inference approach", "author": ["Cane WK Leung", "Stephen CF Chan", "Fu-lai Chung"], "venue": "Proceedings of the ECAI 2006 workshop on recommender systems,", "citeRegEx": "Leung et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Leung et al\\.", "year": 2006}, {"title": "Incorporating reviewer and product information for review rating prediction", "author": ["F. Li", "N. Liu", "H. Jin", "K. Zhao", "Q. Yang", "X. Zhu"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and n\u00e4\u0131ve bayes", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ng and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Ng and Jordan.", "year": 2002}, {"title": "The bag-of-opinions method for review rating prediction from sparse text patterns", "author": ["L. Qu", "G. Ifrim", "G. Weikum"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING", "citeRegEx": "Qu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2010}, {"title": "The perceptron, a perceiving and recognizing automaton Project Para", "author": ["Frank Rosenblatt"], "venue": "Cornell Aeronautical Laboratory,", "citeRegEx": "Rosenblatt.,? \\Q1957\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1957}, {"title": "Independent component analysis", "author": ["James V Stone"], "venue": "Wiley Online Library,", "citeRegEx": "Stone.,? \\Q2004\\E", "shortCiteRegEx": "Stone.", "year": 2004}, {"title": "Sparse nonnegative matrix approximation: new formulations and algorithms", "author": ["Rashish Tandon", "Suvrit Sra"], "venue": "Max Planck Institute for Biological Cybernetics,", "citeRegEx": "Tandon and Sra.,? \\Q2010\\E", "shortCiteRegEx": "Tandon and Sra.", "year": 2010}, {"title": "Support vector learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In Proceedings of the 21st International Conference on Machine Learning (ACM", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "These online reviews function as the \u2018online word-of-mouth\u2019 (Dellarocas, 2003) and a criterion for consumers to choose between similar products.", "startOffset": 60, "endOffset": 78}, {"referenceID": 1, "context": "(Chen et al., 2003)) show that they have a significant impact on consumer purchase decisions as well as on product sales and business revenues.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "Qu et al. (2010) tackle this problem for Amazon.", "startOffset": 0, "endOffset": 17}, {"referenceID": 6, "context": "Latent Semantic Indexing (LSI) (Hofmann, 1999) is a more sophisticated method of lexical matching, which goes beyond exact matching of words.", "startOffset": 31, "endOffset": 46}, {"referenceID": 4, "context": "In logistic regression (Freedman, 2009), the conditional probability function P (s|r) is modelled, where r is a feature vector for review r and s belongs to the set of class labels {1, 2, 3, 4, 5}.", "startOffset": 23, "endOffset": 39}, {"referenceID": 9, "context": "A N\u00e4\u0131ve Bayes (Ng and Jordan, 2002) classifier makes the N\u00e4\u0131ve Bayes assumption (i.", "startOffset": 14, "endOffset": 35}, {"referenceID": 11, "context": "A perceptron (Rosenblatt, 1957) is a linear classifier that outputs class labels instead of probabilities.", "startOffset": 13, "endOffset": 31}, {"referenceID": 14, "context": "Support Vector Machines (SVM) (Tsochantaridis et al., 2004) are enhanced versions of perceptrons, in that they eliminate the non-uniqueness of solutions by optimizing the margin around the decision boundary, and handle non-separable data by allowing misclassifications.", "startOffset": 30, "endOffset": 59}, {"referenceID": 0, "context": "For feature extraction, we could try topic modelling techniques such as Latent Dirichlet Allocation (Blei et al., 2003), Non-negative Matrix Factorization (Tandon and Sra, 2010) and/or Independent Component Analysis (Stone, 2004).", "startOffset": 100, "endOffset": 119}, {"referenceID": 13, "context": ", 2003), Non-negative Matrix Factorization (Tandon and Sra, 2010) and/or Independent Component Analysis (Stone, 2004).", "startOffset": 43, "endOffset": 65}, {"referenceID": 12, "context": ", 2003), Non-negative Matrix Factorization (Tandon and Sra, 2010) and/or Independent Component Analysis (Stone, 2004).", "startOffset": 104, "endOffset": 117}], "year": 2016, "abstractText": "Review websites, such as TripAdvisor and Yelp, allow users to post online reviews for various businesses, products and services, and have been recently shown to have a significant influence on consumer shopping behaviour. An online review typically consists of free-form text and a star rating out of 5. The problem of predicting a user\u2019s star rating for a product, given the user\u2019s text review for that product, is called Review Rating Prediction and has lately become a popular, albeit hard, problem in machine learning. In this paper, we treat Review Rating Prediction as a multi-class classification problem, and build sixteen different prediction models by combining four feature extraction methods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic Indexing, with four machine learning algorithms, (i) logistic regression, (ii) N\u00e4\u0131ve Bayes classification, (iii) perceptrons, and (iv) linear Support Vector Classification. We analyse the performance of each of these sixteen models to come up with the best model for predicting the ratings from reviews. We use the dataset provided by Yelp for training and testing the models.", "creator": "LaTeX with hyperref package"}}}