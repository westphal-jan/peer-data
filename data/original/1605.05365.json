{"id": "1605.05365", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Dynamic Frame skip Deep Q Network", "abstract": "Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain. One of the important parameters in the Arcade Learning Environment (ALE) is the frame skip rate. It decides the granularity at which agents can control game play. A frame skip value of $k$ allows the agent to repeat a selected action $k$ number of times. The current state of the art architectures like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state. In this paper, we propose a new architecture, Dynamic Frame skip Deep Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable parameter. This allows us to choose the number of times an action is to be repeated based on the current state. We show empirically that such a setting improves the performance on relatively harder games like Seaquest.", "histories": [["v1", "Tue, 17 May 2016 20:58:41 GMT  (319kb,D)", "http://arxiv.org/abs/1605.05365v1", "6 pages, 8 figures. arXiv admin note: text overlap witharXiv:1506.08941by other authors"], ["v2", "Sat, 11 Jun 2016 01:04:13 GMT  (421kb,D)", "http://arxiv.org/abs/1605.05365v2", "IJCAI 2016 Workshop on Deep Reinforcement Learning: Frontiers and Challenges; 6 pages, 8 figures"]], "COMMENTS": "6 pages, 8 figures. arXiv admin note: text overlap witharXiv:1506.08941by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["aravind s lakshminarayanan", "sahil sharma", "balaraman ravindran"], "accepted": false, "id": "1605.05365"}, "pdf": {"name": "1605.05365.pdf", "metadata": {"source": "CRF", "title": "Dynamic Frame skip Deep Q Network", "authors": ["Aravind Lakshminarayanan", "Sahil Sharma", "Balaraman Ravindran"], "emails": ["aravindsrinivas@gmail.com,", "ssahil08@gmail.com,", "ravi@cse.iitm.ac.in"], "sections": [{"heading": "1 Introduction", "text": "Video game domains such as Mario and Atari 2600 have served as a test bed to measure performance of learning algorithms in Artificial Intelligence ([Bellemare et al., 2013]). Such games present a unique challenge to an AI agent since performing well in them requires being able to understand the current state of the game (involves pattern recognition) as well as being able to choose actions considering long term reward (involves planning). Recent application of Deep Learning to the Atari 2600 domain has resulted in state of the art performance. The Deep Q-learning neural network architecture [Mnih et al., 2015] is one such method that uses a deep neural network with convolutional filters to learn a hierarchically compositional representation of the state space. Such a\n\u2217 Equal Contribution\nrepresentation is used to approximate the state-action value function and more importantly generalize to unseen states.\nOne of the hyper-parameters used in the DQN framework is the frame skip rate which denotes the number of times an action selected by the agent is to be repeated. If the frame skip rate is low, the decision making frequency of the agent is high. This leads to policies which change rapidly in space and time. It also leads to slower game play since the DQN has to make a decision more often and each such decision involves running convolutional neural networks which are computationally intensive. On the other hand, a high frame skip rate causes infrequent decisions, which reduces the time to train on an episode at the cost of losing fine-grained control. The policies learnt with a higher frame skip rate have fewer action sequences which exhibit super-human reflexes as compared to a DQN with a low frame skip rate ([Mnih et al., 2015] use a frame skip rate of 4). A reduction in decision making frequency gives the agent the advantage of not having to learn control policies in the intermediate states given that a persistent action from the current state can lead the agent to an advantageous next state. Such skills would be useful for the agent in games where good policies require some level of temporal abstraction. An example of this situation in Seaquest is when the agent has to continually shoot at multiple enemies arriving together and at the same depth, one behind another. Having said that, there are also situations where the agent has to take actions which require quick reflexes to perform well with an example from Seaquest being states in which multiple enemies attack at once and are placed at varying depths. The agent is then expected to manoeuvre skillfully to avoid the enemies. Therefore, a high but static frame skip rate is probably not a solution for better game playing strategies in spite of the temporal abstraction it provides. (Refer to Appendix (Page 6) for a snapshot of Seaquest)\nAs a first step in the direction of temporal abstractions in the policy space, we explore a dynamic frame skip model, built on top of the DQN architecture. In this model, the agent can choose to play from a set of options or macro actions corresponding to two different (and fixed) frame skip rate values available at every state of the game. We show the efficacy of the policies learnt using such a model over those learnt using the DQN model (which has a static frame skip) by empirically establishing the fact that dynamic ac-\nar X\niv :1\n60 5.\n05 36\n5v 1\n[ cs\n.L G\n] 1\n7 M\nay 2\n01 6\ntion repetition is an important way in which an AI agent\u2019s capabilities can be extended. Thereby, we make the claim that there is a need to explore and experiment with structured parametrized policies using Actor Critic setup in the Atari 2600 domain. In the model that we propose, the agent decides whether to take a repetitive action sequence that is temporally long (corresponding to a higher frame skip) or a shorter sequence of repeated actions (corresponding to a lower frame skip) based on the input image. We also outline a generic method wherein the agent\u2019s policy for a particular frame would not only contain the action probabilities, but the level of persistence (frame skip) of the actions as well. Note that no experiments are performed for the generic method and only the framework is proposed."}, {"heading": "2 Related work", "text": "One of the first efforts which pushed the state of the art significantly in the Atari 2600 domain was [Mnih et al., 2015]. Their architecture (DQN) motivated the use of convolutional neural networks for playing games in the Atari 2600 domain. DQN combines the image-based-state representational capabilities of deep convolutional neural networks with the robustness of a Q-learning set up. Our work is a direct modification of the DQN architecture.\n[Braylan et al., 2015] is one work that focuses on the power of the frame skip rate parameter with experiments in the Atari 2600 domain. Their learning framework is a variant of Enforced Sub-populations (ESP), a neuroevolution approach which has been successfully trained for complex control tasks such as controlling robots and playing games. They show empirically that the frame skip parameter is an important factor deciding the performance of their agent in the Atari domain. They demonstrate for example that Seaquest achieves best performance when a frame skip rate of 180 frames is used. This is equivalent to the agent pausing (continuing the same action) for three seconds between decisions since the ALE emulator runs at 60 frames per second. They argue that a higher value of frame skip rate allows the agent to not learn action selection for the states that are skipped and hence develop associations between states and actions that are temporally distant. However, they experiment only with a static frame skip setup. The key way in which DFDQN differs from both the ESP and the DQN approaches is that we make frame skip a dynamic learnable parameter.\nThe idea of dynamic length temporal abstractions in the policy space on Atari Domain has been explored by [Vafadost, 2013]. They use a Monte Carlo Tree Search (MCTS) planner with macro-actions that are composed of the same action repeated k times, for different k. The way in which our approach differs from [Vafadost, 2013] is in terms of using DQN to build neural network Q-value approximators instead of making use of search techniques that cannot generalize. This leads to not only a high scoring AI agent but also one which is capable of playing at human speeds because of generalization to states not encountered before.\nSimilar to us, [Ortega et al., 2013] attempt to design high level actions to imitate human-like play in Mario. They compose some high level actions based on human play like Walk,\nRun, Right Small Jump, Right High Jump and Avoid enemy, which are macro actions composed of varying length repetitive key presses. For instance, Right High Jump involves pressing the Jump and Right keys together for 10 frames, while Right Small Jump requires the same for 2 frames."}, {"heading": "3 Background", "text": ""}, {"heading": "3.1 Q Learning", "text": "Reinforcement Learning is a commonly used framework for learning control policies in game environments [Silver et al., 2007; Amato and Shani, 2010; Mnih et al., 2015; Narasimhan et al., 2015]. A game can be formulated as a sequence of state transitions (s, a, r, s\u2032) in a Markov Decision Process (MDP). The agent takes an action a in a state s by consulting the state-action value Q function Q(s, a), which is a measure of the action\u2019s long-term reward. Q-learning ([Watkins and Dayan, 1992]) is an off-policy Temporal Difference (TD) learning ([Sutton and Barto, 1998]) algorithm that can be used to learn an optimal Q(s, a) for the agent. Starting from a random initialization, the agent iteratively updates the Q-values by playing the game and obtaining rewards. The iterative updates are derived from the Bellman optimality equation ([Sutton and Barto, 1998]):\nQi+1(s, a) = E[r + \u03b3argmaxa\u2032Qi(s \u2032, a\u2032)|(s, a)]\nUsing these evolving Q-values, the agent can choose the action with highest Q(s, a) at every state s. However, for the sake of addressing the exploration-exploitation dilemma, the agent follows an -greedy policy whereby a random action is performed with probability . ([Sutton and Barto, 1998])"}, {"heading": "3.2 Deep Q Network (DQN)", "text": "In large games, it is often impractical to maintain the Q-value for all possible state-action pairs. One solution to this problem is to approximate Q(s, a) using a parametrized function Q(s, a; \u03b8) which can generalize over states and actions using higher level features ([Sutton and Barto, 1998]). To be able to discover these features without feature engineering, we need a neural network function approximator. The DQN ([Mnih et al., 2015]) approximates the Q-value function with a deep neural network to be able to predict Q(s, a) over all actions a, for all states s. The loss function used for learning a Deep Q Network is as below :\nLi(\u03b8i) = Es,a,r,s\u2032 [(yDQNi \u2212Q(s, a; \u03b8i)) 2],\nwith yDQNi = (ri + \u03b3maxa\u2032Q(s \u2032, a\u2032, \u03b8\u2212)) Here, Li represents the expected TD error corresponding to current parameter estimate \u03b8i. \u03b8\u2212 represents the parameters of a separate target network, while \u03b8i represents the parameters of the online network. The usage of a target network is to improve the stability of the learning updates. The gradient descent step is shown below:\n\u2207\u03b8iLi(\u03b8i) = Es,a,r,s\u2032 [(y DQN i \u2212Q(s, a; \u03b8i))\u2207\u03b8iQ(s, a)]\nTo avoid correlated updates from learning on the same transitions that the current network simulates, an experience replay ([Lin, 1993]) D (of fixed maximum capacity) is used,\nwhere the experiences are pooled in a FIFO fashion. Transitions from previous episodes are sampled multiple times to update the current network, thereby avoiding divergence issues due to correlated updates. In this paper, we sample transitions from D uniformly at random as in [Mnih et al., 2015]. However, [Schaul et al., 2016] and [Wang et al., 2016] show that having a prioritized sampling can lead to substantial improvement in performance."}, {"heading": "4 The Dynamic Frame skip Deep Q Network", "text": "The key motivation behind the DFDQN architecture is the observation that when humans play games, the actions tend to be temporally correlated, almost always. For certain states of the game, we play long sequences of the same action whereas for some states, we switch the actions performed quickly. For example, in Seaquest, if the agent is low on oxygen and deep inside the sea, we would want the agent to resurface and fill up oxygen using a long sequence of up actions. To incorporate the longer repetitive actions, we introduce the following architecture level changes to the DQN: Let A = {a1, \u00b7 \u00b7 \u00b7 , a|A|} denote the set of all legal actions of a particular Atari domain game (for Seaquest A = {0, 1, \u00b7 \u00b7 \u00b7 , 17}). We introduce |A| new actions {a|A|+1, \u00b7 \u00b7 \u00b7 , a2|A|}. The semantics associated with the actions are: Action ak results in the same basis action being played in the ALE as the action a(k%|A|). The difference is in terms of the number of times the basis action is repeated by the ALE emulator. In [Mnih et al., 2015], the DQN architecture operates with a static frame skip parameter which is equal to 4. Hence any selected action is repeated 4 times by ALE. In DFDQN, action ak is repeated r1 number of times if k < |A| and r2 number of times if k \u2265 |A|. One can think about each ak as representing a temporally repeated action. Given this, the objective of this work is to come up with a model to learn the near-optimal use of such options. This scheme is implemented by doubling the number of neurons in the final layer of the DQN architecture. For a given state s, the DFDQN (with double the number of output neurons as DQN) thus outputs the discrete set of value function approximations {Q(s, a1), \u00b7 \u00b7 \u00b7 , Q(s, a2|A|)}. Q(s, ak) represents an approximation to the return the agent would get on taking action ak in state s and following the optimal policy thereafter. Due to these additional repetitive actions, the value functions learnt by DFDQN differ drastically when compared to those learnt by the DQN (Figures 4 and 5).\nNote that r1 and r2 are hyper-parameters in this model and must be chosen before the learning begins. This architecture presents the agent with 2 discrete levels of control, as far as action repetition is concerned.\nIdeally we would want to learn policies in a parametrized action space where the agent outputs an action ak and a parameter r where r denotes the number of times that the agent wants to play action ak consecutively. Such a framework would have the representative power to select the optimal number of times an action is to be executed. But, it would also be proportionately complex and as a result difficult to train. The DFDQN framework seeks to provide an approximation to this richer model and favors simplicity over optimality of game play. As future work, we wish to explore the learning of parametrized policies for games in the Atari 2600 domain, learnt with the help of an actor-critic algorithm."}, {"heading": "5 Experiments and Results", "text": "We perform general game-playing evaluation on two Atari 2600 domain games, Seaquest and Space Invaders. We deploy a single algorithm and architecture, with a fixed set of hyper-parameters, to learn to play both the games given only raw pixel observations and game rewards. The ALE is a very challenging environment since some of the games such as Seaquest are reasonably complex and the observations used are high-dimensional (each frame is sampled as an image of dimensions 210\u00d7 160).\nOur network architecture has the same low-level convolutional structure and input preprocessing of DQN [Mnih et al., 2015]. Due to the partial observable nature of the Atari 2600 games, the three previous frames are combined with the current frame and given as a 84 \u00d7 84 \u00d7 4 multi-channel input to the DFDQN. This is followed by 3 convolutional layers, which are in turn followed by 2 fully-connected layers. The first convolutional layer has 32 filters of size 8\u00d7 8 with stride 4; the second 64 filters of size 4 \u00d7 4 with stride 2, and the third convolutional layer consists of 64 filters of size 3 \u00d7 3 with stride 1. Since DFDQN has double the number of output neurons as DQN, we wanted more representational power for being able to decide from a larger set of actions. Therefore, we have 1024 units in the pre-output hidden layer as compared to 512 units used in the DQN architecture ([Mnih et al., 2015]). All the hidden layers (3 convolutional and 1 fully connected) are followed by a rectifier nonlinearity.\nThe values of r1, r2 (defined in the previous section) are kept the same for both games and are equal to 4 and 20 respectively. Similar to [Mnih et al., 2015], the hyperparameters of the DFDQN are kept the same for both the games. We have double the number of actions as compared to DQN. Thus, to ensure sufficient exploration, we anneal the exploration probability from 1 to 0.1 over 2 million steps as against 1 million steps used in DQN. We again follow [Mnih et al., 2015] in using RMSProp with a max-norm clipping of 1 for our gradient updates while learning. Even though our Q values are larger due to extended options possibly giving higher rewards, we still use a max-norm clipping of 1 as used in DQN. We leave it for future work to find the best max-norm clip value, as well as other hyper-parameters like\nlearning rate, final exploration probability , replay memory size, freeze interval, etc that would work well for all games in the DFDQN setup. To establish the fact that the improvement in performance is not just due to the increase in the representational power by having double the number of pre-final hidden layer neurons, we run DQN baselines with 1024 units in the pre-final layer for both the games. The DQN implementation we use is: https://github.com/spragunr/ deep_q_rl and is based on Theano [Bastien et al., 2012; Bergstra et al., 2010]. We also report the scores obtained from original DQN architecture with 512 pre-final hidden layer neurons from the most recent usage of DQN in [Wang et al., 2016], where DQN is reported as baseline for their DuDQN model (the current state-of-the-art model for Atari 2600 domain). The training and evaluation framework is the same as that used in [Mnih et al., 2015]. A training epoch consists of 250000 steps (action selections). This is followed by a testing epoch which consists of 125000 steps. The score of a single episode is defined to be the sum of the rewards received from the ALE emulator in that episode. The score of a testing epoch is defined to be the average of the scores obtained in all of the complete episodes played during that testing epoch. The scores reported here are for the testing epoch with the highest average episode score (known as the best score for a game). Table 1 presents the experimental results. HLS denotes the pre-final hidden layer size. FS stands for the frame-skip value used. A value of D for FS stands for dynamic. Arch denotes the architecture used. AS denotes the best average testing epoch score as defined above. In each of the following figures, one epoch consists of 125000 steps (decisions). DQN-a-b refers to DQN architecture that has b units in the pre-output layer and operates with frame skip rate of a. DFDQN-b is the Dynamic Frame skip Deep Q-Network architecture with b units in pre-final layer. Figure 1 and Figure 2 depict the change in the expected cumulative reward per testing episode with time for the games of Seaquest and Space Invaders respectively. DFDQN with 1024 units in the pre-output layer significantly outperforms DQN with 1024 neurons in pre-output layer. Figure 3 and Figure 4 depict the evolution of the average Q-values over all the actions with time for the games of Seaquest and Space Invaders respectively. These figures demonstrate that the Qvalue function learnt by the DFDQN with 1024 neurons in pre-output layer is significantly higher than that learnt by DQN with similar architecture.\nAn interesting characteristic of the graph (Figure 4) is that the DFDQN-1024\u2019s Q-value for Seaquest keeps increasing with time even at the end of 200 epochs. This means that there is still scope for the performance to improve on further training beyond 200 epochs. To verify our claim that temporally extended actions can lead to better policies, we did an analysis of the percentage of times the longer sequence of basis actions was selected. Using the network which yielded the best score (notion of best score defined in the previous section), we ran a testing epoch of 10000 decisionmaking steps, which resulted in 17 episodes for Seaquest and 18 for Space Invaders. We recorded the actions executed at each decision-step. On an average (over the completed episodes), the DFDQN chooses the longer (frame skip 20) actions 69.99% of the times in Seaquest and 78.87% of the times in Space Invaders. This shows that the DFDQN agent is able to make good use of the longer actions most of the times, but does not ignore the shorter actions. The agent has learnt through repeated exploration-feedback cycles to prefer the extended actions but still exercises the fast reflexes when the situation needs it to do so. We can safely claim that the addition of these higher frame skip options has been an important contributing factor to the improvement in performance. To strengthen our claim that there is a need for dynamic frame skip, we show results on one game (Seaquest) that DQN with a purely high but static frame skip of 20 scores poorly on gameplay. Movies of some of the learned policies for Space Invaders and Seaquest are available at https://goo.gl/ VTsen0 and https://goo.gl/D7twZc respectively."}, {"heading": "6 Conclusions", "text": "In this paper, we introduce a new architecture called Dynamic Frame skip Deep Q-Network (DFDQN). It introduces temporal abstractions through extended actions without losing the ability to make quicker reflexes when required. We show empirically that DFDQN leads to significant improvement in the performance compared to DQN with results on two Atari 2600 domain games over DQN : Seaquest and Space Invaders. The results section illustrates the importance of the extended actions with the temporally extended versions of the actions being selected 69% of the times in Seaquest and 78% of the times in Space Invaders."}, {"heading": "7 Discussion", "text": "Our experiments were conducted with frame skip rates of 4 and 20. A generic framework would enable the agent to play with a wide range of frame skip rates selected according to the current state of the game. We outline such a generic framework with a structured policy wherein the agent does not just decide the action but also how long the action should persist. For this, we would need an Actor Critic setup similar to [Hausknecht and Stone, 2016]. [Hausknecht and Stone, 2016] use Deep Actor Critic for learning parametrized policies in the Half Field Offense (Robo Soccer) domain. Their policy is composed of continuous valued parameters like Power, Direction in addition to the action probabilities of Kick, Turn, Tackle, Dash. The parameters are restricted to lie in a fixed continuous interval. We can adapt this setup for\nlearning structures policies in the Atari 2600 domain with the continuous valued parameter being the frame skip rate r.\nIn this setup, we envision the network as being composed of three parts: The core-policy sub-network Nc, a parameter sub-network Np and the critic sub-network Ncr. All three of them share the same lower level convolutional filters up to a desired depth similar to the architecture used in [Mnih et al., 2016] which as far as we know, is the only successful attempt at using Actor Critic algorithms in the Atari 2600 domain. The output of Nc is the set of action probabilities {\u03c01, \u03c02, \u00b7 \u00b7 \u00b7 , \u03c0|A|} where \u03c0k is the probability of choosing action ak and A = {a1, a2, \u00b7 \u00b7 \u00b7 , a|A|} is the set of basis actions for the chosen task. Np has a single scalar output which is the frame skip parameter r. We could constrain r to lie in a fixed interval [rmin, rmax], where rmin could be 1 and rmax could be as high as 100. Nc and Np together constitute the Actor part of the Actor Critic network. The Critic part depicted by Ncr outputs a single scalar value v which is an estimate of the Value Function of the current state. The error function to be optimized and the manner in which gradients are to be backpropagated can closely follow the Actor-Critic implementation given in [Mnih et al., 2016].\nAn alternative way of learning to perform repeated actions could be to use the previous action as input to the policy similar to [Foerster et al., 2016]. This could be combined with the proposed Actor Critic setup to help on deciding the frame skip parameter r. The improvement in performance has been achieved without much tuning of the DQN ([Mnih et al., 2015]) network parameters. We believe the DFDQN setup will further improve with more appropriate hyper-parameter choices. As future work, we would also like to explore the DF paradigm on more games in the Atari 2600 domain with the more recent state of the art methods such as Prioritized Replay [Schaul et al., 2016], Dueling DQN [Wang et al., 2016] and AsynchronousDQN [Mnih et al., 2016]. We believe that a combination of Dynamic Frame skip and Duelling DQN (DFDuDQN) and (or) Prioritized Replay would lead to a state of the art model for games in the Atari 2600 domain.\nAppendix Seaquest:\nFigure 7: Seaquest\nSpace Invaders:\nFigure 8: Space Invaders"}], "references": [{"title": "9th International Conference on Autonomous Agents and Multi-Agent Systems", "author": ["Christopher Amato", "Guy Shani. High-level reinforcement learning in strategy games"], "venue": "1:75\u201382,", "citeRegEx": "Amato and Shani. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["Bastien et al", "2012] Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research, pages 253\u2013 279, June", "citeRegEx": "Bellemare et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra et al", "2010] James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Frame skip is a powerful parameter for learning to play atari", "author": ["Alex Braylan", "Mark Hollenbeck", "Elliot Meyerson", "Risto Miikkulainen"], "venue": "AAAI workshop,", "citeRegEx": "Braylan et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Arxiv", "author": ["Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson. Learning to communicate to solve riddles with deep distributed recurrent q-networks"], "venue": "February", "citeRegEx": "Foerster et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning in parametrized action space", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "ICLR,", "citeRegEx": "Hausknecht and Stone. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Technical Report", "author": ["Long-Ji Lin. Reinforcement learning for robots using neural networks"], "venue": "DTIC Document,", "citeRegEx": "Lin. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Human-level control through deep reinforcement learning", "author": ["Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "Kumaran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2015}, {"title": "Arxiv", "author": ["Volodymyr Mnih", "Adria Puigdom enech Badia", "Mehdi Mirza", "Alex Graves", "Tim Harley", "Timothy P. Lillicrap", "David Silver", "Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning"], "venue": "February", "citeRegEx": "Mnih et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay"], "venue": "EMNLP,", "citeRegEx": "Narasimhan et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Elsevier", "author": ["Juan Ortega", "Noor Shaker", "Julian Togelius", "Georgios N. Yannakakis. Imitating human playing styles in Super Mario Bros. Entertainment Computing"], "venue": "4:93\u2013104,", "citeRegEx": "Ortega et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "ICLR,", "citeRegEx": "Schaul et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "IJCAI", "author": ["David Silver", "Richard Sutton", "Martin Muller. Reinforcement learning of local shape in the game of go"], "venue": "7:1053\u20131058,", "citeRegEx": "Silver et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Introduction to reinforcement learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": "MIT Press,", "citeRegEx": "Sutton and Barto. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Department of Computer Science", "author": ["Mostafa Vafadost. Temporal abstraction in monte carlo tree search. Masters thesis"], "venue": "University of Alberta,", "citeRegEx": "Vafadost. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot", "Nando de Freitas"], "venue": "Arxiv,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Technical note: Q-learning", "author": ["Christopher J.C.H. Watkins", "Peter Dayan"], "venue": "Mach. Learn., 8(3-4):279\u2013292, May", "citeRegEx": "Watkins and Dayan. 1992", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 2, "context": "One of the important parameters in the Arcade Learning Environment (ALE, [Bellemare et al., 2013]) is the frame skip rate.", "startOffset": 73, "endOffset": 97}, {"referenceID": 16, "context": ", 2015]) and Dueling Network Architectures (DuDQN, [Wang et al., 2016]) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state.", "startOffset": 51, "endOffset": 70}, {"referenceID": 2, "context": "served as a test bed to measure performance of learning algorithms in Artificial Intelligence ([Bellemare et al., 2013]).", "startOffset": 95, "endOffset": 119}, {"referenceID": 4, "context": "[Braylan et al., 2015] is one work that focuses on the power of the frame skip rate parameter with experiments in the Atari 2600 domain.", "startOffset": 0, "endOffset": 22}, {"referenceID": 15, "context": "The idea of dynamic length temporal abstractions in the policy space on Atari Domain has been explored by [Vafadost, 2013].", "startOffset": 106, "endOffset": 122}, {"referenceID": 15, "context": "The way in which our approach differs from [Vafadost, 2013] is in terms of using DQN to build neural network Q-value approximators instead of making use of search techniques that cannot generalize.", "startOffset": 43, "endOffset": 59}, {"referenceID": 11, "context": "Similar to us, [Ortega et al., 2013] attempt to design high", "startOffset": 15, "endOffset": 36}, {"referenceID": 13, "context": "Reinforcement Learning is a commonly used framework for learning control policies in game environments [Silver et al., 2007; Amato and Shani, 2010; Mnih et al., 2015; Narasimhan et al., 2015].", "startOffset": 103, "endOffset": 191}, {"referenceID": 0, "context": "Reinforcement Learning is a commonly used framework for learning control policies in game environments [Silver et al., 2007; Amato and Shani, 2010; Mnih et al., 2015; Narasimhan et al., 2015].", "startOffset": 103, "endOffset": 191}, {"referenceID": 10, "context": "Reinforcement Learning is a commonly used framework for learning control policies in game environments [Silver et al., 2007; Amato and Shani, 2010; Mnih et al., 2015; Narasimhan et al., 2015].", "startOffset": 103, "endOffset": 191}, {"referenceID": 17, "context": "Q-learning ([Watkins and Dayan, 1992]) is an off-policy Temporal Difference (TD) learning ([Sutton and Barto, 1998]) algorithm that can be used to learn an optimal Q(s, a) for the agent.", "startOffset": 12, "endOffset": 37}, {"referenceID": 14, "context": "Q-learning ([Watkins and Dayan, 1992]) is an off-policy Temporal Difference (TD) learning ([Sutton and Barto, 1998]) algorithm that can be used to learn an optimal Q(s, a) for the agent.", "startOffset": 91, "endOffset": 115}, {"referenceID": 14, "context": "The iterative updates are derived from the Bellman optimality equation ([Sutton and Barto, 1998]):", "startOffset": 72, "endOffset": 96}, {"referenceID": 14, "context": "([Sutton and Barto, 1998])", "startOffset": 1, "endOffset": 25}, {"referenceID": 14, "context": "One solution to this problem is to approximate Q(s, a) using a parametrized function Q(s, a; \u03b8) which can generalize over states and actions using higher level features ([Sutton and Barto, 1998]).", "startOffset": 170, "endOffset": 194}, {"referenceID": 7, "context": "To avoid correlated updates from learning on the same transitions that the current network simulates, an experience replay ([Lin, 1993]) D (of fixed maximum capacity) is used,", "startOffset": 124, "endOffset": 135}, {"referenceID": 12, "context": "However, [Schaul et al., 2016] and [Wang et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 16, "context": ", 2016] and [Wang et al., 2016] show that having a prioritized sampling can lead to substantial improvement in performance.", "startOffset": 12, "endOffset": 31}, {"referenceID": 16, "context": "We also report the scores obtained from original DQN architecture with 512 pre-final hidden layer neurons from the most recent usage of DQN in [Wang et al., 2016], where DQN is reported as baseline for their DuDQN model (the current state-of-the-art model for Atari 2600 domain).", "startOffset": 143, "endOffset": 162}, {"referenceID": 6, "context": "For this, we would need an Actor Critic setup similar to [Hausknecht and Stone, 2016].", "startOffset": 57, "endOffset": 85}, {"referenceID": 6, "context": "[Hausknecht and Stone, 2016] use Deep Actor Critic for learning parametrized policies in the Half Field Offense (Robo Soccer) domain.", "startOffset": 0, "endOffset": 28}, {"referenceID": 9, "context": "All three of them share the same lower level convolutional filters up to a desired depth similar to the architecture used in [Mnih et al., 2016] which as far as we know, is the only successful attempt at using Actor Critic algorithms in the Atari 2600 domain.", "startOffset": 125, "endOffset": 144}, {"referenceID": 9, "context": "The error function to be optimized and the manner in which gradients are to be backpropagated can closely follow the Actor-Critic implementation given in [Mnih et al., 2016].", "startOffset": 154, "endOffset": 173}, {"referenceID": 5, "context": "An alternative way of learning to perform repeated actions could be to use the previous action as input to the policy similar to [Foerster et al., 2016].", "startOffset": 129, "endOffset": 152}, {"referenceID": 12, "context": "As future work, we would also like to explore the DF paradigm on more games in the Atari 2600 domain with the more recent state of the art methods such as Prioritized Replay [Schaul et al., 2016], Dueling DQN [Wang et al.", "startOffset": 174, "endOffset": 195}, {"referenceID": 16, "context": ", 2016], Dueling DQN [Wang et al., 2016] and AsynchronousDQN [Mnih et al.", "startOffset": 21, "endOffset": 40}, {"referenceID": 9, "context": ", 2016] and AsynchronousDQN [Mnih et al., 2016].", "startOffset": 28, "endOffset": 47}], "year": 2017, "abstractText": "Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain. One of the important parameters in the Arcade Learning Environment (ALE, [Bellemare et al., 2013]) is the frame skip rate. It decides the granularity at which agents can control game play. A frame skip value of k allows the agent to repeat a selected action k number of times. The current state of the art architectures like Deep Q-Network (DQN,[Mnih et al., 2015]) and Dueling Network Architectures (DuDQN, [Wang et al., 2016]) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state. In this paper, we propose a new architecture, Dynamic Frame skip Deep QNetwork (DFDQN) which makes the frame skip rate a dynamic learnable parameter. This allows us to choose the number of times an action is to be repeated based on the current state. We show empirically that such a setting improves the performance on relatively harder games like Seaquest.", "creator": "LaTeX with hyperref package"}}}