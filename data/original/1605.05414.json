{"id": "1605.05414", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2016", "title": "On the Evaluation of Dialogue Systems with Next Utterance Classification", "abstract": "An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed Next-Utterance-Classification (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.", "histories": [["v1", "Wed, 18 May 2016 01:36:29 GMT  (910kb,D)", "https://arxiv.org/abs/1605.05414v1", "5 pages, submitted to SIGDIAL"], ["v2", "Sat, 23 Jul 2016 00:00:36 GMT  (912kb,D)", "http://arxiv.org/abs/1605.05414v2", "Accepted to SIGDIAL 2016 (short paper). 5 pages"]], "COMMENTS": "5 pages, submitted to SIGDIAL", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ryan lowe", "iulian v serban", "mike noseworthy", "laurent charlin", "joelle pineau"], "accepted": false, "id": "1605.05414"}, "pdf": {"name": "1605.05414.pdf", "metadata": {"source": "CRF", "title": "On the Evaluation of Dialogue Systems with Next Utterance Classification", "authors": ["Ryan Lowe", "Iulian V. Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau"], "emails": ["jpineau}@cs.mcgill.ca,", "michael.noseworthy@mail.mcgill.ca", "iulian.vlad.serban@umontreal.ca", "laurent.charlin@hec.ca"], "sections": [{"heading": "1 Introduction", "text": "Significant efforts have been made in recent years to develop computational methods for learning dialogue strategies offline from large amounts of text data. One of the challenges of this line of work is to develop methods to automatically evaluate, either directly or indirectly, models that are trained in this manner (Galley et al., 2015; Schatzmann et al., 2005), without requiring human labels or\n\u2217This work was primarily done while LC was at McGill University.\nhuman user experiments, which are time consuming and expensive. The use of automatic tasks and metrics is one key issue in scaling the development of dialogue systems from small domainspecific systems, which require significant engineering, to general conversational agents (Pietquin and Hastie, 2013).\nIn this paper, we consider tasks and evaluation measures for what we call \u2018unsupervised\u2019 dialogue systems, such as chatbots. These are in contrast to \u2018supervised\u2019 dialogue systems, which we define as those that explicitly incorporate some supervised signal such as task completion or user satisfaction. Unsupervised systems can be roughly separated into response generation systems that attempt to produce a likely response given a conversational context, and retrieval-based systems that attempt to select a response from a (possibly large) list of utterances in a corpus. While there has been significant work on building end-to-end response generation systems (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), it has recently been shown that many of the automatic evaluation metrics used for such systems correlate poorly or not at all with human judgement of the generated responses (Liu et al., 2016).\nRetrieval-based systems are of interest because they admit a natural evaluation metric, namely the recall and precision measures. First introduced for evaluating user simulations by Schatzmann et al. (2005), such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015; Dodge et al., 2016). These models are trained on the task of selecting the correct response from a candidate list, which we call NextUtterance-Classification (NUC, detailed in Section 3), and are evaluated using the metric of recall. NUC is useful for several reasons: 1) the performance (i.e. loss or error) is easy to comar X\niv :1\n60 5.\n05 41\n4v 2\n[ cs\n.C L\n] 2\n3 Ju\nl 2 01\n6\npute automatically, 2) it is simple to adjust the difficulty of the task, 3) the task is interpretable and amenable to comparison with human performance, 4) it is an easier task compared to generative dialogue modeling, which is difficult for endto-end systems (Sordoni et al., 2015; Serban et al., 2016), and 5) models trained with NUC can be converted to dialogue systems by retrieving from the full corpus (Liu et al., 2016). In this case, NUC additionally allows for making hard constraints on the allowable outputs of the system (to prevent offensive responses), and guarantees that the responses are fluent (because they were generated by humans). Thus, NUC can be thought of both as an intermediate task that can be used to evaluate the ability of systems to understand natural language conversations, similar to the bAbI tasks for language understanding (Weston et al., 2016), and as a useful framework for building chatbots. With the huge size of current dialogue datasets that contain millions of utterances (Lowe et al., 2015a; Banchs, 2012; Ritter et al., 2010) and the increasing amount of natural language data, it is conceivable that retrieval-based systems will be able to have engaging conversations with humans.\nHowever, despite the current work with NUC, there has been no verification of whether machine and human performance differ on this task. This cannot be assumed; it is possible that no significant gap exists between the two, as is the case with many current automatic response generation metrics (Liu et al., 2016). Further, it is important to benchmark human performance on new tasks such as NUC to determine when research has outgrown their use. In this paper, we consider to what extent NUC is achievable by humans, whether human performance varies according to expertise, and whether there is room for machine performance to improve (or has reached human performance already) and we should move to more complex conversational tasks. We performed a user study on three different datasets: the SubTle Corpus of movie dialogues (Banchs, 2012), the Twitter Corpus (Ritter et al., 2010), and the Ubuntu Dialogue Corpus (Lowe et al., 2015a). Since conversations in the Ubuntu Dialogue Corpus are highly technical, we recruit \u2018expert\u2019 humans who are adept with the Ubuntu terminology, whom we compare with a state-of-the-art machine learning agent on all datasets. We find that there is indeed a significant separation between machine and expert hu-\nman performance, suggesting that NUC is a useful intermediate task for measuring progress."}, {"heading": "2 Related Work", "text": "Evaluation methods for supervised systems have been well studied. They include the PARADISE framework (Walker et al., 1997), and MeMo (Mo\u0308ller et al., 2006), which include a measure of task completion. A more extensive overview of these metrics can be found in (Jokinen and McTear, 2009). We focus in this paper on unsupervised dialogue systems, for which proper evaluation is an open problem.\nRecent evaluation metrics for unsupervised dialogue systems include BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), which compare the similarity between response generated by the model, and the actual response of the participant, conditioned on some context of the conversation. Word perplexity, which computes a function of the probability of re-generating examples from the training corpus, is also used. However, such metrics have been shown to correlate very weakly with human judgement of the produced responses (Liu et al., 2016). They also suffer from several other drawbacks (Liu et al., 2016), including low scores, lack of interpretability, and inability to account for the vast space of acceptable outputs in natural conversation."}, {"heading": "3 Technical Background on NUC", "text": "Our long-term goal is the development and deployment of artificial conversational agents. Re-\ncent deep neural architectures offer perhaps the most promising framework for tackling this problem. However training such architectures typically requires large amounts of conversation data from the target domain, and a way to automatically assess prediction errors. Next-UtteranceClassification (NUC, see Figure 1) is a task, which is straightforward to evaluate, designed for training and validation of dialogue systems. They are evaluated using the metric of Recall@k, which we define in this section.\nIn NUC, a model or user, when presented with the context of a conversation and a (usually small) pre-defined list of responses, must select the most appropriate response from this list. This list includes the actual next response of the conversation, which is the desired prediction of the model. The other entries, which act as false positives, are sampled from elsewhere in the corpus. Note that no assumptions are made regarding the number of utterances in the context: these can be fixed or sampled from arbitrary distributions. Performance on this task is easy to assess by measuring the success rate of picking the correct next response; more specifically, we measure Recall@k (R@k), which is the percentage of correct responses (i.e. the actual response of the conversation) that are found in the top k responses with the highest rankings according to the model. This task has gained some popularity recently for evaluating dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015).\nThere are several attractive properties of this approach, as detailed in the introduction: the performance is easy to compute automatically, the task is interpretable and amenable to comparison with human performance, and it is easier than generative dialogue modeling. A particularly nice property is that one can adjust the difficulty of NUC by simply changing the number of false responses (from one response to the full corpus), or by altering the selection criteria of false responses (from randomly sampled to intentionally confusing). Indeed, as the number of false responses grows to encompass all natural language responses, the task becomes identical to response generation.\nOne potential limitation of the NUC approach is that, since the other candidate answers are sampled from elsewhere in the corpus, these may also represent reasonable responses given the context. Part of the contribution of this work is determining the significance of this limitation."}, {"heading": "4 Survey Methodology", "text": ""}, {"heading": "4.1 Corpora", "text": "We conducted our analysis on three corpora that have gained recent popularity for training dialogue systems. The SubTle Corpus (Banchs, 2012) consists of movie dialogues as extracted from subtitles, and includes turn-taking information indicating when each user has finished their turn. Unlike the larger OpenSubtitles1 dataset, the SubTle Corpus includes turn-taking information indicating when each user has finished their turn. The Twitter Corpus (Ritter et al., 2010) contains a large number of conversations between users on the microblogging platform Twitter. Finally, the Ubuntu Dialogue Corpus contains conversations extracted from IRC chat logs (Lowe et al., 2015a). 2 For more information on these datasets, we refer the reader to a recent survey on dialogue corpora (Serban et al., 2015). We focus our attention on these as they cover a range of popular domains, and are among the largest available dialogue datasets, making them good candidates for building data-driven dialogue systems. Note that while the Ubuntu Corpus is most relevant to supervised systems, the NUC task still applies in this domain. Models that take semantic information into account (i.e., to solve the user\u2019s problem) can still be validated with NUC, as demonstrated\n1http://www.opensubtitles.org 2http://irclogs.ubuntu.com\nin Lowe et al. (2015b). A group of 145 paid participants were recruited through Amazon Mechanical Turk (AMT), a crowdsourcing platform for obtaining human participants for various studies. Demographic data including age, level of education, and fluency of English were collected from the AMT participants, and is shown in Table 1. An additional 8 volunteers were recruited from the student population in the computer science department at the author\u2019s institution.3 This second group, referred to as \u201cLab experts\u201d, had significant exposure to technical terms prominent in the Ubuntu dataset; we hypothesized that this was an advantage in selecting responses for that corpus."}, {"heading": "4.2 Task description", "text": "Each participant was asked to answer either 30 or 40 questions (mean=31.9). To ensure a sufficient diversity of questions from each dataset, four versions of the survey with different questions were given to participants. For AMT respondents, the questions were approximately evenly distributed across the three datasets, while for the lab experts, half of the questions were related to Ubuntu and the remainder evenly split across Twitter and movies. Each question had 1 correct response, and 4 false responses drawn uniformly at random from elsewhere in the (same) corpus. An example question can be seen in Figure 1. Participants had a time limit of 40 minutes.\nConversations were extracted to form NUC conversation-response pairs as described in Sec. 3. The number of utterances in the context were sampled according to the procedure in (Lowe et al., 2015a), with a maximum context length of 6 turns \u2014 this was done for both the human trials and ANN model. All conversations were preprocessed in order to anonymize the utterances. For the Twitter conversations, this was extended to replacing all user mentions (words beginning with @) throughout the utterance with a placeholder \u2018@user\u2019 symbol, as these are often repeated in a conversation. Hashtags were not removed, as these are often used in the main body of tweets, and many tweets are illegible without them. Conversations were edited or pruned to remove offensive language according to ethical guidelines.\n3None of these participants were directly involved with this research project."}, {"heading": "4.3 ANN model", "text": "In order to compare human results with a strong artificial neural network (ANN) model, we use the dual encoder (DE) model from Lowe et al. (2015a). This model uses recurrent neural networks (RNNs) with long-short term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) to encode the context c of the conversation, and a candidate response r. More precisely, at each time step, a word xt is input into the LSTM, and its hidden state is updated according to: ht = f(Whht\u22121+Wxxt), whereW are weight matrices, and f(\u00b7) is some non-linear activation function. After all T words have been processed, the final hidden state hT can be considered a vector representation of the input sequence.\nTo determine the probability that a response r is the actual next response to some context c, the model computes a weighted dot product between the vector representations c, r \u2208 Rd of the context and response, respectively:\nP (r is correct response) = \u03c3(c>Mr)\nwhere M is a matrix of learned parameters, and \u03c3 is the sigmoid function. The model is trained to minimize the cross-entropy error of contextresponse pairs. For training the authors randomly sample negative examples.\nThe DE model is close to state-of-the-art for neural network dialogue models on the Ubuntu Dialogue Corpus; we obtained further results on the Movie and Twitter corpora in order to facilitate comparison with humans. For further model implementation details, see Lowe et al. (2015a)."}, {"heading": "5 Results", "text": "As we can see from Table 1, the AMT participants are mostly young adults, fluent in English with some undergraduate education. The split across genders is approximately equal, and the majority of respondents had never used Ubuntu before.\nTable 2 shows the NUC results on each corpus. The human results are separated into AMT non-experts, consisting of paid respondents who have \u2018Beginner\u2019 or no knowledge of Ubuntu terminology; AMT experts, who claimed to have \u2018Intermediate\u2019 or \u2018Advanced\u2019 knowledge of Ubuntu; and Lab experts, who are the non-paid respondents with Ubuntu experience and university-level computer science training. We also presents results on the same task for a state-of-the-art artificial neural\nnetwork (ANN) dialogue model (see (Lowe et al., 2015a) for implementation details).\nWe first observe that subjects perform above chance level (20% for R@1) on all domains, thus the task is doable for humans. Second we observe difference in performances between the three domains. The Twitter dataset appears to have the best predictability, with a Recall@1 approximately 8% points higher than for the movie dialogues for AMT workers, and 18% higher for lab experts. Rather than attributing this to greater familiarity with Twitter than movies, it seems more likely that it is because movie utterances are often short, generic (e.g. contain few topic-related words), and lack proper context (e.g., video cues and the movie\u2019s story). Conversely, tweets are typically more specific, and successive tweets may have common hashtags.\nAs expected, untrained respondents scored lowest on the Ubuntu dataset, as it contains the most difficult language with often unfamiliar terminology. Further, since the domain is narrow, randomly drawn false responses could be more likely to resemble the actual next response, especially to someone unfamiliar with Ubuntu terminology. We also observe that the ANN model achieves similar performance to the paid human respondents from AMT. However, the model is still significantly behind the lab experts for Recall@1.\nAn interesting note is that there is very little difference between the paid AMT non-experts and AMT experts on Ubuntu. This suggests that the participants do not provide accurate self-rating of expertise, either intentionally or not. We also found that lab experts took on average approximately 50% more time to complete the survey than paid testers; this is reflected in the results,\nwhere the lab experts score 30% higher on the Ubuntu Corpus, and even 5-10% higher on the non-technical Movie and Twitter corpora. While we included attention check questions to ensure the quality of responses,4 this reflects poorly on the ability of crowdsourced workers to answer technical questions, even if they self-identify as being adept with the technology."}, {"heading": "6 Discussion", "text": "Our results demonstrate that humans outperform current dialogue models on the task of NextUtterance-Classification, indicating that there is plenty of room for improvement for these models to better understand the nature of human dialogue. While our results suggest that NUC is a useful task, it is by no means sufficient; we strongly advocate for automatically evaluating dialogue systems with as many relevant metrics as possible. Further research should be conducted into finding metrics or tasks which accurately reflect human judgement for the evaluation of dialogue systems.\nAcknowledgements The authors gratefully acknowledge financial support for this work by the Samsung Advanced Institute of Technology (SAIT) and the Natural Sciences and Engineering Research Council of Canada (NSERC)."}], "references": [{"title": "Movie-dic: A movie dialogue corpus for research and development", "author": ["R.E. Banchs."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2.", "citeRegEx": "Banchs.,? 2012", "shortCiteRegEx": "Banchs.", "year": 2012}, {"title": "METEOR: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie."], "venue": "ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.", "citeRegEx": "Banerjee and Lavie.,? 2005", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Interval estimation for a binomial proportion", "author": ["L.D. Brown", "T.T. Cai", "A. DasGupta."], "venue": "Statistical science, pages 101\u2013117.", "citeRegEx": "Brown et al\\.,? 2001", "shortCiteRegEx": "Brown et al\\.", "year": 2001}, {"title": "Evaluating prerequisit qualities for learning end-to-end dialog systems", "author": ["J. Dodge", "A. Gane", "X. Zhang", "A. Bordes", "S. Chopra", "A. Miller", "A. Szlam", "J. Weston."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Dodge et al\\.,? 2016", "shortCiteRegEx": "Dodge et al\\.", "year": 2016}, {"title": "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["M. Galley", "C. Brockett", "A. Sordoni", "Y. Ji", "M. Auli", "C. Quirk", "M. Mitchell", "J. Gao", "B. Dolan."], "venue": "Proceedings of the Annual Meeting of the Association", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Spoken Dialogue Systems", "author": ["K. Jokinen", "M. McTear."], "venue": "Morgan Claypool.", "citeRegEx": "Jokinen and McTear.,? 2009", "shortCiteRegEx": "Jokinen and McTear.", "year": 2009}, {"title": "Improved deep learning baselines for ubuntu corpus dialogs", "author": ["R. Kadlec", "M. Schmid", "J. Kleindienst."], "venue": "NIPS on Machine Learning for Spoken Language Understanding.", "citeRegEx": "Kadlec et al\\.,? 2015", "shortCiteRegEx": "Kadlec et al\\.", "year": 2015}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["C. Liu", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau."], "venue": "arXiv preprint arXiv:1603.08023.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau."], "venue": "Proceedings of SIGDIAL.", "citeRegEx": "Lowe et al\\.,? 2015a", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Incorporating unstructured textual knowledge sources into neural dialogue systems", "author": ["R. Lowe", "N. Pow", "I.V. Serban", "L. Charlin", "J. Pineau."], "venue": "NIPS Workshop on Machine Learning for Spoken Language Understanding.", "citeRegEx": "Lowe et al\\.,? 2015b", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Memo: towards automatic usability evaluation of spoken dialogue services by user error simulations", "author": ["S. M\u00f6ller", "R. Englert", "K.-P. Engelbrecht", "V.V. Hafner", "A. Jameson", "A. Oulasvirta", "A. Raake", "N. Reithinger."], "venue": "INTERSPEECH.", "citeRegEx": "M\u00f6ller et al\\.,? 2006", "shortCiteRegEx": "M\u00f6ller et al\\.", "year": 2006}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu."], "venue": "Proceedings of the 40th annual meeting on Association for Computational Linguistics (ACL).", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A survey on metrics for the evaluation of user simulations", "author": ["O. Pietquin", "H. Hastie."], "venue": "The Knowledge Engineering Review.", "citeRegEx": "Pietquin and Hastie.,? 2013", "shortCiteRegEx": "Pietquin and Hastie.", "year": 2013}, {"title": "Unsupervised modeling of twitter conversations", "author": ["A. Ritter", "C. Cherry", "B. Dolan."], "venue": "North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["J. Schatzmann", "K. Georgila", "S. Young."], "venue": "Proceedings of SIGDIAL.", "citeRegEx": "Schatzmann et al\\.,? 2005", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["I.V. Serban", "R. Lowe", "L. Charlin", "J. Pineau."], "venue": "arXiv preprint arXiv:1512.05742.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A.C. Courville", "J. Pineau."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI), 2016, pages", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li."], "venue": "arXiv preprint arXiv:1503.02364.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J. Nie", "J. Gao", "B. Dolan."], "venue": "Conference of the North American Chapter of the Association for", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le."], "venue": "ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Paradise: A framework for evaluating spoken dialogue agents", "author": ["M.A. Walker", "D.J. Litman", "C.A. Kamm", "A. Abella."], "venue": "Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, pages 271\u2013", "citeRegEx": "Walker et al\\.,? 1997", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2016", "shortCiteRegEx": "Weston et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "to develop methods to automatically evaluate, either directly or indirectly, models that are trained in this manner (Galley et al., 2015; Schatzmann et al., 2005), without requiring human labels or", "startOffset": 116, "endOffset": 162}, {"referenceID": 15, "context": "to develop methods to automatically evaluate, either directly or indirectly, models that are trained in this manner (Galley et al., 2015; Schatzmann et al., 2005), without requiring human labels or", "startOffset": 116, "endOffset": 162}, {"referenceID": 13, "context": "neering, to general conversational agents (Pietquin and Hastie, 2013).", "startOffset": 42, "endOffset": 69}, {"referenceID": 20, "context": "While there has been significant work on building end-to-end response generation systems (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), it has recently", "startOffset": 89, "endOffset": 152}, {"referenceID": 18, "context": "While there has been significant work on building end-to-end response generation systems (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), it has recently", "startOffset": 89, "endOffset": 152}, {"referenceID": 17, "context": "While there has been significant work on building end-to-end response generation systems (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016), it has recently", "startOffset": 89, "endOffset": 152}, {"referenceID": 8, "context": "been shown that many of the automatic evaluation metrics used for such systems correlate poorly or not at all with human judgement of the generated responses (Liu et al., 2016).", "startOffset": 158, "endOffset": 176}, {"referenceID": 9, "context": "(2005), such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015; Dodge et al., 2016).", "startOffset": 104, "endOffset": 165}, {"referenceID": 7, "context": "(2005), such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015; Dodge et al., 2016).", "startOffset": 104, "endOffset": 165}, {"referenceID": 3, "context": "(2005), such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015; Dodge et al., 2016).", "startOffset": 104, "endOffset": 165}, {"referenceID": 11, "context": "First introduced for evaluating user simulations by Schatzmann et al. (2005), such a framework has gained recent prominence for the evaluation of end-to-end dialogue systems (Lowe et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 19, "context": "difficulty of the task, 3) the task is interpretable and amenable to comparison with human performance, 4) it is an easier task compared to generative dialogue modeling, which is difficult for endto-end systems (Sordoni et al., 2015; Serban et al., 2016), and 5) models trained with NUC can be", "startOffset": 211, "endOffset": 254}, {"referenceID": 17, "context": "difficulty of the task, 3) the task is interpretable and amenable to comparison with human performance, 4) it is an easier task compared to generative dialogue modeling, which is difficult for endto-end systems (Sordoni et al., 2015; Serban et al., 2016), and 5) models trained with NUC can be", "startOffset": 211, "endOffset": 254}, {"referenceID": 8, "context": "converted to dialogue systems by retrieving from the full corpus (Liu et al., 2016).", "startOffset": 65, "endOffset": 83}, {"referenceID": 22, "context": "for language understanding (Weston et al., 2016), and as a useful framework for building chatbots.", "startOffset": 27, "endOffset": 48}, {"referenceID": 9, "context": "With the huge size of current dialogue datasets that contain millions of utterances (Lowe et al., 2015a; Banchs, 2012; Ritter et al., 2010) and the increas-", "startOffset": 84, "endOffset": 139}, {"referenceID": 0, "context": "With the huge size of current dialogue datasets that contain millions of utterances (Lowe et al., 2015a; Banchs, 2012; Ritter et al., 2010) and the increas-", "startOffset": 84, "endOffset": 139}, {"referenceID": 14, "context": "With the huge size of current dialogue datasets that contain millions of utterances (Lowe et al., 2015a; Banchs, 2012; Ritter et al., 2010) and the increas-", "startOffset": 84, "endOffset": 139}, {"referenceID": 8, "context": "cant gap exists between the two, as is the case with many current automatic response generation metrics (Liu et al., 2016).", "startOffset": 104, "endOffset": 122}, {"referenceID": 0, "context": "We performed a user study on three different datasets: the SubTle Corpus of movie dialogues (Banchs, 2012), the Twitter Corpus (Ritter et al.", "startOffset": 92, "endOffset": 106}, {"referenceID": 14, "context": "We performed a user study on three different datasets: the SubTle Corpus of movie dialogues (Banchs, 2012), the Twitter Corpus (Ritter et al., 2010), and the Ubuntu Dialogue Corpus (Lowe et al.", "startOffset": 127, "endOffset": 148}, {"referenceID": 9, "context": ", 2010), and the Ubuntu Dialogue Corpus (Lowe et al., 2015a).", "startOffset": 40, "endOffset": 60}, {"referenceID": 0, "context": "We find that there is indeed a significant separation between machine and expert huFigure 1: An example NUC question from the SubTle Corpus (Banchs, 2012).", "startOffset": 140, "endOffset": 154}, {"referenceID": 21, "context": "They include the PARADISE framework (Walker et al., 1997), and MeMo (M\u00f6ller et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 11, "context": ", 1997), and MeMo (M\u00f6ller et al., 2006), which include a measure of task completion.", "startOffset": 18, "endOffset": 39}, {"referenceID": 6, "context": "of these metrics can be found in (Jokinen and McTear, 2009).", "startOffset": 33, "endOffset": 59}, {"referenceID": 1, "context": "2002) and METEOR (Banerjee and Lavie, 2005), which compare the similarity between response generated by the model, and the actual response of the participant, conditioned on some context of the conversation.", "startOffset": 17, "endOffset": 43}, {"referenceID": 8, "context": "However, such metrics have been shown to correlate very weakly with human judgement of the produced responses (Liu et al., 2016).", "startOffset": 110, "endOffset": 128}, {"referenceID": 8, "context": "fer from several other drawbacks (Liu et al., 2016), including low scores, lack of interpretability, and inability to account for the vast space of acceptable outputs in natural conversation.", "startOffset": 33, "endOffset": 51}, {"referenceID": 9, "context": "This task has gained some popularity recently for evaluating dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015).", "startOffset": 78, "endOffset": 119}, {"referenceID": 7, "context": "This task has gained some popularity recently for evaluating dialogue systems (Lowe et al., 2015a; Kadlec et al., 2015).", "startOffset": 78, "endOffset": 119}, {"referenceID": 0, "context": "The SubTle Corpus (Banchs, 2012) consists of movie dialogues as extracted from subti-", "startOffset": 18, "endOffset": 32}, {"referenceID": 14, "context": "The Twitter Corpus (Ritter et al., 2010) contains a large number of conversations between users on the microblogging platform Twitter.", "startOffset": 19, "endOffset": 40}, {"referenceID": 9, "context": "Finally, the Ubuntu Dialogue Corpus contains conversations extracted from IRC chat logs (Lowe et al., 2015a).", "startOffset": 88, "endOffset": 108}, {"referenceID": 16, "context": "2 For more information on these datasets, we refer the reader to a recent survey on dialogue corpora (Serban et al., 2015).", "startOffset": 101, "endOffset": 122}, {"referenceID": 9, "context": "in Lowe et al. (2015b).", "startOffset": 3, "endOffset": 23}, {"referenceID": 9, "context": "The number of utterances in the context were sampled according to the procedure in (Lowe et al., 2015a), with a maximum context length of 6 turns \u2014 this was done for both the human trials and ANN model.", "startOffset": 83, "endOffset": 103}, {"referenceID": 9, "context": "In order to compare human results with a strong artificial neural network (ANN) model, we use the dual encoder (DE) model from Lowe et al. (2015a). This model uses recurrent neu-", "startOffset": 127, "endOffset": 147}, {"referenceID": 5, "context": "ral networks (RNNs) with long-short term memory (LSTM) units (Hochreiter and Schmidhuber, 1997) to encode the context c of the conversation, and a candidate response r.", "startOffset": 61, "endOffset": 95}, {"referenceID": 9, "context": "plementation details, see Lowe et al. (2015a).", "startOffset": 26, "endOffset": 46}, {"referenceID": 9, "context": "7% (Lowe et al., 2015a)", "startOffset": 3, "endOffset": 23}, {"referenceID": 2, "context": "Starred (*) results indicate a poor approximation of the confidence interval due to high scores with small sample size, according to the rule of thumb by Brown et al. (2001).", "startOffset": 154, "endOffset": 174}], "year": 2016, "abstractText": "An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed NextUtterance-Classification (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.", "creator": "TeX"}}}