{"id": "1605.05710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2016", "title": "Active Learning On Weighted Graphs Using Adaptive And Non-adaptive Approaches", "abstract": "This paper studies graph-based active learning, where the goal is to reconstruct a binary signal defined on the nodes of a weighted graph, by sampling it on a small subset of the nodes. A new sampling algorithm is proposed, which sequentially selects the graph nodes to be sampled, based on an aggressive search for the boundary of the signal over the graph. The algorithm generalizes a recent method for sampling nodes in unweighted graphs. The generalization improves the sampling performance using the information gained from the available graph weights. An analysis of the number of samples required by the proposed algorithm is provided, and the gain over the unweighted method is further demonstrated in simulations. Additionally, the proposed method is compared with an alternative state of-the-art method, which is based on the graph's spectral properties. It is shown that the proposed method significantly outperforms the spectral sampling method, if the signal needs to be predicted with high accuracy. On the other hand, if a higher level of inaccuracy is tolerable, then the spectral method outperforms the proposed aggressive search method. Consequently, we propose a hybrid method, which is shown to combine the advantages of both approaches.", "histories": [["v1", "Wed, 18 May 2016 19:21:22 GMT  (620kb,D)", "http://arxiv.org/abs/1605.05710v1", "In ICASSP 2016"]], "COMMENTS": "In ICASSP 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eyal en gad", "akshay gadde", "a salman avestimehr", "antonio ortega"], "accepted": false, "id": "1605.05710"}, "pdf": {"name": "1605.05710.pdf", "metadata": {"source": "CRF", "title": "ACTIVE LEARNING ON WEIGHTED GRAPHS USING ADAPTIVE AND NON-ADAPTIVE APPROACHES", "authors": ["Eyal En Gad", "Akshay Gadde", "A. Salman Avestimehr", "Antonio Ortega"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 active learning on graphs, adaptive and nonadaptive sampling of graph signals, sampling complexity"}, {"heading": "1. INTRODUCTION", "text": "This paper studies the problem of binary label prediction on a graph. In this problem, we are given a graph G = (V,E), where the edges E (which can be weighted) capture the similarity relationship between the objects represented by the nodes V . Each node has an initially unknown label associated with it, given by a signal f : V \u2192 {\u22121,+1}. The goal is to reconstruct the entire signal by sampling its values on a small subset of the nodes. This is achievable when the signal bears some degree of smoothness over the graph, which means that similar objects are more likely to have the same label. Active learning aims to minimize the number of samples needed by selecting the most informative nodes.\nThis problem arises in many machine learning applications, where there is an abundance of unlabeled data but labeled data is scarce and expensive to obtain, for example, requiring human expertise or elaborate experiments. Active learning is an effective way to minimize the cost of labeling in such scenarios [1]. A graph based approach to this problem starts by creating a graph where the nodes correspond to the data points X = {x1, . . . ,xn} and the edges capture the similarity between them. Typically, a sparse graph which connects each data point to few of its most similar neighbors is used. The unknown labels fi \u2208 {\u22121,+1} associated with the data points\nThis work is supported in part by NSF under grants CCF-1410009 and CCF-1527874\ndefine a binary function on the nodes. In datasets of interest, the signal is often notably smooth on the graph.\nThere are two approaches to active learning on graphs. The first approach focuses on identifying the nodes near the boundary region, where the signal changes from +1 to\u22121. The methods with this approach [2, 3] sample nodes sequentially, i.e., the nodes to be sampled next are chosen based on the graph structure as well as previously observed signal values. The second approach [4, 5, 6], in contrast, utilizes global properties of the graph in order to identify the most informative nodes, and sample them all at once. Such global approaches usually focus on providing a good approximation of the signal, rather than exact recovery. It is also possible to combine the two approaches, for example, as in [7].\nThe first contribution of this paper is a new sampling algorithm, called weighted S2 which takes the boundary sampling approach. The weighted S2 algorithm is a generalization of a recently proposed algorithm called S2 [2], which is defined only for the case of unweighted edges. The purpose of the generalized algorithm is to take advantage of the additional information available in the form of the edge weights, in order to reduce the sampling complexity. We characterize the sampling budget required for signal recovery by the weighted S2 algorithm, as a function of the complexity of the signal with respect to the graph. We explain how this generalization can be useful in reducing sampling complexity, and demonstrate a significant reduction (nearly 25% in one dataset), when neighboring nodes of opposite labels are considerably less similar to each other than identically labeled neighbors.\nWe further compare the sampling complexity of the weighted S2 algorithm with an alternative state-of-the-art method called the cutoff maximization method [4]. Unlike the S2 methods, which aim for a complete recovery of the signal by aggressively searching for the boundary nodes, the cutoff maximization method is focused only on providing a good approximation of the signal by ensuring that the unsampled nodes are well-connected to the sampled nodes [8]. This method finds a sampling set by optimizing a spectral function defined using the graph Laplacian. We perform the comparison on three realistic data sets, and observe two interesting results:\n1. The cutoff maximization method does not discover the entire boundary (i.e, nodes with oppositely labeled neighbors) unless the sampling set contains almost all the nodes. In contrast, the number of nodes required by the S2 methods to discover the boundary is not considerably larger than the number of boundary nodes.\n2. When the sampling budget is quite limited, the cutoff maximization method provides a much better approximation of the signal. There exists a threshold in terms of the sampling budget that determines which method offers better accuracy. Conversely, the tolerable degree of inaccuracy determines which of the methods offer lower sampling complexity.\nMotivated by the second observation, we propose a hybrid approach (similar in spirit to [7]) which samples the first few nodes\nar X\niv :1\n60 5.\n05 71\n0v 1\n[ cs\n.L G\n] 1\n8 M\nay 2\n01 6\nwith the cutoff maximization method to approximate the boundary and then switches to the weighted S2 method to refine the approximation. The experiments suggest that the hybrid approach combines the advantages of both methods."}, {"heading": "2. S2 ALGORITHM FOR WEIGHTED GRAPHS", "text": "The S2 algorithm was proposed in [2] for unweighted graphs. In this section we describe the principle of the algorithm, and then generalize the algorithm to weighted graphs. In the next section we analyze the query complexity of the generalized algorithm."}, {"heading": "2.1. Original S2 Algorithm", "text": "The goal of the algorithm is to find the signal f . To do this, the algorithm operates by finding the edges that connect oppositely labeled nodes. These edges are called cut edges, and together the set of cut edges is called the cut. The algorithm incrementally identifies the cut, with the rationale that once the entire cut is identified, the signal is completely recovered. To find the cut, the algorithm maintains a copy of the graph G, and each time it samples a node that neighbors previously sampled nodes of opposite label, it removes the newly discovered cut edges from the graph copy. This way, the remaining graph copy contains only the undiscovered part of the cut, and the algorithm can more easily focus on discovering these edges.\nThe main idea of the algorithm is to look for a pair of oppositely labeled nodes, and then to find the cut edge on the shortest path between the nodes, using a binary search procedure. The algorithm begins with a random sampling phase. At this phase the algorithm queries a random node, according to the uniform distribution. After each sample, the algorithm checks whether the sampled node has any previously sampled neighbors of opposite label. If such neighbors exist, then the connecting edges are newly discovered cut edges, and they are removed from the graph. After checking and potentially removing newly discovered cut edges, the algorithm checks whether the remaining graph contains any pair of connected nodes of opposite labels. If no such pair exists, the algorithm proceeds with a random sample. If pairs of connected, oppositely labeled nodes do exist, the algorithm looks for the shortest path among all the paths connecting such pairs, and sample the node in the middle of that path (breaking ties arbitrarily). After each sampling operation, either a random sample or a bisecting sample, the algorithm again removes all newly discovered cut edges.\nThe S2 algorithm is described more formally in Algorithm 1. The algorithm is given a budget, which determines the number of queries to perform. Once the budget is exhausted, the algorithm calls a label completion function, which predicts the labels of the unsampled nodes. Several such label completion algorithms are known, such as the POCS method in [9]. The S2 algorithm uses a function called Middle Shortest Path (MSP), which returns the node in the middle of the shortest path among all the paths connecting a pair of oppositely labels nodes in the remaining graph."}, {"heading": "2.2. Generalization for Weighted Graphs", "text": "The S2 algorithm in [2] is defined only for unweighted graphs. Since many learning scenarios provide a weighted graph, we extend the algorithm to exploit the additional available information by modifying the MSP function in the algorithm. Our modification is based on the assumption that the signal is smooth, which means that high-weight edges connect mostly nodes of the same label. Therefore, the weight of cut edges is generally low.\nIn the unweighted S2 algorithm, each MSP query reduces the number of nodes in the shortest of the paths between any two oppo-\nAlgorithm 1 S2 Algorithm Inputs: Graph G, BUDGET \u2264 n.\n1: L\u2190 \u2205 2: while 1 do 3: x\u2190 Randomly chosen unlabeled node. 4: do 5: Add (x, f(x)) to L. 6: Remove newly discovered cut edges from G. 7: if |L| = BUDGET then 8: return LabelCompletion(G,L) 9: end if\n10: while x\u2190 MSP(G,L) exists 11: end while\nsitely labeled nodes by approximately one half. The main idea in our generalization is to take advantage of the low weights of cut edges in order to reduce this number by more than a half with each query. To do this, we first switch our perspective, for convenience, from the edge weights to the distances associated with the edges, which are inversely proportional to the weights. The distance between nonneighboring nodes is defined as the sum of the lengths of the edges in the shortest path connecting the nodes. Since the weights are a decreasing function of the distances, it follows that cut edges are typically longer than other edges. We take advantage of this fact by modifying the MSP function to sample the node closest to the midpoint of the path, where the midpoint is computed in terms of length, rather than the number of edges in the path. With each query, the proposed sampling rule can potentially reduce the number of nodes along the shortest of the paths between any two oppositely labeled nodes by more than half if the cut edges contribute significantly more than the non-cut edges to the length of the path. Thus, ultimately it requires less samples to discover a cut edge. This intuition is demonstrated in Figure 1. In this example, the nodes labeled +1 are connected with an edge of length l/2, the nodes labeled \u22121 are connected with an edge of length l and the cut edge is of length 3l. Given the labels of the end nodes of this path, the binary search phase of the unweighted S2 algorithm needs to sample labels of 3 extra nodes to discover the cut edge. The weighted S2 algorithm, on the other hand, finds the cut edge with only 2 samples. This type of situation arises more prominently in an unbalanced data set, where the number of nodes in one class is much larger than the other. The advantage of weighted S2 algorithm in such a case is experimentally verified in Section 4."}, {"heading": "3. ANALYSIS OF THE WEIGHTED S2 ALGORITHM", "text": ""}, {"heading": "3.1. Notation", "text": "Note that f partitions the vertices ofG into a collection of connected components with identically labeled vertices. Let V1, V2, . . . , Vk be these k connected components. Notice that the first node that S2 queries in each collection Vi is often queried randomly, and not by a bisection query. Define\n\u03b2 , min 1\u2264i\u2264k |Vi| n .\nIf \u03b2 is small, more random queries are required by S2. Let C be the set of cut edges in G. The length of the shortest cut edge in G is\nFig. 2: The interval of interest is at least halved after two queries.\ndenoted by lcut. Let \u2202C be the set of nodes which share an edge with at least one oppositely labeled node. The nodes in \u2202C are called boundary nodes.\nFor 1 \u2264 i < j \u2264 k, let Ci,j be the subset of C for which each edge {x, y} \u2208 Ci,j satisfies x \u2208 Vi and y \u2208 Vj . If Ci,j is not empty for some 1 \u2264 i < j \u2264 k, then it is called a cut component. The number of cut components is denoted by m. For a pair of nodes v1 and v2, let d(v1, v2) denote the length of the shortest path between v1 and v2. Define ln = maxv1,v2\u2208V d(v1, v2)."}, {"heading": "3.2. Cut Clustering", "text": "For nodes x, y \u2208 V , let dG(x, y) be the length of the shortest path connecting x and y in G. Let e1 = {x1, y1} and e2 = {x2, y2} be a pair of cut edges inG such that f(x1) = f(x2) and f(y1) = f(y2). Define\n\u03b4(e1, e2) = d G\u2212C(x1, x2) + d G\u2212C(y1, y2) + max{le1 , le2},\nwhere G \u2212 C is the graph G with all the cut edges removed. Let Hr = (C, E) be the meta graph whose nodes are the cut edges of G, and {e, e\u2032} \u2208 E iff \u03b4(e, e\u2032) \u2264 r. Let l\u03ba be the smallest number for whichHl\u03ba hasm connected components. The motivation for the definition of l\u03ba is demonstrated in the following lemma.\nLemma 1. Consider a case in which after the removal of an edge e by the weighted S2 algorithm, there exist an undiscovered cut edge e\u2032 in the same cut component of e. Then the length of the shortest path between two oppositely labeled nodes in the remaining graph is at most l\u03ba.\nProof. Consider the connected component of the meta graph Hl\u03ba that contains the removed cut edge (as a meta node). By the assumptions of the lemma, there must be at least one meta edge in this connected component that connects a discovered cut edge to an undiscovered cut edge. This meta edge corresponds to a path length at most l\u03ba in the remaining graph, proving the lemma."}, {"heading": "3.3. Query Complexity", "text": "Theorem 1. Suppose that a graph G = (V,E) and a signal f are such that the induced cut set C has m components with cut clustering l\u03ba. Then for any > 0, the weighted S2 will recover C with probability at least 1\u2212 if the BUDGET is at least\nm \u2308 2 log2 ( ln lcut )\u2309 +(|\u2202C|\u2212m) \u2308 2 log2 ( l\u03ba lcut )\u2309 + log(1/(\u03b2 )) log(1/(1\u2212 \u03b2)) .\nThe proof of Theorem 1 uses the fact that after a pair of connected and oppositely labeled nodes is found, the number of queries until a boundary node is sampled is at most logarithmic in (l/lcut), where l is the length of the path between the nodes. We show this fact in the following lemma.\nLemma 2. The cut-edge of length lcut is found after no more than r = \u2308 2 log2 ( l lcut )\u2309 aggressive steps.\nProof. In Figure 2, let C and D denote the queried nodes, such that C is sampled first. LetE denote the midpoint of the interval between A and B, where there may not be a node. By considering all the cases based on labels of C and D and their positions relative to E,\nit can be shown that after two queries, the length of the interval of interest (i.e., the interval containing the cut-edge) is at least halved. The details are omitted in the interest of space.\nAfter i (where i is even) queries, the length of the interval of interest is at most l\n2i/2 . Note that the cut-edge is found when the\nlength of the interval of interest is less than or equal to lcut. If r is the maximum number of queries required to locate the cut-edge, then\nl\n2r/2 = lcut \u21d2 r =\n\u2308 2 log2 ( l\nlcut\n)\u2309 (1)\nProof of Theorem 1. The random sampling phase follows the same argument as in [2], which gives the term log(1/(\u03b2 ))\nlog(1/(1\u2212\u03b2)) . A sequence of bisection queries that commences after a random query or an edge removal, and terminates with an edge removal, is call a run. In each run, at least one boundary node is being queried. Therefore, the number of runs is no greater than |\u2202C|. In each cut component, after the first cut edge and boundary node are discovered, the rest of the boundary nodes are discovered in \u2308 2 log2 ( l\u03ba lcut )\u2309 queries each, according to Lemmas 1 and 2. For discovering the first cut edge in each cut components, we trivially bound the number of queries by \u2308 2 log2 ( ln lcut )\u2309 . Since there are m cut components, we bound the total number of bisection queries by\nm \u2308 2 log2 ( ln lcut )\u2309 + (|\u2202C| \u2212m) \u2308 2 log2 ( l\u03ba lcut )\u2309 ."}, {"heading": "4. EXPERIMENTS", "text": "We consider the following graph based classification problems: 1. USPS handwritten digit recognition [10]: For our experiments, we consider two binary classification sub-problems, namely, 7 vs. 9 and 2 vs. 4, consisting of 200 randomly selected images of each class represented as vectors of dimension 256. The distance between two data points is d(i, j) = ||xi \u2212 xj ||. An unweighted graph G is constructed by connecting a pair of nodes (i, j) if j is a k-nearest neighbor (k-nn with k = 4) 1 of i or vice versa. A weighted dissimilarity graph Gd is defined to have the same topology as G but the weight associated with edge (i, j) is set to d(i, j). A weighted similarity graph Gw is defined to have the same topology as G but the weight associated with edge (i, j) is set to w(i, j) = exp ( \u2212d(i, j)2/2\u03c32 ) . The parameter \u03c3 is set to be 1/3-rd of the average distance to the k-th nearest neighbor for all datapoints.\n2. Newsgroups text classification [11]: For our experiments, we consider a binary classification sub-problem Baseball vs. Hockey, where each class contains 200 randomly selected documents. Each document i is represented by a 3000 dimensional vector xi whose elements are the tf-idf statistics of the 3000 most frequent words in the dataset [8]. The cosine similarity between a pair data points (i, j) is given by w(i, j) = \u3008xi,xj\u3009||xi||||xj || . The distance between them is defined as d(i, j) = \u221a 1\u2212 w(i, j)2. The k-nn unweighted graph G (with k = 4), the dissimilarity graph Gd and the similarity graph Gw are constructed using these distance and similarity measures as in the previous example.\nIn addition to the above datasets, we generate a synthetic two circles dataset, shown in Figure 4, in order to demonstrate the advantage of weighted S2 over unweighted S2. It contains 900 points\n1Due to lack of space, we do not study the effect of k in detail.\nin one class (marked red) evenly distributed on the inner circle of mean radius 1 and variance 0.05 and 100 points in the second class (marked blue) on the outer circle of mean radius 1.1 and variance 0.45. A 4-nn graph is constructed using the Euclidean distance between the coordinates of the points.\nWe compare the performance of the following active learning methods: (1) unweighted S2 method [2] with graph G, (2) weighted S2 method with dissimilarity graph Gd, (3) cutoff maximization method [8] with similarity graph Gw and (4) a hybrid approach combining cutoff maximization and weighted S2 method. After the nodes selected by each method have been sampled, we reconstruct the unknown label signal using the approximate POCS based bandlimited reconstruction scheme [8] to get the soft labels. We threshold these soft labels to get the final label predictions. The hybrid approach uses the non-adaptive cutoff maximization approach in the beginning and switches to the weighted S2 method after sampling a certain number of nodes nswitch. In order to determine nswitch, after sampling the i-th node with the cutoff method, we compute 1 \u2212 \u3008f\u0302i,f\u0302i\u22121\u3009\u2016f\u0302i\u2016\u2016f\u0302i\u22121\u2016 , where f\u0302i denotes the vector of predicted soft labels. Once this value falls below below 0.001, indicating that the newly added label only marginally changed the predictions, hybrid approach switches to weighted S2.\nTable 1 lists the number of samples required by each of the sam-\npling methods to discover all the cut edges using the observed labels. It shows that weighted S2 can reduce the sample complexity significantly (by 25%) compared to unweighted S2 if the ratio of mean length of cut edges and mean length of non-cut edges is high as is the case in the unbalanced two circles dataset. In rest of the datasets, the gain offered by weighted S2 is negligible since the cut edges are only slightly longer than non-cut edges and as a result, taking lengths into account in the bisection phase does not offer much advantage. We also observe that the number of samples required by the weighted S2 method is close to the size of the cut |\u2202C| in most of the datasets. Table 1 also shows that the adaptive methods S2 and weighted S2 are very efficient at recovering the entire cut exactly, compared to the non-adaptive cutoff maximization method.\nIn practice, it is not necessary to reconstruct the signal exactly and some reconstruction error is allowed. Figure 3 plots the classification error against the number of sampled nodes. It shows that the classification error of the cutoff maximization method decreases rapidly in the early stages of sampling when very few samples are observed. However, the decrease is slow in later stages of sampling. S2 methods, on the other hand, are good at reducing the error in the later stages, but performs poorly with only a few samples. The figure also shows that the hybrid method performs as well as the better method in each region."}, {"heading": "5. CONCLUSIONS", "text": "The paper generalizes the S2 algorithm for the case of weighted graphs. The sampling complexity of the generalized algorithm is analyzed, and the gain over the unweighted version is demonstrated by simulation. Additional experiments identify the region of tolerable reconstruction error in which the S2 algorithms outperforms a graph frequency based global approach. A hybrid approach is proposed with the advantages of both methods. It remains open to analytically characterize of the gain of the weighted S2 method over the unweighted version. Another interesting avenue for future work is to provide a performance analysis for the spectral sampling method which can suggest an optimal switching criterion for the hybrid method."}, {"heading": "6. REFERENCES", "text": "[1] Burr Settles, \u201cActive learning literature survey,\u201d Computer Sciences Technical Report 1648, University of Wisconsin\u2013 Madison, 2010.\n[2] Gautam Dasarathy, Robert Nowak, and Xiaojin Zhu, \u201cS2: An efficient graph based active learning algorithm with application to nonparametric classification,\u201d Journal of Machine Learning Research, vol. 40, pp. 1\u201320, 2015.\n[3] Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani, \u201cCombining active learning and semi-supervised learning using gaussian fields and harmonic functions,\u201d in ICML 2003 workshop on the continuum from labeled to unlabeled data in machine learning and data mining, 2003, pp. 58\u201365.\n[4] A. Anis, A. Gadde, and A. Ortega, \u201cTowards a sampling theorem for signals on arbitrary graphs,\u201d in Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference on, 2014.\n[5] M. Ji and J. Han, \u201cA variance minimization criterion to active learning on graphs,\u201d in International Conference on Artificial Intelligence and Statistics (AISTATS), 2012, vol. 22, pp. 556\u2013 564.\n[6] Andrew Guillory and Jeff Bilmes, \u201cActive semi-supervised learning using submodular functions,\u201d in Proceedings of 27th Conference on Uncertainty in Artificial Intelligence, 2011, pp. 274\u2013282.\n[7] Thomas Osugi, Deng Kim, and Stephen Scott, \u201cBalancing exploration and exploitation: A new algorithm for active machine learning,\u201d in Data Mining, Fifth IEEE International Conference on. IEEE, 2005, pp. 8\u2013pp.\n[8] A. Gadde, A. Anis, and A. Ortega, \u201cActive semi-supervised learning using sampling theory for graph signals,\u201d in ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 492\u2013501.\n[9] S. K. Narang, A. Gadde, E. Sanou, and A. Ortega, \u201cLocalized iterative methods for interpolation in graph structured data,\u201d in Signal and Information Processing (GlobalSIP), IEEE Global Conference on, 2013.\n[10] \u201cUSPS handwritten digits data,\u201d http://www.cs.nyu. edu/\u02dcroweis/data.html.\n[11] \u201c20 newsgroups data,\u201d http://qwone.com/\u02dcjason/ 20Newsgroups/."}], "references": [{"title": "Active learning literature survey", "author": ["Burr Settles"], "venue": "Computer Sciences Technical Report 1648, University of Wisconsin\u2013 Madison, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "S: An efficient graph based active learning algorithm with application to nonparametric classification", "author": ["Gautam Dasarathy", "Robert Nowak", "Xiaojin Zhu"], "venue": "Journal of Machine Learning Research, vol. 40, pp. 1\u201320, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Combining active learning and semi-supervised learning using gaussian fields and harmonic functions", "author": ["Xiaojin Zhu", "John Lafferty", "Zoubin Ghahramani"], "venue": "ICML 2003 workshop on the continuum from labeled to unlabeled data in machine learning and data mining, 2003, pp. 58\u201365.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards a sampling theorem for signals on arbitrary graphs", "author": ["A. Anis", "A. Gadde", "A. Ortega"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference on, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A variance minimization criterion to active learning on graphs", "author": ["M. Ji", "J. Han"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS), 2012, vol. 22, pp. 556\u2013 564.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Active semi-supervised learning using submodular functions", "author": ["Andrew Guillory", "Jeff Bilmes"], "venue": "Proceedings of 27th Conference on Uncertainty in Artificial Intelligence, 2011, pp. 274\u2013282.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Balancing exploration and exploitation: A new algorithm for active machine learning", "author": ["Thomas Osugi", "Deng Kim", "Stephen Scott"], "venue": "Data Mining, Fifth IEEE International Conference on. IEEE, 2005, pp. 8\u2013pp.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Active semi-supervised learning using sampling theory for graph signals", "author": ["A. Gadde", "A. Anis", "A. Ortega"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 492\u2013501.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Localized iterative methods for interpolation in graph structured data", "author": ["S.K. Narang", "A. Gadde", "E. Sanou", "A. Ortega"], "venue": "Signal and Information Processing (GlobalSIP), IEEE Global Conference on, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Active learning is an effective way to minimize the cost of labeling in such scenarios [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "The methods with this approach [2, 3] sample nodes sequentially, i.", "startOffset": 31, "endOffset": 37}, {"referenceID": 2, "context": "The methods with this approach [2, 3] sample nodes sequentially, i.", "startOffset": 31, "endOffset": 37}, {"referenceID": 3, "context": "The second approach [4, 5, 6], in contrast, utilizes global properties of the graph in order to identify the most informative nodes, and sample them all at once.", "startOffset": 20, "endOffset": 29}, {"referenceID": 4, "context": "The second approach [4, 5, 6], in contrast, utilizes global properties of the graph in order to identify the most informative nodes, and sample them all at once.", "startOffset": 20, "endOffset": 29}, {"referenceID": 5, "context": "The second approach [4, 5, 6], in contrast, utilizes global properties of the graph in order to identify the most informative nodes, and sample them all at once.", "startOffset": 20, "endOffset": 29}, {"referenceID": 6, "context": "It is also possible to combine the two approaches, for example, as in [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "The weighted S algorithm is a generalization of a recently proposed algorithm called S [2], which is defined only for the case of unweighted edges.", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "We further compare the sampling complexity of the weighted S algorithm with an alternative state-of-the-art method called the cutoff maximization method [4].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Unlike the S methods, which aim for a complete recovery of the signal by aggressively searching for the boundary nodes, the cutoff maximization method is focused only on providing a good approximation of the signal by ensuring that the unsampled nodes are well-connected to the sampled nodes [8].", "startOffset": 292, "endOffset": 295}, {"referenceID": 6, "context": "Motivated by the second observation, we propose a hybrid approach (similar in spirit to [7]) which samples the first few nodes ar X iv :1 60 5.", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "The S algorithm was proposed in [2] for unweighted graphs.", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Several such label completion algorithms are known, such as the POCS method in [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "The S algorithm in [2] is defined only for unweighted graphs.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "The random sampling phase follows the same argument as in [2], which gives the term log(1/(\u03b2 )) log(1/(1\u2212\u03b2)) .", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "Each document i is represented by a 3000 dimensional vector xi whose elements are the tf-idf statistics of the 3000 most frequent words in the dataset [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 1, "context": "We compare the performance of the following active learning methods: (1) unweighted S method [2] with graph G, (2) weighted S method with dissimilarity graph Gd, (3) cutoff maximization method [8] with similarity graph Gw and (4) a hybrid approach combining cutoff maximization and weighted S method.", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "We compare the performance of the following active learning methods: (1) unweighted S method [2] with graph G, (2) weighted S method with dissimilarity graph Gd, (3) cutoff maximization method [8] with similarity graph Gw and (4) a hybrid approach combining cutoff maximization and weighted S method.", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "After the nodes selected by each method have been sampled, we reconstruct the unknown label signal using the approximate POCS based bandlimited reconstruction scheme [8] to get the soft labels.", "startOffset": 166, "endOffset": 169}], "year": 2016, "abstractText": "This paper studies graph-based active learning, where the goal is to reconstruct a binary signal defined on the nodes of a weighted graph, by sampling it on a small subset of the nodes. A new sampling algorithm is proposed, which sequentially selects the graph nodes to be sampled, based on an aggressive search for the boundary of the signal over the graph. The algorithm generalizes a recent method for sampling nodes in unweighted graphs. The generalization improves the sampling performance using the information gained from the available graph weights. An analysis of the number of samples required by the proposed algorithm is provided, and the gain over the unweighted method is further demonstrated in simulations. Additionally, the proposed method is compared with an alternative stateof-the-art method, which is based on the graph\u2019s spectral properties. It is shown that the proposed method significantly outperforms the spectral sampling method, if the signal needs to be predicted with high accuracy. On the other hand, if a higher level of inaccuracy is tolerable, then the spectral method outperforms the proposed aggressive search method. Consequently, we propose a hybrid method, which is shown to combine the advantages of both approaches.", "creator": "LaTeX with hyperref package"}}}