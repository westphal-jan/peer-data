{"id": "1605.06069", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "abstract": "Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.", "histories": [["v1", "Thu, 19 May 2016 17:59:02 GMT  (786kb,D)", "http://arxiv.org/abs/1605.06069v1", "14 pages, 5 tables, 3 figures"], ["v2", "Fri, 20 May 2016 16:02:30 GMT  (786kb,D)", "http://arxiv.org/abs/1605.06069v2", "15 pages, 5 tables, 3 figures"], ["v3", "Tue, 14 Jun 2016 02:21:04 GMT  (905kb,D)", "http://arxiv.org/abs/1605.06069v3", "15 pages, 5 tables, 4 figures"]], "COMMENTS": "14 pages, 5 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["iulian vlad serban", "alessandro sordoni", "ryan lowe", "laurent charlin", "joelle pineau", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1605.06069"}, "pdf": {"name": "1605.06069.pdf", "metadata": {"source": "CRF", "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "authors": ["Iulian V. Serban", "Alessandro Sordoni", "Ryan Lowe", "Yoshua Bengio"], "emails": ["iulian.vlad.serban@umontreal.ca", "alessandro.sordoni@umontreal.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca", "ryan.lowe@cs.mcgill.ca", "lcharlin@cs.mcgill.ca", "jpineau@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].\nWhile these advances are impressive, the underlying RNNs tend to have a fairly simple structure, in the sense that the only variability or stochasticity in the model occurs when an output is sampled. This is often an inappropriate place to inject variability [2, 6, 1]. This is especially true for sequential data such as speech and natural language that possess a hierarchical generation process with complex intra-sequence dependencies. For instance, natural language dialogue involves at least two levels of structure; within a single utterance the structure is dominated by local statistics of the language, while across utterances there is a distinct source of uncertainty (or variance) characterized by aspects such as conversation topic, speaker goals and speaker style.\nIn this paper we introduce a novel hierarchical stochastic latent variable neural network architecture to explicitly model generative processes that possess multiple levels of variability. We evaluate the proposed model on the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model qualitatively through manual inspection, and quantitatively using a human evaluation on Amazon Mechanical Turk and using automatic evaluation metrics. The results demonstrate that the model improves upon recently proposed models. In particular, the results highlight that the latent variables help to both facilitate the generation of long utterances with more information content, and to maintain the dialogue context.\n1Y.B. is a CIFAR senior Fellow\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 5.\n06 06\n9v 1\n[ cs\n.C L\n] 1\n9 M"}, {"heading": "2 Technical Background", "text": ""}, {"heading": "2.1 Recurrent Neural Network Language Model", "text": "A recurrent neural network (RNN), with parameters \u03b8, models a variable-length sequence of tokens (w1, . . . , wM ) by decomposing the probability distribution over outputs:\nP\u03b8(w1, . . . , wM ) = M\u220f m=2 P\u03b8(wm | w1, . . . , wm\u22121)P (w1). (1)\nThe model processes each observation recursively. At each time step, the model observes an element and updates its internal hidden state, hm = f(hm\u22121, wm), where f is a parametrized non-linear function, such as the hyperbolic tangent, the LSTM gating unit [12] or the GRU gating unit [5].2 The hidden state acts as a sufficient statistic, which summarizes the past sequence and parametrizes the output distribution of the model: P\u03b8(wm+1 | w1, . . . , wm) = P\u03b8(wm+1 | hm). We assume the outputs lie within a discrete vocabulary V . Under this assumption, the RNN Language Model (RNNLM) [18], the simplest possible generative RNN for discrete sequences, parametrizes the output distribution using the softmax function applied to an affine transformation of the hidden state hm. The model parameters are learned by maximizing the training log-likelihood using gradient descent."}, {"heading": "2.2 Hierarchical Recurrent Encoder-Decoder", "text": "The hierarchical recurrent encoder-decoder model (HRED) [26, 24] is an extension of the RNNLM. It extends the encoder-decoder architecture [5] to the natural dialogue setting. The HRED assumes that each output sequence can be modelled in a two-level hierarchy: sequences of sub-sequences, and sub-sequences of tokens. For example, a dialogue may be modelled as a sequence of utterances (sub-sequences), with each utterance modelled as a sequence of words. Similarly, a natural-language document may be modelled as a sequence of sentences (sub-sequences), with each sentence modelled as a sequence of words. The HRED model consists of three RNN modules: an encoder RNN, a context RNN and a decoder RNN. Each sub-sequence of tokens is deterministically encoded into a real-valued vector by the encoder RNN. This is given as input to the context RNN, which updates its internal hidden state to reflect all information up to that point in time. The context RNN deterministically outputs a real-valued vector, which the decoder RNN conditions on to generate the next sub-sequence of tokens. For additional details see [26, 24]."}, {"heading": "2.3 A Deficient Generation Process", "text": "In the recent literature, it has been observed that the RNNLM and HRED, and similar models based on RNN architectures, have critical problems generating meaningful dialogue utterances [24, 15]. We believe that the root cause of these problems arise from the parametrization of the output distribution in the RNNLM and HRED, which imposes a strong constraint on the generation process: the only source of variation is modelled through the conditional output distribution. This is detrimental from two perspectives: from a probabilistic perspective, with stochastic variations injected only at the low level, the model is encouraged to capture local structure in the sequence, rather than global or long-term structure. This is because random variations injected at the lower level are strongly constrained to be in line with the immediate previous observations, but only weakly constrained to be in line with older observations or with future observations. One can think of random variations as injected via i.i.d. noise variables, added to deterministic components, for example. If this noise is injected at a higher level of representation, spanning longer parts of the sequence, its effects could correspond to longer-term dependencies. Second, from a computational learning perspective, the state hm of the RNNLM (or decoder RNN of HRED) has to summarize all the past information up to time step m in order to (a) generate a probable next token (short term goal) and simultaneously (b) to occupy a position in embedding space which sustains a realistic output trajectory, in order to generate probable future tokens (long term goal). Due to the vanishing gradient effect, shorter-term goals will have more influence: finding a compromise between these two disparate forces will likely lead the training procedure to model parameters that focus too much on predicting only the next output token. In particular for high-entropy sequences, the models are very likely to favour short-term predictions\n2We concatenate the LSTM cell and cell input hidden states into a single state hm for notational simplicity.\nas opposed to long-term predictions, because it is easier to only learn hm for predicting the next token compared to sustaining a long-term trajectory, which at every time step is perturbed by a highly noisy source (the observed token)."}, {"heading": "3 Latent Variable Hierarchical Recurrent Encoder-Decoder (VHRED)", "text": "Motived by the previous discussion, we now introduce the latent variable hierarchical recurrent encoder-decoder (VHRED) model. This model augments the HRED model with a latent variable at the decoder, which is trained by maximizing a variational lower-bound on the log-likelihood. This allows it to model hierarchically-structured sequences in a two-step generation process\u2014first sampling the latent variable, and then generating the output sequence\u2014while maintaining long-term context.\nLet w1, . . . ,wN be a sequence consisting of N sub-sequences, where wn = (wn,1, . . . , wn,Mn) is the n\u2019th sub-sequence and wn,m \u2208 V is the m\u2019th discrete token in that sequence. The VHRED model uses a stochastic latent variable zn \u2208 Rdz for each sub-sequence n = 1, . . . , N conditioned on all previous observed tokens. Given zn, the model next generates the n\u2019th sub-sequence tokens wn = (wn,1, . . . , wn,Mn):\nP\u03b8(zn | w1, . . . ,wn\u22121) = N (\u00b5prior(w1, . . . ,wn\u22121),\u03a3prior(w1, . . . ,wn\u22121)), (2)\nP\u03b8(wn | zn,w1, . . . ,wn\u22121) = Mn\u220f m=1 P\u03b8(wn,m | zn,w1, . . . ,wn\u22121, wn,1, . . . , wn,m\u22121), (3)\nwhere N (\u00b5,\u03a3) is the multivariate normal distribution with mean \u00b5 \u2208 Rdz and covariance matrix \u03a3 \u2208 Rdz\u00d7dz , which is constrained to be a diagonal matrix. In detail, The VHRED model (Figure 1) contains the same three components as the HRED model. The encoder RNN deterministically encodes a single sub-sequence into a fixed-size real-valued vector. The context RNN deterministically takes as input the output of the encoder RNN, and encodes all previous sub-sequences into a fixed-size real-valued vector. This vector is fed into a one-layer feed-forward neural network with hyperbolic tangent gating function. A matrix multiplication is applied to the output of the feed-forward network, which defines the multivariate normal mean \u00b5prior. Similarly, for the diagonal covariance matrix \u03a3prior another matrix multiplication is applied to the net\u2019s output followed by softplus function, to ensure positiveness [6].\nThe model\u2019s latent variables are inferred by maximizing the variational lower-bound, which factorizes into independent terms for each sub-sequence:\nlogP\u03b8(w1, . . . ,wN ) \u2265 N\u2211 n=1 \u2212KL [Q\u03c8(zn | w1, . . . ,wn)||P\u03b8(zn | w1, . . . ,wn\u22121)]\n+ EQ\u03c8(zn|w1,...,wn) [logP\u03b8(wn | zn,w1, . . . ,wn\u22121)] , (4)\nwhere KL[Q||P ] is the Kullback-Leibler (KL) divergence between distributions Q and P . The distribution Q\u03c8(z | w1, . . . , wM ) is the approximate posterior distribution (also known as the encoder model or recognition model), which aims to approximate the intractable true posterior distribution:\nQ\u03c8(zn | w1, . . . ,wN ) = Q\u03c8(zn | w1, . . . ,wn) = N (\u00b5posterior(w1, . . . ,wn),\u03a3posterior(w1, . . . ,wn)) \u2248 P\u03c8(zn | w1, . . . ,wN ), (5)\nwhere \u00b5posterior defines the approximate posterior mean and \u03a3posterior defines the approximate posterior covariance matrix (assumed diagonal) as a function of the previous sub-sequences w1, . . . ,wn\u22121 and the current sub-sequence wn. The posterior mean \u00b5posterior and covariance \u03a3posterior are determined in the same way as the prior, via a matrix multiplication with the output of the feed-forward network, with a softplus function applied for the covariance.\nAt test time, conditioned on the previous observed sub-sequences (w1, . . . ,wn\u22121), a sample zn is drawn from the prior N (\u00b5prior(w1, . . . ,wn\u22121),\u03a3prior(w1, . . . ,wn\u22121)) for each sub-sequence. This sample is concatenated with the output of the context RNN and given as input to the decoder RNN as in the HRED model, which then generates the sub-sequence token-by-token.\nAt training time, for n = 1, . . . , N , a sample zn is drawn from the approximate posterior N (\u00b5posterior(w1, . . . ,wn),\u03a3posterior(w1, . . . ,wn)) and used to estimate the gradient of the variational lower-bound given by Eq. (4). The approximate posterior is parametrized by its own one-layer feed-forward neural network, which takes as input the output of the context RNN at the current time step, as well as the output of the encoder RNN for the next sub-sequence.\nThe VHRED model greatly helps to reduce the problems with the generation process used by the RNNLM and HRED model outlined above. The variation of the output sequence is now modelled in two ways: at the sequence-level with the conditional prior distribution over z, and at the subsequence-level (token-level) with the conditional distribution over tokensw1, . . . , wM . The variable z helps model long-term output trajectories, by representing high-level information about the sequence, which in turn allows the variable hm to primarily focus on summarizing the information up to token M . Intuitively, the randomness injected by the variable z corresponds to higher-level decisions, like topic or sentiment of the sentence."}, {"heading": "4 Experimental Evaluation", "text": "We consider the problem of conditional natural language response generation for dialogue. This is an interesting problem with applications in areas such as customer service, technical support, language learning and entertainment [29]. It is also a task domain that requires learning to generate sequences with complex structures while taking into account long-term context [17, 27].\nWe consider two tasks. For each task, the model is given a dialogue context, consisting of one or more utterances, and the goal of the model is to generate an appropriate next response to the dialogue. We first perform experiments on a Twitter Dialogue Corpus [22]. The task is to generate utterances to append to existing Twitter conversations. The dataset is extracted using a procedure similar to Ritter et al. [22], and is split into training, validation and test sets, containing respectively 749, 060, 93, 633 and 10, 000 dialogues. Each dialogue contains 6.27 utterances and 94.16 tokens on average. The dialogues are fairly long compared to recent large-scale language modelling corpora, such as the 1 Billion Word Language Model Benchmark [4], which focus on modelling single sentences. We also experiment on the Ubuntu Dialogue Corpus [17], which contains about 500, 000 dialogues extracted from the #Ubuntu Internet Relayed Chat channel. Users enter the chat channel with a Ubuntu-related technical problem, and other users try to help them. For further details see Appendix 6.1. We chose these corpora because they are large, and have different purposes\u2014Ubuntu dialogues are typically goal driven, where as Twitter dialogues typically contain social interaction (\"chit-chat\")."}, {"heading": "4.1 Training and Evaluation Procedures", "text": "We optimize all models using Adam [13]. We choose our hyperparameters and early stop with patience using the variational lower-bound [9]. At test time, we use beam search with 5 beams for outputting responses with the RNN decoders [10]. For the VHRED models, we sample the latent variable zn, and condition on it when executing beam search with the RNN decoder. For Ubuntu we use word embedding dimensionality of size 300, and for Twitter we use word embedding dimensionality of size 400. All models were trained with a learning rate of 0.0001 or 0.0002 and with mini-batches containing 40 or 80 training examples. We use a variant of truncated back-propagation and we apply gradient clipping. Further details are given in Appendix 6.2.\nBaselines On both Twitter and Ubuntu we compare to an LSTM model of 2000 hidden units. On Ubuntu, the HRED model has 500, 1000 and 500 hidden units for the encoder, context and decoder RNNs respectively. The encoder RNN is a standard GRU RNN. For reference, we also include a non-neural network baseline, specifically the TF-IDF retrieval-based model proposed in [17].\nVHRED The encoder and context RNNs for the VHRED model are parametrized in the same way as the corresponding HRED models. The only difference in the parametrization of the decoder RNN is that the context RNN output vector is now concatenated with the generated stochastic latent variable. Furthermore, we initialize the feed-forward networks of the prior and posterior distributions with values drawn from a zero-mean normal distribution with variance 0.01 and with biases equal to zero. We also multiply the diagonal covariance matrices of the prior and posterior distributions with 0.1 to make training more stable, because a high variance makes the gradients w.r.t. the reconstruction cost unreliable, which is fatal at the beginning of the training process.\nThe VHRED\u2019s encoder and context RNNs are initialized to the parameters of the corresponding converged HRED models. We also use two heuristics proposed by Bowman et al. [3]: we drop words in the decoder with a fixed drop rate of 25% and multiply the KL terms in eq. (4) by a scalar, which starts at zero and linearly increases to 1 over the first 60, 000 and 75, 000 training batches on Twitter and Ubuntu respectively. Applying these heuristics helped substantially to stabilize the training process and make the model use the stochastic latent variables. We experimented with the batch normalization training procedure for the feed-forward neural networks. but found that this made training very unstable without any substantial gains in performance w.r.t. the variational bound.\nEvaluation Accurate evaluation of dialogue system responses is a difficult problem [8, 20]. Inspired by metrics for machine translation and information retrieval, researchers have begun adopting wordoverlap metrics, however Liu et al. [16] show that such metrics have little correlation with human evaluations of response quality. We therefore carry out a human evaluation to compare responses from the different models. We also compute several statistics and automatic metrics on model responses to characterize differences between the model-generated responses.\nWe carry out the human study for the Twitter Dialogue Corpus on Amazon Mechanical Turk (AMT). We do not conduct AMT experiments on Ubuntu as evaluating these responses usually requires technical expertise, which is not prevalent among AMT users. We set up the evaluation study as a series of pairwise comparison experiments.3 We show human evaluators a dialogue context along with two potential responses, one generated from each model (conditioned on dialogue context). We ask participants to choose the response most appropriate to the dialogue context. If the evaluators are indifferent to either of the two responses, or if they cannot understand the dialogue context, they can choose neither response. For each pair of models we conduct two experiments: one where the example contexts contain at least 80 tokens (long context), and one where they contain at least 20 tokens (short context). This helps compare how well each model can integrate the dialogue context into its response, since it has previously been hypothesized that for long contexts hierarchical RNNs models fare better [24, 26]. Screenshots and further details of the experiments are in Appendix 6.4."}, {"heading": "4.2 Results of Human Evaluation", "text": "The results (table 1) show that VHRED is clearly preferred in the majority of the experiments. In particular, VHRED is strongly preferred over the HRED and TF-IDF baseline models for both short and long context settings. VHRED is also preferred over the LSTM baseline model for long contexts; however, the LSTM is preferred over VHRED for short contexts. We believe this is because the\n3Source code for the AMT experiments will be released upon publication."}, {"heading": "Opponent Wins Losses Ties Wins Losses Ties", "text": ""}, {"heading": "Context Response", "text": "LSTM baseline tends to output much more generic responses (see Table 4); since it doesn\u2019t model the hierarchical input structure, the LSTM model has a shorter-term memory, and thus must output a response based primarily on the end of the last utterance. Such \u2018safe\u2019 responses are reasonable for a wider range of contexts, meaning that human evaluators are more likely to rate them as appropriate. However, we argue that a model that only outputs generic responses is undesirable for dialogue, as this leads to uninteresting and less engaging conversations. Conversely, the VHRED model is explicitly designed for long contexts, and to output a diverse set of responses due to the sampling of the latent variable. Thus, the VHRED model generates longer sentences with more semantic content than the LSTM model (see tables 3-4). This can be \u2018riskier\u2019 as longer utterances are more likely to contain small mistakes, which can lead to lower human preference for a single utterance. However, we believe that response diversity is crucial to maintaining interesting conversations \u2014 in the dialogue literature, generic responses are used primarily as \u2018back-off\u2019 strategies in case the agent has no interesting response that is relevant to the context [25].\nThe above hypotheses are confirmed upon qualitative assessment of the generated responses (Table 2). VHRED generates longer and more meaningful responses compared to the LSTM model, which generates mostly generic responses. Additionally, we observed that the VHRED model has learned to better model smilies, slang (see first example in Table 2) and can even continue conversations in different languages (see fth example).4 Such aspects are not measured by the human study. Further, VHRED appears to be better at generating stories or imaginative actions compared to the generative baseline models (see third example). The last example in Table 2 is a case where the VHRED generated response is more interesting, yet may be less prefered by humans as it is slightly incompatible with the context, compared to the generic LSTM response. In the next section, we back\n4There is a notable amount of Spanish and Dutch conversations in the corpus."}, {"heading": "Model Average Greedy Extrema Average Greedy Extrema 1-turn", "text": "these examples quantitatively, showing that the VHRED model learns to generate longer responses with more information content that share semantic similarity to the context and ground-truth response."}, {"heading": "4.3 Results of Metric-based Evaluation", "text": "To show the VHRED responses are more on-topic and share semantic similarity to the ground-truth response, we consider three textual similarity metrics based on word embeddings. The Embedding Average (Average) metric projects the model response and ground truth response into two separate real-valued vectors by taking the mean over the word embeddings in each response, and then computes the cosine similarity between them [19]. This metric is widely used for measuring textual similarity. The Embedding Extrema (Extrema) metric similarly embeds the responses by taking the extremum (maximum of the absolute value) of each dimension, and afterwards computes the cosine similarity between them.The Embedding Greedy (Greedy) metric is more fine-grained; it uses cosine similarity between word embeddings to find the closest word in the human-generated response for each word in the model response. Given the (non-exclusive) alignment between words in the two responses, the mean over the cosine similarities is computed for each pair of questions [23]. Since this metric takes into account the alignment between words, it should be more accurate for long responses. While these metrics do not strongly correlate with human judgements of generated responses, we interpret them as measuring topic similarity: if the model generated response has similar semantic content to the ground truth human response, then the metrics will yield a high score. To ease reproducibility, we use the publicly available Word2Vec word embeddings trained on the Google News Corpus.5\nWe compute these metrics in two settings: one where the models generate a single response (1-turn), and one where they generate the next three consecutive utterances (3-turns) (Table 3). Overall, VHRED seems to better capture the ground truth response topic than either the LSTM or HRED models. The fact that VHRED does better in particular in the setting where the model generates three consecutive utterances strongly suggests that hidden states in both the decoder and context RNNs of the VHRED models are better able to follow trajectories which remain on-topic w.r.t the dialogue context. This supports our computational hypothesis that the stochastic latent variable helps modulate the training procedure to achieve a better trade-off between short-term and long-term generation. We also observed the same trend when computing the similarity metrics between the model generated responses and the corresponding context, which further reinforces this hypothesis.\nTo show that the VHRED responses contain more information content than other model responses, we compute the average response length and average entropy (in bits) w.r.t. the maximum likelihood unigram model over the generated responses (table 4). The unigram entropy is computed on the preprocessed tokenized datasets. VHRED produces responses with higher entropy per word on both Ubuntu and Twitter compared to the HRED and LSTM models. VHRED also produces longer responses overall on Twitter, which translates into responses containing on average 6 bits of information more than the HRED model. Since the actual dialogue responses contain even more information per word than any of the generative models, it reasonable to assume that a higher entropy is desirable. Thus, VHRED compares favourably to recently proposed models in the literature, which often output extremely low-entropy (generic) responses such as OK and I don\u2019t know [24, 15].\n5https://code.google.com/archive/p/word2vec/\nFinally, the fact that VHRED produces responses with higher entropy suggests that its responses are on average more diverse than the responses produced by the HRED and LSTM models. This implies that the trajectories of the hidden states of the VHRED model traverse a larger area of the space compared to the hidden states of the HRED and LSTM baselines, which further supports our hypothesis that the stochastic latent variable helps the VHRED model achieve a better trade-off between short-term and long-term generation."}, {"heading": "5 Related Work", "text": "The use of a stochastic latent variable learned by maximizing a variational lower bound is inspired by the variational autoencoder (VAE) [14, 21]. Such models have been used predominantly for generating images in the continuous domain [11]. However, there has also been recent work applying these architectures for generating sequences, such as the Variational Recurrent Neural Networks (VRNN) [6], which was applied for speech and handwriting synthesis, and Stochastic Recurrent Networks (STORN) [1], which was applied for music generation and motion capture modeling. Both the VRNN and STORN incorporate stochastic latent variables into RNN architectures, but unlike the VHRED they sample a separate latent variable at each time step of the decoder. This does not exploit the hierarchical structure in the data, and thus does not model higher-level variability.\nSimilar to our work is the Variational Recurrent Autoencoder [7] and the Variational Autoencoder Language Model [3], which apply encoder-decoder architectures to generative music modeling and language modeling respectively. The VHRED model is different from these in the following ways. The VHRED latent variable is conditioned on all previous sub-sequences (sentences). This enables the model to generate multiple sub-sequences (sentences), but it also makes the latent variables co-dependent through the observed tokens. The VHRED model builds on the hierarchical architecture of the HRED model, which makes the model applicable to generation conditioned on long contexts. It has a direct deterministic connection between the context and decoder RNN, which allows the model to transfer deterministic pieces of information between its components.6 Crucially, VHRED also demonstrates improved results beyond the autoencoder framework, where the objective is not input reconstruction but the conditional generation of the next utterance in a dialogue."}, {"heading": "6 Discussion", "text": "We have introduced a novel latent variable neural network architecture, called VHRED. The model uses a hierarchical generation process in order to exploit the structure in sequences and is trained using a variational lower bound on the log-likelihood. We have applied the proposed model on the difficult task of dialogue response generation, and have demonstrated that it is an improvement over previous models in several ways, including quality of responses as measured in a human study. The empirical results highlight the advantages of the hierarchical generation process for modelling high-entropy sequences. Finally, it is worth nothing that the proposed model is very general. It can in principle be applied to any sequential generation task that exhibits a hierarchical structure, such as document-level machine translation, web query prediction, multi-sentence document summarization, multi-sentence image caption generation, and others.\n6Our initial experiments confirmed that the deterministic connection between the context RNN to the decoder RNN was indeed beneficial in terms of lowering the variational bound."}, {"heading": "Appendix", "text": ""}, {"heading": "6.1 Dataset Details", "text": "Our Twitter Dialogue Corpus was extracted in 2011. We perform a minimal preprocessing on the dataset to remove irregular punctuation marks and tokenize it using the Moses tokenizer: https://github.com/ moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl.\nWe use the Ubuntu Dialogue Corpus v2.0 extracted in Jamuary 2016 from: http://cs.mcgill.ca/~jpineau/ datasets/ubuntu-corpus-1.0/. The preprocessed version of the dataset will be made available to the public."}, {"heading": "6.2 Model Details", "text": "The model implementations will be released to the public upon acceptance of the paper."}, {"heading": "Training and Generation", "text": "We validate each model on the entire validation set every 5000 training batches.\nAs mentioned in the main text, at test time we use beam search with 5 beams for outputting responses with the RNN decoders [10]. We define the beam cost as the log-likelihood of the tokens in the beam divided by the number of tokens it contains. This is a well-known modification, which is often applied in machine translation models. In principle, we could sample from the RNN decoders of all the models, but is well known that such sampling produces poor results in comparison to the beam search procedure. It also introduces additional variance into the evaluation procedure, which will make the human study very expensive or even impossible within a limited budget."}, {"heading": "Baseline Models", "text": "On Ubuntu, the gating function between the context RNN and decoder RNN is a one-layer feed-forward neural network with hyperbolic tangent activation function.\nOn Twitter, the HRED model encoder RNN is a bidirectional GRU RNN encoder, where the forward and backward RNNs each have 1000 hidden units, and context RNN and decoder RNN have each 1000 hidden units. On Twitter, the decoder RNN computes a 1000 dimensional real-valued vector for each hidden time step, which is multiplied with the output context RNN. The result is feed through a one-layer feed-forward neural network with hyperbolic tangent activation function, which the decoder RNN then takes as input. We found that this worked slightly better for the HRED baseline, but a more careful choice of hyperparameters is likely to make this additional step unnecessary."}, {"heading": "Context Response", "text": ""}, {"heading": "6.3 Model Examples", "text": ""}, {"heading": "6.4 Human Study on Amazon Mechanical Turk", "text": ""}, {"heading": "Setup", "text": "We choose to use crowdsourcing platforms such as AMT rather than carrying out in-lab experiments, even though in-lab experiments usually exhibit less noise and result in higher agreement between human annotators. We do this because AMT experiments involve a larger and more heterogeneous pool of annotators, which implies less cultural and geographic biases, and because such experiments are easier to replicate, which we believe is important for benchmarking future research on these tasks.\nAllowing the AMT human evaluators to not assign preference for either response is important, since there are several reasons why humans may not understand the dialogue context, which include topics they are not familiar with, slang language and non-English language. We refer to such evaluations as \u2018indeterminable\u2019.\nThe evaluation setup resembles the classical Turing Test where human judges have to distinguish between human-human conversations and human-computer conversations. However, unlike the original Turing Test, we only ask human evaluators to consider the next utterance in a given conversation and we do not inform them that any responses were generated by a computer. Apart from minimum context and response lengths we impose no restrictions on the generated responses."}, {"heading": "Selection Process", "text": "At the beginning of each experiment, we briefly instruct the human evaluator on the task and show them a simple example of a dialogue context and two potential responses. To avoid presentation bias, we shuffle the order of the examples and the order of the potential responses for each example. During each experiment, we also show four trivial \u2018attention check\u2019 examples that any human evaluator who has understood the task should be able to answer correctly. We discard responses from human evaluators who fail more than one of these checks.\nWe select the examples shown to human evaluators at random from the test set. We filter out all non-English conversations and conversations containing offensive content. This is done by automatically filtering out all conversations with non-ascii characters and conversations with profanities, curse words and otherwise offensive content. This filtering is not perfect, so we manually skim through many conversations and filter out conversations with non-English languages and offensive content. On average, we remove about 1/80 conversations manually. To ensure that the evaluation process is focused on evaluating conditional dialogue response generation (as opposed to unconditional single sentence generation), we constrain the experiment by filtering out examples with fewer than 3 turns in the context. We also filter out examples where either of the two presented responses contain less than 5 tokens. We remove the special token placeholders and apply regex expressions to detokenize the text."}, {"heading": "Execution", "text": "We run the experiments in batches. For each pairs of models, we carry out 3\u2212 5 human intelligence tests (HITs) on AMT. Each HIT contains 70\u2212 90 examples (dialogue context and two model responses) and is evaluated by 3\u2212 4 unique humans. In total we collect 5363 preferences in 69 HITs.\nThe following are screenshots from one actual Amazon Mechanical Turk (AMT) experiment. These screenshots show the introduction (debriefing) of the experiment, an example dialogue and one dialogue context with two candidate responses, which human evaluators were asked to choose between. The experiment was carried out using psiturk, which can be downloaded from www.psiturk.org. The source code will be released upon publication."}], "references": [{"title": "Learning stochastic recurrent networks", "author": ["J. Bayer", "C. Osendorfer"], "venue": "In NIPS Workshop on Advances in Variational Inference", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Generating sentences from a continuous space. arXiv:1511.06349", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "In INTERSPEECH", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A recurrent latent variable model for sequential data", "author": ["J. Chung", "K. Kastner", "L. Dinh", "K. Goel", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Variational recurrent auto-encoders", "author": ["O. Fabius", "J.R. van Amersfoort"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["M. Galley", "C. Brockett", "A. Sordoni", "Y. Ji", "M. Auli", "C. Quirk", "M. Mitchell", "J. Gao", "B. Dolan"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep Learning", "author": ["I. Goodfellow", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Sequence transduction with recurrent neural networks. In ICML RLW", "author": ["A. Graves"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "In ICLR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "B. Dolan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Liu", "C.-W", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. In SIGDIAL", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "In ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "A survey on metrics for the evaluation of user simulations", "author": ["O. Pietquin", "H. Hastie"], "venue": "The knowledge engineering review,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Data-driven response generation in social media", "author": ["A. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "In EMNLP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics", "author": ["V. Rus", "M. Lintean"], "venue": "In Building Educational Applications Workshop,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A.C. Courville", "J. Pineau"], "venue": "In AAAI,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "VCA: an experiment with a multiparty virtual chat agent", "author": ["S. Shaikh", "T. Strzalkowski", "S. Taylor", "N. Webb"], "venue": "In ACL Workshop on Companionable Dialogue Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J.G. Simonsen", "Nie", "J.-Y"], "venue": "In CIKM", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "Nie", "J.-Y", "J. Gao", "B. Dolan"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "POMDP-based statistical spoken dialog systems: A review", "author": ["S. Young", "M. Gasic", "B. Thomson", "J.D. Williams"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 193, "endOffset": 196}, {"referenceID": 9, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 227, "endOffset": 235}, {"referenceID": 17, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 227, "endOffset": 235}, {"referenceID": 27, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 256, "endOffset": 263}, {"referenceID": 4, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 256, "endOffset": 263}, {"referenceID": 26, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 274, "endOffset": 282}, {"referenceID": 23, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 274, "endOffset": 282}, {"referenceID": 8, "context": "Deep recurrent neural networks (RNNs) have recently demonstrated impressive results on a number of difficult machine learning problems involving the generation of sequential structured outputs [9], including language modelling [10, 18] machine translation [28, 5], dialogue [27, 24] and speech recognition [9].", "startOffset": 306, "endOffset": 309}, {"referenceID": 1, "context": "This is often an inappropriate place to inject variability [2, 6, 1].", "startOffset": 59, "endOffset": 68}, {"referenceID": 5, "context": "This is often an inappropriate place to inject variability [2, 6, 1].", "startOffset": 59, "endOffset": 68}, {"referenceID": 0, "context": "This is often an inappropriate place to inject variability [2, 6, 1].", "startOffset": 59, "endOffset": 68}, {"referenceID": 11, "context": "At each time step, the model observes an element and updates its internal hidden state, hm = f(hm\u22121, wm), where f is a parametrized non-linear function, such as the hyperbolic tangent, the LSTM gating unit [12] or the GRU gating unit [5].", "startOffset": 206, "endOffset": 210}, {"referenceID": 4, "context": "At each time step, the model observes an element and updates its internal hidden state, hm = f(hm\u22121, wm), where f is a parametrized non-linear function, such as the hyperbolic tangent, the LSTM gating unit [12] or the GRU gating unit [5].", "startOffset": 234, "endOffset": 237}, {"referenceID": 17, "context": "Under this assumption, the RNN Language Model (RNNLM) [18], the simplest possible generative RNN for discrete sequences, parametrizes the output distribution using the softmax function applied to an affine transformation of the hidden state hm.", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "The hierarchical recurrent encoder-decoder model (HRED) [26, 24] is an extension of the RNNLM.", "startOffset": 56, "endOffset": 64}, {"referenceID": 23, "context": "The hierarchical recurrent encoder-decoder model (HRED) [26, 24] is an extension of the RNNLM.", "startOffset": 56, "endOffset": 64}, {"referenceID": 4, "context": "It extends the encoder-decoder architecture [5] to the natural dialogue setting.", "startOffset": 44, "endOffset": 47}, {"referenceID": 25, "context": "For additional details see [26, 24].", "startOffset": 27, "endOffset": 35}, {"referenceID": 23, "context": "For additional details see [26, 24].", "startOffset": 27, "endOffset": 35}, {"referenceID": 23, "context": "In the recent literature, it has been observed that the RNNLM and HRED, and similar models based on RNN architectures, have critical problems generating meaningful dialogue utterances [24, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 14, "context": "In the recent literature, it has been observed that the RNNLM and HRED, and similar models based on RNN architectures, have critical problems generating meaningful dialogue utterances [24, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 5, "context": "Similarly, for the diagonal covariance matrix \u03a3prior another matrix multiplication is applied to the net\u2019s output followed by softplus function, to ensure positiveness [6].", "startOffset": 168, "endOffset": 171}, {"referenceID": 28, "context": "This is an interesting problem with applications in areas such as customer service, technical support, language learning and entertainment [29].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "It is also a task domain that requires learning to generate sequences with complex structures while taking into account long-term context [17, 27].", "startOffset": 138, "endOffset": 146}, {"referenceID": 26, "context": "It is also a task domain that requires learning to generate sequences with complex structures while taking into account long-term context [17, 27].", "startOffset": 138, "endOffset": 146}, {"referenceID": 21, "context": "We first perform experiments on a Twitter Dialogue Corpus [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "[22], and is split into training, validation and test sets, containing respectively 749, 060, 93, 633 and 10, 000 dialogues.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "The dialogues are fairly long compared to recent large-scale language modelling corpora, such as the 1 Billion Word Language Model Benchmark [4], which focus on modelling single sentences.", "startOffset": 141, "endOffset": 144}, {"referenceID": 16, "context": "We also experiment on the Ubuntu Dialogue Corpus [17], which contains about 500, 000 dialogues extracted from the #Ubuntu Internet Relayed Chat channel.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "We optimize all models using Adam [13].", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "We choose our hyperparameters and early stop with patience using the variational lower-bound [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "At test time, we use beam search with 5 beams for outputting responses with the RNN decoders [10].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "For reference, we also include a non-neural network baseline, specifically the TF-IDF retrieval-based model proposed in [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "[3]: we drop words in the decoder with a fixed drop rate of 25% and multiply the KL terms in eq.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Evaluation Accurate evaluation of dialogue system responses is a difficult problem [8, 20].", "startOffset": 83, "endOffset": 90}, {"referenceID": 19, "context": "Evaluation Accurate evaluation of dialogue system responses is a difficult problem [8, 20].", "startOffset": 83, "endOffset": 90}, {"referenceID": 15, "context": "[16] show that such metrics have little correlation with human evaluations of response quality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "This helps compare how well each model can integrate the dialogue context into its response, since it has previously been hypothesized that for long contexts hierarchical RNNs models fare better [24, 26].", "startOffset": 195, "endOffset": 203}, {"referenceID": 25, "context": "This helps compare how well each model can integrate the dialogue context into its response, since it has previously been hypothesized that for long contexts hierarchical RNNs models fare better [24, 26].", "startOffset": 195, "endOffset": 203}, {"referenceID": 24, "context": "However, we believe that response diversity is crucial to maintaining interesting conversations \u2014 in the dialogue literature, generic responses are used primarily as \u2018back-off\u2019 strategies in case the agent has no interesting response that is relevant to the context [25].", "startOffset": 266, "endOffset": 270}, {"referenceID": 18, "context": "The Embedding Average (Average) metric projects the model response and ground truth response into two separate real-valued vectors by taking the mean over the word embeddings in each response, and then computes the cosine similarity between them [19].", "startOffset": 246, "endOffset": 250}, {"referenceID": 22, "context": "Given the (non-exclusive) alignment between words in the two responses, the mean over the cosine similarities is computed for each pair of questions [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "Thus, VHRED compares favourably to recently proposed models in the literature, which often output extremely low-entropy (generic) responses such as OK and I don\u2019t know [24, 15].", "startOffset": 168, "endOffset": 176}, {"referenceID": 14, "context": "Thus, VHRED compares favourably to recently proposed models in the literature, which often output extremely low-entropy (generic) responses such as OK and I don\u2019t know [24, 15].", "startOffset": 168, "endOffset": 176}, {"referenceID": 13, "context": "The use of a stochastic latent variable learned by maximizing a variational lower bound is inspired by the variational autoencoder (VAE) [14, 21].", "startOffset": 137, "endOffset": 145}, {"referenceID": 20, "context": "The use of a stochastic latent variable learned by maximizing a variational lower bound is inspired by the variational autoencoder (VAE) [14, 21].", "startOffset": 137, "endOffset": 145}, {"referenceID": 10, "context": "Such models have been used predominantly for generating images in the continuous domain [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "However, there has also been recent work applying these architectures for generating sequences, such as the Variational Recurrent Neural Networks (VRNN) [6], which was applied for speech and handwriting synthesis, and Stochastic Recurrent Networks (STORN) [1], which was applied for music generation and motion capture modeling.", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "However, there has also been recent work applying these architectures for generating sequences, such as the Variational Recurrent Neural Networks (VRNN) [6], which was applied for speech and handwriting synthesis, and Stochastic Recurrent Networks (STORN) [1], which was applied for music generation and motion capture modeling.", "startOffset": 256, "endOffset": 259}, {"referenceID": 6, "context": "Similar to our work is the Variational Recurrent Autoencoder [7] and the Variational Autoencoder Language Model [3], which apply encoder-decoder architectures to generative music modeling and language modeling respectively.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Similar to our work is the Variational Recurrent Autoencoder [7] and the Variational Autoencoder Language Model [3], which apply encoder-decoder architectures to generative music modeling and language modeling respectively.", "startOffset": 112, "endOffset": 115}], "year": 2017, "abstractText": "Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.", "creator": "LaTeX with hyperref package"}}}