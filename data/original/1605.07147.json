{"id": "1605.07147", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Riemannian SVRG: Fast Stochastic Optimization on Riemannian Manifolds", "abstract": "We study optimization of finite sums of \\emph{geodesically} smooth functions on Riemannian manifolds. Although variance reduction techniques for optimizing finite-sum problems have witnessed a huge surge of interest in recent years, all existing work is limited to vector space problems. We introduce \\emph{Riemannian SVRG}, a new variance reduced Riemannian optimization method. We analyze this method for both geodesically smooth \\emph{convex} and \\emph{nonconvex} functions. Our analysis reveals that Riemannian SVRG comes with advantages of the usual SVRG method, but with factors depending on manifold curvature that influence its convergence. To the best of our knowledge, ours is the first \\emph{fast} stochastic Riemannian method. Moreover, our work offers the first non-asymptotic complexity analysis for nonconvex Riemannian optimization (even for the batch setting). Our results have several implications; for instance, they offer a Riemannian perspective on variance reduced PCA, which promises a short, transparent convergence analysis.", "histories": [["v1", "Mon, 23 May 2016 19:28:05 GMT  (104kb,D)", "http://arxiv.org/abs/1605.07147v1", null], ["v2", "Fri, 7 Apr 2017 18:13:53 GMT  (104kb,D)", "http://arxiv.org/abs/1605.07147v2", "This is the final version that appeared in NIPS 2016. Our proof of Lemma 2 was incorrect in the previous arXiv version. (9 pages paper + 6 pages appendix)"]], "reviews": [], "SUBJECTS": "math.OC cs.LG", "authors": ["hongyi zhang", "sashank j reddi", "suvrit sra"], "accepted": true, "id": "1605.07147"}, "pdf": {"name": "1605.07147.pdf", "metadata": {"source": "CRF", "title": "Fast stochastic optimization on Riemannian manifolds", "authors": ["Hongyi Zhang", "Sashank J. Reddi"], "emails": ["hongyiz@mit.edu", "sjakkamr@cs.cmu.edu", "suvrit@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "We study the following rich class of (possibly nonconvex) finite-sum optimization problems:\nmin x\u2208X\u2282M\nf(x) , 1\nn n\u2211 i=1 fi(x), (1)\nwhere (M, g) is a Riemannian manifold with the Riemannian metric g, and X is a geodesically convex set. We further assume that each fi :M\u2192 R is geodesically L-smooth (see \u00a72). Problem (1) is fundamental to machine learning, where it typically arises in the context of empirical risk minimization, albeit usually in its vector space incarnation. It also captures numerous widely used problems such as principal component analysis (PCA), independent component analysis (ICA), dictionary learning, mixture modeling, among others (please see the related work section).\nThe linear space version of (1) where M = Rd has been the subject of intense algorithmic development in machine learning and optimization, starting with the classical work of Robbins and Monro [22] to the recent spate of work on variance reduction methods [7; 14; 16; 21; 24]. However, when M is nonlinear Riemannian manifold, much less attention has been paid [5; 32].\nWhen solving problems with manifold constraints, one common approach is to alternate between optimizing in the ambient Euclidean space and \u201cprojecting\u201d onto the manifold. For example, two well-known methods to compute the leading eigenvector of symmetric matrices, power iteration and Oja\u2019s algorithm [19], are in essence projected gradient and projected stochastic gradient algorithms. For certain manifolds (e.g., positive definite matrices), projections can be too expensive to compute.\nAn effective alternative is to use Riemannian optimization1, which directly operates on the manifold in question. With this Riemannian optimization turns the constrained optimization problem (1) into an unconstrained one defined on the manifold, and thus, \u201cprojection-free.\u201d More importantly, is its conceptual\n1Riemannian optimization is optimization on a known manifold structure. Note the distinction from manifold learning, which attempts to learn a manifold structure from data.\nar X\niv :1\n60 5.\n07 14\n7v 1\n[ m\nat h.\nO C\n] 2\n3 M\nay 2\nvalue: viewing a problem through the Riemannian lens, one can discover new insights into the geometry of a problem, which can even lead to better optimization algorithms.\nAlthough the Riemannian approach is very appealing, our knowledge of it is fairly limited. In particular, there is little analysis about its global complexity (a.k.a. non-asymptotic convergence rate), in part due to the difficulty posed by the nonlinear metric. It is only recently that Zhang and Sra [32] developed the first global complexity analysis of full and stochastic gradient methods for geodesically convex functions. However, the batch and stochastic gradient methods in [32] suffer from problems similar to their vector space counterparts. For solving finite sum problems with n components, the full-gradient method requires n derivatives at each step; the stochastic method requires only one derivative but at the expense of vastly slower O(1/ 2) convergence to an -accurate solution.\nThese issues have driven much of the recent progress on faster stochastic optimization in vector spaces by using variance reduction [7; 14; 24]. However, all of these works critically rely on properties of vector spaces; thus, using them in the context of Riemannian manifolds poses major challenges. Given the potentially vast scope Riemannian optimization and its growing number of applications, developing fast stochastic for it is very important: it will help us apply Riemannian optimization to large-scale problems, while offering a new set of algorithmic tools for the practitioner\u2019s repertoire.\nContributions. In light of the above motivation, let us summarize our key contributions below.\n\u2022 We introduce Riemannian SVRG (Rsvrg), a variance reduced Riemannian stochastic gradient method based on SVRG [14]. We analyze Rsvrg for geodesically strongly convex functions through a novel theoretical analysis that accounts for the nonlinear (curved) geometry of the manifold to yield linear convergence rates. A noteworthy byproduct of our analysis is a generalization of a classic lemma from convex optimization (see Lemma 2) that may be of independent interest.\n\u2022 Inspired by the exciting advances in variance reduction for nonconvex optimization [2; 21], we generalize convergence analysis of Rsvrg to (geodesically) nonconvex functions and also to gradient dominated functions (see \u00a72 for the definition). Our analysis provides the first stochastic Riemannian method this is provably superior to both batch and stochastic (Riemannian) gradient methods for nonconvex finite-sum problems.\n\u2022 Using a Riemannian formulation and applying our result for (geodesically) gradient-dominated functions, we provide new insights, and a short transparent analysis explaining fast convergence of variance reduced PCA for computing the leading eigenvector of a symmetric matrix.\nTo our knowledge, this paper provides the first stochastic gradient method with global linear convergence rates for geodesically strongly convex functions, as well as first non-asymptotic convergence rates for geodesically nonconvex optimization (even in the batch case). Our analysis reveals how manifold geometry, in particular its curvature impacts convergence rates. We illustrate the benefits of Rsvrg by showing an application to computing leading eigenvectors of a symmetric matrix, as well as for accelerating the computation of the Riemannian centroid of covariance matrices, a problem that has received great attention in the literature [4; 12; 32].\nRelated Work. Variance reduction techniques, such as control variates, are widely used in Monte Carlo simulations [23]. In linear spaces, variance reduced methods for solving finite-sum problems have recently witnessed a huge surge of interest [e.g. 3; 7; 14; 16; 24; 31]. They have been shown to accelerate stochastic optimization for strongly convex objectives, convex objectives, nonconvex fi (i \u2208 [n]), and even when both f and fi (i \u2208 [n]) are nonconvex [2; 21]. Reddi et al. [21] further proved global linear convergence for gradient dominated nonconvex problems. Our analysis is inspired by [14; 21], but applies to the substantially more general Riemannian optimization setting.\nReferences of Riemannian optimization can be found in [1; 29], where analysis is limited to asymptotic convergence (except [29, Theorem 4.2] which proved linear rate convergence for first-order line search method with bounded and positive definite hessian). Stochastic Riemannian optimization has been previously considered in [5; 17], though with only asymptotic convergence analysis, and without any rates. Many applications of Riemannian optimization are known, including matrix factorization on fixed-rank manifold [28; 30], dictionary learning [6; 27], optimization under orthogonality constraints [8; 18], learning elliptical\ndistributions [26; 33], and Gaussian mixture models [11]. Notably, some nonconvex Euclidean problems are geodesically convex, for which Riemannian optimization can provide similar guarantees to convex optimization. Zhang and Sra [32] provide the first global complexity analysis for first-order Riemannian algorithms, but their analysis is restricted to geodesically convex problems with full or stochastic gradients. In contrast, we propose Rsvrg, a variance reduced Riemannian stochastic gradient algorithm, and analyze its global complexity for both geodesically convex and nonconvex problems."}, {"heading": "2 Preliminaries", "text": "Before formally discussing Riemannian optimization, let us recall some foundational concepts of Riemannian geometry. For a thorough review one can refer to any classic text, e.g.,[20].\nA Riemannian manifold (M, g) is a real smooth manifold M equipped with a Riemannain metric g. The metric g induces an inner product structure in each tangent space TxM associated with every x \u2208M. We denote the inner product of u, v \u2208 TxM as \u3008u, v\u3009 , gx(u, v); and the norm of u \u2208 TxM is defined as \u2016u\u2016 , \u221a gx(u, u). The angle between u, v is defined as arccos \u3008u,v\u3009 \u2016u\u2016\u2016v\u2016 . A geodesic is constant speed curve \u03b3 : [0, 1]\u2192M that is locally distance minimizing. An exponential map Expx : TxM\u2192M maps v in TxM to y onM, such that there is a geodesic \u03b3 with \u03b3(0) = x, \u03b3(1) = y and \u03b3\u0307(0) , ddt\u03b3(0) = v. If between any two points in X \u2282M there is a unique geodesic, the exponential map has an inverse Exp\u22121x : X \u2192 TxM and the geodesic is the unique shortest path with \u2016Exp\u22121x (y)\u2016 = \u2016Exp \u22121 y (x)\u2016 the geodesic distance between x, y \u2208 X .\nParallel transport \u0393yx : TxM \u2192 TyM maps a vector v \u2208 TxM to \u0393yxv \u2208 TyM, while preserving norm, and roughly speaking, \u201cdirection,\u201d analogous to translation in Rd. A tangent vector of a geodesic \u03b3 remains tangent if parallel transported along \u03b3. Parallel transport preserves inner products.\nThe geometry of a Riemannian manifold is determined by its Riemannian metric tensor through various characterization of curvatures. Let u, v \u2208 TxM be linearly independent, so that they span a two dimensional subspace of TxM. Under the exponential map, this subspace is mapped to a two dimensional submanifold of U \u2282M. The sectional curvature \u03ba(x,U) is defined as the Gauss curvature of U at x. As we will mainly analyze manifold trigonometry, for worst-case analysis, it is sufficient to consider sectional curvature.\nFunction Classes. We now define some key terms. A set X is called geodesically convex if for any x, y \u2208 X , there is a geodesic \u03b3 with \u03b3(0) = x, \u03b3(1) = y and \u03b3(t) \u2208 X for t \u2208 [0, 1]. Throughout the paper, we assume that the function f in (1) is defined on a geodesically convex set X on a Riemannian manifold M.\nWe call a function f : X \u2192 R geodesically convex (g-convex) if for any x, y \u2208 X and any geodesic \u03b3 such that \u03b3(0) = x, \u03b3(1) = y and \u03b3(t) \u2208 X for t \u2208 [0, 1], it holds that\nf(\u03b3(t)) \u2264 (1\u2212 t)f(x) + tf(y).\nIt can be shown that if the inverse exponential map is well-defined, an equivalent definition is that for any x, y \u2208 X , f(y) \u2265 f(x) + \u3008gx,Exp\u22121x (y)\u3009, where gx is a subgradient of f at x (or the gradient if f is differentiable). A function f : X \u2192 R is called geodesically \u00b5-strongly convex (\u00b5-strongly g-convex) if for any x, y \u2208 X and subgradient gx, it holds that\nf(y) \u2265 f(x) + \u3008gx,Exp\u22121x (y)\u3009+ \u00b5 2 \u2016Exp \u22121 x (y)\u20162.\nWe call a vector field g : X \u2192 Rd geodesically L-Lipschitz (L-g-Lipschitz) if for any x, y \u2208 X ,\n\u2016g(x)\u2212 \u0393xyg(y)\u2016 \u2264 L\u2016Exp \u22121 x (y)\u2016,\nwhere \u0393xy is the parallel transport from y to x. We call a differentiable function f : X \u2192 R geodesically L-smooth (L-g-smooth) if its gradient is L-g-Lipschitz, in which case we have\nf(y) \u2264 f(x) + \u3008gx,Exp\u22121x (y)\u3009+ L2 \u2016Exp \u22121 x (y)\u20162.\nWe say f : X \u2192 R is \u03c4 -gradient dominated if x\u2217 is a global minimizer of f and for every x \u2208 X\nf(x)\u2212 f(x\u2217) \u2264 \u03c4\u2016\u2207f(x)\u20162. (2)\nWe recall the following trigonometric distance bound that is essential for our analysis:\nLemma 1 ([5; 32]). If a, b, c are the side lengths of a geodesic triangle in a Riemannian manifold with sectional curvature lower bounded by \u03bamin, and A is the angle between sides b and c (defined through inverse exponential map and inner product in tangent space), then\na2 \u2264 \u221a |\u03bamin|c\ntanh( \u221a |\u03bamin|c) b2 + c2 \u2212 2bc cos(A). (3)\nAn Incremental First-order Oracle (IFO) in (1) takes and i \u2208 [n] and a point x \u2208 X , and returns a pair (fi(x),\u2207fi(x)) \u2208 R\u00d7 TxM. We measure non-asymptotic complexity in terms of IFO calls."}, {"heading": "3 Riemannian SVRG", "text": "In this section we introduce Rsvrg formally. We make the following standing assumptions: (a) f attains its optimum at x\u2217 \u2208 X ; (b) X is compact, and the diameter of X is bounded by D, that is, maxx,y\u2208X d(x, y) \u2264 D; (c) the sectional curvature in X is upper bounded by \u03bamax, and within X the exponential map is invertible; and (d) the sectional curvature in X is lower bounded by \u03bamin. We define the following two key geometric constants that capture the impact of manifold curvature:\n\u03b6 =  \u221a |\u03bamin|D tanh( \u221a |\u03bamin|D) , if \u03bamin < 0,\n1, if \u03bamin \u2265 0, and \u03c3 =\n{ \u221a \u03bamaxD\ntan( \u221a \u03bamaxD) , if \u03bamax > 0, 1, if \u03bamax \u2264 0.\nWe note that most (if not all) practical manifold optimization problems can satisfy these assumptions. Our proposed Rsvrg algorithm is shown in Algorithm 1. Compared with its Euclidean SVRG, it differs in two key aspects: the variance reduction step uses parallel transport to combine gradients from different tangent spaces; and the exponential map is used (instead of the update xs+1t \u2212 \u03b7vs+1t )."}, {"heading": "3.1 Convergence analysis for geodesically convex (g-convex) functions", "text": "In this section, we analyze global complexity of Rsvrg for solving (1), where each fi (i \u2208 [n]) is g-convex and f is strongly g-convex. Crucial to our proof is Lemma 2 that generalizes a classic result from convex optimization.\nLemma 2. For any x, y \u2208 X , if fi is g-convex and L-g-smooth, then\n\u2016\u2207fi(x)\u2212 \u0393xy\u2207fi(y)\u20162 \u2264 2L ( fi(x)\u2212 fi(y)\u2212 \u3008\u2207fi(y),Exp\u22121y (x)\u3009 ) .\nNote that in vector space, this lemma has a textbook proof: define \u03c6y(z) , fi(z)\u2212 fi(y)\u2212 \u3008\u2207fi(y), z\u2212 y\u3009, then we simply need to prove 0 = \u03c6y(y) \u2264 \u03c6y ( x\u2212 1L\u2207\u03c6y(x) ) \u2264 \u03c6y(x)\u2212 12L\u2016\u2207\u03c6y(x)\u2016\n2, which is true since fi is convex and \u03c6y is also L-smooth. However, on a Riemannian manifold this proof does not work, as the term \u3008\u2207fi(y),Exp\u22121y (x)\u3009 is not \u201clinear\u201d \u2013 in fact, it can even be nonsmooth for some Riemannian metrics. However, the following proof shows that the lemma still holds regardless of the underlying Riemannian metric.\nAlgorithm 1: Rsvrg (x0,m, \u03b7, S)\nParameters: update frequency m, learning rate \u03b7, number of epochs S initialize x\u03030 = x0; for s = 0, 1, . . . , S \u2212 1 do\nxs+10 = x\u0303 s; gs+1 = 1 n \u2211n i=1\u2207fi(x\u0303\ns); for t = 0, 1, . . . ,m\u2212 1 do\nRandomly pick it \u2208 {1, . . . , n}; vs+1t = \u2207fit(xs+1t )\u2212 \u0393 xs+1t x\u0303s ( \u2207fit(x\u0303s)\u2212 gs+1 ) ; xs+1t+1 = Expxs+1t ( \u2212\u03b7vs+1t ) ;\nend Option I: set x\u0303s+1 = xs+1t for randomly chosen t \u2208 {0, . . . ,m\u2212 1}; Option II: set x\u0303s+1 = xs+1m ;\nend Option I: output xa = x\u0303 S ; Option II: output xa chosen uniformly randomly from {{xs+1t }m\u22121t=0 } S\u22121 s=0 .\nProof. Let \u03b3(t) : [0, 1]\u2192M be the geodesic from y to x with \u03b3(0) = y, \u03b3(1) = x. We then have\nfi(x)\u2212 fi(y) = \u222b 1 0 \u3008\u2207fi(\u03b3(t)), \u03b3\u0307(t)\u3009dt = \u222b 1 0 \u2329 \u0393 \u03b3(t) y \u2207fi(y) + \u222b t 0 \u0393 \u03b3(t) \u03b3(s)\u2207 2fi(\u03b3(s))\u03b3\u0307(s)ds, \u03b3\u0307(t) \u232a dt\n= \u3008\u2207fi(y),Exp\u22121y (x)\u3009+ \u222b 1 0 \u222b t 0 \u2329 \u0393 \u03b3(t) \u03b3(s)\u2207 2fi(\u03b3(s))\u03b3\u0307(s), \u03b3\u0307(t) \u232a dsdt\n= \u3008\u2207fi(y),Exp\u22121y (x)\u3009+ \u222b 1 0 \u222b t 0 \u2329 \u22072fi(\u03b3(s))\u03b3\u0307(s), \u03b3\u0307(s) \u232a dsdt (4)\nNow consider the variational problem(s)\nmin fi\n\u222b t 0 \u2329 \u22072fi(\u03b3(s))\u03b3\u0307(s), \u03b3\u0307(s) \u232a ds, \u2200t \u2208 [0, 1] (5)\ns.t. \u222b 1 0 \u0393x\u03b3(s)\u2207 2fi(\u03b3(s))\u03b3\u0307(s)ds = \u2207fi(x)\u2212 \u0393xy\u2207fi(y) (6)\nWe now verify that an fi minimize all the objectives in (5) if and only if it satisfies\n\u22072fi(\u03b3(s))\u03b3\u0307(s) \u2261 { 0, s \u2208 [0, \u03bb) L\u03b3\u0307(s) s \u2208 [\u03bb, 1]\n(7)\nand \u222b 1 \u03bb \u0393x\u03b3(s)L\u03b3\u0307(s)ds = \u2207fi(x)\u2212 \u0393 x y\u2207fi(y)\nwhere\n\u03bb , 1\u2212 \u2016\u2207fi(x)\u2212 \u0393xy\u2207fi(y)\u2016\nL\u2016\u03b3\u0307(1)\u2016 In fact, since fi is g-convex, \u2329 \u22072fi(\u03b3(s))\u03b3\u0307(s), \u03b3\u0307(s) \u232a must be nonnegative, thus \u22072fi that satisfy (7) must minimize (5) for t \u2208 [0, \u03bb). On the other hand, no \u22072fi can make a smaller value of the integral in (5) over the interval [\u03bb, t] for any t \u2208 [\u03bb, 1), since otherwise it would have to violate the L-g-smooth assumption in the interval (t, 1] to meet the constraint (6).\nWith the above argument, we plug the solution (7) into (4), whereby\nfi(x)\u2212 fi(y) \u2265 \u3008\u2207fi(y),Exp\u22121y (x)\u3009+ \u222b 1 0 \u222b t \u03bb \u3008L\u03b3\u0307(s), \u03b3\u0307(s)\u3009 dsdt\n= \u3008\u2207fi(y),Exp\u22121y (x)\u3009+ 12L\u2016\u2207fi(x)\u2212 \u0393 x y\u2207fi(y)\u20162,\nwhich completes the proof of the lemma.\nWe are now ready to state our main theorem for this section, which shows that Rsvrg has linear convergence rate for solving (1) when fi (i \u2208 [n]) is g-convex and f is strongly g-convex. This is in contrast with the O(1/t) rate of Riemannian stochastic gradient algorithm [32].\nTheorem 1. Assume in (1) each fi is g-convex and L-g-smooth, f is \u00b5-strongly g-convex, and the sectional curvature in X is lower bounded by \u03bamin; let x\u2217 denote the optimal solution to (1). If we run Algorithm 1 with Option I, step size \u03b7 > 0, and epoch length m such that\n\u03b1 = (\u00b5m\u03b7(1\u2212 2\u03b6\u03b7L))\u22121 + 2\u03b6\u03b7L(1\u2212 2\u03b6\u03b7L)\u22121 < 1,\nthen with S outer loops, the Riemannian SVRG algorithm produces an iterate xa that satisfies\nE[f(xa)\u2212 f(x\u2217)] \u2264 \u03b1SE[f(x0)\u2212 f(x\u2217)].\nThe critical part of the proof of Theorem 1 is bounding the squared distance (Lemma 1) and the gradient variance (Lemma 2). The rest of the proof follows the structure of the original SVRG proof [14]. For completeness, we provide a complete proof in the Appendix. Theorem 1 leads to the following more digestible corollary on the global complexity of the algorithm:\nCorollary 1. With assumptions and parameter settings in Theorem 1, after O((n+ \u03b6L\u00b5 ) log( 1 )) IFO calls, the output xa of Algorithm 1 satisfies E[f(xa)\u2212 f(x\u2217)] \u2264 .\nWe give a proof with specific parameter choices in the appendix. Observe the dependence on \u03b6 in our result: for \u03bamin < 0, we have \u03b6 > 1, which implies that negative space curvature adversarially affects convergence rate; while for \u03bamin \u2265 0, we have \u03b6 = 1, which implies that for nonnegatively curved manifolds, Rsvrg has the same complexity as SVRG. In the rest of our analysis we will see a similar effect of sectional curvature; this phenomenon seems innate to manifold optimization (also see [32]).\nWhen each fi is L-g-smooth and g-convex, but f is not strongly g-convex, the following result holds:\nTheorem 2. If in (1) each fi (i \u2208 [n]) is g-convex and L-g-smooth, f is not strongly g-convex, and the sectional curvature in X lies in the range [\u03bamin, \u03bamax]. If we use Algorithm 1 with Option I to optimize f ,x 0 (x) = \u2211n i=1 f ,x0 i (x), where f ,x0 i (x) , fi(x) + 2\u2016Exp \u22121 x0 (x)\u2016 2 (rsp. \u2207f ,x 0 i (x) = \u2207fi(x)\u2212 Exp \u22121 x (x 0)), then the IFO complexity of computing an -accurate solution for f is O((n+ \u03b6 2\n\u03c3 + \u03b6L \u03c3 ) log( 1 )).\nWe give a proof in appendix. Note that both lower and upper bounds of sectional curvature play a role, since the Riemannian hessian of \u2016Exp\u22121x0 (x)\u2016 2 depends on the space curvature."}, {"heading": "3.2 Convergence analysis for geodesically nonconvex functions", "text": "In this section, we analyze global complexity of Rsvrg for solving (1), where each fi is only required to be L-g-smooth, and neither fi nor f need be g-convex. We measure convergence to a stationary point using \u2016\u2207f(x)\u20162 following [10]. Note, however, that here \u2207f(x) \u2208 TxM and \u2016\u2207f(x)\u2016 is defined via the inner product in TxM. We first note that Riemannian-SGD on nonconvex L-g-smooth problems attains O(1/ 2) convergence as SGD [10] holds; we relegate the details to the appendix.\nRecently, two groups independently proved that variance reduction also benefits stochastic gradient methods for nonconvex smooth finite-sum optimization problems, with different analysis [2; 21]. Our analysis for nonconvex Rsvrg is inspired by [21]. Our main result for this section is Theorem 3.\nTheorem 3. Assume in (1) each fi is L-g-smooth, the sectional curvature in X is lower bounded by \u03bamin, and we run Algorithm 1 with Option II. Then there exist universal constants \u00b50 \u2208 (0, 1), \u03bd > 0 such that if we set \u03b7 = \u00b50/(Ln \u03b11\u03b6\u03b12) (0 < \u03b11 \u2264 1 and 0 \u2264 \u03b12 \u2264 2), m = bn3\u03b11/2/(3\u00b50\u03b61\u22122\u03b12)c and T = mS, we have\nE[\u2016\u2207f(xa)\u20162] \u2264 Ln \u03b11\u03b6\u03b12 [f(x0)\u2212f(x\u2217)]\nT\u03bd ,\nwhere x\u2217 is an optimal solution to (1).\nThe key challenge in proving Theorem 3 in the Riemannian setting is to incorporate the impact of using a nonlinear metric. Similar to the g-convex case, the curvature impacts the convergence, notably through the constant \u03b6 that depends on a lower-bound on sectional curvature.\nReddi et al. [21] suggested setting \u03b11 = 2/3, in which case we obtain the following corollary.\nAlgorithm 2: GD-SVRG(x0,m, \u03b7, S,K)\nParameters: update frequency m, learning rate \u03b7, number of epochs S, K, x0 for k = 0, . . . ,K \u2212 1 do xk+1 = Rsvrg(xk,m, \u03b7, S) with Option II; end Output: xK\nCorollary 2. With assumptions and parameters in Theorem 3, choosing \u03b11 = 2/3, the IFO complexity for achieving an -accurate solution is:\nIFO calls =\n{ O ( n+ (n2/3\u03b61\u2212\u03b12/ ) ) , if \u03b12 \u2264 1/2,\nO ( n\u03b62\u03b12\u22121 + (n2/3\u03b6\u03b12/ ) ) , if \u03b12 > 1/2.\nSetting \u03b11 = 2/3, \u03b12 = 1/2 in Corollary 2 immediately leads to Corollary 3:\nCorollary 3. With assumptions in Theorem 3 and parameters from Corollary 3, the IFO complexity for achieving an -accurate solution is O ( n+ (n2/3\u03b61/2/ ) ) .\nThe same reasoning allows us to also capture the class of gradient dominated functions (2), for which Reddi et al. [21] proved that SVRG converges linearly to a global optimum. We have the following corresponding theorem for Rsvrg:\nTheorem 4. Suppose that in addition to assumptions in Theorem 3, f is \u03c4 -gradient dominated. Then if we run Algorithm 2 with \u03b7 = \u00b51/(Ln 2/3\u03b61/2),m = bn/(3\u00b51)c, S = d(6 + 18\u00b51n\u22123 )L\u03c4\u03b6 1/2\u00b51/(\u03bd1n 1/3)e, we have\nE[\u2016\u2207f(xK)\u20162] \u2264 2\u2212K\u2016\u2207f(x0)\u20162, E[f(xK)\u2212 f(x\u2217)] \u2264 2\u2212K [f(x0)\u2212 f(x\u2217)].\nWe summarize the implication of Theorem 4 as follows (note the dependency on curvature):\nCorollary 4. With Algorithm 2 and the parameters in Theorem 4, the IFO complexity to compute an -accurate solution for gradient dominated function f is O((n+ L\u03c4\u03b61/2n2/3) log(1/ ))."}, {"heading": "4 Applications", "text": ""}, {"heading": "4.1 Computing the leading eigenvector", "text": "In this section, we apply our analysis of Rsvrg for gradient dominated functions (Theorem 4) to fast eigenvector computation, a fundamental problem that is still being actively researched in the big-data setting [9; 13; 25]. For the problem of computing the leading eigenvector, i.e.,\nmin x>x=1\n\u2212x> (\u2211n\ni=1 zz>\n) x , \u2212x>Ax = f(x), (8)\nexisting analyses for state-of-the-art algorithms typically result in O(1/\u03b42) dependency on the eigengap \u03b4 of A, as opposed to the conjectured O(1/\u03b4) dependency [25], as well as the O(1/\u03b4) dependency of power iteration. Here we give new support for the O(1/\u03b4) conjecture. Our key observation is that, although Problem (8) seen as one in Rd is nonconvex, with negative semidefinite Hessian everywhere, and has nonlinear constraints, on the hypersphere Sd\u22121 it is unconstrained, and has gradient dominated objective. In particular we have the following result:\nTheorem 5. Suppose A has eigenvalues \u03bb1 > \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd and \u03b4 = \u03bb1 \u2212 \u03bb2. With probability 1\u2212 p, the random initialization x0 falls in a Riemannian ball of a global optimum of the objective function, within which the objective function is O( dp2\u03b4 )-gradient dominated.\nWe provide the proof of Theorem 5 in appendix. What remains to be shown is that with a constant stepsize and with high probability (both independent of \u03b4), the iterates remain in such type of Riemannian\nball. Once this is shown, applying Corollary 4 one can immediately prove the O(1/\u03b4) dependency conjecture. We leave this analysis as future work.\nWe implement Riemannian SVRG for PCA, and use the code for VR-PCA in [25]. Analytic forms for exponential map and parallel transport on hypersphere can be found in [1]. We conduct well-controlled experiments comparing their performance and showing empirically the O(1/\u03b4) dependency. Specifically, for each \u03b4 = 10\u22123/k where k = 1, . . . , 25, we generate a d\u00d7n matrix Z where d = 103, n = 104 using the method Z = UDV > as described in [25]. All the matrices share the same U, V and only differ in \u03b4 (thus also in D). We also fix the same random initialization x0 and random seed for generating the stochastic gradient sequence. As a result, the only variable in different runs is \u03b4. We run both algorithms on each matrix for 50 epochs. For every five epochs, we evaluate the algorithm\u2019s average convergence speed, and estimate the number of epochs required to double its accuracy. This number can serve as an indicator of the global complexity of the algorithm. We plot this number for different epochs against 1/\u03b4, shown in Figure 2. Note that the performance of RSVRG and VR-PCA with the same stepsize is very similar, which implies a close connection of the two. Indeed, the update x+v\u2016x+v\u2016 used in [25] and others is a well-known approximation to the exponential map Expx(v) with small stepsize (a.k.a. retraction)."}, {"heading": "4.2 Computing the Riemannian centroid", "text": "In this subsection we validate that Rsvrg converges linearly for averaging PSD matrices, which is a geodesically strongly convex problem, yet nonconvex in Euclidean space. This problem has been studied both in matrix computation and in various applications [4; 12]. We use the same experiment setting as described in [32], and compare Rsvrg against Riemannian full gradient (RGD) and stochastic gradient (RSGD) algorithms (Figure 3).\nNote that the objective is sum of squared Riemannian distances on a nonpositively curved space, thus is (2N)-strongly g-convex and (2N\u03b6)-g-smooth. With a reasonable initialization, the conditional number \u03b6 is under control, in which case we choose m = n and the optimal stepsize for Rsvrg is O(1/(\u03b6N3/2)). For all the experiments, we set \u03b7 = 1\n10N3/2 for Rsvrg, and use suggested parameters from [32] for other algorithms."}, {"heading": "5 Discussion", "text": "We introduce Riemannian SVRG, the first (to the best of our knowledge) variance reduced stochastic gradient algorithm for Riemannian optimization. In addition, we analyze its global complexity for optimizing geodesically strongly convex, convex, and nonconvex functions, explicitly showing their dependence on sectional curvature. Our experiments validate our analysis that Riemannian SVRG is much faster than full gradient and stochastic gradient methods for solving finite-sum optimization problems on Riemannian manifolds.\nOur analysis of computing the leading eigenvector as a Riemannian optimization problem is also worth noting: a nonconvex problem with nonpositive Hessian and nonlinear constraints in the ambient space turns out to be gradient dominated on the manifold. We believe this shows the promise of theoretical study of Riemannian optimization, and geometric optimization in general, and we hope it encourages other researchers in the community to join this endeavor.\nOur work also has limitations \u2013 most practical Riemannian optimization algorithms use retraction and vector transport to efficiently approximate the exponential map and parallel transport, which we do not analyze in this work. A systematic study of retraction and vector transport is an important topic for future research."}, {"heading": "Appendix: Fast Stochastic Optimization on Riemannian Manifolds", "text": ""}, {"heading": "A Proofs for Section 3.1", "text": "A.1 Strongly g-convex objective\nTheorem 1. Assume in (1) each fi is g-convex, and f is \u00b5-strongly g-convex, then if we run Algorithm 1 with Option I and parameters that satisfy\n\u03b1 = 1\n\u00b5m\u03b7(1\u2212 2\u03b6\u03b7L) +\n2\u03b6\u03b7L\n1\u2212 2\u03b6\u03b7L < 1\nthen with S outer loops, the Riemannian SVRG algorithm produces an iterate xa that satisfies\nE[f(xa)\u2212 f(x\u2217)] \u2264 \u03b1SE[f(x0)\u2212 f(x\u2217)].\nProof. Using Lemma 2 and the fact that \u2207f(x\u2217) = 0, and summing the inequality over i = 1, . . . , n, we obtain\n1\nn n\u2211 i=1 \u2016\u2207fi(x)\u2212 \u0393xx\u2217\u2207fi(x\u2217)\u20162 \u2264 2L(f(x)\u2212 f(x\u2217)) (9)\nSince vs+1t = \u2207fit(xs+1t )\u2212\u0393 xs+1t x\u0303s ( \u2207fit(x\u0303s)\u2212 gs+1 ) , conditioned on xs+1t and taking expectation with respect to it, we obtain:\nE\u2016vs+1t \u20162 \u2264 2E\u2016\u2207fit(xs+1t )\u2212 \u0393 xs+1t x\u2217 fit(x \u2217)\u20162 + 2E\u2016\u2207fit(x\u0303s)\u2212 \u0393x\u0303 s x\u2217\u2207fit(x\u2217)\u2212\u2207f(x\u0303s)\u20162\n= 2E\u2016\u2207fit(xs+1t )\u2212 \u0393 xs+1t x\u2217 fit(x \u2217)\u20162 + 2E\u2016 ( \u2207fit(x\u0303s)\u2212 \u0393x\u0303 s x\u2217\u2207fit(x\u2217) )\n\u2212 E ( \u2207fit(x\u0303s)\u2212 \u0393x\u0303 s x\u2217\u2207fit(x\u2217) ) \u20162\n\u2264 2E\u2016\u2207fit(xs+1t )\u2212 \u0393 xs+1t x\u2217 fit(x \u2217)\u20162 + 2E\u2016\u2207fit(x\u0303s)\u2212 \u0393x\u0303 s x\u2217\u2207fit(x\u2217)\u20162 \u2264 4L(f(xs+1t )\u2212 f(x\u2217) + f(x\u0303s)\u2212 f(x\u2217))\nThe first inequality uses \u2016a+ b\u20162 \u2264 2\u2016a\u20162 + 2\u2016b\u20162 and \u00b5\u0303 = \u2207f(x\u0303s). The second inequality uses E\u2016\u03be\u2212E\u03be\u20162 = E\u2016\u03be\u20162 \u2212 \u2016E\u03be\u20162 \u2264 E\u2016\u03be\u20162 for any random vector \u03be in any tangent space. The third inequality uses Eq. (9).\nNotice that Evs+1t = \u2207f(xs+1t ) and xs+1t+1 = Expxs+1t (\u2212\u03b7v s+1 t ), we thus have\nEd2(xs+1t+1 , x \u2217) \u2264 d2(xs+1t , x\u2217) + 2\u03b7\u3008Exp\u22121xs+1t (x \u2217),Evt\u3009+ \u03b6\u03b72E\u2016vt\u20162\n\u2264 d2(xs+1t , x\u2217) + 2\u03b7\u3008Exp\u22121xs+1t (x \u2217),\u2207f(xs+1t )\u3009 + 4\u03b6\u03b72L(f(xs+1t )\u2212 f(x\u2217) + f(x\u0303s)\u2212 f(x\u2217)) \u2264 d2(xs+1t , x\u2217) + 2\u03b7(f(x\u2217)\u2212 f(xs+1t )) + 4\u03b6\u03b72L(f(xs+1t )\u2212 f(x\u2217) + f(x\u0303s)\u2212 f(x\u2217)) = d2(xs+1t , x\n\u2217)\u2212 2\u03b7(1\u2212 2\u03b6\u03b7L)(f(xs+1t )\u2212 f(x\u2217)) + 4\u03b6\u03b72L(f(x\u0303s)\u2212 f(x\u2217))\nThe first inequality uses the trigonometric distance lemma, the second one uses previously obtained bound for E\u2016vt\u20162, and the third one uses g-convexity of f(x).\nSince we sample x\u0303s+1 uniformly, by summing the previous inequality over t = 1, . . . ,m, taking expectation\nwith all the history, we obtain\nEd2(xs+1m , x\u2217) + 2m\u03b7(1\u2212 2\u03b6\u03b7L)(f(x\u0303s+1)\u2212 f(x\u2217)) \u2264 Ed2(xs+10 , x\u2217) + 4m\u03b6\u03b72L(f(x\u0303s)\u2212 f(x\u2217)) = Ed2(x\u0303s, x\u2217) + 4m\u03b6\u03b72L(f(x\u0303s)\u2212 f(x\u2217))\n\u2264 2 \u00b5 E(f(x\u0303s)\u2212 f(x\u2217)) + 4m\u03b6\u03b72LE(f(x\u0303s)\u2212 f(x\u2217)) = 2(\u00b5\u22121 + 2m\u03b6\u03b72L)E(f(x\u0303s)\u2212 f(x\u2217))\nThe second inequality holds since f is \u00b5-strongly g-convex. We thus obtain E[f(x\u0303s+1)\u2212 f(x\u2217)] \u2264 [ 1\n\u00b5m\u03b7(1\u2212 2\u03b6\u03b7L) +\n2\u03b6\u03b7L\n1\u2212 2\u03b6\u03b7L\n] E[f(x\u0303s)\u2212 f(x\u2217)]. (10)\nwhich gives us E[f(xa)\u2212 f(x\u2217)] = E[f(x\u0303S)\u2212 f(x\u2217)] \u2264 \u03b1SE[f(x\u03030)\u2212 f(x\u2217)].\nCorollary 1. With assumptions as in Theorem 1 and properly chosen parameters, after O((n+ \u03b6L\u00b5 ) log( 1 )) IFO calls, the output xa satisfies E[f(xa)\u2212 f(x\u2217)] \u2264 .\nProof. We develop the proof by finding suitable choices for stepsize \u03b7 and full gradient update frequency m. Towards this goal, we minimize \u03b1 with respect to stepsize \u03b7. Solving the equation\n\u2202\u03b1 \u2202\u03b7 = 0,\nwe obtain\n\u03b7\u2217 = 1\n\u00b5m\n(\u221a \u00b5m\n2\u03b6L + 1\u2212 1 ) Settingm = d 48\u03b6L\u00b5 e and \u03b7 = 1 12\u03b6L , we get \u03b1 \u2264 1/2. Since each update requires two IFO calls, we thus know from Theorem 1 that with S(n+ d 96\u03b6L\u00b5 e) IFO calls, the output xa satisfies E[f(xa)\u2212 f(x \u2217)] \u2264 2\u2212SE[f(x\u03030)\u2212 f(x\u2217)]. Let 2\u2212SE[f(x\u03030)\u2212 f(x\u2217)] \u2264 and solve for S, it is easy to see the total amount of IFO calls for an -accurate solution is O((n+ \u03b6L\u00b5 ) log( 1 )).\nA.2 Non-strongly g-convex objective\nWhen each fi is L-g-smooth and g-convex, but f is not strongly g-convex, we can use the same perturbation trick as in [16; 31]. That is, we optimize a perturbed objective f ,x 0\n, f + 2\u2016Exp \u22121 x0 (x)\u2016 2. Karcher [15,\nTheorem 1.2] proved that the function 12\u2016Exp \u22121 x0 (x)\u2016 2 is \u03c3-strongly g-convex in X assuming \u221a\u03bamaxD < \u03c0/4. On the other hand, Lemma 1 implies that 12\u2016Exp \u22121 x0 (x)\u2016\n2 is \u03b6-g-smooth. Since \u2207f ,x0 = \u2207f \u2212 Exp\u22121x (x0), we can simply add \u2212 Exp\u22121x (x0) to each gradient-like object in Algorithm 1. Applying Theorem 1 to the perturbed function, we obtain the following result.\nTheorem 2. Suppose in (1) each fi is g-convex and L-g-smooth, f is not strongly g-convex, and the sectional curvature in X is in the range [\u03bamin, \u03bamax]. If we use Algorithm 1 with Option I to optimize f ,x 0 (x) = \u2211n i=1 f ,x0 i (x), where f ,x0 i (x) , fi(x) + 2\u2016Exp \u22121 x0 (x)\u2016 2 (rsp. \u2207f ,x 0 i (x) = \u2207fi(x)\u2212 Exp \u22121 x (x 0)), then the IFO complexity of computing an -accurate solution for f is O((n+ \u03b6 2\n\u03c3 + \u03b6L \u03c3 ) log( 1 )).\nProof. Since 12\u2016Exp \u22121 x0 (x)\u2016 2 is \u03b6-g-smooth and \u03c3-strongly g-convex, we have that each f ,x 0 i (x) is (L+ \u03b6 )-gsmooth and (\u03c3 )-strongly g-convex. Let m = d 48\u03b6(L+\u03b6 )\u03c3 e, \u03b7 = 1 12\u03b6(L+\u03b6 ) , apply the analysis in the proof of Corollary 1, we get\nE[f ,x 0\n(x\u0303S)\u2212min x f ,x\n0 (x)] \u2264 2\u2212SE[f ,x 0\n(x0)\u2212min x f ,x\n0\n(x)] (11)\nNote that f(x\u2217) \u2264 minx f ,x 0 (x) \u2264 f ,x0(x\u2217) where x\u2217 is an optimal solution of f . We thus have\nE[f(x\u0303S)\u2212 f(x\u2217)] = E[f ,x 0 (x\u0303S)\u2212 f ,x 0 (x\u2217)] + 2 E[\u2016Exp\u22121x0 (x \u2217)\u20162 \u2212 \u2016Exp\u22121x0 (x\u0303 S)\u20162]\n\u2264 E[f ,x 0\n(x\u0303S)\u2212min x f ,x\n0\n(x)] + 2 E[\u2016Exp\u22121x0 (x \u2217)\u20162]\n\u2264 2\u2212SE[f ,x 0\n(x0)\u2212min x f ,x\n0\n(x)] + 2 E[\u2016Exp\u22121x0 (x \u2217)\u20162]\n\u2264 2\u2212SE[f(x0)\u2212 f(x\u2217)] + 2 E[\u2016Exp\u22121x0 (x \u2217)\u20162]\nwhere the first inequality is due to minx f ,x0(x) \u2264 f ,x0(x\u2217) and \u2016Exp\u22121x0 (x\u0303 S)\u20162 \u2265 0, the second inequality is due to (11), the third inequality is due to f ,x 0 (x0) = f(x0) and f(x\u2217) = minx f(x) \u2264 minx f ,x 0\n(x). Now it is easy to see that with S = O(log( 1 )) epochs, the output xa = x\u0303 S of Algorithm 1 with Option I can reach\n-accuracy. The total number of IFO calls is S(n+ 2m) = O((n+ \u03b6 2\n\u03c3 + \u03b6L \u03c3 ) log( 1 ))."}, {"heading": "B Proofs for Section 3.2", "text": "Theorem 6. Assuming the inverse exponential map is well-defined on X , f : X \u2192 R is a geodesically L-smooth function, stochastic first-order oracle \u2207f\u0303(x) satisfies E[\u2207f\u0303(xt)] = \u2207f(xt), \u2016\u2207f\u0303(xt)\u20162 \u2264 \u03c32, then the SGD algorithm xt+1 = Expxt(\u2212\u03b7\u2207f\u0303(xt)) with \u03b7 = c/ \u221a T , c = \u221a 2(f(x0)\u2212f(x\u2217)) L\u03c32 satisfies\nmin 0\u2264t\u2264T\u22121\nE[\u2016\u2207f(xt)\u20162] \u2264 \u221a\n2(f(x0)\u2212 f(x\u2217))L T \u03c3.\nProof.\nE[f(xt+1)] \u2264 E[f(xt) + \u3008\u2207f(xt),Exp\u22121xt (x t+1)\u3009+ L 2 \u2016Exp\u22121xt (x t+1)\u20162]\n\u2264 E[f(xt)]\u2212 \u03b7E[\u2016\u2207f(xt)\u20162] + L\u03b7 2\n2 E[\u2016\u2207f\u0303(xt)\u20162]\n\u2264 E[f(xt)]\u2212 \u03b7E[\u2016\u2207f(xt)\u20162] + L\u03b7 2\n2 \u03c32\nAfter rearrangement, we obtain\nE[\u2016\u2207f(xt)\u20162] \u2264 1 \u03b7 E[f(xt)\u2212 f(xt+1)] + L\u03b7 2 \u03c32\nSumming up the above equation from t = 0 to T \u2212 1 and using \u03b7 = c/ \u221a T where\nc =\n\u221a 2(f(x0)\u2212 f(x\u2217))\nL\u03c32\nwe obtain\nmin t E[\u2016\u2207f(xt)\u20162] \u2264 1 T T\u22121\u2211 t=0 E[\u2016f(xt)\u20162] \u2264 1 T\u03b7 E[f(x0)\u2212 f(xT )] + L\u03b7 2 \u03c32\n\u2264 1 T\u03b7 (f(x0)\u2212 f(x\u2217)) + L\u03b7 2 \u03c32\n\u2264 \u221a\n2(f(x0)\u2212 f(x\u2217))L T \u03c3\nLemma 3. Assume in (1) each fi is L-g-smooth, the sectional curvature in X is lower bounded by \u03bamin, and we run Algorithm 1 with Option II. For ct, ct+1, \u03b2, \u03b7 > 0, suppose we have\nct = ct+1 ( 1 + \u03b2\u03b7 + 2\u03b6L2\u03b72 ) + L3\u03b72\nand \u03b4(t) = \u03b7 \u2212 ct+1\u03b7\n\u03b2 \u2212 L\u03b72 \u2212 2ct+1\u03b6\u03b72 > 0,\nthen the iterate xs+1t satisfies the bound:\nE [ \u2016\u2207f(xs+1t )\u20162 ] \u2264 Rs+1t \u2212Rs+1t+1\n\u03b4t\nwhere Rs+1t := E[f(x s+1 t ) + ct\u2016Expx\u0303s(xs+1t )\u20162] for 0 \u2264 s \u2264 S \u2212 1.\nProof. Since f is L-smooth we have\nE[f(xs+1t+1 )] \u2264 E[f(x s+1 t ) + \u3008\u2207f(xs+1t ),Exp\u22121xs+1t (x s+1 t+1 )\u3009+\nL 2 \u2016Exp\u22121 xs+1t (xs+1t+1 )\u20162]\n\u2264 E[f(xs+1t )\u2212 \u03b7\u2016\u2207f(xs+1t )\u20162 + L\u03b72\n2 \u2016vs+1t \u20162] (12)\nConsider now the Lyapunov function\nRs+1t := E[f(x s+1 t ) + ct\u2016Expx\u0303s(xs+1t )\u20162]\nFor bounding it we will require the following:\nE[\u2016Exp\u22121x\u0303s (x s+1 t+1 )\u20162] \u2264 E[\u2016Exp \u22121 x\u0303s (x s+1 t )\u20162 + \u03b6\u2016Exp\u22121xs+1t (x s+1 t+1 )\u20162\n\u2212 2\u3008Exp\u22121 xs+1t (xs+1t+1 ),Exp \u22121 xs+1t (x\u0303s)\u3009]\n= E[\u2016Exp\u22121x\u0303s (x s+1 t )\u20162 + \u03b6\u03b72\u2016vs+1t \u20162\n+ 2\u03b7\u3008\u2207f(xs+1t ),Exp\u22121xs+1t (x\u0303 s)\u3009]\n\u2264 E[\u2016Exp\u22121x\u0303s (x s+1 t )\u20162 + \u03b6\u03b72\u2016vs+1t \u20162] + 2\u03b7E [ 1\n2\u03b2 \u2016\u2207f(xs+1t )\u20162 +\n\u03b2 2 \u2016Exp\u22121x\u0303s (x s+1 t )\u20162\n] (13)\nwhere the first inequality is due to Lemma 1, the second due to 2\u3008a, b\u3009 \u2264 1\u03b2 \u2016a\u2016 2 + \u03b2\u2016b\u20162. Plugging Equation (12) and Equation (13) into Rs+1t+1 , we obtain the following bound:\nRs+1t+1 \u2264 E[f(x s+1 t )\u2212 \u03b7\u2016\u2207f(xs+1t )\u20162 +\nL\u03b72\n2 \u2016vs+1t \u20162]\n+ ct+1E[\u2016Exp\u22121x\u0303s (x s+1 t )\u20162 + \u03b6\u03b72\u2016vs+1t \u20162] + 2ct+1\u03b7E [ 1\n2\u03b2 \u2016\u2207f(xs+1t )\u20162 +\n\u03b2 2 \u2016Exp\u22121x\u0303s (x s+1 t )\u20162 ] = E [ f(xs+1t )\u2212 ( \u03b7 \u2212 ct+1\u03b7\n\u03b2\n) \u2016\u2207f(xs+1t )\u20162 ] + ( L\u03b72\n2 + ct+1\u03b6\u03b7\n2 ) E [ \u2016vs+1t \u20162 ] + (ct+1 + ct+1\u03b7\u03b2)E [ \u2016Exp\u22121x\u0303s (x s+1 t )\u20162 ] (14)\nIt remains to bound E [ \u2016vs+1t \u20162 ] . Denoting \u2206s+1t = \u2207fit(xs+1t ) \u2212 \u0393 xs+1t x\u0303s \u2207fit(x\u0303s), we have E[\u2206 s+1 t ] = \u2207f(xs+1t )\u2212 \u0393 xs+1t x\u0303s \u2207f(x\u0303s), and thus\nE [ \u2016vs+1t \u20162 ] = E [ \u2016\u2206s+1t + \u0393 xs+1t x\u0303s \u2207f(x\u0303 s)\u20162 ]\n= E [ \u2016\u2206s+1t \u2212 E[\u2206s+1t ] +\u2207f(xs+1t )\u20162 ] \u2264 2E[\u2016\u2206s+1t \u2212 E[\u2206s+1t ]\u20162] + 2E[\u2016\u2207f(xs+1t )\u20162] \u2264 2E[\u2016\u2206s+1t \u20162] + 2E[\u2016\u2207f(xs+1t )\u20162] \u2264 2L2E[\u2016Exp\u22121x\u0303s (x s+1 t )\u20162] + 2E[\u2016\u2207f(xs+1t )\u20162] (15)\nwhere the first inequality is due to \u2016a+b\u20162 \u2264 2\u2016a\u20162 +2\u2016b\u20162, the second due to E\u2016\u03be\u2212E\u03be\u20162 = E\u2016\u03be\u20162\u2212\u2016E\u03be\u20162 \u2264 E\u2016\u03be\u20162 for any random vector \u03be in any tangent space, the third due to L-g-smooth assumption. Substituting Equation (15) into Equation (14) we get\nRs+1t+1 \u2264 E [ f(xs+1t )\u2212 ( \u03b7 \u2212 ct+1\u03b7\n\u03b2 \u2212 L\u03b72 \u2212 2ct+1\u03b6\u03b72\n) \u2016\u2207f(xs+1t )\u20162 ] + ( ct+1 ( 1 + \u03b2\u03b7 + 2\u03b6L2\u03b72 ) + L3\u03b72 ) E [ \u2016Exp\u22121x\u0303s (x s+1 t )\u20162\n] = Rs+1t \u2212 ( \u03b7 \u2212 ct+1\u03b7\n\u03b2 \u2212 L\u03b72 \u2212 2ct+1\u03b6\u03b72\n) E [ \u2016\u2207f(xs+1t )\u20162 ] (16)\nRearranging terms completes the proof. Theorem 7. With assumptions as in Lemma 3, let cm = 0, \u03b7 > 0, \u03b2 > 0, and ct = ct+1 ( 1 + \u03b2\u03b7 + 2\u03b6L2\u03b72 ) + L3\u03b72 such that \u03b4(t) > 0 for 0 \u2264 t \u2264 m\u2212 1. Define the quantity \u03b4n := mint \u03b4(t), and let T = mS. Then for the output xa from Option II we have\nE[\u2016\u2207f(xa)\u20162] \u2264 f(x0)\u2212 f(x\u2217)\nT\u03b4n\nProof. Using Lemma 3 and telescoping the sum, we obtain\nm\u22121\u2211 t=0 E[\u2016\u2207f(xs+1t )\u20162] \u2264 Rs+10 \u2212Rs+1m \u03b4n\nSince cm = 0 and x s+1 0 = x\u0303 s, we thus have\nm\u22121\u2211 t=0 E[\u2016\u2207f(xs+1t )\u20162] \u2264 E[f(x\u0303s)\u2212 f(x\u0303s+1)] \u03b4n , (17)\nNow sum over all epochs to obtain\n1\nT S\u22121\u2211 s=0 m\u22121\u2211 t=0 E[\u2016\u2207f(xs+1t )\u20162] \u2264 f(x\u03030)\u2212 f(x\u2217) T\u03b4n (18)\nNote the definition of xa implies that the left hand side of (18) is exactly E[\u2016\u2207f(xa)\u20162].\nTheorem 3. Assume in (1) each fi is L-g-smooth, the sectional curvature in X is lower bounded by \u03bamin, and we run Algorithm 1 with Option II. Then there exist universal constants \u00b50 \u2208 (0, 1), \u03bd > 0 such that if we set \u03b7 = \u00b50/(Ln \u03b11\u03b6\u03b12) (0 < \u03b11 \u2264 1 and 0 \u2264 \u03b12 \u2264 2), m = bn3\u03b11/2/(3\u00b50\u03b61\u22122\u03b12)c and T = mS, we have\nE[\u2016\u2207f(xa)\u20162] \u2264 Ln\u03b11\u03b6\u03b12 [f(x0)\u2212 f(x\u2217)]\nT\u03bd ,\nwhere x\u2217 is an optimal solution to the problem in (1).\nProof. Let \u03b2 = L\u03b61\u2212\u03b12/n\u03b11/2. From the recurrence relation ct = ct+1 ( 1 + \u03b2\u03b7 + 2\u03b6L2\u03b72 ) + L3\u03b72 and cm = 0 we have\nc0 = \u00b520L n2\u03b11\u03b62\u03b12 (1 + \u03b8)m \u2212 1 \u03b8 ,\nwhere\n\u03b8 = \u03b7\u03b2 + 2\u03b6\u03b72L2 = \u00b50\u03b6\n1\u22122\u03b12\nn3\u03b11/2 +\n2\u00b520\u03b6 1\u22122\u03b12 n2\u03b11 \u2208 ( \u00b50\u03b6 1\u22122\u03b12 n3\u03b11/2 , 3\u00b50\u03b6 1\u22122\u03b12 n3\u03b11/2 ) .\nNotice that \u03b8 < 1/m so that (1 + \u03b8)m < e. We can thus bound c0 by\nc0 \u2264 \u00b50L\nn\u03b11/2\u03b6 (e\u2212 1)\nand in turn bound \u03b4n by\n\u03b4n = min t\n( \u03b7 \u2212 ct+1\u03b7\n\u03b2 \u2212 \u03b72L\u2212 2ct+1\u03b6\u03b72 ) \u2265 ( \u03b7 \u2212 c0\u03b7\n\u03b2 \u2212 \u03b72L\u2212 2c0\u03b6\u03b72 ) \u2265 \u03b7 ( 1\u2212 \u00b50(e\u2212 1)\n\u03b62\u2212\u03b12 \u2212 \u00b50 n\u03b11\u03b6\u03b12 \u2212 2\u00b5 2 0(e\u2212 1) n3\u03b11/2\u03b6\u03b12 ) \u2265 \u03bd Ln\u03b11\u03b6\u03b12\nwhere the last inequality holds for small enough \u00b50, as \u03b6, n \u2265 1. For example, it holds for \u00b50 = 1/10, \u03bd = 1/2. Substituting the above bound in Theorem 7 concludes the proof.\nCorollary 2. With assumptions and parameters in Theorem 3, choosing \u03b11 = 2/3, the IFO complexity for achieving an -accurate solution is:\nIFO calls =\n{ O ( n+ (n2/3\u03b61\u2212\u03b12/ ) ) , if \u03b12 \u2264 1/2,\nO ( n\u03b62\u03b12\u22121 + (n2/3\u03b6\u03b12/ ) ) , if \u03b12 > 1/2.\nProof. Note that to reach an -accurate solution, O(n\u03b11\u03b6\u03b12/(m )) = O(1+n\u22121/3\u03b61\u2212\u03b12/ ) epochs are required. On the other hand, one epoch takes O ( n(1 + \u03b62\u03b12\u22121) ) IFO calls. Thus the total amount of IFO calls is\nO ( n(1 + \u03b62\u03b12\u22121)(1 + n\u22121/3\u03b61\u2212\u03b12/ ) ) . Simplify to get the stated result.\nTheorem 4. Suppose that in addition to assumptions in Theorem 3, f is \u03c4 -gradient dominated. Then if we run Algorithm 2 with \u03b7 = \u00b51/(Ln 2/3\u03b61/2),m = bn/(3\u00b51)c, S = d(6 + 18\u00b51n\u22123 )L\u03c4\u03b6 1/2\u00b51/(\u03bd1n 1/3)e, we have\nE[\u2016\u2207f(xK)\u20162] \u2264 2\u2212K\u2016\u2207f(x0)\u20162,\nand E[f(xK)\u2212 f(x\u2217)] \u2264 2\u2212K [f(x0)\u2212 f(x\u2217)].\nProof. Apply Theorem 3. Observe that for each run of Algorithm 1 with Option II we now have T = mS \u2265 2L\u03c4n2/3\u03b61/2/\u03bd1, which implies\n1 \u03c4 E[f(xk+1)\u2212 f(x\u2217)] \u2264 E[\u2016\u2207f(xk+1)\u20162] \u2264 1 2\u03c4 E[f(xk)\u2212 f(x\u2217)] \u2264 1 2 E[\u2016\u2207f(xk)\u20162]\nThe theorem follows by recursive application of the above inequality.\nCorollary 4. With Algorithm 2 and the parameters in Theorem 4, the IFO complexity to compute an -accurate solution for gradient dominated function f is O((n+ L\u03c4\u03b61/2n2/3) log(1/ )).\nProof. We need O((n+m)S) = O(n+L\u03c4\u03b61/2n2/3) IFO calls in a run of Algorithm 1 to double the accuracy, thus in Algorithm 2, K = O(log(1/ )) runs are needed to reach -accuracy."}, {"heading": "C Proof for Section 4.1", "text": "Theorem 5. Suppose A has eigenvalues \u03bb1 > \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd and \u03b4 = \u03bb1 \u2212 \u03bb2. With probability 1\u2212 p, the random initialization x0 falls in a Riemannian ball of a global optimum of the objective function, within which the objective function is O( dp2\u03b4 )-gradient dominated.\nProof. We write x in the basis of A\u2019s eigenvectors {vi}di=1 with corresponding eigenvalues \u03bb1 > \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd, i.e. x = \u2211d i=1 \u03b1ivi. Thus Ax = \u2211d i=1 \u03b1i\u03bbivi and f(x) = \u2212 \u2211d i=1 \u03b1 2 i\u03bbi. The Riemannian gradient of\nf(x) is Px\u2207f(x) = \u22122(I \u2212 xx>)Ax = \u22122(Ax + f(x)x) = \u22122 \u2211d i=1 \u03b1i(\u03bbi \u2212 \u2211d j=1 \u03b1 2 j\u03bbj)vi. Now consider a Riemannian ball on the hypersphere defined by B , {x : x \u2208 Sd\u22121, \u03b11 \u2265 }, note that the center of B is the first eigenvector. We apply a case by case argument with respect to f(x)\u2212 f(x\u2217). If f(x)\u2212 f(x\u2217) \u2265 \u03b42 , we can lower bound the gradient by\n1 4\u2016Px\u2207f(x)\u2016\n2 = \u2211d\ni=1 \u03b12i\n( \u03bbi \u2212 \u2211d j=1 \u03b12j\u03bbj )2 \u2265 \u03b121 ( \u03bb1 \u2212 \u2211d j=1 \u03b12j\u03bbj )2 = \u03b121 (f(x)\u2212 f(x\u2217)) 2\n\u2265 12\u03b1 2 1\u03b4(f(x)\u2212 f(x\u2217)) \u2265 12 2\u03b4(f(x)\u2212 f(x\u2217))\nThe last equality follows from the fact that f(x\u2217) = \u2212\u03bb1 and f(x) = \u2212 \u2211d i=1 \u03b1 2 i\u03bbi. On the other hand, if f(x)\u2212 f(x\u2217) < \u03b42 , for i = 2, . . . , d, since \u2212\u03bbi \u2212 f(x \u2217) \u2265 \u03b4, we have \u2212\u03bbi \u2212 f(x) > 12 (\u2212\u03bbi \u2212 f(x\n\u2217)) \u2265 \u03b4/2. We can, again, lower bound the gradient by\n\u2016Px\u2207f(x)\u20162 = 4 \u2211d\ni=1 \u03b12i\n( \u03bbi \u2212 \u2211d j=1 \u03b12j\u03bbj )2 \u2265 4 \u2211d i=2 \u03b12i ( \u03bbi \u2212 \u2211d j=1 \u03b12j\u03bbj )2 \u2265 \u2211d\ni=2 \u03b12i (\u03bb1 \u2212 \u03bbi)\n2 \u2265 \u03b4 \u2211d\ni=2 \u03b12i (\u03bb1 \u2212 \u03bbi) = \u03b4(f(x)\u2212 f(x\u2217))\nCombining the two cases, we have that within B the objective function (8) is min{ 12 2\u03b4 , 1 \u03b4 }-gradient dominated. Finally, observe that if x0 is chosen uniformly at random on Sd\u22121, then with probability at least 1 \u2212 p, \u03b121 = \u2126( p2 d ), i.e. there exists some constant c > 0 such that 1 2 \u2264 cd p2 ."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We study optimization of finite sums of geodesically smooth functions on Riemannian manifolds.<lb>Although variance reduction techniques for optimizing finite-sum problems have witnessed a huge surge of<lb>interest in recent years, all existing work is limited to vector space problems. We introduce Riemannian<lb>SVRG, a new variance reduced Riemannian optimization method. We analyze this method for both<lb>geodesically smooth convex and nonconvex functions. Our analysis reveals that Riemannian SVRG comes<lb>with advantages of the usual SVRG method, but with factors depending on manifold curvature that<lb>influence its convergence. To the best of our knowledge, ours is the first fast stochastic Riemannian<lb>method. Moreover, our work offers the first non-asymptotic complexity analysis for nonconvex Riemannian<lb>optimization (even for the batch setting). Our results have several implications; for instance, they offer<lb>a Riemannian perspective on variance reduced PCA, which promises a short, transparent convergence<lb>analysis.", "creator": "LaTeX with hyperref package"}}}