{"id": "1605.07588", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "A Consistent Regularization Approach for Structured Prediction", "abstract": "We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed methods. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.", "histories": [["v1", "Tue, 24 May 2016 19:06:43 GMT  (528kb)", "https://arxiv.org/abs/1605.07588v1", "39 pages, 2 Tables, 1 Figure"], ["v2", "Thu, 26 May 2016 15:55:37 GMT  (528kb)", "http://arxiv.org/abs/1605.07588v2", "39 pages, 2 Tables, 1 Figure"], ["v3", "Fri, 28 Jul 2017 09:36:05 GMT  (528kb)", "http://arxiv.org/abs/1605.07588v3", "39 pages, 2 Tables, 1 Figure"]], "COMMENTS": "39 pages, 2 Tables, 1 Figure", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["carlo ciliberto", "lorenzo rosasco", "alessandro rudi"], "accepted": true, "id": "1605.07588"}, "pdf": {"name": "1605.07588.pdf", "metadata": {"source": "CRF", "title": "A Consistent Regularization Approach for Structured Prediction", "authors": ["Carlo Ciliberto", "Alessandro Rudi", "Lorenzo Rosasco"], "emails": ["cciliber@mit.edu", "ale_rudi@mit.edu", "lrosasco@mit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n07 58\n8v 3\n[ cs\n.L G\n] 2"}, {"heading": "1 Introduction", "text": "Many machine learning applications require dealing with data-sets having complex structures, e.g. natural language processing, image segmentation, reconstruction or captioning, pose estimation, protein folding prediction to name a few. [1\u20133]. Structured prediction problems pose a challenge for classic off-the-shelf learning algorithms for regression or binary classification. Indeed, this has motivated the extension of methods such as support vector machines to structured problems [4]. Dealing with structured prediction problems is also a challenge for learning theory. While the theory of empirical risk minimization provides a very general statistical framework, in practice computational considerations make things more involved. Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8]. A natural question is then whether a unifying learning theoretic framework can be developed encompassing a wide range of problems as special cases.\nIn this paper we take a step in this direction proposing and analyzing a regularization approach to a wide class of structured prediction problems defined by loss functions\n1Laboratory for Computational and Statistical Learning - Istituto Italiano di Tecnologia, Genova, Italy & Massachusetts Institute of Technology, Cambridge, MA 02139, USA.\n2Universit\u00e0 degli Studi di Genova, Genova, Italy.\nsatisfying mild conditions. Indeed, our starting observation is that a large class of loss functions naturally define an embedding of the structured outputs in a linear space. This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9\u201311]. The corresponding algorithm essentially reduces to a form of kernel ridge regression and generalizes the approach proposed in [12], hence providing also a novel derivation for this latter algorithm. Our theoretical analysis allows to characterize the generalization properties of the proposed approach. In particular, it allows to quantify the impact due to the surrogate approach and establishes universal consistency as well as finite sample bounds. An experimental analysis shows promising results on a variety of structured prediction problems.\nThe rest of this paper is organized as follows: in Sec. 2 we introduce the structured prediction problem in its generality and present our algorithm to approach it. In Sec. 3 we introduce and discuss a surrogate framework for structured prediction, from which we derive our algorithm. In Sec. 4, we analyze the theoretical properties of the proposed algorithm. In Sec. 5 we draw connections with previous work in structured prediction while in Sec. 6 we report a preliminary empirical analysis and comparison of the proposed approach. Finally, Sec. 7 concludes the paper outlining relevant directions for future research."}, {"heading": "2 A Regularization Approach to Structured prediction", "text": "The goal of supervised learning is to learn functional relations f : X \u2192 Y between two sets X ,Y, given a finite number of examples. In particular in this work we are interested to structured prediction, namely the case where Y is a set of structured outputs (such as histograms, graphs, time sequences, points on a manifold, etc.). Moreover, structure on Y can be implicitly induced by a suitable loss \u25b3 : Y \u00d7 Y \u2192 R (such as edit distance, ranking error, geodesic distance, indicator function of a subset, etc.). Then, the problem of structured prediction becomes\nminimize f:X\u2192Y\nE(f), with E(f) = \u222b\nX\u00d7Y \u25b3(f(x), y) d\u03c1(x, y) (1)\nand the goal is to find a good estimator for the minimizer of the above equation, given a finite number of (training) points {(xi, yi)}ni=1 sampled from a unknown probability distribution \u03c1 on X \u00d7 Y. In the following we introduce an estimator f\u0302 : X \u2192 Y to approach Eq. (1). The rest of this paper is devoted to prove that f\u0302 it a consistent estimator for a minimizer of Eq. (1).\nOur Algorithm for Structured Prediction. In this paper we propose and analyze the following estimator\nf\u0302(x) = argmin y\u2208Y\nn\u2211\ni=1\n\u03b1i(x)\u25b3 (y, yi) with \u03b1(x) = (K + n\u03bbI)\u22121Kx \u2208 Rn (Alg. 1)\ngiven a positive definite kernel k : X \u00d7 X \u2192 R and training set {(xi, yi)}ni=1. In the above expression, \u03b1i(x) is i-th entry in \u03b1(x), K \u2208 Rn\u00d7n is the kernel matrix Ki,j = k(xi, xj),\nKx \u2208 Rn the vector with entires (Kx)i = k(x, xi), \u03bb > 0 a regularization parameter and I the identity matrix. From a computational perspective, the procedure in Alg. 1 is divided in two steps: a learning step where input-dependents weights \u03b1i(\u00b7) are computed (which essentially consists in solving a kernel ridge regression problem) and a prediction step where the \u03b1i(x)-weighted linear combination in Alg. 1 is optimized, leading to a prediction f\u0302(x) given an input x. This idea was originally proposed in [13], where a \u201cscore\u201d function F(x, y) was learned to estimate the \u201clikelihood\u201d of a pair (x, y) sampled from \u03c1, and then used in\nf\u0302(x) = argmin y\u2208Y \u2212 F(x, y), (2)\nto predict the best f\u0302(x) \u2208 Y given x \u2208 X . This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].\nIntuition. While providing a principled derivation of Alg. 1 for a large class of loss functions is a main contribution of this work, it is useful to first consider the special case where \u25b3 is induced by a reproducing kernel h : Y \u00d7 Y \u2192 R on the output set, such that\n\u25b3(y, y \u2032) = h(y, y) \u2212 2h(y, y \u2032) + h(y \u2032, y \u2032). (3)\nThis choice of\u25b3 was originally considered in Kernel Dependency Estimation (KDE) [15]. In particular, for the case of normalized kernels (i.e. h(y, y) = 1 \u2200y \u2208 Y), Alg. 1 essentially reduces to [12, 14] and recalling their derivation is insightful. Note that, since a kernel can be written as h(y, y \u2032) = \u3008\u03c8(y), \u03c8(y \u2032)\u3009HY , with \u03c8 : Y \u2192 HY a non-linear map into a feature space HY [16], then Eq. (3) can be rewritten as\n\u25b3(f(x), y \u2032) = \u2016\u03c8(f(x)) \u2212\u03c8(y \u2032)\u20162HY . (4)\nDirectly minimizing the equation above with respect to f is generally challenging due to the non linearity \u03c8. A possibility is to replace \u03c8 \u25e6 f by a function g : X \u2192 HY that is easier to optimize. We can then consider the regularized problem\nminimize g\u2208G\n1\nn\nn\u2211\ni=1\n\u2016g(xi) \u2212\u03c8(yi)\u20162HY + \u03bb\u2016g\u2016 2 G (5)\nwith G a space of functions1 g : X \u2192 HY of the form g(x) = \u2211\ni=1 k(x, xi)ci with ci \u2208 HY and k a reproducing kernel. Indeed, in this case the solution to Eq. (5) is\ng\u0302(x) =\nn\u2211\ni=1\n\u03b1i(x)\u03c8(yi) with \u03b1(x) = (K+ n\u03bbI)\u22121Kx \u2208 Rn (6)\n1G is the reproducing kernel Hilbert space for vector-valued functions [9] with inner product \u3008k(xi, \u00b7)ci, k(xj, \u00b7)cj\u3009G = k(xi, xj)\u3008ci, cj\u3009HY\nwhere the \u03b1i are the same as in Alg. 1. Since we replaced \u25b3(f(x), y) by \u2016g(x) \u2212\u03c8(y)\u20162HY , a natural question is how to recover an estimator f\u0302 from g\u0302. In [12] it was proposed to consider\nf\u0302(x) = argmin y\u2208Y \u2016\u03c8(y) \u2212 g\u0302(x)\u20162HY = argmin y\u2208Y h(y, y) \u2212 2\nn\u2211\ni=1\n\u03b1i(x)h(y, yi), (7)\nwhich corresponds to Alg. 1 when h is a normalized kernel. The discussion above provides an intuition on how Alg. 1 is derived but raises also a few questions. First, it is not clear if and how the same strategy could be generalized to loss functions that do not satisfy Eq. (3). Second, the above reasoning hinges on the idea of replacing f\u0302 with g\u0302 (and then recovering f\u0302 by Eq. (7)), however it is not clear whether this approach can be justified theoretically. Finally, we can ask what are the statistical properties of the resulting algorithm. We address the first two questions in the next section, while the rest of the paper is devoted to establish universal consistency and generalization bounds for algorithm Alg. 1.\nNotation and Assumptions. We introduce here some minimal technical assumptions that we will use throughout this work. We will assume X and Y to be Polish spaces, namely separable completely metrizable spaces equipped with the associated Borel sigmaalgebra. Given a Borel probability distribution \u03c1 on X \u00d7 Y we denote with \u03c1(\u00b7|x) the associated conditional measure on Y (given x \u2208 X ) and with \u03c1X the marginal distribution on X ."}, {"heading": "3 Surrogate Framework and Derivation", "text": "To derive Alg. 1 we consider ideas from surrogate approaches [7,17,18] and in particular [5]. The idea is to tackle Eq. (1) by substituting \u25b3(f(x), y) with a \u201crelaxation\u201d L(g(x), y) that is easy to optimize. The corresponding surrogate problem is\nminimize g:X\u2192HY\nR(g), with R(g) = \u222b\nX\u00d7Y L(g(x), y) d\u03c1(x, y), (8)\nand the question is how a solution g\u2217 for the above problem can be related to a minimizer f\u2217 of Eq. (1). This is made possible by the requirement that there exists a decoding d : HY \u2192 Y, such that\nFisher Consistency: E(d \u25e6 g\u2217) = E(f\u2217), (9) Comparison Inequality: E(d \u25e6 g) \u2212 E(f\u2217) \u2264 \u03d5(R(g) \u2212R(g\u2217)), (10)\nhold for all g : X \u2192 HY , where \u03d5 : R \u2192 R is such that \u03d5(s) \u2192 0 for s \u2192 0. Indeed, given an estimator g\u0302 for g\u2217, we can \u201cdecode\u201d it considering f\u0302 = d \u25e6 g\u0302 and use the excess risk R(g\u0302) \u2212 R(g\u2217) to control E(f\u0302) \u2212 E(f\u2217) via the comparison inequality in Eq. (10). In particular, if g\u0302 is a data-dependent predictor trained on n points and R(g\u0302) \u2192 R(g\u2217) when n \u2192 +\u221e, we automatically have E(f\u0302) \u2192 E(f\u2217). Moreover, if \u03d5 in Eq. (10) is known explicitly, generalization bounds for g\u0302 are automatically extended to f\u0302.\nProvided with this perspective on surrogate approaches, here we revisit the discussion of Sec. 2 for the case of a loss function induced by a kernel h. Indeed, by assuming the surrogate L(g(x), y) = \u2016g(x) \u2212 \u03c8(y)\u20162HY , Eq. (5) becomes the empirical version of the surrogate problem at Eq. (8) and leads to an estimator g\u0302 of g\u2217 as in Eq. (6). Therefore, the approach in [12, 14] to recover f\u0302(x) = argminy L(g(x), y) can be interpreted as the result f\u0302(x) = d\u25e6 g\u0302(x) of a suitable decoding of g\u0302(x). An immediate question is whether the above framework satisfies Eq. (9) and (10). Moreover, we can ask if the same idea could be applied to more general loss functions.\nIn this work we identify conditions on\u25b3 that are satisfied by a large family of functions and moreover allow to design a surrogate framework for which we prove Eq. (9) and (10). The first step in this direction is to introduce the following assumption.\nAssumption 1. There exists a separable Hilbert space HY with inner product \u3008\u00b7, \u00b7\u3009HY , a continuous embedding \u03c8 : Y \u2192 HY and a bounded linear operator V : HY \u2192 HY , such that\n\u25b3 (y, y \u2032) = \u3008\u03c8(y), V\u03c8(y \u2032)\u3009HY \u2200y, y \u2032 \u2208 Y (11)\nAsm. 1 is similar to Eq. (4) and in particular to the definition of a reproducing kernel. Note however that by not requiring V to be positive semidefinite (or even symmetric), we allow for a surprisingly wide range of functions. Indeed, below we discuss some examples of functions that satisfy Asm. 1 (see Appendix Sec. C for more details):\nExample 1. The following functions of the form \u25b3 : Y \u00d7 Y \u2192 R satisfy Asm. 1: 1. Any loss on Y of finite cardinality. Several problems belong to this setting, such as\nMulti-Class Classification, Multi-labeling, Ranking, predicting Graphs (e.g. protein foldings).\n2. Regression and Classification Loss Functions: Least-squares, Logistic, Hinge, \u01eb-insensitive, \u03c4-Pinball.\n3. Robust Loss Functions Most loss functions used for robust estimation [19] such as the absolute value, Huber, Cauchy, German-McLure, \u201cFair\u201d and L2 \u2212 L1. See [19] or the Appendix for their explicit formulation.\n4. KDE. Loss functions \u25b3 induced by a kernel such as in Eq. (3).\n5. Distances on Histograms/Probabilities. The \u03c72 and the squared Hellinger distances.\n6. Diffusion distances on Manifolds. The squared diffusion distance induced by the heat kernel (at time t > 0) on a compact Reimannian manifold without boundary [20].\nThe Least Squares Loss Surrogate Framework. Asm. 1 implicitly defines the space HY similarly to Eq. (4). The following result motivates the choice of the least squares surrogate and moreover suggests a possible choice for the decoding.\nLemma 1. Let \u25b3 : Y \u00d7Y \u2192 R satisfy Asm. 1 with \u03c8 : Y \u2192 HY bounded. Then the expected risk in Eq. (1) can be written as\nE(f) = \u222b\nX \u3008\u03c8(f(x)), Vg\u2217(x)\u3009HY d\u03c1X (x) (12)\nfor all f : X \u2192 Y, where g\u2217 : X \u2192 HY minimizes\nR(g) = \u222b\nX\u00d7Y \u2016g(x) \u2212\u03c8(y)\u20162HY d\u03c1(x, y). (13)\nLemma 1 shows how Eq. (13) arises naturally as surrogate problem. In particular, Eq. (12) suggests how to chose the decoding. Indeed, consider an f : X \u2192 Y such that for each x \u2208 X , f(x) is a minimizer of the argument in Eq. (1), namely \u3008\u03c8(f(x)), Vg\u2217(x)\u3009HY \u2264 \u3008\u03c8(y,Vg\u2217(x)\u3009HY for all y \u2208 Y. Then we have E(f) \u2264 E(f \u2032) for any f \u2032 : X \u2192 Y. Following this observation, in this work we consider the decoding d : HY \u2192 Y such that\nd(h) = argmin y\u2208Y \u3008 \u03c8(y) , Vh \u3009HY \u2200h \u2208 HY . (14)\nSo that E(d \u25e6 g\u2217) \u2264 E(f \u2032) for all f \u2032 : X \u2192 Y. Indeed, for this choice of decoding, we have the following result.\nTheorem 2. Let \u25b3 : Y \u00d7 Y \u2192 R satisfy Asm. 1 with Y a compact set. Then, for every measurable g : X \u2192 HY and d : HY \u2192 Y satisfying Eq. (14), the following holds\nE(d \u25e6 g\u2217) = E(f\u2217) (15) E(d \u25e6 g) \u2212 E(f\u2217) \u2264 2c\u25b3 \u221a R(g) \u2212R(g\u2217). (16)\nwith c\u25b3 = \u2016V\u2016maxy\u2208Y \u2016\u03c8(y)\u2016HY . Thm. 2 shows that for all \u25b3 satisfying Asm. 1, the corresponding surrogate framework identified by the surrogate in Eq. (13) and decoding Eq. (14) satisfies Fisher consistency and the comparison inequality in Eq. (16). We recall that a finite set Y is always compact, and moreover, assuming the discrete topology on Y, we have that any \u03c8 : Y \u2192 HY is continuous. Therefore, Thm. 2 applies in particular to any structured prediction problem on Y with finite cardinality.\nThm. 2 suggest to approach structured prediction by first learning g\u0302 and then decoding it to recover f\u0302 = d \u25e6 g\u0302. A natural question is how to choose g\u0302 in order to compute f\u0302 in practice. In the rest of this section we propose an approach to this problem.\nDerivation for Alg. 1. Minimizing R in Eq. (13) corresponds to a vector-valued regression problem [9\u201311]. In this work we adopt an empirical risk minimization approach to learn g\u0302 as in Eq. (5). The following result shows that combining g\u0302 with the decoding in Eq. (14) leads to the f\u0302 in Alg. 1.\nLemma 3. Let \u25b3 : Y \u00d7 Y \u2192 R satisfy Asm. 1 with Y a compact set. Let g\u0302 : X \u2192 HY be the minimizer of Eq. (5). Then, for all x \u2208 X\nd \u25e6 g\u0302(x) = argmin y\u2208Y\nn\u2211\ni=1\n\u03b1i(x)\u25b3 (y, yi) \u03b1(x) = (K+ n\u03bbI)\u22121Kx \u2208 Rn (17)\nLemma 3 concludes the derivation of Alg. 1. An interesting observation is that computing f\u0302 does not require explicit knowledge of the embedding \u03c8 and the operator V , which are implicitly encoded within the loss \u25b3 by Asm. 1. In analogy to the kernel trick [21] we informally refer to such assumption as the \u201closs trick\u201d. We illustrate this effect with an example.\nExample 2 (Ranking). In ranking problems the goal is to predict ordered sequences of a fixed number \u2113 of labels. For these problems, Y corresponds to the set of all ordered sequences of \u2113 labels and has cardinality |Y | = \u2113!, which is typically dramatically larger than the number n of training examples (e.g. for \u2113 = 15, \u2113! \u2243 1012). Therefore, given an input x \u2208 X , directly computing g\u0302(x) \u2208 R|Y | is impractical. On the opposite, the loss trick allows to express d\u25e6 g\u0302(x) only in terms of the n weights \u03b1i(x) in Alg. 1, making the computation of the argmin easier to approach in general. For details on the rank loss \u25b3rank and the corresponding optimization over Y we refer to the empirical analysis of Sec. 6.\nIn this section we have shown a derivation for the structured prediction algorithm proposed in this work. In Thm. 2 we have shown how the expected risk of the proposed estimator f\u0302 is related to an estimator g\u0302 via a comparison inequality. In the following we will make use of these results to prove consistency and generalization bounds for Alg. Alg. 1."}, {"heading": "4 Statistical Analysis", "text": "In this section we study the statistical properties of Alg. 1. In particular we made use of the relation between the structured and surrogate problems via the comparison inequality in Thm. 2. We begin our analysis by proving that Alg. 1 is universally consistent.\nTheorem 4 (Universal Consistency). Let\u25b3 : Y\u00d7Y \u2192 R satisfy Asm. 1, X and Y be compact sets and k : X \u00d7 X \u2192 R a continuous universal reproducing kernel2. For any n \u2208 N and any distribution \u03c1 on X \u00d7 Y let f\u0302n : X \u2192 Y be obtained by Alg. 1 with {(xi, yi)}ni=1 training points independently sampled from \u03c1 and \u03bbn = n \u22121/4. Then,\nlim n\u2192+\u221e\nE(f\u0302n) = E(f\u2217) with probability 1 (18)\nThm. 4 shows that, when the \u25b3 satisfies Asm. 1, Alg. 1 approximates a solution f\u2217 to Eq. (1) arbitrarily well, given a sufficient number of training examples. To the best of our knowledge this is the first consistency result for structured prediction in the general setting considered in this work and characterized by Asm. 1, in particular for the case of Y with infinite cardinality (dense or discrete).\nThe No Free Lunch Theorem [22] states that it is not possible to prove uniform convergence rates for Eq. (18). However, by imposing suitable assumptions on the regularity of g\u2217 it is possible to prove generalization bounds for g\u0302 and then, using Thm. 2, extend them to f\u0302. To show this, it is sufficient to require that g\u2217 belongs to G the reproducing kernel Hilbert space used in the ridge regression of Eq. (5). Note that in the proofs of Thm. 4 and Thm. 5, our analysis on g\u0302 borrows ideas from [10] and extends their result to our setting for the case of HY infinite dimensional (i.e. when Y has infinite cardinality). Indeed, note that in this case [10] cannot be applied to the estimator g\u0302 considered in this work (see Appendix Sec. B.3, Lemma 18 for details).\nTheorem 5 (Generalization Bound). Let \u25b3 : Y \u00d7 Y \u2192 R satisfy Asm. 1, Y be a compact set and k : X \u00d7 X \u2192 R a bounded continuous reproducing kernel. Let f\u0302n denote the solution\n2This is a standard assumption for universal consistency (see [18]). An example of continuous universal kernel is the Gaussian k(x, x \u2032) = exp(\u2212\u2016x\u2212 x \u2032\u20162/\u03c3).\nof Alg. 1 with n training points and \u03bb = n\u22121/2. If the surrogate risk R defined in Eq. (13) admits a minimizer g\u2217 \u2208 G, then\nE(f\u0302n) \u2212 E(f\u2217) \u2264 c\u03c42 n\u2212 1 4 (19)\nholds with probability 1\u2212 8e\u2212\u03c4 for any \u03c4 > 0, with c a constant not depending on n and \u03c4.\nThe bound in Eq. (5) is of the same order of the generalization bounds available for the least squares binary classifier [23]. Indeed, in Sec. 5 we show that in classification settings Alg. 1 reduces to least squares classification.\nRemark 1 (Better Comparison Inequality). The generalization bounds for the least squares classifier can be improved by imposing regularity conditions on \u03c1 via the Tsybakov condi-\ntion [23]. This was observed in [23] for binary classification with the least squares surrogate,\nwhere a tighter comparison inequality than the one in Thm. 2 was proved. Therefore, a nat-\nural question is whether the inequality of Thm. 2 could be similarly improved, consequently\nleading to better rates for Thm. 5. Promising results in this direction can be found in [5],\nwhere the Tsybakov condition was generalized to the multi-class setting and led to a tight\ncomparison inequality analogous to the one for the binary setting. However, this question\ndeserves further investigation. Indeed, it is not clear how the approach in [5] could be further generalized to the case where Y has infinite cardinality.\nRemark 2 (Other Surrogate Frameworks). In this paper we focused on a least squares surrogate loss function and corresponding framework. A natural question is to ask whether other\nloss functions could be considered to approach the structured prediction problem, sharing the\nsame or possibly even better properties. This question is related also to Remark 1, since dif-\nferent surrogate frameworks could lead to sharper comparison inequalities. This seems an\ninteresting direction for future work."}, {"heading": "5 Connection with Previous Work", "text": "In this section we draw connections between Alg. 1 and previous methods for structured prediction learning.\nBinary and Multi-class Classification. It is interesting to note that in classification settings, Alg. 1 corresponds to the least squares classifier [23]. Indeed, let Y = {1, . . . , \u2113} be a set of labels and consider the misclassification loss\u25b3(y, y \u2032) = 1 for y 6= y \u2032 and 0 otherwise. Then \u25b3(y, y \u2032) = e\u22a4y Vey \u2032 with ei \u2208 R\u2113 the i-the element of the canonical basis of R\u2113 and V = 1 \u2212 I, where I is the \u2113 \u00d7 \u2113 identity matrix and 1 the matrix with all entries equal to 1. In the notation of surrogate methods adopted in this work, HY = R\u2113 and \u03c8(y) = ey. Note that both Least squares classification and our approach solve the surrogate problem at Eq. (5)\n1\nn\nn\u2211\ni=1\n\u2016g(xi) \u2212 eyi\u20162RT + \u03bb \u2016g\u20162G (20)\nto obtain a vector-valued predictor g\u0302 : X \u2192 RT as in Eq. (6). Then, the least squares classifier c\u0302 and the decoding f\u0302 = d \u25e6 g\u0302 are respectively obtained by\nc\u0302(x) = argmax i=1,...,T g\u0302(x) f\u0302(x) = argmin i=1,...,T Vg\u0302(x). (21)\nHowever, since V = 1\u2212 I, it is easy to see that c\u0302(x) = f\u0302(x) for all x \u2208 X .\nKernel Dependency Estimation. In Sec. 2 we discussed the relation between KDE [12, 15] and Alg. 1. In particular, we have observed that if \u25b3 is induced by a kernel h : Y \u00d7 Y \u2192 R as in Eq. (3) and h is normalized, i.e. h(y, y) = 1 \u2200y \u2208 Y, then algorithm Eq. (7) proposed in [12] leads to the same predictor as Alg. 1. Therefore, we can apply Thm. 4 and 5 to prove universal consistency and generalization bounds for methods such as [12,14]. We are not aware of previous results proving consistency (and generalization bounds) for the KDE methods in [12, 14]. Note however that when the kernel h is not normalized, the \u201cdecoding\u201d in Eq. (7) is not equivalent to Alg. 1. In particular, given the surrogate solution g\u2217, applying Eq. (7) leads to predictors that are do not minimize Eq. (1). As a consequence the approach in [12] is not consistent in the general case.\nSupport Vector Machines for Structured Output. A popular approach to structured prediction is the Support Vector Machine for Structured Outputs (SVMstruct) [4] that extends ideas from the well-known SVM algorithm to the structured setting. One of the main advantages of SVMstruct is that it can be applied to a variety of problems since it does not impose strong assumptions on the loss. In this view, our approach, as well as KDE, shares similar properties, and in particular allows to consider Y of infinite cardinality. Moreover, we note that generalization studies for SVMstruct are available [3] (Ch. 11). However, it seems that these latter results do not allow to derive universal consistency of the method."}, {"heading": "6 Experiments", "text": "Ranking Movies. We considered the problem of ranking movies in the MovieLens dataset [26] (ratings (from 1 to 5) of 1682 movies by 943 users). The goal was to predict preferences of a given user, i.e. an ordering of the 1682 movies, according to the user\u2019s partial ratings. Note that, as observed in Example 2, in ranking problems the output set Y is the collection of all ordered sequences of a predefined length. Therefore, Y is finite (albeit extremely large) and we can apply Alg. 1.\nWe applied Alg. 1 to the ranking problem using the rank loss [7]\n\u25b3rank(y, y \u2032) = M\u2211\ni,j=1\n\u03b3(y \u2032)ij (1\u2212 sign(yi \u2212 yj))/2, (22)\nwith M is the number of movies in the database, y \u2208 Y a vector of the M integers yi \u2208 {1, . . . ,M} without repetition, where yi corresponding to the rank assigned by y to movie i. In the definition of \u25b3rank, \u03b3(y)ij denotes the costs (or reward) of having movie j ranked higher than movie i and, similarly to [7], we set \u03b3(y)ij equal to the difference of ratings provided by user associated to y (from 1 to 5). We chose as k in Alg. 1, a linear kernel on features similar to those proposed in [7], which were computed based on users\u2019 profession, age, similarity of previous ratings, etc. Since solving Alg. 1 for \u25b3rank is NPhard (see [7]) we adopted the Feedback Arc Set approximation (FAS) proposed in [27] to approximate the f\u0302(x) of Alg. 1. Results are reported in Tab. 1 comparing Alg. 1 (Ours) with surrogate ranking methods using a Linear [7], Hinge [24] or Logistic [25] loss and Struct SVM [4]3. We randomly sampled n = 643 users for training and tested on the remaining 300. We performed 5-fold cross-validation for model selection. We report the normalized\u25b3rank, averaged over 10 trials to account for statistical variability. Interestingly, our approach appears to outperform all competitors, suggesting that Alg. 1 is a viable approach to ranking.\nImage Reconstruction with Hellinger Distance. We considered the USPS4 digits reconstruction experiment originally proposed in [15]. The goal is to predict the lower half of an image depicting a digit, given the upper half of the same image in input. The standard approach is to use a Gaussian kernel kG on images in input and adopt KDE methods such as [12,14,15] with loss \u25b3G(y, y \u2032) = 1\u2212 kG(y, y \u2032). Here we take a different approach and, following [28], we interpret an image depicting a digit as an histogram and normalize it to sum up to 1. Therefore, Y becomes is the unit simplex in R128 (16\u00d7 16 images) and we adopt the squared Hellinger distance \u25b3H\n\u25b3H(y, y \u2032) = n\u2211\ni=1\n( \u221a yi \u2212 \u221a yi \u2032 )2 for y = (yi) M i=1 (23)\nto measure distances on Y. We used the kernel kG on the input space and compared Alg. 1 using respectively \u25b3H and \u25b3G. For \u25b3G Alg. 1 correpsponds to [12]. We performed digit\n3implementation from http://svmlight.joachims.org/svm_struct.html 4http://www.cs.nyu.edu/~roweis/data.html\nreconstruction experiments by training on 1000 examples evenly distributed among the 10 digits of USPS and tested on 5000 images. We performed 5-fold cross-validation for model selection. Tab. 2 reports the performance of Alg. 1 and the KDE methods averaged over 10 runs. Performance are reported according to the Gaussian loss \u25b3G Hellinger loss \u25b3H. Unsurprisingly, methods trained with respect to a specific loss perform better than the competitor with respect to such loss. Therefore, as a further measure of performance we also introduced the \u201cRecognition\u201d loss \u25b3R. This loss has to be intended as a measure of how \u201cwell\u201d a predictor was able to correctly reconstruct an image for digit recognition purposes. To this end, we trained an automatic digit classifier and defined \u25b3R to be the misclassification error of such classifier when tested on images reconstructed by the two prediction algorithms. This automatic classifier was trained using a standard SVM [21] (with LIBSVM5) on a separate subset of USPS images and achieved an average 0.04% error rate on the true 5000 test sets. In this case a clear difference in performance can be observed between using two different loss functions, suggesting that \u25b3H is more suited to the reconstruction problem.\nRobust Estimation. We considered a regression problems with many outliers and evaluated Alg. 1 using the Cauchy loss (see Example 1 - (3)) for robust estimation. Indeed, in this setting, Y = [\u2212M,M] \u2282 R is not structured, but the non-convexity of \u25b3 can be an obstacle to the learning process. We generated a dataset according to the model y = sin(6\u03c0x)+\u01eb+ \u03b6, where x was sampled uniformly on [\u22121, 1] and \u01eb according to a zeromean Gaussian with variance 0.1. \u03b6 modeled the outliers and was sampled according to a zero-mean random variable that was 0 with probability 0.90 and a value uniformly at random in [\u22123, 3] with probability 0.1). We compared Alg. 1 with the Nadaraya-Watson robust estimator (RNW) [29] and kernel ridge regression (KRR) with a Gaussian kernel as baseline. To train Alg. 1 we used a Gaussian kernel on the input and performed predictions (i.e. solved Eq. (17)) using Matlab FMINUNC function for unconstrained minimization. Experiments were performed with training sets of increasing dimension (100 repetitions each) and test set of 1000 examples. 5-fold cross-validation for model selection. Results are reported in Fig. 1, showing that our estimator significantly outperforms the others.\n5https://www.csie.ntu.edu.tw/~cjlin/libsvm/\nMoreover, our method appears to greatly benefit from training sets of increasing size."}, {"heading": "7 Conclusions and Future Work", "text": "In this work we considered the problem of structured prediction from a Statistical Learning Theory perspective. We proposed a learning algorithm for structured prediction that is split into a learning and prediction step similarly to previous methods in the literature. We studied the statistical properties of the proposed algorithm by adopting a strategy inspired to surrogate methods. In particular, we identified a large family of loss functions for which it is natural to identify a corresponding surrogate problem. This perspective allows to prove a derivation of the algorithm proposed in this work. Moreover, by exploiting a comparison inequality relating the original and surrogate problems we were able to prove universal consistency and generalization bounds under mild assumption. In particular, the bounds proved in this work recover those already known for least squares classification, of which our approach can be seen as a generalization. We supported our theoretical analysis with experiments showing promising results on a variety of structured prediction problems.\nA few questions were left opened. First, we ask whether the comparison inequality can be improved (under suitable hypotheses) to obtain faster generalization bounds for our algorithm. Second, the surrogate problem in our work consists of a vector-valued regression (in a possibly infinite dimensional Hilbert space), we solved this problem by plain kernel ridge regression but it is natural to ask whether approaches from the multitask learning literature could lead to substantial improvements in this setting. Finally, an interesting question is whether alternative surrogate frameworks could be derived for the setting considered in this work, possibly leading to tighter comparison inequalities. We will investigate these questions in the future."}, {"heading": "Appendix", "text": "The Appendix of this work is divided in the following three sections:\nA Proofs of Fisher consistency and comparison inequality (Thm. 2).\nB Universal Consistency and Generalization Bounds for Alg. 1. (Thm. 4 and 5).\nC The characterization of a large family of \u25b3s satisfying Asm. 1 (Thm. 19)."}, {"heading": "Mathematical Setting", "text": "In the following we will always assume X and Y to be Polish spaces, namely separable complete metrizable spaces, equipped with the associated Borel sigma-algebra. When referring to a probability distribution \u03c1 on X \u00d7 Y we will always assume it to be a Borel probability measure, with \u03c1X the marginal distribution on X and \u03c1(\u00b7|x) the conditional measure on Y given x \u2208 X . We recall [1] that \u03c1(y|x) is a regular conditional distribution and its domain, which we will denoteD\u03c1|X in the following, is a measurable set contained in the support of \u03c1X and corresponds to the support of \u03c1X up to a set of measure zero.\nFor convenience, we recall here the main assumption of our work.\nAssumption 1. There exists a separable Hilbert space HY with inner product \u3008\u00b7, \u00b7\u3009HY , a continuous embedding \u03c8 : Y \u2192 HY and a bounded linear operator V : HY \u2192 HY , such that\n\u25b3 (y, y \u2032) = \u3008\u03c8(y), V\u03c8(y \u2032)\u3009HY \u2200y, y \u2032 \u2208 Y (11)\nBasic notation We recall that a Hilbert space H is a vector space with inner product \u3008\u00b7, \u00b7\u3009H, closed with respect to the norm \u2016h\u2016H = \u221a \u3008h, h\u3009H for any h \u2208 H. We denote with L2(X , \u03c1X ,H) the Lebesgue space of square integrable functions on X with respect to a measure \u03c1X and with values in a separable Hilbert space H. We denote with \u3008f, g\u3009\u03c1X the inner product \u222b \u3008f(x), g(x)\u3009Hd\u03c1X (x), for all f, g \u2208 L2(X , \u03c1X ,H). In particular when H = R we denote with L2(X , \u03c1X ) the space L2(X , \u03c1X ,R). Given a linear operator V : H \u2192 H \u2032 between two Hilbert spaces H,H \u2032, we denote with Tr(V) the trace of V and with V\u2217 : H \u2032 \u2192 H the adjoint operator associated to V , namely such that \u3008Vh, h \u2032\u3009 \u2032H = \u3008h,V\u2217h \u2032\u3009H \u2032 for every h \u2208 H, h \u2032 \u2208 H \u2032. Moreover, we denote with \u2016V\u2016 = sup\u2016h\u2016H\u22641 \u2016Vh\u2016H \u2032 and \u2016V\u2016HS = \u221a Tr(V\u2217V) respectively the operator norm and Hilbert-Schmidt norm of V . We recall that a linear operator V is continuous if and only if \u2016V\u2016 < +\u221e and we denote B(H,H \u2032) the set of all continuous linear operators from H to H \u2032. Moreover, we denote B2(H,H \u2032) the set of all operators V : H \u2192 H \u2032 with \u2016V\u2016HS < +\u221e and recall that B2(H,H \u2032) is isometric to the space H \u2032 \u2297H, with \u2297 denoting the tensor product. Indeed, for the sake of simplicity, with some abuse of notation we will not make the distinction between the two spaces.\nNote that in most of our results we will require Y to be non-empty and compact, so that a continuous functional over Y always attains a minimizer on Y and therefore the operator argminy\u2208Y is well defined. Note that for a finite set Y, we will always assume it endowed with the discrete topology, so that Y is compact and any function\u25b3 : Y\u00d7Y \u2192 R is continuous.\nOn the Argmin Notice that for simplicity of notation, in the paper we denoted the minimizer of Alg. 1 as\nf\u0302(x) = argmin y\u2208Y\nn\u2211\ni=1\n\u03b1i(x)\u25b3 (y, yi). (24)\nHowever note that the correct notation should be\nf\u0302(x) \u2208 argmin y\u2208Y\nn\u2211\ni=1\n\u03b1i(x)\u25b3 (y, yi) (25)\nsince a loss function \u25b3 can have more than one minimizer in general. In the following we keep this more pedantic, yet correct notation.\nExpected Risk Minimization Note that whenever we write an expected risk minimization problem, we implicitly assume the optimization domain to be the space of measurable functions. For instance, Eq. (1) would be written more rigorously as\nminimize {\u222b\nX\u00d7Y \u25b3(f(x), y) d\u03c1(x, y) | f : X \u2192 Y measurable\n}\n(26)\nIn the next Lemma, following [2] we show that the problem in Eq. (1) admits a measurable pointwise minimizer.\nLemma 6 (Existence of a solution for Eq. (1)). Let\u25b3 : Y\u00d7Y \u2192 R be a continuous function. Then, the expected risk minimization at Eq. (1) admits a measurable minimizer f\u2217 : X \u2192 Y such that\nf\u2217(x) \u2208 argmin y\u2208Y\n\u222b\nY \u25b3(y, y \u2032)d\u03c1(y \u2032|x) (27)\nfor every x \u2208 D\u03c1|X . Moreover, the function m : X \u2192 R defined as follows, is measurable\nm(x) = inf y\u2208Y r(x, y), with r(x, y) =\n{ \u222b Y \u25b3(y, y \u2032)d\u03c1(y \u2032|x) if x \u2208 D\u03c1|X\n0 otherwise (28)\nProof. Since \u25b3 is continuous and \u03c1(y|x) is a regular conditional distribution, then r is a Carath\u00e9odory function (see Definition 4.50 (pp. 153) of [3]), namely continuous in y for each x \u2208 X and measurable in x for each y \u2208 Y. Thus, by Theorem 18.19 (pp. 605) of [3] (or Aumann\u2019s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f\u2217 : X \u2192 Y such that r(x, f\u2217(x)) = m(x) for all x \u2208 X . Moreover, by definition of m, given any measurable f : X \u2192 Y, we have m(x) \u2264 r(x, f(x)). Therefore,\nE(f\u2217) = \u222b r(x, f\u2217(x))d\u03c1X (x) = \u222b m(x)d\u03c1X (x) \u2264 \u222b r(x, f(x))d\u03c1X (x) = E(f). (29)\nWe conclude E(f\u2217) \u2264 inff:X\u2192Y E(f) and, since f\u2217 is measurable, E(f\u2217) = minf:X\u2192Y E(f) and f\u2217 is a global minimizer.\nWe have an immediate Corollary to Lemma 6.\nCorollary 7. With the hypotheses of Lemma 6, let f\u0303 : X \u2192 Y such that\nf\u0303(x) \u2208 argmin y\u2208Y\n\u222b\nY \u25b3(y, y \u2032)d\u03c1(y \u2032|x)\nfor almost every x \u2208 D\u03c1|X . Then E(f\u0303) = inff:X\u2192Y E(f).\nProof. The result follows directly from Lemma 6 by noting that r(x, f\u0303(x)) = m(x) almost everywhere onD\u03c1|X . Hence, sinceD\u03c1|X is equal to the support of \u03c1X up to a set of measure zero, E(f\u0303) =\n\u222b X m(x)d\u03c1X (x) = E(f\u2217) = inff E(f).\nWith the above basic notation and results, we can proceed to prove the results presented in this work."}, {"heading": "A Surrogate Problem, Fisher Consistency and Comparison In-", "text": "equality\nIn this section we focus on the surrogate framework introduced in Sec. 3 and prove that it is Fisher consistent and that the comparison inequality. To do so, we will first characterizes the solution(s) of the surrogate expected risk minimization introduced at Eq. (13). We recall that in our setting, the surrogate risk was defined as the functional R(g) =\n\u222b X\u00d7Y \u2016\u03c8(y) \u2212 g\u2217(x)\u20162HYd\u03c1(x, y), where \u03c8 : Y \u2192 HY is continuous (by Asm. 1).\nIn the following, when \u03c8 is bounded, we will denote with Q = supy\u2208Y \u2016\u03c8(y)\u2016HY . Note that in most our results we will assume Y to be compact. In these settings we always have Q = maxy\u2208Y \u2016\u03c8(y)\u2016HY by the continuity of \u03c8.\nWe start with a preliminary lemma necessary to prove Lemma 1 and Thm. 2.\nLemma 8. Let HY a separable Hilbert space and \u03c8 : Y \u2192 HY measurable and bounded. Then, the function g\u2217 : X \u2192 HY such that\ng\u2217(x) = \u222b\nY \u03c8(y)d\u03c1(y|x) \u2200x \u2208 D\u03c1|X (30)\nand g\u2217(x) = 0 otherwise, belongs to L2(X , \u03c1X ,HY) and is a minimizer of the surrogate expected risk at Eq. (13). Moreover, any minimizer of Eq. (13) is equal to g\u2217 almost everywhere on the domain of \u03c1X .\nProof. By hypothesis, \u2016\u03c8\u2016HY is measurable and bounded. Therefore, since \u03c1(y|x) is a regular conditional probability, we have that g\u2217 is measurable on X (see for instance [2]). Moreover, the norm of g\u2217 is dominated by the constant function of value Q, thus g\u2217 is integrable on X with respect to \u03c1X and in particular it is in L2(X , \u03c1X ,HY) since \u03c1X is a finite regular measure. Recall that since \u03c1(y|x) is a regular conditional distribution, for any measurable g : X \u2192 HY , the functional in Eq. (13) can be written as\nR(g) = \u222b\nX\u00d7Y \u2016g(x) \u2212\u03c8(y)\u20162HYd\u03c1(x, y) =\n\u222b\nX\n\u222b\nY \u2016g(x) \u2212\u03c8(y)\u20162HYd\u03c1(y|x)d\u03c1X (x). (31)\nNotice that g\u2217(x) = argmin\u03b7\u2208HY \u222b Y \u2016\u03b7 \u2212 \u03c8(y)\u20162HYd\u03c1(y|x) almost everywhere on D\u03c1|X . Indeed, \u222b\nY \u2016\u03b7\u2212\u03c8(y)\u20162HYd\u03c1(y|x) = \u2016\u03b7\u2016 2 HY \u2212 2\u3008\u03b7,\n(\u222b\nY \u03c8(y)d\u03c1(y|x)\n) \u3009 + \u222b\nY \u2016\u03c8(y)\u20162HYd\u03c1(y|x)\n(32)\n= \u2016\u03b7\u20162HY \u2212 2\u3008\u03b7, g \u2217(x)\u3009HY + const. (33)\nfor all x \u2208 D\u03c1|X , which is minimized by \u03b7 = g\u2217(x) for all x \u2208 D\u03c1|X . Therefore, since D\u03c1|X is equal to the support of \u03c1X up to a set of measure zero, we conclude that R(g\u2217) \u2264 infg:X\u2192HY R(g) and, since g\u2217 is measurable, R(g\u2217) = ming:X\u2192HY R(g) and g\u2217 is a global minimizer as required.\nFinally, notice that for any g : X \u2192 HY we have\nR(g) \u2212R(g\u2217) = \u222b\nX\u00d7Y \u2016g(x) \u2212\u03c8(y)\u20162HY \u2212 \u2016g \u2217(x) \u2212\u03c8(y)\u20162HYd\u03c1(x, y) (34)\n=\n\u222b\nX \u2016g(x)\u20162HY \u2212 2\u3008g(x),\n(\u222b\nY \u03c8(y)d\u03c1(y|x)\n) \u3009HY + \u2016g\u2217(x)\u20162HYd\u03c1X (x) (35)\n=\n\u222b\nX \u2016g(x)\u20162HY \u2212 2\u3008g(x), g \u2217(x)\u3009HY + \u2016g\u2217(x)\u20162HYd\u03c1X (x) (36)\n=\n\u222b\nX \u2016g(x) \u2212 g\u2217(x)\u20162HYd\u03c1X (x) (37)\nTherefore, for any measurable minimizer g \u2032 : X \u2192 HY of the surrogate expected risk at Eq. (13), we have R(g \u2032) \u2212R(g\u2217) = 0 which, by the relation above, implies g \u2032(x) = g\u2217(x) a.e. on D\u03c1|X .\nLemma 1. Let \u25b3 : Y \u00d7Y \u2192 R satisfy Asm. 1 with \u03c8 : Y \u2192 HY bounded. Then the expected risk in Eq. (1) can be written as\nE(f) = \u222b\nX \u3008\u03c8(f(x)), Vg\u2217(x)\u3009HY d\u03c1X (x) (12)\nfor all f : X \u2192 Y, where g\u2217 : X \u2192 HY minimizes\nR(g) = \u222b\nX\u00d7Y \u2016g(x) \u2212\u03c8(y)\u20162HY d\u03c1(x, y). (13)\nProof. By Lemma 8 we know that g\u2217(x) = \u222b Y \u03c8(y)d\u03c1(y|x) almost everywhere onD\u03c1|X and is the minimizer of R. Therefore we have\n\u3008\u03c8(y), Vg\u2217(x)\u3009HY = \u3008\u03c8(y), V \u222b\nY \u03c8(y \u2032)d\u03c1(y \u2032|x)\u3009HY (38)\n=\n\u222b\nY \u3008\u03c8(y), V\u03c8(y \u2032)\u3009HYd\u03c1(y \u2032|x) =\n\u222b\nY \u25b3(y, y \u2032)d\u03c1(y \u2032|x) (39)\nfor almost every x \u2208 D\u03c1|X . Thus, for any measurable function f : X \u2192 Y we have\nE(f) = \u222b\nX\u00d7Y \u25b3(f(x), y)d\u03c1(x, y) =\n\u222b\nX\n\u222b\nY \u25b3(f(x), y)d\u03c1(y|x)d\u03c1X (x) (40)\n=\n\u222b\nX\n\u3008\u03c8(f(x)), Vg\u2217(x)\u3009HYd\u03c1X (x). (41)\nTheorem 2. Let \u25b3 : Y \u00d7 Y \u2192 R satisfy Asm. 1 with Y a compact set. Then, for every measurable g : X \u2192 HY and d : HY \u2192 Y satisfying Eq. (14), the following holds\nE(d \u25e6 g\u2217) = E(f\u2217) (15) E(d \u25e6 g) \u2212 E(f\u2217) \u2264 2c\u25b3 \u221a R(g) \u2212R(g\u2217). (16)\nwith c\u25b3 = \u2016V\u2016maxy\u2208Y \u2016\u03c8(y)\u2016HY . Proof. For the sake of clarity, the result for the fisher consistency and the comparison inequality are proven respectively in Thm. 9, Thm. 12. The two results are proven below.\nTheorem 9 (Fisher Consistency). Let \u25b3 : Y \u00d7 Y \u2192 R satisfy Asm. 1 with Y a compact set. Let g\u2217 : X \u2192 HY be a minimizer of the surrogate problem at Eq. (13). Then, for any decoding d : HY \u2192 Y satisfying Eq. (14)\nE(d \u25e6 g\u2217) = inf f:X\u2192Y E(f) (42)\nProof. It is sufficient to show that d \u25e6 g\u2217 satisfies Eq. (27) almost everywhere on D\u03c1|X . Indeed, by directly applying Cor. 7 we have E(d \u25e6 g\u2217) = E(f\u2217) = inff E(f) as required.\nWe recall that a mapping d : HY \u2192 Y is a decoding for our surrogate framework if it satisfies Eq. (14), namely\nd(\u03b7) \u2208 argmin y\u2208Y \u3008\u03c8(y), V\u03b7\u3009HY \u2200\u03b7 \u2208 HY . (43)\nBy Lemma 8 we know that g\u2217(x) = \u222b Y \u03c8(y)d\u03c1(y|x) almost everywhere onD\u03c1|X . Therefore, we have\n\u3008\u03c8(y), Vg\u2217(x)\u3009HY = \u3008\u03c8(y), V \u222b\nY \u03c8(y \u2032)d\u03c1(y \u2032|x)\u3009HY (44)\n=\n\u222b\nY \u3008\u03c8(y), V\u03c8(y \u2032)\u3009HYd\u03c1(y \u2032|x) =\n\u222b\nY \u25b3(y, y \u2032)d\u03c1(y \u2032|x) (45)\nfor almost every x \u2208 D\u03c1|X . As a consequence, for any d : HY \u2192 Y satisfying Eq. (14), we have\nd \u25e6 g\u2217(x) \u2208 argmin y\u2208Y \u3008\u03c8(y), Vg\u2217(x)\u3009HY = argmin y\u2208Y\n\u222b\nY \u25b3(y, y \u2032)d\u03c1(y \u2032|x) (46)\nalmost everywhere on D\u03c1|X . We are therefore in the hypotheses of Cor. 7 with f\u0303 = d \u25e6 g\u2217, as desired.\nThe Fisher consistency of the surrogate problem allows to prove the comparison inequality (Thm. 12) between the excess risk of the structured prediction problem, namely E(d \u25e6 g) \u2212 E(f\u2217), and the excess risk R(g) \u2212 R(g\u2217) of the surrogate problem. However, before showing such relation, in the following result we prove that for any measurable g : X \u2192 HY and measurable decoding d : HY \u2192 Y, the expected risk E(d \u25e6 g) is well defined.\nLemma 10. Let Y be compact and \u25b3 : Y \u00d7 Y \u2192 R satisfying Asm. 1. Let g : X \u2192 HY be measurable and d : HY \u2192 Y a measurable decoding satisfying Eq. (14). Then E(d \u25e6 g) is well defined and moreover |E(d \u25e6 g)| \u2264 Q2\u2016V\u2016.\nProof. \u25b3(d \u25e6 g(x), y) is measurable in both x and y since \u25b3 is continuous and d \u25e6 g is measurable by hypothesis (combination of measurable functions). Now, \u25b3 is pointwise bounded by Q2\u2016V\u2016 since\n|\u25b3 (y, y \u2032)| = |\u3008\u03c8(y), V\u03c8(y \u2032)\u3009HY | \u2264 \u2016\u03c8(y)\u20162HY \u2016V\u2016 \u2264 Q 2\u2016V\u2016. (47)\nHence, by Theorem 11.23 pp. 416 in [3] the integral of \u25b3(d \u25e6 g(x), y) exists and therefore\n|E(d \u25e6 g)| \u2264 \u222b\nX |\u25b3 (d \u25e6 g(x), y)|d\u03c1(x, y) \u2264 Q2\u2016V\u2016 < +\u221e. (48)\nA question introduced by Lemma 10 is whether a measurable decoding always exists. The following result guarantees that, under the hypotheses introduced in this work, a decoding d : HY \u2192 Y satisfying Eq. (14) always exists.\nLemma 11. Let Y be compact and \u03c8 : Y \u2192 HY and V : HY \u2192 HY satisfy the requirements in Asm. 1. Define m : X \u2192 R as\nm(\u03b7) = min y\u2208Y \u3008\u03c8(y), V\u03b7\u3009HY \u2200y \u2208 Y, \u03b7 \u2208 HY . (49)\nThen, m is measurable and there exists a measurable decoding d : HY \u2192 Y satisfying Eq. (14), namely such that m(\u03b7) = \u3008\u03c8(d(\u03b7)), V\u03b7\u3009HY for each \u03b7 \u2208 HY .\nProof. Similarly to the proof of Lemma 6, the result is a direct application of Theorem 18.19 (pp. 605) of [3] (or Aumann\u2019s measurable selection principle [2,4]).\nWe now prove the comparison inequality at Eq. (16).\nTheorem 12 (Comparison Inequality). Let \u25b3 : Y \u00d7Y \u2192 R satisfy Asm. 1 with Y a compact or finite set. Let f\u2217 : X \u2192 Y and g\u2217 : X \u2192 HY be respectively solutions to the structured and surrogate learning problems at Eq. (1) and Eq. (13). Then, for every measurable g : X \u2192 HY and d : HY \u2192 Y satisfying Eq. (14)\nE(d \u25e6 g) \u2212 E(f\u2217) \u2264 2Q\u2016V\u2016 \u221a R(g) \u2212R(g\u2217). (50)\nProof. Let us denote f = d\u25e6g and f0 = d\u25e6g\u2217. By Thm. 9 we have that E(f0) = inff:X\u2192Y E(f) and so E(f\u2217) = E(f0). Now, by combining Asm. 1 with Lemma 8, we have\nE(f) \u2212 E(f\u2217) = E(f) \u2212 E(f0) = \u222b\nX\u00d7Y \u25b3(f(x), y) \u2212\u25b3(f0(x), y)d\u03c1(x, y) (51)\n=\n\u222b\nX\u00d7Y \u3008\u03c8(f(x)) \u2212\u03c8(f0(x)), V\u03c8(y)\u3009HYd\u03c1(x, y) (52)\n=\n\u222b\nX \u3008\u03c8(f(x)) \u2212\u03c8(f0(x)), V\n(\u222b\nY \u03c8(y)d\u03c1(y|x)\n) \u3009HYd\u03c1X (x) (53)\n=\n\u222b\nX \u3008\u03c8(f(x)) \u2212\u03c8(f0(x)), Vg\u2217(x)\u3009HYd\u03c1X (x) (54) = A+ B. (55)\nwhere\nA =\n\u222b\nX \u3008\u03c8(f(x)), V(g\u2217(x) \u2212 g(x))\u3009HY d\u03c1X (x) (56)\nB =\n\u222b\nX \u3008\u03c8(f(x)), Vg(x)\u3009HYd\u03c1X (x) \u2212\n\u222b\nX \u3008\u03c8(f0(x)), Vg\u2217(x)\u3009HYd\u03c1X (x) (57)\nNow, the term A can be minimized by taking the supremum over Y so that\nA \u2264 \u222b\nX sup y\u2208Y\n\u2223\u2223\u2223\u3008\u03c8(y), V(g\u2217(x) \u2212 g(x))\u3009HY \u2223\u2223\u2223d\u03c1X (x). (58)\nFor B, we observe that, by the definition of the decoding d, we have\n\u3008\u03c8(f0(x)), Vg\u2217(x)\u3009HY = inf y \u2032\u2208Y \u3008\u03c8(y \u2032), Vg\u2217(x)\u3009HY , (59)\n\u3008\u03c8(f(x)), Vg(x)\u3009HY = inf y \u2032\u2208Y \u3008\u03c8(y \u2032), Vg(x)\u3009HY , (60)\nfor all x \u2208 X l. Therefore,\nB =\n\u222b\nX inf y\u2208Y \u3008\u03c8(y), Vg(x)\u3009HY \u2212 inf y\u2208Y \u3008\u03c8(y), Vg\u2217(x)\u3009HY d\u03c1X (x) (61)\n\u2264 \u222b\nX sup y\u2208Y\n\u2223\u2223\u2223\u3008\u03c8(y), V(g(x) \u2212 g\u2217(x))\u3009HY \u2223\u2223\u2223d\u03c1X (x) (62)\nwhere we have used the fact that for any given two functions \u03b7, \u03b6 : Y \u2192 R we have\n| inf y\u2208Y \u03b7(y) \u2212 inf y\u2208Y \u03b6(y)| \u2264 sup y\u2208Y |\u03b7(y) \u2212 \u03b6(y)|. (63)\nTherefore, by combining the bounds on A and B we have\nE(f) \u2212 E(f\u2217) \u2264 2 \u222b\nX sup y\u2208Y\n\u2223\u2223\u2223\u3008\u03c8(y), V(g\u2217(x) \u2212 g(x))\u3009HY \u2223\u2223\u2223d\u03c1X (x)\n\u2264 2 \u222b\nX sup y\u2208Y\n\u2016V\u2217\u03c8(y)\u2016HY \u2016g\u2217(x) \u2212 g(x)\u2016HYd\u03c1X (x)\n\u2264 2Q\u2016V\u2016 \u222b\nX \u2016g\u2217(x) \u2212 g(x)\u2016HYd\u03c1X (x)\n\u2264 2Q\u2016V\u2016 \u221a\u222b\nX \u2016g\u2217(x) \u2212 g(x)\u20162HYd\u03c1X (x),\nwhere for the last inequality we have used the Jensen\u2019s inequality. The proof is concluded by recalling that (see Eq. (34))\nR(g) \u2212R(g\u2217) = \u222b\nX \u2016g(x) \u2212 g\u2217(x)\u20162HYd\u03c1X (x) (64)"}, {"heading": "B Learning Bounds for Structured Prediction", "text": "In this section we focus on the analysis of the structured prediction algorithm proposed in this work (Alg. 1). In particular, we will first prove that, given the minimizer g\u0302 : X \u2192 HY of the empirical risk at Eq. (5), its decoding can be computed in practice according to Alg. 1. Then, we report the proofs for the universal consistency of such approach (Thm. 4) and generalization bounds (Thm. 5)."}, {"heading": "Notation", "text": "Let k : X \u00d7X \u2192 R a positive semidefinite function on X , we denote HX the Hilbert space obtained by the completion\nHX = span{k(x, \u00b7) | x \u2208 X } (65)\naccording to the norm induced by the inner product \u3008k(x, \u00b7), k(x \u2032, \u00b7)\u3009HX = k(x, x \u2032). Spaces HX constructed in this way are known as reproducing kernel Hilbert spaces and there is a one-to-one relation between a kernel k and its associated RKHS. For more details on RKHS we refer the reader to [5]. Given a kernel k, in the following we will denote with \u03d5 : X \u2192 HX the feature map \u03d5(x) = k(x, \u00b7) \u2208 HX for all x \u2208 X . We say that a kernel is bounded if \u2016\u03d5(x)\u2016HX \u2264 \u03ba with \u03ba > 0. Note that k is bounded if and only if k(x, x \u2032) = \u3008\u03d5(x), \u03d5(x \u2032)\u3009HX \u2264 \u2016\u03d5(x)\u2016HX \u2016\u03d5(x \u2032)\u2016 \u2264 \u03ba2 for every x, x \u2032 \u2208 X . In the following we will always assume k to be continuous and bounded by \u03ba > 0. The continuity of k with the fact that X is Polish implies HX to be separable [5].\nWe introduce here the ideal and empirical operators that we will use in the following to prove the main results of this work.\n\u2022 S : HX \u2192 L2(X , \u03c1X ) s.t. f \u2208 HX 7\u2192 \u3008f,\u03d5(\u00b7)\u3009HX \u2208 L2(X , \u03c1X ), with adjoint \u2022 S\u2217 : L2(X , \u03c1X ) \u2192 HX s.t. h \u2208 L2(X , \u03c1X ) 7\u2192 \u222b X h(x)\u03d5(x)d\u03c1X (x) \u2208 HX ,\n\u2022 Z : HY \u2192 L2(X , \u03c1X ) s.t. h \u2208 HY 7\u2192 \u3008h, g\u2217(\u00b7)\u3009HY \u2208 L2(X , \u03c1X ), with adjoint \u2022 Z\u2217 : L2(X , \u03c1X ) \u2192 HY s.t. h \u2208 L2(X , \u03c1X ) 7\u2192 \u222b X h(x)g \u2217(x)d\u03c1X (x) \u2208 HY ,\n\u2022 C = S\u2217S : HX \u2192 HX and L = SS\u2217 : L2(X , \u03c1X ) \u2192 L2(X , \u03c1X ),\nwith g\u2217(x) = \u222b Y \u03c8(y)d\u03c1(y|x) defined according to Eq. (30), (see Lemma 8).\nGiven a set of input-output pairs {(xi, yi)}ni=1 with (xi, yi) \u2208 X \u00d7 Y independently sampled according to \u03c1 on X \u00d7 Y, we define the empirical counterparts of the operators just defined as\n\u2022 S\u0302 : HX \u2192 Rn s.t. f \u2208 HX 7\u2192 1\u221an(\u3008\u03d5(xi), f\u3009HX ) n i=1 \u2208 Rn, with adjoint \u2022 S\u0302\u2217 : Rn \u2192 HX s.t. v = (vi)ni=1 \u2208 Rn 7\u2192 1\u221an \u2211n i=1 vi\u03d5(xi),\n\u2022 Z\u0302 : HY \u2192 Rn s.t. h \u2208 HY 7\u2192 1\u221an(\u3008\u03c8(yi), h\u3009HY ) n i=1 \u2208 Rn, with adjoint \u2022 Z\u0302\u2217 : Rn \u2192 HY s.t. v = (vi)ni=1 \u2208 Rn 7\u2192 1\u221an \u2211n i=1 vi\u03c8(yi),\n\u2022 C\u0302 = S\u0302\u2217S\u0302 : HX \u2192 HX and K = nS\u0302S\u0302\u2217 \u2208 Rn\u00d7n is the empirical kernel matrix.\nIn the rest of this section we denote with A + \u03bb, the operator A + \u03bbI, for any symmetric linear operator A, \u03bb \u2208 R and I the identity operator.\nWe recall here a basic result characterizing the operators introduced above.\nProposition 13. With the notation introduced above,\nC =\n\u222b\nX \u03d5(x)\u2297\u03d5(x)d\u03c1X (x) and Z\u2217S =\n\u222b\nX\u00d7Y \u03c8(y)\u2297\u03d5(x)d\u03c1(x, y) (66)\nwhere \u2297 denotes the tensor product. Moreover, when \u03d5 and \u03c8 are bounded by respectively \u03ba and Q, we have the following facts\n(i) Tr(L) = Tr(C) = \u2016S\u20162HS = \u222b X \u2016\u03d5(x)\u20162HX d\u03c1X (x) \u2264 \u03ba 2\n(ii) \u2016Z\u20162HS = \u222b X \u2016g\u2217(x)\u20162d\u03c1X (x) = \u2016g\u2217\u20162\u03c1X < +\u221e.\nProof. By definition of C = S\u2217S, for each h, h \u2032 \u2208 HX we have\n\u3008h,Ch \u2032\u3009HX = \u3008Sh, Sh \u2032\u3009\u03c1X = \u222b\nX \u3008h,\u03d5(x)\u3009HX \u3008\u03d5(x), h \u2032\u3009HX d\u03c1X (x) (67)\n=\n\u222b\nX\n\u2329 h, ( \u03d5(x)\u3008\u03d5(x), h \u2032\u3009HX )\u232a HX d\u03c1X (x) (68)\n=\n\u222b\nX\n\u2329 h, ( \u03d5(x)\u2297\u03d5(x) ) h \u2032 \u232a d\u03c1X (x) (69)\n= \u3008h, ( \u222b\nX \u03d5(x)\u2297\u03d5(x)d\u03c1X (x)\n) h \u2032\u3009HX (70)\nsince \u03d5(x)\u2297\u03d5(x) : HX \u2192 HX is the operator such that h \u2208 HX 7\u2192 \u03d5(x)\u3008\u03d5(x), h\u3009HX . The characterization for Z\u2217S is analogous.\nNow, (i). The relation Tr(L) = Tr(C) = Tr(S\u2217S) = \u2016S\u20162HS holds by definition. Moreover\nTr(C) = \u222b\nX Tr(\u03d5(x)\u2297\u03d5(x))d\u03c1X (x) =\n\u222b\nX \u2016\u03d5(x)\u20162HX d\u03c1X (x) (71)\nby linearity of the trace. (ii) is analogous. Note that \u2016g\u2217\u20162\u03c1X < +\u221e. by Lemma 8 since \u03c8 is bounded by hypothesis."}, {"heading": "B.1 Reproducing Kernel Hilbert Spaces for Vector-valued Functions", "text": "We begin our analysis by introducing the concept of reproducing kernel Hilbert space (RKHS) for vector-valued functions. Here we provide a brief summary of the main properties that will be useful in the following. We refer the reader to [6,7] for a more in-depth introduction on the topic.\nAnalogously to the case of scalar functions, a RKHS for vector-valued functions g : X \u2192 H, with H a separable Hilbert space, is uniquely characterized by a so-called kernel of positive type, which is an operator-valued \u0393 : X \u00d7X \u2192 B(H,H) generalizing the concept of scalar reproducing kernel.\nDefinition 14. Let X be a set and H be a Hilbert space, then \u0393 : X \u00d7 X \u2192 B(H,H) is a kernel of positive type if for each n \u2208 N, x1, . . . , xn \u2208 X , c1, . . . , cn \u2208 H we have\nn\u2211\ni,j=1\n\u3008\u0393(xi, xj)ci, cj\u3009H \u2265 0 (72)\nA kernel of positive type \u0393 defines an inner product \u3008\u0393(x, \u00b7)c, \u0393(x \u2032, \u00b7)c \u2032\u3009G0 = \u3008\u0393(x, x \u2032)c, c \u2032\u3009H on the space G0 = span{\u0393(x, \u00b7)c | x \u2208 X , c \u2208 H}. (73) Then, the completion G = G0 with respect to the norm induced by \u3008\u00b7, \u00b7\u3009G0 is known as the reproducing Kernel Hilbert space (RKHS) for vector-valued functions associated to the kernel \u0393 . Indeed, we have that a reproducing property holds also for RKHS of vectorvalued functions, namely for any x \u2208 X , c \u2208 H and g \u2208 G we have\n\u3008g(x), c\u3009H = \u3008g, \u0393(x, \u00b7)c\u3009G (74)\nand that for each x \u2208 X the function \u0393(x, \u00b7) : G \u2192 H is the evaluation functional in x on G, namely \u0393(x, \u00b7)(g) = g(x)."}, {"heading": "B.1.1 Separable Vector Valued Kernels", "text": "In this work we restrict to the special case of RKHS for vector-valued functions with associated kernel \u0393 : X \u00d7 X \u2192 HY of the form \u0393(x, x \u2032) = k(x, x \u2032)IHY for each x, x \u2032 \u2208 X , where k : X \u00d7 X \u2192 R is a scalar reproducing kernel and IHY is the identity operator on HY . Notice that this choice is not restrictive in terms of the space of functions that can\nbe learned by our algorithm. Indeed, it was proven in [8] (see Example 14) that if k is a universal scalar kernel, then \u0393(\u00b7, \u00b7) = k(\u00b7, \u00b7)IHY is universal. Below, we report a useful characterization of RKHS G associated to a separable kernel.\nLemma 15. The RKHS G associated to the kernel \u0393(x, x \u2032) = k(x, x \u2032)IHY is isometric to HY \u2297HX and for each g \u2208 G there exists a unique G \u2208 HY \u2297HX such that\ng(x) = G\u03d5(x) \u2208 HY for each x \u2208 X (75)\nProof. We explicitly define the isometry T : G \u2192 HY \u2297HX as the linear operator such that T( \u2211n i=1 \u03b1i\u0393(xi, \u00b7)ci) = \u2211n\ni=1 \u03b1i ci \u2297 \u03d5(xi) for each n \u2208 N, x1, . . . , xn \u2208 X and c1, . . . , cn \u2208 HY . By construction, T(G0) \u2286 HY \u2297HX , with G0 the linear space defined at Eq. (73) and moreover\n\u3008T(\u0393(x, \u00b7)c), T(\u0393(x \u2032, \u00b7)c \u2032)\u3009HS = \u3008c\u2297\u03d5(x)\u2217, c \u2032 \u2297\u03d5(x \u2032)\u2217\u3009HS = \u3008c, c \u2032\u3009HY \u3008\u03d5(x), \u03d5(x \u2032)\u3009HX (76)\n= \u3008k(x, x \u2032)c, c \u2032\u3009HY = \u3008\u0393(x, x \u2032)c, c \u2032\u3009HY = \u3008\u0393(x, \u00b7)c, \u0393(x \u2032, \u00b7)c \u2032\u3009G (77)\nimplying that G0 is isometrically contained in HY \u2297HX . Since HY \u2297HX is complete, also T(G0) \u2286 HY \u2297HX . Therefore G is isometrically contained in HY \u2297HX , since T(G) = T(G0) = T(G0). Moreover note that\nT(G0) = span{c\u2297\u03d5(x) | x \u2208 X , c \u2208 HY } = span{c | c \u2208 HY }\u2297 span{\u03d5(x) | x \u2208 X } (78) = HY \u2297 span{\u03d5(x) | x \u2208 X } = HY \u2297 span{\u03d5(x) | x \u2208 X } = HY \u2297HX (79)\nfrom which we conclude that G is isometric to HY \u2297HX via T . To prove Eq. (75), let us consider x \u2208 X and g \u2208 G with G = T(g) \u2208 HY \u2297HX . Then, \u2200c \u2208 HY we have that\n\u3008c, g(x)\u3009HY = \u3008\u0393(x, \u00b7)c, g\u3009G = \u3008T(\u0393(x, \u00b7)c), G\u3009HS (80) = \u3008c\u2297\u03d5(x), G\u3009HS = Tr((\u03d5(x) \u2297 c)G) = Tr(c\u2217G\u03d5(x)) = \u3008c,G\u03d5(x)\u3009HY . (81)\nSince the equation above is true for each c \u2208 HY we can conclude that g(x) = T(g)\u03d5(x) as desired.\nThe isometry G \u2243 HY \u2297HX allows to characterize the closed form solution for the surrogate risk introduced in Eq. (13). We recall that in Lemma 8, we have shown that R always attains a minimizer on L2(X , \u03c1X ,HY). In the following we show that if R attains a minimum on G \u2243 HY \u2297HX , we are able to provide a close form solution for one such element.\nLemma 16. Let \u03c8 and HY satisfying Asm. 1 and assume that the surrogate expected risk minimization of R at Eq. (13) attains a minimum on G, with G \u2243 HY \u2297HX . Then the minimizer g\u2217 \u2208 G of R with minimal norm \u2016 \u00b7 \u2016G is of the form\ng\u2217(x) = G\u03d5(x), \u2200x \u2208 X with G = Z\u2217SC\u2020 \u2208 HY \u2297HX . (82)\nProof. By hypothesis we have g\u2217 \u2208 G. Therefore, by applying Lemma 15 we have that there exists a unique linear operator G : HX \u2192 HY such that g\u2217(x) = G\u03d5(x),\u2200x \u2208 X . Now, expanding the least squares loss on HY , we obtain\nR(g) = \u222b\nX\u00d7Y \u2016G\u03d5(x) \u2212\u03c8(y)\u20162HYd\u03c1(x, y) (83)\n= Tr(G(\u03d5(x)\u2297\u03d5(x))G\u2217)d\u03c1X (x, y) \u2212 2Tr(G(\u03d5(x)\u2297\u03c8(y))) + \u222b\nX\u00d7Y \u2016\u03c8(y)\u20162HY (84)\n= Tr(GCG\u2217) \u2212 2Tr(GS\u2217Z) + const. (85)\nwhere we have used Prop. 13 and the linearity of the trace. Therefore R is a quadratic functional, and is convex since C is positive semidefinite. We can conclude that R attains a minimum on G if and only if the range of S\u2217Z is contained in the range of C, namely Ran(S\u2217Z) \u2286 Ran(C) \u2282 HX (see [9] Chap. 2). In this case G = Z\u2217SC\u2020 \u2208 HY \u2297HX exists and is the minimum norm minimizer for R, as desired.\nAnalogously to Lemma 16, a closed form solution exists for the regularized surrogate empirical risk minimization problem introduced in Eq. (5). We recall that the associated functional is R\u0302\u03bb : G \u2192 R defined as\nR\u0302\u03bb(g) = R\u0302(g) + \u03bb \u2016g\u20162G = 1\nn\nn\u2211\ni=1\n\u2016g(xi) \u2212\u03c8(yi)\u20162HY + \u03bb \u2016g\u2016 2 HY (86)\nfor all g \u2208 G and {(xi, yi)}ni=1 points in X \u00d7Y. The following result characterizes the closed form solution for the empirical risk minimization when G \u2243 HY \u2297HX and guarantees that such a solution always exists.\nLemma 17. Let G \u2243 HY \u2297HX . For any \u03bb > 0, the solution g\u0302\u03bb \u2208 G of the empirical risk minimization problem at Eq. (5) exists, is unique and is such that\ng\u0302\u03bb(x) = G\u0302\u03bb\u03d5(x), \u2200x \u2208 X with G\u0302\u03bb = Z\u0302\u2217S\u0302(C\u0302 + \u03bb)\u22121 \u2208 HY \u2297HX . (87)\nProof. The proof of is analogous to that of Lemma 16 and we omit it. Note that, since (C\u0302 + \u03bb)\u22121 is always bounded for \u03bb > 0, its range corresponds to HX and therefore the range of S\u2217Z is always contained in it. Then G\u0302\u03bb exists for any \u03bb > 0 and is unique since \u2016 \u00b7 \u20162HY is strictly convex.\nThe closed form solutions provided by Lemma 16 and Lemma 17 will be key in the analysis of the structured prediction algorithm Alg. 1 in the following."}, {"heading": "B.2 The Structured Prediction Algorithm", "text": "In this section we prove that Alg. 1 corresponds to the decoding of the surrogate empirical risk minimizer g\u0302 (Eq. (5)) via a map d : HY \u2192 Y satisfying Eq. (14).\nRecall that in Lemma 15 we proved that the vector-valued RKHS G induced by a kernel \u0393(x, x \u2032) = k(x, x \u2032)IHY , for a scalar kernel k on X , is isometric to HY \u2297HX . For the sake of simplicity, in the following, with some abuse of notation, we will not make the distinction between G and HY \u2297HX when it is clear from context.\nLemma 3. Let \u25b3 : Y \u00d7 Y \u2192 R satisfy Asm. 1 with Y a compact set. Let g\u0302 : X \u2192 HY be the minimizer of Eq. (5). Then, for all x \u2208 X\nd \u25e6 g\u0302(x) = argmin y\u2208Y\nn\u2211\ni=1\n\u03b1i(x)\u25b3 (y, yi) \u03b1(x) = (K+ n\u03bbI)\u22121Kx \u2208 Rn (17)\nProof. From Lemma 17 we know that g\u0302(x) = Z\u0302\u2217S\u0302(C\u0302 + \u03bb)\u22121\u03d5(x) for all x \u2208 X . Recall that C\u0302 = S\u0302\u2217S\u0302 and K = nS\u0302S\u0302\u2217 \u2208 Rn\u00d7n, is the empirical kernel matrix associated to the inputs, namely such that Kij = k(xi, xj) for each i, j = 1, . . . , n. Therefore we have S\u0302(C\u0302 + \u03bb)\u22121 =\u221a n(K + \u03bbn)\u22121S\u0302 : HX \u2192 Rn. Now, by denoting Kx = \u221a nS\u0302\u03d5(x) = (k(xi, x)) n i=1 \u2208 Rn, we have\nS\u0302(C\u0302 + \u03bb)\u22121\u03d5(x) = (K + \u03bbnI)\u22121Kx = \u03b1(x) \u2208 Rn. (88)\nTherefore, by applying the definition of the operator Z\u0302 : HY \u2192 Rn, we have\ng\u0302(x) = Z\u0302\u2217S\u0302(C\u0302+ \u03bb)\u22121\u03d5(x) = Z\u0302\u2217\u03b1(x) = n\u2211\ni=1\n\u03b1i(x)\u03c8(yi), \u2200x \u2208 X (89)\nBy plugging g\u0302(x) in the functional minimized by the decoding (Eq. (14)),\n\u3008\u03c8(y), Vg\u0302(x)\u3009HY = \u3008\u03c8(y),VZ\u0302\u2217\u03b1(x)\u3009HY = \u3008\u03c8(y), n\u2211\ni=1\n\u03b1i(x)\u03c8(yi)\u3009HY (90)\n=\nn\u2211\ni=1\n\u03b1i(x)\u3008\u03c8(y), V\u03c8(yi)\u3009HY = n\u2211\ni=1\n\u03b1i(x)\u25b3 (y, yi), (91)\nwhere we have used the bilinear form for \u25b3 in Asm. 1 for the final equation. We conclude that\nd \u25e6 g\u0302(x) \u2208 argmin y\u2208Y \u3008\u03c8(y), Vg\u0302(x)\u3009HY = argmin y\u2208Y\nn\u2211\ni=1\n\u03b1i(x)\u25b3 (y, yi) (92)\nas required.\nLemma 3 focuses on the computational aspects of Alg. 1. In the following we will analyze its statistical properties."}, {"heading": "B.3 Universal Consistency", "text": "In following, we provide a probabilistic bound on the excess risk E(d \u25e6 g) \u2212 E(f\u2217) for any g \u2208 G \u2243 HY \u2297HX that will be key to prove both universal consistency (Thm. 4) and generalization bounds (Thm. 5). To do so, we will make use of the comparison inequality from Thm. 12 to control the structured excess risk by means of the excess risk of the surrogate R(g) \u2212 R(g\u2217). Note that the surrogate problem consists in a vector-valued kernel ridge regression estimation. In this setting, the problem of finding a probabilistic bound has been studied (see [10] and references therein). Indeed, our proof will consist\nof a decomposition of the surrogate excess risk that is similar to the one in [10]. However, note that we cannot direct apply [10] to our setting, since in [10] the operator-valued kernel \u0393 associated to G is required to be such that \u0393(x, x \u2032) is trace class \u2200x, x \u2032 \u2208 X , which does not hold for the kernel used in this work, namely \u0393(x, x \u2032) = k(x, x \u2032)IHY when HY is infinite dimensional.\nIn order to express the bound on the excess risk more compactly, here we introduce a measure for the approximation error of the surrogate problem. According to [10], we define the following quantity\nA(\u03bb) = \u03bb\u2016Z\u2217(L+ \u03bb)\u22121\u2016HS. (93)\nLemma 18. Let Y be compact, \u25b3 : Y \u00d7 Y \u2192 R satisfying Asm. 1 and k a bounded positive definite kernel on X with supx\u2208X k(x, x) \u2264 \u03ba2 and associated RKHS HX . Let \u03c1 a Borel probability measure on X \u00d7 Y and {(xi, yi)}ni=1 independently sampled according to \u03c1. Let f\u2217 be a solution of the problem in Eq. (1), f\u0302 = d\u25e6 g\u0302 as in Alg. 1. Then, for any \u03bb \u2264 \u03ba2 and \u03b4 > 0, the following holds with probability 1\u2212 \u03b4:\nE(f\u0302) \u2212 E(f\u2217) \u2264 8\u03baQ\u2016V\u2016Q + B(\u03bb)\u221a \u03bbn\n( 1+ \u221a 4\u03ba2\n\u03bb \u221a n\n) log2 8\n\u03b4 + 2Q\u2016V\u2016A(\u03bb), (94)\nwith B(\u03bb) = \u03ba\u2016Z\u2217S(C+ \u03bb)\u22121\u2016HS.\nProof. According to Thm. 12\nE(d \u25e6 g\u0302) \u2212 E(f\u2217) \u2264 2Q\u2016V\u2016 \u221a R(g\u0302) \u2212R(g\u2217). (95)\nFrom Lemma 17 we know that g\u0302(x) = G\u0302\u03d5(x) for all x \u2208 X , with G\u0302 = Z\u0302\u2217S\u0302(C\u0302 + \u03bb)\u22121 \u2208 HY \u2297HX . By Lemma 8, we know that g\u2217(x) = \u222b Y \u03c8(y)d\u03c1(y|x) almost everywhere on the support of \u03c1X . Therefore, a direct application of Prop. 13 leads to\nR(g\u0302) \u2212R(g\u2217) = \u222b\n\u2016g\u0302(x) \u2212 g\u2217(x)\u20162HYd\u03c1X (x) = (96)\n=\n\u222b\nX \u2016G\u0302\u03d5(x)\u20162HY \u2212 2\u3008G\u0302\u03d5(x), g \u2217(x)\u3009HY + \u2016g\u2217(x)\u20162HYd\u03c1X (x) (97)\n=\n\u222b\nX Tr\n( G\u0302 ( \u03d5(x)\u2297\u03d5(x) ) G\u0302\u2217 ) \u2212 2Tr ( G\u0302 ( \u03d5(x)\u2297 g\u2217(x) )) + Tr(g\u2217(x)\u2297 g\u2217(x))d\u03c1X (x)\n(98)\n= Tr(G\u0302S\u2217SG\u0302) \u2212 2Tr(G\u0302S\u2217Z) + Tr(Z\u2217Z) = \u2016G\u0302S\u2217 \u2212 Z\u2217\u20162HS (99)\nTo bound \u2016G\u0302S\u2217 \u2212 Z\u2217\u2016HS, we proceed with a decomposition similar to the one in [10]. In particular \u2016G\u0302S\u2217 \u2212 Z\u2217\u2016HS \u2264 A1 +A2 +A3, with\nA1 = \u2016Z\u0302\u2217S\u0302(C\u0302+ \u03bb)\u22121S\u2217 \u2212 Z\u2217S(C\u0302+ \u03bb)\u22121S\u2217\u2016HS (100) A2 = \u2016Z\u2217S(C\u0302+ \u03bb)\u22121S\u2217 \u2212 Z\u2217S(C+ \u03bb)\u22121S\u2217\u2016HS (101) A3 = \u2016Z\u2217S(C+ \u03bb)\u22121S\u2217 \u2212 Z\u2217\u2016HS. (102)\nLet \u03c4 = \u03b4/4. Now, for the term A1, we have\nA1 \u2264 \u2016Z\u0302\u2217S\u0302\u2212 Z\u2217S\u2016HS\u2016(C\u0302 + \u03bb)\u22121S\u2217\u2016. (103)\nTo control the term \u2016Z\u0302\u2217S\u0302\u2212Z\u2217S\u2016HS, note that Z\u0302\u2217S\u0302 = 1n \u2211n\ni=1 \u03b6i with \u03b6i the random variable \u03b6i = \u03c8(yi)\u2297\u03d5(xi). By Prop. 13, for any 1 \u2264 i \u2264 n we have\nE\u03b6i =\n\u222b\n\u03c8(y)\u2297\u03d5(x)d\u03c1(x, y) = Z\u2217S, (104)\nand\n\u2016\u03b6i\u2016HS \u2264 sup y\u2208Y \u2016\u03c8(y)\u2016HY sup x\u2208X \u2016\u03d5(x)\u2016HX \u2264 Q\u03ba, (105)\nalmost surely on the support of \u03c1 on X \u00d7 Y, and so E\u2016\u03b6i\u20162HS \u2264 Q2\u03ba2. Thus, by applying Lemma 2 of [11], we have\n\u2016Z\u0302\u2217S\u0302\u2212 Z\u2217S\u2016HS \u2264 2Q\u03ba log 2 \u03c4\nn +\n\u221a 2Q2\u03ba2 log 2\n\u03c4 n \u2264 4Q\u03ba log 2 \u03c4 n (106)\nwith probability 1\u2212 \u03c4, since log 2/\u03c4 \u2265 1. To control \u2016(C\u0302+ \u03bb)\u22121S\u2217\u2016 we proceed by recalling that C = S\u2217S and that for any \u03bb > 0 \u2016(C\u0302 + \u03bb)\u22121\u2016 \u2264 \u03bb\u22121 and \u2016C\u0302(C\u0302+ \u03bb)\u22121\u2016 \u2264 1. We have\n\u2016(C\u0302 + \u03bb)\u22121S\u2217\u2016 = \u2016(C\u0302 + \u03bb)\u22121C(C\u0302+ \u03bb)\u22121\u20161/2 (107) \u2264 \u2016(C\u0302 + \u03bb)\u22121(C\u2212 C\u0302)(C\u0302 + \u03bb)\u22121\u20161/2 + \u2016(C\u0302 + \u03bb)\u22121C\u0302(C\u0302 + \u03bb)\u22121\u20161/2 (108) \u2264 \u2016(C\u0302 + \u03bb)\u22121\u2016\u2016C \u2212 C\u0302\u20161/2 + \u2016(C\u0302 + \u03bb)\u22121\u20161/2\u2016C\u0302(C\u0302+ \u03bb)\u22121\u20161/2 (109) \u2264 \u03bb\u22121/2(1+ \u03bb\u22121/2\u2016C \u2212 C\u0302\u20161/2). (110)\nTo control \u2016C \u2212 C\u0302\u2016, note that C\u0302 = 1n \u2211n\ni=1 \u03b6i where \u03b6i is the random variable defined as \u03b6i = \u03d5(xi) \u2297 \u03d5(xi) for 1 \u2264 i \u2264 n. Note that E\u03b6i = C, \u2016\u03b6i\u2016 \u2264 \u03ba2 almost surely and so E\u2016\u03b6i\u20162 \u2264 \u03ba4 for 1 \u2264 i \u2264 n. Thus we can again apply Lemma 2 of [11], obtaining\n\u2016C\u2212 C\u0302\u2016 \u2264 \u2016C \u2212 C\u0302\u2016HS \u2264 2\u03ba2 log 2 \u03c4\nn +\n\u221a 2\u03ba4 log 2\n\u03c4 n \u2264 4\u03ba 2 log 2 \u03c4\u221a n , (111)\nwith probability 1\u2212 \u03c4. Thus, by performing an intersection bound, we have\nA1 \u2264 4Q\u03ba log 2\n\u03c4\u221a \u03bbn\n 1+ \u221a 4\u03ba2 log 2 \u03c4\n\u03bb \u221a n\n  . (112)\nwith probability 1\u2212 2\u03c4. The term A2 can be controlled as follows\nA2 = \u2016Z\u2217S(C\u0302+ \u03bb)\u22121S\u2217 \u2212 Z\u2217S(C+ \u03bb)\u22121S\u2217\u2016HS (113) = \u2016Z\u2217S((C\u0302+ \u03bb)\u22121 \u2212 (C + \u03bb)\u22121)S\u2217\u2016HS (114) = \u2016Z\u2217S(C+ \u03bb)\u22121(C\u2212 C\u0302)(C\u0302 + \u03bb)\u22121S\u2217\u2016HS (115) \u2264 \u2016Z\u2217S(C+ \u03bb)\u22121\u2016HS\u2016C\u2212 C\u0302\u2016\u2016(C\u0302 + \u03bb)\u22121S\u2217\u2016 (116) = k\u22121B(\u03bb)\u2016C \u2212 C\u0302\u2016\u2016(C\u0302 + \u03bb)\u22121S\u2217\u2016 (117)\nwhere we have used the fact that for two invertible operatorsA and Bwe haveA\u22121\u2212B\u22121 = A\u22121(B\u2212A)B\u22121. Now, by controlling \u2016C\u2212 C\u0302\u2016, \u2016(C\u0302+ \u03bb)\u22121S\u2217\u2016 as for A1 and performing an intersection bound, we have\nA2 \u2264 4B(\u03bb)\u03ba log 2\u03c4\u221a\n\u03bbn\n 1+ \u221a 4\u03ba2 log 2\u03c4 \u03bb \u221a n   (118)\nwith probability 1\u2212 2\u03c4. Finally the term A3 is equal to\nA3 = \u2016Z\u2217(S(C+ \u03bb)\u22121S\u2217 \u2212 I)\u2016HS = \u2016Z\u2217(L(L + \u03bb)\u22121 \u2212 I)\u2016HS (119) = \u2016Z\u2217(L(L+ \u03bb)\u22121 \u2212 (L + \u03bb)(L + \u03bb)\u22121)\u2016HS = \u03bb\u2016Z\u2217(L+ \u03bb)\u22121\u2016HS = A(\u03bb) (120)\nwhere I denotes the identity operator. Thus, by performing an intersection bound of the events for A1 and A2, we have\nE(f\u0302) \u2212 E(f\u2217) \u2264 8\u03baQ\u2016V\u2016Q + B(\u03bb)\u221a \u03bbn\n( 1+ \u221a 4\u03ba2\n\u03bb \u221a n\n) log2 2\n\u03c4 + 2Q\u2016V\u2016A(\u03bb). (121)\nwith probability 1\u2212 4\u03c4. Since \u03b4 = 4\u03c4 we obtain the desired bound.\nNow we are ready to give the universal consistency result.\nTheorem 4 (Universal Consistency). Let\u25b3 : Y\u00d7Y \u2192 R satisfy Asm. 1, X and Y be compact sets and k : X \u00d7 X \u2192 R a continuous universal reproducing kernel6. For any n \u2208 N and any distribution \u03c1 on X \u00d7 Y let f\u0302n : X \u2192 Y be obtained by Alg. 1 with {(xi, yi)}ni=1 training points independently sampled from \u03c1 and \u03bbn = n \u22121/4. Then,\nlim n\u2192+\u221e\nE(f\u0302n) = E(f\u2217) with probability 1 (18)\nProof. By applying Lemma 18, we have\nE(f\u0302) \u2212 E(f\u2217) \u2264 8\u03baQ\u2016V\u2016Q + B(\u03bb)\u221a \u03bbn\n( 1+ \u221a 4\u03ba2\n\u03bb \u221a n\n) log2 8\n\u03b4 + 2Q\u2016V\u2016A(\u03bb), (122)\nwith probability 1\u2212 \u03b4. Note that, since C = S\u2217S, \u2016(C+ \u03bb)\u22121\u2016 \u2264 \u03bb\u22121 and \u2016C(C+ \u03bb)\u22121\u2016 \u2264 1, we have\n\u03ba\u22121B(\u03bb) = \u2016Z\u2217S(C+ \u03bb)\u22121\u2016HS \u2264 \u2016Z\u2016HS\u2016S(C+ \u03bb)\u22121\u2016 (123) \u2264 \u2016Z\u2016HS\u2016(C + \u03bb)\u22121S\u2217S(C+ \u03bb)\u22121\u20161/2 (124) \u2264 \u2016Z\u2016HS\u2016(C + \u03bb)\u22121\u20161/2\u2016C(C + \u03bb)\u22121\u20161/2 (125) \u2264 \u2016Z\u2016HS\u03bb\u22121/2. (126)\n6This is a standard assumption for universal consistency (see [2]). An example of continuous universal kernel is the Gaussian k(x, x \u2032) = exp(\u2212\u2016x\u2212 x \u2032\u20162/\u03c3).\nTherefore\nE(f\u0302) \u2212 E(f\u2217) \u2264 8\u03baQ\u2016V\u2016 Q + \u03ba\u221a \u03bb \u2016Z\u2016HS\n\u221a \u03bbn\n( 1+ \u221a 4\u03ba2\n\u03bb \u221a n\n) log2 8\n\u03b4 + 2Q\u2016V\u2016A(\u03bb), (127)\nNow by choosing \u03bb = \u03ba2n\u22121/4, we have\nE(f\u0302n) \u2212 E(f\u2217) \u2264 24Q\u2016V\u2016(Q + \u2016Z\u2016HS)n\u22121/4 log2 8\n\u03b4 + 2Q\u2016V\u2016A(\u03bb), (128)\nwith probability 1\u2212 \u03b4. Now we study A(\u03bb), let L = \u2211i\u2208N \u03c3i ui \u2297 ui be the eigendecomposition of the compact operator L, with \u03c3i \u2265 \u03c3j > 0 for 1 \u2264 i \u2264 j \u2208 N and ui \u2208 L2(X , \u03c1X ). Now, let w2i = \u3008ui, ZZ\u2217ui\u3009L2(X ,\u03c1X ,R) = \u222b \u3008g\u2217(x), ui\u30092HYd\u03c1X (x) for i \u2208 N. We need to prove that (ui)i\u2208N is a basis for L2(X , \u03c1X ). Let W \u2286 X be the support of \u03c1X , note that W is compact and Polish since it is closed and subset of the compact Polish space X . Let L be the RKHS defined by L = span{k(x, \u00b7) | x \u2208 W}, with the same inner product of HX . By the fact that W is a compact polish space and k is continuous, then L is separable. By the universality of k we have that L is dense in C(W), and, by Corollary 5 of [12], we have C(W) = span{ui | i \u2208 N}. Thus, since C(W) is dense in L2(X , \u03c1X ), we have (ui)i\u2208N is a basis of L2(X , \u03c1X ). Thus \u2211 i\u2208N w 2 i = \u222b \u2016g\u2217(x)\u20162HYd\u03c1X (x) = \u2016Z\u2016 2 HS < \u221e. Therefore\nA(\u03bbn)2 = \u03bb2n\u2016Z\u2217(L+ \u03bbn)\u22121\u20162HS = \u2211\ni\u2208N\n\u03bb2nw 2 i\n(\u03c3i + \u03bb)2 . (129)\nLet tn = n\u22121/8, and Tn = {i \u2208 N | \u03c3i \u2265 tn} \u2282 N. For any n \u2208 N we have\nA(\u03bbn) = \u2211\ni\u2208Tn\n\u03bb2nw 2 i\n(\u03c3i + \u03bbn)2 +\n\u2211\ni\u2208N\\Tn\n\u03bb2nw 2 i\n(\u03c3i + \u03bbn)2 (130)\n\u2264 \u03bb 2 n\nt2n\n\u2211 i\u2208Tn w2i + \u2211 i\u2208N\\Tn w2i \u2264 \u03ba4\u2016Z\u20162HS n\u22121/4 + \u2211 i\u2208N\\Tn w2i (131)\nsince \u03bbn/tn = \u03ba2n\u22121/4/n\u22121/8 = \u03ba2n\u22121/8. We recall that L is a trace class operator, namely Tr(L) = \u2211+\u221e i=1 \u03c3i < +\u221e. Therefore \u03c3i \u2192 0 for i \u2192 +\u221e, from which we conclude\n0 \u2264 lim n\u2192\u221e A(\u03bbn) \u2264 lim n\u2192\u221e\n\u03ba4\u2016Z\u20162HSn\u22121/4 + \u2211\ni\u2208N\\Tn w2i = 0. (132)\nNow, for any n \u2208 N, let \u03b4n = n\u22122 and En be the event associated to the equation E(f\u0302n) \u2212 E(f\u2217) > 24Q\u2016V\u2016(Q + \u2016Z\u2016HS)n\u22121/4 log2(8n2) + 2Q\u2016V\u2016A(\u03bb). (133) By Lemma 18, we know that the probability of En is at most \u03b4n. Since \u2211+\u221e\nn=1 \u03b4n < +\u221e, we can apply the Borel-Cantelli lemma (Theorem 8.3.4. pag 263 of [1]) on the sequence (En)n\u2208N and conclude that the statement\nlim n\u2192\u221e\nE(f\u0302n) \u2212 E(f\u2217) > 0, (134)\nholds with probability 0. Thus, the converse statement\nlim n\u2192\u221e\nE(f\u0302n) \u2212 E(f\u2217) = 0. (135)\nholds with probability 1."}, {"heading": "B.4 Generalization Bounds", "text": "Finally, we show that under the further hypothesis that g\u2217 belongs to the RKHS G \u2243 HY \u2297HX , we are able to prove generalization bounds for the structured prediction algorithm.\nTheorem 5 (Generalization Bound). Let \u25b3 : Y \u00d7 Y \u2192 R satisfy Asm. 1, Y be a compact set and k : X \u00d7 X \u2192 R a bounded continuous reproducing kernel. Let f\u0302n denote the solution of Alg. 1 with n training points and \u03bb = n\u22121/2. If the surrogate risk R defined in Eq. (13) admits a minimizer g\u2217 \u2208 G, then\nE(f\u0302n) \u2212 E(f\u2217) \u2264 c\u03c42 n\u2212 1 4 (19)\nholds with probability 1\u2212 8e\u2212\u03c4 for any \u03c4 > 0, with c a constant not depending on n and \u03c4.\nProof. By applying 18, we have\nE(f\u0302) \u2212 E(f\u2217) \u2264 8\u03baQ\u2016V\u2016Q + B(\u03bb)\u221a \u03bbn\n( 1+ \u221a 4\u03ba2\n\u03bb \u221a n\n) log2 8\n\u03b4 + 2Q\u2016V\u2016A(\u03bb), (136)\nwith probability 1\u2212 \u03b4. By assumption, g\u2217 \u2208 G and therefore there exists a G \u2208 HY \u2297HX such that\ng\u2217(x) = G\u03d5(x), \u2200x \u2208 X . (137) This implies that Z\u2217 = GS\u2217 since, by definition of Z and S, for any h \u2208 L2(X , \u03c1X ),\nZ\u2217h = \u222b\nX g\u2217(x)h(x)d\u03c1X (x) =\n\u222b\nX G\u03d5(x)h(x)d\u03c1X (x) = GS\n\u2217h. (138)\nThus, since L = SS\u2217 and \u2016(L + \u03bb)\u22121L\u2016 \u2264 1 and \u2016(L + \u03bb)\u22121\u2016 \u2264 \u03bb\u22121 for any \u03bb > 0, we have A(\u03bb) = \u03bb\u2016Z\u2217(L + \u03bb)\u22121\u2016HS = \u03bb\u2016GS\u2217(L+ \u03bb)\u22121\u2016HS (139)\n\u2264 \u03bb\u2016G\u2016HS\u2016S\u2217(L+ \u03bb)\u22121\u2016 = \u03bb\u2016G\u2016HS\u2016(L + \u03bb)\u22121SS\u2217(L+ \u03bb)\u22121\u20161/2 (140) \u2264 \u03bb\u2016G\u2016HS\u2016(L + \u03bb)\u22121L\u20161/2\u2016(L + \u03bb)\u22121\u20161/2 (141) \u2264 \u03bb1/2\u2016G\u2016HS. (142)\nMoreover, since C = S\u2217S, we have\n\u03ba\u22121B(\u03bb) = \u2016Z\u2217S(C+ \u03bb)\u22121\u2016HS = \u2016GS\u2217S(C+ \u03bb)\u22121\u2016HS (143) \u2264 \u2016G\u2016HS\u2016C(C + \u03bb)\u22121\u2016 (144) \u2264 \u2016G\u2016HS. (145)\nNow, let \u03bb = \u03ba2n\u22121/4, we have\nE(f\u0302) \u2212 E(f\u2217) \u2264 24Q\u2016V\u2016(Q + \u03ba\u2016G\u2016HS)n\u22121/4 log2 8\n\u03b4 + 2Q\u2016V\u2016\u03ban\u22121/4 (146)\n\u2264 24Q\u2016V\u2016(Q + \u03ba\u2016G\u2016HS + \u03ba)n\u22121/4 log2 8\n\u03b4 (147)\n= c\u03c42n\u22121/4 (148)\nwith probability 1\u2212 8e\u2212\u03c4, where we have set \u03b4 = 8e\u2212\u03c4 and c = 24Q\u2016V\u2016(Q + \u03ba\u2016G\u2016HS + \u03ba) to obtain the desired inequality."}, {"heading": "C Examples of Loss Functions", "text": "In this section we prove Thm. 19 to show that a wide range of functions \u25b3 : Y \u00d7 Y \u2192 R useful for structured prediction learning satisfies the loss trick (Asm. 1). In the following we state Thm. 19, then we use it to prove that all the losses considered in Example 1 satisfy Asm. 1. Finally we give two lemmas, necessary to prove Thm. 19 and then conclude with its proof.\nTheorem 19. Let Y be a set. A function \u25b3 : Y \u00d7 Y \u2192 R satisfy Asm. 1 when at least one of the following conditions hold:\n1. Y is a finite set, with discrete topology.\n2. Y = [0, 1]d with d \u2208 N, and the mixed partial derivative L(y, y \u2032) = \u2202 2d\u25b3(y1,...,yd,y \u20321,...,y \u2032d) \u2202y1,...,\u2202yd,\u2202y \u2032 1 ,...,\u2202y \u2032 d\nexists almost everywhere, where y = (yi) d i=1, y \u2032 = (y \u2032i) d i=1 \u2208 Y, and satisfies\n\u222b\nY\u00d7Y |L(y, y \u2032)|1+\u01ebdydy \u2032 < \u221e, with \u01eb > 0. (149)\n3. Y is compact and \u25b3 is a continuous kernel, or \u25b3 is a function in the RKHS induced by a kernel K. Here K is a continuous kernel on Y \u00d7 Y, of the form\nK((y1, y2), (y \u2032 1, y \u2032 2)) = K0(y1, y \u2032 1)K0(y2, y \u2032 2), \u2200yi, y \u2032i \u2208 Y, i = 1, 2,\nwith K0 a bounded and continuous kernel on Y.\n4. Y is compact and Y \u2286 Y0, \u25b3 = \u25b30|Y ,\nthat is the restriction of \u25b30 : Y0 \u00d7 Y0 \u2192 R on Y, and \u25b30 satisfies Asm. 1 on Y0,\n5. Y is compact and \u25b3(y, y \u2032) = f(y)\u25b30 (F(y), G(y \u2032))g(y \u2032),\nwith F,G continuous maps from Y to a set Z with \u25b30 : Z \u00d7 Z \u2192 R satisfying Asm. 1 and f, g : Y \u2192 R, bounded and continuous.\n6. Y compact and \u25b3 = f(\u25b31, . . . ,\u25b3p),\nwhere f : [\u2212M,M]d \u2192 R is an analytic function (e.g. a polynomial), p \u2208 N and \u25b31, . . . ,\u25b3p satisfy Asm. 1 on Y. Here M \u2265 sup1\u2264i\u2264p \u2016Vi\u2016Ci where Vi is the operator associated to the loss \u25b3i and Ci is the value that bounds the norm of the feature map \u03c8i associated to \u25b3i, with i \u2208 {1, . . . , p}.\nBelow we expand Example 1 by proving that the considered losses satisfy Asm. 1. The proofs are typically a direct application of Thm. 19 above.\n1. Any loss with, Y finite. This is a direct application of Thm. 19, point 1.\n2. Regression and classification loss functions. Here Y is an interval on R and the listed loss functions satisfies Thm. 19, point 2. For example, let Y = [\u2212\u03c0, \u03c0] the mixed partial derivative of the Hinge loss \u25b3(y, y \u2032) = max(0, 1 \u2212 yy \u2032) is defined almost everywhere as L(y, y \u2032) = \u22121 when yy \u2032 < 1 and L(y, y \u2032) = 0 otherwise. Note that L satisfies Eq. (149), for any \u03bb > 0.\n3. Robust loss functions. Here, again Y is an interval on R. The listed loss functions are: Cauchy \u03b3 log(1 + |y \u2212 y \u2032|2/\u03b3), German-McLure |y \u2212 y \u2032|2/(1 + |y \u2212 y \u2032|2) \u201cFair\u201d \u03b3|y \u2212 y \u2032| \u2212 \u03b32 log(1 + |y \u2212 y \u2032|/\u03b3) or the \u201cL2 \u2212 L1\u201d \u221a 1+ |y\u2212 y \u2032|2 \u2212 1. They are\ndifferentiable on R, hence satisfy Thm. 19, point 2. The Absolute value |y \u2212 y \u2032| is Lipschitz and satisfies Thm. 19, point 2, as well.\n4. KDE. When Y is a compact set and\u25b3 is a kernel, the point 3 of Thm. 19 is applicable.\n5. Diffusion Distances on Manifolds. Let M \u2208 N and Y \u2282 RM be a compact Reimannian manifold. The heat kernel (at time t > 0), kt : Y \u00d7 Y \u2192 R induced by the LaplaceBeltrami operator of Y is a reproducing kernel [13]. The squared diffusion distance is defined in terms of kt as follows \u25b3(y, y \u2032) = 1 \u2212 kt(y, y \u2032). Then, point 3 of Thm. 19 is applicable.\n6. Distances on Histograms/Probabilities. LetM \u2208 N. A discrete probability distribution (or a normalized histogram) over M entries can be represented as a y = (yi)Mi=1 \u2208 Y \u2282 [0, 1]M the M-simplex, namely \u2211Mi=1 yi = 1 and yi \u2265 0 \u2200i = 1, . . . ,M. A typical measure of distance on Y is the squared Hellinger (or Bhattacharya) \u25b3H(y, y \u2032) = (1/ \u221a 2)\u2016\u221ay \u2212 \u221ay \u2032\u201622, with \u221a y = ( \u221a yi) M i=1. By Thm. 19, points 4, 6 we have that\n\u25b3H satisfies Asm. 1. Indeed, consider the kernel k on R, k(r, r \u2032) = ( \u221a rr \u2032 + 1)2 with\nfeature map \u03d5(r) = (r, \u221a 2r, 1)\u22a4 \u2208 R3, Then\n\u25b30(r, r \u2032) = ( \u221a r\u2212 \u221a r \u2032)2 = r\u22122 \u221a rr \u2032+r \u2032 = \u03d5(r)\u22a4V\u03d5(r \u2032) with V =\n  0 0 1\n0 \u22121 0\n1 0 0\n  .\n\u25b3H is obtained by M summations of \u25b30 on [0, 1], by Thm. 19, point 6 (indeed \u25b3H(y, y \u2032) = f(\u25b30(y1, y \u20321), . . . ,\u25b30(yM, y \u2032M)) with y = (yi)Mi=1, y \u2032 = (y \u2032i)Mi=1 \u2208 Y and the function f : RM \u2192 R defined as f(t1, . . . , tM) = \u2211M i=1 ti, which is analytic onR\nM), and then restriction on Y Thm. 19, point 4. A similar reasoning holds when the loss function is the \u03c72 distance on histograms. Indeed the function (r \u2212 r \u2032)2/(r + r \u2032) satisfies point 2 on Y = [0, 1], then point 6 and 4 are applied.\nTo prove Thm. 19 we need the following two Lemmas.\nLemma 20 (multiple Fourier series). Let D = [\u2212\u03c0, \u03c0]d, (f\u0302h)h\u2208Zd \u2208 C and f : D \u2192 C with d \u2208 N defined as\nf(y) = \u2211\nh\u2208Zd f\u0302he\nih\u22a4y, \u2200y \u2208 D, with \u2211\nh\u2208Zd |f\u0302h| \u2264 B,\nfor a B < \u221e and i = \u221a \u22121. Then the function f is continuous and\nsup y\u2208D\n|f(y)| \u2264 B.\nProof. For the continuity, see [14] (pag. 129 and Example 2). For the boundedness we have\nsup y\u2208D |f(y)| \u2264 sup y\u2208D\n\u2211\nh\u2208Zd |f\u0302h||e\nih\u22a4y| \u2264 \u2211\nh\u2208Zd |f\u0302h| \u2264 B.\nLemma 21. Let Y = [\u2212\u03c0, \u03c0]d with d \u2208 N, and \u25b3 : Y \u00d7 Y \u2192 R defined by\n\u25b3(y, z) = \u2211\nh,k\u2208Zd \u25b3\u0302h,keh(y)ek(z), \u2200y, z \u2208 Y,\nwith eh(y) = e ih\u22a4y for any y \u2208 Y, i = \u221a \u22121 and \u25b3\u0302h,k \u2208 C for any h, k \u2208 Zd. The loss \u25b3 satisfies Asm. 1 when \u2211\nh,k\u2208Zd |\u25b3\u0302h,k| < \u221e.\nProof. Note that by applying Lemma 20 with D = Y \u00d7 Y, the function \u25b3 is bounded continuous. Now we introduce the following sequences\n\u03b1h = \u2211\nk\u2208Zd |\u25b3\u0302h,k|, fh(z) =\n1\n\u03b1h\n\u2211\nh\u2208Zd \u25b3\u0302hkek(z) \u2200h \u2208 Zd, z \u2208 Y.\nNote that (\u03b1h)h\u2208Zd \u2208 \u21131 and that fh bounded by 1 and is continuous by Lemma 20 for any h \u2208 Zd. Therefore\n\u25b3(y, z) = \u2211\nh,k\u2208Zd \u25b3\u0302h,keh(y)eh(z) =\n\u2211\nh\u2208Zd \u03b1heh(y)fh(z).\nNow we define two feature maps \u03c81, \u03c82 : Y \u2192 H0, with H0 = \u21132(Zd), as\n\u03c81(y) = ( \u221a \u03b1heh(y))h\u2208Zd , \u03c82(y) = ( \u221a \u03b1hfh(y))h\u2208Zd , \u2200y \u2208 Y.\nNowwe prove that the two feature maps are continuous. Define k1(y, z) = \u3008\u03c81(y), \u03c81(z)\u3009H0 and k2(y, z) = \u3008\u03c82(y), \u03c82(z)\u3009H0 for all y, z \u2208 Y. We have\nk1(y, z) = \u2211\nh\u2208Zd \u03b1heh(y)eh(z), (150)\nk2(y, z) = \u2211\nh\u2208Zd \u03b1hfh(y)fh(z) =\n\u2211\nk,l\u2208Zd \u03b2k,lek(y)el(z) (151)\nwith \u03b2k,l = \u2211 h\u2208Zd \u25b3\u0302h,k\u25b3\u0302h,l \u03b1h , for k, l \u2208 Zd, therefore k1, k2 are bounded and continuous by\nLemma 20 with D = Y \u00d7 Y, since \u2211h\u2208Zd \u03b1h < \u221e and \u2211\nk,l\u2208Zd |\u03b2k,l| < \u221e. Note that \u03c81 and \u03c82 are bounded, since k1 and k2 are. Moreover for any y, z \u2208 Y, we have\n\u2016\u03c81(y) \u2212\u03c81(z)\u20162 = \u3008\u03c81(z), \u03c81(z)\u3009H0 + \u3008\u03c81(y), \u03c81(y)\u3009H0 \u2212 2\u3008\u03c81(z), \u03c81(y)\u3009H0 (152) = k1(z, z) + k1(y, y) \u2212 2k1(z, y) \u2264 |k1(z, z) \u2212 k1(z, y)| + |k1(z, y) \u2212 k1(y, y)|,\n(153)\nand the same holds for \u03c82 with respect to k2. Thus the continuity of \u03c81 is entailed by the continuity of k1 and the same for \u03c82 with respect to k2. Now we define HY = H0 \u2295 H0 and \u03c8 : Y \u2192 HY and V : HY \u2192 HY as\n\u03c8(y) = (\u03c81(y), \u03c82(y)), \u2200y \u2208 Y and V = ( 0 I\n0 0\n) ,\nwhere I : H0 \u2192 H0 is the identity operator. Note that \u03c8 is bounded continuous, V is bounded and\n\u3008\u03c8(y), V\u03c8(z)\u3009HY = \u3008\u03c81(y), \u03c82(z)\u3009H0 = \u2211\nh\u2208Zd \u03b1heh(y)fh(z) = \u25b3(y, z).\nWe can now prove Thm. 19.\nProof. (Thm. 19)\n1 Let N = {1, . . . , |Y |} and q : Y \u2192 N be a one-to-one function. Let HY = R|Y |, \u03c8(y) : Y \u2192 HY defined by \u03c8(y) = eq(y) for any y \u2208 Y with (ei)|Y |i=1 the canonical basis for HY , finally V \u2208 R|Y |\u00d7|Y | with Vi,j = \u25b3(q\u22121(i), q\u22121(j)) for any i, j \u2208 N. Then \u25b3 satisfies Asm. 1, with \u03c8 and V .\n2 By Lemma 21, we know that any loss \u25b3 whose Fourier expansion is absolutely summable, satisfies Asm. 1. The required conditions in points 2 are sufficient ( see Theorem 6\u2019 pag. 291 of [15]).\n3 Let Y be a compact space. For the first case let \u25b3 be a bounded and continuous reproducing kernel on Y and letHY the associated RKHS, then there exist a bounded and continuous map \u03c8 : Y \u2192 HY such that \u25b3(y, y \u2032) = \u3008\u03c8(y), \u03c8(y \u2032)\u3009 for any y, y \u2032 \u2208 Y, which satisfies Asm. 1 with V = I the identity on HY . For the second case, let K defined in terms of K0 as in equation. Let H0 be the RKHS induced by K0 and \u03c8 the associated feature map, then, by definition, the RKHS induced by K will be H = H0 \u2297 H0. Since \u25b3 belongs to H, then there exists a v \u2208 H such that \u25b3(y, y \u2032) = \u3008v,\u03c8(y) \u2297 \u03c8(y \u2032)\u3009H0\u2297H0 . Now note that H = H0 \u2297 H0 is isomorphic to B2(H0,H0), that is the linear space of Hilbert-Schmidt operators fromH0 toH0, thus, there exist an operator V \u2208 B2(H0,H0) such that\n\u25b3(y, y \u2032) = \u3008v,\u03c8(y)\u2297\u03c8(y \u2032)\u3009H0\u2297H0 = \u3008\u03c8(y), V\u03c8(y \u2032)\u3009H0 , \u2200y, y \u2032 \u2208 Y.\nFinally note that \u03c8 is continuous and bounded, since it is K0, and V is bounded since it is Hilbert-Schmidt. Thus \u25b3 satisfies Asm. 1.\n4 Since\u25b30 satisfies Asm. 1 we have that there exists a kernel on Y0 such that Equation 11 holds. Note that the restriction of a kernel on a subset of its domain is again a kernel. Thus, let \u03c8 = \u03c80|Y , we have that \u25b3|Y satisfies Equation 11 with \u03c8 and the same bounded operator V as \u25b3.\n5 Let\u25b30 : Z\u00d7Z \u2192 R satisfy Asm. 1, then\u25b30(z, z \u2032) = \u3008\u03c80(z), V0\u03c80(z \u2032)\u3009 for all z, z \u2032 \u2208 Z with \u03c80 : Y \u2192 H0 bounded and continuous and H0 a separable Hilbert space. Now we define two feature maps \u03c81(y) = f(y)\u03c80(F(y)) and \u03c82(y) = g(y)\u03c80(G(y)). Note that both \u03c81, \u03c82 : Y \u2192 H0 are bounded and continuous. We define HY = H0 \u2295H0, \u03c8 : Y \u2192 HY as \u03c8(y) = (\u03c81(y), \u03c82(y \u2032)) for any y \u2208 Y, and V = ( 0 V0 0 0 ) . Note that\nnow \u3008\u03c8(y), V\u03c8(y \u2032)\u3009HY = (\u03c81(y) \u03c82(y)) ( 0 V0 0 0 )( \u03c81(y)\n\u03c82(y)\n) = \u3008\u03c81(y), V0\u03c82(y \u2032)\u3009HY\n(154)\n= f(y)\u3008\u03c8(F(y)), V0\u03c8(G(y \u2032))\u3009HYg(y \u2032) = f(y)\u25b30 (F(y), G(y \u2032))g(y \u2032). (155)\n6 Let Y be compact and \u25b3i satisfies Asm. 1 with Hi the associated RKHS, with continuous feature maps \u03c8i : Y \u2192 Hi bounded by Ci and with a bounded operator Vi, for i \u2208 {1, . . . , p}. Since an analytic function is the limit of a series of polynomials, first of all we prove that a finite polynomial in the losses satisfies Asm. 1, then we take the limit. First of all, note that \u03b1\u25b31 +\u03b2\u25b32, satisfies Asm. 1, for any \u03b11, \u03b12 \u2208 R. Indeed we defineHY = H1\u2295H2, and \u03c8(y) = ( \u221a |\u03b1|\u2016V1\u2016\u03c81(y), \u221a |\u03b2|\u2016V2\u2016\u03c82(y)) for\nany y \u2208 Y, so that \u03b11\u25b31(y, y \u2032)+\u03b12\u25b32(y, y \u2032) = \u3008\u03c8(y), V\u03c8(y \u2032)\u3009HY , with V = ( sign(\u03b11) \u2016V1\u2016 V1 0\n0 sign(\u03b12) \u2016V2\u2016 V2\n) ,\nfor any y, y \u2032 \u2208 Y, where \u03c8 is continuous, supy\u2208Y \u2016\u03c8\u2016HY \u2264 |\u03b11|\u2016V1\u2016C1 + |\u03b12|\u2016V2\u2016C2 and \u2016V\u2016 \u2264 1. In a similar way we have that\u25b31\u25b32 satisfies Asm. 1, indeed, we define HY = H1 \u2297H2 and \u03c8 to be \u03c8(y) = \u03c81(y)\u2297\u03c82(y) for any y \u2208 Y, thus\n\u25b31(y, y \u2032)\u25b32 (y, y \u2032) = \u3008\u03c8(y), V\u03c8(y)\u3009HY , with V = V1 \u2297 V2,\nfor any y, y \u2032 \u2208 Y, where \u03c8 is continuous, supy\u2208Y \u2016\u03c8\u2016HY \u2264 C1C2 and \u2016V\u2016 \u2264 \u2016V1\u2016\u2016V2\u2016. Given a polynomial P(\u25b3), with \u25b3 = (\u25b31, . . . ,\u25b3p) we write it as\nP(\u25b3) = \u2211\nt\u2208Np \u03b1t\u25b3t, with \u25b3t =\np\u220f\ni=1\n\u25b3tii , \u2200t \u2208 Np.\nwhere the \u03b1\u2019s are the coefficents of the polynomial and such that only a finite number of them are non-zero. By applying the construction of the product on each monomial and of the sum on the resulting monomials, we have that P(\u25b3) is a loss satisfying Asm. 1 for a continuous \u03c8 and a V such that\nsup y\u2208Y \u2016\u03c8\u2016HY \u2264 P\u0304(\u2016V1\u2016C1, . . . , \u2016V1\u2016C1)\nand \u2016V\u2016 \u2264 1, where P\u0304(\u25b3) = \u2211t\u2208Np |\u03b1t|\u25b3t and HY = \u2295t\u2208Np \u2297 p i=1 H ti i . Note that HY is again separable. Let now consider\nf(\u25b3) = \u2211\nt\u2208Np \u03b1t\u25b3t, f\u0304(\u25b3) =\n\u2211 t\u2208Np |\u03b1t|\u25b3t .\nAssume that f\u0304(\u2016V1\u2016C1, . . . , \u2016Vp\u2016Cp) < \u221e. Then by repeating the construction for the polynomials, we produce a bounded \u03c8 and a bounded V such that\nf(\u25b31(y, y \u2032), . . . ,\u25b3p(y, y \u2032)) = \u3008\u03c8(y), V\u03c8(y \u2032)\u3009HY , \u2200 y, y \u2032 \u2208 Y,\nin particularHY is the same for the polynomial case and\u03c8 = \u2295t\u2208Np \u221a |\u03b1t|vt\u2297pi=1\u03c8 \u2297ti i , with vt = \u220fp\ni=1 \u2016Vi\u2016ti , for any t \u2208 Np. Now we prove that \u03c8 is continuous on Y. Let \u03c8q be the feature map defined for the polynomial P\u0304q(\u25b3) = \u2211 t\u2208Np, \u2016t\u2016\u2264q |\u03b1t|\u25b3t. We have that\nsup y\u2208Y \u2016\u03c8(y) \u2212\u03c8q(y)\u20162HY = sup y\u2208Y\n\u2225\u2225\u2225\u2295t\u2208Np,\u2016t\u2016>q \u221a |\u03b1t|vt \u2297pi=1 \u03c8 \u2297ti i \u2225\u2225\u2225 2\nHY (156)\n\u2264 \u2211\nt\u2208Nd,\u2016t\u2016>q |\u03b1t|vt sup y\u2208Y \u2016 \u2297pi=1 \u03c8 \u2297ti i \u20162HY \u2264\n\u2211\nt\u2208Nd,\u2016t\u2016>q |\u03b1t|vtct,\n(157)\nwith ct = \u220fp i=1 C ti i for any t \u2208 Np. No note that\n\u2211 t\u2208Nd |\u03b1t|vtct = f\u0304(\u2016V1\u2016C1, . . . , \u2016Vp\u2016Cp) < \u221e,\nthus limq\u2192\u221e \u2211 t\u2208Nd,\u2016t\u2016>q |\u03b1t|vtct = 0. Therefore\nlim q\u2192\u221e sup y\u2208Y \u2016\u03c8(y) \u2212\u03c8q(y)\u20162HY \u2264 limq\u2192\u221e \u2211\nt\u2208Np, \u2016t\u2016>q |\u03b1t|vtct = 0. (158)\nNow, since \u03c8q is a sequence of continuous bounded functions, and the sequence converges uniformly to \u03c8, then \u03c8 is continuous bounded. So f(\u25b3) is a loss function satisfying Asm. 1, with a continuous \u03c8 and an operator V such that\nsup y\u2208Y \u2016\u03c8\u2016HY \u2264 f\u0304(\u2016V1\u2016C1, . . . , \u2016V1\u2016C1),\nand \u2016V\u2016 \u2264 1."}], "references": [{"title": "Real analysis and probability, volume 74", "author": ["Richard M Dudley"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Support Vector Machines. Information Science and Statistics", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Infinite dimensional analysis: a hitchhiker\u2019s", "author": ["Charalambos D Aliprantis", "Kim Border"], "venue": "guide. Springer Science & Business Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Convex analysis and measurable multifunctions, volume 580", "author": ["Charles Castaing", "Michel Valadier"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["Alain Berlinet", "Christine Thomas-Agnan"], "venue": "Springer Science & Business Media,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Kernels for multi\u2013task learning", "author": ["Charles A Micchelli", "Massimiliano Pontil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Vector valued reproducing kernel hilbert spaces of integrable functions and mercer theorem", "author": ["Claudio Carmeli", "Ernesto De Vito", "Alessandro Toigo"], "venue": "Analysis and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Umanit\u00e1. Vector valued reproducing kernel hilbert spaces and universality", "author": ["Claudio Carmeli", "Ernesto De Vito", "Alessandro Toigo", "Veronica"], "venue": "Analysis and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Regularization of inverse problems, volume 375", "author": ["Heinz Werner Engl", "Martin Hanke", "Andreas Neubauer"], "venue": "Springer Science & Business Media,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Optimal rates for the regularized leastsquares algorithm", "author": ["Andrea Caponnetto", "Ernesto De Vito"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Learning theory estimates via integral operators and their approximations", "author": ["Steve Smale", "Ding-Xuan Zhou"], "venue": "Constructive approximation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Fourier series and wavelets", "author": ["J-P Kahane"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "On the absolute convergence of multiple fourier series", "author": ["Ferenc M\u00f3ricz", "Antal Veres"], "venue": "Acta Mathematica Hungarica,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "Indeed, this has motivated the extension of methods such as support vector machines to structured problems [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].", "startOffset": 190, "endOffset": 193}, {"referenceID": 8, "context": "This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9\u201311].", "startOffset": 130, "endOffset": 136}, {"referenceID": 9, "context": "This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9\u201311].", "startOffset": 130, "endOffset": 136}, {"referenceID": 10, "context": "This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9\u201311].", "startOffset": 130, "endOffset": 136}, {"referenceID": 3, "context": "This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].", "startOffset": 130, "endOffset": 139}, {"referenceID": 11, "context": "This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].", "startOffset": 130, "endOffset": 139}, {"referenceID": 12, "context": "(3) This choice of\u25b3 was originally considered in Kernel Dependency Estimation (KDE) [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "1 essentially reduces to [12, 14] and recalling their derivation is insightful.", "startOffset": 25, "endOffset": 33}, {"referenceID": 8, "context": "G is the reproducing kernel Hilbert space for vector-valued functions [9] with inner product \u3008k(xi, \u00b7)ci, k(xj, \u00b7)cj\u3009G = k(xi, xj)\u3008ci, cj\u3009HY", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "1 we consider ideas from surrogate approaches [7,17,18] and in particular [5].", "startOffset": 46, "endOffset": 55}, {"referenceID": 4, "context": "1 we consider ideas from surrogate approaches [7,17,18] and in particular [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 11, "context": "Therefore, the approach in [12, 14] to recover f\u0302(x) = argminy L(g(x), y) can be interpreted as the result f\u0302(x) = d\u25e6 \u011d(x) of a suitable decoding of \u011d(x).", "startOffset": 27, "endOffset": 35}, {"referenceID": 8, "context": "(13) corresponds to a vector-valued regression problem [9\u201311].", "startOffset": 55, "endOffset": 61}, {"referenceID": 9, "context": "(13) corresponds to a vector-valued regression problem [9\u201311].", "startOffset": 55, "endOffset": 61}, {"referenceID": 10, "context": "(13) corresponds to a vector-valued regression problem [9\u201311].", "startOffset": 55, "endOffset": 61}, {"referenceID": 9, "context": "5, our analysis on \u011d borrows ideas from [10] and extends their result to our setting for the case of HY infinite dimensional (i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Indeed, note that in this case [10] cannot be applied to the estimator \u011d considered in this work (see Appendix Sec.", "startOffset": 31, "endOffset": 35}, {"referenceID": 4, "context": "Promising results in this direction can be found in [5], where the Tsybakov condition was generalized to the multi-class setting and led to a tight comparison inequality analogous to the one for the binary setting.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "Indeed, it is not clear how the approach in [5] could be further generalized to the case where Y has infinite cardinality.", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "Rank Loss Linear [7] 0.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "012 SVM Struct [4] 0.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "Loss KDE [15] (Gaussian) Alg.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "Table 2: Digit reconstruction on USPS with KDE method [15] (with Gaussian loss) and Alg.", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "2 we discussed the relation between KDE [12, 15] and Alg.", "startOffset": 40, "endOffset": 48}, {"referenceID": 11, "context": "4 and 5 to prove universal consistency and generalization bounds for methods such as [12,14].", "startOffset": 85, "endOffset": 92}, {"referenceID": 11, "context": "We are not aware of previous results proving consistency (and generalization bounds) for the KDE methods in [12, 14].", "startOffset": 108, "endOffset": 116}, {"referenceID": 3, "context": "A popular approach to structured prediction is the Support Vector Machine for Structured Outputs (SVMstruct) [4] that extends ideas from the well-known SVM algorithm to the structured setting.", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "Moreover, we note that generalization studies for SVMstruct are available [3] (Ch.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "1 to the ranking problem using the rank loss [7]", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "In the definition of \u25b3rank, \u03b3(y)ij denotes the costs (or reward) of having movie j ranked higher than movie i and, similarly to [7], we set \u03b3(y)ij equal to the difference of ratings provided by user associated to y (from 1 to 5).", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "1, a linear kernel on features similar to those proposed in [7], which were computed based on users\u2019 profession, age, similarity of previous ratings, etc.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "1 for \u25b3rank is NPhard (see [7]) we adopted the Feedback Arc Set approximation (FAS) proposed in [27] to approximate the f\u0302(x) of Alg.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "1 (Ours) with surrogate ranking methods using a Linear [7], Hinge [24] or Logistic [25] loss and Struct SVM [4]3.", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "1 (Ours) with surrogate ranking methods using a Linear [7], Hinge [24] or Logistic [25] loss and Struct SVM [4]3.", "startOffset": 108, "endOffset": 111}, {"referenceID": 12, "context": "We considered the USPS4 digits reconstruction experiment originally proposed in [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "The standard approach is to use a Gaussian kernel kG on images in input and adopt KDE methods such as [12,14,15] with loss \u25b3G(y, y \u2032) = 1\u2212 kG(y, y \u2032).", "startOffset": 102, "endOffset": 112}, {"referenceID": 12, "context": "The standard approach is to use a Gaussian kernel kG on images in input and adopt KDE methods such as [12,14,15] with loss \u25b3G(y, y \u2032) = 1\u2212 kG(y, y \u2032).", "startOffset": 102, "endOffset": 112}, {"referenceID": 0, "context": "[1] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Andrej Karpathy and Li Fei-Fei.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Thomas Hofmann Bernhard Sch\u00f6lkopf Alexander J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, and Jean-Jacques Slotine.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Wei Gao and Zhi-Hua Zhou.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] John C Duchi, Lester W Mackey, and Michael I Jordan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Ingo Steinwart, Andreas Christmann, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Charles A Micchelli and Massimiliano Pontil.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Andrea Caponnetto and Ernesto De Vito.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Jason Weston, Olivier Chapelle, Vladimir Vapnik, Andr\u00e9 Elisseeff, and Bernhard Sch\u00f6lkopf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "We recall [1] that \u03c1(y|x) is a regular conditional distribution and its domain, which we will denoteD\u03c1|X in the following, is a measurable set contained in the support of \u03c1X and corresponds to the support of \u03c1X up to a set of measure zero.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "In the next Lemma, following [2] we show that the problem in Eq.", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "153) of [3]), namely continuous in y for each x \u2208 X and measurable in x for each y \u2208 Y.", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f\u2217 : X \u2192 Y such that r(x, f\u2217(x)) = m(x) for all x \u2208 X .", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f\u2217 : X \u2192 Y such that r(x, f\u2217(x)) = m(x) for all x \u2208 X .", "startOffset": 56, "endOffset": 62}, {"referenceID": 3, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f\u2217 : X \u2192 Y such that r(x, f\u2217(x)) = m(x) for all x \u2208 X .", "startOffset": 56, "endOffset": 62}, {"referenceID": 1, "context": "Therefore, since \u03c1(y|x) is a regular conditional probability, we have that g\u2217 is measurable on X (see for instance [2]).", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "416 in [3] the integral of \u25b3(d \u25e6 g(x), y) exists and therefore |E(d \u25e6 g)| \u2264 \u222b", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2,4]).", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2,4]).", "startOffset": 56, "endOffset": 61}, {"referenceID": 3, "context": "605) of [3] (or Aumann\u2019s measurable selection principle [2,4]).", "startOffset": 56, "endOffset": 61}, {"referenceID": 4, "context": "For more details on RKHS we refer the reader to [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "The continuity of k with the fact that X is Polish implies HX to be separable [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "We refer the reader to [6,7] for a more in-depth introduction on the topic.", "startOffset": 23, "endOffset": 28}, {"referenceID": 6, "context": "We refer the reader to [6,7] for a more in-depth introduction on the topic.", "startOffset": 23, "endOffset": 28}, {"referenceID": 7, "context": "Indeed, it was proven in [8] (see Example 14) that if k is a universal scalar kernel, then \u0393(\u00b7, \u00b7) = k(\u00b7, \u00b7)IHY is universal.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "We can conclude that R attains a minimum on G if and only if the range of S\u2217Z is contained in the range of C, namely Ran(S\u2217Z) \u2286 Ran(C) \u2282 HX (see [9] Chap.", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "In this setting, the problem of finding a probabilistic bound has been studied (see [10] and references therein).", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "of a decomposition of the surrogate excess risk that is similar to the one in [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "However, note that we cannot direct apply [10] to our setting, since in [10] the operator-valued kernel \u0393 associated to G is required to be such that \u0393(x, x \u2032) is trace class \u2200x, x \u2032 \u2208 X , which does not hold for the kernel used in this work, namely \u0393(x, x \u2032) = k(x, x )IHY when HY is infinite dimensional.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "However, note that we cannot direct apply [10] to our setting, since in [10] the operator-valued kernel \u0393 associated to G is required to be such that \u0393(x, x \u2032) is trace class \u2200x, x \u2032 \u2208 X , which does not hold for the kernel used in this work, namely \u0393(x, x \u2032) = k(x, x )IHY when HY is infinite dimensional.", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "According to [10], we define the following quantity A(\u03bb) = \u03bb\u2016Z\u2217(L+ \u03bb)\u2016HS.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "X Tr ( \u011c ( \u03c6(x)\u2297\u03c6(x) ) \u011c\u2217 ) \u2212 2Tr ( \u011c ( \u03c6(x)\u2297 g\u2217(x) )) + Tr(g\u2217(x)\u2297 g(x))d\u03c1X (x) (98) = Tr(\u011cS\u2217S\u011c) \u2212 2Tr(\u011cS\u2217Z) + Tr(Z\u2217Z) = \u2016\u011cS\u2217 \u2212 Z\u2016HS (99) To bound \u2016\u011cS\u2217 \u2212 Z\u2217\u2016HS, we proceed with a decomposition similar to the one in [10].", "startOffset": 215, "endOffset": 219}, {"referenceID": 10, "context": "Thus, by applying Lemma 2 of [11], we have", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": "Thus we can again apply Lemma 2 of [11], obtaining", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "(126) This is a standard assumption for universal consistency (see [2]).", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "pag 263 of [1]) on the sequence (En)n\u2208N and conclude that the statement lim n\u2192\u221e E(f\u0302n) \u2212 E(f\u2217) > 0, (134) holds with probability 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Y = [0, 1] with d \u2208 N, and the mixed partial derivative L(y, y \u2032) = \u2202 \u25b3(y1,.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "A discrete probability distribution (or a normalized histogram) over M entries can be represented as a y = (yi)i=1 \u2208 Y \u2282 [0, 1] the M-simplex, namely \u2211Mi=1 yi = 1 and yi \u2265 0 \u2200i = 1, .", "startOffset": 121, "endOffset": 127}, {"referenceID": 0, "context": "\u25b3H is obtained by M summations of \u25b30 on [0, 1], by Thm.", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "Indeed the function (r \u2212 r \u2032)2/(r + r \u2032) satisfies point 2 on Y = [0, 1], then point 6 and 4 are applied.", "startOffset": 66, "endOffset": 72}, {"referenceID": 11, "context": "For the continuity, see [14] (pag.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "291 of [15]).", "startOffset": 7, "endOffset": 11}], "year": 2017, "abstractText": "We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed methods. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}