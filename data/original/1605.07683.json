{"id": "1605.07683", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Learning End-to-End Goal-Oriented Dialog", "abstract": "End-to-end dialog systems, in which all components are learnt simultaneously, have recently obtained encouraging successes. However these were mostly on conversations related to chit-chat with no clear objective and for which evaluation is difficult. This paper proposes a set of tasks to test the capabilities of such systems on goal-oriented dialogs, where goal completion ensures a well-defined measure of performance. Built in the context of restaurant reservation, our tasks require to manipulate sentences and symbols, in order to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a).", "histories": [["v1", "Tue, 24 May 2016 23:09:58 GMT  (2723kb,D)", "http://arxiv.org/abs/1605.07683v1", null], ["v2", "Thu, 9 Jun 2016 20:47:49 GMT  (1259kb,D)", "http://arxiv.org/abs/1605.07683v2", null], ["v3", "Wed, 25 Jan 2017 00:11:20 GMT  (1265kb,D)", "http://arxiv.org/abs/1605.07683v3", null], ["v4", "Thu, 30 Mar 2017 23:02:22 GMT  (1265kb,D)", "http://arxiv.org/abs/1605.07683v4", "Accepted as a conference paper at ICLR 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["antoine bordes", "y-lan boureau", "jason weston"], "accepted": true, "id": "1605.07683"}, "pdf": {"name": "1605.07683.pdf", "metadata": {"source": "CRF", "title": "Learning End-to-End Goal-Oriented Dialog", "authors": ["Antoine Bordes", "Jason Weston"], "emails": ["abordes@fb.com", "jase@fb.com"], "sections": [{"heading": "1 Introduction", "text": "Dialog systems, conversational agents or bots, even personal digital assistants, all refer to systems being able to communicate in natural language with humans and perform tasks on their behalf. They are becoming crucial tools in a broad range of applications, from recommendation or news reporting to online shopping, and are likely to play a major role in the future.\nTraditional dialog systems (Young et al., 2013) are composed of several modules including a language interpreter, a dialog state tracker and a response generator, usually each is separately hand-crafted or trained. Such systems are specialized for a single domain and rely heavily on expert knowledge; for instance, most of these systems, called slot-filling methods (Lemon et al., 2006; Wang and Lemon, 2013), require to predefine the structure of the dialog state as a form composed of a set of slots to be filled during the dialog. For a restaurant reservation system, examples of slots can be the location, the price range or the type of cuisine of a restaurant. Even if they have proven to be useful in some narrow domains, they are inherently limited by their design: it is impossible to manually encode all features and slots that users might implicitly refer to in a conversation.\nEnd-to-end dialog systems, usually based on neural networks (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Dodge et al., 2016), currently generate a lot of interest because they do not have such limitations: all their components are directly trained on past dialogs and they make no asumption on the domain or on the structure of the dialog state. Those methods have reached promising performance in non goal-oriented chit-chat settings, where they were trained to predict the next utterrance in social media and forums threads (Ritter et al., 2011; Wang et al., 2013; Lowe et al., 2015) or movie conversations (Banchs, 2012). The choice of non goal-oriented dialog to test those systems is natural since it is close to tasks like language modeling or machine translation, where neural networks have been applied successfully. However, evaluation of models is difficult (Liu et al., 2016) making their resulting utility in end applications unclear.\nCurrently, the most useful applications of dialog systems are goal-oriented and transactional: the system is expected to understand a user request and complete a related task with a clear goal within\nar X\niv :1\n60 5.\n07 68\n3v 1\n[ cs\n.C L\n] 2\n4 M\nay 2\n01 6\na limited number of dialog turns. In this case, evaluation is clear: the task has been successfully completed or not. As illustrated in Figure 1 for a restaurant reservation scenario, conducting goaloriented dialog requires skills that go beyond language modeling such as asking questions to clearly define a user request, querying Knowledge Bases (KBs), interpreting results from queries to display options to users or completing a transaction. It is not obvious that the promising performance achieved by end-to-end models for chit-chat translates into such goal-oriented conversations, and allows them to replace traditional systems.\nThis paper proposes to test the capacity of end-to-end dialog systems for conducting goal-oriented dialog. To this end, in the spirit of the bAbI tasks designed as question answering test-beds (Weston et al., 2015b), we designed a set of tasks in the goal-oriented application of restaurant reservation. Grounded with an underlying KB defining restaurants and their properties (location, type of cuisine, etc.), these five tasks cover several dialog stages and test if models can learn various abilities such as performing dialog management, querying KBs, interpreting the output of such queries to continue the conversation or dealing with new entities not appearing in dialogs from the training set. Solving these requires to manipulate natural language but also symbols from a KB. Evaluation is performed using two metrics, per-response and per-dialog accuracies, the latter validating whether models can complete the actual goal. Figure 1 depicts the tasks and Section 3 details them.\nWe evaluate multiple methods on these tasks, described in Section 4. As an end-to-end neural model, we chose to apply Memory Networks (Weston et al., 2015a), an attention-based architecture, which has proved to be competitive for non goal-oriented dialog (Dodge et al., 2016). Our experiments of Section 5 show that Memory Networks can be trained to perform non-trivial operations such as\nissuing API calls to KBs and manipulating entities unseen in training. We confirm our findings on real human-machine dialogs from the restaurant reservation dataset of the 2nd Dialog State Tracking Challenge (Henderson et al., 2014a), which we converted into our task format, showing that Memory Networks can outperform a dedicated slot-filling rule-based baseline. Overall, even if the per-response performance is encouraging, the per-dialog one remains low and indicates that end-to-end models still require improvements before being able to reliably perform goal-oriented dialog."}, {"heading": "2 Related Work", "text": "The most successful goal-oriented dialog systems model conversation as partially observable Markov decision processes (POMDP) (Young et al., 2013). However, in spite of recent efforts to learn modules (Henderson et al., 2014b), they still require many hand-crafted features for the state and action space representations, which restrict their usage to narrow domains. Our simulation, which we use to generate goal-oriented datasets can be seen as an equivalent of the user simulators used to train POMDP (Young et al., 2013; Pietquin and Hastie, 2013), but for training end-to-end systems.\nSerban et al. (2015b) list all available corpora for training dialog systems, and unfortunately, no reliable resources exist to train and test end-to-end models in goal-oriented scenarios. Indeed, goaloriented datasets are usually designed to train or evaluate dialog state tracker components (Henderson et al., 2014a) and are hence of limited scale and not suitable for end-to-end learning (annotated at the state level and noisy). However, we do convert the Dialog State Tracking Challenge data for our goal.\nThe closest resource to ours might be the set of tasks described in (Dodge et al., 2016), since some of them can be seen as goal-oriented. However, those are question answering tasks rather than dialog, i.e. the bot only responds with answers, never questions, which is not really close to full conversation."}, {"heading": "3 Goal-Oriented Dialog Tasks", "text": "All of our tasks involve a restaurant reservation system, where the goal of users in each conversation is to book a table at a restaurant. The first five tasks are generated using a simulation, and the last involves real human-bot dialogs. The data for all tasks is available from http://fb.ai/babi."}, {"heading": "3.1 Restaurant Reservation Simulation", "text": "The simulation is based on an underlying KB, composed of facts which define all the restaurants that can be booked as well as their properties. Each restaurant is defined by a type of cuisine (10 choices such as French, Thai, etc.), a location (10 choices such as London, Tokyo, etc.), a price range (cheap, moderate or expensive) and a rating (from 1 to 8). Additionally, for simplicity, we assume that each restaurant only has availability for a single party size (2, 4, 6 or 8 people). Each restaurant also has an address and a phone number listed in the KB.\nThe KB can be queried using API calls, which return the list of facts related to the corresponding restaurants. Each query must contain four fields: a location, a type of cuisine, a price range and a party size. It can return facts concerning one, several or no restaurant (depending on the party size).\nUsing the KB, we can generate conversations in the format of the example of Figure 1. Each example is a dialog between a user and a bot, composed of utterances from both as well as API calls and of\nfacts resulting from them. Dialogs are always generated after creating a user request by sampling an entry for each of the four required fields: the request in the example of Figure 1 is (cuisine: British, location: London, party size: six, price range: expensive). We use natural language patterns to create user and bot utterances. There are 43 patterns for the user and 20 for the bot (the user can use up to 4 ways to say something, whereas the bot always uses the same). Those patterns are combined with the KB entities to form thousands of different utterances."}, {"heading": "3.1.1 Task Definitions", "text": "We now detail each task. Tasks 1 and 2 test dialog management to see if end-to-end systems can learn to implicitly track dialog state (never given explicitly), whereas Task 3 and 4 check if they can learn to use KB facts in a dialog setting. Task 3 also requires to learn to sort. Task 5 combines all tasks.\nTask 1: Issuing API calls Given a user request, a user query is generated, that can contain from 0 to 4 of the required fields (sampled uniformly; in Figure 1, it contains 3). The bot must ask questions for filling the missing fields and eventually generate the correct corresponding API call. The order with which the bot asks for information is deterministic, so that it can be predicted.\nTask 2: Updating API calls Starting by issuing an API call as in Task 1, users then ask to update their requests between 1 and 4 times (sampled uniformly). The order in which fields are updated is random. The bot must ask users if they are done with their updates and issue the updated API call.\nTask 3: Displaying options Given a user request, we query the KB using the corresponding API call and add the facts resulting from the call to the dialog history. The bot must propose options to users by listing the restaurant names sorted by their corresponding rating (from higher to lower) until users accept. For each option, users have 25% chance of accepting. If they do, the bot must stop displaying options, otherwise propose the next one. Users always accept the option if this is the last remaining one. We only keep examples with API calls retrieving at least 3 options.\nTask 4: Providing extra-information Given a user request, we sample a restaurant and start the dialog as if users had agreed to book a table there. We add all KB facts corresponding to it to the dialog. Users then ask for the phone number of the restaurant, its address or both, with proportions 25%, 25% and 50% respectively. The bot must learn to use the KB facts correctly to answer.\nTask 5: Conducting full dialogs We combine Tasks 1-4 to generate full dialogs just as in Figure 1. Unlike in Task 3, we keep examples if API calls return at least 1 option instead of 3."}, {"heading": "3.1.2 Datasets", "text": "We want to test the ability of systems to deal with entities appearing in the KB but not in the dialog training sets. To do so, we split types of cuisine and locations in half, and create two KBs, one with all facts concerning restaurants of the first halves and another one with the rest. As a result, we obtain 2 KBs of 4,200 facts and 600 restaurants each (5 types of cuisine \u00d7 5 locations \u00d7 3 price ranges \u00d7 8 ratings) that only share price ranges, ratings and party sizes, but have disjoint sets of restaurants, locations, types of cuisine, phones and addresses. We use one of the KBs to generate the standard training, validation and test dialogs, and use the other KB only to generate test dialogs, termed Out-Of-Vocabulary (OOV) test sets.\nFor training, systems have access to the training examples and both KBs and we evaluate on both test sets, plain and OOV. In addition to the intrinsic difficulty of each task, the challenge on the OOV test sets is for models to be able to generalize to new entities (restaurants, locations and cuisine types) unseen on any training dialog, something natively impossible for embedding methods. Models could, for instance, leverage information coming from the entities of the same type seen during training.\nWe generate five different datasets, one for each of the tasks defined in 3.1.1. Their statistics are given in Table 1. Training sets are relatively small (1,000 examples) to create realistic learning conditions. We make sure that the dialogs from the training and test sets are different \u2013 they are never based on the same user requests. Hence, we test if algorithms can generalize to new combinations of fields. Dialog systems are evaluated in a ranking and not a generation setting: at each turn of the dialog, we test whether they can accurately predict bot utterances and API calls by selecting a candidate, not by generating it. Candidates are ranked from a set composed of all bot utterances and API calls appearing in training, validation and test sets (plain and OOV) for all tasks combined."}, {"heading": "3.2 Dialog State Tracking Challenge", "text": "Since our tasks are only based on synthetically generated language for the user, we also supplement our dataset with real human - bot conversations. We do so by using data from the 2nd Dialog State Tracking Challenge (Henderson et al., 2014a), that is also in the restaurant booking domain. Unlike our tasks, its user requests only require 3 fields: type of cuisine (91 choices), location (5 choices) and price range (3 choices). The dataset was originally designed for dialog state tracking hence every dialog turn is labeled with a state (a user intent + slots) to be predicted. We did not use that but instead converted the data into the format of our 5 tasks and included it in the dataset as Task 6.\nWe used the provided speech transcriptions to create the user and bot utterances, and given the dialog states we created the API calls to the KB and their outputs which we added to the dialogs. We also added ratings to the restaurants returned by the API calls, so that the options proposed by the bots can be consistently predicted (by using the highest rating). We did use the original test set but use a slightly different training/validation split. Our evaluation is different than in the challenge (we do not predict the dialog state), so we can not compare with the results from (Henderson et al., 2014a).\nThis dataset has similar statistics to our Task 5 (see Table 1) but is harder. The dialogs are noisier and the bots made mistakes due to speech recognition errors or misinterpretations and also do not always have a deterministic behavior (the order in which they can ask for information varies)."}, {"heading": "4 Models", "text": "We evaluate several learning methods on our goal-oriented dialog tasks: Memory Networks, supervised embeddings, classical information retrieval methods and rule-based systems."}, {"heading": "4.1 Memory Networks", "text": "Memory Networks (Weston et al., 2015a; Sukhbaatar et al., 2015) are a recent class of models that have been applied to a range of natural language processing tasks, including question answering (Weston et al., 2015b), language modeling (Sukhbaatar et al., 2015), and non-goal-oriented dialog (Dodge et al., 2016). By first writing and then iteratively reading from a memory component that can store historical dialogs and short-term context to reason about the required response, they have shown in (Dodge et al., 2016) to perform well on those tasks and to outperform other end-to-end architectures based on Recurrent Neural Networks. Hence, we chose them as end-to-end model candidates for our tasks of goal-orientated dialog.\nWe use the MemN2N architecture of Sukhbaatar et al. (2015), with an additional modification to deal with exact matches and out-of-vocabulary words, described shortly. Apart from that addition, the main components of the model are (i) how it stores the conversation in memory, (ii) how it reads from the memory in order to reason about the response; and (iii) how it outputs the response.\nStoring and representing the conversation history As the model conducts a conversation with the user, at each time step t the previous utterance (from the user) and response (from the model) are appended to the memory. Hence, at any given time there are cu1 , . . . c u t user utterances and c r 1, . . . c r t\u22121 model responses stored (i.e. the entire conversation).1 The aim at time t is to thus choose the next response crt . We train on existing full dialog transcripts, so at training time we know the upcoming utterance crt and can use it as a training target. Following Dodge et al. (2016), we represent each utterance as a bag-of-words and in memory it is represented as a vector using the embedding matrix A, i.e. the memory is an array with entries:\nm = (A\u03a6(cu1 ), A\u03a6(c r 1) . . . , A\u03a6(c r t\u22121), A\u03a6(c r t\u22121))\nwhere \u03a6(\u00b7) maps the utterance to a bag of dimension V (the vocabulary), and A is a d\u00d7 V matrix, where d is the embedding dimension. We retain the last user utterance cut as the \u201cinput\u201d to be used directly in the controller. The contents of each memory slot mi so far does not contain any information of which speaker spoke an utterance, and at what time during the conversation. We therefore encode both of those pieces of information in the mapping \u03a6 by extending the vocabulary to contain T = 1000 extra \u201ctime features\u201d which encode the index i into the bag-of-words, and two more features that encode whether the utterance was spoken by the user or the model.\n1API calls are stored as bot utterances cri , and KB facts resulting from such calls as user utterances c u i .\nAttention over the memory The last user utterance cut is embedded using the same matrix A giving q = A\u03a6(cut ), which can also be seen as the initial state of the controller. At this point the controller reads from the memory to find salient parts of the previous conversation that are relevant to producing a response. The match between q and the memories is computed by taking the inner product followed by a softmax: pi = Softmax(u>mi), giving a probability vector over the memories. The vector that is returned back to the controller is then computed by o = R \u2211 i pimi where R is a d \u00d7 d square matrix. The controller state is then updated with q2 = o + q. The memory can be iteratively reread to look for additional pertinent information using the updated state of the controller q2 instead of q, and in general using qh on iteration h, with a fixed number of iterations N (termed N hops). Empirically we find improved performance on our tasks with up to 3 or 4 hops.\nChoosing the response The final prediction is then defined as: a\u0302 = Softmax(qN+1>W\u03a6(y1), . . . , qN+1>W\u03a6(yC))\nwhere there are C candidate responses in y, and W is of dimension d\u00d7 V . In our tasks the set y is a (large) set of candidate responses which includes all possible bot utterances and API calls.\nThe entire model is trained using stochastic gradient descent (SGD), minimizing a standard crossentropy loss between a\u0302 and the true label a.\nMatching and OOV Words Models based on word embeddings often suffer from two particular problems which we attempt to remedy here. Firstly, as words are embedded into a low dimensional space the model is often unable to differentiate between exact word matches, and matches between words with similar meaning (Bai et al., 2009). While this can be a virtue (e.g. when using synonyms) it can also be a failing (e.g. failure to differentiate between phone numbers since they have similar embeddings). Secondly, when a new word is used (e.g. the name of a new restaurant), not seen before in training, no word embedding is available, resulting typically in failure (Weston et al., 2015a).\nBoth problems can be ameliorated to a degree with the following strategy: we expand the feature representation \u03a6 of candidate responses with features indicating if there are exact matches between words of the candidate and words in the input or memory and if these words correspond to entities of the KBs. This is done by extending the vocabulary with 7 type features following the KB edge types (cuisine type, location, price range, party size, rating, phone number and address). For any word in the candidate yi, even words never seen in training dialogs, as long as they correspond to a KB entity, we can type it. We then add that type feature to the candidate representation \u03a6 only if that same word is also mentioned earlier in the current conversation. These features allow the model to learn to rely on type information using exact matching words cues when entity embeddings are not known."}, {"heading": "4.2 Supervised Embedding Models", "text": "A standard, often strongly performing, baseline is to use supervised word embedding models for scoring (conversation history, response) pairs. The embedding vectors are trained directly for this goal. In contrast, word embeddings are most well-known in the context of unsupervised training on raw text as in (Mikolov et al., 2013). Such models are trained by learning to predict the middle word given the surrounding window of words, or vice-versa. However, given training data consisting of dialogs, a much more direct and strongly performing training procedure can be used: predict the next response given the previous conversation. In this setting a candidate reponse y is scored against the input x: f(x, y) = (Ax)>By, where A and B are d\u00d7 V word embedding matrices, i.e. input and response are treated as summed bags-of-embeddings. We also consider the case of enforcing A = B, which sometimes works better, and optimize the choice on the validation set.\nThe embeddings are trained with a margin ranking loss: f(x, y) > 1 + f(x, y\u0304) and we sample N negative candidate responses y\u0304 per example, and train with SGD. This approach has been previously shown to be very effective in a range of contexts (Bai et al., 2009; Dodge et al., 2016). This method can be thought of as a Memory Network in the degenerate case of having no memory, or alternatively like a classical information retrieval model, but where the matching function is learnt."}, {"heading": "4.3 Classical Information Retrieval Models", "text": "Classical information retrieval (IR) models with no machine learning are standard baselines that often perform surprisingly well on dialog tasks (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015). We tried two standard variants:\nTF-IDF Match For each possible candidate response, we compute a matching score between the input and the response, and rank the responses by score. The score is the TF\u2013IDF weighted cosine similarity between the bag-of-words of the input and bag-of-words of the candidate response. We consider the case of the input being either only the last utterance or the entire conversation history, and choose the variant that works best on the validation set (typically the latter).\nNearest Neighbor Using the input, we find the most similar conversation in the training set, and output the response from that example. In this case we consider the input to only be the last utterance, and consider the training set as (utterance, response) pairs that we select from. We use word overlap as the scoring method."}, {"heading": "4.4 Rule-based Systems", "text": "As our tasks T1-T5 are built with a simulator in such a way as to be completely predictable, it therefore follows that it is also possible to hand-code a rule based system that achieves 100% on them, similar to the bAbI tasks of Weston et al. (2015b). However, the point of these tasks is not whether a human is smart enough to be able to build a rule-based system to solve them, but to help analyze in which circumstances our machine learning algorithms are smart enough to work.\nOn the Dialog State Tracking Challenge task (T6) which consists of real dialogs, however, building a rule-based system is not so simple and does not end up so accurate (which is where we expect machine learning to be useful). We implemented a rule-based system for this task in the following way. We initialized a dialog state using the 3 relevant slots for this task: cuisine type, location and price range. Then we analyzed the training data and wrote a series of rules that fire for triggers like word matches, positions in the dialog, entity detections or dialog state, to output particular responses, API calls and/or update a dialog state. Responses are created by combining patterns extracted from the training set with entities detected in the previous turns or stored in the dialog state. Overall we built 28 rules and extracted 21 patterns. We optimized the choice of rules and their application priority (when needed) using the validation set, reaching a validation per-response accuracy of 40.7%."}, {"heading": "5 Experiments", "text": "Our main results across all the models and tasks are given in Table 2. The first 5 rows show tasks T1-T5, and rows 6-10 show the same tasks in the out-of-vocabulary (OOV) setting. The final row gives results for the Dialog State Tracking Challenge task (T6). Columns 2-7 give the results of each method tried in terms of per-response accuracy and per-dialog accuracy, the latter given in parenthesis. Per-response accuracy counts the percentage of responses that are correct (i.e., the correct candidate is chosen out of all possible candidates). Per-dialog accuracy counts the percentage of dialogs where every response is correct. Ultimately, if only one response is incorrect this could result in a failed dialog, i.e. failure to achieve the goal (in this case, of achieving a restaurant booking). Note that we test Memory Networks (MemNNs) with and without matching features, the results are shown in the last two columns. The hyperparameters for all models were optimized on the validation sets.\nThe classical IR method TF-IDF Match performs the worst of all methods, and much worse than the Nearest Neighbor IR method, which is true on both the simulated tasks T1-T5 and on the real data of T6. This is in sharp contrast to other recent results on data-driven non-goal directed conversations, e.g. over dialogs on Twitter (Ritter et al., 2011) or Reddit (Dodge et al., 2016), where it was found that TF-IDF Match outperforms Nearest Neighbor, as general conversations on a given subject typically share many words. We conjecture that the goal-oriented nature of the conversation means that the conversation moves forward more quickly, sharing less words per (input, response) pair, e.g. consider the example in Figure 1.\nSupervised embeddings outperform classical IR methods in general, indicating learning mappings between words (via word embeddings) is important. However, only one task (T1, Issuing API calls) is completely successful. In the other tasks while some responses are correct, as shown by the per-response accuracy, there is no dialog where the goal is actually achieved, as the mean dialogaccuracy is 0. Typically the model can provide correct responses for greeting messages, asking to wait, making API calls and asking if there are any other options necessary, and so on. However, it fails to interpret the results of API calls to display options, provide information or update the calls with new information, resulting in most of its errors.\nMemory Networks (without matching features) outperform classical IR and supervised embeddings across all of the tasks. They can solve the first two tasks (issuing and updating API calls) adequately, but while they give improved results on the other tasks they do not solve them. While the per-response accuracy is improved the per-dialog accuracy is still close to 0 on T3 and T4. Some examples of predictions of the MemNN for T1-4 are given in Appendix A. On the OOV tasks again performance is improved, but more because of the relative improvement on known words compared to other methods, as without the matching features unknown words are simply not used. As mentioned before, optimal hyperparameters on several of the tasks involve 3 or 4 hops, indicating that iterative accessing and reasoning over the conversation helps, e.g. on T3 using 1 hop gives 64.8% while 2 hops yields 74.7%.\nMemory Networks with matching features give two performance gains over the same models without matching features: (i) T4 (providing information) becomes solvable because matches can be made to the results of the API call; and (ii) out-of-vocabulary results are significantly improved also. Still, tasks T3 and T5 are still fail cases, performance drops slightly on T2 compared to not using matching features, and no relative improvement is observed on T6. Finally, note that matching words on its own does not bring much use, as evidenced by the poor performance of TF-IDF matching; this idea must be combined with the other properties of the MemNN model.\nPerfectly coded rule-based systems can solve the simulated tasks T1-T5 perfectly, whereas our machine learning methods cannot. The point of these tasks is thus to identify the failings in our machine learning models so that we can correct them. The breakdown into separate tasks hence helps analyse for each learning algorithm what it is good at, and what it is not. However, it is no longer easy to build an effective rule-based system when dealing with real language on real problems, and our rule based system is outperformed by MemNNs on the realistic task T6.\nAs indicated in our observations above, while the methods we tried made some inroads into these tasks, there are still many challenges left unsolved. Our best models can learn to track implicit dialog states and manipulate OOV words and symbols (T1-T2) to issue API calls and progress in conversations, but they are still unable to perfectly handle interpreting knowledge about entities (from returned API calls) to present results to the user, e.g. displaying options in T3. The improvement observed on the simulated tasks e.g. where MemNNs outperform supervised embeddings which in turn outperform IR methods, is also seen on the realistic data of T6 with similar relative gains. This is encouraging as it indicates that future work on breaking down, analysing and developing models over the simulated tasks should help in the real tasks as well."}, {"heading": "6 Conclusion", "text": "We have introduced a methodology for learning and evaluating end-to-end goal-oriented dialog. This is an important area for the progress of end-to-end conversational agents because (i) the existing work has no well defined measures of performance (Liu et al., 2016); (ii) the breakdown in tasks will help focus research and development to improve the learning methods; and (iii) goal-oriented dialog has clear utility in real applications. We showed how Memory Networks are an effective model on\nthese tasks relative to other baselines, but are still lacking in some key areas as indicated by our tasks. Future work should address these issues and improve over them further."}, {"heading": "A Examples of Predictions of a Memory Network", "text": "Tables 3, 4, 5 and 6 below display examples of predictions of the best performing Memory Network (with 3 hops) on test examples of Tasks 1-4 along with the values of the attention over each memory for each hop (pi as defined in Sec. 4.1). This model does not use matching features."}], "references": [{"title": "Supervised semantic indexing", "author": ["B. Bai", "J. Weston", "D. Grangier", "R. Collobert", "K. Sadamasa", "Y. Qi", "O. Chapelle", "K. Weinberger"], "venue": "Proceedings of ACM CIKM, pages 187\u2013196. ACM.", "citeRegEx": "Bai et al\\.,? 2009", "shortCiteRegEx": "Bai et al\\.", "year": 2009}, {"title": "Movie-dic: a movie dialogue corpus for research and development", "author": ["R.E. Banchs"], "venue": "Proceedings of the 50th Annual Meeting of the ACL.", "citeRegEx": "Banchs,? 2012", "shortCiteRegEx": "Banchs", "year": 2012}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["J. Dodge", "A. Gane", "X. Zhang", "A. Bordes", "S. Chopra", "A. Miller", "A. Szlam", "J. Weston"], "venue": "Proc. of ICLR.", "citeRegEx": "Dodge et al\\.,? 2016", "shortCiteRegEx": "Dodge et al\\.", "year": 2016}, {"title": "The second dialog state tracking challenge", "author": ["M. Henderson", "B. Thomson", "J. Williams"], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 263.", "citeRegEx": "Henderson et al\\.,? 2014a", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["M. Henderson", "B. Thomson", "S. Young"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 292\u2013299.", "citeRegEx": "Henderson et al\\.,? 2014b", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Cobot in lambdamoo: A social statistics agent", "author": ["C.L. Isbell", "M. Kearns", "D. Kormann", "S. Singh", "P. Stone"], "venue": "AAAI/IAAI, pages 36\u201341.", "citeRegEx": "Isbell et al\\.,? 2000", "shortCiteRegEx": "Isbell et al\\.", "year": 2000}, {"title": "Filter, rank, and transfer the knowledge: Learning to chat", "author": ["S. Jafarpour", "C.J. Burges", "A. Ritter"], "venue": "Advances in Ranking, 10.", "citeRegEx": "Jafarpour et al\\.,? 2010", "shortCiteRegEx": "Jafarpour et al\\.", "year": 2010}, {"title": "An isu dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the talk in-car system", "author": ["O. Lemon", "K. Georgila", "J. Henderson", "M. Stuttle"], "venue": "Proceedings of the 11th Conference of the European Chapter of the ACL: Posters & Demonstrations, pages 119\u2013122.", "citeRegEx": "Lemon et al\\.,? 2006", "shortCiteRegEx": "Lemon et al\\.", "year": 2006}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Liu", "C.-W.", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau"], "venue": "arXiv preprint arXiv:1603.08023.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau"], "venue": "arXiv preprint arXiv:1506.08909.", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A survey on metrics for the evaluation of user simulations", "author": ["O. Pietquin", "H. Hastie"], "venue": "The knowledge engineering review, 28(01), 59\u201373.", "citeRegEx": "Pietquin and Hastie,? 2013", "shortCiteRegEx": "Pietquin and Hastie", "year": 2013}, {"title": "Data-driven response generation in social media", "author": ["A. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "Proc. of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Serban et al\\.,? 2015a", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["I.V. Serban", "R. Lowe", "L. Charlin", "J. Pineau"], "venue": "arXiv preprint arXiv:1512.05742.", "citeRegEx": "Serban et al\\.,? 2015b", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "arXiv preprint arXiv:1503.02364.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "Nie", "J.-Y.", "J. Gao", "B. Dolan"], "venue": "Proceedings of NAACL.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "Proceedings of NIPS.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le,? 2015", "shortCiteRegEx": "Vinyals and Le", "year": 2015}, {"title": "A dataset for research on short-text conversations", "author": ["H. Wang", "Z. Lu", "H. Li", "E. Chen"], "venue": "EMNLP.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information", "author": ["Z. Wang", "O. Lemon"], "venue": "Proceedings of the SIGDIAL 2013 Conference.", "citeRegEx": "Wang and Lemon,? 2013", "shortCiteRegEx": "Wang and Lemon", "year": 2013}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "Proceedings of ICLR.", "citeRegEx": "Weston et al\\.,? 2015a", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015b", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["S. Young", "M. Gasic", "B. Thomson", "J.D. Williams"], "venue": "Proceedings of the IEEE, 101(5), 1160\u20131179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a).", "startOffset": 145, "endOffset": 170}, {"referenceID": 23, "context": "Traditional dialog systems (Young et al., 2013) are composed of several modules including a language interpreter, a dialog state tracker and a response generator, usually each is separately hand-crafted or trained.", "startOffset": 27, "endOffset": 47}, {"referenceID": 7, "context": "Such systems are specialized for a single domain and rely heavily on expert knowledge; for instance, most of these systems, called slot-filling methods (Lemon et al., 2006; Wang and Lemon, 2013), require to predefine the structure of the dialog state as a form composed of a set of slots to be filled during the dialog.", "startOffset": 152, "endOffset": 194}, {"referenceID": 20, "context": "Such systems are specialized for a single domain and rely heavily on expert knowledge; for instance, most of these systems, called slot-filling methods (Lemon et al., 2006; Wang and Lemon, 2013), require to predefine the structure of the dialog state as a form composed of a set of slots to be filled during the dialog.", "startOffset": 152, "endOffset": 194}, {"referenceID": 15, "context": "End-to-end dialog systems, usually based on neural networks (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Dodge et al., 2016), currently generate a lot of interest because they do not have such limitations: all their components are directly trained on past dialogs and they make no asumption on the domain or on the structure of the dialog state.", "startOffset": 60, "endOffset": 166}, {"referenceID": 18, "context": "End-to-end dialog systems, usually based on neural networks (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Dodge et al., 2016), currently generate a lot of interest because they do not have such limitations: all their components are directly trained on past dialogs and they make no asumption on the domain or on the structure of the dialog state.", "startOffset": 60, "endOffset": 166}, {"referenceID": 16, "context": "End-to-end dialog systems, usually based on neural networks (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Dodge et al., 2016), currently generate a lot of interest because they do not have such limitations: all their components are directly trained on past dialogs and they make no asumption on the domain or on the structure of the dialog state.", "startOffset": 60, "endOffset": 166}, {"referenceID": 13, "context": "End-to-end dialog systems, usually based on neural networks (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Dodge et al., 2016), currently generate a lot of interest because they do not have such limitations: all their components are directly trained on past dialogs and they make no asumption on the domain or on the structure of the dialog state.", "startOffset": 60, "endOffset": 166}, {"referenceID": 2, "context": "End-to-end dialog systems, usually based on neural networks (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Dodge et al., 2016), currently generate a lot of interest because they do not have such limitations: all their components are directly trained on past dialogs and they make no asumption on the domain or on the structure of the dialog state.", "startOffset": 60, "endOffset": 166}, {"referenceID": 12, "context": "Those methods have reached promising performance in non goal-oriented chit-chat settings, where they were trained to predict the next utterrance in social media and forums threads (Ritter et al., 2011; Wang et al., 2013; Lowe et al., 2015) or movie conversations (Banchs, 2012).", "startOffset": 180, "endOffset": 239}, {"referenceID": 19, "context": "Those methods have reached promising performance in non goal-oriented chit-chat settings, where they were trained to predict the next utterrance in social media and forums threads (Ritter et al., 2011; Wang et al., 2013; Lowe et al., 2015) or movie conversations (Banchs, 2012).", "startOffset": 180, "endOffset": 239}, {"referenceID": 9, "context": "Those methods have reached promising performance in non goal-oriented chit-chat settings, where they were trained to predict the next utterrance in social media and forums threads (Ritter et al., 2011; Wang et al., 2013; Lowe et al., 2015) or movie conversations (Banchs, 2012).", "startOffset": 180, "endOffset": 239}, {"referenceID": 1, "context": ", 2015) or movie conversations (Banchs, 2012).", "startOffset": 31, "endOffset": 45}, {"referenceID": 8, "context": "However, evaluation of models is difficult (Liu et al., 2016) making their resulting utility in end applications unclear.", "startOffset": 43, "endOffset": 61}, {"referenceID": 22, "context": "To this end, in the spirit of the bAbI tasks designed as question answering test-beds (Weston et al., 2015b), we designed a set of tasks in the goal-oriented application of restaurant reservation.", "startOffset": 86, "endOffset": 108}, {"referenceID": 21, "context": "As an end-to-end neural model, we chose to apply Memory Networks (Weston et al., 2015a), an attention-based architecture, which has proved to be competitive for non goal-oriented dialog (Dodge et al.", "startOffset": 65, "endOffset": 87}, {"referenceID": 2, "context": ", 2015a), an attention-based architecture, which has proved to be competitive for non goal-oriented dialog (Dodge et al., 2016).", "startOffset": 107, "endOffset": 127}, {"referenceID": 3, "context": "Task 6 was converted from the 2 Dialog State Tracking Challenge (Henderson et al., 2014a).", "startOffset": 64, "endOffset": 89}, {"referenceID": 3, "context": "We confirm our findings on real human-machine dialogs from the restaurant reservation dataset of the 2 Dialog State Tracking Challenge (Henderson et al., 2014a), which we converted into our task format, showing that Memory Networks can outperform a dedicated slot-filling rule-based baseline.", "startOffset": 135, "endOffset": 160}, {"referenceID": 23, "context": "The most successful goal-oriented dialog systems model conversation as partially observable Markov decision processes (POMDP) (Young et al., 2013).", "startOffset": 126, "endOffset": 146}, {"referenceID": 4, "context": "However, in spite of recent efforts to learn modules (Henderson et al., 2014b), they still require many hand-crafted features for the state and action space representations, which restrict their usage to narrow domains.", "startOffset": 53, "endOffset": 78}, {"referenceID": 23, "context": "Our simulation, which we use to generate goal-oriented datasets can be seen as an equivalent of the user simulators used to train POMDP (Young et al., 2013; Pietquin and Hastie, 2013), but for training end-to-end systems.", "startOffset": 136, "endOffset": 183}, {"referenceID": 11, "context": "Our simulation, which we use to generate goal-oriented datasets can be seen as an equivalent of the user simulators used to train POMDP (Young et al., 2013; Pietquin and Hastie, 2013), but for training end-to-end systems.", "startOffset": 136, "endOffset": 183}, {"referenceID": 3, "context": "Indeed, goaloriented datasets are usually designed to train or evaluate dialog state tracker components (Henderson et al., 2014a) and are hence of limited scale and not suitable for end-to-end learning (annotated at the state level and noisy).", "startOffset": 104, "endOffset": 129}, {"referenceID": 2, "context": "The closest resource to ours might be the set of tasks described in (Dodge et al., 2016), since some of them can be seen as goal-oriented.", "startOffset": 68, "endOffset": 88}, {"referenceID": 3, "context": "We do so by using data from the 2 Dialog State Tracking Challenge (Henderson et al., 2014a), that is also in the restaurant booking domain.", "startOffset": 66, "endOffset": 91}, {"referenceID": 3, "context": "Our evaluation is different than in the challenge (we do not predict the dialog state), so we can not compare with the results from (Henderson et al., 2014a).", "startOffset": 132, "endOffset": 157}, {"referenceID": 21, "context": "Memory Networks (Weston et al., 2015a; Sukhbaatar et al., 2015) are a recent class of models that have been applied to a range of natural language processing tasks, including question answering (Weston et al.", "startOffset": 16, "endOffset": 63}, {"referenceID": 17, "context": "Memory Networks (Weston et al., 2015a; Sukhbaatar et al., 2015) are a recent class of models that have been applied to a range of natural language processing tasks, including question answering (Weston et al.", "startOffset": 16, "endOffset": 63}, {"referenceID": 22, "context": ", 2015) are a recent class of models that have been applied to a range of natural language processing tasks, including question answering (Weston et al., 2015b), language modeling (Sukhbaatar et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 17, "context": ", 2015b), language modeling (Sukhbaatar et al., 2015), and non-goal-oriented dialog (Dodge et al.", "startOffset": 28, "endOffset": 53}, {"referenceID": 2, "context": ", 2015), and non-goal-oriented dialog (Dodge et al., 2016).", "startOffset": 38, "endOffset": 58}, {"referenceID": 2, "context": "By first writing and then iteratively reading from a memory component that can store historical dialogs and short-term context to reason about the required response, they have shown in (Dodge et al., 2016) to perform well on those tasks and to outperform other end-to-end architectures based on Recurrent Neural Networks.", "startOffset": 185, "endOffset": 205}, {"referenceID": 17, "context": "We use the MemN2N architecture of Sukhbaatar et al. (2015), with an additional modification to deal with exact matches and out-of-vocabulary words, described shortly.", "startOffset": 34, "endOffset": 59}, {"referenceID": 2, "context": "Following Dodge et al. (2016), we represent each utterance as a bag-of-words and in memory it is represented as a vector using the embedding matrix A, i.", "startOffset": 10, "endOffset": 30}, {"referenceID": 0, "context": "Firstly, as words are embedded into a low dimensional space the model is often unable to differentiate between exact word matches, and matches between words with similar meaning (Bai et al., 2009).", "startOffset": 178, "endOffset": 196}, {"referenceID": 21, "context": "the name of a new restaurant), not seen before in training, no word embedding is available, resulting typically in failure (Weston et al., 2015a).", "startOffset": 123, "endOffset": 145}, {"referenceID": 10, "context": "In contrast, word embeddings are most well-known in the context of unsupervised training on raw text as in (Mikolov et al., 2013).", "startOffset": 107, "endOffset": 129}, {"referenceID": 0, "context": "This approach has been previously shown to be very effective in a range of contexts (Bai et al., 2009; Dodge et al., 2016).", "startOffset": 84, "endOffset": 122}, {"referenceID": 2, "context": "This approach has been previously shown to be very effective in a range of contexts (Bai et al., 2009; Dodge et al., 2016).", "startOffset": 84, "endOffset": 122}, {"referenceID": 5, "context": "Classical information retrieval (IR) models with no machine learning are standard baselines that often perform surprisingly well on dialog tasks (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 145, "endOffset": 233}, {"referenceID": 6, "context": "Classical information retrieval (IR) models with no machine learning are standard baselines that often perform surprisingly well on dialog tasks (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 145, "endOffset": 233}, {"referenceID": 12, "context": "Classical information retrieval (IR) models with no machine learning are standard baselines that often perform surprisingly well on dialog tasks (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 145, "endOffset": 233}, {"referenceID": 16, "context": "Classical information retrieval (IR) models with no machine learning are standard baselines that often perform surprisingly well on dialog tasks (Isbell et al., 2000; Jafarpour et al., 2010; Ritter et al., 2011; Sordoni et al., 2015).", "startOffset": 145, "endOffset": 233}, {"referenceID": 21, "context": "As our tasks T1-T5 are built with a simulator in such a way as to be completely predictable, it therefore follows that it is also possible to hand-code a rule based system that achieves 100% on them, similar to the bAbI tasks of Weston et al. (2015b). However, the point of these tasks is not whether a human is smart enough to be able to build a rule-based system to solve them, but to help analyze in which circumstances our machine learning algorithms are smart enough to work.", "startOffset": 229, "endOffset": 251}, {"referenceID": 12, "context": "over dialogs on Twitter (Ritter et al., 2011) or Reddit (Dodge et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 2, "context": ", 2011) or Reddit (Dodge et al., 2016), where it was found that TF-IDF Match outperforms Nearest Neighbor, as general conversations on a given subject typically share many words.", "startOffset": 18, "endOffset": 38}, {"referenceID": 8, "context": "This is an important area for the progress of end-to-end conversational agents because (i) the existing work has no well defined measures of performance (Liu et al., 2016); (ii) the breakdown in tasks will help focus research and development to improve the learning methods; and (iii) goal-oriented dialog has clear utility in real applications.", "startOffset": 153, "endOffset": 171}], "year": 2016, "abstractText": "End-to-end dialog systems, in which all components are learnt simultaneously, have recently obtained encouraging successes. However these were mostly on conversations related to chit-chat with no clear objective and for which evaluation is difficult. This paper proposes a set of tasks to test the capabilities of such systems on goal-oriented dialogs, where goal completion ensures a well-defined measure of performance. Built in the context of restaurant reservation, our tasks require to manipulate sentences and symbols, in order to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a).", "creator": "LaTeX with hyperref package"}}}