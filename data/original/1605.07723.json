{"id": "1605.07723", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Data Programming: Creating Large Training Sets, Quickly", "abstract": "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users provide a set of labeling functions, which are programs that heuristically label large subsets of data points, albeit noisily. By viewing these labeling functions as implicitly describing a generative model for this noise, we show that we can recover the parameters of this model to \"denoise\" the training set. Then, we show how to modify a discriminative loss function to make it noise-aware. We demonstrate our method over a range of discriminative models including logistic regression and LSTMs. We establish theoretically that we can recover the parameters of these generative models in a handful of settings. Experimentally, on the 2014 TAC-KBP relation extraction challenge, we show that data programming would have obtained a winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a supervised LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way to create machine learning models for non-experts.", "histories": [["v1", "Wed, 25 May 2016 04:14:59 GMT  (69kb,D)", "http://arxiv.org/abs/1605.07723v1", null], ["v2", "Sat, 3 Dec 2016 20:03:26 GMT  (72kb,D)", "http://arxiv.org/abs/1605.07723v2", null], ["v3", "Sun, 8 Jan 2017 19:48:53 GMT  (72kb,D)", "http://arxiv.org/abs/1605.07723v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["alexander j ratner", "christopher de sa", "sen wu 0002", "daniel selsam", "christopher r\u00e9"], "accepted": true, "id": "1605.07723"}, "pdf": {"name": "1605.07723.pdf", "metadata": {"source": "CRF", "title": "Data Programming: Creating Large Training Sets, Quickly", "authors": ["Alexander Ratner", "Christopher De Sa", "Sen Wu", "Daniel Selsam", "Christopher R\u00e9"], "emails": ["ajratner@stanford.edu", "cdesa@stanford.edu", "senwu@stanford.edu", "dselsam@stanford.edu", "chrismre@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Many of the major machine learning breakthroughs of the last decade have been catalyzed by the release of a new labeled training dataset.1 Supervised learning approaches that use such datasets have increasingly become key building blocks of applications throughout science and industry. This trend has also been fueled by the recent empirical success of automated feature generation approaches, notably deep learning methods such as long short term memory (LSTM) networks [12], which ameliorate the burden of feature engineering given large enough labeled training sets. For many real-world applications, however, large hand-labeled training sets do not exist, and are prohibitively expensive to create due to requirements that labelers be experts in the application domain. Furthermore, applications\u2019 needs often change, necessitating new or modified training sets.\nTo help reduce the cost of training set creation, we propose data programming, a paradigm for the programmatic creation of training datasets. Data programming extends the idea of distant supervision, in which an external knowledge base is mapped onto an input dataset to generate training examples [20]. In data programming, users provide a set of heuristic labeling functions, which are user-defined programs that each provide a label for some subset of the data, and collectively generate a large but noisy training set. These labeling functions can be more general than distant supervision mappings\u2014they can use external knowledge bases (as in distant supervision), model an individual annotator\u2019s labels (as in crowdsourcing), or leverage a combination of domain-specific patterns and dictionaries\u2014and thus may have widely varying error rates, may overlap, and may conflict on certain data points. To address this, we model the labeling functions as a generative process, which lets us automatically denoise the resulting training set by learning the accuracies of the labeling functions along with their correlation structure. In turn, we use this model of the training set to optimize a stochastic version of the loss function of the discriminative model that we desire to train. We show that, given certain conditions on the labeling functions, our method achieves the same asymptotic scaling as supervised learning methods,\n1 http://www.spacemachine.net/views/2016/3/datasets-over-algorithms\nar X\niv :1\n60 5.\n07 72\n3v 1\n[ st\nat .M\nL ]\n2 5\nM ay\nbut that our scaling depends on the amount of unlabeled data\u2014using only a fixed number of labeling functions, which is small relative to the training set size.\nData programming is in part motivated by the challenges that users faced when applying prior programmatic supervision approaches, and is intended to be a new software engineering paradigm for the creation and management of training sets. For example, consider the scenario when two labeling functions of differing quality and scope overlap and possibly conflict on certain training examples; in prior approaches the user would have to decide which one to use, or how to somehow integrate the signal from both. In data programming, we accomplish this automatically by learning a model of the training set that includes both labeling functions. Additionally, users are often aware of, or able to induce, dependencies between their labeling functions. In data programming, users can provide a dependency graph to indicate, for example, that two labeling functions are similar, or that one \u201cfixes\u201d or \u201creinforces\u201d another. We describe cases in which we can learn the strength of these dependencies, and for which our generalization is again asymptotically identical to the supervised case.\nOne further motivation for our method is driven by the observation that users often struggle with selecting features for their models, which is a traditional development bottleneck given fixed-size training sets. However, initial feedback from users suggests that writing labeling functions in the framework of data programming may be easier. While the impact of a feature on end performance is dependent on the training set and on statistical characteristics of the model, a labeling function has a simple and intuitive optimality criterion: that it labels data correctly. Motivated by this, we explore whether we can flip the traditional machine learning development process on its head, having users instead focus on generating training sets large enough to support automatically-generated features.\nSummary of Contributions and Outline Our first contribution is the data programming framework, in which users can implicitly describe a rich generative model for a training set in a more flexible and general way than in previous approaches. In Section 3, we first explore a simple model in which labeling functions are conditionally independent. We show here that under certain conditions, the sample complexity is nearly the same as in the labeled case. In Section 4, we extend our results to more sophisticated data programming models, generalizing related results in crowdsourcing [15]. In Section 5, we validate our approach experimentally on large real-world text relation extraction tasks in genomics, pharmacogenomics and news domains, where we show an average 2.34 point F1 score improvement over a baseline programmatic supervision approach\u2014including what would have been a new competition-winning score for the 2014 TAC-KBP Slot Filling competition. Using LSTM-generated features, we would have placed second in this competition, achieving a 5.98 point F1 score gain over a state-of-the-art LSTM baseline trained on hand-labeled data [30]. Additionally, we describe promising feedback from a usability study with a group of bioinformatics users."}, {"heading": "2 Related Work", "text": "Our work builds on many previous approaches in machine learning. Distant supervision is one preceding paradigm for programmatically creating training sets. The canonical example is relation extraction from text, wherein a knowledge base of known relations is heuristically mapped to label a set of mentions in an input corpus as ground truth examples [7, 20]. Basic extensions group these mapped examples by the particular textual pattern w that they occur with, and cast the problem as a multiple instance learning one [13, 23]. Other extensions actually model the accuracy of this pattern w using a discriminative feature-based model [25], or generative models such as hierarchical topic models [1, 24, 29]. Like our approach, these latter methods model a generative process of training set creation, however in a proscribed way that is not based on user input as in our approach. There is also a wealth of examples where additional heuristic patterns used to label training data are collected from unlabeled data [6] or directly from users [19, 27], in a similar manner to our approach, but without a framework to deal with the fact that said labels are explicitly noisy.\nCrowdsourcing is widely used for various machine learning tasks [11, 16]. Of particular relevance to our problem setting is the theoretical question of how to model the accuracy of various experts without ground truth available, classically raised in the context of crowdsourcing [9]. More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31]. Our model can capture the model described in crowdsourcing, and can be equivalent in the independent case (Sec. 3). However, in addition to generalizing beyond getting inputs solely from human annotators, we also model user-supplied dependencies between the \u201clabelers\u201d in our model, which is not natural within the context of crowdsourcing. Additionally, while crowdsourcing results focus on\nthe regime of a large number of labelers each labeling a small subset of the data, we consider a small set of labeling functions each labeling a large portion of the dataset.\nCo-training is a classic procedure for effectively utilizing both a small amount of labeled data and a large amount of unlabeled data by selecting two conditionally independent views of the data [4]. In addition to not needing a set of labeled data, and allowing for more than two views (labeling functions in our case), our approach allows explicit modeling of dependencies between views, for example allowing observed issues with dependencies between views to be explicitly modeled [17].\nBoosting is a well known procedure for combining the output of many \u201cweak\u201d classifiers to create a strong classifier in a supervised setting [26]. Recently, boosting-like methods have been proposed which leverage unlabeled data in addition to labeled data, which is also used to set constraints on the accuracies of the individual classifiers being ensembled [2]. This is similar in spirit to our approach, except that labeled data is not explicitly necessary in ours, and richer dependency structures between our \u201cheuristic\u201d classifiers (labeling functions) are supported.\nThe general case of learning with noisy labels is treated both in classical [18] and more recent contexts [21]. It has also been studied specifically in the context of label-noise robust logistic regression [5]. We consider the more general scenario where multiple noisy labeling functions can conflict and have dependencies."}, {"heading": "3 The Data Programming Paradigm", "text": "In many applications, we would like to use machine learning, but we face the following challenges: (i) hand-labeled training data is not available, and is prohibitively expensive to obtain in sufficient quantities as it requires expensive domain experts; (ii) related external knowledge bases are either unavailable or insufficiently specific, precluding a traditional distant supervision or co-training approach; (iii) application specifications are in flux, changing the model we ultimately wish to learn.\nIn such a setting, we would like a simple, scalable and adaptable approach for supervising a model applicable to our problem. More specifically, we would ideally like our approach to achieve expected loss with high probability, given O(1) inputs of some sort from a domain-expert user, rather than the traditional O\u0303( \u22122) hand-labeled training examples required by most supervised methods (where O\u0303 notation hides logarithmic factors). To this end, we propose data programming, a paradigm for the programmatic creation of training sets, which enables domain-experts to more rapidly train machine learning systems and has the potential for this type of scaling of expected loss. In data programming, rather than manually labeling each example, users instead describe the processes by which these points could be labeled by providing a set of heuristic rules called labeling functions.\nIn the remainder of this paper, we focus on a binary classification task in which we have a distribution \u03c0 over object and class pairs (x, y) \u2208 X \u00d7 {\u22121, 1}, and we are concerned with minimizing the logistic loss under a linear model given some features,\nl(w) = E(x,y)\u223c\u03c0 [ log(1 + exp(\u2212wT f (x)y)) ] ,\nwhere without loss of generality, we assume that \u2016 f (x)\u2016 \u2264 1. Then, a labeling function \u03bbi : X 7\u2192 {\u22121, 0, 1} is a user-defined function that encodes some domain heuristic, which provides a (non-zero) label for some subset of the objects. As part of a data programming specification, a user provides some m labeling functions, which we denote in vectorized form as \u03bb : X 7\u2192 {\u22121, 0, 1}m.\nExample 3.1. To gain intuition about labeling functions, we describe a simple text relation extraction example. In Figure 1, we consider the task of classifying co-occurring gene and disease mentions as either expressing a causal relation or not. For example, given the sentence \u201cGene A causes disease B\u201d, the object x = (A, B) has true class y = 1. To construct a training set, the user writes three labeling functions (Figure 1a). In \u03bb1, an external structured knowledge base is used to label a few objects with relatively high accuracy, and is equivalent to a traditional distant supervision rule (see Sec. 2). \u03bb2 uses a purely heuristic approach to label a much larger number of examples with lower accuracy. Finally, \u03bb3 is a \u201chybrid\u201d labeling function, which leverages both an external knowledge base and a heuristic filter.\nA labeling function need not have perfect accuracy or recall; rather, it represents a pattern that the user wishes to impart to their model and that is easier to encode as a labeling function than as a set of hand-labeled examples. As illustrated in Ex. 3.1, labeling function can be based on external knowledge bases, libraries or ontologies, could be purely a heuristic pattern, or some hybrid of these types; we see evidence for the existence of such diversity in our experiments (Section 5). The use of labeling functions is also strictly more general than manual annotations, as a manual annotation can always be directly encoded by a labeling function. Importantly, labeling functions can overlap, conflict, and even have dependencies which users can provide as part of the data programming specification (see Section 4); our approach provides a simple framework for these inputs.\nIndependent Labeling Functions We first describe a model in which the labeling functions label independently, given the true label class. Under this model, each labeling function \u03bbi has some probability \u03b2i of labeling an object and then some probability \u03b1i of labeling the object correctly; for simplicity we also assume here that each class has probability 0.5. This model has distribution\n\u00b5\u03b1,\u03b2(\u039b,Y) = 1 2 m\u220f i=1 ( \u03b2i\u03b1i1{\u039bi=Y} + \u03b2i(1 \u2212 \u03b1i)1{\u039bi=\u2212Y} + (1 \u2212 \u03b2i)1{\u039bi=0} ) , (1)\nwhere \u039b \u2208 {\u22121, 0, 1}m contains the labels output by the labeling functions, and Y \u2208 {\u22121, 1} is the predicted class. If we allow the parameters \u03b1 \u2208 Rm and \u03b2 \u2208 Rm to vary, (1) specifies a family of generative models. In order to expose the scaling of the expected loss as the size of the unlabeled dataset changes, we will assume here that 0.3 \u2264 \u03b2i \u2264 0.5 and 0.8 \u2264 \u03b1i \u2264 0.9. We note that while these arbitrary constraints can be changed, they are roughly consistent with our applied experience, where users tend to write high-accuracy and high-coverage labeling functions.\nOur first goal will be to learn which parameters (\u03b1, \u03b2) are most consistent with our observations\u2014our unlabeled training set\u2014using maximum likelihood estimation. To do this for a particular training set S \u2282 X, we will solve the problem\n(\u03b1\u0302, \u03b2\u0302) = arg max \u03b1,\u03b2 \u2211 x\u2208S log P(\u039b,Y)\u223c\u00b5\u03b1,\u03b2 (\u039b = \u03bb(x)) . (2)\nIn other words, we are maximizing the probability that the observed labels produced on our training examples occur under the generative model in (1). In our experiments, we use stochastic gradient descent to solve this problem; since this is a standard technique, we defer its analysis to the appendix.\nNoise-Aware Empirical Loss Given that our parameter learning phase has successfully found some \u03b1\u0302 and \u03b2\u0302 that accurately describe the training set, we can now proceed to estimate the parameter w which minimizes the expected risk of a linear model over our feature mapping f , given \u03b1\u0302, \u03b2\u0302. To do so, we define the noise-aware empirical risk L\u03b1\u0302,\u03b2\u0302 with regularization parameter \u03c1, and compute the noise-aware empirical risk minimizer\nw\u0302 = arg min w L\u03b1\u0302,\u03b2\u0302(w; S ) = arg minw 1 |S | \u2211 x\u2208S E(\u039b,Y)\u223c\u00b5\u03b1\u0302,\u03b2\u0302 [ log ( 1 + e\u2212w T f (x)Y )\u2223\u2223\u2223\u2223\u039b = \u03bb(x)] + \u03c1 \u2016w\u20162 (3)\nThis is a logistic regression problem, so it can be solved using stochastic gradient descent as well. We can in fact prove that stochastic gradient descent running on (2) and (3) is guaranteed to produce accurate estimates, under conditions which we describe now. First, the problem distribution \u03c0 needs to be accurately modeled by some distribution \u00b5 in the family that we are trying to learn. That is, for some \u03b1\u2217 and \u03b2\u2217,\n\u2200\u039b \u2208 {\u22121, 0, 1}m,Y \u2208 {\u22121, 1}, P(x,y)\u223c\u03c0\u2217 (\u03bb(x) = \u039b, y = Y) = \u00b5\u03b1\u2217,\u03b2\u2217 (\u039b,Y). (4)\nSecond, given an example (x, y) \u223c \u03c0\u2217, the class label y must be independent of the features f (x) given the labels \u03bb(x). That is, (x, y) \u223c \u03c0\u2217 \u21d2 y \u22a5 f (x) | \u03bb(x). (5) This assumption encodes the idea that the labeling functions, while they may be arbitrarily dependent on the features, provide sufficient information to accurately identify the class. Third, we assume that the algorithm used to solve (3) has bounded generalization risk such that for some parameter \u03c7,\nEw\u0302 [ ES [ L\u03b1\u0302,\u03b2\u0302(w\u0302; T ) ] \u2212min\nw ES\n[ L\u03b1\u0302,\u03b2\u0302(w; S ) ]] \u2264 \u03c7. (6)\nUnder these conditions, we make the following statement about the accuracy of our estimates, which is a simplified version of a theorem that is detailed in the appendix.\nTheorem 1. Suppose that we run data programming, solving the problems in (2) and (3) using stochastic gradient descent to produce (\u03b1\u0302, \u03b2\u0302) and w\u0302. Suppose further that our setup satisfies the conditions (4), (5), and (6), and suppose that m \u2265 2000. Then for any > 0, if the number of labeling functions m and the size of the input dataset S are large enough that\n|S | \u2265 356 2 log ( m 3 ) then our expected parameter error and generalization risk can be bounded by\nE [ \u2016\u03b1\u0302 \u2212 \u03b1\u2217\u20162 ] \u2264 m 2 E [\u2225\u2225\u2225\u03b2\u0302 \u2212 \u03b2\u2217\u2225\u2225\u22252] \u2264 m 2 E [l(w\u0302) \u2212min w l(w) ] \u2264 \u03c7 + 27\u03c1 .\nWe select m \u2265 2000 to simplify the statement of the theorem and give the reader a feel for how scales with respect to |S |. The full theorem with scaling in each parameter (and for arbitrary m) is presented in the appendix.\nThis result establishes that to achieve both expected loss and parameter estimate error , it suffices to have only m = O(1) labeling functions and |S | = O\u0303( \u22122) training examples, which is the same asymptotic scaling exhibited by methods that use labeled data. This means that data programming achieves the same learning rate as methods that use labeled data, while requiring asymptotically less work from its users, who need to specify O(1) labeling functions rather than manually label O\u0303( \u22122) examples. In contrast, in the crowdsourcing setting [15], the number of workers m tends to infinity while here it is constant while the dataset grows. These results provide some explanation of why our experimental results suggest that a small number of rules with a large unlabeled training set can be effective at even complex natural language processing tasks."}, {"heading": "4 Handling Dependencies", "text": "In our experience with data programming, we have found that users often write labeling functions that have clear dependencies among them. As more labeling functions are added as the system is developed, an implicit dependency structure arises naturally amongst the labeling functions: modeling these dependencies can in some cases improve accuracy. We describe a method by which the user can specify this dependency knowledge as a dependency graph, and show how the system can use it to produce better parameter estimates.\nLabel Function Dependency Graph To support the injection of dependency information into the model, we augment the data programming specification with a label function dependency graph, G \u2282 D \u00d7 {1, . . . ,m} \u00d7 {1, . . . ,m}, which is an undirected graph over the labeling functions, each of the edges of which is associated with a dependency type from a class of dependenciesD appropriate to the domain. From our experience with practitioners, we identified four commonly-occurring types of dependencies as illustrative examples: similar, fixing, reinforcing, and exclusive (see Figure 2).\nFor example, suppose that we have two functions \u03bb1 and \u03bb2, and \u03bb2 typically labels only when (i) \u03bb1 also labels, (ii) \u03bb1 and \u03bb2 disagree in their labeling, and (iii) \u03bb2 is actually correct. We call this a fixing dependency, since \u03bb2 fixes mistakes made by \u03bb1. If \u03bb1 and \u03bb2 were to typically agree rather than disagree, this would be a reinforcing dependency, since \u03bb2 reinforces the truth proclaimed by \u03bb1.\nModeling Dependencies The presence of dependency information means that we can no longer model our labels using the simple Bayesian network in (1). Instead, we model our distribution as a factor graph. This standard technique lets us describe the family of generative distributions in terms of a known factor function h : {\u22121, 0, 1}m\u00d7{\u22121, 1} 7\u2192 {\u22121, 0, 1}M (in which each entry hi represents a factor), and an unknown parameter \u03b8 \u2208 RM as\n\u00b5\u03b8(\u039b,Y) = Z\u22121\u03b8 exp(\u03b8 T h(\u039b,Y)),\nwhere Z\u03b8 is the partition function which ensures that \u00b5 is a distribution. Next, we will describe how we define h using information from the dependency graph.\nTo construct h, we will start with some base factors, which we inherit from (1), and then augment them with additional factors representing dependencies. For all i \u2208 {1, . . . ,m}, we let\nh0(\u039b,Y) = Y, hi(\u039b,Y) = \u039biY, hm+i(\u039b,Y) = \u039bi, h2m+i(\u039b,Y) = \u039b2i Y, h3m+i(\u039b,Y) = \u039b 2 i .\nThese factors alone are sufficient to describe any distribution for which the labels are mutually independent, given the class: this includes the independent family in (1).\nWe now proceed by adding additional factors to h, which model the dependencies encoded in G. For each dependency edge (d, i, j), we add one or more factors to h as follows. For a near-duplicate dependency on (i, j), we add a single factor h\u03b9(\u039b,Y) = 1{\u039bi = \u039b j}, which increases our prior probability that the labels will agree. For a fixing dependency, we add two factors, h\u03b9(\u039b,Y) = \u22121{\u039bi = 0 \u2227\u039b j , 0} and h\u03b9+1(\u039b,Y) = 1{\u039bi = \u2212Y \u2227\u039b j = Y}, which encode the idea that \u03bb j labels only when \u03bbi does, and that \u03bb j fixes errors made by \u03bbi. The factors for a reinforcing dependency are the same, except that h\u03b9+1(\u039b,Y) = 1{\u039bi = Y \u2227\u039b j = Y}. Finally, for an exclusive dependency, we have a single factor h\u03b9(\u039b,Y) = \u22121{\u039bi , 0 \u2227 \u039b j , 0}.\nLearning with Dependencies We can again solve a maximum likelihood problem like (2) to learn the parameter \u03b8\u0302. Using the results, we can continue on to find the noise-aware empirical loss minimizer by solving the problem in (3). In order to solve these problems in the dependent case, we typically invoke stochastic gradient descent, using Gibbs sampling to sample from the distributions used in the gradient update. Under conditions similar to those in Section 3, we can again provide a bound the accuracy of these results. We define these conditions now. First, there must be some set \u0398 \u2282 RM that we know our parameter lies in. This is analogous to the assumptions on \u03b1i and \u03b2i we made in Section 3, and we can state the following analog of (4):\n\u2203\u03b8\u2217 \u2208 \u0398 s.t. \u2200(\u039b,Y) \u2208 {\u22121, 0, 1}m \u00d7 {\u22121, 1}, P(x,y)\u223c\u03c0\u2217 (\u03bb(x) = \u039b, y = Y) = \u00b5\u03b8\u2217 (\u039b,Y). (7)\nSecond, for any \u03b8 \u2208 \u0398, it must be possible to accurately learn \u03b8 from full (i.e. labeled) samples of \u00b5\u03b8. More specifically, there exists an unbiased estimator \u03b8\u0302(T ) that is a function of some dataset T of independent samples from \u00b5\u03b8 such that, for some c > 0 and for all \u03b8 \u2208 \u0398,\nCov ( \u03b8\u0302(T ) ) (2c |T |)\u22121I. (8)\nThird, for any two feasible models \u03b81 and \u03b82 \u2208 \u0398, E(\u039b1,Y1)\u223c\u00b5\u03b81 [ Var(\u039b2,Y2)\u223c\u00b5\u03b82 (Y2|\u039b1 = \u039b2) ] \u2264 cM\u22121. (9)\nThat is, we\u2019ll usually be reasonably sure in our guess for the value of Y , even if we guess using distribution \u00b5\u03b82 while the the labeling functions were actually sampled from (the possibly totally different) \u00b5\u03b81 . We can now prove the following result about the accuracy of our estimates.\nTheorem 2. Suppose that we run stochastic gradient descent to produce \u03b8\u0302 and w\u0302, and that our setup satisfies the conditions (5)-(9). Then for any > 0, if the input dataset S is large enough that\n|S | \u2265 2 c2 2\nlog ( 2 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 ) ,\nthen our expected parameter error and generalization risk can be bounded by E [\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u22252] \u2264 M 2 E [l(w\u0302) \u2212min\nw l(w)\n] \u2264 \u03c7 + c\n2\u03c1 .\nAs in the independent case, this shows that we need only |S | = O\u0303( \u22122) unlabeled training examples to achieve error O( ), which is the same asymptotic scaling as supervised learning methods. This suggests that while we pay a computational penalty for richer dependency structures, we are no less statistically efficient. In the appendix, we provide more details, including an explicit description of the algorithm and the step size used to achieve this result."}, {"heading": "5 Experiments", "text": "We seek to experimentally validate three claims about our approach. Our first claim is that data programming can be an effective paradigm for building high quality machine learning systems, and we test this across three real-world text relation extraction applications. Our second claim is that data programming can be used successfully in conjunction with automatic feature generation methods, such as LSTM models. Finally, our third claim is that data programming is an intuitive and productive framework for application domain-expert users, and we report on our initial experiences with a group of bioinformatics researchers.\nRelation Mention Extraction Tasks In the relation mention extraction task, our objects are relation mention candidates x = (e1, e2), which are pairs of entity mentions e1, e2 in unstructured text, and our goal is to learn a model that classifies each candidate as either a true textual assertion of the relation R(e1, e2) or not. We examine a news application from the 2014 TAC-KBP Slot Filling challenge2, where we extract relations between real-world entities from articles; a clinical genomics application, where we extract causal relations between genetic mutations and phenotypes from the scientific literature; and a pharmacogenomics application where we extract interactions between genes, also from the scientific literature.\nFor each application, we or our collaborators originally built a system where a ground truth training set was programmatically generated by ordering the labeling functions as a sequence of if-then-return statements, and for each candidate, taking the first label emitted by this script as the training label. We refer to this as the if-then-return (ITR) approach, and note that it often required significant domain expert development time to tune (weeks or more). For this set of experiments, we then used the same labeling function sets within the framework of data programming. In Table 1, we see that we achieve consistent improvements: on average by 2.34 points in F1 score, including what would have been a winning score on the 2014 TAC-KBP challenge [28].\nWe observed these performance gains across applications with very different labeling function sets. We describe the labeling function summary statistics\u2014coverage is the percentage of objects that had at least one label, overlap is the percentage of objects with more than one label, and conflict is the percentage of objects with conflicting labels\u2014and see in Table 2 that even in scenarios where m is small, and conflict and overlap is relatively less common, we still realize performance gains.\n2http://www.nist.gov/tac/2014/KBP/\nAdditionally, on a disease mention extraction task (see Usability Study), which was written from scratch within the data programming paradigm, we allowed developers to supply dependencies of the basic types outlined in Sec. 4, and report a 2.3 point F1 score boost from incorporating this dependency information, which we believe illustrates the potential of further pursuing this approach of providing dependency structure.\nAutomatically-generated Features We additionally compare both hand-tuned and automatically-generated features, where the latter are learned via an LSTM recurrent neural network (RNN) [12]. Conventional wisdom states that deep learning methods such as RNNs are prone to overfitting, thus rendering them ineffective over distantly-supervised training sets. In our experiments, however, we find that training them with the data programming may be effective, reporting a 9.79 point boost to precision and a 3.12 point F1 score improvement on the benchmark 2014 TAC-KBP (News) relation extraction task, over the baseline if-then-return approach. Additionally for comparison, our approach is a 5.98 point F1 score improvement over a state-of-the-art LSTM approach applied to the TAC-KBP task which was trained on hand-labeled data [30].\nUsability Study One of our hopes is that a non-ML-expert user will be more productive iterating on labeling functions than on features. To test this, we arranged a hackathon involving a handful of bioinformatics researchers. Their goal was to build a disease tagging system, a common and important challenge in the bioinformatics domain [10]. However, the hackathon did not have access to a labeled training set, and no feature engineering was performed. The entire effort was restricted to iterative labeling function development and the setup of candidates to be classified. In under eight hours, they had a dataset that was within 10 points of F1 of the supervised baseline; the gap was mainly due to recall issue in the candidate extraction phase. This suggests data programming may be a promising way to build high quality extractors, quickly."}, {"heading": "6 Conclusion and Future Work", "text": "We introduced data programming, a new approach to generating large labeled training sets. We demonstrate that our approach can be used with automatic feature generation techniques to achieve high quality results. For some relation extraction tasks, we provided anecdotal evidence that our methods may be easier for domain expert users to build on. We hope to explore the limits of our approach on more sophisticated machine learning tasks, notably in imaging and structured prediction.\nAcknowledgements Thanks to Theodoros Rekatsinas, Manas Joglekar, Henry Ehrenberg, Jason Fries, Percy Liang, the many helpful and intrepid DeepDive and DDLite users, and many others for their helpful conversations and feedback.\nWe gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA)SIMPLEX program under No. N66001-15-C-4043, the National Science Foundation (NSF) CAREER Award under No. IIS1353606, the Office of Naval Research (ONR) under awards No. N000141210041 and No. N000141310129, the Sloan Research Fellowship, the Moore Foundation, Toshiba, and Intel. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, NSF, ONR, or the U.S. government."}, {"heading": "A General Theoretical Results", "text": "In this section, we will state the full form of the theoretical results we alluded to in the body of the paper. First, we restate, in long form, our setup and assumptions.\nWe assume that, for some function h : {\u22121, 0, 1}m \u00d7 {\u22121, 1} 7\u2192 {\u22121, 0, 1}M of sufficient statistics, we are concerned with learning distributions, over the set \u2126 = {\u22121, 0, 1}m \u00d7 {\u22121, 1}, of the form\n\u03c0\u03b8(\u039b,Y) = 1 Z\u03b8 exp(\u03b8T h(\u039b,Y)), (10)\nwhere \u03b8 \u2208 RM is a parameter, and Z\u03b8 is the partition function that makes this a distribution. We assume that we are given, i.e. can derive from the data programming specification, some set \u0398 of feasible parameters. This set must have the following two properties.\nFirst, for any \u03b8 \u2208 \u0398, learning the parameter \u03b8 from (full) samples from \u03c0\u03b8 is possible, at least in some sense. More specifically, there exists an unbiased estimator \u03b8\u0302 that is a function of some number D samples from \u03c0\u03b8 (and is unbiased for all \u03b8 \u2208 \u0398) such that, for all \u03b8 \u2208 \u0398 and for some c > 0,\nCov ( \u03b8\u0302 ) I\n2cD . (11)\nSecond, for any \u03b81, \u03b82 \u2208 \u0398, E(\u03bb2,y2)\u223c\u03c0\u03b82 [ Var(\u03bb1,y1)\u223c\u03c0\u03b81 (y1|\u03bb1 = \u03bb2) ] \u2264 c\nM . (12)\nThat is, we\u2019ll always be reasonably certain in our guess for the value of y, even if we are totally wrong about the true parameter \u03b8.\nOn the other hand, we are also concerned with a distribution \u03c0\u2217 which ranges over the set X\u00d7 {\u22121, 1}, and represents the distribution of training and test examples we are using to learn. These objects are associated with a labeling function \u03bb : X 7\u2192 {\u22121, 0, 1}m and a feature function f : X 7\u2192 Rn. We make three assumptions about this distribution. First, we assume that, given (x, y) \u223c \u03c0\u2217, the class label y is independent of the features f (x) given the labels \u03bb(x). That is,\n(x, y) \u223c \u03c0\u2217 \u21d2 y \u22a5 f (x) | \u03bb(x). (13)\nSecond, we assume that we can describe the relationship between \u03bb(x) and y in terms of our family in (10) above. That is, for some parameter \u03b8\u2217 \u2208 \u0398,\nP(x,y)\u223c\u03c0\u2217 (\u03bb(x) = \u039b, y = Y) = \u03c0\u03b8\u2217 (\u039b,Y). (14)\nThird, we assume that the features themselves are bounded; for all x \u2208 X,\n\u2016 f (x)\u2016 \u2264 1. (15)\nOur goal is twofold. First, we want to recover some estimate \u03b8\u0302 of the true parameter \u03b8\u2217. Second, we want to produce a parameter w\u0302 that minimizes the regularized logistic loss\nl(w) = E(x,y)\u223c\u03c0\u2217 [ log(1 + exp(\u2212wT f (x)y)) ] + \u03c1 \u2016w\u20162 .\nWe actually accomplish this by minimizing a noise-aware loss function, given our recovered parameter \u03b8\u0302, l\u03b8\u0302(w) = E(x\u0304,y\u0304)\u223c\u03c0\u2217 [ E(\u039b,Y)\u223c\u03c0\u03b8\u0302 [ log(1 + exp(\u2212wT f (x\u0304)Y)) \u2223\u2223\u2223\u039b = \u03bb(x\u0304)]] + \u03c1 \u2016w\u20162 . In fact we can\u2019t even minimize this; rather, we will be minimizing the empirical noise-aware loss function, which is only this in expectation. Since the analysis of logistic regression is not itself interesting, we assume that we are able to run some algorithm that produces an estimate w\u0302 which satisfies, for some \u03c7 > 0,\nE [ l\u03b8\u0302(w\u0302) \u2212minw l\u03b8\u0302(w) \u2223\u2223\u2223\u2223\u2223\u03b8\u0302] \u2264 \u03c7. (16) The algorithm chosen can be anything, but in practice, we use stochastic gradient descent.\nWe learn \u03b8\u0302 and w\u0302 by running the following algorithm. Under these assumptions, we are able to prove the following theorem about the behavior of Algorithm 1.\nAlgorithm 1 Data Programming Require: Step size \u03b7, dataset S \u2282 X, and initial parameter \u03b80 \u2208 \u0398. \u03b8 \u2192 \u03b80 for all x \u2208 S do\nIndependently sample (\u039b,Y) from \u03c0\u03b8, and (\u039b\u0304, Y\u0304) from \u03c0\u03b8 conditionally given \u039b = \u03bb(x). \u03b8 \u2190 \u03b8 + \u03b7(h(\u039b,Y) \u2212 h(\u039b\u0304, Y\u0304)). \u03b8 = P\u0398(\u03b8) . Here, P\u0398 denotes orthogonal projection onto \u0398.\nend for Compute w\u0302 using the algorithm described in (15) return (\u03b8, w\u0302).\nTheorem A.1. Suppose that we run Algorithm 1 on a data programming specification that satisfies conditions (11), (12), (13), (14), (15), and (16). Suppose further that, for some parameter > 0, we use step size\n\u03b7 = c 2\n4\nand our dataset is of a size that satisfies\n|S | = 2 c2 2\nlog ( 2 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 ) .\nThen, we can bound the expected parameter error with E [\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u22252] \u2264 2M\nand the expected risk with\nE [ l(w\u0302) \u2212min\nw l(w)\n] \u2264 \u03c7 + c\n2\u03c1 .\nThis theorem\u2019s conclusions and assumptions can readily be seen to be identical to those of Theorem 2 in the main body of the paper, except that they apply to the slightly more general case of arbitrary h, rather than h of the explicit form described in the body. Therefore, in order to prove Theorem 2, it suffices to prove Theorem A.1, which we will do in Section C."}, {"heading": "B Theoretical Results for Independent Model", "text": "For the independent model, we can obtain a more specific version of Theorem A.1. In the independent model, the variables are, as before, \u039b \u2208 {\u22121, 0, 1}m and Y \u2208 {\u22121, 1}. The sufficient statistics are \u039biY and \u039b2i .\nTo produce results that make intuitive sense, we also define the alternate parameterization\nP\u03c0 (\u039bi|Y) =  \u03b2i 1+\u03b3i 2 \u039bi = Y (1 \u2212 \u03b2i) \u039b = 0 \u03b2i 1\u2212\u03b3i 2 \u039bi = \u2212Y .\nIn comparison to the parameters used in the body of the paper, we have\n\u03b1i = 1 + \u03b3i\n2 .\nNow, we are concerned with models that are feasible. For a model to be feasible (i.e. for \u03b8 \u2208 \u0398), we require that it satisfy, for some constants \u03b3min > 0, \u03b3max > 0, and \u03b2min,\n\u03b3min \u2264 \u03b3i \u2264 \u03b3max \u03b2min \u2264 \u03b2i \u2264 1 2 .\nFor 0 \u2264 \u03b2 \u2264 1 and \u22121 \u2264 \u03b3 \u2264 1. For this model, we can prove the following corollary to Theorem A.1\nCorollary B.1. Suppose that we run Algorithm 1 on an independent data programming specification that satisfies conditions (13), (14), (15), and (16). Furthermore, assume that the number of labeling functions we use satisfies\nm \u2265 9.34 artanh(\u03b3max) (\u03b3\u03b2)min\u03b32min\nlog (\n24m \u03b2min\n) .\nSuppose further that, for some parameter > 0, we use step size\n\u03b7 = \u03b2min\n2\n16\nand our dataset is of a size that satisfies\n|S | = 32 \u03b22min 2 log\n( 2 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 ) .\nThen, we can bound the expected parameter error with E [\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u22252] \u2264 2M\nand the expected risk with\nE [ l(w\u0302) \u2212min\nw l(w)\n] \u2264 \u03c7 + \u03b2min\n8\u03c1 .\nWe can see that if, as stated in the body of the paper, \u03b2i \u2265 0.3 and 0.8 \u2264 \u03b1i \u2264 0.9 (which is equivalent to 0.6 \u2264 \u03b3i \u2264 0.8), then\n2000 \u2265 1896.13 = 9.34 artanh(0.8) 0.3 \u00b7 0.63 log\n( 24 \u00b7 2000\n0.3\n) .\nThis means that, as stated in the paper, m = 2000 is sufficient for this corollary to hold with\n|S | = 32 0.32 \u00b7 2 log\n( 2m(artanh(0.8) \u2212 artanh(0.6))2 ) =\n356 2 log ( m 3 ) .\nThus, proving Corollary B.1 is sufficient to prove Theorem 1 from the body of the paper. We will prove Corollary B.1 in Section E"}, {"heading": "C Proof of Theorem A.1", "text": "First, we state some lemmas that will be useful in the proof to come.\nLemma D.1. Given a family of maximum-entropy distributions\n\u03c0\u03b8(x) = 1 Z\u03b8 exp(\u03b8T h(x)),\nfor some function of sufficient statistics h : \u2126 7\u2192 RM , if we let J : RM 7\u2192 R be the maximum log-likelihood objective for some event A \u2286 \u2126, J(\u03b8) = log Px\u223c\u03c0\u03b8 (x \u2208 A) , then its gradient is \u2207J(\u03b8) = Ex\u223c\u03c0\u03b8 [h(x)|x \u2208 A] \u2212 Ex\u223c\u03c0\u03b8 [h(x)] and its Hessian is\n\u22072J(\u03b8) = Covx\u223c\u03c0\u03b8 (h(x)|x \u2208 A) \u2212 Covx\u223c\u03c0\u03b8 (h(x)) .\nLemma D.2. Suppose that we are looking at a distribution from a data programming label model. That is, our maximum-entropy distribution can now be written in terms of two variables, the labeling function values \u03bb \u2208 {\u22121, 0, 1} and the class y \u2208 {\u22121, 1}, as\n\u03c0\u03b8(\u03bb, y) = 1 Z\u03b8 exp(\u03b8T h(\u03bb, y)),\nwhere we assume without loss of generality that for some M, h(\u03bb, y) \u2208 RM and \u2016h(\u03bb, y)\u2016\u221e \u2264 1. If we let J : RM 7\u2192 R be the maximum expected log-likelihood objective, under another distribution \u03c0\u2217, for the event associated with the observed labeling function values \u03bb,\nJ(\u03b8) = E(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ log P(\u03bb,y)\u223c\u03c0\u03b8 (\u03bb = \u03bb \u2217) ] ,\nthen its Hessian can be bounded with \u22072J(\u03b8) MIE(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ Var(\u03bb,y)\u223c\u03c0\u03b8 (y|\u03bb = \u03bb\u2217) ] \u2212 I(\u03b8),\nwhere I(\u03b8) is the Fisher information.\nLemma D.3. Suppose that we are looking at a data programming distribution, as described in the text of Lemma D.2. Suppose further that we are concerned with some feasible set of parameters \u0398 \u2282 RM , such that the any model with parameters in this space satisfies the following two conditions.\nFirst, for any \u03b8 \u2208 \u0398, learning the parameter \u03b8 from (full) samples from \u03c0\u03b8 is possible, at least in some sense. More specifically, there exists an unbiased estimator \u03b8\u0302 that is a function of some number D samples from \u03c0\u03b8 (and is unbiased for all \u03b8 \u2208 \u0398) such that, for all \u03b8 \u2208 \u0398 and for some c > 0,\nCov ( \u03b8\u0302 ) I\n2cD .\nSecond, for any \u03b8, \u03b8\u2217 \u2208 \u0398, E(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ Var(\u03bb,y)\u223c\u03c0\u03b8 (y|\u03bb = \u03bb\u2217) ] \u2264 c\nM .\nThat is, we\u2019ll always be reasonably certain in our guess for the value of y, even if we are totally wrong about the true parameter \u03b8\u2217.\nUnder these conditions, the function J is strongly concave on \u0398 with parameter of strong convexity c.\nLemma D.4. Suppose that we are looking at a data programming maximum likelihood estimation problem, as described in the text of Lemma D.2. Suppose further that the objective function J is strongly concave with parameter c > 0.\nIf we run stochastic gradient descent on objective J, using unbiased samples from a true distribution \u03c0\u03b8\u2217 , where \u03b8\u2217 \u2208 \u0398, then if we use step size\n\u03b7 = c 2\n4 and run (using a fresh sample at each iteration) for T steps, where\nT = 2\nc2 2 log\n( 2 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 ) then we can bound the expected parameter estimation error with\nE [\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u22252] \u2264 2M.\nLemma D.5. Assume in our model that, without loss of generality, \u2016 f (x)\u2016 \u2264 1 for all x, and that in our true model \u03c0\u2217, the class y is independent of the features f (x) given the labels \u03bb(x).\nSuppose that we now want to solve the expected loss minimization problem wherein we minimize the objective l(w) = E(x,y)\u223c\u03c0\u2217 [ log(1 + exp(\u2212wT f (x)y)) ] + \u03c1 \u2016w\u20162 .\nWe actually accomplish this by minimizing our noise-aware loss function, given our chosen parameter \u03b8\u0302, l\u03b8\u0302(w) = E(x\u0304,y\u0304)\u223c\u03c0\u2217 [ E(\u039b,Y)\u223c\u03c0\u03b8\u0302 [ log(1 + exp(\u2212wT f (x\u0304)Y)) \u2223\u2223\u2223\u039b = \u03bb(x\u0304)]] + \u03c1 \u2016w\u20162 .\nIn fact we can\u2019t even minimize this; rather, we will be minimizing the empirical noise-aware loss function, which is only this in expectation. Suppose that doing so produces an estimate w\u0302 which satisfies, for some \u03c7 > 0,\nE [ l\u03b8\u0302(w\u0302) \u2212minw l\u03b8\u0302(w) \u2223\u2223\u2223\u2223\u2223\u03b8\u0302] \u2264 \u03c7. (Here, the expectation is taken with respect to only the random variable w\u0302.) Then, we can bound the expected risk with\nE [ l(w\u0302) \u2212min\nw l(w)\n] \u2264 \u03c7 + c\n2\u03c1 .\nNow, we restate and prove our main theorem.\nTheorem A.1. Suppose that we run Algorithm 1 on a data programming specification that satisfies conditions (11), (12), (13), (14), (15), and (16). Suppose further that, for some parameter > 0, we use step size\n\u03b7 = c 2\n4 and our dataset is of a size that satisfies\n|S | = 2 c2 2\nlog ( 2 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 ) .\nThen, we can bound the expected parameter error with E [\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u22252] \u2264 2M\nand the expected risk with\nE [ l(w\u0302) \u2212min\nw l(w)\n] \u2264 \u03c7 + c\n2\u03c1 .\nProof. The bounds on the expected parameter estimation error follow directly from Lemma D.4, and the remainder of the theorem follows directly from Lemma D.5."}, {"heading": "D Proofs of Lemmas", "text": "Lemma D.1. Given a family of maximum-entropy distributions\n\u03c0\u03b8(x) = 1 Z\u03b8 exp(\u03b8T h(x)),\nfor some function of sufficient statistics h : \u2126 7\u2192 RM , if we let J : RM 7\u2192 R be the maximum log-likelihood objective for some event A \u2286 \u2126, J(\u03b8) = log Px\u223c\u03c0\u03b8 (x \u2208 A) , then its gradient is \u2207J(\u03b8) = Ex\u223c\u03c0\u03b8 [h(x)|x \u2208 A] \u2212 Ex\u223c\u03c0\u03b8 [h(x)] and its Hessian is \u22072J(\u03b8) = Covx\u223c\u03c0\u03b8 (h(x)|x \u2208 A) \u2212 Covx\u223c\u03c0\u03b8 (h(x)) . Proof. For the gradient,\n\u2207J(\u03b8) = \u2207 log P\u03c0\u03b8 (A) = \u2207 log (\u2211\nx\u2208A exp(\u03b8T h(x))\u2211 x\u2208\u2126 exp(\u03b8T h(x)) ) = \u2207 log\n\u2211 x\u2208A exp(\u03b8T h(x))  \u2212 \u2207 log \u2211 x\u2208\u2126 exp(\u03b8T h(x))  = \u2211 x\u2208A h(x) exp(\u03b8T h(x))\u2211\nx\u2208A exp(\u03b8T h(x)) \u2212\n\u2211 x\u2208\u2126 h(x) exp(\u03b8T h(x))\u2211\nx\u2208\u2126 exp(\u03b8T h(x)) = Ex\u223c\u03c0\u03b8 [h(x)|x \u2208 A] \u2212 Ex\u223c\u03c0\u03b8 [h(x)] .\nAnd for the Hessian, \u22072J(\u03b8) = \u2207 \u2211\nx\u2208A h(x) exp(\u03b8T h(x))\u2211 x\u2208A exp(\u03b8T h(x))\n\u2212 \u2207 \u2211\nx\u2208\u2126 h(x) exp(\u03b8T h(x))\u2211 x\u2208\u2126 exp(\u03b8T h(x))\n= \u2211 x\u2208A h(x)h(x)T exp(\u03b8T h(x))\u2211\nx\u2208A exp(\u03b8T h(x)) \u2212\n(\u2211 x\u2208A h(x) exp(\u03b8T h(x)) ) (\u2211 x\u2208A h(x) exp(\u03b8T h(x)) )T(\u2211 x\u2208A exp(\u03b8T h(x))\n)2 \u2212  \u2211 x\u2208\u2126 h(x)h(x)T exp(\u03b8T h(x))\u2211 x\u2208\u2126 exp(\u03b8T h(x)) \u2212 (\u2211 x\u2208\u2126 h(x) exp(\u03b8T h(x)) ) (\u2211 x\u2208\u2126 h(x) exp(\u03b8T h(x)) )T(\u2211 x\u2208\u2126 exp(\u03b8T h(x)) )2 \n= Ex\u223c\u03c0\u03b8 [ h(x)h(x)T \u2223\u2223\u2223x \u2208 A] \u2212 Ex\u223c\u03c0\u03b8 [h(x)|x \u2208 A] Ex\u223c\u03c0\u03b8 [h(x)|x \u2208 A]T \u2212 ( Ex\u223c\u03c0\u03b8 [ h(x)h(x)T ] \u2212 Ex\u223c\u03c0\u03b8 [h(x)] Ex\u223c\u03c0\u03b8 [h(x)]T\n) = Covx\u223c\u03c0\u03b8 (h(x)|x \u2208 A) \u2212 Covx\u223c\u03c0\u03b8 (h(x)) .\nLemma D.2. Suppose that we are looking at a distribution from a data programming label model. That is, our maximum-entropy distribution can now be written in terms of two variables, the labeling function values \u03bb \u2208 {\u22121, 0, 1} and the class y \u2208 {\u22121, 1}, as\n\u03c0\u03b8(\u03bb, y) = 1 Z\u03b8 exp(\u03b8T h(\u03bb, y)),\nwhere we assume without loss of generality that for some M, h(\u03bb, y) \u2208 RM and \u2016h(\u03bb, y)\u2016\u221e \u2264 1. If we let J : RM 7\u2192 R be the maximum expected log-likelihood objective, under another distribution \u03c0\u2217, for the event associated with the observed labeling function values \u03bb,\nJ(\u03b8) = E(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ log P(\u03bb,y)\u223c\u03c0\u03b8 (\u03bb = \u03bb \u2217) ] ,\nthen its Hessian can be bounded with \u22072J(\u03b8) MIE(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ Var(\u03bb,y)\u223c\u03c0\u03b8 (y|\u03bb = \u03bb\u2217) ] \u2212 I(\u03b8),\nwhere I(\u03b8) is the Fisher information. Proof. From the result of Lemma D.1, we have that\n\u22072J(\u03b8) = E(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ Cov(\u03bb,y)\u223c\u03c0\u03b8 (h(\u03bb, y)|\u03bb = \u03bb\u2217) ] \u2212 Cov(\u03bb,y)\u223c\u03c0\u03b8 (h(\u03bb, y)) . (17)\nWe start byu defining h0(\u03bb) and h1(\u03bb) such that\nh(\u03bb, y) = h(\u03bb, 1) 1 + y 2 + h(\u03bb,\u22121)1 \u2212 y 2 = h(\u03bb, 1) + h(\u03bb,\u22121) 2 + y h(\u03bb, 1) \u2212 h(\u03bb,\u22121) 2 = h0(\u03bb) + yh1(\u03bb).\nThis allows us to reduce (17) to \u22072J(\u03b8) = E(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ h1(\u03bb\u2217)h1(\u03bb\u2217)T Var(\u03bb,y)\u223c\u03c0\u03b8 (y|\u03bb = \u03bb\u2217) ] \u2212 Cov(\u03bb,y)\u223c\u03c0\u03b8 (h(\u03bb, y)) .\nOn the other hand, the Fisher information of this model at \u03b8 is I(\u03b8) = E [(\u2207\u03b8 log \u03c0\u03b8(x))2]\n= E (\u2207\u03b8 log ( exp(\u03b8T h(x))\u2211\nz\u2208\u2126 exp(\u03b8T h(z)) ))2 = E  \u2207\u03b8 log (exp(\u03b8T h(x))) \u2212 \u2207\u03b8 log \u2211 z\u2208\u2126 exp(\u03b8T h(z))  2\n = E\n(h(x) \u2212 \u2211z\u2208\u2126 h(z) exp(\u03b8T h(z))\u2211 z\u2208\u2126 exp(\u03b8T h(z)) )2 = E [ (h(x) \u2212 E [h(z)])2\n] = Cov (h(x)) .\nTherefore, we can write the second derivative of J as \u22072J(\u03b8) = E(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ h1(\u03bb\u2217)h1(\u03bb\u2217)T Var(\u03bb,y)\u223c\u03c0\u03b8 (y|\u03bb = \u03bb\u2217) ] \u2212 I(\u03b8).\nIf we apply the fact that h1(\u03bb\u2217)h1(\u03bb\u2217)T I \u2016h1(\u03bb\u2217)\u20162 MI \u2016h1(\u03bb\u2217)\u20162\u221e MI,\nthen we can reduce this to \u22072J(\u03b8) MIE(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ Var(\u03bb,y)\u223c\u03c0\u03b8 (y|\u03bb = \u03bb\u2217) ] \u2212 I(\u03b8).\nThis is the desired result.\nLemma D.3. Suppose that we are looking at a data programming distribution, as described in the text of Lemma D.2. Suppose further that we are concerned with some feasible set of parameters \u0398 \u2282 RM , such that the any model with parameters in this space satisfies the following two conditions.\nFirst, for any \u03b8 \u2208 \u0398, learning the parameter \u03b8 from (full) samples from \u03c0\u03b8 is possible, at least in some sense. More specifically, there exists an unbiased estimator \u03b8\u0302 that is a function of some number D samples from \u03c0\u03b8 (and is unbiased for all \u03b8 \u2208 \u0398) such that, for all \u03b8 \u2208 \u0398 and for some c > 0,\nCov ( \u03b8\u0302 ) I\n2cD .\nSecond, for any \u03b8, \u03b8\u2217 \u2208 \u0398, E(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ Var(\u03bb,y)\u223c\u03c0\u03b8 (y|\u03bb = \u03bb\u2217) ] \u2264 c\nM .\nThat is, we\u2019ll always be reasonably certain in our guess for the value of y, even if we are totally wrong about the true parameter \u03b8\u2217.\nUnder these conditions, the function J is strongly concave on \u0398 with parameter of strong convexity c.\nProof. From the Cram\u00e9r-Rao bound, we know in general that the variance of any unbiased estimator is bounded by the reciprocal of the Fisher information Cov ( \u03b8\u0302 ) (I(\u03b8))\u22121 .\nSince for the estimator described in the lemma statement, we have D independent samples from the distribution, it follows that the Fisher information of this experiment is D times the Fisher information of a single sample. Combining this with the bound in the lemma statement on the covariance, we get\nI 2cD\nCov ( \u03b8\u0302 ) (DI(\u03b8))\u22121 .\nIt follows that I(\u03b8) 2cI.\nOn the other hand, also from the lemma statement, we can conclude that MIE(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ Var(\u03bb,y)\u223c\u03c0\u03b8 (y|\u03bb = \u03bb\u2217) ] cI.\nTherefore, for all \u03b8 \u2208 \u0398, \u22072J(\u03b8) MIE(\u03bb\u2217,y\u2217)\u223c\u03c0\u2217 [ Var(\u03bb,y)\u223c\u03c0\u03b8 (y|\u03bb = \u03bb\u2217) ] \u2212 I(\u03b8) \u2212cI.\nThis implies that J is strongly concave over \u0398, with constant c, as desired.\nLemma D.4. Suppose that we are looking at a data programming maximum likelihood estimation problem, as described in the text of Lemma D.2. Suppose further that the objective function J is strongly concave with parameter c > 0.\nIf we run stochastic gradient descent on objective J, using unbiased samples from a true distribution \u03c0\u03b8\u2217 , where \u03b8\u2217 \u2208 \u0398, then if we use step size\n\u03b7 = c 2\n4\nand run (using a fresh sample at each iteration) for T steps, where\nT = 2\nc2 2 log\n( 2 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 ) then we can bound the expected parameter estimation error with\nE [\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u22252] \u2264 2M.\nProof. First, we note that, in the proof to follow, we can ignore the projection onto the feasible set \u0398, since this projection always takes us closer to the optimum \u03b8\u2217.\nIf we track the expected distance to the optimum \u03b8\u2217, then at the next timestep,\n\u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 = \u2016\u03b8t \u2212 \u03b8\u2217\u20162 + 2\u03b3(\u03b8t \u2212 \u03b8\u2217)\u2207J\u0303(\u03b8t) + \u03b32 \u2225\u2225\u2225\u2207J\u0303t(\u03b8t)\u2225\u2225\u22252 .\nSince we can write our stochastic samples in the form\n\u2207J\u0303t(\u03b8t) = h(\u03bbt, yt) \u2212 h(\u03bb\u0304t, y\u0304t),\nfor some samples \u03bbt, yt, \u03bb\u0304t, and y\u0304t, we can conclude that\u2225\u2225\u2225\u2207J\u0303t(\u03b8t)\u2225\u2225\u22252 \u2264 M \u2225\u2225\u2225\u2207J\u0303t(\u03b8t)\u2225\u2225\u22252\u221e \u2264 4M. Therefore, taking the expected value conditioned on the filtration,\nE [ \u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 \u2223\u2223\u2223Ft] = \u2016\u03b8t \u2212 \u03b8\u2217\u20162 + 2\u03b3(\u03b8t \u2212 \u03b8\u2217)\u2207J(\u03b8t) + 4\u03b32M. Since J is strongly concave, (\u03b8t \u2212 \u03b8\u2217)\u2207J(\u03b8t) \u2264 \u2212c \u2016\u03b8t \u2212 \u03b8\u2217\u20162 ; and so,\nE [ \u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 \u2223\u2223\u2223Ft] \u2264 (1 \u2212 2\u03b3c) \u2016\u03b8t \u2212 \u03b8\u2217\u20162 + 4\u03b32M. If we take the full expectation and subtract the fixed point from both sides,\nE [ \u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 ] \u2212 2\u03b3M\nc \u2264 (1 \u2212 2\u03b3c)E\n[ \u2016\u03b8t \u2212 \u03b8\u2217\u20162 ] + 4\u03b32M \u2212 2\u03b3M\nc = (1 \u2212 2\u03b3c)\n( E [ \u2016\u03b8t \u2212 \u03b8\u2217\u20162 ] \u2212 2\u03b3M\nc\n) .\nTherefore,\nE [ \u2016\u03b8t \u2212 \u03b8\u2217\u20162 ] \u2212 2\u03b3M\nc \u2264 (1 \u2212 2\u03b3c)t\n( \u2016\u03b80 \u2212 \u03b8\u2217\u20162 \u2212\n2\u03b3M c\n) ,\nand so E [ \u2016\u03b8t \u2212 \u03b8\u2217\u20162 ] \u2264 exp(\u22122\u03b3ct) \u2016\u03b80 \u2212 \u03b8\u2217\u20162 +\n2\u03b3M c .\nIn order to ensure that E [ \u2016\u03b8t \u2212 \u03b8\u2217\u20162 ] \u2264 2,\nit therefore suffices to pick\n\u03b3 = c 2\n4M and\nt = 2M c2 2\nlog ( 2 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 ) .\nSubstituting 2 \u2192 2M produces the desired result.\nLemma D.5. Assume in our model that, without loss of generality, \u2016 f (x)\u2016 \u2264 1 for all x, and that in our true model \u03c0\u2217, the class y is independent of the features f (x) given the labels \u03bb(x).\nSuppose that we now want to solve the expected loss minimization problem wherein we minimize the objective l(w) = E(x,y)\u223c\u03c0\u2217 [ log(1 + exp(\u2212wT f (x)y)) ] + \u03c1 \u2016w\u20162 .\nWe actually accomplish this by minimizing our noise-aware loss function, given our chosen parameter \u03b8\u0302, l\u03b8\u0302(w) = E(x\u0304,y\u0304)\u223c\u03c0\u2217 [ E(\u039b,Y)\u223c\u03c0\u03b8\u0302 [ log(1 + exp(\u2212wT f (x\u0304)Y)) \u2223\u2223\u2223\u039b = \u03bb(x\u0304)]] + \u03c1 \u2016w\u20162 . In fact we can\u2019t even minimize this; rather, we will be minimizing the empirical noise-aware loss function, which is only this in expectation. Suppose that doing so produces an estimate w\u0302 which satisfies, for some \u03c7 > 0,\nE [ l\u03b8\u0302(w\u0302) \u2212minw l\u03b8\u0302(w) \u2223\u2223\u2223\u2223\u2223\u03b8\u0302] \u2264 \u03c7. (Here, the expectation is taken with respect to only the random variable w\u0302.) Then, we can bound the expected risk with\nE [ l(w\u0302) \u2212min\nw l(w)\n] \u2264 \u03c7 + c\n2\u03c1 .\nProof. (To simplify the symbols in this proof, we freely use \u03b8 when we mean \u03b8\u0302.) The loss function we want to minimize is, in expectation,\nl(w) = E(x,y)\u223c\u03c0\u2217 [ log(1 + exp(\u2212wT f (x)y)) ] + \u03c1 \u2016w\u20162 .\nBy the law of total expectation, l(w) = E(x\u0304,y\u0304)\u223c\u03c0\u2217 [ E(x,y)\u223c\u03c0\u2217 [ log(1 + exp(\u2212wT f (x\u0304)y)) \u2223\u2223\u2223x = x\u0304]] + \u03c1 \u2016w\u20162 , and by our conditional independence assumption,\nl(w) = E(x\u0304,y\u0304)\u223c\u03c0\u2217 [ E(x,y)\u223c\u03c0\u2217 [ log(1 + exp(\u2212wT f (x\u0304)y)) \u2223\u2223\u2223\u03bb(x) = \u03bb(x\u0304)]] + \u03c1 \u2016w\u20162 . Since we know from our assumptions that, for the optimum parameter \u03b8\u2217,\nP(x,y)\u223c\u03c0\u2217 (\u03bb(x) = \u039b, y = Y) = P(\u03bb,y)\u223c\u03c0\u03b8\u2217 (\u03bb = \u039b, y = Y) ,\nwe can rewrite this as l(w) = E(x\u0304,y\u0304)\u223c\u03c0\u2217 [ E(\u039b,Y)\u223c\u03c0\u03b8\u2217 [ log(1 + exp(\u2212wT f (x\u0304)Y)) \u2223\u2223\u2223\u039b = \u03bb(x\u0304)]] + \u03c1 \u2016w\u20162 . On the other hand, if we are minimizing the model we got from the previous step, we will be actually minimizing\nl\u03b8(w) = E(x\u0304,y\u0304)\u223c\u03c0\u2217 [ E(\u039b,Y)\u223c\u03c0\u03b8 [ log(1 + exp(\u2212wT f (x\u0304)Y)) \u2223\u2223\u2223\u039b = \u03bb(x\u0304)]] + \u03c1 \u2016w\u20162 . We can reduce this further by noticing that\nE(\u039b,Y)\u223c\u03c0\u03b8 [ log(1 + exp(\u2212wT f (x\u0304)Y)) \u2223\u2223\u2223\u039b = \u03bb(x\u0304)] = E(\u039b,Y)\u223c\u03c0\u03b8 [ log(1 + exp(\u2212wT f (x\u0304)))1 + Y\n2 + log(1 + exp(wT f (x\u0304))) 1 \u2212 Y 2 \u2223\u2223\u2223\u2223\u2223\u039b = \u03bb(x\u0304)] =\nlog(1 + exp(\u2212wT f (x\u0304))) + log(1 + exp(wT f (x\u0304))) 2\n+ log(1 + exp(\u2212wT f (x\u0304))) \u2212 log(1 + exp(wT f (x\u0304)))\n2 E(\u039b,Y)\u223c\u03c0\u03b8 [Y |\u039b = \u03bb(x\u0304)]\n= log(1 + exp(\u2212wT f (x\u0304))) + log(1 + exp(wT f (x\u0304)))\n2\n\u2212 w T f (x\u0304)\n2 E(\u039b,Y)\u223c\u03c0\u03b8 [Y |\u039b = \u03bb(x\u0304)] .\nIt follows that the difference between the loss functions will be |l(w) \u2212 l\u03b8(w)| = \u2223\u2223\u2223\u2223\u2223\u2223E(x\u0304,y\u0304)\u223c\u03c0\u2217 [ wT f (x\u0304) 2 ( E(\u039b,Y)\u223c\u03c0\u03b8 [Y |\u039b = \u03bb(x\u0304)] \u2212 E(\u039b,Y)\u223c\u03c0\u03b8\u2217 [Y |\u039b = \u03bb(x\u0304)] )]\u2223\u2223\u2223\u2223\u2223\u2223 . Now, we can compute that\n\u2207\u03b8E(\u039b,Y)\u223c\u03c0\u03b8 [Y |\u039b = \u03bb] = \u2207\u03b8 exp(\u03b8T h(\u03bb, 1)) \u2212 exp(\u03b8T h(\u03bb,\u22121)) exp(\u03b8T h(\u03bb, 1)) + exp(\u03b8T h(\u03bb,\u22121))\n= \u2207\u03b8 exp(\u03b8T h1(\u03bb)) \u2212 exp(\u2212\u03b8T h1(\u03bb)) exp(\u03b8T h1(\u03bb)) + exp(\u03b8T h1(\u03bb)) = \u2207\u03b8 tanh(\u03b8T h1(\u03bb)) = h1(\u03bb) ( 1 \u2212 tanh2(\u03b8T h1(\u03bb))\n) = h1(\u03bb)Var(\u039b,Y)\u223c\u03c0\u03b8 (Y |\u039b = \u03bb) .\nIt follows by the mean value theorem that for some \u03c8, a linear combination of \u03b8 and \u03b8\u2217,\n|l(w) \u2212 l\u03b8(w)| = \u2223\u2223\u2223\u2223\u2223\u2223E(x\u0304,y\u0304)\u223c\u03c0\u2217 [ wT f (x\u0304) 2 (\u03b8 \u2212 \u03b8\u2217)T h1(\u03bb)Var(\u039b,Y)\u223c\u03c0\u03c8 (Y |\u039b = \u03bb) ]\u2223\u2223\u2223\u2223\u2223\u2223 . Since \u0398 is convex, clearly \u03c8 \u2208 \u0398. From our assumption on the bound of the variance, we can conclude that\nE(x\u0304,y\u0304)\u223c\u03c0\u2217 [ Var(\u039b,Y)\u223c\u03c0\u03c8 (Y |\u039b = \u03bb) ] \u2264 c\nM .\nBy the Cauchy-Schwarz inequality,\n|l(w) \u2212 l\u03b8(w)| \u2264 1 2 \u2223\u2223\u2223\u2223E(x\u0304,y\u0304)\u223c\u03c0\u2217 [\u2016w\u2016 \u2016 f (x\u0304)\u2016 \u2016\u03b8 \u2212 \u03b8\u2217\u2016 \u2016h1(\u03bb)\u2016Var(\u039b,Y)\u223c\u03c0\u03c8 (Y |\u039b = \u03bb)]\u2223\u2223\u2223\u2223 . Since (by assumption) \u2016 f (x)\u2016 \u2264 1 and \u2016h1(\u03bb)\u2016 \u2264 \u221a M,\n|l(w) \u2212 l\u03b8(w)| \u2264 \u2016w\u2016 \u2016\u03b8 \u2212 \u03b8\u2217\u2016\n\u221a M\n2 \u2223\u2223\u2223\u2223E(x\u0304,y\u0304)\u223c\u03c0\u2217 [Var(\u039b,Y)\u223c\u03c0\u03c8 (Y |\u039b = \u03bb)]\u2223\u2223\u2223\u2223 \u2264 \u2016w\u2016 \u2016\u03b8 \u2212 \u03b8 \u2217\u2016 \u221a\nM 2 \u00b7 c M\n= c \u2016w\u2016 \u2016\u03b8 \u2212 \u03b8\u2217\u2016\n2 \u221a M .\nNow, for any w that could conceivably be a solution, it must be the case that\n\u2016w\u2016 \u2264 1 2\u03c1 ,\nsince otherwise the regularization term would be too large Therefore, for any possible solution w,\n|l(w) \u2212 l\u03b8(w)| \u2264 c \u2016\u03b8 \u2212 \u03b8\u2217\u2016 4\u03c1 \u221a M .\nNow, we apply the assumption that we are able to solve the empirical problem, producing an estimate w\u0302 that satisfies\nE [ l\u03b8(w\u0302) \u2212 l\u03b8(w\u2217\u03b8) ] \u2264 \u03c7, where w\u2217\u03b8 is the true solution to\nw\u2217\u03b8 = arg minw l\u03b8(w).\nTherefore,\nE [ l(w\u0302) \u2212 l(w\u2217)] = E [l\u03b8(w\u0302) \u2212 l\u03b8(w\u2217\u03b8) + l\u03b8(w\u2217\u03b8) \u2212 l\u03b8(w\u0302) + l(w\u0302) \u2212 l(w\u2217)]\n\u2264 \u03c7 + E [l\u03b8(w\u2217) \u2212 l\u03b8(w\u0302) + l(w\u0302) \u2212 l(w\u2217)] \u2264 \u03c7 + E [|l\u03b8(w\u2217) \u2212 l(w\u2217)| + |l\u03b8(w\u0302) \u2212 l(w\u0302)|] \u2264 \u03c7 + E\nc \u2016\u03b8 \u2212 \u03b8\u2217\u2016 2\u03c1 \u221a M  = \u03c7 + c\n2\u03c1 \u221a M E [\u2016\u03b8 \u2212 \u03b8\u2217\u2016] \u2264 \u03c7 + c\n2\u03c1 \u221a M\n\u221a E [ \u2016\u03b8 \u2212 \u03b8\u2217\u20162 ] .\nWe can now bound this using the result of Lemma D.4, which results in\nE [ l(w\u0302) \u2212 l(w\u2217)] \u2264 \u03c7 + c\n2\u03c1 \u221a M\n\u221a M 2\n= \u03c7 + c 2\u03c1 .\nThis is the desired result."}, {"heading": "E Proofs of Results for the Independent Model", "text": "To restate, in the independent model, the variables are, as before, \u039b \u2208 {\u22121, 0, 1}m and Y \u2208 {\u22121, 1}. The sufficient statistics are \u039biY and \u039b2i . That is, for expanded parameter \u03b8 = (\u03c8, \u03c6),\n\u03c0\u03b8(\u039b,Y) = 1 Z exp(\u03c8T \u039bY + \u03c6T \u039b2).\nThis can be combined with the simple assumption that P (Y) = 12 to complete a whole distribution. Using this, we can prove the following simple result about the moments of the sufficient statistics.\nLemma E.1. The expected values and covariances of the sufficient statistics are, for all i , j,\nE [\u039biY] = \u03b2i\u03b3i E [ \u039b2i ] = \u03b2i\nVar (\u039biY) = \u03b2i \u2212 \u03b22i \u03b32i Var ( \u039b2i ) = \u03b2i \u2212 \u03b22i\nCov ( \u039biY,\u039b jY ) = 0\nCov ( \u039b2i ,\u039b 2 j ) = 0\nCov ( \u039biY,\u039b2j ) = 0.\nWe also prove the following basic lemma that relates \u03c8i to \u03b3i.\nLemma E.2. It holds that \u03b3i = tanh(\u03c8i).\nWe also make the following claim about feasible models.\nLemma E.3. For any feasible model, it will be the case that, for any other feasible parameter vector \u03c8\u0302,\nP ( \u03c8\u0302T \u039bY \u2264 m\n2 \u03b3min(\u03b3\u03b2)min\n) \u2264 exp \u2212 m(\u03b3\u03b2)min\u03b32min9.34 artanh(\u03b3max)  .\nWe can also prove the following simple result about the conditional covariances\nLemma E.4. The covariances of the sufficient statistics, conditioned on \u039b, are for all i , j, Cov ( \u039biY,\u039b jY \u2223\u2223\u2223\u039b) = \u039bi\u039b j sech2(\u03c8T \u039b) Cov ( \u039b2i ,\u039b 2 j\n\u2223\u2223\u2223\u039b) = 0. We can combine these two results to bound the expected variance of these conditional statistics.\nLemma E.5. If \u03b8 and \u03b8\u2217 are two feasible models, then for any u,\nE\u03b8\u2217 [Var\u03b8 (Y |\u039b)] \u2264 3 exp \u2212 m\u03b22min\u03b33min8 artanh(\u03b3max)  . We can now proceed to restate and prove the main corollary of Theorem A.1 that applies in the independent case.\nCorollary B.1. Suppose that we run Algorithm 1 on an independent data programming specification that satisfies conditions (13), (14), (15), and (16). Furthermore, assume that the number of labeling functions we use satisfies\nm \u2265 9.34 artanh(\u03b3max) (\u03b3\u03b2)min\u03b32min\nlog (\n24m \u03b2min\n) .\nSuppose further that, for some parameter > 0, we use step size\n\u03b7 = \u03b2min\n2\n16\nand our dataset is of a size that satisfies\n|S | = 32 \u03b22min 2 log\n( 2 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 ) .\nThen, we can bound the expected parameter error with E [\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u22252] \u2264 2M\nand the expected risk with\nE [ l(w\u0302) \u2212min\nw l(w)\n] \u2264 \u03c7 + \u03b2min\n8\u03c1 .\nProof. In order to apply Theorem A.1, we have to verify all its conditions hold in the independent case. First, we notice that (11) is used only to bound the covariance of the sufficient statistics. From Lemma E.1, we know that these can be bounded by \u03b2i \u2212 \u03b22i \u03b32i \u2265 \u03b2min 2 . It follows that we can choose\nc = \u03b2min\n4 ,\nand we can consider (11) satisfied, for the purposes of applying the theorem. Second, to verify (12), we can use Lemma E.5. For this to work, we need\n3 exp \u2212 m(\u03b3\u03b2)min\u03b32min9.34 artanh(\u03b3max)  \u2264 cM = \u03b2min8m . This happens whenever the number of labeling functions satisfies\nm \u2265 9.34 artanh(\u03b3max) (\u03b3\u03b2)min\u03b32min\nlog (\n24m \u03b2min\n) .\nThe remaining assumptions, (13), (14), (15), and (16), are satisfied directly by the assumptions of this corollary. So, we can apply Theorem A.1, which produces the desired result."}, {"heading": "F Proofs of Independent Model Lemmas", "text": "Lemma E.1. The expected values and covariances of the sufficient statistics are, for all i , j,\nE [\u039biY] = \u03b2i\u03b3i E [ \u039b2i ] = \u03b2i\nVar (\u039biY) = \u03b2i \u2212 \u03b22i \u03b32i Var ( \u039b2i ) = \u03b2i \u2212 \u03b22i\nCov ( \u039biY,\u039b jY ) = 0\nCov ( \u039b2i ,\u039b 2 j ) = 0\nCov ( \u039biY,\u039b2j ) = 0.\nProof. We prove each of the statements in turn. For the first statement,\nE [\u039biY] = P (\u039bi = Y) \u2212 P (\u039bi = \u2212Y)\n= \u03b2i 1 + \u03b3i\n2 \u2212 \u03b2i 1 \u2212 \u03b3i 2\n= \u03b2i\u03b3i.\nFor the second statement,\nE [ \u039b2i ] = P (\u039b = Y) + P (\u039b = \u2212Y)\n= \u03b2i 1 + \u03b3i\n2 + \u03b2i 1 \u2212 \u03b3i 2\n= \u03b2i.\nFor the remaining statements, we derive the second moments; converting these to an expression of the covariance is trivial. For the third statement, E [ (\u039biY)2 ] = E [ \u039b2i Y 2 ] = E [ \u039b2i ] = \u03b2i.\nFor the fourth statement, E [ (\u039b2i ) 2 ] = E [ \u039b4i ] = E [ \u039b2i ] = \u03b2i.\nFor subsequent statements, we first derive that\nE [\u039biY |Y] = \u03b2i 1 + \u03b3i\n2 \u2212 \u03b2i 1 \u2212 \u03b3i 2 = \u03b2i\u03b3i\nand E [ \u039b2i \u2223\u2223\u2223Y] = \u03b2i 1 + \u03b3i2 + \u03b2i 1 \u2212 \u03b3i2 = \u03b2i. Now, for the fifth statement,\nE [ (\u039biY)(\u039b jY) ] = E [ E [\u039biY |Y] E [ \u039b jY \u2223\u2223\u2223Y]] = \u03b2i\u03b3i\u03b2 j\u03b3 j. For the sixth statement,\nE [ (\u039b2i )(\u039b 2 j ) ] = E [ E [ \u039b2i \u2223\u2223\u2223Y] E [\u039b2i \u2223\u2223\u2223Y]] = \u03b2i\u03b2 j. Finally, for the seventh statement,\nE [ (\u039biY)(\u039b2j ) ] = E [ E [\u039biY |Y] E [ \u039b2i \u2223\u2223\u2223Y]] = \u03b2i\u03b3i\u03b2 j. This completes the proof.\nLemma E.2. It holds that \u03b3i = tanh(\u03c8i).\nProof. From the definitions,\n\u03b2i = exp(\u03c8i + \u03c6i) + exp(\u2212\u03c8i + \u03c6i)\nexp(\u03c8i + \u03c6i) + exp(\u2212\u03c8i + \u03c6i) + 1 and\n\u03b2i\u03b3i = exp(\u03c8i + \u03c6i) \u2212 exp(\u2212\u03c8i + \u03c6i)\nexp(\u03c8i + \u03c6i) + exp(\u2212\u03c8i + \u03c6i) + 1 .\nTherefore,\n\u03b3i = exp(\u03c8i + \u03c6i) \u2212 exp(\u2212\u03c8i + \u03c6i) exp(\u03c8i + \u03c6i) + exp(\u2212\u03c8i + \u03c6i) = tanh(\u03c8i),\nwhich is the desired result.\nLemma E.3. For any feasible model, it will be the case that, for any other feasible parameter vector \u03c8\u0302,\nP ( \u03c8\u0302T \u039bY \u2264 m\n2 \u03b3min(\u03b3\u03b2)min\n) \u2264 exp \u2212 m(\u03b3\u03b2)min\u03b32min9.34 artanh(\u03b3max)  .\nProof. We start by noticing that\n\u03c8\u0302T \u039bY = m\u2211\ni=1\n\u03c8\u0302i\u039biY.\nSince in this model, all the \u039biY are independent of each other, we can bound this sum using a concentration bound. First, we note that \u2223\u2223\u2223\u03c8\u0302i\u039biY \u2223\u2223\u2223 \u2264 \u03c8\u0302i. Second, we note that\nE [ \u03c8\u0302i\u039biY ] = \u03c8\u0302i\u03b2i\u03b3i\nand Var ( \u03c8\u0302i\u039biY ) = \u03c8\u03022i ( \u03b2i \u2212 \u03b22i \u03b32i ) but \u2223\u2223\u2223\u03c8\u0302i\u039biY \u2223\u2223\u2223 \u2264 \u03c8\u0302i \u2264 artanh(\u03b3max) , \u03c8\u0302max because, for feasible models, by definition\n\u03b3min \u2264 artanh(\u03b3min) \u2264 \u03c8\u0302i \u2264 artanh(\u03b3max).\nTherefore, applying Bernstein\u2019s inequality gives us, for any t, P  m\u2211\ni=1\n\u03c8\u0302i\u039biY \u2212 m\u2211\ni=1 \u03c8\u0302i\u03b2i\u03b3i \u2264 \u2212t  \u2264 exp \u2212 3t26 \u2211mi=1 \u03c8\u03022i \u03b3i\u03b2i\u03b3i + 2\u03c8\u0302maxt  . It follows that, if we let\nt = 1 2 m\u2211 i=1 \u03c8\u0302i\u03b2i\u03b3i,\nthen we get\nP  m\u2211\ni=1\n\u03c8\u0302i\u039biY \u2212 m\u2211\ni=1\n\u03c8\u0302i\u03b2i\u03b3i \u2264 \u2212t  \u2264 exp \u2212 3 ( 1 2 \u2211m i=1 \u03c8\u0302i\u03b2i\u03b3i )2 6 \u2211m\ni=1 \u03c8\u0302 2 i \u03b3i\u03b2i\u03b3i + 2\u03c8\u0302max ( 1 2 \u2211m i=1 \u03c8\u0302i\u03b2i\u03b3i ) \n\u2264 exp ( \u2212\n3 \u2211m\ni=1 \u03c8\u0302i\u03b2i\u03b3i\n24\u03b3max\u03c8\u0302max + 4\u03c8\u0302max ) \u2264 exp ( \u22123m(1 \u2212 \u03b3max)\n28\u03c8\u0302max\n)\n\u2264 exp \u2212 3 (\u2211m i=1 \u03c8\u0302i\u03b2i\u03b3i )2 24 \u2211m\ni=1 \u03c8\u0302 2 i \u03b2i + 4\u03c8\u0302max (\u2211m i=1 \u03c8\u0302i\u03b2i\u03b3i\n) \n\u2264 exp \u2212 3\u03b3min (\u2211m i=1 \u03c8\u0302i\u03b2i ) (\u2211m i=1 \u03c8\u0302i\u03b2i\u03b3i ) 24\u03c8\u0302max \u2211m i=1 \u03c8\u0302i\u03b2i + 4\u03c8\u0302max (\u2211m i=1 \u03c8\u0302i\u03b2i )  \u2264 exp \u22123\u03b3min (\u2211m i=1 \u03c8\u0302i\u03b2i\u03b3i )\n28\u03c8\u0302max  \u2264 exp\n\u2212m\u03b32min(\u03b3\u03b2)min 9.34\u03c8\u0302max  . This is the desired expression.\nLemma E.4. The covariances of the sufficient statistics, conditioned on \u039b, are for all i , j, Cov ( \u039biY,\u039b jY \u2223\u2223\u2223\u039b) = \u039bi\u039b j sech2(\u03c8T \u039b) Cov ( \u039b2i ,\u039b 2 j\n\u2223\u2223\u2223\u039b) = 0. Proof. The second result is obvious, so it suffices to prove only the first result. Clearly,\nCov ( \u039biY,\u039b jY \u2223\u2223\u2223\u039b) = \u039bi\u039b jVar (Y |\u039b) = \u039bi\u039b j (1 \u2212 E [Y |\u039b]2) . Plugging into the distribution formula lets us conclude that\nE [Y |\u039b] = exp(\u03c8 T \u039b + \u03c6T \u039b2) \u2212 exp(\u2212\u03c8T \u039b + \u03c6T \u039b2)\nexp(\u03c8T \u039b + \u03c6T \u039b2) + exp(\u2212\u03c8T \u039b + \u03c6T \u039b2) = tanh 2(\u03c8T \u039b),\nand so Cov ( \u039biY,\u039b jY \u2223\u2223\u2223\u039b) = \u039bi\u039b j (1 \u2212 tanh2(\u03c8T \u039b)) = \u039bi\u039b j sech2(\u03c8T \u039b), which is the desired result.\nLemma E.5. If \u03b8 and \u03b8\u2217 are two feasible models, then for any u,\nE\u03b8\u2217 [Var\u03b8 (Y |\u039b)] \u2264 3 exp \u2212 m\u03b22min\u03b33min8 artanh(\u03b3max)  . Proof. First, we note that, by the result of Lemma E.4,\nVar\u03b8 (Y |\u039b) = sech2(\u03c8T \u039b).\nTherefore, E\u03b8\u2217 [Var\u03b8 (Y |\u039b)] = E\u03b8\u2217 [ sech2(\u03c8T \u039b) ] .\nApplying Lemma E.3, we can bound this with\nE\u03b8\u2217 [ Var\u03b8 ( uT \u039bY \u2223\u2223\u2223\u039b)] \u2264 sech2 (m2 (\u03b3\u03b2)min\u03b32min ) + exp \u2212 m(\u03b3\u03b2)min\u03b32min9.34 artanh(\u03b3max)  \u2264 2 exp (\u2212m2 (\u03b3\u03b2)min\u03b32min ) + exp \u2212 m(\u03b3\u03b2)min\u03b32min9.34 artanh(\u03b3max)\n \u2264 3 exp\n\u2212 m(\u03b3\u03b2)min\u03b32min9.34 artanh(\u03b3max)  .\nThis is the desired expression."}, {"heading": "G Additional Experimental Details", "text": "G.1 Synthetic Experiments In Fig. 3(a-b), we ran synthetic experiments with labeling functions having constant coverage \u03b2 = 0.1, and accuracy drawn from \u03b1 \u223c Uniform(\u00b5\u03b1 \u2212 0.25, \u00b5\u03b1 + 0.25) where \u00b5\u03b1 = 0.75 in the above plots. In both cases we used 1000 normally-drawn features having mean correlation with the true label class of 0.5.\nIn this case we compare data programming (DP-Pipelined) against two baselines. First, we compare against an if-then-return setup where the ordering is optimal (ITR-Oracle). Second, we compare against simple majority vote (MV).\nIn Fig. 3(c), we show an experiment where we add dependent labeling functions to a set of mind = 50 independent labeling functions, and either provided this dependency structure (LDM-Aware) or did not (Independent). In this case, the independent labeling functions had the same configurations as in (a-b), and the dependent labeling functions corresponded to \u201cfixes\u201d or \u201creinforces\u201d-type dependent labeling functions."}], "references": [{"title": "Pattern learning for relation extraction with a hierarchical topic model", "author": ["E. Alfonseca", "K. Filippova", "J.-Y. Delort", "G. Garrido"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 54\u201359. Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable semi-supervised aggregation of classifiers", "author": ["A. Balsubramani", "Y. Freund"], "venue": "Advances in Neural Information Processing Systems, pages 1351\u20131359,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Consistency of weighted majority votes", "author": ["D. Berend", "A. Kontorovich"], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 3446\u20133454. Curran Associates, Inc.,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Proceedings of the eleventh annual conference on Computational learning theory, pages 92\u2013100. ACM,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Label-noise robust logistic regression and its applications", "author": ["J. Bootkrajang", "A. Kab\u00e1n"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 143\u2013158. Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["R. Bunescu", "R. Mooney"], "venue": "Annual meeting-association for Computational Linguistics, volume 45, page 576,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["M. Craven", "J. Kumlien"], "venue": "In ISMB,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Aggregating crowdsourced binary ratings", "author": ["N. Dalvi", "A. Dasgupta", "R. Kumar", "V. Rastogi"], "venue": "Proceedings of the 22Nd International Conference on World Wide Web, WWW \u201913, pages 285\u2013294,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Applied statistics, pages 20\u201328,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1979}, {"title": "An improved corpus of disease mentions in pubmed citations", "author": ["R.I. Do\u011fan", "Z. Lu"], "venue": "Proceedings of the 2012 workshop on biomedical natural language processing, pages 91\u201399. Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Harnessing the crowdsourcing power of social media for disaster relief", "author": ["H. Gao", "G. Barbier", "R. Goolsby", "D. Zeng"], "venue": "Technical report, DTIC Document,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "D.S. Weld"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 541\u2013550. Association for Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Comprehensive and reliable crowd assessment algorithms", "author": ["M. Joglekar", "H. Garcia-Molina", "A. Parameswaran"], "venue": "Data Engineering (ICDE), 2015 IEEE 31st International Conference on, pages 195\u2013206. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Iterative learning for reliable crowdsourcing systems", "author": ["D.R. Karger", "S. Oh", "D. Shah"], "venue": "Advances in neural information processing systems, pages 1953\u20131961,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Multi-relational learning, text mining, and semi-supervised learning for functional genomics", "author": ["M.-A. Krogel", "T. Scheffer"], "venue": "Machine Learning, 57(1-2):61\u201381,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning with an unreliable teacher", "author": ["G. Lugosi"], "venue": "Pattern Recognition, 25(1):79 \u2013 87,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "Large-scale extraction of gene interactions from full-text literature using deepdive", "author": ["E.K. Mallory", "C. Zhang", "C. R\u00e9", "R.B. Altman"], "venue": "Bioinformatics,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, pages 1003\u20131011,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning with noisy labels", "author": ["N. Natarajan", "I.S. Dhillon", "P.K. Ravikumar", "A. Tewari"], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 1196\u20131204. Curran Associates, Inc.,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Ranking and combining multiple predictors without labeled data", "author": ["F. Parisi", "F. Strino", "B. Nadler", "Y. Kluger"], "venue": "Proceedings of the National Academy of Sciences, 111(4):1253\u20131258,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling relations and their mentions without labeled text", "author": ["S. Riedel", "L. Yao", "A. McCallum"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 148\u2013163. Springer,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining generative and discriminative model scores for distant supervision", "author": ["B. Roth", "D. Klakow"], "venue": "EMNLP, pages 24\u201329,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature-based models for improving the quality of noisy training data for relation extraction", "author": ["B. Roth", "D. Klakow"], "venue": "Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pages 1181\u20131184. ACM,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Boosting: Foundations and algorithms", "author": ["R.E. Schapire", "Y. Freund"], "venue": "MIT press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Incremental knowledge base construction using deepdive", "author": ["J. Shin", "S. Wu", "F. Wang", "C. De Sa", "C. Zhang", "C. R\u00e9"], "venue": "Proceedings of the VLDB Endowment, 8(11):1310\u20131321,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Overview of the english slot filling track at the tac2014 knowledge base population evaluation", "author": ["M. Surdeanu", "H. Ji"], "venue": "Proc. Text Analysis Conference (TAC2014),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducing wrong labels in distant supervision for relation extraction", "author": ["S. Takamatsu", "I. Sato", "H. Nakagawa"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 721\u2013729. Association for Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Multilingual relation extraction using compositional universal schema", "author": ["P. Verga", "D. Belanger", "E. Strubell", "B. Roth", "A. McCallum"], "venue": "arXiv preprint arXiv:1511.06396,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral methods meet em: A provably optimal algorithm for crowdsourcing", "author": ["Y. Zhang", "X. Chen", "D. Zhou", "M.I. Jordan"], "venue": "Advances in Neural Information Processing Systems 27, pages 1260\u20131268.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "This trend has also been fueled by the recent empirical success of automated feature generation approaches, notably deep learning methods such as long short term memory (LSTM) networks [12], which ameliorate the burden of feature engineering given large enough labeled training sets.", "startOffset": 185, "endOffset": 189}, {"referenceID": 19, "context": "Data programming extends the idea of distant supervision, in which an external knowledge base is mapped onto an input dataset to generate training examples [20].", "startOffset": 156, "endOffset": 160}, {"referenceID": 14, "context": "In Section 4, we extend our results to more sophisticated data programming models, generalizing related results in crowdsourcing [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 29, "context": "98 point F1 score gain over a state-of-the-art LSTM baseline trained on hand-labeled data [30].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "The canonical example is relation extraction from text, wherein a knowledge base of known relations is heuristically mapped to label a set of mentions in an input corpus as ground truth examples [7, 20].", "startOffset": 195, "endOffset": 202}, {"referenceID": 19, "context": "The canonical example is relation extraction from text, wherein a knowledge base of known relations is heuristically mapped to label a set of mentions in an input corpus as ground truth examples [7, 20].", "startOffset": 195, "endOffset": 202}, {"referenceID": 12, "context": "Basic extensions group these mapped examples by the particular textual pattern w that they occur with, and cast the problem as a multiple instance learning one [13, 23].", "startOffset": 160, "endOffset": 168}, {"referenceID": 22, "context": "Basic extensions group these mapped examples by the particular textual pattern w that they occur with, and cast the problem as a multiple instance learning one [13, 23].", "startOffset": 160, "endOffset": 168}, {"referenceID": 24, "context": "Other extensions actually model the accuracy of this pattern w using a discriminative feature-based model [25], or generative models such as hierarchical topic models [1, 24, 29].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "Other extensions actually model the accuracy of this pattern w using a discriminative feature-based model [25], or generative models such as hierarchical topic models [1, 24, 29].", "startOffset": 167, "endOffset": 178}, {"referenceID": 23, "context": "Other extensions actually model the accuracy of this pattern w using a discriminative feature-based model [25], or generative models such as hierarchical topic models [1, 24, 29].", "startOffset": 167, "endOffset": 178}, {"referenceID": 28, "context": "Other extensions actually model the accuracy of this pattern w using a discriminative feature-based model [25], or generative models such as hierarchical topic models [1, 24, 29].", "startOffset": 167, "endOffset": 178}, {"referenceID": 5, "context": "There is also a wealth of examples where additional heuristic patterns used to label training data are collected from unlabeled data [6] or directly from users [19, 27], in a similar manner to our approach, but without a framework to deal with the fact that said labels are explicitly noisy.", "startOffset": 133, "endOffset": 136}, {"referenceID": 18, "context": "There is also a wealth of examples where additional heuristic patterns used to label training data are collected from unlabeled data [6] or directly from users [19, 27], in a similar manner to our approach, but without a framework to deal with the fact that said labels are explicitly noisy.", "startOffset": 160, "endOffset": 168}, {"referenceID": 26, "context": "There is also a wealth of examples where additional heuristic patterns used to label training data are collected from unlabeled data [6] or directly from users [19, 27], in a similar manner to our approach, but without a framework to deal with the fact that said labels are explicitly noisy.", "startOffset": 160, "endOffset": 168}, {"referenceID": 10, "context": "Crowdsourcing is widely used for various machine learning tasks [11, 16].", "startOffset": 64, "endOffset": 72}, {"referenceID": 15, "context": "Crowdsourcing is widely used for various machine learning tasks [11, 16].", "startOffset": 64, "endOffset": 72}, {"referenceID": 8, "context": "Of particular relevance to our problem setting is the theoretical question of how to model the accuracy of various experts without ground truth available, classically raised in the context of crowdsourcing [9].", "startOffset": 206, "endOffset": 209}, {"referenceID": 2, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 7, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 13, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 14, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 21, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 30, "context": "More recent results provide formal guarantees even in the absence of labeled data using various approaches [3, 8, 14, 15, 22, 31].", "startOffset": 107, "endOffset": 129}, {"referenceID": 3, "context": "Co-training is a classic procedure for effectively utilizing both a small amount of labeled data and a large amount of unlabeled data by selecting two conditionally independent views of the data [4].", "startOffset": 195, "endOffset": 198}, {"referenceID": 16, "context": "In addition to not needing a set of labeled data, and allowing for more than two views (labeling functions in our case), our approach allows explicit modeling of dependencies between views, for example allowing observed issues with dependencies between views to be explicitly modeled [17].", "startOffset": 284, "endOffset": 288}, {"referenceID": 25, "context": "Boosting is a well known procedure for combining the output of many \u201cweak\u201d classifiers to create a strong classifier in a supervised setting [26].", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "Recently, boosting-like methods have been proposed which leverage unlabeled data in addition to labeled data, which is also used to set constraints on the accuracies of the individual classifiers being ensembled [2].", "startOffset": 212, "endOffset": 215}, {"referenceID": 17, "context": "The general case of learning with noisy labels is treated both in classical [18] and more recent contexts [21].", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "The general case of learning with noisy labels is treated both in classical [18] and more recent contexts [21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "It has also been studied specifically in the context of label-noise robust logistic regression [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "In contrast, in the crowdsourcing setting [15], the number of workers m tends to infinity while here it is constant while the dataset grows.", "startOffset": 42, "endOffset": 46}, {"referenceID": 27, "context": "34 points in F1 score, including what would have been a winning score on the 2014 TAC-KBP challenge [28].", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "Automatically-generated Features We additionally compare both hand-tuned and automatically-generated features, where the latter are learned via an LSTM recurrent neural network (RNN) [12].", "startOffset": 183, "endOffset": 187}, {"referenceID": 29, "context": "98 point F1 score improvement over a state-of-the-art LSTM approach applied to the TAC-KBP task which was trained on hand-labeled data [30].", "startOffset": 135, "endOffset": 139}, {"referenceID": 9, "context": "Their goal was to build a disease tagging system, a common and important challenge in the bioinformatics domain [10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "References [1] E.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] Y.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users provide a set of labeling functions, which are programs that heuristically label large subsets of data points, albeit noisily. By viewing these labeling functions as implicitly describing a generative model for this noise, we show that we can recover the parameters of this model to \u201cdenoise\u201d the training set. Then, we show how to modify a discriminative loss function to make it noise-aware. We demonstrate our method over a range of discriminative models including logistic regression and LSTMs. We establish theoretically that we can recover the parameters of these generative models in a handful of settings. Experimentally, on the 2014 TAC-KBP relation extraction challenge, we show that data programming would have obtained a winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a supervised LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way to create machine learning models for non-experts.", "creator": "LaTeX with hyperref package"}}}