{"id": "1605.07874", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "BattRAE: Bidimensional Attention-Based Recursive Autoencoders for Learning Bilingual Phrase Embeddings", "abstract": "In this paper, we propose a bidimensional attention based recursive autoencoder (BattRAE) to integrate cues and source-target interactions at multiple levels of granularity into bilingual phrase representations. We employ recursive autoencoders to generate tree structures of phrase with embeddings at different levels of granularity (e.g., words, sub-phrases, phrases). Over these embeddings on the source and target side, we introduce a bidimensional attention network to learn their interactions encoded in a bidimensional attention matrix, from which we extract two soft attention weight distributions simultaneously. The weight distributions enable BattRAE to generate compositive phrase representations via convolution. Based on the learned phrase representations, we further use a bilinear neural model, trained via a max-margin method, to measure bilingual semantic similarity. In order to evaluate the effectiveness of BattRAE, we incorporate this semantic similarity as an additional feature into a state-of-the-art SMT system. Extensive experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.82 BLEU points over the baseline.", "histories": [["v1", "Wed, 25 May 2016 13:29:07 GMT  (206kb,D)", "https://arxiv.org/abs/1605.07874v1", "9 pages"], ["v2", "Fri, 25 Nov 2016 03:26:45 GMT  (180kb,D)", "http://arxiv.org/abs/1605.07874v2", "7 pages, accepted by AAAI 2017"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biao zhang", "deyi xiong", "jinsong su"], "accepted": true, "id": "1605.07874"}, "pdf": {"name": "1605.07874.pdf", "metadata": {"source": "CRF", "title": "BattRAE: Bidimensional Attention-Based Recursive Autoencoders for Learning Bilingual Phrase Embeddings", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "emails": ["zb@stu.xmu.edu.cn,", "dyxiong@suda.edu.cn,", "jssu@xmu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "As one of the most important components in statistical machine translation (SMT), translation model measures the translation faithfulness of a hypothesis to a source fragment (Och and Ney 2003; Koehn, Och, and Marcu 2003; Chiang 2007). Conventional translation models extract a huge number of bilingual phrases with conditional translation probabilities and lexical weights (Koehn, Och, and Marcu 2003). Due to the heavy reliance of the calculation of these probabilities and weights on surface forms of bilingual phrases, traditional translation models often suffer from the problem of data sparsity. This leads researchers to investigate methods that learn underlying semantic representations of phrases using neural networks (Gao et al. 2014; Zhang et al. 2014; Cho et al. 2014; Su et al. 2015).\nTypically, these neural models learn bilingual phrase embeddings in a way that embeddings of source and corresponding target phrases are optimized to be close as much as possible in a continuous space. In spite of their success, they\n\u2217Corresponding author. Copyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\neither explore clues (linguistic items from contexts) only at a single level of granularity or capture interactions (alignments between source and target items) only at the same level of granularity to learn bilingual phrase embeddings.\nWe believe that clues and interactions from a single level of granularity are not adequate to measure underlying semantic similarity of bilingual phrases due to the high language divergence. Take the Chinese-English translation pairs in Table 1 as examples. At the word level of granularity, we can easily recognize that the translation of the first instance is not faithful as Chinese word \u201cshijie\u201d (world) is not translated at all. While in the second instance, semantic judgment at the word level is not sufficient as there is no translation for single Chinese word \u201cjingji\u201d (economy) or \u201cxuezhe\u201d (scholar). We have to elevate the calculation of semantic similarity to a higher sub-phrase level: \u201cjingji xuezhe\u201d vs. \u201ceconomists\u201d. This suggests that clues and interactions between the source and target side at multiple levels of granularity should be explored to measure semantic similarity of bilingual phrases.\nIn order to capture multi-level clues and interactions, we propose a bidimensional attention based recursive autoencoder (BattRAE). It learns bilingual phrase embeddings according to the strengths of interactions between linguistic items at different levels of granularity (i.e., words, subphrases and entire phrases) on the source side and those on the target side. The philosophy behind BattRAE is twofold: 1) Phrase embeddings are learned from weighted clues at different levels of granularity; 2) The weights of clues are calculated according to the alignments of linguistic items at different levels of granularity between the source and target side. We introduce a bidimensional attention network to\nar X\niv :1\n60 5.\n07 87\n4v 2\n[ cs\n.C L\n] 2\n5 N\nov 2\n01 6\nlearn the strengths of these alignments. Figure 1 illustrates the overall architecture of BattRAE model. Specifically, \u2022 First, we adopt recursive autoencoders to generate hierar-\nchical structures of source and target phrases separately. At the same time, we can also obtain embeddings of multiple levels of granularity, i.e., words, sub-phrases and entire phrases from the generated structures. (see Section 2)\n\u2022 Second, BattRAE projects the representations of linguistic items at different levels of granularity onto an attention space, upon which the alignment strengths of linguistic items from the source and target side are calculated by estimating how well they semantically match. These alignment scores are stored in a bidimensional attention matrix. Over this matrix, we perform row (column)-wise summation and softmax operations to generate attention weights on the source (target) side. The final phrase representations are computed as the weighted sum of their initial embeddings using these attention weights. (see Section3.1)\n\u2022 Finally, BattRAE projects the bilingual phrase representations onto a common semantic space, and uses a bilinear model to measure their semantic similarity. (see Section 3.2)\nWe train the BattRAE model with a max-margin method, which maximizes the semantic similarity of translation equivalents and minimizes that of non-translation pairs (see Section 3.3).\nIn order to verify the effectiveness of BattRAE in learning bilingual phrase representations, we incorporate the learned semantic similarity of bilingual phrases as a new feature into SMT for translation selection. We conduct experiments with a state-of-the-art SMT system on large-scale training data. Results on the NIST 2006 and 2008 datasets show that BattRAE achieves significant improvements over baseline methods. Further analysis on the bidimensional attention matrix reveals that BattRAE is able to detect seman-\ntically related parts of bilingual phrases and assign higher weights to these parts for constructing final bilingual phrase embeddings than those not semantically related."}, {"heading": "2 Learning Embeddings at Different Levels of Granularity", "text": "We use recursive autoencoders (RAE) to learn initial embeddings at different levels of granularity for our model. Combining two children vectors from the bottom up recursively, RAE is able to generate low-dimensional vector representations for variable-sized sequences. The recursion procedure usually consists of two neural operations: composition and reconstruction.\nComposition: Typically, the input to RAE is a list of ordered words in a phrase (x1, x2, x3), each of which is embedded into a d-dimensional continuous vector.1 In each recursion, RAE selects two neighboring children (e.g. c1 = x1 and c2 = x2) via some selection criterion, and then compose them into a parent embedding y1, which can be computed as follows:\ny1 = f(W (1)[c1; c2] + b (1)) (1) where [c1; c2] \u2208 R2d is the concatenation of c1 and c2, W (1) \u2208 Rd\u00d72d is a parameter matrix, b(1) \u2208 Rd is a bias term, and f is an element-wise activation function such as tanh(\u00b7), which is used in our experiments.\nReconstruction: After the composition, we obtain the representation for the parent y1 which is also a d-dimensional vector. In order to measure how well the parent y1 represents its children, we reconstruct the original child nodes via a reconstruction layer:\n[c\u20321; c \u2032 2] = f(W (2)y1 + b (2)) (2)\nhere c\u20321 and c \u2032 2 are the reconstructed children,W (2) \u2208 R2d\u00d7d and b(2) \u2208 R2d. The minimum Euclidean distance between [c\u20321; c \u2032 2] and [c1; c2] is usually used as the selection criterion during composition. These two standard processes form the basic procedure of RAE, which repeat until the embedding of the entire phrase is generated. In addition to phrase embeddings, RAE also constructs a binary tree. The structure of the tree is determined by the used selection criterion in composition. As generating the optimal binary tree for a phrase is usually intractable, we employ a greedy algorithm (Socher et al. 2011b) based on the following reconstruction error:\nErec(x) = \u2211\ny\u2208T (x)\n1 2 \u2016 [c1; c2]y \u2212 [c\u20321; c\u20322]y \u20162 (3)\nParameters W (1) and W (2) are thereby learned to minimize the sum of reconstruction errors at each intermediate node y in the binary tree T (x). For more details, we refer the readers to (Socher et al. 2011b).\nGiven an binary tree learned by RAE, we regard each level of the tree as a level of granularity. In this way, we can use\n1Generally, all these word vectors are stacked into a word embedding matrix L \u2208 Rd\u00d7|V |, where |V | is the size of the vocabulary.\nRAE to produce embeddings of linguistic expressions at different levels of granularity. Unfortunately, RAE is unable to synthesize embeddings across different levels of granularity, which will be discussed in the next section.\nAdditionally, as illustrated in Figure 1, RAEs for the source and target language are learned separately. In our model, we assume that phrase embeddings for different languages are from different semantic spaces. To make this clear, we denote dimensions of source and target phrase embeddings as ds and dt respectively."}, {"heading": "3 Bidimensional Attention-Based Recursive Autoencoders", "text": "In this section, we present the proposed BattRAE model. We first elaborate the bidimensional attention network, and then the semantic similarity model built on phrase embeddings learned with the attention network. Finally, we introduce the objective function and training procedure."}, {"heading": "3.1 Bidimensional Attention Network", "text": "As mentioned in Section 1, we would like to incorporate clues and interactions at multiple levels of granularity into phrase embeddings and further into the semantic similarity model of bilingual phrases. The clues are encoded in multi-level embeddings learned by RAEs. The interactions between linguistic items on the source and target side can be measured by how well they semantically match. In order to jointly model clues and interactions at multiple levels of granularity, we propose the bidimensional attention network, which is illustrated in Figure 2.\nWe take the bilingual phrase (\u201cdui jingji xuezhe\u201d, \u201cto economists\u201d) in Table 1 as an example. Let\u2019s suppose that their phrase structures learned by RAE are \u201c(dui, (jingji, xuezhe))\u201d and \u201c(to, economists)\u201d respectively. We perform a postorder traversal on these structures to extract the embeddings of words, sub-phrases and the entire source/target phrase. We treat each embedding as a column and put them together to form a matrix Ms \u2208 Rds\u00d7ns on the source side and Mt \u2208 Rdt\u00d7nt on the target side. Here, ns=5 and Ms contains embeddings from linguistic items (\u201cdui\u201d, \u201cjingji\u201d, \u201cxuezhe\u201d, \u201cjingji xuezhe\u201d, \u201cdui jingji xuezhe\u201d) at different levels of granularity. Similarly, nt=3 and Mt contains embeddings of (\u201cto\u201d, \u201ceconomists\u201d, \u201cto economists\u201d). Ms and Mt form the input layer for the bidimensional attention network.\nWe further stack an attention layer upon the input matrices to project the embeddings from Ms and Mt onto a common attention space as follows (see the gray circles in Figure 2):\nAs = f(W (3)Ms + b A [:]) (4)\nAt = f(W (4)Mt + b A [:]) (5)\nwhere W (3) \u2208 Rda\u00d7ds , W (4) \u2208 Rda\u00d7dt are transformation matrices, bA \u2208 Rda is the bias term, and da is the dimensionality of the attention space. The subscript [:] indicates a broadcasting operation. Note that we use different transformation matrices but share the same bias term for As and At. This will force our model to learn to encode attention\nsemantics into the two transformation matrices, rather than the bias term.\nOn this attention space, each embedding from the source side is able to interact with all embeddings from the target side and vice versa. The strength of such an interaction can be measured by a semantically matching score, which is calculated via the following equation:\nBi,j = g(A T s,iAt,j) (6)\nwhere Bi,j \u2208 R is the score that measures how well the ith column embedding in As semantically matches the j-th column embedding in At, and g(\u00b7) is a non-linear function, e.g., the sigmoid function used in this paper. All matching scores form a matrix B \u2208 Rns\u00d7nt , which we call the bidimensional attention matrix. Intuitively, this matrix is a result of handshakes between the source and target phrase at multiple levels of granularity.\nGiven the bidimensional attention matrix, our next interest lies in how important an embedding at a specific level of granularity is to the semantic similarity between the corresponding source and target phrase. As each embedding interacts all embeddings on the other side, its importance can be measured as the summation of the strengths of all these interactions, i.e., matching scores computed in Eq. (6). This can be done via a row/column-wise summation operation over the bidimensional attention matrix as follows.\na\u0303s,i = \u2211 j Bi,j , a\u0303t,j = \u2211 i Bi,j (7)\nwhere a\u0303s \u2208 Rns and a\u0303t \u2208 Rnt are the matching score vectors.\nBecause the length of a phrase is uncertain, we apply a Softmax operation on a\u0303s and a\u0303t to keep their values at the same magnitude: as = Softmax(a\u0303s), at = Softmax(a\u0303t). This forces as and at to become real-valued distributions on the attention space. We call them attention weights (see\nFigure 2). An important feature of this attention mechanism is that it naturally deals with variable-length bilingual inputs (as we do not impose any length constrains on ns and nt at all).\nTo obtain final bilingual phrase representations, we convolute the embeddings in phrase structures with the computed attention weights:\nps = \u2211 i as,iMs,i, pt = \u2211 j at,jMt,j (8)\nThis ensures that the generated phrase representations encode weighted clues and interactions at multiple levels of granularity between the source and target phrase. Notice that ps \u2208 Rds and pt \u2208 Rdt still locate in their language-specific vector space."}, {"heading": "3.2 Semantic Similarity", "text": "To measure the semantic similarity for a bilingual phrase, we first transform the learned bilingual phrase representations ps and pt into a common semantic space through a non-linear projection as follows:\nss = f(W (5)ps + b s) (9)\nst = f(W (6)pt + b s) (10)\nwhere W (5) \u2208 Rdsem\u00d7ds , W (6) \u2208 Rdsem\u00d7dt and bs \u2208 Rdsem are the parameters. Similar to the transformation in Eq. (4) and (5), we share the same bias term for both ss and st.\nWe then use a bilingual model to compute the semantic similarity score as follows:\ns(f, e) = sTs Sst (11)\nwhere f and e is the source and target phrase respectively, and s(\u00b7, \u00b7) represents the semantic similarity function. S \u2208 Rdsem\u00d7dsem is a squared matrix of parameters to be learned. We choose this model because that the matrix S actually represents an interaction between ss and st, which is desired for our purpose."}, {"heading": "3.3 Objective and Training", "text": "There are two kinds of errors involved in our objective function: reconstruction error (see Eq. (3)) and semantic error. The latter error measures how well a source phrase semantically match its counterpart target phrase. We employ a maxmargin method to estimate this semantic error. Given a training instance (f, e) with negative samples (f\u2212, e\u2212), we define the following ranking-based error:\nEsem(f, e) = max(0, 1 + s(f, e \u2212)\u2212 s(f, e))\n+max(0, 1 + s(f\u2212, e)\u2212 s(f, e)) (12) Intuitively, minimizing this error will maximize the semantic similarity of the correct translation pair (f, e) and minimize (up to a margin) the similarity of negative translation pairs (f\u2212, e) and (f, e\u2212). In order to generate the negative samples, we replace words in a correct translation pair with random words, which is similar to the sampling method used by Zhang et al. (2014).\nFor each training instance (f, e), the joint objective of BattRAE is defined as follows:\nJ(\u03b8) = \u03b1Erec(f, e) + \u03b2Esem(f, e) +R(\u03b8) (13)\nwhere Erec(f, e) = Erec(f)+Erec(e), parameters \u03b1 and \u03b2 (\u03b1+ \u03b2 = 1) are used to balance the preference between the two errors, and R(\u03b8) is the regularization term. We divide the parameters \u03b8 into four different groups2:\n1. \u03b8L : the word embedding matrices Ls and Lt;\n2. \u03b8rec : the parameters for RAE W (1) s , W (1) t , W (2) s , W (2) t\nand b(1)s , b (1) t , b (2) s , b (2) t ;\n3. \u03b8att : the parameters for the projection of the input matrices onto the attention space W (3),W (4) and bA;\n4. \u03b8sem : the parameters for semantic similarity computation W (5),W (6), S and bs;\nAnd each parameter group is regularized with a unique weight:\nR(\u03b8)= \u03bbL 2 \u2016\u03b8L\u2016 2+\u03bbrec2 \u2016\u03b8rec\u2016 2+ \u03bbatt 2 \u2016\u03b8att\u2016 2+\u03bbsem2 \u2016\u03b8sem\u2016 2 (14)\nwhere \u03bb\u2217 are our hyperparameters. To optimize these parameters, we apply the L-BFGS algorithm which requires two conditions: parameter initialization and gradient calculation.\nParameter Initialization: We randomly initialize \u03b8rec, \u03b8att and \u03b8sem according to a normal distribution (\u00b5=0,\u03c3=0.01). With respect to the word embeddings \u03b8L, we use the toolkit Word2Vec3 to pretrain them on a large scale unlabeled data. All these parameters will be further fine-tuned in our BattRAE model.\nGradient Calculation: We compute the partial gradient for parameter \u03b8k as follows:\n\u2202J \u2202\u03b8k = \u2202Erec(f, e) \u2202\u03b8k + \u2202Esem(f, e) \u2202\u03b8k + \u03bbk\u03b8k (15)\nThis gradient is fed into the toolkit libLBFGS4 for parameter updating in our practical implementation."}, {"heading": "4 Experiment", "text": "In order to examine the effectiveness of BattRAE in learning bilingual phrase embeddings, we carried out large scale experiments on NIST Chinese-English translation tasks.5"}, {"heading": "4.1 Setup", "text": "Our parallel corpus consists of 1.25M sentence pairs extracted from LDC corpora6, with 27.9M Chinese words and\n2The subscript s and t are used to denote the source and the target language.\n3https://code.google.com/p/word2vec/ 4http://www.chokkan.org/software/liblbfgs/ 5Source code is available at https://github.com/DeepLearnXMU/BattRAE. 6This includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n34.5M English words respectively. We trained a 5-gram language model on the Xinhua portion of the GIGAWORD corpus (247.6M English words) using SRILM Toolkit7 with modified Kneser-Ney Smoothing. We used the NIST MT05 data set as the development set, and the NIST MT06/MT08 datasets as the test sets. We used minimum error rate training (Och and Ney 2003) to optimize the weights of our translation system. We used case-insensitive BLEU-4 metric (Papineni et al. 2002) to evaluate translation quality and performed the paired bootstrap sampling (Koehn 2004) for significance test.\nIn order to obtain high-quality bilingual phrases to train the BattRAE model, we used forced decoding (Wuebker, Mauser, and Ney 2010) (but without the leaving-one-out) on the above parallel corpus, and collected 2.8M phrase pairs. From these pairs, we further extracted 34K bilingual phrases as our development data to optimize all hyper-parameters using random search (Bergstra and Bengio 2012). Finally, we set ds=dt=da=dsem=50, \u03b1=0.125 (such that, \u03b2=0.875), \u03bbL=1e\u22125, \u03bbrec=\u03bbatt=1e\u22124 and \u03bbsem=1e\u22123 according to experiments on the development data. Additionally, we set the maximum number of iterations in the L-BFGS algorithm to 100."}, {"heading": "4.2 Translation Performance", "text": "We compared BattRAE against the following three methods:\n\u2022 Baseline: Our baseline decoder is a state-of-the-art bracketing transduction grammar based translation system with a maximum entropy based reordering model (Wu 1997; Xiong, Liu, and Lin 2006). The features used in this baseline include: rule translation probabilities in two directions, lexical weights in two directions, target-side word number, phrase number, language model score, and the score of the maximum entropy based reordering model.\n\u2022 BRAE: The neural model proposed by Zhang et al. (2014). We incorporate the semantic distances computed according to BRAE as new features into the log-linear model of SMT for translation selection.\n\u2022 BCorrRAE: The neural model proposed by Su et al. (2015) that extends BRAE with word alignment information. The structural similarities computed by BCorrRAE are integrated into the Baseline as additional features. 7http://www.speech.sri.com/projects/srilm/download.html\nWith respect to the two neural baselines BRAE and BCorrRAE, we used the same training data as well as the same methods as ours for hyper-parameter optimization, except for the dimensionality of word embeddings, which we set to 50 in experiments.\nTable 2 summaries the experiment results of BattRAE against the other three methods on the test sets. BattRAE significantly improves translation quality on all test sets in terms of BLEU. Especially, it achieves an improvement of up to 1.63 BLEU points on average over the Baseline. Comparing with the two neural baselines, our BattRAE model obtains consistent improvements on all test sets. It significantly outperforms BCorrRAE by 0.48 BLEU points, and BRAE by almost 1 BLEU point on average. We contribute these improvements to the incorporation of clues and interactions at different levels of granularity since neither BCorrRAE nor BRAE explore them."}, {"heading": "4.3 Attention Analysis", "text": "Observing the significant improvements and advantages of BattRAE over BRAE and BCorrRAE, we would like to take a deeper look into how the bidimensional attention mechanism works in the BattRAE model. Specifically, we wonder which words are highly weighted by the attention mechanism. Table 3 shows some examples in our translation model. We provide phrase structures learned by RAE and visualize attention weights for these examples.\nWe do find that the BattRAE model is able to learn what is important for semantic similarity computation. The model can recognize the correspondence between \u201cyige\u201d and \u201csame\u201d, \u201cyanzhong\u201d and \u201cserious concern\u201d, \u201cweizhi\u201d and \u201cso far\u201d. These word pairs tend to give high semantic similarity scores to these translation instances. In contrast, because of incorrect translation pairs \u201ctaidu\u201d (attitude) vs. \u201cvery critical\u201d, \u201cyinwei\u201d (because) vs. \u201cthe part\u201d, \u201cwenti\u201d (problem) vs. \u201chong kong\u201d, the model assigns low semantic similarity scores to these negative instances. These indicate that the BattRAE model is indeed able to detect and focus on those semantically related parts of bilingual phrases.\nFurther observation reveals that there are strong relations between phrase structures and attention weights. Generally, the BattRAE model will assign high weights to words subsumed by many internal nodes of phrase structures. For example, we find that the correct translation of \u201cwenti\u201d actually appears in the corresponding target phrase. However, due to errors in learned phrase structures, the model fails to detect this translation. Instead, it finds an incorrect translation \u201chong kong\u201d. This suggests that the quality of learned phrase structures has an important impact on the performance of our model."}, {"heading": "5 Related Work", "text": "Our work is related to bilingual embeddings and attentionbased neural networks. We will introduce previous work on these two lines in this section."}, {"heading": "5.1 Bilingual Embeddings", "text": "The studies on bilingual embeddings start from bilingual word embedding learning. Zou et al. (2013) use word align-\nments to connect embeddings of source and target words. To alleviate the reliance of bilingual embedding learning on parallel corpus, Vulic\u0301 and Moens (2015) explore documentaligned instead of sentence-aligned data, while Gouws et al. (2015) investigate monolingual raw texts. Different from the abovementioned corpus-centered methods, Koc\u030cisky\u0301 et al. (2014) develop a probabilistic model to capture deep semantic information, while Chandar et al. (2014) testify the use of autoencoder-based methods. More recently, Luong et al. (2015b) jointly model context co-ocurrence information and meaning equivalent signals to learn high quality bilingual embeddings.\nAs phrases have long since been used as the basic translation units in SMT, bilingual phrase embeddings attract increasing interests. Since translation equivalents share the same semantic meaning, embeddings of source/target phrases can be learned with information from their counterparts. Along this line, a variety of neural models are explored: multi-layer perceptron (Gao et al. 2014), RNN encoder-decoder (Cho et al. 2014) and recursive autoencoders (Zhang et al. 2014; Su et al. 2015).\nThe most related work to ours are the bilingual recursive autoencoders (Zhang et al. 2014; Su et al. 2015). Zhang et al. (2014) represent bilingual phrases with embeddings of root nodes in bilingual RAEs, which are learned subject to transformation and distance constraints on the source and target language. Su et al. (2015) extend the model of Zhang et al. (2014) by exploring word alignments and correspondences inside source and target phrases. A major limitation of their models is that they are not able to incorporate clues of multiple levels of granularity to learn bilingual phrase embeddings, which exactly forms our basic motivation."}, {"heading": "5.2 Attention-Based Neural Networks", "text": "Over the last few months, we have seen the tremendous success of attention-based neural networks in a variety of tasks, where learning alignments between different modalities is a key interest. For example, Mnih et al. (2014) learn image objects and agent actions in a dynamic control problem. Xu et al. (2015) exploit an attentional mechanism in the task of image caption generation. With respect to neural machine translation, Bahdanau et al. (2014) succeed in jointly learning to translate and align words. Luong et al. (2015a) further evaluate different attention architectures on translation. In-\nspired by these works, we propose a bidimensional attention network that is suitable in the bilingual context.\nIn addition to the abovementioned neural models, our model is also related to the work of Socher et al. (2011a) and Yin and Schu\u0308tze (2015) in terms of multi-granularity embeddings. The former preserves multi-granularity embeddings in tree structures and introduces a dynamic pooling technique to extract features directly from an attention matrix. The latter extends the idea of the former model to convolutional neural networks. Significantly different from their models, we introduce a bidimensional attention matrix to generate attention weights, instead of extracting features. Additionally, our attention-based model is also significantly different from the tensor networks (Socher et al. 2013) in that the attention-based model is able to handle variable-length inputs while tensor networks often assume fixed-length inputs.\nWe notice that the very recently proposed attentive pooling model (dos Santos et al. 2016) which also aims at modeling mutual interactions between two inputs with a twoway attention mechanism that is similar to ours. The major differences between their and our work lie in the following four aspects. First, we perform a transformation ahead of attention computation in order to deal with language divergences, rather than directly compute the attention matrix. Second, we calculate attention weights via a sum-pooling approach, instead of max pooling, in order to preserve all interactions at each level of granularity. Third, we apply our bidimensional attention technique to recursive autoencoders instead of convolutional neural networks. Last, we aim at learning bilingual phrase representations rather than question answering. Most importantly, our work and theirs can be seen as two independently developed models that provide different perspectives on a new attention mechanism."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we have presented a bidimensional attention based recursive autoencoder to learn bilingual phrase representations. The model incorporates clues and interactions across source and target phrases at multiple levels of granularity. Through the bidimensional attention network, our model is able to integrate them into bilingual phrase embeddings. Experiment results show that our approach significantly improves translation quality.\nIn the future, we would like to exploit different functions to compute semantically matching scores (Eq. (6)), and other neural models for the generation of phrase structures. Additionally, the bidimensional attention mechanism can be used in convolutional neural network and recurrent neural network. Furthermore, we are also interested in adapting our model to semantic tasks such as paraphrase identification and natural language inference."}, {"heading": "Acknowledgements", "text": "The authors were supported by National Natural Science Foundation of China (Grant Nos. 61303082, 61403269, 61622209 and 61672440), Natural Science Foundation of Fujian Province (Grant No. 2016J05161), and Natural Science Foundation of Jiangsu Province (Grant No. BK20140355). We also thank the anonymous reviewers for their insightful comments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Bahdanau", "D. Bengio 2014] Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proc. of ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "and Bengio", "author": ["J. Bergstra"], "venue": "Y.", "citeRegEx": "Bergstra and Bengio 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "V", "author": ["S. Chandar A P", "S. Lauly", "H. Larochelle", "M. Khapra", "B. Ravindran", "Raykar"], "venue": "C.; and Saha, A.", "citeRegEx": "Chandar A P et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho"], "venue": null, "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao"], "venue": "In Proc. of ACL,", "citeRegEx": "Gao,? \\Q2014\\E", "shortCiteRegEx": "Gao", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Bengio Gouws", "S. Corrado 2015] Gouws", "Y. Bengio", "G. Corrado"], "venue": "In ICML,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "F", "author": ["Koehn, P.", "Och"], "venue": "J.; and Marcu, D.", "citeRegEx": "Koehn. Och. and Marcu 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "K", "author": ["Ko\u010disk\u00fd, T.", "Hermann"], "venue": "M.; and Blunsom, P.", "citeRegEx": "Ko\u010disk\u00fd. Hermann. and Blunsom 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Pham Luong", "M. Manning 2015a] Luong", "H. Pham", "C.D. Manning"], "venue": "Proc. of EMNLP 1412\u20131421", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Pham Luong", "T. Manning 2015b] Luong", "H. Pham", "C.D. Manning"], "venue": "In Proc. of VSM-NLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Mnih"], "venue": "In Proc. of NIPS. Curran Associates,", "citeRegEx": "Mnih,? \\Q2014\\E", "shortCiteRegEx": "Mnih", "year": 2014}, {"title": "and Ney", "author": ["F.J. Och"], "venue": "H.", "citeRegEx": "Och and Ney 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni"], "venue": "In Proc. of ACL,", "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher"], "venue": "In Proc. of NIPS,", "citeRegEx": "Socher,? \\Q2011\\E", "shortCiteRegEx": "Socher", "year": 2011}, {"title": "Semisupervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Socher,? \\Q2011\\E", "shortCiteRegEx": "Socher", "year": 2011}, {"title": "C", "author": ["R. Socher", "D. Chen", "Manning"], "venue": "D.; and Ng, A.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Bilingual correspondence recursive autoencoder for statistical machine translation", "author": ["Su"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Su,? \\Q2015\\E", "shortCiteRegEx": "Su", "year": 2015}, {"title": "and Moens", "author": ["I. Vuli\u0107"], "venue": "M.-F.", "citeRegEx": "Vuli\u0107 and Moens 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "D", "author": ["Wu"], "venue": "1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, Volume 23, Number 3, September", "citeRegEx": "Wu 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Training phrase translation models with leaving-one-out", "author": ["Mauser Wuebker", "J. Ney 2010] Wuebker", "A. Mauser", "H. Ney"], "venue": "In Proc. of ACL,", "citeRegEx": "Wuebker et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wuebker et al\\.", "year": 2010}, {"title": "Maximum entropy based phrase reordering model for statistical machine translation", "author": ["Liu Xiong", "D. Lin 2006] Xiong", "Q. Liu", "S. Lin"], "venue": "In Proc. of ACL,", "citeRegEx": "Xiong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2006}, {"title": "R", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "Zemel"], "venue": "S.; and Bengio, Y.", "citeRegEx": "Xu et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Sch\u00fctze", "author": ["W. Yin"], "venue": "H.", "citeRegEx": "Yin and Sch\u00fctze 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilingually-constrained phrase embeddings for machine translation", "author": ["Zhang"], "venue": "In Proc. of ACL,", "citeRegEx": "Zhang,? \\Q2014\\E", "shortCiteRegEx": "Zhang", "year": 2014}, {"title": "C", "author": ["W.Y. Zou", "R. Socher", "D. Cer", "Manning"], "venue": "D.", "citeRegEx": "Zou et al. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we propose a bidimensional attention based recursive autoencoder (BattRAE) to integrate clues and sourcetarget interactions at multiple levels of granularity into bilingual phrase representations. We employ recursive autoencoders to generate tree structures of phrases with embeddings at different levels of granularity (e.g., words, sub-phrases and phrases). Over these embeddings on the source and target side, we introduce a bidimensional attention network to learn their interactions encoded in a bidimensional attention matrix, from which we extract two soft attention weight distributions simultaneously. These weight distributions enable BattRAE to generate compositive phrase representations via convolution. Based on the learned phrase representations, we further use a bilinear neural model, trained via a max-margin method, to measure bilingual semantic similarity. To evaluate the effectiveness of BattRAE, we incorporate this semantic similarity as an additional feature into a state-of-the-art SMT system. Extensive experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.63 BLEU points on average over the baseline.", "creator": "LaTeX with hyperref package"}}}