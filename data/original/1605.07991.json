{"id": "1605.07991", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Efficient Distributed Learning with Sparsity", "abstract": "We propose a novel, efficient approach for distributed sparse learning in high-dimensions, where observations are randomly partitioned across machines. Computationally, at each round our method only requires the master machine to solve a shifted ell_1 regularized M-estimation problem, and other workers to compute the gradient. In respect of communication, the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications (ignoring logarithmic factors). We conduct extensive experiments on both simulated and real world datasets, and demonstrate encouraging performances on high-dimensional regression and classification tasks.", "histories": [["v1", "Wed, 25 May 2016 18:15:43 GMT  (493kb,D)", "http://arxiv.org/abs/1605.07991v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jialei wang", "mladen kolar", "nathan srebro", "tong zhang 0001"], "accepted": true, "id": "1605.07991"}, "pdf": {"name": "1605.07991.pdf", "metadata": {"source": "CRF", "title": "Efficient Distributed Learning with Sparsity", "authors": ["Jialei Wang", "Mladen Kolar", "Nathan Srebro", "Tong Zhang"], "emails": [], "sections": [{"heading": null, "text": "We propose a novel, efficient approach for distributed sparse learning in high-dimensions, where observations are randomly partitioned across machines. Computationally, at each round our method only requires the master machine to solve a shifted `1 regularized M-estimation problem, and other workers to compute the gradient on local data. In respect of communication, the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications (ignoring logarithmic factors). We conduct extensive experiments on both simulated and real world datasets, and demonstrate encouraging performances on high-dimensional regression and classification tasks."}, {"heading": "1 Introduction", "text": "Many problems in machine learning can be cast as a minimization of the expected loss,\nmin \u03b2\nEX,Y D r`pY, xX,\u03b2yqs , (1.1)\nwhere pX, Y q P X Y Rp Y are drawn from an unknown distribution D and `p , q is a convex loss function. Unfortunately the distribution D is generally not known and the minimizer, \u03b2 , of (1.1) needs to be approximated on the basis of N observations txi, yiuNi 1 drawn from D. Modern massive data sets, where both N and p are huge, create challenges to classical approaches. One of the challenges, which we address in the paper, is that often observations cannot fit in memory of a single machine, but are rather distributed across m machines. For simplicity, we will assume that N nm and that j-th machine has access to observations txji, yjiuni 1. All of our results can be easily generalized for a general N . Our particular focus is on the high-dimensional setting where the ambient dimension p is as large, or even larger, as the sample size n, but only a subset of variables is predictive, that is, S : supportp\u03b2 q tj P rps | \u03b2j 0u and s |S| ! p. Learning a sparse \u03b2 in a high-dimensional setting is a well studied statistical problem (Bu\u0308hlmann and van de Geer, 2011; Hastie et al., 2015), however, it creates unique computational challenges in the distributed setting that we address here.\nar X\niv :1\n60 5.\n07 99\n1v 1\n[ st\nat .M\nL ]\n2 5\nThe main contribution of the paper is a novel algorithm for estimating the minimizer \u03b2 of (1.1) in a distributed setting. Our estimator is able to achieve performance of a centralized procedure that has access to all data, while keeping computation and communication costs low. Compared to the existing one-shot estimation approach (Lee et al., 2015b), our method can achieve the same statistical performance faster. If the number of communication rounds is allowed to increase by logarithm on number of total machines, our procedure can keep increasing the statistical performance, until matching the centralized procedure, while keeping the computation time low. Furthermore, these results can be achieved under weaker assumptions on the data generating procedure.\nIn the paper, we assume the communication occurs in rounds, where in each round, machines exchange messages with the master machine and, between two rounds, the machines only compute based on their local information, which includes local data points and messages received before (Zhang et al., 2013b; Shamir and Srebro, 2014; Arjevani and Shamir, 2015). In a non-distributed setting, efficient estimation procedures need to balance statistical efficiency with computation efficiency (runtime). In a distributed setting, the situation gets more complicated and we need to balance two resources, local runtime and number of rounds of communication, with the statistical error. The local runtime refers to the amount of work each machine needs to do. The number of rounds of communication refers to how often do local machines need to exchange messages with the master machine. We compare our procedure to other algorithm using the aforementioned metrics.\nNotations We use rns to denote the set t1, . . . , nu. For a vector a P Rn, we let supportpaq tj : aj 0u be the support set, ||a||q, q P r1,8q, the `q-norm defined as ||a||q p \u00b0 iPrns |ai|qq1{q, and ||a||8 maxiPrns |ai|. For a matrix A P Rn1 n2 , we use the following element-wise `8 matrix norms ||A||8 maxiPrn1s,jPrn2s |aij |. Denote In as n n identity matrix. For two sequences of numbers tanu8n 1 and tbnu8n 1, we use an Opbnq to denote that an \u00a4 Cbn for some finite positive constant C, and for all n large enough. If an Opbnq and bn Opanq, we use the notation an bn. We also use an \u00c0 bn for an Opbnq and an \u00c1 bn for bn Opanq."}, {"heading": "1.1 Overview of main results", "text": "Without loss of generality, let the master machine to be the first machine which has access to local dataset tx1i, y1iuni 1. We consider the following two baseline estimators of the minimizer \u03b2 of (1.1). The Local estimator ignores data available on other machines and computes\n\u03b2\u0302local arg min \u03b2\n1\nn\nn\u0327\ni 1\n`py1i, xx1i,\u03b2yq \u03bb||\u03b2||1 (1.2)\nusing locally available data. The Local procedure is efficient in both communication and computation, however, the resulting estimation error is large compared to an estimator that uses all of the available data. The other idealized baseline is the centralized estimator that we wish we could compute,\n\u03b2\u0302centralize arg min \u03b2\n1\nmn\nm\u0327\nj 1\nn\u0327\ni 1\n`pyji, xxji,\u03b2yq \u03bb||\u03b2||1,\nif storage were available or communication were cheap. The centralized approach achieves the optimal statistical error, however, it is impractical due to expensive communication.\nIn a related setting, Lee et al. (2015b) studied a one-shot approach to learning \u03b2 based on averaging the debiased lasso estimators (Avg-Debias) (Zhang and Zhang, 2013). Under strong assumptions their approach matches the centralized error bound after one round of communication. While an encouraging result, there are limitations to this approach, which our method addresses. In particular, the Avd-Debias method has the following problems:\n\u2022 The debiasing step in Avg-Debias computationally heavy. In particular, the debiasing step requires solving Oppq generalized lasso problems, which is computationally prohibitive for high-dimensional problems (Zhang and Zhang, 2013; Javanmard and Montanari, 2014). Our\nprocedure, on the other hand, requires only solving one `1 penalized objective in each iteration, which has the same time complexity as computing \u03b2\u0302local in (1.2). See Section 2 for details.\n\u2022 Avg-Debias procedure only matches the statistical error of the centralized procedure when the sample size per machine satisfies n \u00c1 ms2 log p. Our approach improves this sample complexity to n \u00c1 s2 log p. \u2022 Avg-Debias procedure requires strong conditions on the data generating process. For example,\nthe generalized coherence condition is required 1 on the data matrix to make debiasing work. Such a condition is not needed for consistent high-dimensional estimation in a distributed setting as we show here.\nTable 1 summarizes the resources required for the approaches discussed above to solve the\ndistributed sparse linear regression problems."}, {"heading": "1.2 Related Work", "text": "Due to its importance, there is a large body of literature on distributed optimization for modern massive data sets. See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in. A popular approach to distributed estimation is averaging estimators\n1The generalized coherence requires there exists a matrix \u0398, such that ||\u03a3\u0302\u0398 Ip||8 \u00c0 b\nlog p n , where \u03a3\u0302 is the\nempirical covariance matrix.\nformed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang and Huo, 2015). Divide-and-conquer procedures also found applications in statistical inference (Zhao et al., 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al. (2014), and Ma et al. (2015) studied distributed optimization using stochastic dual coordinate descent. These approaches try to find a good balance between computation and communication. However, their communication complexity bounds have a bad dependence on the condition number, resulting in a procedure which is not better than first-order approaches in terms of communication, such as (proximal) accelerated gradient descent (Nesterov, 1983). Shamir et al. (2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al., 2011; Shamir and Srebro, 2014; Ma et al., 2015). Lower bounds were studied in (Zhang et al., 2013a; Braverman et al., 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems.\nMost of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting. The situation in a high-dimensional setting is not so straightforward, due to the biased induced by the sparsity inducing penalty. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a highdimensional setting. Averaging debiased high-dimensional estimators was subsequently used in Lee et al. (2015b) for distributed estimation, multi-task learning (Wang et al., 2015), and statistical inference (Battey et al., 2015). Concurrent work of (Jordan et al., 2016) (personal communication) also improved computational efficiency of Lee et al. (2015b) using ideas of Shamir et al. (2014)."}, {"heading": "2 Methodology", "text": "In this section, we detail our procedure for estimating the minimizer \u03b2 of (1.1) in a distributed setting. Algorithm 1 provides an outline of the steps executed by the worker nodes and the master node.\nLet\nLjp\u03b2q 1 n\nn\u0327\ni 1\n`pyji, xxji,\u03b2yq, j P rms,\nbe the empirical loss at each machine. Our method starts by solving a local `1 regularized M - estimation program. At iteration t 0, the master machine minimizes the following program\n\u03b2\u03020 arg minL1p\u03b2q \u03bb0||\u03b2||1. (2.1)\nA minimizer \u03b2\u03020 is broadcasted to all other machines, which use it to compute a gradient of the local loss at \u03b2\u03020. In particular, each local machine computes \u2207Ljp\u03b2\u03020q and communicates this gradient\nAlgorithm 1: Efficient Distributed Sparse Learning (EDSL). 1 Input: Data txji, yjiujPrms,iPrns, loss function `p , q. 2 Workers:\n3 Initialization: The master computes local `1 regularized loss minimization solution \u03b2\u03020 as\n(2.1).\n4 for t 0, 1, . . . do 5 for j 2, 3, . . . ,m do 6 if Receive \u03b2\u0302t from the master then 7 Calculate gradient \u2207Ljp\u03b2\u0302tq and send to the master. 8 end\n9 end\n10 Master: 11 if Receive t\u2207Ljp\u03b2\u0302tqumj 2 from all workers then 12 Solve the shifted `1 regularized problem as (2.2) and obtain solution \u03b2\u0302t 1, then broadcast \u03b2\u0302t 1 to every worker. 13 end\n14 end\nback to the master machine. This constitutes one round of communication. At the iteration t 1, the master solve the following shifted `1 regularized problem\n\u03b2\u0302t 1 arg min \u03b2 L1p\u03b2q\nC 1\nm \u00b8 jPrms\n\u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq,\u03b2 G \u03bbt 1||\u03b2||1. (2.2)\nA minimizer \u03b2\u0302t 1 is communicated to other machines, which use it to compute the local gradient as before.\nFormulation (2.2) is inspired by the proposal in Shamir et al. (2014), where the authors studied distributed optimization for smooth and strongly convex empirical objectives. However, we do not use any averaging scheme, which require additional rounds of communication and, moreover, we add an `1 regularization term to ensure consistent estimation in high-dimensions. Different from the distributed first-order optimization approaches, the refined objective (2.2) leverages both global first-order information and local higher-order information. To see this, suppose we set \u03bbt 1 0 and that Ljp\u03b2q is a quadratic objective with invertible hessian. Then we have the following closed form solution for (2.2),\n\u03b2\u0302t 1 \u03b2\u0302t \u22072L1p\u03b2\u0302tq 1 m 1 \u00b8 jPrms \u2207Ljp\u03b2\u0302tq ,\nwhich is exactly a sub-sampled Newton updating rule. Unfortunately for high-dimensional problems, the Hessian is no longer invertible, and a `1 regularization is added to make the solution well behaved. The regularization parameter \u03bbt will be chosen in a way, so that it decreases with the iteration number t. As a result we will be able to show that the final estimator performs as well at the centralized solution. We discuss in details how to choose \u03bbt in the following section."}, {"heading": "3 Theoretical Results", "text": "In this section, we present the main theoretical results. The proofs are deferred to Appendix. We start by providing a general estimation error bound on \u03b2\u0302 \u03b2 , where \u03b2\u0302 is obtained using Algorithm 1 and \u03b2 is a minimizer of (1.1). Consequences of the main result are illustrated on concrete examples in Section 4. To simplify the presentation, we assume that the domain X is bounded. Furthermore, our analysis relies on the smoothness conditions of the loss function `p , q. Assumption 3.1. The loss `p , q is L-smooth with respect to the second argument, that is, @a, b, c P R, we have `1pa, bq `1pa, cq \u00a4 L|b c|. Furthermore, the loss has a bounded third derivative with respect to the second argument, that is, @a, b P R |`3pa, bq| \u00a4M.\nThe bounded second and third order derivative for `p , q is true for popular loss functions used in statistical learning, such as squared loss for regression and logistic loss for classification (Zhang et al., 2013b).\nOur analysis also require the notion of restricted strong convexity (Negahban et al., 2012).\nAssumption 3.2. The empirical loss function L1 satisfies the following inequality. For any \u2206 P CpS, 3q, L1p\u03b2 \u2206q L1p\u03b2 q x\u2207L1p\u03b2 q,\u2206y \u00a5 \u03ba||\u2206||22, where CpS, 3q is a restricted subset in Rp,\nCpS, 3q t\u2206 P Rp|||\u2206Sc ||1 \u00a4 3||\u2206S ||1u. The restricted strong convexity is an assumption used for showing consistent estimation in highdimensions (van de Geer and Bu\u0308hlmann, 2009; Negahban et al., 2012). It holds with high probability for a wide range of models and designs (see, for example, Negahban et al., 2012; Raskutti et al., 2010; Rudelson and Zhou, 2013, for details).\nOur main theoretical result establishes a recursive estimation error bound, which relates the\nestimation error ||\u03b2\u0302t 1 \u03b2 || to that of the previous iteration ||\u03b2\u0302t \u03b2 ||1. Theorem 3.3. Suppose Assumption 3.1 and 3.2 holds. Let\n\u03bbt 1 2 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 2L max j,i ||xji||28 ||\u03b2 \u03b2\u0302t||1\nc logp2p{\u03b4q\nn\n2M max j,i\n||xji||38\n||\u03b2\u0302t \u03b2 ||21 .\n(3.1)\nThen with probability at least 1 \u03b4, we have\n||\u03b2\u0302t 1 \u03b2 ||1 \u00a4 48s \u03ba 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8\n48sL \u03ba max j,i ||xji||28 ||\u03b2 \u03b2\u0302t||1\nc logp2p{\u03b4q\nn\n48sM \u03ba max j,i ||xji||38\n||\u03b2\u0302t \u03b2 ||21 ,\nand ||\u03b2\u0302t 1 \u03b2 ||2 \u00a4 12 ? s\n\u03ba\n1m \u00b8 jPrms \u2207Ljp\u03b2 q 8\n12 ? sL\n\u03ba\nmax j,i\n||xji||28 ||\u03b2 \u03b2\u0302t||1\nc logp2p{\u03b4q\nn\n4 ? sM\n\u03ba\nmax j,i\n||xji||38\n||\u03b2\u0302t \u03b2 ||21 .\nTheorem 3.3 upper bounds the estimation error ||\u03b2\u0302t 1 \u03b2 ||1 as a function of ||\u03b2\u0302t \u03b2 ||1. Thus by applying Theorem 3.3 iteratively, we immediately obtain the following estimation error bound which depends on the quality of local `1 regularized estimation ||\u03b2\u03020 \u03b2 ||1. Corollary 3.4. Suppose the conditions of Theorem 3.3 are satisfied. Furthermore, suppose that for all t, we have\nM max j,i\n||xji||8 ||\u03b2\u0302t \u03b2 ||1 \u00a4 L\nc logp2p{\u03b4q\nn . (3.2)\nThen with probability at least 1 \u03b4, we have\n||\u03b2\u0302t 1 \u03b2 ||1 \u00a4 p1 anq 1p1 at 1n q 48s \u03ba 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 at 1n ||\u03b2\u03020 \u03b2 ||1 (3.3)\nand\n||\u03b2\u0302t 1 \u03b2 ||2 \u00a4 p1 anq 1p1 at 1n q 12 ? s\n\u03ba 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 atnbn ||\u03b2\u03020 \u03b2 ||1, (3.4)\nwhere\nan 96sL \u03ba max j,i\n||xji||28 c logp2p{\u03b4q n and bn 24 ? sL \u03ba max j,i ||xji||28 c logp2p{\u03b4q n .\nCondition in (3.2) always holds for quadratic loss, since M 0. For other types of losses, condition in (3.2) may not be true for all t \u00a5 0. However, since we want \u03b2\u0302t to be competitive to \u03b2\u0302centralize where ||\u03b2\u0302centralize \u03b2 ||1 \u00c0 s b log p mn , when m \u00c1 s2 the condition will hold for t large enough, leading to local exponential rate of convergence."}, {"heading": "3.1 Sketch of Proof", "text": "We first analyze how the estimation error bound decreases after one round of communication, that is, how ||\u03b2\u0302t 1 \u03b2 || decrease with ||\u03b2\u0302t \u03b2 ||. Define\nL\u03031p\u03b2, \u03b2\u0302tq L1p\u03b2q C 1\nm \u00b8 jPrms\n\u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq,\u03b2 G .\nThen\n\u2207L\u03031p\u03b2, \u03b2\u0302tq \u2207L1p\u03b2q 1 m \u00b8 jPrms \u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq.\nThe following lemma bounds the `8 norm of \u2207L\u03031p\u03b2, \u03b2\u0302tq.\nLemma 3.5. With probability at least 1 \u03b4, we have \u2207L\u03031p\u03b2 , \u03b2\u0302tq\n8\n\u00a4 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 2L max j,i ||xji||28 ||\u03b2 \u03b2\u0302t||1\nc logp2p{\u03b4q\nn\nM max j,i\n||xji||38\n||\u03b2\u0302t \u03b2 ||21 .\nThe lemma bounds the magnitude of the gradient of the loss at optimum point \u03b2 . This will be used to guide our choice of the `1 regularization parameter \u03bbt 1 in (2.2). The following lemma shows that as long as \u03bbt 1 is large enough, it is guaranteed that \u03b2\u0302t 1 \u03b2 is in a restricted cone. Lemma 3.6. Suppose \u03bbt 1 is chosen as in (3.1). Then with probability at least 1 \u03b4, we have \u03b2\u0302t 1 \u03b2 P CpS, 3q.\nBased on the conic condition and restricted strong convexity condition, we can obtain the recursive error bound stated in Theorem 3.3 following the proof strategy as in Negahban et al. (2012)."}, {"heading": "4 Illustrative Examples", "text": "In this section we discuss some representative examples of high-dimensional statistical learning problems, which have been extensively studied in recent years. We will use bounds established in Section 3 to obtain guarantees of the proposed algorithm for these problems. For completeness, we first provide the definition of subgaussian norm (Vershynin, 2012).\nDefinition 4.1 (Subgaussian norm). The subgaussian norm ||X||\u03c82 of a subgaussian p-dimensional random vector X, is defined as\n||X||\u03c82 sup xPSp 1 sup q\u00a11 q 1{2pE|xX,xy|qq1{q,\nwhere Sp 1 is the p-dimensional unit sphere."}, {"heading": "4.1 Sparse Linear Regression", "text": "Sparse linear regression is the most widely studied model in high-dimensional statistics (Bu\u0308hlmann and van de Geer, 2011). In the sparse linear regression setting, data txji, yjiuiPrns,jPrms are generated according to the model\nyji xxji,\u03b2 y ji, (4.1) where ji are i.i.d. mean zero subgaussian random variables. For the regression problem, the typical choice of loss function is the squared loss `pyji, x\u03b2,xjiyq 12pyji x\u03b2,xjiyq2, and adding a `1 penalty to the empirical loss leads to the lasso estimator (Tibshirani, 1996)\n\u03b2\u0302centralize arg min \u03b2\n1\n2mn \u00b8 jPrms \u00b8 iPrns pyji x\u03b2,xjiyq2 \u03bb||\u03b2||1.\nIt is easy to see that the quadratic loss is 1-smooth. Let Ljp\u03b2q 12n \u00b0 iPrnspyji x\u03b2,xjiyq2. When xji are randomly drawn from a subgaussian distribution, L1p\u03b2q satisfies the restricted strong convexity condition defined in Assumption (3.2) with high probability as long as n \u00c1 s log p (Rudelson and Zhou, 2013). Moreover, we have the following control on the quantity 1m \u00b0jPrms\u2207Ljp\u03b2 q\n8\n.\nLemma 4.2. Suppose || ji||\u03c62 \u00a4 \u03c3 in model 4.1. Then with probability at least 1 \u03b4, 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 \u00a4 \u03c3||xji||8 c logpp{\u03b4q mn .\nWhen xji are drawn from a mean zero random subgaussian distribution, then ||xji||8 is upper bounded by constant with high probability.\nLemma 4.3. Suppose ||xji||\u03c62 \u00a4 \u03c3X . Then with probability at least 1 \u03b4, we have max\njPrms,iPrns ||xji||8 \u00a4 \u03c3X\na logpmnp{\u03b4q.\nThe following `1 error bound is standard for lasso with random design, which was established,\nfor example, in (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009)\nLemma 4.4. Under the model 4.1, we have the following estimation error bound for \u03b2\u03020 holds with probability at least 1 \u03b4: ||\u03b2\u03020 \u03b2 ||1 \u00a4 s\u03c3\u03c3X\n\u03ba\nc logpnp{\u03b4q\nn .\nWith above analysis for sparse linear regression model with random design, we are ready to\npresent the results for the estimation error bound.\nCorollary 4.5. Under sparse linear regression model (4.1) with subgaussian design matrix and noise, and set \u03bbt 1 as (3.1). Then with probability at least 1 2\u03b4, we have the following estimation error bounds for t \u00a5 0:\n||\u03b2\u0302t 1 \u03b2 ||1 \u00a41 a t 1 n 1 an 48s\u03c3\u03c3X \u03ba c logpp{\u03b4q mn at 1n s\u03c3\u03c3X \u03ba c logpnp{\u03b4q n , (4.2) ||\u03b2\u0302t 1 \u03b2 ||2 \u00a41 a t 1 n\n1 an 12 ? s\u03c3\u03c3X \u03ba c logpp{\u03b4q mn atnbn s\u03c3\u03c3X \u03ba c logpnp{\u03b4q n , (4.3)\nwhere\nan 96s\u03c3\u03c3X \u03ba\nc logp2p{\u03b4q\nn and bn 24 ? s\u03c3\u03c3X \u03ba\nc logpnp{\u03b4q\nn .\nRemark 1. We can further simplify the bound and look at the scaling with respect to n,m, s, p. When n \u00c1 s2 log p, it is easy to see by choosing\n\u03bbt 1 c log p mn c log p n s c log p n t 1 ,\nthe following error bound holds for the proposed algorithm:\n||\u03b2\u0302t 1 \u03b2 ||1 \u00c0P s c log p\nmn s\nc log p\nn\nt 2 ,\n||\u03b2\u0302t 1 \u03b2 ||2 \u00c0P c s log p mn c s log p n\ns\nc log p\nn\nt 1 .\nWe compare these bounds to the performance of local and centralized lasso (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009). For \u03b2\u0302local, we have\n||\u03b2\u0302local \u03b2 ||1 \u00c0P s c log p\nn and ||\u03b2\u0302local \u03b2 ||2 \u00c0P\nc s log p\nn .\nFor \u03b2\u0302centralize, we have\n||\u03b2\u0302centralize \u03b2 ||1 \u00c0P s c log p\nmn and ||\u03b2\u0302centralize \u03b2 ||2 \u00c0P\nc s log p\nmn .\nWe see that after one round of communications, by choosing\n\u03bb1 c log p\nmn s log p n ,\nwe have\n||\u03b2\u03021 \u03b2 ||1 \u00c0P s c log p\nmn s\n2 log p\nn and ||\u03b2\u03021 \u03b2 ||2 \u00c0P\nc s log p\nmn s\n3{2 log p\nn .\nThese bounds match the results in Lee et al. (2015b). Furthermore, when m \u00c0 n s2 log p , match the performance for centralized lasso. Moreover, as long as t \u00c1 logm and n \u00c1 s2 log p, it is easy to check that s b\nlog p n\nt 1 \u00c0 s b log p mn . Therefore,\n||\u03b2\u0302t 1 \u03b2 ||1 \u00c0P s c log p\nmn and ||\u03b2\u0302t 1 \u03b2 ||2 \u00c0P\nc s log p\nmn ,\nwhich matches the centralized lasso performance without any additional error term."}, {"heading": "4.2 Sparse Logistic Regression", "text": "Logistic regression is a popular classification model where the binary label yji P t 1, 1u is drawn according to a Bernoulli distribution:\nPpyji 1|xjiq exppxxji,\u03b2 yq\nexppxxji,\u03b2 yq 1 and Ppyji 1|xjiq 1 exppxxji,\u03b2 yq 1 . (4.4)\nFor logistic model, performing maximum likelihood estimation (MLE) leads to the logistic loss function `pyji, x\u03b2,xjiyq logp1 expp yjix\u03b2,xjiyqq. For high-dimensional problems, when we add\na `1 regularization, we obtain the `1 regularized logistic regression model (Zhu and Hastie, 2004; Wu et al., 2009):\n\u03b2\u0302centralize arg min \u03b2\n1\nmn \u00b8 jPrms \u00b8 iPrns logp1 expp yjix\u03b2,xjiyqq \u03bb||\u03b2||1.\nThe logistic loss is 14 -smooth, let Ljp\u03b2q 1n \u00b0 iPrns logp1 expp yjix\u03b2,xjiyqq, (Negahban et al., 2012) showed that if xji are drawn from mean zero distribution with sub-Gaussian tails, then L1p\u03b2q satisfies the restricted strong condition (3.2). Moreover, we have the following control on the quantity 1m \u00b0jPrms\u2207Ljp\u03b2 q\n8\n.\nLemma 4.6. Then we have the following upper bound holds in probability at least 1 \u03b4: 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 \u00a4 ||xji||8 c 2 log p mn .\nThe following `1 error bound states the estimation error for logistic regression with `1 regular-\nization, which was established, for example, in (van de Geer, 2008; Negahban et al., 2012).\nLemma 4.7. Under the model (4.4), we have the following estimation error bound for \u03b2\u03020 holds with probability at least 1 \u03b4:\n||\u03b2\u03020 \u03b2 ||1 \u00a4 s\u03c3X c\n2 logpnp{\u03b4q n .\nWith above analysis for sparse logistic regression model with random design, we are ready to present the results for the estimation error bound which established local exponential convergence.\nCorollary 4.8. Under sparse logistic regression model with random design, and set \u03bbt 1 as (3.1). If the following condition holds for some T \u00a5 0:\n||\u03b2\u0302T \u03b2 ||1 \u00a4 4 c\nlogp2p{\u03b4q n . (4.5)\nThen with probability at least 1 2\u03b4, we have the following estimation error bound for all t \u00a5 T :\n||\u03b2\u0302t 1 \u03b2 ||1 \u00a41 a t T 1 n 1 an 96s\u03c3\u03c3X \u03ba c logpp{\u03b4q mn 4at T 1n c logp2p{\u03b4q n , (4.6) ||\u03b2\u0302t 1 \u03b2 ||2 \u00a41 a t T 1 n\n1 an 4 ? s\u03c3\u03c3X \u03ba c logpp{\u03b4q mn 4at Tn bn c logp2p{\u03b4q n , (4.7)\nwhere\nan 24s\u03c3\u03c3X \u03ba\nc logp2p{\u03b4q\nn and bn\n? s\u03c3\u03c3X \u03ba\nc logpnp{\u03b4q\nn ."}, {"heading": "4.2.1 High-dimensional Generalized Linear Models", "text": "The results are readily extendable to other high-dimensional generalized linear models (McCullagh and Nelder, 1989; van de Geer, 2008), where the response variable yji P Y is drawn from the distribution\nPpyji|xjiq9 exp yjixxji,\u03b2 y \u03a6pxxji,\u03b2 yq\nAp\u03c3q ,\nwhere \u03a6p q is a link function and Ap\u03c3q is a scale parameter. Under the random subgaussian design, as long as the loss function has Lipschitz gradient, then the algorithm and corresponding estimation error bound and be applied."}, {"heading": "4.3 High-dimensional Graphical Models", "text": "The results can also be used for the distributed unsupervised learning setting where the task is to learn a sparse graphical structure that represents the conditional independence between variables. Widely studied graphical models are Gaussian graphical models (Meinshausen and Bu\u0308hlmann, 2006; Yuan and Lin, 2007) for continuous data and Ising graphical models (Ravikumar et al., 2010) for binary observations. As shown in (Meinshausen and Bu\u0308hlmann, 2006; Ravikumar et al., 2010), these model selection problems can be reduced to solving parallel `1 regularized linear regression and logistic regression problems, respectively. Thus the approach presented in this paper can be readily applicable for these tasks."}, {"heading": "5 Experiments", "text": "In this section we present extensive comparisons between various approaches on both simulated and real world datasets. We run the algorithms for both distributed regression and classification problems. The algorithms to be compared are:\n\u2022 Local: the first machine just solves a related `1 regularized problem (lasso or `1 regularized logistic regression) with the optimal \u03bb, and outputs the solution. Obviously this approach is\ncommunication free.\n\u2022 Centralize: the master gathers all data from different machines together, and solves a centralized `1 regularized loss minimization problem with the optimal \u03bb, and outputs the solution.\nThis approach is communication expensive as all data needs to be communicated, but it usually gives us the best estimation and prediction performance.\n\u2022 Prox GD: the distributed proximal gradient descent is ran on the `1 regularized objective, where we initialized the starting point with the first machine\u2019s solution.\n\u2022 Avg-Debias: the method proposed in Lee et al. (2015b), with fine tuned regularization and hard thresholding parameters. This approach only requires one round of communication,\nwhere each machine sends a p-dimensional vector. However, Avg-Debias is computationally prohibitive because of the debiasing operation.\n\u2022 EDSL: the proposed efficient distributed sparse learning approach, where the regularization level at each iteration is fine tuned."}, {"heading": "5.1 Simulations", "text": "We first examine the algorithms on simulated data. We generate txjiujPrms,iPrns from multivariate normal distribution with mean zero vector, and covariance matrix \u03a3, which controls the condition number of the problem. We will varying \u03a3 to see how it affects the performance of various methods. We set \u03a3ij 0.5|i j| for the well-conditioned setting, and \u03a3ij 0.5|i j|{5 for the ill-conditioned setting. The response variable tyjiujPrms,iPrns are drawn from (4.1) and (4.4) for regression and classification problems, respectively. For regression problems, the noise ji is sampled from a standard normal distribution. The true model \u03b2 is set to be s-sparse, where the first s-entries are sampled i.i.d. from a uniform distribution in r0, 1s, and the other entries are set to zero. We run experiments with various pn, p,m, sq settings2, and plot how the estimation error ||\u03b2\u0302t \u03b2 ||2 varies for Prox GD and the proposed EDSL algorithm with rounds of communications. We also plot the estimation error of Local, Avg-Debias, and Centralize as a horizontal line, where the communications are fixed for these algorithms3. Figure 1 and 2 summarize the results, where the plots are averaged across 10 independent trials. We have the following observations:\n\u2022 The Avg-Debias approach obtained much better estimation error than Local after one round of communication, and sometimes performed quite close to Centralize. However, in most\ncases there is still a gap compared with Centralize, especially when the problem is not wellconditioned, or the number of machines m is large.\n\u2022 When the problem is well conditioned (\u03a3ij 0.5|i j| case), Prox GD converges reasonably fast, however it becomes very slow when the condition number becomes bad (\u03a3ij 0.5|i j|{5 case). We expect similar phenomenon will to for other first-order distributed optimization\nalgorithms, such as accelerated proximal gradient or ADMM.\n\u2022 As theory suggested, EDSL obtains a solution that is competitive with Avg-Debias after one round of communication, The estimation error decreases to be truly competitive with\nthe Centralize within very few rounds of communications (typically less than 5, where the theory suggested EDSL will match the performance of Centralize within Oplogmq rounds of communications).\nAbove experiments validate the theoretical analysis that when the additional error term in AvgDebias is relatively large, one round of communication is not sufficient to match the performance of centralized procedure. However, EDSL could match the performance of Avg-Debias also with one round of communication, and further improves the estimation quality by exponentially reducing the additional error until matching the centralized lasso performance, within a few rounds of communications. Thus the proposed EDSL improves the Avg-Debias approach both computationally and statistically.\n2n: sample size per machine, p: problem dimension, m: number of machines, s: true support size. 3these algorithms have zero, one-shot and full communications, respectively."}, {"heading": "5.2 Real-world Data Evaluation", "text": "In this section, we compare the distributed sparse learning algorithms on several real world datasets, which are publicly available from the LIBSVM website4 and UCI Machine Learning Repository5. The statistics of these datasets are summarized in Table 2, where some of the multi-class classification datasets are adopted under the regression setting with squared losses. For all of the data, we use 60% of them for training, and 20% as held-out validation set for tuning the parameters, and the remaining 20% for testing. We test 10 randomly partitions of the training, validation and testing sets and report the averaged performance on testing datasets. For regression tasks, the evaluation metric is the normalized Mean Squared Error (normalized MSE), for classification tasks we report the classification error. We randomly partition the data on m 10 machines and run various algorithms tested in the simulation. The results are plotted in Figure 3 where for some datasets the performance of Avg-Debias is significantly worse than others (mostly because the debiasing step fails), thus we omit these plots. We make the following observations based on Figure 3:\n\u2022 Since there is no well-specified model on these datasets, the curves behave quite differently on different datasets. But overall there is a large gap between the Local solution and Centralized\nprocedure, where the later uses 10 times more data.\n\u2022 Avg-Debias often fails on these real datasets, and performs much worse than in simulations. The main reason might be becasue when the assumptions such as well-specified model or\ngeneralized coherence condition fail, Avg-Debias can totally fail and produce solution even much worse than the local.\n\u2022 Prox GD approach still converges quite slow in most of the cases. 4https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/ 5http://archive.ics.uci.edu/ml/\n\u2022 The proposed EDSL are quite robust on real world datasets, and can output a solution which is highly competitive with the centralized model within a few rounds of communications.\n\u2022 There exits a \u201czig-zag\u201d behavior for EDSL approach on some datasets, for example, on mushrooms dataset, the predictive performance of EDSL is not that stable.\nThe experimental results on real world datasets again verified that the proposed EDSL method is an effective method for distributed sparse learning, while maintain efficiency in both computation and communication."}, {"heading": "6 Conclusion and Discussion", "text": "We proposed a novel approach for distributed learning with sparsity, which is efficient in both computation and communication. Our theoretical analysis showed that the proposed method works under weaker conditions than the averaging the debiased estimator. Furthermore, estimation error can be improved over a few rounds of computation as the additional error term exponentially decreases, until matching the centralized procedure. Extensive experiments on both simulated and real-world data demonstrate that the proposed method improves the performance over one shot averaging approach within just a few rounds of communications.\nThere might be several ways to improve this work further. As we see in real data experiments, the proposed approach can still perform slightly worse than the centralized approach on certain datasets. It is interesting to explore how to make EDSL provably work under even weaker assumptions. For example, EDSL requires Ops2 log pq samples per machine to match the centralized method in Oplogmq rounds of communications, however, it is not clear whether the sample size requirement can be improved, while still maintaining low-communication cost. Last but not least, it is interesting to explore the ideas presented to improve the computational cost of communicationefficient distributed multi-task learning with shared support (Wang et al., 2015)."}, {"heading": "A Appendix", "text": "The appendix contains some theorems and lemmas stated in the main paper.\nA.1 Proof of Lemma 3.5\nProof. Based on the definition we know\n\u2207L\u03031p\u03b2 , \u03b2\u0302tq \u2207L1p\u03b2 q 1 m \u00b8 jPrms \u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq\n1 m \u00b8 jPrms \u2207Ljp\u03b2 q \u2207L1p\u03b2 q \u2207L1p\u03b2\u0302tq 1 m \u00b8 jPrms \u2207Ljp\u03b2 q 1 m \u00b8 jPrms \u2207Ljp\u03b2\u0302tq .\nFor the term \u2207L1p\u03b2 q \u2207L1p\u03b2\u0302tq, we have\n\u2207L1p\u03b2 q \u2207L1p\u03b2\u0302tq 1 n \u00b8 iPrns xip`1py1i, x\u03b2 ,x1iyq `1py1i, x\u03b2\u0302t,x1iyqq.\nFor the term 1m \u00b0 jPrms\u2207Ljp\u03b2 q 1m \u00b0 jPrms\u2207Ljp\u03b2\u0302tq, we have\n1\nm \u00b8 jPrms \u2207Ljp\u03b2 q 1 m \u00b8 jPrms \u2207Ljp\u03b2\u0302tq 1 mn \u00b8 jPrms \u00b8 iPrns xip`1pyji, x\u03b2 ,xjiyq `1pyji, x\u03b2\u0302t,xjiyqq.\nDefine random vectors vjip\u03b2\u0302tq P Rp:\nvjip\u03b2\u0302tq xjip`1pyji, x\u03b2 ,xjiyq `1pyji, x\u03b2\u0302t,xjiyqq.\nBy Taylor series expansion we have\n`1pyji, x\u03b2\u0302t,xjiyq `1pyji, x\u03b2 ,xjiyq `2pyji, x\u03b2 ,xjiyqpx\u03b2\u0302t \u03b2 ,xjiyq ` 3pyji,ujiq\n2 px\u03b2\u0302t \u03b2 ,xjiyq2.\nwhere uji is a number between x\u03b2\u0302t,xjiy and x\u03b2 ,xjiy. Define \u03c4ji `1pyji, x\u03b2 ,xjiyq, we have 1n \u00b8 iPrns v1ip\u03b2\u0302tq 1 mn \u00b8 j \u00b8 i vjip\u03b2\u0302tq 8 \u00a4 1n \u00b8 i \u03c41ix1ix T 1ip\u03b2\u0302t \u03b2 q 1 mn \u00b8 j \u00b8 i \u03c4jixjix T jip\u03b2\u0302t \u03b2 q 8\nM max j,i\n||xji||38\n||\u03b2\u0302t \u03b2 ||21 .\nWe can further upper bound 1n \u00b0i \u03c41ix1ixT1ip\u03b2\u0302t \u03b2 q 1mn \u00b0j\u00b0i \u03c4jixjixTjip\u03b2\u0302t \u03b2 q\n8\nby\n1n \u00b8 j \u03c41ix1ix T 1ip\u03b2\u0302t \u03b2 q\n1\nmn \u00b8 j \u00b8 i \u03c4jixjix T jip\u03b2\u0302t \u03b2 q 8 \u00a4 1n \u00b8 j \u03c41ix1ix T 1i\n1\nmn \u00b8 j \u00b8 i \u03c4jixjix T ji 8\n||\u03b2\u0302t \u03b2 ||1.\nAlso Since 1n \u00b8 i \u03c41ix1ix T 1i\n1\nmn \u00b8 j \u00b8 i \u03c4jixjix T ji 8 \u00a4 1n \u00b8 iPrns \u03c41ix1ix T 1i E \u03c4jixjix T ji 8\n1mn \u00b8 j \u00b8 i \u03c4jixjix T ji E \u03c4jixjix T ji 8 .\nSince |\u03c4ji| \u00a4 L, by Hoeffding inequality with a union bound over rps, we have with probability at least 1 \u03b4,\n1n \u00b8 iPrns \u03c41ix1ix T 1i E \u03c4jixjix T ji 8 \u00a4 L max j,i\n||xji||28 c 2 logp2p{\u03b4q n ,\nand 1mn \u00b8 j \u00b8 i \u03c4jixjix T ji E \u03c4jixjix T ji 8 \u00a4 L max j,i\n||xji||28 c 2 logp2p{\u03b4q mn .\nCombining above, with probability at least 1 \u03b4 the following inequality holds: \u2207L\u03031p\u03b2 , \u03b2\u0302tq\n8\n\u00a4 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 L max j,i ||xji||28 ||\u03b2 \u03b2\u0302t||1\nc 4 logp2p{\u03b4q\nn\nM max j,i\n||xji||38\n||\u03b2\u0302t \u03b2 ||21 .\nA.2 Proof of Lemma 3.6\nProof. The proof uses ideas presented in (Negahban et al., 2012). By triangle inequality we have\n||\u03b2\u0302t 1||1 ||\u03b2 ||1 ||\u03b2 p\u03b2\u0302t 1 \u03b2 qSc p\u03b2\u0302t 1 \u03b2 qS ||1 ||\u03b2 ||1 \u00a5||\u03b2 p\u03b2\u0302t 1 \u03b2 qSc ||1 ||p\u03b2\u0302t 1 \u03b2 qS ||1 ||\u03b2 ||1 ||p\u03b2\u0302t 1 \u03b2 qSc ||1 ||p\u03b2\u0302t 1 \u03b2 qS ||1.\nBy the optimality of \u03b2\u0302t 1 for (2.2), we have\nL\u03031p\u03b2\u0302t 1, \u03b2\u0302tq \u03bbt 1||\u03b2\u0302t 1||1 L\u03031p\u03b2 , \u03b2\u0302tq \u03bbt 1||\u03b2 ||1 \u00a4 0. Thus L\u03031p\u03b2\u0302t 1, \u03b2\u0302tq L\u03031p\u03b2 , \u03b2\u0302tq \u03bbt 1p||p\u03b2\u0302t 1 \u03b2 qSc ||1 ||p\u03b2\u0302t 1 \u03b2 qS ||1q \u00a4 0. By the convexity of L\u03031p , \u03b2\u0302tq, we further have\nL\u03031p\u03b2\u0302t 1, \u03b2\u0302tq L\u03031p\u03b2 , \u03b2\u0302tq \u00a5 x\u2207L\u03031p\u03b2 , \u03b2\u0302tq, \u03b2\u0302t 1 \u03b2 y. Thus by Ho\u0308lder\u2019s inequality\n0 \u00a5x\u2207L\u03031p\u03b2 , \u03b2\u0302tq, \u03b2\u0302t 1 \u03b2 y \u03bbt 1p||p\u03b2\u0302t 1 \u03b2 qSc ||1 ||p\u03b2\u0302t 1 \u03b2 qS ||1q \u00a5 ||\u2207L\u03031p\u03b2 , \u03b2\u0302tq||8||\u03b2\u0302t 1 \u03b2 ||1 \u03bbt 1p||p\u03b2\u0302t 1 \u03b2 qSc ||1 ||p\u03b2\u0302t 1 \u03b2 qS ||1q.\nBy Lemma 3.5 and (3.1), we know with probability at least 1 \u03b4,\n\u03bbt 1 2 ||\u03b2\u0302t 1 \u03b2 ||1 \u00a5 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 L max j,i ||xji||28 ||\u03b2 \u03b2t||1\nc 4 logp2p{\u03b4q\nn\n||\u03b2\u0302t 1 \u03b2 ||1 M\nmax j,i\n||xji||38\n||\u03b2\u0302t \u03b2 ||21 ||\u03b2\u0302t 1 \u03b2 ||1\n\u00a5||\u2207L\u03031p\u03b2 , \u03b2\u0302tq||8p||p\u03b2\u0302t 1 \u03b2 qSc ||1 ||p\u03b2\u0302t 1 \u03b2 qS ||1q \u00a5\u03bbt 1p||p\u03b2\u0302t 1 \u03b2 qSc ||1 ||p\u03b2\u0302t 1 \u03b2 qS ||1 ||\u03b2\u0302t 1 \u03b2 ||1q.\nSo we obtain\n0 \u00a5||p\u03b2\u0302t 1 \u03b2 qSc ||1 ||p\u03b2\u0302t 1 \u03b2 qS ||1 1 2 ||\u03b2\u0302t 1 \u03b2 ||1\n1 2 ||p\u03b2\u0302t 1 \u03b2 qSc ||1 3 2 ||p\u03b2\u0302t 1 \u03b2 qS ||1,\nwhich concludes the proof.\nA.3 Proof of Theorem 3.3\nProof. For the term L\u03031p\u03b2\u0302t 1, \u03b2\u0302tq L\u03031p\u03b2 , \u03b2\u0302tq we have\nL\u03031p\u03b2\u0302t 1, \u03b2\u0302tq L\u03031p\u03b2 , \u03b2\u0302tq L1p\u03b2\u0302t 1q C 1\nm \u00b8 jPrms\n\u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq, \u03b2\u0302t 1 G\nL1p\u03b2 q C 1\nm \u00b8 jPrms\n\u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq,\u03b2 G\n\u00a5x\u2207L1p\u03b2 q, \u03b2\u0302t 1 \u03b2 y \u03ba||\u03b2\u0302t 1 \u03b2 ||22 C\n1\nm \u00b8 jPrms\n\u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq, \u03b2\u0302t 1 G\nC\n1\nm \u00b8 jPrms\n\u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq,\u03b2 G\nC \u2207L1p\u03b2 q 1\nm \u00b8 jPrms\n\u2207Ljp\u03b2\u0302tq \u2207L1p\u03b2\u0302tq, \u03b2\u0302t 1 \u03b2 G\n\u03ba||\u03b2\u0302t 1 \u03b2 ||22 x\u2207L\u03031p\u03b2 , \u03b2\u0302tq, \u03b2\u0302t 1 \u03b2 y \u03ba||\u03b2\u0302t 1 \u03b2 ||22,\nwhere the first inequality we use the restricted strong convexity condition (3.2). Also by the optimality of \u03b2\u0302t 1 for (2.2), we have\nL\u03031p\u03b2\u0302t 1, \u03b2\u0302tq L\u03031p\u03b2 , \u03b2\u0302tq \u03bbt 1||\u03b2\u0302t 1||1 \u03bbt 1||\u03b2 ||1 \u00a4 0.\nCombining above two inequalities we obtain with probability at least 1 \u03b4:\n\u03bbt 1||\u03b2 ||1 \u03bbt 1||\u03b2\u0302t 1||1 \u00a5x\u2207L\u03031p\u03b2 , \u03b2\u0302tq, \u03b2\u0302t 1 \u03b2 y \u03ba||\u03b2\u0302t 1 \u03b2 ||22 \u00a5 ||\u2207L\u03031p\u03b2 , \u03b2\u0302tq||8||\u03b2\u0302t 1 \u03b2 ||1 \u03ba||\u03b2\u0302t 1 \u03b2 ||22 \u00a5 \u03bbt 1\n2 ||\u03b2\u0302t 1 \u03b2 ||1 \u03ba||\u03b2\u0302t 1 \u03b2 ||22.\nBy triangle inequality that \u03bbt 1||\u03b2\u0302t 1 \u03b2 ||1 \u00a5 \u03bbt 1||\u03b2 ||1 \u03bbt 1||\u03b2\u0302t 1||1, we have\n\u03ba||\u03b2\u0302t 1 \u03b2 ||22 \u00a4 3\u03bbt 1\n2 ||\u03b2\u0302t 1 \u03b2 ||1\n3\u03bbt 1 2 p||p\u03b2\u0302t 1 \u03b2 qS ||1 ||p\u03b2\u0302t 1 \u03b2 qSc ||1q\n\u00a43\u03bbt 1 2 p||p\u03b2\u0302t 1 \u03b2 qS ||1 3||p\u03b2\u0302t 1 \u03b2 qS ||1q 6\u03bbt 1||p\u03b2\u0302t 1 \u03b2 qS ||1 \u00a46?s\u03bbt 1||p\u03b2\u0302t 1 \u03b2 qS ||2 \u00a46?s\u03bbt 1||\u03b2\u0302t 1 \u03b2 ||2.\nWe get\n||\u03b2\u0302t 1 \u03b2 ||2 \u00a4 6 ? s\u03bbt 1 \u03ba .\nSubstitute \u03bbt 1 in (3.1) concludes the proof for `2 estimation error bound. For ||\u03b2\u0302t 1 \u03b2 ||1, we know\n||\u03b2\u0302t 1 \u03b2 ||1 \u00a4||p\u03b2\u0302t 1 \u03b2 qS ||1 ||p\u03b2\u0302t 1 \u03b2 qSc ||1 \u00a44||p\u03b2\u0302t 1 \u03b2 qS ||1 \u00a4 4 ? s||p\u03b2\u0302t 1 \u03b2 qS ||2\n\u00a44?s||\u03b2\u0302t 1 \u03b2 ||2 \u00a4 24s\u03bbt 1 \u03ba ,\nwhich obtains the desired bound.\nA.4 Proof of Corollary 3.4\nProof. The proof proceeds by recursively applying Theorem 3.3 and sum a geometric sequence. For notation simplicity let\na 48s \u03ba 1m \u00b8 jPrms \u2207Ljp\u03b2 q 8 ,\nb\n48sL\n\u03ba\nmax j,i\n||xji||28 c 4 logp2p{\u03b4q n ,\nc 48sM \u03ba max j,i ||xji||38 .\nBy Theorem 3.3 we have\n||\u03b2\u0302t 1 \u03b2 ||1 \u00a4a b||\u03b2\u0302t \u03b2 ||1 c||\u03b2\u0302t \u03b2 ||21 \u00a4a 2b||\u03b2\u0302t \u03b2 ||1 \u00a4a 2bpa 2b||\u03b2\u0302t 1 \u03b2 ||1q \u00a4 . . .\n\u00a4a t\u0327\nk 0\np2bqk p2bqt 1||\u03b2\u03020 \u03b2 ||1.\nap1 p2bq t 1q\n1 2b p2bq t 1||\u03b2\u03020 \u03b2 ||1, (A.1)\nwhich completes the `1 estimation error bound. For ||\u03b2\u0302t 1 \u03b2 ||2, we first use (A.1) to obtain\n||\u03b2\u0302t \u03b2 ||1 \u00a4 ap1 p2bq tq\n1 p2bq p2bq t||\u03b2\u03020 \u03b2 ||1.\nThen apply Theorem 3.3 to obtain that\n||\u03b2\u0302t 1 \u03b2 ||2 \u00a4 a 4 ? s p2bq 4 ? s ||\u03b2\u0302t \u03b2 ||1 \u00a4 a 4 ? s b 4 ? s ap1 p2bqtq 1 p2bq p2bq t||\u03b2\u03020 \u03b2 ||1\n1 4 ? s\na app2bq p2bq t 1q 1 p2bq\np2bq t 1||\u03b2\u03020 \u03b2 ||1 4 ? s\nap1 p2bq t 1q 4 ? sp1 p2bqq\np2bqt 1||\u03b2\u03020 \u03b2 ||1 4 ? s ,\nwhich concludes the proof.\nA.5 Proof of Lemma 4.2\nProof. By the definition of Ljp\u03b2q, we have 1\nm \u00b8 jPrms \u2207Ljp\u03b2 q 1 mn \u00b8 jPrms \u00b8 iPrns xji ji.\nSince ji is mean zero subgaussian with \u03c62 norm bounded \u03c3, then xji ji is also mean zero subgaussian with \u03c62 norm bounded \u03c3maxji p||xji||8q, then apply Hoeffding-type inequality (Proposition 5.10 in (Vershynin, 2012)) and an union bound over rps leads to the desired bound.\nA.6 Proof of Lemma 4.3\nProof. Applying Hoeffding-type inequality (Proposition 5.10 in (Vershynin, 2012)) and an union bound over j P rms, i P rns and rps leads to the desired bound.\nA.7 Proof of Lemma 4.6\nProof. By the definition of Ljp\u03b2q, we have 1\nm \u00b8 jPrms \u2207Ljp\u03b2 q 1 mn \u00b8 jPrms \u00b8 iPrns xji yji yji 1 expp yjix\u03b2,xjiyq .\nIt is easy to check that\nE yji yji\n1 expp yjix\u03b2,xjiyq 0, and\nyji yji1 expp yjix\u03b2,xjiyq \u00a4 1.\nand thus\nE xji yji yji\n1 expp yjix\u03b2,xjiyq 0, xji yji yji\n1 expp yjix\u03b2,xjiyq 8\n\u00a4 max ji p||xji||8q .\nthen apply Azuma-Hoeffding inequality (Hoeffding, 1963) and an union bound over rps leads to the desired bound."}], "references": [{"title": "Communication complexity of distributed convex learning and optimization", "author": ["Y. Arjevani", "O. Shamir"], "venue": "ArXiv e-prints,", "citeRegEx": "Arjevani and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Arjevani and Shamir.", "year": 2015}, {"title": "Distributed learning, communication complexity and privacy", "author": ["M.-F. Balcan", "A. Blum", "S. Fine", "Y. Mansour"], "venue": "JMLR W&CP 23: COLT 2012,", "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "Distributed estimation and inference with statistical guarantees", "author": ["H. Battey", "J. Fan", "H. Liu", "J. Lu", "Z. Zhu"], "venue": "ArXiv e-prints,", "citeRegEx": "Battey et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Battey et al\\.", "year": 2015}, {"title": "Simultaneous analysis of lasso and Dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": "Ann. Stat.,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S.P. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Communication lower bounds for statistical estimation problems via a distributed data processing inequality", "author": ["M. Braverman", "A. Garg", "T. Ma", "H.L. Nguyen", "D.P. Woodruff"], "venue": "ArXiv e-prints,", "citeRegEx": "Braverman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2015}, {"title": "Statistics for high-dimensional data", "author": ["P. B\u00fchlmann", "S.A. van de Geer"], "venue": null, "citeRegEx": "B\u00fchlmann and Geer.,? \\Q2011\\E", "shortCiteRegEx": "B\u00fchlmann and Geer.", "year": 2011}, {"title": "Computational limits of divide-and-conquer method", "author": ["G. Cheng", "Z. Shang"], "venue": "ArXiv e-prints,", "citeRegEx": "Cheng and Shang.,? \\Q2015\\E", "shortCiteRegEx": "Cheng and Shang.", "year": 2015}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Dual averaging for distributed optimization: convergence analysis and network scaling", "author": ["J.C. Duchi", "A. Agarwal", "M.J. Wainwright"], "venue": "IEEE Trans. Automat. Control,", "citeRegEx": "Duchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2012}, {"title": "Optimality guarantees for distributed statistical estimation", "author": ["J.C. Duchi", "M.I. Jordan", "M.J. Wainwright", "Y. Zhang"], "venue": "ArXiv e-prints,", "citeRegEx": "Duchi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2014}, {"title": "Statistical learning with sparsity: the lasso and generalizations", "author": ["T.J. Hastie", "R.J. Tibshirani", "M.J. Wainwright"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2015}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "J. Am. Stat. Assoc.,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "A distributed one-step estimator", "author": ["C. Huang", "X. Huo"], "venue": "ArXiv e-prints,", "citeRegEx": "Huang and Huo.,? \\Q2015\\E", "shortCiteRegEx": "Huang and Huo.", "year": 2015}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Tak\u00e1c", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Confidence intervals and hypothesis testing for high-dimensional regression", "author": ["A. Javanmard", "A. Montanari"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Javanmard and Montanari.,? \\Q2014\\E", "shortCiteRegEx": "Javanmard and Montanari.", "year": 2014}, {"title": "Communication-efficient distributed statistical learning", "author": ["M.I. Jordan", "J.D. Lee", "Y. Yang"], "venue": "Working Paper,", "citeRegEx": "Jordan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 2016}, {"title": "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity", "author": ["J.D. Lee", "Q. Lin", "T. Ma", "T. Yang"], "venue": "ArXiv e-prints,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Communication-efficient sparse regression: a one-shot approach", "author": ["J.D. Lee", "Y. Sun", "Q. Liu", "J.E. Taylor"], "venue": "ArXiv e-prints,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Nonparametric heterogeneity testing for massive data", "author": ["J. Lu", "G. Cheng", "H. Liu"], "venue": "ArXiv e-prints,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["C. Ma", "V. Smith", "M. Jaggi", "M.I. Jordan", "P. Richtrik", "M. Tak"], "venue": "ArXiv e-prints,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Generalized linear models. Monographs on Statistics and Applied Probability", "author": ["P. McCullagh", "J.A. Nelder"], "venue": null, "citeRegEx": "McCullagh and Nelder.,? \\Q1989\\E", "shortCiteRegEx": "McCullagh and Nelder.", "year": 1989}, {"title": "Efficient large-scale distributed training of conditional maximum entropy models", "author": ["R. Mcdonald", "M. Mohri", "N. Silberman", "D. Walker", "G.S. Mann"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mcdonald et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mcdonald et al\\.", "year": 2009}, {"title": "High dimensional graphs and variable selection with the lasso", "author": ["N. Meinshausen", "P. B\u00fchlmann"], "venue": "Ann. Stat.,", "citeRegEx": "Meinshausen and B\u00fchlmann.,? \\Q2006\\E", "shortCiteRegEx": "Meinshausen and B\u00fchlmann.", "year": 2006}, {"title": "Lasso-type recovery of sparse representations for high-dimensional data", "author": ["N. Meinshausen", "B. Yu"], "venue": "Ann. Stat.,", "citeRegEx": "Meinshausen and Yu.,? \\Q2009\\E", "shortCiteRegEx": "Meinshausen and Yu.", "year": 2009}, {"title": "A unified framework for highdimensional analysis of m-estimators with decomposable regularizers", "author": ["S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Stat. Sci.,", "citeRegEx": "Negahban et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2012}, {"title": "A method of solving a convex programming problem with convergence rate op1{k2q", "author": ["Y. Nesterov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Restricted eigenvalue properties for correlated gaussian designs", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Raskutti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raskutti et al\\.", "year": 2010}, {"title": "High-dimensional ising model selection using `1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "Ann. Stat.,", "citeRegEx": "Ravikumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2010}, {"title": "On the optimality of averaging in distributed statistical learning", "author": ["J. Rosenblatt", "B. Nadler"], "venue": "ArXiv e-prints,", "citeRegEx": "Rosenblatt and Nadler.,? \\Q2014\\E", "shortCiteRegEx": "Rosenblatt and Nadler.", "year": 2014}, {"title": "Reconstruction from anisotropic random measurements", "author": ["M. Rudelson", "S. Zhou"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Rudelson and Zhou.,? \\Q2013\\E", "shortCiteRegEx": "Rudelson and Zhou.", "year": 2013}, {"title": "Distributed stochastic optimization and learning", "author": ["O. Shamir", "N. Srebro"], "venue": "In 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton),", "citeRegEx": "Shamir and Srebro.,? \\Q2014\\E", "shortCiteRegEx": "Shamir and Srebro.", "year": 2014}, {"title": "Communication efficient distributed optimization using an approximate newton-type method", "author": ["O. Shamir", "N. Srebro", "T. Zhang"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R.J. Tibshirani"], "venue": "J. R. Stat. Soc. B,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "High-dimensional generalized linear models and the lasso", "author": ["S.A. van de Geer"], "venue": "Ann. Stat.,", "citeRegEx": "Geer.,? \\Q2008\\E", "shortCiteRegEx": "Geer.", "year": 2008}, {"title": "On the conditions used to prove oracle results for the lasso", "author": ["S.A. van de Geer", "P. B\u00fchlmann"], "venue": "Electron. J. Stat.,", "citeRegEx": "Geer and B\u00fchlmann.,? \\Q2009\\E", "shortCiteRegEx": "Geer and B\u00fchlmann.", "year": 2009}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": null, "citeRegEx": "Vershynin.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin.", "year": 2012}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1constrained quadratic programming (lasso)", "author": ["M.J. Wainwright"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "Wainwright.,? \\Q2009\\E", "shortCiteRegEx": "Wainwright.", "year": 2009}, {"title": "Distributed multitask learning", "author": ["J. Wang", "M. Kolar", "N. Srebro"], "venue": "ArXiv e-prints,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Genome-wide association analysis by lasso penalized logistic regression", "author": ["T.T. Wu", "Y.F. Chen", "T.J. Hastie", "E. Sobel", "K.L. Lange"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yang.,? \\Q2013\\E", "shortCiteRegEx": "Yang.", "year": 2013}, {"title": "Model selection and estimation in the gaussian graphical model", "author": ["M. Yuan", "Y. Lin"], "venue": null, "citeRegEx": "Yuan and Lin.,? \\Q2007\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2007}, {"title": "Confidence intervals for low dimensional parameters in high dimensional linear models", "author": ["C.-H. Zhang", "S.S. Zhang"], "venue": "J. R. Stat. Soc. B,", "citeRegEx": "Zhang and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Zhang and Zhang.", "year": 2013}, {"title": "Communication-efficient distributed optimization of self-concordant empirical loss", "author": ["Y. Zhang", "L. Xiao"], "venue": "ArXiv e-prints,", "citeRegEx": "Zhang and Xiao.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Xiao.", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Y. Zhang", "M.J. Wainwright", "J.C. Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Information-theoretic lower bounds for distributed statistical estimation with communication constraints", "author": ["Y. Zhang", "J.C. Duchi", "M.I. Jordan", "M.J. Wainwright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Y. Zhang", "J.C. Duchi", "M.J. Wainwright"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates", "author": ["Y. Zhang", "J.C. Duchi", "M.J. Wainwright"], "venue": "arXiv preprint arXiv:1305.5029,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "A partially linear framework for massive heterogeneous data", "author": ["T. Zhao", "G. Cheng", "H. Liu"], "venue": "ArXiv e-prints,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "A general framework for robust testing and confidence regions in high-dimensional quantile regression", "author": ["T. Zhao", "M. Kolar", "H. Liu"], "venue": "ArXiv e-prints,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "Classification of gene microarrays by penalized logistic regression", "author": ["J. Zhu", "T.J. Hastie"], "venue": null, "citeRegEx": "Zhu and Hastie.,? \\Q2004\\E", "shortCiteRegEx": "Zhu and Hastie.", "year": 2004}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "A.J. Smola", "L. Li"], "venue": "In Advances in Neural Information Processing,", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "Learning a sparse \u03b2 in a high-dimensional setting is a well studied statistical problem (B\u00fchlmann and van de Geer, 2011; Hastie et al., 2015), however, it creates unique computational challenges in the distributed setting that we address here.", "startOffset": 88, "endOffset": 141}, {"referenceID": 31, "context": "In the paper, we assume the communication occurs in rounds, where in each round, machines exchange messages with the master machine and, between two rounds, the machines only compute based on their local information, which includes local data points and messages received before (Zhang et al., 2013b; Shamir and Srebro, 2014; Arjevani and Shamir, 2015).", "startOffset": 279, "endOffset": 352}, {"referenceID": 0, "context": "In the paper, we assume the communication occurs in rounds, where in each round, machines exchange messages with the master machine and, between two rounds, the machines only compute based on their local information, which includes local data points and messages received before (Zhang et al., 2013b; Shamir and Srebro, 2014; Arjevani and Shamir, 2015).", "startOffset": 279, "endOffset": 352}, {"referenceID": 42, "context": "(2015b) studied a one-shot approach to learning \u03b2 based on averaging the debiased lasso estimators (Avg-Debias) (Zhang and Zhang, 2013).", "startOffset": 112, "endOffset": 135}, {"referenceID": 42, "context": "In particular, the debiasing step requires solving Oppq generalized lasso problems, which is computationally prohibitive for high-dimensional problems (Zhang and Zhang, 2013; Javanmard and Montanari, 2014).", "startOffset": 151, "endOffset": 205}, {"referenceID": 15, "context": "In particular, the debiasing step requires solving Oppq generalized lasso problems, which is computationally prohibitive for high-dimensional problems (Zhang and Zhang, 2013; Javanmard and Montanari, 2014).", "startOffset": 151, "endOffset": 205}, {"referenceID": 16, "context": "In a related setting, Lee et al. (2015b) studied a one-shot approach to learning \u03b2 based on averaging the debiased lasso estimators (Avg-Debias) (Zhang and Zhang, 2013).", "startOffset": 22, "endOffset": 41}, {"referenceID": 8, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 51, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 4, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 1, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 40, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 14, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 20, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 31, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 43, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 0, "context": "See for example, (Dekel et al., 2012; Duchi et al., 2012, 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir and Srebro, 2014; Zhang and Xiao, 2015; Lee et al., 2015a; Arjevani and Shamir, 2015) and reference there in.", "startOffset": 17, "endOffset": 290}, {"referenceID": 22, "context": "formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang and Huo, 2015).", "startOffset": 37, "endOffset": 125}, {"referenceID": 51, "context": "formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang and Huo, 2015).", "startOffset": 37, "endOffset": 125}, {"referenceID": 44, "context": "formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang and Huo, 2015).", "startOffset": 37, "endOffset": 125}, {"referenceID": 13, "context": "formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang and Huo, 2015).", "startOffset": 37, "endOffset": 125}, {"referenceID": 7, "context": "Divide-and-conquer procedures also found applications in statistical inference (Zhao et al., 2014a; Cheng and Shang, 2015; Lu et al., 2016).", "startOffset": 79, "endOffset": 139}, {"referenceID": 19, "context": "Divide-and-conquer procedures also found applications in statistical inference (Zhao et al., 2014a; Cheng and Shang, 2015; Lu et al., 2016).", "startOffset": 79, "endOffset": 139}, {"referenceID": 26, "context": "However, their communication complexity bounds have a bad dependence on the condition number, resulting in a procedure which is not better than first-order approaches in terms of communication, such as (proximal) accelerated gradient descent (Nesterov, 1983).", "startOffset": 242, "endOffset": 258}, {"referenceID": 4, "context": "(2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al., 2011; Shamir and Srebro, 2014; Ma et al., 2015).", "startOffset": 255, "endOffset": 316}, {"referenceID": 31, "context": "(2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al., 2011; Shamir and Srebro, 2014; Ma et al., 2015).", "startOffset": 255, "endOffset": 316}, {"referenceID": 20, "context": "(2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al., 2011; Shamir and Srebro, 2014; Ma et al., 2015).", "startOffset": 255, "endOffset": 316}, {"referenceID": 5, "context": "Lower bounds were studied in (Zhang et al., 2013a; Braverman et al., 2015; Arjevani and Shamir, 2015).", "startOffset": 29, "endOffset": 101}, {"referenceID": 0, "context": "Lower bounds were studied in (Zhang et al., 2013a; Braverman et al., 2015; Arjevani and Shamir, 2015).", "startOffset": 29, "endOffset": 101}, {"referenceID": 38, "context": "(2015b) for distributed estimation, multi-task learning (Wang et al., 2015), and statistical inference (Battey et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 2, "context": ", 2015), and statistical inference (Battey et al., 2015).", "startOffset": 35, "endOffset": 56}, {"referenceID": 16, "context": "Concurrent work of (Jordan et al., 2016) (personal communication) also improved computational efficiency of Lee et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions.", "startOffset": 9, "endOffset": 75}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions.", "startOffset": 9, "endOffset": 108}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al.", "startOffset": 9, "endOffset": 237}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al. (2014), and Ma et al.", "startOffset": 9, "endOffset": 258}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al. (2014), and Ma et al. (2015) studied distributed optimization using stochastic dual coordinate descent.", "startOffset": 9, "endOffset": 280}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al. (2014), and Ma et al. (2015) studied distributed optimization using stochastic dual coordinate descent. These approaches try to find a good balance between computation and communication. However, their communication complexity bounds have a bad dependence on the condition number, resulting in a procedure which is not better than first-order approaches in terms of communication, such as (proximal) accelerated gradient descent (Nesterov, 1983). Shamir et al. (2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al.", "startOffset": 9, "endOffset": 719}, {"referenceID": 3, "context": ", 2014a; Cheng and Shang, 2015; Lu et al., 2016). Shamir and Srebro (2014) and Rosenblatt and Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimensions. Yang (2013), Jaggi et al. (2014), and Ma et al. (2015) studied distributed optimization using stochastic dual coordinate descent. These approaches try to find a good balance between computation and communication. However, their communication complexity bounds have a bad dependence on the condition number, resulting in a procedure which is not better than first-order approaches in terms of communication, such as (proximal) accelerated gradient descent (Nesterov, 1983). Shamir et al. (2014) and Zhang and Xiao (2015) proposed truly communication-efficient distributed optimization algorithms which leveraged the local second-order information, resulting in milder dependence on the condition number, compared to the first-order approaches (Boyd et al.", "startOffset": 9, "endOffset": 745}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting.", "startOffset": 8, "endOffset": 443}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting. The situation in a high-dimensional setting is not so straightforward, due to the biased induced by the sparsity inducing penalty. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a highdimensional setting.", "startOffset": 8, "endOffset": 731}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting. The situation in a high-dimensional setting is not so straightforward, due to the biased induced by the sparsity inducing penalty. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a highdimensional setting. Averaging debiased high-dimensional estimators was subsequently used in Lee et al. (2015b) for distributed estimation, multi-task learning (Wang et al.", "startOffset": 8, "endOffset": 964}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting. The situation in a high-dimensional setting is not so straightforward, due to the biased induced by the sparsity inducing penalty. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a highdimensional setting. Averaging debiased high-dimensional estimators was subsequently used in Lee et al. (2015b) for distributed estimation, multi-task learning (Wang et al., 2015), and statistical inference (Battey et al., 2015). Concurrent work of (Jordan et al., 2016) (personal communication) also improved computational efficiency of Lee et al. (2015b) using ideas of Shamir et al.", "startOffset": 8, "endOffset": 1209}, {"referenceID": 0, "context": ", 2015; Arjevani and Shamir, 2015). However, it is not clear how to extend these approaches with non-smooth objectives, for example, the `1 regularized problems. Most of the above mentioned work is focused on estimators that are (asymptotically) linear. Averaging at the end reduces the variance of these estimators, resulting in an estimator that matches the performance of centralized procedure. With `2 regularization, Zhang et al. (2013c) studied the averaging the estimations with weaker regularization to avoid the large bias problem, under kernel ridge regression setting. The situation in a high-dimensional setting is not so straightforward, due to the biased induced by the sparsity inducing penalty. Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a highdimensional setting. Averaging debiased high-dimensional estimators was subsequently used in Lee et al. (2015b) for distributed estimation, multi-task learning (Wang et al., 2015), and statistical inference (Battey et al., 2015). Concurrent work of (Jordan et al., 2016) (personal communication) also improved computational efficiency of Lee et al. (2015b) using ideas of Shamir et al. (2014).", "startOffset": 8, "endOffset": 1245}, {"referenceID": 32, "context": "2) is inspired by the proposal in Shamir et al. (2014), where the authors studied distributed optimization for smooth and strongly convex empirical objectives.", "startOffset": 34, "endOffset": 55}, {"referenceID": 25, "context": "Our analysis also require the notion of restricted strong convexity (Negahban et al., 2012).", "startOffset": 68, "endOffset": 91}, {"referenceID": 25, "context": "The restricted strong convexity is an assumption used for showing consistent estimation in highdimensions (van de Geer and B\u00fchlmann, 2009; Negahban et al., 2012).", "startOffset": 106, "endOffset": 161}, {"referenceID": 25, "context": "3 following the proof strategy as in Negahban et al. (2012).", "startOffset": 37, "endOffset": 60}, {"referenceID": 36, "context": "For completeness, we first provide the definition of subgaussian norm (Vershynin, 2012).", "startOffset": 70, "endOffset": 87}, {"referenceID": 33, "context": "For the regression problem, the typical choice of loss function is the squared loss `pyji, x\u03b2,xjiyq 1 2pyji x\u03b2,xjiyq, and adding a `1 penalty to the empirical loss leads to the lasso estimator (Tibshirani, 1996)", "startOffset": 193, "endOffset": 211}, {"referenceID": 30, "context": "2) with high probability as long as n \u00c1 s log p (Rudelson and Zhou, 2013).", "startOffset": 48, "endOffset": 73}, {"referenceID": 37, "context": "The following `1 error bound is standard for lasso with random design, which was established, for example, in (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009) Lemma 4.", "startOffset": 110, "endOffset": 175}, {"referenceID": 24, "context": "The following `1 error bound is standard for lasso with random design, which was established, for example, in (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009) Lemma 4.", "startOffset": 110, "endOffset": 175}, {"referenceID": 3, "context": "The following `1 error bound is standard for lasso with random design, which was established, for example, in (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009) Lemma 4.", "startOffset": 110, "endOffset": 175}, {"referenceID": 37, "context": "We compare these bounds to the performance of local and centralized lasso (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009).", "startOffset": 74, "endOffset": 139}, {"referenceID": 24, "context": "We compare these bounds to the performance of local and centralized lasso (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009).", "startOffset": 74, "endOffset": 139}, {"referenceID": 3, "context": "We compare these bounds to the performance of local and centralized lasso (Wainwright, 2009; Meinshausen and Yu, 2009; Bickel et al., 2009).", "startOffset": 74, "endOffset": 139}, {"referenceID": 17, "context": "These bounds match the results in Lee et al. (2015b). Furthermore, when m \u00c0 n s2 log p , match the performance for centralized lasso.", "startOffset": 34, "endOffset": 53}, {"referenceID": 50, "context": "a `1 regularization, we obtain the `1 regularized logistic regression model (Zhu and Hastie, 2004; Wu et al., 2009):", "startOffset": 76, "endOffset": 115}, {"referenceID": 39, "context": "a `1 regularization, we obtain the `1 regularized logistic regression model (Zhu and Hastie, 2004; Wu et al., 2009):", "startOffset": 76, "endOffset": 115}, {"referenceID": 25, "context": "The logistic loss is 14 -smooth, let Ljp\u03b2q 1 n \u00b0 iPrns logp1 expp yjix\u03b2,xjiyqq, (Negahban et al., 2012) showed that if xji are drawn from mean zero distribution with sub-Gaussian tails, then L1p\u03b2q satisfies the restricted strong condition (3.", "startOffset": 80, "endOffset": 103}, {"referenceID": 25, "context": "The following `1 error bound states the estimation error for logistic regression with `1 regularization, which was established, for example, in (van de Geer, 2008; Negahban et al., 2012).", "startOffset": 144, "endOffset": 186}, {"referenceID": 21, "context": "1 High-dimensional Generalized Linear Models The results are readily extendable to other high-dimensional generalized linear models (McCullagh and Nelder, 1989; van de Geer, 2008), where the response variable yji P Y is drawn from the distribution Ppyji|xjiq9 exp yjixxji,\u03b2 y \u03a6pxxji,\u03b2 yq Ap\u03c3q , where \u03a6p q is a link function and Ap\u03c3q is a scale parameter.", "startOffset": 132, "endOffset": 179}, {"referenceID": 23, "context": "Widely studied graphical models are Gaussian graphical models (Meinshausen and B\u00fchlmann, 2006; Yuan and Lin, 2007) for continuous data and Ising graphical models (Ravikumar et al.", "startOffset": 62, "endOffset": 114}, {"referenceID": 41, "context": "Widely studied graphical models are Gaussian graphical models (Meinshausen and B\u00fchlmann, 2006; Yuan and Lin, 2007) for continuous data and Ising graphical models (Ravikumar et al.", "startOffset": 62, "endOffset": 114}, {"referenceID": 28, "context": "Widely studied graphical models are Gaussian graphical models (Meinshausen and B\u00fchlmann, 2006; Yuan and Lin, 2007) for continuous data and Ising graphical models (Ravikumar et al., 2010) for binary observations.", "startOffset": 162, "endOffset": 186}, {"referenceID": 23, "context": "As shown in (Meinshausen and B\u00fchlmann, 2006; Ravikumar et al., 2010), these model selection problems can be reduced to solving parallel `1 regularized linear regression and logistic regression problems, respectively.", "startOffset": 12, "endOffset": 68}, {"referenceID": 28, "context": "As shown in (Meinshausen and B\u00fchlmann, 2006; Ravikumar et al., 2010), these model selection problems can be reduced to solving parallel `1 regularized linear regression and logistic regression problems, respectively.", "startOffset": 12, "endOffset": 68}, {"referenceID": 17, "context": "\u2022 Avg-Debias: the method proposed in Lee et al. (2015b), with fine tuned regularization and hard thresholding parameters.", "startOffset": 37, "endOffset": 56}, {"referenceID": 38, "context": "Last but not least, it is interesting to explore the ideas presented to improve the computational cost of communicationefficient distributed multi-task learning with shared support (Wang et al., 2015).", "startOffset": 181, "endOffset": 200}, {"referenceID": 25, "context": "The proof uses ideas presented in (Negahban et al., 2012).", "startOffset": 34, "endOffset": 57}, {"referenceID": 36, "context": "10 in (Vershynin, 2012)) and an union bound over rps leads to the desired bound.", "startOffset": 6, "endOffset": 23}, {"referenceID": 36, "context": "10 in (Vershynin, 2012)) and an union bound over j P rms, i P rns and rps leads to the desired bound.", "startOffset": 6, "endOffset": 23}, {"referenceID": 12, "context": "then apply Azuma-Hoeffding inequality (Hoeffding, 1963) and an union bound over rps leads to the desired bound.", "startOffset": 38, "endOffset": 55}], "year": 2016, "abstractText": "We propose a novel, efficient approach for distributed sparse learning in high-dimensions, where observations are randomly partitioned across machines. Computationally, at each round our method only requires the master machine to solve a shifted `1 regularized M-estimation problem, and other workers to compute the gradient on local data. In respect of communication, the proposed approach provably matches the estimation error bound of centralized methods within constant rounds of communications (ignoring logarithmic factors). We conduct extensive experiments on both simulated and real world datasets, and demonstrate encouraging performances on high-dimensional regression and classification tasks.", "creator": "LaTeX with hyperref package"}}}