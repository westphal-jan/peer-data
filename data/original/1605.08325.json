{"id": "1605.08325", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Theano-MPI: a Theano-based Distributed Training Framework", "abstract": "We develop a scalable and extendable training framework that can utilize GPUs across nodes in a cluster and accelerate the training of deep learning models based on data parallelism. Both synchronous and asynchronous training are implemented in our framework, where parameter exchange among GPUs is based on CUDA-aware MPI. In this report, we analyze the convergence and capability of the framework to reduce training time when scaling the synchronous training of AlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel ways to reduce the communication overhead caused by exchanging parameters. Finally, we release the framework as open-source for further research on distributed deep learning", "histories": [["v1", "Thu, 26 May 2016 15:13:46 GMT  (127kb,D)", "http://arxiv.org/abs/1605.08325v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["he ma", "fei mao", "graham w taylor"], "accepted": false, "id": "1605.08325"}, "pdf": {"name": "1605.08325.pdf", "metadata": {"source": "CRF", "title": "Theano-MPI: a Theano-based Distributed Training Framework", "authors": ["He Ma", "Fei Mao", "Graham W. Taylor"], "emails": ["hma02@uoguelph.ca", "gwtaylor@uoguelph.ca", "feimao@sharcnet.ca"], "sections": [{"heading": "1 Introduction", "text": "With the constant improvement of hardware and discovery of new architectures, algorithms, and applications, deep learning is gaining popularity in both academia and industry. Object recognition [20], is now dominated by deep learning methods, which in many cases, rival human performance. Recent success in areas such as activity recognition from video [13] and statistical machine translation [14] is an example of deep learning\u2019s ascent both in performance and at scale.\nWith the new generations of GPU cards and increased device memory, researchers are able to design and train models with more than 140 million parameters (c.f. VGGNet [21]) and models that are as deep as 150 layers (c.f. ResNet [9]).\nThe emergence of larger datasets, e.g. ImageNet [20] and MS-COCO [18], challenges artificial intelligence research and leads us to design deeper and more expressive models so that the complexity of models is sufficient for the task.\nDespite of the increased computing power of GPUs, it usually takes weeks to train such large models to desired accuracy on a single GPU. This is due to the increased time associated with training deeper models and iterating over the examples in larger datasets. This is where distributed training of deep learning models becomes crucial, especially for activities such as model search which may involve training and evaluating models thousands of times.\n3 https://github.com/uoguelph-mlrg/Theano-MPI\nar X\niv :1\n60 5.\n08 32\n5v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 6\nA na\u0308\u0131ve approach to scaling up is running several copies of the same model in parallel on multiple computing resources (e.g. GPUs), each computing its share of the dataset and averaging their parameters at every iteration. This strategy is called data parallelism, and its efficient implementation is the focus of our work. More sophisticated forms of distributed training, including model parallelism are important but outside the current scope of our framework.\nTheano [23] is an open-source Python library for developing complex algorithms via mathematical expressions. It is often used for facilitating machine learning research. Its support for automatic symbolic differentiation and GPUaccelerated computing has made it popular within the deep learning community. Like other deep learning platforms, including Cafe [12], Torch [3], TensorFlow [1] and MXNet [2], Theano uses CUDA as one of its main backends for GPU accelerated computation. Since a single GPU is limited by its device memory and available threads when solving compute-intensive problems, very recently researchers have started to build multi-GPU support into the most popular frameworks. This includes the multi-GPU version of Caffe (FireCaffe [11]), Torch and Theano (Platoon).\nBecause the Theano environment usually compiles models for one GPU per process, we need to drive multiple GPUs using multiple processes. So finding a way to communicate between processes becomes a fundamental problem within a multi-GPU framework. There are several existing approaches of implementing inter-process communication besides manually programming on sockets, such as Signals, Message Queues, Message Passing, Pipes, Shared Memory, Memory Mapped Files, etc. However, among those approaches, Message Passing is most suitable for collective communication between multiple programs across a cluster because of its well-developed point-to-point and collective protocols. Message Passing Interface (MPI) is a language-independent communication protocol that can undertake the task of inter-process communication across machines. It is a standardized message-passing system designed for programming on large-scale parallel applications.\nParameter transfer is a basic operation in the distributed training of deep learning models. Therefore, the transfer speed between processes severely impacts the overall data throughput speedup4. Since the parameters to be transferred are computed on GPUs, a GPU-to-GPU transfer is required. Compared to the basic transfer() function in Theano, NVIDIA GPUDirect P2P technology makes this possible by transferring data between GPUs without passing through host memory. Specifically, it enables CUDA devices to perform direct read and write operations on other CUDA host and device memory. In the context of MPI, GPUDirect P2P technology allows a GPUArray memory buffer to be transferred in basic point-to-point and collective operations, making MPI \u201cCUDA-Aware\u201d.\nLeveraging CUDA-aware MPI, we have developed a scalable training framework that provides multi-node and multi-GPU support to Theano and efficient inter-GPU parameter transfer at the same time. To the best of our knowledge,\n4 We define data throughput speedup as the change in total time taken to process a certain amount of examples. It includes both training and communication time.\nthis is to-date the most convenient way to deploy Theano processes on a multinode multi-GPU cluster."}, {"heading": "2 Related Work", "text": "The idea of exploiting data parallelism in machine learning has been widely explored in recent years in both asynchronous and synchronous ways. To accelerate the training of a speech recognition model on distributed CPU cores, DownPour, an asynchronous parameter exchanging method [6], was proposed. It was the largest-scale method to-date for distributed training of neural networks. It was later found that controlling the maximum staleness of parameter updates received by the server leads to faster training convergence [10] on problems like topic modeling, matrix factorization and lasso regression compared to a purely asynchronous approach. For accelerating image classification on the CIFAR and ImageNet datasets, an elastic averaging strategy between asynchronous workers and the server was later proposed [25]. This algorithm allows more exploration of local optima than DownPour and alleviates the need for frequent communication between workers and the server.\nKrizhevsky proposed his trick on parallelizing the training of AlexNet [16] on multiple GPUs in a synchronous way [15]. This work showed that eight GPU workers training on the same batch size of 128 can give up to 6.25\u00d7 data throughput speedup and nearly the same convergence as trained on a single GPU when exploiting both model and data parallelism. Notably, the increase in effective batch size5 leads to very small changes in the final convergence of AlexNet when the learning rate is scaled properly. Following his work, a Theano-based twoGPU synchronous framework [7] for accelerating the training of AlexNet was proposed, where both weights and momentum are averaged between two GPUs after each iteration. The model converges to the same level as using a single GPU but in less time.\nThere has been more development on the acceleration of vision-based deep learning in recent years. NVIDIA developed a multi-GPU deep learning framework, DIGITS, which shows 3.5\u00d7 data throughput speedup when training AlexNet on 4 GPUs. Purine [17] pipelines the propagation of gradients between iterations and overlaps the communication of large weights in fully connected layers with the rest of back-propagation, giving near 12\u00d7 data throughput speedup when training GoogLeNet [22] on 12 GPUs. Similarly, MXNet [2] also shows a super-linear data throughput speedup on training GoogLeNet under a distributed training setting.\nThe Platoon project is a multi-GPU extension for Theano, created and maintained by the official Theano team. It currently supports only asynchronous data parallelism inside one compute node based on posix_ipc shared memory. In comparison, our framework, Theano-MPI, is designed to support GPUs that are distributed over multiple nodes in a cluster, providing convenient process\n5 effective batch size = batch size \u00d7 number of workers\nmanagement and faster inter-GPU memory exchanging based on CUDA-aware MPI."}, {"heading": "3 Implementation", "text": "Our goal is to make the field of distributed deep learning more accessible by developing a scalable training framework with two key components. First is Theano as a means of constructing an architecture and optimizing it by Stochastic Gradient Descent (SGD). Second is Massage Passing Interface (MPI) as an inter-process parameter exchanger. We also aim to explore various ways to reduce communication overhead in parallel SGD and expose some phenomena that affect convergence and speedup when training deep learning models in a distributed framework."}, {"heading": "3.1 The BSP Structure", "text": "Bulk Synchronous Parallel (BSP) [24] is an intuitive way to implement parallel computing. In the BSP paradigm, workers proceed with training in a synchronous way. Figure 1a shows a 4 GPU example of the proposed BSP structure where the same model is built and run within four processes, P0, P1, P2, P3. Each process uses one CPU and one GPU. After the model\u2019s training graph is compiled on the GPU, those parameters in the graph become arrays in GPU memory whose values can be retrieved from device to host and set from host to device. When training starts, the training dataset is split into four parts. In every iteration, each worker process takes a mini-batch of examples from its share and performs SGD on it. After that, all workers are synchronized and model parameters are exchanged between worker processes in a collective way."}, {"heading": "3.2 CUDA-aware Parameter Exchanging", "text": "Synchronous parameter exchange is an array reduction problem which consists of both data transfer and calculation. The GPUDirect P2P technology allows exchanging parameters between GPUs without passing through host memory, making MPI functions \u201cCUDA-aware\u201d. Based on this, we explored various strategies trying to minimize the data transfer and calculation time, and make more efficient use of QPI, PCIe and network card bandwidth during data transfer. The basic strategy is to use the MPI Allreduce() function. However, the CUDAaware version of it in OpenMPI 1.8.7 does not give much improvement since any collective MPI function with arithmetic operations still needs to copy data to host memory. Functions like Alltoall() and Allgather() do not involve any arithmetic and therefore the CUDA-aware version of them (Fig. 1b) can avoid passing through host memory unless data transfer crossing the QPI bus is needed. We therefore implemented a CUDA-aware Alltoall-sum-Allgather strategy which separates the data transfer and computation. An example of this strategy is demonstrated in Fig. 2. Here, the summation kernels required for parameter exchange are executed in parallel on GPUs. Our test shows the GPU summation kernel takes only 1.6% of the total communication time.\nUsing low precision data types for weights or activations (or both) in the forward pass during training of deep neural networks has received much recent interest [4,5]. It was shown that training Maxout [8] networks at 10 bits fixed point precision can still yield near state-of-art test accuracy [4]. In light of this, we also implemented the transfer of parameters at half-precision while summing them at full precision, in order to further reduce communication overhead.\nFigure 3 shows the improvement of the combination of strategies over MPI Allreduce. The \u201cASA\u201d strategy shows three times faster communication relative to MPI Allreduce and the half precision version of it gives nearly 6 times faster\nperformance. Those results are obtained on distributed GPUs on 8 nodes in a cluster. Each node hosts one GPU.\nDue to the limitation imposed by the Global Interpreter Lock (GIL) in Python, overlapping the communication with the gradient calculation as in [17] has not yet been implemented in our framework. We expect this, if implemented, would substantially reduce the communication cost of exchanging large matrices in fully-connected layers."}, {"heading": "3.3 Parallel Loading", "text": "For large-scale visual recognition applications such as ImageNet LSVRC, the data required for training is on the order of hundreds of Gigabytes. Therefore, it is difficult to load all image data completely into memory after training starts. Instead, images are stored as batch files on local or remote disks and loaded one file at a time by each process. Loading image batches x from disk can be time consuming6. It is affected by various factors, including file size, file format, disk I/O capability and network bandwidth if reading from remote disks. If in every iteration, the training process should wait for data loading to be ready in order to proceed, one can imagine the time cost by loading data will be critical to the total performance. One way to circumvent this, given the independence of loading and training, is to load those files in parallel with the forward and backward propagations on the last loaded batch. However, this assumes loading one batch of images takes shorter than one iteration of training the model. This auxiliary loading process should follow procedures in Alg. 1 to collaborate efficiently with its corresponding training process:\n6 Loading labels y, on the other hand, is much faster, therefore labels can be loaded completely into memory.\nAlgorithm 1 The parallel loading process\nRequire: Host memory allocated for loading image batch hostdatax. GPU memory allocated for preprocessed image batch gpudatax GPU memory allocated for the actual model graph input inputx, mode=None, recv=None, filename=None. Mean image image mean Ensure: 1: while True do 2: Receive the mode (train, validate or stop) from training process 3: if recv=\u201cstop\u201d then 4: break 5: else 6: mode\u2190 recv 7: Receive the first filename to be loaded from training process filename\u2190 recv 8: while True do 9: Load file \u201c filename\u201d from disk into host memory hostdatax. 10: hostdatax = hostdatax \u2212 image mean 11: Crop and mirror hostdatax according to mode. 12: Transfer hostdatax from host to GPU device memory gpudatax. 13: Wait for training on the last inputx to finish by receiving the next filename\nto be loaded. 14: if recv in [\u201cstop\u201d, \u201ctrain\u201d, \u201cval\u201d] then 15: break 16: else 17: filename\u2190 recv 18: Transfer gpudatax to inputx. 19: Synchronize GPU context. 20: Notify training process to precede with the newly loaded inputx\nDifferent from the multiprocessing and Queue messaging method in [7], we used the MPI Spawn function to start a child process from each training process and used the resulting MPI intra-communicator to pass messages between the training process and its child process. As shown in Algorithm 1, the parallel loading process can read image files, subtract the mean image, crop sub-images and finally load preprocessed data onto GPUs. By doing this, we are able to overlap the most compute-intensive part (Step 10 to 13 in Algorithm 1) with forward and backward graph propagation in the training process."}, {"heading": "4 Benchmarking", "text": "Exchanging parameters is a necessary aspect of parallel SGD, however, it can be achieved in a variety of different ways. Parameters updated during SGD include weights (and biases), momentum (if using momentum SGD) and raw gradients. Averaging weights after gradient descent (AWAGD) [15,7] is a straightforward parallel SGD scheme. We have proved [19] that training a perceptron using this\nscheme on multiple GPUs can either be equivalent to or a close approximate of sequential SGD training on a single GPU depending on whether or not effective batch size is kept constant. In this scheme, the learning rate is scaled with the number of GPUs used [15], namely k. It can also be shown that this scheme is equivalent to summing up the parameter updates from all GPUs before performing gradient descent (SUBGD), which does not require scaling up the learning rate. However, our experiments show that tuning the learning rate is still dependent on k to ensure initial convergence of the model. Table 1 lists the learning rates we used and the convergence we achieved in training AlexNet7 and GoogLeNet8 at different scales (number of workers).\nRecent work has applied low precision to weights and activations during training [4]. In the extreme, binary data types have been considered [5]. This enables efficient operation of the low-precision network both at deployment (test time) and during the forward propagation stage during training. However, gradients used for parameter updates must still be stored at high-precision or learning will fail. Related to this, we see a drop in accuracy due to reduced-precision parameter exchange. The validation top-5 error of the \u201c8GPU-32b\u201d AlexNet in Table 1 increased from 19.9% to 20.3% and that of the \u201c8GPU\u201d GoogLeNet increased from 10.65% to 11.7%. Parameter exchange is an important part of the update stage in a distributed training framework. Therefore, there is a necessary accuracy-speed tradeoff to consider when adopting a low-precision strategy.\n7 Top-5 error at epoch 62. The implementation is based on theano alexnet from uoguelph-mlrg. https://github.com/uoguelph-mlrg/theano_alexnet. 8 Top-5 error at epoch 70. BVLC GoogLeNet implementation in Caffe is referenced in building the model. https://github.com/BVLC/caffe/tree/master/models/bvlc_ googlenet. * At the time of submission, the GoogLeNet 1GPU test is still ongoing. The top-5 error is taken from [22].\nFigures 4 and 5 show the convergence of two models trained with SUBGD and the Alltoall-sum-Allgather strategy, in which AlexNet is trained on 1, 2, 4 and 8 GPUs with momentum SGD and 128 batch size on each GPU9. Similarly, GoogLeNet is trained on 2, 4 and 8 GPUs with a batch size of 32. We see that as more workers are used, the effective batch size becomes too large and the approximation from parallel SGD to sequential SGD becomes worse. As shown in Table 1, one way to preserve convergence at such a large-scale is to reduce the batch size on each worker so that the effective batch size stays small. This gives the model more potential to explore further at low learning rates, though the accuracy improvement at the beginning is slow. However, using smaller batch sizes means more frequent parameter exchanges between workers, which demands attention toward further reducing the communication overhead.\nThe speedup of training AlexNet and GoogLeNet are evaluated on 8 distributed GPU nodes (1 GPU per node). To show the performance of accelerating\n9 Tested on the ILSVRC14 dataset [20].\nlarger models, we build VGGNet and test its scaling performance on 8 GPUs in a single node with shared memory. This setup meets the memory requirements of VGGNet. Table 2 gives an overview of the structural difference between those three models. Table 3 reports the training and communication time taken to process 5,120 images across different models. We see that these three models scale differently in the framework due to differences in the complexity of their operations as well as the number of free parameters. CUDA-aware parameter exchanging helps boost the speedup of the framework, especially when the number of parameters is relatively large.\nObserving our GoogLeNet13 benchmark result in Fig. 5, we would expect that the framework provides a convergence speedup close to the throughput speedup reported in Table 3, if the convergence of parallel SGD closely ap-\n10 In terms of the amount of parameter-containing layers. 11 In terms of the amount of float32 parameters. 12 This includes the parameters of the two auxiliary classifiers. 13 The learning rate tuning policy of GoogLeNet used was:\n\u03b7 = \u03b70(1\u2212 epoch\u00d7 number of minibatches\nmax iterations )0.5.\nThe learning rate tuning policy of AlexNet used was: scaling down by a factor of 10 every 20 epochs.\nproximates that of sequential SGD. However, it is difficult to give the exact convergence speedup provided by the framework, since different settings of the hyper-parameters (learning rate tunning policy, weight decay, batch size, cropping randomness) leads to a different convergence path and complicates comparison.\nBesides the synchronous framework, we also explored reducing the communication overhead in the asynchronous setting. Referencing the implementation of EASGD in Platoon, a Theano-based multi-GPU framework that exploits data parallelism, we re-implemented the framework based on the CUDA-aware MPI SendRecv() function without the Round-Robin scheme [25]. Our test shows, when training AlexNet on 8 GPUs, the asynchronous communication overhead in our framework is 42% lower than that in Platoon when worker processes communicate with the server in the most frequent way (\u03c4 = 1). We also performed a grid search on the hyper-parameters \u03b1 and \u03c4 to achieve better convergence when training AlexNet on eight distributed GPUs, each processing a batch size of 128. The best top-5 error we achieved from this framework was 21.12% at a global epoch of 49 when the moving rate was \u03b1 = 0.5 and averaging period was \u03c4 = 1 with a data throughput speedup of 6.7\u00d7."}, {"heading": "5 Hardware and Software Environment", "text": "The software was developed and tested on a PI-contributed SHARCNET cluster, named copper. As shown in Fig. 6, each node in the cluster is a dual socket system with two NVIDIA Tesla K80 GPUs on each socket. The whole cluster is interconnected with Mellonox Infiniband FDR. We also tested on another cluster, mosaic, which features distributed GPUs across nodes connected by Infiniband QDR. Each node has one NVIDIA K20m GPU.\nFor high-level access to MPI functionality, we use its Python binding mpi4py, compiled against OpenMPI 1.8.7. All models mentioned in this report are constructed in Theano 0.8 and their implementation is available in our Github\nproject. Convolution and pooling operations in the computational graph depend on CUDA 7.0 and the cuDNN v4 library. We also support cudaconvnet as an alternative backend."}, {"heading": "6 Discussion", "text": "We have attempted to scale up the training of deep learning models in an accessible way by developing a scalable training framework built around Theano. Key technical features of our framework are more efficient interprocess communication strategies and parallel data loading techniques. Factors affecting the speedup of the framework can be associated with the model to be trained (i.e. architectural), the training data loading strategy, synchronization in the computational graph, implementation of GPU kernels, system memory and network bandwidth.\nImportantly, we try not to compromise the convergence of models trained under our framework since measured speedup is based on the time taken to reach a certain error rate. However, the convergence achieved by a parallel framework also depends on the tuning of that framework\u2019s hyper-parameters. The convergence results in Table 1 can therefore be improved if better hyper-parameters are found. Factors affecting model convergence include the number of worker processes, effective batch size and corresponding learning rate, parameter averaging frequency \u03c414, moving rate \u03b1 in EASGD and the initialization of model parameters.\nThe main contributions of our work include: providing multi-node and improved multi-GPU support to the Theano library based on MPI, eliminating substantial communication overhead, exposing convergence and speedup phenomena in parallel SGD, and an implementation of a more efficient parallel loading method.\nOur effort towards eliminating the communication overhead involves several aspects: leveraging CUDA-aware MPI for direct data transfer, separating data transfer and summation for more efficient summation on GPUs, and exploring half precision data transfer for faster communication. Our benchmarking results show that our effort on eliminating communication overhead works well on both the 1-GPU-per-node cluster, mosaic, and the 8-GPU-per-node cluster, copper.\nNote that the multi-node testing results in this report are obtained without GPUDirect RDMA support due to a limitation in the cluster configuration. Also, the QPI bus topology of a copper node limits the usage of GPUDirect P2P technology. This is because the GPUDirect P2P requires all GPUs to be under the same PCIe switch. If a path traversing the QPI is needed, the data transfer would go through CPU RAM first. As a result, further improvement of communication performance based on the current hardware setting would involve consideration of overlapping data transfer with the summation kernel, overlapping parameter exchange with gradient calculation, and designing better\n14 In BSP, we use \u03c4 = 1 since larger \u03c4 tends to affect convergence in the same way as increasing batch size.\ninter-node and intra-node strategies that could balance the bandwidth usage among QPI, PCIe and Infiniband networking."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z Chen"], "venue": "ArXiv eprints", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems", "author": ["T. Chen", "M. Li", "Y. Li", "M. Lin", "N Wang"], "venue": "CoRR abs/1512.01274", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Low precision arithmetic for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "J. David"], "venue": "CoRR abs/1412.7024", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M Devin"], "venue": "Advances in Neural Information Processing Systems 25, pp. 1232\u20131240", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano-based large-scale visual recognition with multiple gpus", "author": ["W. Ding", "R. Wang", "F. Mao", "G.W. Taylor"], "venue": "CoRR abs/1412.2302", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout Networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ArXiv e-prints", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR abs/1512.03385", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "More effective distributed ml via a stale synchronous parallel parameter server", "author": ["Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J Kim"], "venue": "Advances in Neural Information Processing Systems 26, pp. 1223\u20131231. Curran Associates, Inc.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Firecaffe: nearlinear acceleration of deep neural network training on compute clusters", "author": ["F.N. Iandola", "K. Ashraf", "M.W. Moskewicz", "K. Keutzer"], "venue": "CoRR abs/1511.00175", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J Long"], "venue": "CoRR abs/1408.5093", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R Sukthankar"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards effective use of training data in statistical machine translation", "author": ["P. Koehn", "B. Haddow"], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation. pp. 317\u2013321. WMT \u201912, Association for Computational Linguistics, Stroudsburg, PA, USA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["A. Krizhevsky"], "venue": "CoRR abs/1404.5997", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pp. 1097\u20131105. Curran Associates, Inc.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Purine: A bi-graph based deep learning framework", "author": ["M. Lin", "S. Li", "X. Luo", "S. Yan"], "venue": "CoRR abs/1412.6249", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Computer Vision \u2013 ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P Perona"], "venue": "Springer International Publishing, Cham", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Developing a Scalable Deep Learning Framework Based on MPI", "author": ["H. Ma"], "venue": "Master\u2019s thesis, University of Guelph, Guelph, ON, CA", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S Satheesh"], "venue": "International Journal of Computer Vision 115(3), 211\u2013252", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR abs/1409.1556", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "Reed", "S.E"], "venue": "CoRR abs/1409.4842", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints abs/1605.02688", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "A Bridging Model for parallel computation", "author": ["L.G. Valiant"], "venue": "Communications of the ACM 33(8), 103", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Deep learning with elastic averaging sgd", "author": ["S. Zhang", "A.E. Choromanska", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems 28, pp. 685\u2013693. Curran Associates, Inc.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Object recognition [20], is now dominated by deep learning methods, which in many cases, rival human performance.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "Recent success in areas such as activity recognition from video [13] and statistical machine translation [14] is an example of deep learning\u2019s ascent both in performance and at scale.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Recent success in areas such as activity recognition from video [13] and statistical machine translation [14] is an example of deep learning\u2019s ascent both in performance and at scale.", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "VGGNet [21]) and models that are as deep as 150 layers (c.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "ResNet [9]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 19, "context": "ImageNet [20] and MS-COCO [18], challenges artificial intelligence research and leads us to design deeper and more expressive models so that the complexity of models is sufficient for the task.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "ImageNet [20] and MS-COCO [18], challenges artificial intelligence research and leads us to design deeper and more expressive models so that the complexity of models is sufficient for the task.", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "Theano [23] is an open-source Python library for developing complex algorithms via mathematical expressions.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "Like other deep learning platforms, including Cafe [12], Torch [3], TensorFlow [1] and MXNet [2], Theano uses CUDA as one of its main backends for GPU accelerated computation.", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "Like other deep learning platforms, including Cafe [12], Torch [3], TensorFlow [1] and MXNet [2], Theano uses CUDA as one of its main backends for GPU accelerated computation.", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "Like other deep learning platforms, including Cafe [12], Torch [3], TensorFlow [1] and MXNet [2], Theano uses CUDA as one of its main backends for GPU accelerated computation.", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "Like other deep learning platforms, including Cafe [12], Torch [3], TensorFlow [1] and MXNet [2], Theano uses CUDA as one of its main backends for GPU accelerated computation.", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "This includes the multi-GPU version of Caffe (FireCaffe [11]), Torch and Theano (Platoon).", "startOffset": 56, "endOffset": 60}, {"referenceID": 5, "context": "To accelerate the training of a speech recognition model on distributed CPU cores, DownPour, an asynchronous parameter exchanging method [6], was proposed.", "startOffset": 137, "endOffset": 140}, {"referenceID": 9, "context": "It was later found that controlling the maximum staleness of parameter updates received by the server leads to faster training convergence [10] on problems like topic modeling, matrix factorization and lasso regression compared to a purely asynchronous approach.", "startOffset": 139, "endOffset": 143}, {"referenceID": 24, "context": "For accelerating image classification on the CIFAR and ImageNet datasets, an elastic averaging strategy between asynchronous workers and the server was later proposed [25].", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "Krizhevsky proposed his trick on parallelizing the training of AlexNet [16] on multiple GPUs in a synchronous way [15].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "Krizhevsky proposed his trick on parallelizing the training of AlexNet [16] on multiple GPUs in a synchronous way [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 6, "context": "Following his work, a Theano-based twoGPU synchronous framework [7] for accelerating the training of AlexNet was proposed, where both weights and momentum are averaged between two GPUs after each iteration.", "startOffset": 64, "endOffset": 67}, {"referenceID": 16, "context": "Purine [17] pipelines the propagation of gradients between iterations and overlaps the communication of large weights in fully connected layers with the rest of back-propagation, giving near 12\u00d7 data throughput speedup when training GoogLeNet [22] on 12 GPUs.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "Purine [17] pipelines the propagation of gradients between iterations and overlaps the communication of large weights in fully connected layers with the rest of back-propagation, giving near 12\u00d7 data throughput speedup when training GoogLeNet [22] on 12 GPUs.", "startOffset": 243, "endOffset": 247}, {"referenceID": 1, "context": "Similarly, MXNet [2] also shows a super-linear data throughput speedup on training GoogLeNet under a distributed training setting.", "startOffset": 17, "endOffset": 20}, {"referenceID": 23, "context": "Bulk Synchronous Parallel (BSP) [24] is an intuitive way to implement parallel computing.", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "Using low precision data types for weights or activations (or both) in the forward pass during training of deep neural networks has received much recent interest [4,5].", "startOffset": 162, "endOffset": 167}, {"referenceID": 4, "context": "Using low precision data types for weights or activations (or both) in the forward pass during training of deep neural networks has received much recent interest [4,5].", "startOffset": 162, "endOffset": 167}, {"referenceID": 7, "context": "It was shown that training Maxout [8] networks at 10 bits fixed point precision can still yield near state-of-art test accuracy [4].", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "It was shown that training Maxout [8] networks at 10 bits fixed point precision can still yield near state-of-art test accuracy [4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 16, "context": "Due to the limitation imposed by the Global Interpreter Lock (GIL) in Python, overlapping the communication with the gradient calculation as in [17] has not yet been implemented in our framework.", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "Different from the multiprocessing and Queue messaging method in [7], we used the MPI Spawn function to start a child process from each training process and used the resulting MPI intra-communicator to pass messages between the training process and its child process.", "startOffset": 65, "endOffset": 68}, {"referenceID": 14, "context": "Averaging weights after gradient descent (AWAGD) [15,7] is a straightforward parallel SGD scheme.", "startOffset": 49, "endOffset": 55}, {"referenceID": 6, "context": "Averaging weights after gradient descent (AWAGD) [15,7] is a straightforward parallel SGD scheme.", "startOffset": 49, "endOffset": 55}, {"referenceID": 18, "context": "We have proved [19] that training a perceptron using this", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "In this scheme, the learning rate is scaled with the number of GPUs used [15], namely k.", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "Recent work has applied low precision to weights and activations during training [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "In the extreme, binary data types have been considered [5].", "startOffset": 55, "endOffset": 58}, {"referenceID": 21, "context": "The top-5 error is taken from [22].", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "9 Tested on the ILSVRC14 dataset [20].", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "Referencing the implementation of EASGD in Platoon, a Theano-based multi-GPU framework that exploits data parallelism, we re-implemented the framework based on the CUDA-aware MPI SendRecv() function without the Round-Robin scheme [25].", "startOffset": 230, "endOffset": 234}], "year": 2016, "abstractText": "We develop a scalable and extendable training framework that can utilize GPUs across nodes in a cluster and accelerate the training of deep learning models based on data parallelism. Both synchronous and asynchronous training are implemented in our framework, where parameter exchange among GPUs is based on CUDA-aware MPI. In this report, we analyze the convergence and capability of the framework to reduce training time when scaling the synchronous training of AlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel ways to reduce the communication overhead caused by exchanging parameters. Finally, we release the framework as open-source for further research on distributed deep learning.", "creator": "LaTeX with hyperref package"}}}