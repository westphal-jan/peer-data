{"id": "1605.08481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "Open Problem: Best Arm Identification: Almost Instance-Wise Optimality and the Gap Entropy Conjecture", "abstract": "The best arm identification problem (BEST-1-ARM) is the most basic pure exploration problem in stochastic multi-armed bandits. The problem has a long history and attracted significant attention for the last decade. However, we do not yet have a complete understanding of the optimal sample complexity of the problem: The state-of-the-art algorithms achieve a sample complexity of $O(\\sum_{i=2}^{n} \\Delta_{i}^{-2}(\\ln\\delta^{-1} + \\ln\\ln\\Delta_i^{-1}))$ ($\\Delta_{i}$ is the difference between the largest mean and the $i^{th}$ mean), while the best known lower bound is $\\Omega(\\sum_{i=2}^{n} \\Delta_{i}^{-2}\\ln\\delta^{-1})$ for general instances and $\\Omega(\\Delta^{-2} \\ln\\ln \\Delta^{-1})$ for the two-arm instances. We propose to study the instance-wise optimality for the BEST-1-ARM problem. Previous work has proved that it is impossible to have an instance optimal algorithm for the 2-arm problem. However, we conjecture that modulo the additive term $\\Omega(\\Delta_2^{-2} \\ln\\ln \\Delta_2^{-1})$ (which is an upper bound and worst case lower bound for the 2-arm problem), there is an instance optimal algorithm for BEST-1-ARM. Moreover, we introduce a new quantity, called the gap entropy for a best-arm problem instance, and conjecture that it is the instance-wise lower bound. Hence, resolving this conjecture would provide a final answer to the old and basic problem.", "histories": [["v1", "Fri, 27 May 2016 00:23:39 GMT  (8kb)", "http://arxiv.org/abs/1605.08481v1", "To appear in COLT 2016 Open Problems"]], "COMMENTS": "To appear in COLT 2016 Open Problems", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lijie chen", "jian li"], "accepted": false, "id": "1605.08481"}, "pdf": {"name": "1605.08481.pdf", "metadata": {"source": "CRF", "title": "Open Problem: Best Arm Identification: Almost Instance-Wise Optimality and the Gap Entropy Conjecture", "authors": ["Lijie Chen", "Jian Li"], "emails": ["CHENLJ13@MAILS.TSINGHUA.EDU.CN", "LIJIAN83@MAIL.TSINGHUA.EDU.CN"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n08 48\n1v 1\n[ cs\n.L G\n] 2\n7 M\nay 2\n\u2211\nn i=2 \u2206\u22122 i (ln \u03b4\u22121 + ln ln\u2206\u22121 i )) (\u2206i is the difference between the largest mean and the\nith mean), while the best known lower bound is \u2126( \u2211n i=2 \u2206\u22122 i ln \u03b4\u22121) for general instances and \u2126(\u2206\u22122 ln ln\u2206\u22121) for the two-arm instances. We propose to study the instance-wise optimality for the BEST-1-ARM problem. Previous work has proved that it is impossible to have an instance optimal algorithm for the 2-arm problem. However, we conjecture that modulo the additive term \u2126(\u2206\u22122\n2 ln ln\u2206\u22121 2 ) (which is an upper bound and worst case lower bound for the 2-arm problem), there is an instance optimal algorithm for BEST-1-ARM. Moreover, we introduce a new quantity, called the gap entropy for a best-arm problem instance, and conjecture that it is the instance-wise lower bound. Hence, resolving this conjecture would provide a final answer to the old and basic problem."}, {"heading": "1. Introduction", "text": "In the BEST-1-ARM problem, we are given n stochastic arms A1, . . . , An. The ith arm Ai has a reward distribution Di with an unknown mean \u00b5i \u2208 [0, 1]. We assume that all reward distributions are Gaussian distributions with variance 1. Upon each play of Ai, we can get a reward value sampled i.i.d. from Di. Our goal is to identify the arm with largest mean using as few samples as possible. We assume here that the largest mean is strictly larger than the second largest (i.e., \u00b5[1] > \u00b5[2]) to ensure the uniqueness of the solution, where \u00b5[i] denotes the i\nth largest mean. The problem is also called the pure exploration problem in the stochastic multi-armed bandit literature.\nWe say an algorithm A is \u03b4-correct for BEST-1-ARM, if it outputs the correct answer on any instance with probability at 1\u2212 \u03b4, and we use TA(I) to denote the expected number of total samples taken by algorithm A on instance I . We also define the gap of ith arm, \u2206[i] := \u00b5[1] \u2212 \u00b5[i]."}, {"heading": "2. Background", "text": "During the last decade, the BEST-1-ARM problem and its optimal sample complexity have attracted significant attention. We only mention a small subset that are most relevant to us. The current best lower bound is due to Mannor and Tsitsiklis (2004), who showed that for any \u03b4-correct algorithm for BEST-1-ARM, it requires \u2126 (\n\u2211n i=2 \u2206 \u22122 [i] ln \u03b4\n\u22121 )\n(referred to as the MT lower bound from now\n\u2217 Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University, Beijing, China. Research supported in part by the National Basic Research Program of China grants 2015CB358700, 2011CBA00300, 2011CBA00301, and the National NSFC grants 61033001, 61361136003.\nc\u00a9 2016 L. Chen & J. Li.\non) samples in expectation for any instance. We note that the MT lower bound is an instance-wise lower bound, i.e., any BEST-1-ARM instance requires the stated number of samples. On the other hand, the current published best known upper bound is O (\n\u2211n i=2\u2206 \u22122 [i]\n(\nln ln\u2206\u22121[i] + ln \u03b4 \u22121\n))\n, due\nto Karnin et al. (2013). Jamieson et al. (2014) obtained a UCB-type algorithm (called lil\u2019UCB), which achieves the same sample complexity. We refer the above bound as the KKS-JMNS bound. Back in 1964, Farrell (1964) provided an \u2126(\u2206\u22122 ln ln\u2206\u221212 ) lower bound for the two-arm cases (which matches the KKS-JMNS bound for two arms).\nVery recently, in an unpublished manuscript (Chen and Li (2015)), the authors obtained improved lower and upper bounds for BEST-1-ARM. The work lead the authors to make an intriguing conjecture which we detail in the next section. We will also state the improved bounds and their connection to the conjecture in more details."}, {"heading": "3. Open Problem: Almost Instance Optimality and the Gap Entropy Conjecture", "text": "We propose to study BEST-1-ARM from the perspective of instance optimality, the ultimate notion of optimality (see e.g., Fagin et al. (2003); Afshani et al. (2009)).\nFor the 2-arm cases, the KKS-JMNS bound O(\u2206\u22122 ln ln\u2206\u221212 ) is an upper bound for every instance, and the Farrell lower bound \u2126(\u2206\u22122 ln ln\u2206\u221212 ) is a lower bound for the worst case instances. As we observed in (Chen and Li (2015)), it is impossible to obtain an instance optimal algorithm even for the 2-arm cases. While the observation has ruled out any hope of an instance optimal algorithm for BEST-1-ARM, however, as we will see, it is still possible to obtain very satisfiable answer in terms of instance optimality.\nNow, we formally define what is an instance-wise lower bound. Clearly, two arm instances differ only by a permutation of arms should be considered as the same instance. Inspired by Afshani et al. (2009), we give the following natural definition.\nDefinition 3.1 (Order-Oblivious Instance-wise Lower Bound) Given a BEST-1-ARM instance I and a confidence level \u03b4, we define\nL(I, \u03b4) := inf A:A is \u03b4-correct for BEST-1-ARM\n1\nn! \u00b7 \u2211 \u03c0\u2208Sym(n) TA(\u03c0 \u25e6 I),\nwhere the summation is over all n! permutations of {1, . . . , n}.\nThe MT lower bound immediately implies that L(I, \u03b4) = \u2126( \u2211n i=2 \u2206 \u22122 [i] ln \u03b4 \u22121). We conjecture that the two-arm instance is the only obstruction toward an instance-wise optimal algorithm. More precisely, we have the following conjecture.\nConjecture 3.2 There is an algorithm for BEST-1-ARM with sample complexity O(L(I, \u03b4) + \u2206\u221222 ln ln\u2206 \u22121 2 ),\nfor any instance I and \u03b4 < 0.1. And we say such an algorithm is almost instance-wise optimal for BEST-1-ARM.\nIn the light of the discussion for the 2-arm cases, there must be a gap between the sample complexity of a \u03b4-correct algorithm and L(I, \u03b4), and Conjecture 3.2 states that the gap can be as small as an additive factor \u2206\u221222 ln ln\u2206 \u22121 2 , which is all we need to find out the best arm from the top-2 arms, and is an inevitable gap even for the 2-arm instances. Moreover, we provide an explicit formula for L(I, \u03b4). Interestingly, the formula involves an entropy term (similar entropy terms also appear in Afshani et al. (2009) for completely different problems). We define the entropy term first.\nDefinition 3.3 Given a BEST-1-ARM instance I , let\nGk = {i \u2208 [2, n] | 2 \u2212k \u2264 \u2206[u] < 2 \u2212k+1}, Hk = \u2211 i\u2208Gk \u2206\u22122[i] , and pk = Hk/ \u2211 j Hj.\nWe can view {pk} as a discrete probability distribution. We define the following quantity as the gap entropy for the instance I\nEnt(I) = \u2211\nGk 6=\u2205 pk log p\n\u22121 k . 1\nRemark 3.4 We choose to partition the arms based on the powers of 2. There is nothing special about 2 and replacing it by any other constant only changes Ent(I) by a constant factor.\nThen we formally state our conjecture.\nConjecture 3.5 For any BEST-1-ARM instance I and \u03b4 < 0.1, we have\nL(I, \u03b4) = \u0398 ( \u2211n\ni=2 \u2206\u22122 [i] \u00b7 ( ln \u03b4\u22121 + Ent(I) )\n)\n.\nIn the next section, we will try to motivate the term Ent(I) and explain the reasons that lead us to make the above conjecture."}, {"heading": "4. Motivation and Current Progress", "text": "In our recent work (Chen and Li (2015)), we provide an algorithm with the following sample complexity:\nO (\n\u2206\u22122[2] ln ln\u2206 \u22121 [2] +\n\u2211n\ni=2 \u2206\u22122[i] ln \u03b4\n\u22121 + \u2211n\ni=2 \u2206\u22122[i] ln lnmin(n,\u2206 \u22121 [i] )\n)\n. (1)\nFurthermore, the algorithm achieves a sample complexity of\nO (\n\u2206\u22122[2] ln ln\u2206 \u22121 [2] +\n\u2211n\ni=2 \u2206\u22122[i] ln \u03b4\n\u22121 )\n, (2)\nfor clustered instances (We say an instance is clustered if the number of nonempty Gks is bounded by a constant).\nOur new upper bounds (1) and (2) match our conjectured gap entropy lower bound in two extreme cases. On one extreme, the maximum value Ent(I) can get is O(ln lnn). This can be achieved by instances in which there are log n nonempty groups Gi and they have almost the same weight Hi. Hence, (1) is optimal for such instances. On the other extreme where there is only a constant number of nonempty groups (i.e., the instance is clustered), Ent(I) = O(1), and our algorithm can achieve almost instance optimality (without relying on the Conjecture 3.5, due to the MT lower bound) in this case.\nBesides the fact that our algorithm can achieve optimal results for both extreme cases, we have more reasons to believe why Ent(I) should enter the picture.\nUpper Bounds: First, we motivate the gap entropy Ent from the algorithmic side. Consider an elimination-based algorithm (such as Karnin et al. (2013) or our algorithm). We must ensure that the best arm is not eliminated in any round. Recall that in the rth round, we want to eliminate arms with gap \u2206r = \u0398(2\u2212r), which is done by obtaining an approximation of the best arm, then take O(\u2206\u22122r ln \u03b4 \u22121 r ) samples from each arm and eliminate the arms with smaller empirical means. Roughly speaking, we\n1. Note that it is exactly the Shannon entropy for the distribution defined by {pk}.\nneed to assign the failure probability \u03b4r carefully to each round (by union bound, we need \u2211\nr \u03b4r \u2264 \u03b4). The algorithm in Karnin et al. (2013) used \u03b4r = O(\u03b4 \u00b7 r\u22122), and we used a better way to assign \u03b4r. Indeed, if one can assign \u03b4r\u2019s optimally (i.e., minimize \u2211 r Hr ln \u03b4 \u22121 r subject to \u2211\nr \u03b4r \u2264 \u03b4), one could achieve the entropy bound \u2211\nr Hr \u00b7 (ln \u03b4 \u22121 + Ent(I)) (by letting \u03b4r = \u03b4Hr/\n\u2211\niHi). Of course, this does not lead to an algorithm directly, as we do not know His in advance.\nUsing our techniques, we can estimate the values Hr\u2019s when we enter the rth elimination stage. The only obstacle for implementing the above idea of assigning \u03b4r\u2019s optimally is that we do not know \u2211\nr Hr initially. We believe the difficulty can be overcome by additional new algorithmic ideas.\nLower Bounds: In Chen and Li (2015), we prove the following lower bound, improving the MT lower bound.\nTheorem 4.1 (Theorem 1.6 in Chen and Li (2015)) There exist constants c, c1 > 0 and N \u2208 N such that, for any \u03b4 < 0.005 and any \u03b4-correct algorithm A, and any n \u2265 N , there exists an n arms instance I such that TA[I] \u2265 c \u00b7 \u2211n i=2 \u2206 \u22121 [i] ln lnn. Furthermore, \u2206 \u22122 [2] ln ln\u2206 \u22121 [2] < c1 lnn \u00b7 \u2211n i=2\u2206 \u22121 [i] ln lnn.\nIn fact, in the lower bound instances, there are log n nonempty groups Gi and they have almost the same weight Hi (hence, Ent(I) = \u0398(ln lnn)). Combining with the MT lower bound, we have covered the two extreme ends of Conjecture 3.5.\nMoreover, it is possible to extend our current technique to construct many instances IS such that any algorithm A requires at least \u2126(H(IS) \u00b7 Ent(IS)) samples. This strongly suggests \u2126(H(I) \u00b7 Ent(I)) is the right lower bound. However, a complete resolution of Conjecture 3.5 seems to require new techniques."}], "references": [{"title": "Instance-optimal geometric algorithms", "author": ["Peyman Afshani", "J\u00e9r\u00e9my Barbay", "Timothy M Chan"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Afshani et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Afshani et al\\.", "year": 2009}, {"title": "On the optimal sample complexity for best arm identification", "author": ["Lijie Chen", "Jian Li"], "venue": "arXiv preprint arXiv:1511.03774,", "citeRegEx": "Chen and Li.,? \\Q2015\\E", "shortCiteRegEx": "Chen and Li.", "year": 2015}, {"title": "Optimal aggregation algorithms for middleware", "author": ["Ronald Fagin", "Amnon Lotem", "Moni Naor"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Fagin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fagin et al\\.", "year": 2003}, {"title": "Asymptotic behavior of expected sample size in certain one sided tests", "author": ["RH Farrell"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Farrell.,? \\Q1964\\E", "shortCiteRegEx": "Farrell.", "year": 1964}, {"title": "lil\u2019ucb: An optimal exploration algorithm for multi-armed bandits", "author": ["Kevin Jamieson", "Matthew Malloy", "Robert Nowak", "S\u00e9bastien Bubeck"], "venue": null, "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Zohar Karnin", "Tomer Koren", "Oren Somekh"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["Shie Mannor", "John N Tsitsiklis"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}], "referenceMentions": [{"referenceID": 6, "context": "The current best lower bound is due to Mannor and Tsitsiklis (2004), who showed that for any \u03b4-correct algorithm for BEST-1-ARM, it requires \u03a9 (", "startOffset": 39, "endOffset": 68}, {"referenceID": 0, "context": ", due to Karnin et al. (2013). Jamieson et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 0, "context": "Jamieson et al. (2014) obtained a UCB-type algorithm (called lil\u2019UCB), which achieves the same sample complexity.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Back in 1964, Farrell (1964) provided an \u03a9(\u2206\u22122 ln ln\u2206 2 ) lower bound for the two-arm cases (which matches the KKS-JMNS bound for two arms).", "startOffset": 14, "endOffset": 29}, {"referenceID": 0, "context": "Very recently, in an unpublished manuscript (Chen and Li (2015)), the authors obtained improved lower and upper bounds for BEST-1-ARM.", "startOffset": 45, "endOffset": 64}, {"referenceID": 0, "context": "Very recently, in an unpublished manuscript (Chen and Li (2015)), the authors obtained improved lower and upper bounds for BEST-1-ARM. The work lead the authors to make an intriguing conjecture which we detail in the next section. We will also state the improved bounds and their connection to the conjecture in more details. 3. Open Problem: Almost Instance Optimality and the Gap Entropy Conjecture We propose to study BEST-1-ARM from the perspective of instance optimality, the ultimate notion of optimality (see e.g., Fagin et al. (2003); Afshani et al.", "startOffset": 45, "endOffset": 542}, {"referenceID": 0, "context": "(2003); Afshani et al. (2009)).", "startOffset": 8, "endOffset": 30}, {"referenceID": 0, "context": "(2003); Afshani et al. (2009)). For the 2-arm cases, the KKS-JMNS bound O(\u2206\u22122 ln ln\u2206 2 ) is an upper bound for every instance, and the Farrell lower bound \u03a9(\u2206\u22122 ln ln\u2206 2 ) is a lower bound for the worst case instances. As we observed in (Chen and Li (2015)), it is impossible to obtain an instance optimal algorithm even for the 2-arm cases.", "startOffset": 8, "endOffset": 257}, {"referenceID": 0, "context": "(2003); Afshani et al. (2009)). For the 2-arm cases, the KKS-JMNS bound O(\u2206\u22122 ln ln\u2206 2 ) is an upper bound for every instance, and the Farrell lower bound \u03a9(\u2206\u22122 ln ln\u2206 2 ) is a lower bound for the worst case instances. As we observed in (Chen and Li (2015)), it is impossible to obtain an instance optimal algorithm even for the 2-arm cases. While the observation has ruled out any hope of an instance optimal algorithm for BEST-1-ARM, however, as we will see, it is still possible to obtain very satisfiable answer in terms of instance optimality. Now, we formally define what is an instance-wise lower bound. Clearly, two arm instances differ only by a permutation of arms should be considered as the same instance. Inspired by Afshani et al. (2009), we give the following natural definition.", "startOffset": 8, "endOffset": 752}, {"referenceID": 0, "context": "Interestingly, the formula involves an entropy term (similar entropy terms also appear in Afshani et al. (2009) for completely different problems).", "startOffset": 90, "endOffset": 112}, {"referenceID": 1, "context": "Motivation and Current Progress In our recent work (Chen and Li (2015)), we provide an algorithm with the following sample complexity:", "startOffset": 52, "endOffset": 71}, {"referenceID": 5, "context": "Consider an elimination-based algorithm (such as Karnin et al. (2013) or our algorithm).", "startOffset": 49, "endOffset": 70}, {"referenceID": 5, "context": "The algorithm in Karnin et al. (2013) used \u03b4r = O(\u03b4 \u00b7 r\u22122), and we used a better way to assign \u03b4r.", "startOffset": 17, "endOffset": 38}], "year": 2016, "abstractText": "The best arm identification problem (BEST-1-ARM) is the most basic pure exploration problem in stochastic multi-armed bandits. The problem has a long history and attracted significant attention for the last decade. However, we do not yet have a complete understanding of the optimal sample complexity of the problem: The state-of-the-art algorithms achieve a sample complexity of O( \u2211 n i=2 \u2206 i (ln \u03b4 + ln ln\u2206 i )) (\u2206i is the difference between the largest mean and the i mean), while the best known lower bound is \u03a9( \u2211n i=2 \u2206 i ln \u03b4) for general instances and \u03a9(\u2206 ln ln\u2206) for the two-arm instances. We propose to study the instance-wise optimality for the BEST-1-ARM problem. Previous work has proved that it is impossible to have an instance optimal algorithm for the 2-arm problem. However, we conjecture that modulo the additive term \u03a9(\u2206 2 ln ln\u2206 2 ) (which is an upper bound and worst case lower bound for the 2-arm problem), there is an instance optimal algorithm for BEST-1-ARM. Moreover, we introduce a new quantity, called the gap entropy for a best-arm problem instance, and conjecture that it is the instance-wise lower bound. Hence, resolving this conjecture would provide a final answer to the old and basic problem.", "creator": "LaTeX with hyperref package"}}}