{"id": "1605.09186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Does Multimodality Help Human and Machine for Translation and Image Captioning?", "abstract": "This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.", "histories": [["v1", "Mon, 30 May 2016 11:47:00 GMT  (218kb,D)", "https://arxiv.org/abs/1605.09186v1", "7 pages, 2 figures, Submitted to ACL 2016 First Conference on Machine Translation (WMT16)"], ["v2", "Thu, 2 Jun 2016 13:52:45 GMT  (218kb,D)", "http://arxiv.org/abs/1605.09186v2", "7 pages, 2 figures, Submitted to ACL 2016 First Conference on Machine Translation (WMT16) v2: Added softmax equations for attentions and revised conclusion"], ["v3", "Mon, 13 Jun 2016 15:33:11 GMT  (218kb,D)", "http://arxiv.org/abs/1605.09186v3", "7 pages, 2 figures, Submitted to ACL 2016 First Conference on Machine Translation (WMT16) v2: Added softmax equations for attentions and revised conclusion v3: Small revisions"], ["v4", "Tue, 16 Aug 2016 12:11:29 GMT  (209kb,D)", "http://arxiv.org/abs/1605.09186v4", "7 pages, 2 figures, v4: Small clarification in section 4 title and content"]], "COMMENTS": "7 pages, 2 figures, Submitted to ACL 2016 First Conference on Machine Translation (WMT16)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["ozan caglayan", "walid aransa", "yaxing wang", "marc masana", "mercedes garc\\'ia-mart\\'inez", "fethi bougares", "lo\\\"ic barrault", "joost van de weijer"], "accepted": false, "id": "1605.09186"}, "pdf": {"name": "1605.09186.pdf", "metadata": {"source": "CRF", "title": "Does Multimodality Help Human and Machine for Translation and Image Captioning?", "authors": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer", "Le Mans"], "emails": ["FirstName.LastName@lium.univ-lemans.fr", "joost@cvc.uab.es", "mmasana@cvc.uab.es", "yaxing@cvc.uab.es", "ocaglayan@gsu.edu.tr"], "sections": [{"heading": "1 Introduction", "text": "Recently, deep learning has greatly impacted the natural language processing field as well as computer vision. Machine translation (MT) with deep neural networks (DNN), proposed by (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and (Bahdanau et al., 2014) competed successfully in the last year\u2019s WMT evaluation campaign (Bojar et al., 2015).\nIn the same trend, generating descriptions from images using DNNs has been proposed by (Elliott et al., 2015). Several attempts have been made to incorporate features from different modalities in order to help the automatic system to better model the task at hand (Elliott et al., 2015; Kiros et al., 2014b; Kiros et al., 2014a).\nThis paper describes the systems developed by LIUM and CVC who participated in the two proposed tasks for the WMT 2016 Multimodal Machine Translation evaluation campaign: Multimodal machine translation (Task 1) and multimodal image description (Task 2).\nThe remainder of this paper is structured in two parts: The first part (section 2) describes the architecture of the four systems (two monomodal and two multimodal) submitted for Task 1. The standard phrase-based SMT systems based on Moses are described in section 2.1 while the neural MT systems are described in section 2.2 (monomodal) and section 3.2 (multimodal). The second part (section 3) contains the description of the two systems submitted for Task 2: The first one is a monomodal neural MT system similar to the one presented in section 2.2, and the second one is a multimodal neural machine translation (MNMT) with shared attention mechanism.\nIn order to evaluate the feasibility of the multimodal approach, we also asked humans to perform the two tasks of this evaluation campaign. Results show that the additional English description sentences improved performance while the straightforward translation of the sentence without the image did not provide as good results. The results of these experiments are presented in section 4."}, {"heading": "2 Multimodal Machine Translation", "text": "This task consists in translating an English sentence that describes an image into German, given the English sentence itself and the image that it describes."}, {"heading": "2.1 Phrase-based System", "text": "Our baseline system for task 1 is developed following the standard phrase-based Moses pipeline as described in (Koehn et al., 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003). This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores on the validation set.\nar X\niv :1\n60 5.\n09 18\n6v 4\n[ cs\n.C L\n] 1\n6 A\nug 2\n01 6\nWe also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al., 2015). This CSLM architecture allows us to use sentence-level features for each line in the training data (i.e. all n-grams in the same sentence will have the same auxiliary features). By this means, better context specific LM estimations can be obtained.\nWe used four additional scores to rerank 1000- best outputs of our baseline system: The first two scores are obtained from two separate CSLM(s) trained on the target side (i.e. German) of the parallel training corpus and each one of the following auxiliary features:\n\u2022 VGG19-FC7 image features: The auxiliary feature used in the first CSLM are the image features provided by the organizers which are extracted from the FC7 layer (relu7) of the VGG-19 network (Simonyan and Zisserman, 2014). This allows us to train a multimodal CSLM that uses additional context learned from the image features.\n\u2022 Source side sentence representation vectors: We used the method described in (Le and Mikolov, 2014) to compute continuous space representation vector for each source (i.e. English) sentence that will be provided to the second CSLM as auxiliary feature. The idea behind this is to condition our target language model on the source side as additional context.\nThe two other scores used for n-best reranking are the log probability computed by our NMT system that will be described in the following section and the score obtained by a Recurrent Neural Network Language Model (RNNLM) (Mikolov et al., 2010). The weights of the original moses features and our additional features were optimized to maximize the BLEU score on the validation set."}, {"heading": "2.2 Neural MT System", "text": "The fundamental model that we experimented2 is an attention based encoder-decoder approach (Bahdanau et al., 2014) except some notable changes in the recurrent decoder called Conditional GRU.\n1github.com/hschwenk/cslm-toolkit 2github.com/nyu-dl/dl4mt-tutorial\nWe define by X and Y , a source sentence of length N and a target sentence of length M respectively. Each source and target word is represented with an embedding vector of dimension EX and EY respectively:\nX = (x1, x2, ..., xN ), xi \u2208 REX (1) Y = (y1, y2, ..., yM ), yj \u2208 REY (2)\nA bidirectional recurrent encoder reads an input sequenceX in forwards and backwards to produce two sets of hidden states based on the current input and the previous hidden state. An annotation vector hi for each position i is then obtained by concatenating the produced hidden states.\nAn attention mechanism, implemented as a simple fully-connected feed-forward neural network, accepts the hidden state ht of the decoder\u2019s recurrent layer and one input annotation at a time, to produce the attention coefficients. A softmax activation is applied on those attention coefficients to obtain the attention weights used to generate the weighted annotation vector for time t. The initial hidden state h0 of the decoder is determined by a feed-forward layer receiving the mean annotation vector.\nWe use Gated Recurrent Unit (GRU) (Chung et al., 2014) activation function for both recurrent encoders and decoders."}, {"heading": "2.2.1 Training", "text": "We picked the following hyperparameters for all NMT systems both for Task 1 and Task 2. All embedding and recurrent layers have a dimensionality of 620 and 1000 respectively. We used Adam as the stochastic optimizer with a minibatch size of 32, Xavier weight initialization (Glorot and Bengio, 2010) and L2 regularization with \u03bb = 0.0001 except the monomodal Task 1 system for which the choices were Adadelta, sampling from N (0, 0.01) and L2 regularization with \u03bb = 0.0005 respectively.\nThe performance of the network is evaluated on the validation split using BLEU after each 1000 minibatch updates and the training is stopped if BLEU does not improve for 20 evaluation periods. The training times were 16 and 26 hours respectively for monomodal and multimodal systems on a Tesla K40 GPU.\nFinally, we used a classical left to right beamsearch with a beam size of 12 for sentence generation during test time."}, {"heading": "2.3 Data", "text": "Phrase-based and NMT systems for Task 1 are trained using the dataset provided by the organizers and described in Table 1. This dataset consists of 29K parallel sentences (direct translations of image descriptions from English to German) for training, 1014 for validation and finally 1000 for the test set. We preprocessed the dataset using the punctuation normalization, tokenization and lowercasing scripts from Moses. In order to generalize better over the compound structs in German, we trained and applied a compound splitter3 (Sennrich and Haddow, 2015) over the German vocabulary of training and validation sets. This reduces the target vocabulary from 18670 to 15820 unique tokens. During translation generation, the splitted compounds are stitched back together."}, {"heading": "2.4 Results and Analysis", "text": "The results of our phrase-based baseline and the four submitted systems are presented in Table 2. The BL+4Features system is the rescoring of the baseline 1000-best output using all the features described in 2.1 while BL+3Features is the same but excluding FC7 image features. Overall, we were able to improve test set scores by around 0.4 and 0.8 on METEOR and BLEU respectively over a strong phrase-based baseline using auxiliary features.\nRegarding the NMT systems, the monomodal NMT achieved a comparative BLEU score of 32.50 on the test set compared to 33.45 of the phrase-based baseline. The multimodal NMT system that will be described in section 3.2, obtained relatively lower scores when trained using Task 1\u2019s data."}, {"heading": "3 Multimodal Image Description Generation", "text": "The objective of Task 2 is to produce German descriptions of images given the image itself and one or more English descriptions as input.\n3github.com/rsennrich/wmt2014-scripts"}, {"heading": "3.1 Visual Data Representation", "text": "To describe the image content we make use of Convolutional Neural Networks (CNN). In a breakthrough work, Krizhevsky et al. (Krizhevsky et al., 2012) convincingly show that CNNs yield a far superior image representation compared to previously used hand-crafted image features. Based on this success an intensified research effort started to further improve the representations based on CNNs. The work of Simonyan and Zisserman (Simonyan and Zisserman, 2014) improved the network by breaking up large convolutional features into multiple layers of small convolutional features, which allowed to train a much deeper network. The organizers provide these features to all participants. More precisely they provide the features from the fifth convolutional layer, and the features from the second fully connected layer of VGG-19. Recently, Residual Networks (ResNet) have been proposed (He et al., 2015). These networks learn residual functions which are constructed by adding skip layers (or projection layers) to the network. These skip layers prevent the vanishing gradient problem, and allow for much deeper networks (over hundred layers) to be trained.\nTo select the optimal layer for image representation we performed an image classification task on a subsection of images from SUN scenes (Xiao et al., 2010). We extract the features from the various layers of ResNet-50 and evaluate the classification performance (Figure 1). The results increase during the first layers but stabilize from Block-4 on. Based on these results and considering that a higher spatial resolution is better, we have selected layer \u2019res4f relu\u2019 (end of Block-4, after ReLU) for the experiments on multimodal MT. We also compared the features from different networks on the task of image description generation with the system of Xu et al. (Xu et al., 2015). The results for generating English descriptions (Table 3) show a clear performance improvement from VGG-19 to ResNet-50, but comparable results are obtained when going to ResNet-152. Therefore, given the increase in computational cost, we have decided to use ResNet-50 features for our submission."}, {"heading": "3.2 Multimodal NMT System", "text": "The multimodal NMT system is an extension of (Xu et al., 2015) and the monomodal NMT system described in Section 2.2.\nThe model involves two GRU layers and an attention mechanism. The first GRU layer computes an intermediate representation s\n\u2032 j as follows:\ns \u2032 j = (1\u2212 z \u2032 j) s \u2032 j + z \u2032 j sj\u22121 (3) s \u2032 j = tanh(W \u2032 E[yj\u22121] + r \u2032 j (U \u2032 sj\u22121)) (4) r \u2032 j = \u03c3(W \u2032 rE[yj\u22121] + U \u2032 rsj\u22121) (5) z \u2032 j = \u03c3(W \u2032 zE[yj\u22121] + U \u2032 zsj\u22121) (6)\nwhere E is the target word embedding, s \u2032 j is the hidden state, r \u2032 j and z \u2032 j are the reset and update gate activations. W \u2032 , U \u2032 r, W \u2032 r , U \u2032 r, W \u2032 z and U \u2032 z are the parameters to be learned. A shared attention layer similar to (Firat et al., 2016) that consists of a fully-connected feedforward network is used to compute a set of modality specific attention coefficients emodij at\neach timestep j:\nemodij = Uatt tanh(Wcatth mod i +Watts \u2032 j) (7)\nThe attention weight between source modality context i and target word j is computed by applying a softmax on emodij :\n\u03b1ij = exp(etxtij )\u2211N k=1 exp(e txt kj )\n(8)\n\u03b2ij = exp(eimgij )\u2211196 k=1 exp(e img kj )\n(9)\nThe final multimodal context vector cj is obtained as follows:\ncj = tanh( N\u2211 i=1 \u03b1ij h txt i + 196\u2211 i=1 \u03b2ij h img i ) (10)\nThe second GRU generates sj from the intermediate representation s\n\u2032 j and the context vector cj as\nfollows:\nsj = (1\u2212 zj) sj + zj s \u2032 j (11) sj = tanh(Wcj + rj (Us \u2032 j)) (12) rj = \u03c3(Wrcj + Urs \u2032 j) (13) zj = \u03c3(Wzcj + Uzs \u2032 j) (14)\nwhere s \u2032 j is the hidden state, rj and zj are the reset and update gate activations. W , Ur, Wr, Ur, Wz and Uz are the parameters to be learned.\nFinally, in order to compute the target word, the following formulations are applied:\noj = Lo tanh(E[yj\u22121] + Lssj + Lccj) (15)\nP (yj |yj\u22121, sj , cj) = Softmax(oj) (16)\nwhere Lo, Ls and Lc are trained parameters."}, {"heading": "3.2.1 Generation", "text": "Since we are provided 5 source descriptions for each image in order to generate a single German description, we let the NMT generate a German description for each source and pick the one with the highest probability and preferably without an UNK token."}, {"heading": "3.3 Data", "text": "The organizers provided an extended version of the Flickr30K Entities dataset (Elliott et al., 2016) which contains 5 independently crowd-sourced German descriptions for each image in addition to the 5 English descriptions originally found in the dataset. It is possible to use this dataset either by considering the cross product of 5 source and 5 target descriptions (a total of 25 description pairs for each image) or by only taking the 5 pairwise descriptions leading to 725K and 145K training pairs respectively. We decided to use the smaller subset of 145K sentences.\nThe preprocessing is exactly the same as Task 1 except that we only kept sentence pairs with sentence lengths \u2208 [3, 50] and with a ratio of at most 3. This results in a final training dataset of 131K\nsentences (Table 4). We picked the most frequent 10K German words and replaced the rest with an UNK token for the target side. Note that compound splitting was not done for this task."}, {"heading": "3.4 Results and Analysis", "text": "As we can see in Table 5, the multimodal system does not surpass monomodal NMT system. Several explanations can clarify this behavior. First, the architecture is not well suited for integrating image and text representations. This is possible as we did not explore all the possibilities to benefit from both modalities. Another explanation is that the image context contain too much irrelevant information which cannot be discriminated by the lone attention mechanism. This would need a deeper analysis of the attention weights in order to be answered."}, {"heading": "4 Human multimodal description", "text": "To evaluate the importance of the different modalities for the image description generation task, we have performed an experiment where we replace\nthe computer algorithm with human participants. The two modalities are the five English description sentences, and the image. The output is a single description sentence in German. The experiment asks the participants for the following tasks:\n\u2022 Given both the image and the English descriptions: \u2019Describe the image in one sentence in German. You can get help from the English sentences provided.\u2019 \u2022 Given only the image: \u2019Describe the image\nin one sentence in German.\u2019 \u2022 Given only one English sentence: \u2019Translate\nthe English sentence into German.\u2019\nThe experiment was performed by 16 native German speakers proficient in English with age ranging from 23 to 54 (coming from Austria, Germany and Switzerland, of which 10 are female and 6 male). The experiment is performed on the first 80 sentences of the validation set. Participants performed 10 repetitions for each task, and not repeating the same image across tasks. The results of the experiments are presented in Table 6. For humans, the English description sentences help to obtain better performance. Removing the image altogether and providing only a single English description sentence results in a significant drop. We were surprised to observe such a drop, whereas we expected good translations to obtain competitive results. In addition, we have provided the results of our submission on the same subset of images; humans clearly obtain better performance using METEOR metrics, but our approach is clearly outperforming on the BLEU metrics. The participants were not trained on the train set before performing the tasks, which could be one of the reasons for the difference. Furthermore, given the lower performance of only translating one of the English description sentences on both metrics, it could possibly be caused by existing biases in the data set."}, {"heading": "5 Conclusion and Discussion", "text": "We have presented the systems developed by LIUM and CVC for the WMT16 Multimodal Ma-\nchine Translation challenge. Results show that integrating image features into a multimodal neural MT system with shared attention mechanism does not yet surpass the performance obtained with a monomodal system using only text input. However, our multimodal systems do improve upon an image captioning system (which was expected). The phrase-based system can benefit from rescoring with multimodal neural language model as well as rescoring with a neural MT system.\nWe have also presented the results of a human evaluation performing the same tasks as proposed in the challenge. The results are rather clear: image captioning can benefit from multimodality."}, {"heading": "Acknowledgments", "text": "This work was supported by the Chist-ERA project M2CR4. We kindly thank KyungHyun Cho and Orhan Firat for providing the DL4MT tutorial as open source and Kelvin Xu for the arcticcaptions5 system."}], "references": [{"title": "Improving continuous space language models using auxiliary features", "author": ["Walid Aransa", "Holger Schwenk", "Loic Barrault."], "venue": "Proceedings of the 12th International Workshop on Spoken Language Translation, pages 151\u2013158, Da Nang, Vietnam, Decem-", "citeRegEx": "Aransa et al\\.,? 2015", "shortCiteRegEx": "Aransa et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Multi-language image description with neural sequence models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR, abs/1510.04709. m2cr.univ-lemans.fr github.com/kelvinxu/arctic-captions", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1601.01073.", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910). Society for Artificial Intelligence and", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187\u2013197, Edinburgh, Scotland, United Kingdom, July.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Seattle, October. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel."], "venue": "Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 595\u2013603. JMLR Work-", "citeRegEx": "Kiros et al\\.,? 2014a", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel."], "venue": "CoRR, abs/1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014b", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments", "author": ["Alon Lavie", "Abhaya Agarwal."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, StatMT \u201907, pages 228\u2013231, Strouds-", "citeRegEx": "Lavie and Agarwal.,? 2007", "shortCiteRegEx": "Lavie and Agarwal.", "year": 2007}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1405.4053.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Comput. Linguist., 29:19\u201351, March.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL \u201903, pages 160\u2013 167, Stroudsburg, PA, USA. Association for Com-", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Continuous space language models for statistical machine translation", "author": ["Holger Schwenk."], "venue": "The Prague Bulletin of Mathematical Linguistics, (93):137\u2013146.", "citeRegEx": "Schwenk.,? 2010", "shortCiteRegEx": "Schwenk.", "year": 2010}, {"title": "A joint dependency model of morphological and syntactic structure for statistical machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 114\u2013", "citeRegEx": "Sennrich and Haddow.,? 2015", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Srilm - an extensible language modeling toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP 2002), pages 901\u2013904.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR, abs/1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["Jianxiong Xiao", "James Hays", "Krista A Ehinger", "Aude Oliva", "Antonio Torralba."], "venue": "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pages 3485\u20133492. IEEE.", "citeRegEx": "Xiao et al\\.,? 2010", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "Proceedings of The 32nd International Con-", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Machine translation (MT) with deep neural networks (DNN), proposed by (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and (Bahdanau et al.", "startOffset": 70, "endOffset": 126}, {"referenceID": 23, "context": "Machine translation (MT) with deep neural networks (DNN), proposed by (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and (Bahdanau et al.", "startOffset": 70, "endOffset": 126}, {"referenceID": 1, "context": ", 2014) and (Bahdanau et al., 2014) competed successfully in the last year\u2019s WMT evaluation campaign (Bojar et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 3, "context": "In the same trend, generating descriptions from images using DNNs has been proposed by (Elliott et al., 2015).", "startOffset": 87, "endOffset": 109}, {"referenceID": 3, "context": "Several attempts have been made to incorporate features from different modalities in order to help the automatic system to better model the task at hand (Elliott et al., 2015; Kiros et al., 2014b; Kiros et al., 2014a).", "startOffset": 153, "endOffset": 217}, {"referenceID": 11, "context": "Several attempts have been made to incorporate features from different modalities in order to help the automatic system to better model the task at hand (Elliott et al., 2015; Kiros et al., 2014b; Kiros et al., 2014a).", "startOffset": 153, "endOffset": 217}, {"referenceID": 10, "context": "Several attempts have been made to incorporate features from different modalities in order to help the automatic system to better model the task at hand (Elliott et al., 2015; Kiros et al., 2014b; Kiros et al., 2014a).", "startOffset": 153, "endOffset": 217}, {"referenceID": 22, "context": ", 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003).", "startOffset": 15, "endOffset": 30}, {"referenceID": 8, "context": ", 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003).", "startOffset": 38, "endOffset": 54}, {"referenceID": 16, "context": ", 2007), SRILM (Stolcke, 2002), KenLM (Heafield, 2011), and GIZA++ (Och and Ney, 2003).", "startOffset": 67, "endOffset": 86}, {"referenceID": 17, "context": "This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al.", "startOffset": 86, "endOffset": 97}, {"referenceID": 18, "context": "This system is trained using the data provided by the organizers and tuned using MERT (Och, 2003) to maximize BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores on the validation set.", "startOffset": 115, "endOffset": 138}, {"referenceID": 13, "context": ", 2002) and METEOR (Lavie and Agarwal, 2007) scores on the validation set.", "startOffset": 19, "endOffset": 44}, {"referenceID": 19, "context": "We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al.", "startOffset": 53, "endOffset": 68}, {"referenceID": 0, "context": "We also used Continuous Space Language Model1 (CSLM) (Schwenk, 2010) with the auxiliary features support as proposed by (Aransa et al., 2015).", "startOffset": 120, "endOffset": 141}, {"referenceID": 21, "context": "\u2022 VGG19-FC7 image features: The auxiliary feature used in the first CSLM are the image features provided by the organizers which are extracted from the FC7 layer (relu7) of the VGG-19 network (Simonyan and Zisserman, 2014).", "startOffset": 192, "endOffset": 222}, {"referenceID": 14, "context": "\u2022 Source side sentence representation vectors: We used the method described in (Le and Mikolov, 2014) to compute continuous space representation vector for each source (i.", "startOffset": 79, "endOffset": 101}, {"referenceID": 15, "context": "The two other scores used for n-best reranking are the log probability computed by our NMT system that will be described in the following section and the score obtained by a Recurrent Neural Network Language Model (RNNLM) (Mikolov et al., 2010).", "startOffset": 222, "endOffset": 244}, {"referenceID": 1, "context": "The fundamental model that we experimented2 is an attention based encoder-decoder approach (Bahdanau et al., 2014) except some notable changes in the recurrent decoder called Conditional GRU.", "startOffset": 91, "endOffset": 114}, {"referenceID": 2, "context": "We use Gated Recurrent Unit (GRU) (Chung et al., 2014) activation function for both recurrent encoders and decoders.", "startOffset": 34, "endOffset": 54}, {"referenceID": 6, "context": "We used Adam as the stochastic optimizer with a minibatch size of 32, Xavier weight initialization (Glorot and Bengio, 2010) and L2 regularization with \u03bb = 0.", "startOffset": 99, "endOffset": 124}, {"referenceID": 20, "context": "In order to generalize better over the compound structs in German, we trained and applied a compound splitter3 (Sennrich and Haddow, 2015) over the German vocabulary of training and validation sets.", "startOffset": 111, "endOffset": 138}, {"referenceID": 12, "context": "(Krizhevsky et al., 2012) convincingly show that CNNs yield a far superior image representation compared to previously used hand-crafted image features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 21, "context": "The work of Simonyan and Zisserman (Simonyan and Zisserman, 2014) improved the network by breaking up large convolutional features into multiple layers of small convolutional features, which allowed to train a much deeper network.", "startOffset": 35, "endOffset": 65}, {"referenceID": 7, "context": "Recently, Residual Networks (ResNet) have been proposed (He et al., 2015).", "startOffset": 56, "endOffset": 73}, {"referenceID": 24, "context": "To select the optimal layer for image representation we performed an image classification task on a subsection of images from SUN scenes (Xiao et al., 2010).", "startOffset": 137, "endOffset": 156}, {"referenceID": 25, "context": "(Xu et al., 2015).", "startOffset": 0, "endOffset": 17}, {"referenceID": 25, "context": "The multimodal NMT system is an extension of (Xu et al., 2015) and the monomodal NMT system described in Section 2.", "startOffset": 45, "endOffset": 62}, {"referenceID": 24, "context": "Figure 1: Classification accuracy on a subset of SUN scenes (Xiao et al., 2010) for ResNet-50: The colored groups represent the building blocks while the bars inside are the stacked blocks (He et al.", "startOffset": 60, "endOffset": 79}, {"referenceID": 7, "context": ", 2010) for ResNet-50: The colored groups represent the building blocks while the bars inside are the stacked blocks (He et al., 2015).", "startOffset": 117, "endOffset": 134}, {"referenceID": 25, "context": "(Xu et al., 2015).", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "A shared attention layer similar to (Firat et al., 2016) that consists of a fully-connected feedforward network is used to compute a set of modality specific attention coefficients emod ij at each timestep j:", "startOffset": 36, "endOffset": 56}, {"referenceID": 4, "context": "The organizers provided an extended version of the Flickr30K Entities dataset (Elliott et al., 2016) which contains 5 independently crowd-sourced German descriptions for each image in addition to the 5 English descriptions originally found in the dataset.", "startOffset": 78, "endOffset": 100}], "year": 2016, "abstractText": "This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.", "creator": "TeX"}}}