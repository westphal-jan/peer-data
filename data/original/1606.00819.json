{"id": "1606.00819", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations", "abstract": "In this paper, we propose LexVec, a new method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent co-occurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks.", "histories": [["v1", "Thu, 2 Jun 2016 19:35:46 GMT  (23kb)", "https://arxiv.org/abs/1606.00819v1", null], ["v2", "Tue, 7 Jun 2016 02:20:23 GMT  (30kb)", "http://arxiv.org/abs/1606.00819v2", "Converted paper size from A4 to US Letter to avoid margin issues on arXiv"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexandre salle", "aline villavicencio", "marco idiart"], "accepted": true, "id": "1606.00819"}, "pdf": {"name": "1606.00819.pdf", "metadata": {"source": "CRF", "title": "Matrix Factorization using Window Sampling and Negative Sampling for Improved Word Representations", "authors": ["Alexandre Salle", "Marco Idiart", "Aline Villavicencio"], "emails": ["atsalle@inf.ufrgs.br,", "avillavicencio@inf.ufrgs.br,", "idiart@if.ufrgs.br"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n00 81\n9v 2\n[ cs\n.C L\n] 7\nJ un"}, {"heading": "1 Introduction", "text": "Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013). Traditionally, word representations have been obtained using countbased methods (Baroni et al., 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014).\nTechniques for generating lower-rank representations have also been employed, such as PPMI-SVD (Levy et al., 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks.\n\u2217This is a preprint of the paper that will be presented at the 54th Annual Meeting of the Association for Computational Linguistics.\nAlternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skip-gram with Negative Sampling (SGNS, Mikolov et al. (2013b)), which uses a neural network to generate embeddings. It implicitly factorizes a shifted PMI matrix, and its performance has been linked to the weighting of positive and negative co-occurrences (Levy and Goldberg, 2014).\nIn this paper, we present Lexical Vectors (LexVec), a method for factorizing PPMI matrices that combines characteristics of all these methods. On the one hand, it uses SGNS window sampling, negative sampling, and stochastic gradient descent (SGD) to minimize a loss function that weights frequent co-occurrences heavily but also takes into account negative co-occurrence. However, since PPMI generally outperforms PMI on semantic similarity tasks (Bullinaria and Levy, 2007), rather than implicitly factorize a shifted PMI matrix (like SGNS), LexVec explicitly factorizes the PPMI matrix.\nThis paper is organized as follows: First, we describe PPMI-SVD, GloVe, and SGNS (\u00a72) before introducing the proposed method, LexVec (\u00a73), and evaluating it on word similarity and analogy tasks (\u00a74). We conclude with an analysis of results and discussion of future work.\nWe provide source code for the model at https: //github.com/alexandres/lexvec."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 PPMI-SVD", "text": "Given a word w and a symmetric window of win context words to the left and win to the right, the co-occurrence matrix of elements Mwc is defined as the number of times a target word w and the context word c co-occurred in the corpus within the window. The PMI matrix is defined as\nPMIwc = log Mwc M\u2217\u2217 Mw\u2217 M\u2217c\n(1)\nwhere \u2019*\u2019 represents the summation of the corresponding index. As this matrix is unbounded in the inferior limit, in most applications it is replaced by its positive definite version, PPMI, where negative values are set to zero. The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing (Levy et al., 2015) and subsampling the corpus (Mikolov et al., 2013b). As word embeddings with lower dimensionality may improve efficiency and generalization (Levy et al., 2015), the improved PPMI\u2217 matrix can be factorized as a product of two lower rank matrices.\nPPMI\u2217wc \u2243 WwW\u0303 \u22a4 c (2)\nwhere Ww and W\u0303c are d-dimensional row vectors corresponding to vector embeddings for the target and context words. Using the truncated SVD of size d yields the factorization U\u03a3T\u22a4 with the lowest possible L2 error (Eckert and Young, 1936).\nLevy et al. (2015) recommend using W = U\u03a3p\nas the word representations, as suggested by Bullinaria and Levy (2012), who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis. Although the optimal value of p is highly task-dependent (O\u0308sterlund et al., 2015), we set p = 0.5 as it has been shown to perform well on the word similarity and analogy tasks we use in our experiments (Levy et al., 2015)."}, {"heading": "2.2 GloVe", "text": "GloVe (Pennington et al., 2014) factors the logarithm of the co-occurrence matrix M\u0302 that considers the position of the context words in the window. The\nloss function for factorization is\nLGloV ewc = 1\n2 f(M\u0302wc)(WwW\u0303\n\u22a4 c +bw+ b\u0303c\u2212log M\u0302wc) 2\n(3) where bw and b\u0303c are bias terms, and f is a weighting function defined as\nf(x) =\n{\n(x/xmax) \u03b2 if x < xmax 1 otherwise (4)\nW and W\u0303 are obtained by iterating over all non-zero (w, c) cells in the co-occurrence matrix and minimizing eq. (3) through SGD.\nThe weighting function (in eq. (3)) penalizes more heavily reconstruction error of frequent cooccurrences, improving on PPMI-SVD\u2019s L2 loss, which weights all reconstruction errors equally. However, as it does not penalize reconstruction errors for pairs with zero counts in the co-occurrence matrix, no effort is made to scatter the vectors for these pairs."}, {"heading": "2.3 Skip-gram with Negative Sampling (SGNS)", "text": "SGNS (Mikolov et al., 2013b) trains a neural network to predict the probability of observing a context word c given a target word w, sliding a symmetric window over a subsampled training corpus with the window size being sampled uniformly from the range [1, win]. Each observed (w, c) pair is combined with k randomly sampled noise pairs (w,wi) and used to calculate the loss function\nLSGNSwc = log \u03c3(WwW\u0303c \u22a4 )+\nk \u2211\ni=1\nEwi\u223cPn(w) log \u03c3(\u2212WwW\u0303 \u22a4 wi )\n(5)\nwhere Pn(w) is the distribution from which noise words wi are sampled.1 We refer to this routine which SGNS uses for selecting (w, c) pairs by sliding a context window over the corpus for loss calculation and SGD as window sampling.\nSGNS is implicitly performing the weighted factorization of a shifted PMI matrix (Levy and Goldberg, 2014). Window sampling ensures the factorization weights frequent co-occurrences heavily, but also takes into account negative co-occurrences, thanks to negative sampling.\n1Following Mikolov et al. (2013b) it is the unigram distribution raised to the 3/4 power."}, {"heading": "3 LexVec", "text": "LexVec is based on the idea of factorizing the PPMI matrix using a reconstruction loss function that does not weight all errors equally, unlike SVD, but instead penalizes errors of frequent cooccurrences more heavily, while still treating negative co-occurrences, unlike GloVe. Moreover, given that using PPMI results in better performance than PMI on semantic tasks, we propose keeping the SGNS weighting scheme by using window sampling and negative sampling, but explicitly factorizing the PPMI matrix rather than implicitly factorizing the shifted PMI matrix. The LexVec loss function has two terms\nLLexV ecwc = 1\n2 (WwW\u0303c\n\u22a4\n\u2212 PPMI\u2217wc) 2 (6)\nLLexV ecw = 1\n2\nk \u2211\ni=1\nEwi\u223cPn(w)(WwW\u0303wi \u22a4 \u2212 PPMI\u2217wwi) 2\n(7)\nWe minimize eqs. (6) and (7) using two alternative approaches: Mini-Batch (MB): This variant executes gradient descent in exactly the same way as SGNS. Every time a pair (w, c) is observed by window sampling and pairs (w,w1...k) drawn by negative sampling, Ww, W\u0303c, and W\u0303w1...k are updated by gradient descent on the sum of eq.(6) and eq.(7). The global loss for this approach is\nLLexV ec = \u2211\n(w,c)\n#(w, c) (LLexV ecwc +L LexV ec w ) (8)\nwhere #(w, c) is the number of times (w, c) is observed in the subsampled corpus. Stochastic (St): Every context window is extended with k negative samples w1...k. Iterative gradient descent of eq. (6) is then run on pairs (w, cj), for j = 1, .., 2\u2217win and (w, ci), j = 1, .., k for each window. The global loss for this approach is\nLLexV ec \u2032 = \u2211\n(w,c)\n#(w, c)LLexV ecwc +\n\u2211\nw\n#(w)LLexV ecw\n(9)\nwhere #(w) is the number of times w is observed in the subsampled corpus.\nIf a pair (w, c) co-occurs frequently, #(w, c) will weigh heavily in both eqs. (8) and (9), giving the desired weighting for frequent co-occurrences. The noise term, on the other hand, has corrections proportional to #(w) and #(wi), for each pair (w,wi). It produces corrections in pairs that due to frequency should be in the corpus but are not observed, therefore accounting automatically for negative cooccurrences."}, {"heading": "4 Materials", "text": "All models were trained on a dump of Wikipedia from June 2015, split into sentences, with punctuation removed, numbers converted to words, and lower-cased. Words with less than 100 counts were removed, resulting in a vocabulary of 302,203 words. All models generate embeddings of 300 dimensions.\nThe PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of \u03b1 = 3/4 suggested in (Levy et al., 2015) and an unweighted window of size 2. A dirty subsampling of the corpus is adopted for PPMI* and SGNS with threshold of t = 10\u22125 (Mikolov et al., 2013b).2 Additionally, SGNS uses 5 negative samples (Mikolov et al., 2013b), a window of size 10 (Levy et al., 2015), for 5 iterations with initial learning rate set to the default 0.025. GloVe is run with a window of size 10, xmax = 100, \u03b2 = 3/4, for 50 iterations and initial learning rate of 0.05 (Pennington et al., 2014).\nIn LexVec two window sampling alternatives are compared: WSPPMI , which keeps the same fixed size win = 2 as used to create the PPMI\u2217 matrix; or WSSGNS , which adopts identical SGNS settings (win = 10 with size randomization). We run LexVec for 5 iterations over the training corpus.\nAll methods generate both word and context matrices (W and W\u0303 ): W is used for SGNS, PPMI-SVD and W+W\u0303 for GloVe (following Levy et al. (2015), and W and W + W\u0303 for LexVec.\nFor evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015). We examine, in particular, if LexVec weighted PPMI\u2217 factorization outperforms SVD, GloVe (weighted\n2Words with unigram relative frequency f > t are discarded from the training corpus with probability pw = 1\u2212 \u221a t/f .\nfactorization of log M\u0302 ) and Skip-gram (implicit factorization of the shifted PMI matrix), and compare the stochastic and mini-batch approaches.\nWord similarity tasks are:3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001), MEN (Bruni et al., 2012), MTurk (Radinsky et al., 2011), RW (Luong et al., 2013), SimLex999 (Hill et al., 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012), calculated using cosine. Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c), using 3CosAdd and 3CosMul (Levy et al., 2014)."}, {"heading": "5 Results", "text": "Results for word similarity and for the analogy tasks are in tables 1 and 2, respectively. Compared with PPMI-SVD, LexVec performs better in all tasks. As they factorize the same PPMI\u2217 matrix, it is the\n3http://www.cs.cmu.edu/ mfaruqui/suite.html\nloss weighting from window sampling that is an improvement over L2 loss. As expected, due to PPMI, LexVec performs better than SGNS in several word similarity tasks, but in addition it also does so on the semantic analogy task, nearly approaching GloVe. LexVec generally outperforms GloVe on word similarity tasks, possibly due to the factorization of the PPMI matrix and to window sampling\u2019s weighting of negative co-occurrences.\nWe believe LexVec fares well on semantic analogies because its vector-space does a good job of preserving semantics, as evidenced by its performance on word similarity tasks. We believe the poor syntactic performance is a result of the PPMI measure. PPMI-SVD also struggled with syntactic analogies more than any other task. Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by Levy et al. (2014) might help in recovering syntactic analogies.\nIn terms of configurations, WSSGNS performed marginally better than WSPPMI . We hypothesize it is simply because of the additional computation.\nWhile W and (W + W\u0303 ) are roughly equivalent on word similarity tasks, W is better for analogies. This is inline with results for PPMI-SVD and SGNS models (Levy et al., 2015). Both mini-batch and stochastic approaches result in similar scores for all tasks. For the same parameter k of negative samples, the mini-batch approach uses 2 \u2217 winWSPPMI times more negative samples than stochastic when using WSPPMI , and winWSSGNS times more samples when using WSSGNS. Therefore, the stochastic approach is more computationally efficient while delivering similar performance."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we introduced LexVec, a method for low-rank, weighted factorization of the PPMI matrix that generates distributed word representations, favoring low reconstruction error on frequent co-occurrences, whilst accounting for negative cooccurrences as well. This is in contrast with PPMISVD, which does no weighting, and GloVe, which only considers positive co-occurrences. Finally, its PPMI factorization seems to better capture semantics when compared to the shifted PMI factorization of SGNS. As a result, it outperforms PPMI-SVD and SGNS in a variety of word similarity and semantic analogy tasks, and generally outperforms GloVe on similarity tasks.\nFuture work will examine the use of positional contexts for improving performance on syntactic analogy tasks. Moreover, we will explore further the hyper-parameter space to find globally optimal values for LexVec, and will experiment with the factorization of other matrices for developing alternative word representations."}, {"heading": "Acknowledgments", "text": "This work has been partly funded by CAPES and by projects AIM-WEST (FAPERGS-INRIA 1706- 2551/13-7), CNPq 482520/2012-4, 312114/2015-0, \u201cSimplificac\u0327a\u0303o Textual de Expresso\u0303es Complexas\u201d, sponsored by Samsung Eletro\u0302nica da Amazo\u0302nia Ltda. under the terms of Brazilian federal law No. 8.248/91."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Associ-", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Asso-", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Extracting semantic representations from word cooccurrence statistics: A computational study", "author": ["John A. Bullinaria", "Joseph P. Levy."], "venue": "Behavior research methods 39(3):510\u2013526.", "citeRegEx": "Bullinaria and Levy.,? 2007", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "Extracting semantic representations from word cooccurrence statistics: stop-lists, stemming, and svd", "author": ["John A Bullinaria", "Joseph P Levy."], "venue": "Behavior research methods 44(3):890\u2013907.", "citeRegEx": "Bullinaria and Levy.,? 2012", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2012}, {"title": "Experiments with lsa scoring: Optimal rank and basis", "author": ["John Caron."], "venue": "Proceedings of the SIAM Computational Information Retrieval Workshop. pages 157\u2013169.", "citeRegEx": "Caron.,? 2001", "shortCiteRegEx": "Caron.", "year": 2001}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth W. Church", "Patrick Hanks."], "venue": "Computational Linguistics 16(1):22\u2013", "citeRegEx": "Church and Hanks.,? 1990", "shortCiteRegEx": "Church and Hanks.", "year": 1990}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The approximation of one matrix by another of lower rank", "author": ["C. Eckert", "G. Young."], "venue": "Psych. 1:211\u2013218.", "citeRegEx": "Eckert and Young.,? 1936", "shortCiteRegEx": "Eckert and Young.", "year": 1936}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web. ACM,", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computa-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Advances in Neural Information Processing Systems. pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics pages 211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan."], "venue": "CoNLL-2014 page 171.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Dekang Lin."], "venue": "of the 36th and 17th , Volume 2. Montreal, Quebec, Canada, pages 768\u2013 774.", "citeRegEx": "Lin.,? 1998", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "CoNLL-2013 104.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "HLT-NAACL. pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["George A. Miller", "Walter G. Charles."], "venue": "Language and cognitive processes 6(1):1\u201328.", "citeRegEx": "Miller and Charles.,? 1991", "shortCiteRegEx": "Miller and Charles.", "year": 1991}, {"title": "Factorization of latent variables in distributional semantic models", "author": ["Arvid \u00d6sterlund", "David \u00d6dling", "Magnus Sahlgren."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "\u00d6sterlund et al\\.,? 2015", "shortCiteRegEx": "\u00d6sterlund et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "Proceedings of the 20th international conference on World wide", "citeRegEx": "Radinsky et al\\.,? 2011", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough."], "venue": "Communications of the ACM 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "ACL (1). pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics. Association", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013).", "startOffset": 107, "endOffset": 173}, {"referenceID": 6, "context": "Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013).", "startOffset": 107, "endOffset": 173}, {"referenceID": 24, "context": "Distributed word representations, or word embeddings, have been successfully used in many NLP applications (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013).", "startOffset": 107, "endOffset": 173}, {"referenceID": 0, "context": "Traditionally, word representations have been obtained using countbased methods (Baroni et al., 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 14, "context": ", 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al.", "startOffset": 78, "endOffset": 89}, {"referenceID": 5, "context": ", 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al.", "startOffset": 161, "endOffset": 185}, {"referenceID": 2, "context": ", 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014).", "startOffset": 210, "endOffset": 256}, {"referenceID": 13, "context": ", 2014), where the cooccurrence matrix is derived directly from corpus counts (Lin, 1998) or using association measures like Point-wise Mutual Information (PMI) (Church and Hanks, 1990) and Positive PMI (PPMI) (Bullinaria and Levy, 2007; Levy et al., 2014).", "startOffset": 210, "endOffset": 256}, {"referenceID": 12, "context": "Techniques for generating lower-rank representations have also been employed, such as PPMI-SVD (Levy et al., 2015) and GloVe (Pennington et al.", "startOffset": 95, "endOffset": 114}, {"referenceID": 21, "context": ", 2015) and GloVe (Pennington et al., 2014), both achieving state-of-the-art performance on a variety of tasks.", "startOffset": 18, "endOffset": 43}, {"referenceID": 0, "context": "Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skip-gram with Negative Sampling (SGNS, Mikolov et al.", "startOffset": 128, "endOffset": 149}, {"referenceID": 11, "context": "It implicitly factorizes a shifted PMI matrix, and its performance has been linked to the weighting of positive and negative co-occurrences (Levy and Goldberg, 2014).", "startOffset": 140, "endOffset": 165}, {"referenceID": 0, "context": "Alternatively, vector-space models can be generated with predictive methods, which generally outperform the count-based methods (Baroni et al., 2014), the most notable of which is Skip-gram with Negative Sampling (SGNS, Mikolov et al. (2013b)), which uses a neural network to generate embeddings.", "startOffset": 129, "endOffset": 243}, {"referenceID": 2, "context": "However, since PPMI generally outperforms PMI on semantic similarity tasks (Bullinaria and Levy, 2007), rather than implicitly factorize a shifted PMI matrix (like SGNS), LexVec explicitly factorizes the PPMI matrix.", "startOffset": 75, "endOffset": 102}, {"referenceID": 12, "context": "The performance of the PPMI matrix on word similarity tasks can be further improved by using context-distribution smoothing (Levy et al., 2015) and subsampling the corpus (Mikolov et al.", "startOffset": 124, "endOffset": 143}, {"referenceID": 17, "context": ", 2015) and subsampling the corpus (Mikolov et al., 2013b).", "startOffset": 35, "endOffset": 58}, {"referenceID": 12, "context": "As word embeddings with lower dimensionality may improve efficiency and generalization (Levy et al., 2015), the improved PPMI matrix can be factorized as a product of two lower rank matrices.", "startOffset": 87, "endOffset": 106}, {"referenceID": 7, "context": "Using the truncated SVD of size d yields the factorization U\u03a3T with the lowest possible L2 error (Eckert and Young, 1936).", "startOffset": 97, "endOffset": 121}, {"referenceID": 20, "context": "Although the optimal value of p is highly task-dependent (\u00d6sterlund et al., 2015), we set p = 0.", "startOffset": 57, "endOffset": 81}, {"referenceID": 12, "context": "5 as it has been shown to perform well on the word similarity and analogy tasks we use in our experiments (Levy et al., 2015).", "startOffset": 106, "endOffset": 125}, {"referenceID": 4, "context": "Using the truncated SVD of size d yields the factorization U\u03a3T with the lowest possible L2 error (Eckert and Young, 1936). Levy et al. (2015) recommend using W = U\u03a3 as the word representations, as suggested by Bullinaria and Levy (2012), who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis.", "startOffset": 98, "endOffset": 142}, {"referenceID": 2, "context": "(2015) recommend using W = U\u03a3 as the word representations, as suggested by Bullinaria and Levy (2012), who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis.", "startOffset": 75, "endOffset": 102}, {"referenceID": 2, "context": "(2015) recommend using W = U\u03a3 as the word representations, as suggested by Bullinaria and Levy (2012), who borrowed the idea of weighting singular values from the work of Caron (2001) on Latent Semantic Analysis.", "startOffset": 75, "endOffset": 184}, {"referenceID": 21, "context": "GloVe (Pennington et al., 2014) factors the logarithm of the co-occurrence matrix M\u0302 that considers the position of the context words in the window.", "startOffset": 6, "endOffset": 31}, {"referenceID": 17, "context": "SGNS (Mikolov et al., 2013b) trains a neural network to predict the probability of observing a context word c given a target word w, sliding a symmetric window over a subsampled training corpus with the window size being sampled uniformly from the range [1, win].", "startOffset": 5, "endOffset": 28}, {"referenceID": 11, "context": "SGNS is implicitly performing the weighted factorization of a shifted PMI matrix (Levy and Goldberg, 2014).", "startOffset": 81, "endOffset": 106}, {"referenceID": 16, "context": "Following Mikolov et al. (2013b) it is the unigram distribution raised to the 3/4 power.", "startOffset": 10, "endOffset": 33}, {"referenceID": 12, "context": "The PPMI* matrix used by both PPMI-SVD and LexVec was constructed using smoothing of \u03b1 = 3/4 suggested in (Levy et al., 2015) and an unweighted window of size 2.", "startOffset": 106, "endOffset": 125}, {"referenceID": 17, "context": "A dirty subsampling of the corpus is adopted for PPMI* and SGNS with threshold of t = 10\u22125 (Mikolov et al., 2013b).", "startOffset": 91, "endOffset": 114}, {"referenceID": 17, "context": "2 Additionally, SGNS uses 5 negative samples (Mikolov et al., 2013b), a window of size 10 (Levy et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 12, "context": ", 2013b), a window of size 10 (Levy et al., 2015), for 5 iterations with initial learning rate set to the default 0.", "startOffset": 30, "endOffset": 49}, {"referenceID": 21, "context": "05 (Pennington et al., 2014).", "startOffset": 3, "endOffset": 28}, {"referenceID": 12, "context": "All methods generate both word and context matrices (W and W\u0303 ): W is used for SGNS, PPMI-SVD and W+W\u0303 for GloVe (following Levy et al. (2015), and W and W + W\u0303 for LexVec.", "startOffset": 124, "endOffset": 143}, {"referenceID": 17, "context": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015).", "startOffset": 66, "endOffset": 152}, {"referenceID": 13, "context": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015).", "startOffset": 66, "endOffset": 152}, {"referenceID": 21, "context": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015).", "startOffset": 66, "endOffset": 152}, {"referenceID": 12, "context": "For evaluation, we use standard word similarity and analogy tasks (Mikolov et al., 2013b; Levy et al., 2014; Pennington et al., 2014; Levy et al., 2015).", "startOffset": 66, "endOffset": 152}, {"referenceID": 8, "context": "Word similarity tasks are:3 WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001), MEN (Bruni et al.", "startOffset": 76, "endOffset": 102}, {"referenceID": 1, "context": ", 2001), MEN (Bruni et al., 2012), MTurk (Radinsky et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 22, "context": ", 2012), MTurk (Radinsky et al., 2011), RW (Luong et al.", "startOffset": 15, "endOffset": 38}, {"referenceID": 15, "context": ", 2011), RW (Luong et al., 2013), SimLex999 (Hill et al.", "startOffset": 12, "endOffset": 32}, {"referenceID": 9, "context": ", 2013), SimLex999 (Hill et al., 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 19, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 12, "endOffset": 38}, {"referenceID": 23, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 43, "endOffset": 76}, {"referenceID": 10, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012), calculated using cosine.", "startOffset": 87, "endOffset": 107}, {"referenceID": 16, "context": "Word analogy tasks are: Google semantic (GSem) and syntactic (GSyn) (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 18, "context": ", 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c), using 3CosAdd and 3CosMul (Levy et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 13, "context": ", 2013c), using 3CosAdd and 3CosMul (Levy et al., 2014).", "startOffset": 36, "endOffset": 55}, {"referenceID": 12, "context": "Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by Levy et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Levy et al. (2015) obtained similar results, and suggest that using positional contexts as done by Levy et al. (2014) might help in recovering syntactic analogies.", "startOffset": 0, "endOffset": 118}, {"referenceID": 12, "context": "This is inline with results for PPMI-SVD and SGNS models (Levy et al., 2015).", "startOffset": 57, "endOffset": 76}], "year": 2016, "abstractText": "In this paper, we propose LexVec, a new method for generating distributed word representations that uses low-rank, weighted factorization of the Positive Point-wise Mutual Information matrix via stochastic gradient descent, employing a weighting scheme that assigns heavier penalties for errors on frequent co-occurrences while still accounting for negative co-occurrence. Evaluation on word similarity and analogy tasks shows that LexVec matches and often outperforms state-of-the-art methods on many of these tasks.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}