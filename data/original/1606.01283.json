{"id": "1606.01283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Enhancing the LexVec Distributed Word Representation Model Using Positional Contexts and External Memory", "abstract": "In this paper we take a state-of-the-art model for distributed word representation that explicitly factorizes the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling and address two of its shortcomings. We improve syntactic performance by using positional contexts, and solve the need to store the PPMI matrix in memory by working on aggregate data in external memory. The effectiveness of both modifications is shown using word similarity and analogy tasks.", "histories": [["v1", "Fri, 3 Jun 2016 21:39:42 GMT  (21kb)", "http://arxiv.org/abs/1606.01283v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexandre salle", "marco idiart", "aline villavicencio"], "accepted": false, "id": "1606.01283"}, "pdf": {"name": "1606.01283.pdf", "metadata": {"source": "CRF", "title": "Enhancing the LexVec Distributed Word Representation Model Using Positional Contexts and External Memory", "authors": ["Alexandre Salle", "Marco Idiart", "Aline Villavicencio"], "emails": ["atsalle@inf.ufrgs.br,", "avillavicencio@inf.ufrgs.br,", "idiart@if.ufrgs.br"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n01 28\n3v 1\n[ cs\n.C L\n] 3\nJ un"}, {"heading": "1 Introduction", "text": "Distributed word representations have become a mainstay in natural language processing, enjoying a slew of applications (Sebastiani, 2002; Turian et al., 2010; Socher et al., 2013). Though Baroni et al. (2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al., 2014; Salle et al., 2016).\nIn this paper, we focus on improving a state-ofthe-art counting model, LexVec (Salle et al., 2016), which performs factorization of the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling (WSNS). Salle et al. (2016) suggest that LexVec matches and often outperforms competing models in word similarity and semantic analogy tasks. Here we show that using positional contexts to approximate syntactic\ndependencies yields state-of-the-art performance on syntactic analogy tasks as well. We also show how it is possible to approximate WSNS using aggregate data, eliminating random access to the PPMI matrix, enabling the use of external memory. Though not undertaken in this paper, this modification effectively allows LexVec to be trained on web-scale corpora.\nThis paper is organized as follows: we review the LexVec model (\u00a72) and detail how positional contexts and external memory can be incorporated into the model (\u00a73). We describe evaluation methods (\u00a74) and discuss results in terms of related work (\u00a75) and finish with conclusions and future work.\nSource code for the enhanced model is available at https://github.com/alexandres/ lexvec."}, {"heading": "2 LexVec", "text": "LexVec uses WSNS to factorize the PPMI matrix into two lower rank matrices. The co-occurrence matrix M is calculated from word-context pairs (w, c) obtained by sliding a symmetric window of size win over the training corpus C . The PPMI matrix is then calculated as follows\nPPMIwc = max(0, log Mwc M\u2217\u2217 Mw\u2217 M\u2217c ) (1)\nwhere \u2217 represents index summation.\nThe word and context embeddings W and W\u0303 , with dimensions |V | \u00d7 d (where V is the vocabulary and d the embedding dimension), are obtained by the minimization via stochastic gradient descent (SGD) of a combination of the loss functions\nLwc = 1\n2 (WwW\u0303c\n\u22a4\n\u2212 PPMIwc) 2 (2)\nLw = 1\n2\nk\u2211\ni=1\nEwi\u223cPn(w)(WwW\u0303wi \u22a4 \u2212 PPMIwwi) 2 (3)\nusing WSNS. Pn is the distribution used for drawing negative samples, chosen to be\nPn(w) = #(w) \u03b1/\n\u2211\nw\n#(w)\u03b1 (4)\nwith \u03b1 = 3/4 (Mikolov et al., 2013b; Salle et al., 2016), and #(w) the unigram frequency of w.\nTwo methods were defined for the minimization of eqs. (2) and (3): Mini-batch and Stochastic (Salle et al., 2016). Since the latter is more computationally efficient and yields equivalent results, we adopt it in this paper. The Stochastic method extends the context window with noise words drawn using negative sampling according to eq. (4). The key idea is that window sampling is likely to sample related words, approximating their vectors using eq. (2), while negative sampling is likely to select unrelated words, scattering their vectors using eq. (3). The resulting global loss, where #(w, c) = Mwc is thus\nL = \u2211\n(w,c)\n#(w, c)Lwc + \u2211\nw\n#(w)Lw (5)\nGiven a word w and context word c, eq. (5) is proportional to #(w) and #(c). This is the desired behaviour for the global loss function, since the more frequent w or c are in the corpus, the more confident we can be about the corpus estimated PPMIwc. Suppose both #(w) and #(c) are high, but PPMIwc is low. This is unequivocal evidence of negative correlation between them, and so we should put more effort into approximating their PPMI . The argument is analogous for high PPMI . If on the other hand #(w) and #(c) are low, we cannot be too confident about the corpus estimated PPMIwc, and so less effort should be spent on its approximation."}, {"heading": "3 Enhancing LexVec", "text": ""}, {"heading": "3.1 Positional Contexts", "text": "As suggested by Levy et al. (2015) and Salle et al. (2016), positional contexts (introduced in Levy et al. (2014)) are a potential solution to poor performance on syntactic analogy tasks. Rather\nthan only accounting for which context words appear around a target word, positional contexts also account for their position relative to the target word. For example, in the sentence \u201cthe big dog barked loudly\u201d, target word dog has contexts (the\u22122, big\u22121, barked1, loudly2). The cooccurrence matrix, before having dimensions |V | \u00d7 |V |, takes on dimensions |V | \u00d7 2 \u2217 win \u2217 |V | when using positional contexts.\nThis can be incorporated into LexVec with two minor modifications: 1) The context embedding W\u0303 takes on dimensions 2 \u2217 win \u2217 |V | \u00d7 d, 2) Negative sampling must now sample positional contexts rather than simple contexts. This latter point requires that the distribution from which negative samples are drawn become\nPn(c i) = M\u03b1 \u2217ci/ \u2211\nci\nM\u03b1 \u2217ci (6)\nWithout positional contexts, either W or W + W\u0303 can be used as embeddings. Since positional contexts make the dimensions of both matrices incompatible, W\u0303 cannot be used directly. We propose using the sum of all positional context vectors as the context vector for a word (W\u0303 pos) ."}, {"heading": "3.2 External Memory", "text": "As window sampling scans over the training corpus and negative sampling selects random contexts, (w, ci) pairs are generated and the corresponding PPMIwci cell must be accessed so that eqs. (2) and (3) can be minimized. Unfortunately, this results in random access to the PPMI matrix which requires it to be kept in main memory. Pennington et al. (2014) show that the under certain assumptions, this sparse matrix grows as O(|C|0.8), which bounds the maximum corpus size that can be processed by LexVec.\nWe propose an approximation to WSNS that works as follows: All the word-context pairs (w, ci) generated by window sampling the corpus and by negative sampling each target word are first written to a file F . The file F is then sorted with duplicated lines collapsed, and the lines written in the format (w, ci,+/\u2212, tot,Mwci), where +/\u2212 indicates if the pair occurred or not in the corpus, tot is the number of times the pair occurs including negative sampling, and Mwci the number of times it occurred in\nthe corpus. F \u2019s construction requires O(|C|) external memory, and only O(1) main memory. Additionally, all Mw\u2217 and M\u2217ci are kept in main memory, using O(|V |) space. This is nearly identical to the way in which GloVe builds its sparse co-occurrence matrix on disk, with the additional logic for adding and merging negatively sampled pairs.\nWe now present two ways to proceed with training: multiple iteration or single iteration.\nMultiple Iteration (MI): In this variant, F is shuffled. For each tuple (w, ci,+/\u2212, tot,Mwci) in F , eq. (2) is minimized tot times, using Mw\u2217, M\u2217ci , and Mwci to calculate PPMIwci if marker is +, else PPMIwci is equal to zero.\nSingle Iteration (SI): For every tuple (w, ci,+/\u2212, tot,Mwci) in F , write the tuple (w, ci,+/\u2212, 1,Mwci) tot times to a new file F\n\u2032. Then shuffle F \u2032 and execute the MI algorithm described above on F \u2032.\nBoth MI and SI are minimizing the exact same global loss function given by eq. (5) as LexVec without external memory, the only difference between the three being the order in which word-context pairs are processed."}, {"heading": "4 Materials", "text": "We report results from Salle et al. (2016) and use the same training corpus and parameters to train LexVec with positional contexts and external memory. The corpus is a Wikipedia dump from June 2015, tokenized, lowercased, and split into sentences, removing punctuation and converting numbers to words, for a final vocabulary of 302,203 words.\nAll generated embeddings have dimensionality equal to 300. As recommended in Levy et al. (2015) and used in Salle et al. (2016), the PPMI matrix used in all LexVec models and in PPMI-SVD is transformed using context distribution smoothing exponentiating context frequencies to the power 0.75. PPMI-SVD is the singular value decomposition of the PPMI matrix. LexVec and PPMI-SVD use symmetric windows of size 2. Both GloVe (Pennington et al., 2014) and Skip-gram with negative sampling (SGNS) (Mikolov et al., 2013b) were trained using a symmetric window of size 10. GloVe was run for 50 iterations, using parameters xmax = 100, \u03b2 = 3/4, and learning rate of 0.05. LexVec and SGNS were\nrun for 5 iterations, using 5 negative samples, and initial learning rate of 0.025. LexVec, PPMI-SVD, and SGNS use dirty subsampling (Mikolov et al., 2013b; Levy et al., 2015) with threshold t = 10\u22125. Words in the training corpus with unigram probability f greater than t are discarded with probability 1\u2212 \u221a\nt/f . For LexVec, we report results for W and (W+W\u0303 pos) embeddings when using positional contexts, otherwise W and (W + W\u0303 ). For PPMI-SVD and GloVe we report (W + W\u0303 ), and for SGNS, W , that correspond to their best results.\nThe goal of our evaluation is to determine whether: 1) Positional contexts improve syntactic performance 2) The use of external memory is a good approximation of WSNS. Therefore, we perform the exact same evaluation as Salle et al. (2016), namely the WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001), MEN (Bruni et al., 2012), MTurk (Radinsky et al., 2011), RW (Luong et al., 2013), SimLex-999 (Hill et al., 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012) word similarity tasks1, and the Google semantic (GSem) and syntactic (GSyn) analogy (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) tasks. Word similarity tasks use cosine similarity. Word analogy tasks are solved using both 3CosAdd and 3CosMul (Levy et al., 2014)."}, {"heading": "5 Results", "text": "Positional contexts improved performance in both similarity (table 1) and analogy tasks (table 2). As hypothesized, their use significantly improved LexVec\u2019s performance on syntactic analogies, leading to the highest score on GSyn, surpassing GloVe and SGNS. This confirms the relevance of using positional contexts to capture syntactic dependencies.\nSalle et al. (2016) reported that combining word and context embeddings scored marginally higher in word similarity tasks, and that holds true in our experiments, even for W\u0303 pos. In the analogy tasks, using only the word embedding leads to far better syntactic performance, indicating that the embedding W strikes a better balance between syntax and semantics than does W + W\u0303 .\n1http://www.cs.cmu.edu/ mfaruqui/suite.html\nThe SI external memory implementation very closely approximates the standard variant (without the use of external memory), which was expected given that they minimize the exact same loss function. The gap between MI and standard was much wider. It seems that there is value in the way WSNS uses corpus ordering of word-context pairs to train the model. The SI variant more closely mimics this order, distributing same pair occurrences over the entire training. MI, on the other hand, has a completely artificial ordering, distant from corpus and SI\u2019s ordering."}, {"heading": "6 Conclusion and Future Work", "text": "This paper presented two improvements to the word embedding model LexVec. The first yields state-of-\nthe-art performance on syntactic analogies through the use of positional contexts. The second solves the need to store the PPMI matrix in main memory by using external memory. The SI variant of the external memory implementation was a good approximation of standard LexVec\u2019s WSNS, enabling future training using web-scale corpora.\nIn future work, we plan to explore the model\u2019s hyperparameter space, which could potentially boost model performance, having so far restricted ourselves to parameters recommended in Levy et al. (2015). Finally, following Tsvetkov et al. (2015), we will pursue evaluation of the model on downstream tasks in addition to the intrinsic evaluations used in this paper."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Associ-", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Asso-", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web. ACM,", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computa-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics pages 211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan."], "venue": "CoNLL-2014 page 171.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "CoNLL-2013 104.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "HLT-NAACL. pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["George A. Miller", "Walter G. Charles."], "venue": "Language and cognitive processes 6(1):1\u201328.", "citeRegEx": "Miller and Charles.,? 1991", "shortCiteRegEx": "Miller and Charles.", "year": 1991}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "Proceedings of the 20th international conference on World wide", "citeRegEx": "Radinsky et al\\.,? 2011", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough."], "venue": "Communications of the ACM 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Matrix factorization using window sampling and negative sampling for improved word representations", "author": ["Alexandre Salle", "Marco Idiart", "Aline Villavicencio."], "venue": "arXiv preprint arXiv:1606.00819 .", "citeRegEx": "Salle et al\\.,? 2016", "shortCiteRegEx": "Salle et al\\.", "year": 2016}, {"title": "Machine learning in automated text categorization", "author": ["Fabrizio Sebastiani."], "venue": "ACM computing surveys (CSUR) 34(1):1\u201347.", "citeRegEx": "Sebastiani.,? 2002", "shortCiteRegEx": "Sebastiani.", "year": 2002}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "ACL (1). pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics. Association", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "Distributed word representations have become a mainstay in natural language processing, enjoying a slew of applications (Sebastiani, 2002; Turian et al., 2010; Socher et al., 2013).", "startOffset": 120, "endOffset": 180}, {"referenceID": 18, "context": "Distributed word representations have become a mainstay in natural language processing, enjoying a slew of applications (Sebastiani, 2002; Turian et al., 2010; Socher et al., 2013).", "startOffset": 120, "endOffset": 180}, {"referenceID": 17, "context": "Distributed word representations have become a mainstay in natural language processing, enjoying a slew of applications (Sebastiani, 2002; Turian et al., 2010; Socher et al., 2013).", "startOffset": 120, "endOffset": 180}, {"referenceID": 6, "context": "(2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al., 2014; Salle et al., 2016).", "startOffset": 266, "endOffset": 305}, {"referenceID": 15, "context": "(2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al., 2014; Salle et al., 2016).", "startOffset": 266, "endOffset": 305}, {"referenceID": 0, "context": "Though Baroni et al. (2014) suggested that predictive models which use neural networks to generate the distributed word representations (also known as embeddings in this context) outperform counting models which work on co-occurrence matrices, recent work shows evidence to the contrary (Levy et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 15, "context": "In this paper, we focus on improving a state-ofthe-art counting model, LexVec (Salle et al., 2016), which performs factorization of the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling (WSNS).", "startOffset": 78, "endOffset": 98}, {"referenceID": 15, "context": "In this paper, we focus on improving a state-ofthe-art counting model, LexVec (Salle et al., 2016), which performs factorization of the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling (WSNS). Salle et al. (2016) suggest that LexVec matches and often outperforms competing models in word similarity and semantic analogy tasks.", "startOffset": 79, "endOffset": 260}, {"referenceID": 9, "context": "with \u03b1 = 3/4 (Mikolov et al., 2013b; Salle et al., 2016), and #(w) the unigram frequency of w.", "startOffset": 13, "endOffset": 56}, {"referenceID": 15, "context": "with \u03b1 = 3/4 (Mikolov et al., 2013b; Salle et al., 2016), and #(w) the unigram frequency of w.", "startOffset": 13, "endOffset": 56}, {"referenceID": 15, "context": "(2) and (3): Mini-batch and Stochastic (Salle et al., 2016).", "startOffset": 39, "endOffset": 59}, {"referenceID": 5, "context": "As suggested by Levy et al. (2015) and Salle et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 5, "context": "As suggested by Levy et al. (2015) and Salle et al. (2016), positional contexts (introduced in Levy et al.", "startOffset": 16, "endOffset": 59}, {"referenceID": 5, "context": "As suggested by Levy et al. (2015) and Salle et al. (2016), positional contexts (introduced in Levy et al. (2014)) are a potential solution to poor performance on syntactic analogy tasks.", "startOffset": 16, "endOffset": 114}, {"referenceID": 12, "context": "Pennington et al. (2014) show that the under certain assumptions, this sparse matrix grows as O(|C|), which bounds the maximum corpus size that can be processed by LexVec.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "We report results from Salle et al. (2016) and use the same training corpus and parameters to train LexVec with positional contexts and external memory.", "startOffset": 23, "endOffset": 43}, {"referenceID": 12, "context": "Both GloVe (Pennington et al., 2014) and Skip-gram with negative sampling (SGNS) (Mikolov et al.", "startOffset": 11, "endOffset": 36}, {"referenceID": 9, "context": ", 2014) and Skip-gram with negative sampling (SGNS) (Mikolov et al., 2013b) were trained using a symmetric window of size 10.", "startOffset": 52, "endOffset": 75}, {"referenceID": 9, "context": "LexVec, PPMI-SVD, and SGNS use dirty subsampling (Mikolov et al., 2013b; Levy et al., 2015) with threshold t = 10.", "startOffset": 49, "endOffset": 91}, {"referenceID": 5, "context": "LexVec, PPMI-SVD, and SGNS use dirty subsampling (Mikolov et al., 2013b; Levy et al., 2015) with threshold t = 10.", "startOffset": 49, "endOffset": 91}, {"referenceID": 5, "context": "As recommended in Levy et al. (2015) and used in Salle et al.", "startOffset": 18, "endOffset": 37}, {"referenceID": 5, "context": "As recommended in Levy et al. (2015) and used in Salle et al. (2016), the PPMI matrix used in all LexVec models and in PPMI-SVD is transformed using context distribution smoothing exponentiating context frequencies to the power 0.", "startOffset": 18, "endOffset": 69}, {"referenceID": 2, "context": "(2016), namely the WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001), MEN (Bruni et al.", "startOffset": 67, "endOffset": 93}, {"referenceID": 1, "context": ", 2001), MEN (Bruni et al., 2012), MTurk (Radinsky et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 13, "context": ", 2012), MTurk (Radinsky et al., 2011), RW (Luong et al.", "startOffset": 15, "endOffset": 38}, {"referenceID": 7, "context": ", 2011), RW (Luong et al., 2013), SimLex-999 (Hill et al.", "startOffset": 12, "endOffset": 32}, {"referenceID": 3, "context": ", 2013), SimLex-999 (Hill et al., 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 11, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 12, "endOffset": 38}, {"referenceID": 14, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 43, "endOffset": 76}, {"referenceID": 4, "context": ", 2015), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012) word similarity tasks1, and the Google semantic (GSem) and syntactic (GSyn) analogy (Mikolov et al.", "startOffset": 87, "endOffset": 107}, {"referenceID": 8, "context": ", 2012) word similarity tasks1, and the Google semantic (GSem) and syntactic (GSyn) analogy (Mikolov et al., 2013a) and MSR syntactic analogy dataset (Mikolov et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 10, "context": ", 2013a) and MSR syntactic analogy dataset (Mikolov et al., 2013c) tasks.", "startOffset": 43, "endOffset": 66}, {"referenceID": 6, "context": "Word analogy tasks are solved using both 3CosAdd and 3CosMul (Levy et al., 2014).", "startOffset": 61, "endOffset": 80}, {"referenceID": 2, "context": "Therefore, we perform the exact same evaluation as Salle et al. (2016), namely the WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 5, "context": "In future work, we plan to explore the model\u2019s hyperparameter space, which could potentially boost model performance, having so far restricted ourselves to parameters recommended in Levy et al. (2015). Finally, following Tsvetkov et al.", "startOffset": 182, "endOffset": 201}, {"referenceID": 5, "context": "In future work, we plan to explore the model\u2019s hyperparameter space, which could potentially boost model performance, having so far restricted ourselves to parameters recommended in Levy et al. (2015). Finally, following Tsvetkov et al. (2015), we will pursue evaluation of the model on downstream tasks in addition to the intrinsic evaluations used in this paper.", "startOffset": 182, "endOffset": 244}], "year": 2016, "abstractText": "In this paper we take a state-of-the-art model for distributed word representation that explicitly factorizes the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling and address two of its shortcomings. We improve syntactic performance by using positional contexts, and solve the need to store the PPMI matrix in memory by working on aggregate data in external memory. The effectiveness of both modifications is shown using word similarity and analogy tasks.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}