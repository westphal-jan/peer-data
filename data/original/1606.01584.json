{"id": "1606.01584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Curie: A method for protecting SVM Classifier from Poisoning Attack", "abstract": "Machine learning is used in a number of security related applications such as biometric user authentication, speaker identification etc. A type of causative integrity attack against machine learning called Poisoning attack works by injecting specially crafted data points in the training data so as to increase the false positive rate of the classifier. In the context of the biometric authentication, this means that more intruders will be classified as valid user, and in case of speaker identification system, user A will be classified user B. In this paper, we examine poisoning attack against SVM and introduce - Curie - a method to protect the SVM classifier from the poisoning attack. The basic idea of our method is to identify the poisoned data points injected by the adversary and filter them out. Our method is light weight and can be easily integrated into existing systems. Experimental results shows that it works very well in filtering out the poisoned data.", "histories": [["v1", "Sun, 5 Jun 2016 23:42:56 GMT  (416kb,D)", "https://arxiv.org/abs/1606.01584v1", "19 pages, 10 figures"], ["v2", "Tue, 7 Jun 2016 00:41:08 GMT  (416kb,D)", "http://arxiv.org/abs/1606.01584v2", "19 pages, 10 figures"]], "COMMENTS": "19 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["ricky laishram", "vir virander phoha"], "accepted": false, "id": "1606.01584"}, "pdf": {"name": "1606.01584.pdf", "metadata": {"source": "CRF", "title": "Curie: A method for protecting SVM Classifier from Poisoning Attack", "authors": ["Ricky Laishram", "Vir Virander Phoha", "R. Laishram", "V. V. Phoha"], "emails": ["rlaishra@syr.edu", "vvphoha@syr.edu"], "sections": [{"heading": null, "text": "Keywords: Machine Learning, Security, Poisoning Attack, SVM"}, {"heading": "1 Introduction", "text": "Nowadays machine learning is used in a number of diverse applications. In the security domain, the use of machine learning techniques has become very important due to the emergence of big data. Machine learning techniques are widely used in areas such as biometric authentication [14], speaker identification [12], malware detection in mobile platforms [17] etc.\nIn the security domain, the learning components are not static - they are continuously trained with new incoming data. This allows the classifier to adapt to changes in the system. For example, consider a biometric authentication system that uses machine learning to classify users as invalid or valid. The behavior of both the invalid and valid users will change over time. So the machine learning components need to be continuously updated to keep up with the new valid users. However, from a security perspective this opens up a new attack surface.\nIn such applications, the environment that the system operates in is not completely under the control of the learner. There might be adversary1 that actively\n1 In this paper, the terms attacker and adversary are used interchangeably, and they refer to the same thing.\nar X\niv :1\n60 6.\n01 58\n4v 2\n[ cs\n.C R\n] 7\nJ un\n2 01\n6\ntries to manipulate the classifier [10]. This creates a possible vulnerability in the security system because most of the machine learning algorithms were not originally developed to operate in such adversarial environments.\nMost machine learning algorithms assume that the training and test data have the same distribution [10,7]. In the presence of an adversary, this assumption is no longer valid. The adversary can manipulate some of the training data to achieve some malicious goal - for example, to increase the false positive rate. In the case of the authentication system, this means that more invalid users will be classified as valid users.\nBarreno et al. [3] explored the poisoning attacks and broadly categorized them through two aspects. In terms of the influence that an attacker has over the training data, they classified the attack as causative and exploratory. In causative attacks, the attacker has knowledge of the classifier and some influence over the training data. In explorative attacks, the attacker can only probe the learner for information. In an online learning environment, the learner is susceptible to causative attacks.\nIn terms of the type of security violation, Barreno et al. classified the attacks as integrity attack and availability attack. In an integrity attack, the goal of the attacker is to increase the false positive rate. For example in an authentication system, an integrity attack would be an attempt to increase the rate of invalid users being classified as valid user.\nOver the years, various attack strategies have been developed against different learning algorithms. In this paper, we examine causative integrity attacks against a Support Vector Machine learner proposed by Biggio et al. [6] and Xiao et al. [18]. This is an important research topic because machine learning is used in a variety of environments where an adversary might be present, and SVM is one of the most widely used machine learning algorithms.\nThe main contribution of this paper is a method, which we refer to as Curie2, to protect an SVM classifier against poisoned data injected by an adversary. Curie works by identifying the poisoned data points and filtering them out. To do this it relies on clustering the data in feature space, and then detecting the distances of the data points from other points in the same cluster in the (feature + label) space (Section 3.2). Experimental results (Section 5) show that our proposed method can successfully identify most of the poisoned data. The method we propose is very light weight and can be easily integrated into any existing system.\nIn the next section, we explore some background and related works. We provide a brief description of poisoning attacks against machine learning in Section 2.1. In Section 2.2, we examine the poisoning attack against SVM proposed by Baggio et al. [6] and Xiao et al. [18]. In the next section (Section 2.3), we examine some proposed solutions and their shortcomings. In Section 3, we describe the threat model and our proposed solution. We describe the experimental setup and\n2 We called our method Curie from the word cure, since the method is supposed to \u201dcure\u201d the classifier from the poisone data. It is also inspired by the character Curie in the game Fallout 4.\nthe dataset used in Section 4. Section 5 presents the results of the experiments and some analysis. Finally, we present some concluding remarks and potential future works in Section 6."}, {"heading": "2 Background and Related Works", "text": "In this section, we will discuss some background related to poisoning attacks against machine learning algorithms. We will also discuss the poisoning attack againsts SVM proposed by Baggio et al. [6] and Xiao [18], and some proposed solutions to protect against such attacks."}, {"heading": "2.1 Poisoning Attack", "text": "In many applications of machine learning in the security domain, the data is nonstationary, that is, the distribution of the data shifts over time. For example, in case, of a biometric authentication system, the features, such as walking pattern etc., for a valid user might change over time for a variety of reasons.\nThere are two ways to handle this non-stationary distribution - incremental algorithm or periodic retraining (Figure 1). In the first case, the model is updated incrementally as new data comes in, and in the second case, the data is buffered and the model is retrained periodically using the buffered data.\nIn the model with periodic retraining (Figure 1b), the inputs are stored in a buffer. The model is retrained after a fixed interval of time has passed or if the classifier performance falls below a pre-defined threshold. In our paper, this is the type of model that we are considering.\nRetraining the system opens up an attack surface [3,2]. The data for retraining is collected from an adversarial environment. This opens up the possibility that an attacker could inject specially crafted data in the new training data\nwith the goal of increasing the false positive rate. In the case of authentication system, this would mean that the retrained system will not be able to detect some invalid users.\nIn the next sub-section, we will discuss some proposed poison attack mechanisms against SVM classifier."}, {"heading": "2.2 Poisoning Attack against SVM", "text": "In SVM, the fundamental idea is to find a decision surface in the feature space that separates the training data based on the class labels [9]. To perform a poisoning attack against SVM, the attacker inserts some data in the training data with the labels flipped [6,18].\nIn all the attack scenario described, the ability of the attacker is over-estimated. It is assumed that the attacker has complete knowledge of the classification system, and has the capability to insert some data into the retraining data.\nThe naive way to perform the attack is to randomly flip the labels of the data to be used for retraining. This is equivalent to adding noise to the training data [18]. Another way is to select the points that are near the support vectors and flip their labels. A third way is to select points that are farthest from the support vectors and flip the labels.\nAnother method [5] is to select a combination of points that maximizes the classification error (or false positive rate). The problem with this approach, however, is that selection of the optimal set of such points is unfeasible for many real world data due to data size.\nMore intelligent methods of selecting the data points to flip have been proposed. These methods [18,6] works by selecting points that maximizes the loss function of the SVM classifier. In the work by Xiao et al. [18], a set of points is selected based on the loss maximization framework and their labels are flipped. In the work by Baggio et al.[6], some data points are selected and they are moved to other points in the feature space such that the loss is maximized. According to the results presented, both methods work very well in deteriorating the classification accuracy. In this paper, this is the type of attack we are addressing.\nIn the next sub-section, we will describe some proposed solutions to protect a classsifier from poisoning attack."}, {"heading": "2.3 Previous Methods to protect against Poisoning Attack", "text": "There are a few methods proposed for protection against poisoning attack. The first approach is to develop secure classifiers that are resistant to the attack. Most of the works [8,7] on secure classifier algorithms uses a game theory model - the problem is modeled as a game between the adversary and the learner. The problem with this approach is that it is difficult to integrate with existing systems. Since our goal is to protect a system that uses SVM, this method is not of interest.\nAnother method is to use multiple classifiers [4]. In the approach proposed, the poisoned data are treated as outliers and an ensemble of classifiers is used.\nHowever, a shortcoming of this method that they use from 3 to 50 base classifiers. The use of this many multiple classifiers is very resource intensive for learner. In addition, in the experiments performed in that work, they did not use the more intelligent methods [6,18] of generating the poison data.\nA third approach is to hide information about the classifier from the adversary. However, as mentioned in Section 2.2, we are considering the worst case scenario where he attacker has complete knowledge of the classifier. So, we will not consider this approach.\nIn this paper, we consider the poisoning attack with gradient ascent proposed by Biggio and Xiao [6,18]. As far as we know, there has been no work done the SVM classifier against such attacks. In the next section, we will describe the threat model and our methodology to protect SVM against such attack."}, {"heading": "3 Methodology", "text": "In Section 2.2, we described various poisoning attack strategies. In this paper, we are addressing the type of attack [18,6] in which the attack points are selected to maximize the loss function. We propose a method to protect an SVM classifier from this type of poison attack. Our goal is to develop a method that can identify the attack points added by the adversary, and filter them out before the data is used for retraining.\nIn this section we will discuss the threat model first. Then, we will describe our proposed method in detail."}, {"heading": "3.1 Threat Model", "text": "In this paper, the type of attack we are taking into consideration is a causative integrity attack - the adversary has complete knowledge of the system and is trying to increase the false positive rate of the system. In the real world, the adversary is not likely to possess complete knowledge about the classifier. So, the adversary would perform exploratory attacks first. In our case, we make the assumption that the adversary has complete knowledge of the classifier for simplicity 3.\nAssume that there is a classifier that uses SVM. For simplicity, assume that we are dealing with a two class classification - positive and negative classes. In the attack scenario considered in this paper, the attacker has the capability to inject some data into the training data. In the experiments presented is Section 4, we consider different cases according to the amount of data that the attacker has added.\nThe attacker injects as few specially crafted data points as possible so that the false positive rate is increased. So, the objective of the attack is to increase the number of negative class misclassified as positive, without significantly changing\n3 By complete knowledge we mean that the adversary knows everything about the classifier - from the training data to the hyper-parameters used.\nthe number of positive class misclassified as negative. The attack points are generated using the method proposed by Biggio et al. [6]. The attack method proposed by Xioa et al. [18] is very similar. So, we consider only one attack method in this paper."}, {"heading": "3.2 Curie - Poison Points Filter", "text": "The core idea of our proposed method, Curie, lies in the identification of the data points that the attacker has added. We will refer to this data as poison points. Once these poison points are identified, they can be filtered out before the data is used for retraining the classifier. Figure 2 shows how our method fits in with the existing periodic retraining model.\nIn the model with periodic retraining (Figure 1b) described in Section 2.1, the attacker injects the poison points in the buffer. So, in our method, we add a filter between the buffer and the retrain module as shown in Figure 2.\nAs mentioned in Section 2.2, the attack works by selecting a set of points in the feature space that maximizes the loss function, and then flipping their labels. An additional constrain for the attacker is that these points should be well hidden within the rest of the valid points.\nFigure 3a shows the clusters in the feature space after the poison points have been injected. It can be observed that there are some outliers but experiments (Section 4) shows that these are not the poison points.\nHowever, we can make use of the label flipping to distinguish the poison points from the normal points. Because of the fact that labels of the poison points have been flipped, the poison points will be very similar to the normal points in the feature space.\nIn our approach, we extend the feature space to an additional dimension which maps the class labebls to some values. We will refer to this additional dimension as the label dimnension and the new feature space will be refered to as (feature + label) space.\nIn the (feature + label) space, the poison points will be well seperated from the rest of the cluster in feature space if the mapping of the label dimension is done properly as shown in 3b. (We will talk about how to do the mapping in the later part of this section.)\nFrom these observations, we develop the attack points identification method as described in Algorithm 1.\nThe first step in our method is to cluster the data in the feature space. In our algorithm, the clustering algorithm used is DBSCAN [11]. In our algorithm, DBSCAN is used because it can find clusters of arbitrary shape and we do not need to specify the number of clusters.\nAs mentioned earlier, we are dealing with a two class classification. Assume that the two classes are {0, 1}. It does not matter which class is 0 or 1. Let class(x) be the class label of x. The mapping in the label dimension is done such that of class(x) is mapped to \u03c9 \u00b7 class(x).\nThen a point x in the (feature + label) space is given by,\nF (x) = {x1, x2, . . . , \u03c9 \u00b7 class(x)}\nLet us assume that the cluster to which x belongs to is given by cluster(x). Let S represent the entire set of data points. Let S\u2032(x) represent the set of points that belongs to the same cluster as x.\nS\u2032(x) = {y|y \u2208 S \u2227 Cluster(x) = Cluster(y)}\nCompute the average squared Euclidean distance for each point in S\u2032(x) from x. Let us call this the distance score for x and denote it by d(x).\nd(x) = 1 |S\u2032(x)| \u00b7 \u2211\ny\u2208S\u2032(x)\ndist(x, y)\nwhere, dist(x, y) is the squared Euclidean distance between x and y. Let D be the set of distance score for all points in the dataset.\nD = {d(y)|y \u2208 S}\nAssume \u00b5 and \u03c3 are the mean and standard deviation of D. Let Z(x) represent the z-score of a data point x.\nZ(x) = x\u2212 \u00b5 \u03c3\nTransform every point in D to its z-score. Let D denote this new set.\nD = {Z(x)|\u2200x \u2208 D}\nFilter out the points with confidence less than conf(\u03b8),\nS\u2217 = {y|y \u2208 S \u2227D(y) \u2264 \u03b8}\nThe set S\u2217 is the training data from which the poisoned data has been removed, and it is the output of Curie which will be used for future retaining of the SVM classifier.\nIn the algorithm described, there are two hyperparameters - \u03c9 and \u03b8. In Appendix A we show that,\n\u03c92 \u2265 \u03b8 2 \u2212 1\n(1\u2212 \u03c1) \u00b7 icdmax\nwhere icdmax is the maximum intra-cluster distance and \u03c1 is the expected probability that a data point will be a poison point.\nIn the next subsection, we will describe an extension of Curie to take in account multi-class classification."}, {"heading": "3.3 Curie in a Multi-Class Classification System", "text": "In the previous sections, we considered only two-class classification. This is valid for authentication systems as they only need to classify users as valid or invalid. However, there are security systems in which the classifier has to handle multiple classes. In this section, we present an extension to Curie for multi-class classification.\nConsider a multi-class classifier with NC number of classes. From the perspective of the attacker, this is not different from the two-class classification.\nAlgorithm 1: Curie: Algorithm to filter the poison points\nData: Data = (F,C) such that F is a set of feature vectors and C is a set of class labels Result: M vectors after removal of attack points /* Reduce the number of dimensions */ PcaData\u2190 PCA(Data.F ) ; /* Cluster the data thorough DBSCAN */ Clusters\u2190 DBSCAN(PcaData); foreach point \u2208 Data do\n/* Append the weighted class as feature */ point.F \u2190 Append(point.F, point.C \u00d7 \u03c9); /* Calculate the average distance to 10 randomly selected points\nin the same cluster */\ncls\u2190 GetCluster(point, Clusters); sample\u2190 Sample(cls, count); foreach s \u2208 sample do\ns.F \u2190 Append(s.F, s.C \u00d7 weight); d\u2190 EucledianDistance(point.F, s.F ) ; Dist.point\u2190 Dist.point+ d ;\nend Dist.point\u2190 Dist.point/Size(cls);\nend /* Perform Z-score standarization and select only points with more\nthan conf(\u03b8) confidence */ Dist\u2190 ZScore(Dist); foreach point \u2208 Data do\nif Dist.point \u2264 \u03b8 then Result\u2190 Append(Result, point); end\nend\nIf the class of interest to the attacker is CA, the attack can be performed by considering the classifier as binary classifier with classes - CA and the rest.\nSince Curie is an unsupervised algorithm, it can be easily extended to apply to such cases of multi-class classification. The defender does not know the class that the attacker is targeting. So, we consider every possible one-vs-rest pairs. This means that instead of adding one label dimension in the (feature + label) space like in Section 3.2, (NC \u2212 1) additional dimensions are added in this case. The rest of the algorithm remains the same. To differentiate this from the twoclass version, we will refer to this as MultiClass-Curie. As shown in Appendix B, the equation for \u03c9 given by 13 will apply for MultiClass-Curie as well.\nIn the next section, we will describe the dataset used for experiments, and experimental setups."}, {"heading": "4 Datasets and Experiments", "text": "In this section, we will first describe the datasets that we are using for our experiments. Then we will describe the experimental setups for the three different experiments."}, {"heading": "4.1 Datasets", "text": "To verify our method, we perform experiments using the MNIST dataset [15]. The MNIST dataset consist of 10 classes and aproximately 60000 images of dimensions 28 \u00d7 28. We convert each of the images into a vector of length 784 where each value represents a pixel in the image. Each item of the vector has a value in the range [0, 255].\nAs described in Section 4.2, we need two datasets - one with 2 classes and one with 3 classes.\nTo create the 2 class dataset, we randomly sample 1250 points without replacement from classes 0 and 1 in the original dataset. For the 3 class dataset, we randomly sample 1500 data points without replacement from classes 0, 1 and 2. So the first dataset is of size 2500 and the second is 4500.\nThen we generate poison points for each of these two datasets. We use the method proposed by Biggio et al. [6] using the AdversariaLib library. For each of the datasets, we generate 25, 50, 75, 100 and 125 poison points to create 5 additional instances of each dataset."}, {"heading": "4.2 Experimental Setup", "text": "For our experiments, we use a linear SVM classifier, with penalty (C) of value 1. We do not implement the entire system shown in Figure 2 since the purpose of Curie is only to find the poison points.\nWe have three separate experimental setups. The first experiment is to determine the effectiveness of Curie in removing the poison points compared to an outlier detection algorithm. The second experiment is to explore the effect of changes in the performance of Curie due to change in the hyperparameters. The third experiment is to determine the effectiveness of MultiClass-Curie.\nAs mentioned in Section 2.3, we are not aware of any method to protect SVM classifiers against the attacks proposed in [6,18]. So, we evaluate the performance of Curie by comparing it to the case without Curie.\nExperiment 1 As mentioned in the Section 4.1, we have five instances of the 2 class MNIST dataset based on the amount of poison data injected, and one instance without any poison points.\nIn this experiment, the filtered data from Curie is used to an train an SVM classifier and we compare the changes compared to the case when the training data is used for training directly. We perform 10-fold cross validation and user the accuracy and false positive rate for comparison.\nExperiment 2 In the second experiment, we fixed the threshold (\u03b8) at 1.645 and change the values of the weight (\u03c9) to observe the effect on the performance of Curie.\nWe use the two class dataset described in Section 4.1 and inject 2% poison points. For this part of this experiment, we vary \u03c9 from 1 to 104. We perform 10-fold cross validation and calculate the classifier accuracy and false positive rate for each value of \u03c9.\nExperiment 3 In this experiment, we use the three class data described in Section 4.1. Since SVM does not natively support multi-class classification, we use the one-vs-rest approach. We train the classifier on the different instance of this dataset, and calculate the accuracy 10-fold cross validation.\nThe training data is passed through MultiClass-Curie and the classification is performed again. We then compare the change in the classifier performance. We are not aware of any other works on preventing poisoning attacks in multi-class SVM classifier. So, we evaluate the performance of our algorithm by the amount of improvement over the case with no protection.\nIn the next section, we will present the results of the experiments described here and some analysis."}, {"heading": "5 Results and Analysis", "text": "In the previous section we described the MNIST dataset and the experimental setup. In this section, we present the results of the experiments described is Section 4.2."}, {"heading": "5.1 Results of Experiment 1", "text": "The performance comparison of the SVM classifier with and without Curie is given in Table 1. Figure 4a and 4b presents a plot of the accuracy and false positive rate for various amount of poison points injected.\nIn the plots in Figure 4, the green line represents our method - Curie. In both Figure 4a and 4b, it can be observed that with Curie the accuracy and false positive rate of the classifier remains almost constant even when the amount of poison points are increased. This indicates that it is filtering out the poison points before they are used to train the SVM classifier. These results shows that Curie is very effective in identifying and removing poison points regardless of the amount of poison points injected."}, {"heading": "5.2 Results of Experiment 2", "text": "In this section, we present the results of Experiment 2. For this experiment we used only the instance of the 2 class dataset with 50 poison points.\nIn Figure 5a, the accuracy of the classifier for various values of \u03c9 is shown and Figure 5b shows the false positive rate of the classifier. The vertical green dashed line represents the theoretical value of \u03c9 given by 13 with \u03c1 = 0.02.\nThese results shows that Curie is not very sensitive to \u03c9, and in turn \u03c1 (Equation 13). In a real world dataset, the exact value of \u03c1 in unknown. So, the results shows that Curie can work in such cases."}, {"heading": "5.3 Results of Experiment 3", "text": "In this subsection, we present the results of the third experiment. The accuracy comparison of the classifier with and without MultiClass-Curie for different amount of poison points is given in Table 3.\nFigure 6 shows the comparison of the classifier accuracy for different amount of poison points with and without MultiClass-Curie. It can be observed that\non its own the accuracy of the classifier decreases as the number of the poison points increases. However, with MultiClass-Curie, the accuracy decreases very slowly with the number of poison points. These results shows that in the case of multi-class classifier, MultiClass-Curie can still be used to protect the SVM classifier from poisoning attacks."}, {"heading": "6 Conclusion", "text": "The use of machine learning for security applications is becoming very widespread. So, an investigation of the behavior of machine learning algorithms in an adversarial environment is important. Poisoning attacks present a very real threat to security systems (Section 2) that employ machine learning techniques. Researches [6,18] have shown that SVM, a very widely used machine learning algorithm, is very susceptible to poisoning attacks.\nIn this paper, we examined the poisoning attack against SVM, and propose a method, which we refer to as Curie, to protect a system that uses SVM classifier from such attacks. Curie works as a filter before the buffered data are used to retrain the SVM classifier.\nCurie works by exploiting the fact that the poison data looks like a normal point in the feature space, but has their labels flipped. The data are clustered in the feature space, and the average distance of each point from the other points in the same cluster is calculated with the class label considered as a feature with proper weight. The data points with confidence less than 95% confidence are removed from the training data.\nWe tested our algorithm on the MNSIT dataset (Section 4.1), and evaluated with the case in which the training data is used directly for retraining. The results obtained (Section 5) show that when Curie is used, the accuracy and false\npositive rate of the classifier does not change significantly when the amount of poison points in the training data is changed. This indicates that Curie successfully filtered out most of the poison data. We also showed that Curie is not very sensitive to \u03c9. So the estimation of \u03c1 for an unknown dataset need not be very accurate. We also demonstrated experimentally that we can extend Curie to make it work in a multi-class classification system.\nIn addition to the performance benifits, Curie can be easily integrated into any system that uses periodic retraining (Figure 2).\nSince Curie is an unsupervised method, an attacker would have to use evasion attacks to pass through Curie. This means that the attacker would have to inject data that works both for evasion attack and poison attack. (The injected data point has to pass thorough Curie undetected, and then poison the classifier.) We are not aware of any works on attacks that does both both evasion and poisoning simultaneously. An interesting research area would be to explore if this is possible.\nAlthough we tested Curie with only SVM classifier, it should be possible to extend the work to protect other classifiers, such as perceptron, against poisoning attack. Like in the case of SVM, perceptron also works by creating a decision boundary. So the poisoning attack against perceptron would also work through some form of label flipping. As we demonstrated in Section 3.2, label flipping creates poison points that are similar to normal points in feature space but different in the (feature + label) space. This is what Curie exploits to filter out the poison points."}, {"heading": "A Curie Hyperparameters", "text": "In Section 3.2, we described the algorithm for Curie. In the algorithm described, we use two hyperpareters - \u03b8 and \u03c9. In this section, we will describe these hyperparemets and the relation between them.\nIn Curie, the hyperparameter \u03c9 is the weight used for mapping the the class label to the additional dimension in the (feature + label) space. The parameter \u03b8 defines the threshold above which the a data point in the training data is considered to be a poison point.\nAssume that the positive class is mapped to x+ and the negative class to x\u2212 in the additional dimension introduced in the (feature + label) space. We define \u03c9 in our algorithm (Section 3.2) such that,\n\u03c9 \u2265 |x+ \u2212 x\u2212| (1)\nConsider a cluster C of size n in the feature space. Let Sc be the set of data points in cluster C. Suppose that there are l dimensions in the data originally.\nAs mentioned in Section 3.2, the distance scores are computed only for pairs of points that belong to the same cluster. Then, in if we consider the cluster C in the (feature + label) space, for xc+, x c \u2212 \u2208 Sc, let \u03c9c be,\n\u03c9c \u2265 |xc+ \u2212 xc\u2212| (2)\nSince Equation 1 is describes \u03c9 over the entire dataset,\n\u03c9 \u2265 \u03c9c (3)\nConsider a poison point p in cluster C. The average distance4 of p from the rest of the points in cluster C in the feature space is given by,\ndistf (p) = 1 n\u2212 1 \u2211\nx\u2208Sc\u2212{p}\n( l\u2211\ni=1\n(xi \u2212 pi)2 )\n(4)\nConsider a data point d in the cluster C. Assume dm is the value of the additional dimention of d in (feature + label) space. If p is a poison point in cluster C, the label of p was flipped. So,\ndm = d+ =\u21d2 pm = p\u2212\ndm = d\u2212 =\u21d2 pm = p+\nIn the (feature + label) space, the average distance of p from the rest of the points in C is given by,\ndistf+l(p) = 1 n\u2212 1 \u2211\nx\u2208Sc\u2212{p}\n( l+1\u2211 i=1 (xi \u2212 pi)2 )\n= 1 n\u2212 1 \u2211\nx\u2208Sc\u2212{p}\n( (xm \u2212 pm)2 +\nl\u2211 i=1\n(xi \u2212 pi)2 ) (5)\nFor the algorithm to be able to distinguish p in the (feature + label) space from non-poison data, the average distance of the poison points in the (feature + label) space should be significantly greater than that in the feature space. That is,\n\u03b8 \u00b7 distf (p) \u2264 distf+l(p) (6)\nSubstituting Equations 4 and 5 in Equation 6, we get,\n4 The distance metric used is the squared euclidean distance.\n\u03b8 n\u2212 1 \u2211\nx\u2208Sc\u2212{p}\n( l\u2211\ni=1\n(xi \u2212 pi)2 ) \u2264 1 n\u2212 1 \u2211 x\u2208Sc\u2212{p} ( (xm \u2212 pm)2 + l\u2211 i=1 (xi \u2212 pi)2 )\n\u03b8 \u2211\nx\u2208Sc\u2212{p}\nl\u2211 i=1 (xi \u2212 pi)2 \u2264 \u2211\nx\u2208Sc\u2212{p}\n( (xm \u2212 pm)2 +\nl\u2211 i=1\n(xi \u2212 pi)2 )\n(\u03b8 \u2212 1) \u2211\nx\u2208Sc\u2212{p}\nl\u2211 i=1 (xi \u2212 pi)2 \u2264 \u2211\nx\u2208Sc\u2212{p}\n(xm \u2212 pm)2\n(7) Consider the sum on the right hand side of Equation 7. For an x \u2208 S \u2212 {p}, if x were also a poison point, xm = pm. Then, in this case, (xm \u2212 pm)2 = 0. In case x is not a poison point,\n(xm \u2212 pm)2 \u2264 \u03c92c Assume that there is a probability \u03c1 of a point being a poison point. The\nnumber of normal points in Sc \u2212 {p} is (1\u2212 \u03c1)(n\u2212 1). Then,\n(\u03b8 \u2212 1) \u2211\nx\u2208Sc\u2212p l\u2211 i=1 (xi \u2212 pi)2 \u2264 (1\u2212 \u03c1)(n\u2212 1)\u03c92c (8)\n\u03c92c \u2265 \u03b82 \u2212 1 (1\u2212 \u03c1)(n\u2212 1) \u2211\nx\u2208Sc\u2212p l\u2211 i=1 (xi \u2212 pi)2 (9)\nIn the feature space, the poison point indistinguishable a normal point. So, the average intra-cluster distance of cluster C can be approximated by,\nicd(C) \u2248 1 (n\u2212 1) \u2211 x\u2208Sc\u2212p l\u2211 i=1 (xi \u2212 pi)2 (10)\nThen Equation 9 can be written as,\n\u03c92c \u2265 \u03b8 \u2212 1\n(1\u2212 \u03c1) \u00b7 icd(C) (11)\nIf there are m clusters, let us use C to represent the set of all clusters. Then from Equation 3,\n\u03c9 \u2265 max({\u03c9i|i \u2208 C}) (12)\nThe value \u03b8 and \u03c1 is defined over the entire dataset. Let us use icdmax to denote the maximum value of intra cluster distance over all the clusters in C.\nicdmax = max({icd(i)|i \u2208 C})\nThen the overall value of \u03c9 for the entire dataset is,\n\u03c92 \u2265 \u03b8 \u2212 1 (1\u2212 \u03c1) \u00b7 icdmax (13)"}, {"heading": "B MultiClass-Curie Hyperparameters", "text": "In this Section, we will show that the equation for \u03c9 given by Equation 13 is valid for MultiClass-Curie as well.\nAs mentioned in Section 3.3, in MultiClass-Curie there are (NC\u22121) additional dimensions in the (feature + label) space. So, Equation 5 will be rewritten as,\ndistf+l(p) = 1 n\u2212 1 \u2211\nx\u2208Sc\u2212p\n( l+NC\u22121\u2211\ni=1\n(xi \u2212 pi)2 )\n= 1 n\u2212 1 \u2211\nx\u2208Sc\u2212p  l\u2211 i=1 (xi \u2212 pi)2 + NC\u22121\u2211 j=0 (xj \u2212 pj)2  (14)\nAs mentioned in Section 3.3, when the attacker construct the attack, the classifier is considered as a two classifier - one class representing the class of interest CA and the other class representing the rest of the classes. So, the difference xj \u2212 pj will be 0 in all cases except the one that corresponds to CA vs the rest. That is,\nNC\u22121\u2211 j=0 (xj \u2212 pj)2 = (xm \u2212 pm)2\nSo, Equation 14 can be written as,\ndistf+l(p) = 1 n\u2212 1 \u2211\nx\u2208Sc\u2212p\n( (xm \u2212 pm)2 +\nl\u2211 i=1\n(xi \u2212 pi)2 )\n(15)\nEquation 15 is identical to 5 and the number of classes does not come up anywhere else in the proof in Appendix A. So, the rest of the proof is identical and Equation 13 will give the value of \u03c9 for MultiClass-Curie as well."}], "references": [{"title": "New outlier detection method based on fuzzy clustering", "author": ["M. Al-Zoubi", "A. Al-Dahoud", "A.A. Yahya"], "venue": "WSEAS transactions on information science and applications 7(5), 681\u2013690", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J. Tygar"], "venue": "Machine Learning 81(2), 121\u2013148", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Can machine learning be secure? In: Proceedings of the 2006 ACM Symposium on Information, computer and communications security", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "pp. 16\u201325. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Bagging classifiers for fighting poisoning attacks in adversarial classification tasks", "author": ["B. Biggio", "I. Corona", "G. Fumera", "G. Giacinto", "F. Roli"], "venue": "Multiple Classifier Systems, pp. 350\u2013359. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Support vector machines under adversarial label noise", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "ACML 20, 97\u2013112", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "arXiv preprint arXiv:1206.6389", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "The Journal of Machine Learning Research 13(1), 2617\u20132654", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["M. Br\u00fcckner", "T. Scheffer"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 547\u2013555. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning 20(3), 273\u2013297", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "S. Sanghai", "D Verma"], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 99\u2013108. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H.P. Kriegel", "J. Sander", "X. Xu"], "venue": "Kdd. vol. 96, pp. 226\u2013231", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "The nist 2014 speaker recognition i-vector machine learning challenge", "author": ["C.S. Greenberg", "D. Bans\u00e9", "G.R. Doddington", "D. Garcia-Romero", "J.J. Godfrey", "T. Kinnunen", "A.F. Martin", "A. McCree", "M. Przybocki", "D.A. Reynolds"], "venue": "Odyssey: The Speaker and Language Recognition Workshop", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Discovering cluster-based local outliers", "author": ["Z. He", "X. Xu", "S. Deng"], "venue": "Pattern Recognition Letters 24(9), 1641\u20131650", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Authenticating users through their arm movement patterns", "author": ["R. Kumar", "V.V. Phoha", "R. Raina"], "venue": "arXiv preprint arXiv:1603.02211", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "X-means: Extending k-means with efficient estimation of the number of clusters", "author": ["D. Pelleg", "Moore", "A.W"], "venue": "ICML. vol. 1", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Experimental study with real-world data for android app security analysis using machine learning", "author": ["S. Roy", "J. DeLoach", "Y. Li", "N. Herndon", "D. Caragea", "X. Ou", "V.P. Ranganath", "H. Li", "N. Guevara"], "venue": "Proceedings of the 31st Annual Computer Security Applications Conference. pp. 81\u201390. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial label flips attack on support vector machines", "author": ["H. Xiao", "H. Xiao", "C. Eckert"], "venue": "ECAI. pp. 870\u2013875", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Machine learning techniques are widely used in areas such as biometric authentication [14], speaker identification [12], malware detection in mobile platforms [17] etc.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Machine learning techniques are widely used in areas such as biometric authentication [14], speaker identification [12], malware detection in mobile platforms [17] etc.", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Machine learning techniques are widely used in areas such as biometric authentication [14], speaker identification [12], malware detection in mobile platforms [17] etc.", "startOffset": 159, "endOffset": 163}, {"referenceID": 9, "context": "tries to manipulate the classifier [10].", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Most machine learning algorithms assume that the training and test data have the same distribution [10,7].", "startOffset": 99, "endOffset": 105}, {"referenceID": 6, "context": "Most machine learning algorithms assume that the training and test data have the same distribution [10,7].", "startOffset": 99, "endOffset": 105}, {"referenceID": 2, "context": "[3] explored the poisoning attacks and broadly categorized them through two aspects.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] and Xiao et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] and Xiao et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] and Xiao [18], and some proposed solutions to protect against such attacks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[6] and Xiao [18], and some proposed solutions to protect against such attacks.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "Retraining the system opens up an attack surface [3,2].", "startOffset": 49, "endOffset": 54}, {"referenceID": 1, "context": "Retraining the system opens up an attack surface [3,2].", "startOffset": 49, "endOffset": 54}, {"referenceID": 8, "context": "2 Poisoning Attack against SVM In SVM, the fundamental idea is to find a decision surface in the feature space that separates the training data based on the class labels [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 5, "context": "To perform a poisoning attack against SVM, the attacker inserts some data in the training data with the labels flipped [6,18].", "startOffset": 119, "endOffset": 125}, {"referenceID": 17, "context": "To perform a poisoning attack against SVM, the attacker inserts some data in the training data with the labels flipped [6,18].", "startOffset": 119, "endOffset": 125}, {"referenceID": 17, "context": "This is equivalent to adding noise to the training data [18].", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "Another method [5] is to select a combination of points that maximizes the classification error (or false positive rate).", "startOffset": 15, "endOffset": 18}, {"referenceID": 17, "context": "These methods [18,6] works by selecting points that maximizes the loss function of the SVM classifier.", "startOffset": 14, "endOffset": 20}, {"referenceID": 5, "context": "These methods [18,6] works by selecting points that maximizes the loss function of the SVM classifier.", "startOffset": 14, "endOffset": 20}, {"referenceID": 17, "context": "[18], a set of points is selected based on the loss maximization framework and their labels are flipped.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6], some data points are selected and they are moved to other points in the feature space such that the loss is maximized.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Most of the works [8,7] on secure classifier algorithms uses a game theory model - the problem is modeled as a game between the adversary and the learner.", "startOffset": 18, "endOffset": 23}, {"referenceID": 6, "context": "Most of the works [8,7] on secure classifier algorithms uses a game theory model - the problem is modeled as a game between the adversary and the learner.", "startOffset": 18, "endOffset": 23}, {"referenceID": 3, "context": "Another method is to use multiple classifiers [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "In addition, in the experiments performed in that work, they did not use the more intelligent methods [6,18] of generating the poison data.", "startOffset": 102, "endOffset": 108}, {"referenceID": 17, "context": "In addition, in the experiments performed in that work, they did not use the more intelligent methods [6,18] of generating the poison data.", "startOffset": 102, "endOffset": 108}, {"referenceID": 5, "context": "In this paper, we consider the poisoning attack with gradient ascent proposed by Biggio and Xiao [6,18].", "startOffset": 97, "endOffset": 103}, {"referenceID": 17, "context": "In this paper, we consider the poisoning attack with gradient ascent proposed by Biggio and Xiao [6,18].", "startOffset": 97, "endOffset": 103}, {"referenceID": 17, "context": "In this paper, we are addressing the type of attack [18,6] in which the attack points are selected to maximize the loss function.", "startOffset": 52, "endOffset": 58}, {"referenceID": 5, "context": "In this paper, we are addressing the type of attack [18,6] in which the attack points are selected to maximize the loss function.", "startOffset": 52, "endOffset": 58}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] is very similar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In our algorithm, the clustering algorithm used is DBSCAN [11].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "To verify our method, we perform experiments using the MNIST dataset [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "[6] using the AdversariaLib library.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "3, we are not aware of any method to protect SVM classifiers against the attacks proposed in [6,18].", "startOffset": 93, "endOffset": 99}, {"referenceID": 17, "context": "3, we are not aware of any method to protect SVM classifiers against the attacks proposed in [6,18].", "startOffset": 93, "endOffset": 99}, {"referenceID": 5, "context": "Researches [6,18] have shown that SVM, a very widely used machine learning algorithm, is very susceptible to poisoning attacks.", "startOffset": 11, "endOffset": 17}, {"referenceID": 17, "context": "Researches [6,18] have shown that SVM, a very widely used machine learning algorithm, is very susceptible to poisoning attacks.", "startOffset": 11, "endOffset": 17}], "year": 2016, "abstractText": "Machine learning is used in a number of security related applications such as biometric user authentication, speaker identification etc. A type of causative integrity attack against machine le arning called Poisoning attack works by injecting specially crafted data points in the training data so as to increase the false positive rate of the classifier. In the context of the biometric authentication, this means that more intruders will be classified as valid user, and in case of speaker identification system, user A will be classified user B. In this paper, we examine poisoning attack against SVM and introduce Curie a method to protect the SVM classifier from the poisoning attack. The basic idea of our method is to identify the poisoned data points injected by the adversary and filter them out. Our method is light weight and can be easily integrated into existing systems. Experimental results show that it works very well in filtering out the poisoned data.", "creator": "LaTeX with hyperref package"}}}