{"id": "1606.01603", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution", "abstract": "Most existing approaches for zero pronoun resolution are supervised approaches, where annotated data are released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in zero pronoun resolution task. The existing approaches mainly face the challenge of costing manpower on labeling the extended data for better training performance and domain adaption. To alleviate the problem above, in this paper we propose a simple but novel approach to automatically produce large-scale pseudo training data for zero pronoun resolution. Furthermore, to avoid the drawbacks of the feature engineering based approaches, we proposed an attention-based LSTM model for this task. Experimental results show that our proposed approach outperforms the state-of-the-art methods significantly with an absolute improvement of 5.1% F-score in OntoNotes 5.0 corpus.", "histories": [["v1", "Mon, 6 Jun 2016 02:45:47 GMT  (534kb,D)", "https://arxiv.org/abs/1606.01603v1", null], ["v2", "Fri, 17 Jun 2016 09:18:35 GMT  (267kb,D)", "http://arxiv.org/abs/1606.01603v2", "10pages, under review at EMNLP2016, some typos have been fixed"], ["v3", "Tue, 6 Jun 2017 02:47:41 GMT  (249kb,D)", "http://arxiv.org/abs/1606.01603v3", "8+2 pages, published as a conference paper at ACL2017 (long paper)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ting liu", "yiming cui", "qingyu yin", "weinan zhang", "shijin wang", "guoping hu"], "accepted": true, "id": "1606.01603"}, "pdf": {"name": "1606.01603.pdf", "metadata": {"source": "CRF", "title": "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution", "authors": ["Ting Liu", "Yiming Cui", "Qingyu Yin", "Weinan Zhang", "Shijin Wang", "Guoping Hu"], "emails": ["tliu@ir.hit.edu.cn", "qyyin@ir.hit.edu.cn", "wnzhang@ir.hit.edu.cn", "ymcui@iflytek.com", "sjwang3@iflytek.com", "gphu@iflytek.com"], "sections": [{"heading": "1 Introduction", "text": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013). However, a major obstacle for training the supervised learning models for ZP resolution is the lack of annotated data. An important step is to organize the shared task on anaphora and coreference resolution, such as the ACE evaluations, SemEval-2010 shared task on Coreference Resolution in Multiple Languages (Marta Recasens, 2010) and CoNLL2012 shared task on Modeling Multilingual Unre-\nstricted Coreference in OntoNotes (Sameer Pradhan, 2012). Following these shared tasks, the annotated evaluation data can be released for the following researches. Despite the success and contributions of these shared tasks, it still faces the challenge of spending manpower on labeling the extended data for better training performance and domain adaptation.\nTo address the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Inspired by data generation on cloze-style reading comprehension, we can treat the zero pronoun resolution task as a special case of reading comprehension problem. So we can adopt similar data generation methods of reading comprehension to the zero pronoun resolution task. For the noun or pronoun in the document, which has the frequency equal to or greater than 2, we randomly choose one position where the noun or pronoun is located on, and replace it with a specific symbol \u3008blank\u3009. Let query Q and answer A denote the sentence that contains a \u3008blank\u3009, and the noun or pronoun which is replaced by the \u3008blank\u3009, respectively. Thus, a pseudo training sample can be represented as a triple:\n\u3008D,Q,A\u3009 (1)\nFor the zero pronoun resolution task, a \u3008blank\u3009 represents a zero pronoun (ZP) in query Q, and A indicates the corresponding antecedent of the ZP. In this way, tremendous pseudo training samples can be generated from the various documents, such as news corpus.\nTowards the shortcomings of the previous approaches that are based on feature engineering, we propose a neural network architecture, which is an attention-based neural network model, for zero pronoun resolution. Also we propose a two-step\nar X\niv :1\n60 6.\n01 60\n3v 3\n[ cs\n.C L\n] 6\nJ un\n2 01\n7\ntraining method, which benefit from both largescale pseudo training data and task-specific data, showing promising performance.\nTo sum up, the contributions of this paper are listed as follows.\n\u2022 To our knowledge, this is the first time that utilizing reading comprehension neural network model into zero pronoun resolution task.\n\u2022 We propose a two-step training approach, namely pre-training-then-adaptation, which benefits from both the large-scale automatically generated pseudo training data and taskspecific data.\n\u2022 Towards the shortcomings of the feature engineering approaches, we first propose an attention-based neural network model for zero pronoun resolution."}, {"heading": "2 The Proposed Approach", "text": "In this section, we will describe our approach in detail. First, we will describe our method of generating large-scale pseudo training data for zero pronoun resolution. Then we will introduce twostep training approach to alleviate the gaps between pseudo and real training data. Finally, the attention-based neural network model as well as associated unknown words processing techniques will be described."}, {"heading": "2.1 Generating Pseudo Training Data", "text": "In order to get large quantities of training data for neural network model, we propose an approach, which is inspired by (Hermann et al., 2015), to automatically generate large-scale pseudo training data for zero pronoun resolution. However, our approach is much more simple and general than that of (Hermann et al., 2015). We will introduce the details of generating the pseudo training data for zero pronoun resolution as follows.\nFirst, we collect a large number of documents that are relevant (or homogenous in some sense) to the released OntoNote 5.0 data for zero pronoun resolution task in terms of its domain. In our experiments, we used large-scale news data for training.\nGiven a certain document D, which is composed by a set of sentences D = {s1, s2, ..., sn},\nwe randomly choose an answer wordA in the document. Note that, we restrictA to be either a noun or pronoun, where the part-of-speech is identified using LTP Toolkit (Che et al., 2010), as well as the answer word should appear at least twice in the document. Second, after the answer word A is chosen, the sentence that contains A is defined as a queryQ, in which the answer wordA is replaced by a specific symbol \u3008blank\u3009. In this way, given the queryQ and documentD, the target of the prediction is to recover the answer A. That is quite similar to the zero pronoun resolution task. Therefore, the automatically generated training samples is called pseudo training data. Figure 1 shows an example of a pseudo training sample.\nIn this way, we can generate tremendous triples of \u3008D,Q,A\u3009 for training neural network, without making any assumptions on the nature of the original corpus."}, {"heading": "2.2 Two-step Training", "text": "It should be noted that, though we have generated large-scale pseudo training data for neural network training, there is still a gap between pseudo training data and the real zero pronoun resolution task in terms of the query style. So we should do some adaptations to our model to deal with the zero pronoun resolution problems ideally.\nIn this paper, we used an effective approach to deal with the mismatch between pseudo training data and zero pronoun resolution task-specific data. Generally speaking, in the first stage, we use a large amount of the pseudo training data to train a fundamental model, and choose the best model according to the validation accuracy. Then we continue to train from the previous best model using the zero pronoun resolution task-specific training data, which is exactly the same domain and query type as the standard zero pronoun resolution task data.\nThe using of the combination of proposed pseudo training data and task-specific data, i.e. zero pronoun resolution task data, is far more effective than using either of them alone. Though there is a gap between these two data, they share many similar characteristics to each other as illustrated in the previous part, so it is promising to utilize these two types of data together, which will compensate to each other.\nThe two-step training procedure can be concluded as,\n\u2022 Pre-training stage: by using large-scale training data to train the neural network model, we can learn richer word embeddings, as well as relatively reasonable weights in neural networks than just training with a small amount of zero pronoun resolution task training data;\n\u2022 Adaptation stage: after getting the best model that is produced in the previous step, we continue to train the model with task-specific data, which can force the previous model to adapt to the new data, without losing much knowledge that has learned in the previous stage (such as word embeddings).\nAs we will see in the experiment section that the proposed two-step training approach is effective and brings significant improvements."}, {"heading": "2.3 Attention-based Neural Network Model", "text": "Our model is primarily an attention-based neural network model, which is similar to Attentive Reader proposed by (Hermann et al., 2015). Formally, when given a set of training triple \u3008D,Q,A\u3009, we will construct our network in the following way.\nFirstly, we project one-hot representation of document D and query Q into a continuous space with the shared embedding matrix We. Then we input these embeddings into different bidirectional RNN to get their contextual representations respectively. In our model, we used the bidirectional Gated Recurrent Unit (GRU) as RNN implementation (Cho et al., 2014).\ne(x) =We \u00b7 x, where x \u2208 D,Q (2)\n\u2212\u2192 hs = \u2212\u2212\u2212\u2192 GRU(e(x)); \u2190\u2212 hs = \u2190\u2212\u2212\u2212 GRU(e(x)) (3)\nhs = [ \u2212\u2192 hs; \u2190\u2212 hs] (4)\nFor the query representation, instead of concatenating the final forward and backward states as its representations, we directly get an averaged representations on all bi-directional RNN slices, which can be illustrated as\nhquery = 1\nn n\u2211 t=1 hquery(t) (5)\nFor the document, we place a soft attention over all words in document (Bahdanau et al., 2014), which indicate the degree to which part of document is attended when filling the blank in the query sentence. Then we calculate a weighted sum of all document tokens to get the attended representation of document.\nm(t) = tanh(W \u00b7 hdoc(t) + U \u00b7 hquery) (6)\n\u03b1(t) = exp(Ws \u00b7m(t)) n\u2211 j=1 exp(Ws \u00b7m(j)) (7)\nhdoc att = hdoc \u00b7 \u03b1 (8)\nwhere variable \u03b1(t) is the normalized attention weight at tth word in document, hdoc is a matrix that concatenate all hdoc(t) in sequence.\nhdoc = concat[hdoc(1), hdoc(2), ..., hdoc(t)] (9)\nThen we use attended document representation and query representation to estimate the final answer, which can be illustrated as follows, where V\nis the vocabulary,\nr = concat[hdoc att, hquery] (10)\nP (A|D,Q) \u221d softmax(Wr \u00b7 r) , s.t. A \u2208 V (11)\nFigure 2 shows the proposed neural network architecture.\nNote that, for zero pronoun resolution task, antecedents of zero pronouns are always noun phrases (NPs), while our model generates only one word (a noun or a pronoun) as the result. To better adapt our model to zero pronoun resolution task, we further process the output result in the following procedure. First, for a given zero pronoun, we extract a set of NPs as its candidates utilizing the same strategy as (Chen and Ng, 2015). Then, we use our model to generate an answer (one word) for the zero pronoun. After that, we go through all the candidates from the nearest to the far-most. For an NP candidate, if the produced answer is its head word, we then regard this NP as the antecedent of the given zero pronoun. By doing so, for a given zero pronoun, we generate an NP as the prediction of its antecedent."}, {"heading": "2.4 Unknown Words Processing", "text": "Because of the restriction on both memory occupation and training time, it is usually suggested to use a shortlist of vocabulary in neural network training. However, we often replace the out-ofvocabularies to a unique special token, such as \u3008unk\u3009. But this may place an obstacle in real\nworld test. When the model predicts the answer as \u3008unk\u3009, we do not know what is the exact word it represents in the document, as there may have many \u3008unk\u3009s in the document.\nIn this paper, we propose to use a simple but effective way to handle unknown words issue. The idea is straightforward, which can be illustrated as follows.\n\u2022 Identify all unknown words inside of each \u3008D,Q,A\u3009;\n\u2022 Instead of replacing all these unknown words into one unique token \u3008unk\u3009, we make a hash table to project these unique unknown words to numbered tokens, such as \u3008unk1\u3009, \u3008unk2\u3009, ..., \u3008unkN\u3009 in terms of its occurrence order in the document. Note that, the same words are projected to the same unknown word tokens, and all these projections are only valid inside of current sample. For example, \u3008unk1\u3009 indicate the first unknown word, say \u201capple\u201d, in the current sample, but in another sample the \u3008unk1\u3009 may indicate the unknown word \u201corange\u201d. That is, the unknown word labels are indicating position features rather than the exact word;\n\u2022 Insert these unknown marks in the vocabulary. These marks may only take up dozens of slots, which is negligible to the size of shortlists (usually 30K \u223c 100K).\nWe take one sentence \u201cThe weather of today is not as pleasant as the weather of yesterday.\u201d as an example to show our unknown word processing method, which is shown in Figure 3.\nIf we do not discriminate the unknown words and assign different unknown words with the same token \u3008unk\u3009, it would be impossible for us to know what is the exact word that \u3008unk\u3009 represents for in the real test. However, when using our proposed unknown word processing method, if the model predicts a \u3008unkX\u3009 as the answer,\nwe can simply scan through the original document and identify its position according to its unknown word number X and replace the \u3008unkX\u3009 with the real word. For example, in Figure 3, if we adopt original unknown words processing method, we could not know whether the \u3008unk\u3009 is the word \u201cweather\u201d or \u201cpleasant\u201d. However, when using our approach, if the model predicts an answer as \u3008unk1\u3009, from the original text, we can know that \u3008unk1\u3009 represents the word \u201cweather\u201d."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Data", "text": "In our experiments, we choose a selection of public news data to generate large-scale pseudo training data for pre-training our neural network model (pre-training step)1. In the adaptation step, we used the official dataset OntoNotes Release 5.02 which is provided by CoNLL-2012 shared task, to carry out our experiments. The CoNLL2012 shared task dataset consists of three parts: a training set, a development set and a test set. The datasets are made up of 6 different domains, namely Broadcast News (BN), Newswires (NW), Broadcast Conversations (BC), Telephone Conversations (TC), Web Blogs (WB), and Magazines (MZ). We closely follow the experimental settings as (Kong and Zhou, 2010; Chen and Ng, 2014, 2015, 2016), where we treat the training set for training and the development set for testing, because only the training and development set are annotated with ZPs. The statistics of training and testing data is shown in Table 1 and 2 respectively."}, {"heading": "3.2 Neural Network Setups", "text": "Training details of our neural network models are listed as follows.\n1The news data is available at http://www.sogou. com/labs/dl/cs.html\n2http://catalog.ldc.upenn.edu/ LDC2013T19\nDocs Sentences Words AZPs\nTest 172 6,083 110K 1,713\nAll models are trained on Tesla K40 GPU. Our model is implemented with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015)."}, {"heading": "3.3 Experimental results", "text": "Same to the previous researches that are related to zero pronoun resolution, we evaluate our system performance in terms of F-score (F). We focus on AZP resolution process, where we assume that gold AZPs and gold parse trees are given3. The same experimental setting is utilized in (Chen and Ng, 2014, 2015, 2016). The overall results are shown in Table 3, where the performances of each domain are listed in detail and overall performance is also shown in the last column.\n\u2022 Overall Performance We employ four Chinese ZP resolution baseline systems on OntoNotes 5.0 dataset. As we can\n3All gold information are provided by the CoNLL-2012 shared task dataset\nsee that our model significantly outperforms the previous state-of-the-art system (Chen and Ng, 2016) by 3.1% in overall F-score, and substantially outperform the other systems by a large margin. When observing the performances of different domains, our approach also gives relatively consistent improvements among various domains, except for BN and TC with a slight drop. All these results approve that our proposed approach is effective and achieves significant improvements in AZP resolution.\nIn our quantitative analysis, we investigated the reasons of the declines in the BN and TC domain. A primary observation is that the word distributions in these domains are fairly different from others. The average document length of BN and TC are quite longer than other domains, which suggest that there is a bigger chance to have unknown words than other domains, and add difficulties to the model training. Also, we have found that in the BN and TC domains, the texts are often in oral form, which means that there are many irregular expressions in the context. Such expressions add noise to the model, and it is difficult for the model to extract useful information in these contexts. These phenomena indicate that further improvements can be obtained by filtering stop words in contexts, or increasing the size of task-specific data, while we leave this in the future work.\n\u2022 Effect of UNK processing As we have mentioned in the previous section, traditional unknown word replacing methods are vulnerable to the real word test. To alleviate this issue, we proposed the UNK processing mechanism to recover the UNK tokens to the real words. In Table 4, we compared the performance that with and without the proposed UNK processing,\nto show whether the proposed UNK processing method is effective. As we can see that, by applying our UNK processing mechanism, the model do learned the positional features in these lowfrequency words, and brings over 3% improvements in F-score, which indicated the effectiveness of our UNK processing approach.\n\u2022 Effect of Domain Adaptation We also tested out whether our domain adaptation method is effective. In this experiments, we used three different types of training data: only pseudo training data, only task-specific data, and our adaptation method, i.e. using pseudo training data in the pre-training step and task-specific data for domain adaptation step. The results are given in Table 5. As we can see that, using either pseudo training data or task-specific data alone can not bring inspiring result. By adopting our domain adaptation method, the model could give significant improvements over the other models, which demonstrate the effectiveness of our proposed two-step training approach. An intuition behind this phenomenon is that though pseudo training data is fairly big enough to train a reliable model parameters, there is still a gap to the real zero pronoun resolution tasks. On the contrary, though task-specific training data is exactly the same type as the real test, the quantity is not enough to train a reasonable model (such as word embedding). So it is better to make use of both to\ntake the full advantage. However, as the original task-specific data is fairly small compared to pseudo training data, we also wondered if the large-scale pseudo training data is only providing rich word embedding information. So we use the large-scale pseudo training data for embedding training using GloVe toolkit (Pennington et al., 2014), and initialize the word embeddings in the \u201conly task-specific data\u201d system. From the result we can see that the pseudo training data provide more information than word embeddings, because though we used GloVe embeddings in \u201conly task-specific data\u201d, it still can not outperform the system that uses domain adaptation which supports our claim."}, {"heading": "4 Error Analysis", "text": "To better evaluate our proposed approach, we performed a qualitative analysis of errors, where two major errors are revealed by our analysis, as discussed below."}, {"heading": "4.1 Effect of Unknown Words", "text": "Our approach does not do well when there are lots of \u3008unk\u3009s in the context of ZPs, especially when the \u3008unk\u3009s appears near the ZP. An example is given below, where words with # are regarded as \u3008unk\u3009s in our model.\n\u03c6 \u767b\u4e0a# \u592a\u5e73\u5c71# \u9876 , \u5c06\u9999\u6e2f\u5c9b# \u548c\u7ef4\u591a \u5229\u4e9a\u6e2f# \u7684\u7f8e\u666f\u5c3d\u6536\u773c\u5e95\u3002 \u03c6 Successfully climbed# the peak of [Taiping Mountain]#, to have a panoramic view of the beauty of [Hong Kong Island]# and [Victoria Harbour]#.\nIn this case, the words \u201c\u767b\u4e0a/climbed\u201d and \u201c\u592a \u5e73\u5c71/Taiping Mountain\u201d that appears immediately after the ZP \u201c\u03c6\u201d are all regarded as \u3008unk\u3009s in our model. As we model the sequence of words by RNN, the \u3008unk\u3009s make the model more difficult to capture the semantic information of the sentence, which in turn influence the overall performance. Especially for the words that are near\nthe ZP, which play important roles when modeling context information for the ZP. By looking at the word \u201c\u9876/peak\u201d, it is hard to comprehend the context information, due to the several surrounding \u3008unk\u3009s. Though our proposed unknown words processing method is effective in empirical evaluation, we think that more advanced method for unknown words processing would be of a great help in improving comprehension of the context."}, {"heading": "4.2 Long Distance Antecedents", "text": "Also, our model makes incorrect decisions when the correct antecedents of ZPs are in long distance. As our model chooses answer from words in the context, if there are lots of words between the ZP and its antecedent, more noise information are introduced, and adds more difficulty in choosing the right answer. For example:\n\u6211\u5e2e\u4e0d\u4e86\u90a3\u4e2a\u4eba ... ...\u90a3\u5929\u7ed3\u675f\u540e \u03c6 \u56de\u5230 \u5bb6\u4e2d\u3002 I can\u2019t help that guy ... ... After that day, \u03c6 return home.\nIn this case, the correct antecedent of ZP \u201c\u03c6\u201d is the NP candidate \u201c\u6211/I\u201d. By seeing the contexts, we observe that there are over 30 words between the ZP and its antecedent. Although our model does not intend to fill the ZP gap only with the words near the ZP, as most of the antecedents appear just a few words before the ZPs, our model prefers the nearer words as correct antecedents. Hence, once there are lots of words between ZP and its nearest antecedent, our model can sometimes make wrong decisions. To correctly handle such cases, our model should learn how to filter the useless words and enhance the learning of longterm dependency."}, {"heading": "5 Related Work", "text": ""}, {"heading": "5.1 Zero pronoun resolution", "text": "For Chinese zero pronoun (ZP) resolution, early studies employed heuristic rules to Chinese ZP resolution. Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents. Then, supervised approaches to this task have been vastly explored. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs. Kong and Zhou (2010) develop a tree-kernel based approach for Chinese ZP resolution. More recently, unsupervised approaches\nhave been proposed. Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferra\u0301ndez and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution. Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011). Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution.\nIn sum, most recent researches on ZP resolution are supervised approaches, which means that their performance highly relies on large-scale annotated data. Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs. Therefore, the advantage of our proposed approach is obvious. We are able to generate large-scale pseudo training data for ZP resolution, and also we can benefit from the task-specific data for fine-tuning via the proposed two-step training approach."}, {"heading": "5.2 Cloze-style Reading Comprehension", "text": "Our neural network model is mainly motivated by the recent researches on cloze-style reading comprehension tasks, which aims to predict one-word answer given the document and query. These models can be seen as a general model of mining the relations between the document and query, so it is promising to combine these models to the specific domain.\nA representative work of cloze-style reading comprehension is done by Hermann et al. (2015). They proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples. By using this method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network. They used attention-based neural networks for this task. Evaluation on CNN/DailyMail datasets showed that their approach is much effective than\ntraditional baseline systems. While our work is similar to Hermann et al. (2015), there are several differences which can be illustrated as follows. Firstly, though we both utilize the large-scale corpus, they require that the document should accompany with a brief summary of it, while this is not always available in most of the document, and it may place an obstacle in generating limitless training data. In our work, we do not assume any prerequisite of the training data, and directly extract queries from the document, which makes it easy to generate large-scale training data. Secondly, their work mainly focuses on reading comprehension in the general domain. We are able to exploit large-scale training data for solving problems in the specific domain, and we proposed two-step training method which can be easily adapted to other domains as well."}, {"heading": "6 Conclusion", "text": "In this study, we propose an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task. The main idea behind our approach is to automatically generate large-scale pseudo training data and then utilize an attention-based neural network model to resolve zero pronouns. For training purpose, two-step training approach is employed, i.e. a pre-training and adaptation step, and this can be also easily applied to other tasks as well. The experimental results on OntoNotes 5.0 corpus are encouraging, showing that the proposed model and accompanying approaches significantly outperforms the stateof-the-art systems.\nThe future work will be carried out on two main aspects: First, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more effective way to handle the UNK issue. Second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolution task."}, {"heading": "Acknowledgements", "text": "We would like to thank the anonymous reviewers for their thorough reviewing and proposing thoughtful comments to improve our paper. This work was supported by the National 863 Leading Technology Research Project via grant 2015AA015407, Key Projects of National Natural Science Foundation of China via grant 61632011,\nand National Natural Science Youth Foundation of China via grant 61502120."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Ltp: A chinese language technology platform", "author": ["Wanxiang Che", "Zhenghua Li", "Ting Liu."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations. Association for Computational Linguistics, pages 13\u201316.", "citeRegEx": "Che et al\\.,? 2010", "shortCiteRegEx": "Che et al\\.", "year": 2010}, {"title": "Chinese zero pronoun resolution: Some recent advances", "author": ["Chen Chen", "Vincent Ng."], "venue": "EMNLP. pages 1360\u20131365.", "citeRegEx": "Chen and Ng.,? 2013", "shortCiteRegEx": "Chen and Ng.", "year": 2013}, {"title": "Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming", "author": ["Chen Chen", "Vincent Ng."], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Chen and Ng.,? 2014", "shortCiteRegEx": "Chen and Ng.", "year": 2014}, {"title": "Chinese zero pronoun resolution: A joint unsupervised discourseaware model rivaling state-of-the-art resolvers", "author": ["Chen Chen", "Vincent Ng."], "venue": "Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natu-", "citeRegEx": "Chen and Ng.,? 2015", "shortCiteRegEx": "Chen and Ng.", "year": 2015}, {"title": "Chinese zero pronoun resolution with deep neural networks", "author": ["Chen Chen", "Vincent Ng."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,", "citeRegEx": "Chen and Ng.,? 2016", "shortCiteRegEx": "Chen and Ng.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Pronominal anaphora resolution in chinese", "author": ["Susan P Converse"], "venue": null, "citeRegEx": "Converse.,? \\Q2006\\E", "shortCiteRegEx": "Converse.", "year": 2006}, {"title": "A computational approach to zero-pronouns in spanish", "author": ["Antonio Ferr\u00e1ndez", "Jes\u00fas Peral."], "venue": "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 166\u2013172.", "citeRegEx": "Ferr\u00e1ndez and Peral.,? 2000", "shortCiteRegEx": "Ferr\u00e1ndez and Peral.", "year": 2000}, {"title": "Korean zero pronouns: analysis and resolution", "author": ["Na-Rae Han."], "venue": "Ph.D. thesis, Citeseer.", "citeRegEx": "Han.,? 2006", "shortCiteRegEx": "Han.", "year": 2006}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Resolving pronoun references", "author": ["Jerry R Hobbs."], "venue": "Lingua 44(4):311\u2013338.", "citeRegEx": "Hobbs.,? 1978", "shortCiteRegEx": "Hobbs.", "year": 1978}, {"title": "Exploiting syntactic patterns as clues in zero-anaphora resolution", "author": ["Ryu Iida", "Kentaro Inui", "Yuji Matsumoto."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-", "citeRegEx": "Iida et al\\.,? 2006", "shortCiteRegEx": "Iida et al\\.", "year": 2006}, {"title": "Zero-anaphora resolution by learning rich syntactic pattern features", "author": ["Ryu Iida", "Kentaro Inui", "Yuji Matsumoto."], "venue": "ACM Transactions on Asian Language Information Processing (TALIP) 6(4):1.", "citeRegEx": "Iida et al\\.,? 2007", "shortCiteRegEx": "Iida et al\\.", "year": 2007}, {"title": "A cross-lingual ilp solution to zero anaphora resolution", "author": ["Ryu Iida", "Massimo Poesio."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computa-", "citeRegEx": "Iida and Poesio.,? 2011", "shortCiteRegEx": "Iida and Poesio.", "year": 2011}, {"title": "Japanese zero pronoun resolution based on ranking rules and machine learning", "author": ["Hideki Isozaki", "Tsutomu Hirao."], "venue": "Proceedings of the 2003 conference on Empirical methods in natural language processing. Association for Computational Linguis-", "citeRegEx": "Isozaki and Hirao.,? 2003", "shortCiteRegEx": "Isozaki and Hirao.", "year": 2003}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A tree kernelbased unified framework for chinese zero anaphora resolution", "author": ["Fang Kong", "Guodong Zhou."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "Kong and Zhou.,? 2010", "shortCiteRegEx": "Kong and Zhou.", "year": 2010}, {"title": "Semeval-2010 task 1: Coreference resolution in multiple languages", "author": [], "venue": null, "citeRegEx": "Recasens.,? \\Q2010\\E", "shortCiteRegEx": "Recasens.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes", "author": [], "venue": null, "citeRegEx": "Pradhan.,? \\Q2012\\E", "shortCiteRegEx": "Pradhan.", "year": 2012}, {"title": "A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames", "author": ["Ryohei Sasano", "Sadao Kurohashi."], "venue": "IJCNLP. pages 758\u2013766.", "citeRegEx": "Sasano and Kurohashi.,? 2011", "shortCiteRegEx": "Sasano and Kurohashi.", "year": 2011}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli."], "venue": "arXiv preprint arXiv:1312.6120 .", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Identification and resolution of chinese zero pronouns: A machine learning approach", "author": ["Shanheng Zhao", "Hwee Tou Ng."], "venue": "EMNLP-CoNLL. volume 2007, pages 541\u2013550.", "citeRegEx": "Zhao and Ng.,? 2007", "shortCiteRegEx": "Zhao and Ng.", "year": 2007}], "referenceMentions": [{"referenceID": 10, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 25, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 14, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 18, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 15, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 2, "context": "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).", "startOffset": 100, "endOffset": 212}, {"referenceID": 11, "context": "In order to get large quantities of training data for neural network model, we propose an approach, which is inspired by (Hermann et al., 2015), to automatically generate large-scale pseudo training data for zero pronoun resolution.", "startOffset": 121, "endOffset": 143}, {"referenceID": 11, "context": "However, our approach is much more simple and general than that of (Hermann et al., 2015).", "startOffset": 67, "endOffset": 89}, {"referenceID": 1, "context": "Note that, we restrictA to be either a noun or pronoun, where the part-of-speech is identified using LTP Toolkit (Che et al., 2010), as well as the answer word should appear at least twice in the document.", "startOffset": 113, "endOffset": 131}, {"referenceID": 11, "context": "Our model is primarily an attention-based neural network model, which is similar to Attentive Reader proposed by (Hermann et al., 2015).", "startOffset": 113, "endOffset": 135}, {"referenceID": 6, "context": "In our model, we used the bidirectional Gated Recurrent Unit (GRU) as RNN implementation (Cho et al., 2014).", "startOffset": 89, "endOffset": 107}, {"referenceID": 0, "context": "For the document, we place a soft attention over all words in document (Bahdanau et al., 2014), which indicate the degree to which part of document is attended when filling the blank in the query sentence.", "startOffset": 71, "endOffset": 94}, {"referenceID": 4, "context": "First, for a given zero pronoun, we extract a set of NPs as its candidates utilizing the same strategy as (Chen and Ng, 2015).", "startOffset": 106, "endOffset": 125}, {"referenceID": 18, "context": "We closely follow the experimental settings as (Kong and Zhou, 2010; Chen and Ng, 2014, 2015, 2016), where we treat the training set for training and the development set for testing, because only the training and development set are annotated with ZPs.", "startOffset": 47, "endOffset": 99}, {"referenceID": 23, "context": "\u2022 Hidden Layer: We use GRU with 256 units, and initialize the internal matrix by random orthogonal matrices (Saxe et al., 2013).", "startOffset": 108, "endOffset": 127}, {"referenceID": 17, "context": "\u2022 Optimization: We used ADAM update rule (Kingma and Ba, 2014) with an initial learning rate of 0.", "startOffset": 41, "endOffset": 62}, {"referenceID": 7, "context": "Our model is implemented with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015).", "startOffset": 79, "endOffset": 94}, {"referenceID": 2, "context": "9 Chen and Ng (2014) 38.", "startOffset": 2, "endOffset": 21}, {"referenceID": 2, "context": "9 Chen and Ng (2014) 38.1 31.0 50.4 45.9 53.8 54.9 48.7 Chen and Ng (2015) 46.", "startOffset": 2, "endOffset": 75}, {"referenceID": 2, "context": "9 Chen and Ng (2014) 38.1 31.0 50.4 45.9 53.8 54.9 48.7 Chen and Ng (2015) 46.4 39.0 51.8 53.8 49.4 52.7 50.2 Chen and Ng (2016) 48.", "startOffset": 2, "endOffset": 129}, {"referenceID": 5, "context": "see that our model significantly outperforms the previous state-of-the-art system (Chen and Ng, 2016) by 3.", "startOffset": 82, "endOffset": 101}, {"referenceID": 20, "context": "So we use the large-scale pseudo training data for embedding training using GloVe toolkit (Pennington et al., 2014), and initialize the word embeddings in the \u201conly task-specific data\u201d system.", "startOffset": 90, "endOffset": 115}, {"referenceID": 12, "context": "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents.", "startOffset": 104, "endOffset": 117}, {"referenceID": 8, "context": "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents.", "startOffset": 0, "endOffset": 16}, {"referenceID": 8, "context": "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents. Then, supervised approaches to this task have been vastly explored. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs.", "startOffset": 0, "endOffset": 227}, {"referenceID": 8, "context": "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents. Then, supervised approaches to this task have been vastly explored. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs. Kong and Zhou (2010) develop a tree-kernel based approach for Chinese ZP resolution.", "startOffset": 0, "endOffset": 354}, {"referenceID": 10, "context": "Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al.", "startOffset": 80, "endOffset": 91}, {"referenceID": 16, "context": "Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011).", "startOffset": 105, "endOffset": 183}, {"referenceID": 22, "context": "Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011).", "startOffset": 105, "endOffset": 183}, {"referenceID": 2, "context": "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information.", "startOffset": 0, "endOffset": 163}, {"referenceID": 2, "context": "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferr\u00e1ndez and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution.", "startOffset": 0, "endOffset": 489}, {"referenceID": 2, "context": "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferr\u00e1ndez and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution. Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011). Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution.", "startOffset": 0, "endOffset": 761}, {"referenceID": 3, "context": "Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs.", "startOffset": 35, "endOffset": 54}, {"referenceID": 10, "context": "A representative work of cloze-style reading comprehension is done by Hermann et al. (2015). They proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples.", "startOffset": 70, "endOffset": 92}, {"referenceID": 11, "context": "While our work is similar to Hermann et al. (2015), there are several differences which can be illustrated as follows.", "startOffset": 29, "endOffset": 51}], "year": 2017, "abstractText": "Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.", "creator": "LaTeX with hyperref package"}}}