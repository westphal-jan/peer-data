{"id": "1606.01614", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification", "abstract": "In recent years deep neural networks have achieved great success in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most other languages do not enjoy such an abundance of annotated data for sentiment analysis. To combat this problem, we propose the Adversarial Deep Averaging Network (ADAN) to transfer sentiment knowledge learned from labeled English data to low-resource languages where only unlabeled data exists. ADAN is a \"Y-shaped\" network with two discriminative branches: a sentiment classifier and an adversarial language predictor. Both branches take input from a feature extractor that aims to learn hidden representations that capture the underlying sentiment of the text and are invariant across languages. Experiments on Chinese sentiment classification demonstrate that ADAN significantly outperforms several baselines, including a strong pipeline approach that relies on Google Translate, the state-of-the-art commercial machine translation system.", "histories": [["v1", "Mon, 6 Jun 2016 05:04:23 GMT  (4327kb,D)", "http://arxiv.org/abs/1606.01614v1", null], ["v2", "Thu, 27 Oct 2016 15:28:02 GMT  (6231kb,D)", "http://arxiv.org/abs/1606.01614v2", null], ["v3", "Thu, 16 Feb 2017 01:30:30 GMT  (1655kb,D)", "http://arxiv.org/abs/1606.01614v3", null], ["v4", "Mon, 17 Apr 2017 18:48:19 GMT  (2431kb,D)", "http://arxiv.org/abs/1606.01614v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xilun chen", "yu sun", "ben athiwaratkun", "claire cardie", "kilian weinberger"], "accepted": false, "id": "1606.01614"}, "pdf": {"name": "1606.01614.pdf", "metadata": {"source": "CRF", "title": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification", "authors": ["Xilun Chen", "Ben Athiwaratkun", "Yu Sun", "Kilian Weinberger", "Claire Cardie"], "emails": ["xlchen@cs.cornell.edu", "pa338@cornell.edu", "ys646@cornell.edu", "kqw4@cornell.edu", "cardie@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "There has been significant progress on English sentence- and document-level sentiment classification in recent years using models based on neural networks (Socher et al., 2013; I\u0307rsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015). Most of these, however, rely on a massive amount of labeled training data or fine-grained annotations such as the Stanford Sentiment Treebank (Socher et al., 2013), which provides senti-\nment annotations for each phrase in the parse tree of every sentence. On the other hand, such a luxury is not available to many other languages, for which only a handful of sentence- or document-level sentiment annotations exist. To aid the creation of sentiment classification systems in such low-resource languages, we propose the ADAN model that uses the abundant resources for a source language (here, English) to produce sentiment analysis models for a target language with (little or) no available labeled data. Our system is unsupervised in the sense that it does not require annotations in the target language. In this paper, we use Chinese sentiment classification as a motivating example, but our method can be readily applied to other languages as we only require unlabeled text in the target language, which is fairly accessible for most languages.\nIn particular, Chinese sentiment analysis remains much less explored compared to English, mostly due to the lack of large-scale labeled training corpora. Previous methods perform Chinese sentiment classification by training linear classifiers on small domain-specific datasets with hundreds to a few thousand instances. Training modern deep neural networks is impossible on such small datasets, and consequently a lot of research effort goes into hand-crafting better features that do not necessarily generalize well (Tan and Zhang, 2008). Although some prior work tries to alleviate the scarcity of sentiment annotations by leveraging labeled English data (Wan, 2008; Wan, 2009; Lu et al., 2011), these methods rely on external knowledge such as bilingual lexicons or machine translation (MT) systems, both of which are difficult and expensive to obtain. ar X iv :1\n60 6.\n01 61\n4v 1\n[ cs\n.C L\n] 6\nJ un\n2 01\nIn this work, we propose an end-to-end neural network model that only requires labeled English data and unlabeled Chinese text as input, and explicitly transfers the knowledge learned on English sentiment analysis to Chinese. Our trained system directly operates on Chinese sentences to predict their sentiment (e.g. positive or negative).\nWe hypothesize that an ideal model for crosslingual sentiment analysis should learn features that both perform well on the English sentiment classification task, and are invariant with respect to the shift in language. Therefore, ADAN simultaneously optimizes two components: i) a sentiment classifier P for English; and ii) an adversarial language predictor Q that tries to predict whether a sentence x is from English or Chinese. The structure of the model is shown in Figure 1. The two classifiers take input from the jointly learned feature extractor F , which is trained to maximize accuracy on English sentiment analysis and simultaneously to minimize the language predictor\u2019s chance of correctly predicting the language of the text. This is why the language predictor Q is called \u201cadversarial\u201d.\nThe model is exposed to both English and Chinese sentences during training, but only the labeled English sentences pass through the sentiment classifier. The feature extractor and the sentiment classifier are then used for Chinese sentences at test time. In this manner, we can train the system with massive amounts of unlabeled text in Chinese. Upon convergence, the joint features (output of F) are thus encouraged to be both discriminative for sentiment\nanalysis and invariant across languages. The idea of incorporating an adversary in a neural network model has achieved great success in computer vision for image generation (Goodfellow et al., 2014) and visual domain adaptation (Ganin and Lempitsky, 2015; Ajakan et al., 2014). However, to the best of our knowledge, this work is the first to develop an adversarial network for a cross-lingual NLP task. While conceptually similar to domain adaptation, most research on domain adaptation assumes that the input from both domains share the same representation, such as image pixels for image recognition and bag of words for text classification.However, in our setting, the bag-of-word representation is infeasible because the two languages have completely different vocabularies.\nIn the following sections, we present our method in more detail and show experimental results in which ADAN significantly outperforms several baselines, some even with access to the powerful commercial MT system of Google Translate1."}, {"heading": "2 Related Work", "text": "Sentence-level Sentiment Classification is a popular NLP task on which neural network models have demonstrated tremendous power when coupled with copious data (Socher et al., 2013; I\u0307rsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015). In principle, our cross-lingual framework could use any one of those methods for its feature extractor and the sentiment classifier.\n1http://translate.google.com\nWe choose to build upon the Deep Averaging Network (DAN), a very simple neural network model that yields surprisingly good performance, comparable to complicated syntactic models like recursive neural networks (Iyyer et al., 2015). The simplicity of DAN helps to illustrate the effectiveness of our framework. For each document, DAN takes the arithmetic mean of the document word vectors (Mikolov et al., 2013; Pennington et al., 2014) as input, and passes it through several fully-connected layers until a softmax for classification.\nCross-lingual Sentiment Analysis (Mihalcea et al., 2007; Banea et al., 2008; Banea et al., 2010) is motivated by the lack of high-quality labeled data in many non-English languages. For Chinese in particular, there have been several representative works in both the machine learning direction (Wan, 2008; Wan, 2009; Lu et al., 2011) and the more traditional lexical direction (He et al., 2010). Our work is comparable to these papers in objective but very different in method. The work by Wan uses machine translation to directly convert English training data to Chinese; this is one of our baselines. Lu et al. (2011) instead uses labeled data from both languages to improve the performance on both.\nDomain Adaptation Blitzer et al. (2007), Glorot et al. (2011) and Chen et al. (2012) try to learn effective classifiers for which the training and test samples are from different underlying distributions. This can be thought of as a generalization of cross-lingual text classification. However, one main difference is that, when applied to text classification tasks such as sentiment analysis, most works in domain adaptation evaluate on adapting product reviews from one domain to another (e.g. books to electronics), where the divergence in distribution is much less significant than that between two languages. In addition, for cross-lingual sentiment analysis, it might be difficult to find data from exactly the same domain in two languages, in which case our model still demonstrates impressive performance.\nAdversarial Networks (Goodfellow et al., 2014; Ganin and Lempitsky, 2015) have enjoyed much success in computer vision, but to the best of our knowledge, have not yet been applied in NLP with comparable success. We are the first to apply adver-\nsarial training to the cross-lingual setting in NLP. A series of work in image generation has used architectures similar to ours, by pitching a neural image generator against a discriminator that learns to classify real versus generated images (Goodfellow et al., 2014; Denton et al., 2015). More relevant to this work, adversarial architectures have produced the state-of-the-art in unsupervised domain adaptation for image object recognition where Ganin and Lempitsky (2015) train with many labeled source images and unlabeled target images, similar to our setup.\n3 The ADAN Model"}, {"heading": "3.1 Network Architecture", "text": "As illustrated in Figure 1, the ADAN model is a feedforward network with two branches. Hence there are three main components in the network, a joint feature extractor F that maps an input sequence x to the feature space, a sentiment classifier P that predicts the sentiment label for x given the feature representation F(x), and a language predictor Q that also takes the feature F(x) but predicts whether x is from English or Chinese.\nAn input document is modeled as a sequence of words x = {w1, . . . , wn} where n is the number of tokens in x. Each word w \u2208 x is represented by its word embedding vw (Mikolov et al., 2013). As the same feature extractor F is trained and tested on both English and Chinese sentences, it is favorable if the word representations for both languages align approximately in a shared space. Prior work on bilingual word embeddings (Zou et al., 2013; Vulic\u0301 and Moens, 2015; Gouws et al., 2015) attempts to induce distributed word representations that encode semantic relatedness between words across languages, so that similar words are closer in the embedded space regardless of language.\nSince many of these methods require a large amount of parallel corpus or extended training time, in this work we leverage the pre-trained bilingual word embeddings (BWE) in (Zou et al., 2013). Their work provides 50-dimensional embeddings for 100k English words and a different set of 100k Chinese words. Note that using the word embeddings in (Zou et al., 2013) makes our work implicitly dependent on a parallel corpus from the two lan-\nguages, since it is required to train the bilingual embeddings. However, more recent approaches to train bilingual word embeddings require little or even no parallel corpus (Gouws et al., 2015; Vulic\u0301 and Moens, 2015). These methods can alleviate or eliminate this dependence. For experiments and more discussions on word embeddings, see Section 4.5.\nThe feature extractor F is a Deep Averaging Network (DAN) (Iyyer et al., 2015). F first calculates the arithmetic mean of the word vectors in the input sequence, then passes the average through a feedforward network with ReLU nonlinearities. The activations of the last layer in F is considered the extracted features for the input and is then passed on to P and Q. The sentiment classifier P and the language predictorQ are standard feed-forward networks, with a softmax layer on top for classification."}, {"heading": "3.2 Training", "text": "The ADAN model can be trained end-to-end with standard back-propagation, which we detail in this section. For the two classifiers P and Q parametrized by \u03b8p and \u03b8q respectively, we use the traditional cross-entropy loss, denoted as Lp(y\u0302, y) andLq(y\u0302, y). Lp is the negative log-likelihood of the model P predicting the correct sentiment label, and Lq is that of Q predicting the correct language. We therefore seek the minimum of the following loss functions for P and Q:\nmin \u03b8p Jp(\u03b8p, \u03b8f ) \u2261 E (xi,yi)\nq Lp(P(F(xi; \u03b8f ); \u03b8p), yi) y\nmin \u03b8q Jq(\u03b8q, \u03b8f ) \u2261 E (xi,yi)\nq Lq(Q(F(xi; \u03b8f ); \u03b8q), yi) y\nTo accomplish the aforementioned joint learning of features, the feature extractor F , parameterized by \u03b8f , solves the following optimization problem:\nmin \u03b8f\nJp(\u03b8p, \u03b8f )\u2212 \u03bbJq(\u03b8q, \u03b8f ) (1)\nwhere \u03bb is a hyper-parameter that balances between the two branches P and Q. The intuition is that while P and Q are individually trying to excel at their own classification tasks, F drives its parameters to extract hidden representations that help the sentiment prediction of P and hamper the language prediction of Q. Therefore, upon successful training, F extracts language-invariant features suitable for sentiment analysis.\nThere are two approaches to train ADAN. The first one, inspired by (Goodfellow et al., 2014), performs alternate training: first the sentiment classifier and the feature extractor are trained together, then the adversarial language predictor is trained while the other two are \u201cfrozen\u201d. This method has more control towards convergence when the learning progress of P and Q are not fully in sync. For instance, in (Goodfellow et al., 2014), a hyper-parameter k is introduced to train one component for k iterations then the other for one, which is helpful since their generator learns faster than the discriminator.\nThe other method is to use a Gradient Reversal Layer (GRL) proposed in (Ganin and Lempitsky, 2015). The GRL does nothing during forward propagation, but negates the gradients it receives during backward propagation. Specifically, a GRL R\u03bb with hyper-parameter \u03bb behaves as follows:{\nR\u03bb(x) = x \u2207xR\u03bb(x) = \u2212\u03bbI (2)\nwhere I is the identity matrix. After a GRL is inserted between F and Q, running standard Stochastic Gradient Descent (SGD) on the entire network optimizes for (1).\nIn our experiments, we found that GRL is easier to work with since the entire network can be optimized en masse. On the other hand, we also observed that the training progress of P and Q is not usually in sync and it takes more than one batch of training for the language predictor to adapt to the changes in the joint features. Therefore, the first method\u2019s flexibility in coordinating the training of P and Q is indeed important. Thus, we combine both approaches by adapting the GRL method to perform alternate training. This is achieved by setting \u03bb to a non-zero value only once out of k batches. When \u03bb = 0, the gradients from Q will not be back-propagated to F . This allows Q more iterations to adapt to F before F makes another adversarial update. We set \u03bb = 0.02 and k = 3 in all experiments, which is selected based on the performance on a held-out validation set.\nFor unsupervised cross-lingual sentiment analysis, we have access to labeled English data and unlabeled Chinese data during training. To train the ADAN model, we assemble mini-batches of sam-\nples drawn from both English and Chinese training data with equal probability, and use them to train the ADAN network. However, only the labeled English instances go through the sentiment classifier P during training, while both English and Chinese instances go through the language predictorQ. At test time, to predict the sentiment of a Chinese sentence, we pass it through the trained F and then P ."}, {"heading": "4 Experiments and Discussions", "text": "To demonstrate the effectiveness of our model, we experiment on sentence-level sentiment classification with 5 labels (strongly negative, slightly negative, neutral, slightly positive, and strongly positive)."}, {"heading": "4.1 Data", "text": "Labeled English Data. We use a balanced dataset of 700k Yelp reviews from Zhang et al. (2015) with their ratings as labels (scale 1-5). We also adopt their train-validation split: 650k reviews form the training set and the remaining 50k form a validation set used in English-only benchmarks. Labeled Chinese Data. Since ADAN does not require labeled Chinese data for training, this annotated data is solely used to validate the performance of our model. 10k balanced Chinese hotel reviews from Lin et al. (2015) are used as validation set for model selection and parameter tuning. The results are reported on a separate test set of another 10k hotel reviews. Unlabeled Chinese Data. We use another 150k unlabeled Chinese hotel reviews for training."}, {"heading": "4.2 Chinese Sentiment Classification", "text": "Our main results are shown in Table 1. The left table shows the unsupervised setting where no labeled data is used in Chinese. The DAN+BWE baseline model uses bilingual word embeddings to map both English and Chinese reviews to the same space and is trained using only English Yelp reviews. We can see from Table 1 that the bilingual embedding by itself does not suffice to transfer knowledge of English sentiment classification to Chinese, and the performance is poor (29.11%).\nWe then compares ADAN with domain adaptation baselines, which did not yield satisfactory results for our task. TCA (Pan et al., 2011) did not work since it requires quadratic space in terms of the number of samples (650k in our case). SDA (Glorot et al., 2011) and the subsequent mSDA (Chen et al., 2012) are proven very effective for cross-domain sentiment classification on Amazon reviews. However, as shown in Table 1, mSDA did not perform competitively. We postulate that this is because many domain adaptation models including mSDA were designed for the use of bag-of-words features, which are ill-suited in these tasks where the two languages have completely different vocabularies. We instead use the same input representation as DAN for mSDA, the averaged word vector for each instance, which may violate the underlying assumption of mSDA and thus leading to poor results. In summary, this suggests that even strong domain adaptation algorithms cannot be used out of the box for our task to get satisfactory results.\nOne strong baseline we compare against is English DAN with MT. We use the commercial Google\nTranslate engine2, which is highly engineered, trained on enormous resources, and arguably one of the best MT systems in the world. It translates the Chinese reviews into English and makes predictions using the best-performing DAN model trained on Yelp reviews. Previous studies (Banea et al., 2008; Salameh et al., 2015) on sentiment analysis for Arabic and European languages claim this MT approach to be very competitive and can sometimes match the state-of-the-art system trained on that language. On the other hand, as shown in Table 1, our ADAN model significantly outperforms the MT baseline, vindicating that our adversarial model can successfully perform cross-lingual sentiment analysis without annotated data on the target language.\nSemi-supervised Learning In practice, it is usually not very difficult to obtain at least a little bit of annotated data. ADAN can be readily adapted to exploit such extra labeled data in the target language, by letting those labeled instances pass through the\n2https://translate.google.com\nsentiment classifierP as the English samples do during training. We simulate this semi-supervised scenario by using 1k labeled Chinese reviews for training. As shown in Table 1 (right), ADAN significantly outperforms DAN trained on the combination of the English and Chinese training data."}, {"heading": "4.3 Qualitative Analysis and Visualizations", "text": "To qualitatively demonstrate how ADAN bridges the distributional discrepancies between English and Chinese instances, t-SNE (Van der Maaten and Hinton, 2008) visualizations of the activations at various layers are shown in Figure 2. We randomly select 1000 sentences from the Chinese and English validation sets respectively, and plot the t-SNE of the hidden node activations at three locations in our model: the averaging layer, the end of the joint feature extractor, and the last hidden layer in the sentiment classifier before softmax. The train-on-English model is the DAN+BWE baseline in Table 1. Note that there is actually only one \u201cbranch\u201d in this baseline model, but in order to compare to ADAN, we\nconceptually treat the first three layers as the feature extractor. In addition, t-SNE plot of the bilingual word embeddings of the most frequent 1000 words in both languages are shown in Figure 3.\nIt can be seen from Figure 3 that BWE indeed provide a more or less mixed distribution for words across languages. There are still some dense clusters of monolingual words in the pre-trained BWE, which is slightly alleviated after ADAN training which updates the word embeddings. However, a somehow surprising finding in Figure 2a is the clear dichotomy between the averaged word vectors in the two languages despite the distributions of the word embeddings themselves being mixed (Figure 3). This might suggest that the diversion between languages are not only determined by word semantics, but also largely depends on the way how words are used. Therefore, one needs to look beyond word representations when tackling crosslingual NLP problems. Furthermore, we can see in Figure 2b that the distributional discrepancies between Chinese and English are significantly reduced after passing through the joint feature extractor (F), and the learned feature in ADAN brings the distributions in the two languages dramatically closer compared to the monolingually trained baseline. Finally, when looking at the last hidden layer activations in the sentiment classifier of the baseline model (Figure 2c), there are several notable clusters of the red dots (English data) that roughly correspond to the class labels. However, most Chinese samples are not close to one of those clusters due to the distributional diversion and may thus cause degraded performance in Chinese sentiment classification. On the other hand, the Chinese samples are more in line with English ones in ADAN model, which results in the accuracy boost over the baseline model."}, {"heading": "4.4 Side: English Sentiment Classification", "text": "Although not an objective of this work, we also present that DAN performs well in the pure supervised setting of English sentiment classification. The result indicates that the DAN baselines in our experiments are competitive for English sentiment classification. While Iyyer et al. (2015) tested DAN for sentiment analysis on the smaller Stanford Sentiment Treebank dataset, we also demonstrate its effectiveness for the large Yelp reviews dataset. Ta-\nble 2 compares our DAN with results in (Zhang et al., 2015). DAN beats most of the baselines including LSTM, and is close to the best model in (Zhang et al., 2015), a very large convolutional neural network. For more discussions on the different embedding choices, please refer to Section 4.5."}, {"heading": "4.5 Impact of Bilingual Word Embeddings", "text": "In this section we discuss the effect of the bilingual word embeddings, which we deem as a key factor for future improvement. Table 2 shows that the small dimensionality of the BWE poses a limitation to DAN\u2019s performance on English sentiment classification, especially since there is a large gap between the dimensionality of the embeddings and the hidden layers (50 vs. 900). Even using 300 dimensional random embeddings outperforms the 50d BWE, and is only marginally worse than BWE (50d) + Random 250d (padding the BWE to 300 dimensions). Furthermore, with a better word embedding word2vec (Mikolov et al., 2013), the performance is 2% higher and close to the state of the art.\nNevertheless, when performing cross-lingual training for ADAN, random WE no longer works, as seen in Table 1. This intuitively makes sense because the word representations from both languages are drawn from one identical random distribution, so the adversarial language predictor receives little useful information to distinguish between the underlying language distributions, hence producing rather useless gradients. We can see that adding randomness to the BWE also yields worse performance and the best performing model only uses the 50 dimen-\nsional BWE. However, from the results in Table 2, the accuracy of ADAN could probably be further improved if we can increase the quality or at least the dimensionality of the BWE."}, {"heading": "4.6 Implementation Details", "text": "For all our experiments, the feature extractor F has three fully-connected hidden layers with ReLU nonlinearities, while both P and Q have two. All hidden layers contains 900 hidden units. This choice is more or less ad-hoc, and the performance could potentially be improved with more careful model selection. Batch Normalization (Ioffe and Szegedy, 2015) is used in each hidden layer. We corroborate the observations by Ioffe and Szegedy (2015) that Batch Normalization can sometimes eliminate the need for Dropout (Srivastava et al., 2014) or Word Dropout (Iyyer et al., 2015), which make little difference in our experiments. We use Adam (Kingma and Ba, 2014) with a learning rate of 0.05 for optimization. ADAN is implemented in Torch7 (Collobert et al., 2011), and the code will be made available after the review process.\nTraining is very efficient for ADAN due to its simplicity. It takes less than 6 minutes to finish one epoch (1.3M instances) on one Titan X GPU. We train ADAN for 30 epochs and use early stopping to select the best model on the validation set."}, {"heading": "5 Conclusion and Future Work", "text": "In this work, we presented ADAN, an adversarial deep averaging network for cross-lingual sentiment classification. ADAN leverages the affluent resources on English to help sentiment categorization on other languages where few or no annotated data exist. We validated our hypothesis by empirical experiments on Chinese sentiment categorization, where we have labeled English training data and only unlabeled Chinese data. Experiments show that ADAN outperforms several baselines including domain adaptation models and a highly competitive MT baseline using Google Translate. Furthermore, we showed that in the presence of labeled data in the target language, ADAN can naturally incorporate this additional supervision and yields even more competitive results.\nFor future work, one direction is to explore better bilingual word embeddings, which we identify as one key factor limiting the performance of ADAN. In another direction, our adversarial framework for cross-lingual text categorization can be applied not only to DAN, but also many other neural models such as LSTM, etc. Further, our framework is not limited to text classification tasks, and can be extended to, for instance, phrase level opinion mining (I\u0307rsoy and Cardie, 2014b) by extracting phrase-level opinion expressions from sentences using deep recurrent neural networks. Our framework can be applied to these phrase-level models for languages where labeled data might not exist."}], "references": [{"title": "Domain-adversarial neural networks", "author": ["Ajakan et al.2014] Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand"], "venue": "In Second Workshop on transfer and Multi-Task Learning (NIPS", "citeRegEx": "Ajakan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ajakan et al\\.", "year": 2014}, {"title": "Multilingual subjectivity analysis using machine translation", "author": ["Banea et al.2008] Carmen Banea", "Rada Mihalcea", "Janyce Wiebe", "Samer Hassan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Banea et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2008}, {"title": "Multilingual subjectivity: Are more languages better", "author": ["Banea et al.2010] Carmen Banea", "Rada Mihalcea", "Janyce Wiebe"], "venue": "In Proceedings of the 23rd international conference on computational linguistics,", "citeRegEx": "Banea et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Banea et al\\.", "year": 2010}, {"title": "Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification", "author": ["Blitzer et al.2007] John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguis-", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha"], "venue": "In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learn-", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF192376", "author": ["Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Soumith Chintala", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Ganin", "Lempitsky2015] Yaroslav Ganin", "Victor Lempitsky"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Ganin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ganin et al\\.", "year": 2015}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 28th International Con-", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Generative adversarial nets", "author": ["Jean PougetAbadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Exploring english lexicon knowledge for chinese sentiment analysis", "author": ["He et al.2010] Yulan He", "Harith Alani", "Deyu Zhou"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2010\\E", "shortCiteRegEx": "He et al\\.", "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Szegedy2015] Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["\u0130rsoy", "Cardie2014a] Ozan \u0130rsoy", "Claire Cardie"], "venue": null, "citeRegEx": "\u0130rsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "\u0130rsoy et al\\.", "year": 2014}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["\u0130rsoy", "Cardie2014b] Ozan \u0130rsoy", "Claire Cardie"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "\u0130rsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "\u0130rsoy et al\\.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "An empirical study on sentiment classification of chinese review using word embedding", "author": ["Lin et al.2015] Yiou Lin", "Hang Lei", "Jia Wu", "Xiaoyu Li"], "venue": "In Proceedings of the 29th Pacific Asia Conference on Language,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Joint bilingual sentiment classification with unlabeled parallel corpora", "author": ["Lu et al.2011] Bin Lu", "Chenhao Tan", "Claire Cardie", "Benjamin K Tsou"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Lu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2011}, {"title": "Learning multilingual subjective language via cross-lingual projections", "author": ["Carmen Banea", "Janyce M Wiebe"], "venue": null, "citeRegEx": "Mihalcea et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2007}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Domain adaptation via transfer component analysis", "author": ["Pan et al.2011] Sinno Jialin Pan", "Ivor W Tsang", "James T Kwok", "Qiang Yang"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Sentiment after translation: A case-study on arabic social media posts", "author": ["Saif Mohammad", "Svetlana Kiritchenko"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Salameh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salameh et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "An empirical study of sentiment analysis for chinese documents", "author": ["Tan", "Zhang2008] Songbo Tan", "Jin Zhang"], "venue": "Expert Systems with Applications,", "citeRegEx": "Tan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2008}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Bilingual word embeddings from nonparallel document-aligned data applied to bilingual lexicon induction", "author": ["Vuli\u0107", "Moens2015] Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Using bilingual knowledge and ensemble techniques for unsupervised chinese sentiment analysis", "author": ["Xiaojun Wan"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Wan.,? \\Q2008\\E", "shortCiteRegEx": "Wan.", "year": 2008}, {"title": "Co-training for crosslingual sentiment classification", "author": ["Xiaojun Wan"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume", "citeRegEx": "Wan.,? \\Q2009\\E", "shortCiteRegEx": "Wan.", "year": 2009}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 25, "context": "There has been significant progress on English sentence- and document-level sentiment classification in recent years using models based on neural networks (Socher et al., 2013; \u0130rsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015).", "startOffset": 155, "endOffset": 261}, {"referenceID": 27, "context": "There has been significant progress on English sentence- and document-level sentiment classification in recent years using models based on neural networks (Socher et al., 2013; \u0130rsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015).", "startOffset": 155, "endOffset": 261}, {"referenceID": 15, "context": "There has been significant progress on English sentence- and document-level sentiment classification in recent years using models based on neural networks (Socher et al., 2013; \u0130rsoy and Cardie, 2014a; Le and Mikolov, 2014; Tai et al., 2015; Iyyer et al., 2015).", "startOffset": 155, "endOffset": 261}, {"referenceID": 25, "context": "Most of these, however, rely on a massive amount of labeled training data or fine-grained annotations such as the Stanford Sentiment Treebank (Socher et al., 2013), which provides sentiment annotations for each phrase in the parse tree of every sentence.", "startOffset": 142, "endOffset": 163}, {"referenceID": 31, "context": "Although some prior work tries to alleviate the scarcity of sentiment annotations by leveraging labeled English data (Wan, 2008; Wan, 2009; Lu et al., 2011), these methods rely on external knowledge such as bilingual lexicons or machine translation (MT) systems, both of which are difficult and expensive to obtain.", "startOffset": 117, "endOffset": 156}, {"referenceID": 32, "context": "Although some prior work tries to alleviate the scarcity of sentiment annotations by leveraging labeled English data (Wan, 2008; Wan, 2009; Lu et al., 2011), these methods rely on external knowledge such as bilingual lexicons or machine translation (MT) systems, both of which are difficult and expensive to obtain.", "startOffset": 117, "endOffset": 156}, {"referenceID": 19, "context": "Although some prior work tries to alleviate the scarcity of sentiment annotations by leveraging labeled English data (Wan, 2008; Wan, 2009; Lu et al., 2011), these methods rely on external knowledge such as bilingual lexicons or machine translation (MT) systems, both of which are difficult and expensive to obtain.", "startOffset": 117, "endOffset": 156}, {"referenceID": 0, "context": ", 2014) and visual domain adaptation (Ganin and Lempitsky, 2015; Ajakan et al., 2014).", "startOffset": 37, "endOffset": 85}, {"referenceID": 15, "context": "neural networks (Iyyer et al., 2015).", "startOffset": 16, "endOffset": 36}, {"referenceID": 21, "context": "For each document, DAN takes the arithmetic mean of the document word vectors (Mikolov et al., 2013; Pennington et al., 2014) as input, and", "startOffset": 78, "endOffset": 125}, {"referenceID": 23, "context": "For each document, DAN takes the arithmetic mean of the document word vectors (Mikolov et al., 2013; Pennington et al., 2014) as input, and", "startOffset": 78, "endOffset": 125}, {"referenceID": 20, "context": "Cross-lingual Sentiment Analysis (Mihalcea et al., 2007; Banea et al., 2008; Banea et al., 2010) is motivated by the lack of high-quality labeled data in", "startOffset": 33, "endOffset": 96}, {"referenceID": 1, "context": "Cross-lingual Sentiment Analysis (Mihalcea et al., 2007; Banea et al., 2008; Banea et al., 2010) is motivated by the lack of high-quality labeled data in", "startOffset": 33, "endOffset": 96}, {"referenceID": 2, "context": "Cross-lingual Sentiment Analysis (Mihalcea et al., 2007; Banea et al., 2008; Banea et al., 2010) is motivated by the lack of high-quality labeled data in", "startOffset": 33, "endOffset": 96}, {"referenceID": 31, "context": "For Chinese in particular, there have been several representative works in both the machine learning direction (Wan, 2008; Wan, 2009; Lu et al., 2011) and the more traditional lexical direction (He et al.", "startOffset": 111, "endOffset": 150}, {"referenceID": 32, "context": "For Chinese in particular, there have been several representative works in both the machine learning direction (Wan, 2008; Wan, 2009; Lu et al., 2011) and the more traditional lexical direction (He et al.", "startOffset": 111, "endOffset": 150}, {"referenceID": 19, "context": "For Chinese in particular, there have been several representative works in both the machine learning direction (Wan, 2008; Wan, 2009; Lu et al., 2011) and the more traditional lexical direction (He et al.", "startOffset": 111, "endOffset": 150}, {"referenceID": 11, "context": ", 2011) and the more traditional lexical direction (He et al., 2010).", "startOffset": 51, "endOffset": 68}, {"referenceID": 19, "context": "Lu et al. (2011) instead uses labeled data from both languages to im-", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "Domain Adaptation Blitzer et al. (2007), Glorot et al.", "startOffset": 18, "endOffset": 40}, {"referenceID": 3, "context": "Domain Adaptation Blitzer et al. (2007), Glorot et al. (2011) and Chen et al.", "startOffset": 18, "endOffset": 62}, {"referenceID": 3, "context": "Domain Adaptation Blitzer et al. (2007), Glorot et al. (2011) and Chen et al. (2012) try to learn effective classifiers for which the training and test samples are from different underlying distributions.", "startOffset": 18, "endOffset": 85}, {"referenceID": 9, "context": "Adversarial Networks (Goodfellow et al., 2014; Ganin and Lempitsky, 2015) have enjoyed much success in computer vision, but to the best of our knowledge, have not yet been applied in NLP with comparable success.", "startOffset": 21, "endOffset": 73}, {"referenceID": 9, "context": "chitectures similar to ours, by pitching a neural image generator against a discriminator that learns to classify real versus generated images (Goodfellow et al., 2014; Denton et al., 2015).", "startOffset": 143, "endOffset": 189}, {"referenceID": 6, "context": "chitectures similar to ours, by pitching a neural image generator against a discriminator that learns to classify real versus generated images (Goodfellow et al., 2014; Denton et al., 2015).", "startOffset": 143, "endOffset": 189}, {"referenceID": 21, "context": "Each word w \u2208 x is represented by its word embedding vw (Mikolov et al., 2013).", "startOffset": 56, "endOffset": 78}, {"referenceID": 34, "context": "Prior work on bilingual word embeddings (Zou et al., 2013; Vuli\u0107 and Moens, 2015; Gouws et al., 2015) attempts to induce distributed word representations that encode semantic relatedness between words across languages, so that similar words are closer in the embedded space regardless of language.", "startOffset": 40, "endOffset": 101}, {"referenceID": 10, "context": "Prior work on bilingual word embeddings (Zou et al., 2013; Vuli\u0107 and Moens, 2015; Gouws et al., 2015) attempts to induce distributed word representations that encode semantic relatedness between words across languages, so that similar words are closer in the embedded space regardless of language.", "startOffset": 40, "endOffset": 101}, {"referenceID": 34, "context": "word embeddings (BWE) in (Zou et al., 2013).", "startOffset": 25, "endOffset": 43}, {"referenceID": 34, "context": "Note that using the word embeddings in (Zou et al., 2013) makes our work implic-", "startOffset": 39, "endOffset": 57}, {"referenceID": 15, "context": "The feature extractor F is a Deep Averaging Network (DAN) (Iyyer et al., 2015).", "startOffset": 58, "endOffset": 78}, {"referenceID": 9, "context": "The first one, inspired by (Goodfellow et al., 2014), performs alternate training: first the sentiment classifier and the feature extractor are trained together, then the", "startOffset": 27, "endOffset": 52}, {"referenceID": 9, "context": "For instance, in (Goodfellow et al., 2014), a hyper-parameter k", "startOffset": 17, "endOffset": 42}, {"referenceID": 4, "context": "11% mSDA (Chen et al., 2012) 31.", "startOffset": 9, "endOffset": 28}, {"referenceID": 33, "context": "We use a balanced dataset of 700k Yelp reviews from Zhang et al. (2015) with their ratings as labels (scale 1-5).", "startOffset": 52, "endOffset": 72}, {"referenceID": 18, "context": "10k balanced Chinese hotel reviews from Lin et al. (2015) are used as validation set for model selection and parameter tuning.", "startOffset": 40, "endOffset": 58}, {"referenceID": 22, "context": "TCA (Pan et al., 2011) did not work since it requires quadratic space in terms of the number", "startOffset": 4, "endOffset": 22}, {"referenceID": 8, "context": "SDA (Glorot et al., 2011) and the subsequent mSDA (Chen et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 4, "context": ", 2011) and the subsequent mSDA (Chen et al., 2012) are proven very effective for cross-domain sentiment classification on Amazon reviews.", "startOffset": 32, "endOffset": 51}, {"referenceID": 1, "context": "Previous studies (Banea et al., 2008; Salameh et al., 2015) on sentiment analysis for Arabic and European languages claim this MT approach to be very competitive and can sometimes match the state-of-the-art system trained on that language.", "startOffset": 17, "endOffset": 59}, {"referenceID": 24, "context": "Previous studies (Banea et al., 2008; Salameh et al., 2015) on sentiment analysis for Arabic and European languages claim this MT approach to be very competitive and can sometimes match the state-of-the-art system trained on that language.", "startOffset": 17, "endOffset": 59}, {"referenceID": 15, "context": "While Iyyer et al. (2015) tested DAN for sentiment analysis on the smaller Stanford Sen-", "startOffset": 6, "endOffset": 26}, {"referenceID": 33, "context": "\u2020 Results from (Zhang et al., 2015).", "startOffset": 15, "endOffset": 35}, {"referenceID": 33, "context": "ble 2 compares our DAN with results in (Zhang et al., 2015).", "startOffset": 39, "endOffset": 59}, {"referenceID": 33, "context": "DAN beats most of the baselines including LSTM, and is close to the best model in (Zhang et al., 2015), a very large convolutional neural network.", "startOffset": 82, "endOffset": 102}, {"referenceID": 21, "context": "word2vec (Mikolov et al., 2013), the performance is 2% higher and close to the state of the art.", "startOffset": 9, "endOffset": 31}, {"referenceID": 34, "context": "embeddings from (Zou et al., 2013), and (b) is the updated embeddings in the trained ADAN model.", "startOffset": 16, "endOffset": 34}, {"referenceID": 26, "context": "We corroborate the observations by Ioffe and Szegedy (2015) that Batch Normalization can sometimes eliminate the need for Dropout (Srivastava et al., 2014) or Word Dropout (Iyyer et al.", "startOffset": 130, "endOffset": 155}, {"referenceID": 15, "context": ", 2014) or Word Dropout (Iyyer et al., 2015), which make little difference in our experiments.", "startOffset": 24, "endOffset": 44}, {"referenceID": 5, "context": "ADAN is implemented in Torch7 (Collobert et al., 2011), and the code will be made avail-", "startOffset": 30, "endOffset": 54}], "year": 2017, "abstractText": "In recent years deep neural networks have achieved great success in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most other languages do not enjoy such an abundance of annotated data for sentiment analysis. To combat this problem, we propose the Adversarial Deep Averaging Network (ADAN) to transfer sentiment knowledge learned from labeled English data to lowresource languages where only unlabeled data exists. ADAN is a \u201cY-shaped\u201d network with two discriminative branches: a sentiment classifier and an adversarial language predictor. Both branches take input from a feature extractor that aims to learn hidden representations that capture the underlying sentiment of the text and are invariant across languages. Experiments on Chinese sentiment classification demonstrate that ADAN significantly outperforms several baselines, including a strong pipeline approach that relies on Google Translate, the state-of-the-art commercial machine translation system.", "creator": "LaTeX with hyperref package"}}}