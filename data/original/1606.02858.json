{"id": "1606.02858", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.", "histories": [["v1", "Thu, 9 Jun 2016 08:19:16 GMT  (93kb,D)", "http://arxiv.org/abs/1606.02858v1", "ACL 2016"], ["v2", "Mon, 8 Aug 2016 21:21:19 GMT  (247kb,D)", "http://arxiv.org/abs/1606.02858v2", "ACL 2016, updated results"]], "COMMENTS": "ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["danqi chen", "jason bolton", "christopher d manning"], "accepted": true, "id": "1606.02858"}, "pdf": {"name": "1606.02858.pdf", "metadata": {"source": "CRF", "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "authors": ["Danqi Chen", "Christopher D. Manning"], "emails": ["danqi@cs.stanford.edu", "jebolton@cs.stanford.edu", "manning@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Reading comprehension (RC) is the ability to read text, process it, and understand its meaning.2 How to endow computers with this capacity has been an elusive challenge and a long-standing goal of Artificial Intelligence (e.g., (Norvig, 1978)). Genuine reading comprehension involves interpretation of\n1Our code is available at https://github.com/ danqi/rc-cnn-dailymail.\n2https://en.wikipedia.org/wiki/ Reading_comprehension\nthe text and making complex inferences. Human reading comprehension is often tested by asking questions that require interpretive understanding of a passage, and the same approach has been suggested for testing computers (Burges, 2013).\nIn recent years, there have been several strands of work which attempt to collect human-labeled data for this task \u2013 in the form of document, question and answer triples \u2013 and to learn machine learning models directly from it (Richardson et al., 2013; Berant et al., 2014; Wang et al., 2015). However, these datasets consist of only hundreds of documents, as the labeled examples usually require considerable expertise and neat design, making the annotation process quite expensive. The subsequent scarcity of labeled examples prevents us from training powerful statistical models, such as deep learning models, and would seem to prevent a system from learning complex textual reasoning capacities.\nRecently, researchers at DeepMind (Hermann et al., 2015) had the appealing, original idea of exploiting the fact that the abundant news articles of CNN and Daily Mail are accompanied by bullet point summaries in order to heuristically create large-scale supervised training data for the reading comprehension task. Figure 1 gives an example. Their idea is that a bullet point usually summarizes one or several aspects of the article. If the computer understands the content of the article, it should be able to infer the missing entity in the bullet point.\nThis is a clever way of creating supervised data cheaply and holds promise for making progress on training RC models; however, it is unclear what level of reading comprehension is actually needed to solve this somewhat artificial task and, indeed, what statistical models that do reasonably well on this task have actually learned.\nIn this paper, our aim is to provide an in-depth and thoughtful analysis of this dataset and what\nar X\niv :1\n60 6.\n02 85\n8v 1\n[ cs\n.C L\n] 9\nJ un\n2 01\n6\nlevel of natural language understanding is needed to do well on it. We demonstrate that simple, carefully designed systems can obtain high, state-ofthe-art accuracies of 72.4% and 75.8% on CNN and Daily Mail respectively. We do a careful hand-analysis of a small subset of the problems to provide data on their difficulty and what kinds of language understanding are needed to be successful and we try to diagnose what is learned by the systems that we have built. We conclude that: (i) this dataset is easier than previously realized, (ii) straightforward, conventional NLP systems can do much better on it than previously suggested, (iii) the distributed representations of deep learning systems are very effective at recognizing paraphrases, (iv) partly because of the nature of the questions, current systems much more have the nature of single-sentence relation extraction systems than larger-discourse-context text understanding systems, (v) the systems that we present here are close to the ceiling of performance for singlesentence and unambiguous cases of this dataset, and (vi) the prospects for getting the final 20% of questions correct appear poor, since most of them involve issues in the data preparation which undermine the chances of answering the question (coreference errors or anonymization of entities making understanding too difficult)."}, {"heading": "2 The Reading Comprehension Task", "text": "The RC datasets introduced in (Hermann et al., 2015) are made from articles on the news websites CNN and Daily Mail, utilizing articles and their bullet point summaries.3 Figure 1 demonstrates\n3The datasets are available at https://github.com/ deepmind/rc-data.\nan example4: it consists of a passage p, a question q and an answer a, where the passage is a news article, the question is a cloze-style task, in which one of the article\u2019s bullet points has had one entity replaced by a placeholder, and the answer is this questioned entity. The goal is to infer the missing entity (answer a) from all the possible entities which appear in the passage. A news article is usually associated with a few (e.g., 3\u20135) bullet points and each of them highlights one aspect of its content.\nThe text has been run through a Google NLP pipeline. It it tokenized, lowercased, and named entity recognition and coreference resolution have been run. For each coreference chain containing at least one named entity, all items in the chain are replaced by an @entityn marker, for a distinct index n. Hermann et al. (2015) argue convincingly that such a strategy is necessary to ensure that systems approach this task by understanding the passage in front of them, rather than by using world knowledge or a language model to answer questions without needing to understand the passage. However, this also gives the task a somewhat artificial character. On the one hand, systems are greatly helped by entity recognition and coreference having already been performed; on the other, they suffer when either of these modules fail, as they do (in Figure 1, \u201cthe character\u201d should probably be coreferent with @entity14; clearer examples of failure appear later on in our data analysis). Moreover, this inability to use world knowledge also makes it much more difficult for a human to do this task \u2013 occasionally it is very difficult or impossible for a human to determine the correct answer when presented with an item anonymized in this way.\nThe creation of the datasets benefits from the sheer volume of news articles available online, so they offer a large and realistic testing ground for statistical models. Table 1 provides some statistics on the two datasets: there are 380k and 879k training examples for CNN and Daily Mail respectively. The passages are around 30 sentences and 800 tokens on average, while each question contains around 12\u201314 tokens.\nIn the following sections, we seek to more deeply understand the nature of this dataset. We first build some straightforward systems in order to get a better idea of a lower-bound for the performance of\n4The original article can be found at http: //www.cnn.com/2015/03/10/entertainment/ feat-star-wars-gay-character/.\ncurrent NLP systems. Then we turn to data analysis of a sample of the items to examine their nature and an upper bound on performance."}, {"heading": "3 Our Systems", "text": "In this section, we describe two systems we implemented \u2013 a conventional entity-centric classifier and an end-to-end neural network. While Hermann et al. (2015) do provide several baselines for performance on the RC task, we suspect that their baselines are not that strong. They attempt to use a frame-semantic parser, and we feel that the poor coverage of that parser undermines the results, and is not representative of what a straightforward NLP system \u2013 based on standard approaches to factoid question answering and relation extraction developed over the last 15 years \u2013 can achieve. Indeed, their frame-semantic model is markedly inferior to another baseline they provide, a heuristic word distance model. At present just two papers are available presenting results on this RC task, both presenting neural network approaches: (Hermann et al., 2015) and (Hill et al., 2016). While the latter is wrapped in the language of end-to-end memory networks, it actually presents a fairly simple window-based neural network classifier running on the CNN data. Its success again raises questions about the true nature and complexity of the RC task provided by this dataset, which we seek to clarify by building a simple attention-based neural net classifier.\nGiven the (passage, question, answer) triple (p, q, a), p = {p1, . . . , pm} and q = {q1, . . . , ql} are sequences of tokens for the passage and\nquestion sentence, with q containing exactly one \u201c@placeholder\u201d token. The goal is to infer the correct entity a \u2208 p \u2229 E that the placeholder corresponds to, where E is the set of all abstract entity markers. Note that the correct answer entity must appear in the passage p."}, {"heading": "3.1 Entity-Centric Classifier", "text": "We first build a conventional feature-based classifier, aiming to explore what features are effective for this task. This is similar in spirit to (Wang et al., 2015), which at present has very competitive performance on the MCTest RC dataset (Richardson et al., 2013). The setup of this system is to design a feature vector fp,q(e) for each candidate entity e, and to learn a weight vector \u03b8 such that the correct answer a is expected to rank higher than all other candidate entities:\n\u03b8\u1d40fp,q(a) > \u03b8 \u1d40fp,q(e), \u2200e \u2208 E \u2229 p \\ {a} (1)\nWe employ the following feature templates:\n1. Whether entity e occurs in the passage.\n2. Whether entity e occurs in the question.\n3. The frequency of entity e in the passage.\n4. The first position of occurence of entity e in the passage.\n5. n-gram exact match: whether there is an exact match between the text surrounding the placeholder and the text surrounding entity e. We have features for all combinations of matching left and/or right one or two words.\n6. Word distance: we align the placeholder with each occurrence of entity e, and compute the average minimum distance of each non-stop question word from the entity in the passage.\n7. Sentence co-occurrence: whether entity e cooccurs with another entity or verb that appears in the question, in some sentence of the passage.\n8. Dependency parse match: we dependency parse both the question and all the sentences in the passage, and extract an indicator feature of whether w r\u2212\u2192 @placeholder and w r\u2212\u2192 e are both found; similar features are constructed for @placeholder r\u2212\u2192 w and e r\u2212\u2192 w."}, {"heading": "3.2 End-to-end Neural Network", "text": "Our neural network system is based on the AttentiveReader model proposed by (Hermann et al., 2015). The framework can be described in the following three steps (see Figure 2):\nEncoding: First, all the words are mapped to ddimensional vectors via an embedding matrix E \u2208 Rd\u00d7|V|; therefore we have p: p1, . . . ,pm \u2208 Rd and q : q1, . . . ,ql \u2208 Rd. Next we use a shallow bi-directional LSTM with hidden size h\u0303 to encode contextual embeddings p\u0303i of each word in the passage, \u2212\u2192 h i = LSTM( \u2212\u2192 h i\u22121,pi), i = 1, . . . ,m\n\u2190\u2212 h i = LSTM( \u2190\u2212 h i+1,pi), i = m, . . . , 1\nand p\u0303i = concat( \u2212\u2192 h i, \u2190\u2212 h i) \u2208 Rh, where h = 2h\u0303. Meanwhile, we use another bi-directional LSTM to map the question q1, . . . ,ql to an embedding q \u2208 Rh.\nAttention: In this step, the goal is to compare the question embedding and all the contextual embeddings, and select the pieces of information that are relevant to the question. We compute a probability distribution \u03b1 depending on the degree of relevance between word pi (in its context) and the question q and then produce an output vector o which is a weighted combination of all contextual embeddings {p\u0303i}:\n\u03b1i = softmaxi q \u1d40Wsp\u0303i (2) o = \u2211\ni \u03b1ip\u0303i (3)\nWs \u2208 Rh\u00d7h is used in a bilinear term, which allows us to compute a similarity between q and p\u0303i more flexibly than with just a dot product.\nPrediction: Using the output vector o, the system outputs the most likely answer using:\na = argmaxa\u2208p\u2229EW \u1d40 a o (4)\nFinally, the system adds a softmax function on top of W \u1d40a o and adopts a negative loglikelihood objective for training.\nDifferences from (Hermann et al., 2015). Our model basically follows the AttentiveReader. However, to our surprise, our experiments observed nearly 8\u201310% improvement over the original AttentiveReader results on CNN and Daily Mail datasets (discussed in Sec. 4). Concretely, our model has the following differences:\n\u2022 We use a bilinear term, instead of a tanh layer to compute the relevance (attention) between question and contextual embeddings. The effectiveness of the simple bilinear attention function has been shown previously for neural machine translation by (Luong et al., 2015).\n\u2022 After obtaining the weighted contextual embeddings o, we use o for direct prediction. In contrast, the original model in (Hermann et al., 2015) combined o and the question embedding q via another non-linear layer before making final predictions. We found that we could remove this layer without harming performance. We believe it is sufficient for the model to learn to return the entity to which it maximally gives attention.\n\u2022 The original model considers all the words from the vocabulary V in making predictions. We think this is unnecessary, and only predict among entities which appear in the passage.\nOf these changes, only the first seems important; the other two just aim at keeping the model simple.\nWindow-based MemN2Ns (Hill et al., 2016). Another recent neural network approach proposed by (Hill et al., 2016) is based on a memory network architecture (Weston et al., 2015). We think it is highly similar in spirit. The biggest difference is their way of encoding passages: they demonstrate that it is most effective to only use a 5-word context window when evaluating a candidate entity and they use a positional unigram approach to encode the contextual embeddings: if a window consists of 5 words x1, . . . , x5, then it is encoded as \u22115 i=1Ei(xi), resulting in 5 separate embedding matrices to learn. They encode the 5-word window surrounding the placeholder in a similar way and all other words in the question text are ignored. In addition, they simply use a dot product to compute the \u201crelevance\u201d between the question and a contextual embedding. This simple model nevertheless works well, showing the extent to which this RC task can be done by very local context matching."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Training Details", "text": "For training our conventional classifier, we use the implementation of LambdaMART (Wu et al., 2010) in the RankLib package.5 We use this ranking algorithm since our problem is naturally a ranking problem and forests of boosted decision trees have been very successful lately (as seen, e.g., in many recent Kaggle competitions). We do not use all the features of LambdaMART since we are only scoring 1/0 loss on the first ranked proposal, rather than using an IR-style metric to score ranked results. We use Stanford\u2019s neural network dependency parser (Chen and Manning, 2014) to parse all our document and question text, and all other features can be extracted without additional tools.\nFor training our neural networks, we only keep the most frequent |V| = 50k words (including entity and placeholder markers), and map all other words to an \u00a1unk\u00bf token. We choose word embedding size d = 100, and use the 100-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014) for initialization. The attention and output parameters are initialized from a uniform distribution between (\u22120.01, 0.01), and the LSTM weights are initialized from a Gaussian distribution N (0, 0.1).\n5https://sourceforge.net/p/lemur/wiki/ RankLib/.\nWe use hidden size h = 128 for CNN and 256 for Daily Mail. Optimization is carried out using vanilla stochastic gradient descent (SGD), with a fixed learning rate of 0.1. We sort all the examples by the length of its passage, and randomly sample a mini-batch of size 32 for each update. We also apply dropout with probability 0.2 to the embedding layer and gradient clipping when the norm of gradients exceeds 10.\nAll of our models are run on a single GPU (GeForce GTX TITAN X), with roughly a runtime of 6 hours per epoch for CNN, and 15 hours per epoch for Daily Mail. We run all the models up to 30 epochs and select the model that achieves the best accuracy on the development set."}, {"heading": "4.2 Main Results", "text": "Table 2 presents our main results. The conventional feature-based classifier obtains 67.9% accuracy on the CNN test set. Not only does this significantly outperform any of the symbolic approaches reported in (Hermann et al., 2015), it also outperforms all the neural network systems from their paper and the best single-system result reported so far from (Hill et al., 2016). This suggests that the task might not be as difficult as suggested, and a simple feature set can cover many of the cases. Table 3 presents a feature ablation analysis of our entity-centric classifier on the development portion of the CNN dataset. It shows that n-gram match and frequency of entities are the two most important classes of features.\nMore dramatically, our single-model neural net-\nwork surpasses the previous results by a large margin (over 5%), pushing up the state-of-the-art accuracies to 72.4% and 75.8% respectively. Due to resource constraints, we have not had a chance to investigate ensembles of models, which generally can bring further gains, as demonstrated in (Hill et al., 2016) and many other papers.\nConcurrently with our paper, Kadlec et al. (2016) and Kobayashi et al. (2016) also experiment on these two datasets and report competitive results. However, our single model not only still outperforms theirs, but also appears to be structurally simpler. All these recent efforts converge to similar numbers, and we believe that they are approaching the ceiling performance of this task, as we will indicate in the next section."}, {"heading": "5 Data Analysis", "text": "So far, we have good results via either of our systems. In this section, we aim to conduct an indepth analysis and answer the following questions: (i) Since the dataset was created in an automatic and heuristic way, how many of the questions are trivial to answer, and how many are noisy and not answerable? (ii) What have these models learned? What are the prospects for further improving them? To study this, we randomly sampled 100 examples from the dev portion of the CNN dataset for analysis (see more details in Appendix A)."}, {"heading": "5.1 Breakdown of the Examples", "text": "After carefully analyzing these 100 examples, we roughly classify them into the following categories (if an example satisfies more than one category, we classify it into the earliest one):\nExact match The nearest words around the placeholder are also found in the passage surrounding an entity marker; the answer is selfevident.\nSentence-level paraphrasing The question text is entailed/rephrased by exactly one sentence in the passage, so the answer can definitely be identified from that sentence.\nPartial clue In many cases, even though we cannot find a complete semantic match between the question text and some sentence, we are still able to infer the answer through partial clues, such as some word/concept overlap.\nMultiple sentences It requires processing multiple sentences to infer the correct answer.\nCoreference errors It is unavoidable that there are many coreference errors in the dataset. This category includes those examples with critical coreference errors for the answer entity or key entities appearing in the question. Basically we treat this category as \u201cnot answerable\u201d.\nCategory Question Passage\nExact Match\nit \u2019s clear @entity0 is leaning toward @placeholder , says an expert who monitors @entity0 . . . @entity116 , who follows @entity0 \u2019s operations and propaganda closely , recently told @entity3 , it \u2019s clear @entity0 is leaning toward @entity60 in terms of doctrine , ideology and an emphasis on holding territory after operations . . . .\nParaphrase\n@placeholder says he understands why @entity0 wo n\u2019t play at his tournament . . . @entity0 called me personally to let me know that he would n\u2019t be playing here at @entity23 , \u201d @entity3 said on his @entity21 event \u2019s website . . . .\nPartial clue\na tv movie based on @entity2 \u2019s book @placeholder casts a @entity76 actor as @entity5 . . . to @entity12 @entity2 professed that his @entity11 is not a religious book . . . .\nMultiple sent. he \u2019s doing a his - and - her duet all by himself , @entity6 said of @placeholder . . . we got some groundbreaking performances , here too , tonight , @entity6 said . we got @entity17 , who will be doing some musical performances . he \u2019s doing a his - and - her duet all by himself . . . .\nCoref. Error\nrapper @placeholder \u201d disgusted , \u201d cancels upcoming show for @entity280 . . . with hip - hop star @entity246 saying on @entity247 that he was canceling an upcoming show for the @entity249 . . . . (but @entity249 = @entity280 = SAEs)\nHard pilot error and snow were reasons stated for @placeholder plane crash . . . a small aircraft carrying @entity5 , @entity6 and @entity7 the @entity12 @entity3 crashed a few miles from @entity9 , near @entity10 , @entity11 . . . .\nTable 4: Some representative examples from each category.\nAmbiguous or very hard This category includes examples for which we think humans are not able to obtain the correct answer (confidently).\nTable 5 provides our estimate of the percentage for each category, and Table 4 presents one representative example from each category. To our surprise, \u201ccoreference errors\u201d and \u201cambiguous/hard\u201d cases account for 25% of this sample set, based on our manual analysis, and this certainly will be a barrier for training models with an accuracy much\nabove 75% (although, of course, a model can sometimes make a lucky guess). Additionally, only 2 examples require multiple sentences for inference \u2013 this is a lower rate than we expected and Hermann et al. (2015) suggest. Therefore, we hypothesize that in most of the \u201canswerable\u201d cases, the goal is to identify the most relevant (single) sentence, and then to infer the answer based upon it."}, {"heading": "5.2 Per-category Performance", "text": "Now, we further analyze the predictions of our two systems, based on the above categorization.\nAs seen in Table 6, we have the following observations: (i) The exact-match cases are quite simple and both systems get 100% correct. (ii) For the ambiguous/hard and entity-linking-error cases, meeting our expectations, both of the systems perform poorly. (iii) The two systems mainly differ in paraphrasing cases, and some of the \u201cpartial clue\u201d cases. This clearly shows how neural networks are better capable of learning semantic matches involving paraphrasing or lexical variation between the two sentences. (iv) We believe that the neural-net system already achieves near-optimal performance\non all the single-sentence and unambiguous cases. There does not seem to be much useful headroom for exploring more sophisticated natural language understanding approaches on this dataset."}, {"heading": "6 Related Tasks", "text": "We briefly survey other tasks related to reading comprehension.\nMCTest (Richardson et al., 2013) is an opendomain reading comprehension task, in the form of fictional short stories, accompanied by multiplechoice questions. It was carefully created using crowd sourcing, and aims at a 7-year-old reading comprehension level.\nOn the one hand, this dataset has a high demand on various reasoning capacities: over 50% of the questions require multiple sentences to answer and also the questions come in assorted categories (what, why, how, whose, which, etc). On the other hand, the full dataset has only 660 paragraphs in total (each paragraph is associated with 4 questions), which renders training statistical models (especially complex ones) very difficult.\nUp to now, the best solutions (Sachan et al., 2015; Wang et al., 2015) are still heavily relying on manually curated syntactic/semantic features, with the aid of additional knowledge (e.g., word embeddings, lexical/paragraph databases).\nChildren Book Test (Hill et al., 2016) was developed in a similar spirit to the CNN/Daily Mail datasets. It takes any consecutive 21 sentences from a children\u2019s book \u2013 the first 20 sentences are used as the passage, and the goal is to infer a missing word in the 21st sentence (question and answer). The questions are also categorized by the type of the missing word: named entity, common noun, preposition or verb. According to the first study on this dataset (Hill et al., 2016), a language\nmodel (an n-gram model or a recurrent neural network) with local context is sufficient for predicting verbs or prepositions; however, for named entities or common nouns, it improves performance to scan through the whole paragraph to make predictions. So far, the best published results are reported by window-based memory networks.\nbAbI (Weston et al., 2016) is a collection of artificial datasets, consisting of 20 different reasoning types. It encourages the development of models with the ability to chain reasoning, induction/ deduction, etc., so that they can answer a question like \u201cThe football is in the playground\u201d after reading a sequence of sentences \u201cJohn is in the playground; Bob is in the office; John picked up the football; Bob went to the kitchen.\u201d Various types of memory networks (Sukhbaatar et al., 2015; Kumar et al., 2016) have been shown effective on these tasks, and Lee et al. (2016) show that vector space models based on extensive problem analysis can obtain near-perfect accuracies on all the categories. Despite these promising results, this dataset is limited to a small vocabulary (only 100\u2013200 words) and simple language variations, so there is still a huge gap from real-world datasets that we need to fill in."}, {"heading": "7 Conclusion", "text": "In this paper, we carefully examined the recent CNN/Daily Mail reading comprehension task. Our systems demonstrated state-of-the-art results, but more importantly, we performed a careful analysis of the dataset by hand.\nOverall, we think the CNN/Daily Mail datasets are valuable datasets, which provide a promising avenue for training effective statistical models for reading comprehension tasks. Nevertheless, we argue that: (i) this dataset is still quite noisy due to its method of data creation and coreference errors; (ii) current neural networks have almost reached a performance ceiling on this dataset; and (iii) the required reasoning and inference level of this dataset is still quite simple.\nAs future work, we need to consider how we can utilize these datasets (and the models trained upon them) to help solve more complex RC reasoning tasks (with less annotated data)."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their thoughtful feedback. Stanford University gratefully\nacknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government."}], "references": [{"title": "Modeling biological processes for reading comprehension", "author": ["Jonathan Berant", "Vivek Srikumar", "Pei-Chun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing", "citeRegEx": "Berant et al\\.,? 2014", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Towards the machine comprehension of text: An essay", "author": ["Christopher J.C. Burges."], "venue": "Technical report, Microsoft Research Technical Report MSR-TR-2013125.", "citeRegEx": "Burges.,? 2013", "shortCiteRegEx": "Burges.", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Dynamic entity representation with max-pooling improves machine reading", "author": ["Sosuke Kobayashi", "Ran Tian", "Naoaki Okazaki", "Kentaro Inui."], "venue": "North American Association for Computational Linguistics (NAACL).", "citeRegEx": "Kobayashi et al\\.,? 2016", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2016}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher."], "venue": "International Conference on", "citeRegEx": "Kumar et al\\.,? 2016", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Reasoning in vector space: An exploratory study", "author": ["Moontae Lee", "Xiaodong He", "Wen-tau Yih", "Jianfeng Gao", "Li Deng", "Paul Smolensky"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A Unified Theory of Inference for Text Understanding", "author": ["Peter Norvig."], "venue": "Ph.D. thesis, University of California, Berkeley.", "citeRegEx": "Norvig.,? 1978", "shortCiteRegEx": "Norvig.", "year": 1978}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 193\u2013203.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Learning answerentailing structures for machine comprehension", "author": ["Mrinmaya Sachan", "Kumar Dubey", "Eric Xing", "Matthew Richardson."], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language", "citeRegEx": "Sachan et al\\.,? 2015", "shortCiteRegEx": "Sachan et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "arthur szlam", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Machine comprehension with syntax, frames, and semantics", "author": ["Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "Association for Computational Linguistics and International Joint Conference on Natural Language Processing", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2016", "shortCiteRegEx": "Weston et al\\.", "year": 2016}, {"title": "Adapting boosting for information retrieval measures", "author": ["Qiang Wu", "Christopher J. Burges", "Krysta M. Svore", "Jianfeng Gao."], "venue": "Information Retrieval, pages 254\u2013270.", "citeRegEx": "Wu et al\\.,? 2010", "shortCiteRegEx": "Wu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": ", (Norvig, 1978)).", "startOffset": 2, "endOffset": 16}, {"referenceID": 1, "context": "Human reading comprehension is often tested by asking questions that require interpretive understanding of a passage, and the same approach has been suggested for testing computers (Burges, 2013).", "startOffset": 181, "endOffset": 195}, {"referenceID": 12, "context": "In recent years, there have been several strands of work which attempt to collect human-labeled data for this task \u2013 in the form of document, question and answer triples \u2013 and to learn machine learning models directly from it (Richardson et al., 2013; Berant et al., 2014; Wang et al., 2015).", "startOffset": 226, "endOffset": 291}, {"referenceID": 0, "context": "In recent years, there have been several strands of work which attempt to collect human-labeled data for this task \u2013 in the form of document, question and answer triples \u2013 and to learn machine learning models directly from it (Richardson et al., 2013; Berant et al., 2014; Wang et al., 2015).", "startOffset": 226, "endOffset": 291}, {"referenceID": 15, "context": "In recent years, there have been several strands of work which attempt to collect human-labeled data for this task \u2013 in the form of document, question and answer triples \u2013 and to learn machine learning models directly from it (Richardson et al., 2013; Berant et al., 2014; Wang et al., 2015).", "startOffset": 226, "endOffset": 291}, {"referenceID": 3, "context": "Recently, researchers at DeepMind (Hermann et al., 2015) had the appealing, original idea of exploiting the fact that the abundant news articles of CNN and Daily Mail are accompanied by bullet point summaries in order to heuristically create large-scale supervised training data for the reading comprehension task.", "startOffset": 34, "endOffset": 56}, {"referenceID": 3, "context": "The RC datasets introduced in (Hermann et al., 2015) are made from articles on the news websites CNN and Daily Mail, utilizing articles and their bullet point summaries.", "startOffset": 30, "endOffset": 52}, {"referenceID": 3, "context": "Hermann et al. (2015) argue convincingly that such a strategy is necessary to ensure that systems approach this task by understanding the passage in front of them, rather than by using world knowledge or a language model to answer questions without needing to understand the passage.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "At present just two papers are available presenting results on this RC task, both presenting neural network approaches: (Hermann et al., 2015) and (Hill et al.", "startOffset": 120, "endOffset": 142}, {"referenceID": 4, "context": ", 2015) and (Hill et al., 2016).", "startOffset": 12, "endOffset": 31}, {"referenceID": 3, "context": "While Hermann et al. (2015) do provide several baselines for performance on the RC task, we suspect that their baselines are not that strong.", "startOffset": 6, "endOffset": 28}, {"referenceID": 15, "context": "This is similar in spirit to (Wang et al., 2015), which at present has very competitive performance on the MCTest RC dataset (Richardson et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 12, "context": ", 2015), which at present has very competitive performance on the MCTest RC dataset (Richardson et al., 2013).", "startOffset": 84, "endOffset": 109}, {"referenceID": 3, "context": "Our neural network system is based on the AttentiveReader model proposed by (Hermann et al., 2015).", "startOffset": 76, "endOffset": 98}, {"referenceID": 3, "context": "Differences from (Hermann et al., 2015).", "startOffset": 17, "endOffset": 39}, {"referenceID": 9, "context": "The effectiveness of the simple bilinear attention function has been shown previously for neural machine translation by (Luong et al., 2015).", "startOffset": 120, "endOffset": 140}, {"referenceID": 3, "context": "In contrast, the original model in (Hermann et al., 2015) combined o and the question embedding q via another non-linear layer before making final predictions.", "startOffset": 35, "endOffset": 57}, {"referenceID": 4, "context": "Window-based MemN2Ns (Hill et al., 2016).", "startOffset": 21, "endOffset": 40}, {"referenceID": 4, "context": "Another recent neural network approach proposed by (Hill et al., 2016) is based on a memory network architecture (Weston et al.", "startOffset": 51, "endOffset": 70}, {"referenceID": 16, "context": ", 2016) is based on a memory network architecture (Weston et al., 2015).", "startOffset": 50, "endOffset": 71}, {"referenceID": 18, "context": "For training our conventional classifier, we use the implementation of LambdaMART (Wu et al., 2010) in the RankLib package.", "startOffset": 82, "endOffset": 99}, {"referenceID": 2, "context": "We use Stanford\u2019s neural network dependency parser (Chen and Manning, 2014) to parse all our document and question text, and all other features can be extracted without additional tools.", "startOffset": 51, "endOffset": 75}, {"referenceID": 11, "context": "We choose word embedding size d = 100, and use the 100-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014) for initialization.", "startOffset": 101, "endOffset": 126}, {"referenceID": 3, "context": "Not only does this significantly outperform any of the symbolic approaches reported in (Hermann et al., 2015), it also outperforms all the neural network systems from their paper and the best single-system result reported so far from (Hill et al.", "startOffset": 87, "endOffset": 109}, {"referenceID": 4, "context": ", 2015), it also outperforms all the neural network systems from their paper and the best single-system result reported so far from (Hill et al., 2016).", "startOffset": 132, "endOffset": 151}, {"referenceID": 3, "context": "Results marked \u2020 are from (Hermann et al., 2015) and results marked \u2021 are from (Hill et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 4, "context": ", 2015) and results marked \u2021 are from (Hill et al., 2016).", "startOffset": 38, "endOffset": 57}, {"referenceID": 4, "context": "Due to resource constraints, we have not had a chance to investigate ensembles of models, which generally can bring further gains, as demonstrated in (Hill et al., 2016) and many other papers.", "startOffset": 150, "endOffset": 169}, {"referenceID": 5, "context": "Concurrently with our paper, Kadlec et al. (2016) and Kobayashi et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 5, "context": "Concurrently with our paper, Kadlec et al. (2016) and Kobayashi et al. (2016) also experiment on these two datasets and report competitive results.", "startOffset": 29, "endOffset": 78}, {"referenceID": 3, "context": "Additionally, only 2 examples require multiple sentences for inference \u2013 this is a lower rate than we expected and Hermann et al. (2015) suggest.", "startOffset": 115, "endOffset": 137}, {"referenceID": 12, "context": "MCTest (Richardson et al., 2013) is an opendomain reading comprehension task, in the form of fictional short stories, accompanied by multiplechoice questions.", "startOffset": 7, "endOffset": 32}, {"referenceID": 13, "context": "Up to now, the best solutions (Sachan et al., 2015; Wang et al., 2015) are still heavily relying on manually curated syntactic/semantic features, with the aid of additional knowledge (e.", "startOffset": 30, "endOffset": 70}, {"referenceID": 15, "context": "Up to now, the best solutions (Sachan et al., 2015; Wang et al., 2015) are still heavily relying on manually curated syntactic/semantic features, with the aid of additional knowledge (e.", "startOffset": 30, "endOffset": 70}, {"referenceID": 4, "context": "Children Book Test (Hill et al., 2016) was developed in a similar spirit to the CNN/Daily Mail datasets.", "startOffset": 19, "endOffset": 38}, {"referenceID": 4, "context": "According to the first study on this dataset (Hill et al., 2016), a language model (an n-gram model or a recurrent neural network) with local context is sufficient for predicting verbs or prepositions; however, for named entities or common nouns, it improves performance to scan through the whole paragraph to make predictions.", "startOffset": 45, "endOffset": 64}, {"referenceID": 17, "context": "bAbI (Weston et al., 2016) is a collection of artificial datasets, consisting of 20 different reasoning types.", "startOffset": 5, "endOffset": 26}, {"referenceID": 14, "context": "\u201d Various types of memory networks (Sukhbaatar et al., 2015; Kumar et al., 2016) have been shown effective on these tasks, and Lee et al.", "startOffset": 35, "endOffset": 80}, {"referenceID": 7, "context": "\u201d Various types of memory networks (Sukhbaatar et al., 2015; Kumar et al., 2016) have been shown effective on these tasks, and Lee et al.", "startOffset": 35, "endOffset": 80}, {"referenceID": 4, "context": "Children Book Test (Hill et al., 2016) was developed in a similar spirit to the CNN/Daily Mail datasets. It takes any consecutive 21 sentences from a children\u2019s book \u2013 the first 20 sentences are used as the passage, and the goal is to infer a missing word in the 21st sentence (question and answer). The questions are also categorized by the type of the missing word: named entity, common noun, preposition or verb. According to the first study on this dataset (Hill et al., 2016), a language model (an n-gram model or a recurrent neural network) with local context is sufficient for predicting verbs or prepositions; however, for named entities or common nouns, it improves performance to scan through the whole paragraph to make predictions. So far, the best published results are reported by window-based memory networks. bAbI (Weston et al., 2016) is a collection of artificial datasets, consisting of 20 different reasoning types. It encourages the development of models with the ability to chain reasoning, induction/ deduction, etc., so that they can answer a question like \u201cThe football is in the playground\u201d after reading a sequence of sentences \u201cJohn is in the playground; Bob is in the office; John picked up the football; Bob went to the kitchen.\u201d Various types of memory networks (Sukhbaatar et al., 2015; Kumar et al., 2016) have been shown effective on these tasks, and Lee et al. (2016) show that vector space models based on extensive problem analysis can obtain near-perfect accuracies on all the categories.", "startOffset": 20, "endOffset": 1403}], "year": 2016, "abstractText": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1", "creator": "LaTeX with hyperref package"}}}