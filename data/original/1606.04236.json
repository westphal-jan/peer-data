{"id": "1606.04236", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Context-Aware Proactive Content Caching with Service Differentiation in Wireless Networks", "abstract": "Content caching in small base stations (SBSs) or wireless infostations is considered as a suitable approach to improve the efficiency in wireless content delivery. Due to storage limitations, placing the optimal content into local caches is crucial. Cache content placement is challenging since it requires knowledge about the content popularity distribution, which is often not available in advance. Moreover, content popularity is subject to fluctuations as mobile users with different interests connect to the caching entity over time. In this paper, we propose a novel algorithm for context-aware proactive cache content placement. By regularly observing context information of connected users, updating the cache content accordingly and observing the demands for cache content subsequently, the algorithm learns context-specific content popularity online over time. We derive a sub-linear regret bound, which characterizes the learning speed and proves that our algorithm asymptotically maximizes the average number of cache hits. Furthermore, our algorithm supports service differentiation by allowing operators of caching entities to prioritize groups of customers. Our numerical results confirm that by exploiting contextual information, our algorithm outperforms state-of-the-art algorithms in a real world data set, with an increase in the number of cache hits of at least 14%.", "histories": [["v1", "Tue, 14 Jun 2016 07:53:47 GMT  (568kb,D)", "http://arxiv.org/abs/1606.04236v1", "30 pages, 9 figures"], ["v2", "Fri, 16 Dec 2016 15:03:53 GMT  (940kb,D)", "http://arxiv.org/abs/1606.04236v2", "32 pages, 9 figures, to appear in IEEE Transactions on Wireless Communications, seethis http URL"]], "COMMENTS": "30 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["sabrina m\\\"uller", "onur atan", "mihaela van der schaar", "anja klein"], "accepted": false, "id": "1606.04236"}, "pdf": {"name": "1606.04236.pdf", "metadata": {"source": "CRF", "title": "Context-Aware Proactive Content Caching with Service Differentiation in Wireless Networks", "authors": ["Sabrina M\u00fcller", "Onur Atan", "Mihaela van der Schaar", "Anja Klein"], "emails": ["a.klein}@nt.tu-darmstadt.de", "oatan@ucla.edu,", "mihaela@ee.ucla.edu"], "sections": [{"heading": null, "text": "Content caching in small base stations (SBSs) or wireless infostations is considered as a suitable approach to improve the efficiency in wireless content delivery. Due to storage limitations, placing the optimal content into local caches is crucial. Cache content placement is challenging since it requires knowledge about the content popularity distribution, which is often not available in advance. Moreover, content popularity is subject to fluctuations as mobile users with different interests connect to the caching entity over time. In this paper, we propose a novel algorithm for context-aware proactive cache content placement. By regularly observing context information of connected users, updating the cache content accordingly and observing the demands for cache content subsequently, the algorithm learns contextspecific content popularity online over time. We derive a sub-linear regret bound, which characterizes the learning speed and proves that our algorithm asymptotically maximizes the average number of cache hits. Furthermore, our algorithm supports service differentiation by allowing operators of caching entities to prioritize groups of customers. Our numerical results confirm that by exploiting contextual information, our algorithm outperforms state-of-the-art algorithms in a real world data set, with an increase in the number of cache hits of at least 14%.\nIndex Terms\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Part of this work has been published at IEEE International Conference on Communications (ICC) 2016 [22].\nar X\niv :1\n60 6.\n04 23\n6v 1\n[ cs\n.N I]\n2 Wireless Networks, Caching at the Edge, Cache Content Placement, Online Learning\nI. INTRODUCTION\nNowadays, more than half of the consumer Internet traffic is video traffic, whose growth is foreseen to continue both within wired and wireless networks [1], [2]. In response to the requirements imposed by the growing demand for content of various types, which has to be delivered to a large number of end users, Content Delivery Networks (CDNs) were developed to serve data traffic [3]\u2013[5]. Already today, more than half of the Internet video traffic is delivered by CDNs and this trend is expected to increase to 72% by 2019 [1]. One of the major components of CDNs is content caching. Its basic idea is to bring content closer to the end user. For this purpose, servers for content storage are placed across the network and users are served from the most suitable server that stores the requested content. Caching content and serving users from nearby servers alleviates both the traffic load in the network and the latency for the end user.\nWireless networks have been experiencing a steep increase in data traffic in recent years [2]. Fueled by the emergence of smart mobile devices with advanced multimedia capabilities and the trend towards high data rate applications such as video streaming, especially mobile video traffic is foreseen to account for the majority of mobile data traffic in a few years [2]. However, despite recent advancements in cellular mobile radio networks, these networks cannot keep up with the massive growth of mobile data traffic [6]. As in its wired counterpart, content caching is envisioned to improve the throughput in wireless content delivery. This is not only due to decreasing disk storage prices, but also due to the fact that typically only a small number of very popular contents account for the majority of data traffic [7].\nWithin wireless networks, caching at the edge has been extensively studied [8]\u2013[22]. At the radio access network level, current approaches comprise two different types of local caching entities. The first type of caching entities are macro base stations (MBSs) as well as small base stations (SBSs) in wireless small cell networks that dispose of limited storage capacities and are typically owned by the mobile network operator (MNO). The second type of caching entities are wireless infostations that provide high bandwidth local data communication [23]. Cachingenabled infostations could be owned by content providers (CPs) to provide their end users with higher quality of experience or alternatively, third parties could offer caching at infostations as a service to CPs or their end users [19]. For both types of local caching entities, a fraction of\n3 available popular content, such as popular videos, is stored at the caching entities in a placement phase to serve end users\u2019 requests in their vicinity directly via localized communication in a delivery phase.\nSince due to the vast amount of content available in multimedia platforms, not all available content can be stored in local caches, intelligent algorithms for cache content placement are required. Many challenges of cache content placement concern content popularity. Firstly, optimal cache content placement primarily depends on the content popularity distribution, but when caching content at a particular point in time, it is unknown which exact content will be requested in future and even an estimation of the content popularity distribution might not be at hand, but has to be learned by the local caching entity itself [15]\u2013[22]. This assumption is not only legitimate from an overhead point of view, since else a periodic coordination with the global multimedia application would be required. More importantly, content popularity at a local caching entity might not even replicate global content popularity as monitored by the global multimedia application [24]\u2013[26]. Hence, local caching entities should learn local content popularity for proactive cache content placement. Secondly, due to the mobility of users in the vicinity of a wireless caching entity, the local content popularity is in general not fixed, but may change according to the interests of fluctuating mobile users. The users\u2019 interests may depend on various factors, such as their personal characteristics, their devices\u2019 characteristics [27] or environmental conditions [26]. Hence, cache content placement should be context-aware by exploiting information that influences the users\u2019 interests. Thirdly, while the typical goal of cache content placement might lie in the maximization of the number of cache hits, cache content placement should take into account the operator\u2019s specific objective. In particular, appropriate caching algorithms should be capable of incorporating business models of operators to offer service differentiation to their customers, e.g., by optimizing cache content according to different prioritization levels [28], [29].\nIn this paper, we propose a novel context-aware proactive caching algorithm. Instead of assuming a priori knowledge about content popularity, which might be externally given or estimated in a separate training phase, our algorithm learns context-specific content popularity online over time by observing users\u2019 requests for content. By taking into account the users\u2019 context information, the algorithm learns context-specific content popularity, instead of an averaged content popularity over all possible users. The context information can include personal characteristics about the\n4 user and his equipment as well as environmental conditions. Our algorithm learns online the best caching strategy to maximize the number of cache hits at the local caching entity. Moreover, the algorithm allows for operator-specific service differentiation by prioritizing different types of customers. The contributions of this paper are as follows:\n\u2022 We present a context-aware proactive caching algorithm based on contextual multi-armed\nbandit optimization which supports service differentiation by prioritization. Our algorithm learns optimal cache content placement online over time in order to maximize the number of cache hits while exploiting the users\u2019 context information. \u2022 We theoretically bound the loss of the algorithm compared to the optimal benchmark,\nwhich assumes a priori knowledge about content popularity. By deriving a sub-linear regret bound, we characterize the speed of convergence and prove that our algorithm asymptotically maximizes the average number of cache hits. \u2022 We present additional extensions of our approach, such as its combination with multicast\ntransmissions and the incorporation of rating-based caching decisions.\n\u2022 We numerically evaluate the context-aware proactive caching algorithm based on a real\nworld data set. A comparison shows that by exploiting contextual information, our algorithm outperforms reference algorithms.\nThe remainder of the paper is organized as follows. Section II gives an overview of related works. In Section III, we describe the system model, including an architecture and a formal problem formulation for context-aware proactive caching. In Section IV, we propose a contextaware proactive caching algorithm. Theoretical analysis of regret and memory requirements are provided in Sections V and VI, respectively. In Section VII, we present some extensions of the algorithm. Numerical results of the learning algorithm are presented in Section VIII. Section IX concludes the paper."}, {"heading": "II. RELATED WORK", "text": "In this paper, we consider cache content placement for a caching problem with two phases, given by a placement phase and a delivery phase. However, practical caching systems often still use simple cache replacement algorithms that update the cache continuously during the delivery phase. Common examples of cache replacement algorithms are Least Recently Used (LRU) or Least Frequently Used (LFU) (for a survey, see [30]). While these simple cache\n5\nreplacement algorithms do not consider future popularity of content, recent work has been devoted to developing sophisticated cache replacement algorithms by learning future content popularity [31].\nIn our overview given below, we focus on research on cache content placement for wireless caching problems with a placement phase and a delivery phase. We start by discussing related work that assumes a priori knowledge about content popularity. Reference [9] calculates information-theoretic gains achieved by combining cache content placement at end user devices with a coded multicast transmission in the content delivery phase. The proposed coded caching approach is optimal up to a constant factor. Reference [10] combines content caching at end devices with collaborative device-to-device communication to increase the efficiency of content delivery. In [11], an approximation algorithm is given for uncoded caching among cache-enabled SBSs which minimizes the average delay experienced by users that can be connected to several SBSs simultaneously. Additionally, it is proven that the problem is efficiently solvable in the coded caching case. Building upon the same caching architecture, in [12], an approximation algorithm is presented for minimizing the probability that moving users have to request parts of content from the MBS instead of the SBSs in distributed coded cache content placement. In [13], a multicast-aware caching scheme is proposed for minimizing the energy consumption in a small cell network, in which both the MBS as well as the SBSs can perform multicast transmissions. Reference [14] analytically calculates the outage probability and average content delivery rate in a network of SBSs equipped with caches.\nNext, we discuss related work on cache content placement without prior knowledge about\n6 content popularity. A comparison of the characteristics of our proposed algorithm with related work of this type is given in Table I. Driven by a proactive caching paradigm, [15], [16] propose a cache content placement algorithm for small cell networks based on collaborative filtering. Fixed global content popularity is estimated using a training set and then exploited for caching decisions to maximize the average user request satisfaction based on their required rates of content delivery. While their approach requires a training set of known content popularities and only learns during a training phase, our proposed algorithm does not need a training phase, but learns content popularity online over time, thus also adapting to varying content popularities. In [17], using a multi-armed bandit algorithm, an SBS learns a fixed content popularity online by refreshing its cache content and observing instantaneous demands for cached files over time. In this way, the cache content placement at an SBS is optimized over time to maximize the traffic that can be served by the SBS. The authors extend their framework for a wireless infostation network in [18], [19], where they additionally take into account the costs for adding new files to the cache. Moreover, they provide theoretical sub-linear regret bounds for their algorithms. A different extension of the multi-armed bandit framework is given in [20], which by incorporating coded cache content placement exploits the topology of users\u2019 connections to the SBSs. The approach in [20] assumes a specific model of content popularity distribution. Since in practice the content popularity distribution is unknown a priori, such an assumption is restrictive. In contrast, our proposed algorithm is model-free since it does not assume a specific content popularity distribution. Moreover, in [17]\u2013[20], the optimal caching strategy is learned over time based only on previous observations of instantaneous demands. In contrast, our proposed algorithm additionally exploits users\u2019 context information, such that diversity of content popularity within the user population is taken into account. Diversity in content popularity among different users is for example taken into account in [21], but again without considering the users\u2019 contexts. Users are clustered into groups of similar interests by a spectral clustering algorithm based on their requested contents in a training phase. Each user group is assigned to an SBS which then learns the content popularity of its fixed user group over time. Hence, each SBS learns a fixed content popularity under the assumption of a stable user population, whereas our approach allows reacting to arbitrary arrivals of users with content popularity depending on their contexts.\nIn our previous work [22], we presented a learning algorithm for smart caching at SBSs based on the contextual multi-armed bandit problem [32], [33], an extension of the standard multi-\n7 armed bandit problem [34]. Our proposed algorithm in [22] was inspired by the distributed contextual learning algorithm presented in [33], which considers a general multi-agent learning setting. While several learners select one action each at a time in [33], we considered the case of one learner taking multiple actions at a time, since the sBS has to select multiple files to cache. Our algorithm in [22] took only aggregated context information of connected users into account. In this paper, we extend our approach from [22] in order to take into account context information at a single user level, which enables more fine-grained learning. Additionally, we incorporate service differentiation. Moreover, we present extensions to multicast transmission and ratingbased caching decisions, provide a sophisticated regret analysis and evaluate our algorithm with a real world data set."}, {"heading": "III. SYSTEM MODEL", "text": ""}, {"heading": "A. Wireless Caching Entity", "text": "We consider a wireless caching entity that can either be a caching-enabled SBS in a small cell network or a wireless infostation. The caching entity is characterized by a limited storage capacity and a reliable backhaul link to the core network. In its cache memory, the caching entity can store up to m files from a finite content library F containing |F | files, where we assume for simplicity that all files are of the same size. To inform users in its vicinity about available files, the caching entity broadcasts the information about currently cached files periodically [17]\u2013[19]. If a user is interested in a file that the caching entity stored in its cache, he requests the file from the caching entity and is served via localized communication. In this case, no additional load is put on neither the macro cellular network nor the backhaul network. If the file is not stored in the caching entity, the user does not request the file from the caching entity. Instead, the user requests the file from the macro cellular network by connecting to an MBS. The MBS downloads the file from the core network via its backhaul connection, such that in this case, load is put on both the macro cellular as well as the backhaul network. In this way, the caching entity cannot observe the requests for files that it has not cached, but at the same time, it is not congested by such requests [17]\u2013[19].\nFigure 1 shows an illustration of the considered system model. Within this architecture, the caching entity can only observe the requests for cached files, i.e., cache hits, but it cannot observe the requests for non-cached files, i.e., cache misses. In order to reduce the load on the macro\ncellular as well as the backhaul network, a caching entity operated by an MNO might aim at optimizing the cache content such that the traffic it can serve directly is maximized. Maximizing the traffic served by the caching entity corresponds to maximizing the number of cache hits, i.e., the number of requests for files cached at the caching entity. For this purpose, the caching entity should learn which files are most popular over time."}, {"heading": "B. Customer-Specific Service Differentiation", "text": "Maximization of the number of cache hits as the goal of cache content placement might be adequate in case of an MNO operating an SBS, one reason being limitations imposed by net neutrality restrictions. However, for an operator of an infostation, e.g. a CP or third party operator, the goal might not simply be to cache the most popular content given a set of users. In contrast, according to his business model, the operator may want to provide differentiated services to his customers (that can be both end users and CPs), e.g., by offering different service types corresponding to different prioritization levels. These service types could refer to end users and for instance concern the pricing policy, e.g., a paying user might be prioritized compared to a user with free access to the cache content. This could mean that popular content among paying users is cached preferably. Another possibility is to differentiate end users by interest intensity, e.g., subscribers might obtain priority compared to one-time users. Yet another differentiation could be based on usage patterns. For example, experimental end users might indicate their interest in exploring diverse types of content, while conservative end users might want to be served with only their most preferred content.\n9 Consider a set S of different service types. For service type s \u2208 S, let vs \u2265 1 denote a weight associated with serving one request of a user with service type s by a cached file in the caching entity. The weights vs are fixed and known to the learning process. Let vmax := maxs\u2208S vs. The weights vs can be of monetary nature or correspond to some virtual value, depending on the purpose of service differentiation. For example, the weight of a service type might be related to the pricing policy applied to that service type, a priority factor for the influence of the service type in terms of advertisement or for the reputation of the operator, or describe a trade-off between experimental and conservative users. The goal of the operator for cache content placement then becomes the maximization of the number of weighted cache hits.\nA second service differentiation can be applied in case of a third party operator whose customers are different CPs which buy caching at the wireless infostation as a service. The operator may want to prioritize content of certain CPs according to his business model. More general, the operator could prioritize specific content according to its value for the operator. In this case, cache content placement should take into account each contents\u2019 priority. Consider a prioritization weight wf \u2265 1 for each file f \u2208 F and let wmax := maxf\u2208F wf . The prioritization weights could either be chosen individually for each file or per CP and are fixed and known as well.\nThe case without any service differentiation, where the goal of cache content placement is the pure maximization of the number of cache hits, is a special case of the scenario described above, in which there is only one service type s with weight vs = 1 and the prioritization weights satisfy wf = 1 for all f \u2208 F . While we will refer to the more general case in the subsequent sections, this special case is naturally contained in our analysis."}, {"heading": "C. Context-Specific Content Popularity", "text": "Content popularity is in general not static, but depends on a user\u2019s preferences. From a user\u2019s point of view, his personal preferences for content may depend on various factors. We refer to such factors as context dimensions and categorize them in the following classes as summarized in Table II. Personal characteristics are for example demographic factors (e.g. age, gender), emotional state and previous experiences. User equipment refers to the type of device (e.g. smart phone, tablet) which is used to access and consume the content, as well as its capabilities and battery status. Environmental conditions are for example given by the location of the user\n10\nand its characteristics, the time of day or the day of the week and external events (e.g. soccer match, concert) that can be both periodic as well as aperiodic. Clearly, this categorization of possible context dimensions is not exhaustive and the impact of each single dimension on content popularity is unknown a priori. Moreover, the caching entity may only have access to part of the context dimensions, for example due to privacy reasons. Under the assumption that the caching entity has access to some information, by observing not only demands for cached files, but also the contexts of users requesting the files, it can learn to cache the most suitable files for the contexts of currently connected users over time. For this purpose, the caching entity updates its cache content periodically."}, {"heading": "D. Context-Aware Proactive Caching Architecture", "text": "In this subsection, we describe the architecture for context-aware proactive caching, which is designed similarly to an architecture for popularity-driven cache content replacement presented in [31]. An illustration of the context-aware proactive caching architecture is given in Figure 2. The main building blocks of the context-aware proactive caching architecture are a Cache Management entity, the Local Cache, a Storage Interface and a User Interface, a Context Monitor and a Learning Module. The Cache Management consists of a Cache Controller and a Request Handler. The Learning Module contains a Decision Engine, a Learning Database and a Context Database. The workflow consists of several phases as enumerated in Figure 2 and is described\n11\nbelow.\n\u2022 Initialization\n(1) The Learning Module is provided with the goal of content caching (i.e. pure maximization of number of cache hits or operator-specific goal). Moreover, the Learning Module fixes an appropriate time duration, which specifies the periodicity of context monitoring and cache refreshment and informs the Cache Management and the Context Monitor about the periodicity.\n\u2022 Periodic Context Monitoring and Cache Refreshment\n(2) The Context Monitor periodically gathers context information by accessing the information available at the User Interface about currently connected users and by collecting information from external sources (such as additional information about users from social media platforms). If different service types exist, the Context Monitor also retrieves the service types of connected users. (3) The Context Monitor delivers the gathered information to the Context Database in the Learning Module. (4) The Decision Engine periodically extracts the newly monitored context information from the Context Database. (5) Upon comparison with the results from previous time slots as stored in the Learning Database, (6) the Decision Engine decides which files to cache for the next time slot. (7) The Decision Engine instructs the Cache Controller to refresh the cache content accordingly. (8) The Cache\n12\nController compares the current cache content with the required one and removes nonrequired content from the cache. If some required content is missing, the Cache Controller directs the Storage Interface to fetch the content from storage servers and to store it into the local cache. (9) Then, the Cache Controller informs the User Interface about the new cache content. (10) The User Interface pushes the information about the new cache content to currently connected users.\n\u2022 User Requests\n(11) When a user requests a file available in the local cache, the User Interface forwards the request to the Request Handler. The Request Handler stores the request information. Then it retrieves the requested file from the local cache and serves the request.\n\u2022 Periodic Learning\n(12) Upon completion of a time slot, the Request Handler hands the information about all user requests from that time slot to the Learning Module. The Learning Module updates the Learning Database with the context information from the beginning of the time slot and the number of requests for cached files in that time slot."}, {"heading": "E. Formal Problem Formulation", "text": "Next, we give a formal problem formulation for context-aware proactive caching at the caching entity. As described above, the caching system operates in discrete time slots t = 1, 2, ..., T , where T denotes the finite time horizon. Each time slot t consists of the following sequence of events: (i) The context of currently connected users and their service types are monitored and collected. Let Ut be the number of users currently connected to the caching entity. We assume that 1 \u2264 Ut \u2264 Umax, where Umax is the maximum number of users that can be connected simultaneously. Let D be the number of monitored context dimensions per user. We denote the D-dimensional context space by X . It is assumed to be bounded and can hence be set to X := [0, 1]D without loss of generality. Let xt,i \u2208 X be the context vector of user i observed in time slot t. Let xt = (xt,i)i=1,...,Ut be the collection of contexts of all users in time slot t. Let st,i \u2208 S be the service type of user i in time slot t and let st = (st,i)i=1,...,Ut be the collection\nof service types of all users in time slot t. (ii) Based on the contexts xt \u2208 \u220fUt i=1X , the service types st and their corresponding service weights, as well as the prioritization weights wf of each file f \u2208 F and knowledge from previous time slots, the cache content is refreshed. Then, all\n13\nusers in the vicinity are informed about the set of currently cached files. This set is denoted by Ct = {ct,1, ..., ct,m}. (iii) Until the end of this time slot, users can request currently cached files. Their requests are served. The demands dct,j(xt,i, t) of each user i = 1, ..., Ut for all cached files ct,j \u2208 Ct in this time slot are observed, i.e., the number of requests for each single file currently stored in the cache is monitored. Then, the total sum of (weighted) cache hits in this time slot can be calculated.\nThe number of times a user with context vector x \u2208 X requests a file f \u2208 F within one time slot is a random variable with unknown distribution. We denote this random demand by df (x) and its expected value by \u00b5f (x). The random demand is assumed to take values in [0, Rmax], where Rmax is the maximum number of possible requests a user can submit within one time slot. This explicitly incorporates that a user can request the same file several times within one time slot. In time slot t, the random variables df (xt,i), i = 1, .., Ut, are assumed to be independent, i.e., given a set of currently connected users, their requests for content are independent of one another. This allows to learn for each of the users\u2019 contexts separately. Moreover, each random variable df (xt,i) is assumed to be independent of past caching decisions and previous demands. Note that compared to our formulation in [22], where the context space contained context dimensions based on averaged information over all currently connected user, here, we define the context space on a per-user level. Hence, in our new formulation, cache content placement takes the distribution of user contexts into account instead of averaging the context information over all currently connected users, which makes the cache content placement more accurate.\nThe goal of the caching entity is to optimize the cache content to maximize the total number of (weighted) cache hits up to the finite time horizon T . Suppose that for each context vector x \u2208 X , the expected demand \u00b5f (x) of all files f \u2208 F would be known. Then, in each time slot t, given the context xt = (xt,i)i=1,...,Ut and the service types st = (st,i)i=1,...,Ut , the optimal solution would be to cache the m files with highest expected (weighted) demands given the\nusers\u2019 contexts and service types. This can be formalized as follows. For contexts xt \u2208 \u220fUt i=1X and service types st, we define the top-m files for the pair (xt, st) as the following m files\n14\nf \u22171 (xt, st), f \u2217 2 (xt, st), ..., f \u2217 m(xt, st) \u2208 F which satisfy 1\nf \u22171 (xt, st) \u2208 argmax f\u2208F wf Ut\u2211 i=1 vst,i\u00b5f (xt,i)\nf \u22172 (xt, st) \u2208 argmax f\u2208F\\{f\u22171 (xt,st)} wf Ut\u2211 i=1 vst,i\u00b5f (xt,i) (1)\n...\nf \u2217m(xt, st) \u2208 argmax f\u2208F\\{f\u22171 (xt,st),...,f\u2217m\u22121(xt,st)} wf Ut\u2211 i=1 vst,i\u00b5f (xt,i).\nThen, an optimal choice of files to cache given contexts xt and service types st is given by the set\nC\u2217t (xt, st) = {f \u22171 (xt, st), ..., f \u2217m(xt, st)}. (2)\nHowever, we assume that the expected demands are unknown a priori. In this case, the caching entity has to learn the expected demands over time. The optimal solution given in (2) then serves as a benchmark to evaluate the loss of learning the expected demands instead of knowing them a priori. Below, a formal definition of this loss is given, known as the regret of learning. Under the assumption that the context-specific expected demands are unknown a priori, the caching entity has to learn these expected demands over time. For this purpose, the caching entity has to find a trade-off between caching files about which little information is available (exploration) and files of which it believes that they will yield the highest demands (exploitation). In each time slot, the choice of files to be cached depends on the history of choices in the past and the corresponding observed demands. An algorithm which maps the history to the choices of files to cache is called a learning algorithm.\nThe regret of learning with respect to the optimal benchmark solution is then given by\nR(T ) = T\u2211 t=1 m\u2211 j=1 Ut\u2211 i=1 vst,i ( wf\u2217j (xt,st)E ( df\u2217j (xt,st)(xt,i) )\n\u2212 wct,jE ( dct,j(xt,i, t) )) , (3)\n1Several files may have the same expected demands, i.e., the optimal set of files may not be unique. This is also captured\nhere.\n15\nwhere dct,j(xt,i, t) denotes the random demand for the cached file ct,j \u2208 Ct of user i with context vector xt,i at time t. Here, the expectation is taken with respect to the choices made by the learning algorithm and the distributions of the demands."}, {"heading": "IV. A CONTEXT-AWARE PROACTIVE CACHING ALGORITHM", "text": "To optimally select which files to cache given the context information about currently connected users, the caching entity should learn context-specific content popularity. The basic idea of our algorithm for context-aware proactive caching is based on the assumption that users with similar context information will more likely request similar files. If this natural assumption holds true, the users\u2019 context information together with their requests for cached files can be exploited to learn for future caching decisions. For this purpose, our algorithm partitions the context space uniformly into smaller sets. Then, the caching entity learns the expected demands for files independently in each of the sets, by estimating the expected demands for files based on the observed demands of users whose contexts belonged to that set. In the algorithm, a time slot t can either be an exploration or an exploitation phase. In exploration phases, the caching entity chooses a random set of files to cache. Theses phases are needed to update the estimated demands for files which have not been cached often before. In exploitation phases, the caching entity ranks the files according to their estimated demands and caches the ones with the highest estimated demands.\nThe algorithm for selecting m files is called Context-Aware Proactive Caching with Cache Size m (m-CAC) and its pseudocode is given in Figure 3. Next, we describe the algorithm in more detail. In its initialization phase, m-CAC creates a partition PT of the context space X = [0, 1]D into (hT )D sets, that are given by D-dimensional hypercubes of identical size 1hT \u00d7 . . . \u00d7 1 hT . Here, hT is an input parameter which determines the number of sets in the partition. Additionally, m-CAC keeps a counter Nf,p(t) for each pair consisting of a file f \u2208 F and a set p \u2208 PT . The counter Nf,p(t) is the number of times in which file f \u2208 F was cached after a user with context from set p was connected to the caching entity up to time slot t (i.e., when 2 users with context from set p were connected in one time slot and file f was cached, this counter is increased by 2). Moreover, m-CAC initializes the estimated demand d\u0302f,p(t) of each pair consisting of a file f \u2208 F and a set p \u2208 PT up to time slot t. This estimated demand is calculated as follows: Let Ef,p(t) be the set of observed demands of users with context from set p when file f was cached\n16\nm-CAC: Context-Aware Proactive Caching Algorithm\n17\nup to time slot t. Then, the estimated demand of file f in set p is given by the sample mean d\u0302f,p(t) := 1 |Ef,p(t)| \u2211 d\u2208Ef,p(t) d. 2 3\nIn each time slot t, m-CAC first observes the number of currently connected users Ut, their contexts xt = (xt,i)i=1,...,Ut and the service types st = (st,i)i=1,...,Ut . For each user context vector xt,i m-CAC determines the set pt,i \u2208 PT , to which the context vector belongs, i.e., such that xt,i \u2208 pt,i holds. The collection of all these sets is given by pt = (pt,1, ..., pt,Ut). Then, the algorithm can be in one of the two phases mentioned above, in an exploration phase or in an exploitation phase. In order to determine the correct phase for the current time slot, the algorithm checks if there are files that have not been explored sufficiently often. For this purpose, the set of under-explored files F uept (t) is calculated based on\nF uept (t) = \u222a Ut i=1F ue pt,i (t)\n:= \u222aUti=1{f \u2208 F : Nf,pt,i(t) \u2264 K(t)}, (4)\nwhere K(t) is a deterministic, monotonically increasing control function, which is an input to the algorithm. The control function has to be set adequately to balance the trade-off between exploration and exploitation. In Section V, we will select a control function that guarantees a good balance in terms of this trade-off.\nIf the set of under-explored files is non-empty, m-CAC enters the exploration phase. Let u(t) be the size of the set of under-explored files. If the set of under-explored files contains at least m elements, i.e., u(t) \u2265 m, the algorithm randomly selects m files from F uept (t) to cache. If the set of under-explored files contains less than m elements, u(t) < m, it selects all u(t) files from F uept (t) to cache. Since the cache is not fully filled by u(t) < m files, additionally (m\u2212u(t)) other files can be cached. In order to exploit knowledge obtained so far, m-CAC selects (m \u2212 u(t)) files from F \\ F uept (t) based on a file ranking according to the estimated weighted demands, as defined by the files f\u03021,pt,st(t), ..., f\u0302m\u2212u(t),pt,st(t) \u2208 F \\ F uept (t), which satisfy:\nf\u0302j,pt,st(t) \u2208 argmax f\u2208F\\(Fuept (t)\u222a \u22c3j\u22121 k=1{f\u0302k,pt,st (t)}) wf Ut\u2211 i=1 vst,i d\u0302f,pt,i(t) (5)\n2The set Ef,p(t) does not have to be stored since the estimated demand d\u0302f,p(t) can be updated based on d\u0302f,p(t\u2212 1) and on the observed demands at time t. 3Note that in the pseudocode in Figure 3, the argument t is dropped from counters Nf,p(t) and d\u0302f,p(t) since previous values of these counters do not have to be stored.\n18\nfor j = 1, ...,m \u2212 u(t), where \u22c30 k=1{f\u0302k,pt,st(t)} = \u2205. If the set of files defined by (5) is not unique, ties are broken arbitrarily. Note that by this procedure, even in exploration phases, the algorithm additionally exploits, whenever the number of under-explored files is smaller than the cache size.\nIf the set of under-explored files F uept (t) is empty, m-CAC enters the exploitation phase. It selects m files from F based on a file ranking according to the estimated weighted demands, as defined by the files f\u03021,pt,st(t), ..., f\u0302m,pt,st(t) \u2208 F , which satisfy:\nf\u0302j,pt,st(t) \u2208 argmax f\u2208F\\(Fuept (t)\u222a \u22c3j\u22121 k=1{f\u0302k,pt,st (t)}) wf Ut\u2211 i=1 vst,i d\u0302f,pt,i(t) (6)\nfor j = 1, ...,m. If the set of files defined by (6) is not unique, again ties are broken arbitrarily.\nAfter selecting the files to be cached, the user demands for these files in this time slot are\nobserved. Then, the estimated demands and the counters for cached files are updated."}, {"heading": "V. ANALYSIS OF THE REGRET", "text": "In this section, we give an upper bound on the regret R(T ) of the learning algorithm m-CAC\nas given in (3).\nThe regret bound is based on the natural assumption that expected demands for files are similar for similar contexts. For example, at a similar time of day and for users with similar characteristics, the requests for files will also be similar. Such an assumption is crucial to learn from previous observations and can be captured by the following Ho\u0308lder condition.\nAssumption 1. There exists L > 0, \u03b1 > 0 such that for all f \u2208 F and for all x, y \u2208 X , it holds that\n|\u00b5f (x)\u2212 \u00b5f (y)| \u2264 L||x\u2212 y||\u03b1,\nwhere || \u00b7 || denotes the Euclidian norm in RD.\nThe Ho\u0308lder condition allows us to derive a regret bound. Assumption 1 is needed for the analysis of the regret, but it should be noted that m-CAC can even be applied when this assumption does not hold true. However, in such a more general setting, a regret bound might not be guaranteed.\n19\nNext, we state our main theorem which shows that the regret of m-CAC is sublinear in the time horizon T , i.e., R(T ) = O(T \u03b3) with \u03b3 < 1. This bound on the regret guarantees that the algorithm has an asymptotically optimal performance, since then limT\u2192\u221e R(T ) T = 0 holds. This means, that asymptotically the average number of weighted cache hits is maximized with respect to the benchmark solution given by (2). In detail, the regret of m-CAC can be bounded as follows for any finite time horizon T .\nTheorem 1 (Bound for R(T )). Let K(t) = t 2\u03b1 3\u03b1+D log(t) and hT = dT 1 3\u03b1+D e. If m-CAC is run with these parameters and Assumption 1 holds true, the leading order of the regret is\nO ( vmaxwmaxmUmaxRmax|F |T 2\u03b1+D 3\u03b1+D log(T ) ) .\nThe proof can be found in our online appendix [35]. The regret bound given in Theorem 1 is sublinear in the time horizon T , proving that m-CAC asymptotically maximizes the average number of weighted cache hits. Additionally, Theorem 1 is applicable for any finite time horizon T , such that it provides a bound on the loss incurred by m-CAC for any finite number of cache replacement phases. Thus, Theorem 1 provides a characterization of the speed of convergence of the algorithm. Furthermore, from Theorem 1 it can be seen that the regret is a constant multiple of the regret bound for the special case without service differentiation, in which vmax = 1 and wmax = 1. Hence, also for the case of pure maximization of the number of cache hits, the order of the regret is O(T 2\u03b1+D 3\u03b1+D log(T ))."}, {"heading": "VI. MEMORY REQUIREMENTS", "text": "The memory requirements of m-CAC are mainly determined by the counters that the algorithm keeps during its runtime. For each set p in the partition PT and each file f , the algorithm keeps the counters Nf,p and d\u0302f,p. The number of files is |F |. If m-CAC is run with the parameters from Theorem 1, the number of sets in PT is upper bounded by (hT )D = dT 1 3\u03b1+D eD \u2264 2DT D 3\u03b1+D . Hence, the required memory is upper bounded by |F |2DT D 3\u03b1+D and thus sublinear in the time horizon T . This means, that for T \u2192\u221e, the algorithm would need infinite memory. However, for practical approaches, only the counters of such sets p have to be kept to which at least one of the connected users\u2019 context vectors has already belonged to. Hence, depending on the heterogeneity in the connecting users\u2019 context vectors, the required number of counters that have to be kept can be much smaller than given by the upper bound above.\n20"}, {"heading": "VII. EXTENSIONS", "text": ""}, {"heading": "A. Exploiting the Multicast Gain", "text": "So far, we assumed that in the delivery phase of each time slot t, upon each content request, the requesting user is served with a file immediately. However, our algorithm can be extended to profit from the broadcast nature of the wireless medium by multicasting, which has been shown to be beneficial for caching in small cell networks [13]. For this purpose, each time slot t is divided into a number of intervals. Within each of the intervals, user requests are monitored and accumulated. Then, after a short waiting time, the user requests that were monitored in the meanwhile are served. By such multicast transmissions, the traffic needed for data transmission is reduced. To exploit knowledge about content popularity learned so far, user requests for files with low estimated demand could, however, still be served immediately. In this way, unnecessary delays are prevented in cases in which another user request and thus a multicast transmission is not expected. Moreover, service differentiation could be taken into account. For example, high-priority users or high-priority content could be served immediately, such that their delay is not increased due to waiting times for multicast transmissions."}, {"heading": "B. Rating-Based Context-Aware Proactive Caching", "text": "In the previous sections, we assumed that the caching entity performs cache content placement with respect to the demands of the contents, i.e., the number df (x) of requests, to maximize the number of (weighted) cache hits. However, in case of a CP operating an infostation, his goal might be to cache not only the content that is requested often, but also receives high ratings from his end users. Consider the case that end users rate contents after their consumption in a range [rmin, rmax] \u2282 R+. For a context x, let rf (x) be the random variable describing the rating of a user with context x if he requests file f and makes a rating thereafter. 4 Then, we define the random variable\ndf (x) := 0, if file f not requestedrf (x) if file f requested, (7) 4E.g., no request could correspond to 0 and ratings could be given in the range {1, ..., 5}.\n21\nwhich instead of counting the number of requests, represents the rating of users with context x if they request a content. By carefully designing the range of possible ratings, the CP can trade-off ratings and cache hits to a greater or lesser extend. With this definition, we can apply the algorithm from Section IV. In this case, m-CAC not only relies on observing the number of achieved cache hits after placing new content into the cache, but additionally, the users\u2019 ratings have to be observed in order to learn content popularity in terms of ratings. If the users\u2019 ratings are always available, the regret bound from Theorem 1 applies.\nHowever, users might not always reveal a rating after consumption of a content. When the rating of a user is missing, we assume that m-CAC does not update the counters based on this user\u2019s request. Hence, if no user within one time slot rates his requested content, m-CAC does not update its counters at all in this time slot. This results in a higher required number of exploration phases. Hence, the regret of the learning algorithm is influenced by the users\u2019 willingness to reveal ratings for requested contents. Let q \u2208 (0, 1) be the probability that a user reveals his rating after requesting a content. Then, the regret of the learning algorithm is bounded as given below.\nTheorem 2 (Bound for R(T ) with missing ratings). Let K(t) = t 2\u03b1 3\u03b1+D log(t) and hT = dT 1 3\u03b1+D e. If m-CAC is run with these parameters, Assumption 1 holds true and a user reveals his rating with\nprobability q, the leading order of the regret is O (\n1 q vmaxwmaxmUmaxRmax|F |T\n2\u03b1+D 3\u03b1+D log(T ) ) .\nThe proof can be found in our online appendix [35]. Comparing Theorem 2 with the result from Theorem 1, the regret of m-CAC is scaled up by a factor 1 q > 1 in case of rating-based context-aware proactive caching with missing ratings. This factor corresponds to the expected number of user requests until the caching entity receives one rating. However, the time order of the regret remains the same. Hence, the algorithm is robust under missing ratings in the sense that if some users refuse to rate requested contents, the algorithm is still asymptotically optimal."}, {"heading": "C. Asynchronous User Arrival", "text": "So far, we assumed that in one time slot, exactly those users connected to the local caching entity when the user context is monitored, will request files within that time slot after the cache content is updated. However, if users connect to the local caching entity asynchronously, our\n22\nproposed algorithm should be adapted. If a user directly disconnects after the context monitoring and does not request any file, even though he was considered for cache content placement, it should be taken into account that his vanishing demand results from disconnection, i.e., he should be neglected when updating estimated demands. Hence, in m-CAC, the counters are not updated for disconnecting users. If a user connects to the local caching entity after cache content placement, his context was not considered in the caching decision. However, his requests can be used to learn faster by updating the estimated demands based on his requests. Hence, in m-CAC, the counters are updated based on these requests."}, {"heading": "VIII. NUMERICAL RESULTS", "text": "In this section, we numerically evaluate the proposed learning algorithm m-CAC by comparing\nits solution for a real world data set to several reference algorithms."}, {"heading": "A. Description of the Data Set", "text": "We use a data set from MovieLens to evaluate our presented algorithm. MovieLens is an online movie recommender operated by the research group GroupLens from the University of Minnesota. The MovieLens 1M DataSet [36] contains 1000209 ratings of 3952 movies. These ratings were made by 6040 users of MovieLens within the years 2000 to 2003. Each data set entry consists of an anonymous user ID, a movie ID, a rating (in whole numbers between 1 and 5) and a timestamp. Additionally, demographic information about the users is given: Their gender, age (in 7 categories), occupation (in 20 categories) as well as their Zip-code. For our numerical evaluations, we assume that the movie rating process in the data set corresponds to a content request process of users connected to a local caching entity (see [31] for a similar approach). Hence, a user rating a movie at a certain time in the data set for us corresponds to a request to either the local caching entity (in case the movie is cached at the local caching entity) or to the macro cellular network (in case the movie is not cached at the local caching entity). This approach is reasonable since users rate movies typically after they watched them.\nIn our simulations, we use only the data gathered within the first year of the data set, since around 94% of the user ratings were provided within this timeframe, while only very few ratings were made after this first year. Then, we divide a year\u2019s time into 8760 time slots of one hour each (T = 8760), assuming that the caching entity updates its cache content at an hourly basis,\n23\nand assign the user requests to the time slots according to their timestamps. Figure 4 shows that the corresponding content request process is bursty and flattens out towards the end. As context dimensions, we select the dimensions gender and age.5"}, {"heading": "B. Reference Algorithms", "text": "We compare the performance of m-CAC with five reference algorithms. The first algorithm is the optimum oracle. This algorithm works under the assumption of foresight, i.e., complete knowledge about user demands within the coming time slot. The optimum oracle selects the m files that will yield the highest number of requests within this time slot. 6\nThe second reference algorithm is called m-UCB, which consists of a variant of the UCB algorithm. UCB is a classical learning algorithm for multi-armed bandit problems [34], which has logarithmic regret order. However, it does not take into account context information, i.e., the logarithmic regret is with respect to the average expected demand over the whole context space. While in classical UCB, one action is taken in each period, we modify UCB to take m actions at a time, which corresponds to selecting m files.\n5We neglect the occupation as context dimension since by mapping them to a [0,1] variable, we would have to classify which occupations are more similar to each other than others. 6Note that the optimum oracle based on foresight yields even better results than the benchmark used to define the regret in (3). In the definition of regret, the benchmark solution exploits only complete knowledge about expected content demands, instead of foresight of exact future demands. Since we use a data set with unknown expected content demands, we compare here to the optimum oracle.\n24\nThe third reference algorithm is the m- -Greedy. This is a variant of the simple -Greedy [34] algorithm, which does not consider context information. The m- -Greedy caches a random set of m files with probability \u2208 (0, 1). With probability (1\u2212 ), the algorithm caches the m files with highest to m-th highest estimated demands. These estimated demands are calculated based on previous demands for cached files.\nThe forth reference algorithm is called m-Myopic. This is an algorithm taken from [17], which is investigated since it is comparable to the well known Least Recently Used algorithm (LRU) for caching. m-Myopic learns only from one period in the past. It starts with a random set of files and in each of the following periods discards all files that have not been requested in the previous period. Then it replaces the discarded files randomly by other files.\nThe fifth reference algorithm is called Random. It is a lower bound for the other algorithms,\nsince it caches a random subset of files in each period."}, {"heading": "C. Performance Measures", "text": "The following performance measures are used in our analysis. The evolution of per-time slot or aggregated number of cache hits allows comparing the absolute performance of the algorithms. A relative performance measure is given by the cache efficiency, which is defined as the ratio of cache hits compared to the overall demand, i.e.,\ncache efficiency in % = cache hits\ncache hits + cache misses \u00b7 100.\nThe cache efficiency describes the percentage of user requests which can be served by the files cached at the caching entity. Note that in general, even the optimum oracle cannot serve all user requests because of the limited cache size."}, {"heading": "D. Results", "text": "In our simulations, we set = 0.09 in m- -Greedy, which is the value at which heuristically the algorithm on average performed best. In m-CAC, we set the control function to K(t) = c\u00b7t 2\u03b1 3\u03b1+D log(t) with c = 1/(|F |D). Compared to the control function in Theorem 1, the additional factor reduces the number of exploration phases which allows for better performance.\nFirst, we consider the case of pure cache hit maximization, i.e., without incorporating service differentiation. The long-term behavior of our proposed algorithm is investigated with the\n25\nfollowing scenario. We assume that the caching entity can store m = 200 movies out of the |F | = 3952 available movies. Hence, the cache size corresponds to about 5% of the overall number of files [17]. We run all algorithms on the data set and study their results as a function of time, i.e. over the time slots t = 1, ..., T . Figures 5 and 6 show the per-time slot and the aggregated number of cache hits up to period t as a function of time, respectively. As reflected in Figures 5 and 6, due to the bursty content request process (compare Figure 4), also the number of cache hits over time is bursty for all considered algorithms. As expected, the optimum oracle gives an upper bound to the other algorithms. Among the non-optimal algorithms, our proposed m-CAC, as well as m- -Greedy and m-UCB clearly outperform m-Myopic and Random. This is due to the fact that these three algorithms learn from the history of observed demands, while m-Myopic only learns from one time slot in the past and Random does not learn at all. It can be observed that m- -Greedy shows a better performance than m-UCB, even though it uses a simpler learning strategy. Overall, m-CAC outperforms all other non-optimal algorithms by additionally learning from context information. At the end of the time horizon, the aggregated number of cache hits of m-CAC is 14.6%, 37.7%, 298.5% and 450.6% higher than the ones of m- -Greedy, m-UCB, m-Myopic and Random, respectively.\nNext, we investigate the impact of the cache size m by varying it between 50 and 400 files. This range of cache sizes corresponds to about 1.3% to 10.1% of the overall number of files, which is a realistic assumption. All remaining parameters are kept as in the first scenario. Figure 7 shows the overall cache efficiency at the end of the time horizon T as a function of cache size, i.e., the total number of cache hits at T is normalized by the total number of requests. For\n26\nall algorithms, the overall cache efficiency is increasing for increasing cache size. Moreover, the results indicate that again m-CAC and m- -Greedy slightly outperform m-UCB and clearly outperform m-Myopic and Random. Averaged over the range of cache sizes, the cache efficiency of m-CAC is 15.0%, 43.6%, 299.0% and 451.2% higher than the ones of m- -Greedy, m-UCB, m-Myopic and Random, respectively.\nNow we consider a case of service differentiation, in which two different service types 1 and 2 exist with weights v1 = 5 and v2 = 1. Hence, service type 1 should be prioritized due to the higher value it represents. We randomly assign 10% of the users to service type 1 and classify all remaining users as service type 2. Then we adjust all algorithms to take into account service differentiation by incorporating the weights according to the service types. Figure 8 shows the aggregated number of weighted cache hits up to period t as a function of time. At the end of the\n27\ntime horizon, the aggregated number of weighted cache hits of m-CAC is 15.6%, 21.9%, 291.4% and 436.2% higher than the ones of m- -Greedy, m-UCB, m-Myopic and Random, respectively. A comparison with Figure 6 shows that the behaviour of the algorithms is similar to the case without service differentiation.\nFinally, we investigate the extension of the algorithm to rating-based context-aware proactive caching. We use the same data set as above, but while we neglected the user ratings from the original data set so far by only counting every rating in the original data set as a content request, here, we take the users\u2019 ratings into account as defined in (7). We do not consider service differentiation, but it could be applied here as well. Figure 9 shows the aggregated ratings achieved by all algorithms for a cache size of m = 200 movies as a function of time. The long-term behavior shows that m-CAC outperforms m-UCB and m- -Greedy, while m-Myopic and Random perform again much worse."}, {"heading": "IX. CONCLUSION", "text": "In this paper, we presented a context-aware proactive caching algorithm based on contextual multi-armed bandits for cache content placement at a local caching entity in a wireless network. To cope with unknown and fluctuating content popularity among the permanently changing local user population, the algorithm regularly observes context information of connected users, updates the cache content and observes the demands for cached content. Thereby, the algorithm learns context-specific content popularity online over time, which allows for a proactive adaptation of cache content according to fluctuating local content popularity. We derived a sub-linear regret\n28\nbound, which characterizes how fast the algorithm converges to optimal cache content placement and proves that our proposed algorithm asymptotically maximizes the average number of cache hits. Moreover, the algorithm supports customer prioritization. Additionally, the algorithm can be combined with multicast transmissions and it can be extended to allow rating-based caching decisions. Numerical studies showed that by exploiting contextual information, our algorithm outperforms state-of-the-art algorithms in a real world data set."}, {"heading": "ACKNOWLEDGMENT", "text": "The work by Sabrina Mu\u0308ller and Anja Klein has been funded by the German Research Foundation (DFG) as part of project B03 within the Collaborative Research Center (CRC) 1053 \u2013 MAKI. The work by Onur Atan and Mihaela van der Schaar is supported by a DDDAS Airforce grant."}], "references": [{"title": "Content delivery networks: status and trends", "author": ["A. Vakali", "G. Pallis"], "venue": "IEEE Internet Computing, vol. 7, no. 6, pp. 68\u201374, Nov. 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Globally distributed content delivery", "author": ["J. Dilley", "B. Maggs", "J. Parikh", "H. Prokop", "R. Sitaraman", "B. Weihl"], "venue": "IEEE Internet Computing, vol. 6, no. 5, pp. 50\u201358, Sep. 2002.  29", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Distributed caching algorithms for content distribution networks", "author": ["S. Borst", "V. Gupta", "A. Walid"], "venue": "Proc. IEEE INFOCOM, 2010, pp. 1\u20139.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Cache in the air: exploiting content caching and delivery techniques for 5G systems", "author": ["X. Wang", "M. Chen", "T. Taleb", "A. Ksentini", "V. Leung"], "venue": "IEEE Communications Magazine, vol. 52, no. 2, pp. 131\u2013139, Feb. 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Web caching and zipf-like distributions: evidence and implications", "author": ["L. Breslau", "P. Cao", "L. Fan", "G. Phillips", "S. Shenker"], "venue": "Proc. IEEE INFOCOM, vol. 1, 1999, pp. 126\u2013134.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "To cache or not to cache: The 3G case", "author": ["J. Erman", "A. Gerber", "M. Hajiaghayi", "D. Pei", "S. Sen", "O. Spatscheck"], "venue": "IEEE Internet Computing, vol. 15, no. 2, pp. 27\u201334, Mar. 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Fundamental limits of caching", "author": ["M. Maddah-Ali", "U. Niesen"], "venue": "IEEE Transactions on Information Theory, vol. 60, no. 5, pp. 2856\u20132867, May 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Femtocaching and device-to-device collaboration: A new architecture for wireless video distribution", "author": ["N. Golrezaei", "A. Molisch", "A. Dimakis", "G. Caire"], "venue": "IEEE Communications Magazine, vol. 51, no. 4, pp. 142\u2013149, Apr. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Femtocaching: Wireless content delivery through distributed caching helpers", "author": ["K. Shanmugam", "N. Golrezaei", "A. Dimakis", "A. Molisch", "G. Caire"], "venue": "IEEE Transactions on Information Theory, vol. 59, no. 12, pp. 8402\u20138413, Dec. 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting user mobility for wireless content delivery", "author": ["K. Poularakis", "L. Tassiulas"], "venue": "Proc. IEEE International Symposium on Information Theory (ISIT), 2013, pp. 1017\u20131021.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting caching and multicast for 5G wireless networks", "author": ["K. Poularakis", "G. Iosifidis", "V. Sourlas", "L. Tassiulas"], "venue": "IEEE Transactions on Wireless Communications, vol. 15, no. 4, pp. 2995\u20133007, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Cache-enabled small cell networks: Modeling and tradeoffs", "author": ["E. Bastug", "M. Bennis", "M. Debbah"], "venue": "Proc. International Symposium on Wireless Communications Systems (ISWCS), 2014, pp. 649\u2013653.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Living on the edge: The role of proactive caching in 5G wireless networks", "author": ["\u2014\u2014"], "venue": "IEEE Communications Magazine, vol. 52, no. 8, pp. 82\u201389, Aug. 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Big data meets telcos: A proactive caching perspective", "author": ["E. Bastug", "M. Bennis", "E. Zeydan", "M. Abdel Kader", "A. Karatepe", "A. Salih Er", "M. Debbah"], "venue": "Journal of Communications and Networks, Special Issue on Big Data Networking- Challenges and Applications, vol. 17, no. 6, pp. 549\u2013558, Dec. 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning-based optimization of cache content in a small cell base station", "author": ["P. Blasco", "D. G\u00fcnd\u00fcz"], "venue": "Proc. IEEE International Conference on Communications (ICC), 2014, pp. 1897\u20131903.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-armed bandit optimization of cache content in wireless infostation networks", "author": ["\u2014\u2014"], "venue": "Proc. IEEE International Symposium on Information Theory (ISIT), 2014, pp. 51\u201355.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Content-level selective offloading in heterogeneous networks: Multi-armed bandit optimization and regret bounds", "author": ["\u2014\u2014"], "venue": "arXiv preprint, arXiv: 1407.6154, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning distributed caching strategies in small cell networks", "author": ["A. Sengupta", "S. Amuru", "R. Tandon", "R. Buehrer", "T. Clancy"], "venue": "Proc. IEEE International Symposium on Wireless Communications Systems (ISWCS), 2014, pp. 917\u2013921.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Content-aware user clustering and caching in wireless small cell networks", "author": ["M. ElBamby", "M. Bennis", "W. Saad", "M. Latva-aho"], "venue": "Proc. IEEE International Symposium on Wireless Communications Systems (ISWCS), 2014, pp. 945\u2013949.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Smart caching in wireless small cell networks via contextual multi-armed bandits", "author": ["S. M\u00fcller", "O. Atan", "M. van der Schaar", "A. Klein"], "venue": "Proc. IEEE International Conference on Communications (ICC), 2016, pp. 1\u20137.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Infostations: a new system model for data and messaging services", "author": ["D. Goodman", "J. Borras", "N.B. Mandayam", "R. Yates"], "venue": "Proc. IEEE Vehicular Technology Conference, vol. 2, May 1997, pp. 969\u2013973.  30", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Youtube traffic characterization: A view from the edge", "author": ["P. Gill", "M. Arlitt", "Z. Li", "A. Mahanti"], "venue": "Proc. ACM SIGCOMM Conference on Internet Measurement, ser. IMC \u201907, 2007, pp. 15\u201328.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Characteristics of youtube network traffic at a campus network - measurements, models, and implications", "author": ["M. Zink", "K. Suh", "Y. Gu", "J. Kurose"], "venue": "Comput. Netw., vol. 53, no. 4, pp. 501\u2013514, Mar. 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "YouTube around the world: Geographic popularity of videos", "author": ["A. Brodersen", "S. Scellato", "M. Wattenhofer"], "venue": "Proc. ACM International Conference on World Wide Web, 2012, pp. 241\u2013250.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Characterizing user watching behavior and video quality in mobile devices", "author": ["C. Zhou", "Y. Guo", "Y. Chen", "X. Nie", "W. Zhu"], "venue": "Proc. IEEE International Conference on Computer Communication and Networks (ICCCN), Aug 2014, pp. 1\u20136.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable service differentiation in a shared storage cache", "author": ["B.-J. Ko", "K.-W. Lee", "K. Amiri", "S. Calo"], "venue": "Proc. IEEE International Conference on Distributed Computing Systems, May 2003, pp. 184\u2013193.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Design, implementation, and evaluation of differentiated caching services", "author": ["Y. Lu", "T.F. Abdelzaher", "A. Saxena"], "venue": "IEEE Trans. Parallel Distrib. Syst., vol. 15, no. 5, pp. 440\u2013452, May 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Cost-aware www proxy caching algorithms", "author": ["P. Cao", "S. Irani"], "venue": "Proc. USENIX Symposium on Internet Technologies and Systems, 1997, pp. 193\u2013206.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1997}, {"title": "Popularity-driven content caching", "author": ["S. Li", "J. Xu", "M. van der Schaar", "W. Li"], "venue": "Proc. IEEE INFOCOM, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Contextual multi-armed bandits", "author": ["T. Lu", "D. Pal", "M. Pal"], "venue": "Proc. International Conference on Artificial Intelligence and Statistics (AISTATS), 2010, pp. 485\u2013492.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed online learning via cooperative contextual bandits", "author": ["C. Tekin", "M. van der Schaar"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no. 14, pp. 3700\u20133714, Mar. 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Mach. Learn., vol. 47, no. 2-3, pp. 235\u2013256, May 2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association, vol. 58, no. 301, pp. 13\u201330, 1963.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1963}, {"title": "An approximate formula for a partial sum of the divergent p-series", "author": ["E. Chlebus"], "venue": "Applied Mathematics Letters, vol. 22, no. 5, pp. 732 \u2013 737, May 2009.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 19, "context": "Part of this work has been published at IEEE International Conference on Communications (ICC) 2016 [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "In response to the requirements imposed by the growing demand for content of various types, which has to be delivered to a large number of end users, Content Delivery Networks (CDNs) were developed to serve data traffic [3]\u2013[5].", "startOffset": 220, "endOffset": 223}, {"referenceID": 2, "context": "In response to the requirements imposed by the growing demand for content of various types, which has to be delivered to a large number of end users, Content Delivery Networks (CDNs) were developed to serve data traffic [3]\u2013[5].", "startOffset": 224, "endOffset": 227}, {"referenceID": 3, "context": "However, despite recent advancements in cellular mobile radio networks, these networks cannot keep up with the massive growth of mobile data traffic [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "This is not only due to decreasing disk storage prices, but also due to the fact that typically only a small number of very popular contents account for the majority of data traffic [7].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "Within wireless networks, caching at the edge has been extensively studied [8]\u2013[22].", "startOffset": 75, "endOffset": 78}, {"referenceID": 19, "context": "Within wireless networks, caching at the edge has been extensively studied [8]\u2013[22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "The second type of caching entities are wireless infostations that provide high bandwidth local data communication [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Cachingenabled infostations could be owned by content providers (CPs) to provide their end users with higher quality of experience or alternatively, third parties could offer caching at infostations as a service to CPs or their end users [19].", "startOffset": 238, "endOffset": 242}, {"referenceID": 12, "context": "Firstly, optimal cache content placement primarily depends on the content popularity distribution, but when caching content at a particular point in time, it is unknown which exact content will be requested in future and even an estimation of the content popularity distribution might not be at hand, but has to be learned by the local caching entity itself [15]\u2013[22].", "startOffset": 358, "endOffset": 362}, {"referenceID": 19, "context": "Firstly, optimal cache content placement primarily depends on the content popularity distribution, but when caching content at a particular point in time, it is unknown which exact content will be requested in future and even an estimation of the content popularity distribution might not be at hand, but has to be learned by the local caching entity itself [15]\u2013[22].", "startOffset": 363, "endOffset": 367}, {"referenceID": 21, "context": "More importantly, content popularity at a local caching entity might not even replicate global content popularity as monitored by the global multimedia application [24]\u2013[26].", "startOffset": 164, "endOffset": 168}, {"referenceID": 23, "context": "More importantly, content popularity at a local caching entity might not even replicate global content popularity as monitored by the global multimedia application [24]\u2013[26].", "startOffset": 169, "endOffset": 173}, {"referenceID": 24, "context": "The users\u2019 interests may depend on various factors, such as their personal characteristics, their devices\u2019 characteristics [27] or environmental conditions [26].", "startOffset": 123, "endOffset": 127}, {"referenceID": 23, "context": "The users\u2019 interests may depend on various factors, such as their personal characteristics, their devices\u2019 characteristics [27] or environmental conditions [26].", "startOffset": 156, "endOffset": 160}, {"referenceID": 25, "context": ", by optimizing cache content according to different prioritization levels [28], [29].", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": ", by optimizing cache content according to different prioritization levels [28], [29].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Common examples of cache replacement algorithms are Least Recently Used (LRU) or Least Frequently Used (LFU) (for a survey, see [30]).", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "[15], [16] [17]\u2013[19] [20] [21] This work", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15], [16] [17]\u2013[19] [20] [21] This work", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "[15], [16] [17]\u2013[19] [20] [21] This work", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "[15], [16] [17]\u2013[19] [20] [21] This work", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "[15], [16] [17]\u2013[19] [20] [21] This work", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "[15], [16] [17]\u2013[19] [20] [21] This work", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "replacement algorithms do not consider future popularity of content, recent work has been devoted to developing sophisticated cache replacement algorithms by learning future content popularity [31].", "startOffset": 193, "endOffset": 197}, {"referenceID": 6, "context": "Reference [9] calculates information-theoretic gains achieved by combining cache content placement at end user devices with a coded multicast transmission in the content delivery phase.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "Reference [10] combines content caching at end devices with collaborative device-to-device communication to increase the efficiency of content delivery.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "In [11], an approximation algorithm is given for uncoded caching among cache-enabled SBSs which minimizes the average delay experienced by users that can be connected to several SBSs simultaneously.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Building upon the same caching architecture, in [12], an approximation algorithm is presented for minimizing the probability that moving users have to request parts of content from the MBS instead of the SBSs in distributed coded cache content placement.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "In [13], a multicast-aware caching scheme is proposed for minimizing the energy consumption in a small cell network, in which both the MBS as well as the SBSs can perform multicast transmissions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Reference [14] analytically calculates the outage probability and average content delivery rate in a network of SBSs equipped with caches.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Driven by a proactive caching paradigm, [15], [16] propose a cache content placement algorithm for small cell networks based on collaborative filtering.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "Driven by a proactive caching paradigm, [15], [16] propose a cache content placement algorithm for small cell networks based on collaborative filtering.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "In [17], using a multi-armed bandit algorithm, an SBS learns a fixed content popularity online by refreshing its cache content and observing instantaneous demands for cached files over time.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "The authors extend their framework for a wireless infostation network in [18], [19], where they additionally take into account the costs for adding new files to the cache.", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "The authors extend their framework for a wireless infostation network in [18], [19], where they additionally take into account the costs for adding new files to the cache.", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "A different extension of the multi-armed bandit framework is given in [20], which by incorporating coded cache content placement exploits the topology of users\u2019 connections to the SBSs.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "The approach in [20] assumes a specific model of content popularity distribution.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Moreover, in [17]\u2013[20], the optimal caching strategy is learned over time based only on previous observations of instantaneous demands.", "startOffset": 13, "endOffset": 17}, {"referenceID": 17, "context": "Moreover, in [17]\u2013[20], the optimal caching strategy is learned over time based only on previous observations of instantaneous demands.", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "Diversity in content popularity among different users is for example taken into account in [21], but again without considering the users\u2019 contexts.", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": "In our previous work [22], we presented a learning algorithm for smart caching at SBSs based on the contextual multi-armed bandit problem [32], [33], an extension of the standard multi-", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "In our previous work [22], we presented a learning algorithm for smart caching at SBSs based on the contextual multi-armed bandit problem [32], [33], an extension of the standard multi-", "startOffset": 138, "endOffset": 142}, {"referenceID": 30, "context": "In our previous work [22], we presented a learning algorithm for smart caching at SBSs based on the contextual multi-armed bandit problem [32], [33], an extension of the standard multi-", "startOffset": 144, "endOffset": 148}, {"referenceID": 31, "context": "armed bandit problem [34].", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "Our proposed algorithm in [22] was inspired by the distributed contextual learning algorithm presented in [33], which considers a general multi-agent learning setting.", "startOffset": 26, "endOffset": 30}, {"referenceID": 30, "context": "Our proposed algorithm in [22] was inspired by the distributed contextual learning algorithm presented in [33], which considers a general multi-agent learning setting.", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "While several learners select one action each at a time in [33], we considered the case of one learner taking multiple actions at a time, since the sBS has to select multiple files to cache.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "Our algorithm in [22] took only aggregated context information of connected users into account.", "startOffset": 17, "endOffset": 21}, {"referenceID": 19, "context": "In this paper, we extend our approach from [22] in order to take into account context information at a single user level, which enables more fine-grained learning.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "To inform users in its vicinity about available files, the caching entity broadcasts the information about currently cached files periodically [17]\u2013[19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "To inform users in its vicinity about available files, the caching entity broadcasts the information about currently cached files periodically [17]\u2013[19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 14, "context": "In this way, the caching entity cannot observe the requests for files that it has not cached, but at the same time, it is not congested by such requests [17]\u2013[19].", "startOffset": 153, "endOffset": 157}, {"referenceID": 16, "context": "In this way, the caching entity cannot observe the requests for files that it has not cached, but at the same time, it is not congested by such requests [17]\u2013[19].", "startOffset": 158, "endOffset": 162}, {"referenceID": 28, "context": "In this subsection, we describe the architecture for context-aware proactive caching, which is designed similarly to an architecture for popularity-driven cache content replacement presented in [31].", "startOffset": 194, "endOffset": 198}, {"referenceID": 19, "context": "Note that compared to our formulation in [22], where the context space contained context dimensions based on averaged information over all currently connected user, here, we define the context space on a per-user level.", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "However, our algorithm can be extended to profit from the broadcast nature of the wireless medium by multicasting, which has been shown to be beneficial for caching in small cell networks [13].", "startOffset": 188, "endOffset": 192}, {"referenceID": 28, "context": "For our numerical evaluations, we assume that the movie rating process in the data set corresponds to a content request process of users connected to a local caching entity (see [31] for a similar approach).", "startOffset": 178, "endOffset": 182}, {"referenceID": 31, "context": "UCB is a classical learning algorithm for multi-armed bandit problems [34], which has logarithmic regret order.", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "This is a variant of the simple -Greedy [34] algorithm, which does not consider context information.", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "This is an algorithm taken from [17], which is investigated since it is comparable to the well known Least Recently Used algorithm (LRU) for caching.", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "Hence, the cache size corresponds to about 5% of the overall number of files [17].", "startOffset": 77, "endOffset": 81}], "year": 2016, "abstractText": "Content caching in small base stations (SBSs) or wireless infostations is considered as a suitable approach to improve the efficiency in wireless content delivery. Due to storage limitations, placing the optimal content into local caches is crucial. Cache content placement is challenging since it requires knowledge about the content popularity distribution, which is often not available in advance. Moreover, content popularity is subject to fluctuations as mobile users with different interests connect to the caching entity over time. In this paper, we propose a novel algorithm for context-aware proactive cache content placement. By regularly observing context information of connected users, updating the cache content accordingly and observing the demands for cache content subsequently, the algorithm learns contextspecific content popularity online over time. We derive a sub-linear regret bound, which characterizes the learning speed and proves that our algorithm asymptotically maximizes the average number of cache hits. Furthermore, our algorithm supports service differentiation by allowing operators of caching entities to prioritize groups of customers. Our numerical results confirm that by exploiting contextual information, our algorithm outperforms state-of-the-art algorithms in a real world data set, with an increase in the number of cache hits of at least 14%.", "creator": "LaTeX with hyperref package"}}}