{"id": "1606.05233", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2016", "title": "Learning feed-forward one-shot learners", "abstract": "One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.", "histories": [["v1", "Thu, 16 Jun 2016 15:49:26 GMT  (2553kb,D)", "http://arxiv.org/abs/1606.05233v1", "The first three authors contributed equally, and are listed in alphabetical order"]], "COMMENTS": "The first three authors contributed equally, and are listed in alphabetical order", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["luca bertinetto", "jo\u00e3o f henriques", "jack valmadre", "philip h s torr", "andrea vedaldi"], "accepted": true, "id": "1606.05233"}, "pdf": {"name": "1606.05233.pdf", "metadata": {"source": "CRF", "title": "Learning feed-forward one-shot learners", "authors": ["Luca Bertinetto", "Jo\u00e3o F. Henriques", "Jack Valmadre"], "emails": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "jvlmdr@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Deep learning methods have taken by storm areas such as computer vision, natural language processing, and speech recognition. One of their key strengths is the ability to leverage large quantities of labelled data and extract meaningful and powerful representations from it. However, this capability is also one of their most significant limitations since using large datasets to train deep neural network is not just an option, but a necessity. It is well known, in fact, that these models are prone to overfitting.\nThus, deep networks seem less useful when the goal is to learn a new concept on the fly, from a few or even a single example as in one shot learning. These problems are usually tackled by using generative models [18, 12] or, in a discriminative setting, using ad-hoc solutions such as exemplar support vector machines (SVMs) [14]. Perhaps the most common discriminative approach to one-shot learning is to learn off-line a deep embedding function and then to define on-line simple classification rules such as nearest neighbors in the embedding space [4, 16, 13]. However, computing an embedding is a far cry from learning a model of the new object.\nIn this paper, we take a very different approach and ask whether we can induce, from a single supervised example, a full, deep discriminative model to recognize other instances of the same object class. Furthermore, we do not want our solution to require a lengthy optimization process, but to be computable on-the-fly, efficiently and in one go. We formulate this problem as the one of learning a\n\u2217The first three authors contributed equally, and are listed in alphabetical order.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 6.\n05 23\n3v 1\n[ cs\n.C V\n] 1\n6 Ju\ndeep neural network, called a learnet, that, given a single exemplar of a new object class, predicts the parameters of a second network that can recognize other objects of the same type.\nOur model has several elements of interest. Firstly, if we consider learning to be any process that maps a set of images to the parameters of a model, then it can be seen as a \u201clearning to learn\u201d approach. Clearly, learning from a single exemplar is only possible given sufficient prior knowledge on the learning domain. This prior knowledge is incorporated in the learnet in an off-line phase by solving millions of small one-shot learning tasks and back-propagating errors end-to-end. Secondly, our learnet provides a feed-forward learning algorithm that extracts from the available exemplar the final model parameters in one go. This is different from iterative approaches such as exemplar SVMs or complex inference processes in generative modeling. It also demonstrates that deep neural networks can learn at the \u201cmeta-level\u201d of predicting filter parameters for a second network, which we consider to be an interesting result in its own right. Thirdly, our method provides a competitive, efficient, and practical way of performing one-shot learning using discriminative methods.\nThe rest of the paper is organized as follows. Sect. 1.1 discusses the works most related to our. Sect. 2 describes the learnet approaches and nuances in its implementation. Sect. 3 demonstrates empirically the potential of the method in image classification and visual tracking tasks. Finally, sect. 4 summarizes our findings."}, {"heading": "1.1 Related work", "text": "Our work is related to several others in the literature. However, we believe to be the first to look at methods that can learn the parameters of complex discriminative models in one shot.\nOne-shot learning has been widely studied in the context of generative modeling, which unlike our work is often not focused on solving discriminative tasks. One very recent example is by Rezende et al. [18], which uses a recurrent spatial attention model to generate images, and learns by optimizing a measure of reconstruction error using variational inference [8]. They demonstrate results by sampling images of novel classes from this generative model, not by solving discriminative tasks. Another notable work is by Lake et al. [12], which instead uses a probabilistic program as a generative model. This model constructs written characters as compositions of pen strokes, so although more general programs can be envisioned, they demonstrate it only on Optical Character Recognition (OCR) applications.\nA different approach to one-shot-learning is to learn an embedding space, which is typically done with a siamese network [1]. Given an exemplar of a novel category, classification is performed in the embedding space by a simple rule such as nearest-neighbor. Training is usually performed by classifying pairs according to distance [4], or by enforcing a distance ranking with a triplet loss [16]. A variant is to combine embeddings using the outer-product, which yields a bilinear classification rule [13].\nThe literature on zero-shot learning (as opposed to one-shot learning) has a different focus, and thus different methodologies. It consists of learning a new object class without any example image, but based solely on a description such as binary attributes or text. It is usually framed as a modality transfer problem and solved through transfer learning [20].\nThe general idea of predicting parameters has been explored before by Denil et al. [3], who showed that it is possible to linearly predict as much as 95% of the parameters in a layer given the remaining 5%. This is a very different proposition from ours, which is to predict all of the parameters of a layer given an external exemplar image, and to do so non-linearly.\nOur proposal allows generating all the parameters from scratch, generalizing across tasks defined by different exemplars, and can be seen as a network that effectively \u201clearns to learn\u201d."}, {"heading": "2 One-shot learning as dynamic parameter prediction", "text": "Since we consider one-shot learning as a discriminative task, our starting point is standard discriminative learning. It generally consists of finding the parameters W that minimize the average loss L of\na predictor function \u03d5(x; W ), computed over a dataset of n samples xi and corresponding labels `i:\nmin W\n1\nn n\u2211 i=1 L(\u03d5(xi; W ), `i). (1)\nUnless the model space is very small, generalization also requires constraining the choice of model, usually via regularization. However, in the extreme case in which the goal is to learn W from a single exemplar z of the class of interest, called one-shot learning, even regularization may be insufficient and additional prior information must be injected into the learning process. The main challenge in discriminative one-shot learning is to find a mechanism to incorporate domain-specific information in the learner, i.e. learning to learn. Another challenge, which is of practical importance in applications of one-shot learning, is to avoid a lengthy optimization process such as eq. (1).\nWe propose to address both challenges by learning the parameters W of the predictor from a single exemplar z using a meta-prediction process, i.e. a non-iterative feed-forward function \u03c9 that maps (z; W \u2032) to W . Since in practice this function will be implemented using a deep neural network, we call it a learnet. The learnet depends on the exemplar z, which is a single representative of the class of interest, and contains parametersW \u2032 of its own. Learning to learn can now be posed as the problem of optimizing the learnet meta-parameters W \u2032 using an objective function defined below. Furthermore, the feed-forward learnet evaluation is much faster than solving the optimization problem (1).\nIn order to train the learnet, we require the latter to produce good predictors given any possible exemplar z, which is empirically evaluated as an average over n training samples zi:\nmin W \u2032\n1\nn n\u2211 i=1 L(\u03d5(xi; \u03c9(zi; W \u2032)), `i). (2)\nIn this expression, the performance of the predictor extracted by the learnet from the exemplar zi is assessed on a single \u201cvalidation\u201d pair (xi, `i), comprising another exemplar and its label `i. Hence, the training data consists of triplets (xi, zi, `i). Notice that the meaning of the label `i is subtly different from eq. (1) since the class of interest changes depending on the exemplar zi: `i is positive when xi and zi belong to the same class and negative otherwise. Triplets are sampled uniformly with respect to these two cases. Importantly, the parameters of the original predictor \u03d5 of eq. (1) now change dynamically with each exemplar zi.\nNote that the training data is reminiscent of that of siamese networks [1], which also learn from labeled sample pairs. However, siamese networks apply the same model \u03d5(x; W ) with shared weights W to both xi and zi, and compute their inner-product to produce a similarity score:\nmin W\n1\nn n\u2211 i=1 L(\u3008\u03d5(xi; W ), \u03d5(zi; W )\u3009, `i). (3)\nThere are two key differences with our model. First, we treat xi and zi asymmetrically, which results in a different objective function. Second, and most importantly, the output of \u03c9(z; W \u2032) is used to parametrize linear layers that determine the intermediate representations in the network \u03d5. This is significantly different to computing a single inner product in the last layer (eq. (3)). A similar argument can be made of bilinear networks [13].\nEq. (2) specifies the optimization objective of one-shot learning as dynamic parameter prediction. By application of the chain rule, backpropagating derivatives through the computational blocks of \u03d5(x; W ) and \u03c9(z; W \u2032) is no more difficult than through any other standard deep network. Nevertheless, when we dive into concrete implementations of such models we face a peculiar challenge, discussed next."}, {"heading": "2.1 The challenge of naive parameter prediction", "text": "In order to analyse the practical difficulties of implementing a learnet, we will begin with one-shot prediction of a fully-connected layer, as it is simpler to analyse. This is given by\ny = Wx+ b, (4)\ngiven an input x \u2208 Rd, output y \u2208 Rk, weights W \u2208 Rd\u00d7k and biases b \u2208 Rk.\nWe now replace the weights and biases with their functional counterparts, w(z) and b(z), representing two outputs of the learnet \u03c9(z; W \u2032) given the exemplar z \u2208 Rm as input (to avoid clutter, we omit the implicit dependence on W \u2032):\ny = w(z)x+ b(z). (5)\nWhile eq. (5) seems to be a drop-in replacement for linear layers, careful analysis reveals that it scales extremely poorly. The main cause is the unusually large output space of the learnet w : Rm \u2192 Rd\u00d7k. For a comparable number of input and output units in a linear layer (d ' k), the output space of the learnet grows quadratically with the number of units.\nWhile this may seem to be a concern only for large networks, it is actually extremely difficult also for networks with few units. Consider a simple linear learnet w(z) = W \u2032z. Even for a very small fullyconnected layer of only 100 units (d = k = 100), and an exemplar z with 100 features (m = 100), the learnet already contains 1M parameters that must be learned. Overfitting and space and time costs make learning such a regressor infeasible. Furthermore, reducing the number of features in the exemplar can only achieve a small constant-size reduction on the total number of parameters. The bottleneck is the quadratic size of the output space dk, not the size of the input space m."}, {"heading": "2.2 Factorized linear layers", "text": "A simple way to reduce the size of the output space is to consider a factorized set of weights, by replacing eq. (5) with:\ny = M \u2032 diag (w(z))Mx+ b(z). (6)\nThe product M \u2032diag (w(z))M can be seen as a factorized representation of the weights, analogous to the Singular Value Decomposition. The matrix M \u2208 Rd\u00d7d projects x into a space where the elements of w(z) represent disentangled factors of variation. The second projection M \u2032 \u2208 Rd\u00d7k maps the result back from this space.\nBoth M and M \u2032 contain additional parameters to be learned, but they are modest in size compared to the case discussed in sect. 2.1. Importantly, the one-shot branch w(z) now only has to predict a set of diagonal elements (see eq. (6)), so its output space grows linearly with the number of units in the layer (i.e. w(z): Rm \u2192 Rd)."}, {"heading": "2.3 Factorized convolutional layers", "text": "The factorization of eq. (6) can be generalized to convolutional layers as follows. Given an input tensor x \u2208 Rr\u00d7c\u00d7d, weights W \u2208 Rf\u00d7f\u00d7d\u00d7k (where f is the filter support size), and biases b \u2208 Rk, the output y \u2208 Rr\u2032\u00d7c\u2032\u00d7k of the convolutional layer is given by\ny = W \u2217 x+ b, (7)\nwhere \u2217 denotes convolution, and the biases b are applied to each of the k channels. Projections analogous to M and M \u2032 in eq. (6) can be incorporated in the filter bank in different ways and it is not obvious which one to pick. Here we take the view that M and M \u2032 should disentangle the feature channels (i.e. third dimension of x), allowing w(z) to choose which filter to apply to each channel. As such, we consider the following factorization:\ny = M \u2032 \u2217 w(z) \u2217d M \u2217 x+ b(z), (8)\nwhere M \u2208 R1\u00d71\u00d7d\u00d7d, M \u2032 \u2208 R1\u00d71\u00d7d\u00d7k, and w(z) \u2208 Rf\u00d7f\u00d7d. Convolution with subscript d denotes independent filtering of d channels, i.e. each channel of x \u2217d y is simply the convolution of the corresponding channel in x and y. In practice, this can be achieved with filter tensors that are diagonal in the third and fourth dimensions, or using d filter groups [11], each group containing a single filter. An illustration is given in fig. 1. The predicted filters w(z) can be interpreted as a filter basis, as described in the supplementary material (sec. A).\nNotice that, under this factorization, the number of elements to be predicted by the one-shot branch w(z) is only f2d (the filter size f is typically very small, e.g. 3 or 5 [4, 21]). Without the factorization, it would be f2dk (the number of elements of W in eq. (7)). Similarly to the case of fully-connected layers (sect. 2.2), when d ' k this keeps the number of predicted elements from growing quadratically with the number of channels, allowing them to grow only linearly.\nExamples of filters that are predicted by learnets are shown in figs. 3 and 4. The resulting activations confirm that the networks induced by different exemplars do indeed possess different internal representations of the same input."}, {"heading": "3 Experiments", "text": "We evaluate learnets against baseline one-shot architectures (sect. 3.1) on two one-shot learning problems in Optical Character Recognition (OCR; sect. 3.2) and visual object tracking (sect. 3.3)."}, {"heading": "3.1 Architectures", "text": "As noted in sect. 2, the closest competitors to our method in discriminative one-shot learning are embedding learning using siamese architectures. Therefore, we structure the experiments to compare against this baseline. In particular, we choose to implement learnets using similar network topologies for a fairer comparison.\nThe baseline siamese architecture comprises two parallel streams \u03d5(x;W ) and \u03d5(z;W ) composed of a number of layers, such as convolution, max-pooling, and ReLU, sharing parameters W (fig. 2.a). The outputs of the two streams are compared by a layer \u0393(\u03d5(x;W ), \u03d5(z;W )) computing a measure of similarity or dissimilarity. We consider in particular: the dot product \u3008a, b\u3009 between vectors a and b, the Euclidean distance \u2016a\u2212 b\u2016, and the weighted l1-norm \u2016w a\u2212 w b\u20161 where w is a vector of learnable weights and the Hadamard product). The first modification to the siamese baseline is to use a learnet to predict some of the intermediate shared stream parameters (fig. 2.b). In this case W = \u03c9(z;W \u2032) and the siamese architecture writes \u0393(\u03d5(x;\u03c9(z;W \u2032)), \u03d5(z;\u03c9(z;W \u2032))). Note that the siamese parameters are still the same in the two streams, whereas the learnet is an entirely new subnetwork whose purpose is to map the exemplar image to the shared weights. We call this model the siamese learnet.\nThe second modification is a single-stream learnet configuration, using only one stream \u03d5 of the siamese architecture and predicting its parameter using the learnet \u03c9. In this case, the comparison block \u0393 is reinterpreted as the last layer of the stream \u03d5 (fig. 2.c). Note that: i) the single predicted stream and learnet are asymmetric and with different parameters and ii) the learnet predicts both the final comparison layer parameters \u0393 as well as intermediate filter parameters.\nThe single-stream learnet architecture can be understood to predict a discriminant function from one example, and the siamese learnet architecture to predict an embedding function for the comparison of two images. These two variants demonstrate the versatility of the dynamic convolutional layer from eq. (6).\nFinally, in order to ensure that any difference in performance is not simply due to the asymmetry of the learnet architecture or to the induced filter factorizations (sect. 2.2 and sect. 2.3), we also compare unshared siamese nets, which use distinct parameters for each stream, and factorized siamese nets, where convolutions are replaced by factorized convolutions as in learnet."}, {"heading": "3.2 Character recognition in foreign alphabets", "text": "This section describes our experiments in one-shot learning on OCR. For this, we use the Omniglot dataset [12], which contains images of handwritten characters from 50 different alphabets. These alphabets are divided into 30 background and 20 evaluation alphabets. The associated one-shot learning problem is to develop a method for determining whether, given any single exemplar of a character in an evaluation alphabet, any other image in that alphabet represents the same character or not. Importantly, all methods are trained using only background alphabets and tested on the evaluation alphabets.\nDataset and evaluation protocol. Character images are resized to 28\u00d7 28 pixels in order to be able to explore efficiently several variants of the proposed architectures. There are exactly 20 sample images for each character, and an average of 32 characters per alphabet. The dataset contains a total of 19,280 images in the background alphabets and 13,180 in the evaluation alphabets.\nAlgorithms are evaluated on a series of recognition problems. Each recognition problem involves identifying the image in a set of 20 that shows the same character as an exemplar image (there is always exactly one match). All of the characters in a single problem belong to the same alphabet. At test time, given a collection of characters (x1, . . . , xm), the function is evaluated on each pair (z, xi) and the candidate with the highest score is declared the match. In the case of the learnet architectures, this can be interpreted as obtaining the parameters W = \u03c9(z;W \u2032) and then evaluating a static network \u03d5(xi;W ) for each xi.\nArchitecture. The baseline stream \u03d5 for the siamese, siamese learnet, and single-stream learnet architecture consists of 3 convolutional layers, with 2\u00d7 2 max-pooling layers of stride 2 between them. The filter sizes are 5\u00d7 5\u00d7 1\u00d7 16, 5\u00d7 5\u00d7 16\u00d7 64 and 4\u00d7 4\u00d7 64\u00d7 512. For both the siamese learnet and the single-stream learnet, \u03c9 consists of the same layers as \u03d5, except the number of outputs is 1600 \u2013 one for each element of the 64 predicted filters (of size 5\u00d7 5). To keep the experiments simple, we only predict the parameters of one convolutional layer. We conducted cross-validation to choose the predicted layer and found that the second convolutional layer yields the best results for both of the proposed variants.\nSiamese nets have previously been applied to this problem by Koch et al. [9] using much deeper networks applied to images of size 105 \u00d7 105. However, we have restricted this investigation to relatively shallow networks to enable a thorough exploration of the parameter space. A more powerful algorithm for one-shot learning, Hierarchical Bayesian Program Learning [12], is able to achieve human-level performance. However, this approach involves computationally expensive inference at test time, and leverages extra information at training time that describes the strokes drawn by the human author.\nLearning. Learning involves minimizing the objective function specific to each method (e.g. eq. (2) for learnet and eq. (3) for siamese architectures) and uses stochastic gradient descent (SGD) in all cases. As noted in sect. 2, the objective is obtained by sampling triplets (zi, xi, `i) where exemplars zi and xi are congruous (`i = +1) or incongruous (`i = \u22121) with 50% probability. We consider 100,000 random pairs for training per epoch, and train for 60 epochs. We conducted a random search to find the best hyper-parameters for each algorithm (initial learning rate and geometric decay, standard deviation of Gaussian parameter initialization, and weight decay).\nResults and discussion. Tab. 1 shows the classification error obtained using variants of each architecture. A dash indicates a failure to converge given a large range of hyper-parameters. The two learnet architectures combined with the weighted `1 distance are able to achieve significantly better results than other methods. The best architecture reduced the error from 37.3% for a siamese network with shared parameters to 28.6% for a single-stream learnet.\nWhile the Euclidean distance gave the best results for siamese networks with shared parameters, better results were achieved by learnets (and siamese networks with unshared parameters) using a weighted `1 distance. In fact, none of the alternative architectures are able to achieve lower error under the Euclidean distance than the shared siamese net. The dot product was, in general, less effective than the other two metrics.\nThe introduction of the factorization in the convolutional layer might be expected to improve the quality of the estimated model by reducing the number of parameters, or to worsen it by diminishing the capacity of the hypothesis space. For this relatively simple task of character recognition, the factorization did not seem to have a large effect."}, {"heading": "3.3 Object tracking", "text": "The task of single-target object tracking requires to locate an object of interest in a sequence of video frames. A video frame can be seen as a collection F = {w1, . . . , wK} of image windows; then, in a one-shot setting, given an exemplar z \u2208 F1 of the object in the first frame F1, the goal is to identify the same window in the other frames F2, . . . ,FM . Datasets. The method is trained using the ImageNet Large Scale Visual Recognition Challenge 2015 [19], with 3,862 videos totalling more than one million annotated frames. Instances of objects of thirty different classes (mostly vehicles and animals) are annotated throughout each video with bounding boxes. For tracking, instance labels are retained but object class labels are ignored. We use 90% of the videos for training, while the other 10% are held-out to monitor validation error during network training. Testing uses the VOT 2015 benchmark [10].\nArchitecture. We experiment with siamese and siamese learnet architectures (fig. 2) where the learnet \u03c9 predicts the parameters of the second (dynamic) convolutional layer of the siamese streams. Each siamese stream has five convolutional layers and we test three variants of those: variant (A) has the same configuration as AlexNet [11] but with stride 2 in the first layer, and variants (B) and (C) reduce to 50% the number of filters in the first two convolutional layers and, respectively, to 25% and 12.5% the number of filters in the last layer.\nTraining. In order to train the architecture efficiently from many windows, the data is prepared as follows. Given an object bounding box sampled at random, a crop z double the size of that is extracted from the corresponding frame, padding with the average image color when needed. The border is included in order to incorporate some visual context around the exemplar object. Next, ` \u2208 {+1,\u22121} is sampled at random with 75% probability of being positive. If ` = \u22121, an image x is extracted by choosing at random a frame that does not contain the object. Otherwise, a second frame containing the same object and within 50 temporal samples of the first is selected at random. From that, a patch x centered around the object and four times bigger is extracted. In this way, x\ncontains both subwindows that do and do not match z. Images z and x are resized to 127\u00d7 127 and 255\u00d7 255 pixels, respectively, and the triplet (z, x, `) is formed. All 127\u00d7 127 subwindows in x are considered to not match z except for the central 2\u00d7 2 ones when ` = +1. All networks are trained from scratch using SGD for 50 epoch of 50,000 sample triplets (zi, xi, `i). The multiple windows contained in x are compared to z efficiently by making the comparison layer \u0393 convolutional (fig. 2), accumulating a logistic loss across spatial locations. The same hyperparameters (learning rate of 10\u22122 geometrically decaying to 10\u22125, weight decay of 0.005, and small mini-batches of size 8) are used for all experiments, which we found to work well for both the baseline and proposed architectures. The weights are initialized using the improved Xavier [5] method, and we use batch normalization [7] after all linear layers.\nTesting. Adopting the initial crop as exemplar, the object is sought in a new frame within a radius of the previous position, proceeding sequentially. This is done by evaluating the pupil net convolutionally, as well as searching at five possible scales in order to track the object through scale space.\nResults and discussion. Tab. 3 compares the methods in terms of the official metrics (accuracy and number of failures) for the VOT 2015 benchmark [10]. The ranking plot produced by the VOT toolkit is provided in the supplementary material (fig. B.1). From tab. 3, it can be observed that factorizing the filters in the siamese architecture significantly diminishes its performance, but using a learnet to predict the filters in the factorization recovers this gap and in fact achieves better performance than the original siamese net. The performance of the learnet architectures is not adversely affected by using the slimmer prediction networks B and C (with less channels).\nAn elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21]. SO-DLT in particular is a good example of direct adaptation of standard batch deep learning methodology to online learning, as it uses SGD during tracking to fine-tune an ensemble of deep convolutional networks. However, the online adaptation of the model comes at a big computational cost and affects the speed of the method, which runs at 5 frames-persecond (FPS) on a GPU. Due to the feed-forward nature of our one-shot learnets, they can track objects in real-time at framerates in excess of 60 FPS, while achieving less tracking failures. We consider, however, that our implementation serves mostly as a proof-of-concept, using tracking as an interesting demonstration of one-shot-learning, and is orthogonal to many technical improvements found in the tracking literature [10]."}, {"heading": "4 Conclusions", "text": "In this work, we have shown that it is possible to obtain the parameters of a deep neural network using a single, feed-forward prediction from a second network. This approach is desirable when iterative methods are too slow, and when large sets of annotated training samples are not available. We have demonstrated the feasibility of feed-forward parameter prediction in two demanding one-shot learning tasks in OCR and visual tracking. Our results hint at a promising avenue of research in \u201clearning to learn\u201d by solving millions of small discriminative problems in an offline phase. Possible extensions include domain adaptation and sharing a single learnet between different pupil networks."}, {"heading": "A Basis filters", "text": "This appendix provides an additional interpretation for the role of the predicted filters in a factorized convolutional layer (Section 2.3).\nTo make the presentation succint, we will use a notation that is slightly different from the main text. Let x be a tensor of activations, then xi denotes channel i of x. If a is a multi-channel filter, then aij denotes the filter for output channel i and input channel j. That is, if a is m\u00d7 n\u00d7 p\u00d7 q then aij is m\u00d7 n for i \u2208 [p], j \u2208 [q]. The set {1, . . . , n} is denoted [n]. The factorised convolution is y = Ax = M \u2032WMx . (9) where M and M \u2032 are pixel-wise projections and W is a diagonal convolution. While a general convolution computes\n(Av)i = \u2211\nj aij \u2217 vj (10) where each aij is a single-channel filter, a diagonal convolution computes\n(Wv)i = wi \u2217 vi (11)\nwhere each wi is a single-channel filter, and a pixel-wise projection computes (Mv)i = \u2211 j mijvj (12)\nwhere each mij is a scalar.\nLet q be the number of channels of x, let p be the number of channels of y and let r be the number of channels of the intermediate activations. Combining the above gives\n(WMx)k = wk \u2217 (\u2211 j\u2208[q]mkjxj ) = \u2211 j\u2208[q]mkjwk \u2217 xj (13)\n(M \u2032WMx)i = \u2211 k\u2208[r]m \u2032 ik (\u2211 j\u2208[q]mkjwk \u2217 xj ) = \u2211 j\u2208[q] (\u2211 k\u2208[r]m \u2032 ikmkjwk ) \u2217 xj . (14)\nThis is therefore equivalent to a general convolution y = Ax where each filter aij is a combination of r single-channel basis filters wk\naij = \u2211 k\u2208[r]m \u2032 ikmkjwk (15)\nThe predictions used in the dynamic convolution (Section 2.3) essentially modify these r basis filters."}, {"heading": "B Additional results on object tracking", "text": ""}], "references": [{"title": "Signature verification using a \u201csiamese\u201d time delay neural network", "author": ["J. Bromley", "J.W. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "E. S\u00e4ckinger", "R. Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "Accurate scale estimation for robust visual tracking", "author": ["M. Danelljan", "G. H\u00e4ger", "F. Khan", "M. Felsberg"], "venue": "BMVC,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning deep face representation", "author": ["H. Fan", "Z. Cao", "Y. Jiang", "Q. Yin", "C. Doudou"], "venue": "arXiv CoRR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ICCV,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-store tracker (MUSTER): A cognitive psychology inspired approach to object tracking", "author": ["Z. Hong", "Z. Chen", "C. Wang", "X. Mei", "D. Prokhorov", "D. Tao"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv CoRR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv CoRR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["G. Koch", "R. Zemel", "R. Salakhutdinov"], "venue": "ICML 2015 Deep Learning Workshop,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "The visual object tracking VOT2015 challenge results", "author": ["M. Kristan", "J. Matas", "A. Leonardis", "M. Felsberg", "L. Cehovin", "G. Fernandez", "T. Vojir", "G. Hager", "G. Nebehay", "R. Pflugfelder"], "venue": "ICCV Workshop,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science, 350(6266):1332\u20131338,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilinear CNN models for fine-grained visual recognition", "author": ["T.-Y. Lin", "A. RoyChowdhury", "S. Maji"], "venue": "ICCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Ensemble of exemplar-SVMs for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A.A. Efros"], "venue": "ICCV,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "arXiv CoRR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep face recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "BMVC,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "In defense of color-based model-free tracking", "author": ["H. Possegger", "T. Mauthner", "H. Bischof"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "One-shot generalization in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "I. Danihelka", "K. Gregor", "D. Wierstra"], "venue": "arXiv CoRR,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Transferring rich feature hierarchies for robust visual tracking", "author": ["N. Wang", "S. Li", "A. Gupta", "D.-Y. Yeung"], "venue": "arXiv CoRR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "MEEM: Robust tracking via multiple experts using entropy minimization", "author": ["J. Zhang", "S. Ma", "S. Sclaroff"], "venue": "ECCV.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "These problems are usually tackled by using generative models [18, 12] or, in a discriminative setting, using ad-hoc solutions such as exemplar support vector machines (SVMs) [14].", "startOffset": 62, "endOffset": 70}, {"referenceID": 11, "context": "These problems are usually tackled by using generative models [18, 12] or, in a discriminative setting, using ad-hoc solutions such as exemplar support vector machines (SVMs) [14].", "startOffset": 62, "endOffset": 70}, {"referenceID": 13, "context": "These problems are usually tackled by using generative models [18, 12] or, in a discriminative setting, using ad-hoc solutions such as exemplar support vector machines (SVMs) [14].", "startOffset": 175, "endOffset": 179}, {"referenceID": 3, "context": "Perhaps the most common discriminative approach to one-shot learning is to learn off-line a deep embedding function and then to define on-line simple classification rules such as nearest neighbors in the embedding space [4, 16, 13].", "startOffset": 220, "endOffset": 231}, {"referenceID": 15, "context": "Perhaps the most common discriminative approach to one-shot learning is to learn off-line a deep embedding function and then to define on-line simple classification rules such as nearest neighbors in the embedding space [4, 16, 13].", "startOffset": 220, "endOffset": 231}, {"referenceID": 12, "context": "Perhaps the most common discriminative approach to one-shot learning is to learn off-line a deep embedding function and then to define on-line simple classification rules such as nearest neighbors in the embedding space [4, 16, 13].", "startOffset": 220, "endOffset": 231}, {"referenceID": 17, "context": "[18], which uses a recurrent spatial attention model to generate images, and learns by optimizing a measure of reconstruction error using variational inference [8].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[18], which uses a recurrent spatial attention model to generate images, and learns by optimizing a measure of reconstruction error using variational inference [8].", "startOffset": 160, "endOffset": 163}, {"referenceID": 11, "context": "[12], which instead uses a probabilistic program as a generative model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "A different approach to one-shot-learning is to learn an embedding space, which is typically done with a siamese network [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "Training is usually performed by classifying pairs according to distance [4], or by enforcing a distance ranking with a triplet loss [16].", "startOffset": 73, "endOffset": 76}, {"referenceID": 15, "context": "Training is usually performed by classifying pairs according to distance [4], or by enforcing a distance ranking with a triplet loss [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "A variant is to combine embeddings using the outer-product, which yields a bilinear classification rule [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "It is usually framed as a modality transfer problem and solved through transfer learning [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "[3], who showed that it is possible to linearly predict as much as 95% of the parameters in a layer given the remaining 5%.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Note that the training data is reminiscent of that of siamese networks [1], which also learn from labeled sample pairs.", "startOffset": 71, "endOffset": 74}, {"referenceID": 12, "context": "A similar argument can be made of bilinear networks [13].", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "In practice, this can be achieved with filter tensors that are diagonal in the third and fourth dimensions, or using d filter groups [11], each group containing a single filter.", "startOffset": 133, "endOffset": 137}, {"referenceID": 3, "context": "3 or 5 [4, 21]).", "startOffset": 7, "endOffset": 14}, {"referenceID": 20, "context": "3 or 5 [4, 21]).", "startOffset": 7, "endOffset": 14}, {"referenceID": 11, "context": "For this, we use the Omniglot dataset [12], which contains images of handwritten characters from 50 different alphabets.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "[9] using much deeper networks applied to images of size 105 \u00d7 105.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "A more powerful algorithm for one-shot learning, Hierarchical Bayesian Program Learning [12], is able to achieve human-level performance.", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "The method is trained using the ImageNet Large Scale Visual Recognition Challenge 2015 [19], with 3,862 videos totalling more than one million annotated frames.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Testing uses the VOT 2015 benchmark [10].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Each siamese stream has five convolutional layers and we test three variants of those: variant (A) has the same configuration as AlexNet [11] but with stride 2 in the first layer, and variants (B) and (C) reduce to 50% the number of filters in the first two convolutional layers and, respectively, to 25% and 12.", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "497 93 DAT [17] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "442 113 SO-DLT [21] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "491 106 DSST [2] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 21, "context": "483 163 MEEM [22] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "458 107 MUSTer [6] 0.", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "471 132 Table 2: Tracking accuracy and number of tracking failures in the VOT 2015 Benchmark, as reported by the toolkit [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 4, "context": "The weights are initialized using the improved Xavier [5] method, and we use batch normalization [7] after all linear layers.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "The weights are initialized using the improved Xavier [5] method, and we use batch normalization [7] after all linear layers.", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "3 compares the methods in terms of the official metrics (accuracy and number of failures) for the VOT 2015 benchmark [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 180, "endOffset": 183}, {"referenceID": 21, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 190, "endOffset": 194}, {"referenceID": 5, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 203, "endOffset": 206}, {"referenceID": 20, "context": "An elementary tracker based on learnet compares favourably against recent tracking systems, which make use of different features and online model update strategies: DAT [17], DSST [2], MEEM [22], MUSTer [6] and SO-DLT [21].", "startOffset": 218, "endOffset": 222}, {"referenceID": 9, "context": "We consider, however, that our implementation serves mostly as a proof-of-concept, using tracking as an interesting demonstration of one-shot-learning, and is orthogonal to many technical improvements found in the tracking literature [10].", "startOffset": 234, "endOffset": 238}], "year": 2016, "abstractText": "One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.", "creator": "LaTeX with hyperref package"}}}