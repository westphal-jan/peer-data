{"id": "1606.06259", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos", "abstract": "People are sharing their opinions, stories and reviews through online video sharing websites every day. Studying sentiment and subjectivity in these opinion videos is experiencing a growing attention from academia and industry. While sentiment analysis has been successful for text, it is an understudied research question for videos and multimedia content. The biggest setbacks for studies in this direction are lack of a proper dataset, methodology, baselines and statistical analysis of how information from different modality sources relate to each other. This paper introduces to the scientific community the first opinion-level annotated corpus of sentiment and subjectivity analysis in online videos called Multimodal Opinion-level Sentiment Intensity dataset (MOSI). The dataset is rigorously annotated with labels for subjectivity, sentiment intensity, per-frame and per-opinion annotated visual features, and per-milliseconds annotated audio features. Furthermore, we present baselines for future studies in this direction as well as a new multimodal fusion approach that jointly models spoken words and visual gestures.", "histories": [["v1", "Mon, 20 Jun 2016 19:23:53 GMT  (750kb)", "http://arxiv.org/abs/1606.06259v1", null], ["v2", "Fri, 12 Aug 2016 02:39:40 GMT  (759kb)", "http://arxiv.org/abs/1606.06259v2", "Accepted as Journal Publication in IEEE Intelligent Systems"]], "reviews": [], "SUBJECTS": "cs.CL cs.MM", "authors": ["amir zadeh", "rowan zellers", "eli pincus", "louis-philippe morency"], "accepted": false, "id": "1606.06259"}, "pdf": {"name": "1606.06259.pdf", "metadata": {"source": "CRF", "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos", "authors": ["Amir Zadeh", "Rown Zellers", "Eli Pincus"], "emails": ["abagherz@cs.cmu.edu", "rzellers@hmc.edu", "pincus@ict.usc.edu", "morency@cs.cmu.edu"], "sections": [{"heading": null, "text": "1 \u00a0 Introduction Video sharing websites such as YouTube, Vine, and Vimeo are increasingly popular. For example,\n1 http://www.youtube.com/yt/press/statistics.html\nmore than 300 hours of video are uploaded every minute to YouTube 1 . Many people share their opinions, stories and reviews through these online video postings. This phenomenon has seen a growing attention from many companies, researchers and consumers interested in building better opinion-mining applications for summarization, question answering and video retrieval. We highlight bellow three main challenges of studying sentiment in these online opinion videos.\nFirst challenge comes from the volatile and high-tempo nature of these opinion videos where speakers will often switch between topics and opinions. This makes it challenging to identify and segment the different opinions expressed by the speakers. For example a speaker can express more than one opinion in the same spoken utterance, as in: \u201cthat was a great effect, there is a lot of cheap childish humor everyone can relate to but I thought it was hilarious\u201d.\nThe second challenge comes with the range and subtlety of sentiment intensities expressed in these opinion videos. We want approaches not only able to recognize the polarity of a video segment (e.g., positive or negative) but also estimate the strength of the expressed sentiment.\nThird challenge is a fundamental research question on how to use information more than text. To simply focus on the spoken words (e.g., text-based sentiment analysis) may bring an ambiguity that would be resolved with the visual information. For example, the sentiment expressed by the spoken opinion \u201cthis movie was different\u201d may not be clear by itself but observing a strong frown expression from the speaker can make it clear that it is negative.\nIn this paper we introduce a novel corpus for studying sentiment and subjectivity in opinion videos from online sharing websites such as YouTube2. To address the first challenge, we present a subjectivity annotation scheme for finegrained opinion segmentation in online multimedia content. 3702 video segments were reliably identified in our MOSI dataset using this scheme, including 2199 opinion segments. Sentiment in each opinion segment was annotated as a spectrum between highly positive and highly negative to address the second challenge. As enabling steps toward the third challenge, we present a multimodal study of language and gesture related to sentiment intensity that leads to the idea of multimodal dictionary. We also make available, as part of the MOSI dataset, transcriptions that were carefully synchronized with acoustic and visual features at both word and phoneme level, to ensure the usability of dataset for future multimodal studies of language.\nThe following section presents related work on text-based sentiment analysis and multimodal analysis. Section 3 introduces our new dataset for opinion-level multimodal sentiment analysis as well as primary analysis of multimodal data to find interaction patterns between words and gestures. Section 4 presents baseline for sentiment and subjectivity studies. We conclude this paper with Section 5.\n2 \u00a0 Related Work This work is connected to areas in natural language processing, multimodal analysis and emotion recognition.\nMultimodal sentiment analysis datasets YouTube Opinion Dataset created by Morency et.al. (2011) is a dataset for multimodal analysis of sentiment. It contains 47 videos from YouTube annotated for sentiment polarity at video level by three workers. The dataset consists of manually transcribed text and automatically extracted audio and visual features, as well as automatically extracted utterances. MMMO dataset (Wollmer et.al. 2013) is an extension of YouTube Opinion Dataset that extends the number of videos from 47 to 370. Spanish Multimodal Opinion Dataset created by Rosas et.al. (2013) is a Spanish multimodal sentiment analysis dataset. It consists of 105 videos annotated for sentiment polarity at ut-\n2 Videos under creative commons license can be edited and used without the need for authors consent.\nterance level. Utterances are extracted automatically based on long pauses with most videos having 6-8 utterances. The dataset contains 550 utterances in total. None of the proposed datasets have sentiment intensity annotations; they rather focus on polarity. Also they mostly focus on analysis of videos or utterances rather than fine grained analysis of sentiment as mentioned in introduction.\nText based sentiment analysis Research in text based sentiment analysis has been an active and extremely successful field (Pang and Lee, 2004, Vinodhini et.al., 2012). Among notable efforts are works done in automatically identifying opinion words and their sentiment polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), studies using n-grams and more complex language models (Takamura et al., 2006; Yang and Cardie, 2012) and works addressing sentiment compositionality by using polarity shifting rules or careful feature engineering (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). These methods have been used in many different applications including opinion mining in tweets and online forums (Pak et.al. 2010; Argawal et.al. 2011; Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). Recursive Neural Networks have also been successful in sentiment analysis. Socher et.al (2013) addressed sentiment analysis as a 5 class classification task and managed to get accuracy of 45.7% using Recursive Neural Tensor Networks. Their results show that sentiment intensity analysis is far from solved for text. All these approaches are primarily focusing on the (spoken or written) text, ignoring the other communicative modalities which are helpful when analyzing videos.\nMultimodal analysis Some earlier work introduced acoustic and paralinguistic features to the text-based analysis for the purpose of subjectivity or sentiment analysis (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009). Closer to our current study is the work reported in (Morency et al., 2011; PerezRosas et al., 2013), where multimodal cues, including visual cues, have been used for the sentiment analysis in product and movie reviews. Their approach directly concatenated modalities in an\nearly fusion representation, without studying the relations between different modalities. Their experiments were also performed in a speaker-dependent manner, with no analysis of sentiment intensity. Poria et.al. (2015) used convolutional neural networks for multimodal sentiment analysis. However, their experiments also focused on utterances rather than opinion segments and their approach was speaker dependent with focus on sentiment polarity rather than intensity.\nAudio-visual emotion recognition Among the related works are research done in the fields of affective computing and computer vision to detect emotions based on visual cues. Facial expressions are among the most powerful means of communicating emotions and intentions in human interactions (Tian et al., 2001). Interesting studies discuss findings about mental state of the speaker based on facial expressions, head gestures and other visual cues (El Kaliouby and Robinson, 2005; Baltrusaitis et.al., 2011; Calder et al., 2001; Rosenblum et al., 1996). More recently, many challenges have been organized focusing on the recognition of emotions using audio and visual cues (Dhall et al., 2014; Valstar et al., 2014), which brings the participation of many teams from around the world.\n3 \u00a0 MOSI: Multimodal Opinion-level Sentiment Intensity Corpus\nIn this section, we introduce the Multimodal Opinion-level Sentiment Intensity (MOSI) dataset which contains: (1) multimodal observations including transcribed speech and visual gestures as well as automatic audio and visual features, (2) opinion-level subjectivity segmentation, (3) sentiment intensity annotations with high coder agreement, and (4) alignment between words, visual and acoustic features. The following sub-sections are describing the dataset in more details.\n3.1 \u00a0 Acquisition Methodology Videos were collected from the YouTube website with a focus on video-blogs, or vlogs: popular update videos used by many YouTube users to express opinions about many different subjects, often indexed by #vlog. One big advantage of this type of videos is that they usually contain only one speaker, looking primarily at the camera. The videos are recorded in diverse setups, some users\n3 The link to dataset is omitted because of blind review. 4 https://github.com/srubin/p2fa-vislab\nhave high-tech microphones and cameras, while others are using less professional recording devices. Users are in different distances from the camera. Background and lighting condition is variable between videos. The videos were kept in their original resolution and recorded in MP4 format. The length of the videos vary from 2-5 minutes.\nA total of 93 videos were randomly selected using these guidelines. The final set of videos contained 89 distinct speakers, including 41 female and 48 male speakers. Most of the speakers were approximately between the ages of 20 and 30. Although the speakers were from different ethnic backgrounds (e.g., Caucasian, African-American, Hispanic, Asian), all speakers expressed themselves in English and the videos originated from either United States of America or United Kingdom. Sample snapshots of our MOSI dataset3 are shown in Figure 1.\nAll video clips are manually transcribed to extract spoken words as well as the start time of each spoken utterance. Our transcription methodology consisted of four stages. First, an expert transcriber manually transcribed all the videos followed by a second transcriber reviewing and correcting all the transcriptions. Our transcription scheme contained details about pause fillers (umm, uhh, etc.), stresses and speech pause. In the third stage, the text was aligned at word and phoneme levels with the audio using a forced aligner called P2FA4. During the final stage, the results of the alignment were manually checked and if necessary corrected using PRAAT (Paul and Weenink, 2001).\n3.2 \u00a0 Opinion-level Subjectivity Segmentation As previously discussed, an important requirement of our dataset is to perform subjectivity segmentation at the opinion level so that we can achieve fine-grained sentiment analysis. Following the work of Wiebe et.al. (2005), subjective sentences are defined as expressions of one\u2019s opinions while objective sentences express facts and truth. This section presents the formal definition of our segmentation scheme which expands their work to extract spoken opinion segments5.\nWe define subjectivity as an expression of a private state, one that is distinguishable by carrying an opinion, belief, thought, feeling, emotion,\n5 In the remaining of the paper, the terms subjective segment, opinion segment and opinion are interchangeably used and refer to the same concept.\ngoal, evaluation or judgment. The following three rules are used to define subjectivity: \u2022 \u00a0Explicit mention of a private state \u2013 a direct\nmention of private state. Example: \u201cI also love the casting of Mark Strong as Sinestro.\u201d \u2022 \u00a0Speech events expressing private states \u2013 private state has been said or written by another. Example: \u201cShia LaBeouf said that the second movie lacked um heart.\u201d \u2022 \u00a0Expressive subjective \u2013 not a direct opinion but implicit reference to an opinion. Example: \u201cI would never recommend watching this movie.\u201d To more accurately annotate the boundaries of each subjective segment, following rules have been defined, where utterances are processed sequentially. If the utterance contains expression of private state (i.e. subjective content defined as three bullet items above) the following rules of segmentation apply (brackets are used to hold segments from here on):\n\u2022 \u00a0Segment the subjective content based on number of private states revealed. For example: \u201c[I love Shawshank Redemption][and I love transformers]. While the whole utterance is subjective, proposed annotation scheme signals a clear distinction between the marked segments as the speaker shows an opinion about two different subjects, in this case movies. However if there are syntactic dependencies between the prospective segments, this rule will not apply. As an example consider \u201c[I love books and movies]\u201d versus \u201c[I love books][and I love movies]\u201d. In the first sentence, it is not possible to identify the speaker\u2019s sentiment towards movies if \u201c[and movies]\u201d is considered in isolation; so here the second private state is considered to have syntactic dependence on the first private state. However, this is not the case in the second sentence and therefore the sentence is decomposed into 2 subjective segments; one for each private state.\n\u2022 \u00a0Segment if the utterance contains a modification of a private state while maintaining the subject. Example: \u201c[Well, based on what I saw today, I feel like the movie industry is going crazy][or maybe it\u2019s just me being so hard on the poor actors.]\u201d. Here the speaker adds a corollary or addendum to his general feelings on the movie industry; that perhaps the feeling stems from him being \u201ctoo hard on the poor actors.\u201d This type of corollary or addendum is considered a modification. Another case comes up when sentence contains reasoning. This rule applies to any type of modification.\nReasoning being one example, based on rules of subjectivity a new segment should be created for reason if the sentiment is also revealed in that reason; otherwise we combine the reason with the rest of the utterance into one segment. To better elaborate consider \u201c[I love movies because of the acting]\u201d vs \u201c[I love movies][because people pretending to be someone else is funny.]\u201d. The former actually has only one opinion in it, but the latter has two; one for the person liking movies and the other one for people\u2019s acting being funny.\n\u2022 \u00a0Segment if subjective utterance ended by the start of an objective segment. For example: \u201c[In my opinion the movie was all about eating healthy food], you could see banners of different organic brands in several shots.\u201d\n\u2022 \u00a0 If there is subjective content and the subjective content extends beyond the boundary of the utterance while retaining the opinion, we merge the extension with the original utterance (the extension can be multiple sentences or part of a sentence). For example: \u201c[I dont like it! It\u2019s not a likable movie!\u201d\nThe subjectivity annotation was done by two trained annotators. The inter-annotator agreement is calculated via a discretizing measure. For each annotator we labeled each word in the transcript as a 1 if it is in a subjective segment and 0 if it is in an objective segment. This way each annotator is represented as a string of 0\u2019s and 1\u2019s. This information was then used to calculate Krippendorf\u2019s alpha (Krippendorf, 1970) to produce a final measurement of inter-coder agreement that accounts for chance agreement. The two annotations resulted in a Krippendorf\u2019s alpha of 0.68.\nTo merge the annotation results of the two annotators, an intersection approach of the two annotation sets is taken. The annotations are aligned and a segment is reported in the final set only if its\ncontent are reported as subjective by both annotators. In case the annotators have reported segments that overlap, the partial segment, from the intersection of two, is then manually extended to fit the boundaries of the sentence it occurred in. In cases that subjective segment reported by one author covers multiple segments of the other, the smaller ones are reported in the final set, because they are syntactically independent and have different opinions.\nThe subjectivity annotation resulted in 2199 subjective segments and 1503 objective ones. Both subjective and objective segments are considered for subjectivity studies while for sentiment annotations we only focus on subjective segments. Detailed statistics of the dataset and opinion segments can be found in Table 1.\n3.3 \u00a0 Sentiment Intensity Annotation Sentiment intensity is defined from strongly negative to strongly positive with a linear scale from -3 to +3. The intensity annotations were performed by online workers from Amazon Mechanical Turk website. Only master workers with approval rate of higher than 95% were selected to participate. A total of 2199 short video clips were created from the subjective opinion segments (see Section 3.2). For each video the annotators had 8 choices: strongly positive (labeled as +3), positive (+2), weakly positive (+1), neutral (0), weakly negative (-1), negative (-2), strongly negative (-3) and also they were given a choice \u201cuncertain\u201d if they were not sure.\nThe instructions were kept simple on purpose to reduce any bias. The only tutorial was on how to use the online system (e.g., how to submit the form). The task was phrased as following: \u201cHow would you rate the sentiment expressed in this video segment? (Please note that you may or may\n6 http://imotions.com/products/\nnot agree with what the speaker says. It is imperative that you only rate the sentiment state of the speaker, not yourself)\u201d. Each video clip was annotated by 5 different workers. The workers were offered to annotate as many video clips as they wanted (but could not annotate the same clip twice). The inter annotator agreement between workers was 0.77 in terms of Krippendorf\u2019s Alpha. The final sentiment intensity of each segment is the average of all 5 workers. Figure 2 shows the distribution of sentiment intensities for all opinion segment in MOSI dataset on the left side. On the right side of Figure 2, we show how the sentiment distribution changes as the size of the opinion (number of words in that opinion) increases. It is interesting to see that the proportions are mostly constant across all segment sizes signaling a difference between sentiment analysis in videos and text where most of the short opinions are neutral (Socher et.al. 2013).\n3.4 \u00a0 Visual Gesture Annotation Audio and visual features have been automatically extracted from MPEG files with framerates of 1000 for audio and 30 for video. Visual features include 16 Facial Action Units, 68 Facial Landmarks, Head Pose and Orientation, 6 Basic Emotions6 and Eye Gaze (Wood et.al., 2015, Baltrusaitis et.al., 2012, Baltrusaitis et.al., 2014). More than 32 audio features including pitch, energy, NAQ (Normalized Amplitude Quotient), MFCCs (Mel-frequency Cepstral Coefficients), Peak Slope, Energy Slope have also been extracted using COVAREP (Degottext et.al., 2014). All the audio and visual features are publicly available with the dataset.\nFurthermore, manual gesture annotations are provided to study the relations between words and gestures. Since hands were not always visible in the YouTube videos, we decided to focus on facial\ngestures. We selected four gestures and expressions: smile, frown, headnod and headshake. These are expressive of emotions and regularly happen in MOSI. The annotation was carried out by simply marking the opinion segment for having each one of these expressions. An expert coder manually annotated all 2199 video segments and a second coder annotated a subset of this dataset to confirm a high coder agreement. For all 4 gestures the average coder agreement was 80.8%.\n3.5 \u00a0 Multimodal analysis of words and visual gestures\nMOSI dataset also enables detailed statistical study of language as a multimodal signal. The alignment between text, audio and video enables such analysis even at phoneme level. In this sec-\ntion we present a study to find a suitable multimodal representation for sentiment analysis. We want to understand the interaction patterns between spoken words and visual gestures. To study these interaction patterns, we propose to study the changes in the distribution of perceived sentiment intensity when a specific facial gesture is present or not. Our main research question can be rephrased as whether all spoken words are interacting similarly with facial gestures or if there are prototypical patterns in these multimodal interactions? This analysis was performed at the opinionsegment level where we studied the multimodal interactions of the top 100 spoken words with all 4 facial gestures (smile, head nod, frowning and head shake).\nFigure 3 shows representative examples from our multimodal analysis where we identified four different types of interaction patterns between spoken words and facial gestures: neutral, emphasizer, positive and negative patterns. Each subgraph shown in Figure 3 is a histograms representing the distribution of perceived sentiment intensities per opinion segments.\nTo help understand the average interaction of facial gestures on spoken words, the first row of Figure 3 shows how the sentiment intensities are distributed for all opinion segments (the top left histogram of Figure 3 is repeated from Figure 2). These histograms are regarded as the common effect of gestures on spoken words. It is not surprising to see that opinion segments with a smile or a\nhead nod are perceived as more positive. The opposite effect is observed for frown and head shake gestures.\nNeutral Interaction Pattern To exemplify the neutral interaction pattern, we selected the most frequent word in our dataset, the word \u201cthe\u201d. \u201cThe\u201d is considered a sentimentally neutral word since it is not positive or negative. The second row of Figure 3 shows the interaction between the facial gestures and the spoken word \u201cthe\u201d. We can observe that the pattern is mostly following the common interaction patterns in the first row (for all opinion segments).\nEmphasizer Interaction Pattern A second interaction pattern was observed in our multimodal analysis. To exemplify this pattern, we showed in the third row of Figure 3 how facial gestures are interacting with the word \u201creally\u201d. When accompanied by a smile or head nod, the distribution tends to shift to positive sentiment intensity, with less negative or neutral intensities. The opposite effect happens when it is accompanied by a frown or head shake, where the distribution is biased toward negative sentiment. In other words this interaction patterns tends to shift the sentiment towards the extremes. We define this type of interaction pattern as emphasizer.\nNegative or Positive Interaction Patterns A third and fourth type of interaction seem to appear when studying sentimentally polarized words since their sentiment distribution are not affected the same way as the neutral and emphasizer. For example, the positively polarized word \u201clove\u201d is shown in the fourth row of Figure 3 (we merged the words \u201clove\u201d and \u201cloved\u201d in these histograms for simplification). We observe that the sentiment distributions do not significantly change polarity when accompanied by frown or head shake. An opposite trend happens when we study a negatively polarized word such as \u201cdon\u2019t\u201d shown in the last row of Figure 3 (all the instances of \u201cdon\u2019t\u201d, \u201cdoesn\u2019t\u201d and \u201cdidn\u2019t\u201d have been merged in these histograms). We observe limited changes in the sentiment distributions for the smile and head nod. These are examples of positive and negative interaction patterns between words and facial gestures.\nBased on these interaction patterns between words and gestures, we present a simple representation model that jointly takes into account words\n7 A baseline model that is slightly smarter than random would always predict the average intensity of training samples. This will give MAE of 1.39.\nand gestures in each opinion segment. The detailed procedure to build this representation is presented in the next section.\n4 \u00a0 Sentiment and Subjectivity Analysis Baselines\nWe designed a set of experiments to present baselines for sentiment and subjectivity analysis on MOSI dataset. All the experiment are done in a speaker independent framework; opinion segments of each speaker is either in training, validation or testing set (i.e. each speaker is only present in one of the sets; this is because models trained and tested on the same set of speakers are not generalizable to unseen speakers).\n4.1 \u00a0 Sentiment Analysis Baselines Methodology: All prediction models were trained using nu-SVR (Smola and Sch\u00f6lkopf, 2004; Chang and Lin, 2011) and tested using 5- fold cross-validation methodology. The automatic validation of the hyper-parameters was performed with 4-fold cross-validation on the training sets. The hyper parameters of linear nu-SVR are C and nu. In validation phase, C was automatically selected from powers of 10 in range [-5,3] and nu from [0.1,1] in steps of 0.1. The performance of the regressors is calculated based on mean absolute error (MAE) and correlation. In these studies we train the following models:\nRandom 7 We included in our experiments a simple baseline model which always predicts a random sentiment intensity between [3,-3]. This baseline gives an overall idea about how random models will work.\nVerbal The second model was trained using only verbal features from MOSI. A very simple bag of words feature set was created from monograms and bigrams created from words in speech segments, including speech pause and pause fillers. All the features with less than 10 instances in the dataset were removed from the bag of words set given their infrequency.\nVisual The third model was trained using facial gestures described in Section 4.1. A binary feature is assigned for each of the 4 facial gesture: smile, frown, head nod and head shake.\nVerbal + Visual The fourth model was trained on verbal and visual data combined. The verbal\nand visual features were simply concatenated for each opinion segment.\nMultimodal Dictionary The fifth model is trained on joint representation of words and gestures. The procedure to build the multimodal dictionary is as follows: for each word Wi from verbal features and gesture Gj, the set {(Wi & Gj), (Wi & ~Gj)} is added to the multimodal dictionary. (Wi & Gj) captures the co-occurrence of word Wi and gesture Gj. (Wi & ~Gj) captures the occurrences of word Wi that did not occur with Gj. If both of them\nare present in the speech segment, the value of (Wi & Gj) will be equal to 1. If only word Wi is present without gesture Gj, the value of (Wi & ~Gj) will be equal to 1.\nHuman Baseline Humans are asked to predict the sentiment score of each opinion segment. This will be a baseline for how well humans can predict sentiment intensity and also a target for machine learning methods to reach in future.\nThe results sentiment analysis baselines are in table 2. As it can be seen multimodal dictionary built to model words and gestures jointly is outperforming simple concatenation of features (early fusion used in most of the previous works in multimodal sentiment analysis).\n4.2 \u00a0 Subjectivity Analysis Baselines The results for subjectivity analysis following two methods are presented here 1) linear C-SVM and 2) deep neural network. The training procedure for subjectivity uses information from automatically extracted audio visual features as well. Linear SVM hyperparameters are validated the same way as sentiment studies (section 4.1). Deep neural networks model is fully connected network with number of layers validated between [1, 5] and number of neurons in each layer validated from [10,50] in steps of 10. Table 4 presents the results of different baseline models for multimodal subjectivity analysis.\n5 \u00a0 Discussion Our experimental results in Table 2 show how combining verbal and visual cues help better predict sentiment. The multimodal dictionary built based on analysis from Section 3.5 has the best performance among all baselines. Table 3 shows examples on how information from visual gestures is helping the multimodal dictionary make more accurate predictions for sentiment. In the first case, it can be seen that verbal prediction is neutral while strong positive visual cues help multimodal dictionary to predict the intensity of the opinion more accurately. In the second case while verbal cues indicate highly positive, showing negative emotions carried by a headshake is an indicator that the opinion should not be considered highly positive. The results for sentiment and subjectivity analyses shown in Tables 2 and 4 highlight how using multimodal information helps in these tasks but the human performance results show that there is room for future research in these directions.\n6 \u00a0 Conclusion This paper introduces the MOSI dataset for multimodal sentiment intensity and subjectivity analysis. The dataset is the first multimodal sentiment analysis dataset with sentiment intensity and sub-\nMAE Correlation Random 1.88 0 Visual 1.24 0.36 Verbal 1.18 0.46 Verbal+Visual 1.14 0.49 Multimodal Dictionary 1.1 0.53 Human Performance 0.61 0.83\nTable 2: Mean absolute error and correlation for each of the trained baseline models.\nSpoken words Verbal-\nonly prediction\nVisual gestures Visualonly prediction\nMultimodal Dic-\ntionary prediction\nGround Truth annotation\n1 And quite honestly I wish I\u2019ve seen this over the summer. 0.14 Smile, Head nod 1.97 1.4 1.6 2 Now I\u2019m not gonna lie there\u2019re a few parts that have great action sequences now even though it is an animated film it did have some great fight scenes. 2.3 Head shake -0.77 1.44 1.4\nTable 3: Examples from dataset with predicted intensities for different models: verbal, visual and multimodal dictionary.\njectivity annotations at opinion level. It has manual and automatic annotations of text, visual and audio features. Alignment between modalities opens the door to future multimodal studies of language. A new representation that captures the join distribution of words and gestures is presented based on statistical observations on the dataset. It is shown that using information more than just text can help models make better sentiment intensity predictions. The same is also true for subjectivity. We hope that this dataset opens the door to more detailed studies of sentiment and subjectivity analysis in multimedia content. Finally, the dataset is publicly available for download with all the extracted features."}], "references": [{"title": "Context-enhanced citation sentiment detection", "author": ["A. Athar", "S. Teufel."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Montr \u0301eal, Can-", "citeRegEx": "Athar and Teufel.,? 2012", "shortCiteRegEx": "Athar and Teufel.", "year": 2012}, {"title": "Why are they excited? identifying and explaining spikes in blog mood levels", "author": ["K. Balog", "G. Mishne", "M. de Rijke."], "venue": "Proceedings of the 11th Meeting of the European Chapter of the As sociation for Computational Linguistics (EACL-2006).", "citeRegEx": "Balog et al\\.,? 2006", "shortCiteRegEx": "Balog et al\\.", "year": 2006}, {"title": "A principal component analysis of facial expressions", "author": ["A.J. Calder", "A.M. Burton", "P. Miller", "A.W. Young", "S. Akamatsu."], "venue": "Vision research, 41(9):1179\u2013 1208, April.", "citeRegEx": "Calder et al\\.,? 2001", "shortCiteRegEx": "Calder et al\\.", "year": 2001}, {"title": "Summarizing emails with conversational cohesion and subjectivity", "author": ["G. Carenini", "R. Ng", "X. Zhou."], "venue": "Proceedings of the Association for Computational Linguistics: Human Language Technologies (ACLHLT 2008), Columbus, Ohio.", "citeRegEx": "Carenini et al\\.,? 2008", "shortCiteRegEx": "Carenini et al\\.", "year": 2008}, {"title": "Liars and saviors in a sentiment annotated corpus of comments to political debates", "author": ["P. Carvalho", "L. Sarmento", "J. Teixeira", "M. Silva."], "venue": "Proceedings of the Association for Computational Linguistics (ACL 2011), Portland, OR.", "citeRegEx": "Carvalho et al\\.,? 2011", "shortCiteRegEx": "Carvalho et al\\.", "year": 2011}, {"title": "LIBSVM: a library for support vector machines.", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Emotion Recognition In The Wild Challenge 2014: Baseline, Data and Protocol", "author": ["A. Dhall", "R. Goecke", "J. Joshi", "K. Sikka", "T. Gedeon."], "venue": "proceedings of ACM International Conference on Multimodal Interaction.", "citeRegEx": "Dhall et al\\.,? 2014", "shortCiteRegEx": "Dhall et al\\.", "year": 2014}, {"title": "Praat, a system for doing phonetics by computer.", "author": ["B. Paul", "D. Weenink"], "venue": null, "citeRegEx": "Paul and Weenink.,? \\Q2001\\E", "shortCiteRegEx": "Paul and Weenink.", "year": 2001}, {"title": "A tutorial on support vector regression.\" Statistics and computing", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "Smola and Sch\u00f6lkopf.,? \\Q2004\\E", "shortCiteRegEx": "Smola and Sch\u00f6lkopf.", "year": 2004}, {"title": "Predicting the semantic orientation of adjectives", "author": ["V. Hatzivassiloglou", "K. McKeown."], "venue": "Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, pages 174\u2013181.", "citeRegEx": "Hatzivassiloglou and McKeown.,? 1997", "shortCiteRegEx": "Hatzivassiloglou and McKeown.", "year": 1997}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu."], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, Seattle, Washington.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Cross-domain co-extraction of sentiment and topic lexicons", "author": ["F. Li", "S.J. Pan", "O. Jin", "Q. Yang", "X. Zhu."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Jeju Island, Korea.", "citeRegEx": "Li et al\\.,? 2012", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Can prosody inform sentiment analysis? Experiments on short spoken reviews", "author": ["F. Mairesse", "J. Polifroni", "G. Di Fabbrizio."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 5093 \u2013", "citeRegEx": "Mairesse et al\\.,? 2012", "shortCiteRegEx": "Mairesse et al\\.", "year": 2012}, {"title": "Fusion of acoustic and linguistic features for emotion detection", "author": ["F. Metze", "T. Polzehl", "M. Wagner."], "venue": "Semantic Computing, 2009. ICSC \u201909. IEEE International Conference on, pages 153 \u2013 160, sept.", "citeRegEx": "Metze et al\\.,? 2009", "shortCiteRegEx": "Metze et al\\.", "year": 2009}, {"title": "Learning multilingual subjective language via cross-lingual projections", "author": ["R. Mihalcea", "C. Banea", "J. Wiebe."], "venue": "Proceedings of the Association for Computational Linguistics, Prague, Czech Republic.", "citeRegEx": "Mihalcea et al\\.,? 2007", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2007}, {"title": "Towards multimodal sentiment analysis: Harvesting opinions from the web", "author": ["L.P. Morency", "R. Mihalcea", "P. Doshi."], "venue": "Proceedings of the International Conference on Multimodal Computing, Alicante, Spain.", "citeRegEx": "Morency et al\\.,? 2011", "shortCiteRegEx": "Morency et al\\.", "year": 2011}, {"title": "Why question answering using sentiment analysis and word classes", "author": ["J. Oh", "K. Torisawa", "C. Hashimoto", "T. Kawada", "S. De Saeger", "J. Kazama", "Y. Wang."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Oh et al\\.,? 2012", "shortCiteRegEx": "Oh et al\\.", "year": 2012}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee."], "venue": "Proceedings of the 42 Meeting of the Association for Computational Linguistics, Barcelona, Spain, July.", "citeRegEx": "Pang and Lee.,? 2004", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Multimodal sentiment analysis of spanish online videos", "author": ["V. Perez-Rosas", "R. Mihalcea", "L.-P. Morency."], "venue": "IEEE Intelligent Systems.", "citeRegEx": "Perez.Rosas et al\\.,? 2013", "shortCiteRegEx": "Perez.Rosas et al\\.", "year": 2013}, {"title": "Multimodal subjectivity analysis of multiparty conversation", "author": ["S. Raaijmakers", "K. Truong", "T. Wilson."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 466\u2013474, Honolulu, Hawaii.", "citeRegEx": "Raaijmakers et al\\.,? 2008", "shortCiteRegEx": "Raaijmakers et al\\.", "year": 2008}, {"title": "Human expression recognition from motion using a radial basis function network architecture", "author": ["M. Rosenblum", "Y. Yacoob", "L.S. Davis."], "venue": "Neural Networks, IEEE Transactions on, 7(5):1121 \u2013 1138, sep.", "citeRegEx": "Rosenblum et al\\.,? 1996", "shortCiteRegEx": "Rosenblum et al\\.", "year": 1996}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank.\" Proceedings of the conference on empirical methods in natural language processing (EMNLP)", "author": ["R. Socher"], "venue": null, "citeRegEx": "Socher,? \\Q2013\\E", "shortCiteRegEx": "Socher", "year": 2013}, {"title": "Manual annotation of opinion categories in meetings", "author": ["S. Somasundaran", "J. Wiebe", "P. Hoffmann", "D. Litman."], "venue": "Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006.", "citeRegEx": "Somasundaran et al\\.,? 2006", "shortCiteRegEx": "Somasundaran et al\\.", "year": 2006}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["M. Taboada", "J. Brooke", "M. Tofiloski", "K. Voli", "M. Stede."], "venue": "Computational Linguistics, 37(3).", "citeRegEx": "Taboada et al\\.,? 2011", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "Latent variable models for semantic orientations of phrases", "author": ["H. Takamura", "T. Inui", "M. Okumura."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics.", "citeRegEx": "Takamura et al\\.,? 2006", "shortCiteRegEx": "Takamura et al\\.", "year": 2006}, {"title": "Recognizing action units for facial expression analysis", "author": ["Y.-I. Tian", "T. Kanade", "J.F. Cohn."], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(2):97 \u2013115, feb.", "citeRegEx": "Tian et al\\.,? 2001", "shortCiteRegEx": "Tian et al\\.", "year": 2001}, {"title": "Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews", "author": ["P. Turney."], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 417\u2013424, Philadel-", "citeRegEx": "Turney.,? 2002", "shortCiteRegEx": "Turney.", "year": 2002}, {"title": "AVEC 2014 - 3D Dimensional Affect and Depression Recognition Challenge", "author": ["M. Valstar", "B. Schuller", "K. Smith", "T. Almaev", "F. Eyben", "J. Krajewski", "R. Cowie", "M. Pantic"], "venue": "proc. 4th ACM international workshop on Audio/visual", "citeRegEx": "Valstar et al\\.,? 2014", "shortCiteRegEx": "Valstar et al\\.", "year": 2014}, {"title": "Exploring fusion methods for multimodal emotion recognition with missing data", "author": ["J. Wagner", "E. Andre", "F. Lingenfelser", "Jonghwa Kim."], "venue": "Affective Computing, IEEE Transactions on, 2(4):206\u2013218, oct.-dec.", "citeRegEx": "Wagner et al\\.,? 2011", "shortCiteRegEx": "Wagner et al\\.", "year": 2011}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["J. Wiebe", "T. Wilson", "C. Cardie."], "venue": "Language Resources and Evaluation, 39(23):165\u2013210.", "citeRegEx": "Wiebe et al\\.,? 2005", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Extracting opinion expressions with semi-markov conditional random fields", "author": ["B. Yang", "C. Cardie."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Yang and Cardie.,? 2012", "shortCiteRegEx": "Yang and Cardie.", "year": 2012}, {"title": "Youtube movie reviews: Sentiment analysis in an audio-visual context", "author": ["M. Wollmer", "F. Weninger", "T. Knaup", "B. Schuller", "C. Sun", "K. Sagae", "L.P. Morency"], "venue": "Intelligent Systems,", "citeRegEx": "Wollmer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wollmer et al\\.", "year": 2013}, {"title": "Sentiment analysis and opinion mining: a survey.", "author": ["G. Vinodhini", "R.M. Chandrasekaran"], "venue": "International Journal 2, no", "citeRegEx": "Vinodhini and Chandrasekaran.,? \\Q2012\\E", "shortCiteRegEx": "Vinodhini and Chandrasekaran.", "year": 2012}, {"title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis", "author": ["S. Poria", "E. Cambria", "A. Gelbukh"], "venue": null, "citeRegEx": "Poria et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Poria et al\\.", "year": 2015}, {"title": "Rendering of eyes for eye-shape registration and gaze estimation", "author": ["E. Wood", "T. Baltrusaitis", "X. Zhang", "Y. Sugano", "P. Robinson", "A. Bulling"], "venue": null, "citeRegEx": "Wood et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2015}, {"title": "Continuous conditional neural fields for structured regression", "author": ["T. Baltru\u0161aitis", "P. Robinson", "L.P. Morency"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Baltru\u0161aitis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baltru\u0161aitis et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "opinion words and their sentiment polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), studies using n-grams and more complex language models (Takamura et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 26, "context": "opinion words and their sentiment polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), studies using n-grams and more complex language models (Takamura et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 10, "context": "opinion words and their sentiment polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), studies using n-grams and more complex language models (Takamura et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 23, "context": "opinion words and their sentiment polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), studies using n-grams and more complex language models (Takamura et al.", "startOffset": 43, "endOffset": 133}, {"referenceID": 24, "context": ", 2011), studies using n-grams and more complex language models (Takamura et al., 2006; Yang and Cardie, 2012) and works addressing sentiment compositionality by using polarity shifting rules or careful feature engineering (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 30, "context": ", 2011), studies using n-grams and more complex language models (Takamura et al., 2006; Yang and Cardie, 2012) and works addressing sentiment compositionality by using polarity shifting rules or careful feature engineering (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 1, "context": "These methods have been used in many different applications including opinion mining in tweets and online forums (Pak et.al. 2010; Argawal et.al. 2011; Balog et al., 2006), analysis of political debates (Carvalho et al.", "startOffset": 113, "endOffset": 171}, {"referenceID": 4, "context": ", 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 16, "context": ", 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 3, "context": ", 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012).", "startOffset": 36, "endOffset": 59}, {"referenceID": 0, "context": ", 2008), and citation sentiment detection (Athar and Teufel, 2012).", "startOffset": 42, "endOffset": 66}, {"referenceID": 22, "context": "Multimodal analysis Some earlier work introduced acoustic and paralinguistic features to the text-based analysis for the purpose of subjectivity or sentiment analysis (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009).", "startOffset": 167, "endOffset": 263}, {"referenceID": 19, "context": "Multimodal analysis Some earlier work introduced acoustic and paralinguistic features to the text-based analysis for the purpose of subjectivity or sentiment analysis (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009).", "startOffset": 167, "endOffset": 263}, {"referenceID": 12, "context": "Multimodal analysis Some earlier work introduced acoustic and paralinguistic features to the text-based analysis for the purpose of subjectivity or sentiment analysis (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009).", "startOffset": 167, "endOffset": 263}, {"referenceID": 13, "context": "Multimodal analysis Some earlier work introduced acoustic and paralinguistic features to the text-based analysis for the purpose of subjectivity or sentiment analysis (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009).", "startOffset": 167, "endOffset": 263}, {"referenceID": 15, "context": "Closer to our current study is the work reported in (Morency et al., 2011; PerezRosas et al., 2013), where multimodal cues, including visual cues, have been used for the sentiment analysis in product and movie reviews.", "startOffset": 52, "endOffset": 99}, {"referenceID": 0, "context": ", 2008), and citation sentiment detection (Athar and Teufel, 2012). Recursive Neural Networks have also been successful in sentiment analysis. Socher et.al (2013) addressed sentiment analysis as a 5 class classification task and managed to get accuracy of 45.", "startOffset": 43, "endOffset": 163}, {"referenceID": 25, "context": "Facial expressions are among the most powerful means of communicating emotions and intentions in human interactions (Tian et al., 2001).", "startOffset": 116, "endOffset": 135}, {"referenceID": 2, "context": "Interesting studies discuss findings about mental state of the speaker based on facial expressions, head gestures and other visual cues (El Kaliouby and Robinson, 2005; Baltrusaitis et.al., 2011; Calder et al., 2001; Rosenblum et al., 1996).", "startOffset": 136, "endOffset": 240}, {"referenceID": 20, "context": "Interesting studies discuss findings about mental state of the speaker based on facial expressions, head gestures and other visual cues (El Kaliouby and Robinson, 2005; Baltrusaitis et.al., 2011; Calder et al., 2001; Rosenblum et al., 1996).", "startOffset": 136, "endOffset": 240}, {"referenceID": 6, "context": "More recently, many challenges have been organized focusing on the recognition of emotions using audio and visual cues (Dhall et al., 2014; Valstar et al., 2014), which brings the participation of many teams from around the world.", "startOffset": 119, "endOffset": 161}, {"referenceID": 27, "context": "More recently, many challenges have been organized focusing on the recognition of emotions using audio and visual cues (Dhall et al., 2014; Valstar et al., 2014), which brings the participation of many teams from around the world.", "startOffset": 119, "endOffset": 161}, {"referenceID": 7, "context": "During the final stage, the results of the alignment were manually checked and if necessary corrected using PRAAT (Paul and Weenink, 2001).", "startOffset": 114, "endOffset": 138}, {"referenceID": 8, "context": "Methodology: All prediction models were trained using nu-SVR (Smola and Sch\u00f6lkopf, 2004; Chang and Lin, 2011) and tested using 5fold cross-validation methodology.", "startOffset": 61, "endOffset": 109}, {"referenceID": 5, "context": "Methodology: All prediction models were trained using nu-SVR (Smola and Sch\u00f6lkopf, 2004; Chang and Lin, 2011) and tested using 5fold cross-validation methodology.", "startOffset": 61, "endOffset": 109}], "year": 2016, "abstractText": "People are sharing their opinions, stories and reviews through online video sharing websites every day. Studying sentiment and subjectivity in these opinion videos is experiencing a growing attention from academia and industry. While sentiment analysis has been successful for text, it is an understudied research question for videos and multimedia content. The biggest setbacks for studies in this direction are lack of a proper dataset, methodology, baselines and statistical analysis of how information from different modality sources relate to each other. This paper introduces to the scientific community the first opinion-level annotated corpus of sentiment and subjectivity analysis in online videos called Multimodal Opinionlevel Sentiment Intensity dataset (MOSI). The dataset is rigorously annotated with labels for subjectivity, sentiment intensity, per-frame and per-opinion annotated visual features, and per-milliseconds annotated audio features. Furthermore, we present baselines for future studies in this direction as well as a new multimodal fusion approach that jointly models spoken words and visual gestures.", "creator": "Word"}}}