{"id": "1606.06424", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "A Novel Framework to Expedite Systematic Reviews by Automatically Building Information Extraction Training Corpora", "abstract": "A systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question. The process is manual and involves lot of time. A tool to automate this process is lacking. The aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication, without having to go through the expensive task of manual annotation to build gold standards for each data element type. The system is developed in two stages. Initially, it uses information contained in existing systematic reviews to identify the sentences from the PDF files of the included references that contain specific data elements of interest using a modified Jaccard similarity measure. These sentences have been treated as labeled data.A Support Vector Machine (SVM) classifier is trained on this labeled data to extract data elements of interests from a new article. We conducted experiments on Cochrane Database systematic reviews related to congestive heart failure using inclusion criteria as an example data element. The empirical results show that the proposed system automatically identifies sentences containing the data element of interest with a high recall (93.75%) and reasonable precision (27.05% - which means the reviewers have to read only 3.7 sentences on average). The empirical results suggest that the tool is retrieving valuable information from the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction from biomedical research publications. The future scope of this work is to generalize this information framework for all types of systematic reviews.", "histories": [["v1", "Tue, 21 Jun 2016 04:56:33 GMT  (284kb)", "http://arxiv.org/abs/1606.06424v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["tanmay basu", "shraman kumar", "abhishek kalyan", "priyanka jayaswal", "pawan goyal", "stephen pettifer", "siddhartha r jonnalagadda"], "accepted": false, "id": "1606.06424"}, "pdf": {"name": "1606.06424.pdf", "metadata": {"source": "CRF", "title": "A Novel Framework to Expedite Systematic Reviews by Automatically Building Information Extraction Training Corpora", "authors": ["Tanmay Basu", "Shraman Kumar", "Abhishek Kalyan", "Priyanka Jayaswal", "Pawan Goyal", "Stephen Pettifer", "Siddhartha R. Jonnalagadda"], "emails": [], "sections": [{"heading": "A Novel Framework to Expedite Systematic Reviews by", "text": ""}, {"heading": "Automatically Building Information Extraction Training Corpora", "text": "Tanmay Basu1, Shraman Kumar2, Abhishek Kalyan2, Priyanka Jayaswal2, Pawan Goyal2, Stephen Pettifer3 and Siddhartha R. Jonnalagadda1 1 Division of Biomedical Informatics, Feinberg School of Medicine, Northwestern University, USA 2 Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, India 3 School of Computer Science, University of Manchester, Manchester, UK"}, {"heading": "Abstract", "text": "Background: A systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question. The process is manual and involves lot of time. A tool to automate this process is lacking. Objective: The aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication, without having to go through the expensive task of manual annotation to build gold standards for each data element type. Method: The system is developed in two stages. Initially, it uses information contained in existing systematic reviews to identify the sentences from the PDF files of the included references that contain specific data elements of interest using a modified Jaccard similarity measure. These sentences have been treated as labeled data. A Support Vector Machine (SVM) classifier is trained on this labeled data to extract data elements of interests from a new article. Results: We conducted experiments on Cochrane Database systematic reviews related to congestive heart failure using inclusion criteria as an example data element. The empirical results show that the proposed system automatically identifies sentences containing the data element of interest with a high recall (93.75%) and reasonable precision (27.05% - which means the reviewers have to read only 3.7 sentences on average). Conclusions: The empirical results suggest that the tool is retrieving valuable information from the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction from biomedical research publications. The future scope of this work is to generalize this information framework for all types of systematic reviews Keywords: systematic review; evidence synthesis; machine learning; natural language processing;"}, {"heading": "Introduction", "text": "A systematic review identifies and collates various clinical studies, and compares data elements and results in order to provide an evidence-based answer for a particular clinical question [1]. Despite their widely\nacknowledged usefulness, the data extraction phase of the systematic review process is time-consuming [2]. Some reviews on the automation of systematic review process describe technologies needed for automating the overall process or individual steps. Tsafnat et al. [3] described four main tasks in systematic review: identifying the relevant studies, evaluating risk of bias in selected trials, synthesis of the evidence, and publishing the systematic reviews by generating human-readable text from trial reports. The use of text mining to reduce screening workload in systematic reviews has been discussed by Shemilt et al. [4]. Miwa et al. [5] proposed an active learning framework to reduce the workload in citation screening for inclusion in the systematic reviews. A classification system for screening articles for systematic review has been presented by Adeva et al. [6]. Choong et al. developed a system for automatic citation snowballing that recursively pursues relevant literature for helping in evidence retrieval for systematic reviews [7]. Kiritchenko et al. [8] developed ExaCT, a tool that assists users with locating and extracting key trial characteristics such as eligibility criteria, sample size, drug dosage, and primary outcomes from full-text journal articles. The major limitation of ExaCT is that it uses manually labelled training set for information extraction from a new article. Unfortunately, none of these existing reviews and articles focus on automatic data extraction from a full text article [2]. In a recent study, Tsafnat et al. [1] have described the informatics systems that automate some of the tasks of systematic review and report systems for each stage of systematic review and while data extraction has been described as a task in their review, they only highlighted three studies as an acknowledgement of the ongoing work. Moreover, as we have noted in our review on this topic [2], there is a dearth in gold standards for information extraction from literature for the purpose of systematic reviews. It has been discussed that only three out of 26 studies included in that systematic review, use a common corpus, namely 1000 medical abstracts from the PIBOSO corpus [2]. Unfortunately, even that corpus facilitates only classification of sentences into whether they contain one of the data elements corresponding to the PIBOSO categories. No two other studies shared the same gold standard or dataset for evaluation. State of the art text mining techniques use gold standards of documents with concepts annotated manually and then builds machinelearning systems. However, it will be expensive to build gold standards even for 50 standard data elements and practically impossible to build gold standards for hundreds of domain-specific data elements. We, therefore, present a framework to leverage the wealth of human curated information already present in the existing systematic reviews. We propose to develop our training set by retrieving sentences from the full text articles that correspond to the manually extracted data elements present in the systematic reviews. Instead of having humans to go through the content of the full PDF article, we expedite this process by first developing a heuristics based system for automatically annotating included studies to assist the human annotator or directly use the outputs of this system for data elements where the accuracy is high enough. It is significantly lesser effort on the part of the humans. The aim of this work is two-fold. The first objective is to develop a framework to generate gold standard using existing systematic reviews by applying natural language processing (NLP) techniques such as sentence similarity algorithms to automatically create training corpora from the references mentioned in the systematic reviews. The other objective is to use these gold standards to train machine learning\nclassifiers for extracting data elements from a new study. A SVM classifier has been used towards this learning task."}, {"heading": "Methods", "text": "The proposed system is developed in two stages \u2013 training corpus (or gold standard) generation and data extraction using this training corpus. A graphical abstract of the proposed framework is presented in Figure 1. Gold Standard Generation: Initially, different data elements (e.g., the inclusion criteria of a health care study) and their references are extracted from each systematic review. The synopsis of different data elements are generally mentioned in systematic reviews. Therefore the entire information about the data elements are extracted from the associated studies included in the systematic reviews. A modified Jaccard similarity measure is used to identify specific sentences from an included reference that represent individual data elements. The modified Jaccard similarity measure (Jac_Mod) finds the similarity between the sentence (Sx) that represents a data element (D) in a systematic review and each sentence (Sy) in an included reference as follows:\nJac_Mod (Sx, Sy) = (number of common terms between Sx and Sy)/ (number of terms in Sx) Note that the modified Jaccard similarity measure ranges between [0, 1]. The sentences are sorted according to their similarity values and the top few sentences are considered as the positive instance for D. Note that a specific data element may be represented in the references by more than one sentence. If the difference of Jac_Mod values between the top-most sentence and the next few sentences lie between 0 and \u03b1 (say), then all of these sentences are considered as positive instance for D. The sentences that have very low similarity values are considered as negative instance for D. A range of [0, \u03b2] of Jac_Mod scores has been considered for creating negative instances. Thus, the gold standard for each data element is developed by combining the positive and negative instances from all the included studies in two different groups. Here \u03b1, \u03b2 are two different thresholds on Jac_Mod score and their value should lie in range [0, 1]. The values of \u03b1 and \u03b2 are data dependent and could be fixed at runtime based on the characteristics of gold standard.\nAutomatic Data Extraction: In the second phase, a machine learning model has been developed, where this gold standard is used to train a classifier to determine whether a sentence contains the data element of interest (in our evaluation case \u2013 inclusion criteria) from a new clinical article. Unigrams, Bigrams and Trigrams generated from sentences are used as features with SVM classifier in the experiments. In its simplest form, SVM is a hyperplane that separates a set of positive examples from a set of negative examples with maximum margin [9]. Given a set of training documents in a vector space, SVM finds the best decision hyperplane that separates two categories. The quality of a decision hyperplane is determined by a margin between two hyperplanes that are parallel to the decision hyperplane and touch the closest documents of each category. Therefore best decision hyperplane is the one with the maximum margin, and is used to categorize the new test documents."}, {"heading": "Results", "text": "Datasets: 31 Cochrane Database systematic reviews (CDSR) related to congestive heart failure are\nrandomly identified from PubMed. PDFs of the corresponding full-texts of the systematic reviews and the PDFs of the associated clinical studies are obtained automatically through APIs accessible from Northwestern University\u2019s Galter library for health sciences [10]. For obtaining the PDFs of the included references, the titles of these references are automatically extracted from the webpage of the corresponding systematic reviews in online Cochrane Library [11]. Therefore these titles are passed as individual queries to PubMed using eUtils webservice [12] to automatically retrieve the EndNotes of those references. These EndNotes are used to download the corresponding full-texts (in PDF format) through EndNote API of Galter library. The systematic reviews contain a section (typically called \u2018Characteristics of Studies\u2019 or a variation thereof) in which the properties of each clinical study are summarized by means of a table. A snapshot of a section of such table is shown in Figure 2 from the systematic review by Mart\u00ed-Carvajal et al. [13]. In this figure, the keys, e.g., eligibility, inclusion criteria, exclusion criteria etc. along with their values are the data elements. Either the exact sentences or some significant information about these data elements are manually extracted from the referred articles and noted in such tables to develop systematic reviews. Study Design: The PDF files of each of these 31 systematic reviews are converted to JSON format using a version of the PDFx tool [14] customized for the structure of Cochrane Systematic Reviews. The JSON file format represents a machine readable version of these tables from which data elements can easily be retrieved programmatically. We have tested the performance of the proposed system on a commonly used data element (inclusion criteria of Randomized Control Trials (RCT)). Therefore the tables that contain inclusion criteria as data element are considered in the experiments. Eventually 69 such references have been considered to generate the gold standard. The PDF version of each of these references is converted to text format using a PDF to text converter tool freely available on internet. The proposed system is implemented in scikit-learn, a machine learning platform developed in Python [15]. The gold standard is automatically developed following the proposed algorithm using these 69 references in scikit-learn. The values of \u03b1 and \u03b2 of Jac_Mod similarity measure have been fixed as 0.2 and 0.005 respectively. The inclusion criteria are extracted from each reference, which is considered as positive instance. After applying the method for gold standard generation described above for these 69 references, the gold standard contains 122 such positive instances and 12651 negative instances for inclusion criteria. Each instance represents a sentence. The test set contains 24 new RCT publications that are chosen randomly from the Cochrane Database [2] and are not included in the gold standard. These publications have been cited in some of the Cochrane Database Systematic Reviews. It has been confirmed that every reference contains some inclusion criteria based on them being explicitly mentioned in the corresponding systematic review. Therefore the inclusion criteria have been manually identified from these references.\nMethods Multicentre study: yes Country: USA Intention-to-treat: yes (an intention-to-treat analysis was performed for all participants who had carotid ultrasonography at baseline and at least 1 follow-up visit, page 731) Unit of randomization: patients Follow-up period (years): B vitamins group (3.14 (0.48 to 4.56) versus placebo group (3.07 (0.46 to 5.0))\nThe aim is to automatically identify the inclusion criteria in each of these new references using the proposed tool and evaluate the performance using the manually labeled information from systematic reviews. Each sentence of a new test reference is categorized to either positive (i.e., inclusion criteria) or negative instance by the system.\nExperimental Analysis: The performance of the proposed tool has been evaluated by precision and recall measure [7]. The precision and recall can be calculated using the standard formulae as follow:\nPrecision = (True positives) / (True positives + False positives) Recall = (True positives) / (True positives + False negatives)\nHere, true positive stands for the number of sentences that are correctly retrieved as positive instances. False positive is the number of sentences that the tool marked as positive, but actually belong to the negative category. False negative is the number of sentences that are retrieved by the machine as negative instances, but actually belong to the positive category.\nNote that the objective of the proposed tool is to achieve high accuracy in terms of identifying inclusion criteria (or other data elements in general) from new articles. Therefore, a high recall is desirable for the system. The recall and precision for each new test article is measured using the manually labeled inclusion criteria described above. The individual true positive, false positive, false negative, recall and precision values of each of the 24 new articles are presented in Table1.\nIt may be observed from Table 1 that the number of false negatives is zero for most of the test samples. It indicates that the proposed system is able to retrieve information of interest from new references with the framework being trained by automatically generated corpora. The aggregate recall and precision of all the test references are 0.93 and 0.27 respectively. A precision of 0.27 is still efficient because the reviewers now need to review only 1/0.27=3.7 sentences per article for the data elements as opposed to reading the entire article or training a corpus using completely new annotations. Thus the performance shows the effectiveness of the proposed tool."}, {"heading": "Discussion", "text": "Principal Observations: It has been suggested in earlier studies that SVM performs well for text classification [16, 17]. Hence, SVM is chosen as the classifier in the proposed system. Moreover, we have done extensive experiments using several other classifiers, such as Random Forests, Na\u00efve Bayes using 10 fold cross validation on the training corpus and eventually SVM outperforms the other classifiers in terms of recall. The training corpus is imbalanced, since it contains many negative instances (12651) and few positive instances (122). Hence we have used balanced class weights to avoid the proportional influence of the negative class. It has been recommended in the study by Hsu et al. [17] that the linear kernel is useful, when the number of features is very large. It should be noted that the natural language texts generally produce large number of features [18]. Therefore linear kernel is used in the experiments. In SVM, C is a regularization parameter that controls the trade-off between achieving a low error on the training data and minimizing the norm of the weights [19]. For large values of C, the optimization algorithm of SVM chooses a small margin separating hyperplane, if that hyperplane could correctly classify all the training instances. Conversely, if the value of C is too small then the optimization algorithm chooses a large margin separating hyperplane, resulting a high misclassification rate. It may be noted that the value of C lies in (0, \u221e]. The goal is to identify a good C value, so that SVM can accurately predict unknown test samples i.e., to minimize the misclassification rate. Usually the value of C is determined by grid search on different class intervals of C values [17]. In grid search, SVM is implemented by performing k-fold cross validation on the training set using every C value in a given range. A finer grid search is performed on a narrow range on the C value that has achieved best accuracy. The process continues to refine C value up to third or fourth decimal place, which is predefined. For example, if the initial range is considered as [1, 10] then the subsequent finer ranges may be (4.1, 4.2\u2026 4.9), (4.21, 422\u2026 4.29), (4.261, 4.262\u2026 4.269) etc. and a particular value from the last interval say, 4.263 could fixed as C. Eventually, this value of C is used for classifying the test samples. In k-fold cross-validation, the training set is randomly divided into k subsets of equal size. Sequentially one subset is tested using the classifier trained on the remaining k-1 subsets. Thus, each instance of the whole training set is predicted once and therefore the cross-validation accuracy is the percentage of data that are correctly classified. In our experiments, we have performed 10-fold cross validation on the training set to determine the value of C ranges in (0, 1000] and refined it up to fourth decimal place. It has been observed from various publications included in the systematic reviews that the sentences which represent the inclusion criteria generally contain the keywords or phrases like, eligibility, entry conditions, eligible patients, etc. Sometimes the phrase inclusion criteria is explicitly mentioned before its description. Therefore these keywords/ phrases help to reduce the manual load. But, it is difficult to identify an inclusion\ncriteria where it is not represented by such regular keywords. The following inclusion criterion has been extracted from test sample 7 [20] that is used in the experimental analysis.\n\u2022 This prospective cohort is composed by patients who are 18 years old or older, diagnosed as having chronic heart failure of ischemic or non-ischemic etiology, at least 3 months before inclusion into the registry.\nAlthough the sentence does not contain any discriminating keyword for inclusion criteria, the proposed system extracted this sentence from the reference. The most interesting aspect of the proposed framework, however, is that no knowledge about the meaning of \u201cinclusion criteria\u201d is used in generating the training corpus or building machine learning features. The system is retrieving the data element of interest as well as some sentences similar to that data element from a new article, which results to a low precision score. The following sentences have been extracted by the proposed tool from test sample 18 [21] to identify inclusion criteria.\n\u2022 Inclusion criteria were diagnosed heart failure, either by echocardiography, radiographic evidence of pulmonary congestion or typical symptoms and signs of heart failure.\n\u2022 Since the majority of patients hospitalized with heart failure were over 75 years of age, other limitations of recruitment were that patients were in an end-stage of heart failure or other severe disease or had cognitive dysfunction.\n\u2022 A total of 161 patients met the inclusion criteria, 55 patients declined to participate mainly due to fatigue.\n\u2022 According to age, sex and co-morbidities the patients included in our study are representative for the heart failure population.\nIt should be noted that only the first sentence represents the inclusion criteria. The other sentences appear either in the results section or discussion section in test sample 18 [21], but they have some relevance with the first sentence. Hence the tool has extracted all of them as data elements, which results in a low precision score. The same scenario is observed in almost all the test samples. Note that the aggregate precision score of the system is reasonable (0.27), since the reviewers have to read only 3.7 sentences in a new article on average. Limitations: The first limitation of the tool is that the study cannot guarantee that SVM will be useful in every case. It may be noted here that a particular classifier cannot provide an optimum solution in every situation and for any type of data [9]. Hence, the tool may show good results using other classifiers in machine learning for various other types of articles. Moreover, the values of the thresholds \u03b1 and \u03b2 of Jac_Mod similarity measure should be chosen carefully to create the training corpus. It is recommended that both of the values should be close to 0. Otherwise many irrelevant sentences could be selected in the training corpus, which may degrade the performance of the machine learning classifier.\nThe performance of the tool is tested on some randomly selected articles from PubMed, mostly related to heart disease. The performance should be tested on studies from different topics of medical research e.g., internal medicine, oncology, neuroscience etc. Conclusions: The literature survey has shown that a tool is lacking for automatic data extraction from a full text article. Therefore, a framework has been proposed in this study to overcome this limitation. It has been observed from the experiments that the tool is retrieving valuable information from some references, even when it is difficult to identify them manually. Thus, we hope that the tool will be useful for automatic data extraction from biomedical literature. The future scope of this work is to generalize this information extraction framework for all topics and to validate it for all frequently used data elements."}], "references": [{"title": "Systematic review automation technologies", "author": ["G Tsafnat", "P Glasziou", "MK Choong", "A Dunn", "F Galgani", "E. Coiera"], "venue": "Systematic Reviews;", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Automating data extraction in systematic reviews: a systematic review", "author": ["S.R. Jonnalagadda", "P. Goyal", "M.D. Huffman"], "venue": "Systematic Reviews;", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The automation of systematic reviews. BMJ; 2013; 346:f139", "author": ["G Tsafnat", "A Dunn", "P Glasziou", "E. Coiera"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Pinpointing needles in giant haystacks: use of text mining to reduce impractical screening workload in extremely large scoping reviews", "author": ["I Shemilt", "A Simon", "GJ Hollands", "TM Marteau", "D Ogilvie", "A O\u2019Mara\u2010Eves"], "venue": "Research Synthesis Methods;", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Reducing systematic review workload through certaintybased screening", "author": ["MTJ Miwa", "A O'Mara-Eves", "S. Ananiadou"], "venue": "Journal of Biomedical Informatics;", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Automatic text classification to support systematic reviews in medicine", "author": ["JJG Adeva", "JM Pikatza Atxa", "M Ubeda Carrillo", "ZE. Ansuategi"], "venue": "Expert Systems and Applications;", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Automatic Evidence Retrieval for Systematic Reviews", "author": ["MK Choong", "F Galgani", "AG Dunn", "G. Tsafnat"], "venue": "JMIR;", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "ExaCT: Automatic Extraction of Clinical Trial Characteristics from Journal Publications", "author": ["S Kiritchenko", "B de Bruijn", "S Carini", "J Martin", "I. Sim"], "venue": "BMC Med Inform Decis Mak", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["IH Witten", "E frank", "MA. Hall"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "A General Introduction to the E-utilities,", "author": ["E Sayers"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Homocysteine lowering interventions for preventing cardiovascular events", "author": ["AJ Mart\u00ed-Carvajal", "I Sola", "D Lathyris", "G. Salanti"], "venue": "Cochrane Database Systematic Review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Scikit-learn: Machine Learning in Python, The Journal of Machine Learning Research", "author": ["F Pedregosa", "G Varoquaux"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "Proceedings of the European Conference on Machine Learning;", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "A Practical Guide to Support", "author": ["CW Hsu", "CC Chang", "CJ. Lin"], "venue": "Vector Classification,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "A supervised term selection technique for effective text categorization", "author": ["Basu T", "Murthy CA"], "venue": "International Journal of Machine Learning and Cybernetics;", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Randomized Trial of Telephone Intervention in Chronic Heart Failure (DIAL): study design and preliminary observations", "author": ["Grancelli H", "Varini S", "Ferrante D", "Schwartzman R", "Zambrano C", "Soifer S", "Nul D", "Doval H", "GESICA Investigators"], "venue": "Journal of Cardiac Failure;", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Nurse-led heart failure clinics improve survival and self-care behaviour in patients with heart failure: results from a prospective, randomised trial", "author": ["A Stromberg", "J Martensson", "B Fridlund", "LA Levin", "JE Karlsson", "U Dahlstrom"], "venue": "European Heart Journal;", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Introduction A systematic review identifies and collates various clinical studies, and compares data elements and results in order to provide an evidence-based answer for a particular clinical question [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 1, "context": "acknowledged usefulness, the data extraction phase of the systematic review process is time-consuming [2].", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "[3] described four main tasks in systematic review: identifying the relevant studies, evaluating risk of bias in selected trials, synthesis of the evidence, and publishing the systematic reviews by generating human-readable text from trial reports.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] proposed an active learning framework to reduce the workload in citation screening for inclusion in the systematic reviews.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "developed a system for automatic citation snowballing that recursively pursues relevant literature for helping in evidence retrieval for systematic reviews [7].", "startOffset": 156, "endOffset": 159}, {"referenceID": 7, "context": "[8] developed ExaCT, a tool that assists users with locating and extracting key trial characteristics such as eligibility criteria, sample size, drug dosage, and primary outcomes from full-text journal articles.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Unfortunately, none of these existing reviews and articles focus on automatic data extraction from a full text article [2].", "startOffset": 119, "endOffset": 122}, {"referenceID": 0, "context": "[1] have described the informatics systems that automate some of the tasks of systematic review and report systems for each stage of systematic review and while data extraction has been described as a task in their review, they only highlighted three studies as an acknowledgement of the ongoing work.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Moreover, as we have noted in our review on this topic [2], there is a dearth in gold standards for information extraction from literature for the purpose of systematic reviews.", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "It has been discussed that only three out of 26 studies included in that systematic review, use a common corpus, namely 1000 medical abstracts from the PIBOSO corpus [2].", "startOffset": 166, "endOffset": 169}, {"referenceID": 0, "context": "Note that the modified Jaccard similarity measure ranges between [0, 1].", "startOffset": 65, "endOffset": 71}, {"referenceID": 0, "context": "Here \u03b1, \u03b2 are two different thresholds on Jac_Mod score and their value should lie in range [0, 1].", "startOffset": 92, "endOffset": 98}, {"referenceID": 8, "context": "In its simplest form, SVM is a hyperplane that separates a set of positive examples from a set of negative examples with maximum margin [9].", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "Therefore these titles are passed as individual queries to PubMed using eUtils webservice [12] to automatically retrieve the EndNotes of those references.", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The proposed system is implemented in scikit-learn, a machine learning platform developed in Python [15].", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "The test set contains 24 new RCT publications that are chosen randomly from the Cochrane Database [2] and are not included in the gold standard.", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "Experimental Analysis: The performance of the proposed tool has been evaluated by precision and recall measure [7].", "startOffset": 111, "endOffset": 114}, {"referenceID": 12, "context": "Discussion Principal Observations: It has been suggested in earlier studies that SVM performs well for text classification [16, 17].", "startOffset": 123, "endOffset": 131}, {"referenceID": 13, "context": "Discussion Principal Observations: It has been suggested in earlier studies that SVM performs well for text classification [16, 17].", "startOffset": 123, "endOffset": 131}, {"referenceID": 13, "context": "[17] that the linear kernel is useful, when the number of features is very large.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "It should be noted that the natural language texts generally produce large number of features [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Usually the value of C is determined by grid search on different class intervals of C values [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "For example, if the initial range is considered as [1, 10] then the subsequent finer ranges may be (4.", "startOffset": 51, "endOffset": 58}, {"referenceID": 15, "context": "The following inclusion criterion has been extracted from test sample 7 [20] that is used in the experimental analysis.", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "The following sentences have been extracted by the proposed tool from test sample 18 [21] to identify inclusion criteria.", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "The other sentences appear either in the results section or discussion section in test sample 18 [21], but they have some relevance with the first sentence.", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "It may be noted here that a particular classifier cannot provide an optimum solution in every situation and for any type of data [9].", "startOffset": 129, "endOffset": 132}], "year": 2016, "abstractText": "Background: A systematic review identifies and collates various clinical studies and compares data elements and results in order to provide an evidence based answer for a particular clinical question. The process is manual and involves lot of time. A tool to automate this process is lacking. Objective: The aim of this work is to develop a framework using natural language processing and machine learning to build information extraction algorithms to identify data elements in a new primary publication, without having to go through the expensive task of manual annotation to build gold standards for each data element type. Method: The system is developed in two stages. Initially, it uses information contained in existing systematic reviews to identify the sentences from the PDF files of the included references that contain specific data elements of interest using a modified Jaccard similarity measure. These sentences have been treated as labeled data. A Support Vector Machine (SVM) classifier is trained on this labeled data to extract data elements of interests from a new article. Results: We conducted experiments on Cochrane Database systematic reviews related to congestive heart failure using inclusion criteria as an example data element. The empirical results show that the proposed system automatically identifies sentences containing the data element of interest with a high recall (93.75%) and reasonable precision (27.05% which means the reviewers have to read only 3.7 sentences on average). Conclusions: The empirical results suggest that the tool is retrieving valuable information from the reference articles, even when it is time-consuming to identify them manually. Thus we hope that the tool will be useful for automatic data extraction from biomedical research publications. The future scope of this work is to generalize this information framework for all types of systematic reviews", "creator": "Acrobat PDFMaker 15 for Word"}}}