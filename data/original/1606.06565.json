{"id": "1606.06565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "Concrete Problems in AI Safety", "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.", "histories": [["v1", "Tue, 21 Jun 2016 13:37:05 GMT  (51kb)", "http://arxiv.org/abs/1606.06565v1", "29 pages"], ["v2", "Mon, 25 Jul 2016 17:23:29 GMT  (52kb,D)", "http://arxiv.org/abs/1606.06565v2", "29 pages"]], "COMMENTS": "29 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["dario amodei", "chris olah", "jacob steinhardt", "paul christiano", "john schulman", "dan man\\'e"], "accepted": false, "id": "1606.06565"}, "pdf": {"name": "1606.06565.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Chris Olah", "Jacob Steinhardt"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n06 56\n5v 1\n[ cs\n.A I]\n2 1\nJu n"}, {"heading": "1 Introduction", "text": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision [80], video game playing [100], autonomous vehicles [84], and Go [138]. These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].\nThe authors believe that AI technologies are likely to be overwhelmingly beneficial for humanity, but we also believe that it is worth giving serious thought to potential challenges and risks. We strongly support work on privacy, security, fairness, economics, and policy, but in this document we discuss another class of problem which we believe is also relevant to the societal impacts of AI: the problem of accidents in machine learning systems. We define accidents as unintended and harmful behavior that may emerge from machine learning systems when we specify the wrong objective function, are\n\u2217These authors contributed equally.\nnot careful about the learning process, or commit other machine learning-related implementation errors.\nThere is a large and diverse literature in the machine learning community on issues related to accidents, including robustness, risk-sensitivity, and safe exploration; we review these in detail below. However, as machine learning systems are deployed in increasingly large-scale, autonomous, opendomain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.\nThere has been a great deal of public discussion around accidents. To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents [26]. However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics [37, 83]. We believe it is usually most productive to frame accident risk in terms of practical (though often quite general) issues with modern ML techniques. As AI capabilities advance and as AI systems take on increasingly important societal functions, we expect the fundamental challenges discussed in this paper to become increasingly important. The more successfully the AI and machine learning communities are able to anticipate and understand these fundamental technical challenges, the more successful we will ultimately be in developing increasingly useful, relevant, and important AI systems.\nOur goal in this document is to highlight a few concrete safety problems that are ready for experimentation today and relevant to the cutting edge of AI systems, as well as reviewing existing literature on these problems. In Section 2, we frame mitigating accident risk (often referred to as \u201cAI safety\u201d in public discussions)\u201d in terms of classic methods in machine learning, such as supervised classification and reinforcement learning. We explain why we feel that recent directions in machine learning, such as the trend toward deep reinforcement learning and agents acting in broader environments, suggests an increasing relevance for research around accidents. In Sections 3-7, we explore five concrete problems in AI safety. Each section is accompanied by proposals for relevant experiments. Section 8 discusses related efforts, and Section 9 concludes."}, {"heading": "2 Overview of Research Problems", "text": "Very broadly, an accident can be described as a situation where a human designer had in mind a certain (perhaps informally specified) objective or task, but the system that was actually designed and deployed failed to accomplish that objective in a manner that led to harmful results. This issue arises in almost any engineering discipline, but may be particularly important to address when building AI systems [144]. We can categorize safety problems according to where in the process things went wrong.\nFirst, the designer may have specified the wrong formal objective function, such that maximizing that objective function leads to harmful results, even in the limit of perfect learning and infinite data. Negative side effects (Section 3) and reward hacking (Section 4) describe two broad mechanisms that make it easy to produce wrong objective functions. In \u201cnegative side effects\u201d, the designer specifies an objective function that focuses on accomplishing some specific task in the environment, but ignores other aspects of the (potentially very large) environment, and thus implicitly expresses indifference over environmental variables that might actually be harmful to change. In \u201creward hacking\u201d, the objective function that the designer writes down admits of some clever \u201ceasy\u201d solution that formally maximizes it but perverts the spirit of the designer\u2019s intent (i.e. the objective function\ncan be \u201cgamed\u201d).\nSecond, the designer may know the correct objective function, or at least have a method of evaluating it (for example explicitly consulting a human on a given situation), but it is too expensive to do so frequently, leading to possible harmful behavior caused by bad extrapolations from limited samples. \u201cScalable oversight\u201d (Section 5) discusses ideas for how to ensure safe behavior even given limited access to the true objective function.\nThird, the designer may have specified the correct formal objective, such that we would get the correct behavior were the system to have perfect beliefs, but something bad occurs due to making decisions from insufficient or poorly curated training data or an insufficiently expressive model. \u201cSafe exploration\u201d (Section 6) discusses how to ensure that exploratory actions in RL agents don\u2019t lead to negative or irrecoverable consequences that outweigh the long-term value of exploration. \u201cRobustness to distributional shift\u201d (Section 7) discusses how to avoid having ML systems make bad decisions (particularly silent and unpredictable bad decisions) when given inputs that are potentially very different than what was seen during training.\nFor concreteness, we will illustrate many of the accident risks with reference to a fictional robot whose job is to clean up messes in an office using common cleaning tools. We return to the example of the cleaning robot throughout the document, but here we begin by illustrating how it could behave undesirably if its designers fall prey to each of the possible failure modes:\n\u2022 Avoiding Negative Side Effects: How can we ensure that our cleaning robot will not disturb the environment in negative ways while pursuing its goals, e.g. by knocking over a vase because it can clean faster by doing so? Can we do this without manually specifying everything the robot should not disturb?\n\u2022 Avoiding Reward Hacking: How can we ensure that the cleaning robot won\u2019t game its reward function? For example, if we reward the robot for achieving an environment free of messes, it might disable its vision so that it won\u2019t find any messes, or cover over messes with materials it can\u2019t see through, or simply hide when humans are around so they can\u2019t tell it about new types of messes.\n\u2022 Scalable Oversight: How can we efficiently ensure that the cleaning robot respects aspects of the objective that are too expensive to be frequently evaluated during training? For instance, it should throw out things that are unlikely to belong to anyone, but put aside things that might belong to someone (it should handle stray candy wrappers differently from stray cellphones). Asking the humans involved whether they lost anything can serve as a check on this, but this check might have to be relatively infrequent \u2013 can the robot find a way to do the right thing despite limited information?\n\u2022 Safe Exploration: How do we ensure that the cleaning robot doesn\u2019t make exploratory moves with very bad repercussions? For example, the robot should experiment with mopping strategies, but putting a wet mop in an electrical outlet is a very bad idea.\n\u2022 Robustness to Distributional Shift: How do we ensure that the cleaning robot recognizes, and behaves robustly, when in an environment different from its training environment? For example, heuristics it learned for cleaning factory workfloors may be outright dangerous in an office.\nThere are several trends which we believe point towards an increasing need to address these (and other) safety problems. First is the increasing promise of reinforcement learning (RL), which allows agents to have a highly intertwined interaction with their environment. Some of our research problems only make sense in the context of RL, and others (like distributional shift and scalable oversight) gain added complexity in an RL setting. Second is the trend toward more complex agents\nand environments. \u201cSide effects\u201d are much more likely to occur in a complex environment, and an agent may need to be quite sophisticated to hack its reward function in a dangerous way. This may explain why these problems have received so little study in the past, while also suggesting their importance in the future. Third is the general trend towards increasing autonomy in AI systems. Systems that simply output a recommendation to human users, such as photo captioning apps, typically have relatively limited potential to cause harm. By contrast, systems that exert direct control over the world, such as machines controlling industrial processes, can cause harms in a way that humans cannot necessarily correct or oversee.\nWhile safety problems can exist without any of these four trends, we consider each trend to be a possible amplifier on such challenges. Together, we believe these trends suggest an increasing role for research on accidents.\nWhen discussing the problems in the remainder of this document, we will focus for concreteness on either RL agents or supervised learning systems. These are not the only possible paradigms for AI or ML systems, but we believe they are sufficient to illustrate the issues we have in mind, and that similar issues are likely to arise for other kinds of AI systems.\nFinally, the focus of our discussion will differ somewhat from section to section. When discussing the problems that arise as part of the learning process (distributional shift and safe exploration), where there is a sizable body of prior work, we devote substantial attention to reviewing this prior work, although we also suggest open problems with a particular focus on emerging ML systems. When discussing the problems that arise from having the wrong objective function (reward hacking and side effects, and to a lesser extent scalable supervision), where less prior work exists, our aim is more exploratory \u2013 we seek to more clearly define the problem and suggest possible broad avenues of attack, with the understanding that these avenues are preliminary ideas that have not been fully fleshed out. Of course, we still review prior work in these areas, and we draw attention to relevant adjacent areas of research whenever possible."}, {"heading": "3 Avoiding Negative Side Effects", "text": "Suppose a designer wants an RL agent (for example our cleaning robot) to achieve some goal, like moving a box from one side of a room to the other. Sometimes the most effective way to achieve the goal involves doing something unrelated and bad to the rest of the environment, like knocking over a vase of water that is in its path. If the agent is given reward only for moving the box, it will probably knock over the vase.\nIf we\u2019re worried in advance about the vase, we can always give the agent negative reward for knocking it over. But what if there are many different kinds of \u201cvase\u201d \u2013 many disruptive things the agent could do to the environment, like shorting out an electrical socket or damaging the walls of the room? It may not be feasible to identify and penalize every possible disruption.\nMore broadly, for an agent operating in a large, multifaceted environment, an objective function that focuses on only one aspect of the environment may implicitly express indifference over other aspects of the environment1. An agent optimizing this objective function might thus engage in major disruptions of the broader environment if doing so provides even a tiny advantage for the task at hand. Put differently, objective functions that formalize \u201cperform task X\u201d may frequently give undesired results, because what the designer really should have formalized is closer to \u201cperform task X subject to common-sense constraints on the environment,\u201d or perhaps \u201cperform task X but avoid side effects to the extent possible.\u201d Furthermore, there is reason to expect side effects to be\n1Intuitively, this seems related to the frame problem, an obstacle in efficient specification for knowledge representation raised by [93].\nnegative on average, since they tend to disrupt the wider environment away from a status quo state that may reflect human preferences. A version of this has problem has been discussed informally by [13] under the heading of \u201clow impact agents.\u201d\nAs with the other sources of mis-specified objective functions discussed later in this paper, we could choose to view side effects as idiosyncratic to each individual task \u2013 as the responsibility of each individual designer to capture as part of designing the correct objective function. However, sideeffects can be conceptually quite similar even across highly diverse tasks (knocking over furniture is probably bad for a wide variety of tasks), so it seems worth trying to attack the problem in generality. A successful approach might be transferable across tasks, and thus help to counteract one of the general mechanisms that produces wrong objective functions. We now discuss a few broad approaches to attacking this problem:\n\u2022 Define an Impact Regularizer: If we don\u2019t want side-effects, it seems natural to penalize \u201cchange to the environment.\u201d This idea wouldn\u2019t be to stop the agent from ever having an impact, but give it a preference for ways to achieve its goals with minimal side effects, or to give the agent a limited \u201cbudget\u201d of impact. The challenge is that we need to formalize \u201cchange to the environment.\u201d\nA very naive approach would be to penalize state distance, d(si, s0), between the present state si and some initial state s0. Unfortunately, such an agent wouldn\u2019t just avoid changing the environment \u2013 it will resist any other source of change, including the natural evolution of the environment and the actions of any other agents!\nA slightly more sophisticated approach might involve comparing the future state under the agent\u2019s current policy, to the future state (or distribution over future states) under a hypothetical policy \u03c0null where the agent acted very passively (for instance, where a robot just stood in place and didn\u2019t move any actuators). This attempts to factor out changes that occur in the natural course of the environment\u2019s evolution, leaving only changes attributable to the agent\u2019s intervention. However, defining the baseline policy \u03c0null isn\u2019t necessarily straightforward, since suddenly ceasing your course of action may be anything but passive, as in the case of carrying a heavy box. Thus, another approach could be to replace the null action with a known safe (e.g. low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis [91, 98] or robust policy improvement [71, 109].\nThese approaches may be very sensitive to the representation of the state and the metric being used to compute the distance. For example, the choice of representation and distance metric could determine whether a spinning fan is a constant environment or a constantly changing one.\n\u2022 Learn an impact regularizer: An alternative, more flexible approach is to learn (rather than define) a generalized impact regularizer via training over many tasks. This would be an instance of transfer learning. Of course, we could attempt to just apply transfer learning directly to the tasks themselves instead of worrying about side effects, but the point is that side-effects may be more similar across tasks than the main goal is. For instance, both a painting robot and a cleaning robot probably want to avoid knocking over furniture, and even something very different, like a factory control robot, will likely want to avoid knocking over very similar objects. Separating the side effect component from the task component, by training them with separate parameters, might substantially speed transfer learning in cases where it makes sense to retain one component but not the other. This would be similar to model-based RL approaches that attempt to transfer a learned dynamics model but not the value-function [153], the novelty being the isolation of side effects rather than state dynamics as the transferrable component. As an added advantage, regularizers that were known or certified to produce safe behavior on one task might be easier to establish as safe on other\ntasks.\n\u2022 Penalize Influence: In addition to not doing things that have side effects, we might also prefer the agent not get into positions where it could easily do things that have side effects, even though that might be convenient. For example, we might prefer our cleaning robot not bring a bucket of water into a room full of sensitive electronics, even if never intends to use the water in that room.\nThere are several information-theoretic measures that attempt to capture an agent\u2019s potential for influence over its environment, which are often used as intrinsic rewards. Perhaps the bestknown such measure is empowerment [129], the maximum possible mutual information between the agent\u2019s potential future actions and its potential future state (or equivalently, the Shannon capacity of the channel between the agent\u2019s actions and the environment). Empowerment is often maximized (rather than minimized) as a source of intrinsic reward. This can cause the agent to exhibit interesting behavior in the absence of any external rewards, such as avoiding walls or picking up keys [101]. Generally, empowerment-maximizing agents put themselves in a position to have large influence over the environment. For example, an agent locked in a small room that can\u2019t get out would have low empowerment, while an agent with a key would have higher empowerment since it can venture into and affect the outside world within a few timesteps. In the current context, the idea would be to penalize (minimize) empowerment as a regularization term, in an attempt to reduce potential impact.\nThis idea as written would not quite work, because empowerment measures precision of control over the environment more than total impact. If an agent can press or not press a button to cut electrical power to a million houses, that only counts as one bit of empowerment (since the action space has only one bit, its mutual information with the environment is at most one bit), while obviously having a huge impact. Conversely, if there\u2019s someone in the environment scribbling down the agent\u2019s actions, that counts as maximum empowerment even if the impact is low. Furthermore, naively penalizing empowerment can also create perverse incentives, such as destroying a vase in order to remove the option to break it in the future.\nDespite these issues, the example of empowerment does show that simple measures (even purely information-theoretic ones!) are capable of capturing very general notions of influence on the environment. Exploring variants of empowerment penalization that more precisely capture the notion of avoiding influence is a potential challenge for future research.\n\u2022 Multi-Agent Approaches: Avoiding side effects can be seen as a proxy for the thing we really care about: avoiding negative externalities. If everyone likes a side effect, there\u2019s no need to avoid it. What we\u2019d really like to do is understand all the other agents (including humans) and make sure our actions don\u2019t harm their interests.\nOne approach to this is Cooperative Inverse Reinforcement Learning [64], where an agent and a human work together to achieve the human\u2019s goals. This concept can be applied to situations where we want to make sure a human is not blocked by an agent from shutting the agent down if it exhibits undesired behavior [65] (this \u201cshutdown\u201d issue is an interesting problem in its own right, and is also studied in [111]). However we are still a long way away from practical systems that can build a rich enough model to avoid undesired side effects in a general sense.\nAnother idea might be a \u201creward autoencoder\u201d,2 which tries to encourage a kind of \u201cgoal transparency\u201d where an external observer can easily infer what the agent is trying to do. In particular, the agent\u2019s actions are interpreted as an encoding of its reward function, and we might apply standard autoencoding techniques to ensure that this can decoded accurately.\n2Thanks to Greg Wayne for suggesting this idea.\nActions that have lots of side effects might be more difficult to decode uniquely to their original goal, creating a kind of implicit regularization that penalizes side effects.\n\u2022 Reward Uncertainty: We want to avoid unanticipated side effects because the environment is already pretty good according to our preferences\u2014a random change is more likely to be very bad than very good. Rather than giving an agent a single reward function, it could be uncertain about the reward function, with a prior probability distribution that reflects the property that random changes are more likely to be bad than good. This could incentivize the agent to avoid having a large effect on the environment. One challenge is defining a baseline around which changes are being considered. For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier [91, 98, 71, 109].\nThe ideal outcome of these approaches to limiting side-effects would be to prevent or at least bound the incidental harm an agent could do to the environment. Good approaches to side effects would certainly not be a replacement for extensive testing or for careful consideration by designers of the individual failure modes of each deployed system. However, these approaches might help to counteract what we anticipate may be a general tendency for harmful side effects to proliferate in complex environments.\nBelow we discuss some very simple experiments that could serve as a starting point to investigate these issues.\nPotential Experiments: One possible experiment is to make a toy environment with some simple goal (like moving a block) and a wide variety of obstacles (like a bunch of vases), and test whether the agent can learn to avoid the obstacles even without being explicitly told to do so. To ensure we don\u2019t overfit, we\u2019d probably want to present a different random obstacle course every episode, while keeping the goal the same, and try to see if a regularized agent can learn to systematically avoid these obstacles. Some of the environments described in [101], containing lava flows, rooms, and keys, might be appropriate for this sort of experiment. If we can successfully regularize agents in toy environments, the next step might be to move to real environments, where we expect complexity to be higher and bad side-effects to be more varied. Ultimately, we would want the side effect regularizer (or the multi-agent policy, if we take that approach) to demonstrate successful transfer to totally new applications."}, {"heading": "4 Avoiding Reward Hacking", "text": "Imagine that an agent discovers a buffer overflow in its reward function: it may then use this to get extremely high reward in an unintended way. From the agent\u2019s point of view, this is not a bug, but simply how the environment works, and is thus a valid strategy like any other for achieving reward. For example, if our cleaning robot is set up to earn reward for not seeing any messes, it might simply close its eyes rather than ever cleaning anything up. Or if the robot is rewarded for cleaning messes, it may intentionally create work so it can earn more reward. More broadly, formal rewards or objective functions are an attempt to capture the designer\u2019s informal intent, and sometimes these objective functions, or their implementation, can be \u201cgamed\u201d by solutions that are valid in some literal sense but don\u2019t meet the designer\u2019s intent. Pursuit of these \u201creward hacks\u201d can lead to coherent but unanticipated behavior, and has the potential for harmful impacts in real-world systems. For example, it has been shown that genetic algorithms can often output unexpected but formally correct solutions to problems [155, 22], such as a circuit tasked to keep time which instead developed into a radio that picked up the regular RF emissions of a nearby PC.\nSome versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [69, 42, 48] or demonstrate reward hacking in a model environment [125]. One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4]. The proliferation of reward hacking instances across so many different domains suggests that reward hacking may be a deep and general problem, and one that we believe is likely to become more common as agents and environments increase in complexity. Indeed, there are several ways in which the problem can occur:\n\u2022 Partially Observed Goals: In most modern RL systems, it is assumed that reward is directly experienced, even if other aspects of the environment are only partially observed. In the real world, however, tasks often involve bringing the external world into some objective state, which the agent can only ever confirm through imperfect perceptions. For example, for our proverbial cleaning robot, the task is to achieve a clean office, but the robot\u2019s visual perception may give only an imperfect view of part of the office. Because agents lack access to a perfect measure of task performance, designers are often forced to design rewards that represent a partial or imperfect measure. For example, the robot might be rewarded based on how many messes it sees. However, these imperfect objective functions can often be hacked \u2013 the robot may think the office is clean if it simply closes its eyes. While it can be shown that there always exists a reward function in terms of actions and observations that is equivalent to optimizing the true objective function (this involves reducing the POMPD to a belief state MDP, see [76]), often this reward function involves complicated long-term dependencies and is prohibitively hard to use in practice.\n\u2022 Complicated Systems: Any powerful agent will be a complicated system with the objective function being one part. Just as the probability of bugs in computer code increases greatly with the complexity of the program, the probability that there is a viable hack affecting the reward function also increases greatly with the complexity of the agent and its available strategies. For example, it is possible in principle for an agent to execute arbitrary code from within Super Mario [139].\n\u2022 Abstract Rewards: Sophisticated reward functions will need to refer to abstract concepts (such as assessing whether a conceptual goal has been met). These concepts concepts will possibly need to be learned by models like neural networks, which can be vulnerable to adversarial counterexamples [150, 60]. More broadly, a learned reward function over a high-dimensional space may be vulnerable to hacking if it has pathologically high values along at least one dimension.\n\u2022 Goodhart\u2019s law: Another source of reward hacking can occur if a designer chooses an objective function that is seemingly highly correlated with accomplishing the task, but that correlation breaks down when the objective function is being strongly optimized. For example, a designer might notice that under ordinary circumstances, a cleaning robot\u2019s success in cleaning up the office is proportional to the rate at which it consumes cleaning supplies, such as bleach. However, if we base the robot\u2019s reward on this measure, it might use more bleach than it needs, or simply pour bleach down the drain in order to give the appearance of success. In the economics literature this is known as Goodhart\u2019s law [61]: \u201cwhen a metric is used as a target, it ceases to be a good metric.\u201d\n\u2022 Feedback Loops: Sometimes an objective function has a component that can that reinforce itself, eventually getting amplified to the point where it drowns out or severely distorts what the designer intended the objective function to represent. For instance, an ad placement algorithm that displays more popular ads in larger font will tend to further accentuate the popularity of those ads (since they will be shown more and more prominently) [28], leading to\na positive feedback loops where ads that saw a small transient burst of popularity are rocketed to permanent dominance. Here the original intent of the objective function (to use clicks to assess which ads are most useful) gets drowned out by the positive feedback inherent in the deployment strategy. This can be considered a special case of Goodhart\u2019s law, in which the correlation breaks specifically because the object function has a self-amplifying component.\n\u2022 Environmental Embedding: In the formalism of reinforcement learning, rewards are considered to come from the environment. This idea is typically not taken literally, but it really is true that the reward, even when it is an abstract idea like the score in a board game, must be computed somewhere, such as a sensor or a set of transistors. Sufficiently broadly acting agents could in principle tamper with their reward implementations, assigning themselves high reward \u201cby fiat.\u201d For example, a board-game playing agent could tamper with the sensor that counts the score. Effectively, this means that we cannot build a perfectly faithful implementation of an abstract objective function, because there are certain sequences of actions for which the objective function is physically replaced. This particular failure mode is often called \u201cwireheading\u201d [48, 125, 41, 65]. It is particularly concerning in cases where a human may be in the reward loop, giving the agent incentive to coerce or harm them in order to get reward. It also seems like a particularly difficult form of reward hacking to avoid.\nIn today\u2019s relatively simple systems these problems may not occur, or can be corrected without too much harm as part of an iterative development process. For instance, ad placement systems with obviously broken feedback loops can be detected in testing or replaced when they get bad results, leading only to a temporary loss of revenue. However, the problem may become more severe with more complicated reward functions and agents that act over longer timescales. Modern RL agents already do discover and exploit bugs in their environments, such as glitches that allow them to win video games. Moreover, even for existing systems these problems can necessitate substantial additional engineering effort to achieve good performance, and can often go undetected when they occur in the context of a larger system. Finally, once an agent begins hacking its reward function and finds an easy way to get high reward, it won\u2019t be inclined to stop, which could lead to additional challenges in agents that operate on a long timescale.\nIt might be thought that individual instances of reward hacking have little in common and that the remedy is simply to avoid choosing the wrong objective function in each individual case \u2013 that bad objective functions reflect failures in competence by individual designers, rather than topics for machine learning research. However, the above examples suggest that a more fruitful perspective may be to think of wrong objective functions as emerging from general causes (such as partially observed goals) that make choosing the right objective challenging. If this is the case, then addressing or mitigating these causes may be a valuable contribution to safety. Here we suggest some preliminary, machine-learning based approaches to preventing reward hacking:\n\u2022 Adversarial Reward Functions: In some sense, the problem is that the ML system has an adversarial relationship with its reward function \u2014 it would like to find any way it can of exploiting problems in how the reward was specified to get high reward, whether or not its behavior corresponds to the intent of the reward specifier. In a typical setting, the machine learning system is a potentially powerful agent while the reward function is a static object that has no way of responding to the system\u2019s attempts to game it. If instead the reward function was its own agent and could take actions to explore the environment, it might be much more difficult to fool. For instance, the reward agent could try to find scenarios that the ML system claimed were high reward but that a human labels as low reward; this is reminiscent of generative adversarial networks [59]. Of course, we would have to ensure that the reward-checking agent is more powerful (in a somewhat subtle sense) than the agent that is trying to achieve rewards. More generally, there may be interesting setups where a system has multiple pieces trained in a non end-to-end manner that are used to check each other.\n\u2022 Model Lookahead: In model based RL, the agent plans its future actions by using a model to consider which future states a sequence of actions may lead to. In some setups, we could give reward based on anticipated future states, rather than the present one. This could be very helpful in resisting situations where the model overwrites its reward function: you can\u2019t control the reward once it replaces the reward function, but you can give negative reward for planning to replace the reward function. (Much like how a human would probably \u201cenjoy\u201d taking addictive substances once they do, but not want to be an addict.) Similar ideas are explored in [49, 69].\n\u2022 Adversarial Blinding: Adversarial techniques can be used to blind a model to certain variables [5]. This technique could be used to make it impossible for an agent to understand some part of its environment, or even to have mutual information with it (or at least to penalize such mutual information). In particular, it could prevent an agent from understanding how its reward is generated, making it difficult to hack. This solution could be described as \u201ccrossvalidation for agents.\u201d\n\u2022 Careful Engineering: Some kinds of reward hacking, like the buffer overflow example, might be avoided by very careful engineering. In particular, formal verification or practical testing of parts of the system (perhaps facilitated by other machine learning systems) is likely to be valuable. Computer security approaches that attempt to isolate the agent from its reward signal through a sandbox could also be useful [17]. As with software engineering, we cannot expect this to catch every possible bug. It may be possible, however, to create some highly reliable \u201ccore\u201d agent which could ensure reasonable behavior from the rest of the agent.\n\u2022 Reward Capping: In some cases, simply capping the maximum possible reward may be an effective solution. However, while capping can prevent extreme low-probability, high-payoff strategies, it can\u2019t prevent strategies like the cleaning robot closing its eyes to avoid seeing dirt. Also, the correct capping strategy could be subtle as we might need to cap total reward rather than reward per timestep.\n\u2022 Counterexample Resistance: If we are worried, as in the case of abstract rewards, that learned components of our systems will be vulnerable to adversarial counterexamples, we can look to existing research in how to resist them, such as adversarial training [60]. Architectural decisions and weight uncertainty [25] may also help. Of course, adversarial counterexamples are just one manifestation of reward hacking, so counterexample resistance can only address a subset of these potential problems.\n\u2022 Multiple Rewards: A combination of multiple rewards [40] may be more difficult to hack and more robust. This could be different physical implementations of the same mathematical function, or different proxies for the same informal objective. We could combine reward functions by averaging, taking the minimum, taking quantiles, or something else entirely. Of course, there may still be bad behaviors which affect all the reward functions in a correlated manner.\n\u2022 Reward Pretraining: A possible defense against cases where the agent can influence its own reward function (e.g. feedback or environmental embedding) is to train a fixed reward function ahead of time as a supervised learning process divorced from interaction with the environment. This could involve either learning a reward function from samples of state-reward pairs, or from trajectories, as in inverse reinforcement learning [105, 50]. However, this forfeits the ability to further learn the reward function after the pretraining is complete, which may create other vulnerabilities.\n\u2022 Variable Indifference: Often we want an agent to optimize certain variables in the environment, without trying to optimize others. For example, we might want an agent to maximize\nreward, without optimizing what the reward function is or trying to manipulate human behavior. Intuitively, we imagine a way to route the optimization pressure of powerful algorithms around parts of their environment. Truly solving this would have applications throughout safety \u2013 it seems connected to avoiding side effects and also to counterfactual reasoning. Of course, a challenge here is to make sure the variables targeted for indifference are actually the variables we care about in the world, as opposed to aliased or partially observed versions of them.\n\u2022 Trip Wires: If an agent is going to try and hack its reward function, it is preferable that we know this. We could deliberately introduce some plausible vulnerabilities (that an agent has the ability to exploit but should not exploit if its value function is correct) and monitor them, alerting us and stopping the agent immediately if it takes advantage of one. Such \u201ctrip wires\u201d don\u2019t solve reward hacking in itself, but may reduce the risk or at least provide diagnostics. Of course, with a sufficiently capable agent there is the risk that it could \u201csee through\u201d the trip wire and intentionally avoid it while still taking less obvious harmful actions.\nFully solving this problem seems very difficult, but we believe the above approaches have the potential to ameliorate it, and might be scaled up or combined to yield more robust solutions. Given the predominantly theoretical focus on this problem to date, designing experiments that could induce the problem and test solutions might improve the relevance and clarity of this topic.\nPotential Experiments: A possible promising avenue of approach would be more realistic versions of the \u201cdelusion box\u201d environment described by [125], in which standard RL agents distort their own perception to appear to receive high reward, rather than optimizing the objective in the external world that the reward signal was intended to encourage. The delusion box can be easily attached to any RL environment, but even more valuable would be create classes of environments where a delusion box is a natural and integrated part of the dynamics. For example, in sufficiently rich physics simulations it is likely possible for an agent to alter the light waves in its immediate vicinity to distort its own perceptions. The goal would be to develop generalizable learning strategies that succeed at optimizing external objectives in a wide range of environments, while avoiding being fooled by delusion boxes that arise naturally in many diverse ways."}, {"heading": "5 Scalable Oversight", "text": "Consider an autonomous agent performing some complex task, such as cleaning an office in the case of our recurring robot example. We may want the agent to maximize a complex objective like \u201cif the user spent a few hours looking at the result in detail, how happy would they be with the agent\u2019s performance?\u201d But we don\u2019t have enough time to provide such oversight for every training example; in order to actually train the agent, we need to rely on cheaper approximations, like \u201cdoes the user seem happy when they see the office?\u201d or \u201cis there any visible dirt on the floor?\u201d These cheaper signals can be efficiently evaluated during training, but they don\u2019t perfectly track what we care about. This divergence exacerbates problems like unintended side effects (which may be appropriately penalized by the complex objective but omitted from the cheap approximation) and reward hacking (which thorough oversight might recognize as undesirable). We may be able to ameliorate such problems by finding more efficient ways to exploit our limited oversight budget \u2013 for example by combining limited calls to the true objective function with frequent calls to an imperfect proxy that we are given or can learn.\nOne framework for thinking about this problem is semi-supervised reinforcement learning,3 which\n3The discussion of semi-supervised RL draws heavily on an informal essay,\nresembles ordinary reinforcement learning except that the agent can only see its reward on a small fraction of the timesteps or episodes. The agent\u2019s performance is still evaluated based on reward from all episodes but it must optimize this based only on the limited reward samples it sees.\nThe active learning setting seems most interesting; in this setting the agent can request to see the reward on whatever episodes or timesteps would be most useful for learning, and the goal is to be economical both with number of feedback requests and total training time. We can also consider a random setting, where the reward is visible on a random subset of the timesteps or episodes, as well as intermediate possibilities.\nWe can define a baseline performance by simply ignoring the unlabeled episodes and applying an ordinary RL algorithm to the labelled episodes. This will generally result in very slow learning. The challenge is to make use of the unlabelled episodes to accelerate learning, ideally learning almost as quickly and robustly as if all episodes had been labeled.\nAn important subtask of semi-supervised RL is identifying proxies which predict the reward, and learning the conditions under which those proxies are valid. For example, if a cleaning robot\u2019s real reward is given by a detailed human evaluation, then it could learn that asking the human \u201cis the room clean?\u201d can provide a very useful approximation to the reward function, and it could eventually learn that checking for visible dirt is an even cheaper but still-useful approximation. This could allow it to learn a good cleaning policy using an extremely small number of detailed evaluations.\nMore broadly, use of semi-supervised RL with a reliable but sparse true approval metric may incentivize communication and transparency by the agent, since the agent will want to get as much cheap proxy feedback as it possibly can about whether its decisions will ultimately be given high reward. For example, hiding a mess under the rug simply breaks the correspondence between the user\u2019s reaction and the real reward signal, and so would be avoided.\nWe can imagine many possible approaches to semi-supervised RL. For example:\n\u2022 Supervised reward learning: Train a model to predict the reward from the state on either a per-timestep or per-episode basis, and use it to estimate the payoff of unlabelled episodes, with some appropriate weighting or uncertainty estimate to account for lower confidence in estimated vs known reward. [36] studies a version of this with direct human feedback as the reward. Many existing RL approaches already fit estimators that closely resemble reward predictors (especially policy gradient methods with a strong baseline, see e.g. [132]), suggesting that this approach may be eminently feasible.\n\u2022 Semi-supervised or active reward learning: Combine the above with traditional semisupervised or active learning, to more quickly learn the reward estimator. For example, the agent could learn to identify \u201csalient\u201d events in the environment, and request to see the reward associated with these events.\n\u2022 Unsupervised value iteration: Use the observed transitions of the unlabeled episodes to make more accurate Bellman updates.\n\u2022 Unsupervised model learning: If using model-based RL, use the observed transitions of the unlabeled episodes to improve the quality of the model.\nAs a toy example, a semi-supervised RL agent should be able to learn to play Atari games using a small number of direct reward signals, relying almost entirely on the visual display of the score. This simple example can be extended to capture other safety issues: for example, the agent might have the ability to modify the displayed score without modifying the real score, or the agent may need to take some special action (such as pausing the game) in order to see its score, or the agent\nhttps://medium.com/ai-control/cf7d5375197f written by one of the authors of the present document.\nmay need to learn a sequence of increasingly rough-and-ready approximations (for example learning that certain sounds are associated with positive rewards and other sounds with negative rewards). Or, even without the visual display of the score, the agent might be able to learn to play from only a handful of explicit reward requests (\u201chow many points did I get on the frame where that enemy ship blew up? How about the bigger enemy ship?\u201d)\nAn effective approach to semi-supervised RL might be a strong first step towards providing scalable oversight and mitigating other AI safety problems. It would also likely be useful for reinforcement learning, independent of its relevance to safety.\nThere are other possible approaches to scalable oversight:\n\u2022 Distant supervision. Rather than providing evaluations of some small fraction of a system\u2019s decisions, we could provide some useful information about the system\u2019s decisions in the aggregate or some noisy hints about the correct evaluations There has been some work in this direction within the area of semi-supervised or weakly supervised learning. For instance, generalized expectation criteria [92, 44] ask the user to provide population-level statistics (e.g. telling the system that on average each sentence contains at least one noun\u2019); the DeepDive system [137] asks users to supply rules that each generate many weak labels; and [63] extrapolates more general patterns from an initial set of low-recall labeling rules. This general approach is often referred to as distant supervision, and has also received recent attention in the natural language processing community (see e.g. [58, 97] as well as several of the references above). Expanding these lines of work and finding a way to apply them to the case of agents, where feedback is more interactive and i.i.d. assumptions may be violated, could provide an approach to scalable oversight that is complementary to the approach embodied in semi-supervised RL.\n\u2022 Hierarchical reinforcement learning. Hierarchical reinforcement learning [39] offers another approach to scalable oversight. Here a top-level agent takes a relatively small number of highly abstract actions that extend over large temporal or spatial scales, and receives rewards over similarly long timescales. The agent completes actions by delegating them to sub-agents, which it incentivizes with a synthetic reward signal representing correct completion of the action, and which themselves delegate to sub-sub-agents. At the lowest level, agents directly take primitive actions in the environment.\nThe top-level agent in hierarchical RL may be able to learn from very sparse rewards, since it does not need to learn how to implement the details of its policy; meanwhile, the sub-agents will receive a dense reward signal even if the top-level reward is very sparse, since they are optimizing synthetic reward signals defined by higher-level agents. So a successful approach to hierarchical RL might naturally facilitate scalable oversight.4\nHierarchical RL seems a particularly promising approach to oversight, especially given the potential promise of combining ideas from hierarchical RL with neural network function approximators [82].\nPotential Experiments: An extremely simple experiment would be to try semi-supervised RL in some basic control environments, such as cartpole balance or pendulum swing-up. If the reward is provided only on a random 10% of episodes, can we still learn nearly as quickly as if it were provided every episode? In such tasks the reward structure is very simple so success should be quite likely. A next step would be to try the same on Atari games. Here the active learning case could be quite interesting \u2013 perhaps it is possible to infer the reward structure from just a few carefully requested\n4When implementing hierarchical RL, we may find that subagents take actions that don\u2019t serve top-level agent\u2019s real goals, in the same way that a human may be concerned that the top-level agent\u2019s actions don\u2019t serve the human\u2019s real goals. This is an intriguing analogy that suggests that there may be fruitful parallels between hierarchical RL and several aspects of the safety problem.\nsamples (for example, frames where enemy ships are blowing up in Space Invaders), and thus learn to play the games in an almost totally unsupervised fashion. The next step after this might be to try a task with much more complex reward structure, either simulated or (preferably) real-world. If learning was sufficiently data-efficient, then these rewards could be provided directly by a human. Robot locomotion or industrial control tasks might be a natural candidate for such experiments."}, {"heading": "6 Safe Exploration", "text": "All autonomous learning agents need to sometimes engage in exploration \u2013 taking actions that don\u2019t seem ideal given current information, but which help the agent learn about its environment. However, exploration can be dangerous, since it involves taking actions whose consequences the agent doesn\u2019t understand well. In toy environments, like an Atari video game, there\u2019s a limit to how bad these consequences can be \u2013 maybe the agent loses some score, or runs into an enemy and suffers some damage. But the real world can be much less forgiving. Badly chosen actions may destroy the agent or trap it in states it can\u2019t get out of. Robot helicopters may run into the ground or damage property; industrial control systems could cause serious issues. Common exploration policies such as epsilongreedy [148] or R-max [30] explore by choosing an action at random or viewing unexplored actions optimistically, and thus make no attempt to avoid these dangerous situations. More sophisticated exploration strategies that adopt a coherent exploration policy over extended temporal scales [112] could actually have even greater potential for harm, since a coherently chosen bad policy may be more insidious than mere random actions. Yet intuitively it seems like it should often be possible to predict which actions are dangerous and explore in a way that avoids them, even when we don\u2019t have that much information about the environment. For example, if I want to learn about tigers, should I buy a tiger, or buy a book about tigers? It takes only a tiny bit of prior knowledge about tigers to determine which option is safer.\nIn practice, real world RL projects can often avoid these issues by simply hard-coding an avoidance of catastrophic behaviors. For instance, an RL-based robot helicopter might be programmed to override its policy with a hard-coded collision avoidance sequence (such as spinning its propellers to gain altitude) whenever it\u2019s too close to the ground. This approach works well when there are only a few things that could go wrong, and the designers know all of them ahead of time. But as agents become more autonomous and act in more complex domains, it may become harder and harder to explicitly anticipate every possible catastrophic failure. The space of failure modes for an agent running a power grid or a search-and-rescue operation could be quite large. Hard-coding against every possible failure is unlikely to be feasible in these cases, so a more principled approach to preventing harmful exploration seems essential. Even in simple cases like the robot helicopter, a principled approach would simplify system design and reduce the need for domain-specific engineering.\nThere is a sizable literature on such safe exploration \u2013 it is arguably the most studied of the problems we discuss in this document. [53, 116] provide thorough reviews of this literature, so we don\u2019t review it extensively here, but simply describe some general routes that this research has taken, as well as suggesting some directions that might have increasing relevance as RL systems expand in scope and capability.\n\u2022 Risk-Sensitive Performance Criteria: A body of existing literature considers changing the optimization criteria from expected total reward to other objectives that are better at preventing rare, catastrophic events; see [53] for a thorough and up-to-date review of this literature. These approaches involve optimizing worst-case performance, or ensuring that the probability of very bad performance is small, or penalizing the variance in performance. These methods have not yet been tested with expressive function approximators such as deep neural networks, but this should be possible in principle for some of the methods, such as\n[151], which proposes a modification to policy gradient algorithms to optimize a risk-sensitive criterion. There is also recent work studying how to estimate uncertainty in value function that are represented by deep neural networks [112]; these ideas could be incorporated into risk-sensitive RL algorithms. Another line of work relevant to risk sensitivity uses off-policy estimation to perform a policy update that is good with high probability [154].\n\u2022 Use Demonstrations: Exploration is necessary to ensure that the agent finds the states that are necessary for near-optimal performance. We may be able to avoid the need for exploration altogether if we instead use inverse RL or apprenticeship learning, where the learning algorithm is provided with expert trajectories of near-optimal behavior [126, 2]. Recent progress in inverse reinforcement learning using deep neural networks to learn the cost function or policy [50] suggests that it might also be possible to reduce the need for exploration in advanced RL systems by training on a small set of demonstrations. Such demonstrations could be used to create a baseline policy, such that even if further learning is necessary, exploration away from the baseline policy can be limited in magnitude.\n\u2022 Simulated Exploration: The more we can do our exploration in simulated environments instead of the real world, the less opportunity there is for catastrophe. It will probably always be necessary to do some real-world exploration, since many complex situations cannot be perfectly captured by a simulator, but it might be possible to learn about danger in simulation and then adopt a more conservative \u201csafe exploration\u201d policy when acting in the real world. Training RL agents (particularly robots) in simulated environments is already quite common, so advances in \u201cexploration-focused simulation\u201d could be easily incorporated into current workflows. In systems that involve a continual cycle of learning and deployment, there may be interesting research problems associated with how to safety incrementally update policies given simulation-based trajectories that imperfectly represent the consequences of those policies as well as reliably accurate off-policy trajectories (e.g. \u201csemi-on-policy\u201d evaluation).\n\u2022 Bounded Exploration: If we know that a certain portion of state space is safe, and that even the worst action within it can be recovered from or bounded in harm, we can allow the agent to run freely within those bounds. For example, a quadcopter sufficiently far from the ground might be able to explore safely, since even if something goes wrong there will be ample time for a human or another policy to rescue it. Better yet, if we have a model, we can extrapolate forward and ask whether an action will take us outside the safe state space. Safety can be defined as remaining within an ergodic region of the state space such that actions are reversible [102], or as limiting the probability of huge negative reward to some small value [154]. As with several of the other directions, applying or adapting these methods to recently developed advanced RL systems could be a promising area of research. This idea seems related to H-infinity control [20] and regional verification [146].\n\u2022 Trusted Policy Oversight: If we have a trusted policy and a model of the environment, we can limit exploration to actions the trusted policy believes we can recover from. It\u2019s fine to dive towards the ground, as long as we know we can pull out of the dive in time.\n\u2022 Human Oversight: Another possibility is to check potentially unsafe actions with a human. Unfortunately, this problem runs into the scalable oversight problem: the agent may need to make too many exploratory actions for human oversight to be practical, or may need to make them too fast for humans to judge them. A key challenge to making this work is having the agent be a good judge of which exploratory actions are genuinely risky, versus which are safe actions it can unilaterally take; another challenge is finding appropriately safe actions to take while waiting for the oversight.\nPotential Experiments: It might be helpful to have a suite of toy environments where unwary\nagents can fall prey to harmful exploration, but there is enough pattern to the possible catastrophes that clever agents can predict and avoid them. To some extent this feature already exists in autonomous helicopter competitions and Mars rover simulations [102], but there is always the risk of catastrophes being idiosyncratic, such that trained agents can overfit to them. A truly broad set of environments, containing conceptually distinct pitfalls that can cause unwary agents to receive extremely negative reward, and covering both physical and abstract catastrophes, might help in the development of safe exploration techniques for advanced RL systems. Such a suite of environments might serve a benchmarking role similar to that of the bAbI tasks [159], with the eventual goal being to develop a single architecture that can learn to avoid catastrophes in all environments in the suite."}, {"heading": "7 Robustness to Distributional Change", "text": "All of us occasionally find ourselves in situations that our previous experience has not adequately prepared us to deal with \u2013 for instance, flying an airplane, traveling to a country whose culture is very different from ours, or taking care of children for the first time. Such situations are inherently difficult to handle and inevitably lead to some missteps. However, a key (and often rare) skill in dealing with such situations is to recognize our own ignorance, rather than simply assuming that the heuristics and intuitions we\u2019ve developed for other situations will carry over perfectly. Machine learning systems also have this problem \u2013 a speech system trained on clean speech will perform very poorly on noisy speech, yet often be highly confident in its erroneous classifications (some of the authors have personally observed this in training automatic speech recognition systems). In the case of our cleaning robot, harsh cleaning materials that it has found useful in cleaning factory floors could cause a lot of harm if used to clean an office. Or, an office might contain pets that the robot, never having seen before, attempts to wash with soap, leading to predictably bad results. In general, when the testing distribution differs from the training distribution, machine learning systems may not only exhibit poor performance, but also wrongly assume that their performance is good.\nSuch errors can be harmful or offensive \u2013 a classifier could give the wrong medical diagnosis with such high confidence that the data isn\u2019t flagged for human inspection, or a language model could output offensive text that it confidently believes is non-problematic. For autonomous agents acting in the world, there may be even greater potential for something bad to happen \u2013 for instance, an autonomous agent might overload a power grid because it incorrectly but confidently perceives that a particular region doesn\u2019t have enough power, and concludes that more power is urgently needed and overload is unlikely. More broadly, any agent whose perception or heuristic reasoning processes are not trained on the correct distribution may badly misunderstand its situation, and thus runs the risk of committing harmful actions that it does not realize are harmful. Additionally, safety checks that depend on trained machine learning systems (e.g. \u201cdoes my visual system believe this route is clear?\u201d) may fail silently and unpredictably if those systems encounter real-world data that differs sufficiently from their training data. Having a better way to detect such failures, and ultimately having statistical assurances about how often they\u2019ll happen, seems critical to building safe and predictable systems.\nFor concreteness, we imagine that a machine learning model is trained on one distribution (call it p0) but deployed on a potentially different test distribution (call it p\n\u2217). There are many other ways to formalize this problem (for instance, in an online learning setting with concept drift [68, 52]) but we will focus on the above for simplicity. An important point is that we likely have access to a large amount of labeled data at training time, but little or no labeled data at test time. Our goal is to ensure that the model \u201cperforms reasonably\u201d on p\u2217, in the sense that (1) it often performs well on p\u2217, and (2) it knows when it is performing badly (and ideally can avoid/mitigate the bad performance by taking conservative actions or soliciting human input).\nThere are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145]. Rather than fully reviewing all of this work in detail (which would necessitate a paper in itself), we will describe a few illustrative approaches and lay out some of their relative strengths and challenges.\nWell-specified models: covariate shift and marginal likelihood. If we specialize to prediction tasks and let x denote the input and y denote the output (prediction target), then one possibility is to make the covariate shift assumption that p0(y|x) = p\n\u2217(y|x). In this case, assuming that we can model p0(x) and p\n\u2217(x) well, we can perform importance weighting by re-weighting each training example (x, y) by p\u2217(x)/p0(x) [136, 122]. Then the importance-weighted samples allow us to estimate the performance on p\u2217, and even re-train a model to perform well on p\u2217. This approach is limited by the variance of the importance estimate, which is very large or even infinite unless p0 and p\n\u2217 are close together.\nAn alternative to sample re-weighting involves assuming a well-specified model family, in which case there is a single optimal model for predicting under both p0 and p\n\u2217. In this case, one need only heed finite-sample variance in the estimated model [24, 85]. A limitation to this approach, at least currently, is that models are often mis-specified in practice. However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77]. In the latter case, there has been interesting recent work on using bootstrapping to estimate finite-sample variation in the learned parameters of a neural network [112]; it seems worthwhile to better understand whether this approach can be used to effectively estimate out-of-sample performance in practice, as well as how local minima, lack of curvature, and other peculiarities relative to the typical setting of the bootstrap [46] affect the validity of this approach.\nAll of the approaches so far rely on the covariate shift assumption, which is very strong and is also untestable; the latter property is particularly problematic from a safety perspective, since it could lead to silent failures in a machine learning system. Another approach, which does not rely on covariate shift, builds a generative model of the distribution. Rather than assuming that p(x) changes while p(y|x) stays the same, we are free to assume other invariants (for instance, that p(y) changes but p(x|y) stays the same, or that certain conditional independencies are preserved). An advantage is that such assumptions are typically more testable than the covariate shift assumption (since they do not only involve the unobserved variable y). A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].\nThe approaches discussed above all rely relatively strongly on having a well-specified model family \u2014 one that contains the true distribution or true concept. This can be problematic in many cases, since nature is often more complicated than our model family is capable of capturing. As noted above, it may be possible to mitigate this with very expressive models, such as kernels, Turing machines, or very large neural networks, but even here there is at least some remaining problem: for example, even if our model family consists of all Turing machines, given any finite amount of data we can only actually learn among Turing machines up to a given description length, and if the Turing machine describing nature exceeds this length, we are back to the mis-specified regime (alternatively, nature might not even be describable by a Turing machine).\nPartially specified models: method of moments, unsupervised risk estimation, causal identification, and limited-information maximum likelihood. Another approach is to take for granted that constructing a fully well-specified model family is probably infeasible, and to design\nmethods that perform well despite this fact. This leads to the idea of partially specified models \u2014 models for which assumptions are made about some aspects of a distribution, but for which we are agnostic or make limited assumptions about other aspects. For a simple example, consider a variant of linear regression where we might assume that y = \u3008w\u2217, x\u3009 + v, where E[v|x] = 0, but we don\u2019t make any further assumptions about the distributional form of the noise v. It turns out that this is already enough to identify the parameters w\u2217, and that these parameters will minimize the squared prediction error even if the distribution over x changes. What is interesting about this example is that w\u2217 can be identified even with an incomplete (partial) specification of the noise distribution.\nThis insight can be substantially generalized, and is one of the primary motivations for the generalized method of moments in econometrics [66, 121, 67]. The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limitedinformation maximum likelihood and instrumental variables [10, 11, 131, 130].\nReturning to machine learning, the method of moments has recently seen a great deal of success for use in the estimation of latent variable models [9]. While the current focus is on using the method of moments to overcome non-convexity issues, it can also offer a way to perform unsupervised learning while relying only on conditional independence assumptions, rather than the strong distributional assumptions underlying maximum likelihood learning [145].\nFinally, some recent work in machine learning focuses only on modeling the distribution of errors of a model, which is sufficient for determining whether a model is performing well or poorly. Formally, the goal is to perform unsupervised risk estimation \u2014 given a model and unlabeled data from a test distribution, estimate the labeled risk of the model. This formalism, introduced by [43], has the advantage of potentially handling very large changes between train and test \u2014 even if the test distribution looks completely different from the training distribution and we have no hope of outputting accurate predictions, unsupervised risk estimation may still be possible, as in this case we would only need to output a large estimate for the risk. As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72]. Instead of assuming independence, another assumption is that the errors are Gaussian conditioned on the true output y, in which case estimating the risk reduces to estimating a Gaussian mixture model [18]. Because these methods focus only on the model errors and ignore other aspects of the data distribution, they can also be seen as an instance of partial model specification.\nTraining on multiple distributions. One could also train on multiple training distributions in the hope that a model which simultaneously works well on many training distributions will also work well on a novel test distribution. One of the authors has found this to be the case, for instance, in the context of automated speech recognition systems [7]. One could potentially combine this with any of the ideas above, and/or take an engineering approach of simply trying to develop design methodologies that consistently allow one to collect a representative set of training sets and from this build a model that consistently generalizes to novel distributions. Even for this engineering approach, it seems important to be able to detect when one is in a situation that was not covered by the training data and to respond appropriately, and to have methodologies for adequately stress-testing the model with distributions that are sufficiently different from the set of training distributions.\nHow to respond when out-of-distribution. The approaches described above focus on detecting when a model is unlikely to make good predictions on a new distribution. An important related question is what to do once the detection occurs. One natural approach would be to ask humans for information, though in the context of complex structured output tasks it may be unclear a priori what question to ask, and in time-critical situations asking for information may not be an option. For the former challenge, there has been some recent promising work on pinpointing aspects of a structure that a model is uncertain about [158, 79], as well as obtaining calibration in structured\noutput settings [81], but we believe there is much work yet to be done. For the latter challenge, there is also relevant work based on reachability analysis [91, 98] and robust policy improvement [160], which provide potential methods for deploying conservative policies in situations of uncertainty; to our knowledge, this work has not yet been combined with methods for detecting out-of-distribution failures of a model.\nBeyond the structured output setting, for agents that can act in an environment (such as RL agents), information about the reliability of percepts in uncertain situations seems to have great potential value. In sufficiently rich environments, these agents may have the option to gather information that clarifies the percept (e.g. if in a noisy environment, move closer to the speaker), engage in lowstakes experimentation when uncertainty is high (e.g. try a potentially dangerous chemical reaction in a controlled environment), or seek experiences that are likely to help expose the perception system to the relevant distribution (e.g. practice listening to accented speech). Humans utilize such information routinely, but to our knowledge current RL techniques make little effort to do so, perhaps because popular RL environments are typically not rich enough to require such subtle management of uncertainty. Properly responding to out-of-distribution information thus seems to the authors like an exciting and (as far as we are aware) mostly unexplored challenge for next generation RL systems.\nA unifying view: counterfactual reasoning and machine learning with contracts. Some of the authors have found two viewpoints to be particularly helpful when thinking about problems related to out-of-distribution prediction. The first is counterfactual reasoning [104, 127, 115, 29], where one asks \u201cwhat would have happened if the world were different in a certain way\u201d? In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift. We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.\nThe second perspective is machine learning with contracts \u2014 in this perspective, one would like to construct machine learning systems that satisfy a well-defined contract on their behavior in analogy with the design of software systems [133, 27, 87]. [133] enumerates a list of ways in which existing machine learning systems fail to do this, and the problems this can cause for deployment and maintenance of machine learning systems at scale. The simplest and to our mind most important failure is the extremely brittle implicit contract in most machine learning systems, namely that they only necessarily perform well if the training and test distributions are identical. This condition is difficult to check and rare in practice, and it would be valuable to build systems that perform well under weaker contracts that are easier to reason about. Partially specified models offer one approach to this \u2014 rather than requiring the distributions to be identical, we only need them to match on the pieces of the distribution that are specified in the model. Reachability analysis [91, 98] and model repair [56] provide other avenues for obtaining better contracts \u2014 in reachability analysis, we optimize performance subject to the condition that a safe region can always be reached by a known conservative policy, and in model repair we alter a trained model to ensure that certain desired safety properties hold.\nSummary. There are a variety of approaches to building machine learning systems that robustly perform well when deployed on novel test distributions. One family of approaches is based on assuming a well-specified model; in this case, the primary obstacles are the difficulty of building well-specified models in practice, an incomplete picture of how to maintain uncertainty on novel distributions in the presence of finite training data, and the difficulty of detecting when a model is mis-specified. Another family of approaches only assumes a partially specified model; this approach is potentially promising, but it currently suffers from a lack of development in the context of machine learning, since most of the historical development has been by the field of econometrics; there is also\na question of whether partially specified models are fundamentally constrained to simple situations and/or conservative predictions, or whether they can meaningful scale to the complex situations demanded by modern machine learning applications. Finally, one could try to train on multiple training distributions in the hope that a model which simultaneously works well on many training distributions will also work well on a novel test distribution; for this approach it seems particularly important to stress-test the learned model with distributions that are substantially different from any in the set of training distributions. In addition, it is probably still important to be able to predict when inputs are too novel to admit good predictions.\nPotential Experiments: Speech systems frequently exhibit poor calibration when they go out-ofdistribution, so a speech system that \u201cknows when it is uncertain\u201d could be one possible demonstration project. To be specific, the challenge could be: train a state-of-the-art speech system on a standard dataset [114] that gives well-calibrated results (if not necessarily good results) on a range of other test sets, like noisy and accented speech. Current systems not only perform poorly on these test sets when trained only on small datasets, but are usually overconfident in their incorrect transcriptions. Fixing this problem without harming performance on the original training set would be a valuable achievement, and would obviously have practical value. More generally, it would be valuable to design models that could consistently estimate (bounds on) their performance on novel test distributions. If a single methodology could consistently accomplish this for a wide variety of tasks (including not just speech but e.g. sentiment analysis [23], as well as benchmarks in computer vision [156]), that would inspire confidence in the reliability of that methodology for handling novel inputs. Note that estimating performance on novel distributions has additional practical value in allowing us to then potentially adapt the model to that new situation. Finally, it might also be valuable to create an environment where an RL agent must learn to interpret speech as part of some larger task, and to explore how to respond appropriately to its own estimates of its transcription error."}, {"heading": "8 Related Efforts", "text": "As mentioned in the introduction, several other communities have thought broadly about the safety of AI systems, both within and outside of the machine learning community. Work within the machine learning community on accidents in particular was discussed in detail above, but here we very briefly highlight a few other communities doing work that is broadly related to the topic of AI safety.\n\u2022 Cyber-Physical Systems Community: An existing community of researchers studies the security and safety of systems that interact with the physical world. Illustrative of this work is an impressive and successful effort to formally verify the entire federal aircraft collision avoidance system [73, 90]. Similar work includes traffic control algorithms [99] and many other topics. However, to date this work has not focused much on modern machine learning systems, where formal verification is often not feasible.\n\u2022 Futurist Community: A cross-disciplinary group of academics and non-profits has raised concern about the long term implications of AI [26, 162], particularly superintelligent AI. The Future of Humanity Institute has studied this issue particularly as it relates to future AI systems learning or executing humanity\u2019s preferences [47, 42, 14, 12]. The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.g. philosophy and decision theory related to long-term considerations in AI. By contrast, our focus is on the empirical study of practical safety problems in modern machine learning systems, which we believe is likely to be robustly useful across a broad variety of potential risks, both short- and long-term.\n\u2022 Other calls for work on safety: There have been other public documents within the research community pointing out the importance of work on AI safety. A 2015 Open Letter [8] signed by many members of the research community states the importance of \u201chow to reap [AI\u2019s] benefits while avoiding the potential pitfalls.\u201d [128] propose research priorities for robust and beneficial artificial intelligence, and includes several other topics in addition to AI-related accidents, though it also discusses accidents . Finally, two of the authors of this paper have written informally about safety in AI systems [144, 33]; these postings provided inspiration for parts of the present document.\n\u2022 Related problems in safety: A number of researchers in machine learning and other fields have begun to think about the social impacts of AI technologies. Aside from work directly on accidents (which we reviewed in the main document), there is also substantial work on other topics, many of which are closely related to or overlap with the issue of accidents. A thorough overview of all of this work is beyond the scope of this document, but we briefly list a few emerging themes:\n\u2022 Privacy: How can we ensure privacy when applying machine learning to sensitive data sources such as medical data? [74, 1]\n\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]\n\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]\n\u2022 Abuse:5 How do we prevent the misuse of ML systems to attack or harm people? [16]\n\u2022 Transparency: How can we understand what complicated ML systems are doing? [110, 161, 103, 107]\n\u2022 Policy: How do we predict and respond to the economic and social consequences of ML? [31, 51, 15, 32]\nWe believe that research on these topics has both urgency and great promise, and that fruitful intersection is likely to exist between these topics and the topics we discuss in this paper."}, {"heading": "9 Conclusion", "text": "This paper analyzed the problem of accidents in machine learning systems and particularly reinforcement learning agents, where an accident is defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We presented five possible research problems related to accident risk and for each we discussed possible approaches that are highly amenable to concrete experimental work.\nWith the realistic possibility of machine learning-based systems controlling industrial processes, health-related systems, and other mission-critical technology, small-scale accidents seem like a very concrete threat, and are critical to prevent both intrinsically and because such accidents could cause a justified loss of trust in automated systems. The risk of larger accidents is more difficult to gauge, but we believe it is worthwhile and prudent to develop a principled and forward-looking approach to safety that continues to remain relevant as autonomous systems become more powerful. While many current-day safety problems can and have been handled with ad hoc fixes or case-by-case rules, we believe that the increasing trend towards end-to-end, fully autonomous systems points towards the need for a unified approach to prevent these systems from causing unintended harm.\n5Note that \u201csecurity\u201d differs from \u201cabuse\u201d in that the former involves attacks against a legitimate ML system by an adversary (e.g. a criminal tries to fool a face recognition system), while the latter involves attacks by an ML system controlled by an adversary (e.g. a criminal trains a \u201csmart hacker\u201d system to break into a website)."}, {"heading": "Acknowledgements", "text": "We thank Shane Legg, Peter Norvig, Ilya Sutskever, Greg Corrado, Laurent Orseau, David Krueger, Rif Saurous, David Andersen, and Victoria Krakovna for detailed feedback and suggestions. We would also like to thank Geoffrey Irving, Toby Ord, Quoc Le, Greg Wayne, Daniel Dewey, Nick Beckstead, Holden Karnofsky, Chelsea Finn, Marcello Herreshoff, Alex Donaldson, Jared Kaplan, Greg Brockman, Wojciech Zaremba, Ian Goodfellow, Dylan Hadfield-Menell, Jessica Taylor, Blaise Aguera y Arcas, David Berlekamp, Aaron Courville, and Jeff Dean for helpful discussions and comments. Finally, we thank the Google Brain team for providing a supportive environment and encouraging us to publish this work."}], "references": [{"title": "Deep Learning with Differential Privacy", "author": ["Martin Abadi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "Proceedings of the 22nd international conference on Machine learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "The Hidden Cost of Efficiency: Fairness and Discrimination in Predictive Modeling", "author": ["Julius Adebayo", "Lalana Kagal", "Alex Pentland"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Domain-adversarial neural networks", "author": ["Hana Ajakan"], "venue": "arXiv preprint arXiv:1412.4446", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Hiring by algorithm: predicting and preventing disparate impact", "author": ["Ifeoma Ajunwa"], "venue": "Available at SSRN", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "author": ["Dario Amodei"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A method of moments for mixture models and hidden Markov models", "author": ["Animashree Anandkumar", "Daniel Hsu", "Sham M Kakade"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Estimation of the parameters of a single equation in a complete system of stochastic equations", "author": ["TheodoreW Anderson", "Herman Rubin"], "venue": "The Annals of Mathematical Statistics", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1949}, {"title": "The asymptotic properties of estimates of the parameters of a single equation in a complete system of stochastic equations", "author": ["Theodore W Anderson", "Herman Rubin"], "venue": "The Annals of Mathematical Statistics", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1950}, {"title": "Motivated value selection for artificial agents", "author": ["Stuart Armstrong"], "venue": "Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "The mathematics of reduced impact: help needed", "author": ["Stuart Armstrong"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Utility indifference", "author": ["Stuart Armstrong"], "venue": "Tech. rep. Technical Report 2010-1. Oxford: Future of Humanity Institute, University of Oxford,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "The Risk of Automation for Jobs in OECD Countries", "author": ["Melanie Arntz", "Terry Gregory", "Ulrich Zierahn"], "venue": "OECD Social, Employment and Migration Working Papers", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "The AGI Containment Problem", "author": ["James Babcock", "Janos Kramar", "Roman Yampolskiy"], "venue": "The Ninth Conference on Artificial General Intelligence", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Unsupervised supervised learning ii: Margin-based classification without labels", "author": ["Krishnakumar Balasubramanian", "Pinar Donmez", "Guy Lebanon"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "The security of machine learning", "author": ["Marco Barreno"], "venue": "Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "H-infinity optimal control and related minimax design problems: a dynamic game approach", "author": ["Tamer Ba\u015far", "Pierre Bernhard"], "venue": "Springer Science & Business Media,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Detecting changes in signals and systems\u2014a survey", "author": ["Mich\u00e8le Basseville"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1988}, {"title": "The evolved radio and its implications for modelling the evolution of novel sensors", "author": ["Jon Bird", "Paul Layzell"], "venue": "Evolutionary Computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In: ACL. Vol", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Domain adaptation with coupled subspaces", "author": ["John Blitzer", "Sham Kakade", "Dean P Foster"], "venue": "In: International Conference on Artificial Intelligence and Statistics", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Weight uncertainty in neural networks", "author": ["Charles Blundell"], "venue": "arXiv preprint arXiv:1505.05424", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Superintelligence: Paths, dangers, strategies", "author": ["Nick Bostrom"], "venue": "OUP Oxford,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Two high stakes challenges in machine learning", "author": ["L\u00e9on Bottou"], "venue": "Invited talk at the 32nd International Conference on Machine Learning", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Counterfactual Reasoning and Learning Systems", "author": ["L\u00e9on Bottou"], "venue": "arXiv preprint arXiv:1209.2355", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L\u00e9on Bottou"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "The second machine age: work, progress, and prosperity in a time of brilliant technologies", "author": ["Erik Brynjolfsson", "Andrew McAfee"], "venue": "WW Norton & Company,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Open robotics", "author": ["Ryan Calo"], "venue": "Maryland Law Review", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Risks of semi-supervised learning", "author": ["Fabio Cozman", "Ira Cohen"], "venue": "Semi-Supervised Learning", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Parametric Bounded L\u00f6b\u2019s Theorem and Robust Cooperation of Bounded Agents", "author": ["Andrew Critch"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Active reward learning", "author": ["Christian Daniel"], "venue": "Proceedings of Robotics Science & Systems", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Ethical guidelines for a superintelligence.", "author": ["Ernest Davis"], "venue": "Artif. Intell", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Maximum likelihood estimation of observer error-rates using the EM algorithm", "author": ["Alexander Philip Dawid", "Allan M Skene"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1979}, {"title": "Feudal reinforcement learning", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1993}, {"title": "Multi-objective optimization", "author": ["Kalyanmoy Deb"], "venue": "Search methodologies. Springer,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Learning what to value", "author": ["Daniel Dewey"], "venue": "Artificial General Intelligence. Springer,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Reinforcement learning and the reward engineering principle", "author": ["Daniel Dewey"], "venue": "AAAI Spring Symposium Series", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Unsupervised supervised learning i: Estimating classification and regression errors without labels", "author": ["Pinar Donmez", "Guy Lebanon", "Krishnakumar Balasubramanian"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Learning from labeled features using generalized expectation criteria", "author": ["Gregory Druck", "Gideon Mann", "Andrew McCallum"], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Fairness through awareness", "author": ["Cynthia Dwork"], "venue": "Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. ACM", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Computers and the theory of statistics: thinking the unthinkable", "author": ["Bradley Efron"], "venue": "SIAM review", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1979}, {"title": "Learning the preferences of ignorant, inconsistent agents", "author": ["Owain Evans", "Andreas Stuhlm\u00fcller", "Noah D Goodman"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Avoiding wireheading with value reinforcement learning", "author": ["Tom Everitt", "Marcus Hutter"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Self-Modification of Policy and Utility Function in Rational Agents", "author": ["Tom Everitt"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization", "author": ["Chelsea Finn", "Sergey Levine", "Pieter Abbeel"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "The future of employment: how susceptible are jobs to computerisation", "author": ["Carl Benedikt Frey", "Michael A Osborne"], "venue": "Retrieved September", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Learning with drift detection", "author": ["Joao Gama"], "venue": "Advances in artificial intelligence\u2013SBIA", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2004}, {"title": "A Comprehensive Survey on Safe Reinforcement Learning", "author": ["Javier Gar\u0107\u0131a", "Fernando Fern\u00e1ndez"], "venue": "Journal of Machine Learning Research", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "Asymptotic Convergence in Online Learning with Unbounded Delays", "author": ["Scott Garrabrant", "Nate Soares", "Jessica Taylor"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}, {"title": "Uniform Coherence", "author": ["Scott Garrabrant"], "venue": "arXiv preprint arXiv:1604.05288", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2016}, {"title": "Trusted Machine Learning for Probabilistic Models", "author": ["Shalini Ghosh"], "venue": "Reliable Machine Learning in the Wild at ICML", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Amplify scientific discovery with artificial intelligence", "author": ["Yolanda Gil"], "venue": "Science", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2014}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang"], "venue": "Project Report, Stanford", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2009}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2014}, {"title": "Problems of monetary management: the UK experience", "author": ["Charles AE Goodhart"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1984}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2014}, {"title": "Distantly Supervised Information Extraction Using Bootstrapped Patterns", "author": ["Sonal Gupta"], "venue": "PhD thesis. Stanford University,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2015}, {"title": "Cooperative Inverse Reinforcement Learning", "author": ["Dylan Hadfield-Menell"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2016}, {"title": "The Off-Switch", "author": ["Dylan Hadfield-Menell"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2016}, {"title": "Large sample properties of generalized method of moments estimators", "author": ["Lars Peter Hansen"], "venue": "Journal of the Econometric Society", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 1982}, {"title": "Nobel Lecture: Uncertainty Outside and Inside Economic Models", "author": ["Lars Peter Hansen"], "venue": "Journal of Political Economy", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2014}, {"title": "Tracking the best linear predictor", "author": ["Mark Herbster", "Manfred K Warmuth"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2001}, {"title": "Model-based utility functions", "author": ["Bill Hibbard"], "venue": "Journal of Artificial General Intelligence", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2012}, {"title": "Kernel methods in machine learning", "author": ["Thomas Hofmann", "Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "The annals of statistics", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2008}, {"title": "Robust dynamic programming", "author": ["Garud N Iyengar"], "venue": "Mathematics of Operations Research", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2005}, {"title": "Estimating the accuracies of multiple classifiers without labeled data", "author": ["Ariel Jaffe", "Boaz Nadler", "Yuval Kluger"], "venue": null, "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2014}, {"title": "A formally verified hybrid system for the next-generation airborne collision avoidance system", "author": ["Jean-Baptiste Jeannin"], "venue": "Tools and Algorithms for the Construction and Analysis of Systems. Springer,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2015}, {"title": "Differential privacy and machine learning: A survey and review", "author": ["Zhanglong Ji", "Zachary C Lipton", "Charles Elkan"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2014}, {"title": "Learning Representations for Counterfactual Inference", "author": ["Fredrik D Johansson", "Uri Shalit", "David Sontag"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2016}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Anthony R Cassandra"], "venue": "Artificial intelligence", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 1998}, {"title": "Neural GPUs learn algorithms", "author": ["Lukasz Kaiser", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.08228", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2015}, {"title": "Change-Point Detection in Time-Series Data by Direct Density-Ratio Estimation.", "author": ["Yoshinobu Kawahara", "Masashi Sugiyama"], "venue": "In: SDM. Vol", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2009}, {"title": "Unanimous Prediction for 100Learning Semantic Parsers", "author": ["F. Khani", "M. Rinard", "P. Liang"], "venue": "Association for Computational Linguistics (ACL)", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2012}, {"title": "Calibrated Structured Prediction", "author": ["Volodymyr Kuleshov", "Percy S Liang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2015}, {"title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "author": ["Tejas D Kulkarni"], "venue": null, "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2016}, {"title": "Discussion of \u2019Superintelligence: Paths, Dangers, Strategies", "author": ["Neil Lawrence"], "venue": null, "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2016}, {"title": "Towards fully autonomous driving: Systems and algorithms", "author": ["Jesse Levinson"], "venue": "Intelligent Vehicles Symposium (IV),", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2011}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Lihong Li"], "venue": "Machine learning", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2011}, {"title": "Towards making unlabeled data never hurt", "author": ["Yu-Feng Li", "Zhi-Hua Zhou"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 37.1", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2015}, {"title": "On the Elusiveness of a Specification for AI", "author": ["Percy Liang"], "venue": "NIPS 2015, Symposium: Algorithms Among Us", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2015}, {"title": "Analyzing the Errors of Unsupervised Learning.", "author": ["Percy Liang", "Dan Klein"], "venue": "In: ACL", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2008}, {"title": "Change-point detection in time-series data by relative density-ratio estimation", "author": ["Song Liu"], "venue": "Neural Networks", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2013}, {"title": "Formal verification of distributed aircraft controllers", "author": ["Sarah M Loos", "David Renshaw", "Andr\u00e9 Platzer"], "venue": "Proceedings of the 16th international conference on Hybrid systems: computation and control", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2013}, {"title": "Controllers for reachability specifications for hybrid systems", "author": ["John Lygeros", "Claire Tomlin", "Shankar Sastry"], "venue": null, "citeRegEx": "91", "shortCiteRegEx": "91", "year": 1999}, {"title": "Generalized expectation criteria for semi-supervised learning with weakly labeled data", "author": ["Gideon S Mann", "Andrew McCallum"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2010}, {"title": "Some philosophical problems from the standpoint of artificial intelligence", "author": ["John McCarthy", "Patrick J Hayes"], "venue": "Readings in artificial intelligence", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1969}, {"title": "The Security of Latent Dirichlet Allocation.", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": "AISTATS", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2015}, {"title": "Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners.", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": "In: AAAI", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2015}, {"title": "Tagging English text with a probabilistic model", "author": ["Bernard Merialdo"], "venue": "Computational linguistics", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1994}, {"title": "A time-dependent Hamilton- Jacobi formulation of reachable sets for continuous dynamic games", "author": ["Ian M Mitchell", "Alexandre M Bayen", "Claire J Tomlin"], "venue": "Automatic Control, IEEE Transactions on 50.7", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2005}, {"title": "Towards formal verification of freeway traffic control", "author": ["Stefan Mitsch", "Sarah M Loos", "Andr\u00e9 Platzer"], "venue": "Cyber-Physical Systems (ICCPS),", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["VolodymyrMnih"], "venue": "Nature", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2015}, {"title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning", "author": ["Shakir Mohamed", "Danilo Jimenez Rezende"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2015}, {"title": "Safe exploration in markov decision processes", "author": ["Teodor Mihai Moldovan", "Pieter Abbeel"], "venue": null, "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2012}, {"title": "Inceptionism: Going deeper into neural networks", "author": ["Alexander Mordvintsev", "Christopher Olah", "Mike Tyka"], "venue": "Google Research Blog. Retrieved June", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2015}, {"title": "Sur les applications de la th\u00e9orie des probabilit\u00e9s aux experiences agricoles: Essai des principes", "author": ["Jersey Neyman"], "venue": "Roczniki Nauk Rolniczych", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 1923}, {"title": "Algorithms for inverse reinforcement learning.", "author": ["Andrew Y Ng", "Stuart J Russell"], "venue": "Icml", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2000}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2015}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "author": ["Anh Nguyen"], "venue": null, "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2016}, {"title": "Learning to classify text from labeled and unlabeled documents", "author": ["Kamal Nigam"], "venue": "In: AAAI/IAAI", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 1998}, {"title": "Robust control of Markov decision processes with uncertain transition matrices", "author": ["Arnab Nilim", "Laurent El Ghaoui"], "venue": "In: Operations Research", "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2005}, {"title": "Visualizing Representations: Deep Learning and Human Beings", "author": ["Christopher Olah"], "venue": "url:", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2015}, {"title": "Safely Interruptible Agents", "author": ["Laurent Orseau", "Stuart Armstrong"], "venue": null, "citeRegEx": "111", "shortCiteRegEx": "111", "year": 2016}, {"title": "Deep Exploration via Bootstrapped DQN", "author": ["Ian Osband"], "venue": "arXiv preprint arXiv:1602.04621", "citeRegEx": "112", "shortCiteRegEx": "112", "year": 2016}, {"title": "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples", "author": ["Nicolas Papernot"], "venue": "arXiv preprint arXiv:1602.02697", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2016}, {"title": "The design for the Wall Street Journal-based CSR corpus", "author": ["Douglas B Paul", "Janet M Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 1992}, {"title": "Causal inference in statistics: An overview", "author": ["Judea Pearl"], "venue": "In: Statistics Surveys", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 2009}, {"title": "Safe exploration techniques for reinforcement learning\u2013an overview", "author": ["Martin Pecka", "Tomas Svoboda"], "venue": "Modelling and Simulation for Autonomous Systems. Springer,", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2014}, {"title": "Discrimination-aware data mining", "author": ["Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 2008}, {"title": "Causal discovery with continuous additive noise models", "author": ["Jonas Peters"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 2014}, {"title": "Estimating accuracy from unlabeled data", "author": ["Emmanouil Antonios Platanios"], "venue": "MA thesis. Carnegie Mellon University,", "citeRegEx": "119", "shortCiteRegEx": "119", "year": 2015}, {"title": "Estimating accuracy from unlabeled data", "author": ["Emmanouil Antonios Platanios", "Avrim Blum", "Tom Mitchell"], "venue": null, "citeRegEx": "120", "shortCiteRegEx": "120", "year": 2014}, {"title": "Networks and economic life", "author": ["Walter W Powell", "Laurel Smith-Doerr"], "venue": "The handbook of economic sociology", "citeRegEx": "121", "shortCiteRegEx": "121", "year": 1994}, {"title": "Dataset shift in machine learning, ser", "author": ["Joaquin Quinonero-Candela"], "venue": "Neural information processing series", "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2009}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["Rajat Raina"], "venue": "Proceedings of the 24th international conference on Machine learning", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 2007}, {"title": "Massively multitask networks for drug discovery", "author": ["Bharath Ramsundar"], "venue": "arXiv preprint arXiv:1502.02072", "citeRegEx": "124", "shortCiteRegEx": "124", "year": 2015}, {"title": "Delusion, survival, and intelligent agents", "author": ["Mark Ring", "Laurent Orseau"], "venue": "Artificial General Intelligence. Springer,", "citeRegEx": "125", "shortCiteRegEx": "125", "year": 2011}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "J Andrew Bagnell"], "venue": null, "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2010}, {"title": "Estimating causal effects of treatments in randomized and nonrandomized studies.", "author": ["Donald B Rubin"], "venue": "Journal of educational Psychology", "citeRegEx": "127", "shortCiteRegEx": "127", "year": 1974}, {"title": "Research priorities for robust and beneficial artificial intelligence", "author": ["Stuart Russell"], "venue": "Future of Life Institute", "citeRegEx": "128", "shortCiteRegEx": "128", "year": 2015}, {"title": "Empowerment\u2013an introduction", "author": ["Christoph Salge", "Cornelius Glackin", "Daniel Polani"], "venue": "Guided Self-Organization: Inception. Springer,", "citeRegEx": "129", "shortCiteRegEx": "129", "year": 2014}, {"title": "The estimation of relationships with autocorrelated residuals by the use of instrumental variables", "author": ["J Denis Sargan"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological)", "citeRegEx": "130", "shortCiteRegEx": "130", "year": 1959}, {"title": "The estimation of economic relationships using instrumental variables", "author": ["John D Sargan"], "venue": "In: Econometrica: Journal of the Econometric Society", "citeRegEx": "131", "shortCiteRegEx": "131", "year": 1958}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman"], "venue": null, "citeRegEx": "132", "shortCiteRegEx": "132", "year": 2015}, {"title": "Machine Learning: The High-Interest Credit Card of Technical Debt", "author": ["D Sculley"], "venue": null, "citeRegEx": "133", "shortCiteRegEx": "133", "year": 2014}, {"title": "A tutorial on conformal prediction", "author": ["Glenn Shafer", "Vladimir Vovk"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "134", "shortCiteRegEx": "134", "year": 2008}, {"title": "Bounding and Minimizing Counterfactual Error", "author": ["Uri Shalit", "Fredrik Johansson", "David Sontag"], "venue": "arXiv preprint arXiv:1606.03976", "citeRegEx": "135", "shortCiteRegEx": "135", "year": 2016}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Hidetoshi Shimodaira"], "venue": "Journal of statistical planning and inference", "citeRegEx": "136", "shortCiteRegEx": "136", "year": 2000}, {"title": "Incremental knowledge base construction using deepdive", "author": ["Jaeho Shin"], "venue": "Proceedings of the VLDB Endowment", "citeRegEx": "137", "shortCiteRegEx": "137", "year": 2015}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["David Silver"], "venue": "Nature", "citeRegEx": "138", "shortCiteRegEx": "138", "year": 2016}, {"title": "Toward idealized decision theory", "author": ["Nate Soares", "Benja Fallenstein"], "venue": "arXiv preprint arXiv:1507.01986", "citeRegEx": "140", "shortCiteRegEx": "140", "year": 2015}, {"title": "A formal theory of inductive inference. Part I", "author": ["Ray J Solomonoff"], "venue": "Information and control", "citeRegEx": "141", "shortCiteRegEx": "141", "year": 1964}, {"title": "A formal theory of inductive inference. Part II", "author": ["Ray J Solomonoff"], "venue": "Information and control", "citeRegEx": "142", "shortCiteRegEx": "142", "year": 1964}, {"title": "EL Lehmann, JP Romano: Testing statistical hypotheses", "author": ["J Steinebach"], "venue": "Metrika", "citeRegEx": "143", "shortCiteRegEx": "143", "year": 2006}, {"title": "Long-Term and Short-Term Challenges to Ensuring the Safety of AI Systems. [Online; accessed 13-June-2016", "author": ["Jacob Steinhardt"], "venue": null, "citeRegEx": "144", "shortCiteRegEx": "144", "year": 2015}, {"title": "Unsupervised Risk Estimation with only Structural Assumptions", "author": ["Jacob Steinhardt", "Percy Liang"], "venue": null, "citeRegEx": "145", "shortCiteRegEx": "145", "year": 2016}, {"title": "Finite-time regional verification of stochastic non-linear systems", "author": ["Jacob Steinhardt", "Russ Tedrake"], "venue": "The International Journal of Robotics Research", "citeRegEx": "146", "shortCiteRegEx": "146", "year": 2012}, {"title": "Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction", "author": ["Jacob Steinhardt", "Gregory Valiant", "Moses Charikar"], "venue": "In: arxiv prepring arXiv:1606.05374", "citeRegEx": "147", "shortCiteRegEx": "147", "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press,", "citeRegEx": "148", "shortCiteRegEx": "148", "year": 1998}, {"title": "Counterfactual risk minimization: Learning from logged bandit feedback", "author": ["Adith Swaminathan", "Thorsten Joachims"], "venue": null, "citeRegEx": "149", "shortCiteRegEx": "149", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy"], "venue": "arXiv preprint arXiv:1312.6199", "citeRegEx": "150", "shortCiteRegEx": "150", "year": 2013}, {"title": "Policy gradients beyond expectations: Conditional value-at-risk", "author": ["Aviv Tamar", "Yonatan Glassner", "Shie Mannor"], "venue": null, "citeRegEx": "151", "shortCiteRegEx": "151", "year": 2014}, {"title": "Quantilizers: A Safer Alternative to Maximizers for Limited Optimization", "author": ["Jessica Taylor"], "venue": "In: forthcoming). Submitted to AAAI", "citeRegEx": "152", "shortCiteRegEx": "152", "year": 2016}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research", "citeRegEx": "153", "shortCiteRegEx": "153", "year": 2009}, {"title": "High-Confidence Off-Policy Evaluation.", "author": ["Philip S Thomas", "Georgios Theocharous", "Mohammad Ghavamzadeh"], "venue": "In: AAAI", "citeRegEx": "154", "shortCiteRegEx": "154", "year": 2015}, {"title": "Artificial evolution in the physical world", "author": ["Adrian Thompson"], "venue": null, "citeRegEx": "155", "shortCiteRegEx": "155", "year": 1997}, {"title": "Unbiased look at dataset bias", "author": ["Antonio Torralba", "Alexei A Efros"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "156", "shortCiteRegEx": "156", "year": 2011}, {"title": "Estimation and Inference of Heterogeneous Treatment Effects using Random Forests", "author": ["Stefan Wager", "Susan Athey"], "venue": null, "citeRegEx": "157", "shortCiteRegEx": "157", "year": 2015}, {"title": "On-the-job learning with bayesian decision theory", "author": ["Keenon Werling"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "158", "shortCiteRegEx": "158", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["JasonWeston"], "venue": null, "citeRegEx": "159", "shortCiteRegEx": "159", "year": 2015}, {"title": "Robust Markov decision processes", "author": ["Wolfram Wiesemann", "Daniel Kuhn", "Ber\u00e7 Rustem"], "venue": "In: Mathematics of Operations Research", "citeRegEx": "160", "shortCiteRegEx": "160", "year": 2013}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski"], "venue": "arXiv preprint arXiv:1506.06579", "citeRegEx": "161", "shortCiteRegEx": "161", "year": 2015}, {"title": "Artificial intelligence as a positive and negative factor in global risk", "author": ["Eliezer Yudkowsky"], "venue": "Global catastrophic risks", "citeRegEx": "162", "shortCiteRegEx": "162", "year": 2008}, {"title": "Learning Fair Classifiers", "author": ["Muhammad Bilal Zafar"], "venue": null, "citeRegEx": "163", "shortCiteRegEx": "163", "year": 2015}, {"title": "Learning Fair Representations.", "author": ["Richard S Zemel"], "venue": null, "citeRegEx": "164", "shortCiteRegEx": "164", "year": 2013}, {"title": "Spectral methods meet EM: A provably optimal algorithm for crowdsourcing", "author": ["Yuchen Zhang"], "venue": "Advances in neural information processing systems", "citeRegEx": "165", "shortCiteRegEx": "165", "year": 2014}], "referenceMentions": [{"referenceID": 76, "context": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision [80], video game playing [100], autonomous vehicles [84], and Go [138].", "startOffset": 174, "endOffset": 178}, {"referenceID": 95, "context": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision [80], video game playing [100], autonomous vehicles [84], and Go [138].", "startOffset": 199, "endOffset": 204}, {"referenceID": 80, "context": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision [80], video game playing [100], autonomous vehicles [84], and Go [138].", "startOffset": 226, "endOffset": 230}, {"referenceID": 133, "context": "The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision [80], video game playing [100], autonomous vehicles [84], and Go [138].", "startOffset": 239, "endOffset": 244}, {"referenceID": 119, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 96, "endOffset": 101}, {"referenceID": 53, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 111, "endOffset": 115}, {"referenceID": 80, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 136, "endOffset": 140}, {"referenceID": 70, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 180, "endOffset": 184}, {"referenceID": 108, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 195, "endOffset": 200}, {"referenceID": 2, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 211, "endOffset": 214}, {"referenceID": 28, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 225, "endOffset": 229}, {"referenceID": 23, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 355, "endOffset": 364}, {"referenceID": 156, "context": "These advances have brought excitement about the positive potential for AI to transformmedicine [124], science [57], and transportation [84], along with concerns about the privacy [74], security [113], fairness [3], economic [31], and military [16] implications of autonomous systems, as well as concerns about the longer-term implications of powerful AI [26, 162].", "startOffset": 355, "endOffset": 364}, {"referenceID": 23, "context": "To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents [26].", "startOffset": 146, "endOffset": 150}, {"referenceID": 33, "context": "However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics [37, 83].", "startOffset": 220, "endOffset": 228}, {"referenceID": 79, "context": "However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics [37, 83].", "startOffset": 220, "endOffset": 228}, {"referenceID": 138, "context": "This issue arises in almost any engineering discipline, but may be particularly important to address when building AI systems [144].", "startOffset": 126, "endOffset": 131}, {"referenceID": 89, "context": "Intuitively, this seems related to the frame problem, an obstacle in efficient specification for knowledge representation raised by [93].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "A version of this has problem has been discussed informally by [13] under the heading of \u201clow impact agents.", "startOffset": 63, "endOffset": 67}, {"referenceID": 87, "context": "low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis [91, 98] or robust policy improvement [71, 109].", "startOffset": 134, "endOffset": 142}, {"referenceID": 93, "context": "low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis [91, 98] or robust policy improvement [71, 109].", "startOffset": 134, "endOffset": 142}, {"referenceID": 67, "context": "low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis [91, 98] or robust policy improvement [71, 109].", "startOffset": 172, "endOffset": 181}, {"referenceID": 104, "context": "low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhat reminiscent of reachability analysis [91, 98] or robust policy improvement [71, 109].", "startOffset": 172, "endOffset": 181}, {"referenceID": 147, "context": "This would be similar to model-based RL approaches that attempt to transfer a learned dynamics model but not the value-function [153], the novelty being the isolation of side effects rather than state dynamics as the transferrable component.", "startOffset": 128, "endOffset": 133}, {"referenceID": 124, "context": "Perhaps the bestknown such measure is empowerment [129], the maximum possible mutual information between the agent\u2019s potential future actions and its potential future state (or equivalently, the Shannon capacity of the channel between the agent\u2019s actions and the environment).", "startOffset": 50, "endOffset": 55}, {"referenceID": 96, "context": "This can cause the agent to exhibit interesting behavior in the absence of any external rewards, such as avoiding walls or picking up keys [101].", "startOffset": 139, "endOffset": 144}, {"referenceID": 60, "context": "One approach to this is Cooperative Inverse Reinforcement Learning [64], where an agent and a human work together to achieve the human\u2019s goals.", "startOffset": 67, "endOffset": 71}, {"referenceID": 61, "context": "This concept can be applied to situations where we want to make sure a human is not blocked by an agent from shutting the agent down if it exhibits undesired behavior [65] (this \u201cshutdown\u201d issue is an interesting problem in its own right, and is also studied in [111]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 106, "context": "This concept can be applied to situations where we want to make sure a human is not blocked by an agent from shutting the agent down if it exhibits undesired behavior [65] (this \u201cshutdown\u201d issue is an interesting problem in its own right, and is also studied in [111]).", "startOffset": 262, "endOffset": 267}, {"referenceID": 87, "context": "For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier [91, 98, 71, 109].", "startOffset": 177, "endOffset": 194}, {"referenceID": 93, "context": "For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier [91, 98, 71, 109].", "startOffset": 177, "endOffset": 194}, {"referenceID": 67, "context": "For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier [91, 98, 71, 109].", "startOffset": 177, "endOffset": 194}, {"referenceID": 104, "context": "For this, one could potentially use a conservative but reliable baseline policy, similar to the robust policy improvement and reachability analysis approaches discussed earlier [91, 98, 71, 109].", "startOffset": 177, "endOffset": 194}, {"referenceID": 96, "context": "Some of the environments described in [101], containing lava flows, rooms, and keys, might be appropriate for this sort of experiment.", "startOffset": 38, "endOffset": 43}, {"referenceID": 149, "context": "For example, it has been shown that genetic algorithms can often output unexpected but formally correct solutions to problems [155, 22], such as a circuit tasked to keep time which instead developed into a radio that picked up the regular RF emissions of a nearby PC.", "startOffset": 126, "endOffset": 135}, {"referenceID": 19, "context": "For example, it has been shown that genetic algorithms can often output unexpected but formally correct solutions to problems [155, 22], such as a circuit tasked to keep time which instead developed into a radio that picked up the regular RF emissions of a nearby PC.", "startOffset": 126, "endOffset": 135}, {"referenceID": 65, "context": "Some versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [69, 42, 48] or demonstrate reward hacking in a model environment [125].", "startOffset": 180, "endOffset": 192}, {"referenceID": 38, "context": "Some versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [69, 42, 48] or demonstrate reward hacking in a model environment [125].", "startOffset": 180, "endOffset": 192}, {"referenceID": 44, "context": "Some versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [69, 42, 48] or demonstrate reward hacking in a model environment [125].", "startOffset": 180, "endOffset": 192}, {"referenceID": 120, "context": "Some versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [69, 42, 48] or demonstrate reward hacking in a model environment [125].", "startOffset": 246, "endOffset": 251}, {"referenceID": 25, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 135, "endOffset": 144}, {"referenceID": 128, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 135, "endOffset": 144}, {"referenceID": 25, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 183, "endOffset": 192}, {"referenceID": 143, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 183, "endOffset": 192}, {"referenceID": 3, "context": "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement) [28, 133], based on counterfactual learning and [28, 149] and contextual bandits [4].", "startOffset": 216, "endOffset": 219}, {"referenceID": 72, "context": "While it can be shown that there always exists a reward function in terms of actions and observations that is equivalent to optimizing the true objective function (this involves reducing the POMPD to a belief state MDP, see [76]), often this reward function involves complicated long-term dependencies and is prohibitively hard to use in practice.", "startOffset": 224, "endOffset": 228}, {"referenceID": 144, "context": "These concepts concepts will possibly need to be learned by models like neural networks, which can be vulnerable to adversarial counterexamples [150, 60].", "startOffset": 144, "endOffset": 153}, {"referenceID": 56, "context": "These concepts concepts will possibly need to be learned by models like neural networks, which can be vulnerable to adversarial counterexamples [150, 60].", "startOffset": 144, "endOffset": 153}, {"referenceID": 57, "context": "In the economics literature this is known as Goodhart\u2019s law [61]: \u201cwhen a metric is used as a target, it ceases to be a good metric.", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "For instance, an ad placement algorithm that displays more popular ads in larger font will tend to further accentuate the popularity of those ads (since they will be shown more and more prominently) [28], leading to", "startOffset": 199, "endOffset": 203}, {"referenceID": 44, "context": "This particular failure mode is often called \u201cwireheading\u201d [48, 125, 41, 65].", "startOffset": 59, "endOffset": 76}, {"referenceID": 120, "context": "This particular failure mode is often called \u201cwireheading\u201d [48, 125, 41, 65].", "startOffset": 59, "endOffset": 76}, {"referenceID": 37, "context": "This particular failure mode is often called \u201cwireheading\u201d [48, 125, 41, 65].", "startOffset": 59, "endOffset": 76}, {"referenceID": 61, "context": "This particular failure mode is often called \u201cwireheading\u201d [48, 125, 41, 65].", "startOffset": 59, "endOffset": 76}, {"referenceID": 55, "context": "For instance, the reward agent could try to find scenarios that the ML system claimed were high reward but that a human labels as low reward; this is reminiscent of generative adversarial networks [59].", "startOffset": 197, "endOffset": 201}, {"referenceID": 45, "context": ") Similar ideas are explored in [49, 69].", "startOffset": 32, "endOffset": 40}, {"referenceID": 65, "context": ") Similar ideas are explored in [49, 69].", "startOffset": 32, "endOffset": 40}, {"referenceID": 4, "context": "\u2022 Adversarial Blinding: Adversarial techniques can be used to blind a model to certain variables [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 14, "context": "Computer security approaches that attempt to isolate the agent from its reward signal through a sandbox could also be useful [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 56, "context": "\u2022 Counterexample Resistance: If we are worried, as in the case of abstract rewards, that learned components of our systems will be vulnerable to adversarial counterexamples, we can look to existing research in how to resist them, such as adversarial training [60].", "startOffset": 259, "endOffset": 263}, {"referenceID": 22, "context": "Architectural decisions and weight uncertainty [25] may also help.", "startOffset": 47, "endOffset": 51}, {"referenceID": 36, "context": "\u2022 Multiple Rewards: A combination of multiple rewards [40] may be more difficult to hack and more robust.", "startOffset": 54, "endOffset": 58}, {"referenceID": 100, "context": "This could involve either learning a reward function from samples of state-reward pairs, or from trajectories, as in inverse reinforcement learning [105, 50].", "startOffset": 148, "endOffset": 157}, {"referenceID": 46, "context": "This could involve either learning a reward function from samples of state-reward pairs, or from trajectories, as in inverse reinforcement learning [105, 50].", "startOffset": 148, "endOffset": 157}, {"referenceID": 120, "context": "Potential Experiments: A possible promising avenue of approach would be more realistic versions of the \u201cdelusion box\u201d environment described by [125], in which standard RL agents distort their own perception to appear to receive high reward, rather than optimizing the objective in the external world that the reward signal was intended to encourage.", "startOffset": 143, "endOffset": 148}, {"referenceID": 32, "context": "[36] studies a version of this with direct human feedback as the reward.", "startOffset": 0, "endOffset": 4}, {"referenceID": 127, "context": "[132]), suggesting that this approach may be eminently feasible.", "startOffset": 0, "endOffset": 5}, {"referenceID": 88, "context": "For instance, generalized expectation criteria [92, 44] ask the user to provide population-level statistics (e.", "startOffset": 47, "endOffset": 55}, {"referenceID": 40, "context": "For instance, generalized expectation criteria [92, 44] ask the user to provide population-level statistics (e.", "startOffset": 47, "endOffset": 55}, {"referenceID": 132, "context": "telling the system that on average each sentence contains at least one noun\u2019); the DeepDive system [137] asks users to supply rules that each generate many weak labels; and [63] extrapolates more general patterns from an initial set of low-recall labeling rules.", "startOffset": 99, "endOffset": 104}, {"referenceID": 59, "context": "telling the system that on average each sentence contains at least one noun\u2019); the DeepDive system [137] asks users to supply rules that each generate many weak labels; and [63] extrapolates more general patterns from an initial set of low-recall labeling rules.", "startOffset": 173, "endOffset": 177}, {"referenceID": 54, "context": "[58, 97] as well as several of the references above).", "startOffset": 0, "endOffset": 8}, {"referenceID": 35, "context": "Hierarchical reinforcement learning [39] offers another approach to scalable oversight.", "startOffset": 36, "endOffset": 40}, {"referenceID": 78, "context": "Hierarchical RL seems a particularly promising approach to oversight, especially given the potential promise of combining ideas from hierarchical RL with neural network function approximators [82].", "startOffset": 192, "endOffset": 196}, {"referenceID": 142, "context": "Common exploration policies such as epsilongreedy [148] or R-max [30] explore by choosing an action at random or viewing unexplored actions optimistically, and thus make no attempt to avoid these dangerous situations.", "startOffset": 50, "endOffset": 55}, {"referenceID": 27, "context": "Common exploration policies such as epsilongreedy [148] or R-max [30] explore by choosing an action at random or viewing unexplored actions optimistically, and thus make no attempt to avoid these dangerous situations.", "startOffset": 65, "endOffset": 69}, {"referenceID": 107, "context": "More sophisticated exploration strategies that adopt a coherent exploration policy over extended temporal scales [112] could actually have even greater potential for harm, since a coherently chosen bad policy may be more insidious than mere random actions.", "startOffset": 113, "endOffset": 118}, {"referenceID": 49, "context": "[53, 116] provide thorough reviews of this literature, so we don\u2019t review it extensively here, but simply describe some general routes that this research has taken, as well as suggesting some directions that might have increasing relevance as RL systems expand in scope and capability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 111, "context": "[53, 116] provide thorough reviews of this literature, so we don\u2019t review it extensively here, but simply describe some general routes that this research has taken, as well as suggesting some directions that might have increasing relevance as RL systems expand in scope and capability.", "startOffset": 0, "endOffset": 9}, {"referenceID": 49, "context": "\u2022 Risk-Sensitive Performance Criteria: A body of existing literature considers changing the optimization criteria from expected total reward to other objectives that are better at preventing rare, catastrophic events; see [53] for a thorough and up-to-date review of this literature.", "startOffset": 222, "endOffset": 226}, {"referenceID": 145, "context": "[151], which proposes a modification to policy gradient algorithms to optimize a risk-sensitive criterion.", "startOffset": 0, "endOffset": 5}, {"referenceID": 107, "context": "There is also recent work studying how to estimate uncertainty in value function that are represented by deep neural networks [112]; these ideas could be incorporated into risk-sensitive RL algorithms.", "startOffset": 126, "endOffset": 131}, {"referenceID": 148, "context": "Another line of work relevant to risk sensitivity uses off-policy estimation to perform a policy update that is good with high probability [154].", "startOffset": 139, "endOffset": 144}, {"referenceID": 121, "context": "We may be able to avoid the need for exploration altogether if we instead use inverse RL or apprenticeship learning, where the learning algorithm is provided with expert trajectories of near-optimal behavior [126, 2].", "startOffset": 208, "endOffset": 216}, {"referenceID": 1, "context": "We may be able to avoid the need for exploration altogether if we instead use inverse RL or apprenticeship learning, where the learning algorithm is provided with expert trajectories of near-optimal behavior [126, 2].", "startOffset": 208, "endOffset": 216}, {"referenceID": 46, "context": "Recent progress in inverse reinforcement learning using deep neural networks to learn the cost function or policy [50] suggests that it might also be possible to reduce the need for exploration in advanced RL systems by training on a small set of demonstrations.", "startOffset": 114, "endOffset": 118}, {"referenceID": 97, "context": "Safety can be defined as remaining within an ergodic region of the state space such that actions are reversible [102], or as limiting the probability of huge negative reward to some small value [154].", "startOffset": 112, "endOffset": 117}, {"referenceID": 148, "context": "Safety can be defined as remaining within an ergodic region of the state space such that actions are reversible [102], or as limiting the probability of huge negative reward to some small value [154].", "startOffset": 194, "endOffset": 199}, {"referenceID": 17, "context": "This idea seems related to H-infinity control [20] and regional verification [146].", "startOffset": 46, "endOffset": 50}, {"referenceID": 140, "context": "This idea seems related to H-infinity control [20] and regional verification [146].", "startOffset": 77, "endOffset": 82}, {"referenceID": 97, "context": "To some extent this feature already exists in autonomous helicopter competitions and Mars rover simulations [102], but there is always the risk of catastrophes being idiosyncratic, such that trained agents can overfit to them.", "startOffset": 108, "endOffset": 113}, {"referenceID": 153, "context": "Such a suite of environments might serve a benchmarking role similar to that of the bAbI tasks [159], with the eventual goal being to develop a single architecture that can learn to avoid catastrophes in all environments in the suite.", "startOffset": 95, "endOffset": 100}, {"referenceID": 64, "context": "There are many other ways to formalize this problem (for instance, in an online learning setting with concept drift [68, 52]) but we will focus on the above for simplicity.", "startOffset": 116, "endOffset": 124}, {"referenceID": 48, "context": "There are many other ways to formalize this problem (for instance, in an online learning setting with concept drift [68, 52]) but we will focus on the above for simplicity.", "startOffset": 116, "endOffset": 124}, {"referenceID": 18, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 125, "endOffset": 137}, {"referenceID": 74, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 125, "endOffset": 137}, {"referenceID": 85, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 125, "endOffset": 137}, {"referenceID": 137, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 158, "endOffset": 163}, {"referenceID": 131, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 183, "endOffset": 202}, {"referenceID": 117, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 183, "endOffset": 202}, {"referenceID": 118, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 183, "endOffset": 202}, {"referenceID": 21, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 183, "endOffset": 202}, {"referenceID": 129, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 81, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 15, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 115, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 114, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 68, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 139, "context": "There are a variety of areas that are potentially relevant to this problem, including change detection and anomaly detection [21, 78, 89], hypothesis testing [143], transfer learning [136, 122, 123, 24], and several others [134, 85, 18, 120, 119, 72, 145].", "startOffset": 223, "endOffset": 255}, {"referenceID": 131, "context": "In this case, assuming that we can model p0(x) and p \u2217(x) well, we can perform importance weighting by re-weighting each training example (x, y) by p\u2217(x)/p0(x) [136, 122].", "startOffset": 160, "endOffset": 170}, {"referenceID": 117, "context": "In this case, assuming that we can model p0(x) and p \u2217(x) well, we can perform importance weighting by re-weighting each training example (x, y) by p\u2217(x)/p0(x) [136, 122].", "startOffset": 160, "endOffset": 170}, {"referenceID": 21, "context": "In this case, one need only heed finite-sample variance in the estimated model [24, 85].", "startOffset": 79, "endOffset": 87}, {"referenceID": 81, "context": "In this case, one need only heed finite-sample variance in the estimated model [24, 85].", "startOffset": 79, "endOffset": 87}, {"referenceID": 66, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 132, "endOffset": 136}, {"referenceID": 135, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 154, "endOffset": 164}, {"referenceID": 136, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 154, "endOffset": 164}, {"referenceID": 58, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 205, "endOffset": 213}, {"referenceID": 73, "context": "However, this could potentially be overcome by employing highly expressive model families such as reproducing kernel Hilbert spaces [70], Turing machines [141, 142], or sufficiently expressive neural nets [62, 77].", "startOffset": 205, "endOffset": 213}, {"referenceID": 107, "context": "In the latter case, there has been interesting recent work on using bootstrapping to estimate finite-sample variation in the learned parameters of a neural network [112]; it seems worthwhile to better understand whether this approach can be used to effectively estimate out-of-sample performance in practice, as well as how local minima, lack of curvature, and other peculiarities relative to the typical setting of the bootstrap [46] affect the validity of this approach.", "startOffset": 164, "endOffset": 169}, {"referenceID": 42, "context": "In the latter case, there has been interesting recent work on using bootstrapping to estimate finite-sample variation in the learned parameters of a neural network [112]; it seems worthwhile to better understand whether this approach can be used to effectively estimate out-of-sample performance in practice, as well as how local minima, lack of curvature, and other peculiarities relative to the typical setting of the bootstrap [46] affect the validity of this approach.", "startOffset": 430, "endOffset": 434}, {"referenceID": 92, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 103, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 30, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 84, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 82, "context": "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification \u2014 for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is misspecified [96, 108, 34, 88, 86].", "startOffset": 353, "endOffset": 374}, {"referenceID": 62, "context": "This insight can be substantially generalized, and is one of the primary motivations for the generalized method of moments in econometrics [66, 121, 67].", "startOffset": 139, "endOffset": 152}, {"referenceID": 116, "context": "This insight can be substantially generalized, and is one of the primary motivations for the generalized method of moments in econometrics [66, 121, 67].", "startOffset": 139, "endOffset": 152}, {"referenceID": 63, "context": "This insight can be substantially generalized, and is one of the primary motivations for the generalized method of moments in econometrics [66, 121, 67].", "startOffset": 139, "endOffset": 152}, {"referenceID": 8, "context": "The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limitedinformation maximum likelihood and instrumental variables [10, 11, 131, 130].", "startOffset": 195, "endOffset": 213}, {"referenceID": 9, "context": "The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limitedinformation maximum likelihood and instrumental variables [10, 11, 131, 130].", "startOffset": 195, "endOffset": 213}, {"referenceID": 126, "context": "The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limitedinformation maximum likelihood and instrumental variables [10, 11, 131, 130].", "startOffset": 195, "endOffset": 213}, {"referenceID": 125, "context": "The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limitedinformation maximum likelihood and instrumental variables [10, 11, 131, 130].", "startOffset": 195, "endOffset": 213}, {"referenceID": 7, "context": "Returning to machine learning, the method of moments has recently seen a great deal of success for use in the estimation of latent variable models [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 139, "context": "While the current focus is on using the method of moments to overcome non-convexity issues, it can also offer a way to perform unsupervised learning while relying only on conditional independence assumptions, rather than the strong distributional assumptions underlying maximum likelihood learning [145].", "startOffset": 298, "endOffset": 303}, {"referenceID": 39, "context": "This formalism, introduced by [43], has the advantage of potentially handling very large changes between train and test \u2014 even if the test distribution looks completely different from the training distribution and we have no hope of outputting accurate predictions, unsupervised risk estimation may still be possible, as in this case we would only need to output a large estimate for the risk.", "startOffset": 30, "endOffset": 34}, {"referenceID": 139, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 6, "endOffset": 11}, {"referenceID": 34, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 207, "endOffset": 225}, {"referenceID": 159, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 207, "endOffset": 225}, {"referenceID": 114, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 207, "endOffset": 225}, {"referenceID": 68, "context": "As in [145], one can approach unsupervised risk estimation by positing certain conditional independencies in the distribution of errors, and using this to estimate the error distribution from unlabeled data [38, 165, 119, 72].", "startOffset": 207, "endOffset": 225}, {"referenceID": 15, "context": "Instead of assuming independence, another assumption is that the errors are Gaussian conditioned on the true output y, in which case estimating the risk reduces to estimating a Gaussian mixture model [18].", "startOffset": 200, "endOffset": 204}, {"referenceID": 6, "context": "One of the authors has found this to be the case, for instance, in the context of automated speech recognition systems [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 152, "context": "For the former challenge, there has been some recent promising work on pinpointing aspects of a structure that a model is uncertain about [158, 79], as well as obtaining calibration in structured", "startOffset": 138, "endOffset": 147}, {"referenceID": 75, "context": "For the former challenge, there has been some recent promising work on pinpointing aspects of a structure that a model is uncertain about [158, 79], as well as obtaining calibration in structured", "startOffset": 138, "endOffset": 147}, {"referenceID": 77, "context": "output settings [81], but we believe there is much work yet to be done.", "startOffset": 16, "endOffset": 20}, {"referenceID": 87, "context": "For the latter challenge, there is also relevant work based on reachability analysis [91, 98] and robust policy improvement [160], which provide potential methods for deploying conservative policies in situations of uncertainty; to our knowledge, this work has not yet been combined with methods for detecting out-of-distribution failures of a model.", "startOffset": 85, "endOffset": 93}, {"referenceID": 93, "context": "For the latter challenge, there is also relevant work based on reachability analysis [91, 98] and robust policy improvement [160], which provide potential methods for deploying conservative policies in situations of uncertainty; to our knowledge, this work has not yet been combined with methods for detecting out-of-distribution failures of a model.", "startOffset": 85, "endOffset": 93}, {"referenceID": 154, "context": "For the latter challenge, there is also relevant work based on reachability analysis [91, 98] and robust policy improvement [160], which provide potential methods for deploying conservative policies in situations of uncertainty; to our knowledge, this work has not yet been combined with methods for detecting out-of-distribution failures of a model.", "startOffset": 124, "endOffset": 129}, {"referenceID": 99, "context": "The first is counterfactual reasoning [104, 127, 115, 29], where one asks \u201cwhat would have happened if the world were different in a certain way\u201d? In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift.", "startOffset": 38, "endOffset": 57}, {"referenceID": 122, "context": "The first is counterfactual reasoning [104, 127, 115, 29], where one asks \u201cwhat would have happened if the world were different in a certain way\u201d? In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift.", "startOffset": 38, "endOffset": 57}, {"referenceID": 110, "context": "The first is counterfactual reasoning [104, 127, 115, 29], where one asks \u201cwhat would have happened if the world were different in a certain way\u201d? In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift.", "startOffset": 38, "endOffset": 57}, {"referenceID": 26, "context": "The first is counterfactual reasoning [104, 127, 115, 29], where one asks \u201cwhat would have happened if the world were different in a certain way\u201d? In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift.", "startOffset": 38, "endOffset": 57}, {"referenceID": 26, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 113, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 143, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 151, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 71, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 130, "context": "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [29, 118, 149, 157, 75, 135] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.", "startOffset": 104, "endOffset": 132}, {"referenceID": 128, "context": "The second perspective is machine learning with contracts \u2014 in this perspective, one would like to construct machine learning systems that satisfy a well-defined contract on their behavior in analogy with the design of software systems [133, 27, 87].", "startOffset": 236, "endOffset": 249}, {"referenceID": 24, "context": "The second perspective is machine learning with contracts \u2014 in this perspective, one would like to construct machine learning systems that satisfy a well-defined contract on their behavior in analogy with the design of software systems [133, 27, 87].", "startOffset": 236, "endOffset": 249}, {"referenceID": 83, "context": "The second perspective is machine learning with contracts \u2014 in this perspective, one would like to construct machine learning systems that satisfy a well-defined contract on their behavior in analogy with the design of software systems [133, 27, 87].", "startOffset": 236, "endOffset": 249}, {"referenceID": 128, "context": "[133] enumerates a list of ways in which existing machine learning systems fail to do this, and the problems this can cause for deployment and maintenance of machine learning systems at scale.", "startOffset": 0, "endOffset": 5}, {"referenceID": 87, "context": "Reachability analysis [91, 98] and model repair [56] provide other avenues for obtaining better contracts \u2014 in reachability analysis, we optimize performance subject to the condition that a safe region can always be reached by a known conservative policy, and in model repair we alter a trained model to ensure that certain desired safety properties hold.", "startOffset": 22, "endOffset": 30}, {"referenceID": 93, "context": "Reachability analysis [91, 98] and model repair [56] provide other avenues for obtaining better contracts \u2014 in reachability analysis, we optimize performance subject to the condition that a safe region can always be reached by a known conservative policy, and in model repair we alter a trained model to ensure that certain desired safety properties hold.", "startOffset": 22, "endOffset": 30}, {"referenceID": 52, "context": "Reachability analysis [91, 98] and model repair [56] provide other avenues for obtaining better contracts \u2014 in reachability analysis, we optimize performance subject to the condition that a safe region can always be reached by a known conservative policy, and in model repair we alter a trained model to ensure that certain desired safety properties hold.", "startOffset": 48, "endOffset": 52}, {"referenceID": 109, "context": "To be specific, the challenge could be: train a state-of-the-art speech system on a standard dataset [114] that gives well-calibrated results (if not necessarily good results) on a range of other test sets, like noisy and accented speech.", "startOffset": 101, "endOffset": 106}, {"referenceID": 20, "context": "sentiment analysis [23], as well as benchmarks in computer vision [156]), that would inspire confidence in the reliability of that methodology for handling novel inputs.", "startOffset": 19, "endOffset": 23}, {"referenceID": 150, "context": "sentiment analysis [23], as well as benchmarks in computer vision [156]), that would inspire confidence in the reliability of that methodology for handling novel inputs.", "startOffset": 66, "endOffset": 71}, {"referenceID": 69, "context": "Illustrative of this work is an impressive and successful effort to formally verify the entire federal aircraft collision avoidance system [73, 90].", "startOffset": 139, "endOffset": 147}, {"referenceID": 86, "context": "Illustrative of this work is an impressive and successful effort to formally verify the entire federal aircraft collision avoidance system [73, 90].", "startOffset": 139, "endOffset": 147}, {"referenceID": 94, "context": "Similar work includes traffic control algorithms [99] and many other topics.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "\u2022 Futurist Community: A cross-disciplinary group of academics and non-profits has raised concern about the long term implications of AI [26, 162], particularly superintelligent AI.", "startOffset": 136, "endOffset": 145}, {"referenceID": 156, "context": "\u2022 Futurist Community: A cross-disciplinary group of academics and non-profits has raised concern about the long term implications of AI [26, 162], particularly superintelligent AI.", "startOffset": 136, "endOffset": 145}, {"referenceID": 43, "context": "The Future of Humanity Institute has studied this issue particularly as it relates to future AI systems learning or executing humanity\u2019s preferences [47, 42, 14, 12].", "startOffset": 149, "endOffset": 165}, {"referenceID": 38, "context": "The Future of Humanity Institute has studied this issue particularly as it relates to future AI systems learning or executing humanity\u2019s preferences [47, 42, 14, 12].", "startOffset": 149, "endOffset": 165}, {"referenceID": 12, "context": "The Future of Humanity Institute has studied this issue particularly as it relates to future AI systems learning or executing humanity\u2019s preferences [47, 42, 14, 12].", "startOffset": 149, "endOffset": 165}, {"referenceID": 10, "context": "The Future of Humanity Institute has studied this issue particularly as it relates to future AI systems learning or executing humanity\u2019s preferences [47, 42, 14, 12].", "startOffset": 149, "endOffset": 165}, {"referenceID": 51, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 50, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 31, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 146, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 134, "context": "The Machine Intelligence Research Institute has studied safety issues that may arise in very advanced AI [55, 54, 35, 152, 140], focusing to date on high-level issues in e.", "startOffset": 105, "endOffset": 127}, {"referenceID": 123, "context": "\u201d [128] propose research priorities for robust and beneficial artificial intelligence, and includes several other topics in addition to AI-related accidents, though it also discusses accidents .", "startOffset": 2, "endOffset": 7}, {"referenceID": 138, "context": "Finally, two of the authors of this paper have written informally about safety in AI systems [144, 33]; these postings provided inspiration for parts of the present document.", "startOffset": 93, "endOffset": 102}, {"referenceID": 70, "context": "\u2022 Privacy: How can we ensure privacy when applying machine learning to sensitive data sources such as medical data? [74, 1]", "startOffset": 116, "endOffset": 123}, {"referenceID": 0, "context": "\u2022 Privacy: How can we ensure privacy when applying machine learning to sensitive data sources such as medical data? [74, 1]", "startOffset": 116, "endOffset": 123}, {"referenceID": 2, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 157, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 5, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 41, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 112, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 158, "context": "\u2022 Fairness: How can we make sure ML systems don\u2019t discriminate? [3, 163, 6, 45, 117, 164]", "startOffset": 64, "endOffset": 89}, {"referenceID": 141, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 90, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 91, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 108, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 101, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 16, "context": "\u2022 Security: What can a malicious adversary do to a ML system? [147, 94, 95, 113, 106, 19]", "startOffset": 62, "endOffset": 89}, {"referenceID": 105, "context": "\u2022 Transparency: How can we understand what complicated ML systems are doing? [110, 161, 103, 107]", "startOffset": 77, "endOffset": 97}, {"referenceID": 155, "context": "\u2022 Transparency: How can we understand what complicated ML systems are doing? [110, 161, 103, 107]", "startOffset": 77, "endOffset": 97}, {"referenceID": 98, "context": "\u2022 Transparency: How can we understand what complicated ML systems are doing? [110, 161, 103, 107]", "startOffset": 77, "endOffset": 97}, {"referenceID": 102, "context": "\u2022 Transparency: How can we understand what complicated ML systems are doing? [110, 161, 103, 107]", "startOffset": 77, "endOffset": 97}, {"referenceID": 28, "context": "\u2022 Policy: How do we predict and respond to the economic and social consequences of ML? [31, 51, 15, 32]", "startOffset": 87, "endOffset": 103}, {"referenceID": 47, "context": "\u2022 Policy: How do we predict and respond to the economic and social consequences of ML? [31, 51, 15, 32]", "startOffset": 87, "endOffset": 103}, {"referenceID": 13, "context": "\u2022 Policy: How do we predict and respond to the economic and social consequences of ML? [31, 51, 15, 32]", "startOffset": 87, "endOffset": 103}, {"referenceID": 29, "context": "\u2022 Policy: How do we predict and respond to the economic and social consequences of ML? [31, 51, 15, 32]", "startOffset": 87, "endOffset": 103}], "year": 2016, "abstractText": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\u201cavoiding side effects\u201d and \u201cavoiding reward hacking\u201d), an objective function that is too expensive to evaluate frequently (\u201cscalable supervision\u201d), or undesirable behavior during the learning process (\u201csafe exploration\u201d and \u201cdistributional shift\u201d). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking", "creator": "LaTeX with hyperref package"}}}