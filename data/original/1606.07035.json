{"id": "1606.07035", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Ancestral Causal Inference", "abstract": "Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions. Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising, existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time. Additionally, we propose a method to score causal predictions based on their confidence. Crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge. We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-of-the-art on synthetic data, achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it on a challenging protein data set.", "histories": [["v1", "Wed, 22 Jun 2016 18:26:27 GMT  (7276kb,D)", "http://arxiv.org/abs/1606.07035v1", null], ["v2", "Fri, 11 Nov 2016 22:23:32 GMT  (1594kb,D)", "http://arxiv.org/abs/1606.07035v2", "Accepted in NIPS 2016"], ["v3", "Thu, 26 Jan 2017 14:26:27 GMT  (1603kb,D)", "http://arxiv.org/abs/1606.07035v3", "In Proceedings of Advances in Neural Information Processing Systems 29 (NIPS 2016)"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["sara magliacane", "tom claassen", "joris m mooij"], "accepted": true, "id": "1606.07035"}, "pdf": {"name": "1606.07035.pdf", "metadata": {"source": "CRF", "title": "Ancestral Causal Inference", "authors": ["Sara Magliacane", "Tom Claassen", "Joris M. Mooij"], "emails": ["s.magliacane@uva.nl", "tomc@cs.ru.nl", "j.m.mooij@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "Discovering causal relations from data is one of the foundations of the scientific method. In most cases, cause-effect relations are recovered from experimental data in which the variable of interest is perturbed, but seminal work like the do-calculus [16] and the PC/FCI algorithms [21, 24] demonstrate that, under certain assumptions (e.g., the well-known Causal Markov and Faithfulness assumptions [21]), it is already possible to obtain significant causal information by using only observational data.\nCausal discovery methods are historically divided into two categories: constraint-based and scorebased methods. Constraint-based causal discovery methods use statistical independences to express constraints over possible causal models, while score-based methods typically evaluate models using a penalized likelihood score. Compared to score-based approaches for causal discovery, constraintbased methods are able to handle latent confounders and selection bias naturally, and do not require certain parametric modeling assumptions which may not be realistic. On the other hand, constraintbased methods are more vulnerable to errors in statistical independence test results that are common in real-world applications. In this paper we propose a hybrid approach that combines the best of both worlds by solving a constrained optimization problem.\nSince in practice independence tests can be unreliable, the resulting statistical errors can create conflicting constraints. Classical constraint-based algorithms are not able to resolve such conflicts, and often simply ignore, or at best try to avoid them [17]. Different approaches to resolving such conflicts have been proposed recently [3, 23, 9]. The general idea is to assign weights to the input statements that reflect the reliability of the inputs, and then use a reasoning scheme that takes\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 6.\n07 03\n5v 1\n[ cs\n.L G\n] 2\n2 Ju\nthese weights into account. Several weighting schemes can be defined, from simple ways to attach weights to single independence statements [9] to more complicated schemes to obtain weights for combinations of independence statements [23, 3]. Reasoning can then be formulated as a step-by-step approach in which the statements are processed in order of their weight and conflicts are ignored [23, 3], or as a global optimization problem as in [9]. The latter approach has been shown to be able to actually correct errors in statistical test results.\nWhile offering a better accuracy than greedy approaches, one major challenge for the conflict resolution methods that use a global optimization like [9] consists in the scalability of the algorithms due to the vastness of the search space. In this work, we address the scalability issue by using a more coarse-grained, and therefore more efficient, representation of the causal structure: we only represent ancestral relations (both direct and \u201cindirect\u201d causal relations). This representation turns out to be coarse enough to keep the computational burden low, yet rich enough for many practical applications, since in real-world applications the distinction between direct causal relations and ancestral relations is not always clear or necessary.\nAnother major challenge for constraint-based methods is the estimation of the confidence in the predicted causal relations. In this paper, we propose a method to score predictions according to their confidence. The confidence score can be thought of as an approximation to the marginal probability of an ancestral relation. Scoring predictions enables one to rank them according to their reliability, allowing for higher accuracy. This is very important for practical applications, as the low reliability of the predictions of constraint-based methods has been a major impediment to their wide-spread usage.\nOne of the most flexible formulations of constraint-based causal discovery is in logic [2, 3, 23, 9]. Due to its declarative nature and expressive power, logic enables an easy integration of complex background knowledge. This is not trivial in traditional constraint-based approaches like FCI that use a set of fixed rules following a strict order of execution, and even the common task of incorporating ancestral knowledge can require a complex post-processing step [1]. In this work, we exploit the flexibility and ease-of-implementation offered by a formulation in logic, and harness the power of general-purpose logical reasoning engines.\nHere, we propose Ancestral Causal Inference (ACI), a method that combines the ideas described above in order to accurately reconstruct ancestral relations from observational (and possibly interventional) data for causal systems with possible latent variables without feedback. ACI solves an optimization problem in the spirit of [9], but uses an entirely different encoding using novel ancestral reasoning rules that bypasses the construction of fine-grained representations and thereby can achieve speedups of several orders of magnitude over the method of [9]. We apply our method to score confidences of ancestral relations to both ACI and the method from [9] and show on synthetic data that it can outperform bootstrapped (C)FCI. We prove that under mild conditions on the statistical tests, the confidence scores are (asymptotically) consistent. We showcase the flexibility of logic-based approaches by integrating constraints obtained from a combination of observational data and interventional data in the form of weighted ancestral relations, substantially increasing the reliability of the predictions. We apply ACI to a challenging real-world data set [20] that so far had only been addressed with score-based methods and observe that it successfully recovers from faithfulness violations. Finally, we provide an open-source version of our algorithms and the evaluation framework, which can be easily extended (a link will be added in the camera-ready version)."}, {"heading": "2 Preliminaries and related work", "text": "Preliminaries We will assume that the data generating process can be modeled by a causal Directed Acyclic Graph (DAG) that may contain latent variables. Throughout the paper we represent variables with uppercase letters, while sets of variables are denoted by boldface. A directed edge X \u2192 Y in the causal DAG represents a direct causal relationship of cause X on effect Y . A sequence of directed edges X1 \u2192 X2 \u2192 \u00b7 \u00b7 \u00b7 \u2192 Xn is a directed path. If there is a directed path from X to Y (or X = Y ) then X is an ancestor of Y (denoted as X 99K Y ). For a set of variables W , we write X 99K W if there exists a Y \u2208 W with X 99K Y . If there is no directed path from X to Y we denote this as X 699K Y . For a set of variables W , we write X 699K W if X 699K Y for all Y \u2208W .\nWe call any relation on the observed variables that satisfies the non-strict partial order axioms: (reflexivity) : X 99K X (1) (transitivity) : X 99K Y \u2227 Y 99K Z =\u21d2 X 99K Z (2) (antisymmetry) : X 99K Y \u2227 Y 99K X =\u21d2 X = Y (3)\nan ancestral structure. The underlying causal DAG induces a unique ancestral structure on the observed variables. We will assume that the Causal Markov Assumption and the Causal Faithfulness Assumption [21] both hold. In other words, the conditional independences in the observational distribution correspond one-to-one with the d-separations in the causal DAG. For simplicity we do not consider selection bias.\nFor disjoint sets X,Y ,W we denote conditional independence of X and Y given W as X \u22a5 Y |W , and conditional dependence as X 6\u22a5 Y |W . We call the cardinality |W | the order of the conditional (in)dependence relation. Following [2] we define a minimal conditional independence by:\nX \u22a5 Y |W \u222a [Z] := { X \u22a5 Y |W \u222aZ, and \u2200Z\u2032 ( Z : X 6\u22a5 Y |W \u222aZ\u2032,\nand similarly, a minimal conditional dependence by: X 6\u22a5 Y |W \u222a [Z] := { X 6\u22a5 Y |W \u222aZ, and \u2200Z\u2032 ( Z : X \u22a5 Y |W \u222aZ\u2032.\nThe square brackets indicate that the variables in Z are needed for the (in)dependence to hold in the context of W . Minimal conditional (in)dependences are closely related to ancestral relations, as pointed out in [2]: Lemma 1. For disjoint (sets of) variables X,Y, Z,W :\nX \u22a5 Y |W \u222a [Z] =\u21d2 Z 99K ({X,Y } \u222aW ), (4) X 6\u22a5 Y |W \u222a [Z] =\u21d2 Z 699K ({X,Y } \u222aW ). (5)\n(All proofs are provided in the Supplementary Material.)\nExploiting these rules (as well as others that will be introduced in Section 3) to deduce ancestral relations directly from (in)dependences is key to the greatly improved scalability of our method.\nRelated work on conflict resolution One of the earliest algorithms to deal with conflicting inputs in constraint-based causal discovery is Conservative PC [17], which adds \u201credundant\u201d checks to the PC algorithm that allow to detect inconsistencies in the inputs, and then makes only predictions that do not rely on the ambiguous inputs. The same idea can be applied to FCI, yielding Conservative FCI (CFCI) [4, 11]. BCCD [3] uses Bayesian confidence estimates to process information in decreasing order of reliability, discarding contradictory input as they arise. COMBINE [23] is an algorithm that combines the output of FCI on several overlapping observational and experimental datasets into a single causal model by first pooling and recalibrating the independence test p-values, and then adding each constraint incrementally in order of reliability to a SAT instance. Any constraint that makes the problem unsatisfiable is discarded.\nOur approach is inspired by recent work [9], in which causal discovery is formulated as a constrained discrete minimization problem. Given a list of weighted independence statements, the method by [9] searches for the optimal causal graph G that minimizes the sum of the weights of the independence statements that are violated according to G. In order to test whether a causal graph G induces a certain independence, the method makes use of an encoding DAG of d-connection graphs. D-connection graphs are graphs that can be obtained from a causal graph through a series of operations (conditioning, marginalization and interventions). An encoding DAG of d-connection graphs represents a complex structure encoding all possible d-connection graphs and the sequence of operations that generated them from a given causal graph. This approach has been shown to be able to correct errors in the inputs, but is computationally very demanding because of the huge search space."}, {"heading": "3 ACI: Ancestral Causal Inference", "text": "In this paper we propose Ancestral Causal Inference (ACI), a causal discovery method that accurately reconstructs ancestral structures, also in the presence of latent variables and statistical errors. ACI\nbuilds on the work of [9], but rather than optimizing over encoding DAGs, ACI optimizes over the much simpler (but still very expressive) ancestral structures, giving a huge computational speedup. To get an idea of the reduction of the search space: for 7 variables, there are 6\u00d7 106 ancestral structures but already more than 2.3\u00d7 1015 encoding DAGs. ACI also allows for weighted ancestral relations as inputs, which enables to take into account interventional data as illustrated in the real-world application in Section 5.\nAncestral Causal Inference rules The rules from [9] explicitly encode marginalization and conditioning operations on d-connection graphs, so they cannot be easily adapted to work directly with ancestral relations. Instead, ACI encodes the ancestral reasoning rules (1)\u2013(5) and the following six novel causal reasoning rules (proofs are in the Supplementary Material): Lemma 2. For disjoint (sets) of variables X,Y, U, Z,W :\n(X \u22a5 Y | Z) \u2227 (X 699K Z) =\u21d2 X 699K Y (6) X 6\u22a5 Y |W \u222a [Z] =\u21d2 X 6\u22a5 Z |W (7) X \u22a5 Y |W \u222a [Z] =\u21d2 X 6\u22a5 Z |W (8) (X \u22a5 Y |W \u222a [Z]) \u2227 (X \u22a5 Z |W \u222a U) =\u21d2 (X \u22a5 Y |W \u222a U) (9) (Z 6\u22a5 X |W ) \u2227 (Z 6\u22a5 Y |W ) \u2227 (X \u22a5 Y |W ) =\u21d2 X 6\u22a5 Y |W \u222a Z (10) (X 6\u22a5 Y |W ) \u2227 (X 6\u22a5 U |W ) \u2227 (Y \u22a5 U |W ) =\u21d2 X 699K Y (11)\nOptimization of loss function In order to handle conflicts in the inputs, we follow [9] and formulate the causal discovery problem as an optimization problem where a loss function is optimized over possible causal structures. Intuitively, the loss function sums the weights of all the inputs that are violated in a candidate causal structure. Given a list I of weighted input statements (li, wi), where li is the input statement and wi is the associated weight, we define the loss function as the sum of the weights of the input statements that are not satisfied in a given possible structure W \u2208 W , where W denotes the set of all possible causal structures. Causal discovery can then be formulated as the following discrete optimization problem:\nW \u2217 = argmin W\u2208W L(W ; I) (12)\nwhere the loss function is defined as L(W ; I) := \u2211\n(li,wi)\u2208I: W\u222aR6|=li\nwi, (13)\nwhere W \u222aR 6|= li means that li is not satisfied in structure W according to the rulesR. In [9], the possible structuresW correspond with all possible causal graphs (ADMGs in the acyclic case) and the rules correspond with operations on d-connection graphs, whereas in ACIW corresponds with all ancestral structures and the rulesR are rules (1)\u2013(11).\nConstrained optimization in Answer Set Programming (ASP) The constrained optimization problem in (12) can be implemented using a variety of optimization methods. Given the complexity of the rules, a formulation in an expressive logical language that supports optimization, e.g., Answer Set Programming (ASP), is very convenient and is used both in [9] and in this paper. ASP is a widely used declarative programming language based on the stable model semantics[12, 8] that has successfully been applied to several NP-hard problems. To implement ACI we use the state-of-the-art ASP solver clingo [7]. We provide the encoding in the Supplementary Material. Here we show an example of how an ACI rule is implemented: rule 6 for a single variable Z can be written:\n:- indep(X,Y,Z), not causes(X,Z), causes(X,Y), X!=Y, X!=Z, Y!=Z.\nThis rule states that for any disjoint variables X,Y, Z, it cannot happen that X \u22a5 Y | Z, X 699K Z and X 99K Y in any model.\nWeighting schemes ACI allows for two types of input statements: conditional independences and ancestral relations. These statements can each be assigned a weight that reflect their confidence. We propose two simple approaches with the desirable properties of making ACI asymptotically consistent under mild assumptions (as described in the end of this Section), and assigning a much smaller weight\nto independences than to dependences (which agrees with the intuition that one is confident about a strong dependence, but not about independence vs. weak dependence). The approaches are: (1) a frequentist approach, in which one performs any appropriate frequentist statistical test with as null hypothesis independence (resp. a non-ancestral relation) and then uses as weight:\nw = | log p\u2212 log\u03b1|,where p = p-value of the test, \u03b1 = significance level (e.g. 5%) (14)\n(2) a Bayesian approach, in which one assigns a weight to each input statement l using data set D as:\nw = log p(l|D) p(\u00acl|D) = log p(D|l) p(D|\u00acl) p(l) p(\u00acl) . (15)\nwhere the prior probability p(l) can be used as a tuning parameter. Furthermore, we use infinite weights when dealing with oracle inputs.\nIf observational and suitable interventional data is available, a simple way to obtain a weighted ancestral statement X 99K Y is by doing a two-sample test that tests whether the distribution of Y changes with respect to its observational distribution when intervening on X . This approach conveniently applies to various types of interventions: perfect interventions [16], soft interventions [14], mechanism changes [22], and activity interventions [15]. Note that the two-sample test can actually also be implemented as an independence test that tests for the independence of Y and IX , the indicator variable that has value 0 for observational samples and 1 for samples from the interventional distribution in which X has been intervened upon."}, {"heading": "4 Scoring Causal Predictions", "text": "The constrained minimization (12) may produce several optimal solutions, because the underlying structure may not be identifiable from the inputs. In order to address this issue, we propose to use the loss function (13) in the following way in order to score the confidence of a particular feature p (say, an ancestral relation X 99K Y ):\nC(p) = min W\u2208W L(W ; I \u222a {(\u00acp,\u221e)})\u2212 min W\u2208W L(W ; I \u222a {(p,\u221e)}). (16)\nWithout going into details here, we note that the confidence (16) can be interpreted as a MAP approximation of the log-odds ratio of the probability that feature p is true in a Markov Logic model:\nP(p | I,R) P(\u00acp | I,R) =\n\u2211 W\u2208W e\n\u2212L(W ;I)1W\u222aR|=p\u2211 W\u2208W e \u2212L(W ;I)1W\u222aR6|=p \u2248 maxW\u2208W e \u2212L(W ;I\u222a{(p,\u221e)}) maxW\u2208W e\u2212L(W ;I\u222a{(\u00acp,\u221e)}) = eC(p).\nIn this paper, we will usually consider the features p to be ancestral relations between variables, but this idea is more generally applicable. For example, in combination with the method of [9] it can be used to score direct causal relations.\nSoundness and completeness Our scoring method is sound for predicting ancestral relations: Theorem 1. Suppose the rules inR are sound. For any pair X,Y of variables, the confidence score C(X 99K Y ) of (16) is sound for oracle inputs with infinite weights.\nHere, soundness means that C(X 99K Y ) = \u221e if X 99K Y is identifiable from the inputs, C(X 99K Y ) = \u2212\u221e if X 699K Y is identifiable from the inputs, and C(X 99K Y ) = 0 otherwise (neither are identifiable). We conjecture that the ACI rules (1)\u2013(11) are \u201corder-1-complete\u201d, i.e., they allow to deduce all (non)ancestral relations that are identifiable from oracle inputs for all conditional independences of order \u2264 1 in observational data. For higher-order inputs additional ACI rules can be derived, however, our primary interest in this work is improving computation time and accuracy, and we are willing to sacrifice completeness for that. A more detailed theoretical study of the completeness properties is left as future work.\nAsymptotic consistency Denote the number of samples by N . For the frequentist weights in (14), we assume that the statistical tests are consistent in the following sense:\nlog pN \u2212 log\u03b1N P\u2192 { \u2212\u221e H1 +\u221e H0,\n(17)\nas N \u2192\u221e, where the null hypothesis H0 is independence/nonancestral relation and the alternative hypothesis H1 is dependence/ancestral relation. Note that we need to choose a sample-size dependent threshold \u03b1N such that \u03b1N \u2192 0 at a suitable rate. [10] show how this can be done for partial correlation tests assuming the distribution is multivariate Gaussian.\nFor the Bayesian weighting scheme in (15), we assume that wN P\u2192 { \u2212\u221e if l is true +\u221e if l is false. (18)\nThis will hold (as long as there is no model misspecification) under mild technical conditions for exponential family models. In both cases, the probability of a type I or type II error will converge to 0, and in addition, the corresponding weight will converge to\u221e. Theorem 2. Suppose the rules inR are sound. For any pair X,Y of variables, the confidence score C(X 99K Y ) of (16) is asymptotically consistent under assumption (17) or (18).\nHere, \u201casymptotically consistent\u201d means that the confidence score C(X 99K Y )\u2192\u221e in probability if X 99K Y is identifiably true, C(X 99K Y )\u2192 \u2212\u221e in probability if X 99K Y is identifiably false, and C(X 99K Y )\u2192 0 in probability otherwise."}, {"heading": "5 Evaluation", "text": "In this Section we report evaluations on synthetically generated data and an application on a real dataset. We perform all evaluations on a 40 core machine with 2.80GHz CPUs, and run several experiments in parallel, so each instance runs on a single core.\nSynthetic data We simulate the data using the simulator from [9]: for each experimental condition (e.g., a given number of variables n and order c), we generate randomly M linear acyclic models with latent variables and Gaussian noise and sample N = 500 data points. We then perform independence tests up to order c and weight the (in)dependence statements using the weighting schemes described in Section (3). For the frequentist weights we use tests based on partial correlations and Fisher\u2019s z-transform to obtain approximate p-values (see, e.g., [10]) setting the significance level to \u03b1 = 0.05. For the Bayesian weights, we use the Bayesian test for conditional independence presented in [13] as implemented by [9] with a prior probability of 0.1 for independence. We score confidences using (16) for ACI and for the acyclic causally insufficient models presented in [9], and use the same independence tests as input, optionally up to a maximum order c.\nIn Figure 1(a) we show the average execution times for different combinations of n and c, while in Figure 1(b) we show the distribution of runtimes for different instances of randomly generated models. On 7 variables, the scoring method with ACI is almost 3 orders of magnitude faster than [9], and the difference grows exponentially with n increases. On 8 variables the method from [9] is able to complete only four of the first 40 simulated models before the timeout 25\u00d7 103 s. In Figure 2 we show the accuracy of the predictions with precision-recall (PR) curves for both ancestral (X 99K Y ) and nonancestral (X 699K Y ) relations, in different settings. As baselines we\nuse a bootstrapped version (which averages of the results of 100 executions using random subsets of half the data) of FCI [24] and Conservative FCI (CFCI) [4], as implemented in the pcalg R package [11]. The bootstrap is used to rank the predictions, which are otherwise limited to positives, negatives or unknown relations. In order to retrieve the ancestral relations from the PAGs we apply Theorem 3.1 from [19]. For reference we also include the (unbootstrapped) performance of COMBINE [23], although the method is not designed for this setting with only observational data.\nIn the first row of Figure 2, we show the setting with 6 variables and frequentist weights. The performances of the scoring method with [9] and ACI coincide, performing significantly better for nonancestral predictions and the top ancestral predictions (see zoomed-in version in Figure 2(b)), even when using only as inputs independence test results up to maximum order c = 1, instead of independence test results of any order as FCI, CFCI and COMBINE. Interestingly, the two global optimization algorithms do not seem to benefit much from higher order independence tests, thus we omit them from the plots (although we add the graphs in the Supplementary Material). On the other hand, bootstrapping traditional methods that are oblivious to the (in)dependence weights seems to produce surprisingly good results. Nevertheless, our scoring approach (16) outperforms bootstrapped (C)FCI for both encodings [9] and ACI, suggesting that nontrivial error-correction is going on.\nIn the second row of Figure 2, we show the setting with 8 variables and frequentist weights. In this setting the approach from [9] is too slow. In addition to the previous plot, we plot the accuracy of ACI when there is oracle background knowledge on the descendants of one variable (i = 1). This setting simulates the effect of using interventional data, and we can see that the performance of ACI improves significantly, especially in the prediction of the ancestral relations. On the other side, FCI and CFCI cannot take advantage of this knowledge.\nApplication on real data We consider the challenging task of reconstructing a signalling network from flow cytometry data [20]. One of the 14 experimental conditions can be considered as the observational setting, the others as interventional settings. In contrast with likelihood-based approaches to causal discovery that have been applied in earlier work [20, 5, 15, 18], in our approach we do not need to model the interventions quantitatively. We only need to know the intervention targets, whereas the intervention types do not matter. Here we used the subset of 5 experimental conditions described, along with more results, in the Table in the Supplementary Material.\nWe use a t-test to test for each intervention and for each variable whether its distribution changes with respect to the observational condition. We use the p-values of these tests as in (14) in order to obtain weighted ancestral relations that are used as input (with threshold \u03b1 = 0.05). For example, if adding U0126 (a MEK inhibitor) changes the distribution of RAF significantly with respect to the observational baseline condition, we get a weighted ancestral relation MEK99KRAF. In addition, we use partial correlations up to first order (tested in the observational data only) to obtain weighted conditional independences used as input. We use ACI with (16) to score the ancestral relations for each ordered pair of variables. The main results are illustrated in Figure 3. We compare with FCI and CFCI, but those algorithms can only use the independences in the observational data as input and therefore miss the strongest signal, which is in the interventional data. We note that the method of [9] is computationally infeasible, while COMBINE assumes perfect interventions (while here we mostly have activity interventions).\nIt is interesting to note that our algorithms can correctly recover from faithfulness violations like the independence between MEK and ERK, because they are taking into account the weight of the input statements (the weight of the independence is considerably smaller than that of the ancestral relation, which corresponds with a quite significant change in distribution). In contrast, methods that start by reconstructing the skeleton like (C)FCI would decide that MEK and ERK are nonadjacent, and are unable to recover from that erroneous decision. This illustrates one of the advantages of our approach."}, {"heading": "6 Discussion and conclusions", "text": "As illustrated by our example, in real-world experiments finding cause-effect relations is paramount, and ancestral structures are very well-suited to that end. They also offer a natural way to incorporate background causal knowledge, e.g., from other experiments. On top of that, in the context of algorithms that aim for error-correction by exploiting redundant information, they also allow a huge computational advantage over existing edge-based representations such as [9]. When needed, the ancestral structures are easily mapped to a more fine-grained structural representation to indicate direct and indirect causal relations, using the skeleton implied by the output independence weights.\nProviding confidence estimates on causal predictions is extremely helpful in practice, and can significantly boost reliability of the output as perceived by researchers. Although standard methods to do so, like bootstrapping (C)FCI, already provide reasonable estimates, having a global optimization method that take into account all confidences in the input causal and independence statements is likely to lead to further improvements of the reliability of causal relations inferred from data.\nStrangely (or fortunately) enough, neither of the global optimization methods seems to improve much with higher order independence test results. In other words: both already obtain near optimal error-correction behaviour just exploiting up to order-1 independences. We conjecture that this may happen because our loss function essentially assumes that the test results are independent from another (which is not true). Finding a way to take this into account in the loss function may further improve the achievable accuracy, but at the moment the exact relationship between test results is still unclear. Finally, we plan to explore the possibility of extending our ancestral approach to much larger models by incorporating it as a modular routine in a large-scale causal discovery method."}, {"heading": "Acknowledgments", "text": "SM and JMM were supported by NWO, the Netherlands Organization for Scientific Research (VIDI grant 639.072.410). SM was also supported by the Dutch national programme COMMIT/ under the Data2Semantics project. TC was supported by funding from the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n. 603016 (MATRICS)."}, {"heading": "7 Appendix A: Proofs", "text": ""}, {"heading": "7.1 ACI causal reasoning rules", "text": "We give a combined proof of all the ACI reasoning rules. Note that the numbering here is slightly different than in the main paper.\nLemma 3. For X , Y , Z, U , W disjoint (sets of) variables:\n1. (X \u22a5 Y | Z) \u2227 (X 699K Z) =\u21d2 X 699K Y\n2. X 6\u22a5 Y |W \u222a [Z] =\u21d2 (X 6\u22a5 Z |W ) \u2227 (Z 699K {X,Y } \u222aW )\n3. X \u22a5 Y |W \u222a [Z] =\u21d2 (X 6\u22a5 Z |W ) \u2227 (Z 99K {X,Y } \u222aW )\n4. (X \u22a5 Y |W \u222a [Z]) \u2227 (X \u22a5 Z |W \u222a U) =\u21d2 (X \u22a5 Y |W \u222a U)\n5. (Z 6\u22a5 X |W ) \u2227 (Z 6\u22a5 Y |W ) \u2227 (X \u22a5 Y |W ) =\u21d2 X 6\u22a5 Y |W \u222a Z\n6. (X 6\u22a5 Y |W ) \u2227 (X 6\u22a5 U |W ) \u2227 (Y \u22a5 U |W ) =\u21d2 X 699K Y\nProof. We assume a causal DAG with possible latent variables, the causal Markov assumption, and the causal faithfulness assumption.\n1. This is a strengthened version of rule R2(i) in [6]: note that the additional assumptions made there (Y 699K Z, Y 699K X) are redundant and not actually used in their proof. For completeness, we give the proof here. If X 99K Y , then there is a directed path from X to Y . As all paths between X and Y are blocked by Z, the directed path from X to Y must contain a node Z \u2208 Z. Hence X 99K Z, a contradiction with X 699K Z.\n2. If X 6\u22a5 Y |W \u222a [Z] then there exists a path \u03c0 between X and Y such that each noncollider on \u03c0 is not in W \u222a {Z}, every collider on \u03c0 is ancestor of W \u222a {Z}, and there exists a collider on \u03c0 that is ancestor of Z but not of W . Let C be the collider on \u03c0 closest to X that is ancestor of Z but not of W . Note that\n(a) The path X \u00b7 \u00b7 \u00b7C \u2192 \u00b7 \u00b7 \u00b7 \u2192 Z is d-connected given W . (b) Z 699K W (because otherwise C 99K Z 99K W , a contradiction). (c) Z 699K Y (because otherwise the path X \u00b7 \u00b7 \u00b7C \u2192 \u00b7 \u00b7 \u00b7 \u2192 Z \u2192 \u00b7 \u00b7 \u00b7 \u2192 Y would be\nd-connected given W , a contradiction).\nHence we conclude that X 6\u22a5 Z |W , Z 699K W , Z 699K Y , and by symmetry also Z 699K X .\n3. Suppose X \u22a5 Y |W \u222a [Z]. Then there exists a path \u03c0 between X and Y , such that each noncollider on \u03c0 is not in W , each collider on \u03c0 is an ancestor of W , and Z is a noncollider on \u03c0. Note that\n(a) The subpath X . . . Z must be d-connected given W . (b) Z has at least one outgoing edge on \u03c0. Follow this edge further along \u03c0 until reaching\neither X , Y , or the first collider. When a collider is reached, follow the directed path to W . Hence there is a directed path from Z toX or Y or to W , i.e., Z 99K {X,Y }\u222aW .\n4. If in addition, X \u22a5 Z | W \u222a U , then U must be a noncollider on the subpath X . . . Z. Therefore, X \u22a5 Y |W \u222a U .\n5. Assume that Z 6\u22a5 X |W and Z 6\u22a5 Y |W . Then there must be paths \u03c0 between Z and X and \u03c1 between Z and Y such that each noncollider is not in W and each collider is ancestor of W . Let U be the node on \u03c0 closest to X that is also on \u03c1 (this could be Z). Then we have a path X \u00b7 \u00b7 \u00b7U \u00b7 \u00b7 \u00b7Y such that each collider (except U ) is ancestor of W and each noncollider (except U ) is not in W . This path must be blocked given W as X \u22a5 Y |W . If U would be a noncollider on this path, it would need to be in W in order to block it; however, it must then also be a noncollider on \u03c0 or \u03c1 and hence cannot be in W . Therefore, U must be a collider on this path and cannot be ancestor of W . We have to show that U is ancestor of Z. If U were a collider on \u03c0 or \u03c1, it would be ancestor of W , a contradiction. Hence U must have an outgoing arrow pointing towards Z on \u03c0 and \u03c1. If we encounter a collider following the directed edges, we get a contradiction, as that collider, and hence U , would be ancestor of W . Hence U is ancestor of Z, and therefore, X 6\u22a5 Y |W \u222a Z.\n6. The assumptions X 6\u22a5 Y |W and X 6\u22a5 U |W imply that there are paths from U to X and from X to Y that are both d-connected given W . Concatenate them and construct a path \u03c0 from U to Y by removing cycles, if necessary. As Y \u22a5 U |W , this path must be blocked given W . The only possibility for this to happen is that X must be a collider on that path, and neither X or any of its descendants must be in W . Therefore, if X 99K Y then there is a directed path from X to Y that cannot pass through nodes in W . It also cannot pass through U as this would give a contradiction with Y \u22a5 U |W . Therefore, this path from X to Y is d-connected given W . By concatenating it with one of the paths from U to X that are d-connected given W and removing cycles if necessary we obtain a path from U to Y that is d-connected given W . This contradicts Y \u22a5 U |W . Therefore, X 699K Y ."}, {"heading": "7.2 Soundness", "text": "Theorem 3. Suppose the rules inR are sound. For any pair X,Y of variables, the confidence score C(X 99K Y ) of (16) is sound for oracle inputs with infinite weights, i.e., C(X 99K Y ) = \u221e if X 99K Y is identifiable from the inputs, C(X 99K Y ) = \u2212\u221e if X 699K Y is identifiable from the inputs, and C(X 99K Y ) = 0 otherwise (neither are identifiable).\nProof. We assume that the data generating process is described by a causal DAG which may contain additional latent variables, and that the distributions are faithful to the DAG. The theorem then follows directly from the soundness of the rules and the soundness of logical reasoning."}, {"heading": "7.3 Asymptotic consistency of scoring method", "text": "Theorem 4. Suppose the rules inR are sound. For any pair X,Y of variables, the confidence score C(X 99K Y ) of (16) is asymptotically consistent under assumption (14) or (15) in the main paper, i.e.,\n\u2022 C(X 99K Y )\u2192\u221e in probability if X 99K Y is identifiably true,\n\u2022 C(X 99K Y )\u2192 \u2212\u221e in probability if X 99K Y is identifiably false,\n\u2022 C(X 99K Y )\u2192 0 in probability otherwise (neither are identifiable).\nProof. As the number of statistical tests is fixed (or at least bounded from above), the probability of any error in the test results converges to 0 asymptotically. The loss function of all structures that do not correspond with the properties of the true causal DAG converges to +\u221e in probability, whereas\nthe loss function of all structures that are compatible with properties of the true causal DAG converges to 0 in probability."}, {"heading": "8 Appendix B: Additional results on synthetic data", "text": "in Figure (4) we report accuracy results on synthetic data also for the Bayesian test described in the main paper. As prior probability of independence we use p = 0.1. The overall conclusions do not change: ACI and [9] overlap for order c = 1 and they perform better than bootstrapped (C)FCI.\nIn Figure (5) we show the performance of ACI and [9] for higher order c = 4 on the same setting as Figure 2 (a-c) in the main paper. As we see, the performances of ACI and [9] do not really improve with higher order but actually seem to deteriorate."}, {"heading": "9 Appendix C: Application on real data", "text": "We provide more details and more results on the real-world dataset we describe in the main paper, the flow cytometry data [20]. The data consists of simultaneous measurements of expression levels of eleven biochemical agents in individual cells of the human immune system under 14 different experimental conditions, in which different activators and inhibitors have been added to the cells. One of the 14 experimental conditions can be considered as the observational setting, the others as interventional settings.\nIn Table 1 we show the subset of interventions that we consider in our experiments. In Figure 6 we show the inputs (first row) and the results for the different algorithms where we study a similar setting to the one presented in the main paper, but now always adding Intercellular Adhesion Protein-2\n(ICAM-2) reagent in combination with the other reagents. The results are very similar to the results in the main paper (without ICAM present), showing that the conclusions on ancestral relations generalize. In particular it is clear that also in this setting weighted ancestral relations are a very strong signal and that methods that can exploit them (e.g., ACI) have a distinct advantage over methods that cannot (e.g., FCI and CFCI).\nThere appear to be various faithfulness violations. For example, it is well-known that MEK causes ERK, yet in the observational data these two variables are independent. Nevertheless, we can see in the data that an intervention on MEK leads to a change of ERK, as expected. It is interesting to note that our approach can correctly recover from this faithfulness violation because they are taking into account the weight of the input statements (note that the weight of the independence is smaller than that of the ancestral relation, which corresponds with a quite significant change in distribution). In contrast, methods that start by reconstructing the skeleton (like (C)FCI or LoCI) would decide that MEK and ERK are nonadjacent, unable to recover from that erroneous decision. This illustrates one of the advantages of our approach."}], "references": [{"title": "Incorporating causal prior knowledge as path-constraints in bayesian networks and maximal ancestral graphs", "author": ["G. Borboudakis", "I. Tsamardinos"], "venue": "ICML, pages 1799\u20131806,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A logical characterization of constraint-based causal discovery", "author": ["T. Claassen", "T. Heskes"], "venue": "UAI, pages 135\u2013144,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A Bayesian approach to constraint-based causal inference", "author": ["T. Claassen", "T. Heskes"], "venue": "UAI, pages 207\u2013216,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning high-dimensional directed acyclic graphs with latent and selection variables", "author": ["D. Colombo", "M.H. Maathuis", "M. Kalisch", "T.S. Richardson"], "venue": "The Annals of Statistics, 40(1):294\u2013321,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Exact Bayesian structure learning from uncertain interventions", "author": ["D. Eaton", "K. Murphy"], "venue": "AISTATS, pages 107\u2013114,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Data-driven covariate selection for nonparametric estimation of causal effects", "author": ["D. Entner", "P.O. Hoyer", "P. Spirtes"], "venue": "AISTATS, volume 31 of JMLR Proceedings, pages 256\u2013264. JMLR.org,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Clingo = ASP + control: Extended report", "author": ["M. Gebser", "R. Kaminski", "B. Kaufmann", "T. Schaub"], "venue": "Technical report, University of Potsdam,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Answer sets", "author": ["M. Gelfond"], "venue": "F. van Harmelen, V. Lifschitz, and B. W. Porter, editors, Handbook of Knowledge Representation, volume 3 of Foundations of Artificial Intelligence, pages 285\u2013316. Elsevier,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Constraint-based causal discovery: Conflict resolution with Answer Set Programming", "author": ["A. Hyttinen", "F. Eberhardt", "M. J\u00e4rvisalo"], "venue": "UAI, pages 340\u2013349,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating high-dimensional directed acyclic graphs with the PC-algorithm", "author": ["M. Kalisch", "P. B\u00fchlmann"], "venue": "JMLR, 8:613\u2013636, Mar.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Causal inference using graphical models with the R package pcalg", "author": ["M. Kalisch", "M. M\u00e4chler", "D. Colombo", "M. Maathuis", "P. B\u00fchlmann"], "venue": "Journal of Statistical Software, 47(1):1\u201326,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "What is answer set programming? In D", "author": ["V. Lifschitz"], "venue": "Fox and C. P. Gomes, editors, AAAI, pages 1594\u20131597. AAAI Press,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient Markov network discovery using particle filters", "author": ["D. Margaritis", "F. Bromberg"], "venue": "Computational Intelligence, 25(4):367\u2013394,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic soft interventions in conditional Gaussian networks", "author": ["F. Markowetz", "S. Grossmann", "R. Spang"], "venue": "AISTATS, pages 214\u2013221,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Cyclic causal discovery from continuous equilibrium data", "author": ["J.M. Mooij", "T. Heskes"], "venue": "A. Nicholson and P. Smyth, editors, UAI, pages 431\u2013439. AUAI Press,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Causality: models, reasoning and inference", "author": ["J. Pearl"], "venue": "Cambridge University Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Adjacency-faithfulness and conservative causal inference", "author": ["J. Ramsey", "J. Zhang", "P. Spirtes"], "venue": "UAI. AUAI Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "BACKSHIFT: Learning causal cyclic graphs from unknown shift interventions", "author": ["D. Rothenh\u00e4usler", "C. Heinze", "J. Peters", "N. Meinshausen"], "venue": "NIPS, pages 1513\u20131521. Curran Associates, Inc.,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Marginal causal consistency in constraint-based causal learning", "author": ["A. Roumpelaki", "G. Borboudakis", "S. Triantafillou", "I. Tsamardinos"], "venue": "UAI,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Causal protein-signaling networks derived from multiparameter singlecell", "author": ["K. Sachs", "O. Perez", "D. Pe\u2019er", "D. Lauffenburger", "G. Nolan"], "venue": "data. Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Causation, Prediction, and Search", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": "MIT press, 2nd edition,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Causal discovery from changes", "author": ["J. Tian", "J. Pearl"], "venue": "UAI, pages 512\u2013521,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Constraint-based causal discovery from multiple interventions over overlapping variable sets", "author": ["S. Triantafillou", "I. Tsamardinos"], "venue": "JMLR, 16:2147\u20132205,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias", "author": ["J. Zhang"], "venue": "Artif. Intell., 172(16-17):1873\u20131896, Nov.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 15, "context": "In most cases, cause-effect relations are recovered from experimental data in which the variable of interest is perturbed, but seminal work like the do-calculus [16] and the PC/FCI algorithms [21, 24] demonstrate that, under certain assumptions (e.", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "In most cases, cause-effect relations are recovered from experimental data in which the variable of interest is perturbed, but seminal work like the do-calculus [16] and the PC/FCI algorithms [21, 24] demonstrate that, under certain assumptions (e.", "startOffset": 192, "endOffset": 200}, {"referenceID": 23, "context": "In most cases, cause-effect relations are recovered from experimental data in which the variable of interest is perturbed, but seminal work like the do-calculus [16] and the PC/FCI algorithms [21, 24] demonstrate that, under certain assumptions (e.", "startOffset": 192, "endOffset": 200}, {"referenceID": 20, "context": ", the well-known Causal Markov and Faithfulness assumptions [21]), it is already possible to obtain significant causal information by using only observational data.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Classical constraint-based algorithms are not able to resolve such conflicts, and often simply ignore, or at best try to avoid them [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "Different approaches to resolving such conflicts have been proposed recently [3, 23, 9].", "startOffset": 77, "endOffset": 87}, {"referenceID": 22, "context": "Different approaches to resolving such conflicts have been proposed recently [3, 23, 9].", "startOffset": 77, "endOffset": 87}, {"referenceID": 8, "context": "Different approaches to resolving such conflicts have been proposed recently [3, 23, 9].", "startOffset": 77, "endOffset": 87}, {"referenceID": 8, "context": "Several weighting schemes can be defined, from simple ways to attach weights to single independence statements [9] to more complicated schemes to obtain weights for combinations of independence statements [23, 3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 22, "context": "Several weighting schemes can be defined, from simple ways to attach weights to single independence statements [9] to more complicated schemes to obtain weights for combinations of independence statements [23, 3].", "startOffset": 205, "endOffset": 212}, {"referenceID": 2, "context": "Several weighting schemes can be defined, from simple ways to attach weights to single independence statements [9] to more complicated schemes to obtain weights for combinations of independence statements [23, 3].", "startOffset": 205, "endOffset": 212}, {"referenceID": 22, "context": "Reasoning can then be formulated as a step-by-step approach in which the statements are processed in order of their weight and conflicts are ignored [23, 3], or as a global optimization problem as in [9].", "startOffset": 149, "endOffset": 156}, {"referenceID": 2, "context": "Reasoning can then be formulated as a step-by-step approach in which the statements are processed in order of their weight and conflicts are ignored [23, 3], or as a global optimization problem as in [9].", "startOffset": 149, "endOffset": 156}, {"referenceID": 8, "context": "Reasoning can then be formulated as a step-by-step approach in which the statements are processed in order of their weight and conflicts are ignored [23, 3], or as a global optimization problem as in [9].", "startOffset": 200, "endOffset": 203}, {"referenceID": 8, "context": "While offering a better accuracy than greedy approaches, one major challenge for the conflict resolution methods that use a global optimization like [9] consists in the scalability of the algorithms due to the vastness of the search space.", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "One of the most flexible formulations of constraint-based causal discovery is in logic [2, 3, 23, 9].", "startOffset": 87, "endOffset": 100}, {"referenceID": 2, "context": "One of the most flexible formulations of constraint-based causal discovery is in logic [2, 3, 23, 9].", "startOffset": 87, "endOffset": 100}, {"referenceID": 22, "context": "One of the most flexible formulations of constraint-based causal discovery is in logic [2, 3, 23, 9].", "startOffset": 87, "endOffset": 100}, {"referenceID": 8, "context": "One of the most flexible formulations of constraint-based causal discovery is in logic [2, 3, 23, 9].", "startOffset": 87, "endOffset": 100}, {"referenceID": 0, "context": "This is not trivial in traditional constraint-based approaches like FCI that use a set of fixed rules following a strict order of execution, and even the common task of incorporating ancestral knowledge can require a complex post-processing step [1].", "startOffset": 246, "endOffset": 249}, {"referenceID": 8, "context": "ACI solves an optimization problem in the spirit of [9], but uses an entirely different encoding using novel ancestral reasoning rules that bypasses the construction of fine-grained representations and thereby can achieve speedups of several orders of magnitude over the method of [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "ACI solves an optimization problem in the spirit of [9], but uses an entirely different encoding using novel ancestral reasoning rules that bypasses the construction of fine-grained representations and thereby can achieve speedups of several orders of magnitude over the method of [9].", "startOffset": 281, "endOffset": 284}, {"referenceID": 8, "context": "We apply our method to score confidences of ancestral relations to both ACI and the method from [9] and show on synthetic data that it can outperform bootstrapped (C)FCI.", "startOffset": 96, "endOffset": 99}, {"referenceID": 19, "context": "We apply ACI to a challenging real-world data set [20] that so far had only been addressed with score-based methods and observe that it successfully recovers from faithfulness violations.", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "We will assume that the Causal Markov Assumption and the Causal Faithfulness Assumption [21] both hold.", "startOffset": 88, "endOffset": 92}, {"referenceID": 1, "context": "Following [2] we define a minimal conditional independence by:", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "Minimal conditional (in)dependences are closely related to ancestral relations, as pointed out in [2]: Lemma 1.", "startOffset": 98, "endOffset": 101}, {"referenceID": 16, "context": "Related work on conflict resolution One of the earliest algorithms to deal with conflicting inputs in constraint-based causal discovery is Conservative PC [17], which adds \u201credundant\u201d checks to the PC algorithm that allow to detect inconsistencies in the inputs, and then makes only predictions that do not rely on the ambiguous inputs.", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "The same idea can be applied to FCI, yielding Conservative FCI (CFCI) [4, 11].", "startOffset": 70, "endOffset": 77}, {"referenceID": 10, "context": "The same idea can be applied to FCI, yielding Conservative FCI (CFCI) [4, 11].", "startOffset": 70, "endOffset": 77}, {"referenceID": 2, "context": "BCCD [3] uses Bayesian confidence estimates to process information in decreasing order of reliability, discarding contradictory input as they arise.", "startOffset": 5, "endOffset": 8}, {"referenceID": 22, "context": "COMBINE [23] is an algorithm that combines the output of FCI on several overlapping observational and experimental datasets into a single causal model by first pooling and recalibrating the independence test p-values, and then adding each constraint incrementally in order of reliability to a SAT instance.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "Our approach is inspired by recent work [9], in which causal discovery is formulated as a constrained discrete minimization problem.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "Given a list of weighted independence statements, the method by [9] searches for the optimal causal graph G that minimizes the sum of the weights of the independence statements that are violated according to G.", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "builds on the work of [9], but rather than optimizing over encoding DAGs, ACI optimizes over the much simpler (but still very expressive) ancestral structures, giving a huge computational speedup.", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "Ancestral Causal Inference rules The rules from [9] explicitly encode marginalization and conditioning operations on d-connection graphs, so they cannot be easily adapted to work directly with ancestral relations.", "startOffset": 48, "endOffset": 51}, {"referenceID": 8, "context": "Optimization of loss function In order to handle conflicts in the inputs, we follow [9] and formulate the causal discovery problem as an optimization problem where a loss function is optimized over possible causal structures.", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "In [9], the possible structuresW correspond with all possible causal graphs (ADMGs in the acyclic case) and the rules correspond with operations on d-connection graphs, whereas in ACIW corresponds with all ancestral structures and the rulesR are rules (1)\u2013(11).", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": ", Answer Set Programming (ASP), is very convenient and is used both in [9] and in this paper.", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "ASP is a widely used declarative programming language based on the stable model semantics[12, 8] that has successfully been applied to several NP-hard problems.", "startOffset": 89, "endOffset": 96}, {"referenceID": 7, "context": "ASP is a widely used declarative programming language based on the stable model semantics[12, 8] that has successfully been applied to several NP-hard problems.", "startOffset": 89, "endOffset": 96}, {"referenceID": 6, "context": "To implement ACI we use the state-of-the-art ASP solver clingo [7].", "startOffset": 63, "endOffset": 66}, {"referenceID": 15, "context": "This approach conveniently applies to various types of interventions: perfect interventions [16], soft interventions [14], mechanism changes [22], and activity interventions [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "This approach conveniently applies to various types of interventions: perfect interventions [16], soft interventions [14], mechanism changes [22], and activity interventions [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "This approach conveniently applies to various types of interventions: perfect interventions [16], soft interventions [14], mechanism changes [22], and activity interventions [15].", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "This approach conveniently applies to various types of interventions: perfect interventions [16], soft interventions [14], mechanism changes [22], and activity interventions [15].", "startOffset": 174, "endOffset": 178}, {"referenceID": 8, "context": "For example, in combination with the method of [9] it can be used to score direct causal relations.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "Average execution time (s) n c ACI [9] 6 1 0.", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "Instances (sorted by solu&on &me) Hy/nen14 [9]", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "[10] show how this can be done for partial correlation tests assuming the distribution is multivariate Gaussian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Synthetic data We simulate the data using the simulator from [9]: for each experimental condition (e.", "startOffset": 61, "endOffset": 64}, {"referenceID": 9, "context": ", [10]) setting the significance level to \u03b1 = 0.", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "For the Bayesian weights, we use the Bayesian test for conditional independence presented in [13] as implemented by [9] with a prior probability of 0.", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "For the Bayesian weights, we use the Bayesian test for conditional independence presented in [13] as implemented by [9] with a prior probability of 0.", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "We score confidences using (16) for ACI and for the acyclic causally insufficient models presented in [9], and use the same independence tests as input, optionally up to a maximum order c.", "startOffset": 102, "endOffset": 105}, {"referenceID": 8, "context": "On 7 variables, the scoring method with ACI is almost 3 orders of magnitude faster than [9], and the difference grows exponentially with n increases.", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "On 8 variables the method from [9] is able to complete only four of the first 40 simulated models before the timeout 25\u00d7 10 s.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "Hyttinen14 [9] (c=1)", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "Hyttinen14 [9] (c=1)", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "Hyttinen14 [9] (c=1)", "startOffset": 11, "endOffset": 14}, {"referenceID": 23, "context": "use a bootstrapped version (which averages of the results of 100 executions using random subsets of half the data) of FCI [24] and Conservative FCI (CFCI) [4], as implemented in the pcalg R package [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 3, "context": "use a bootstrapped version (which averages of the results of 100 executions using random subsets of half the data) of FCI [24] and Conservative FCI (CFCI) [4], as implemented in the pcalg R package [11].", "startOffset": 155, "endOffset": 158}, {"referenceID": 10, "context": "use a bootstrapped version (which averages of the results of 100 executions using random subsets of half the data) of FCI [24] and Conservative FCI (CFCI) [4], as implemented in the pcalg R package [11].", "startOffset": 198, "endOffset": 202}, {"referenceID": 18, "context": "1 from [19].", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "For reference we also include the (unbootstrapped) performance of COMBINE [23], although the method is not designed for this setting with only observational data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 8, "context": "The performances of the scoring method with [9] and ACI coincide, performing significantly better for nonancestral predictions and the top ancestral predictions (see zoomed-in version in Figure 2(b)), even when using only as inputs independence test results up to maximum order c = 1, instead of independence test results of any order as FCI, CFCI and COMBINE.", "startOffset": 44, "endOffset": 47}, {"referenceID": 8, "context": "Nevertheless, our scoring approach (16) outperforms bootstrapped (C)FCI for both encodings [9] and ACI, suggesting that nontrivial error-correction is going on.", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "In this setting the approach from [9] is too slow.", "startOffset": 34, "endOffset": 37}, {"referenceID": 19, "context": "Application on real data We consider the challenging task of reconstructing a signalling network from flow cytometry data [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 19, "context": "In contrast with likelihood-based approaches to causal discovery that have been applied in earlier work [20, 5, 15, 18], in our approach we do not need to model the interventions quantitatively.", "startOffset": 104, "endOffset": 119}, {"referenceID": 4, "context": "In contrast with likelihood-based approaches to causal discovery that have been applied in earlier work [20, 5, 15, 18], in our approach we do not need to model the interventions quantitatively.", "startOffset": 104, "endOffset": 119}, {"referenceID": 14, "context": "In contrast with likelihood-based approaches to causal discovery that have been applied in earlier work [20, 5, 15, 18], in our approach we do not need to model the interventions quantitatively.", "startOffset": 104, "endOffset": 119}, {"referenceID": 17, "context": "In contrast with likelihood-based approaches to causal discovery that have been applied in earlier work [20, 5, 15, 18], in our approach we do not need to model the interventions quantitatively.", "startOffset": 104, "endOffset": 119}, {"referenceID": 8, "context": "We note that the method of [9] is computationally infeasible, while COMBINE assumes perfect interventions (while here we mostly have activity interventions).", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "On top of that, in the context of algorithms that aim for error-correction by exploiting redundant information, they also allow a huge computational advantage over existing edge-based representations such as [9].", "startOffset": 208, "endOffset": 211}, {"referenceID": 5, "context": "This is a strengthened version of rule R2(i) in [6]: note that the additional assumptions made there (Y 6 99K Z, Y 6 99K X) are redundant and not actually used in their proof.", "startOffset": 48, "endOffset": 51}, {"referenceID": 8, "context": "Bootstrapped (100) CFCI Bootstrapped (100) FCI Hyttinen14 [9] (c=1) ACI (c=1) CFCI FCI", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "Bootstrapped (100) CFCI Bootstrapped (100) FCI Hyttinen14 [9] (c=1) ACI (c=1) CFCI FCI", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "Bootstrapped (100) CFCI Bootstrapped (100) FCI Hyttinen14 [9] (c=1) ACI (c=1) CFCI FCI", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "The overall conclusions do not change: ACI and [9] overlap for order c = 1 and they perform better than bootstrapped (C)FCI.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "In Figure (5) we show the performance of ACI and [9] for higher order c = 4 on the same setting as Figure 2 (a-c) in the main paper.", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "As we see, the performances of ACI and [9] do not really improve with higher order but actually seem to deteriorate.", "startOffset": 39, "endOffset": 42}, {"referenceID": 19, "context": "We provide more details and more results on the real-world dataset we describe in the main paper, the flow cytometry data [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 8, "context": "Bootstrapped (100) CFCI COMBINE Bootstrapped (100) FCI Hyttinen14 [9] (c=1)", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Hyttinen14 [9] (c=4)", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "Bootstrapped (100) CFCI COMBINE Bootstrapped (100) FCI Hyttinen14 [9] (c=1)", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Hyttinen14 [9] (c=4) ACI (c=1) ACI (c=4) CFCI FCI", "startOffset": 11, "endOffset": 14}], "year": 2017, "abstractText": "Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions. Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising, existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time. Additionally, we propose a method to score causal predictions based on their confidence. Crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge. We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-ofthe-art on synthetic data, achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it on a challenging protein data set.", "creator": "LaTeX with hyperref package"}}}