{"id": "1606.07496", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?", "abstract": "While textual reviews have become prominent in many recommendation-based systems, automated frameworks to provide relevant visual cues against text reviews where pictures are not available is a new form of task confronted by data mining and machine learning researchers. Suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review. We propose a deep learning-based framework to automatically: (1) tag the images available in a review dataset, (2) generate a caption for each image that does not have one, and (3) enhance each review by recommending relevant images that might not be uploaded by the corresponding reviewer. We evaluate the proposed framework using the Yelp Challenge Dataset. While a subset of the images in this particular dataset are correctly captioned, the majority of the pictures do not have any associated text. Moreover, there is no mapping between reviews and images. Each image has a corresponding business-tag where the picture was taken, though. The overall data setting and unavailability of crucial pieces required for a mapping make the problem of recommending images for reviews a major challenge. Qualitative and quantitative evaluations indicate that our proposed framework provides high quality enhancements through automatic captioning, tagging, and recommendation for mapping reviews and images.", "histories": [["v1", "Thu, 23 Jun 2016 22:04:08 GMT  (4716kb,D)", "http://arxiv.org/abs/1606.07496v1", "10 pages, 11 figures, \"for associated results, see http://this http URL\" \"submitted to DLRS 2016 workshop\""]], "COMMENTS": "10 pages, 11 figures, \"for associated results, see http://this http URL\" \"submitted to DLRS 2016 workshop\"", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.IR cs.LG cs.NE", "authors": ["roberto camacho barranco", "laura m rodriguez", "rebecca urbina", "m shahriar hossain the university of texas at el paso)"], "accepted": false, "id": "1606.07496"}, "pdf": {"name": "1606.07496.pdf", "metadata": {"source": "CRF", "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?", "authors": ["Roberto Camacho", "Laura M. Rodriguez", "Rebecca Urbina", "Shahriar Hossain"], "emails": ["rcamachobarranco@utep.edu,", "rurbina5}@miners.utep.edu,", "mhossain@utep.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS Concepts \u2022Information systems\u2192 Data mining; Document topic models; \u2022Computing methodologies\u2192 Neural networks;\nKeywords Yelp dataset, review enhancement, recommender systems, image captioning, image classification"}, {"heading": "1. INTRODUCTION", "text": "The usefulness of a review-based website (e.g., Yelp) largely depends on the quality of the materials produced by the rePermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nviewers. The heterogeneous nature of these materials provides a tremendous possibility to enhance user experience. For example, text reviews and images shared by many reviewers can be used to create snippets for users to quickly obtain a feeling about the business. Karvonen, et al. [19] show that visually prominent UI elements, such as images, play an important role in review-based decision making. However, production of mixtures of text and images is a difficult task for a review dataset due to the use of colloquial language, incorrect captioning of images, and insufficient labels for each of the images. Moreover, the images are captured by cameras of unknown configuration in uncontrolled environments, thus making extraction of image-features and mapping the features with textual units very challenging for any kind of enhancement. We propose a framework composed of a palette of deep learning and data mining techniques to recommend images for each review, even if a review was not originally submitted with a picture. In doing so, we predict tags for each image, generate captioning phrases for these images, and finally map reviews with the most relevant images.\nWe leverage the data published by Yelp for the Yelp Dataset Challenge [39]. The scope of this paper is limited to the review and photo datasets pertaining only to restaurants. The dataset contains 25,071 restaurants, 1,363,242 reviews of these restaurants, and 98,786 images. Many of the images do not have labels or captions, which are the primary links to connect reviews with images because the dataset does not provide an existing mapping between them. Our framework consists of three main components: 1) an image classifier\nWu et al. used machine learning methods to automate image tagging. In their paper they explore and compare several di\u21b5erent methods to cluster similar images and the relevant tags. The machine learning algorithm would first identify the images and categorize them. After that, the similar images\u2019 tags were compared to tag the images in a better way. Therefore we researched more about each part of their methodology: Image similarity, object recognition and synonym extraction [7]."}, {"heading": "4.1 Image similarity", "text": "Measuring image similarity involves the complicated task of extracting low-level features (such as color, texture, shape, etc.). Liu surveys several content based image retrieval (CBIR) systems, which usually use segmentation techniques in order to identify the di\u21b5erent regions within the image. The closeness between the regions in one image and another are then measured with metrics such as the Minkowsky distance or the cosine coe cient. However, the problem with segmentation techniques, is that they are mostly ine cient if applied to natural-scene images because they do not have homogeneous colors and textures [4].\nRiu and Huang present some of the most basic similarity measures that exist, including the histogram and RGB algorithms which focus on the grayscale and color intensities, respectively, of all the pixels in an image. Another approach that has been previously explored is that of unsupervised machine learning [5]. This is the approach that we will probably rely more on for our project. Clustering algorithms are central to unsupervised machine learning. Thus, we will have to work with techniques such as k-means and NCut clustering."}, {"heading": "4.2 Object recognition", "text": "One of the goals we are trying to realize is finding similar objects in a picture in order to give better related images and tags. Unlike image similarity, which uses the attributes of photographs, object recognition is used to extract and classify objects in pictures. The usage of object recognition to classify an image has been previously studied by Li [3] which system finds objects in a photo, and comes up with a semantic label for them, in order to classify what type of\n4.3 Synonym extraction There are di\u21b5erent techniques used for synonym extraction.Wang and Hirst explored the extraction of synonyms and hyponyms using 3 di\u21b5erent methods and compared them to each other. The methods they used were: K-th nearest neighbor, dictionary graph, exploring patterns in dictionary definitions (Inverted Index Extraction). The third method was the most precise in simple environments whereas the dictionary graph using a machine learning algorithm worked best under context [6].\n5. TENTATIVE SOLUTION PLAN This project will be divided by using our team\u2019s strength to maximize our potential and e ciency. Laura and Omar have experience working with big data so the two of them will be responsible for the reviews. Roberto and Lucero have experience working with the analysis of pictures so their strengths will be maximized there. Rebecca is knowledgeable with GUI interfaces so she will be responsible for that.\n6. REFERENCES [1] S. Belongie, J. Malik, and J. Puzicha. Shape matching\nand object recognition using shape contexts. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(4):509\u2013522, 2002.\n[2] A. C. Berg, T. L. Berg, and J. Malik. Shape matching and object recognition using low distortion correspondences. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 26\u201333. IEEE, 2005.\n[3] L.-J. Li and L. Fei-Fei. What, where and who? classifying events by scene and object recognition. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1\u20138. IEEE, 2007.\n[4] Y. Liu, D. Zhang, G. Lu, and W.-Y. Ma. A survey of content-based image retrieval with high-level semantics. Pattern Recognition, 40(1):262\u2013282, 2007.\n[5] Y. Rui, T. S. Huang, and S.-F. Chang. Image retrieval: Current techniques, promising directions, and open\nar X\niv :1\n60 6.\n07 49\n6v 1\n[ cs\n.C V\n] 2\n3 Ju\nn 20\n16\nused to predict the label of each image, 2) a captioning algorithm that generates possible captions for images that were not captioned by the reviewer, and 3) a mechanism to map a review to a number of most relevant images. As an outcome of the proposed framework, we will be able to recommend images for each review as shown in Figure 1.\nThe paper contributes to the literature by describing a systematic approach to image recommendation for textual reviews, with minimal information available to create a mapping between both types. Section 2 outlines the problem and Section 3 describes the overall framework. Section 4 lists the evaluation techniques used. Section 5 provides descriptions of the experiments we performed. The related literature is described in Section 6 and we conclude the paper in Section 7. We deployed a Django-based website1 to visualize the outcomes."}, {"heading": "2. PROBLEM DESCRIPTION", "text": "Let D = {(i1, l1, c1, b1), (i2, l2, c2, b2), ..., (iN , lN , cN , bN )} be an image dataset containing N images (i), along with their corresponding labels (l), captions (c), and business id (b). A label can be a value from the following set of categories: {food, inside, outside, drink, menu, none}. A caption is expected to be a sentence but can be empty as well. Each image has exactly one business id.\nLet R = {r1, r2, ..., r|R|} be a text dataset which contains all the reviews of a specific restaurant b. We seek for a mapping M , such that given a review r \u2208 R we can select top k images (i, l, c, b) \u2208 D such that the image i is closely related to review r. To establish such a mapping, we rely on relationships between a caption c of an image and review r. Thus, a major task in the proposed framework is to generate a caption c for image i, in case one does not exist, which can then be used to relate i to review r. In turn, generating a caption c is performed using the probabilities of i belonging to the different possible categories of l. In summary, we have three subtasks: (1) generate a label for images with none label, (2) generate caption-words based on the labels and the subset of images that has captions, and (3) for each review, find a set of relevant images by topically comparing the review and the image captions."}, {"heading": "3. METHODOLOGY", "text": "Our framework solves the problem of recommending images for each review in three major steps. First, we use an image classifier to predict a label for images categorized as none. Second, we use a captioning algorithm, with the image features (obtained in the first step) and existing captions as inputs. This generates caption-words for images that do not have captions. Finally, we apply topic modeling on the reviews and captions separately to be able to create a probabilistic mapping. The following subsections describe these steps."}, {"heading": "3.1 Image classification", "text": "The first step in our framework is to categorize each of the images labeled as none to one of the following categories: food, inside, outside, drink or menu. Our preliminary data analysis reveals that more than 25% of the restaurant images are labeled as none. We use a Convolutional Neural\n1Available at: https://auto-captioning.herokuapp.com\nNetwork (CNN) image classifier to obtain class probabilities for all images in the test set (labeled as none).\nConvolutional Neural Network algorithms require that all of the images have the same dimension and are shaped as a square. We resized the images so that the smallest dimension of the image is 64 or 224 pixels, and then cropped the image in the other dimension to obtain a 64-by-64-pixel or 224-by-224-pixel image. We tested and implemented CNN models using two Python 2.7 libraries based on the Theano deep-learning library [34]: Keras [6], and Lasagne [10]. Keras and Lasagne provide high-level functions for deep learning algorithms, including convolution, pooling and fullyconnected layers, as well as backpropagation and optimization routines, whereas Theano provides the back-end of the computation and includes GPU support.\nWe used a number of different CNN models to evaluate their accuracy. One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33]. The CIFAR10 model is relatively simple, with only 11 layers. The VGG-16 model adds four convolutional layers and one fully-connected layer, which significantly increases the complexity of the model. The VGG-19 and GoogleNet models add even a larger number of layers, consisting of 19 and 22, respectively. We also used MATLABR\u00a9\u2019s Bag-of-Features with SVM classification algorithm as a baseline. We used six-fold cross validation for evaluation of all these approaches."}, {"heading": "3.2 Image captioning", "text": "We leverage a Lasagne-based implementation of the Neural Image Caption (NIC) generator [35] to predict captions for images with no caption. The NIC generator uses a special form of a recurrent neural network (RNN) called LongShort Term Memory (LSTM) network to sequentially create a fixed-dimensional vector, required due to the variable length of the input and output sentences. LSTM nets are a special type of RNN capable of learning long-term dependencies. LSTM nets apply weighted layered gates between the input, output and previous hidden states. By assigning different magnitudes to every gate (between 0 and 1), the information flow is modified so that the previous information is useful for the model, and if not, the model forgets the information. LSTM nets are able to train the gates automatically through backpropagation, obtaining a more robust\nmodel with higher accuracy [14]. The LSTM net uses information about an image as input. We obtain a lower-dimensional representation for each image using a Convolutional Neural Network (CNN). Out of the several CNN models described in Section 3.1, we chose the GoogLeNet model trained on 224-by-224 images to feed the image features to the LSTM net because of GoogLeNet\u2019s flexible compatibility with LSTM. The captioning results presented in this paper are resultant from image features generated by GoogLeNet. The complete model is outlined in Figure 2. In the figure, Si represents a word of a sentence, We represents the trained parameters, pi is a probability distribution over all the words in the vocabulary and the log pi(si) is the log-likelihood of the correct word at each step.\nCaptions are generated by maximizing the log-likelihood of the probability of obtaining the correct caption given an image. The LSTM net is trained sequentially and takes into account the image as well as all of the previously seen words to infer the next word of the output sentence. At each position of the output sentence only the word with the highest probability is selected, which has the disadvantage of not providing the globally optimal solution. The detailed sampling method required for this inference model is described in [35].\nWhile prediction of caption-words by learning relationships using existing captions seems a logical direction, we did not target the problem of correcting captions in case they are not appropriate. Based on our study, not reported in this paper, many of the captions do not describe the relevant image well. For example, a caption \u201cHurrah it\u2019s my birthday\u201d for a picture of a pasta dish only increases the noise-level rather than providing informative features. A possible solution is crowdsourcing a subset of data to obtain an accurate training set. However, this aspect is out of the scope of the current paper.\nFigure 3 shows a screenshot of our website for a picture of a glass of margarita. The suggested caption-words include margarita as well as another cocktail, Bloody Mary. This sample indicates that our proposed system was able to closely predict content of the image and map them with textual snippets."}, {"heading": "3.3 Topic modeling and review enhancement", "text": "We leverage Latent Dirichlet Allocation (LDA) [4] to model the topics of the reviews. For each review, we select the best topic and select the top t representative terms of that topic, regardless if they appear on the review or not. For each review, we recommend the top \u03c6 images based on the presence of the t representative terms in the review and in the captions of the images available for the business for which the review was written. An image is ranked higher for a particular review if a representative term is present both in the image caption and in the review, compared to an image which contains the representative term only in its caption.\nWe start by selecting images using representative terms that are present in both the review and the image caption. If \u03c6 images cannot be found, we select images for which captions contain representative terms but the review does not. This process ensures that the image selection is not solely driven by overlaps between a review and a caption, rather reviews and image captions without any overlap may become candidates for potential mapping due to the use of topical terms during the ranking. Figure 1 shows a sample of recommended images for a text review written for a burger."}, {"heading": "4. EVALUATION", "text": "We use different metrics to evaluate the quality of the results for each of the main components of the framework: image classification, image captioning, and topic modeling. For image classification, we use the top-1 accuracy, which is the percentage of test images that were classified correctly,\nas defined by Equation 1. We only use the top-1 accuracy because of the small number of possible labels.\naccuracy = images labeled correctly\ntotal images \u2217 100% (1)\nThe evaluation of the quality of the image captioning results was performed using a combination of two different metrics: Bilingual Evaluation Understudy (BLEU) and a confidence score. The BLEU method, proposed by Panineni et al. [28], computes the geometric mean of n-gram precisions. Since the training set consists mostly of short sentences (captions), we removed the brevity penalty typically used, which prevents that very short sentences have very high scores with just a few words match. We obtained a range of BLEU-1 to BLEU-4 scores using the corpus_bleu function of NLTK, a Python library focused on Natural Language Processing. The BLEU metric has some shortcomings, particularly because it does not take into account the probability with which a caption is generated.\nBecause of this limitation, we designed a metric that measures a confidence score for each generated caption. When a caption is being generated, we take the top k candidate words at each position in a sentence. For each position, we use Equation 2 to measure the non-uniformity of probabilities for the k candidates:\n\u03bd(X) = ||U( 1 k )\u2212X||1\n2\u2212 2/k (2)\nwhere X is the normalized probability distribution of the top k candidates where k > 1, and U is a uniform probability distribution over size k. Ideally, we would like a very high non-uniformity value (\u03bd(X) = 1) which means that we are very confident that the top word should be next. A uniform distribution (\u03bd(X) = 0) would indicate a random selection of top words. We compute the confidence for a generated caption with the following equation:\nConfidence(W ) =\n[\u2211m i=1(e \u03bd(v(wi))\u2217p(wi)) ] \u2212m\n[m \u2217 e1]\u2212m (3)\nwhere W is an array of m words, W = {w1, w2, ..., wm} representing the generated sentence, v(w) is the normalized truncated probability distribution of top k words for each position in the sentence in which w has the highest probability, and p(w) is the probability of the word w from the original distribution. This metric has a range of [0.0, 1.0], where higher confidence is better.\nThe evaluation of the LDA topics was performed using perplexity, which measures the model fit of an unseen set of documents, where the value decreases as a function of the log-likelihood of the holdout documents. An LDA model with lower perplexity is better. We use the following bounded definition of perplexity, presented by Hoffman, et al. [17]:\nperplexity(ntest, \u03bb, \u03b1) \u2264 exp{\u2212( \u2211 i Eq[log p(ntesti , \u03b8i, zi|\u03b1, \u03b2)]\n\u2212Eq[log q(\u03b8i, zi)])( \u2211 i,w ntestiw )}\n(4)\nWhere ntest is the total word count of the holdout documents, \u03bb is the posterior parametrization over \u03b2, \u03b1 is the Dirichlet parameter, ntesti is the word count of holdout document i, \u03b8i is a vector of topic weights for document i, zi\nis a vector of per-word topic assignments for the words in document i, \u03b2 is a dictionary of topics, q is a variational distribution which is indexed by a set of free parameters, and ntestiw is the number of times the word w appears in the holdout document i."}, {"heading": "5. EXPERIMENTS", "text": "In this section, we seek to answer the following questions about the proposed framework.\n1. How do different CNN architectures compare in terms of the accuracy of image classification? How do these results compare with a simpler method, e.g. SVM using bag-of-features? (Section 5.1)\n2. How does changing model hyperparameters for the NIC generator affect the results in terms of confidence and BLEU-4 score? What is the relationship between the confidence score and the BLEU score for the generated captions? (Section 5.2)\n3. How well does our framework generate captions for the images of the Yelp dataset? (Section 5.3)\n4. How does varying different hyperparameters affect the perplexity of the resulting LDA model? (Section 5.4)\n5. Can we relate reviews with images based on the top words obtained through topic modeling? (Section 5.5)\nWe focused on the 25,071 (out of 77,445) restaurants detected in the Yelp dataset. The restaurant image dataset contains 98,786 images with an average of 4 images per restaurant; 62% of the images do not contain any caption and 25% are labeled as none. The total number of reviews for restaurants is 1,363,242.\nDeep Neural Networks, such as CNNs and LSTM nets, require large memory and computing-power. We used two different devices for our experiments: an ASUS K501UX laptop, and the Griffin computing cluster, at the University of Texas at El Paso. The ASUS laptop has a 2.5 GHz Intel Core i7 6500U processor, 8GB memory, and NVIDIA GTX950M 914 MHz 2GB video card. Due to the low memory capacity of the NVIDIA GTX950M, the laptop was limited to training images of 64 by 64 pixels. Table 1 provides the configuration of the Griffin computing cluster."}, {"heading": "5.1 Image classification", "text": "We tested different CNN architectures to compare the accuracy obtained with each of them for the image classification problem. The architectures include: a simple CIFAR10 model (11-layer deep), VGG-16 [31] (16-layer deep), VGG19 [31] (19-layer deep) and GoogleNet [33] (22-layer deep). We also used the MATLABR\u00a9 Bag-of-Features-based SVM classifier as a baseline, to verify the gain in accuracy obtained by using a neural network versus a simpler method.\nAll of the models were used to classify the images into five different classes. The weight initialization for all the CNN models was a uniform random distribution between \u22120.5 and 0.5. Table 2 presents the configurations used and the accuracy obtained for each model. The results indicate that using neural networks improves significantly the accuracy of the image classification models. The models using 64-by-64-pixel images were trained from scratch using Keras, while the models using 224-by-224-pixel images were trained with Lasagne after initializing the model weights to the ones obtained using the ImageNet dataset. This, along with the smaller learning rate and the higher number of epochs trained, might explain the higher accuracy of the models trained with 64-by-64-pixel images.\nTable 3 presents more details about the results for the models that use images of 64 by 64 pixels. Each of these models was trained for 160 epochs. The VGG-16 model obtained 94.78% accuracy on the test data, while the CIFAR10 model obtained a 94.12% accuracy. Our observation is that the difference in the accuracy for the training data is significantly larger, and that the error on this dataset for the\nVGG-16 model is very small (0.22%). Thus, the VGG-16 model might be overfitting the training data.\nThe VGG-16 model took significantly longer to train compared to the simple model, with 54 hours against the 8 hours that the CIFAR10 model required. This is a result of the difference in the number of layers, and in particular the number of convolution operations required by each model. While the CIFAR10 model only has one convolutional layer, the VGG16 has five.\nFigure 4 shows how the accuracy for both of the models changes as the number of epochs increases. As can be seen, the CIFAR10 model outperforms the VGG-16 for the first five epochs. After the sixth iteration, the VGG-16 model appears to be consistently better than CIFAR10, albeit by a small margin. The graph also shows that the accuracy of each classifier seems to reach a steady state, but the accuracy still varies from epoch to epoch, which indicates that the training may be reduced to around 60 epochs.\nTo further evaluate the CNN model used for the image captioning component of our framework, GoogLeNet, we performed a participant-based evaluation of the predicted labels of 240 randomly selected images originally labeled as None. All these images and the predicted labels were given to one participant for evaluation. The participant was asked to mark the predicted labels as correct or incorrect and comment on anything observed. Based on the participant\u2019s comments, there were 38 images that did not belong to any of the five classes because of lack of relevance of the image contents with the labels. These 38 images include phone numbers, face images, group photos, and other irrelevant images. Out of the 202 remaining images, according to the participant, 149 images were labeled correctly (73.76%). Our observation from this study was that the CNN model did not perform as good when attempting to distinguish between inside and outside labels, particularly in dark settings. We proceed with the results obtained from GoogLeNet because this model provides a decent level of accuracy on average and is compatible with the neural image caption generator."}, {"heading": "5.2 Image captioning", "text": "The NIC generator has several hyperparameters that can affect the quality of the resulting captions: maximum sequence length (max_seq_len), batch size (batch_sz), embedding size (embedding_sz), learning rate (lr), and num-\nber of iterations (iters). To select the hyperparameters that resulted in the best confidence and BLEU-4 score, we created an initial suite of experiments setting max_seq_len to 6, batch_sz to 50 and iters to 20,000. Figure 5 depicts the results of setting embedding_sz to {128, 256, 512, 768, 1024} and lr to {0.01, 0.001, 0.0001}. The Y-axis represents the maximum value, across the iterations of one model, of adding the median of the confidence scores for all the generated sentences to the corpus-level BLEU-4 score. Thus, the hypothetical maximum value in this dimension is 2.0. In this case, it is clear that the optimal result is obtained when embedding_sz is 1,024 and lr is 0.001.\nUsing these values, we now create a new experiment to obtain the locally optimal values for max_seq_len and batch_sz, setting them to {6, 10, 15, 20} and {50, 100, 200}, respectively, while embedding_sz is set to 1,024, iters to 20,000 and lr to 0.001. The results are shown in Figure 6, which indicate that the optimal maximum sequence length is 13 and the optimal batch size is 100.\nThe BLEU-1 score obtained by our corpus of generated captions is 12.5%, while the BLEU-4 score is 6.9%. The latter value is significantly lower than that obtained using the Microsoft COCO dataset [26], 27.7%. This could be explained by the low quality of many of the captions used. Unconventional style and colloquial language in captions effect\nthe BLEU score because this score is highly dependent on the sequence of selected words. We use a confidence score, as described in Section 4, to measure the strength of each BLEU value. This allows us to isolate high quality BLEU scores. Figure 7 shows how the BLEU-n scores change with increasing confidence score of the captions. For each point in the figure, we compute the BLEU-n score using only the samples that have a confidence score greater than the value on the X-axis. The generated captions exhibit a confidence level higher than 95% to obtain a corpus-level BLEU-1 score of 70% and a BLEU-4 score of 45%. Evaluation using BLEU1 score is more appropriate for this study given that captions are generally small and frequent larger-grams are scarce."}, {"heading": "5.3 Caption evaluation", "text": "To qualitatively evaluate the generated captions, we asked five participants to manually assign a score to each generated caption for one hundred images. For each image, we presented the participant with five captions generated by the NIC generator (or less, if there were repetitions). Each set of captions was evaluated using a score on a scale of 1 to 10, with 10 meaning that all five captions have terms related to the image and 1 meaning that no caption has any term related to the image. As a baseline, the participants were asked to rate an image captioning with a score of 2 if only one caption out of five contained exactly one term related to the image.\nThe one hundred images used for this evaluation are the first hundred images that appear in our website2. This subset of images contains 40 images labeled by our CNN predictor as food, 17 images labeled as drink, 16 images as outside, 14 images as menu, and 13 images labeled as inside.\nTable 4 presents a summary of the scores provided by each of the participants. In general, the average of these scores is 5.646, and the median is 6 which indicates that, on average, at least half of the captions generated for an image have some terms related to the image content. Based on an overall calculation of all the scores provided by all participants, 73.2% of the images have a score of 3 or higher. This indicates that 73.2% of the images have at least one predicted caption containing terms related to the image. This result demonstrates high quality prediction given that there can be a tremendous amount of possible word combinations for captioning."}, {"heading": "5.4 Topic modeling", "text": "The Gensim implementation [29] of Latent Dirichlet Allocation used in our framework has the following parameters: number of topics (n_topics), words per topic, iterations(iters), \u03b1, \u03b7, \u03ba and \u03c40. For all of the experiments, we used the ten top words for each topic. As mentioned in Section 4, lower perplexity is expected in a better model. In Figure 8, we show the effect of setting \u03ba to {0.5, 0.6, 0.7, 2Available at: https://auto-captioning.herokuapp.com\n0.8, 0.9, 1.0} and \u03c40 to {1, 64, 256, 1024} on the perplexity of the LDA model, while both \u03b1 and \u03b7 are set to symmetric, n_topics is 20, and iters is 50. The results show that the locally optimal values for \u03ba and \u03c40 are 0.5 and 1, respectively.\nUsing these values for \u03ba and \u03c40, we test changing \u03b1 between {\u2018symmetric\u2019, \u2018asymmetric\u2019, \u2018auto\u2019} and \u03b7 between {\u2018symmetric\u2019, \u2018auto\u2019}, while the other hyperparameters remain the same. Figure 9 presents the effect of changing these variables, where the optimal values for \u03b1 and \u03b7 are \u2018auto\u2019 and \u2018symmetric\u2019, respectively.\nFinally, we test setting n_topics to {20, 50, 100} and iters to {50, 100, 150}, while the other hyperparameters remain the same. Figure 10 presents the effect of changing these variables, where the optimal values for n_topics and iters are 20 and 150, respectively.\nIn this section we have shown that certain hyperparameters, such as \u03ba and \u03c40, can have a significant impact on the perplexity of the model. Our observation is that choosing the wrong values for \u03ba and \u03c40 may result in a 1.45 times increase in perplexity, while a modification of \u03b1 and \u03b7 can result in a 1.2 times increase in perplexity. Changes in the number of topics and iterations demonstrate a 1.1 times increase in perplexity. Thus, a careful optimization of these parameters is required to obtain the optimal LDA model in terms of perplexity."}, {"heading": "5.5 Recommending images for reviews", "text": "In this subsection, we describe a few review-to-image recommendations obtained by our framework. We provided a sample in Figure 1 of Section 1 that illustrates that our framework was able to recommend relevant images for a review on a burger-meal. Another review, its top topical words, and recommended images are shown in Figure 11. The review was taken from Yelp\u2019s entry for the Mon Ami Gabi restaurant in Las Vegas. This example shows that the recommended set contains the images of the fountains of the Bellagio hotel across the street as described in the review.\nSummaries of five more reviews with the top three recommended images for each are provided in Table 5. The table shows that our framework is able to recommend images of main dishes, as well as outside features such as the Bellagio\nfountains, which are relevant to the reviews. Recommended images for a few thousand reviews of the Mon Ami Gabi restaurant are provided in our website3."}, {"heading": "6. RELATED WORK", "text": "Yelp introduced images in the Yelp Challenge recently. To the best of our knowledge, no previous publications have focused on enhancing Yelp reviews by recommending related images. The literature associated with the tasks involved in this paper is described below.\nThe problem of image classification has been studied for at least half a century, with initial approaches focusing on manual extraction of textural features [15]. Due to the difficulty of manual feature extraction, several automatic algorithms have been developed including histogram-based SVM classification [5], as well as pyramid matching with sparse coding [37] and locality-constrained linear coding [36]. Other methods use a bag-of-features approach, followed by a classifier such as SVM [7].\nRecently, there has been a lot of interest in deep neural networks. In the area of image classification, a surge has been observed in convolutional neural networks (CNNs). CNNs are neural networks formed by three different types of layers: convolutional layers, pooling layers and fully-connected layers. These layers can be stacked in many different ways, and research is advancing in the direction of deeper networks. Some of the most relevant CNN models include AlexNet [20], VGG [31], GoogLeNet [33] and ResNet [16].\nThe image captioning algorithm used in this paper, based on the work by Vinyals, et al. [35], combines a Convolutional Neural Network (CNN) with a special form of a Recurrent Neural Network (RNN) called Long Short-Term Memory (LSTM). Alternatives to using LSTM include primitive rule-\n3https://auto-captioning.herokuapp.com/reviewSuggest. html\nBased systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation. These are heavily hand-designed alternatives and would be too laborious to implement. Another alternative method for captioning and ranking of these captions is co-embedding images and text in the same vector space [32].\nTopic modeling has also been widely used in text mining in the past decade. Of particular interest are latent semantic indexing (LSI) [8] and probabilistic LSI (pLSI) [18], which map documents to a latent semantic space. Latent Dirichlet Allocation (LDA) [4], which is used in this work, is a probabilistic approach that generalizes pLSI. Variations of this algorithm include dynamic topic modeling [3] and online LDA [2]. Neural networks have also been used for topic modeling [24, 30]."}, {"heading": "7. CONCLUSIONS", "text": "The framework we designed to enhance Yelp reviews by recommending images requires no supervision. The training samples are gathered from the existing information pieces available with the data. A part of the proposed methodology focuses on enhancing and improving the existing data by providing additional information, i.e., categorizing images and predicting caption-words for them. One of the future directions of this work is to provide further enhancements through the use of multi-label classification where existing caption-words will be used as labels. Another future goal is to develop models to track the performance of a business and the sentiment detected in review and caption texts."}, {"heading": "8. REFERENCES", "text": "[1] A. Aker and R. Gaizauskas. Generating image\ndescriptions using dependency relational patterns. In Proc. 48th Annu. Meeting Assoc. Comput. Ling., pages 1250\u20131258. ACL, 2010.\n[2] L. AlSumait, D. Barbara\u0301, and C. Domeniconi. On-line LDA: Adaptive topic models for mining text streams with applications to topic detection and tracking. In Proc. 8th IEEE Int. Conf. Data Mining, pages 3\u201312. IEEE, Dec 2008.\n[3] D. M. Blei and J. D. Lafferty. Dynamic topic models. In Proc. 23rd Int. Conf. Mach. Learn., pages 113\u2013120. ACM, 2006.\n[4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:993\u20131022, Mar. 2003.\n[5] O. Chapelle, P. Haffner, and V. N. Vapnik. Support vector machines for histogram-based image classification. IEEE Trans. Neural Netw., 10(5):1055\u20131064, Sep 1999.\n[6] F. Chollet. Keras. https://github.com/fchollet/keras, 2016.\n[7] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray. Visual categorization with bags of keypoints. In Workshop on Statistical Learn. in Comp. Vision, volume 1, pages 1\u20132. Prague, 2004.\n[8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. Indexing by latent semantic analysis. JASIS, 41(6):391\u2013407, 1990.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 248\u2013255. IEEE, 2009.\n[10] S. Dieleman, J. Schlu\u0308ter, C. Raffel, E. Olson, S. S\u00f8nderby, D. Nouri, D. Maturana, M. Thoma, E. Battenberg, J. Kelly, et al. Lasagne: First release. Zenodo: Geneva, Switzerland, 2015.\n[11] D. Elliott and F. Keller. Image description using visual dependency representations. In Proc. Conf. Empirical Methods Natural Language Processing, pages 1292\u20131302. ACL, 2013.\n[12] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images. In Proc. 11th European Conf. Comp. Vision, pages 15\u201329. Springer, 2010.\n[13] R. Gerber and H.-H. Nagel. Knowledge representation for the generation of quantified natural language descriptions of vehicle traffic in image sequences. In Proc. Int. Conf. Image Processing, volume 1, pages 805\u2013808. IEEE, 1996.\n[14] A. Graves and J. Schmidhuber. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5):602\u2013610, 2005.\n[15] R. M. Haralick, K. Shanmugam, and I. Dinstein. Textural features for image classification. IEEE Trans. Syst. Man Cybern., SMC-3(6):610\u2013621, Nov 1973.\n[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.\n[17] M. Hoffman, F. R. Bach, and D. M. Blei. Online learning for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 23, pages 856\u2013864. Curran Associates, Inc., 2010.\n[18] T. Hofmann. Probabilistic latent semantic indexing. In Proc. 22nd Annu. Int. ACM SIGIR Conf. Res. Devel. Info. Retrieval, SIGIR \u201999, pages 50\u201357, New York, NY, USA, 1999. ACM.\n[19] K. Karvonen, T. Kilinkaridis, and O. Immonen. Widsets: A usability study of widget sharing. In Proc. Int. Conf. Human-Computer Interaction, volume 5727, pages 461\u2013464. Springer, 2009.\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc., 2012.\n[21] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. Berg. Babytalk: Understanding and generating simple image descriptions. IEEE Trans. Pattern Anal. Mach. Intell., 35(12):2891\u20132903, 2013.\n[22] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and Y. Choi. Collective generation of natural image descriptions. In Proc. 50th Annu. Meeting Assoc. Comp. Ling.: Long Papers-Volume 1, pages 359\u2013368. ACL, 2012.\n[23] P. Kuznetsova, V. Ordonez, T. L. Berg, and Y. Choi. Treetalk: Composition and compression of trees for image descriptions. Trans. ACL, 2(10):351\u2013362, 2014.\n[24] H. Larochelle and S. Lauly. A neural autoregressive topic model. In Advances in Neural Information\nProcessing Systems 25, pages 2708\u20132716. Curran Associates, Inc., 2012.\n[25] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi. Composing simple image descriptions using web-scale n-grams. In Proc. 15th Conf. Comp. Natural Language Learn., pages 220\u2013228. ACL, 2011.\n[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla\u0301r, and C. L. Zitnick. Microsoft coco: Common objects in context. In Proc. 15th European Conf. Comp. Vision, pages 740\u2013755. Springer, 2014.\n[27] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. Berg, K. Yamaguchi, T. Berg, K. Stratos, and H. Daume\u0301 III. Midge: Generating image descriptions from computer vision detections. In Proc. 13th Conf. European Chapter Assoc. Comp. Ling., pages 747\u2013756. ACL, 2012.\n[28] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a method for automatic evaluation of machine translation. In Proc. of the 40th Annu. Meeting on Assoc. for Comp. Ling., pages 311\u2013318. ACL, 2002.\n[29] R. R\u030ceh\u030aur\u030cek and P. Sojka. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May 2010. ELRA.\n[30] D. Shamanta, S. M. Naim, P. Saraf, N. Ramakrishnan, and M. S. Hossain. Concurrent Inference of Topic Models and Distributed Vector Representations, pages 441\u2013457. Springer International Publishing, Cham, 2015.\n[31] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.\n[32] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng. Grounded compositional semantics for finding and describing images with sentences. Trans. ACL, 2:207\u2013218, 2014.\n[33] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 1\u20139. IEEE, 2015.\n[34] Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May 2016.\n[35] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 3156\u20133164. IEEE, 2015.\n[36] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained linear coding for image classification. In Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 3360\u20133367. IEEE, June 2010.\n[37] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding for image classification. In Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 1794\u20131801. IEEE, June 2009.\n[38] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S. C. Zhu.\nI2T: image parsing to text description. Proc. IEEE, 98(8):1485\u20131508, 2010.\n[39] Yelp. Yelp dataset challenge. https://www.yelp.com/dataset challenge, 2016."}], "references": [{"title": "Generating image descriptions using dependency relational patterns", "author": ["A. Aker", "R. Gaizauskas"], "venue": "Proc. 48th Annu. Meeting Assoc. Comput. Ling., pages 1250\u20131258. ACL", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "On-line LDA: Adaptive topic models for mining text streams with applications to topic detection and tracking", "author": ["L. AlSumait", "D. Barbar\u00e1", "C. Domeniconi"], "venue": "In Proc. 8th IEEE Int. Conf. Data Mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "Proc. 23rd Int. Conf. Mach. Learn., pages 113\u2013120. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Support vector machines for histogram-based image classification", "author": ["O. Chapelle", "P. Haffner", "V.N. Vapnik"], "venue": "IEEE Trans. Neural Netw.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Dance", "L. Fan", "J. Willamowski", "C. Bray"], "venue": "Workshop on Statistical Learn. in Comp. Vision, volume 1, pages 1\u20132. Prague", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "JASIS, 41(6):391\u2013407", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Imagenet: A large-scale hierarchical image  database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 248\u2013255. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["S. Dieleman", "J. Schl\u00fcter", "C. Raffel", "E. Olson", "S. S\u00f8nderby", "D. Nouri", "D. Maturana", "M. Thoma", "E. Battenberg", "J. Kelly"], "venue": "Lasagne: First release. Zenodo: Geneva, Switzerland", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Image description using visual dependency representations", "author": ["D. Elliott", "F. Keller"], "venue": "Proc. Conf. Empirical Methods Natural Language Processing, pages 1292\u20131302. ACL", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "Proc. 11th European Conf. Comp. Vision, pages 15\u201329. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge representation for the generation of quantified natural language descriptions of vehicle traffic in image sequences", "author": ["R. Gerber", "H.-H. Nagel"], "venue": "Proc. Int. Conf. Image Processing, volume 1, pages 805\u2013808. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, 18(5):602\u2013610", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Textural features for image classification", "author": ["R.M. Haralick", "K. Shanmugam", "I. Dinstein"], "venue": "IEEE Trans. Syst. Man Cybern.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1973}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Online learning for latent Dirichlet allocation", "author": ["M. Hoffman", "F.R. Bach", "D.M. Blei"], "venue": "Advances in Neural Information Processing Systems 23, pages 856\u2013864. Curran Associates, Inc.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proc. 22nd Annu. Int. ACM SIGIR Conf. Res. Devel. Info. Retrieval, SIGIR \u201999, pages 50\u201357, New York, NY, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Widsets: A usability study of widget sharing", "author": ["K. Karvonen", "T. Kilinkaridis", "O. Immonen"], "venue": "Proc. Int. Conf. Human-Computer Interaction, volume 5727, pages 461\u2013464. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T. Berg"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 35(12):2891\u20132903", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "Proc. 50th Annu. Meeting Assoc. Comp. Ling.: Long Papers-Volume 1, pages 359\u2013368. ACL", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "T.L. Berg", "Y. Choi"], "venue": "Trans. ACL, 2(10):351\u2013362", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Lauly"], "venue": "Advances in Neural Information  Processing Systems 25, pages 2708\u20132716. Curran Associates, Inc.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "Proc. 15th Conf. Comp. Natural Language Learn., pages 220\u2013228. ACL", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Proc. 15th European Conf. Comp. Vision, pages 740\u2013755. Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "Proc. 13th Conf. European Chapter Assoc. Comp. Ling., pages 747\u2013756. ACL", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proc. of the 40th Annu. Meeting on Assoc. for Comp. Ling., pages 311\u2013318. ACL", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u030au\u0159ek", "P. Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Concurrent Inference of Topic Models and Distributed Vector Representations", "author": ["D. Shamanta", "S.M. Naim", "P. Saraf", "N. Ramakrishnan", "M.S. Hossain"], "venue": "pages 441\u2013457. Springer International Publishing, Cham", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "Trans. ACL, 2:207\u2013218", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 1\u20139. IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 3156\u20133164. IEEE", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "In Proc. IEEE Conf. Comp. Vision and Pattern Recognition,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In Proc. IEEE Conf. Comp. Vision and Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "I2T: image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu"], "venue": "Proc. IEEE, 98(8):1485\u20131508", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 17, "context": "[19] show that visually prominent UI elements, such as images, play an important role in review-based decision making.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Therefore we researched more about each part of their methodology: Image similarity, object recognition and synonym extraction [7].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "Another approach that has been previously explored is that of unsupervised machine learning [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "The usage of object recognition to classify an image has been previously studied by Li [3] which system finds objects in a photo, and comes up with a semantic label for them, in order to classify what type of event is happening in the photo.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "A few problems with using object recognition have been classified as correspondence problems by Berg [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "This method blurs an object from di\u21b5erent points in its shape and gives an edge channel that can then be tested for correspondence with another image [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "It takes the distance of points in a subset, an image\u2019s shape context, to create a histogram that it then uses for comparison[1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "The third method was the most precise in simple environments whereas the dictionary graph using a machine learning algorithm worked best under context [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "[1] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "7 libraries based on the Theano deep-learning library [34]: Keras [6], and Lasagne [10].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "7 libraries based on the Theano deep-learning library [34]: Keras [6], and Lasagne [10].", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 116, "endOffset": 119}, {"referenceID": 29, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 128, "endOffset": 132}, {"referenceID": 29, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 141, "endOffset": 145}, {"referenceID": 31, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 160, "endOffset": 164}, {"referenceID": 32, "context": "We leverage a Lasagne-based implementation of the Neural Image Caption (NIC) generator [35] to predict captions for images with no caption.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "model with higher accuracy [14].", "startOffset": 27, "endOffset": 31}, {"referenceID": 32, "context": "The detailed sampling method required for this inference model is described in [35].", "startOffset": 79, "endOffset": 83}, {"referenceID": 26, "context": "[28], computes the geometric mean of n-gram precisions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17]:", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The architectures include: a simple CIFAR10 model (11-layer deep), VGG-16 [31] (16-layer deep), VGG19 [31] (19-layer deep) and GoogleNet [33] (22-layer deep).", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "The architectures include: a simple CIFAR10 model (11-layer deep), VGG-16 [31] (16-layer deep), VGG19 [31] (19-layer deep) and GoogleNet [33] (22-layer deep).", "startOffset": 102, "endOffset": 106}, {"referenceID": 31, "context": "The architectures include: a simple CIFAR10 model (11-layer deep), VGG-16 [31] (16-layer deep), VGG19 [31] (19-layer deep) and GoogleNet [33] (22-layer deep).", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "The latter value is significantly lower than that obtained using the Microsoft COCO dataset [26], 27.", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "The Gensim implementation [29] of Latent Dirichlet Allocation used in our framework has the following parameters: number of topics (n_topics), words per topic, iterations(iters), \u03b1, \u03b7, \u03ba and \u03c40.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "The problem of image classification has been studied for at least half a century, with initial approaches focusing on manual extraction of textural features [15].", "startOffset": 157, "endOffset": 161}, {"referenceID": 3, "context": "Due to the difficulty of manual feature extraction, several automatic algorithms have been developed including histogram-based SVM classification [5], as well as pyramid matching with sparse coding [37] and locality-constrained linear coding [36].", "startOffset": 146, "endOffset": 149}, {"referenceID": 34, "context": "Due to the difficulty of manual feature extraction, several automatic algorithms have been developed including histogram-based SVM classification [5], as well as pyramid matching with sparse coding [37] and locality-constrained linear coding [36].", "startOffset": 198, "endOffset": 202}, {"referenceID": 33, "context": "Due to the difficulty of manual feature extraction, several automatic algorithms have been developed including histogram-based SVM classification [5], as well as pyramid matching with sparse coding [37] and locality-constrained linear coding [36].", "startOffset": 242, "endOffset": 246}, {"referenceID": 5, "context": "Other methods use a bag-of-features approach, followed by a classifier such as SVM [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 18, "context": "Some of the most relevant CNN models include AlexNet [20], VGG [31], GoogLeNet [33] and ResNet [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "Some of the most relevant CNN models include AlexNet [20], VGG [31], GoogLeNet [33] and ResNet [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "Some of the most relevant CNN models include AlexNet [20], VGG [31], GoogLeNet [33] and ResNet [16].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "Some of the most relevant CNN models include AlexNet [20], VGG [31], GoogLeNet [33] and ResNet [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "[35], combines a Convolutional Neural Network (CNN) with a special form of a Recurrent Neural Network (RNN) called Long Short-Term Memory (LSTM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 19, "endOffset": 27}, {"referenceID": 35, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 19, "endOffset": 27}, {"referenceID": 10, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 72, "endOffset": 84}, {"referenceID": 23, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 72, "endOffset": 84}, {"referenceID": 19, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 72, "endOffset": 84}, {"referenceID": 25, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 0, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 20, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 21, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 9, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 30, "context": "Another alternative method for captioning and ranking of these captions is co-embedding images and text in the same vector space [32].", "startOffset": 129, "endOffset": 133}, {"referenceID": 6, "context": "Of particular interest are latent semantic indexing (LSI) [8] and probabilistic LSI (pLSI) [18], which map documents to a latent semantic space.", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "Of particular interest are latent semantic indexing (LSI) [8] and probabilistic LSI (pLSI) [18], which map documents to a latent semantic space.", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "Variations of this algorithm include dynamic topic modeling [3] and online LDA [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "Variations of this algorithm include dynamic topic modeling [3] and online LDA [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 22, "context": "Neural networks have also been used for topic modeling [24, 30].", "startOffset": 55, "endOffset": 63}, {"referenceID": 28, "context": "Neural networks have also been used for topic modeling [24, 30].", "startOffset": 55, "endOffset": 63}], "year": 2016, "abstractText": "While textual reviews have become prominent in many recommendation-based systems, automated frameworks to provide relevant visual cues against text reviews where pictures are not available is a new form of task confronted by data mining and machine learning researchers. Suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review. We propose a deep learning-based framework to automatically: (1) tag the images available in a review dataset, (2) generate a caption for each image that does not have one, and (3) enhance each review by recommending relevant images that might not be uploaded by the corresponding reviewer. We evaluate the proposed framework using the Yelp Challenge Dataset. While a subset of the images in this particular dataset are correctly captioned, the majority of the pictures do not have any associated text. Moreover, there is no mapping between reviews and images. Each image has a corresponding business-tag where the picture was taken, though. The overall data setting and unavailability of crucial pieces required for a mapping make the problem of recommending images for reviews a major challenge. Qualitative and quantitative evaluations indicate that our proposed framework provides high quality enhancements through automatic captioning, tagging, and recommendation for mapping reviews and images. CCS Concepts \u2022Information systems\u2192 Data mining; Document topic models; \u2022Computing methodologies\u2192 Neural networks;", "creator": "LaTeX with hyperref package"}}}