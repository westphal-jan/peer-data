{"id": "1606.08270", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Evaluating Informal-Domain Word Representations With UrbanDictionary", "abstract": "Existing corpora for intrinsic evaluation are not targeted towards tasks in informal domains such as Twitter or news comment forums. We want to test whether a representation of informal words fulfills the promise of eliding explicit text normalization as a preprocessing step. One possible evaluation metric for such domains is the proximity of spelling variants. We propose how such a metric might be computed and how a spelling variant dataset can be collected using UrbanDictionary.", "histories": [["v1", "Mon, 27 Jun 2016 13:39:54 GMT  (21kb)", "http://arxiv.org/abs/1606.08270v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["naomi saphra", "adam lopez"], "accepted": false, "id": "1606.08270"}, "pdf": {"name": "1606.08270.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["n.saphra@ed.ac.uk", "alopez@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n08 27\n0v 1\n[ cs\n.C L\n] 2\n7 Ju\nn 20\n16"}, {"heading": "1 Introduction", "text": "Recent years have seen a surge of interest in training effective models for informal domains such as Twitter or discussion forums. Several new works have thus targeted social media platforms by learning word representations specific to such domains (Tang et al., 2014); (Benton et al., 2016).\nTraditional NLP techniques have often relied on text normalization methods when applied to informal domains. For example, \u201cu want 2 chill wit us 2nite\u201d may be transcribed as \u201cyou want to chill with us tonight\u201d, and the normalized transcription would be used as input for a text processing system. This method makes it easier to apply models that are successful on formal language to more informal language. However, there are several drawbacks to this method.\nBuilding an accurate text normalization component for a text processing pipeline can require substantial engineering effort and collection of manually annotated training data. Even evaluating text normalization models is a difficult problem and often subjective (Eisenstein, 2013b).\nEven when the model accurately transcribes informal spelling dialects to a standard dialect, text normalization methods may not be appropriate.\nConverting text to a style more consistent with The Wall Street Journal than Twitter may make parsing easier, but it loses much of the nuance in a persona deliberately adopted by the writer. Twitter users often express their spoken dialect through spelling, so regional and demographic information may also be lost in the process of text normalization (Eisenstein, 2013a).\nDistributional word representations hold promise to replace this flawed preprocessing step. By making the shared semantic content of spelling variants implicit in the representation of words, text processing models can be more flexible. They can extract persona or dialect information while handling the semantic or syntactic features of words (Benton et al., 2016).\nIn this proposal, we will present a method of evaluating whether a particular set of word representations can make text normalization unnecessary. Because the intrinsic evaluation we present is inexpensive and simple, it can be easily used to validate representations during training. An evaluation dataset can be collected easily from UrbanDictionary by methods we will outline."}, {"heading": "2 Evaluating By Spelling Variants", "text": "Several existing metrics for evaluating word representations assume that similar words will have similar representations in an ideal embedding space. A natural question is therefore whether a representation of words in social media text would place spelling variants of the same word close to each other. For example, while the representation of \u201cur\u201d may appear close to \u201cbabylon\u201d and \u201cmesopotamia\u201d in a formal domain like Wikipedia, on Twitter it should be closer to \u201cyour\u201d.\nWe can evaluate these representations based on the proximity of spelling variants. Given a corpus of common spelling variant pairs (one informal variant and one formal), we will accept\nor reject each word pair\u2019s relative placement in our dictionary. For example, we may consider (ur, your) to be such a pair. To evaluate this pair, we rank the words in our vocabulary by cosine-similarity to ur.\nWe could then count the pair correct if your appears in the top k most similar tokens. A similar method is common in assessing performance on analogical reasoning tasks (Mikolov et al., 2013). Having thus accepted or rejected the relationship for each pair, we can summarize our overall performance as accuracy statistic.\nThe disadvantage of this method is that performance will not be robust to vocabulary size. Adding more informal spelling variants of the same word may push the formal variant down the ranked list (for example, yr may be closer to ur than your is). However, if these new variants are not in the formal vocabulary, they should not affect the ability to elide text normalization into the representation.\nTo make the metric robust to vocabulary size, instead of ranking all tokens by similarity to the first word in the variant pair, we rank only tokens that we consider to be formal. We consider a token to be formal if it appears on a list of formal vocabulary. Such a list can be collected, for example, by including all vocabulary appearing in Wikipedia or the Wall Street Journal."}, {"heading": "3 Gathering Spelling Variants", "text": "If we have an informal text corpus, we can use it to generate a set of likely spelling variants to validate by hand. An existing unsupervised method to do so is outlined as part of the text normalization pipeline described by (Gouws et al., 2011).\nThis technique requires a formal vocabulary corpus such as Wikipedia as well as a social media corpus such as Twitter. They start by exhaustively ranking all word pairs by their distributional similarity in both Wikipedia and Twitter. The word pairs that are distributionally similar in Twitter but not in Wikipedia are considered to be candidate spelling variants. These candidates are then reranked by lexical similarity, providing a list of likely spelling variants.\nThis method is inappropriate when collecting datasets for the purpose of evaluation. When we rely on co-occurrence information in a social media corpus to identify potential spelling variants, we provide an advantage to representations\nlearned using co-occurrence information. When we rely on lexical similarity to find variants, we also offer an unfair advantage to representations that include character-level similarity as part of the model, such as (Dhingra et al., 2016).\nWe therefore collected a dataset from an independent source of spelling variants, UrbanDictionary.\nUrbanDictionary\nUrbanDictionary is a crowd-compiled dictionary of informal words and slang with over 7 million entries. We can use UrbanDictionary as a resource for identifying likely spelling variants. One advantage of this system is that UrbanDictionary will typically be independent of the corpus used for training, and therefore we will not use the same training features for evaluation.\nTo identify spelling variants on UrbanDictionary, we scrape all words and definitions from the site. In the definitions, we search for a number of common strings that signal spelling variants. To cast a very wide net, we could search for all instances of \u201cspelling\u201d and then validate a large number of results by hand. More reliably, we can search for strings like:\n\u2022 misspelling of [your]1\n\u2022 misspelling of \u201cyour\u201d\n\u2022 way of spelling [your]\n\u2022 spelling for [your]\nA cursory filter will yield thousands of definitions that follow similar templates. The word pairs extracted from these definitions can then be validated by Mechanical Turk or study participants.\nScripts for scraping and filtering UrbanDictionary are released with this proposal, along with a small sample of hand-validated word pairs selected in this way2."}, {"heading": "4 Experiments", "text": "Restricting ourselves to entries for ASCII-only words, we identified 5289 definitions on UrbanDictionary that contained the string \u201cspelling\u201d. Many entries explicitly describe a word as a spelling variant of a different \u201ccorrectly\u201d spelled word, as in the following definition of \u201cneice\u201d:\n1Brackets indicate a link to another page of definitions, in this case for \u201cyour\u201d.\n2https://github.com/nsaphra/urbandic-scraper\nspelling[\u02c6\\.,]* (\u2019|\\\"|\\[)(?P<variant>\\w+)(\\1)\nFigure 1: Regular expression to identify spelling variants.\nNeice is a common misspelling of the word niece, meaning the daughter of one\u2019s brother or sister. The correct spelling is niece.\nEven this relatively wide net misses many definitions that identify a spelling variant, including this one for \u201cdefinately\u201d:\nThe wrong way to spell definitely.\nWe extracted respelling candidates using the regular expression in Figure 1, where the group variant contains the candidate variant. We thus required the variant word to be either quoted or a link to a different word\u2019s page, in order to simplify the process of automatically extracting the informal-formal word pairs, as in the following definition of \u201csuxx\u201d:\n[Demoscene] spelling of \u201dSucks\u201d.\nWe excluded all definitions containing the word \u201cname\u201d and definitions of words that appeared less than 100 times in a 4-year sample of English tweets. This template yielded 923 candidate pairs. 7 of these pairs were people\u2019s names, and thus excluded. 760 (83%) of the remaining candidate pairs were confirmed to be informal-toformal spelling variant pairs.\nSome definitions that yielded false spelling variants using this template, with the candidate highlighted, were:\n1. recieve: The spelling bee champion of his 1st grade class above me neglected to correctly spell \u201cacquired\u201d, so it seems all of you who are reading this get a double-dose of spelling corrections.\n2. Aryan: The ancient spelling of the word \u201cIranian\u201d.\n3. moran: The correct spelling of moran when posting to [fark]\n4. mosha: . . . However, the younger generation (that were born after 1983) think it is a great word for someone who likes \u201cNu Metal\u201d And go around calling people fake moshas (or as the spelling was originally \u201cMoshers\u201d.\nMost of the false spelling variants were linked to commentary about usage, such as descriptions of the typical speaker (e.g., \u201cironic\u201d) or domains (e.g., \u201cYouTube\u201d or \u201cFark\u201d).\nWhen using the word pairs to evaluate trained embeddings, we excluded examples where the second word in the pair was not on a formal vocabulary list (e.g., \u201dEonnie\u201d, a word borrowed from Korean meaning \u201dbig sister\u201d, was mapped to an alternative transcription, \u201dunni\u201d)."}, {"heading": "4.1 Filtering by a Formal Vocabulary List", "text": "Some tokens which UrbanDictionary considers worth mapping to may not appear in the formal corpus. For example, UrbanDictionary considers the top definition of \u201cbraj\u201d to be:\nPronounced how it is spelled. Means bro, or dude. Developed over numerous times of misspelling [brah] over texts and online chats.\nBoth \u201cbraj\u201d and \u201cbrah\u201d are spelling variants of \u201cbro\u201d, itself an abbreviation of \u201cbrother\u201d. If we extract (braj, brah) as a potential spelling pair based on this definition, we cannot evaluate it if brah does not appear in the formal corpus. Representations of these words should probably reflect their similarity, but using the method described in Section 2, we cannot evaluate spelling pairs of two informal words.\nUsing a vocabulary list compiled from English Wikipedia, we removed 140 (18%) of the remaining pairs. Our final set of word pairs contained 620 examples."}, {"heading": "4.2 Results on GloVe", "text": "As a test, we performed an evaluation on embeddings trained with GloVe (Pennington et al., 2014) on a 121GB English Twitter corpus. We used a formal vocabulary list based on English Wikipedia. We found that 146 (24%) of the informal word representations from the word pairs in our dataset had the target formal word in the top 20 most similar formal words from the vocabulary. Only 70 (11%) of the informal word representations had the target formal word as the most similar formal word.\nThe word pairs with representations that appeared far apart often featured an informal word that appeared closer to words that were related by topic, but not similar in meaning. The representation of \u201corgasim\u201d was closer to a number of medical terms, including \u201cabscess\u201d, \u201chysterectomy\u201d, \u201chematoma\u201d, and \u201ccochlear\u201d, than it was to \u201corgasm\u201d.\nOther word pairs were penalized when the \u201cformal\u201d vocabulary list failed to filter out informal words that appeared in the same online dialect. The five closest \u201cformal\u201d words to \u201cqurl\u201d (\u201cgirl\u201d), which were \u201ccoot\u201d, \u201cdht\u201d, \u201caaw\u201d, \u201cluff\u201d, and \u201co.k\u201d.\nStill other word pairs were counted as wrong, but were in fact polysemous. The representation of \u201ctarp\u201d did not appear close to \u201ctrap\u201d, which was its formal spelling according to UrbanDictionary. Instead, the closest formal word was \u201ctarpaulin\u201d, which is commonly abbreviated as \u201ctarp\u201d.\nThese results suggest that current systems based exclusively on distributional similarity may be insufficient for the task of representing informaldomain words."}, {"heading": "5 Biases and Drawbacks", "text": "Evaluating performance on spelling variant pairs could predict performance on a number of tasks that are typically solved with a text normalization step in the system pipeline. In a task like sentiment analysis, however, the denotation of the word is not the only source of information. For example, a writer may use more casual spelling to convey sarcasm:\nI see women who support Trump or Brock Turner and I\u2019m like \u201cwow u r such a good example for ur daughter lol not poor bitch\u201d (Twitter, 18 Jun 2016)\nor whimsy:\n*taking a personalitey test* ugh i knew i shoud have studied harder for this (Twitter, 6 Jun 2016)\nAn intrinsic measure of spelling variant similarity will not address these aspects.\nSome of the disadvantages of metrics based on cosine similarity, as discussed in Faruqui et al. (2016), apply here as well. In particular, we do not know if performance would correlate well with extrinsic metrics; we do not\naccount for the role of word frequency in cosine similarity; and we cannot handle polysemy. Novel issues of polysemy also emerge in cases such as \u201ctarp\u201d; \u201cwit\u201d, which represents either cleverness or a spelling variant of \u201cwith\u201d; and \u201cur\u201d, which maps to both \u201cyour\u201d and \u201cyou are\u201d.\nHowever, compared to similarity scores in general (Gladkova and Drozd, 2016), spelling variant pairs are less subjective."}, {"heading": "6 Conclusions", "text": "The heuristics used to collect the small dataset released with this paper were restrictive. It is possible to collect more spelling variant pairs by choosing more common patterns (such as the over 5000 entries containing the string \u201cspelling\u201d) to pick candidate definitions. We could then use more complex rules, a learned model, or human participants to extract the spelling variants from the definitions. However, the simplicity of our system, which requires minimal human labor, makes it a practical option for evaluating specialized word embeddings for social media text.\nOur experiments with GloVe indicate that models based only on the distributional similarity of words may be limited in their ability to represent the semantics of online speech. Some recent work has learned representations of embeddings for Twitter using character sequences as well as distributional information (Dhingra et al., 2016); (Vosoughi et al., 2016). These models should have a significant advantage in any metric relying on spelling variants, which are likely to exhibit character-level similarity."}], "references": [{"title": "Learning multiview embeddings of twitter", "author": ["Benton et al.2016] Adrian Benton", "Raman Arora", "Mark Dredze"], "venue": null, "citeRegEx": "Benton et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Benton et al\\.", "year": 2016}, {"title": "Tweet2vec: Character-based distributed representations for social media", "author": ["Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W Cohen"], "venue": "In Proceedings of ACL", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Phonological factors in social media writing", "author": ["Jacob Eisenstein"], "venue": "In Proc. of the Workshop on Language Analysis in Social Media,", "citeRegEx": "Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "What to do about bad language on the internet", "author": ["Jacob Eisenstein"], "venue": "In HLT-NAACL,", "citeRegEx": "Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "Problems with evaluation of word embeddings using word similarity tasks. In RepEval", "author": ["Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Intrinsic evaluations of word embeddings: What can we do better? In RepEval", "author": ["Gladkova", "Drozd2016] Anna Gladkova", "Aleksandr Drozd"], "venue": null, "citeRegEx": "Gladkova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gladkova et al\\.", "year": 2016}, {"title": "Unsupervised mining of lexical variants from noisy text", "author": ["Gouws et al.2011] Stephan Gouws", "Dirk Hovy", "Donald Metzler"], "venue": "In Proceedings of the First Workshop on Unsupervised Learning in NLP,", "citeRegEx": "Gouws et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["Tang et al.2014] Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": "ACL", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Tweet2vec: Learning tweet embeddings using character-level cnn-lstm encoder-decoder", "author": ["Prashanth Vijayaraghavan", "Deb Roy"], "venue": null, "citeRegEx": "Vosoughi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vosoughi et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Several new works have thus targeted social media platforms by learning word representations specific to such domains (Tang et al., 2014); (Benton et al.", "startOffset": 118, "endOffset": 137}, {"referenceID": 0, "context": ", 2014); (Benton et al., 2016).", "startOffset": 9, "endOffset": 30}, {"referenceID": 0, "context": "They can extract persona or dialect information while handling the semantic or syntactic features of words (Benton et al., 2016).", "startOffset": 107, "endOffset": 128}, {"referenceID": 7, "context": "A similar method is common in assessing performance on analogical reasoning tasks (Mikolov et al., 2013).", "startOffset": 82, "endOffset": 104}, {"referenceID": 6, "context": "pipeline described by (Gouws et al., 2011).", "startOffset": 22, "endOffset": 42}, {"referenceID": 1, "context": "When we rely on lexical similarity to find variants, we also offer an unfair advantage to representations that include character-level similarity as part of the model, such as (Dhingra et al., 2016).", "startOffset": 176, "endOffset": 198}, {"referenceID": 8, "context": "As a test, we performed an evaluation on embeddings trained with GloVe (Pennington et al., 2014) on a 121GB English Twitter corpus.", "startOffset": 71, "endOffset": 96}, {"referenceID": 4, "context": "based on cosine similarity, as discussed in Faruqui et al. (2016), apply here as well.", "startOffset": 44, "endOffset": 66}, {"referenceID": 1, "context": "Some recent work has learned representations of embeddings for Twitter using character sequences as well as distributional information (Dhingra et al., 2016); (Vosoughi et al.", "startOffset": 135, "endOffset": 157}, {"referenceID": 10, "context": ", 2016); (Vosoughi et al., 2016).", "startOffset": 9, "endOffset": 32}], "year": 2016, "abstractText": "Existing corpora for intrinsic evaluation are not targeted towards tasks in informal domains such as Twitter or news comment forums. We want to test whether a representation of informal words fulfills the promise of eliding explicit text normalization as a preprocessing step. One possible evaluation metric for such domains is the proximity of spelling variants. We propose how such a metric might be computed and how a spelling variant dataset can be collected using UrbanDictionary.", "creator": "LaTeX with hyperref package"}}}