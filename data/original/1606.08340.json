{"id": "1606.08340", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "Topic Aware Neural Response Generation", "abstract": "We consider incorporating topic information as prior knowledge into the sequence to sequence (Seq2Seq) network structure with attention mechanism for response generation in chatbots. To this end, we propose a topic augmented joint attention based Seq2Seq (TAJA-Seq2Seq) model. In TAJA-Seq2Seq, information from input posts and information from topics related to the posts are simultaneously embedded into vector spaces by a content encoder and a topic encoder respectively. The two kinds of information interact with each other and help calibrate weights of each other in the joint attention mechanism in TAJA2Seq2Seq, and jointly determine the generation of responses in decoding. The model simulates how people behave in conversation and can generate well-focused and informative responses with the help of topic information. Empirical study on large scale human judged generation results show that our model outperforms Seq2Seq with attention on both response quality and diversity.", "histories": [["v1", "Tue, 21 Jun 2016 05:47:59 GMT  (341kb)", "http://arxiv.org/abs/1606.08340v1", null], ["v2", "Mon, 19 Sep 2016 02:09:13 GMT  (637kb,D)", "http://arxiv.org/abs/1606.08340v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chen xing", "wei wu", "yu wu", "jie liu 0007", "yalou huang", "ming zhou", "wei-ying ma"], "accepted": true, "id": "1606.08340"}, "pdf": {"name": "1606.08340.pdf", "metadata": {"source": "CRF", "title": "Topic Augmented Neural Response Generation with a Joint Attention Mechanism", "authors": ["Chen Xing", "Jie Liu", "Yalou Huang"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n08 34\n0v 1\n[ cs\n.C L\n] 2\n1 Ju\n1"}, {"heading": "1 Introduction", "text": "Chatbots, which are designed for natural and human-like conversation with people in open domains, have become hot in recent years. Many companies include Microsoft, Apple, Facebook and Google have released their chatbot products. The core of a chatbot is a response generation engine. In recent two years, with the success of long shortterm memory recurrent neural network (LSTMRNN) (Hochreiter and Schmidhuber, 1997) in capturing long-term information in sequences, a lot of\neffort on building a response generator has been paid to neural network models. A popular network structure for response generation is the encoder-decoder structure (Sutskever et al., 2014; Shang et al., 2015) or more commonly referred to as \u201csequence to sequence\u201d (Seq2Seq) model. In Seq2Seq, an input post is encoded by the encoder as a context vector. Then, a language model like decoder decodes the semantic information in the vector and generates final responses. On top of Seq2Seq, attention mechanism (Bahdanau et al., 2014; Cho et al., 2015) which is first proposed for machine translation (MT), is then added to further improve generation quality. In Seq2Seq with attention, different words in decoding are generated from different context vectors. Each context vector is a linear combination of the hidden states of the encoder with weights reflecting importance of different parts of the input post. These weights are distinct for every corresponding word in the generated response. These structures have achieved great success on response generation and outperforms traditional retrieval based approaches (Shang et al., 2015) to a great deal.\nIn this paper, we consider incorporating topic information as prior knowledge into Seq2Seq with attention for response generation in chatbots. The idea is inspired by our observation on conversation between humans. In human-human conversation, people often associate an input post with topically related concepts and create their responses according to these concepts. For example, to respond to \u201cI\u2019ve watched Orphan\u201d 1, people who know the movie may think it is a thriller, horrible and fright-\n1Orphan is a movie https://en.wikipedia.org/wiki/Orphan_(film)\n2 ening. Then based on this knowledge, they may give more targeted and well-focused responses like \u201dit is a thriller\u201d or \u201dthis movie is so frightening\u201d, instead of plain responses like \u201dI haven\u2019t watched it\u201d or \u201dI don\u2019t know\u201d. \u201cThriller\u201d, \u201chorrible\u201d, and \u201cfrightening\u201d are concepts under the topic of the post. They represent people\u2019s prior knowledge regarding to the input post, and help them form their responses. We would like to model this process of response generation with topics and use topics to enhance the performance of Seq2Seq with attention. We propose a topic augmented joint attention based Seq2Seq (TAJA-Seq2Seq) model to leverage topic information in response generation. TAJASeq2Seq is equipped with two encoders, and each one has an attention module. The first encoder sequentially represents different parts of an input post as content vectors, and the second encoder compresses various topic words to topic vectors as representations of topic information. Both the content vectors and the topic vectors are averaged with weights that are calculated by the attention modules, and fed to the decoder as context vectors to jointly determine the response generation. In TAJASeq2Seq, information from the input post and information from the topic words interact with each other in the joint attention modules. Specifically, in the attention module of the content encoder, a topic vector obtained from a topic summarizer is taken as an extra input in the learning of the weights of the content vectors. In the attention module of the topic encoder, the final state of the content encoder is used to learn the weights of the topic vectors. By this means, the topic information acts as prior knowledge and helps calibrate the content emphasis in the input post which the response should focus on, and the content information helps select semantically relevant words from topic information that the response should be related to. We obtain the topic words from a pre-trained Twitter LDA model. Empirical study on large scale human judged generation results show that with topic information as prior knowledge, TAJA-Seq2Seq can generate more informative, diverse, and topically relevant responses than the traditional Seq2Seq model with attention. The contributions of this paper includes, 1) proposal of using topic information as prior knowledge for response generation; 2) proposal of a TAJASeq2Seq model that naturally incorporates topic information into the encoder-decoder structure; 3) empirical verification of the effectiveness of TAJASeq2Seq."}, {"heading": "2 Background: sequence-to-sequence model and attention mechanism", "text": "Before introducing our model, let us first briefly review the Seq2Seq model and the attention mechanism."}, {"heading": "2.1 Sequence-to-sequence model", "text": "In Seq2Seq, given a source sequence (post) X = (x1, x2, . . . , xT ) and a target sequence (response) Y = (y1, y2, . . . , yT \u2032), the model maximizes the generation probability of Y conditioned on X: p(y1, ..., yT \u2032 |x1, ..., xT ). Specifically, the Seq2Seq model is in an encoder-decoder structure. The encoder reads X word by word and represents it as a context vector c through a recurrent neural network (RNN), and then the decoder estimates the generation probability of Y with c as input. The objective function of Seq2Seq can be written as p(y1, ..., yT \u2032 |x1, ..., xT ) = T \u2032\u220f t=1 p(yt|c, y1, ..., yt\u22121). (1) The encoder RNN calculates the context vector c by ht = f(xt,ht\u22121); c = hT , (2) where ht is the hidden state at time t and f is a non-linear transformation which can be either an long-short term memory unit (LSTM) (Hochreiter and Schmidhuber, 1997) or a gated recurrent unit (GRU) (Cho et al., 2014). In this paper, we use LSTM as an implementation of f . The specific parametrization of LSTM is it = \u03c3(Wxixt +Whiht\u22121) ft = \u03c3(Wxfxt +Whfht\u22121) ot = \u03c3(Wxoxt +Whoht\u22121) c\u0302t = tanh(Wxcxt +Whcht\u22121) ct = ft \u2299 ct\u22121 + it \u2299 c\u0302t ht = ot \u2299 tanh(ct), (3) where it, ft,ot are the input gate, forget gate and output gate respectively and c\u0302t, ct are proposed and true cell values.\n3 The decoder is a standard RNN language model(Mikolov et al., 2010) except conditioned on the context vector c. The probability distribution pt of candidate words at every time t is calculated as st = f(yt\u22121, st\u22121, c);pt = softmax(st, yt\u22121) (4) where st is the hidden state of the decoder RNN at time t and yt\u22121 is the word at time t \u2212 1 in the response sequence."}, {"heading": "2.2 Attention mechanism", "text": "The traditional Seq2Seq model assumes that every word is generated from the same context vector. In practice, however, different words in Y could be semantically related to different parts of X. To solve this problem, attention mechanism (Bahdanau et al., 2014) which is first proposed for MT, is added to Seq2Seq for response generation(Shang et al., 2015). In Seq2Seq with attention, each yi in Y corresponds to a context vector ci, and ci is a weighted average of all hidden states {ht} T t=1. Formally, ci is defined as ci = \u03a3 T j=1\u03b1ijhj, (5) where \u03b1ij is given by \u03b1ij = exp(eij) \u03a3T k=1 exp(eik) ; eij = \u03b7(si\u22121,hj) (6) \u03b7 is usually implemented as a multi-layer perceptron (MLP)."}, {"heading": "3 Topic augmented joint attention based Seq2Seq", "text": "We propose a topic augmented joint attention based Seq2Seq (TAJA-Seq2Seq) model to incorporate topic information as prior knowledge into the process of response generation. In the following sections, we first show how we obtain the topic information related to the contents of posts, then we describe details of the network structure of TAJASeq2Seq and show how the topic information works in the structure."}, {"heading": "3.1 Topic acquisition", "text": "We obtain a topic word list for each input post from a Twitter LDA model (Zhao et al., 2011).\nTwitter LDA belongs to the family of probabilistic topic models (Blei et al., 2003) and represents the state-of-the-art topic model for short texts (Zhao et al., 2011). We choose Twitter LDA among various probabilistic topic models because in conversation data, the input posts are short, informal, and contain quite a lot of scattered topics. These characteristics are similar with those of Twitter data and meet the assumptions of Twitter LDA quite well. The basic assumption of Twitter LDA is that each post corresponds to only one topic. Each word in the post is either a background word or a topic word under the topic of the post. Specifically, Twitter LDA first draws a mutlinomial distribution \u03b8 from a Dirichlet prior Dir(\u03b1) that represents the topic distribution of the whole data set. Second it draws P multinomial distributions {\u03c6p}Pp=1 from Dir(\u03b2). They model the word distributions for the P topics. Finally a Bernoulli distribution \u03c0 from Dir(\u03b3) and another multinomial distribution \u03c6B from Dir(\u03b2) are set to model the existence of background words. Given an input message m, the model then draws a topic zm based on \u03b8. For the l-th word wm,l in m, an indicator Ym,l is first sampled from \u03c0. If Ym,l = 1, then wm,l is a topic word and is sampled from \u03c6zm ; otherwise, wm,l would be treated as a background word and sampled from \u03c6B . Figure 1 gives the graphical model of Twitter LDA.\nIn training, we concatenate the post and the response of each sample to form a short document and employ the collapsed Gibbs sampling algorithm (Zhao et al., 2011) to estimate the parameters. After we get the estimations of the parameters, we use them to assign a topic p to each post X by the generation process described above. Then, we pick top n\n4 words with the highest probabilities in \u03c6p for topic p as the topic word list. Each word w in the topic word list corresponds to a topic distribution which is calculated by Equation (7), where Cwp is the number of times that w is assigned to topic p in training. These topic distributions will be used as topic vectors in TAJA-Seq2Seq. p(p|w) \u221d Cwp\u2211 p\u2032 Cwp\u2032 . (7)"}, {"heading": "3.2 Structure of TAJA-Seq2Seq network", "text": "The data format of TAJA-Seq2Seq is (Kp,X,Y), where p is the topic assigned to X and Kp = (kp 1 ,kp 2 , ...,kpn) are the topic words of X. k p j represents the topic vector of the j-th word calculated using Equation (7). Figure 2 gives the structure of our topic augmented joint attention based Seq2Seq model (TAJA-Seq2Seq) for response generation. TAJA-Seq2Seq has a content encoder and a topic encoder. The content encoder embeds an input post from both ends into vector space with a bidirectional LSTM-RNN. The topic encoder obtains the vectors of the topic words of the input post by looking up the topic vector table. In addition to the two encoders, TAJA-Seq2Seq also contains a topic summarizer which transforms the vectors of topic words to a single vector q with an MLP. On top of each encoder, there is an attention module. For the i-th word in the decoder, the content attention module takes the former hidden state of the decoder si\u22121, the hidden states of the content encoder {ht}Tt=1, and q from the topic summarizer as input, and calculate combination weights of {ht}Tt=1. Similarly, in the topic attention module, for the i-th word in the decoder, the weights for topic vectors {kpj} n j=1 are calculated according to the former hidden state of the decoder si\u22121, the topic vector themselves, and the final state of the content encoder hT . Here, q encodes the topic information and helps calibrate the content emphasis in the input post, and hT encodes the content information and helps select topic words that are more relevant to the i-th word in the generated response. Different from the attention module in Seq2Seq, topic information and content information determine the combination weights of each other in the joint attention mechanism and make more accurate decisions on the semantic focus of every re-\nsponse word. Specifically, \u2200i in the decoder, the content attention module calculates unnormalized weights Eic = (eic1, e i c2, . . . , e i cT ) for {ht} T t=1. e i ct corresponds to ht and is calculated as eict = \u03b7c(si\u22121,ht, q), (8) where \u03b7c is an MLP with tanh as the activation function and q is the output of the topic summarizer. The topic summarizer concatenates (kp 1 , kp 2 , . . . , kpn) as a vector q\u2032, then it calculates q as q = tanh(Ws \u00b7 q \u2032 + bs), (9) where Ws and bs are parameters. The topic attention module calculates unnormalized weights Eio = (eio1, e i o2, ..., e i on) for (k p 1 , kp 2 , ..., kpn). \u2200j, eioj is represented as eioj = \u03b7o(si\u22121,k p j ,hT ). (10) Both Eic and E i o are further normalized with a softmax function as described in Equation (6). The decoder of TAJA-Seq2Seq is an RNN language model with the output of the content attention module and the output of the topic attention module as input. Specifically, for step i, the calculation details of the decoder are given by si = f(yi\u22121, si\u22121, ci,oi) pi = softmax(si, yi\u22121), (11)\n5\nwhere ci and oi are the output of the content attention module and the topic attention module respectively, and pi is the probability distribution of the ith word in response generation. Figure 3 shows how the two attention modules work with the decoder. We use beam search(Steinbiss et al., 1994) to generate responses using the trained model. We publish the Python code of TAJA-Seq2Seq in Github2. TAJA-Seq2Seq makes use of the topic information provided by Twitter LDA as prior knowledge to enhance the performance of response generation. Since topic keywords of a post connect it with other semantic related responses under the same topic during training, the topic keywords mainly play two roles, classification and association, during the generation of the response. On the one hand, some of the topic keywords, like \u201cthriller\u201d for post \u201cI\u2019ve watched The Orphan\u201d , classify the post to a specific semantic group (such as descriptions of thrillers ) and relate it to many candidate responses in this topic group, hence makes the model to generate more targeted and well-focused responses such as \u201cThis movie is so disgusting\u201d. On the other hand some topic words like \u201chigh-heels\u201d for post \u201cYou are so tall\u201d, extend the semantic meaning of posts and can lead to more diverse and novel responses like \u201cI am not tall. This is because of my high-heels\u201d. Moreover, in TAJA-Seq2Seq, content vectors and 2https://github.com/LynetteXing1991/TAJA-Seq2Seq topical vectors jointly affect the weight learning of each other in the joint attention module during the generation of each word in the response sequence. This simulates how people behave in conversation. When reacting to an input post, people find relevant information from their prior knowledge according to the content of the post, let the information help them determine the important parts in the post that they want to focus, and use both the information from prior knowledge and the main content focus from the post to create their reply. In our model, topic attention simulates the process of relevant information finding and content attention simulates the determination of the content focus. TAJA-Seq2Seq also helps make better choice on the first word in the response. First words matter much because the decoder contains a language model part to guarantee the fluency of the generated responses. If the first word is wrongly chosen, then the whole sentence will never have a chance to go back to a proper meaning. In traditional attention based Seq2Seq, the weights of {ht}Tt=1 of the first word in response only depend on themselves since there is no si\u22121 when i equals to 0. In TAJASeq2Seq, for the two attention modules, the weights of {ht}Tt=1 and {k p j} n j=1 for the first decoder state depend not only on themselves, but also on the topic vector q and the content vector hT . Topics provide extra information for a better choice of the first word in response generation.\n6"}, {"heading": "4 Experiments", "text": "We evaluate TAJA-Seq2Seq from different perspectives and compare the model with a series of encoder-decoder based models using real-world data."}, {"heading": "4.1 Experiment setup", "text": "Data sets we crawled 5, 671, 846 post-response pairs from Sina Weibo3. We then use Stanford Chinese word segmenter4 to tokenize post-response pairs as inputs of all models. Pairs with posts or responses longer than 50 tokens are removed and we also remove responses with frequency higher than 10 since they would dominate the generated results if they are included in training data. 1, 147, 758 distinct post-response pairs are left, and we use them to build the training data set D. We then separately construct vocabulary sets for posts and responses. The sizes of two vocabulary sets are both 20, 000. The post vocabulary covers 98.8% words that appear in posts and the response vocabulary covers 98.3% in responses. Other words are treated as \u201cUNK\u201d in training. Our test data contains 154 randomly sampled posts which are not in D. We follow the same procedure as the training set D to construct the test set. Parameter tuning in Twitter LDA, we set the number of topics T as 200 and \u03b1 = 1/T , \u03b2 = 0.01, \u03b3 = 0.01. For each topic, we set the number of topic words n in the topic word list as 10. In all Seq2Seq based models, including our TAJA-Seq2Seq and all other baseline models, we set the dimensions of the hidden states of the encoder and the decoder as 1000, and the dimensions of all word embeddings as 620. We initialize the parameters of the Seq2Seq based models using isotropic Gaussian distributions and then use AdaDelta algorithm (Zeiler, 2012) to train the models on a NVIDIA Tesla K40 GPU."}, {"heading": "4.2 Baselines", "text": "We compare our model with three baselines. All of the models are implemented with an open source deep learning tool, Blocks5. The three baselines are as follows, 3www.sina.com 4http://nlp.stanford.edu/software/segmenter.shtml 5https://github.com/mila-udem/blocks TA-Seq2Seq: this is a simplified version of TAJASeq2Seq in which we remove the topic encoder and topic attention module, and only leave the content encoder, content attention and the topic summarizer. S2SA: standard Seq2Seq model with attention mechanism which is firstly proposed in (Bahdanau et al., 2014) for machine translation. We implement it with a standard bi-directional encoder and an MLP as the attention network. S2SA-NMI: Seq2Seq model with a NMI objective function proposed in (Li et al., 2015) to improve the diversity of the generated results. Since NMI-bidi performs better among the two models in (Li et al., 2015) according to the existing work, we select NMI-bidi as a baseline. We do not compare our model with Seq2Seq without attention and the traditional retrievalbased methods because it has been proven in (Shang et al., 2015) that S2SA outperforms these two methods and represents the state-of-the-art in response generation."}, {"heading": "4.3 Evaluation Methods", "text": "Human Annotation: we recruit human annotators to evaluate the quality of the generated responses of different models. For every post, we show top 10 generated responses from every model. 4 labelers with Weibo experience are asked to judge the quality of the responses. Query-response pairs from different models are randomly shuffled and split to 4 parts. The labeling criteria is +2: The response is relevant, natural, wellfocused and informative. +1: The response is fair to answer this post, but its relevance with the post is not very strong. For example, some responses are general and can be used to respond to many posts, like \u201cYes, I see\u201d , \u201cWow\u201d and \u201cNo I don\u2019t know\u201d. 0: The response cannot be used to answer this post, semantically irrelevant or disfluent. -1: The post is hard to understand so that the quality of the response cannot be judged. We set the \u22121 score because some posts in Sina Weibo need special background knowledge to understand. D-score: we use D-score to evaluate the diversity of the generated responses. For every post p, we pick responses labeled +1 and +2 from the judged\n7\n10 responses to construct a candidate list Rp = (rp 1 , rp 2 , ..., rpy), in which y is the total number of proper generated responses of post p. Given the candidates of all posts, we calculate D-score as follows, Dscore = 1 P \u2217 y \u03a3p\u03a3 y i=1drpi (12) where drp i is the number of distinct words in rpi and P is the number of posts. D-score represents the average number of distinct words in generated responses with good quality, and thus to some extent reflects the diversity of the generated responses of every model."}, {"heading": "4.4 Results", "text": "For human annotation results, it is clear from Table 1 that TAJA-Seq2Seq and TA-Seq2Seq generate much more high-quality responses (responses labeled as +2) than other baselines, outperform 6% and 13.4% respectively. We conducted sign test, and the results show that the improvement is statistically significant (with p-value < 0.01). The total numbers of positive responses (responses labeled as +1 or +2) for all 4 models are relatively consistent and are all around 70%, while TAJA-Seq2Seq and TASeq2Seq decrease the number of fair and plain responses and increase the number of natural and interesting ones. This supports our claim on the advantage of our model, that is by leveraging topic information as prior knowledge, our model could involve various relevant information to enrich generated responses and make them more informative and diverse. An interesting phenomenon observed from the human annotation results is that the TA-Seq2Seq model contributes half of the improvement of the whole model. More precisely, the topic summarizer that helps calculate the weights of the context vectors can make considerable improvement itself. It indicates that topic information is really useful for response generation even in a simple way. For D-score, we can see from Table 1 that TAJASeq2Seq has more distinct words in its good responses and outperforms traditional S2SA models to 4 points. The performance of models on diversity is consistent with that on quality in Table 1."}, {"heading": "4.5 Case Study", "text": "In Table 2, we list some cases to show the quality of the generated responses and the effectiveness of topic information as prior knowledge. For every post, we list the top 3 topic words as showcase of topic information. From the table, the advantages of TAJA-Seq2Seq over the traditional S2SA can be summarized as: 1) TAJA-Seq2Seq connects the post with a topic that generally and accurately describes the characteristics of the post, like case 2,3 and 5. In case 2 TAJA-Seq2Seq categorizes the movie \u201cThe Orphan\u201d directly into the \u201cthriller\u201d topic thus the generated responses are very targeted and wellfocused on the characteristics of thrillers. On the other hand, responses from traditional S2SA fails to capture these characteristics and gives too general responses related to \u201cwatch movies\u201d; 2) TAJASeq2Seq associates the post with other relevant elements, and makes the responses more informative and diverse. For example, in case 4, TAJA-Seq2Seq associates \u201ctall\u201d with relevant elements like \u201chigh heels\u201d, and can generate interesting responses like \u201cNo this is because of my high heels\u201d."}, {"heading": "5 Related work", "text": "Besides traditional attention module introduced in background knowledge section, there are also other encoder-decoder based models aiming at improving the quality of generated responses from various perspectives. Similar with TAJA-Seq2Seq, there are encoder-decoder models that take the benefits of extra informations. For involving context information into one-round chat, A. Sordoni(Sordoni et al., 2015) proposed a encoder-\n8\ndecoder based model, the encoder of which is a two-part feed-forward network that concludes context and post information separately and fed them both to the decoder for generating responses. To keep speaker information consistent during chat, J. Li (Li et al., 2016) added personal information from knowledge base as an extra input of Seq2Seq and concatenate it with the word embedding for every time step. Before TAJA-Seq2Seq, there are also works aiming at simulating humans\u2019 behaviour during chat to improve chat machine. J. Gu(Gu et al., 2016) proposed copying mechanism to mimic the repeating phenomenon during human conversation. K. Yao (Yao et al., 2015) added an extra RNN between the encoder and the decoder with attention to conclude and explain intentions from posts. Moreover, topic information has been used in similar tasks and has been proven effective. S. Ghosh(Ghosh et al., 2016) proposed Contextual LSTM that involved topic information to represent contexts and concatenated topic information with the word embedding for every time step. CLSTM improved 3 NLP tasks, word prediction, next sentence selection and sentence topic prediction. These success of topical information in similar NLP tasks provokes our trial of involving it into neural response generation."}, {"heading": "6 Conclusion and Future work", "text": "We proposed TAJA-Seq2Seq to naturally incorporate topic information as prior knowledge into Seq2Seq network and improved the quality and diversity of response generation. In future work, we will implement this structure for other kinds of extra information, such as contexts and knowledge from knowledge base, to enhance response generation quality in different aspects.\n9"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder\u2013 decoder approaches. Syntax, Semantics and Structure in Statistical Translation, page 103", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Describing multimedia content using attention-based encoder-decoder networks. Multimedia", "author": ["Cho et al.2015] Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "IEEE Transactions on,", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Contextual lstm (clstm) models for large scale nlp tasks. arXiv preprint arXiv:1602.06291", "author": ["Ghosh et al.2016] Shalini Ghosh", "Oriol Vinyals", "Brian Strope", "Scott Roy", "Tom Dean", "Larry Heck"], "venue": null, "citeRegEx": "Ghosh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Gu et al.2016] Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A diversitypromoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055", "author": ["Li et al.2015] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A persona-based neural conversation model", "author": ["Li et al.2016] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Neural responding machine for shorttext conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1503.02364", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Improvements in beam search", "author": ["Bach-Hiep Tran", "Hermann Ney"], "venue": "In ICSLP,", "citeRegEx": "Steinbiss et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Steinbiss et al\\.", "year": 1994}, {"title": "Sequence to sequence learning", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Attention with intention for a neural network conversation model", "author": ["Yao et al.2015] Kaisheng Yao", "Geoffrey Zweig", "Baolin Peng"], "venue": "Computer Science", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Comparing twitter and traditional media using topic models", "author": ["Zhao et al.2011] Wayne Xin Zhao", "Jing Jiang", "Jianshu Weng", "Jing He", "Ee-Peng Lim", "Hongfei Yan", "Xiaoming Li"], "venue": "In Advances in Information Retrieval,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "A popular network structure for response generation is the encoder-decoder structure (Sutskever et al., 2014; Shang et al., 2015) or more commonly referred to as \u201csequence to sequence\u201d (Seq2Seq) model.", "startOffset": 85, "endOffset": 129}, {"referenceID": 9, "context": "A popular network structure for response generation is the encoder-decoder structure (Sutskever et al., 2014; Shang et al., 2015) or more commonly referred to as \u201csequence to sequence\u201d (Seq2Seq) model.", "startOffset": 85, "endOffset": 129}, {"referenceID": 0, "context": "On top of Seq2Seq, attention mechanism (Bahdanau et al., 2014; Cho et al., 2015) which is first proposed for machine translation (MT), is then added to further improve generation quality.", "startOffset": 39, "endOffset": 80}, {"referenceID": 2, "context": "On top of Seq2Seq, attention mechanism (Bahdanau et al., 2014; Cho et al., 2015) which is first proposed for machine translation (MT), is then added to further improve generation quality.", "startOffset": 39, "endOffset": 80}, {"referenceID": 9, "context": "These structures have achieved great success on response generation and outperforms traditional retrieval based approaches (Shang et al., 2015) to a great deal.", "startOffset": 123, "endOffset": 143}, {"referenceID": 1, "context": "where ht is the hidden state at time t and f is a non-linear transformation which can be either an long-short term memory unit (LSTM) (Hochreiter and Schmidhuber, 1997) or a gated recurrent unit (GRU) (Cho et al., 2014).", "startOffset": 201, "endOffset": 219}, {"referenceID": 8, "context": "3 The decoder is a standard RNN language model(Mikolov et al., 2010) except conditioned on the context vector c.", "startOffset": 46, "endOffset": 68}, {"referenceID": 0, "context": "To solve this problem, attention mechanism (Bahdanau et al., 2014) which is first proposed for MT, is added to Seq2Seq for response generation(Shang et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 9, "context": ", 2014) which is first proposed for MT, is added to Seq2Seq for response generation(Shang et al., 2015).", "startOffset": 83, "endOffset": 103}, {"referenceID": 15, "context": "We obtain a topic word list for each input post from a Twitter LDA model (Zhao et al., 2011).", "startOffset": 73, "endOffset": 92}, {"referenceID": 15, "context": ", 2003) and represents the state-of-the-art topic model for short texts (Zhao et al., 2011).", "startOffset": 72, "endOffset": 91}, {"referenceID": 15, "context": "In training, we concatenate the post and the response of each sample to form a short document and employ the collapsed Gibbs sampling algorithm (Zhao et al., 2011) to estimate the parameters.", "startOffset": 144, "endOffset": 163}, {"referenceID": 11, "context": "We use beam search(Steinbiss et al., 1994) to generate responses using the trained model.", "startOffset": 18, "endOffset": 42}, {"referenceID": 14, "context": "We initialize the parameters of the Seq2Seq based models using isotropic Gaussian distributions and then use AdaDelta algorithm (Zeiler, 2012) to train the models on a NVIDIA Tesla K40 GPU.", "startOffset": 128, "endOffset": 142}, {"referenceID": 0, "context": "S2SA: standard Seq2Seq model with attention mechanism which is firstly proposed in (Bahdanau et al., 2014) for machine translation.", "startOffset": 83, "endOffset": 106}, {"referenceID": 6, "context": "S2SA-NMI: Seq2Seq model with a NMI objective function proposed in (Li et al., 2015) to improve the diversity of the generated results.", "startOffset": 66, "endOffset": 83}, {"referenceID": 6, "context": "Since NMI-bidi performs better among the two models in (Li et al., 2015) according to the existing work, we select NMI-bidi as a baseline.", "startOffset": 55, "endOffset": 72}, {"referenceID": 9, "context": "We do not compare our model with Seq2Seq without attention and the traditional retrievalbased methods because it has been proven in (Shang et al., 2015) that S2SA outperforms these two methods and represents the state-of-the-art in response generation.", "startOffset": 132, "endOffset": 152}, {"referenceID": 10, "context": "Sordoni(Sordoni et al., 2015) proposed a encoder-", "startOffset": 7, "endOffset": 29}, {"referenceID": 7, "context": "Li (Li et al., 2016) added personal information from knowledge base as an extra input of Seq2Seq and concatenate it with the word embedding for every time step.", "startOffset": 3, "endOffset": 20}, {"referenceID": 4, "context": "Gu(Gu et al., 2016) proposed copying mechanism to mimic the repeating phenomenon during human conversation.", "startOffset": 2, "endOffset": 19}, {"referenceID": 13, "context": "Yao (Yao et al., 2015) added an extra RNN between the encoder and the decoder with attention to conclude and explain intentions from posts.", "startOffset": 4, "endOffset": 22}, {"referenceID": 3, "context": "Ghosh(Ghosh et al., 2016) proposed Contextual LSTM that involved topic information to represent contexts and concatenated topic information with the word embedding for every time step.", "startOffset": 5, "endOffset": 25}], "year": 2016, "abstractText": "We consider incorporating topic information as prior knowledge into the sequence to sequence (Seq2Seq) network structure with attention mechanism for response generation in chatbots. To this end, we propose a topic augmented joint attention based Seq2Seq (TAJASeq2Seq) model. In TAJA-Seq2Seq, information from input posts and information from topics related to the posts are simultaneously embedded into vector spaces by a content encoder and a topic encoder respectively. The two kinds of information interact with each other and help calibrate weights of each other in the joint attention mechanism in TAJA2Seq2Seq, and jointly determine the generation of responses in decoding. The model simulates how people behave in conversation and can generate well-focused and informative responses with the help of topic information. Empirical study on large scale human judged generation results show that our model outperforms Seq2Seq with attention on both response quality and diversity.", "creator": "LaTeX with hyperref package"}}}