{"id": "1606.09375", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering", "abstract": "Convolutional neural networks (CNNs) have greatly improved state-of-the-art performances in a number of fields, notably computer vision and natural language processing. In this work, we are interested in generalizing the formulation of CNNs from low-dimensional regular Euclidean domains, where images (2D), videos (3D) and audios (1D) are represented, to high-dimensional irregular domains such as social networks or biological networks represented by graphs. This paper introduces a formulation of CNNs on graphs in the context of spectral graph theory. We borrow the fundamental tools from the emerging field of signal processing on graphs, which provides the necessary mathematical background and efficient numerical schemes to design localized graph filters efficient to learn and evaluate. As a matter of fact, we introduce the first technique that offers the same computational complexity than standard CNNs, while being universal to any graph structure. Numerical experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs, as long as the graph is well-constructed.", "histories": [["v1", "Thu, 30 Jun 2016 07:42:13 GMT  (676kb,D)", "http://arxiv.org/abs/1606.09375v1", null], ["v2", "Mon, 31 Oct 2016 15:24:49 GMT  (719kb,D)", "http://arxiv.org/abs/1606.09375v2", "NIPS 2016 camera-ready"], ["v3", "Sun, 5 Feb 2017 17:04:39 GMT  (166kb,D)", "http://arxiv.org/abs/1606.09375v3", "NIPS 2016 final revision"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["micha\u00ebl defferrard", "xavier bresson", "pierre vandergheynst"], "accepted": true, "id": "1606.09375"}, "pdf": {"name": "1606.09375.pdf", "metadata": {"source": "CRF", "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering", "authors": ["Micha\u00ebl Defferrard", "Xavier Bresson", "Pierre Vandergheynst"], "emails": ["michael.defferrard@epfl.ch", "xavier.bresson@epfl.ch", "pierre.vandergheynst@epfl.ch"], "sections": [{"heading": "1 Introduction", "text": "Convolutional neural networks [20] offer an efficient architecture to extract highly meaningful statistical patterns in large-scale and high-dimensional datasets. The key success of CNNs is its advanced ability to learn local stationary structures and compose them to form multi-scale hierarchical patterns. Precisely, CNNs extract the local stationarity property of the input data or signals by revealing local features that are shared across the data domain. These similar features are identified with localized convolutional filters or kernels, which are learned from the data. Convolutional filters are shift- or translation-invariant filters, meaning they are able to recognize identical features independently of their spatial locations. Localized kernels or compactly supported filters refer to filters that extract local features independently of the input data size, with a support size that can be much smaller than the input size.\nThe compositional local stationarity property of CNNs can be efficiently implemented on regular grids, like image and sound domains, as convolutional, downsampling and pooling operators are mathematically well-defined on such discretized Euclidean spaces. This has led to the breakthrough of CNNs in image, video, and sound recognition tasks [19] as these data lie on low-dimensional regular lattices. But not all data lie on regular grids. User data on social networks, gene data on biological\nar X\niv :1\n60 6.\n09 37\n5v 1\n[ cs\n.L G\n] 3\nregulatory networks, log data on telecommunication networks, or text documents on words\u2019 embedding are important examples of data lying on irregular or non-Euclidean domains. Unfortunately, CNNs cannot be directly applied to such complex domains.\nA generalization to irregular grids is not straightforward as the standard operators of convolution, downsampling, and pooling, are only defined for regular grids. This makes this extension challenging, both theoretically and implementation-wise. This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs. Graphs are universal representations of heterogeneous pairwise relationships between possibly highdimensional data. Graphs can encode complex geometric structures, and can be studied with strong mathematical tools such as spectral graph theory [7], a blend between graph theory and harmonic analysis.\nThe major bottleneck of generalizing CNNs to graphs, and one of the primary goals of this work, is the definition of graph localized filters which are efficient to evaluate and learn. We will make use of recent tools developed in the context of graph signal processing (GSP) [32] to achieve such goals. Precisely, the main contributions of this work are summarized below:\n1. Spectral formulation. A graph spectral theoretical formulation of CNNs on graphs built on established tools in GSP.\n2. Strictly localized filters. Enhancing [4, 15], the proposed spectral filters are provable to be strictly localized in a ball of radius K, i.e. K hops from the central vertex.\n3. Low computational complexity. The evaluation complexity of our filters is linear w.r.t. the filters support\u2019s size K and the number of edges |E|. Importantly, as most real-world graphs are highly sparse, we have |E| n2 and |E| = kn for the widespread k-nearest neighbor (NN) graphs, leading to a linear complexity w.r.t the input data size. Moreover, this method avoids the Fourier basis altogether, thus the expensive eigenvalue decomposition (EVD) necessary to\ncompute it as well as the need to store the basis, a matrix of size n2. That is especially relevant when working with limited GPU memory. Besides the data, our method only requires to store the Laplacian, a sparse matrix of |E| non-zero values.\n4. Efficient pooling. We propose an efficient pooling strategy on graphs which, after a rearrangement of the vertices as a binary tree structure, is analog to pooling of 1D signals.\n5. Experimental results. We present many experiments that ultimately show that our formulation is (1) a useful model, (2) computationally efficient and (3) superior both in accuracy and complexity to the pioneer spectral graph CNNs introduced in [4, 15]. We also show that our graph formulation performs similarly as a classical CNNs on MNIST and study the impact of various graph constructions on classification performance.\nEventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data."}, {"heading": "2 Related Works", "text": ""}, {"heading": "2.1 Graph Signal Processing", "text": "GSP is an emerging field that aims at bridging the gap between signal processing techniques like wavelet analysis [22] and graph theory such as spectral graph analysis [3, 21]. A goal is to generalize fundamental analysis operations for signals from regular grids to irregular structures embodied by graphs. We refer the reader to [32] for an introduction of the field. Standard operations on grids such as convolution, translation, filtering, dilatation, modulation or downsampling do not extend directly to graphs and thus require new mathematical definitions while keeping the original intuitive concepts. In this context, the authors of [14, 9, 11] revisited the construction of wavelet operators on graphs. In [33, 28], the authors designed a technique to perform mutli-scale pyramid transforms on graphs. The works of [36, 26, 27] redefined uncertainty principles on graphs, and showed that intuitive concepts may be lost, but can also produce enhanced localization principles for signals on graphs. In [13], it was shown how to carry out lasso-based signal regularization on graphs, and studied the intertwined relationships between smoothness and sparsity on graphs. In [35], the authors investigated compressed sensing recovery conditions for graph spectral signal analysis.\nIn the future, we expect to further benefit from newly developed tools in GSP to enhance our mathematical formulation of CNNs on graphs. This paper introduces the mathematical and computational foundations of such models, while future developments may consider to enhance the building blocks, namely graph filtering and coarsening."}, {"heading": "2.2 CNNs on Non-Euclidean Domains", "text": "Extending CNNs to non-Euclidean domains has been a recent line of work. A preliminary work was proposed with the so-called local reception fields [12, 8], successfully applied to image recognition. The main idea is to group together features based upon a measure of similarity such as to select a limited number of connections between two successive layers. While this model reduced the number of parameters by exploiting the locality assumption, it did not attempt to exploit any stationarity property of data, i.e. no weight-sharing strategy.\nA first attempt to design CNNs on graphs was introduced in [4] by adopting a spatial approach. The idea was to construct a multiresolution graph by exploiting graph neighbourhood structures, and learned a deep neural network. However, such approach did not consider convolution operations, and thus no stationarity property.\nShapeNet in [24, 23] is a generalization of CNNs to 3D-meshes, a class of smooth low-dimensional non-Euclidean spaces. The authors used geodesic polar coordinates to define convolution operations\non mesh patches, and formulated a deep learning architecture which allows comparison across different manifolds. They obtained state-of-the-art results for 3D shape recognition tasks.\nIn [6] and [30], the authors investigated the construction of Haar wavelet transforms on graphs using a deep hierarchical architecture. They applied the method to object recognition on sphere, and to sparse reconstruction of faces.\nFinally, [4] and [15] introduced a generalization of CNNs on graphs using graph counterparts of grid convolution and downsampling. They were able to learn convolutional filters on graphs using the convolution theorem [22] that defines convolutions as linear operators that diagonalize in the Fourier basis (represented by the eigenvectors of the Laplacian operator). They were thus the first to introduce a spectral formulation to extend CNNs on graphs. They also introduced a strategy to learn the graph structure from the data, and applied their model on image recognition, text categorization and bioinformatics. This approach does however not scale up due to the necessary multiplications by the graph Fourier basis, a matrix of n2 coefficients, where n is the data dimensionality. Despite the cost of computing this matrix, which requires an EVD on the graph Laplacian, the dominant cost is the need to multiply the data by this matrix twice (forward and inverse Fourier transforms) at a cost of O(n2) operations per forward and backward pass, a computational bottleneck already identified by the authors. Besides, as they rely on smoothness (via a spline parametrization) in the Fourier domain to bring localization in the vertex domain, their model does not provide a precise control over the local support of their kernels, which is essential to learn localized filters. Our technique leverages on this work, and we will show how to overcome these limitations and beyond."}, {"heading": "3 Proposed Technique", "text": "Generalizing CNNs to graphs requires three fundamental steps. First step is the design of localized convolutional filters on graphs. Second step is a graph coarsening procedure that groups together similar features on graphs. And the last step is the graph pooling operation that trades spatial resolution for higher filter resolution."}, {"heading": "3.1 Learning Fast Localized Spectral Filters", "text": "The goal of this section is to define fast convolutional localized filters on graphs. There are two strategies to define convolutional localized filters; either from a spatial approach or from a spectral approach. By construction, spatial approaches provide filter localization via the finite size of the kernel. However, although graph convolution directly in the spatial domain is conceivable, it also faces the challenge of matching local neighbourhoods, as pointed out in [4]. Consequently, there is no unique mathematical definition of spatial translations on graphs from a spatial perspective. On the other side, a spectral approach provides a well-defined translation operator on graphs via convolutions with a Kronecker delta function implemented in the spectral domain [32]. However, a filter defined in the spectral domain is not naturally localized and translations are costly with O(n2) operations due to multiplications with the graph Fourier basis. Both limitations can be overcome with a special choice of filter parametrization, as explained below.\nGraph Fourier Transform. We are interested in processing signals defined on undirected and connected graphs G = (V, E ,W ), where V is a finite set of vertices with |V| = n, E is a set of edges and W is a weighted adjacency matrix encoding the connection weight between two vertices. A signal x : V \u2192 R defined on the nodes of the graph may be regarded as a vector x \u2208 Rn where xi is the value of x at the ith node. An essential operator in spectral graph analysis is the graph Laplacian [7], which combinatorial definition is L = D \u2212W \u2208 Rn\u00d7n where D \u2208 Rn\u00d7n is the diagonal degree matrix with Dii = \u2211 jWij , and its normalized definition is L = In \u2212 D\u22121/2WD\u22121/2 where In is the identity matrix. As L is a real symmetric positive semidefinite matrix, it has a complete set of orthonormal eigenvectors {ul}n\u22121l=0 \u2208 Rn, known as the graph Fourier modes, and their associated\nordered real nonnegative eigenvalues {\u03bbl}n\u22121l=0 , identified as the frequencies of the graph. The Laplacian is indeed diagonalized by the Fourier basis U = [u0, . . . , un\u22121] \u2208 Rn\u00d7n such that L = U\u039bUT where \u039b = diag([\u03bb0, . . . , \u03bbn\u22121]) \u2208 Rn\u00d7n. We can now define the graph Fourier transform (GFT) of a spatial signal x \u2208 Rn as x\u0302 = UTx \u2208 Rn, and its inverse as x = Ux\u0302 [32]. Similarly to the Fourier transform on Euclidean spaces, the GFT enables the formulation of fundamental operations such as filtering.\nSpectral filtering of graph signals. As we cannot express a meaningful translation operator in the vertex domain, the convolution operator on graph \u2217G is defined in the Fourier domain such that x \u2217G y = U((UTx) (UT y)), where is the element-wise Hadamard product. It follows that a signal x is filtered by g\u03b8 as\ny = g\u03b8(L)x = g\u03b8(U\u039bU T )x = Ug\u03b8(\u039b)U Tx. (1)\nA non-parametric filter, i.e. a filter whose parameters are all free, would be defined as\ng\u03b8(\u039b) = diag(\u03b8), (2)\nwhere the parameter \u03b8 \u2208 Rn is a vector of Fourier coefficients.\nPolynomial parametrization for localized filters. There are however two limitations with nonparametric filters: (1) they are not localized in space and (2) their learning complexity is in O(n), the dimensionality of the data. These issues can be overcome with the use of Laplacian-based polynomial spectral filters:\ng\u03b8(\u039b) = K\u22121\u2211 k=0 \u03b8k\u039b k, (3)\nwhere the parameter \u03b8 \u2208 RK is a vector of polynomial coefficients. The value at vertex j of the filter g\u03b8 centered at vertex i is given by (g\u03b8(L)\u03b4i)j = (g\u03b8(L))i,j = \u2211 k \u03b8k(L\nk)i,j , where the kernel is translated via a convolution with a Kronecker delta function \u03b4i \u2208 Rn. By [14, Lemma 5.2], dG(i, j) > K implies (LK)i,j = 0, where dG is the shortest path distance, i.e. the minimum number of edges connecting two vertices on the graph. Consequently, spectral filters represented by Kth-order polynomials of the Laplacian are exactly K-localized. Besides, their learning complexity is O(K), the support size of the filter, and thus the same complexity as in standard CNNs.\nRecursive formulation for fast filtering. While we have shown how to learn localized filters with K parameters, the cost to filter a signal x as y = Ug\u03b8(\u039b)UTx is still high with O(n2) operations because of the multiplications with the Fourier basis U . A solution of this problem is to parametrize g\u03b8(L) as a polynomial function that can be computed recursively from L, as K multiplications by a sparse L costs O(K|E|) O(n2). One such polynomial, traditionally used in GSP to approximate kernels (like wavelets), is the Chebyshev expansion [14]. Another option, the Lanczos algorithm [34], which constructs an orthonormal basis of the Krylov subspace KK(L, x) = span{x, Lx, . . . , LK\u22121x}, seems attractive because of the coefficients\u2019 independence. It is however more convoluted and thus left as a future work.\nRecall that the Chebyshev polynomial Tk(x) of order k may be generated by the stable recurrence relation Tk(x) = 2xTk\u22121(x)\u2212Tk\u22122(x) with T0 = 1 and T1 = x. These polynomials form an orthogonal basis for L2([\u22121, 1], dy/ \u221a 1\u2212 y2), the Hilbert space of square integrable functions with respect to the\nmeasure dy/ \u221a\n1\u2212 y2. A filter can thus be parametrized as a truncated expansion of order K\u2212 1 such that\ng\u03b8(\u039b) = K\u22121\u2211 k=0 \u03b8kTk(\u039b\u0303), (4)\nwhere the parameter \u03b8 \u2208 RK is a vector of Chebyshev coefficients and Tk(\u039b\u0303) \u2208 Rn\u00d7n is the Chebyshev polynomial of order k evaluated at \u039b\u0303 = 2\u039b/\u03bbmax \u2212 In, a diagonal matrix of scaled eigenvalues that lie in [\u22121, 1].\nThe filtering operation can then be written as y = g\u03b8(L)x = \u2211K\u22121 k=0 \u03b8kTk(L\u0303)x, where Tk(L\u0303) \u2208 Rn\u00d7n is the Chebyshev polynomial of order k evaluated at the scaled Laplacian L\u0303 = 2L/\u03bbmax\u2212In. Denoting x\u0304k = Tk(L\u0303)x \u2208 Rn, we can use the recurrence relation to compute x\u0304k = 2L\u0303x\u0304k\u22121 \u2212 x\u0304k\u22122 with x\u03040 = x and x\u03041 = L\u0303x. The entire filtering operation\ny = g\u03b8(L)x = [x\u03040, . . . , x\u0304K\u22121]\u03b8 (5)\nthen costs O(K|E|) operations.\nLearning filters. The jth output feature map of the sample s is given by\nys,j = Fin\u2211 i=1 g\u03b8i,j (L)xs,i \u2208 Rn, (6)\nwhere the xs,i are the input feature maps and the Fin\u00d7Fout vectors of Chebyshev coefficients \u03b8i,j \u2208 RK are the layer\u2019s trainable parameters. When training multiple convolutional layers with the backpropagation algorithm, one needs the two gradients:\n\u2202E\n\u2202\u03b8i,j = S\u2211 s=1 [x\u0304s,i,0, . . . , x\u0304s,i,K\u22121] T \u2202E \u2202ys,j and\n\u2202E\n\u2202xs,i = Fout\u2211 j=1 g\u03b8i,j (L) \u2202E \u2202ys,j , (7)\nwhere E is the loss energy over a mini-batch of S samples. Each of the above three computations boils down to K sparse matrix-vector multiplications and one dense matrix-vector multiplication for a cost of O(K|E|FinFoutS) operations. These can be efficiently computed on parallel architectures by leveraging tensor operations. Eventually, [x\u0304s,i,0, . . . , x\u0304s,i,K\u22121] only need to be computed once."}, {"heading": "3.2 Graph Coarsening", "text": "Local stationarity property of data is extracted via localized convolutional kernels. We are now interested to extract the multi-scale hierarchical composition property of data. In standard CNNs, this is efficiently achieved via grid subsampling and pooling, which trades spatial resolution with feature resolution reducing the learning complexity without compromising the system performances. In contrast with regular domains, the subsampling operation on graphs or graph coarsening is not mathematically sound. It requires to construct meaningful neighborhoods on graphs where similar vertices are clustered together. Doing this for multiple layers is equivalent to construct a multi-scale clustering of the graph that preserves local geometric structures. It is however well-known that graph clustering is a NP-hard problem [5] and that approximations must be used. While there exist many clustering techniques (e.g. the popular spectral clustering [21]), we are most interested in multilevel clustering algorithms, where each level produces a coarser graph which corresponds to the data domain seen at a different resolution. Moreover, clustering techniques that reduce the size of the graph by a factor two at each level offers a precise control on the coarsening and pooling size. In this work, we make use of the coarsening phase of the Graclus multilevel clustering algorithm [10], which has been shown to be extremely efficient at clustering a large variety of graphs. We briefly review it below. Algebraic multigrid techniques on graphs [29] and the Kron reduction [33] are two methods worth exploring in future works.\nGraclus [10], built on Metis [17], uses a greedy algorithm to compute successive coarser versions of a given graph and is able to minimize several popular spectral clustering objectives. We chose the normalized cut [31], which is an excellent clustering energy. Graclus\u2019 greedy rule consists, at any given coarsening level, in picking an unmarked vertex i and matching it with one of its unmarked neighbors j that maximizes the local normalized cut Wij(1/di + 1/dj). The two matched vertices are then marked and the coarsened weights are set as the sum of their weights. The matching is repeated until all nodes have been explored. This is an extremely fast coarsening scheme which divides the number of nodes by approximately two (there may exist a few singletons, non-matched nodes) from one level to the next coarser level."}, {"heading": "3.3 Fast Pooling of Graph Signals", "text": "As standard CNNs, pooling operations reduce the spatial resolution allowing higher filter resolution. Pooling operations, such as max pooling or average pooling, are carried out many times during the optimization, and thus must be as efficient as possible. After the graph coarsening operation in the previous section, the graph and its coarsened versions have an unstructured arrangement of the vertices. Hence, if the pooling is directly applied then it would need a table to store all matched vertices, which would result in memory consuming, slow, and not (easily) parallelizable pooling operations.\nIt is however possible to structure the arrangement of the vertices, and achieve a graph pooling operation as efficient as a 1D grid pooling. We proceed as follows. After coarsening, each node has either two parents, if it was matched at the finer level, or one, if it was not, i.e the node was a singleton. From the coarsest to finest level, fake nodes, i.e. disconnected nodes, are added to pair with the singletons such that each node has two children. This structure is a balanced binary tree: (1) regular nodes (and singletons) either have two regular nodes (e.g. level 1 vertex 0 in Figure 2) or (2) one singleton and a fake node as children (e.g. level 2 vertex 0 in Figure 2), and (3) fake nodes always have two fake nodes as children (e.g. level 1 vertex 1 in Figure 2). Input signals are initialized with a neutral value at the fake nodes, e.g. 0 when using a ReLU activation with max pooling. Because these nodes are disconnected, filtering does not impact the initial neutral value. While those fake nodes do artificially increase the dimensionality thus the computational cost, we found that, in practice, the number of singletons left by Graclus is quite low compared to the number of vertices.\nArbitrarily ordering the nodes at the coarsest level, then propagating this ordering to the finest levels, i.e. node k has nodes 2k and 2k+ 1 as children, produces a regular ordering in the finest level. Regular in the sense that adjacent nodes are hierarchically merged at coarser levels. Pooling such a rearranged graph signal is analog to pooling a regular 1D signal. Figure 2 shows an example of the whole coarsening and pooling process. This regular arrangement makes the pooling operation very efficient and satisfies parallel architectures such as GPUs as memory accesses are local, i.e. matched nodes do not have to be fetched."}, {"heading": "4 Numerical Experiments", "text": "All experiments were performed with TensorFlow, an open-source library for numerical computation using data flow graphs, especially suited for deep learning [1]. It features various backends, notably CUDA to compute on Nvidia GPUs. All computations are carried on an Nvidia Tesla K40c GPU.\nIn the sequel, we refer to filters defined by (2) as Non-Param, i.e. the spatially non-localized filters, and the proposed filters defined by (4) as Chebyshev. Filters referred to as Spline are defined by\ng\u03b8(\u039b) = B\u03b8, (8)\nwhere B \u2208 Rn\u00d7K is the cubic B-spline basis and the parameter \u03b8 \u2208 RK is a vector of control points, as proposed in [4, 15]. We use for all techniques the advanced Graclus coarsening algorithm introduced in Section 3.2 rather than the simple agglomerative method of [4, 15]. The motivation is to compare the quality of learned filters, not the coarsening algorithms.\nWe use the following notation when describing network architectures: FCk denotes a fully connected layer with k hidden units, Pk denotes a (graph or classical) pooling layer of size and stride k, Ck denotes a convolutional layer with k feature maps and GCk denotes a graph convolutional layer with Fout = k feature maps (with Chebyshev filters if not specified otherwise). All FCk, Ck and GCk layers are followed by a ReLU activation max(x, 0). The final layer is always a softmax regression and the loss energy E is the cross-entropy loss with an `2 regularization on the weights of all FCk layers. Mini-batches are of size S = 100."}, {"heading": "4.1 Revisiting Standard CNNs on MNIST", "text": "To validate our model, we first apply it to the Euclidean case with the benchmark MNIST classification problem [20]. In this situation, the graph is simply a k-NN graph of the Euclidean 2D grid. This is an important sanity check for our model, which must be able to extract features on any graph, including the regular 2D grid on which images reside.\nWe recall that MNIST is a dataset of 70,000 digit numbers represented on a 2D grid of size 28\u00d728, such that data points lie on a space of 784 dimensions. For our graph model, we construct a 8-NN graph of the 2D Euclidean grid, which produces a graph of n = |V| = 976 nodes (784 pixels and 192 fake nodes as explained in Section 3.3) and |E| = 3198 edges.\nTable 1 confirms the ability of our model to learn localized filters as it achieves a performance very close to the classical CNNs with the same LeNet-5-like architecture. The LeNet-5-like network architecture and the following hyper-parameters are borrowed from the TensorFlow MNIST tutorial1: dropout probability of 0.5, regularization weight of 5\u00d710\u22124, initial learning rate of 0.03, learning rate decay of 0.95, momentum of 0.9. Convolutional layer have filters of size 5\u00d7 5 while graph convolution layers have the same support of K = 52 = 25. All models were trained for 20 epochs."}, {"heading": "4.2 Text Categorization with 20NEWS", "text": "To demonstrate the versatility of our model to work with graphs generated from unstructured data, we applied our technique to the text categorization problem with the 20NEWS dataset.\n1https://www.tensorflow.org/versions/r0.8/tutorials/mnist/pros\n20NEWS consists of 18,846 (11,314 for training and 7,532 for testing) text documents associated with 20 classes [16]. We extracted the 10,000 most common words from the 93,953 unique words in this corpus. Each document is represented using the bag-of-words model, normalized across words. To test our model, we constructed a 16-NN graph of the word2vec [25] embedding of those words, which produced a graph of n = |V| = 10, 000 nodes and |E| = 132, 834 edges. All CNN models were trained for 20 epochs by the Adam optimizer [18] with a learning rate of 0.1, without regularization nor dropout. The filter support was set to K = 5.\nTable 2 shows that CNNs on graphs provide decent performances. While it does not outperform the kernel-based method of multinomial naive Bayes classifier on this small dataset, it does defeat fully connected networks, which require much more parameters to be learned. More importantly, these results verify the validity of the statistical assumptions made for the data, that are locality and stationarity, and which are at the core of the design of any CNN technique. While we know that these data properties are true for low-dimensional Euclidean data like audios, images and videos, we also show experimentally that they are also satisfied for text documents as long as the graph is properly constructed. Section 4.5 will study the influence of the graph quality."}, {"heading": "4.3 Numerical Comparison between Spectral Filters", "text": "In this section, we compare the proposed spectral filters to the non-parametric filters, and those proposed in [4, 15] on the MNIST and 20NEWS datasets. Table 3 reports that the proposed kernel parametrization outperforms [4, 15] as well as the non-parametric filters, which are not localized and require O(n) parameters to learn."}, {"heading": "4.4 Computational Efficiency", "text": "Figure 3 validates the low computational complexity of the proposed CNN technique on graphs. The training time of our model scales as O(n), while [4, 15] scales as O(n2). Moreover, Figure 4 gives a sense of how the validation accuracy as well as the loss energy converges w.r.t. the three filter definitions. Finally, Table 4 compares training time on CPU and GPU. The fact that we observe the\nsame order of speedup as classical CNNs exemplifies the natural parallelization opportunity offered by our model. That is possible because our method relies solely on matrix multiplications which are efficiently implemented by cuBLAS, the linear algebra routines provided by NVIDIA."}, {"heading": "4.5 Influence of Graph Quality", "text": "For the proposed method to be successful, the statistical assumptions of locality, stationarity, and compositionality regarding the data must be fulfilled on the graph where the data resides. Therefore, the classification performance, the quality of learned filters, all critically depends on the quality of the graph. For data lying on Euclidean space, experiments in Section 4.1 show that a simple k-NN graph of the grid is good enough to recover almost exactly the performance of standard CNNs. We also noticed that the value of k does not have a strong influence on the results. We can witness the importance of a graph satisfying the data assumptions by comparing its performance with a random graph. Table 5 reports a large drop of accuracy when using a random graph, that is when the data structure is lost and the convolutional layers are not useful anymore to extract meaningful features.\nWhile images naturally lie on Euclidean 2D spaces, represented by graphs of regular grids, this is not the case for text documents. For our method to be applied to this class of data, a feature graph does have to be built. We investigate here 5 16-NN similarity graphs of words. The simplest option is to represent each word as its corresponding column in the bag-of-words matrix. Another approach is to learn embeddings on the corpus with word2vec [25] or to use pre-trained embeddings on Google News (given by the authors). As the dataset gets larger (in number of samples and dimensionality), it is often not an option to compute the distance between all features, such that an approximate nearest neighbors algorithm shall be used. We used the LSHForest [2] on the learned word2vec embedding. Table 6 reports the classification results. It shows that the graph built on the learned word2vec embedding is the best at capturing the local and stationarity properties of text documents. It is worth noticing that the approximate k-NN graph constructed from it is almost as bad as the random graph, meaning that it may be a better strategy to learn a good low-dimensional embedding first and then construct an exact k-NN graph from this embedding."}, {"heading": "5 Conclusion and Future Work", "text": "We have introduced an efficient implementation of CNNs on non-Euclidean manifolds represented by graphs using tools from GSP. Experiments have shown the ability of the model to extract local and stationary features through the graph convolutional layers.\nCompared with the first works of spectral graph CNNs introduced in [4, 15], our model provides a strict control over the local support of filters, is computationally more efficient by avoiding an explicit use of the Graph Fourier basis, and experimentally shows a better test accuracy. Besides, we addressed the three concerns raised by [15]. First, we introduced a model whose computational complexity is\nlinear with the dimensionality of the data. Second, we confirmed that the quality of the input graph is of paramount importance. Along this line, the idea of [15] to learn the input graph from data is highly meaningful. And a natural and future approach would be to alternate the learning of the CNN parameters and the graph, as a virtuous circle. Third, we showed that the statistical assumptions of local stationarity and compositionality made by the model are verified for text documents as long as the graph is well constructed.\nFuture works will investigate two directions. On one hand, we will explore applications of this generic model to important fields where the data naturally lie on non-artificially constructed graphs such as social networks, biological networks, or telecommunication networks. It makes actually more sense to apply this generalized CNN framework to natural network-based data, rather than artificially created networks which quality may vary as seen in the experiments. On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27]."}], "references": [{"title": "LSH Forest: Self-Tuning Indexes for Similarity Search", "author": ["M. Bawa", "T. Condie", "P. Ganesan"], "venue": "International Conference on World Wide Web, pages 651\u2013660,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Towards a Theoretical Foundation for Laplacian-based Manifold Methods", "author": ["M. Belkin", "P. Niyogi"], "venue": "Journal of Computer and System Sciences, 74(8):1289\u20131308,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral Networks and Deep Locally Connected Networks on Graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "arXiv:1312.6203,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding Good Approximate Vertex and Edge Partitions is NP-hard", "author": ["T.N. Bui", "C. Jones"], "venue": "Information Processing Letters, 42(3):153\u2013159,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Unsupervised Deep Haar Scattering on Graphs", "author": ["X. Chen", "X. Cheng", "S. Mallat"], "venue": "Neural Information Processing Systems, pages 1709\u20131717,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral Graph Theory, volume 92", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Selecting Receptive Fields in Deep Networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "Neural Information Processing Systems (NIPS), pages 2528\u20132536,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Diffusion Maps", "author": ["R.R. Coifman", "S. Lafon"], "venue": "Applied and Computational Harmonic Analysis, 21(1):5\u201330,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Weighted Graph Cuts Without Eigenvectors: A Multilevel Approach", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 29(11):1944\u2013 1957,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiscale Wavelets on Trees, Graphs and High Dimensional Data: Theory and Applications to Semi Supervised Learning", "author": ["M. Gavish", "B. Nadler", "R. Coifman"], "venue": "International Conference on Machine Learning (ICML), pages 367\u2013374,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Emergence of Complex-like Cells in a Temporal Product Network with Local Receptive Fields", "author": ["K. Gregor", "Y. LeCun"], "venue": "arXiv:1006.0448,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Image Denoising with Nonlocal Spectral Graph Wavelets", "author": ["D. Hammond", "K. Raoaroor", "L. Jacques", "P. Vandergheynst"], "venue": "SIAM Conference on Imaging Science,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Wavelets on Graphs via Spectral Graph Theory", "author": ["D. Hammond", "P. Vandergheynst", "R. Gribonval"], "venue": "Applied and Computational Harmonic Analysis, 30(2):129\u2013150,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Convolutional Networks on Graph-Structured Data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv:1506.05163,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization", "author": ["T. Joachims"], "venue": "Carnegie Mellon University, Computer Science Technical Report, CMU-CS-96-118,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM Journal on Scientific Computing (SISC), 20(1):359\u2013392,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-Based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11), pages 2278\u20132324,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "A Tutorial on Spectral Clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and Computing, 17(4):395\u2013416,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "A Wavelet Tour of Signal Processing", "author": ["S. Mallat"], "venue": "Academic press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "ShapeNet: Convolutional Neural Networks on Non-Euclidean Manifolds", "author": ["J. Masci", "D. Boscaini", "M. Bronstein", "P. Vandergheynst"], "venue": "arXiv:1501.06297,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Geodesic Convolutional Neural Networks on Riemannian Manifolds", "author": ["J. Masci", "D. Boscaini", "M.M. Bronstein", "P. Vandergheynst"], "venue": "InWorkshop on 3D Representation and Recognition (3dRR),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "International Conference on Learning Representations,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward an Uncertainty Principle for Weighted Graphs", "author": ["B. Pasdeloup", "R. Alami", "V. Gripon", "M. Rabbat"], "venue": "Signal Processing Conference (EUSIPCO), pages 1496\u20131500,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Global and Local Uncertainty Principles for Signals on Graphs", "author": ["N. Perraudin", "B. Ricaud", "D. Shuman", "P. Vandergheynst"], "venue": "arXiv:1603.03030,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Generalized Tree-based Wavelet Transform", "author": ["I. Ram", "M. Elad", "I. Cohen"], "venue": "IEEE Transactions on Signal Processing,, 59(9):4199\u20134209,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Relaxation-based Coarsening and Multiscale Graph Organization", "author": ["D. Ron", "I. Safro", "A. Brandt"], "venue": "SIAM Iournal on Multiscale Modeling and Simulation, 9:407\u2013423,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Wavelets on Graphs via Deep Learning", "author": ["R. Rustamov", "L.J. Guibas"], "venue": "pages 998\u20131006,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Normalized Cuts and Image Segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 22(8):888\u2013905,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "The Emerging Field of Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and other Irregular Domains", "author": ["D. Shuman", "S. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine, 30(3):83\u201398,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "A Multiscale Pyramid Transform for Graph Signals", "author": ["D.I. Shuman", "M.J. Faraji", "P. Vandergheynst"], "venue": "IEEE Transactions on Signal Processing, 64(8):2119\u20132134,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Accelerated Filtering on Graphs using Lanczos Method", "author": ["A. Susnjara", "N. Perraudin", "D. Kressner", "P. Vandergheynst"], "venue": "preprint arXiv:1509.04537,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressive Spectral Clustering", "author": ["N. Tremblay", "G. Puy", "R. Gribonval", "P. Vandergheynst"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "On the Degrees of Freedom of Signals on Graphs", "author": ["M. Tsitsvero", "S. Barbarossa"], "venue": "Signal Processing Conference (EUSIPCO), pages 1506\u20131510,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Convolutional neural networks [20] offer an efficient architecture to extract highly meaningful statistical patterns in large-scale and high-dimensional datasets.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "This has led to the breakthrough of CNNs in image, video, and sound recognition tasks [19] as these data lie on low-dimensional regular lattices.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 6, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 2, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 13, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 12, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 30, "context": "This work leverages the recent works of [12, 8, 4, 15, 14, 32] to introduce an efficient implementation of CNNs on irregular domains represented here as graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 5, "context": "Graphs can encode complex geometric structures, and can be studied with strong mathematical tools such as spectral graph theory [7], a blend between graph theory and harmonic analysis.", "startOffset": 128, "endOffset": 131}, {"referenceID": 30, "context": "We will make use of recent tools developed in the context of graph signal processing (GSP) [32] to achieve such goals.", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "Enhancing [4, 15], the proposed spectral filters are provable to be strictly localized in a ball of radius K, i.", "startOffset": 10, "endOffset": 17}, {"referenceID": 13, "context": "Enhancing [4, 15], the proposed spectral filters are provable to be strictly localized in a ball of radius K, i.", "startOffset": 10, "endOffset": 17}, {"referenceID": 2, "context": "We present many experiments that ultimately show that our formulation is (1) a useful model, (2) computationally efficient and (3) superior both in accuracy and complexity to the pioneer spectral graph CNNs introduced in [4, 15].", "startOffset": 221, "endOffset": 228}, {"referenceID": 13, "context": "We present many experiments that ultimately show that our formulation is (1) a useful model, (2) computationally efficient and (3) superior both in accuracy and complexity to the pioneer spectral graph CNNs introduced in [4, 15].", "startOffset": 221, "endOffset": 228}, {"referenceID": 10, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 6, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 22, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 21, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 2, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 13, "context": "Eventually, generic and efficient implementations of CNNs on graphs, as proposed here and related works [12, 8, 24, 23, 4, 15], is essential to pave the way to the development of a new class of deep learning techniques for graph-based data.", "startOffset": 104, "endOffset": 126}, {"referenceID": 20, "context": "1 Graph Signal Processing GSP is an emerging field that aims at bridging the gap between signal processing techniques like wavelet analysis [22] and graph theory such as spectral graph analysis [3, 21].", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "1 Graph Signal Processing GSP is an emerging field that aims at bridging the gap between signal processing techniques like wavelet analysis [22] and graph theory such as spectral graph analysis [3, 21].", "startOffset": 194, "endOffset": 201}, {"referenceID": 19, "context": "1 Graph Signal Processing GSP is an emerging field that aims at bridging the gap between signal processing techniques like wavelet analysis [22] and graph theory such as spectral graph analysis [3, 21].", "startOffset": 194, "endOffset": 201}, {"referenceID": 30, "context": "We refer the reader to [32] for an introduction of the field.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "In this context, the authors of [14, 9, 11] revisited the construction of wavelet operators on graphs.", "startOffset": 32, "endOffset": 43}, {"referenceID": 7, "context": "In this context, the authors of [14, 9, 11] revisited the construction of wavelet operators on graphs.", "startOffset": 32, "endOffset": 43}, {"referenceID": 9, "context": "In this context, the authors of [14, 9, 11] revisited the construction of wavelet operators on graphs.", "startOffset": 32, "endOffset": 43}, {"referenceID": 31, "context": "In [33, 28], the authors designed a technique to perform mutli-scale pyramid transforms on graphs.", "startOffset": 3, "endOffset": 11}, {"referenceID": 26, "context": "In [33, 28], the authors designed a technique to perform mutli-scale pyramid transforms on graphs.", "startOffset": 3, "endOffset": 11}, {"referenceID": 34, "context": "The works of [36, 26, 27] redefined uncertainty principles on graphs, and showed that intuitive concepts may be lost, but can also produce enhanced localization principles for signals on graphs.", "startOffset": 13, "endOffset": 25}, {"referenceID": 24, "context": "The works of [36, 26, 27] redefined uncertainty principles on graphs, and showed that intuitive concepts may be lost, but can also produce enhanced localization principles for signals on graphs.", "startOffset": 13, "endOffset": 25}, {"referenceID": 25, "context": "The works of [36, 26, 27] redefined uncertainty principles on graphs, and showed that intuitive concepts may be lost, but can also produce enhanced localization principles for signals on graphs.", "startOffset": 13, "endOffset": 25}, {"referenceID": 11, "context": "In [13], it was shown how to carry out lasso-based signal regularization on graphs, and studied the intertwined relationships between smoothness and sparsity on graphs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "In [35], the authors investigated compressed sensing recovery conditions for graph spectral signal analysis.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "A preliminary work was proposed with the so-called local reception fields [12, 8], successfully applied to image recognition.", "startOffset": 74, "endOffset": 81}, {"referenceID": 6, "context": "A preliminary work was proposed with the so-called local reception fields [12, 8], successfully applied to image recognition.", "startOffset": 74, "endOffset": 81}, {"referenceID": 2, "context": "A first attempt to design CNNs on graphs was introduced in [4] by adopting a spatial approach.", "startOffset": 59, "endOffset": 62}, {"referenceID": 22, "context": "ShapeNet in [24, 23] is a generalization of CNNs to 3D-meshes, a class of smooth low-dimensional non-Euclidean spaces.", "startOffset": 12, "endOffset": 20}, {"referenceID": 21, "context": "ShapeNet in [24, 23] is a generalization of CNNs to 3D-meshes, a class of smooth low-dimensional non-Euclidean spaces.", "startOffset": 12, "endOffset": 20}, {"referenceID": 4, "context": "In [6] and [30], the authors investigated the construction of Haar wavelet transforms on graphs using a deep hierarchical architecture.", "startOffset": 3, "endOffset": 6}, {"referenceID": 28, "context": "In [6] and [30], the authors investigated the construction of Haar wavelet transforms on graphs using a deep hierarchical architecture.", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "Finally, [4] and [15] introduced a generalization of CNNs on graphs using graph counterparts of grid convolution and downsampling.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": "Finally, [4] and [15] introduced a generalization of CNNs on graphs using graph counterparts of grid convolution and downsampling.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "They were able to learn convolutional filters on graphs using the convolution theorem [22] that defines convolutions as linear operators that diagonalize in the Fourier basis (represented by the eigenvectors of the Laplacian operator).", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "However, although graph convolution directly in the spatial domain is conceivable, it also faces the challenge of matching local neighbourhoods, as pointed out in [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 30, "context": "On the other side, a spectral approach provides a well-defined translation operator on graphs via convolutions with a Kronecker delta function implemented in the spectral domain [32].", "startOffset": 178, "endOffset": 182}, {"referenceID": 5, "context": "An essential operator in spectral graph analysis is the graph Laplacian [7], which combinatorial definition is L = D \u2212W \u2208 Rn\u00d7n where D \u2208 Rn\u00d7n is the diagonal degree matrix with Dii = \u2211 jWij , and its normalized definition is L = In \u2212 D\u22121/2WD\u22121/2 where In is the identity matrix.", "startOffset": 72, "endOffset": 75}, {"referenceID": 30, "context": "We can now define the graph Fourier transform (GFT) of a spatial signal x \u2208 R as x\u0302 = Ux \u2208 R, and its inverse as x = Ux\u0302 [32].", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "One such polynomial, traditionally used in GSP to approximate kernels (like wavelets), is the Chebyshev expansion [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Another option, the Lanczos algorithm [34], which constructs an orthonormal basis of the Krylov subspace KK(L, x) = span{x, Lx, .", "startOffset": 38, "endOffset": 42}, {"referenceID": 3, "context": "It is however well-known that graph clustering is a NP-hard problem [5] and that approximations must be used.", "startOffset": 68, "endOffset": 71}, {"referenceID": 19, "context": "the popular spectral clustering [21]), we are most interested in multilevel clustering algorithms, where each level produces a coarser graph which corresponds to the data domain seen at a different resolution.", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "In this work, we make use of the coarsening phase of the Graclus multilevel clustering algorithm [10], which has been shown to be extremely efficient at clustering a large variety of graphs.", "startOffset": 97, "endOffset": 101}, {"referenceID": 27, "context": "Algebraic multigrid techniques on graphs [29] and the Kron reduction [33] are two methods worth exploring in future works.", "startOffset": 41, "endOffset": 45}, {"referenceID": 31, "context": "Algebraic multigrid techniques on graphs [29] and the Kron reduction [33] are two methods worth exploring in future works.", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "Graclus [10], built on Metis [17], uses a greedy algorithm to compute successive coarser versions of a given graph and is able to minimize several popular spectral clustering objectives.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "Graclus [10], built on Metis [17], uses a greedy algorithm to compute successive coarser versions of a given graph and is able to minimize several popular spectral clustering objectives.", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "We chose the normalized cut [31], which is an excellent clustering energy.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "g\u03b8(\u039b) = B\u03b8, (8) where B \u2208 Rn\u00d7K is the cubic B-spline basis and the parameter \u03b8 \u2208 R is a vector of control points, as proposed in [4, 15].", "startOffset": 129, "endOffset": 136}, {"referenceID": 13, "context": "g\u03b8(\u039b) = B\u03b8, (8) where B \u2208 Rn\u00d7K is the cubic B-spline basis and the parameter \u03b8 \u2208 R is a vector of control points, as proposed in [4, 15].", "startOffset": 129, "endOffset": 136}, {"referenceID": 2, "context": "2 rather than the simple agglomerative method of [4, 15].", "startOffset": 49, "endOffset": 56}, {"referenceID": 13, "context": "2 rather than the simple agglomerative method of [4, 15].", "startOffset": 49, "endOffset": 56}, {"referenceID": 18, "context": "1 Revisiting Standard CNNs on MNIST To validate our model, we first apply it to the Euclidean case with the benchmark MNIST classification problem [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "20NEWS consists of 18,846 (11,314 for training and 7,532 for testing) text documents associated with 20 classes [16].", "startOffset": 112, "endOffset": 116}, {"referenceID": 23, "context": "To test our model, we constructed a 16-NN graph of the word2vec [25] embedding of those words, which produced a graph of n = |V| = 10, 000 nodes and |E| = 132, 834 edges.", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "All CNN models were trained for 20 epochs by the Adam optimizer [18] with a learning rate of 0.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "3 Numerical Comparison between Spectral Filters In this section, we compare the proposed spectral filters to the non-parametric filters, and those proposed in [4, 15] on the MNIST and 20NEWS datasets.", "startOffset": 159, "endOffset": 166}, {"referenceID": 13, "context": "3 Numerical Comparison between Spectral Filters In this section, we compare the proposed spectral filters to the non-parametric filters, and those proposed in [4, 15] on the MNIST and 20NEWS datasets.", "startOffset": 159, "endOffset": 166}, {"referenceID": 2, "context": "Table 3 reports that the proposed kernel parametrization outperforms [4, 15] as well as the non-parametric filters, which are not localized and require O(n) parameters to learn.", "startOffset": 69, "endOffset": 76}, {"referenceID": 13, "context": "Table 3 reports that the proposed kernel parametrization outperforms [4, 15] as well as the non-parametric filters, which are not localized and require O(n) parameters to learn.", "startOffset": 69, "endOffset": 76}, {"referenceID": 2, "context": "Accuracy Dataset Architecture Non-Param (2) Spline (8) [4, 15] Chebyshev (4) MNIST GC10 95.", "startOffset": 55, "endOffset": 62}, {"referenceID": 13, "context": "Accuracy Dataset Architecture Non-Param (2) Spline (8) [4, 15] Chebyshev (4) MNIST GC10 95.", "startOffset": 55, "endOffset": 62}, {"referenceID": 2, "context": "The training time of our model scales as O(n), while [4, 15] scales as O(n).", "startOffset": 53, "endOffset": 60}, {"referenceID": 13, "context": "The training time of our model scales as O(n), while [4, 15] scales as O(n).", "startOffset": 53, "endOffset": 60}, {"referenceID": 23, "context": "Another approach is to learn embeddings on the corpus with word2vec [25] or to use pre-trained embeddings on Google News (given by the authors).", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "We used the LSHForest [2] on the learned word2vec embedding.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "Compared with the first works of spectral graph CNNs introduced in [4, 15], our model provides a strict control over the local support of filters, is computationally more efficient by avoiding an explicit use of the Graph Fourier basis, and experimentally shows a better test accuracy.", "startOffset": 67, "endOffset": 74}, {"referenceID": 13, "context": "Compared with the first works of spectral graph CNNs introduced in [4, 15], our model provides a strict control over the local support of filters, is computationally more efficient by avoiding an explicit use of the Graph Fourier basis, and experimentally shows a better test accuracy.", "startOffset": 67, "endOffset": 74}, {"referenceID": 13, "context": "Besides, we addressed the three concerns raised by [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "Along this line, the idea of [15] to learn the input graph from data is highly meaningful.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 7, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 9, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 4, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 28, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 118, "endOffset": 136}, {"referenceID": 31, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 175, "endOffset": 179}, {"referenceID": 34, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 218, "endOffset": 230}, {"referenceID": 24, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 218, "endOffset": 230}, {"referenceID": 25, "context": "On the other hand, we will also improve this framework with tools developed in GSP, including graph wavelet operators [14, 9, 11, 6, 30], spectral graph coarsening techniques [33], and improved localization properties [36, 26, 27].", "startOffset": 218, "endOffset": 230}], "year": 2016, "abstractText": "Convolutional neural networks (CNNs) have greatly improved state-of-the-art performances in a number of fields, notably computer vision and natural language processing. In this work, we are interested in generalizing the formulation of CNNs from low-dimensional regular Euclidean domains, where images (2D), videos (3D) and audios (1D) are represented, to high-dimensional irregular domains such as social networks or biological networks represented by graphs. This paper introduces a formulation of CNNs on graphs in the context of spectral graph theory. We borrow the fundamental tools from the emerging field of signal processing on graphs, which provides the necessary mathematical background and efficient numerical schemes to design localized graph filters efficient to learn and evaluate. As a matter of fact, we introduce the first technique that offers the same computational complexity than standard CNNs, while being universal to any graph structure. Numerical experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs, as long as the graph is well-constructed.", "creator": "LaTeX with hyperref package"}}}