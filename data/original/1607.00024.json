{"id": "1607.00024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Review Based Rating Prediction", "abstract": "Recommendation systems are an important units in today's e-commerce applications, such as targeted advertising, personalized marketing and information retrieval. In recent years, the importance of contextual information has motivated generation of personalized recommendations according to the available contextual information of users.", "histories": [["v1", "Thu, 30 Jun 2016 20:16:58 GMT  (266kb,D)", "http://arxiv.org/abs/1607.00024v1", "9 pages"], ["v2", "Tue, 5 Jul 2016 18:50:22 GMT  (180kb,D)", "http://arxiv.org/abs/1607.00024v2", "9 pages"], ["v3", "Wed, 27 Jul 2016 11:06:09 GMT  (180kb,D)", "http://arxiv.org/abs/1607.00024v3", "9 pages"], ["v4", "Thu, 28 Jul 2016 07:48:46 GMT  (267kb,D)", "http://arxiv.org/abs/1607.00024v4", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["tal hadad"], "accepted": false, "id": "1607.00024"}, "pdf": {"name": "1607.00024.pdf", "metadata": {"source": "CRF", "title": "Review-Based Rating Prediction", "authors": ["Tal Hadad"], "emails": [], "sections": [{"heading": null, "text": "Compared to the traditional systems which mainly utilize users\u2019 rating history, review-based recommendation hopefully provide more relevant results to users. We introduce a review-based recommendation approach that obtains contextual information by mining user reviews. The proposed approach relate to features obtained by analyzing textual reviews using methods developed in Natural Language Processing (NLP) and information retrieval discipline to compute a utility function over a given item. An item utility is a measure that shows how much it is preferred according to user\u2019s current context.\nIn our system, the context inference is modeled as similarity between the users reviews history and the item reviews history. As an example application, we used our method to mine contextual data from customers\u2019 reviews of movies and use it to produce review-based rating prediction. The predicted ratings can generate recommendations that are item-based and should appear at the recommended items list in the product page. Our evaluations (surprisingly) suggest that our system can help produce better prediction rating scores in comparison to the standard prediction methods.\nIntroduction In recent years, recommendation systems (RecSys) have been extensively used in various domains to recommend items of interest to users based on their profiles. RecSys are an integral part of many online stores such as Alibaba.com, Amazon.com, etc. One of the most famous examples of a recommendation system is Amazon [? ]. This system contains movie ratings for over 100,000 movies.\nA user\u2019s profile is a reflection of the user\u2019s previous selections and preferences that can be captured as rating scores or textual review given to different items in the system. Using preference data, different systems have been developed to produce personalized recommendations based on collaborative filtering, content-based or a hybrid approach.\nDespite the broad used of such recommendation systems, they fail to consider the users\u2019 latent preferences, thus may result in performance degradation. For example, a customer who has once viewed a movie with his friend\u2019s child may repeatedly receive suggestions to view kid\u2019s movies as the recommendation algorithm select base on the whole history in user\u2019s profile without prioritizing his interests. To address this issue, review-based recommendation systems has been introduced.\nContextual information about a user preference can be explicit or implicit and can be inferred in different ways such as user score ratings or textual reviews. We concentrate on deriving context from textual reviews.\nAs an example application of our approach, we have used our method to mine contextual data from customers\u2019 reviews of movies domain.\nIn order to evaluate our method, we have used Amazon movies reviews [? ]. The reason for choosing this dataset is that users usually provide some contextual cues in their comments. For example, they may mention that they are very fond of a specific actor or director, or they may express their opinions about the\n1\nar X\niv :1\n60 7.\n00 02\n4v 1\n[ cs\n.I R\n] 3\n0 Ju\nn 20\n16\nmovie subject or genre that are important to them. In this dataset each review contains an overall rating, and textual comment.\nMethod We assume that user reviews have contextual data about the user preferences, thus comparing the similarity with the item reviews can infer similarity between the two (user preference and item). Moreover, similarity between two users\u2019 reviews can infer similarity between the two users\u2019 preferences. We use this approach to predict the rating score that a user will rate an item."}, {"heading": "1 User Context Representation", "text": "Each user will be represented as a set of his reviews, as presented in figure 1a. User representation composed of 5 strings, for each possible rating value. Each string is a concatenation of the user\u2019s reviews with the corresponding rating value."}, {"heading": "2 Item Context Representation", "text": "Similar to user representation, each item will be represented as a set of his reviews, as presented in figure 1b. Item representation composed of 5 strings, for each possible rating value. Each string is a concatenation of the item\u2019s reviews with the corresponding rating value."}, {"heading": "3 Preproccesing", "text": "In this study, we deal with textual dataset (corpuse) which presented in natural-language form (human language), which present difficulties as described in Stanford Handbook [? ]. Therefore, text normalization is required in order to reduce language diversity including transformation to canonical form for further processing. We achieve this by performing textual normalization as presented in algorithm 1. The main\nBen-Gurion University of the Negev, Faculty of Engineering Science Department of Information Systems Engineering\nPage 3 of 9 steps are: (1) removal of punctuation, numbers and stop-words as described in Onix [? ] ; (2) replacement of slang words as described in Twitter Dictionary[? ]; and (3) stem each word to its root, as described in Stanford Handbook [? ].\nAlgorithm 1 Textual Normalization Step 1: Convert characters into lower cases Step 2: Remove punctuation Step 3: Remove numbers Step 4: Remove stop words, described in Onix [? ] Step 5: Replace slang terms, described in Twitter Dictionary [? ] Step 6: Word stemming to its root, described in Stanford Handbook [? ]"}, {"heading": "4 Predicting Item Rating: User-Item Based", "text": "For user u and item i, we predict the rating (as presented in figure 2) by computing utility function for each possible rating value r (1-5 stars) as follow: First, extract and normalize reviews related to u and i (i.e. all reviews written by u or written about i). Second, for each possible rating r (r \u2208{1,2,3,4,5}) compare the similarity between u and i reviews who has been rated r. Finally, return the value of r which produced maximum similarity, as the predicted rating for i by u. We present a pseudo code for this process in algorithm 2, and a graphic representation in figure 2.\nAlgorithm 2 User-Item Rating Prediction Process 1: procedure PREDICT(userID, itemID) 2: userReviews\u2190 list of userID\u2019s reviews 3: for each review RVu \u2208 userReviews do 4: RVu \u2190 normalized text of RVu 5: end for 6: itemReviews\u2190 list of itemID\u2019s reviews 7: for each review RVi \u2208 itemReviews do 8: RVi \u2190 normalized text of RVi 9: end for 10: for each possible rating value r \u2208{1,2,3,4,5} do 11: userReviewsr \u2190reviews whose rated r in userReviews 12: itemReviewsr \u2190reviews whose rated r in itemReviews 13: similarityr \u2190similarity value between userReviewsr and itemReviewsr 14: end for 15: return maxr(similarityr) 16: end procedure\nSimilarity of two textual collections (user and item reviews with same rating score) can be computed in several ways, for that we need to answer two question: (1) what to compare, e.g. each user review to each item review or concatenated text of all reviews in each side; and (2) how to compare and aggregate, information retrieval discipline have a lot of functions to offer in this subject, e.g. cosine similarity.\nBen-Gurion University of the Negev, Faculty of Engineering Science Department of Information Systems Engineering\nFor user-item based approach we experimented with three different comparison methods, that are different answers to the mentioned questions, proposed (and argued) by this study authors. For a given set of user\u2019s reviews rated r, and a set of item\u2019s reviews rated r (line 13 in algorithm 2), similarity defined in three ways:\n1. CM : Cosine similarity between concatenated text of user reviews and concatenated text of item reviews, as presented in figure (3).\n2. MCM : Maximum cosine similarity between each user review and each item review, as presented in figure (4).\n3. ACM : Average cosine similarity between each user review and each item review, as presented in figure (4).\nBen-Gurion University of the Negev, Faculty of Engineering Science Department of Information Systems Engineering\nPage 5 of 9 5 Predicting Item Rating: User-User Based\nSimilar User-Item Based approach, for given user u and item i, we predict i rating score rated by u. However unlike previous approach, we use collaborate filtering (CF) [? ] method when users similarity (W vector in equation 1) defined as cosine similarity between two users textual reviews. CF is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating).\nThe prediction process is as follow: First, we extract and normalize reviews that was written by u. Second, for all users ui that rated item i, we extract and normalize reviews that was written by ui. Third, for each user ui, and for each possible rating r (r \u2208{1,2,3,4,5}), we compute cosine similarity between u and ui reviews that been rated r, as presented in figure 5. Finally, using equation 1, we predict the rating for item i.\nr\u0302u,i = r\u0304u +\n\u2211 wui,u \u2217 (rui,i \u2212 r\u0304ui)\u2211\nwui,u . (1)\nWhere \u02c6ru,i is the predicted item i rating rated by user u, r\u0304u is ratings average of u, wui,u similarity value between u and user ui and rui,i rating for item i by ui.\nWe present a pseudo code for this process in algorithm 3, and a graphic representation of the algorithm in figure 5.\nAlgorithm 3 User-User Rating Prediction Process procedure PREDICT(userID, itemID)\nusersReviews\u2190 Dictionary of Dictionary {user, {rating, concatReviews}} 3: for each review RVu \u2208 usersReviews do\nRVu \u2190 normalized text of RVu end for\n6: for each user ui \u2208 usersRatings[itemID] do for each review RVu \u2208 usersRevies[userID] do for each possible rating value r \u2208{1,2,3,4,5} do 9: userReviewsr \u2190reviews whose rated r in usersReviews[userID]\nuiReviewsr \u2190reviews whose rated r in usersReviews[ui] similarityr \u2190similarity value between userReviewsr and uiReviewsr\n12: end for wui,u = maxr(similarityr) end for 15: end for\nreturn r\u0304u + \u2211\nwui,u\u2217(rui,i\u2212 \u00afrui )\u2211 wui,u\nend procedure\nSimilar User-Item Based approach, we deal with similarity of two textual collections issue, in order to observe there effect on the results. A graphic representation of the two algorithm in figure 5. For a given set of the user\u2019s (u) reviews rated r, and a set of users that rated the given item (ui) reviews rated ru (line 13 in algorithm 3):\n1. CF \u2212MCM : Maximum cosine similarity between concatenated text of u\u2019s reviews in rating j and concatenated text of ui reviews in rating j.\nBen-Gurion University of the Negev, Faculty of Engineering Science Department of Information Systems Engineering\n2. CF \u2212 ACM : Average cosine similarity between concatenated text of u\u2019s reviews in rating j and concatenated text of ui\u2019s reviews in rating j. (changing line 13 in algorithm 3 to AVG).\nEvaluation\n6 Dataset1\nThis dataset consists of movie reviews from amazon. The data span a period of more than 10 years, including all \u223c8 million reviews up to October 2012. Reviews include product and user information, ratings, and a plaintext review. The dataset can be found at Stanford Large Network Dataset Collection [? ].\nSource: J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013.\nDataset statistics: Number of reviews 7,911,684\nNumber of users 889,176 Number of products 253,059\nUsers with > 50 reviews 16,341 Median no. of words per review 101\nTimespan Aug 1997 - Oct 2012\nData Format: product/productId: B00006HAXW review/userId: A1RSDE90N6RSZF review/profileName: Joseph M. Kotow review/helpfulness: 9/9 review/score: 5.0 review/time: 1042502400\n1dataset description was taken from http://snap.stanford.edu/data/web-Amazon.html\nBen-Gurion University of the Negev, Faculty of Engineering Science Department of Information Systems Engineering\nPage 7 of 9 review/summary: Pittsburgh - Home of the OLDIES review/text: \u201dI have all of the doo wop DVD\u2019s and this one is as good or better than the 1st ones. Remember once these performers are gone, we\u2019ll never get to see them again. Rhino did an excellent job and if you like or love doo wop and Rock n Roll you\u2019ll LOVE this DVD !!\u201d. Where:\n\u2022 product/productId: asin, e.g. amazon.com/dp/B00006HAXW\n\u2022 review/userId: id of the user, e.g. A1RSDE90N6RSZF\n\u2022 review/profileName: name of the user\n\u2022 review/helpfulness: fraction of users who found the review helpful\n\u2022 review/score: rating of the product\n\u2022 review/time: time of the review (unix time)\n\u2022 review/summary: review summary\n\u2022 review/text: text of the review"}, {"heading": "7 Results", "text": "Commonly, datasets for recommendation system evaluation are sparse dataset, thus a second preprocessing phase has been added to the method in order to prune the ratings matrix by removing all those users and items that have less than 5 ratings. Moreover, due to time limits we could not execute our methods over the \u223c8 million reviews, thus we used 100,000 reviews randomly selected from the Amazon dataset. We used two datasets: (1) imbalanced dataset that contains 55% 5-star reviews, 21% 4-star reviews, 10% 3- stars reviews. 6% 2-stars reviews and 8% 1-star reviews; and (2) balanced dataset that contains 33% 5-star reviews, 31% 4-star reviews, 15% 3-stars reviews. 9% 2-stars reviews and 12% 1-star reviews. Each dataset was splitted into two parts: (1) 80% training dataset and (2) 20% testing dataset, for evaluation purposes.\nIn previous chapter, we introduced a review-based recommendation system that produce recommendations for a user based on a utility function that depends both the user\u2019s context and also the predicted rating for that item. As for recommendations that based on predicted ratings, it is logical to use metrics such as MAE and RMSE that compare the predicted rating with the actual ones."}, {"heading": "7.1 MAE and RMSE", "text": "The mean absolute error (MAE) is a quantity used to measure how close predictions are to the real observations. The MSE is given by:\nMAE = 1\nn n\u2211 i=1 |fi \u2212 yi| = 1 n n\u2211 i=1 |ei| .\nThe mean absolute error is an average of the absolute errors |ei| = |fi \u2212 yi|, where fi is the prediction and yi the true value.\nBen-Gurion University of the Negev, Faculty of Engineering Science Department of Information Systems Engineering\nPage 8 of 9 The root mean square error (RMSE) is also a frequently used measure of the differences between predicted values and the values actually observed. The RMSD represents the sample standard deviation of the differences between predicted values and observed values. The RMSE is given by:\nRMSE =\n\u221a\u2211n t=1(y\u0302t \u2212 yt)2\nn .\nThe RMSE of predicted values y\u0302t for times t of a dependent variable yt is computed for n different predictions as the square root of the mean of the squares of the deviations."}, {"heading": "7.2 Preproccesing", "text": "First we evaluated how effective our preproccesing phase described in algorithm 1, used to normalized the texual content by reduce diversity of human language to canonical form. For this task, we executed CM , MCM and ACM algorithms without preproccesing and compared the results to CM , MCM and ACM with preproccesing. Result presented in table 1 shows a significant improvement when using our preproccesing phase."}, {"heading": "7.3 Baseline", "text": "Popular automatic predictions method is collaborative filtering (CF) [? ]. CF is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same ratings as a person B on an issue, A is more likely to have B\u2019s opinion on a different issue x than to have the opinion on x of a person chosen randomly. CF equation to predict user rating over an item was mentioned in equation 1, where vector Wu is users similarities for user u, computed by pearson (CF \u2212 Pearson) and cosine (CF \u2212 Cosine) similarity functions.\nIn this study we evaluated our method\u2019s prediction over the baseline results of CF and other automatic predictions method presented in this course. The results are presented in table 2.\nConclusion This study has presented a novel approach for mining context from unstructured text and using it to produce predicted rating scores for a given item and given user. In our proposed methods, the context inference is modeled in two ways: (1) cosine similarity between user and item textual reviews; and (2) cosine similarity between user and other users textual reviews. The inferred context is used to define a utility function for all possible rating values for an item, reflecting how much each item rating value reviews is similar to a user rating value reviews.\nFive novel prediction method was presented: CM , MCM , ACM , CF \u2212MCM and CF \u2212 ACM . As an example application, we have used our methods to mine contextual data from customers\u2019 reviews\nBen-Gurion University of the Negev, Faculty of Engineering Science Department of Information Systems Engineering\nof movies in Amazon\u2019s dataset and used it to produce review-based recommendations. Our evaluations (surprisingly) suggest that our system can help produce better prediction rating scores in comparison to the standard prediction methods.\nBen-Gurion University of the Negev, Faculty of Engineering Science Department of Information Systems Engineering"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Recommendation systems are an important units in today\u2019s e-commerce applications, such as targeted advertising, personalized marketing and information retrieval. In recent years, the importance of contextual information has motivated generation of personalized recommendations according to the available contextual information of users. Compared to the traditional systems which mainly utilize users\u2019 rating history, review-based recommendation hopefully provide more relevant results to users. We introduce a review-based recommendation approach that obtains contextual information by mining user reviews. The proposed approach relate to features obtained by analyzing textual reviews using methods developed in Natural Language Processing (NLP) and information retrieval discipline to compute a utility function over a given item. An item utility is a measure that shows how much it is preferred according to user\u2019s current context. In our system, the context inference is modeled as similarity between the users reviews history and the item reviews history. As an example application, we used our method to mine contextual data from customers\u2019 reviews of movies and use it to produce review-based rating prediction. The predicted ratings can generate recommendations that are item-based and should appear at the recommended items list in the product page. Our evaluations (surprisingly) suggest that our system can help produce better prediction rating scores in comparison to the standard prediction methods. Introduction In recent years, recommendation systems (RecSys) have been extensively used in various domains to recommend items of interest to users based on their profiles. RecSys are an integral part of many online stores such as Alibaba.com, Amazon.com, etc. One of the most famous examples of a recommendation system is Amazon [? ]. This system contains movie ratings for over 100,000 movies. A user\u2019s profile is a reflection of the user\u2019s previous selections and preferences that can be captured as rating scores or textual review given to different items in the system. Using preference data, different systems have been developed to produce personalized recommendations based on collaborative filtering, content-based or a hybrid approach. Despite the broad used of such recommendation systems, they fail to consider the users\u2019 latent preferences, thus may result in performance degradation. For example, a customer who has once viewed a movie with his friend\u2019s child may repeatedly receive suggestions to view kid\u2019s movies as the recommendation algorithm select base on the whole history in user\u2019s profile without prioritizing his interests. To address this issue, review-based recommendation systems has been introduced. Contextual information about a user preference can be explicit or implicit and can be inferred in different ways such as user score ratings or textual reviews. We concentrate on deriving context from textual reviews. As an example application of our approach, we have used our method to mine contextual data from customers\u2019 reviews of movies domain. In order to evaluate our method, we have used Amazon movies reviews [? ]. The reason for choosing this dataset is that users usually provide some contextual cues in their comments. For example, they may mention that they are very fond of a specific actor or director, or they may express their opinions about the 1 ar X iv :1 60 7. 00 02 4v 1 [ cs .I R ] 3 0 Ju n 20 16", "creator": "LaTeX with hyperref package"}}}