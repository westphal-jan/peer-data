{"id": "1607.00534", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jul-2016", "title": "Text comparison using word vector representations and dimensionality reduction", "abstract": "This paper describes a technique to compare large text sources using word vector representations (word2vec) and dimensionality reduction (t-SNE) and how it can be implemented using Python. The technique provides a bird's-eye view of text sources, e.g. text summaries and their source material, and enables users to explore text sources like a geographical map. Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like \"capital city of\". Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. The technique uses the word2vec model from the gensim Python library and t-SNE from scikit-learn.", "histories": [["v1", "Sat, 2 Jul 2016 17:17:22 GMT  (1059kb,D)", "http://arxiv.org/abs/1607.00534v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hendrik heuer"], "accepted": false, "id": "1607.00534"}, "pdf": {"name": "1607.00534.pdf", "metadata": {"source": "CRF", "title": "Text comparison using word vector representations and dimensionality reduction", "authors": ["Hendrik Heuer"], "emails": ["hendrikh@kth.se"], "sections": [{"heading": null, "text": "Index Terms\u2014Text Comparison, Topic Comparison, word2vec, t-SNE"}, {"heading": "1 INTRODUCTION", "text": "When summarizing a large text, only a subset of the available topics and stories can be taken into account. The decision which topics to cover is largely editorial. This paper introduces a tool that assists this editorial process using word vector representations and dimensionality reduction. It enables a user to visually identify agreement and disagreement between two text sources.\nThere are a variety of different ways to approach the problem of visualizing the topics present in a text. The simplest approach is to look at unique words and their occurrences and visualize the words in a list. Topics could also be visualized using word clouds, where the font size of a word is determined by the frequency of the word. Word clouds have a variety of shortcomings: they can only visualize small subsets, they focus on the most frequent words and they do not take synonyms and semantically similar words into account.\nThis paper describes a human-computer interaction-inspired approach of comparing two text sources. The approach yields a bird\u2019s-eye view of different text sources, including text summaries and their source material, and enables users to explore a text source like a geographical map. As similar words are close to each other, the user can visually identify clusters of topics that are present in the text.\nThis paper describes a tool, which can be used to visualize the topics in a single text source as well as to compare different\n* Corresponding author: hendrikh@kth.se \u2020 Aalto University, Finland\nCopyright c\u25cb 2015 Hendrik Heuer. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. http://creativecommons.org/licenses/by/3.0/\ntext sources. To compare the topics in source A and source B, three different sets of words can be computed: a set of unique words in source A, a set of unique words in source B as well as the intersection set of words both in source A and B. These three sets are then plotted at the same time. For this, a colour is assigned to each set of words. This enables the user to visually compare the different text sources and makes it possible to see which topics are covered where. The user can explore the word map and zoom in and out. He or she can also toggle the visibility, i.e. show and hide, certain word sets.\nThe comparison can be used to visualize the difference between a text summary and its source material. It can also help to compare Wikipedia revisions in regards to the topics they cover. Another possible application is the visualization of heterogeneous data sources like a list of search queries and keywords.\nThe Github repository of the tool includes an online demo [Heu15]. The tool can be used to explore the precomputed topic sets of the Game of Thrones Wikipedia article revisions from 2013 and 2015. The repository also includes the precomputed topic sets for the Wikipedia article revisions for the articles on World War 2, Facebook, and the United States of America."}, {"heading": "2 DISTRIBUTIONAL SEMANTIC MODELS", "text": "The distributional hypothesis by Harris states that words with similar meaning occur in similar contexts [Sah05]. This implies that the meaning of a word can be inferred from its distribution across contexts. The goal of distributional semantics is to find a representation, e.g. a vector, that approximates the meaning of a word [Bru14]. The traditional approach to statistical modeling of language is based on counting frequencies of occurrences of short word sequences of length up to N and did not exploit distributed representations [Cun15]. Distributional semantics takes word co-occurrence in context windows into account.\nThe general idea behind word space models is to use distributional statistics to generate high-dimensional vector spaces, where a word is represented by a context vector that encodes semantic similarity [Sah05]. The representations are called distributed representations because the features are not mutually exclusive and because their configurations correspond to the variations seen in the observed data [Cun15].\nar X\niv :1\n60 7.\n00 53\n4v 1\n[ cs\n.C L\n] 2\nJ ul\n2 01\n6\nLeCun et al. provide the example of a news story. When the task is to predict the next word in a news story, the learned word vectors for Tuesday and Wednesday will be very similar as they can be easily replaced by each other when used in a sentence [Cun15].\nThere are a variety of computational models that implement the distributional hypothesis, including word2vec [Che13], GloVe [Pen14], dependency-based word embeddings [Lev14] and Random Indexing [Sah05]. There are a variety of Python implementations of these techniques. word2vec is available in gensim [R\u030ceh10]. For GloVe, the C source code was ported to Python [Gau15, Kul15]. The dependency-based word embeddings by Levy and Goldberg are implemented in spaCy [Hon15]. Random Indexing is available in an implementation by Joseph Turian [Tur10].\nFor this paper, word2vec was selected because Mikolov et al. provide 1.4 million pre-trained entity vectors trained on 100 billion words from various news articles in the Google News dataset [Che13]. However, other models might perform equally well for the purpose of text comparison. Moreover, custom word vectors trained on a large domain-specific dataset, e.g. the Wikipedia encyclopedia for the Wikipedia revision comparison, could potentially yield even better results."}, {"heading": "2.1 word2vec", "text": "word2vec is a tool developed by Mikolov, Sutskever, Chen, Corrado, and Dean at Google. The two model architectures in the C tool were made available under an open-source license [Mik13]. Gensim provides a Python reimplementation of word2vec [R\u030ceh10].\nWord vectors encode semantic meaning and capture many different degrees of similarity [Lev14]. word2vec word vectors can capture linguistic properties such as gender, tense, plurality, and even semantic concepts such as \"is the capital city of\". word2vec captures domain similarity while other more dependency-based approaches capture functional similarity.\nIn the word2vec vector space, linear algebra can be used to exploit the encoded dimensions of similarity. Using this, a computer system can complete tasks like the Scholastic Assessment Test (SAT) analogy quizzes, that measure relational similarity.\nking\u2212man+woman \u2248 queen\nIt works for the superlative:\nf astest \u2212 f ast + slow \u2248 slowest\nAs well as the past participle:\nwoken\u2212wake+be \u2248 been\nIt can infer the Finnish national sport from the German national sport.\nf ootball \u2212Germany+Finland \u2248 hockey\nBased on the last name of the current Prime Minister of the United Kingdom, it identifies the last name of the German Bundeskanzlerin:\nCameron\u2212England +Germany \u2248 Merkel\nThe analogies can also be applied to the national dish of a country:\nhaggis\u2212Scotland +Germany \u2248Currywurst\nFig. 1 shows the clusters of semantically similar words and how they form semantic units, which can be easily interpreted by humans."}, {"heading": "3 DIMENSIONALITY REDUCTION WITH T-SNE", "text": "t-distributed Stochastic Neighbour Embedding (t-SNE) is a dimensionality reduction technique that retains the local structure of data and that helps to visualize large real-world datasets with limited computational demands [Maa08]. Vectors that are similar in a high-dimensional vector space get represented by two- or three-dimensional vectors that are close to each other in the two- or three-dimensional vector space. Dissimilar high-dimensional vectors are distant in the two- or threedimensional vector space. Meanwhile, the global structure of the data and the presence of clusters at several scales is revealed. t-SNE is well-suited for high-dimensional data that lies on several different, but related, low-dimensional manifolds [Maa08].\nt-SNE achieves this by minimizing the Kullback-Leibler divergence between the joint probabilities of the highdimensional data and the low-dimensional representation. The Kullback-Leibler divergence measures the faithfulness with which a probability distribution q represents a probability distribution p by a discrete scalar and equals zero if the distributions are the same [Maa08]. The Kullback-Leibler divergence is minimized using the gradient descent method. In contrast to other Stochastic Neighbor Embedding methods that use Gaussian distributions, it uses a Student t-distribution."}, {"heading": "4 IMPLEMENTATION", "text": "The text comparison tool implements a workflow that consists of a Python tool for the back-end and a Javascript tool for the front-end. With the Python tool, a text is converted into a collection of two-dimensional word vectors. These are visualized using the Javascript front-end. With the Javascript front-end, the user can explore the word map and zoom in and out to investigate both the local and the global structure of the text sources. The Javascript front-end can be published online.\nThe workflow of the tool includes the following four steps:"}, {"heading": "4.1 Pre-processing", "text": "In the pre-processing step, all sentences are tokenized to extract single words. The tokenization is done using the Penn Treebank Tokenizer implemented in the Natural Language Processing Toolkit (NLTK) for Python [Bir09]. Alternatively, this could also be achieved with a regular expression.\nUsing a hash map, all words are counted. Only unique words, i.e. the keys of the hash map, are taken into account for the dimensionality reduction. The 3000 most frequent English words according to a frequency list collected from Wikipedia are ignored to reduce the amount of data."}, {"heading": "4.2 Word representations", "text": "For all unique non-frequent words, the word representation vectors are collected from the word2vec model from the gensim Python library [R\u030ceh10]. Each word is represented by an N-dimensional vector (N=300, informed by the best accuracy in [Mik13] and following the default in [Che13]). from gensim.models import Word2Vec\nmodel = Word2Vec.load_word2vec_format( word_vectors_filename, binary=True )\nfor word in words: if word in model: print model[word]"}, {"heading": "4.3 Dimensionality Reduction", "text": "The resulting N-dimensional word2vec vectors are projected down to 2D using the t-SNE Python implementation in scikitlearn [Ped11].\nIn the dimensionality reduction step, the N-dimensional word vectors are projected down to a two-dimensional space so that they can be easily visualized in a 2D coordinate system (see Fig. 2).\nFor the implementation, the t-SNE implementation in scikitlearn is used: from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2) tsne.fit_transform(word_vectors)"}, {"heading": "4.4 Visualization", "text": "After the dimensionality reduction, the vectors are exported to a JSON file. The vectors are visualized using the D3.js JavaScript data visualization library [Bos12]. Using D3.js, an\ninteractive map was developed. With this map, the user can move around and zoom in and out. The colour coding helps to judge the ratio of dissimilar and similar words. At the global scale, the map can be used to assess how similar two text sources are to each other. At the local scale, clusters of similar words can be explored."}, {"heading": "5 RESULTS", "text": "As with many unsupervised methods, the evaluation can be difficult and the quality of the visualization is hard to quantify. The goal of this section is, therefore, to introduce relevant use cases and illustrate how the technique can be applied.\nThe flow described in the previous section can be applied to different revisions of Wikipedia articles. For this, a convenience sample of the most popular articles in 2013 from the English Wikipedia was used. For each article, the last revision from the 31st of December 2013 and the most recent revision on the 26th of May 2015 were collected. The assumption was that popular articles will attract sufficient changes to be interesting to compare. The list of the most popular Wikipedia articles includes Facebook, Game of Thrones, the United States, and World War 2.\nThe article on Game of Thrones was deemed especially illustrative for the task of comparing the topics in a text, as the storyline of the TV show developed between the two different snapshot dates as new characters were introduced. Other characters became less relevant and were removed from the article. The article on World War 2 was especially interesting as one of the motivations for the topic tool is to find subtle changes in data.\nFig. 3 shows how different the global cluster, i.e. the full group of words on the minimum zoom setting, of the Wikipedia articles on the United States, Game of Thrones and World War 2 are.\nFig. 4 shows four screenshots of the visualization of the Wikipedia articles on the United States including an overview and detail views that only show the intersection set of words, words only present in the 2013 revision of the article, and words only present in the 2015 revision of the article.\nWhen applied to Game of Thrones, it is e.g. easy to visually compare names that were removed since 2013 and that were added in 2015 (Fig. 5). Using the online demo available [Heu15], this technique can be applied to the Wikipedia articles on the United States and World War 2.\nThe technique can also be applied to visualize the Google search history of an individual. Similar words are represented by similar vectors. Thus, terms related to different topics,\ne.g. technology, philosophy or music, will end up in separate clusters."}, {"heading": "6 CONCLUSION", "text": "Word2vec word vector representations and t-SNE dimensionality reduction can be used to provide a bird\u2019s-eye view of different text sources, including text summaries and their source material. This enables users to explore a text source like a geographical map.\nThe paper gives an overview of an ongoing investigation of the usefulness of word vector representations and dimensionality reduction in the text and topic comparison context. The major flaw of this paper is that the introduced text visualization and text comparison approach is not validated empirically.\nAs many researchers publish their source code under open source licenses and as the Python community embraces and supports these publications, it was possible to integrate the\nfindings from the literature review of my Master\u2019s thesis into a useable tool. Distributed representations are an active field of research. New findings on word, sentence or paragraph vectors can be easily integrated into the workflow of the tool.\nBoth the front-end and the back-end of the implementation were made available on GitHub under GNU General Public License 3 [Heu15]. The repository includes the necessary Python code to collect the word2vec representations using Gensim, to project them down to 2D using t-SNE and to output them as JSON. The repository also includes the front-end code to explore the JSON file as a geographical map.\nThe tool can be used in addition to topic modeling techniques like LDA. It enables the comparison of large text sources at a glance and is aimed at similar text sources with subtle differences."}], "references": [{"title": "Natural Language Processing with Python, 1st ed", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "D3.js - Data-Driven Documents", "author": ["M. Bostock"], "venue": null, "citeRegEx": "Bostock,? \\Q2012\\E", "shortCiteRegEx": "Bostock", "year": 2012}, {"title": "Multimodal Distributional Semantics,", "author": ["E. Bruni", "N.K. Tran", "M. Baroni"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Dependency-Based Word Embeddings,\u201d in Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "author": ["O. Levy", "Y. Goldberg"], "venue": null, "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Visualizing data using tSNE,", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Efficient Estimation of Word Representations in Vector Space,", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, vol. abs/1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "GloVe: Global Vectors for Word Representation,", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "An introduction to random indexing,\u201d in Methods and applications of semantic indexing workshop at the 7th international conference on terminology and knowledge engineering, TKE", "author": ["M. Sahlgren"], "venue": null, "citeRegEx": "Sahlgren,? \\Q2005\\E", "shortCiteRegEx": "Sahlgren", "year": 2005}, {"title": "Software Framework for Topic Modelling with Large Corpora,", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka,? \\Q2010\\E", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka", "year": 2010}], "referenceMentions": [], "year": 2016, "abstractText": "This paper describes a technique to compare large text sources using word vector representations (word2vec) and dimensionality reduction (tSNE) and how it can be implemented using Python. The technique provides a bird\u2019s-eye view of text sources, e.g. text summaries and their source material, and enables users to explore text sources like a geographical map. Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like \"capital city of\". Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. The technique uses the word2vec model from the gensim Python library and t-SNE from scikit-learn.", "creator": "LaTeX with hyperref package"}}}