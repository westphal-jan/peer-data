{"id": "1607.01149", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jul-2016", "title": "Target-Side Context for Discriminative Models in Statistical Machine Translation", "abstract": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.", "histories": [["v1", "Tue, 5 Jul 2016 08:51:21 GMT  (31kb,D)", "http://arxiv.org/abs/1607.01149v1", "Accepted as a long paper for ACL 2016"]], "COMMENTS": "Accepted as a long paper for ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ales tamchyna", "alexander m fraser", "ondrej bojar", "marcin junczys-dowmunt"], "accepted": true, "id": "1607.01149"}, "pdf": {"name": "1607.01149.pdf", "metadata": {"source": "CRF", "title": "Target-Side Context for Discriminative Models in Statistical Machine Translation", "authors": ["Ale\u0161 Tamchyna", "Alexander Fraser", "Ond\u0159ej Bojar", "Marcin Junczys-Dowmunt"], "emails": ["tamchyna@ufal.mff.cuni.cz", "bojar@ufal.mff.cuni.cz", "fraser@cis.uni-muenchen.de", "junczys@amu.edu.pl"], "sections": [{"heading": null, "text": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses."}, {"heading": "1 Introduction", "text": "Discriminative lexicons address some of the core challenges of phrase-based MT (PBMT) when translating to morphologically rich languages, such as Czech, namely sense disambiguation and morphological coherence. The first issue is semantic: given a source word or phrase, which of its possible meanings (i.e., which stem or lemma) should we choose? Previous work has shown that this can be addressed using a discriminative lexicon. The second issue has to do with morphology (and syntax): given that we selected the correct meaning, which of its inflected surface forms is appropriate? In this work, we integrate such a model directly into the SMT decoder. This enables our classifier to extract features not only from the full source sentence but also from a limited targetside context. This allows the model to not only\nhelp with semantics but also to improve morphological and syntactic coherence.\nFor sense disambiguation, source context is the main source of information, as has been shown in previous work (Vickrey et al., 2005), (Carpuat and Wu, 2007), (Gimpel and Smith, 2008) inter alia. Consider the first set of examples in Figure 1, produced by a strong baseline PBMT system. The English word \u201cshooting\u201d has multiple senses when translated into Czech: it may either be the act of firing a weapon or making a film. When the cue word \u201cfilm\u201d is close, the phrase-based model is able to use it in one phrase with the ambiguous \u201cshooting\u201d, disambiguating correctly the translation. When we add a single word in between, the model fails to capture the relationship and the most frequent sense is selected instead. Wider source context information is required for correct disambiguation.\nWhile word/phrase senses can usually be inferred from the source sentence, the correct selection of surface forms requires also information from the target. Note that we can obtain some information from the source. For example, an English subject is often translated into a Czech subject; in which case the Czech word should be in nominative case. But there are many decisions that happen during decoding which determine morphological and syntactic properties of words \u2013 verbs can have translations which differ in valency frames, they may be translated in either active or passive voice (in which case subject and object would be switched), nouns may have different possible translations which differ in gender, etc.\nThe correct selection of surface forms plays a crucial role in preserving meaning in morphologically rich languages because it is morphology rather than word order that expresses relations between words. (Word order tends to be\nar X\niv :1\n60 7.\n01 14\n9v 1\n[ cs\n.C L\n] 5\nJ ul\n2 01\n6\nrelatively free and driven more by semantic constraints rather than syntactic constraints.)\nThe language model is only partially able to capture this phenomenon. It has a limited scope and perhaps more seriously, it suffers from data sparsity. The units captured by both the phrase table and the LM are mere sequences of words. In order to estimate their probability, we need to observe them in the training data (many times, if the estimates should be reliable). However, the number of possible n-grams grows exponentially as we increase n, leading to unrealistic requirements on training data sizes. This implies that the current models can (and often do) miss relationships between words even within their theoretical scope.\nThe second set of sentences in Figure 1 demonstrates the problem of data sparsity for morphological coherence. While the phrase-based system can correctly transfer the morphological case of \u201ccat\u201d and even \u201cblack cat\u201d, the less usual \u201cyellowish cat\u201d is mistranslated into nominative case, even though the correct phrase \u201cyellowish ||| naz\u030cloutlou\u201d exists in the phrase table. A model with a suitable representation of two preceding words could easily infer the correct case in this example.\nOur contributions are the following:\n\u2022 We show that the addition of a feature-rich discriminative model significantly improves translation quality even for large data sizes and that target-side context information consistently further increases this improvement.\n\u2022 We provide an analysis of the outputs which confirms that source-context features indeed help with semantic disambiguation (as is well\nknown). Importantly, we also show that our novel use of target context improves morphological and syntactic coherence.\n\u2022 In addition to extensive experimentation on translation from English to Czech, we also evaluate English to German, English to Polish and English to Romanian tasks, with improvements on translation quality in all tasks, showing that our work is broadly applicable.\n\u2022 We describe several optimizations which allow target-side features to be used efficiently in the context of phrase-based decoding.\n\u2022 Our implementation is freely available in the widely used open-source MT toolkit Moses, enabling other researchers to explore discriminative modelling with target context in MT."}, {"heading": "2 Discriminative Model with Target-Side Context", "text": "Several different ways of using feature-rich models in MT have been proposed, see Section 6. We describe our approach in this section."}, {"heading": "2.1 Model Definition", "text": "Let f be the source sentence and e its translation. We denote source-side phrases (given a particular phrasal segmentation) (f\u03041, . . . , f\u0304m) and the individual words (f1, . . . , fn). We use a similar notation for target-side words/phrases.\nFor simplicity, let eprev, eprev\u22121 denote the words preceding the current target phrase. Assuming target context size of two, we model the following probability distribution:\nP (e|f) \u221d \u220f\n(e\u0304i,f\u0304i)\u2208(e,f)\nP (e\u0304i|f\u0304i, f, eprev, eprev\u22121)\n(1) The probability of a translation is the product of phrasal translation probabilities which are conditioned on the source phrase, the full source sentence and several previous target words.\nLet GEN(f\u0304i) be the set of possible translations of the source phrase f\u0304i according to the phrase table. We also define a \u201cfeature vector\u201d function fv(e\u0304i, f\u0304i, f, eprev, eprev\u22121) which outputs a vector of features given the phrase pair and its context information. We also have a vector of feature weights w estimated from the training data. Then our model defines the phrasal translation probability simply as follows:\nP (e\u0304i|f\u0304i, f, eprev, eprev\u22121)\n= exp(w \u00b7 fv(e\u0304i, f\u0304i, f, eprev, eprev\u22121))\u2211\ne\u0304\u2032\u2208GEN(f\u0304i) exp(w \u00b7 fv(e\u0304\u2032, f\u0304i, f, eprev, eprev\u22121))\n(2)\nThis definition implies that we have to locally normalize the classifier outputs so that they sum to one.\nIn PBMT, translations are usually scored by a log-linear model. Our classifier produces a single score (the conditional phrasal probability) which we add to the standard log-linear model as an additional feature. The MT system therefore does not have direct access to the classifier features, only to the final score."}, {"heading": "2.2 Global Model", "text": "We use the Vowpal Wabbit (VW) classifier1 in this work. Tamchyna et al. (2014) already integrated VW into Moses. We started from their implementation in order to carry out our work. Classifier features are divided into two \u201cnamespaces\u201d:\n\u2022 S. Features that do not depend on the current phrasal translation (i.e., source- and targetcontext features).\n\u2022 T. Features of the current phrasal translation.\nWe make heavy use of feature processing available in VW, namely quadratic feature expansions\n1http://hunch.net/\u02dcvw/\nand label-dependent features. When generating features for a particular set of translations, we first create the shared features (in the namespace S). These only depend on (source and target) context and are therefore constant for all possible translations of a given phrase. (Note that target-side context naturally depends on the current partial translation. However, when we process the possible translations for a single source phrase, the target context is constant.)\nThen for each translation, we extract its features and store them in the namespace T . Note that we do not provide a label (or class) to VW \u2013 it is up to these translation features to describe the target phrase. (And this is what is referred to as \u201clabeldependent features\u201d in VW.)\nFinally, we add the Cartesian product between the two namespaces to the feature set: every shared feature is combined with every translation feature.\nThis setting allows us to train only a single, global model with powerful feature sharing. For example, thanks to the label-dependent format, we can decompose both the source phrase and the target phrase into words and have features such as s cat t koc\u030cka which capture phrase-internal word translations. Predictions for rare phrase pairs are then more robust thanks to the rich statistics collected for these word-level feature pairs."}, {"heading": "2.3 Extraction of Training Examples", "text": "Discriminative models in MT are typically trained by creating one training instance per extracted phrase from the entire training data. The target side of the extracted phrase is a positive label, and all other phrases observed aligned to the extracted phrase (anywhere in the training data) are the negative labels.\nWe train our model in a similar fashion: for each sentence in the parallel training data, we look at all possible phrasal segmentations. Then for each source span, we create a training example. We obtain the set of possible translations GEN(f\u0304) from the phrase table. Because we do not have actual classes, each translation is defined by its labeldependent features and we associate a loss with it: 0 loss for the correct translation and 1 for all others.\nBecause we train both our model and the standard phrase table on the same dataset, we use leaving-one-out in the classifier training to avoid\nover-fitting. We look at phrase counts and cooccurrence counts in the training data, we subtract one from the number of occurrences for the current source phrase, target phrase and the phrase pair. If the count goes to zero, we skip the training example. Without this technique, the classifier might learn to simply trust very long phrase pairs which were extracted from the same training sentence.\nFor target-side context features, we simply use the true (gold) target context. This leads to training which is similar to language model estimation; this model is somewhat similar to the neural joint model for MT (Devlin et al., 2014), but in our case implemented using a linear (maximumentropy-like) model."}, {"heading": "2.4 Training", "text": "We use Vowpal Wabbit in the --csoaa ldf mc setting which reduces our multi-class problem to one-against-all binary classification. We use the logistic loss as our objective. We experimented with various settings of L2 regularization but were not able to get an improvement over not using regularization at all. We train each model with 10 iterations over the data.\nWe evaluate all of our models on a held-out set. We use the same dataset as for MT system tuning because it closely matches the domain of our test set. We evaluate model accuracy after each pass over the training data to detect over-fitting and we select the model with the highest held-out accuracy."}, {"heading": "2.5 Feature Set", "text": "Our feature set requires some linguistic processing of the data. We use the factored MT setting\n(Koehn and Hoang, 2007) and we represent each type of information as an individual factor. On the source side, we use the word surface form, its lemma, morphological tag, analytical function (such as Subj for subjects) and the lemma of the parent node in the dependency parse tree. On the target side, we only use word lemmas and morphological tags.\nTable 1 lists our feature sets for each language pair. We implemented indicator features for both the source and target side; these are simply concatenations of the words in the current phrase into a single feature. Internal features describe words within the current phrase. Context features are extracted either from a window of a fixed size around the current phrase (on the source side) or from a limited left-hand side context (on the target side). Bilingual context features are concatenations of target-side context words and their sourceside counterparts (according to word alignment); these features are similar to bilingual tokens in bilingual LMs (Niehues et al., 2011). Each of our feature types can be configured to look at any individual factors or their combinations.\nThe features in Table 1 are divided into three sets. The first set contains label-independent (=shared) features which only depend on the source sentence. The second set contains shared features which depend on target-side context; these can only be used when VW is applied during decoding. We use target context size two in all our experiments.2 Finally, the third set contains label-dependent features which describe the currently predicted phrasal translation.\n2In preliminary experiments we found that using a single word was less effective and larger context did not bring improvements, possibly because of over-fitting.\nGoing back to the examples from Figure 1, our model can disambiguate the translation of \u201cshooting\u201d based on the source-context features (either the full form or lemma). For the morphological disambiguation of the translation of \u201cyellowish cat\u201d, the model has access to the morphological tags of the preceding target words which can disambiguate the correct morphological case.\nWe used slightly different subsets of the full feature set for different languages. In particular, we left out surface form features and/or bilingual features in some settings because they decreased performance, presumably due to over-fitting."}, {"heading": "3 Efficient Implementation", "text": "Originally, we assumed that using target-side context features in decoding would be too expensive, considering that we would have to query our model roughly as often as the language model. In preliminary experiments, we therefore focused on n-best list re-ranking. We obtained small gains but all of our results were substantially worse than with the integrated model, so we omit them from the paper.\nWe find that decoding with a feature-rich targetcontext model is in fact feasible. In this section, we describe optimizations at different stages of our pipeline which make training and inference with our model practical."}, {"heading": "3.1 Feature Extraction", "text": "We implemented the code for feature extraction only once; identical code is used at training time and in decoding. At training time, the generated features are written into a file whereas at test time, they are fed directly into the classifier via its library interface.\nThis design decision not only ensures consistency in feature representation but also makes the process of feature extraction efficient. In training, we are easily able to use multi-threading (already implemented in Moses) and because the processing of training data is a trivially parallel task, we can also use distributed computation and run separate instances of (multi-threaded) Moses on several machines. This enables us to easily produce training files from millions of parallel sentences within a short time."}, {"heading": "3.2 Model Training", "text": "VW is a very fast classifier by itself, however for very large data, its training can be further sped up by using parallelization. We take advantage of its implementation of the AllReduce scheme which we utilize in a grid engine environment. We shuffle and shard the data and then assign each shard to a worker job. With AllReduce, there is a master job which synchronizes the learned weight vector with all workers. We have compared this approach with the standard single-threaded, single-process training and found that we obtain identical model accuracy. We usually use around 10-20 training jobs.\nThis way, we can process our large training files quickly and train the full model (using multiple passes over the data) within hours; effectively, neither feature extraction nor model training become a significant bottleneck in the full MT system training pipeline."}, {"heading": "3.3 Decoding", "text": "In phrase-based decoding, translation is generated from left to right. At each step, a partial translation (initially empty) is extended by translating a previously uncovered part of the source sentence. There are typically many ways to translate each source span, which we refer to as translation options. The decoding process gradually extends the generated partial translations until the whole source sentence is covered; the final translation is then the full translation hypothesis with the highest model score. Various pruning strategies are applied to make decoding tractable.\nEvaluating a feature-rich classifier during decoding is a computationally expensive operation. Because the features in our model depend on target-side context, the feature function which computes the classifier score cannot evaluate the translation options in isolation (independently of the partial translation). Instead, similarly to a language model, it needs to look at previously generated words. This also entails maintaining a state which captures the required context information.\nA naive integration of the classifier would simply generate all source-context features, all targetcontext features and all features describing the translation option each time a partial hypothesis is evaluated. This is a computationally very expensive approach.\nWe instead propose several technical solutions\nwhich make decoding reasonably fast. Decoding a single sentence with the naive approach takes 13.7 seconds on average. With our optimization, this average time is reduced to 2.9 seconds, i.e. almost by 80 per cent. The baseline system produces a translation in 0.8 seconds on average.\nSeparation of source-context and targetcontext evaluation. Because we have a linear model, the final score is simply the dot product between a weight vector and a (sparse) feature vector. It is therefore trivial to separate it into two components: one that only contains features which depend on the source context and the other with target context features. We can pre-compute the source-context part of the score before decoding (once we have all translation options for the given sentence). We cache these partial scores and when the translation option is evaluated, we add the partial score of the target-context features to arrive at the final classifier score.\nCaching of feature hashes. VW uses feature hashing internally and it is possible to obtain the hash of any feature that we use. When we encounter a previously unseen target context (=state) during decoding, we store the hashes of extracted features in a cache. Therefore for each context, we only run the expensive feature extraction once. Similarly, we pre-compute feature hash vectors for all translation options.\nCaching of final results. Our classifier locally normalizes the scores so that the probabilities of translations for a given span sum to one. This cannot be done without evaluating all translation options for the span at the same time. Therefore, when we get a translation option to be scored, we fetch all translation options for the given source span and evaluate all of them. We then normalize the scores and add them to a cache of final results. When the other translation options come up, their scores are simply fetched from the cache. This can also further save computation when we get into a previously seen state (from the point of view of our classifier) and we evaluate the same set of translation options in that state; we will simply find the result in cache in such cases.\nWhen we combine all of these optimizations, we arrive at the query algorithm shown in Figure 2."}, {"heading": "4 Experimental Evaluation", "text": "We run the main set of experiments on English to Czech translation. To verify that our method is\napplicable to other language pairs, we also present experiments in English to German, Polish, and Romanian.\nIn all experiments, we use Treex (Popel and Z\u030cabokrtsky\u0301, 2010) to lemmatize and tag the source data and also to obtain dependency parses of all English sentences."}, {"heading": "4.1 English-Czech Translation", "text": "As parallel training data, we use (subsets of) the CzEng 1.0 corpus (Bojar et al., 2012). For tuning, we use the WMT13 test set (Bojar et al., 2013) and we evaluate the systems on the WMT14 test set (Bojar et al., 2014). We lemmatize and tag the Czech data using Morphodita (Strakova\u0301 et al., 2014).\nOur baseline system is a standard phrase-based Moses setup. The phrase table in both cases is factored and outputs also lemmas and morphological tags. We train a 5-gram LM on the target side of parallel data.\nWe evaluate three settings in our experiments:\n\u2022 baseline \u2013 vanilla phrase-based system,\n\u2022 +source \u2013 our classifier with source-context features only,\n\u2022 +target \u2013 our classifier with both sourcecontext and target-context features.\nFor each of these settings, we vary the size of the training data for our classifier, the phrase table and the LM. We experiment with three different sizes: small (200 thousand sentence pairs), medium (5 million sentence pairs), and full (the whole CzEng corpus, over 14.8 million sentence pairs).\nFor each setting, we run system weight optimization (tuning) using minimum error rate training (Och, 2003) five times and report the average BLEU score. We use MultEval (Clark et al., 2011) to compare the systems and to determine whether the differences in results are statistically significant. We always compare the baseline with +source and +source with +target.\nTable 2 shows the obtained results. Statistically significant differences (\u03b1=0.01) are marked in bold. The source-context model does not help in the small data setting but brings a substantial improvement of 0.7-0.8 BLEU points for the medium and full data settings, which is an encouraging result.\nTarget-side context information allows our model to push the translation quality further: even for the small data setting, it brings a substantial improvement of 0.5 BLEU points and the gain remains significant as the data size increases. Even in the full data setting, target-side features improve the score by roughly 0.2 BLEU points.\nOur results demonstrate that feature-rich models scale to large data size both in terms of technical feasibility and of translation quality improvements. Target side information seems consistently beneficial, adding further 0.2-0.5 BLEU points on top of the source-context model.\nIntrinsic Evaluation. For completeness, we report intrinsic evaluation results. We evaluate the classifier on a held-out set (WMT13 test set) by extracting all phrase pairs from the test input aligned with the test reference (similarly as\nwe would in training) and scoring each phrase pair (along with other possible translations of the source phrase) with our classifier. An instance is classified correctly if the true translation obtains the highest score by our model. A baseline which always chooses the most frequent phrasal translation obtains accuracy of 51.5. For the sourcecontext model, the held-out accuracy was 66.3, while the target context model achieved accuracy of 74.8. Note that this high difference is somewhat misleading because in this setting, the targetcontext model has access to the true target context (i.e., it is cheating)."}, {"heading": "4.2 Additional Language Pairs", "text": "We experiment with translation from English into German, Polish, and Romanian.\nOur English-German system is trained on the data available for the WMT14 translation task: Europarl (Koehn, 2005) and the Common Crawl corpus,3 roughly 4.3 million sentence pairs altogether. We tune the system on the WMT13 test set and we test on the WMT14 set. We use TreeTagger (Schmid, 1994) to lemmatize and tag the German data.\nEnglish-Polish has not been included in WMT shared tasks so far, but was present as a language pair for several IWSLT editions which concentrate on TED talk translation. Full test sets are only available for 2010, 2011, and 2012. The references for 2013 and 2014 were not made public. We use the development set and test set from 2010 as development data for parameter tuning. The remaining two test sets (2011, 2012) are our test data. We train on the concatenation of Europarl and WIT3 (Cettolo et al., 2012), ca. 750 thousand sentence pairs. The Polish half has been tagged using WCRFT (Radziszewski, 2013) which produces full morphological tags compatible with the NKJP tagset (Przepio\u0301rkowski, 2009).\nEnglish-Romanian was added in WMT16. We train our system using the available parallel data \u2013 Europarl and SETIMES2 (Tiedemann, 2009), roughly 600 thousand sentence pairs. We tune the English-Romanian system on the official development set and we test on the WMT16 test set. We use the online tagger by Tufis et al. (2008) to preprocess the data.\nTable 3 shows the obtained results. Similarly to English-Czech experiments, BLEU scores are av-\n3http://commoncrawl.org/\neraged over 5 independent optimization runs. Our system outperforms the baseline by 0.5-0.7 BLEU points in all cases, showing that the method is applicable to other languages with rich morphology."}, {"heading": "5 Analysis", "text": "We manually analyze the outputs of EnglishCzech systems. Figure 3 shows an example sentence from the WMT14 test set translated by all the system variants. The baseline system makes an error in verb valency; the Czech verb \u201cdos\u030clo\u201d could be used but this verb already has an (implicit) subject and the translation of \u201cmining\u201d (\u201cte\u030cz\u030cba\u201d) would have to be in a different case and at a different position in the sentence. The second error is more interesting, however: the baseline system fails to correctly identify the word sense of the particle \u201cto\u201d and translates it in the sense of purpose, as in \u201cin order to\u201d. The source-context model takes the context (span of years) into consideration and correctly disambiguates the translation of \u201cto\u201d, choosing the temporal meaning. It still fails to translate the main verb correctly, though. Only the full model with target-context information is able to also correctly translate the verb and inflect its arguments according to their roles in the valency frame. The translation produced by this final system in this case is almost flawless.\nIn order to verify that the automatically measured results correspond to visible improvements in translation quality, we carried out two annota-\ntion experiments. We took a random sample of 104 sentences from the test set and blindly ranked two competing translations (the selection of sentences was identical for both experiments). In the first experiment, we compared the baseline system with +source. In the other experiment, we compared the baseline with +target. The instructions for annotation were simply to compare overall translation quality; we did not ask the annotator to look for any specific phenomena. In terms of automatic measures, our selection has similar characteristics as the full test set: BLEU scores obtained on our sample are 15.08, 16.22 and 16.53 for the baseline, +source and +target respectively.\nIn the first case, the annotator marked 52 translations as equal in quality, 26 translations produced by +source were marked as better and in the remaining 26 cases, the baseline won the ranking. Even though there is a difference in BLEU, human annotation does not confirm this measurement, ranking both systems equally.\nIn the second experiment, 52 translations were again marked as equal. In 34 cases, +target produced a better translation while in 18 cases, the baseline output won. The difference between the baseline and +target suggests that the targetcontext model may provide information which is useful for translation quality as perceived by humans.\nOur overall impression from looking at the system outputs was that both the source-context and target-context model tend to fix many morphosyntactic errors. Interestingly, we do not observe as many improvements in the word/phrase sense disambiguation, though the source context does help semantics in some sentences. The targetcontext model tends to preserve the overall agreement and coherence better than the system with a source-context model only. We list several such examples in Figure 4. Each of them is fully cor-\nrected by the target-context model, producing an accurate translation of the input."}, {"heading": "6 Related Work", "text": "Discriminative models in MT have been proposed before. Carpuat and Wu (2007) trained a maximum entropy classifier for each source phrase type which used source context information to disambiguate its translations. The models did not capture target-side information and they were independent; no parameters were shared between classifiers for different phrases. They used a strong feature set originally developed for word sense disambiguation. Gimpel and Smith (2008) also used wider source-context information but did not train a classifier; instead, the features were included directly in the log-linear model of the decoder. Mauser et al. (2009) introduced the \u201cdiscriminative word lexicon\u201d and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features.\nSubotin (2011) included target-side context information in a maximum-entropy model for the prediction of morphology. The work was done within the paradigm of hierarchical PBMT and assumes that cube pruning is used in decoding. Their algorithm was tailored to the specific problem of passing non-local information about morphological agreement required by individual rules (such as\nexplicit rules enforcing subject-verb agreement). Our algorithm only assumes that hypotheses are constructed left to right and provides a general way for including target context information in the classifier, regardless of the type of features. Our implementation is freely available and can be further extended by other researchers in the future."}, {"heading": "7 Conclusions", "text": "We presented a discriminative model for MT which uses both source and target context information. We have shown that such a model can be used directly during decoding in a relatively efficient way. We have shown that this model consistently significantly improves the quality of English-Czech translation over a strong baseline with large training data. We have validated the effectiveness of our model on several additional language pairs. We have provided an analysis showing concrete examples of improved lexical selection and morphological coherence. Our work is available in the main branch of Moses for use by other researchers."}, {"heading": "Acknowledgements", "text": "This work has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreements no. 644402 (HimL) and 645452 (QT21), from the European Research Council (ERC) under grant agreement no. 640550, and from the SVV project number 260 333. This work has been using language resources stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2015071)."}], "references": [{"title": "Findings of the 2013", "author": ["Lucia Specia"], "venue": null, "citeRegEx": "Specia.,? \\Q2013\\E", "shortCiteRegEx": "Specia.", "year": 2013}, {"title": "Better hypothesis testing", "author": ["Noah A. Smith"], "venue": null, "citeRegEx": "Smith.,? \\Q2011\\E", "shortCiteRegEx": "Smith.", "year": 2011}, {"title": "Factored translation models", "author": ["Philipp Koehn", "Hieu Hoang."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 868\u2013876,", "citeRegEx": "Koehn and Hoang.,? 2007", "shortCiteRegEx": "Koehn and Hoang.", "year": 2007}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Conference Proceedings: the tenth Machine Translation Summit, pages 79\u201386, Phuket, Thailand. AAMT, AAMT.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models", "author": ["Arne Mauser", "Sasa Hasan", "Hermann Ney."], "venue": "pages 210\u2013218, Suntec, Singapore.", "citeRegEx": "Mauser et al\\.,? 2009", "shortCiteRegEx": "Mauser et al\\.", "year": 2009}, {"title": "Wider Context by Using Bilingual Language Models in Machine Translation", "author": ["Jan Niehues", "Teresa Herrmann", "Stephan Vogel", "Alex Waibel."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 198\u2013206, Edinburgh,", "citeRegEx": "Niehues et al\\.,? 2011", "shortCiteRegEx": "Niehues et al\\.", "year": 2011}, {"title": "Minimum Error Rate Training in Statistical Machine Translation", "author": ["Franz Josef Och."], "venue": "Proc. of ACL, pages 160\u2013167, Sapporo, Japan. ACL.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "TectoMT: Modular NLP Framework", "author": ["Martin Popel", "Zden\u011bk \u017dabokrtsk\u00fd."], "venue": "Hrafn Loftsson, Eirikur R\u00f6gnvaldsson, and Sigrun Helgadottir, editors, IceTAL 2010, volume 6233 of Lecture Notes in Computer Science, pages 293\u2013304. Iceland Cen-", "citeRegEx": "Popel and \u017dabokrtsk\u00fd.,? 2010", "shortCiteRegEx": "Popel and \u017dabokrtsk\u00fd.", "year": 2010}, {"title": "A comparison of two morphosyntactic tagsets of Polish", "author": ["Adam Przepi\u00f3rkowski."], "venue": "Violetta Koseska-Toszewa, Ludmila Dimitrova, and Roman Roszko, editors, Representing Semantics in Digital Lexicography: Proceedings of MONDILEX Fourth", "citeRegEx": "Przepi\u00f3rkowski.,? 2009", "shortCiteRegEx": "Przepi\u00f3rkowski.", "year": 2009}, {"title": "A tiered CRF tagger for Polish", "author": ["Adam Radziszewski."], "venue": "Robert Bembenik, Lukasz Skonieczny, Henryk Rybinski, Marzena Kryszkiewicz, and Marek Niezgodka, editors, Intelligent Tools for Building a Scientific Information Platform, volume", "citeRegEx": "Radziszewski.,? 2013", "shortCiteRegEx": "Radziszewski.", "year": 2013}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["Helmut Schmid."], "venue": "International Conference on New Methods in Language Processing, pages 44\u201349, Manchester, UK.", "citeRegEx": "Schmid.,? 1994", "shortCiteRegEx": "Schmid.", "year": 1994}, {"title": "Open-Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition", "author": ["Jana Strakov\u00e1", "Milan Straka", "Jan Haji\u010d"], "venue": null, "citeRegEx": "Strakov\u00e1 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Strakov\u00e1 et al\\.", "year": 2014}, {"title": "An exponential translation model for target language morphology", "author": ["Michael Subotin."], "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011,", "citeRegEx": "Subotin.,? 2011", "shortCiteRegEx": "Subotin.", "year": 2011}, {"title": "Integrating a discriminative classifier into phrase-based and hierarchical decoding", "author": ["Ale\u0161 Tamchyna", "Fabienne Braune", "Alexander Fraser", "Marine Carpuat", "Hal Daum\u00e9 III", "Chris Quirk."], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Tamchyna et al\\.,? 2014", "shortCiteRegEx": "Tamchyna et al\\.", "year": 2014}, {"title": "News from OPUS - A collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, vol-", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Racai\u2019s linguistic web services", "author": ["Dan Tufis", "Radu Ion", "Alexandru Ceausu", "Dan Stefanescu."], "venue": "Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, 26 May - 1 June 2008, Marrakech, Morocco.", "citeRegEx": "Tufis et al\\.,? 2008", "shortCiteRegEx": "Tufis et al\\.", "year": 2008}, {"title": "Word-Sense Disambiguation for Machine Translation", "author": ["D. Vickrey", "L. Biewald", "M. Teyssier", "D. Koller."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), Vancouver, Canada, October.", "citeRegEx": "Vickrey et al\\.,? 2005", "shortCiteRegEx": "Vickrey et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 16, "context": "For sense disambiguation, source context is the main source of information, as has been shown in previous work (Vickrey et al., 2005), (Carpuat and", "startOffset": 111, "endOffset": 133}, {"referenceID": 13, "context": "Tamchyna et al. (2014) already integrated VW into Moses.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "We use the factored MT setting (Koehn and Hoang, 2007) and we represent each type of information as an individual factor.", "startOffset": 31, "endOffset": 54}, {"referenceID": 5, "context": "Bilingual context features are concatenations of target-side context words and their sourceside counterparts (according to word alignment); these features are similar to bilingual tokens in bilingual LMs (Niehues et al., 2011).", "startOffset": 204, "endOffset": 226}, {"referenceID": 7, "context": "In all experiments, we use Treex (Popel and \u017dabokrtsk\u00fd, 2010) to lemmatize and tag the source data and also to obtain dependency parses of all", "startOffset": 33, "endOffset": 61}, {"referenceID": 11, "context": "We lemmatize and tag the Czech data using Morphodita (Strakov\u00e1 et al., 2014).", "startOffset": 53, "endOffset": 76}, {"referenceID": 6, "context": "For each setting, we run system weight optimization (tuning) using minimum error rate training (Och, 2003) five times and report the average BLEU score.", "startOffset": 95, "endOffset": 106}, {"referenceID": 3, "context": "data available for the WMT14 translation task: Europarl (Koehn, 2005) and the Common Crawl corpus,3 roughly 4.", "startOffset": 56, "endOffset": 69}, {"referenceID": 10, "context": "Tagger (Schmid, 1994) to lemmatize and tag the German data.", "startOffset": 7, "endOffset": 21}, {"referenceID": 9, "context": "The Polish half has been tagged using WCRFT (Radziszewski, 2013) which produces full morphological tags compatible with the NKJP tagset (Przepi\u00f3rkowski, 2009).", "startOffset": 44, "endOffset": 64}, {"referenceID": 8, "context": "The Polish half has been tagged using WCRFT (Radziszewski, 2013) which produces full morphological tags compatible with the NKJP tagset (Przepi\u00f3rkowski, 2009).", "startOffset": 136, "endOffset": 158}, {"referenceID": 14, "context": "We train our system using the available parallel data \u2013 Europarl and SETIMES2 (Tiedemann, 2009), roughly 600 thousand sentence pairs.", "startOffset": 78, "endOffset": 95}, {"referenceID": 14, "context": "We train our system using the available parallel data \u2013 Europarl and SETIMES2 (Tiedemann, 2009), roughly 600 thousand sentence pairs. We tune the English-Romanian system on the official development set and we test on the WMT16 test set. We use the online tagger by Tufis et al. (2008) to preprocess the data.", "startOffset": 79, "endOffset": 285}, {"referenceID": 1, "context": "Gimpel and Smith (2008) also", "startOffset": 11, "endOffset": 24}, {"referenceID": 4, "context": "Mauser et al. (2009) introduced the \u201cdiscriminative word lexicon\u201d and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence).", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "Mauser et al. (2009) introduced the \u201cdiscriminative word lexicon\u201d and trained a binary classifier for each target word, using as features only the bag of words (from the whole source sentence). Training sentences where the target word occurred were used as positive examples, other sentences served as negative examples. Jeong et al. (2010) proposed a discriminative lexicon with a rich feature set tailored to translation into morphologically rich languages; unlike our work, their model only used source-context features.", "startOffset": 0, "endOffset": 341}], "year": 2016, "abstractText": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.", "creator": "LaTeX with hyperref package"}}}