{"id": "1607.02061", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2016", "title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity", "abstract": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, which causes the loss of important information for word expectations, such as word order and interrelations. In this paper, we present a DSM which addresses the issue by defining verb contexts as joint dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts are more efficient than single dependencies, even with a relatively small amount of training data.", "histories": [["v1", "Thu, 7 Jul 2016 16:00:33 GMT  (19kb)", "http://arxiv.org/abs/1607.02061v1", "5 pages"], ["v2", "Wed, 5 Oct 2016 10:49:58 GMT  (17kb)", "http://arxiv.org/abs/1607.02061v2", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["emmanuele chersoni", "enrico santus", "alessandro lenci", "philippe blache", "chu-ren huang"], "accepted": true, "id": "1607.02061"}, "pdf": {"name": "1607.02061.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Philippe Blache"], "emails": ["emmanuelechersoni@gmail.com", "esantus@gmail.com", "alessandro.lenci@unipi.it", "blache@lpl-aix.fr", "churen.huang@polyu.edu.hk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 7.\n02 06\n1v 1\n[ cs"}, {"heading": "1 Introduction", "text": "Distributional Semantic Models (DSMs) rely on the Distributional Hypothesis (Harris, 1954; Firth, 1957; Sahlgren, 2008), stating that words occurring in similar contexts have similar meanings. On such theoretical grounds, words co-occurrences extracted from corpora are used to build semantic representations, in the form of vectors, which have become very popular in the NLP community. Proximity between word vectors is taken as an index of meaning similarity, and vector cosine\nis generally adopted to measure such proximity, even though other measures have been proposed (Weeds et al., 2004).\nMost of DSMs adopt a bag-of-words approach, that is they turn a text span (i.e. a word window) into a set of words and they register separately the cooccurrence of each word with a given target. A major problem of this representation is that information concerning word order and interrelations gets lost. This is why works like Ruiz-Casado et al. (2005), Agirre et al. (2009), and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, i.e. using entire word windows as features. These richer contexts proved to be helpful to semantically represent verbs, which are characterized by complex argument structures and highly contextsensitive meanings. Nonetheless, they suffer of data sparsity, therefore requiring either larger corpora or complex smoothing processes.\nIn this paper, we propose a syntactically savy notion of joint contexts. To test our representation, we implement several DSMs and we evaluate them in a verb similarity task on two datasets. The results show that, even using a corpus of limited size, our syntactic joint contexts are more robust with respect to data sparseness and perform better than single dependencies in a wider range of parameter settings.\nThe paper is organized as follows. In Section 2, we provide a psycholinguistic and a computational background, describing recent models based on word windows. In Section 3, we describe our reinterpretation of joint contexts with syntactic dependencies. Evaluation settings and results are presented in Section 4."}, {"heading": "2 Related Work", "text": "A number of studies in sentence processing suggest that verbs activate expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998; Matsuki et al., 2011), supports the idea of a mental lexicon arranged as a web of mutual expectations.\nSome recent works, like (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015), modeled thematic fit estimations by means of dependency-based or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants; but they do not encode directly the relation between participants co-occurring in the same event.\nOn the other hand, some research efforts in the NLP community aimed at introducing richer contextual features in DSMs, mostly based on word windows. A first example was the compositefeature model by Ruiz-Casado et al. (2005), who extracted word windows through a Web Search engine. They motivated such choice to reduce data sparseness. A composite feature for the target word watches is Alicia always romantic movies, extracted from the sentence I heard that Alicia always watches romantic movies with Antony (the placeholder represents the target position). Thanks to this approach, Ruiz-Casado and\ncolleagues achieved 82.50 in the TOEFL synonym detection test, outperforming the Latent Semantic Analysis (LSA; see (Landauer and Dumais, 1997; Landauer et al., 1998)) and several other methods.\nAgirre et al. (2009) adopted an analogous approach, relying on a huge learning corpus (1.6 Teraword) to build composite-feature vectors. Their model outperformed a traditional DSM on the similarity subset of the WordSim-353 test set (Finkelstein et al., 2001).\nMelamud et al. (2014) introduced a probabilistic similarity scheme for modeling the so-called joint context. By making use of the Kneser-Ney language model (Kneser and Ney, 1995) and of a probabilistic distributional measure, they were able to overcome data sparsity, outperforming a wide variety of DSMs on two similarity tasks, evaluated on VerbSim (Yang and Powers, 2006) and on a set of 1,000 verbs extracted from WordNet (Fellbaum, 1998). On the basis of their results, the authors claimed that composite-feature models are particularly advantageous for measuring verb similarity.\nFinally, in a recent paper, Melamud et al. (2015) represented contexts as substitute vectors, which are second-order potential filler words for the first-order target word slots (which are in turn word windows, such as \u2019I my phone\u2019, whose substitute vector may contain: love, lost, upgraded etc.). Using such representation, the authors outperformed state-ofthe-art models in both predicting and ranking paraphrases of words in context."}, {"heading": "3 Syntactic Joint Contexts", "text": "A joint context, as defined in Melamud et al. (2014), is a word window of order n around a target word. The target is replaced by a placeholder, and the value of the feature for a word w is the probability of w to fill the placeholder position. Assuming n=3, a word like love would be represented by a collection of contexts such as the new students the school campus, my cousin Bob to play basketball etc. Such representation introduces data sparseness, which has been addressed by previous studies either by adopting huge corpora or by relying on ngram language models to approximate the probabilities of long sequences of words.\nHowever, features based on word windows do not\nguarantee to include all the most salient event participants. Moreover, they could include unrelated words, also differentiating contexts substantially describing the same event (e.g. consider Luis the red ball and Luis the blue ball).\nFor these reasons, we introduce the notion of syntactic joint context, further abstracting from linear word windows by using syntactic dependencies. Our assumption is that knowledge about typical event participants can be inferred by observing the most frequent argument combinations. Below, we describe the procedure we followed to define our syntactic joint contexts.\n\u2022 First, we extracted a list of verb-argument dependencies from a parsed corpus. For each target, we extracted all the direct dependencies from the sentence in which it occurs. For instance, in Finally, the dictator acknowledged his failure, we will have: target = \u2019acknowledge-v\u2019; subject = \u2019dictator-n\u2019; and object = \u2019failure-n\u2019.\n\u2022 Second, for each sentence, we generated a joint context feature by joining all the dependencies for the grammatical relations of interest. From the example above, we would generate the feature dictator-n.subj+ +failure-n.obj.\nFor our experiments, the grammatical relations that we used are subject, object and complement, where complement is meant as a generic relation grouping together all complements introduced by a preposition. During our experimentations, we decided to represent those complements just through the preposition and to omit the noun arguments, in order to reduce data sparsity. Finally, our distributional representation for a target word is a vector of joint features, such as astronaut-n.subj+ +on-p.comp+, athleten.subj+ +medal-n.obj."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 Corpus and DSMs", "text": "We trained our DSMs on the RCV1 corpus, which contains approximately 150 million words (Lewis et al., 2004). The corpus was tagged with the tagger described in\n(Dell\u2019Orletta, 2009) and dependency-parsed with DeSR (Attardi et al., 2009). RCV1 was chosen for two reasons: i) to allow a direct comparison with the results reported by Melamud et al., 2014; ii) to show that our joint context-based representation can deal with data sparseness even with a training corpus of limited size.\nAll DSMs adopt Positive Pointwise Mutual Information (PPMI; (Church and Hanks, 1990)) as a context weighting scheme and vary according to three main parameters: i) type of contexts; ii) number of dimensions; iii) application of Singular Value Decomposition (SVD; see (Landauer and Dumais, 1997; Landauer et al., 1998)).\nFor what concerns i), beyond the joint contextbased DSMs, we have also developed DSMs with single dependencies, to be used as baselines. ii) refers instead to the number of contexts that have been used as vector dimensions. Several values were explored (i.e. 10K, 50K and 100K), selecting the contexts according to their frequency. Finally, iii) concerns the application of SVD to reduce the matrix. We report only the results for a number k of latent dimensions ranging from 200 to 400, since the performance drops significantly out of this interval.\nAll the models have been implemented by means of the DISSECT toolkit (Dinu et al., 2013)."}, {"heading": "4.2 Similarity Measures", "text": "As a similarity measure, we used vector cosine, which is by far the most popular in the literature (Turney and Pantel, 2010). Melamud et al. (2014) have proposed the Probabilistic Distributional Similarity (PDS), which is based on the intuition that two words, w1 and w2, are similar if they are likely to occur in each other\u2019s contexts. The authors define it such that it assigns a high similarity score when both p(w1 \u2014 contexts of w2) and p(w2 \u2014 contexts of w1) are high. We tried to test variations of this measure with our syntactic joint contexts, but we were not able to achieve satisfying results. Therefore, we have decided to report here only the scores with the cosine."}, {"heading": "4.3 Datasets", "text": "The DSMs are evaluated on two test sets: VerbSim (Yang and Powers, 2006) and the verb subset of the\npopular SimLex-999 (Hill et al., 2015). The former includes 130 verb pairs, while the latter includes 222 verb pairs. We focus on verb similarity because verb meaning is highly context sensitive and, therefore, verb representation should benefit more from the introduction of joint features (Melamud et al., 2014).\nBoth datasets are annotated with similarity judgements, so we measured the Spearman correlation between them and the scores assigned by the model. The Verbsim dataset allows for direct comparison with Melamud et al. (2014), since they also evaluated their model on this test set, achieving a Spearman correlation score of 0.616 and outperforming all the baseline methods.\nThe verb subset of Simlex-999, at the best of our knowledge, has never been used as a benchmark dataset for verb similarity. The Simlex dataset is known for being quite challenging: as reported by Hill et al. (2015), the average performances of similarity models on this dataset are much lower than on alternative benchmarks like WordSim (Finkelstein et al., 2001) and MEN (Bruni et al., 2014). Considering also that verbs are generally the toughest part-of-speech for distributional modeling, SimLex verbs should represent a nontrivial task.\nWe exclude from the evaluation datasets all the target words occurring less than 100 times in our corpus. Consequently, we have coverage for 107 pairs in the VerbSim dataset (82.3, the same of Melamud et al. (2014)) and for 214 pairs in the SimLex verbs dataset (96.3)."}, {"heading": "4.4 Results", "text": "Table 1 reports the Spearman correlation scores for the vector cosine on several DSMs, implementing single dependencies and syntactic joint contexts. At a glance, we can see the discrepancy between the results obtained in the two datasets, as Simlex verbs confirms to be very difficult to model. However, in both datasets we can recognize a trend related to the number of contexts taken into account: the higher the number, the higher the performance. Single dependencies and joint context perform very similarly, and no one has an edge on the other. It is noticeable that the score obtained on VerbSim by the joint context model with 100K dimensions goes very close to the result reported by Melamud et al. (2014) (0.616).\nA clear advantage of the joint contexts can be noticed instead in Table 2 and Table 3, when SVD is applied to reduce the matrix. Independently of k, the joint contexts almost always outperform the models using single dependencies. Overall, the performance of the joint contexts seems to be more stable across several parameter configurations, whereas single dependencies are subject to bigger performance drops. Exceptions can be noticed only for the VerbSim dataset, and only with a low number of dimensions.\nThe best performances are obtained with SVD reduction and k=200. In VerbSim, we achieve 0.650, which is above the result of Melamud et al. (2014). In the verb subset of SimLex-999, we obtain 0.283, which is very close to the score obtained by Hill et al. (2015) by applying word2vec (Mikolov et al., 2013) on the full dataset (0.282). Such performance confirms the validity of the representation, leaving the door open to future investigations."}, {"heading": "4.5 Conclusions", "text": "In this paper, we have presented our proposal for a new type of vector representation based on joint features, which should emulate more closely the general knowledge about event participants that seems to be the organizing principle of our mental lexicon. A core issue of previous studies was the data sparseness challenge, and we coped with it by means of a more abstract, syntactic notion of joint context.\nThe models using joint dependencies proved to be competitive, and were able to perform comparably to -and in some cases even better than- some of the best results reported in the literature. More importantly, they were able to outperform the models based on single dependencies across several parameter settings, especially after the application of SVD. Similarly, Agirre et al. (2009) showed that large word windows had a higher discriminative power than indipendent features, but they did it by using a huge training corpus. The fact that we achieved our results with a small corpus like RCV1 strengthen our belief that syntactic dependencies are a possible solution for the data sparsity problem of joint feature spaces.\nThe performance in the verb similarity task motivates us to further test this representation on a larger range of tasks, such as word sense disambiguation, textual entailment and classification of semantic relations. Moreover, our proposal open interesting perspectives for computational psycholinguistics, especially for modeling those semantic phenomena that are inherently related to the activation of event knowledge and argument structure information (e.g. thematic fit)."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proceedings of the 2009 conference of the NAACL-", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Accurate dependency parsing with a stacked multilayer perceptron", "author": ["Felice Dell\u2019Orletta", "Maria Simi", "Joseph Turian"], "venue": "In Proceedings of EVALITA,", "citeRegEx": "Attardi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Attardi et al\\.", "year": 2009}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Effects of event knowledge in processing verbal arguments", "author": ["Jeffrey L Elman", "Mary Hare", "Ken McRae", "Marta Kutas"], "venue": "Journal of Memory and Language,", "citeRegEx": "Bicknell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bicknell et al\\.", "year": 2010}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Church", "Hanks1990] Kenneth Ward Church", "Patrick Hanks"], "venue": "Computational linguistics,", "citeRegEx": "Church et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Church et al\\.", "year": 1990}, {"title": "DISSECT-DIStributional SEmantics Composition Toolkit", "author": ["Dinu et al.2013] Georgiana Dinu", "Nghia The Pham", "Marco Baroni"], "venue": "Proceedings of the System Demonstrations of ACL 2013,", "citeRegEx": "Dinu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2013}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering", "author": ["Asad Sayeed", "Vera Demberg"], "venue": "In Proceedings of the 2015 conference of the NAACL-HLT,", "citeRegEx": "Greenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greenberg et al\\.", "year": 2015}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Ney1995] Reinhard Kneser", "Hermann Ney"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997] Thomas K Landauer", "Susan Dumais"], "venue": "Psychological review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "An introduction to latent semantic analysis", "author": ["Peter W Foltz", "Darrell Laham"], "venue": "Discourse processes,", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "Composing and updating verb argument expectations: A distributional semantic model", "author": ["Alessandro Lenci"], "venue": "In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics,", "citeRegEx": "Lenci.,? \\Q2011\\E", "shortCiteRegEx": "Lenci.", "year": 2011}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["Lewis et al.2004] David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Event-based plausibility immediately influences on-line language comprehension", "author": ["Tracy Chow", "Mary Hare", "Jeffrey L Elman", "Christoph Scheepers", "Ken McRae"], "venue": "Journal of Experimental Psychology,", "citeRegEx": "Matsuki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Matsuki et al\\.", "year": 2011}, {"title": "Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension", "author": ["McRae et al.1998] Ken McRae", "Michael J SpiveyKnowlton", "Michael K Tanenhaus"], "venue": "Journal of Memory and Language,", "citeRegEx": "McRae et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McRae et al\\.", "year": 1998}, {"title": "A basis for generating expectancies for verbs from nouns", "author": ["McRae et al.2005] Ken McRae", "Mary Hare", "Jeffrey L Elman", "Todd Ferretti"], "venue": "Memory & Cognition,", "citeRegEx": "McRae et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McRae et al\\.", "year": 2005}, {"title": "Modeling Word Meaning in Context with Substitute Vectors", "author": ["Melamud et al.2015] Oren Melamud", "Ido Dagan", "Jacob Goldberger"], "venue": "In Proceedings of the 2015 conference of the NAACL-HLT,", "citeRegEx": "Melamud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2015}, {"title": "Probabilistic modeling of joint-context in distributional similarity", "author": ["Melamud et al.2014] Oren Melamud", "Ido Dagan", "Jacob Goldberger", "Idan Szpektor", "Deniz Yuret"], "venue": "In CoNLL,", "citeRegEx": "Melamud et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2014}, {"title": "Efficient estimation", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Using contextwindow overlapping in synonym discovery and ontology extension", "author": ["Enrique Alfonseca", "Pablo Castells"], "venue": "In Proceedings of RANLP,", "citeRegEx": "Ruiz.Casado et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ruiz.Casado et al\\.", "year": 2005}, {"title": "The distributional hypothesis", "author": ["Magnus Sahlgren"], "venue": "Italian Journal of Linguistics,", "citeRegEx": "Sahlgren.,? \\Q2008\\E", "shortCiteRegEx": "Sahlgren.", "year": 2008}, {"title": "Combining unsupervised syntactic and semantic models of thematic fit", "author": ["Sayeed", "Demberg2014] Asad Sayeed", "Vera Demberg"], "venue": null, "citeRegEx": "Sayeed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sayeed et al\\.", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D Turney", "Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Weeds et al.2004] Julie Weeds", "David Weir", "Diana McCarthy"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Weeds et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 22, "context": "Distributional Semantic Models (DSMs) rely on the Distributional Hypothesis (Harris, 1954; Firth, 1957; Sahlgren, 2008), stating that words occurring in similar contexts have similar meanings.", "startOffset": 76, "endOffset": 119}, {"referenceID": 25, "context": "(Weeds et al., 2004).", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "This is why works like Ruiz-Casado et al. (2005), Agirre et al.", "startOffset": 23, "endOffset": 49}, {"referenceID": 0, "context": "(2005), Agirre et al. (2009), and Melamud et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2005), Agirre et al. (2009), and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, i.", "startOffset": 8, "endOffset": 56}, {"referenceID": 16, "context": "A number of studies in sentence processing suggest that verbs activate expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 17, "context": "A number of studies in sentence processing suggest that verbs activate expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al.", "startOffset": 131, "endOffset": 171}, {"referenceID": 3, "context": ", 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010).", "startOffset": 92, "endOffset": 134}, {"referenceID": 16, "context": "(McRae et al., 1998; Matsuki et al., 2011), supports", "startOffset": 0, "endOffset": 42}, {"referenceID": 15, "context": "(McRae et al., 1998; Matsuki et al., 2011), supports", "startOffset": 0, "endOffset": 42}, {"referenceID": 21, "context": "A first example was the compositefeature model by Ruiz-Casado et al. (2005), who extracted word windows through a Web Search engine.", "startOffset": 50, "endOffset": 76}, {"referenceID": 12, "context": "50 in the TOEFL synonym detection test, outperforming the Latent Semantic Analysis (LSA; see (Landauer and Dumais, 1997; Landauer et al., 1998)) and several other methods.", "startOffset": 93, "endOffset": 143}, {"referenceID": 7, "context": "Their model outperformed a traditional DSM on the similarity subset of the WordSim-353 test set (Finkelstein et al., 2001).", "startOffset": 96, "endOffset": 122}, {"referenceID": 18, "context": "Finally, in a recent paper, Melamud et al. (2015)", "startOffset": 28, "endOffset": 50}, {"referenceID": 18, "context": "A joint context, as defined in Melamud et al. (2014), is a word window of order n around a target word.", "startOffset": 31, "endOffset": 53}, {"referenceID": 14, "context": "We trained our DSMs on the RCV1 corpus, which contains approximately 150 million words (Lewis et al., 2004).", "startOffset": 87, "endOffset": 107}, {"referenceID": 1, "context": "pus was tagged with the tagger described in (Dell\u2019Orletta, 2009) and dependency-parsed with DeSR (Attardi et al., 2009).", "startOffset": 97, "endOffset": 119}, {"referenceID": 12, "context": "All DSMs adopt Positive Pointwise Mutual Information (PPMI; (Church and Hanks, 1990)) as a context weighting scheme and vary according to three main parameters: i) type of contexts; ii) number of dimensions; iii) application of Singular Value Decomposition (SVD; see (Landauer and Dumais, 1997; Landauer et al., 1998)).", "startOffset": 267, "endOffset": 317}, {"referenceID": 6, "context": "of the DISSECT toolkit (Dinu et al., 2013).", "startOffset": 23, "endOffset": 42}, {"referenceID": 18, "context": "Melamud et al. (2014) have proposed the Probabilistic Distributional Sim-", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "popular SimLex-999 (Hill et al., 2015).", "startOffset": 19, "endOffset": 38}, {"referenceID": 19, "context": "We focus on verb similarity because verb meaning is highly context sensitive and, therefore, verb representation should benefit more from the introduction of joint features (Melamud et al., 2014).", "startOffset": 173, "endOffset": 195}, {"referenceID": 18, "context": "The Verbsim dataset allows for direct comparison with Melamud et al. (2014), since they also evaluated their model on this test set, achieving a Spearman correlation score of 0.", "startOffset": 54, "endOffset": 76}, {"referenceID": 9, "context": "reported by Hill et al. (2015), the average per-", "startOffset": 12, "endOffset": 31}, {"referenceID": 7, "context": "are much lower than on alternative benchmarks like WordSim (Finkelstein et al., 2001) and MEN (Bruni et al.", "startOffset": 59, "endOffset": 85}, {"referenceID": 4, "context": ", 2001) and MEN (Bruni et al., 2014).", "startOffset": 16, "endOffset": 36}, {"referenceID": 18, "context": "3, the same of Melamud et al. (2014)) and for 214 pairs in the SimLex verbs dataset (96.", "startOffset": 15, "endOffset": 37}, {"referenceID": 18, "context": "It is noticeable that the score obtained on VerbSim by the joint context model with 100K dimensions goes very close to the result reported by Melamud et al. (2014) (0.", "startOffset": 142, "endOffset": 164}, {"referenceID": 17, "context": "650, which is above the result of Melamud et al. (2014). In the verb subset of SimLex-999, we obtain 0.", "startOffset": 34, "endOffset": 56}, {"referenceID": 9, "context": "283, which is very close to the score obtained by Hill et al. (2015) by applying word2vec", "startOffset": 50, "endOffset": 69}, {"referenceID": 20, "context": "(Mikolov et al., 2013) on the full dataset (0.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Similarly, Agirre et al. (2009) showed that large word windows had a higher discriminative power than indipendent features, but they did it by using a huge training corpus.", "startOffset": 11, "endOffset": 32}], "year": 2017, "abstractText": "Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, which causes the loss of important information for word expectations, such as word order and interrelations. In this paper, we present a DSM which addresses the issue by defining verb contexts as joint dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts are more efficient than single dependencies, even with a relatively small amount of training data.", "creator": "LaTeX with hyperref package"}}}