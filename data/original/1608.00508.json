{"id": "1608.00508", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2016", "title": "Blind phoneme segmentation with temporal prediction errors", "abstract": "Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods.", "histories": [["v1", "Mon, 1 Aug 2016 17:51:03 GMT  (435kb,D)", "http://arxiv.org/abs/1608.00508v1", "5 pages 3 figures. Submitted to SLT"], ["v2", "Sat, 27 May 2017 04:01:13 GMT  (444kb,D)", "http://arxiv.org/abs/1608.00508v2", "7 pages 3 figures. Presented at ACL SRW 2017"]], "COMMENTS": "5 pages 3 figures. Submitted to SLT", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["paul michel", "okko r\\\"as\\\"anen", "roland thiolli\\`ere", "emmanuel dupoux"], "accepted": true, "id": "1608.00508"}, "pdf": {"name": "1608.00508.pdf", "metadata": {"source": "CRF", "title": "IMPROVING PHONEME SEGMENTATION WITH RECURRENT NEURAL NETWORKS", "authors": ["Paul Michel", "Okko Rasanen", "Roland Thiolli\u00e8re", "Emmanuel Dupoux"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Recurrent neural networks, statistical learning, phone segmentation, speech segmentation"}, {"heading": "1. INTRODUCTION", "text": "One of the main difficulty of speech processing as opposed to text processing is the continuous, time-dependent nature of the signal. As a consequence, pre-segmentation of the speech signal into words or sub-words units such as phonemes syllables or words is an essential first step of a variety of speech recognition tasks.\nSegmentation in phonemes is useful for a number of applications (annotation of speech for the purpose of phonetic analysis, computation of speech rate, keyword spotting, etc), and can be done in two ways. Supervised methods are based on an existing phoneme or word recognition system, which is used to decode the incoming speech into phonemes. Phonemes boundaries can then be extracted as a by-product of the alignment of the phoneme models with the speech. Unsupervised methods (also called blind segmentation) consist in finding phonemes boundaries using the acoustic signals only. Supervised methods depend on the training of acoustic and language models, which requires access large amounts of linguistic resources (annotated speech, phonetic dictionary, text). Unsupervised methods do not require these resources and are therefore appropriate for so-called under-resourced languages, such as endangered languages, or languages without consistent orthographies.\nWe propose a blind phoneme segmentation method based on short term statistical properties of the speech signal. We\ndesignate peaks in the error curve of a model trained to predict speech frame by frame as potential boundaries. Three different models are tested. The first is an approximated Markov model of the transition probabilities between categorical speech features. We then replace it by a recurrent neural network operating on the same categorical features. Finally, a recurrent neural network is directly trained to predict the raw speech features. This last model is especially interesting in that it couples our statistical approach with more common spectral transition based methods ([1] for instance).\nWe first describe the various models used and the pre- and post-processing procedures, before presenting and discussing our results in the light of previous work."}, {"heading": "2. RELATED WORK", "text": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain. The idea is to design robust acoustic features that are supposed to remain stable within a phoneme, and change when transitioning from one phoneme to the next. The algorithm then define a measure of change, which is then used to detect phoneme boundaries.\nApart from this line of research, three main approaches have been explored. The first idea is to use short term statistical dependencies. In [7], the idea was to first discretize the signal using a clustering algorithm and then compute discrete sequence statistics, over which a threshold can be defined. This is the idea that we follow in the current paper. The second approach is to use dynamic programming methods inspired by text segmentation [8], in order to derive optimal segmentation [9]. In this line of research, however, the number of segments is assumed to be known in advance, so this cannot count as blind segmentation. The third approach consists in jointly segmenting and learning the acoustic models for phonemes [10, 11, 12]. These models are much more computationally involved than the other methods. Interestingly they all use a simpler, blind segmentation as an initialization phase. Therefore, improving on pure blind segmentation could be useful for joint models as well.\nThe principal source of inspiration for our work comes\nar X\niv :1\n60 8.\n00 50\n8v 1\n[ cs\n.C L\n] 1\nA ug\n2 01\nfrom previous work by Elman [13] and Christiansen et al. [14] published in the 90s. In the former, the author uses recurrent neural networks to train character-based language models on text and notices that \u201dThe error provides a good clue as to what the recurring sequences in the input are, and these correlate highly with words.\u201d [13]. More precisely, the error tends to be higher at the beginning of new words than in the middle. In the latter, the author uses Elman recurrent neural networks to predict boundaries between words given the character sequence and phonological cues.\nOur work uses the same idea, using prediction error as a cue for segmentation, but with two important changes: we apply it to speech instead of text, and we use it to segment in terms of phoneme units instead of word units."}, {"heading": "3. SYSTEM", "text": "We used two kinds of speech features : 13 dimensional MFCCs [15] (with 12 mel-cepstrum coefficients and 1 energy coefficient) and categorical one-hot vectors derived from MFCCs inspired by [7].\nThe latter are computed according to [7] : K-means clustering is performed on a random subset of the MFCCs (10000 frames were selected at random), with a target number of clusters of 8, then each MFCC is identified to the closest centroid. Each frame is then represented by a cluster number c \u2208 {1, ..., 8}, or alternatively by the corresponding one-hot vector of dimension 8. These hyper-parameters were chosen according to [7].\nFigure 1 allows for a visual comparison of the three signals (waveform, MFCC, categorical).\nThe entire dataset is split between a training and a testing subset. A randomly selected subset of the training part is used as validation data to prevent overfitting.\nTraining phase A frame-by-frame prediction model is then learned on the training set. The three different models used are described below :\nPseudo-markov model\nWhen trying to predict the frame xt given the previous frames xt\u22121, ..., x0, a simplifying assumption is to model the transition probabilities with a Markov chain of higher order K, i.e. p(xt|xt\u22121, ..., x0) = p(xt|xt\u22121, ..., xt\u2212K). Provided each frame is part of a finite alphabet, a finite (albeit exponential in K) number of transition probabilities must be learned.\nHowever, as the order rises, the ratio between the size of the data and the number of transition probability being learned makes the exact calculation more difficult and less relevant.\nIn order to circumvent this issue, we approximate the Korder Markov chain with the mean of 1-order markov chain of the lag-transition probabilities p(xt|xt\u2212i) for 1 6 i 6 K, so that\np(xt|xt\u22121, ..., x0) = 1\nK K\u2211 i=1 p(xt|xt\u2212i)\nwith p(xt|xt\u2212i) = f(xt,xt\u2212i)f(xt\u2212i) .\nRecurrent neural network on categorical features\nAlternatively to Markov chains, the transition probability p(xt|xt\u22121, ..., x0) can be modeled by a recurrent neural network (RNN). RNN can theoretically model indefinite order temporal dependencies, hence their advantage over Markov chains for long sequence modeling.\nGiven a set of examples {(xt, (xt\u22121, ..., x0))|t \u2208 {0, .., tmax}}, the networks parameters are learned so that the cross-entropy H(xt, RNN(xt\u22121, ..., x0)) is minimized using back propagation through time [16] and stochastic gradient descent or a variant thereof (we have found RMSProp [17] to give the best results).\nIn our case, the network itself consists of two LSTM layers [18] stacked on one another followed by a linear layer and a softmax. The input and output units have both dimension 8, whereas all other layers have the same hidden dimension 40. Dropout [19] with probability 0.2 was used after each LSTM layer to prevent overfitting.\nA pitfall of this method is the tendency of the network to predict the last frame it is fed. This is due to the fact that\nthe sequences of categorical features extracted from speech contain a lot of constant sub-sequences length > 2.\nAs a consequence, around 80% of the data fed to the network consists of sub-sequences where xt = xt\u22121 . Despite the fact that phone boundaries are somewhat correlated with changes of categories (around 65% of the time), this leads the network to a local minimum where it only tries to predict the same characters.\nTo mitigate this effect, examples where xt = xt\u22121 were removed with probability 0.8, leading to substantial improvement.\nRecurrent neural network on raw MFCCs\nThe recurrent neural network model can be adapted to raw speech features simply by changing the loss function from categorical cross-entropy to mean squared error, which is the direct translation from a categorical distribution to a Gaussian density (2\u2016x\u2212 y\u201622 + d is the Kullback-Leibler divergence of two d-dimensional normal distributions centered in x and y with the same scalar covariance matrix).\nWe used the same architecture than in the categorical case, simply removing the softmax layer and decreasing the hidden dimension size to 20. In this case, no selection of the samples is needed since the sequences vary continuously.\nTest phase\nEach model is run on the test set and the prediction error is calculated at each time step according to the formula :\nErrormarkov(t) = \u2212 log( K\u2211 i=1 p(xt|xt\u2212i))\nErrorRNN\u2212cat(t) = \u2212 d\u2211\ni=1\n1xt=i log(RNN(xt\u22121, ..., x0))\nErrorRNN\u2212MFCC(t) = 1\nd \u2016xt \u2212RNN(xt\u22121, ..., x0)\u201622\nIn each case this corresponds, up to a scaling factor constant across the dataset, to the Kullback-Leibler divergence between the predicted and actual probability distribution for xt in the feature space.\nSince all three systems predict probabilities conditioned by the preceding frames, they cannot be expected to give meaningful results for the first frames of each utterance. To be consistent, the first 7 frames (70ms) of the error signal for each utterance were set to 0.\nA peak detection procedure is then applied to the resulting error. As we are looking for sudden bursts in the prediction error, a local maximum is labeled as a potential boundary if and only if the difference between its value and the one of the previous minimum is superior to a certain threshold \u03b4."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1. Dataset", "text": "We evaluated our methods on the TIMIT dataset [20]. The TIMIT dataset consists of 6300 utterances (\u223c 5.4 hours) from 630 speakers spanning 8 dialects of the English language. The corpus was divided into a training and test set according to the standard split. The training set contains 4620 utterances (172,460 boundaries) and the test set 1680 (65,825 boundaries)."}, {"heading": "4.2. Evaluation", "text": "The performance evaluation of our system is based on precision and recall. With Ng being the number of gold boundaries, Nh the number of hypothesized boundaries and Nc the number of correctly detected boundaries, we define precision (P ), recall (R) and F-score (F ) as follows :\nP = NcNh\nR = NcNg\nF = 2PRP+R\nThe F-score, defined as the harmonic mean of precision and recall, provides a single value which is often used as a comparison for different algorithms. A drawback of this metric is that high recall, low precision results, such as the ones produces by hypothesizing a boundary every 5ms (P : 58%, R : 91%) yield high F-score (70%).\nOther metrics have been designed to tackle this issue. One such example is the R-value [21] :\nR\u2212 val = 1\u2212\n\u221a (1\u2212R)2 +OS2 + |R+1\u2212OS\u221a\n2 |\n2\nWhere OS = RP \u2212 1 is the over-segmentation measure. The R value represents how close the segmentation is from the ideal 0 OS, 1 R point and the P=1 line in the R, OS space. Further details can be found in [21].\nDetermining whether gold boundary is detected or not is a crucial part of the evaluation procedure. On our test set for instance, which contains 65,825 gold boundaries partitioned into 1,680 files, adding or removing one correctly detected boundary per utterance leads to a change of \u00b1 2.5% in precision. This means that minor changes in the evaluation process (such as removing the trailing silence parts of each file, removing the opening and closing boundary) yield non-trivial variations in the end result.\nA common condition for a gold boundary to be considered as \u2019correctly detected\u2019 is to have a proposed boundary within a 20ms distance on either side. Without any other specification, this means that a proposed boundary may be matched to several gold boundaries, provided these are within 40ms from each other, leading to an increase of up to 4% F-score in some of our results (74%-78%). Unfortunately this point is seldom detailed in the literature.\nWe decided to use the procedure described in [21] to match gold boundaries and hypothesized boundaries : overlapping tolerance windows are cropped in the middle of the two boundaries."}, {"heading": "4.3. Results", "text": "The current state of the art in blind phoneme segmentation on the TIMIT corpus is provided by [6]. It evaluates to 78.16% F-score and 81.11 R-value on the training part of the dataset, using an evaluation method similar to our own.\nIn Tables 1 and 2 we compare our best results to the previous statistical approach evoked in [7] and the naive periodic boundaries segmentation (one boundary each 5ms). Since [7] used an evaluation method allowing for tolerance windows to overlap, we provide our results with both evaluation methods (full windows and cropped windows) for the sake of consistency.\nFigure 2 provides an overview of the precision/recall scores when varying the peak detection threshold (and, in case of periodic boundaries, the period). This gives some insight about the actual behavior of the various algorithms, especially in the high precision, low recall region where the RNN on actual MFCCs seems to outperform the methods based on discrete features.\nWe provide Figure 3 as a qualitative assessment of the error profiles of all three algorithms on one specific utterance.\nNotably, the error profile of the markov model contains distinct isolated peaks of similar height. As expected, the error curve is much more noisy in the case of the RNN on MFCCs, due to the greater variability in the feature space."}, {"heading": "5. DISCUSSION", "text": "In terms of optimal F-score and R values, the simple Markov model outperformed the previously published paper using short term sequential statistics [7], as well as the recurrent\nneural networks. However, these optimal values may mask the differential behavior of these algorithms in different sections of the precision/recall curve. In particular, it is interesting to notice that the neural network based model trained on the raw MFCCs gave very good results in the low recall, high precision domain. Indeed, the precision can reach 90% with a recall of 40%. Such a regime could be useful, for instance, if blind phoneme segmentation is used to help with word segmentation.\nThe reason of the increase precision of neural networks may be that it combines the sensitivity of this model to sequential statistical regularities of the signal, but also to the spectral variations, i.e. the error is also correlated to the spectral changes, meaning that some peaks are associated with a high error because the euclidean distance \u2016xt+1 \u2212 xt\u20162 itself is big. This is why the height difference is much more significant in this case.\nAlthough we only reported the best results, we also tested our model on two other neural network architectures : a single vanilla RNN and a single LSTM cell. Both architecture did not yield significantly different results (\u223c 1-2% F-score, mainly dropping precision). Similarly, different hidden dimension were tested. In the extreme cases (very low - 8 - or high - 128 - dimension), the output signal proved too noisy so be of any significance, yielding results comparable to naive periodic segmentation.\nAs far as the computational cost is concerned, all three models had very decent performance. On a quad core Intel Xeon R\u00a9CPU running at 2.80 GHz, the training phases (the most expensive part of the pipeline along with the computation of the features) took 1 and 10 minutes for the Markov and RNN model respectively, using a python/NumPy/Theano implementation."}, {"heading": "6. CONCLUSIONS", "text": "We have presented a fast, lightweight blind phoneme segmentation method predicting boundaries at peaks of the prediction loss of transition probabilities models. The different models we tested produced satisfying results while remaining computationally tractable.\nOur recurrent neural network trained on speech features in particular hints at a way of combining both the statistical and spectral information into a single model.\nOn a machine learning point of view, we highlighted the use that can be made of side channel information (in this case the test error) in order to extract structure from raw data in an unsupervised setting.\nFuture work may involve exploring different RNN models, assessing the stability of these methods on simpler features such as raw spectrograms, or exploring the representation of each frame in the hidden layers of the networks."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "This project is supported by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR10-IDEX-0001-02 PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, the Region Ile de France (DIM cerveau et pense\u0301e), and an AWS in Education Research Grant award."}, {"heading": "8. REFERENCES", "text": "[1] Sorin Dusan and Lawrence R Rabiner, \u201cOn the relation between maximum spectral transition positions and phone boundaries.,\u201d in INTERSPEECH. Citeseer, 2006.\n[2] Anna Esposito and Guido Aversano, \u201cText independent methods for speech segmentation,\u201d in Nonlinear Speech Modeling and Applications, pp. 261\u2013290. Springer, 2005.\n[3] Yago Pereiro Estevan, Vincent Wan, and Odette Scharenborg, \u201cFinding maximum margin segments in speech,\u201d in 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP\u201907. IEEE, 2007, vol. 4, pp. IV\u2013937.\n[4] George Almpanidis and Constantine Kotropoulos, \u201cPhonemic segmentation using the generalised gamma distribution and small sample bayesian information criterion,\u201d Speech Communication, vol. 50, no. 1, pp. 38\u2013 55, 2008.\n[5] Okko Rasanen, Toomas Altosaar, and Unto Laine, Blind segmentation of speech using non-linear filtering methods, INTECH Open Access Publisher, 2011.\n[6] Dac-Thang Hoang and Hsiao-Chuan Wang, \u201cBlind phone segmentation based on spectral change detection using legendre polynomial approximation,\u201d The Journal of the Acoustical Society of America, vol. 137, no. 2, pp. 797\u2013805, 2015.\n[7] Okko Ra\u0308sa\u0308nen, \u201cBasic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level,\u201d in Proceedings of the 36th Annual Conference of the Cognitive Science Society. Quebec, Canada, 2014.\n[8] Robert Wilber, \u201cThe concave least-weight subsequence problem revisited,\u201d Journal of Algorithms, vol. 9, no. 3, pp. 418\u2013425, 1988.\n[9] Yu Qiao, Naoya Shimomura, and Nobuaki Minematsu, \u201cUnsupervised optimal phoneme segmentation: Objectives, algorithm and comparisons,\u201d in 2008 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2008, pp. 3989\u20133992.\n[10] Herman Kamper, Aren Jansen, and Sharon Goldwater, \u201cFully unsupervised small-vocabulary speech recognition using a segmental bayesian model,\u201d in Proc. Interspeech, 2015.\n[11] James R Glass, \u201cA probabilistic framework for segmentbased speech recognition,\u201d Computer Speech & Language, vol. 17, no. 2, pp. 137\u2013152, 2003. [12] Man-hung Siu, Herbert Gish, Arthur Chan, William Belfield, and Steve Lowe, \u201cUnsupervized training of an HMM-based self-organizing recognizer with applications to topic classification and keyword discovery,\u201d Computer Speech & Language, vol. preprint, 2013.\n[13] Jeffrey L Elman, \u201cFinding structure in time,\u201d Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.\n[14] Morten H Christiansen, Joseph Allen, and Mark S Seidenberg, \u201cLearning to segment speech using multiple cues: A connectionist model,\u201d Language and cognitive processes, vol. 13, no. 2-3, pp. 221\u2013268, 1998.\n[15] Steven Davis and Paul Mermelstein, \u201cComparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,\u201d IEEE transactions on acoustics, speech, and signal processing, vol. 28, no. 4, pp. 357\u2013366, 1980.\n[16] Paul J Werbos, \u201cBackpropagation through time: what it does and how to do it,\u201d Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.\n[17] Tijmen Tieleman and Geoffrey Hinton, \u201cLecture 6.5- rmsprop: Divide the gradient by a running average of its recent magnitude,\u201d COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.\n[18] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong shortterm memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[19] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from overfitting.,\u201d Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\n[20] William M. Fischer, George R. Doddington, and Kathleen M Goudie-Marshall, \u201cThe darpa speech recognition research database: Specifications and status,\u201d in Proceedings of DARPA Workshop on Speech Recognition, 1986, pp. 93\u201399.\n[21] Okko Ra\u0308sa\u0308nen, Unto Kalervo Laine, and Toomas Altosaar, \u201cAn improved speech segmentation quality measure: the r-value.,\u201d in Proceedings of Interspeech, 2009."}], "references": [{"title": "On the relation between maximum spectral transition positions and phone boundaries", "author": ["Sorin Dusan", "Lawrence R Rabiner"], "venue": "INTERSPEECH. Citeseer, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Text independent methods for speech segmentation", "author": ["Anna Esposito", "Guido Aversano"], "venue": "Nonlinear Speech Modeling and Applications, pp. 261\u2013290. Springer, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Finding maximum margin segments in speech", "author": ["Yago Pereiro Estevan", "Vincent Wan", "Odette Scharenborg"], "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP\u201907. IEEE, 2007, vol. 4, pp. IV\u2013937.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Phonemic segmentation using the generalised gamma distribution and small sample bayesian information criterion", "author": ["George Almpanidis", "Constantine Kotropoulos"], "venue": "Speech Communication, vol. 50, no. 1, pp. 38\u2013 55, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Blind segmentation of speech using non-linear filtering methods, INTECH", "author": ["Okko Rasanen", "Toomas Altosaar", "Unto Laine"], "venue": "Open Access Publisher,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Blind phone segmentation based on spectral change detection using legendre polynomial approximation", "author": ["Dac-Thang Hoang", "Hsiao-Chuan Wang"], "venue": "The Journal of the Acoustical Society of America, vol. 137, no. 2, pp. 797\u2013805, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a pre-linguistic level", "author": ["Okko R\u00e4s\u00e4nen"], "venue": "Proceedings of the 36th Annual Conference of the Cognitive Science Society. Quebec, Canada, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The concave least-weight subsequence problem revisited", "author": ["Robert Wilber"], "venue": "Journal of Algorithms, vol. 9, no. 3, pp. 418\u2013425, 1988.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Unsupervised optimal phoneme segmentation: Objectives, algorithm and comparisons", "author": ["Yu Qiao", "Naoya Shimomura", "Nobuaki Minematsu"], "venue": "2008 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2008, pp. 3989\u20133992.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental bayesian model", "author": ["Herman Kamper", "Aren Jansen", "Sharon Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A probabilistic framework for segmentbased speech recognition", "author": ["James R Glass"], "venue": "Computer Speech & Language, vol. 17, no. 2, pp. 137\u2013152, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Unsupervized training of an HMM-based self-organizing recognizer with applications to topic classification and keyword discovery", "author": ["Man-hung Siu", "Herbert Gish", "Arthur Chan", "William Belfield", "Steve Lowe"], "venue": "Computer Speech & Language, vol. preprint, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning to segment speech using multiple cues: A connectionist model", "author": ["Morten H Christiansen", "Joseph Allen", "Mark S Seidenberg"], "venue": "Language and cognitive processes, vol. 13, no. 2-3, pp. 221\u2013268, 1998.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["Steven Davis", "Paul Mermelstein"], "venue": "IEEE transactions on acoustics, speech, and signal processing, vol. 28, no. 4, pp. 357\u2013366, 1980.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1980}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1929}, {"title": "The darpa speech recognition research database: Specifications and status", "author": ["William M. Fischer", "George R. Doddington", "Kathleen M Goudie-Marshall"], "venue": "Proceedings of DARPA Workshop on Speech Recognition, 1986, pp. 93\u201399.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1986}, {"title": "An improved speech segmentation quality measure: the r-value", "author": ["Okko R\u00e4s\u00e4nen", "Unto Kalervo Laine", "Toomas Altosaar"], "venue": "Proceedings of Interspeech, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "This last model is especially interesting in that it couples our statistical approach with more common spectral transition based methods ([1] for instance).", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 2, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 3, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 4, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 5, "context": "Most previous work on blind phoneme segmentation [2, 3, 4, 5, 6] has focused on the analysis of the rate of change in the spectral domain.", "startOffset": 49, "endOffset": 64}, {"referenceID": 6, "context": "In [7], the idea was to first discretize the signal using a clustering algorithm and then compute discrete sequence statistics, over which a threshold can be defined.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "The second approach is to use dynamic programming methods inspired by text segmentation [8], in order to derive optimal segmentation [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "The second approach is to use dynamic programming methods inspired by text segmentation [8], in order to derive optimal segmentation [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "The third approach consists in jointly segmenting and learning the acoustic models for phonemes [10, 11, 12].", "startOffset": 96, "endOffset": 108}, {"referenceID": 10, "context": "The third approach consists in jointly segmenting and learning the acoustic models for phonemes [10, 11, 12].", "startOffset": 96, "endOffset": 108}, {"referenceID": 11, "context": "The third approach consists in jointly segmenting and learning the acoustic models for phonemes [10, 11, 12].", "startOffset": 96, "endOffset": 108}, {"referenceID": 12, "context": "from previous work by Elman [13] and Christiansen et al.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "[14] published in the 90s.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "\u201d [13].", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "We used two kinds of speech features : 13 dimensional MFCCs [15] (with 12 mel-cepstrum coefficients and 1 energy coefficient) and categorical one-hot vectors derived from MFCCs inspired by [7].", "startOffset": 60, "endOffset": 64}, {"referenceID": 6, "context": "We used two kinds of speech features : 13 dimensional MFCCs [15] (with 12 mel-cepstrum coefficients and 1 energy coefficient) and categorical one-hot vectors derived from MFCCs inspired by [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 6, "context": "The latter are computed according to [7] : K-means clustering is performed on a random subset of the MFCCs (10000 frames were selected at random), with a target number of clusters of 8, then each MFCC is identified to the closest centroid.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "These hyper-parameters were chosen according to [7].", "startOffset": 48, "endOffset": 51}, {"referenceID": 15, "context": ", x0)) is minimized using back propagation through time [16] and stochastic gradient descent or a variant thereof (we have found RMSProp [17] to give the best results).", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": ", x0)) is minimized using back propagation through time [16] and stochastic gradient descent or a variant thereof (we have found RMSProp [17] to give the best results).", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "In our case, the network itself consists of two LSTM layers [18] stacked on one another followed by a linear layer and a softmax.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Dropout [19] with probability 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "9 Rasanen, 2014[7] 68.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "8 Rasanen, 2014[7] 74.", "startOffset": 15, "endOffset": 18}, {"referenceID": 19, "context": "We evaluated our methods on the TIMIT dataset [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 20, "context": "One such example is the R-value [21] :", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "Further details can be found in [21].", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "We decided to use the procedure described in [21] to match gold boundaries and hypothesized boundaries : overlapping tolerance windows are cropped in the middle of the two boundaries.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "The current state of the art in blind phoneme segmentation on the TIMIT corpus is provided by [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "In Tables 1 and 2 we compare our best results to the previous statistical approach evoked in [7] and the naive periodic boundaries segmentation (one boundary each 5ms).", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Since [7] used an evaluation method allowing for tolerance windows to overlap, we provide our results with both evaluation methods (full windows and cropped windows) for the sake of consistency.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "In terms of optimal F-score and R values, the simple Markov model outperformed the previously published paper using short term sequential statistics [7], as well as the recurrent", "startOffset": 149, "endOffset": 152}], "year": 2016, "abstractText": "Phonemic segmentation of speech is a critical step of speech recognition systems. We propose a novel unsupervised algorithm based on sequence prediction models such as Markov chains and recurrent neural network. Our approach consists in analyzing the error profile of a model trained to predict speech features frame-by-frame. Specifically, we try to learn the dynamics of speech in the MFCC space and hypothesize boundaries from local maxima in the prediction error. We evaluate our system on the TIMIT dataset, with improvements over similar methods.", "creator": "LaTeX with hyperref package"}}}