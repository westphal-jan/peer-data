{"id": "1608.00869", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Aug-2016", "title": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity", "abstract": "Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.", "histories": [["v1", "Tue, 2 Aug 2016 15:35:12 GMT  (101kb,D)", "http://arxiv.org/abs/1608.00869v1", "EMNLP 2016"], ["v2", "Wed, 3 Aug 2016 15:39:53 GMT  (104kb,D)", "http://arxiv.org/abs/1608.00869v2", "EMNLP 2016"], ["v3", "Tue, 9 Aug 2016 06:20:24 GMT  (382kb,D)", "http://arxiv.org/abs/1608.00869v3", "EMNLP 2016"], ["v4", "Tue, 20 Sep 2016 14:35:14 GMT  (378kb,D)", "http://arxiv.org/abs/1608.00869v4", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniela gerz", "ivan vulic", "felix hill", "roi reichart", "anna korhonen"], "accepted": true, "id": "1608.00869"}, "pdf": {"name": "1608.00869.pdf", "metadata": {"source": "CRF", "title": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity", "authors": ["Daniela Gerz", "Ivan Vuli\u0107", "Felix Hill", "Roi Reichart", "Anna Korhonen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Verbs are famously both complex and variable. They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation,\ntext mining) as well as research on human language acquisition and processing (Korhonen, 2010). Precise methods for representing and understanding verb semantics will undoubtedly be necessary for machines to interpret the meaning of sentences with similar accuracy to humans.\nNumerous algorithms for acquiring word representations from text and/or more structured knowledge bases have been developed in recent years (Mikolov et al., 2013; Pennington et al., 2014; Faruqui et al., 2015). These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010). Nevertheless, the predominant approaches to distributed representation learning apply a single learning algorithm and representational form for all words in a vocabulary. This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015).\nOne factor behind the lack of more nuanced word representation learning methods is the scarcity of satisfactory ways to evaluate or analyse representations of particular word types. Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling. Consequently, for word classes such as verbs, whose semantics is critical for language understanding, it is practically impossible to achieve statistically robust analyses and comparisons between different\nar X\niv :1\n60 8.\n00 86\n9v 1\n[ cs\n.C L\n] 2\nA ug\nrepresentation learning architectures. To overcome this barrier to verb semantics research, we introduce SimVerb-3500 \u2013 an extensive intrinsic evaluation resource that is unprecedented in both size and coverage. SimVerb-3500 includes 827 verb types from the University of South Florida Free Association Norms (USF) (Nelson et al., 2004), and at least 3 member verbs from each of the 101 top-level VerbNet classes (Kipper et al., 2008). This coverage enables researchers to better understand the complex diversity of syntactic-semantic verb behaviours, and provides direct links to other established semantic resources such as WordNet (Miller, 1995) and PropBank (Palmer et al., 2005). Moreover, the large standardised development and test sets in SimVerb-3500 allow for principled tuning of hyperparameters, a critical aspect of achieving strong performance with the latest representation learning architectures.\nIn \u00a7 2, we discuss previous evaluation resources targeting verb similarity. We present the new SimVerb-3500 data set along with our design choices and the pair selection process in \u00a7 3, while the annotation process is detailed in \u00a7 4. In \u00a7 5 we report the performance of a diverse range of popular representation learning architectures, together with benchmark performance on existing evaluation sets. In \u00a7 6, we show how SimVerb-3500 enables a variety of new linguistic analyses, which were previously impossible due to the lack of coverage and scale in existing resources."}, {"heading": "2 Related Work", "text": "A natural way to evaluate representation quality is by judging the similarity of representations assigned to similar words. The most popular evaluation sets at present consist of word pairs with similarity ratings produced by human annotators.1 Nevertheless, we find that all available datasets of this kind are insufficient for judging verb similarity due to their small size or narrow coverage of verbs.\nIn particular, a number of word pair evaluation sets are prominent in the distributional semantics\n1In some existing evaluation sets pairs are scored for relatedness which has some overlap with similarity. SimVerb-3500 focuses on similarity as this is a more focused semantic relation that seems to yield a higher agreement between human annotators. For a broader discussion see (Hill et al., 2015).\nliterature. Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015).\nTwo datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 and 222, respectively), making them unrepresentative of the rich variety of verb semantic phenomena.\nIn this paper we provide a remedy for this problem by presenting a more comprehensive and representative verb pair evaluation resource."}, {"heading": "3 The SimVerb-3500 Data Set", "text": "In this section, we discuss the design principles behind SimVerb-3500. We first demonstrate that a new evaluation resource for verb similarity is a necessity. We then describe how the final verb pairs were selected with the goal to be representative, that is, to guarantee a wide coverage of two standard semantic resources: USF and VerbNet."}, {"heading": "3.1 Design Motivation", "text": "Hill et al. (2015) argue that comprehensive highquality evaluation resources have to satisfy the following three criteria: (C1) Representative (the resource covers the full range of concepts occurring in natural language); (C2) Clearly defined (it clearly defines the annotated relation, e.g., similarity); (C3) Consistent and reliable (untrained native speakers must be able to quantify the target relation consistently relying on simple instructions).\nBuilding on the same annotation guidelines as Simlex-999 that explicitly targets similarity, we ensure that criteria C2 and C3 are satisfied. However, even SimLex, as the most extensive evaluation resource for verb similarity available at present, is still of limited size, spanning only 222 verb pairs and 170 distinct verb lemmas in total. Given that 39 out of the\n101 top-level VerbNet classes are not represented at all in SimLex, while 20 classes have only one member verb,2 one may conclude that the criterion C1 is not at all satisfied with current resources.\nThere is another fundamental limitation of all current verb similarity evaluation resources: automatic approaches have reached or surpassed the interannotator agreement ceiling. For instance, while the average pairwise correlation between annotators on SL-222 is Spearman\u2019s \u03c1 correlation of 0.717, the best performing automatic system reaches \u03c1 = 0.727 (Mrk\u0161ic\u0301 et al., 2016). SimVerb-3500 does not inherit this anomaly (see Tab. 2) and demonstrates that there still exists an evident gap between the human and system performance.\nIn order to satisfy C1-C3, the new SimVerb-3500 evaluation set contains similarity ratings for 3,500 verb pairs, containing 827 verb types in total and 3 member verbs for each top-level VerbNet class. The rating scale goes from 0 (not similar at all) to 10 (synonymous). We employed the SimLex-999 annotation guidelines. In particular, we instructed annotators to give low ratings to antonyms, and to distinguish between similarity and relatedness. Pairs that are related but not similar (e.g., to snore / to snooze, to walk / to crawl) thus have a fairly low rating. Several example pairs are provided in Tab. 1."}, {"heading": "3.2 Choice of Verb Pairs and Coverage", "text": "To ensure a wide coverage of a variety of syntacticosemantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set3 (Nelson et al., 2004), and (2) the VerbNet verb lexicon4 (Kipper et al., 2004; Kipper et al., 2008).\nThe USF norms data set (further USF) is the largest database of free association collected for English. It was generated by presenting human subjects with one of 5, 000 cue concepts and asking them to write the first word coming to mind that is associated with that concept. Each cue concept c was normed in this way by over 10 participants, resulting in a set of\n2Note that verbs in VerbNet are soft clustered, and one verb type may be associated with more than one class. When computing coverage, we assume that such verbs attribute to counts of all their associated classes.\n3http://w3.usf.edu/FreeAssociation/ 4http://verbs.colorado.edu/verb-index/\nassociates a for each cue, for a total of over 72, 000 (c, a) pairs. For each such pair, the proportion of participants who produced associate a when presented with cue c can be used as a proxy for the strength of association between the two words.\nThe norming process guarantees that two words in a pair have a degree of semantic association which correlates well with semantic relatedness and similarity. Sampling from the USF set ensures that both related but non-similar pairs (e.g., to run / to sweat) as well as similar pairs (e.g., to reply / to respond) are represented in the final list of pairs. Further, the rich annotations of the output USF data (e.g., concreteness scores, association strength) can be directly combined with the SimVerb-3500 similarity scores to yield additional analyses and insight.\nVerbNet (VN) is the largest online verb lexicon currently available for English. It is hierarchical, domain-independent, and broad-coverage. VN is organised into verb classes extending the classes from Levin (1993) through further refinement to achieve syntactic and semantic coherence among class members. According to the official VerbNet guidelines,5 \u201cVerb Classes are numbered according to shared semantics and syntax, and classes which share a toplevel number (9-109) have corresponding semantic relationships.\u201d For instance, all verbs from the toplevel Class 9 are labelled \u201cVerbs of Putting\u201d, all verbs from Class 30 are labelled \u201cVerbs of Perception\u201d, while Class 39 contains \u201cVerbs of Ingesting\u201d.\nAmong others, three basic types of information are covered in VN: (1) verb subcategorization frames (SCFs), which describe the syntactic realization of the predicate-argument structure (e.g. The window broke), (2) selectional preferences (SPs), which capture the semantic preferences verbs have for their arguments (e.g. a breakable physical object broke)\n5http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf\nand (3) lexical-semantic verb classes (VCs) which provide a shared level of abstraction for verbs similar in their (morpho-)syntactic and semantic properties (e.g. BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45).6 The basic overview of the VerbNet structure already suggests that measuring verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties.\nThe wide coverage of VN in SimVerb-3500 assures the wide coverage of distinct verb groups/classes and their related linguistic phenomena. Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007).7\nSampling Procedure We next sketch the complete sampling procedure which resulted in the final set of 3500 distinct verb pairs finally annotated in a crowdsourcing study (\u00a7 4). (Step 1) We extracted all possible verb pairs from USF based on the associated POS tags available as part of USF annotations. To ensure that semantic association between verbs in a pair is not accidental, we then discarded all such USF pairs that had been associated by 2 or less participants in USF. (Step 2) We then manually cleaned the list of pairs by removing all pairs with multi-word verbs (e.g., quit / give up), all pairs that contained the non-infinitive form of a verb (e.g., accomplished / finished, hidden / find), removing all pairs containing at least one auxiliary verb (e.g., must / to see, must / to be). The first two steps resulted in 3,072 USF-based verb pairs. (Step 3) After this stage, we noticed that several toplevel VN classes are not part of the extracted set. For instance, 5 VN classes did not have any member verbs included, 22 VN classes had only 1 verb, and 6 VN classes had 2 verbs included in the current set.\nWe resolved the VerbNet coverage issue by sampling from such \u2019under-represented\u2019 VN classes directly. Note that this step is not related to USF at all. For each such class we sampled additional verb types until the class was represented by 3 or 4 mem-\n6https://verbs.colorado.edu/verb-index/vn/break-45.1.php 7https://verbs.colorado.edu/semlink/\nber verbs (chosen randomly).8 Following that, we sampled at least 2 verb pairs for each previously \u2019under-represented\u2019 VN class by pairing 2 member verbs from each such class. This procedure resulted in 81 additional pairs, now 3,153 in total.\n(Step 4) Finally, to complement this set with a sample of entirely unassociated pairs, we followed the SimLex-999 setup. We paired up the verbs from the 3,153 associated pairs at random. From these random parings, we excluded those that coincidentally occurred elsewhere in USF (and therefore had a degree of association). We sampled the remaining 347 pairs from this resulting set of unassociated pairs.\n(Output) The final SimVerb-3500 data set contains 3,500 verb pairs in total, covering all associated verb pairs from USF, and (almost) all top-level VerbNet classes. All pairs were manually checked post-hoc by the authors plus 2 additional native English speakers to verify that the final data set does not contain unknown or invalid verb types.\nFrequency Statistics The 3,500 pairs consist of 827 distinct verbs. 29 top-level VN classes are represented by 3 member verbs, while the three most represented classes cover 79, 85, and 93 member verbs. 40 verbs are not members of any VN class.\nWe performed an initial frequency analysis of SimVerb-3500 relying on the BNC counts available online (Kilgarriff, 1997).9 After ranking all BNC verbs according to their frequency, we divided the list into quartiles: Q1 (most frequent verbs in BNC) - Q4 (least frequent verbs in BNC). Out of the 827 SimVerb-3500 verb types, 677 are contained in Q1, 122 in Q2, 18 in Q3, 4 in Q4 (to enroll, to hitchhike, to implode, to whelp), while 6 verbs are not covered in the BNC list. 2,818 verb pairs contain Q1 verbs, while there are 43 verb pairs with both verbs not in Q1. Further empirical analyses are provided in \u00a7 6.10\n8The following three VN classes are exceptions: (1) Class 56, consisting of words that are dominantly tagged as nouns, but can be used as verbs exceptionally (e.g., holiday, summer, honeymoon); (2) Class 91, consisting of 2 verbs (count, matter); (3) Class 93, consisting of 2 single word verbs (adopt, assume).\n9https://www.kilgarriff.co.uk/bnc-readme.html 10Annotations such as VerbNet class membership, relations between WordNet synsets of each verb, and frequency statistics are available as supplementary material."}, {"heading": "4 Word Pair Scoring", "text": "We employ the Prolific Academic (PA) crowdsourcing platform,11 an online marketplace very similar to Amazon Mechanical Turk and to CrowdFlower."}, {"heading": "4.1 Survey Structure", "text": "Following the SimLex-999 annotation guidelines, we had each of the 3500 verb pairs rated by at least 10 annotators. To distribute the workload, we divided the 3500 pairs into 70 tranches, with 79 pairs each. Out of the 79 pairs, 50 are unique to one tranche, while 20 manually chosen pairs are in all tranches to ensure consistency. The remaining 9 are duplicate pairs displayed to the same participant multiple times to detect inconsistent annotations.\nParticipants see 7-8 pairs per page. Pairs are rated on a scale of 0-6 by moving a slider. The first page shows 7 pairs, 5 unique ones and 2 from the consistency set. The following pages are structured the same but display one extra pair from the previous page. Participants are explicitly asked to give these duplicate pairs the same rating. We use them as quality control so that we can identify and exclude participants giving several inconsistent answers.\nCheckpoint Questions The survey contains three control questions in which participants are asked to select the most similar pair out of three choices. For instance, the first checkpoint is: Which of these pairs of words is the *most* similar? 1. to run / to jog 2. to run / to walk 3. to jog / to sweat. One checkpoint occurs right after the instructions and the other two later in the survey. The purpose is to check that annotators have understood the guidelines and to have another quality control measure for ensuring that they are paying attention throughout the survey. If just one of the checkpoint questions is answered incorrectly, the survey ends immediately and all scores from the annotator in question are discarded.\nParticipants 843 raters participated in the study, producing over 65,000 ratings. Unlike other crowdsourcing platforms, PA collects and stores detailed demographic information from the participants upfront. This information was used to carefully select the pool of eligible participants. We restricted the pool to native English speakers with a 90% approval\n11https://prolific.ac/ (We chose PA for logistic reasons.)\nrate (maximum rate on PA), of age 18-50, born and currently residing in the US (45% out of 843 raters), UK (53%), or Ireland (2%). 54% of the raters were female and 46% male, with the average age of 30. Participants took 8 minutes on average to complete the survey containing 79 questions."}, {"heading": "4.2 Post-Processing", "text": "We excluded ratings of annotators who (a) answered one of the checkpoint questions incorrectly (75% of exclusions); (b) did not give equal ratings to duplicate pairs; (c) showed suspicious rating patterns (e.g., randomly alternating between two ratings or using one single rating throughout). The final acceptance rate was 84%. We then calculated the average of all ratings from the accepted raters ( \u2265 10 ) for each pair. The score was finally scaled linearly from the 0-6 to the 0-10 interval as in (Hill et al., 2015)."}, {"heading": "5 Analysis", "text": "Inter-Annotator Agreement We employ two measures. IAA-1 (pairwise) computes the average pairwise Spearman\u2019s \u03c1 correlation between any two raters \u2013 a common choice in previous data collection in distributional semantics (Pad\u00f3 et al., 2007; Reisinger and Mooney, 2010a; Silberer and Lapata, 2014; Hill et al., 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. SimVerb-3500 obtains \u03c1 = 0.84 (IAA-1) and \u03c1 = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab. 2).\nVector Space Models We compare the performance of prominent representation models on SimVerb-3500. We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), and the symmetric-pattern based vectors by Schwartz et al. (2015); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use sparse binary vectors built from linguistic resources (Non-Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al.,\n2015)) further refined using linguistic constraints (Paragram+CF, (Mrk\u0161ic\u0301 et al., 2016)). Descriptions of these models are in the supplementary material.\nComparison to SimLex-999 (SL-222) 170 pairs from SL-222 also appear in SimVerb-3500. The correlation between the two data sets calculated on the shared pairs is \u03c1 = 0.91. This proves, as expected, that the ratings are consistent across the two data sets.\nTab. 3 shows a comparison of models\u2019 performance on SimVerb-3500 against SL-222. Since the number of evaluation pairs may influence the results, we ideally want to compare sets of equal size for a fair comparison. Picking one random subset of 222 pairs would bias the results towards the selected pairs, and even using 10-fold cross-validation we found variations up to 0.05 depending on which subsets were used. Therefore, we employ a 2-level 10-fold crossvalidation where new random subsets are picked in each iteration of each model. The numbers reported as CV-222 are averages of these ten 10-fold crossvalidation runs. The reported results come very close to the correlation on the full data set for all models.\nMost models perform much better on SL-222, especially those employing additional databases or linguistic resources. The performance of the best scoring Paragram+CF model is even on par with the IAA-1 of 0.72. The same model obtains the highest score on SV-3500 (\u03c1 = 0.628), with a clear gap to IAA-1 of 0.84. We attribute these differences in performance largely to SimVerb-3500 being a more extensive and diverse resource in terms of verb pairs.\nDevelopment Set A common problem in scored word pair datasets is the lack of a standard split to development and test sets. Previous works often optimise models on the entire dataset, which leads to overfitting (Faruqui et al., 2016) or use custom splits, e.g., 10-fold cross-validation (Schwartz et al., 2015), which make results incomparable with others. The lack of standard splits stems mostly from small size and poor coverage \u2013 issues which we have solved with SimVerb-3500.\nOur development set contains 500 pairs, selected to ensure a broad coverage in terms of similarity ranges (i.e., non-similar and highly similar pairs, as well as pairs of medium similarity are represented) and top-level VN classes (each class is represented by at least 1 member verb). The test set includes the remaining 3,000 verb pairs. The performances of representation learning architectures on the dev and test sets are reported in Tab. 3. The ranking of models is identical on the test and the full SV-3500 set, with slight differences in ranking on the development set."}, {"heading": "6 Evaluating Subsets", "text": "The large coverage and scale of SimVerb-3500 enables model evaluation based on selected criteria. In this section, we showcase a few example analyses.\nFrequency In the first analysis, we select pairs based on their lemma frequency in the BNC corpus and form three groups, with 390-490 pairs in each group (Fig. 1). The results from Fig. 1 suggest that the performance of all models improves as the frequency of the verbs in the pair increases, with much steeper curves for the purely distributional models (e.g., SGNS and SymPat). The non-distributional non data-driven model of Faruqui and Dyer (2015) is only slightly affected by frequency.\nWordNet Synsets Intuitively, representations for verbs with more diverse usage patterns are more difficult to learn with statistical models. To examine this hypothesis, we resort to WordNet (Miller, 1995), where different semantic usages of words are listed as so-called synsets. Fig. 2 shows a clear downward trend for all models, confirming that polysemous verbs are more difficult for current verb representation models. Nevertheless, approaches which use additional information beyond corpus co-occurrence\nare again more robust. Their performance only drops substantially for verbs with more than 10 synsets, while the performance of other models deteriorates already when tackling verbs with more than 5 synsets.\nVerbNet Classes Another analysis enabled by SimVerb-3500 is investigating the connection between VerbNet classes and human similarity judgments. We find that verbs in the same top-level VerbNet class are often not assigned high similarity score. Out of 1378 pairs where verbs share the top-level VerbNet class, 603 have a score lower than 5. Tab. 4 reports scores per VerbNet class. When a verb belongs to multiple classes, we count it for each class (see Footnote 2). We run the analysis on the five largest VN classes, each with more than 100 pairs with paired verbs belonging to the same class.\nThe results indicate clear differences between classes (e.g., Class 31 vs Class 51), and suggest that further developments in verb representation learning should also focus on constructing specialised representations at the finer-grained level of VN classes.\nLexical Relations SimVerb-3500 contains relation annotations (e.g., antonyms, synonyms, hyper/hyponyms, no relation) for all pairs extracted automatically from WordNet. Evaluating per-relation subsets, we observe that some models draw their strength from good performance across different relations. Others have low performance on these pairs, but do very well on synonyms and hyper-/hyponyms. Selected results of this analysis are in Tab. 5.12\n12 Evaluation based on Spearman\u2019s \u03c1 may be problematic with certain categories, e.g., with antonyms. It evaluates pairs\nHuman Agreement Motivated by the varying performance of computational models regarding frequency and ambiguous words with many synsets, we analyse what disagreement effects may be captured in human ratings. We therefore compute the average standard deviation of ratings per subset: avgstdd(S) = 1n \u2211 p\u2208S \u03c3(rp), where S is one subset of pairs, n is the number of pairs in this subset, p is one pair, and rp are all human ratings for this pair.\nWhile the standard deviation of ratings is diverse for individual pairs, overall the average standard deviations per subset are almost identical. For both the frequency and the WordNet synset analyses it is around \u22481.3 across all subsets, and with only little according to their ranking; for antonyms the ranking is arbitrary - every antonym pair should have a very low rating, hence they are not included in Tab. 5. A similar effect occurs with highly ranked synonyms, but to a much lesser degree than with antonyms.\ndifference for the subsets based on VerbNet. The only subsets where we found significant variations is the grouping by relations, where ratings tend to be more similar especially on antonyms (0.86) and pairs with no relation (0.92), much less similar on synonyms (1.34) and all other relations (\u22481.4). These findings suggest that humans are much less influenced by frequency or polysemy in their understanding of verb semantics compared to computational models."}, {"heading": "7 Conclusions", "text": "SimVerb-3500 is a verb similarity resource for analysis and evaluation that will be of use to researchers involved in understanding how humans or machines represent the meaning of verbs, and, by extension, scenes, events and full sentences. The size and coverage of syntactico-semantic phenomena in SimVerb3500 makes it possible to compare the strengths and weaknesses of various representation models via statistically robust analyses on specific word classes.\nTo demonstrate the utility of SimVerb-3500, we conducted a selection of analyses with existing representation-learning models. One clear conclusion is that distributional models trained on raw text (e.g. SGNS) perform very poorly on low frequency and highly polysemous verbs. This degradation in performance can be partially mitigated by focusing models on more principled distributional contexts, such as those defined by symmetric patterns. More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015). In future work we aim to apply such methods to the task of verb acquisition.\nBeyond the preliminary conclusions from these initial analyses, the benefit of SimLex-3500 will become clear as researchers use it to probe the relationship between architectures, algorithms and representation quality for a wide range of verb classes. Better understanding of how to represent the full diversity of verbs should in turn yield improved methods for encoding and interpreting the facts, propositions, relations and events that constitute much of the important information in language."}], "references": [{"title": "A study on similarity and relatedness using distributional and WordNet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith B. Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa."], "venue": "NAACL-HLT, pages 19\u201327.", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "The Berkeley FrameNet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."], "venue": "ACLCOLING, pages 86\u201390.", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "An unsupervised model for instance level subcategorization acquisition", "author": ["Simon Baker", "Roi Reichart", "Anna Korhonen."], "venue": "EMNLP, pages 278\u2013289.", "citeRegEx": "Baker et al\\.,? 2014", "shortCiteRegEx": "Baker et al\\.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."], "venue": "Journal of Artificial Intelligence Research, 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "ICML, pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Non-distributional word vector representations", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "ACL, pages 464\u2013469.", "citeRegEx": "Faruqui and Dyer.,? 2015", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy", "Noah A. Smith."], "venue": "NAACL-HLT, pages 1606\u20131615.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Problems with evaluation of word embeddings using word similarity tasks", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer."], "venue": "CoRR, abs/1605.02276.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "ACM Transactions on Information Systems, 20(1):116\u2013 131.", "citeRegEx": "Finkelstein et al\\.,? 2002", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Lexical structure in syntax and semantics", "author": ["Jeffrey Gruber."], "venue": "North-Holland Pub. Co.", "citeRegEx": "Gruber.,? 1976", "shortCiteRegEx": "Gruber.", "year": 1976}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics, 41(4):665\u2013695.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Semantic interpretation in generative grammar", "author": ["Ray S. Jackendoff."], "venue": "MIT Press.", "citeRegEx": "Jackendoff.,? 1972", "shortCiteRegEx": "Jackendoff.", "year": 1972}, {"title": "Putting frequencies in the dictionary", "author": ["Adam Kilgarriff."], "venue": "International Journal of Lexicography, 10(2):135\u2013 155.", "citeRegEx": "Kilgarriff.,? 1997", "shortCiteRegEx": "Kilgarriff.", "year": 1997}, {"title": "Extending a verb-lexicon using a semantically annotated corpus", "author": ["Karin Kipper", "Benjamin Snyder", "Martha Palmer."], "venue": "LREC, pages 1557\u20131560.", "citeRegEx": "Kipper et al\\.,? 2004", "shortCiteRegEx": "Kipper et al\\.", "year": 2004}, {"title": "A large-scale classification of English verbs", "author": ["Karin Kipper", "Anna Korhonen", "Neville Ryant", "Martha Palmer."], "venue": "Language Resource and Evaluation, 42(1):21\u2013", "citeRegEx": "Kipper et al\\.,? 2008", "shortCiteRegEx": "Kipper et al\\.", "year": 2008}, {"title": "Automatic lexical classification: bridging research and practice", "author": ["Anna Korhonen."], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 368(1924):3621\u2013 3632.", "citeRegEx": "Korhonen.,? 2010", "shortCiteRegEx": "Korhonen.", "year": 2010}, {"title": "One shot learning of simple visual concepts", "author": ["Brenden M. Lake", "Ruslan Salakhutdinov", "Jason Gross", "Joshua B. Tenenbaum."], "venue": "CogSci.", "citeRegEx": "Lake et al\\.,? 2011", "shortCiteRegEx": "Lake et al\\.", "year": 2011}, {"title": "English verb classes and alternation, A preliminary investigation", "author": ["Beth Levin."], "venue": "The University of Chicago Press.", "citeRegEx": "Levin.,? 1993", "shortCiteRegEx": "Levin.", "year": 1993}, {"title": "Dependency-based word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "ACL, pages 302\u2013308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Combining lexical resources: Mapping between PropBank and VerbNet", "author": ["Edward Loper", "Szu-Ting Yi", "Martha Palmer."], "venue": "IWCS.", "citeRegEx": "Loper et al\\.,? 2007", "shortCiteRegEx": "Loper et al\\.", "year": 2007}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher Manning."], "venue": "CoNLL, pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "ICLR: Workshop Papers.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Counter-fitting word vectors to linguistic constraints", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Maria Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve J. Young."], "venue": "NAACL-HLT.", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? 2016", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2016}, {"title": "The University of South Florida free association, rhyme, and word fragment norms", "author": ["Douglas L. Nelson", "Cathy L. McEvoy", "Thomas A. Schreiber."], "venue": "Behavior Research Methods, Instruments, & Computers, 36(3):402\u2013407.", "citeRegEx": "Nelson et al\\.,? 2004", "shortCiteRegEx": "Nelson et al\\.", "year": 2004}, {"title": "Flexible, corpus-based modelling of human plausibility judgements", "author": ["Sebastian Pad\u00f3", "Ulrike Pad\u00f3", "Katrin Erk."], "venue": "EMNLP-CoNLL, pages 400\u2013409.", "citeRegEx": "Pad\u00f3 et al\\.,? 2007", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2007}, {"title": "The Proposition Bank: An annotated corpus of semantic roles", "author": ["Martha Palmer", "Paul Kingsbury", "Daniel Gildea."], "venue": "Computational Linguistics, 31(1):71\u2013106.", "citeRegEx": "Palmer et al\\.,? 2005", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "EMNLP, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A mixture model with sharing for lexical semantics", "author": ["Joseph Reisinger", "Raymond J. Mooney."], "venue": "EMNLP, pages 1173\u20131182.", "citeRegEx": "Reisinger and Mooney.,? 2010a", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Multiprototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J Mooney."], "venue": "NAACL-HTL, pages 109\u2013117.", "citeRegEx": "Reisinger and Mooney.,? 2010b", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Symmetric pattern based word embeddings for improved word similarity prediction", "author": ["Roy Schwartz", "Roi Reichart", "Ari Rappoport."], "venue": "CoNLL, pages 258\u2013267.", "citeRegEx": "Schwartz et al\\.,? 2015", "shortCiteRegEx": "Schwartz et al\\.", "year": 2015}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Carina Silberer", "Mirella Lapata."], "venue": "ACL, pages 721\u2013732.", "citeRegEx": "Silberer and Lapata.,? 2014", "shortCiteRegEx": "Silberer and Lapata.", "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio."], "venue": "ACL, pages 384\u2013394.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Word representations via Gaussian embedding", "author": ["Luke Vilnis", "Andrew McCallum."], "venue": "ICLR.", "citeRegEx": "Vilnis and McCallum.,? 2015", "shortCiteRegEx": "Vilnis and McCallum.", "year": 2015}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Transactions of the ACL, 3:345\u2013358.", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993).", "startOffset": 174, "endOffset": 219}, {"referenceID": 9, "context": "They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993).", "startOffset": 174, "endOffset": 219}, {"referenceID": 17, "context": "They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993).", "startOffset": 174, "endOffset": 219}, {"referenceID": 15, "context": "machine translation, text mining) as well as research on human language acquisition and processing (Korhonen, 2010).", "startOffset": 99, "endOffset": 115}, {"referenceID": 21, "context": "Numerous algorithms for acquiring word representations from text and/or more structured knowledge bases have been developed in recent years (Mikolov et al., 2013; Pennington et al., 2014; Faruqui et al., 2015).", "startOffset": 140, "endOffset": 209}, {"referenceID": 27, "context": "Numerous algorithms for acquiring word representations from text and/or more structured knowledge bases have been developed in recent years (Mikolov et al., 2013; Pennington et al., 2014; Faruqui et al., 2015).", "startOffset": 140, "endOffset": 209}, {"referenceID": 6, "context": "Numerous algorithms for acquiring word representations from text and/or more structured knowledge bases have been developed in recent years (Mikolov et al., 2013; Pennington et al., 2014; Faruqui et al., 2015).", "startOffset": 140, "endOffset": 209}, {"referenceID": 4, "context": "These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010).", "startOffset": 124, "endOffset": 173}, {"referenceID": 33, "context": "These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010).", "startOffset": 124, "endOffset": 173}, {"referenceID": 31, "context": "This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015).", "startOffset": 189, "endOffset": 212}, {"referenceID": 3, "context": "Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al.", "startOffset": 22, "endOffset": 42}, {"referenceID": 20, "context": ", 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 10, "context": ", 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling.", "startOffset": 23, "endOffset": 42}, {"referenceID": 24, "context": "SimVerb-3500 includes 827 verb types from the University of South Florida Free Association Norms (USF) (Nelson et al., 2004), and at least 3 member verbs from each of the 101 top-level VerbNet classes (Kipper et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 14, "context": ", 2004), and at least 3 member verbs from each of the 101 top-level VerbNet classes (Kipper et al., 2008).", "startOffset": 84, "endOffset": 105}, {"referenceID": 22, "context": "This coverage enables researchers to better understand the complex diversity of syntactic-semantic verb behaviours, and provides direct links to other established semantic resources such as WordNet (Miller, 1995) and PropBank (Palmer et al.", "startOffset": 198, "endOffset": 212}, {"referenceID": 26, "context": "This coverage enables researchers to better understand the complex diversity of syntactic-semantic verb behaviours, and provides direct links to other established semantic resources such as WordNet (Miller, 1995) and PropBank (Palmer et al., 2005).", "startOffset": 226, "endOffset": 247}, {"referenceID": 10, "context": "For a broader discussion see (Hill et al., 2015).", "startOffset": 29, "endOffset": 48}, {"referenceID": 30, "context": "Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim353 (Finkelstein et al.", "startOffset": 38, "endOffset": 71}, {"referenceID": 8, "context": "Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively).", "startOffset": 87, "endOffset": 134}, {"referenceID": 0, "context": "Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively).", "startOffset": 87, "endOffset": 134}, {"referenceID": 20, "context": "Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena.", "startOffset": 61, "endOffset": 81}, {"referenceID": 10, "context": "Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015).", "startOffset": 74, "endOffset": 93}, {"referenceID": 0, "context": ", 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena.", "startOffset": 8, "endOffset": 241}, {"referenceID": 10, "context": "(2014) and Simlex-999 (Hill et al., 2015).", "startOffset": 22, "endOffset": 41}, {"referenceID": 1, "context": "Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al.", "startOffset": 76, "endOffset": 96}, {"referenceID": 23, "context": "727 (Mrk\u0161i\u0107 et al., 2016).", "startOffset": 4, "endOffset": 25}, {"referenceID": 24, "context": "To ensure a wide coverage of a variety of syntacticosemantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set3 (Nelson et al., 2004), and (2) the VerbNet verb lexicon4 (Kipper et al.", "startOffset": 194, "endOffset": 215}, {"referenceID": 13, "context": ", 2004), and (2) the VerbNet verb lexicon4 (Kipper et al., 2004; Kipper et al., 2008).", "startOffset": 43, "endOffset": 85}, {"referenceID": 14, "context": ", 2004), and (2) the VerbNet verb lexicon4 (Kipper et al., 2004; Kipper et al., 2008).", "startOffset": 43, "endOffset": 85}, {"referenceID": 17, "context": "VN is organised into verb classes extending the classes from Levin (1993) through further refinement to achieve syntactic and semantic coherence among class members.", "startOffset": 61, "endOffset": 74}, {"referenceID": 1, "context": "Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al.", "startOffset": 115, "endOffset": 135}, {"referenceID": 22, "context": ", 1998), WordNet (Miller, 1995), and PropBank (Palmer et al.", "startOffset": 17, "endOffset": 31}, {"referenceID": 26, "context": ", 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 19, "context": ", 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007).", "startOffset": 79, "endOffset": 99}, {"referenceID": 12, "context": "We performed an initial frequency analysis of SimVerb-3500 relying on the BNC counts available online (Kilgarriff, 1997).", "startOffset": 102, "endOffset": 120}, {"referenceID": 10, "context": "The score was finally scaled linearly from the 0-6 to the 0-10 interval as in (Hill et al., 2015).", "startOffset": 78, "endOffset": 97}, {"referenceID": 25, "context": "IAA-1 (pairwise) computes the average pairwise Spearman\u2019s \u03c1 correlation between any two raters \u2013 a common choice in previous data collection in distributional semantics (Pad\u00f3 et al., 2007; Reisinger and Mooney, 2010a; Silberer and Lapata, 2014; Hill et al., 2015).", "startOffset": 169, "endOffset": 263}, {"referenceID": 28, "context": "IAA-1 (pairwise) computes the average pairwise Spearman\u2019s \u03c1 correlation between any two raters \u2013 a common choice in previous data collection in distributional semantics (Pad\u00f3 et al., 2007; Reisinger and Mooney, 2010a; Silberer and Lapata, 2014; Hill et al., 2015).", "startOffset": 169, "endOffset": 263}, {"referenceID": 32, "context": "IAA-1 (pairwise) computes the average pairwise Spearman\u2019s \u03c1 correlation between any two raters \u2013 a common choice in previous data collection in distributional semantics (Pad\u00f3 et al., 2007; Reisinger and Mooney, 2010a; Silberer and Lapata, 2014; Hill et al., 2015).", "startOffset": 169, "endOffset": 263}, {"referenceID": 10, "context": "IAA-1 (pairwise) computes the average pairwise Spearman\u2019s \u03c1 correlation between any two raters \u2013 a common choice in previous data collection in distributional semantics (Pad\u00f3 et al., 2007; Reisinger and Mooney, 2010a; Silberer and Lapata, 2014; Hill et al., 2015).", "startOffset": 169, "endOffset": 263}, {"referenceID": 5, "context": "Here, we use sparse binary vectors built from linguistic resources (Non-Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al.", "startOffset": 88, "endOffset": 112}, {"referenceID": 17, "context": "We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), and the symmetric-pattern based vectors by Schwartz et al.", "startOffset": 220, "endOffset": 245}, {"referenceID": 17, "context": "We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), and the symmetric-pattern based vectors by Schwartz et al. (2015); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases.", "startOffset": 220, "endOffset": 312}, {"referenceID": 23, "context": "2015)) further refined using linguistic constraints (Paragram+CF, (Mrk\u0161i\u0107 et al., 2016)).", "startOffset": 66, "endOffset": 87}, {"referenceID": 7, "context": "Previous works often optimise models on the entire dataset, which leads to overfitting (Faruqui et al., 2016) or use custom splits, e.", "startOffset": 87, "endOffset": 109}, {"referenceID": 31, "context": ", 10-fold cross-validation (Schwartz et al., 2015), which make results incomparable with others.", "startOffset": 27, "endOffset": 50}, {"referenceID": 5, "context": "The non-distributional non data-driven model of Faruqui and Dyer (2015) is only slightly affected by frequency.", "startOffset": 48, "endOffset": 72}, {"referenceID": 22, "context": "To examine this hypothesis, we resort to WordNet (Miller, 1995), where different semantic usages of words are listed as so-called synsets.", "startOffset": 49, "endOffset": 63}, {"referenceID": 16, "context": "More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015).", "startOffset": 185, "endOffset": 204}, {"referenceID": 29, "context": ", 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015).", "startOffset": 76, "endOffset": 132}, {"referenceID": 34, "context": ", 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015).", "startOffset": 76, "endOffset": 132}], "year": 2017, "abstractText": "Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.", "creator": "LaTeX with hyperref package"}}}