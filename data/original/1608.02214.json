{"id": "1608.02214", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Aug-2016", "title": "Robsut Wrod Reocginiton via Semi-Character Recurrent Neural Network", "abstract": "The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated a robust word processing mechanism in humans, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recursive neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model.", "histories": [["v1", "Sun, 7 Aug 2016 13:28:46 GMT  (58kb,D)", "http://arxiv.org/abs/1608.02214v1", null], ["v2", "Tue, 7 Feb 2017 07:56:39 GMT  (103kb,D)", "http://arxiv.org/abs/1608.02214v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["keisuke sakaguchi", "kevin duh", "matt post", "benjamin van durme"], "accepted": true, "id": "1608.02214"}, "pdf": {"name": "1608.02214.pdf", "metadata": {"source": "CRF", "title": "Robsut Wrod Reocginiton via semi-Character Recurrent Neural Network", "authors": ["Keisuke Sakaguchi", "Kevin Duh", "Matt Post", "Benjamin Van Durme"], "emails": ["keisuke@cs.jhu.edu", "kevinduh@cs.jhu.edu", "post@cs.jhu.edu", "vandurme@cs.jhu.edu"], "sections": [{"heading": null, "text": "Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recursive neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model."}, {"heading": "1 Introduction", "text": "Despite the rapid improvement in natural language processing by computers, humans still have advantages in situations where the text contains noise. For example, the following sentences, introduced by a psycholinguist (Davis, 2003), provide a great demonstration of the robust word recognition mechanism in humans.\nAoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn\u2019t mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae. The rset can be a toatl mses and you can sitll raed it wouthit porbelm. Tihs is bcuseae the huamn mnid deos not raed ervey lteter by istlef, but the wrod as a wlohe.\nThis example shows the Cmabrigde Uinervtisy (Cambridge University) effect, which demon-\nAccording to a research\nstrates that human reading is resilient to (particularly internal) letter transposition.\nRobustness is important and useful property for various NLP tasks, and we propose a computational model which replicates this robust word recognition mechanism. The model is based on a standard recurrent neural network with a memory cell as in LSTM (Hochreiter and Schmidhuber, 1997). The input layer of the model consists of three sub-vectors: beginning (b), internal (i), and ending (e) character(s) of the input word (Figure 1). This semi-character level recurrent neural network is referred as scRNN.\nFirst, we review previous work on the robust word recognition mechanism from psycholinguistics literature (\u00a72). Next, we describe technical details of scRNN which capture the robust human mechanism (\u00a73) using recent developments in neural networks. Our experiments show that the scRNN outperforms commonly used spelling checkers by 42% for jumbled word correction (\u00a74.1) and 5% and 27% in other noise types (insertion and deletion). We also show that scRNN replicates recent findings from psycholinguistics experiments on reading difficulty (i.e. accuracy) depending on the position of jumbled letters, which indicates that scRNN successfully mimics ar X\niv :1\n60 8.\n02 21\n4v 1\n[ cs\n.C L\n] 7\nA ug\n2 01\n6\n(at least a part of) the human word recognition mechanism (\u00a74.2)."}, {"heading": "2 Raeding Wrods with Jumbled Lettres", "text": "Sentence processing with jumbled words has been a major research topic in psycholinguistics literature. Forster et al. (1987) show that a jumbled word (e.g. anwser-ANSWER) facilitates primes as large as identity primes (answer-ANSWER) in a masked priming paradigm and these results have been confirmed (Perea and Lupker, 2004; Guerrera and Forster, 2008).\nThese findings about robust word processing mechanism by human have been further investigated by looking at other types of noise in addition to simple letter transpositions. Humphreys et al. (1990) show that deleting a letter in a word still produces significant priming effect (e.g. blckBLACK), and similar results have been shown in other research (Peressotti and Grainger, 1999; Grainger et al., 2006). Van Assche and Grainger (2006) demonstrate that a priming effect remains when inserting a character into a word (e.g. juastice-JUSTICE).\nWith an eye-movement paradigm, Rayner et al. (2006) and Johnson et al. (2007) conduct more detailed experiments on the robust word recognition mechanism with jumbled letters. They show that letter transposition affects fixation time measures during reading depending on which part of the word is jumbled. Table 1 presents the result from Rayner et al. (2006). It is obvious that human can read smoothly (i.e. smaller number of fixations, regression, and average of fixation duration) when a given sentence has no noise (referred this condition as N). When the characters at the beginning of word are jumbled (referred this condition as BEG), human have more difficulty. The other two conditions, where words are internally jumbled (INT) or letters at word endings are jumbled\n(END), have similar amount of effect, although the number of fixations between them showed a statistically significant difference (p < 0.01). In short, the reading difficulty with different jumble conditions is summarized as follows: N < INT \u2264 END < BEG.\nIt may be surprising that there is statistically significant difference between END and BEG conditions despite the difference being very subtle (i.e. fixing either the first or the last character). This result demonstrates the importance of beginning letters for human word recognition.1"}, {"heading": "3 Semi-Character Recurrent Neural Net", "text": "In order to achieve the human-like robust word processing mechanism, we propose a semi-character based recurrent neural network (scRNN). The scRNN has the same structure as a standard recurrent neural network except that the input vector consists of three sub-vectors that correspond to the characters\u2019 position. The first and third sub-vectors (bn, en) represent the first and last character of the n-th word. These two subvectors are therefore one-hot representations. The second sub-vector (in) represents a bag of characters of the word not including the initial and final positions. For example, the word \u201cUniversity\u201d is represented as bn = {U = 1}, en = {y = 1}, and in = {e = 1, i = 2, n = 1, s = 1, r = 1, t = 1, v = 1}, with all the other elements being zero. The size of sub-vectors (bn, in, en) is equal to the number of characters (N ) in our language, and xn has therefore the size of 3N by concatenating the sub-vectors.\nWith this input vector (xn), the recurrent neural\n1There is still a debate in psycholinguistics community whether or not the order of internal letters does not matter at all. In this paper, we keep the original hypothesis in order for our model to be simple.\nnet model is described as follows.\nxn = bnin en  (1) hn = LSTM(Wxh \u00b7 xn +Whh \u00b7 hn\u22121) (2) yn = softmax(Why \u00b7 hn) (3)\nThe input vector is used as input to a hidden layer (hn) which is an LSTM. The LSTM layer has a recurrent input from its previous state (hn\u22121). The output of the hidden layer is taken as input to the softmax function layer, which predicts an output word (yn). We use the cross-entropy training criterion applied to the output layer as in most LSTM language modeling works; the model learns the weight matrices (Whh, Why) to maximize the likelihood of the training data. This should approximately correlate with maximizing the number of exact word match in the predicted outputs. Figure 1 shows a pictorial overview of scRNN.\nIn order to check if the scRNN can recognize the jumbled words correctly, we test it in spelling correction experiments. If the hypothesis about the robust word processing mechanism is correct, scRNN will also be able to read sentences with jumbled words robustly."}, {"heading": "4 Experiments", "text": "We conducted spelling correction experiments to judge how well scRNN can recognize noisy word sentences under different conditions. We used\nPenn Treebank for training, tuning, and testing.2 The input layer consists of a vector with length of 76 (A-Z, a-z and 24 symbol characters). The hidden layer units had size 650, and total vocabulary size was set to 10k. We apply one type of noise to every word except that all words with numbers (e.g. 1980s) and short words (length \u2264 3) are not subjected to jumbling and were left as is, and therefore these words are excluded in evaluation. We trained the model by running 5 epochs with batch size 20. In order to make the training efficient, we set the backpropagation through time (BPTT) parameter to 3."}, {"heading": "4.1 Spelling correction results", "text": "We tested different noise types: jumble, delete, and insert, where the jumble changes the internal characters (e.g. Cambridge \u2192 Cmbarigde), delete randomly deletes one of the internal characters (Cambridge \u2192 Camridge), and insert randomly inserts an alphabet into an internal position (Cambridge \u2192 Cambpridge). None of the noise types change the first and last characters. For comparison, we ran two widely-used spelling checkers (Commercial3 and Hunspell4).\nother spelling checkers. The only error in scRNN may be because the last character (rscheearch) activated the scRNN nodes strongly toward research instead of researcher.6\nTable 3 shows the result with respect to noise type. Overall, scRNN outperforms the other two spelling checkers across all three different noise types. It is striking that scRNN shows robustness in jumble noise, whereas the other models are significantly affected. All three models suffer under the delete condition, but scRNN still maintains 85% of accuracy whereas the other models decreased to 58% and 35% accuracy.\nThe relatively large drop in delete in scRNN may be because the information lost by deleting character is significant. For example, when the word place has dropped the character l, the surface form becomes pace, which is also a valid word. Also, the word mess with e being deleted produces the form of mss, which can be recovered as mess, mass, miss, etc. In the Cmabrigde Uinervtisy sentences, in both cases, the local context support other phrase such as \u2018at the right pace/place\u2019 and \u2018a total mass/mess\u2019. These examples clearly demonstrate that deleting characters harm the word recognition more significantly than other noise types.\nFinally, all the three models perform well on insert noise, indicating that adding extraneous information by inserting a letter does not change the original information significantly."}, {"heading": "4.2 Corroboration with psycholinguistic experiments", "text": "As seen in \u00a72, the position of transposition affects the cognitive load of human word recognition. We investigate this phenomenon with scRNN by manipulating the structure of input vector. We replicate the experimental paradigm in Rayner et al. (2006), but using scRNNs rather than human sub-\n6There is also an deletion of \u2019r\u2019.\njects. We trained the scRNN on different jumble conditions: INT, END, and BEG. INT is the same model as \u00a73, END represents an input word as a concatenation of the initial character vector (b) and a vector for the rest of characters (i.e. the internal and last characters are subject to jumbling), and BEG combines a vector for the final character (e) and a vector for the rest of characters (i.e. the initial and internal characters are subject to jumbling). We also add another jumble type ALL, where all the letters are subject to jumble (e.g. research vs. eesrhrca) and represented as a single vector (i.e. bag of characters).\nTable 4.1 shows the result. While scRNN achieves the high accuracy for all the jumble types, the statistical test revealed that the difficulty of spelling correction is summarized as INT < END < BEG < ALL, which has the same order as \u00a72. It is especially interesting to see the same pattern between END and BEG. This indicates the scRNN replicates (at least a part of) the human word recognition mechanism."}, {"heading": "5 Related Work", "text": "Character-based recurrent neural networks have been investigated and used for a variety of NLP tasks such as language modeling (Sutskever et al., 2011), segmentation (Chrupala, 2013), dependency parsing (Ballesteros et al., 2015), machine translation (Ling et al., 2015), and text normalization (Chrupa\u0142a, 2014). Although scRNN has some similarity in terms of model architecture with these recent works, our contribution is the demonstration of the robustness and cognitive plausibility of semi-character-based recurrent neural networks for word recognition.\nCharacter-level convolutional neural nets (CNN) have also been noted (Kim et al., 2015) and used for spelling correction (Schmaltz et al.,\n2016). While CNNs have a richer representation, scRNN still achieves high accuracy in jumbled word recognition with a simpler RNN structure (i.e. no convolution layers are required) resulting in fast training time and small model size."}, {"heading": "6 Summary", "text": "We have presented a semi-character recurrent neural network model, scRNN, which is inspired by the robust word recognition mechanism known in psycholinguistics literature as the Cmabrigde Uinervtisy effect. Despite the model\u2019s simplicity, it significantly outperforms existing spelling checkers with respect to various noise types. We also have demonstrated a similarity between scRNN and human word recognition mechanisms, by showing that scRNN replicates a psycholinguistics experiment about word recognition difficulty in terms of the position of jumbled characters.\nThere are a variety of potential NLP applications for scRNN where robustness plays an important role, such as normalizing social media text (e.g. Cooooolll \u2192 Cool) and modeling morphologically rich languages."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349\u2013", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Text segmentation with character-level text embeddings", "author": ["Grzegorz Chrupala."], "venue": "arXiv preprint arXiv:1309.4628.", "citeRegEx": "Chrupala.,? 2013", "shortCiteRegEx": "Chrupala.", "year": 2013}, {"title": "Normalizing tweets with edit scripts and recurrent neural embeddings", "author": ["Grzegorz Chrupa\u0142a."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 680\u2013686, Baltimore, Mary-", "citeRegEx": "Chrupa\u0142a.,? 2014", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2014}, {"title": "Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy", "author": ["Matt Davis."], "venue": "http://www.mrccbu.cam.ac.uk/people/matt.davis/cmabridge/.", "citeRegEx": "Davis.,? 2003", "shortCiteRegEx": "Davis.", "year": 2003}, {"title": "Masked priming with graphemically related forms: Repetition or partial activation", "author": ["Kenneth I Forster", "C Davis", "C Schoknecht", "R Carter"], "venue": null, "citeRegEx": "Forster et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Forster et al\\.", "year": 1987}, {"title": "Letter position information and printed word perception: the relative-position priming constraint", "author": ["Jonathan Grainger", "Jean-Pierre Granier", "Fernand Farioli", "Eva Van Assche", "Walter JB van Heuven."], "venue": "Journal of Experimental Psychology: Human Per-", "citeRegEx": "Grainger et al\\.,? 2006", "shortCiteRegEx": "Grainger et al\\.", "year": 2006}, {"title": "Masked form priming with extreme transposition", "author": ["Christine Guerrera", "Kenneth Forster."], "venue": "Language and Cognitive Processes, 23(1):117\u2013142.", "citeRegEx": "Guerrera and Forster.,? 2008", "shortCiteRegEx": "Guerrera and Forster.", "year": 2008}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Orthographic processing in visual word identification", "author": ["Glyn W Humphreys", "Lindsay J Evett", "Philip T Quinlan."], "venue": "Cognitive Psychology, 22(4):517 \u2013 560.", "citeRegEx": "Humphreys et al\\.,? 1990", "shortCiteRegEx": "Humphreys et al\\.", "year": 1990}, {"title": "Transposed-letter effects in reading: Evidence from eye movements and parafoveal preview", "author": ["Rebecca L Johnson", "Manuel Perea", "Keith Rayner."], "venue": "Journal of Experimental Psychology: Human Perception and Performance, 33(1):209.", "citeRegEx": "Johnson et al\\.,? 2007", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "arXiv preprint arXiv:1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Can CANISO activate casino? transposed-letter similarity effects with nonadjacent letter positions", "author": ["Manuel Perea", "Stephen J Lupker."], "venue": "Journal of Memory and Language, 51(2):231 \u2013 246.", "citeRegEx": "Perea and Lupker.,? 2004", "shortCiteRegEx": "Perea and Lupker.", "year": 2004}, {"title": "The role of letter identity and letter position in orthographic priming", "author": ["Francesca Peressotti", "Jonathan Grainger."], "venue": "Perception & Psychophysics, 61(4):691\u2013706.", "citeRegEx": "Peressotti and Grainger.,? 1999", "shortCiteRegEx": "Peressotti and Grainger.", "year": 1999}, {"title": "Raeding wrods with jubmled lettres: There is a cost", "author": ["Keith Rayner", "Sarah J. White", "Rebecca L. Johnson", "Simon P. Liversedge."], "venue": "Psychological Science, 17(3):192\u2013193.", "citeRegEx": "Rayner et al\\.,? 2006", "shortCiteRegEx": "Rayner et al\\.", "year": 2006}, {"title": "Sentence-level grammatical error identification as sequence-to-sequence correction", "author": ["Allen Schmaltz", "Yoon Kim", "Alexander M. Rush", "Stuart Shieber."], "venue": "Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Appli-", "citeRegEx": "Schmaltz et al\\.,? 2016", "shortCiteRegEx": "Schmaltz et al\\.", "year": 2016}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "A study of relative-position priming with superset primes", "author": ["Eva Van Assche", "Jonathan Grainger."], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition, 32(2):399.", "citeRegEx": "Assche and Grainger.,? 2006", "shortCiteRegEx": "Assche and Grainger.", "year": 2006}], "referenceMentions": [{"referenceID": 3, "context": "For example, the following sentences, introduced by a psycholinguist (Davis, 2003), provide a great demonstration of the robust word recognition mechanism in humans.", "startOffset": 69, "endOffset": 82}, {"referenceID": 7, "context": "The model is based on a standard recurrent neural network with a memory cell as in LSTM (Hochreiter and Schmidhuber, 1997).", "startOffset": 88, "endOffset": 122}, {"referenceID": 14, "context": "Table 1: Example sentences and results for measures of fixation (excerpt from (Rayner et al., 2006)).", "startOffset": 78, "endOffset": 99}, {"referenceID": 12, "context": "anwser-ANSWER) facilitates primes as large as identity primes (answer-ANSWER) in a masked priming paradigm and these results have been confirmed (Perea and Lupker, 2004; Guerrera and Forster, 2008).", "startOffset": 145, "endOffset": 197}, {"referenceID": 6, "context": "anwser-ANSWER) facilitates primes as large as identity primes (answer-ANSWER) in a masked priming paradigm and these results have been confirmed (Perea and Lupker, 2004; Guerrera and Forster, 2008).", "startOffset": 145, "endOffset": 197}, {"referenceID": 4, "context": "Forster et al. (1987) show that a jumbled word (e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "blckBLACK), and similar results have been shown in other research (Peressotti and Grainger, 1999; Grainger et al., 2006).", "startOffset": 66, "endOffset": 120}, {"referenceID": 5, "context": "blckBLACK), and similar results have been shown in other research (Peressotti and Grainger, 1999; Grainger et al., 2006).", "startOffset": 66, "endOffset": 120}, {"referenceID": 7, "context": "Humphreys et al. (1990) show that deleting a letter in a word still produces significant priming effect (e.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "blckBLACK), and similar results have been shown in other research (Peressotti and Grainger, 1999; Grainger et al., 2006). Van Assche and Grainger (2006) demonstrate that a priming effect remains", "startOffset": 98, "endOffset": 153}, {"referenceID": 13, "context": "With an eye-movement paradigm, Rayner et al. (2006) and Johnson et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 9, "context": "(2006) and Johnson et al. (2007) conduct more detailed experiments on the robust word recognition mechanism with jumbled letters.", "startOffset": 11, "endOffset": 33}, {"referenceID": 9, "context": "(2006) and Johnson et al. (2007) conduct more detailed experiments on the robust word recognition mechanism with jumbled letters. They show that letter transposition affects fixation time measures during reading depending on which part of the word is jumbled. Table 1 presents the result from Rayner et al. (2006). It is obvious that human can read smoothly (i.", "startOffset": 11, "endOffset": 314}, {"referenceID": 14, "context": "We replicate the experimental paradigm in Rayner et al. (2006), but using scRNNs rather than human sub-", "startOffset": 42, "endOffset": 63}, {"referenceID": 16, "context": "Character-based recurrent neural networks have been investigated and used for a variety of NLP tasks such as language modeling (Sutskever et al., 2011), segmentation (Chrupala, 2013), dependency parsing (Ballesteros et al.", "startOffset": 127, "endOffset": 151}, {"referenceID": 1, "context": ", 2011), segmentation (Chrupala, 2013), dependency parsing (Ballesteros et al.", "startOffset": 22, "endOffset": 38}, {"referenceID": 0, "context": ", 2011), segmentation (Chrupala, 2013), dependency parsing (Ballesteros et al., 2015), machine translation (Ling et al.", "startOffset": 59, "endOffset": 85}, {"referenceID": 11, "context": ", 2015), machine translation (Ling et al., 2015), and text normalization (Chrupa\u0142a, 2014).", "startOffset": 29, "endOffset": 48}, {"referenceID": 2, "context": ", 2015), and text normalization (Chrupa\u0142a, 2014).", "startOffset": 32, "endOffset": 48}, {"referenceID": 10, "context": "Character-level convolutional neural nets (CNN) have also been noted (Kim et al., 2015) and used for spelling correction (Schmaltz et al.", "startOffset": 69, "endOffset": 87}], "year": 2017, "abstractText": "The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated a robust word processing mechanism in humans, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recursive neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model.", "creator": "TeX"}}}