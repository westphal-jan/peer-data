{"id": "1608.04493", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Dynamic Network Surgery for Efficient DNNs", "abstract": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code will be made publicly available.", "histories": [["v1", "Tue, 16 Aug 2016 06:23:05 GMT  (1327kb,D)", "http://arxiv.org/abs/1608.04493v1", "Accepted by NIPS 2016"], ["v2", "Thu, 10 Nov 2016 00:17:25 GMT  (1229kb,D)", "http://arxiv.org/abs/1608.04493v2", "Accepted by NIPS 2016"]], "COMMENTS": "Accepted by NIPS 2016", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["yiwen guo", "anbang yao", "yurong chen"], "accepted": true, "id": "1608.04493"}, "pdf": {"name": "1608.04493.pdf", "metadata": {"source": "CRF", "title": "Dynamic Network Surgery for Efficient DNNs", "authors": ["Yiwen Guo", "Anbang Yao", "Yurong Chen"], "emails": ["yiwen.guo@intel.com", "anbang.yao@intel.com", "yurong.chen@intel.com"], "sections": [{"heading": "1 Introduction", "text": "As a family of brain inspired models, deep neural networks (DNNs) have substantially advanced a variety of artificial intelligence tasks including image classification [13, 19, 11], natural language processing, speech recognition and face recognition.\nDespite these tremendous successes, recently designed networks tend to have more stacked layers, and thus more learnable parameters. For instance, AlexNet [13] designed by Krizhevsky et al. has 61 million parameters to win the ILSVRC 2012 classification competition, which is over 100 times more than that of LeCun\u2019s conventional model [15] (e.g., LeNet-5), let alone the much more complex models like VGGNet [19]. Since more parameters means more storage requirement and more floating-point operations (FLOPs), it increases the difficulty of applying DNNs on mobile platforms with limited memory and processing units. Moreover, the battery capacity can be another bottleneck [9].\nAlthough DNN models normally require a vast number of parameters to guarantee their superior performance, significant redundancies have been reported in their parameterizations [4]. Therefore, with a proper strategy, it is possible to compress these models without significantly losing their prediction accuracies. Among existing methods, network pruning appears to be an outstanding one due to its surprising ability of accuracy loss prevention. For instance, Han et al. [9] recently propose to make \"lossless\" DNN compression by deleting unimportant parameters and retraining the remaining ones (as illustrated in Figure 1(b)), somehow similar to a surgery process.\nHowever, due to the complex interconnections among hidden neurons, parameter importance may change dramatically once the network surgery begins. This leads to two main issues in [9] (and some other classical methods [16, 10] as well). The first issue is the possibility of irretrievable network damage. Since the pruned connections have no chance to come back, incorrect pruning may cause\n\u2217This work was done when Yiwen Guo was an intern at Intel Labs China supervised by Anbang Yao who is responsible for correspondence.\nar X\niv :1\n60 8.\n04 49\n3v 1\n[ cs\n.N E\n] 1\n6 A\nug 2\n01 6\nsevere accuracy loss. In consequence, the compression rate must be over suppressed to avoid such loss. Another issue is learning inefficiency. As in the paper [9], several iterations of alternate pruning and retraining are necessary to get a fair compression rate on AlexNet, while each retraining process consists of millions of iterations, which can be very time consuming.\nIn this paper, we attempt to address these issues and pursue the compression limit of the pruning method. To be more specific, we propose to sever redundant connections by means of continual network maintenance, which we call dynamic network surgery. The proposed method involves two key operations: pruning and splicing, conducted with two different purposes. Apparently, the pruning operation is made to compress network models, but over pruning or incorrect pruning should be responsible for the accuracy loss. In order to compensate the unexpected loss, we properly incorporate the splicing operation into network surgery, and thus enabling connection recovery once the pruned connections are found to be important any time. These two operations are integrated together by updating parameter importance whenever necessary, making our method dynamic.\nIn fact, the above strategies help to make the whole process flexible. They are beneficial not only to better approach the compression limit, but also to improve the learning efficiency, which will be validated in Section 4. In our method, pruning and splicing naturally constitute a circular procedure and dynamically divide the network connections into two categories, akin to the synthesis of excitatory and inhibitory neurotransmitter in human nervous systems [17].\nThe rest of this paper is structured as follows. In Section 2, we introduce the related methods of DNN compression by briefly discussing their merits and demerits. In Section 3, we highlight our intuition of dynamic network surgery and introduce its implementation details. Section 4 experimentally analyses our method and Section 5 draws the conclusions."}, {"heading": "2 Related Works", "text": "In order to make DNN models portable, a variety of methods have been proposed. Vanhoucke et al. [20] analyse the effectiveness of data layout, batching and the usage of Intel fixed-point instructions, making a 3\u00d7 speedup on x86 CPUs. Mathieu et al. [18] explore the fast Fourier transforms (FFTs) on GPUs and improve the speed of CNNs by performing convolution calculations in the frequency domain.\nAn alternative category of methods resorts to matrix (or tensor) decomposition. Denil et al. [4] propose to approximate parameter matrices with appropriately constructed low-rank decompositions. Their method achieves 1.6\u00d7 speedup on the convolutional layer with 1% drop in prediction accuracy. Following similar ideas, some subsequent methods can provide more significant speedups [5, 22, 14]. Although matrix (or tensor) decomposition can be beneficial to DNN compression and speedup, these methods normally incur severe accuracy loss under high compression requirement.\nVector quantization is another way to compress DNNs. Gong et al. [6] explore several such methods and point out the effectiveness of product quantization. HashNet proposed by Chen et al. [1] handles network compression by grouping its parameters into hash buckets. It is trained with a standard backpropagation procedure and should be able to make substantial storage savings. The recently proposed BinaryConnect [2] and Binarized Neural Networks [3] are able to compress DNNs by a factor of 32\u00d7, while a noticeable accuracy loss is sort of inevitable.\nThis paper follows the idea of network pruning. It starts from the early work of LeCun et al.\u2019s [16], which makes use of the second derivatives of loss function to balance training loss and model complexity. As an extension, Hassibi and Stork [10] propose to take non-diagonal elements of the Hessian matrix into consideration, producing compression results with less accuracy loss. In spite of their theoretical optimization, these two methods suffer from the high computational complexity when tackling large networks, regardless of the accuracy drop. Very recently, Han et al. [9] explore the magnitude-based pruning in conjunction with retraining, and report promising compression results without accuracy loss. It has also been validated that the sparse matrix-vector multiplication can further be accelerated by certain hardware design, making it more efficient than traditional CPU and GPU calculations [7]. The drawback of Han et al.\u2019s method [9] is mostly its potential risk of irretrievable network damage and learning inefficiency.\nOur research on network pruning is partly inspired by [9], not only because it can be very effective to compress DNNs, but also because it makes no assumption on the network structure. In particular, this branch of methods can be naturally combined with many other methods introduced above, to further reduce the network complexity. In fact, Han et al. [8] have already tested such combinations and obtained excellent results."}, {"heading": "3 Dynamic Network Surgery", "text": "In this section, we highlight the intuition of our method and present its implementation details. In order to simplify the explanations, we only talk about the convolutional layers and the fully connected layers. However, as claimed in [8], our pruning method can also be applied to some other layer types as long as their underlying mathematical operations are inner products on vector spaces."}, {"heading": "3.1 Notations", "text": "First of all, we clarify the notations in this paper. Suppose a DNN model can be represented as {Wk : 0 \u2264 k \u2264 C}, in which Wk denotes a matrix of connection weights in the kth layer. For the fully connected layers with p-dimensional input and q-dimensional output, the size of Wk is simply qk \u00d7 pk. For the convolutional layers with learnable kernels, we unfold the coefficients of each kernel into a vector and concatenate all of them to Wk as a matrix.\nIn order to represent a sparse model with part of its connections pruned away, we use {Wk,Tk : 0 \u2264 k \u2264 C}. Each Tk is a binary matrix with its entries indicating the states of network connections, i.e., whether they are currently pruned or not. Therefore, these additional matrices can be considered as the mask matrices."}, {"heading": "3.2 Pruning and Splicing", "text": "Since our goal is network pruning, the desired sparse model shall be learnt from its dense reference. Apparently, the key is to abandon unimportant parameters and keep the important ones. However, the parameter importance (i.e., the connection importance) in a certain network is extremely difficult to measure because of the mutual influences and mutual activations among interconnected neurons. That is, a network connection may be redundant due to the existence of some others, but it will soon\nbecome crucial once the others are removed. Therefore, it should be more appropriate to conduct a learning process and continually maintain the network structure.\nTaking the kth layer as an example, we propose to solve the following optimization problem:\nmin Wk,Tk\nL (Wk Tk) s.t. T(i,j)k = hk(W (i,j) k ), \u2200(i, j) \u2208 I, (1)\nin which L(\u00b7) is the network loss function, indicates the Hadamard product operator, set I consists of all the entry indices in matrix Wk, and hk(\u00b7) is a discriminative function, which satisfies hk(w) = 1 if parameter w seems to be crucial in the current layer, and 0 otherwise. Function hk(\u00b7) is designed on the base of some prior knowledge so that it can constrain the feasible region of Wk Tk and simplify the original NP-hard problem. For the sake of topic conciseness, we leave the discussions of function hk(\u00b7) in Section 3.3. Problem (1) can be solved by alternately updating Wk and Tk through the stochastic gradient descent (SGD) method, which will be introduced in the following paragraphs.\nSince binary matrix Tk can be determined with the constraints in (1), we only need to investigate the update scheme of Wk. Inspired by the method of Lagrange Multipliers and gradient descent, we give the following scheme for updating Wk. That is,\nW (i,j) k \u2190W (i,j) k \u2212 \u03b2\n\u2202\n\u2202(W (i,j) k T (i,j) k )\nL (Wk Tk) , \u2200(i, j) \u2208 I, (2)\nin which \u03b2 indicates a positive learning rate. It is worth mentioning that we update not only the important parameters, but also the ones corresponding to zero entries of Tk, which are considered unimportant and ineffective to decrease the network loss. This strategy is beneficial to improve the flexibility of our method because it enables the splicing of improperly pruned connections.\nThe partial derivatives in formula (2) can be calculated by the chain rule with a randomly chosen minibatch of samples. Once matrix Wk and Tk are updated, they shall be applied to re-calculate the whole network activations and loss function gradient. Repeat these steps iteratively, the sparse model will be able to produce excellent accuracy. The above procedure is summarized in Algorithm 1.\nAlgorithm 1 Dynamic network surgery: the SGD method for solving optimization problem (1):\nInput: X: training datum (with or without label), {W\u0302k : 0 \u2264 k \u2264 C}: the reference model, \u03b1: base learning rate, f : learning policy. Output: {Wk,Tk : 0 \u2264 k \u2264 C}: the updated parameter matrices and their binary masks. Initialize Wk \u2190 W\u0302k, Tk \u2190 1, \u22000 \u2264 k \u2264 C, \u03b2 \u2190 1 and iter\u2190 0 repeat\nChoose a minibatch of network input from X Forward propagation and loss calculation with (W0 T0),...,(WC TC) Backward propagation of the model output and generate\u2207L for k = 0, ..., C do\nUpdate Tk by function hk(\u00b7) and the current Wk, with a probability of \u03c3(iter) Update Wk by formula (2) and the current loss function gradient\u2207L\nend for Update: iter\u2190 iter + 1 and \u03b2 \u2190 f(\u03b1, iter)\nuntil iter reaches its desired maximum\nNote that, the dynamic property of our method is shown in two aspects. On one hand, pruning operations can be performed whenever the existing connections seem to become unimportant. Yet, on the other hand, the mistakenly pruned connections shall be re-established if they once appear to be important. The latter operation plays a dual role of network pruning, and thus it is called \"network splicing\" in this paper. Pruning and splicing constitute a circular procedure by constantly updating the connection weights and setting different entries in Tk, which is analogical to the synthesis of excitatory and inhibitory neurotransmitter in human nervous system [17]. See Figure 2 for the overview of our method and the method pipeline can be found in Figure 1(a)."}, {"heading": "3.3 Parameter Importance", "text": "Since the measure of parameter importance influences the state of network connections, function hk(\u00b7),\u22000 \u2264 k \u2264 C, can be essential to our dynamic network surgery. We have tested several\ncandidates and finally found the absolute value of the input to be the best choice, as claimed in [9]. That is, the parameters with relatively small magnitude are temporarily pruned, while the others with large magnitude are kept or spliced in each iteration of Algorithm 1. Obviously, the threshold values have a significant impact on the final compression rate. For a certain layer, a single threshold can be set based on the average absolute value and variance of its connection weights. However, to improve the robustness of our method, we use two thresholds ak and bk by importing a small margin t and set bk as ak + t in Equation (3). For the parameters out of this range, we set their function outputs as the corresponding entries in Tk, which means these parameters will neither be pruned nor spliced in the current iteration.\nhk(W (i,j) k ) =  0 if ak > |W(i,j)k | T (i,j) k if ak \u2264 |W (i,j) k | < bk\n1 if bk \u2264 |W(i,j)k | (3)"}, {"heading": "3.4 Convergence Acceleration", "text": "Considering that Algorithm 1 is a bit more complicated than the standard backpropagation method, we shall take a few more steps to boost its convergence. First of all, we suggest slowing down the pruning and splicing frequencies, because these operations lead to network structure change. This can be done by triggering the update scheme of Tk stochastically, with a probability of p = \u03c3(iter), rather than doing it constantly. Function \u03c3(\u00b7) shall be monotonically non-increasing and satisfy \u03c3(0) = 1. After a prolonged decrease, the probability p may even be set to zero, i.e., no pruning or splicing will be conducted any longer.\nAnother possible reason for slow convergence is the vanishing gradient problem. Since a large percentage of connections are pruned away, the network structure should become much simpler and probably even much \"thinner\" by utilizing our method. Thus, the loss function derivatives are likely to be very small, especially when the reference model is very deep. We resolve this problem by pruning the convolutional layers and fully connected layers separately, in the dynamic way still, which is somehow similar to [9]."}, {"heading": "4 Experimental Results", "text": "In this section, we will experimentally analyse the proposed method and apply it on some popular network models. For fair comparison and easy reproduction, all the reference models are trained by the GPU implementation of Caffe package [12] with .prototxt files provided by the community.2 Also, we follow the default experimental settings for SGD method, including the training batch size, base learning rate, learning policy and maximal number of training iterations. Once the reference models are obtained, we directly apply our method to reduce their model complexity. A brief summary of the compression results are shown in Table 1.\n2Except for the simulation experiment and LeNet-300-100 experiments which we create the .prototxt files by ourselves, because they are not available in the Caffe model zoo."}, {"heading": "4.1 The Exclusive-OR Problem", "text": "To begin with, we consider an experiment on the synthetic data to preliminary testify the effectiveness of our method and visualize its compression quality. The exclusive-OR (XOR) problem can be a good option. It is a nonlinear classification problem as illustrated in Figure 3(a). In this experiment, we turn the original problem to a more complicated one as Figure 3(b), in which some Gaussian noises are mixed up with the original data (0, 0), (0, 1), (1, 0) and (1, 1).\nIn order to classify these samples, we design a network model as illustrated in the left part of Figure 4(a), which consists of 21 connections and each of them has a weight to be learned. The sigmoid function is chosen as the activation function for all the hidden and output neurons. Twenty thousand samples were randomly generated for the experiment, in which half of them were used as training samples and the rest as test samples.\nBy 100,000 iterations of learning, this three-layer neural network achieves a prediction error rate of 0.31%. The weight matrix of network connections between input and hidden neurons can be found in Figure 4(b). Apparently, its first and last row share the similar elements, which means there are two hidden neurons functioning similarly. Hence, it is appropriate to use this model as a compression reference, even though it is not very large. After 150,000 iterations, the reference model will be compressed into the right side of Figure 4(a), and the new connection weights and their masks are shown in Figure 4(b). The grey and green patches in T1 stand for those entries equal to one, and the corresponding connections shall be kept. In particular, the green ones indicate the connections were mistakenly pruned in the beginning but spliced during the surgery. The other patches (i.e., the black ones) indicate the corresponding connections are permanently pruned in the end.\nThe compressed model has a prediction error rate of 0.30%, which is slightly better than that of the reference model, even though 40% of its parameters are set to be zero. Note that, the remaining hidden neurons (excluding the bias unit) act as three different logic gates and altogether make up\nthe XOR classifier. However, if the pruning operations are conducted only on the initial parameter magnitude (as in [9]), then probably four hidden neurons will be finally kept, which is obviously not the optimal compression result.\nIn addition, if we reduce the impact of Gaussian noises and enlarge the margin between positive and negative samples, then the current model can be further compressed, so that one more hidden neuron will be pruned by our method.\nSo far, we have carefully explained the mechanism behind our method and preliminarily testified its effectiveness. In the following subsections, we will further test our method on three popular NN models and make quantitative comparisons with other network compression methods."}, {"heading": "4.2 The MNIST database", "text": "MNIST is a database of handwritten digits and it is widely used to experimentally evaluate machine learning methods. Same with [9], we test our method on two network models: LeNet-5 and LeNet300-100.\nLeNet-5 is a conventional CNN model which consists of 4 learnable layers, including 2 convolutional layers and 2 fully connected layers. It is designed by LeCun et al. [15] for document recognition. With 431K parameters to be learned, we train this model for 10,000 iterations and obtain a prediction error rate of 0.91%. LeNet-300-100, as described in [15], is a classical feedforward neural network with three fully connected layers and 267K learnable parameters. It is also trained for 10,000 iterations, following the same learning policy as with LeNet-5. The well trained LeNet-300-100 model achieves an error rate of 2.28%.\nWith the proposed method, we are able to compress these two models. The same batch size, learning rate and learning policy are set as with the reference training processes, except for the maximal number of iterations, which is properly increased. The results are shown in Table 1. After convergence, the network parameters of LeNet-5 and LeNet-300-100 are reduced by a factor of 108\u00d7 and 56\u00d7, respectively, which means less than 1% and 2% of the network connections are kept, while the prediction accuracies are as good or slightly better.\nTo better demonstrate the advantage of our method, we make layer-by-layer comparisons between our compression results and Han et al.\u2019s [9] in Table 2. To the best of our knowledge, their method is so far the most effective pruning method, if the learning inefficiency is not a concern. However, our method still achieves at least 4 times the compression improvement against their method. Besides, due to the significant advantage over Han et al.\u2019s models [9], our compressed models will also be undoubtedly much faster than theirs."}, {"heading": "4.3 ImageNet and AlexNet", "text": "In the final experiment, we apply our method to AlexNet [13], which wins the ILSVRC 2012 classification competition. As with the previous experiments, we train the reference model first.\nWithout any data augmentation, we obtain a reference model with 61M well-learned parameters after 450K iterations of training (i.e., roughly 90 epochs). Then we perform the network surgery on it. AlexNet consists of 8 learnable layers, which is considered to be deep. So we prune the convolutional layers and fully connected layers separately, as previously discussed in Section 3.4. The training batch size, base learning rate and learning policy still keep the same with reference training process. We run 320K iterations for the convolutional layers and 380K iterations for the fully connected layers, which means 700K iterations in total (i.e., roughly 140 epochs). In the test phase, we use just the center crop and test our compressed model on the validation set.\nTable 3 compares the result of our method with some others. The four compared models are built by applying Han et al.\u2019s method [9] and the adaptive fastfood transform method [21]. When compared with these \"lossless\" methods, our method achieves the best result in terms of the compression rate. Besides, after acceptable number of epochs, the prediction error rate of our model is comparable or even better than those models compressed from better references.\nIn order to make more detailed comparisons, we compare the percentage of remaining parameters in our compressed model with that of Han et al.\u2019s [9], since they achieve the second best compression rate. As shown in Table 4, our method compresses more parameters on almost every single layer in AlexNet, which means both the storage requirement and the number of FLOPs are better reduced when compared with [9]. Besides, our learning process is also much more efficient thus considerable less epochs are needed (as least 6.8 times decrease)."}, {"heading": "5 Conclusions", "text": "In this paper, we have investigated the way of compressing DNNs and proposed a novel method called dynamic network surgery. Unlike the previous methods which conduct pruning and retraining alternately, our method incorporates connection splicing into the surgery and implements the whole process in a dynamic way. By utilizing our method, most parameters in the DNN models can be deleted, while the prediction accuracy does not decrease. The experimental results show that our method compresses the number of parameters in LeNet-5 and AlexNet by a factor of 108\u00d7 and 17.7\u00d7, respectively, which is superior to the recent pruning method by considerable margins. Besides, the learning efficiency of our method is also better thus less epochs are needed."}], "references": [{"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "BinaryConnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830v3,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L. Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "EIE: Efficient inference engine on compressed deep neural network", "author": ["Song Han", "Xingyu Liu", "Huizi Mao", "Jing Pu", "Ardavan Pedram", "Mark A Horowitz", "William J. Dally"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding", "author": ["Song Han", "Huizi Mao", "William J. Dally"], "venue": "In ICLR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G. Stork"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In ACM MM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["Vadim Lebedev", "Yaroslav Ganin", "Maksim Rakhuba", "Ivan Oseledets", "Victor Lempitsky"], "venue": "In ICLR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S Denker", "Sara A Solla", "Richard E Howard", "Lawrence D Jackel"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1989}, {"title": "Molecular Cell Biology: Neurotransmitters, Synapses, and Impulse Transmission", "author": ["Harvey Lodish", "Arnold Berk", "S Lawrence Zipursky", "Paul Matsudaira", "David Baltimore", "James Darnell"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Fast training of convolutional networks through FFTs", "author": ["Michael Mathieu", "Mikael Henaff", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Improving the speed of neural networks on CPUs", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z. Mao"], "venue": "In NIPS Workshop,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "In ICCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Efficient and accurate approximations of nonlinear convolutional networks", "author": ["Xiangyu Zhang", "Jianhua Zou", "Xiang Ming", "Kaiming He", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "As a family of brain inspired models, deep neural networks (DNNs) have substantially advanced a variety of artificial intelligence tasks including image classification [13, 19, 11], natural language processing, speech recognition and face recognition.", "startOffset": 168, "endOffset": 180}, {"referenceID": 18, "context": "As a family of brain inspired models, deep neural networks (DNNs) have substantially advanced a variety of artificial intelligence tasks including image classification [13, 19, 11], natural language processing, speech recognition and face recognition.", "startOffset": 168, "endOffset": 180}, {"referenceID": 10, "context": "As a family of brain inspired models, deep neural networks (DNNs) have substantially advanced a variety of artificial intelligence tasks including image classification [13, 19, 11], natural language processing, speech recognition and face recognition.", "startOffset": 168, "endOffset": 180}, {"referenceID": 12, "context": "For instance, AlexNet [13] designed by Krizhevsky et al.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "has 61 million parameters to win the ILSVRC 2012 classification competition, which is over 100 times more than that of LeCun\u2019s conventional model [15] (e.", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": ", LeNet-5), let alone the much more complex models like VGGNet [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "Moreover, the battery capacity can be another bottleneck [9].", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "Although DNN models normally require a vast number of parameters to guarantee their superior performance, significant redundancies have been reported in their parameterizations [4].", "startOffset": 177, "endOffset": 180}, {"referenceID": 8, "context": "[9] recently propose to make \"lossless\" DNN compression by deleting unimportant parameters and retraining the remaining ones (as illustrated in Figure 1(b)), somehow similar to a surgery process.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "This leads to two main issues in [9] (and some other classical methods [16, 10] as well).", "startOffset": 33, "endOffset": 36}, {"referenceID": 15, "context": "This leads to two main issues in [9] (and some other classical methods [16, 10] as well).", "startOffset": 71, "endOffset": 79}, {"referenceID": 9, "context": "This leads to two main issues in [9] (and some other classical methods [16, 10] as well).", "startOffset": 71, "endOffset": 79}, {"referenceID": 8, "context": "As in the paper [9], several iterations of alternate pruning and retraining are necessary to get a fair compression rate on AlexNet, while each retraining process consists of millions of iterations, which can be very time consuming.", "startOffset": 16, "endOffset": 19}, {"referenceID": 16, "context": "In our method, pruning and splicing naturally constitute a circular procedure and dynamically divide the network connections into two categories, akin to the synthesis of excitatory and inhibitory neurotransmitter in human nervous systems [17].", "startOffset": 239, "endOffset": 243}, {"referenceID": 8, "context": "\u2019s method [9], using AlexNet as an example.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "[9] needs more than 4800K iterations to get a fair compression rate (9\u00d7), while our method runs only 700K iterations to yield a significantly better result (17.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] analyse the effectiveness of data layout, batching and the usage of Intel fixed-point instructions, making a 3\u00d7 speedup on x86 CPUs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] explore the fast Fourier transforms (FFTs) on GPUs and improve the speed of CNNs by performing convolution calculations in the frequency domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] propose to approximate parameter matrices with appropriately constructed low-rank decompositions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Following similar ideas, some subsequent methods can provide more significant speedups [5, 22, 14].", "startOffset": 87, "endOffset": 98}, {"referenceID": 21, "context": "Following similar ideas, some subsequent methods can provide more significant speedups [5, 22, 14].", "startOffset": 87, "endOffset": 98}, {"referenceID": 13, "context": "Following similar ideas, some subsequent methods can provide more significant speedups [5, 22, 14].", "startOffset": 87, "endOffset": 98}, {"referenceID": 5, "context": "[6] explore several such methods and point out the effectiveness of product quantization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] handles network compression by grouping its parameters into hash buckets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "The recently proposed BinaryConnect [2] and Binarized Neural Networks [3] are able to compress DNNs by a factor of 32\u00d7, while a noticeable accuracy loss is sort of inevitable.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "The recently proposed BinaryConnect [2] and Binarized Neural Networks [3] are able to compress DNNs by a factor of 32\u00d7, while a noticeable accuracy loss is sort of inevitable.", "startOffset": 70, "endOffset": 73}, {"referenceID": 15, "context": "\u2019s [16], which makes use of the second derivatives of loss function to balance training loss and model complexity.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "As an extension, Hassibi and Stork [10] propose to take non-diagonal elements of the Hessian matrix into consideration, producing compression results with less accuracy loss.", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "[9] explore the magnitude-based pruning in conjunction with retraining, and report promising compression results without accuracy loss.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "It has also been validated that the sparse matrix-vector multiplication can further be accelerated by certain hardware design, making it more efficient than traditional CPU and GPU calculations [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 8, "context": "\u2019s method [9] is mostly its potential risk of irretrievable network damage and learning inefficiency.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "Our research on network pruning is partly inspired by [9], not only because it can be very effective to compress DNNs, but also because it makes no assumption on the network structure.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "[8] have already tested such combinations and obtained excellent results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "However, as claimed in [8], our pruning method can also be applied to some other layer types as long as their underlying mathematical operations are inner products on vector spaces.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "Pruning and splicing constitute a circular procedure by constantly updating the connection weights and setting different entries in Tk, which is analogical to the synthesis of excitatory and inhibitory neurotransmitter in human nervous system [17].", "startOffset": 243, "endOffset": 247}, {"referenceID": 8, "context": "candidates and finally found the absolute value of the input to be the best choice, as claimed in [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "We resolve this problem by pruning the convolutional layers and fully connected layers separately, in the dynamic way still, which is somehow similar to [9].", "startOffset": 153, "endOffset": 156}, {"referenceID": 11, "context": "For fair comparison and easy reproduction, all the reference models are trained by the GPU implementation of Caffe package [12] with .", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "However, if the pruning operations are conducted only on the initial parameter magnitude (as in [9]), then probably four hidden neurons will be finally kept, which is obviously not the optimal compression result.", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "Same with [9], we test our method on two network models: LeNet-5 and LeNet300-100.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "[15] for document recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "LeNet-300-100, as described in [15], is a classical feedforward neural network with three fully connected layers and 267K learnable parameters.", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "Table 2: Compare our compression results on LeNet-5 and LeNet-300-100 with that of [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 8, "context": "The percentage of remaining parameters after applying Han et al\u2019s method [9] and our method are shown in the last two columns.", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "% [9] Params.", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "\u2019s [9] in Table 2.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "\u2019s models [9], our compressed models will also be undoubtedly much faster than theirs.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "In the final experiment, we apply our method to AlexNet [13], which wins the ILSVRC 2012 classification competition.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "Model Top-1 error Top-5 error Epochs Compression Fastfood 32 (AD) [21] 41.", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "93% 2\u00d7 Fastfood 16 (AD) [21] 42.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "7\u00d7 Naive Cut [9] 57.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "[9] 42.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "\u2019s method [9] and the adaptive fastfood transform method [21].", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "\u2019s method [9] and the adaptive fastfood transform method [21].", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "\u2019s [9], since they achieve the second best compression rate.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "As shown in Table 4, our method compresses more parameters on almost every single layer in AlexNet, which means both the storage requirement and the number of FLOPs are better reduced when compared with [9].", "startOffset": 203, "endOffset": 206}, {"referenceID": 8, "context": "Table 4: Compare our method with [9] on AlexNet.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "% [9] Params.", "startOffset": 2, "endOffset": 5}], "year": 2016, "abstractText": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of 108\u00d7 and 17.7\u00d7 respectively, proving that it outperforms the recent pruning method by considerable margins. The code will be made publicly available.", "creator": "LaTeX with hyperref package"}}}