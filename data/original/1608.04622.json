{"id": "1608.04622", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Training Echo State Networks with Regularization through Dimensionality Reduction", "abstract": "In this paper we introduce a new framework to train an Echo State Network to predict real valued time-series. The method consists in projecting the output of the internal layer of the network on a space with lower dimensionality, before training the output layer to learn the target task. Notably, we enforce a regularization constraint that leads to better generalization capabilities. We evaluate the performances of our approach on several benchmark tests, using different techniques to train the readout of the network, achieving superior predictive performance when using the proposed framework. Finally, we provide an insight on the effectiveness of the implemented mechanics through a visualization of the trajectory in the phase space and relying on the methodologies of nonlinear time-series analysis. By applying our method on well known chaotic systems, we provide evidence that the lower dimensional embedding retains the dynamical properties of the underlying system better than the full-dimensional internal states of the network.", "histories": [["v1", "Tue, 16 Aug 2016 14:41:12 GMT  (1538kb,D)", "http://arxiv.org/abs/1608.04622v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sigurd l{\\o}kse", "filippo maria bianchi", "robert jenssen"], "accepted": false, "id": "1608.04622"}, "pdf": {"name": "1608.04622.pdf", "metadata": {"source": "CRF", "title": "Training Echo State Networks with Regularization through Dimensionality Reduction", "authors": ["Sigurd L\u00f8kse", "Filippo Maria Bianchi", "Robert Jenssen"], "emails": ["sigurd.lokse@uit.no", "filippo.m.bianchi@uit.no", "robert.jenssen@uit.no"], "sections": [{"heading": null, "text": "Keywords\u2014 Echo state network, nonlinear time-series analysis, dimensionality reduction, timeseries prediction"}, {"heading": "1 Introduction", "text": "Echo State Networks (ESN) belong to the class of computational dynamical systems, implemented according to the so-called reservoir computing approach [39]. An input signal is fed to a large, recurrent and randomly connected dynamic hidden layer, the reservoir, whose outputs are combined by a memory-less layer called readout to solve a specified task. Contrary to most hard computing approaches, which demand long training procedures to learn model parameters through an optimization algorithm [27], ESN is characterized by a very fast learning procedure that usually consists in solving a convex optimization problem.\nESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24]. The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54]. Outstanding results have also been achieved by ESN in prediction of chaotic time-series [31, 36], which highlighted the capability of these neural networks to learn amazingly accurate models to forecast a chaotic process from almost noise-free training data.\nAlthough a large reservoir could capture the dynamics of the underlying system more accurately, it results in a model of increased complexity, with an inherent risk of overfitting that leads to lower\n\u2217sigurd.lokse@uit.no \u2020Corresponding author \u2021filippo.m.bianchi@uit.no \u00a7robert.jenssen@uit.no\nar X\niv :1\n60 8.\n04 62\n2v 1\n[ cs\n.N E\n] 1\n6 A\nug 2\n01 6\ngeneralization capabilities. Additionally, several regression methods adopted to train the readout layer could be affected by the curse of dimensionality in case of high dimensional data, which could also cause increments in both the computational requirements in software and the resource needed in hardware [7]. Several tasks in signal processing and machine learning applications have been tackled by evaluating regression functions [58], performing classification [13] or finding neighbors [28] in a reduced dimensional space. In fact, in many cases it is possible to maintain meaningful distance relationships between original data and to deal with the curse of dimensionality at the same time. Through dimensionality reduction, redundant features are removed, noise can be filtered and algorithms that are unfit for a large number of dimensions become applicable. In the ESN literature, different methods have been proposed to increase the generalization ability of the network and to regularize the output. For example, in [16], the authors propose a form of regularization by shrinking the weights of the connections from the reservoir to the readout layer. In [47] by pruning some connections from the reservoir to the readout layer, better generalization capabilities are achieved along with some insight on which neurons are actually useful for the output, providing clues on how to create a good reservoir.\nIn this paper we propose a novel framework for training an ESN, where an additional computational block is introduced to process the output of the internal reservoir, before being fed into the readout layer. In particular, the internal state of the network is mapped to a properly chosen lower dimensional subspace, using both linear and non-linear transformations. Accordingly, we are able to use a large reservoir to capture the dynamics of the underlying system, while increasing the generalization capabilities of the model due to implicit regularization constraints provided by dimensionality reduction in the recurrent layer. Even if additional operations are introduced to compute the reduced dimensionality embedding, training the readout layer becomes less demanding, especially in regression methods whose computational complexity depends on input dimension [17]. With the proposed procedure we improve the generalization capabilities of the network, achieving better results on well-known benchmarking problems with respect to the standard ESN architecture. Additionally, in cases where data can be mapped to spaces with 2 or 3 dimensions, internal network dynamics can be visualized precisely and relevant patterns can be detected. To justify the results obtained and to understand the mechanisms which determines the effectiveness of the proposed system, we provide a theoretical study based on methods coming from the field of nonlinear time-series analysis. To the best of the authors\u2019 knowledge, the coupling of dimensionality reduction with the ESN architecture has not been explored before.\nThe remainder of the paper is organized as follows. In Sect. 2 we describe the ESN structure along with existing approaches for its training and we review the dimensionality reduction methods adopted in this work. In Sect. 3 we present our proposed architecture, providing implementation details. In Sect. 4 we describe the datasets used to test our system, the experimental settings adopted and the performance reached on several prediction problems. In Sect. 5 we analyze the results and the functioning of our system through the perspective of nonlinear time-series analysis. Finally, in Sect. 6 we draw our conclusions."}, {"heading": "2 Background material", "text": "In the following, we shortly review the methodologies adopted in our framework. Initially, we describe the classic ESN architecture and two effective approaches adopted for its training. Successively, we summarize two well-know methods used for reducing the dimensionality of the data and for mapping them in a smaller subspace."}, {"heading": "2.1 Echo state Network", "text": "An ESN consists of a large, untrained recurrent layer of non-linear units and a linear, memory-less readout layer, usually trained with a linear regression. A visual representation of an ESN is reported in Fig. 1\nThe equations describing the ESN state-update and output are, respectively, defined as follows:\nh[k] =\u03c6(Wrrh[k \u2212 1] + Wrix[k] + Wroy[k \u2212 1] + \u03be), (1) y[k] =Woix[k] + W o rh[k], (2)\nwhere \u03be is a small i.i.d. noise term. The reservoir consists of Nr neurons characterized by a transfer/activation function \u03c6(\u00b7), typically implemented as a hyperbolic tangent function. At time instant k, the network is driven by the input signal x[k] \u2208 RNi and generates the output y[k] \u2208 RNo , being Ni and No the dimensionality of input and output, respectively. The vector h[k] \u2208 RNr describes the ESN (instantaneous) state. The weight matrices Wrr \u2208 RNr\u00d7Nr (reservoir connections), Wri \u2208 RNr\u00d7Ni (input-to-reservoir), and Wro \u2208 RNr\u00d7No (output-to-reservoir feedback) contain real values in the [\u22121, 1] interval, sampled from a uniform distribution.\nAccording to the ESN theory, the reservoir Wrr must satisfies the so-called \u201cecho state property\u201d (ESP) [39]. This guarantees that the effect of a given input on the state of the reservoir vanish in a finite number of time intervals. A widely used rule-of-thumb suggests to rescale the matrix Wrr to have \u03c1(Wrr) < 1, where \u03c1(\u00b7) denotes the spectral radius, but several theoretically-founded approaches have been proposed in the literature to properly tune \u03c1 in an ESN driven by a specific input [8, 9, 55].\nThe weight matrices Woi and W o r instead, are optimized for the task at hand. To determine them,\nlet us consider the training sequence sequence of Ttr desired input-outputs pairs given by:\n(x[1], y\u2217[1]) . . . , (x[Ttr], y[Ttr]), (3)\nIn the initial phase of training, called state harvesting, the inputs are fed to the reservoir in accordance with Eq. 1, producing a sequence of internal states h[1], . . . ,h[Ttr]. Since, by definition, the outputs of the ESN are not available for feedback, according to the teacher forcing procedure, the desired output is used instead in Eq. 2. States are stacked in a matrix S \u2208 RTtr\u00d7Ni+Nr and the desired outputs in a vector y\u2217 \u2208 RTtr:\nS =  x T [1], hT [1]\n... xT [Ttr], h T [Ttr]\n , y\u2217 =  y \u2217[1] ...\ny\u2217[Ttr]  . The initial D rows S and y\u2217 are the washout elements that should be discarded, since they refer to a transient phase in the ESN\u2019s behavior.\nSince the gain of the sigmoid non-linearity in the neurons is largest around the origin, three coefficients \u03c9i, \u03c9o and \u03c9f are used to scale the input, desired output and feedback signals respectively. In this way, it is possible to control the amount of non-linearity introduced by the processing units.\nThe training of the readout consists in solving a convex optimization problem, for which several closed form solution have been proposed in the literature. The standard procedure to train the readout, originally proposed in [29], consists in a regularized least-square regression, which can be easily computed through the Moore-Penrose pseudo-inverse. However, to learn the optimal readout we also consider the Support Vector Regression (SVR), a supervised learning model that can efficiently perform a non-linear separation of data using a kernel function to map the inputs into high-dimensional feature spaces, where they are linearly separable [11].\nRidge Regression: to train the readout with a linear regressor we adopted ridge regression, whose solution can be computed by solving the following regularized least-square problem:\nW\u2217ls = arg min W\u2208RNi+Nr\n1 2 \u2016SW \u2212 y\u2217\u20162 + \u03bb 2 \u2016W\u20162 =\n( STS + \u03bbI )\u22121 STy\u2217 , (4)\nwhere W = [Woi W o r ] T and \u03bb \u2208 R+ is the L2 regularization coefficient.\nSupport Vector Regression: we adopt a \u03bd-SVR [49] with a Gaussian kernel, initially proposed in [7] as method for readout training. In this case, the ESN acts as a preprocessor for a \u03bd-SVR kernel and their combination can be seen as an adaptive kernel, capable of learning a task-specific time dependency.\nThe state si = [ xT [i] hT [i] ]T is projected to a higher dimensional feature space \u03c6(si), and the \u03bd-SVR is applied on the resulting space. The dual optimization problem can be written as:\nW\u2217svr =  min \u03b1,\u03b1\u2217\u2208RTtr 1 2 (\u03b1\u2212\u03b1\u2217) K (\u03b1\u2212\u03b1\u2217) + y\u2217T (\u03b1\u2212\u03b1\u2217) subject to 1T (\u03b1\u2212\u03b1\u2217) = 0 , 1T (\u03b1 + \u03b1\u2217) \u2264 \u03bb\u03bd ,\n0 \u2264 \u03b1i, \u03b1\u2217i \u2264 \u03bb\nTtr , i = . . . , Ttr\n(5)\nwhere each entry Kij is given by K (si, sj), with K(\u00b7, \u00b7) being a reproducing Gaussian kernel associated to the feature mapping, given by K(si, sj) = exp { \u2212\u03b3\u2016si \u2212 sj\u20162 } , where \u03b3 is denoted as the scale parameter.\nBy an extension of the representer\u2019s theorem, the output of the ESN at a generic time-instant t in this case is given by:\ny[t] = Ttr\u2211 i=1 (\u03b1i \u2212 \u03b1\u2217i )K (si, st) , (6)\nwhere \u03b1i and \u03b1 \u2217 i are the entries of the optimal solution to problem Eq. 5, and they are non-zero only for patterns that are support vectors."}, {"heading": "2.2 Dimensionality reduction methods", "text": "In the following, we describe the dimensionality reduction techniques that we implemented in our framework. First of all, we underline that several approaches can be followed for reducing the dimensionality of the data and to learn underlying manifold on a subspace of the data space [33, 34, 53]. In this work, we limit our analysis to the well know and effective, yet simple procedures, namely Principal Component Analysis (PCA) [26] and kernel Principal Component Analysis (kPCA) [48].\nPCA is a statistically motivated method, which projects the data onto an orthonormal basis that preserves most variance in the input signal, while ensuring that the individual components are uncorrelated. These basis vectors are called the Principal Components. Let X \u2208 Rp be a random vector and let \u03a3X = E\u039bE T be its covariance matrix, where E = ( e1 e2 \u00b7 \u00b7 \u00b7 ep ) and \u039b = diag(\u03bbi) is the orthogonal eigenvector matrix and the diagonal eigenvalue matrix respectively. Then the linear transformation Y = ETX ensures that the covariance matrix of Y is \u03a3Y = \u039b, which clearly implies that the components of Y are uncorrelated. We also see that\np\u2211 i=1 VarXi = p\u2211 i=1 VarYi = p\u2211 i=1 \u03bbi. (7)\nTo reduce the dimensionality to d dimensions, we project the data onto the d eigenvectors with the largest eigenvalues. That is,\nY\u0302 = ETd X, where Ed = ( e1 e2 \u00b7 \u00b7 \u00b7 ed ) is the truncated eigenvector matrix associated with the eigenvalues \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd. According to Eq. 7, this ensures that Y\u0302 preserves most of the variance of X.\nKernel Principal Component Analysis (kPCA) is a nonlinear extension of PCA. Given a valid positive semidefinite (psd) Mercer Kernel\nK(h[i],h[j]) = \u3008\u03a6(h[i]),\u03a6(h[j])\u3009H,\nwhere \u03a6 is some nonlinear mapping from feature space to a Hilbert space H, Kernel PCA implicitely performs PCA in H.\nLet K = {Kij}N\u00d7N , where Kij = K(h[i],h[j]) be the kernel matrix and let E and \u039b be its eigenvector and eigenvalue matrix respectively with the eigenvalues sorted in descending order. Then the projection of the in-sample data onto the principal components in H is given by\nH\u0304 = E\u039b 1 2 . (8)\nThe out-of-sample approximation for the projection of a data point h[k] onto the `th principal component is given by\nh\u0304`[k] = 1\u221a \u03bb` N\u2211 i=1 e`(i)K(h[i],h[k]). (9)\nJust like canonical PCA, to perform dimensionality reduction with kPCA, one need to use the truncated eigenvector- and eigenvalue matrix with Eq. 8 and Eq. 9.\nThe kernel function that is commonly used in practice is the Gaussian kernel which is given by K(h[i],h[j]) = exp { \u2212\u03b3\u2016h[i]\u2212 h[j]\u20162 } , where \u03b3 controls the width of the kernel.\nBoth PCA and kPCA methods admit an out of sample extension, a feature which is required in our framework, as discussed later."}, {"heading": "3 Proposed architecture", "text": "In this section, we provide the details of the architecture of the framework proposed. The large size of the reservoir, specified by the amount Nr of hidden neurons, is one of the main features that determines the effectiveness of the reservoir computing paradigm. Due to the high quantity of neurons, the internal recurrent connections in the reservoir are capable of generating a rich and heterogeneous dynamic to solve complex memory-dependent tasks. However, as the size of the reservoir increases, also the complexity of the model grows, with a consequent risk of overfitting caused by a reduced generalization capability [4]. Dimensionality reduction and manifold learning are techniques that allows to diminish the variance in the data and to introduce a bias, which can reduce the expected value on the prediction error [19]. In the architecture proposed, we use a large reservoir in order to capture the dynamic of the underlying unknown process and then, through a dimensionality reduction procedure, we enforce regularization constraints to increase the generalization capability of our model. Another important consequence that follows from reducing the dimensionality of the reservoir is that complex regression methods can benefit from a reduced computational complexity if the internal states are described by a lower number of variables. Additionally, several methods used to identify, in an unsupervised way, the configurations of hyperparameters which maximize the computational capabilities of the network, require computational demanding procedures of analysis [8, 9, 38]. These procedures would greatly benefit from the simplification offered by our proposed architecture.\nAt each time step t, the vector h[t] \u2208 RNr that represents the internal state of the reservoir, is mapped into a lower dimensional space by a projector P : RNr \u2192 Rd. The new d-dimensional state vector h\u0304[t] = P(h[t]) is then processed by the readout to compute the predicted value y[t].\nTo train our system, the time-series is split in three contiguous parts, namely the training {Xtr,Ytr}, validation {Xvs,Yvs} and test set {Xts,Yts}. Since we deal with time-series prediction problems, each set contains coupled real values, which represent the input value and the ground truth of the associated prediction. For example in the training set we have {x[t],y[t]}Ttrt=1, where y[t] is the predicted value of x[t]. The regression function in the readout is implemented according to one of the two procedures proposed in Sect. 2.1 and the model parameters are learned on the training data. The system depends on several hyperparameters, which affects the network behavior and they must be carefully tuned on the specific problem at hand by performing a cross-validation procedure on the validation set, with a method whose details are provided in the next section.\nOnce the model has been trained, a new test element x[t] of the test set is processed and the relative internal reservoir state h[t] is generated. Successively, the projection h\u0304[t] in the subspace with reduced dimensionality is evaluated using a suitable out of sample approximation. In the case of PCA this can be done by projecting h[t] on the basis defined by the covariance matrix computed on the Ttr states relative to the elements in training set, which are collected in the matrix Htr during the training phase. For kPCA it is possible to use the Ny\u0308strom approximation [2], which specifies an interpolating function for determining the values of out of samples data points.\nA schematic representation of the whole procedure is depicted in Fig. 2."}, {"heading": "3.1 Hyperparameter optimization", "text": "The set of hyperparameters \u03b8 that are used to control the architecture of the ESN, the regression in the readout and the dimensionality-reduction procedure are optimized by minimizing a loss function L(\u00b7), defined as\nL(\u03b8i) = (1\u2212 \u03b1)Err(Yvs) + \u03b1\u03b8(d)i , (10)\nwhere \u03b8(d) = dNr is the hyperparameter that defines the number of dimensions, d, of the new subspace. In order to lower the complexity of the model, L(\u00b7) jointly penalizes prediction error on the validation set and the number of dimensions retained after the dimensionality reduction.\nThe loss function is minimized using a standard genetic algorithm with Gaussian mutation, random crossover, elitism and tournament selection [51]. While the hyperparameter optimization is performed on the validation set, the best individual found is stored and it is successively used to configure the network during the training phase. A schematic description of the training procedure is depicted in Fig. 3."}, {"heading": "4 Experiments", "text": "The component of the loss function (Eq. 10) relative to the error on the given task, is implemented by the Normalized Root Mean Squared Error (NRMSE):\nNRMSE = \u221a \u3008\u2016y[k]\u2212 y\u2217[k]\u20162\u3009 \u3008\u2016y[k]\u2212 \u3008y\u2217[k]\u3009\u20162\u3009 ,\nwhere y[k] is the ESN prediction and y\u2217[k] the desired/teacher output. The GA uses a population size of 50 individuals and evaluates 20 generations. The individuals are mutated and bred at each generation with a mutation probability of Pmut = 0.2 and a crossover probability of Pcx = 0.5. The individuals in the next generation are selected by a tournament strategy with a tournament size of 4 individuals. The bounds for all parameters are shown in Tab. 4. The weight parameter \u03b1 in the loss function (Eq. 10) is set to 0.1.\nDue to the stochastic nature of the ESN, which is a consequence of the random initialization of the weight matrices Wri , W r r and W r o, each individual is evaluated on the validation set using 5 networks initialized with different weight parameters. The fitness is then given by the NRMSE, averaged over these 5 networks. Once the optimal set of parameters \u03b8\u2217 has been found, we predict values for the test set using 32 randomly initialized networks, using the same set of optimal parameters."}, {"heading": "4.1 Datasets description", "text": "To test our system, we consider 3 benchmark tasks commonly used in time-series forecasting, namely the prediction of Mackey-Glass time-series, of multiple superimposed oscillator and of the NARMA signal. The forecasting problems that we consider have a different level of difficulty, given by the nature of the signal and the complexity of the prediction task. Accordingly to a commonly used approach [32], in each prediction task we set the forecast step \u03c4f by computing a statistic that measures the independence of \u03c4f - separated points in the time series. One usually wants the smallest \u03c4f that guarantees the measurements to be decorrelated. Hence, we considered the first zero of the autocorrelation function of the time series, which yields the smallest \u03c4f that maximizes the linear independence between the samples. Alternatively, it is possible to choose the forecast step by considering more general forms of independence, such as the first local minimum on the average mutual information [18] or on the correlation sum [37].\nMackey-Glass time-series: the input signal is generated from the Mackey-Glass (MG) time-delay differential system, described by the following equation:\ndx dt = \u03b1x(t\u2212 \u03c4MG) 1 + x(t\u2212 \u03c4MG)10 \u2212 \u03b2x(t).\nWe generated a time-series of 150000 time-steps using \u03c4MG = 17, \u03b1 = 0.2, \u03b2 = 0.1, initial condition x(0) = 1.2, and 0.1 as integration step for (4.1).\nNARMA signal: the Non-Linear Auto-Regressive Moving Average (NARMA) task, originally proposed in [30], consists in modeling the output of the following r-order system:\ny(t+ 1) = 0.3y(t) + 0.05y(t) [ r\u2211 i=0 y(t\u2212 i) ] + 1.5x(t\u2212 r)x(t) + 0.1.\nThe input to the system x(t) is a uniform random noise in [0, 1], and the model is trained to reproduce y(t+1). The NARMA task is known to require a memory of at least r past time-steps, since the output is determined by the current input and outputs relative to the last r time-steps. In our test we set r = 20.\nMultiple superimposed oscillator: The prediction of a sinusoidal signal is a relatively simple task, which demands a minimum amount of memory to determine the next network output. However, superimposed sine waves with not integer frequencies are much harder to predict, since the wavelength can be extremely long. The signal we consider is the multiple superimposed oscillator (MSO), studied in [31] and defined as:\ny(t) = sin(0.2t) + sin(0.311t) + sin(0.42t) + sin(0.51t) + sin(0.63t) + sin(0.74t)\nESN struggles to solve this task, since neurons in the reservoir tends to couple, while the task requires the simultaneous existence of multiple decoupled internal states [56]."}, {"heading": "4.2 Results", "text": "The averaged prediction results and the standard deviations are reported in Tab. 2. The convergence rate during the optimization of the hyperparameters for each method, expressed as the NRMSE error on the validation set, is depicted in Fig. 4.\nThe prediction of MG is a quite simple task and each model manages to achieve high forecast accuracy. However, by applying a dimensionality reduction on the states of the reservoir, it is possible to lower the error by one or more order of magnitude. Also the standard deviation of the prediction error decreases, especially in the models using kPCA. The best results are achieved by \u03bd-SVR + PCA and \u03bd-SVR + kPCA, while using \u03bd-SVR without reducing reducing the dimensions of the reservoir demonstrated to be less effective. This means that non-linearities benefits the training, but without enforcing the regularization constraint the complexity of the model is to high to fit well testing points. As we can see, in every case the number of dimensions d retained by both PCA and kPCA is much lower than the optimal number of neurons Nr identified. This underline the effectiveness of the regularization conveyed by our architecture. From Fig. 4(a) results that the model implementing ridge regression + kPCA achieves the lowest convergence rate during the cross-validation step. However, thanks to the generalization power provided by the nonlinear dimensionality reduction, the test error is lower than the other models, whose readout is trained with ridge regression.\nIn NARMA prediction task, the best result is achieved by training the readout function with \u03bd-SVR on a reservoir output, whose dimensionality is reduced by kPCA. NARMA is a more complex task which requires a higher amount on nonlinearity to be solved. This is clearly reflected by the results, which improve as more nonlinearity is introduced to learn the function, both in the readout training and in the dimensionality reduction procedure. At the same time, the bias introduced by the regularization enhance the generalization capability of the network significantly. For what concerns the number of dimensions of the optimal subspace, it is higher than in MG task, except for the model implemented with ridge regression + PCA. In this latter cases, however, we obtain the worst performance. Interestingly, from Fig. 4(b) we observe that kPCA has the lower convergence rate, even if this is the best performing model in the testing phase. In this case, the dimensionality reduction introduces a bias, which prevents the model to overfit on the validation data and to develop a high predictive power. On the other hand, the model with \u03bd-SVR and no dimensionality reduction, overfits on the validation data with a consequent poor performance in the test phase.\nFinally, in the MSO task the model with the highest prediction performance is \u03bd-SVR without the dimensionality reduction. In this case, the signal to predict has an extremely long periodicity, which demands a high amount of memory in the network. Hence, the compression of the information through the dimensionality reduction could hamper the memory capacity of the network. Furthermore, due to\nthe long periodicity, the slice of time-series used to train the network can be quite different from the slice to be predicted in the test. Consequently, test points are projected in a subspace which is not optimal, as the basis is learned from the training data. As expected, the number of dimensions kept after the dimensionality reduction is larger than in the other tasks. The need of a high degree of complexity is also denoted by the poor results obtained by using ridge regression in the readout training. From Fig. 4(c), we observe the convergence rate to be faster in models equipped with \u03bd-SVR, which obtain better results both in validation and in testing phase. This symmetry on performances on test and validation reflects the scarce effectiveness of the regularization constraints for this task."}, {"heading": "5 Discussion", "text": "To understand the mechanics and the effectiveness of the proposed architecture, we analyze the results through the theory of nonlinear time-series analysis, which offer powerful methods to retrieve dynamical information from time-ordered data [10]. The objective of time-series analysis is to reconstruct the full dynamics of a complex nonlinear dynamical system, starting from a measurement of only one of its\nvariables. In fact, in many cases it is possible to observe only a subset of the components necessary to determine the time-evolution law which governs the dynamical system.\nThe main idea which inspires this analysis is that a dynamic system is completely described by the time-dependent trajectory in its phase space. Hence, a recurrent neural network that is capable of reconstructing with a high degree of accuracy the dynamic attractor can calculate future states assumed by the system, given a state at any particular moment.\nA frequently used method for phase space reconstruction is the delay-coordinate embedding, which provides an estimation of the attractor that is topologically identical to the true one. From this reconstruction, it is possible to infer several properties of the hidden dynamical system, which are invariant under diffeomorhpism. We refer to these measures as the dynamical invariants of the system. The most commonly studied are the fractal dimension of the attractor, the Lyapuanov exponents and the Re\u0301nyi entropy. In the following, we briefly introduce the delay-coordinate embedding procedure and two approaches used to estimate the aforementioned dynamical invariants. We refer the interested reader to [20, 35] for a comprehensive overview of these methods and many other aspects of time-series analysis.\nDelay-coordinate embedding: a dynamical system is characterized by a time-evolution law, which determines its trajectory in the phase space. Each specific state of the system at time t is defined by a d-dimensional vector in the state space: s(t) = [s1(t), . . . , sd(t)]\nT , being d the number of variables of the system. The delay-coordinate embedding method allows to reconstruct such state vectors from a time-discrete measurement of only one generic smooth function of the state space [42]. Given a discrete time-series x = {x(i\u2206t)}Ni=1 evenly sampled at rate \u2206t, the embedding is defined as:\ns\u0302(i) = m\u2211 j=1 x(i+ (j \u2212 1)\u03c4e)ej , (11)\nwhere m is the embedding dimension, \u03c4e is the time delay and ej form an orthonormal basis in Rm. With a proper choice of embedding parameters m and \u03c4e, Taken theorem guarantees the existence of a diffeomorhpism between the real and reconstructed dynamic [52]. A sufficient condition for a correct reconstruction is m \u2265 2d + 1. The value of m is usually computed with the false nearest-neighbors algorithm [46], which provides an estimation of the smallest sufficient embedding dimension. On the other hand, a suitable time-delay \u03c4e can be estimated looking at the first zero of the autocorrelation function of x or by relying on nonlinear time dependencies, such as the mutual information [12].\nCorrelation dimension: dimension is a measurement invariant under diffeomorhpism that allows to quantify the similarity of geometrical objects. Attractors of dissipative chaotic systems often exhibit complicated geometries (hence the name strange) which are contained in a fractal dimension Dq, called Re\u0301nyi dimension [45]. An efficient estimator of fractal dimensions is Grassberger-Procaccia algorithm [21], which computes the correlation dimension D2 through the correlation sum C2:\nC2(m, ) = 1 2N (N \u2212 \u03c4c) \u2211 i \u2211 j<i\u2212\u03c4c \u0398 ( \u2212 \u2016x(i)\u2212 x(j)\u2016) . (12)\nThe temporal spacing parameter \u03c4c it chosen to ensure temporal independence between samples, \u0398 is the Heaviside function and is the dimension of a set of N small boxes used to cover the geometric shape of the attractor. If m \u2265 D2, C2(m, ) \u221d D2 .\nLyapuanov exponent: the Lyapuanov spectrum {\u03bb1, . . . , \u03bbd} is another invariant measure that characterizes the predictability of a dynamical system. Lyapuanov exponents quantify the rate of separability of two infinitesimal close trajectories and are closely related to the 2nd order Re\u0301nyi entropy K2: K2 \u2264 \u2211 \u03bbi>0\n\u03bbi. This quantity measures the number of possible trajectories that the system can take for a given number of time steps in the future. A perfectly deterministic can only evolve along one possible trajectory and hence K2 = 0. In contrast, for purely stochastic systems the number of possible future trajectories increases to infinity, so K2 \u2192\u221e. Chaotic systems are characterized by a finite value of K2, as the number of possible trajectories diverges but not as fast as in the stochastic case.\nThe largest Lyapuanov exponent (LLE) \u03bb1 is a good estimate of K2, and its sign determines whether a system is chaotic or not. The so-called direct methods can be used to compute \u03bb1 by estimating the\ndivergent motion of the reconstructed space, without fitting a model to the data [43, 57]. In particular, the average exponential growth of the distance of neighboring orbits can be studied on a logarithmic scale by monitoring the prediction error p(t):\np(t) = 1\nN N\u2211 k=1 log2 ( \u2016x[k + t]\u2212 x[k]nn[t]\u2016 \u2016x[k]\u2212 x[k]nn\u2016 ) , (13)\nbeing x [k] nn the nearest neighbor of x at time k. The LLE is estimated as \u03bb1 \u221d p(t)/T with t \u2208 [1, T ], where T is the forecast horizon within which the divergence of the trajectory in the phase space is evaluated."}, {"heading": "5.1 ESN phase space reconstruction", "text": "In the following, we analyze two chaotic time-series generated by the Lorenz and the Moore\u2013Spiegel system respectively. We evaluate the accuracy of the phase space reconstruction performed with our ESN by comparing the topological properties of the true attractor of the dynamic, with the one obtained by applying a dimensionality reduction to the network reservoir. The equivalence of attractors geometries are computed by measuring the dynamical invariants, estimated through the correlation sum and the divergent motion of the reconstructed spaces.\nIn the following, we refer to true attractor, as the trajectory in the phase space generated directly by the differential equations of the dynamic system. With delay-embedding attractor we refer at the trajectory described by the embedding, generated with the delay-coordinate procedure. Finally, ESN attractor is the trajectory spanned by the component of the multivariate vector h\u0304. The latter is the output of the dimensionality reduction procedure applied to the multivariate vector h, which contains the sequence of the states of the reservoir (see Sect. 3). For these tests we considered only the component of the loss function relative to the prediction error, by setting \u03b1 = 0 in Eq. 10, and we fixed the number of dimensions in PCA and kPCA to 3. Finally, to further empathize the effectiveness of the architecture proposed, we also consider the phase space reconstruction obtained directly from h, in the case where the reservoir contains only 3 neurons (Nr = 3).\nLorenz: the system is governed by the following ordinary differential equations:\ndx dt = \u03c3(y \u2212 x), dy dt = x(\u03c1\u2212 z)\u2212 y, dz dt = xy \u2212 \u03b2z, (14)\nwhere variables x, y and z define the state of the system, while \u03c3, \u03c1 and \u03b2 are system parameters. In this work we set \u03c3 = 10 , \u03b2 = 8/3 and \u03c1 = 28, values for which the system exhibits chaotic behavior.\nFig. 5 depicts the geometric shapes of the true attractor, the delay-embedding attractor, the two ESN attractors, generated using a dimensionality reduction or a reservoir with 3 neurons. As is it possible to observe visually, both the embedding and ESN with dimensionality reduction manage to reconstruct well the trajectory described by the differential equations of the dynamic system. To quantify formally this similarity, we compute on the dynamical invariants previously introduced each attractor. In Tab. 3, we report for each phase space trajectory the estimated correlation dimension and the largest Lyapuanov\nexponent, which as previously discussed, represents a good approximation of the K2 entropy. Due to the stochastic nature of the approaches adopted for estimating these quantities, we repeated the procedure 10 different times and we report their average values and the standard deviations. As we can see from the results, both the trajectories described by h\u0304 in the subspace computed using PCA and kPCA generate an attractor whose dynamic invariants are well approximated. In particular, the accuracy of the reconstruction is comparable to the one obtained by the classic time-delay embedding method and in some case it is even better. The standard deviations in the measurements of both correlation dimension and LLE are very small, which indicates a high degree of reliability on both measurements. For what concerns the ESN with 3 neurons, the trajectory described is more \u201cflat\u201d, as it can be seen in the figure. This is confirmed by the estimated correlation dimension and LLE, whose values are much lower than in the other cases. This denotes that the reconstructed dynamic is not rich enough, a symptom that the complexity and the memory of the network is not sufficient to model the underlying system.\nMoore\u2013Spiegel: this dynamical systems manifests interesting synchronization properties, generated by complicated patterns of period-doubling, saddle-node and homoclinic bifurcations [3]. The differential equations which governs system dynamics are the following:\ndx dt = y, dy dt = z, dz dt = \u2212z \u2212 (t\u2212 r + rx2)y \u2212 tx, (15)\nwhere x, y and z form the state of the system and r and t are the parameters of the model. In this study, we set r = 100, b = 10 and c = 14, for which the dynamics of the system exhibits a chaotic behavior.\nIn Fig. 6 we show the shape of the attractors of the dynamic, evaluated directly on the differential equations of the system, on the time-delay embedding, on the internal state of the ESN reduced through PCA and on the state of the ESN with 3 neurons. In this second test, the reconstructed trajectories of the Moore\u2013Spiegel system are more jagged and irregular, with respect to the original one. This suggest a poorer approximation of the true dynamic of the system and is confirmed by the results in Tab. 3.\nCompared to the Lorenz case, the dynamical invariants estimated on the time-delay embedding and on ESN state trajectories approximate with less accuracy the real ones. The reconstructed attractors have a lower correlation dimension, which usually denotes a poor embedding [40]. However, it is worth to notice that the two attractors reconstructed by the ESN+PCA and ESN+kPCA have a larger C2 value than the time-delay embedding and hence they approximate better the true dynamics. For what concerns the LLE, the estimated value in each reconstructed dynamic is larger than in the original one. This means that both the time-delay embedding and the ESNs generate a more chaotic dynamic, as is also reflected by the jagged trajectories in Fig. 6. Even in this case, however, LLE is better approximated by ESN+PCA and ESN+kPCA than by the time-delay embedding. Like before, the standard deviations of the estimates of the two dynamical invariants is very small, which provides a high degree of confidence on the measurements. For what concerns the trajectory described by the ESN state with a small reservoir of 3 neurons, the geometric properties of the reconstruct attractor are even more different from the real ones. This confirm that also in this case such a small amount of neurons cannot catch the dynamic properties of the system to be modeled.\nAs a concluding remark, it is important to understand another aspect of the utility of the ESN in reproducing the attractor of the system dynamic. In fact, this provides a valid alternative to the standard approach based on the time-delay embedding for reconstructing the phase of the system, which presents several caveats and pitfalls [10]. This a fundamental tool for a wide set of applications, where an accurate estimation of the phase space of the system is required [35]."}, {"heading": "6 Conclusions and future directions", "text": "In this work we have presented a new framework for training an Echo State Network, which enhances its generalization capabilities through the regularization constraints introduced by the smoothing effect of a dimensionality reduction procedure. Through a series of test on benchmark dataset, we have demonstrated how the proposed architecture can achieve better prediction performance in different contexts.\nSuccessively, we provided a theoretically grounded explanation of the functioning of the proposed architecture, based on the theory of nonlinear time-series analysis. By studying the dynamical properties of the network under this novel perspective, we showed that through an ESN it is possible to reconstruct the phase space of the dynamic system; this offers a solid, yet simple alternative to the time-delay embedding procedure.\nWe believe that this work could be useful not only to enhance the prediction capabilities of an ESN, but also provide a new instrument for analysis of dynamical systems. As a follow-up of a recent work focused on identifying the edge of criticality of an ESN by evaluating the Fisher information on the state matrix [38], we plan to study the criticality using more reliable Fisher Information Matrix estimators, which are capable of working only on space with few dimensions (e.g., [25]). We also plan on investigating other dimensionality reduction methods, manifold learning and semi-supervised learning approaches to shrink and regularize the output of the network recurrent layer [4, 5]. Finally, as a future work, we propose to use different dimensionality reduction techniques in parallel and combine their result through a single reservoir to produce the final result."}], "references": [{"title": "Benchmarking reservoir computing on time-independent classification tasks", "author": ["L.A. Alexandre", "M.J. Embrechts", "J. Linton"], "venue": "In Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Synchronizing moore and spiegel", "author": ["N. Balmforth", "R. Craster"], "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering", "author": ["Y. Bengio", "J.-F. Paiement", "P. Vincent", "O. Delalleau", "N. Le Roux", "M. Ouimet"], "venue": "Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Short-term electric load forecasting using echo state networks and PCA decomposition", "author": ["F.M. Bianchi", "E. De Santis", "A. Rizzi", "A. Sadeghian"], "venue": "IEEE Access,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Prediction of telephone calls load using Echo State Network with exogenous variables", "author": ["F.M. Bianchi", "S. Scardapane", "A. Uncini", "A. Rizzi", "A. Sadeghian"], "venue": "Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Investigating echo state networks dynamics by means of recurrence analysis", "author": ["F.M. Bianchi", "L. Livi", "C. Alippi"], "venue": "arXiv preprint arXiv:1601.07381,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Information processing in echo state networks at the edge of chaos", "author": ["J. Boedecker", "O. Obst", "J.T. Lizier", "N.M. Mayer", "M. Asada"], "venue": "Theory in Biosciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Nonlinear time-series analysis revisited", "author": ["E. Bradley", "H. Kantz"], "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J. Burges"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Practical method for determining the minimum embedding dimension of a scalar time series", "author": ["L. Cao"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "The smashed filter for compressive classification and target recognition", "author": ["M.A. Davenport", "M.F. Duarte", "M.B. Wakin", "J.N. Laska", "D. Takhar", "K.F. Kelly", "R.G. Baraniuk"], "venue": "In Electronic Imaging 2007, pages 64980H\u201364980H. International Society for Optics and Photonics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Application of echo state networks in short-term electric load", "author": ["A. Deihimi", "H. Showkati"], "venue": "forecasting. Energy,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Short-term electric load and temperature forecasting using wavelet echo state networks with neural reconstruction", "author": ["A. Deihimi", "O. Orang", "H. Showkati"], "venue": "Energy, 57:382\u2013401,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Pruning and regularization in reservoir computing", "author": ["X. Dutoit", "B. Schrauwen", "J.V. Campenhout", "D. Stroobandt", "H.V. Brussel", "M. Nuttin"], "venue": "doi: http://dx.doi.org/10.1016/ j.neucom.2008.12.020. Advances in Machine Learning and Computational Intelligence16th European Symposium on Artificial Neural Networks 200816th European Symposium on Artificial Neural Networks", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "A survey of dimension reduction techniques", "author": ["I.K. Fodor"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Independent coordinates for strange attractors from mutual information", "author": ["A.M. Fraser", "H.L. Swinney"], "venue": "Physical review A,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1986}, {"title": "On bias, variance, 0/1\u2014loss, and the curse-of-dimensionality", "author": ["J.H. Friedman"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Multiscale analysis of complex time series: integration of chaos and random fractal theory, and beyond", "author": ["J. Gao", "Y. Cao", "W.-w. Tung", "J. Hu"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Measuring the strangeness of strange attractors", "author": ["P. Grassberger", "I. Procaccia"], "venue": "In The Theory of Chaotic Attractors,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "A multiple objective optimization based echo state network tree and application to intrusion detection", "author": ["D. Hai-yan", "P. Wen-jiang", "H. Zhen-ya"], "venue": "In VLSI Design and Video Technology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Fuzzy echo state neural networks and funnel dynamic surface control for prescribed performance of a nonlinear dynamic system", "author": ["S. Han", "J. Lee"], "venue": "Industrial Electronics, IEEE Transactions on,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Fuzzy echo state neural networks and funnel dynamic surface control for prescribed performance of a nonlinear dynamic system", "author": ["S.I. Han", "J.M. Lee"], "venue": "Industrial Electronics, IEEE Transactions on,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Nonparametric estimation of Fisher information from real data", "author": ["O. Har-Shemesh", "R. Quax", "B. Mi\u00f1ano", "A.G. Hoekstra", "P.M.A. Sloot"], "venue": "Physical Review E,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1933}, {"title": "A particle swarm optimization to identifying the armax model for short-term load forecasting", "author": ["C.-M. Huang", "C.-J. Huang", "M.-L. Wang"], "venue": "Power Systems, IEEE Transactions on,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks-with an erratum note", "author": ["H. Jaeger"], "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2001}, {"title": "Adaptive nonlinear system identification with echo state networks", "author": ["H. Jaeger"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["H. Jaeger", "H. Haas"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Did the ecmwf seasonal forecast model outperform statistical enso forecast models over the last 15 years", "author": ["G. Jan van Oldenborgh", "M.A. Balmaseda", "L. Ferranti", "T.N. Stockdale", "D.L. Anderson"], "venue": "Journal of climate,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Kernel entropy component analysis", "author": ["R. Jenssen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Entropy-relevant dimensions in the kernel feature space: Cluster-capturing dimensionality reduction", "author": ["R. Jenssen"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Nonlinear time series analysis, volume 7", "author": ["H. Kantz", "T. Schreiber"], "venue": "Cambridge university press,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "Chaotic time series prediction based on a novel robust echo state network", "author": ["D. Li", "M. Han", "J. Wang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Proper choice of the time delay for the analysis of chaotic time series", "author": ["W. Liebert", "H. Schuster"], "venue": "Physics Letters A,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1989}, {"title": "Determination of the edge of criticality in echo state networks through fisher information maximization", "author": ["L. Livi", "F.M. Bianchi", "C. Alippi"], "venue": "arXiv preprint arXiv:1603.03685,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger"], "venue": "Computer Science Review,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Recurrence plots for the analysis of complex systems", "author": ["N. Marwan", "M.C. Romano", "M. Thiel", "J. Kurths"], "venue": "Physics reports,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}, {"title": "Utilization of echo state networks for differentiating source and nonlinear load harmonics in the utility network", "author": ["J. Mazumdar", "R. Harley"], "venue": "Power Electronics, IEEE Transactions on,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Geometry from a time series", "author": ["N.H. Packard", "J.P. Crutchfield", "J.D. Farmer", "R.S. Shaw"], "venue": "Physical Review Letters,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1980}, {"title": "Nonlinear time-series analysis", "author": ["U. Parlitz"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1998}, {"title": "A novel hybridization of echo state networks and multiplicative seasonal ARIMA model for mobile communication traffic series forecasting", "author": ["Y. Peng", "M. Lei", "J.-B. Li", "X.-Y. Peng"], "venue": "Neural Computing and Applications,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "On the dimension and entropy of probability distributions", "author": ["A. R\u00e9nyi"], "venue": "Acta Mathematica Academiae Scientiarum Hungarica,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1959}, {"title": "The false nearest neighbors algorithm: An overview", "author": ["C. Rhodes", "M. Morari"], "venue": "Computers & Chemical Engineering,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1997}, {"title": "Significance-Based Pruning for Reservoir\u2019s Neurons in Echo State Networks, pages 31\u201338", "author": ["S. Scardapane", "D. Comminiello", "M. Scarpiniti", "A. Uncini"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Kernel principal component analysis", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1997}, {"title": "New support vector algorithms", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "R.C. Williamson", "P.L. Bartlett"], "venue": "Neural computation,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2000}, {"title": "Automatic speech recognition using a predictive echo state network classifier", "author": ["M.D. Skowronski", "J.G. Harris"], "venue": "Neural networks,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2007}, {"title": "Genetic algorithms: a survey", "author": ["M. Srinivas", "L.M. Patnaik"], "venue": "ISSN 0018-9162", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1994}, {"title": "Detecting strange attractors in turbulence", "author": ["F. Takens"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1981}, {"title": "Dimensionality reduction: a comparative", "author": ["L. Van Der Maaten", "E. Postma", "J. Van den Herik"], "venue": "J Mach Learn Res,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}, {"title": "Half Hourly Electricity Load Prediction using Echo State Network", "author": ["S. Varshney", "T. Verma"], "venue": "International Journal of Science and Research,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "On the quantification of dynamics in reservoir computing", "author": ["D. Verstraeten", "B. Schrauwen"], "venue": "Artificial Neural Networks \u2013 ICANN 2009,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Modeling systems with internal state using evolino", "author": ["D. Wierstra", "F.J. Gomez", "J. Schmidhuber"], "venue": "In Proceedings of the 7th annual conference on Genetic and evolutionary computation,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2005}, {"title": "Determining lyapunov exponents from a time series", "author": ["A. Wolf", "J.B. Swift", "H.L. Swinney", "J.A. Vastano"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1985}, {"title": "Compressed and privacy-sensitive sparse regression", "author": ["S. Zhou", "J. Lafferty", "L. Wasserman"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2009}], "referenceMentions": [{"referenceID": 37, "context": "Echo State Networks (ESN) belong to the class of computational dynamical systems, implemented according to the so-called reservoir computing approach [39].", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "Contrary to most hard computing approaches, which demand long training procedures to learn model parameters through an optimization algorithm [27], ESN is characterized by a very fast learning procedure that usually consists in solving a convex optimization problem.", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 88, "endOffset": 91}, {"referenceID": 48, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 161, "endOffset": 165}, {"referenceID": 39, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 199, "endOffset": 203}, {"referenceID": 22, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 283, "endOffset": 287}, {"referenceID": 4, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 12, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 13, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 13, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 42, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 52, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 29, "context": "Outstanding results have also been achieved by ESN in prediction of chaotic time-series [31, 36], which highlighted the capability of these neural networks to learn amazingly accurate models to forecast a chaotic process from almost noise-free training data.", "startOffset": 88, "endOffset": 96}, {"referenceID": 34, "context": "Outstanding results have also been achieved by ESN in prediction of chaotic time-series [31, 36], which highlighted the capability of these neural networks to learn amazingly accurate models to forecast a chaotic process from almost noise-free training data.", "startOffset": 88, "endOffset": 96}, {"referenceID": 5, "context": "Additionally, several regression methods adopted to train the readout layer could be affected by the curse of dimensionality in case of high dimensional data, which could also cause increments in both the computational requirements in software and the resource needed in hardware [7].", "startOffset": 280, "endOffset": 283}, {"referenceID": 56, "context": "Several tasks in signal processing and machine learning applications have been tackled by evaluating regression functions [58], performing classification [13] or finding neighbors [28] in a reduced dimensional space.", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "Several tasks in signal processing and machine learning applications have been tackled by evaluating regression functions [58], performing classification [13] or finding neighbors [28] in a reduced dimensional space.", "startOffset": 154, "endOffset": 158}, {"referenceID": 26, "context": "Several tasks in signal processing and machine learning applications have been tackled by evaluating regression functions [58], performing classification [13] or finding neighbors [28] in a reduced dimensional space.", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "For example, in [16], the authors propose a form of regularization by shrinking the weights of the connections from the reservoir to the readout layer.", "startOffset": 16, "endOffset": 20}, {"referenceID": 45, "context": "In [47] by pruning some connections from the reservoir to the readout layer, better generalization capabilities are achieved along with some insight on which neurons are actually useful for the output, providing clues on how to create a good reservoir.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Even if additional operations are introduced to compute the reduced dimensionality embedding, training the readout layer becomes less demanding, especially in regression methods whose computational complexity depends on input dimension [17].", "startOffset": 236, "endOffset": 240}, {"referenceID": 37, "context": "According to the ESN theory, the reservoir W r must satisfies the so-called \u201cecho state property\u201d (ESP) [39].", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "A widely used rule-of-thumb suggests to rescale the matrix W r to have \u03c1(W r) < 1, where \u03c1(\u00b7) denotes the spectral radius, but several theoretically-founded approaches have been proposed in the literature to properly tune \u03c1 in an ESN driven by a specific input [8, 9, 55].", "startOffset": 261, "endOffset": 271}, {"referenceID": 7, "context": "A widely used rule-of-thumb suggests to rescale the matrix W r to have \u03c1(W r) < 1, where \u03c1(\u00b7) denotes the spectral radius, but several theoretically-founded approaches have been proposed in the literature to properly tune \u03c1 in an ESN driven by a specific input [8, 9, 55].", "startOffset": 261, "endOffset": 271}, {"referenceID": 53, "context": "A widely used rule-of-thumb suggests to rescale the matrix W r to have \u03c1(W r) < 1, where \u03c1(\u00b7) denotes the spectral radius, but several theoretically-founded approaches have been proposed in the literature to properly tune \u03c1 in an ESN driven by a specific input [8, 9, 55].", "startOffset": 261, "endOffset": 271}, {"referenceID": 0, "context": "To determine them, let us consider the training sequence sequence of Ttr desired input-outputs pairs given by: (x[1], y\u2217[1]) .", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "To determine them, let us consider the training sequence sequence of Ttr desired input-outputs pairs given by: (x[1], y\u2217[1]) .", "startOffset": 120, "endOffset": 123}, {"referenceID": 0, "context": "1, producing a sequence of internal states h[1], .", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "S = \uf8ef\uf8f0 x T [1], h [1] .", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "S = \uf8ef\uf8f0 x T [1], h [1] .", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "x [Ttr], h T [Ttr] \uf8fa\uf8fb , y\u2217 = \uf8ef\uf8f0 y \u2217[1] .", "startOffset": 35, "endOffset": 38}, {"referenceID": 27, "context": "The standard procedure to train the readout, originally proposed in [29], consists in a regularized least-square regression, which can be easily computed through the Moore-Penrose pseudo-inverse.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "However, to learn the optimal readout we also consider the Support Vector Regression (SVR), a supervised learning model that can efficiently perform a non-linear separation of data using a kernel function to map the inputs into high-dimensional feature spaces, where they are linearly separable [11].", "startOffset": 295, "endOffset": 299}, {"referenceID": 47, "context": "Support Vector Regression: we adopt a \u03bd-SVR [49] with a Gaussian kernel, initially proposed in [7] as method for readout training.", "startOffset": 44, "endOffset": 48}, {"referenceID": 5, "context": "Support Vector Regression: we adopt a \u03bd-SVR [49] with a Gaussian kernel, initially proposed in [7] as method for readout training.", "startOffset": 95, "endOffset": 98}, {"referenceID": 31, "context": "First of all, we underline that several approaches can be followed for reducing the dimensionality of the data and to learn underlying manifold on a subspace of the data space [33, 34, 53].", "startOffset": 176, "endOffset": 188}, {"referenceID": 32, "context": "First of all, we underline that several approaches can be followed for reducing the dimensionality of the data and to learn underlying manifold on a subspace of the data space [33, 34, 53].", "startOffset": 176, "endOffset": 188}, {"referenceID": 51, "context": "First of all, we underline that several approaches can be followed for reducing the dimensionality of the data and to learn underlying manifold on a subspace of the data space [33, 34, 53].", "startOffset": 176, "endOffset": 188}, {"referenceID": 24, "context": "In this work, we limit our analysis to the well know and effective, yet simple procedures, namely Principal Component Analysis (PCA) [26] and kernel Principal Component Analysis (kPCA) [48].", "startOffset": 133, "endOffset": 137}, {"referenceID": 46, "context": "In this work, we limit our analysis to the well know and effective, yet simple procedures, namely Principal Component Analysis (PCA) [26] and kernel Principal Component Analysis (kPCA) [48].", "startOffset": 185, "endOffset": 189}, {"referenceID": 2, "context": "However, as the size of the reservoir increases, also the complexity of the model grows, with a consequent risk of overfitting caused by a reduced generalization capability [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 17, "context": "Dimensionality reduction and manifold learning are techniques that allows to diminish the variance in the data and to introduce a bias, which can reduce the expected value on the prediction error [19].", "startOffset": 196, "endOffset": 200}, {"referenceID": 6, "context": "Additionally, several methods used to identify, in an unsupervised way, the configurations of hyperparameters which maximize the computational capabilities of the network, require computational demanding procedures of analysis [8, 9, 38].", "startOffset": 227, "endOffset": 237}, {"referenceID": 7, "context": "Additionally, several methods used to identify, in an unsupervised way, the configurations of hyperparameters which maximize the computational capabilities of the network, require computational demanding procedures of analysis [8, 9, 38].", "startOffset": 227, "endOffset": 237}, {"referenceID": 36, "context": "Additionally, several methods used to identify, in an unsupervised way, the configurations of hyperparameters which maximize the computational capabilities of the network, require computational demanding procedures of analysis [8, 9, 38].", "startOffset": 227, "endOffset": 237}, {"referenceID": 49, "context": "The loss function is minimized using a standard genetic algorithm with Gaussian mutation, random crossover, elitism and tournament selection [51].", "startOffset": 141, "endOffset": 145}, {"referenceID": 30, "context": "Accordingly to a commonly used approach [32], in each prediction task we set the forecast step \u03c4f by computing a statistic that measures the independence of \u03c4f separated points in the time series.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "Alternatively, it is possible to choose the forecast step by considering more general forms of independence, such as the first local minimum on the average mutual information [18] or on the correlation sum [37].", "startOffset": 175, "endOffset": 179}, {"referenceID": 35, "context": "Alternatively, it is possible to choose the forecast step by considering more general forms of independence, such as the first local minimum on the average mutual information [18] or on the correlation sum [37].", "startOffset": 206, "endOffset": 210}, {"referenceID": 28, "context": "NARMA signal: the Non-Linear Auto-Regressive Moving Average (NARMA) task, originally proposed in [30], consists in modeling the output of the following r-order system:", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "The input to the system x(t) is a uniform random noise in [0, 1], and the model is trained to reproduce y(t+1).", "startOffset": 58, "endOffset": 64}, {"referenceID": 29, "context": "The signal we consider is the multiple superimposed oscillator (MSO), studied in [31] and defined as:", "startOffset": 81, "endOffset": 85}, {"referenceID": 54, "context": "ESN struggles to solve this task, since neurons in the reservoir tends to couple, while the task requires the simultaneous existence of multiple decoupled internal states [56].", "startOffset": 171, "endOffset": 175}, {"referenceID": 8, "context": "To understand the mechanics and the effectiveness of the proposed architecture, we analyze the results through the theory of nonlinear time-series analysis, which offer powerful methods to retrieve dynamical information from time-ordered data [10].", "startOffset": 243, "endOffset": 247}, {"referenceID": 18, "context": "We refer the interested reader to [20, 35] for a comprehensive overview of these methods and many other aspects of time-series analysis.", "startOffset": 34, "endOffset": 42}, {"referenceID": 33, "context": "We refer the interested reader to [20, 35] for a comprehensive overview of these methods and many other aspects of time-series analysis.", "startOffset": 34, "endOffset": 42}, {"referenceID": 40, "context": "The delay-coordinate embedding method allows to reconstruct such state vectors from a time-discrete measurement of only one generic smooth function of the state space [42].", "startOffset": 167, "endOffset": 171}, {"referenceID": 50, "context": "With a proper choice of embedding parameters m and \u03c4e, Taken theorem guarantees the existence of a diffeomorhpism between the real and reconstructed dynamic [52].", "startOffset": 157, "endOffset": 161}, {"referenceID": 44, "context": "The value of m is usually computed with the false nearest-neighbors algorithm [46], which provides an estimation of the smallest sufficient embedding dimension.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "On the other hand, a suitable time-delay \u03c4e can be estimated looking at the first zero of the autocorrelation function of x or by relying on nonlinear time dependencies, such as the mutual information [12].", "startOffset": 201, "endOffset": 205}, {"referenceID": 43, "context": "Attractors of dissipative chaotic systems often exhibit complicated geometries (hence the name strange) which are contained in a fractal dimension Dq, called R\u00e9nyi dimension [45].", "startOffset": 174, "endOffset": 178}, {"referenceID": 19, "context": "An efficient estimator of fractal dimensions is Grassberger-Procaccia algorithm [21], which computes the correlation dimension D2 through the correlation sum C2:", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "divergent motion of the reconstructed space, without fitting a model to the data [43, 57].", "startOffset": 81, "endOffset": 89}, {"referenceID": 55, "context": "divergent motion of the reconstructed space, without fitting a model to the data [43, 57].", "startOffset": 81, "endOffset": 89}, {"referenceID": 1, "context": "Moore\u2013Spiegel: this dynamical systems manifests interesting synchronization properties, generated by complicated patterns of period-doubling, saddle-node and homoclinic bifurcations [3].", "startOffset": 182, "endOffset": 185}, {"referenceID": 38, "context": "The reconstructed attractors have a lower correlation dimension, which usually denotes a poor embedding [40].", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "In fact, this provides a valid alternative to the standard approach based on the time-delay embedding for reconstructing the phase of the system, which presents several caveats and pitfalls [10].", "startOffset": 190, "endOffset": 194}, {"referenceID": 33, "context": "This a fundamental tool for a wide set of applications, where an accurate estimation of the phase space of the system is required [35].", "startOffset": 130, "endOffset": 134}, {"referenceID": 36, "context": "As a follow-up of a recent work focused on identifying the edge of criticality of an ESN by evaluating the Fisher information on the state matrix [38], we plan to study the criticality using more reliable Fisher Information Matrix estimators, which are capable of working only on space with few dimensions (e.", "startOffset": 146, "endOffset": 150}, {"referenceID": 23, "context": ", [25]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "We also plan on investigating other dimensionality reduction methods, manifold learning and semi-supervised learning approaches to shrink and regularize the output of the network recurrent layer [4, 5].", "startOffset": 195, "endOffset": 201}, {"referenceID": 3, "context": "We also plan on investigating other dimensionality reduction methods, manifold learning and semi-supervised learning approaches to shrink and regularize the output of the network recurrent layer [4, 5].", "startOffset": 195, "endOffset": 201}], "year": 2016, "abstractText": "In this paper we introduce a new framework to train an Echo State Network to predict real valued time-series. The method consists in projecting the output of the internal layer of the network on a space with lower dimensionality, before training the output layer to learn the target task. Notably, we enforce a regularization constraint that leads to better generalization capabilities. We evaluate the performances of our approach on several benchmark tests, using different techniques to train the readout of the network, achieving superior predictive performance when using the proposed framework. Finally, we provide an insight on the effectiveness of the implemented mechanics through a visualization of the trajectory in the phase space and relying on the methodologies of nonlinear time-series analysis. By applying our method on well known chaotic systems, we provide evidence that the lower dimensional embedding retains the dynamical properties of the underlying system better than the full-dimensional internal states of the network. Keywords\u2014 Echo state network, nonlinear time-series analysis, dimensionality reduction, timeseries prediction", "creator": "LaTeX with hyperref package"}}}