{"id": "1608.05014", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Path-based vs. Distributional Information in Recognizing Lexical Semantic Relations", "abstract": "Recognizing various semantic relations between terms is crucial for many NLP tasks. While path-based and distributional information sources are considered complementary, the strong results the latter showed on recent datasets suggested that the former's contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. We demonstrate that these two information sources are indeed complementary, and analyze the contributions of each source.", "histories": [["v1", "Wed, 17 Aug 2016 16:27:49 GMT  (144kb)", "http://arxiv.org/abs/1608.05014v1", null], ["v2", "Thu, 6 Oct 2016 09:46:36 GMT  (19kb)", "http://arxiv.org/abs/1608.05014v2", "4 pages"], ["v3", "Thu, 27 Oct 2016 15:27:21 GMT  (149kb)", "http://arxiv.org/abs/1608.05014v3", "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), in COLING 2016"], ["v4", "Wed, 2 Nov 2016 08:57:39 GMT  (151kb)", "http://arxiv.org/abs/1608.05014v4", "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), in COLING 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vered shwartz", "ido dagan"], "accepted": false, "id": "1608.05014"}, "pdf": {"name": "1608.05014.pdf", "metadata": {"source": "CRF", "title": "The Roles of Path-based and Distributional Information in Recognizing Lexical Semantic Relations", "authors": ["Vered Shwartz", "Ido Dagan"], "emails": ["vered1986@gmail.com", "dagan@cs.biu.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n05 01\n4v 1\n[ cs\n.C L\n] 1\n7 A\nug 2\n01 6\nRecognizing various semantic relations between terms is crucial for many NLP tasks. While path-based and distributional information sources are considered complementary, the strong results the latter showed on recent datasets suggested that the former\u2019s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. We demonstrate that these two information sources are indeed complementary, and analyze the contributions of each source."}, {"heading": "1 Introduction", "text": "Automated methods to recognize the lexical semantic relation between terms are valuable for NLP applications, and several datasets have been developed for training and evaluating such methods (e.g. Baroni and Lenci, 2011; Santus et al., 2015).\nTwo main information sources have been used to recognize lexical semantic relations: path-based and distributional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), and consistently outperformed other methods (Santus et al., 2016; Necs\u0327ulescu et al., 2015).\nWhile these two sources have been considered complementary, recent results seemed to suggest that path-based methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, yielding notably improved results on a new dataset.\nIn this paper, we investigate the extension of Shwartz et al.\u2019s (2016) method to recognize multiple semantic relations, and reexamine their findings over common datasets for the broader task. We show that this integrated method is indeed effective also in the multiclass setting, performing better than each individual method. We further assess the contribution of path-based information to semantic relation classification. Even though the distributional source is dominant across most datasets, path-based information always contributed to the distributional signal. In particular, path-based information seems to better capture the relationship between terms, rather than their individual properties, and can do so even for rare words or senses. Finally, we analyze the performance of the two information sources for different relation types.1\n1Our code and data are available at https://github.com/vered1986/LexNET"}, {"heading": "2 Background: HypeNET", "text": "Recently, Shwartz et al. (2016) introduced HypeNET, a hypernymy detection method based on the integration of the best-performing distributional method for the task with a novel neural path representation, improving upon state-of-the-art methods.\nIn HypeNET, a term-pair (x, y) is represented as a feature vector, consisting of both distributional and path-based features:\n~vxy = [~vwx , ~vpaths(x,y), ~vwy ] (1)\nwhere ~vwx and ~vwy are x and y\u2019s word embeddings, providing their distributional representation, and ~vpaths(x,y) is an embedding vector representing all the dependency paths that connect x and y in the corpus. A binary classifier is trained on these vectors, yielding c = softmax(W \u00b7 ~vxy), predicting hypernymy if c[1] > 0.5.\nEach dependency path is embedded using a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997), resulting in a path vector space in which semantically-similar paths (e.g. X is defined as Y and X is described as Y) have similar vectors. The vectors of all the paths that connect x and y are averaged to create ~vpaths(x,y).\nShwartz et al. (2016) showed that this new path representation, when used on its own, outperforms prior path-based methods for hypernymy detection. Further, they show that the above two representations are indeed complementary, with the combined model yielding a substantial improvement over each individual model. While HypeNET is designed for detecting hypernymy relations, it seems straightforward to extend it to classify term-pairs simultaneously to multiple semantic relations, as follows."}, {"heading": "3 Classification Methods", "text": "We experiment with several methods:\nPath-based (PB) The HypeNET path-based model is a binary classifier trained on the path vectors alone: ~vpaths(x,y). We adapt this model to classify multiple semantic relations by changing the network softmax output c to a distribution over k target relations, and classifying a pair to the highest scoring relation: r = argmaxic[i].\nDistributional We train an SVM classifier on the concatenation of x and y\u2019s word embeddings [~vwx , ~vwy ] (Baroni et al., 2012) (DS).\n2 Prior work suggested that such a linear classifier is incapable of capturing interactions between x and y\u2019s features, and that instead it learns separate properties of x and y (Levy et al., 2015), e.g. by memorizing that animal is a prototypical hypernym. To examine the effect of non-linear expressive power on the model, we experiment with a neural network with a single hidden layer trained on [~vwx , ~vwy ] (DSh). This was previously done by Bowman et al. (2014), with promising results, but on a small artificial vocabulary.\nIntegrated We similarly adapt the HypeNET integrated model to classify multiple semantic relations (LexNET). Based on the same motivation of DSh, we also experiment with a version of the network with a hidden layer (LexNETh):\n~h = tanh(W1 \u00b7 ~vxy + b1) (2)\nc = softmax(W2 \u00b7 h+ b2) (3)\nwhere Wi and bi are network parameters, and h \u2208 R |~vxy|+k\n2 is a hidden layer.3 The technical details of our network are identical to Shwartz et al. (2016).\n2We experimented also with difference ~vwx \u2212~vwy and some other classifiers, and chose concatenation as it yielded the best performance on the validation sets.\n3We also tried |~vxy|+ k which performed similarly."}, {"heading": "4 Datasets", "text": "We use four common semantic relation datasets, detailed in Table 1: K&H+N ((Necs\u0327ulescu et al., 2015), an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), and ROOT09 (Santus et al., 2016). All datasets extracted instances from semantic resources (Fellbaum, 1998; Speer and Havasi, 2013). Most relations are parallel to WordNet relations, such as hypernymy (cat, animal), meronymy (hand, body) and co-hyponymy (cat, dog), with an additional random relation for negative instances. BLESS contains the event and attribute relations, connecting a concept with a typical activity/property (e.g. (alligator, swim) and (alligator, aquatic)). EVALution contains a richer schema of semantic relations, with some redundancy: it contains both meronymy and holonymy (e.g. for bicycle and wheel), and the fine-grained substance-holonymy relation. We removed two relations with too few instances: Entails and MemberOf.\nTo prevent a possible lexical memorization effect (Levy et al., 2015), Santus et al. (2016) added negative switched hyponym-hypernym pairs (e.g. (apple, animal), (cat, fruit)) to ROOT09, which were reported to reduce this effect."}, {"heading": "5 Results and Analysis", "text": "Table 2 displays the performance of the different methods (\u00a7 3) on all datasets. Like Shwartz et al. (2016), we tuned the hyper-parameters of all methods on the validation set of each dataset, and used Wikipedia as corpus."}, {"heading": "5.1 Classification Architecture", "text": "The results show that LexNET is indeed effective in the multiclass setting, outperforming each individual method on all datasets, and successfully integrating distributional and path-based information, as analyzed in Section 5.2. The only dataset on which performance is mediocre is EVALution, which seems to be inherently harder for all methods, due to its large number of relations and small size.\nDSh substantially improves upon DS on all datasets. The hidden layer seems to enable interactions between x and y\u2019s features, which is especially noticed in ROOT09 hypernymy-relation F1 score that went up from 0.25 in DS to 0.45 in DSh.\nNevertheless, we did not observe a similar behavior in LexNETh, which worked similarly or slightly worse than LexNET. It is possible that the contributions of the hidden layer and the path-based source over the distributional signal are redundant.4 It may also be that the larger number of parameters in LexNETh prevents convergence to the optimal values given the modest amount of training data, stressing the need for large-scale datasets that will facilitate training neural methods."}, {"heading": "5.2 Analysis of Information Sources", "text": "Table 2 demonstrates that the distributional source is dominant across most datasets, with DS performing better than PB. Although by design DS does not consider the relation between x and y, but rather learns properties of x or y, it performs well on BLESS and K&H+N. DSh further manages to capture relationships at the distributional level, leaving the path-based source little room for improvement on these two datasets.\nOn ROOT09, on the other hand, DS achieved the lowest performance. Our analysis reveals that this is due to the switched hypernym pairs, which drastically hurt the ability to memorize, hence reducing performance. The F1 scores of DS on this dataset were 0.91 for co-hyponyms but only 0.25 for hypernyms, while PB scored 0.87 and 0.66 respectively. Moreover, LexNET gains 10 points over DSh, suggesting the better capacity of path-based methods to capture relations between terms.\nTo analyze the contribution of the path-based information source, we examined the term-pairs that were correctly classified by the best performing integrated model (LexNET or LexNETh) while being incorrectly classified by DSh. Table 3 displays the number of such pairs in each dataset, with corresponding term-pair examples.\nOverall we noticed that in many cases, (x, y) term-pairs were classified by DSh according to the most frequent relation of y in the train set, i.e. the memorization effect identified by Levy et al. (2015). This is mostly frequent in ROOT09 (405 pairs, which is 72%), where the switched hypernym pairs make the model less able to predict the relation according to the label frequency of a single term. However, this is also common in other datasets (47% in BLESS, 43% in EVALution and 34% K&H+N). The integrated model relies also on path-based information, which refers to both terms in the pair rather than to only one of them. When such information is available, the integrated model succeeds to overcome the most frequent label bias which is dominant for one of the pairs (usually y), and classifies these pairs correctly.\nAnother common error of DSh is that it often fails to recognize the relation between x and y when the relation is not prototypical, e.g. when y is a less-prototypical holonym, event or attribute of x, as in mero:(villa, guest), event:(cherry, pick), and attri:(piano, electric). While the distributional methods overlook these relations, it seems that they were expressed in joint occurrences in the corpus, allowing the path-based component to capture the relations.\nWe also noticed that the integrated method often managed to classify term-pairs in which one of the terms is rare, where the distributional method failed, as in hyper:(mastodon, proboscidean). It is well known that the relative disadvantage of path-based methods is that they require informative cooccurrences of x and y, which are not always available. With that said, thanks to the averaged path representation, PB can capture the relation between terms even if they only co-occur once within an informative path, while the distributional representation of rare terms may be of low-quality.5\nSimilarly, the integrated methods (as well as PB) succeeded to capture relations for rare senses where DSh failed: mero:(piano, key), event:(table, draw). Distributional representations aggregate a term\u2019s contexts in all senses, leading to insufficient representation of rare senses. For PB, even a single meaningful occurrence of the rare sense with its related term may suffice to capture the relation. At the same time,\n4We also tried adding a hidden layer only over the distributional features of LexNET, but it did not improve performance. 5Notice that the path-based information of a term-pair (x, y) is encoded in the vector ~vpaths(x,y), which is the averaged vector representation of all paths that connected x and y in the corpus. Unlike other path-based methods in the literature, this representation is indifferent to the total number of paths, and as a result, even a single informative path can lead to successful classification.\nit is less likely for such a polysemous term x to co-occur in its other senses with y (e.g. door-key and piano). Therefore, in an example like piano-key, all or most paths representing the pair would correspond to the keyboard sense of key, and hence would be likely to indicate the semantic relation that does hold for this pair with respect to this rare sense.\nFinally, we note that all these common datasets were created synthetically using available semantic resources. As seen above, this leads to inconsistent behavior of the different methods, depending on the particular distribution of examples in each dataset. This stresses the need to develop \u201cnaturally\u201d distributed datasets that would be drawn from corpora, while reflecting realistic distributions encountered by semantic applications."}, {"heading": "5.3 Analysis of Semantic Relations", "text": "To analyze the contribution of each information source for specific relations, we merged all datasets to a single dataset with the WordNet relations in Table 1, and evaluated all methods on this dataset.6\nThe confusion matrices (Figure 1) identify the relations recognized by each information source, reassessing prior findings (Mirkin et al., 2006). PB is unsurprisingly weak in recognizing synonyms, which do not tend to co-occur. It also performed badly on co-hyponyms and antonyms. Using the analysis procedure of Shwartz et al. (2016), we found that co-hyponymy indicating paths include many lists (e.g. X, banjo, mandolin and Y), but having most of these pairs classified as random suggests that most pairs in the dataset never occurred in such lists. DSh performed poorly on synonyms and antonyms, which are often mistaken for each other, since both tend to occur in the same contexts.\nTable 4 displays the per relation F1 score of each method. The path-based source consistently adds over the distributional one, while most contribution occurs either for relations that PB learned well (attribute) or for those that DSh did not (synonym). Since DSh fails to recognize most of the synonyms, for\n6We collapsed holonymy and meronymy, removed substance-meronymy, and subsampled from random and coord, which had significantly more instances than other relations.\nthe few synonyms that did co-occur within informative paths, the path-based component contributed to the classification. An example is the pair (connect, join), that occurs in \u201cconnect the dots (also known as join the dots)\u201d.\nWe note that LexNET, as well as the individual methods, perform poorly on synonyms and antonyms. It seems worthwhile to try improving the model with insights from prior work on these specific relations (Santus et al., 2014; Mohammad et al., 2013) or by using additional information sources, like multilingual data (Pavlick et al., 2015)."}, {"heading": "6 Conclusion", "text": "We presented an adaptation to HypeNET (Shwartz et al., 2016) that classifies term-pairs to one of multiple semantic relations. Evaluation on common datasets shows that HypeNET is extensible to the multiclass setting and performs better than each individual method.\nAlthough the distributional information source is dominant across most datasets, it consistently benefits from path-based information, particularly when finer modeling of inter-term relationship is needed. Finally, the behavior of the different methods is sensitive to the distribution of instances in each dataset, stressing the need to develop large realistic datasets with a natural term distribution."}], "references": [{"title": "How we blessed distributional semantic evaluation", "author": ["Baroni", "Lenci2011] Marco Baroni", "Alessandro Lenci"], "venue": "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,", "citeRegEx": "Baroni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2011}, {"title": "Entailment above the word level in distributional semantics", "author": ["Baroni et al.2012] Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan"], "venue": "In EACL,", "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Learning distributed word representations for natural logic reasoning", "author": ["Christopher Potts", "Christopher D Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2014}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti A Hearst"], "venue": "In ACL,", "citeRegEx": "Hearst.,? \\Q1992\\E", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A semi-supervised method to learn and construct taxonomies using the web", "author": ["Kozareva", "Hovy2010] Zornitsa Kozareva", "Eduard Hovy"], "venue": "In EMNLP,", "citeRegEx": "Kozareva et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kozareva et al\\.", "year": 2010}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Levy et al.2015] Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Integrating pattern-based and distributional similarity methods for lexical entailment acquisition", "author": ["Ido Dagan", "Maayan Geffet"], "venue": "In COLING and ACL,", "citeRegEx": "Mirkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mirkin et al\\.", "year": 2006}, {"title": "Computing lexical contrast", "author": ["Bonnie J Dorr", "Graeme Hirst", "Peter D Turney"], "venue": null, "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Patty: a taxonomy of relational patterns with semantic types", "author": ["Gerhard Weikum", "Fabian Suchanek"], "venue": "In EMNLP and CoNLL,", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships", "author": ["N\u00faria Bel", "Sara Mendes", "David Jurgens", "Roberto Navigli"], "venue": "Lexical and Computational Semantics", "citeRegEx": "Nec\u015fulescu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nec\u015fulescu et al\\.", "year": 2015}, {"title": "Adding semantics to data-driven paraphrasing", "author": ["Johan Bos", "Malvina Nissim", "Charley Beller", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M Marlin"], "venue": null, "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Katrin Erk", "Gemma Boleda"], "venue": "In COLING,", "citeRegEx": "Roller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Unsupervised antonymsynonym discrimination in vector space", "author": ["Santus et al.2014] Enrico Santus", "Qin Lu", "Alessandro Lenci", "Churen Huang"], "venue": null, "citeRegEx": "Santus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Evalution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models", "author": ["Santus et al.2015] Enrico Santus", "Frances Yung", "Alessandro Lenci", "Chu-Ren Huang"], "venue": null, "citeRegEx": "Santus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2015}, {"title": "Nine features in a random forest to learn taxonomical semantic relations", "author": ["Santus et al.2016] Enrico Santus", "Alessandro Lenci", "Tin-Shing Chiu", "Qin Lu", "Chu-Ren Huang"], "venue": null, "citeRegEx": "Santus et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2016}, {"title": "Improving hypernymy detection with an integrated path-based and distributional method", "author": ["Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Shwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shwartz et al\\.", "year": 2016}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Snow et al.2004] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": null, "citeRegEx": "Snow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2004}, {"title": "Conceptnet 5: A large semantic network for relational knowledge", "author": ["Speer", "Havasi2013] Robert Speer", "Catherine Havasi"], "venue": "In The Peoples Web Meets NLP,", "citeRegEx": "Speer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Speer et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations.", "startOffset": 84, "endOffset": 106}, {"referenceID": 17, "context": "Automated methods to recognize the lexical semantic relation between terms are valuable for NLP applications, and several datasets have been developed for training and evaluating such methods (e.g. Baroni and Lenci, 2011; Santus et al., 2015).", "startOffset": 192, "endOffset": 242}, {"referenceID": 3, "context": "Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013).", "startOffset": 179, "endOffset": 257}, {"referenceID": 20, "context": "Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013).", "startOffset": 179, "endOffset": 257}, {"referenceID": 10, "context": "Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013).", "startOffset": 179, "endOffset": 257}, {"referenceID": 14, "context": "Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013).", "startOffset": 179, "endOffset": 257}, {"referenceID": 7, "context": "Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term.", "startOffset": 129, "endOffset": 176}, {"referenceID": 13, "context": "Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term.", "startOffset": 129, "endOffset": 176}, {"referenceID": 1, "context": "These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), and consistently outperformed other methods (Santus et al.", "startOffset": 87, "endOffset": 129}, {"referenceID": 15, "context": "These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), and consistently outperformed other methods (Santus et al.", "startOffset": 87, "endOffset": 129}, {"referenceID": 18, "context": ", 2014), and consistently outperformed other methods (Santus et al., 2016; Nec\u015fulescu et al., 2015).", "startOffset": 53, "endOffset": 99}, {"referenceID": 11, "context": ", 2014), and consistently outperformed other methods (Santus et al., 2016; Nec\u015fulescu et al., 2015).", "startOffset": 53, "endOffset": 99}, {"referenceID": 0, "context": "These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), and consistently outperformed other methods (Santus et al., 2016; Nec\u015fulescu et al., 2015). While these two sources have been considered complementary, recent results seemed to suggest that path-based methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, yielding notably improved results on a new dataset.", "startOffset": 88, "endOffset": 441}, {"referenceID": 0, "context": "These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), and consistently outperformed other methods (Santus et al., 2016; Nec\u015fulescu et al., 2015). While these two sources have been considered complementary, recent results seemed to suggest that path-based methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, yielding notably improved results on a new dataset. In this paper, we investigate the extension of Shwartz et al.\u2019s (2016) method to recognize multiple semantic relations, and reexamine their findings over common datasets for the broader task.", "startOffset": 88, "endOffset": 706}, {"referenceID": 19, "context": "Recently, Shwartz et al. (2016) introduced HypeNET, a hypernymy detection method based on the integration of the best-performing distributional method for the task with a novel neural path representation, improving upon state-of-the-art methods.", "startOffset": 10, "endOffset": 32}, {"referenceID": 19, "context": "Shwartz et al. (2016) showed that this new path representation, when used on its own, outperforms prior path-based methods for hypernymy detection.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Distributional We train an SVM classifier on the concatenation of x and y\u2019s word embeddings [~vwx , ~vwy ] (Baroni et al., 2012) (DS).", "startOffset": 107, "endOffset": 128}, {"referenceID": 6, "context": "2 Prior work suggested that such a linear classifier is incapable of capturing interactions between x and y\u2019s features, and that instead it learns separate properties of x and y (Levy et al., 2015), e.", "startOffset": 178, "endOffset": 197}, {"referenceID": 0, "context": "Distributional We train an SVM classifier on the concatenation of x and y\u2019s word embeddings [~vwx , ~vwy ] (Baroni et al., 2012) (DS). 2 Prior work suggested that such a linear classifier is incapable of capturing interactions between x and y\u2019s features, and that instead it learns separate properties of x and y (Levy et al., 2015), e.g. by memorizing that animal is a prototypical hypernym. To examine the effect of non-linear expressive power on the model, we experiment with a neural network with a single hidden layer trained on [~vwx , ~vwy ] (DSh). This was previously done by Bowman et al. (2014), with promising results, but on a small artificial vocabulary.", "startOffset": 108, "endOffset": 605}, {"referenceID": 19, "context": "3 The technical details of our network are identical to Shwartz et al. (2016). We experimented also with difference ~vwx \u2212~vwy and some other classifiers, and chose concatenation as it yielded the best performance on the validation sets.", "startOffset": 56, "endOffset": 78}, {"referenceID": 11, "context": "We use four common semantic relation datasets, detailed in Table 1: K&H+N ((Nec\u015fulescu et al., 2015), an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al.", "startOffset": 75, "endOffset": 100}, {"referenceID": 17, "context": ", 2015), an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), and ROOT09 (Santus et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 18, "context": ", 2015), and ROOT09 (Santus et al., 2016).", "startOffset": 20, "endOffset": 41}, {"referenceID": 6, "context": "To prevent a possible lexical memorization effect (Levy et al., 2015), Santus et al.", "startOffset": 50, "endOffset": 69}, {"referenceID": 10, "context": "We use four common semantic relation datasets, detailed in Table 1: K&H+N ((Nec\u015fulescu et al., 2015), an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al.", "startOffset": 76, "endOffset": 143}, {"referenceID": 6, "context": "To prevent a possible lexical memorization effect (Levy et al., 2015), Santus et al. (2016) added negative switched hyponym-hypernym pairs (e.", "startOffset": 51, "endOffset": 92}, {"referenceID": 19, "context": "Like Shwartz et al. (2016), we tuned the hyper-parameters of all methods on the validation set of each dataset, and used Wikipedia as corpus.", "startOffset": 5, "endOffset": 27}, {"referenceID": 6, "context": "the memorization effect identified by Levy et al. (2015). This is mostly frequent in ROOT09 (405 pairs, which is 72%), where the switched hypernym pairs make the model less able to predict the relation according to the label frequency of a single term.", "startOffset": 38, "endOffset": 57}, {"referenceID": 8, "context": "6 The confusion matrices (Figure 1) identify the relations recognized by each information source, reassessing prior findings (Mirkin et al., 2006).", "startOffset": 125, "endOffset": 146}, {"referenceID": 8, "context": "6 The confusion matrices (Figure 1) identify the relations recognized by each information source, reassessing prior findings (Mirkin et al., 2006). PB is unsurprisingly weak in recognizing synonyms, which do not tend to co-occur. It also performed badly on co-hyponyms and antonyms. Using the analysis procedure of Shwartz et al. (2016), we found that co-hyponymy indicating paths include many lists (e.", "startOffset": 126, "endOffset": 337}, {"referenceID": 16, "context": "It seems worthwhile to try improving the model with insights from prior work on these specific relations (Santus et al., 2014; Mohammad et al., 2013) or by using additional information sources, like multilingual data (Pavlick et al.", "startOffset": 105, "endOffset": 149}, {"referenceID": 9, "context": "It seems worthwhile to try improving the model with insights from prior work on these specific relations (Santus et al., 2014; Mohammad et al., 2013) or by using additional information sources, like multilingual data (Pavlick et al.", "startOffset": 105, "endOffset": 149}, {"referenceID": 12, "context": ", 2013) or by using additional information sources, like multilingual data (Pavlick et al., 2015).", "startOffset": 75, "endOffset": 97}, {"referenceID": 19, "context": "We presented an adaptation to HypeNET (Shwartz et al., 2016) that classifies term-pairs to one of multiple semantic relations.", "startOffset": 38, "endOffset": 60}], "year": 2017, "abstractText": "Recognizing various semantic relations between terms is crucial for many NLP tasks. While path-based and distributional information sources are considered complementary, the strong results the latter showed on recent datasets suggested that the former\u2019s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. We demonstrate that these two information sources are indeed complementary, and analyze the contributions of each source.", "creator": "LaTeX with hyperref package"}}}