{"id": "1608.05639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Operator-Valued Bochner Theorem, Fourier Feature Maps for Operator-Valued Kernels, and Vector-Valued Learning", "abstract": "This paper presents a framework for computing random operator-valued feature maps for operator-valued positive definite kernels. This is a generalization of the random Fourier features for scalar-valued kernels to the operator-valued case. Our general setting is that of operator-valued kernels corresponding to RKHS of functions with values in a Hilbert space. We show that in general, for a given kernel, there are potentially infinitely many random feature maps, which can be bounded or unbounded. Most importantly, given a kernel, we present a general, closed form formula for computing a corresponding probability measure, which is required for the construction of the Fourier features, and which, unlike the scalar case, is not uniquely and automatically determined by the kernel. We also show that, under appropriate conditions, random bounded feature maps can always be computed. Furthermore, we show the uniform convergence, under the Hilbert-Schmidt norm, of the resulting approximate kernel to the exact kernel on any compact subset of Euclidean space. Our convergence requires differentiable kernels, an improvement over the twice-differentiability requirement in previous work in the scalar setting. We then show how operator-valued feature maps and their approximations can be employed in a general vector-valued learning framework. The mathematical formulation is illustrated by numerical examples on matrix-valued kernels.", "histories": [["v1", "Fri, 19 Aug 2016 15:34:43 GMT  (48kb)", "http://arxiv.org/abs/1608.05639v1", "31 pages"]], "COMMENTS": "31 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ha quang minh"], "accepted": false, "id": "1608.05639"}, "pdf": {"name": "1608.05639.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["minh.haquang@iit.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n05 63\n9v 1\n[ cs\n.L G\n] 1\n9 A"}, {"heading": "1. Introduction", "text": "The current work is concerned with the construction of random feature maps for operatorvalued kernels and their applications in vector-valued learning. Much work has been done in machine learning recently on these kernels and their associated RKHS of vectorvalued functions, both theoretically and practically, see e.g. (Micchelli and Pontil, 2005; Carmeli et al., 2006; Reisert and Burkhardt, 2007; Caponnetto et al., 2008; Brouard et al., 2011; Dinuzzo et al., 2011; Kadri et al., 2011; Minh and Sindhwani, 2011; Zhang et al., 2012; Sindhwani et al., 2013). While rich in theory and potentially powerful in applications, one of the main challenges in applying operator-valued kernels is that they are computationally intensive on large datasets. In the scalar setting, one of the most powerful approaches for scaling up kernel methods is Random Fourier Features (Rahimi and Recht, 2007), which applies Bochner\u2019s Theorem and the Inverse Fourier Transform to build random features that approximate a given shift-invariant kernel. The approach in (Rahimi and Recht, 2007) has been improved both in terms of computational speed (Le et al., 2013) and rates of convergence (Sutherland and Schneider, 2015; Sriperumbudur and Szabo\u0301, 2015).\nc\u00a9201x Ha\u0300 Quang Minh.\nOur contributions. The following are the contributions of this work.\n1. Firstly, we construct random feature maps for operator-valued shift-invariant kernels using the operator-valued version of Bochner\u2019s Theorem. The key differences between the operator-valued and scalar settings are the following. The first key difference is that, in the scalar setting, a positive definite function k, with normalization, is the Fourier transform of a probability measure \u03c1, which is uniquely determined as the inverse Fourier transform of k. In the operator-valued setting, k is the Fourier transform of a unique finite positive operator-valued measure \u00b5. However, the probability measure \u03c1, which is necessary for constructing the random feature maps, must be explicitly constructed, that is it is not automatically determined by k. In this work, we present a general formula for computing a probability measure \u03c1 given a kernel k. The second key difference is that, in the operator-valued setting, the probability measure \u03c1 is generally non-unique, being a factor of \u00b5. As a consequence, we show that in general, there are (potentially infinitely) many random feature maps, which may be either unbounded or bounded. However, under appropriate assumptions, we show that there always exist bounded feature maps. This is true for many of the commonly encountered kernels, including separable kernels and curl-free and divergence-free kernels.\n2. Secondly, for the bounded feature maps, we show that the associated approximate kernel converges uniformly to the exact kernel in Hilbert-Schmidt norm on any compact subset in Euclidean space.\n3. Thirdly, when restricting to the scalar setting, our convergence holds for differentiable kernels, which is an improvement over the hypothesis of (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabo\u0301, 2015; Brault et al., 2016), which all require the kernels to be twice-differentiable.\n4. Fourthly, we show how operator-valued feature maps and their approximations can be used directly in a general learning formulation in RKHS.\nRelated work. The work most closely related to our present work is (Brault et al., 2016). While the formal constructions of the Fourier feature maps in (Brault et al., 2016) and our work are similar, there are several crucial differences. The first and most important difference is that in (Brault et al., 2016) there is no general mechanism for computing a probability measure \u03c1, which is required for the construction of the Fourier feature maps. As such, the results presented in (Brault et al., 2016) are only for three specific kernels, namely separable kernels, curl-free and div-free kernels, not for a general kernel as in our setting. Moreover, for the curl-free and div-free kernels, (Brault et al., 2016) presented unbounded feature maps, whereas we show that, apart from unbounded feature maps, there are generally infinitely many bounded feature maps associated with these kernels. Secondly, more general than the matrix-valued kernel, i.e finite-dimensional, setting in (Brault et al., 2016), we work in the operator-valued kernel setting, with RKHS of functions with values in a Hilbert space. In this setting, the convergence in the Hilbert-Schmidt norm that we present is strictly stronger than the convergence in spectral norm given in (Brault et al., 2016). At the same time, our convergence requires weaker assumptions than\nthose in (Brault et al., 2016) and previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabo\u0301, 2015).\nOrganization. We first briefly review random Fourier features and operator-valued kernels in Section 2. Feature maps for operator-valued kernels are described in Section 2.1. The core of the paper is Section 3, which describes the construction of random feature maps using operator-valued Bochner\u2019s Theorem, the computation of the required probability measure, along with the uniform convergence of the corresponding approximate kernels. Section 4 employs feature maps and their approximations in a general vector-valued learning formulation, with the accompanying experiments in Section 5. All mathematical proofs are given in Appendix A."}, {"heading": "2. Background", "text": "Throughout the paper, we work with shift-invariant positive definite kernels K on Rn\u00d7Rn, so that K(x, t) = k(x\u2212 t)\u2200x, t \u2208 Rn for some function k : Rn \u2192 R, which is then said to be a positive definite function on Rn.\nRandom Fourier features for scalar-valued kernels (Rahimi and Recht, 2007). Bochner\u2019s Theorem in the scalar setting, see e.g. (Reed and Simon, 1975), states that a complex-valued, continuous function k on Rn is positive definite if and only if it is the Fourier transform of a finite, positive measure \u00b5 on Rn, that is\nk(x) = \u00b5\u0302(x) =\n\u222b\nRn e\u2212i\u3008\u03c9,x\u3009d\u00b5(\u03c9). (1)\nFor our purposes, we consider exclusively the real-valued setting for k. Since \u00b5 is a finite positive measure, without loss of generality, we assume that \u00b5 is a probability measure, so that k(x) = E\u00b5[e \u2212i\u3008\u03c9,x\u3009]. The measure \u00b5 is uniquely determined via \u00b5\u0302 = k. For the Gaussian function k(x) = e\u2212 ||x||2 \u03c32 , we have \u00b5(\u03c9) = (\u03c3 \u221a \u03c0)n\n(2\u03c0)n e \u2212\u03c3\n2||\u03c9||2 4 \u223c N ( 0, 2\u03c32 ) . Consider now the\nkernel K(x, t) = k(x\u2212 t) = \u222b Rn\ne\u2212i\u3008\u03c9,x\u2212t\u3009d\u00b5(\u03c9). Using the symmetry of K and the relation 1 2(e ix + e\u2212ix) = cos(x), we obtain\nK(x, t) = 1\n2\n\u222b\nRn [ei\u3008\u03c9,x\u2212t\u3009 + e\u2212i\u3008\u03c9,x\u2212t\u3009]d\u00b5(\u03c9) =\n\u222b\nRn cos(\u3008\u03c9, x\u2212 t\u3009)d\u00b5(\u03c9). (2)\nLet {\u03c9j}Dj=1 be points in Rn, independently sampled according to the measure \u00b5. Then we have an empirical approximation K\u0302D of K and the associated feature map \u03a6\u0302D : R\nn \u2192 R2D, as follows\nK\u0302D(x, t) = 1\nD\nD\u2211\nj=1\ncos(\u3008\u03c9j , x\u2212 t\u3009) = 1\nD\nD\u2211\nj=1\n[cos(\u3008\u03c9j , x\u3009) cos(\u3008\u03c9j , t\u3009) + sin(\u3008\u03c9j , x\u3009) sin(\u3008\u03c9j , t\u3009)\n= \u3008\u03a6\u0302D(x), \u03a6\u0302D(t)\u3009, where \u03a6\u0302D(x) = (cos(\u3008\u03c9j , x\u3009), sin(\u3008\u03c9j , x\u3009))Dj=1 \u2208 R2D. (3)\nThe current work generalizes the feature map \u03a6\u0302D above to the case K is an operator-valued kernel and the corresponding \u00b5 is a positive operator-valued measure.\nVector-valued RKHS. Let us now briefly recall operator-valued kernels and their corresponding RKHS of vector-valued functions, for more detail see e.g. (Carmeli et al.,\n2006; Micchelli and Pontil, 2005; Caponnetto et al., 2008; Minh and Sindhwani, 2011). Let X be a nonempty set, W a real, separable Hilbert space with inner product \u3008\u00b7, \u00b7\u3009W , L(W) the Banach space of bounded linear operators on W. Let WX denote the vector space of all functions f : X \u2192 W. A function K : X \u00d7 X \u2192 L(W) is said to be an operator-valued positive definite kernel if for each pair (x, t) \u2208 X \u00d7X , K(x, t)\u2217 = K(t, x), and for every set of points {xi}Ni=1 in X and {wi}Ni=1 in W, N \u2208 N, \u2211N i,j=1\u3008wi,K(xi, xj)wj\u3009W \u2265 0. For x \u2208 X and w \u2208 W, form a function Kxw = K(., x)w \u2208 WX by\n(Kxw)(t) = K(t, x)w \u2200t \u2208 X . (4)\nConsider the set H0 = span{Kxw|x \u2208 X , w \u2208 W} \u2282 WX . For f = \u2211N\ni=1Kxiwi, g =\u2211N i=1Kziyi \u2208 H0, we define the inner product \u3008f, g\u3009HK = \u2211N i,j=1\u3008wi,K(xi, zj)yj\u3009W , which makes H0 a pre-Hilbert space. Completing H0 by adding the limits of all Cauchy sequences gives the Hilbert space HK . This is the reproducing kernel Hilbert space (RKHS) of Wvalued functions on X . The reproducing property is\n\u3008f(x), y\u3009W = \u3008f,Kxy\u3009HK for all f \u2208 HK . (5)"}, {"heading": "2.1 Operator-Valued Feature Maps for Operator-Valued Kernels", "text": "Feature maps for operator-valued kernels were first considered in (Caponnetto et al., 2008). Let FK be a separable Hilbert space and L(W,FK) be the Banach space of all bounded linear operators mapping from W to FK . A feature map for K with corresponding feature space FK is a mapping\n\u03a6K : X \u2192 L(W,FK), such that K(x, t) = \u03a6K(x)\u2217\u03a6K(t) \u2200(x, t) \u2208 X \u00d7 X . (6)\nThe operator-valued map \u03a6K is generally nonlinear as a function on X . For each x \u2208 X , \u03a6K(x) \u2208 L(W,FK) and\n\u3008w,K(x, t)w\u3009W = \u3008w,\u03a6K(x)\u2217\u03a6K(t)\u3009W = \u3008\u03a6K(x)w,\u03a6K(t)w\u3009FK . (7)\nIn the following, for brevity, we also refer to the pair (\u03a6K ,FK) as a feature map for K. Existence of operator-valued feature maps and the canonical feature map. Let K be any operator-valued positive definite kernel on X \u00d7 X , we now show that then there always exists at least one feature map, as follows. For each x \u2208 X , consider the linear operator Kx : W \u2192 HK defined by Kxw(t) = K(t, x)w, x, t \u2208 X , as above. Then\n||Kxw||2HK = \u3008K(x, x)w,w\u3009W \u2264 ||K(x, x)|| ||w|| 2 W , (8)\nwhich implies that Kx is a bounded operator, with\n||Kx : W \u2192 HK || \u2264 \u221a ||K(x, x)||, (9)\nLet K\u2217x : HK \u2192 W be the adjoint operator for Kx. The reproducing property states that \u2200w \u2208 W,\n\u3008f(x), w\u3009W = \u3008f,Kxw\u3009HK = \u3008K\u2217xf,w\u3009W \u21d2 K\u2217xf = f(x). (10)\nFor any u, v \u2208 W, we have\n\u3008u,K(x, t)v\u3009W = \u3008u,Ktv(x)\u3009W = \u3008u,K\u2217xKtv\u3009W = \u3008Kxu,Ktv\u3009HK \u21d2 K(x, t) = K\u2217xKt, (11)\nfrom which it follows that\n\u03a6K : X \u2192 L(W,HK), \u03a6K(x) = Kx \u2208 L(W,HK) (12)\nis a feature map for K with feature space HK , which exists for any positive definite kernel K. Following the terminology in the scalar setting (Minh et al., 2006), we also call it the canonical feature map for K.\nRemark 1 In (Caponnetto et al., 2008), it is assumed that the kernel has the representation K(x, t) = \u03a6K(x)\n\u2217\u03a6K(t). However, as we have just shown, for any positive definite kernel K, there is always at least one such representation, given by the canonical feature map above.\nSimilar to the scalar setting (Minh et al., 2006), feature maps are generally non-unique, as we show below. However, they are all essentially equivalent, similar to the scalar case, as shown by the following.\nLemma 2 Let (\u03a6K ,FK) be any feature map for K. Then \u2200f \u2208 HK , there exists an h \u2208 FK such that\nf(x) = K\u2217xf = \u03a6K(x) \u2217h, \u2200x \u2208 X . (13)\nFurthermore, ||f ||HK = ||h||FK ."}, {"heading": "3. Random Operator-Valued Feature Maps", "text": "We now present the generalization of the random Fourier feature map from the scalar setting to the operator-valued setting. We begin by reviewing Bochner\u2019s Theorem in the operatorvalued setting in Section 3.1, which immediately leads to the formal construction of the Fourier feature maps in Section 3.2. As we stated, in the operator-valued setting, we need to explicitly construct the required probability measure. This is done individually for some specific kernels in Section 3.3 and for a general kernel in Section 3.4."}, {"heading": "3.1 Operator-Valued Bochner Theorem", "text": "The operator-valued version of Bochner\u2019s Theorem that we present here is from (Neeb, 1998), see also (Falb, 1969; Carmeli et al., 2010). Throughout this section, let H be a separable Hilbert space. Let L(H) denote the Banach space of bounded linear operators on H, Sym(H) \u2282 L(H) denote the subspace of bounded, self-adjoint operators on H, and Sym+(H) \u2282 Sym(H) denote the set of self-adjoint, bounded, positive operators on H. An operator A \u2208 L(H) is said to be trace class, denoted by A \u2208 Tr(H), if\u2211\u221e\nk=1\u3008ek, (A\u2217A)1/2ek\u3009 < \u221e for any orthonormal basis {ek}\u221ek=1 in H. If A \u2208 Tr(H), then the trace of A is tr(A) = \u2211\u221e k=1\u3008ek, Aek\u3009, which is independent of the orthonormal basis.\nPositive operator-valued measures. Let (X ,\u03a3) be a measurable space, where X is a non-empty set and \u03a3 is a \u03c3-algebra of subsets of X . A Sym+(H)-valued measure \u00b5 is a\ncountably additive1 function \u00b5 : \u03a3 \u2192 Sym+(H), with \u00b5(\u2205) = 0, so that for any sequence of pairwise disjoint subsets {Aj}\u221ej=1 in \u03a3,\n\u00b5(\u222a\u221ej=1Aj) = \u221e\u2211\nj=1\n\u00b5(Aj), which converges in the operator norm on L(H). (14)\nTo state Bochner\u2019s Theorem for operator-valued measures, we need the notions of finite Sym+(H)-valued measure and ultraweak continuity. Let X = Rn (a locally compact space in general). A finite Sym+(H)-valued Radon measure is a Sym+(H)-valued measure such that for any operator A \u2208 Sym+(H) \u2229Tr(H), the scalar measure\n\u00b5A : \u03a3 \u2192 R+, \u00b5A(B) = tr(A\u00b5(B)), B \u2208 \u03a3, (15)\nis a finite positive Radon measure on Rn. A function k : Rn \u2192 L(H) is said to be ultraweakly continuous if for each operator A \u2208 Tr(H), the following scalar function is continuous\nkA : R n \u2192 R, kA(x) = tr(Ak(x)). (16)\nThe following is then the generalization of Bochner\u2019s Theorem to the vector-valued setting.\nTheorem 3 (Operator-valued Bochner Theorem (Neeb, 1998)) An ultraweakly continuous function k : Rn \u2192 L(H) is positive definite if and only if there exists a finite Sym+(H)-valued measure \u00b5 on Rn such that\nk(x) = \u00b5\u0302(x) =\n\u222b\nRn exp(i\u3008\u03c9, x\u3009)d\u00b5(\u03c9) =\n\u222b\nRn exp(\u2212i\u3008\u03c9, x\u3009)d\u00b5(\u03c9). (17)\nThe Radon measure \u00b5 is uniquely determined by \u00b5\u0302 = K.\nGeneral case. The above version of Bochner\u2019s Theorem holds in a much more general setting, where Rn is replaced by a locally compact abelian group G. For the general version, we refer to (Neeb, 1998).\nDetermining \u00b5 from k. In order to compute feature maps using Bochner\u2019s Theorem, we need to compute \u00b5 from the given operator-valued function k. Suppose that the density function \u00b5(\u03c9) of \u00b5 with respect to the Lebesgue measure on Rn exists. Let {ej}\u221ej=1 be any orthonormal basis for H. For any vector a = \u2211\u221ej=1 ajej \u2208 H, we have\n\u00b5(\u03c9)a = \u221e\u2211\nj=1\n\u3008ej , \u00b5(\u03c9)a\u3009ej = \u221e\u2211\nj,l=1\nal\u3008ej , \u00b5(\u03c9)el\u3009ej .\nThus \u00b5(\u03c9) is completely determined by the infinite matrix of inner products (\u3008ej , \u00b5(\u03c9)el\u3009)\u221ej,l=1, which can be computed from k via the inverse Fourier transform F\u22121 as follows.\n1. Falb (Falb, 1969) used weakly countably additive vector measures, which are in fact countably additive (Diestel, 1984).\nProposition 4 Assume that \u3008ej , k(x)el\u3009 \u2208 L1(Rn) \u2200j, l \u2208 N. Then the density function \u00b5(\u03c9) of \u00b5 with respect to the Lebesgue measure on Rn exists and is given by\n\u3008ej, \u00b5(\u03c9)el\u3009 = F\u22121[\u3008ej , k(x)el\u3009]. (18)\nThe positive definite function k gives rise to the shift-invariant positive definite kernel\nK(x, t) = k(x\u2212 t) = \u222b\nRn exp(\u2212i\u3008\u03c9, x\u2212 t\u3009)d\u00b5(\u03c9). (19)\nSimilar to the scalar case, using the property K(x, t) = K(t, x)\u2217 and the symmetry of \u00b5, we obtain\nK(x, t) =\n\u222b\nRn cos(\u3008\u03c9, x\u2212 t\u3009)d\u00b5(\u03c9). (20)\nIn order to generalize the random Fourier feature approach to the operator-valued kernel K(x, t) we need to construct a probability measure \u03c1 on Rn such that K(x, t) is the expectation of an operator-valued random variable with respect to \u03c1. Equivalently, we need to factorize the density \u00b5(\u03c9) as\n\u00b5(\u03c9) = \u00b5\u0303(\u03c9)\u03c1(\u03c9), (21)\nwhere \u00b5\u0303(\u03c9) is a finite Sym+(H)-valued function on Rn and \u03c1(\u03c9) is the density function of the probability measure \u03c1.\nRemark 5 Throughout the rest of the paper, we assume that k satisfies the assumptions of Proposition 4. We then identify the measures \u00b5 and \u03c1 by their density functions \u00b5(\u03c9) and \u03c1(\u03c9), respectively, with respect to the Lebesgue measure.\nKey differences between the scalar and operator-valued settings. Before proceeding with the probability measure and feature map construction, we point out two key differences between the scalar and operator-valued settings.\n1. In the scalar setting, with normalization, \u00b5 is a probability measure uniquely determined via \u00b5\u0302 = k. In the operator-valued setting, the operator-valued measure \u00b5 is also uniquely determined by k, as stated in Proposition 4. However, the probability measure \u03c1 in Eq. (21) needs to be explicitly constructed, that is it is not automatically determined by k. We present a general formula for computing \u03c1 in Section 3.4.\n2. The factorization stated in Eq. (21) is generally non-unique. As we show below, in general, there are many (in fact, potentially infinitely many) pairs (\u00b5\u0303, \u03c1) such that Eq. (21) holds. Thus there are generally (infinitely) many operator-valued feature maps corresponding to the operator-valued version of Bochner\u2019s Theorem. We illustrate this property via examples in Sections 3.2 and 3.4 below."}, {"heading": "3.2 Formal Construction of Approximate Fourier Feature Maps", "text": "Assuming for the moment that we have a pair (\u00b5\u0303, \u03c1) satisfying the factorization in Eq. (21), then Eq. (20) takes the form\nK(x, t) =\n\u222b\nRn cos(\u3008\u03c9, x\u2212 t\u3009)\u00b5\u0303(\u03c9)d\u03c1(\u03c9) = E\u03c1[cos(\u3008\u03c9, x\u2212 t\u3009)\u00b5\u0303(\u03c9)]. (22)\nLet {\u03c9j}Dj=1, D \u2208 N, be D points in Rn randomly sampled independently from \u03c1. Then K(x, t) can be approximated by by the empirical sum\nK\u0302D(x, t) = k\u0302D(x\u2212 t) = 1\nD\nD\u2211\nl=1\ncos(\u3008\u03c9l, x\u2212 t\u3009)\u00b5\u0303(\u03c9l)\n= 1\nD\nD\u2211\nl=1\n[cos(\u3008\u03c9l, x\u3009) cos(\u3008\u03c9l, t)]\u00b5\u0303(\u03c9l) + 1\nD\nD\u2211\nl=1\nsin(\u3008\u03c9l, x\u3009) sin(\u3008\u03c9l, t\u3009)]\u00b5\u0303(\u03c9l). (23)\nLet F be a separable Hilbert space and \u03c8 : Rn \u2192 L(H,F) be such that\n\u00b5\u0303(\u03c9) = \u03c8(\u03c9)\u2217\u03c8(\u03c9), \u03c8(\u03c9) : H \u2192 F (24)\nSuch a pair (\u03c8,F) always exists, with one example being F = H and \u03c8(\u03c9) = \u221a \u00b5\u0303(\u03c9).\nRemark 6 As we demonstrate via the examples below, the decomposition \u00b5\u0303(\u03c9) = \u03c8(\u03c9)\u2217\u03c8(\u03c9) is also generally non-unique, which is another reason for the non-uniqueness of the approximate feature maps.\nOperator-valued Fourier feature map. The decompositions for K\u0302D in Eqs. (23) and (24) immediately give us the following approximate feature map\n\u03a6\u0302D(x) = 1\u221a D   cos(\u3008\u03c91, x\u3009)\u03c8(\u03c91) sin(\u3008\u03c91, x\u3009)\u03c8(\u03c91)\n\u00b7 \u00b7 \u00b7 cos(\u3008\u03c9D, x\u3009)\u03c8(\u03c9D) sin(\u3008\u03c9D, x\u3009)\u03c8(\u03c9D)\n  : H \u2192 F2D. (25)\nwith\nKD(x, t) = [\u03a6\u0302D(x)] \u2217[\u03a6\u0302D(t)]. (26)\nSpecial cases. For H = R, we have \u00b5\u0303 = 1 (assuming normalization) and \u03c1 = \u00b5, and we thus recover the Fourier features in the scalar setting. For H = Rd, for some d \u2208 N, we obtain the feature map in (Brault et al., 2016)."}, {"heading": "3.3 Probability Measure and Feature Map Construction in Some Special Cases", "text": "We first consider several examples of operator-valued kernels arising from scalar-valued kernels. For these examples, both the Sym+(H)-valued measure \u00b5 and the probability measure \u03c1 can be derived from the corresponding probability measure for the scalar kernels.\nThese examples have also been considered by (Brault et al., 2016), however we treat them in greater depth here, particularly the curl-free and div-free kernels (see detail below). One important aspect that we note is that the approach for computing the probability measure \u03c1 in this section is specific for each kernel and does not generalize to a general kernel. We return to these examples in the general setting of Section 3.4, where we present a general formula for computing \u03c1 for a general kernel k.\nExample 1 (Separable kernels)\nConsider the simplest case, where the operator-valued positive definite function k has the form\nk(x) = g(x)A, (27)\nwhere A \u2208 Sym+(H) and g : Rn \u2192 R is a scalar-valued positive definite function. Let \u03c10 be the probability measure on Rn such that g(x) = E\u03c10 [e \u2212i\u3008\u03c9,x\u3009]. It follows immediately that\nk(x) =\n\u222b\nRn e\u2212i\u3008\u03c9,x\u3009d\u00b5(\u03c9) where \u00b5(\u03c9) = A\u03c10(\u03c9). (28)\nThus we can set\n\u00b5\u0303(\u03c9) = A, \u03c1 = \u03c10. (29)\nFor the operator \u03c8(\u03c9) in Eq. (24), we can set either\n\u03c8(\u03c9) = \u221a A, (30)\nor, if A is a symmetric positive definite matrix, we can also compute \u03c8 via the Cholesky decomposition of A by setting\n\u03c8(\u03c9) = U, where A = UTU, (31)\nwith U being an upper triangular matrix. Thus in this case, with the probability measure \u03c1 = \u03c10, there are at least two choices for the feature map \u03a6\u0302D, each resulting from one choice of \u03c8(\u03c9) as discussed above. In practice, a particular \u03c8 should be chosen based on its computational complexity, which in turn depends on the structure of A itself.\nExample 2 (Curl-free and divergence-free kernels)\nConsider next the matrix-valued curl-free and divergence kernels in (Fuselier, 2006). In (Brault et al., 2016), the authors present what we call the unbounded feature maps below for these kernels, without, however, the analytical expression for the feature map of the div-free kernel. We now present the analytical expressions for the feature maps for both these kernels. More importantly, we show that, apart from the unbounded feature maps, there are generally infinitely many bounded feature maps associated with these kernels.\nLet \u03c6 be a scalar-valued twice-differentiable positive definite function on Rn. Let \u2207 denote the n\u00d7 1 gradient operator and \u2206 = \u2207T\u2207 denote the Laplacian operator. Define\nkdiv = (\u2212\u2206In +\u2207\u2207T )\u03c6, kcurl = \u2212\u2207\u2207T\u03c6. (32)\nThen kdiv and kcurl are n \u00d7 n matrices, whose columns are divergence-free and curl-free functions, respectively. The functions kcurl and kdiv give rise to the corresponding positive definite kernels\nKcurl(x, t) = kcurl(x\u2212 t), and Kdiv(x, t) = kdiv(x\u2212 t).\nFor the Gaussian case \u03c6(x) = exp(\u2212 ||x||2 \u03c32 ), the functions kcurl and kdiv are given by\nkcurl(x) = 2 \u03c32 exp(\u2212||x|| 2 \u03c32 )[In \u2212 2 \u03c32 xxT ]. (33)\nkdiv(x) = 2 \u03c32 exp(\u2212||x|| 2 \u03c32 )[((n \u2212 1)\u2212 2 \u03c32 ||x||2)In + 2 \u03c32 xxT ]. (34)\nLemma 7 Let \u03c10 be the probability measure on R n such that \u03c6(x) = E\u03c10 [e \u2212i\u3008\u03c9,x\u3009] = \u03c1\u03020(x). Then, under the condition \u222b Rn ||\u03c9||2d\u03c10(\u03c9) < \u221e, we have\nkcurl(x) =\n\u222b\nRn e\u2212i\u3008\u03c9,x\u3009\u03c9\u03c9T\u03c10(\u03c9)d\u03c9 =\n\u222b\nRn e\u2212i\u3008\u03c9,x\u3009(\u00b5curl)(\u03c9)d\u03c9. (35)\nkdiv(x) =\n\u222b\nRn e\u2212i\u3008\u03c9,x\u3009[||\u03c9||2In \u2212 \u03c9\u03c9T ]\u03c10(\u03c9)d\u03c9 =\n\u222b\nRn e\u2212i\u3008\u03c9,x\u3009(\u00b5div)(\u03c9)d\u03c9, (36)\nwhere \u00b5curl(\u03c9) = \u03c9\u03c9 T\u03c10(\u03c9) and \u00b5div(\u03c9) = [||\u03c9||2In \u2212 \u03c9\u03c9T ]\u03c10(\u03c9). The condition \u222b Rn\n||\u03c9||2d\u03c10(\u03c9) < \u221e in Lemma 7 guarantees that \u03c6 is twice-differentiable, which is the underlying assumption for curl-free and divergence-free kernels.\nUnbounded feature maps. Consider first the curl-free kernel. From the expression \u00b5curl(\u03c9) = \u03c9\u03c9 T\u03c10(\u03c9), we immediately see that for the factorization in Eq. (21), we can set\n\u00b5curl(\u03c9) = \u00b5\u0303(\u03c9)\u03c1(\u03c9), with \u00b5\u0303(\u03c9) = \u03c9\u03c9 T , \u03c1 = \u03c10.\nFor the Gaussian case, \u03c10(\u03c9) = (\u03c3\n\u221a \u03c0)n\n(2\u03c0)n e \u2212\u03c3\n2||\u03c9||2\n4 \u223c N (0, 2 \u03c32 In). In Eq. (24), we can set\n\u03c8(\u03c9) = \u03c9T , so that \u03a6\u0302D(x) is a matrix of size 2D \u00d7 n. (37)\nWe can also set\n\u03c8(\u03c9) = \u221a \u03c9\u03c9T = \u03c9\u03c9T\n||\u03c9|| , so that \u03a6\u0302D(x) is a matrix of size 2Dn\u00d7 n. (38)\nClearly the choice for \u03c8(\u03c9) in Eq. (37) is preferable computationally to that in Eq. (38). One thing that can be observed immediately is that both \u00b5curl and \u03c8 are unbounded functions of \u03c9, which complicates the convergence analysis of the corresponding kernel approximation (see Section 3.5 for further discussion).\nBounded feature maps. The unbounded feature maps above correspond to one particular choice of the probability measure \u03c1, namely \u03c1 = \u03c10. However, this is not the only valid choice for \u03c1. We now exhibit another choice for \u03c1 that results in a bounded feature\nmap, whose convergence behavior is much simpler to analyze. Consider the Gaussian case, with \u03c10 as given above. Clearly, we can choose for another factorization of \u00b5curl the factors\n\u00b5\u0303(\u03c9) = \u03c9\u03c9T e\u2212 \u03c32||\u03c9||2 8 2n/2, \u03c1(\u03c9) = 1 2n/2 (\u03c3 \u221a \u03c0)n (2\u03c0)n e\u2212 \u03c32||\u03c9||2 8 \u223c N (0, 4 \u03c32 In). (39)\nThen \u00b5\u0303(\u03c9) is a bounded function of \u03c9, with the corresponding bounded map\n\u03c8(\u03c9) = \u03c9T e\u2212 \u03c32||\u03c9||2 16 2n/4. (40)\nFor the divergence-free kernel, we have \u221a ||\u03c9||2In \u2212 \u03c9\u03c9T = ||\u03c9||In \u2212 \u03c9\u03c9 T\n||\u03c9|| , giving the corresponding maps\n\u03c8(\u03c9) = (||\u03c9||In \u2212 \u03c9\u03c9T\n||\u03c9|| ) (unbounded feature map), (41)\n\u03c8(\u03c9) = (||\u03c9||In \u2212 \u03c9\u03c9T\n||\u03c9|| )e \u2212\u03c3\n2||\u03c9||2\n16 2n/4 (bounded feature map). (42)\nSince there are infinitely many ways to split the Gaussian function e\u2212 \u03c32||\u03c9||2\n4 into a product of two Gaussian functions, it follows that there are infinitely many bounded Fourier feature maps associated with both the curl-free and div-free kernels induced by the Gaussian kernel. We show below that, under appropriate conditions on k, bounded feature maps always exist."}, {"heading": "3.4 First Main Result: Probability Measure Construction in the General Case", "text": "For the separable and curl-free and div-free kernels, we obtain a probability measure \u03c1 directly from the corresponding scalar-valued kernels. We now show how to construct \u03c1 given a general k, under appropriate assumptions on k. Furthermore, we show that the corresponding feature map is bounded, in the sense that \u00b5\u0303(\u03c9) is a bounded function of \u03c9 (see the precise statement in Corollary 10).\nProposition 8 Let K : Rn\u00d7Rn \u2192 L(H) be an ultraweakly continuous shift-invariant positive definite kernel. Let \u00b5 be the unique finite Sym+(H)-valued measure satisfying Eq. (19). Then \u2200a \u2208 H, a 6= 0, the scalar-valued kernel defined by Ka(x, t) = \u3008a,K(x, t)a\u3009 is positive definite. Furthermore, there exists a unique finite positive Borel measure \u00b5a on R\nn such that Ka is the Fourier transform of \u00b5a, that is\nKa(x, t) =\n\u222b\nRn exp(\u2212i\u3008\u03c9, x\u2212 t\u3009)d\u00b5a(\u03c9). (43)\nThe measure \u00b5a is given by\n\u00b5a(\u03c9) = \u3008a, \u00b5(\u03c9)a\u3009, \u03c9 \u2208 Rn. (44)\nLet {ej}\u221ej=1 be any orthonormal basis for H. By Proposition 8, \u2200j \u2208 N, the scalar-valued kernel\nKjj(x, t) = Kej (x, t) = \u3008ej ,K(x, t)ej\u3009 (45)\nis positive definite and is the Fourier transform of the finite positive Borel measure\n\u00b5jj(\u03c9) = \u3008ej, \u00b5(\u03c9)ej\u3009, \u03c9 \u2208 Rn. (46)\nThe measures \u00b5jj, j \u2208 N, which depend on the choice of orthonormal basis {ej}j\u2208N, collectively give rise to the following measure, which is independent of {ej}j\u2208N.\nTheorem 9 (Probability Measure Construction) Assume that the positive definite function k in Bochner\u2019s Theorem satisfies: (i) k(x) \u2208 Tr(H) \u2200x \u2208 Rn, and (ii) \u222b Rn\n|tr[k(x)]|dx < \u221e. Then its corresponding finite Sym+(H)-valued measure \u00b5 satisfies\n\u00b5(\u03c9) \u2208 Tr(H) \u2200\u03c9 \u2208 Rn, tr[\u00b5(\u03c9)] \u2264 1 (2\u03c0)n\n\u222b\nRn |tr[k(x)]|dx. (47)\nThe following is a finite positive Borel measure on Rn\n\u00b5tr(\u03c9) = tr(\u00b5(\u03c9)) =\n\u221e\u2211\nj=1\n\u00b5jj(\u03c9) = 1\n(2\u03c0)n\n\u222b\nRn exp(i\u3008\u03c9, x\u3009)tr[k(x)]dx. (48)\nThe normalized measure \u00b5tr(\u03c9)tr[k(0)] is a probability measure on R n.\nSpecial case. For H = R, we obtain\n\u00b5tr(\u03c9) = 1\n(2\u03c0)n\n\u222b\nRn exp(i\u3008\u03c9, x\u3009)k(x)dx = \u00b5(\u03c9), (49)\nso that the scalar-setting is a special case of Theorem 9, as expected.\nCorollary 10 Under the hypothesis of Theorem 9, in Eq. (21) we can set\n\u03c1(\u03c9) = \u00b5tr(\u03c9)\ntr[k(0)] , \u00b5\u0303(\u03c9) =\n{ tr[k(0)] \u00b5(\u03c9)\u00b5tr(\u03c9) , \u00b5tr(\u03c9) > 0\n0, \u00b5tr(\u03c9) = 0. (50)\nThe function \u00b5\u0303 in Eq. (50) satisfies \u00b5\u0303(\u03c9) \u2208 Sym+(H) and has bounded trace, i.e.\n||\u00b5\u0303(\u03c9)||tr = tr[\u00b5\u0303(\u03c9)] \u2264 tr[k(0)] \u2200\u03c9 \u2208 Rn. (51)\nLet us now illustrate Theorem 9 and 10 on the separable kernels and curl-free and divfree kernels. We note that for the separable kernels, we obtain the same probability measure \u03c1 as in Section 3.3. However, for the curl-free and div-free kernels, we obtain a different probability measure compared to Section 3.3, which illustrates the non-uniqueness of \u03c1.\nExample 3 (Separable kernels)\nFor the separable kernels of the form k(x) = g(x)A, with A \u2208 Sym+(H) \u2229 Tr(H) and g \u2208 L1(Rn), we have\n\u00b5tr(\u03c9) = tr(A) 1\n(2\u03c0)n\n\u222b\nRn exp(i\u3008\u03c9, x\u3009)g(x)dx = tr(A)\u03c10(\u03c9). (52)\nSince tr[k(0)] = tr(A), we recover \u03c1(\u03c9) = \u03c10(\u03c9). Here \u00b5\u0303(\u03c9) = A and ||\u00b5\u0303(\u03c9)||tr = tr(A) < \u221e.\nExample 4 (Curl-free and div-free kernels)\nFor the curl-free kernel, we have \u00b5(\u03c9) = \u03c9\u03c9T\u03c10(\u03c9) and thus\n\u00b5tr(\u03c9) = ||\u03c9||2\u03c10(\u03c9), (53)\nwhich is a finite measure by the assumption \u222b Rn\n||\u03c9||2d\u03c10(\u03c9) < \u221e. For the Gaussian case, since tr[k(0)] = 2n\u03c32 , the corresponding probability measure is\n\u03c1(\u03c9) = \u03c32\n2n ||\u03c9||2\u03c10(\u03c9). (54)\nSimilarly, for the div-free kernel, we have \u00b5(\u03c9) = [||\u03c9||2In \u2212 \u03c9\u03c9T ]\u03c10(\u03c9) and thus\n\u00b5tr(\u03c9) = (n\u2212 1)||\u03c9||2\u03c10(\u03c9). (55)\nFor the Gaussian case, since tr[k(0)] = 2n(n\u22121)\u03c32 , the corresponding probability measure is\n\u03c1(\u03c9) = \u03c32\n2n ||\u03c9||2\u03c10(\u03c9). (56)\nClearly, for both the curl-free and div-free kernels, the probability measure \u03c1 is non-unique. We can, for example, obtain \u03c1 by normalizing the measure\n(1 + ||\u03c9||2)\u03c10(\u03c9) (57)\nand the corresponding \u00b5\u0303(\u03c9) still has bounded trace.\nExample 5 (Sum of kernels)\nConsider now the probability measure and feature maps corresponding to the sum of two kernels, which is readily generalizable to any finite sum of kernels. Let k1, k2 be two positive definite functions satisfying the assumptions of Theorem 9, which are the Fourier transforms of two Sym+(H)-valued measures \u00b51 and \u00b52, respectively. Then their sum k = k1 + k2 is clearly the Fourier transform of \u00b5 = \u00b51 + \u00b52. Then the probability measure \u03c1 and the function \u00b5\u0303 corresponding to k is given by\n\u03c1(\u03c9) = \u00b5tr(\u03c9)\ntr[k(0)] =\n\u00b51,tr(\u03c9) + \u00b52,tr(\u03c9)\ntr[k1(0)] + tr[k2(0)] , (58)\n\u00b5\u0303(\u03c9) =\n{ (tr[k1(0)] + tr[k2(0)])\n\u00b51(\u03c9)+\u00b52(\u03c9) \u00b51,tr(\u03c9)+\u00b52,tr(\u03c9) , \u00b51,tr(\u03c9) + \u00b52,tr(\u03c9) > 0,\n0, \u00b51,tr(\u03c9) + \u00b52,tr(\u03c9) = 0. (59)\nWe the obtain the Fourier feature map for the kernel K(x, t) = k(x\u2212 t) using Eqs. (24) and (25).\nWe contrast this approach with the following concatenation approach. Let (\u03a6Kj ,FKj ) be the feature maps associated with Kj(x, t) = kj(x \u2212 t), j = 1, 2. Let FK be the direct\nHilbert sum of FK1 and FK2 . Consider the map \u03a6K : X \u2192 L(H,FK), FK = FK1 \u2295 FK2 , defined by\n\u03a6K(x) = ( \u03a6K1(x) \u03a6K2(x) ) , \u03a6K(x)w = ( \u03a6K1(x)w \u03a6K2(x)w ) , w \u2208 H, (60)\nwhich essentially stacks to the two maps \u03a6K1 , \u03a6K2 on top of each other. Then clearly\n\u03a6K(x) \u2217\u03a6K(t) = \u03a6K1(x) \u2217\u03a6K1(t) + \u03a6K2(x) \u2217\u03a6K2(t) = K1(x, t) +K2(x, t) = K(x, t),\nso that (\u03a6K ,FK) is a feature map representation for K = K1 + K2. If dim(FKj ) < \u221e, then we have dim(FK) = dim(FK1) + dim(FK2). Thus, from a practical viewpoint, this approach can be computationally expensive, since the dimension of the feature map for the sum kernel can be very large, especially if we have a sum of many kernels."}, {"heading": "3.5 Second Main Result: Uniform Convergence Analysis", "text": "Having computed the approximate version K\u0302D for K, we need to show that this approximation is consistent, that is K\u0302D approaches K in some sense, as D \u2192 \u221e. Since K(x, t) = k(x\u2212 t) it suffices for us to consider the convergence of k\u0302D towards k.\nRecall the Hilbert space of Hilbert-Schmidt operators HS(H), that is of bounded operators on H satisfying\n||A||2HS = tr(A\u2217A) = \u221e\u2211\nj=1\n||Aej ||2 < \u221e,\nfor any orthonormal basis {ej}\u221ej=1 in H. Here || ||HS denotes the Hilbert-Schmidt norm, which is induced by the Hilbert-Schmidt inner product\n\u3008A,B\u3009HS = tr(A\u2217B) = \u221e\u2211\nj=1\n\u3008Aej , Bej\u3009, A,B \u2208 HS(H).\nIn the following, we assume that k(x) \u2208 HS(H). Since we have shown that, under appropriate assumptions, bounded feature maps always exist, we focus exclusively on analyzing the convergence associated with them. Specifically, we show that for bounded feature maps, for any compact set \u2126 \u2282 Rn, we have\nsup x\u2208\u2126\n||k\u0302D(x)\u2212 k(x)||HS \u2192 0 as D \u2192 \u221e, (61)\nwith high probability. This generalizes the convergence of supx\u2208\u2126 |k\u0302D(x) \u2212 k(x)| in the scalar setting. If dim(H) < \u221e, then this is convergence in the Frobenius norm || ||F .\nTheorem 11 (Pointwise Convergence) Assume that ||\u00b5\u0303(\u03c9)||HS \u2264 M almost surely and that \u03c32(\u00b5\u0303(\u03c9)) = E\u03c1[||\u00b5\u0303(\u03c9)||2HS] < \u221e. Then for any fixed x \u2208 Rn,\nP[||k\u0302D(x)\u2212 k(x)||HS \u2265 \u01eb] \u2264 2 exp ( \u2212 D\u01eb 2M log [ 1 +\nM\u01eb\n\u03c32(\u00b5\u0303(\u03c9))\n]) \u2200\u01eb > 0. (62)\nAssumption 1. Our uniform convergence analysis requires the following condition\nm1 =\n\u222b\nRn ||\u03c9|| ||\u00b5(\u03c9)||HSd(\u03c9) =\n\u222b\nRn ||\u03c9|| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9) < \u221e. (63)\nIn the scalar setting, we have \u00b5\u0303(\u03c9) = 1, and Assumption 1 becomes \u222b Rn\n||\u03c9||d\u03c1(\u03c9) < \u221e, so that k is differentiable. This is weaker than the assumptions in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabo\u0301, 2015), which all require that\u222b Rn ||\u03c9||2d\u03c1(\u03c9) < \u221e, that is k is twice-differentiable.\nTheorem 12 (Uniform Convergence) Let \u2126 \u2282 Rn be compact with diameter diam(\u2126). Assume that ||\u00b5\u0303(\u03c9)||HS \u2264 M almost surely and that \u03c32(\u00b5\u0303(\u03c9)) = E\u03c1[||\u00b5\u0303(\u03c9)||2HS] < \u221e. Then for any \u01eb > 0,\nP ( sup x\u2208\u2126 ||k\u0302D(x)\u2212 k(x)||HS \u2265 \u01eb ) \u2264a(n) ( m1diam(\u2126) \u01eb ) n n+1\n\u00d7 exp ( \u2212 D\u01eb 4(n+ 1)M log [ 1 +\nM\u01eb\n2\u03c32(\u00b5\u0303(\u03c9))\n]) , (64)\nwhere a(n) = 2 3n+1 n+1 ( n 1 n+1 + n\u2212 n n+1 ) .\nExample 6 (Separable kernels)\nFor the separable kernel, Assumption 1 becomes ||A||HS < \u221e, in which case we have uniform convergence with M = ||A||HS and \u03c32(\u00b5\u0303) = ||A||2HS.\nExample 7 (Curl-free and div-free kernels)\nFor the curl-free kernel, we have ||\u00b5(\u03c9)||HS = ||\u03c9||2\u03c10(\u03c9), thus Assumption 1 becomes \u222b\nRn ||\u03c9||3d\u03c10(\u03c9) < \u221e, (65)\nwhich, being stronger than the assumption \u222b Rn\n||\u03c9||2d\u03c10(\u03c9) < \u221e in Section 3.4, guarantees that a bounded feature map can be constructed, with\n||\u00b5\u0303(\u03c9)||HS \u2264 ||\u00b5\u0303(\u03c9)||tr \u2264 tr[k(0)] and \u03c32[\u00b5\u0303(\u03c9)] \u2264 (tr[k(0)])2. (66)\nThe case of the div-free kernel is entirely similar. Comparison with the convergence analysis in (Brault et al., 2016). In (Brault et al., 2016), the authors carried out convergence analysis in the spectral norm for matrix-valued kernels. Our results are for the more general setting of operator-valued kernels, which induce RKHS of functions with values in a Hilbert space. In this setting, convergence in the HilbertSchmidt norm is strictly stronger than convergence in the spectral norm. Furthermore, as with previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szabo\u0301, 2015), the convergence in (Brault et al., 2016) also requires twice-differentiable kernels, whereas we require the weaker assumption of C1-differentiability."}, {"heading": "4. Vector-Valued Learning with Operator-Valued Feature Maps", "text": "Having discussed operator-valued feature maps and their random approximations, we now show how they can be applied in the context of learning in RKHS of vector-valued functions. Let W,Y be two Hilbert spaces, C : W \u2192 Y a bounded operator, K : Rn \u00d7Rn \u2192 L(W) be a positive definite definite kernel with the corresponding RKHS HK of W-valued functions, and V be a convex loss function. Consider the following general learning problem from (Minh et al., 2016)\nfz,\u03b3 = argminf\u2208HK 1\nl\nl\u2211\ni=1\nV (yi, Cf(xi)) + \u03b3A||f ||2HK + \u03b3I\u3008f ,M f\u3009Wu+l . (67)\nHere z = (x,y) = {(xi, yi)}li=1\u222a{xi}u+li=l+1, u, l \u2208 N, with u, l denoting unlabeled and labeled data points, respectively, f = (f(xj)) u+l j=1 \u2208 Wu+l, M : Wu+l \u2192 Wu+l a positive operator, and \u03b3A > 0, \u03b3I > 0. In (Minh et al., 2016), it is shown that the optimization problem (67) represents a general learning formulation in RKHS that encompasses supervised and semi-supervised learning via manifold regularization, multi-view learning, and multi-class classification. For the case V is the least square and SVM loss, both binary and multiclass, the solution of (67) has been obtained in dual form, that is in terms of kernel matrices.\nWe now present the solution of problem (67) in terms of feature map representation, that is in primal form. Let (\u03a6K ,FK) be any feature map for K. On the set x, we define the following operator\n\u03a6K(x) : Wu+l \u2192 FK , \u03a6K(x)w = u+l\u2211\nj=1\n\u03a6K(xj)wj . (68)\nWe also view \u03a6K(x) as a (potentially infinite) matrix\n\u03a6K(x) = [\u03a6K(x1), . . . ,\u03a6K(xu+l)] : Wu+l \u2192 FK , (69)\nwith the jth column being \u03a6K(xj). The following is the corresponding version of the Representer Theorem in (Minh et al., 2016) in feature map representation.\nTheorem 13 (Representer Theorem) The optimization problem (67) has a unique solution fz,\u03b3(x) = \u2211u+l i=1 K(x, xi)ai for ai \u2208 W, i = 1, . . . , u + l. In terms of feature maps, fz,\u03b3(x) = \u03a6K(x) \u2217h, where\nh =\nu+l\u2211\ni=1\n\u03a6K(xi)ai = \u03a6K(x)a \u2208 FK , a = (aj)u+lj=1 \u2208 Wu+l. (70)\nIn the case V is the least square loss, the optimization problem (67) has a closed-form solution, which is expressed explicitly in terms of the operator-valued feature map \u03a6K . In the following, let I(u+l)\u00d7l = [Il, 0l\u00d7u] T and Ju+ll = I(u+l)\u00d7lI T (u+l)\u00d7l, which is a (u+ l)\u00d7(u+ l) diagonal matrix, with the first l entries on the main diagonal equal to 1 and the rest being zero. The following is the corresponding version of Theorem 4 in (Minh et al., 2016) in feature map representation.\nTheorem 14 (Vector-Valued Least Square Algorithm) In the case V is the least square loss, that is V (y, f(x)) = ||y \u2212 Cf(x)||2Y , the solution of the optimization problem (67) is fz,\u03b3(x) = \u03a6K(x) \u2217h, with h \u2208 FK given by\nh = ( \u03a6K(x)[(J u+l l \u2297 C\u2217C) + l\u03b3IM ]\u03a6K(x)\u2217 + l\u03b3AIFK )\u22121 \u03a6K(x)(I(u+l)\u00d7l \u2297 C\u2217)y. (71)\nComparison with the dual formulation. In Theorem 4 in (Minh et al., 2016), the solution of the least square problem above is equivalently given by fz,\u03b3(x) = \u2211u+l j=1K(x, xj)aj , where a = (aj) u+l j=1 \u2208 Wu+l is given by\n(C\u2217CJW ,u+ll K[x] + l\u03b3IMK[x] + l\u03b3AIWu+l)a = C \u2217y, (72)\nwhere C\u2217 = I(u+l)\u00d7l \u2297C\u2217 and K[x] is the (u+ l)\u00d7 (u+ l) operator-valued matrix with the (i, j) entry being K(xi, xj).\nFor concreteness, consider the case W = Rd for some d \u2208 N. Then Eq. (72) is a system of linear equations of size d(u+ l)\u00d7 d(u+ l), which depends only on the dimension d of the output space and the number of data points (u+ l).\nIf the feature space FK is infinite-dimensional, then Eq. (71) is an infinite-dimensional system of linear equations.\nApproximate feature map vector-valued least square regression. Consider now the approximate finite-dimensional feature map \u03a6\u0302D(x) : R\nd \u2192 R2Dr, for some r, 1 \u2264 r \u2264 d. Here r depends on the decomposition \u00b5\u0303 = \u03c8(\u03c9)\u2217\u03c8(\u03c9) in Eq. (24), with r = d corresponding to e.g. \u03c8(\u03c9) = \u221a \u00b5\u0303(\u03c9). Then instead of the operator \u03a6K(x) : R\nd(u+l) \u2192 FK , we consider its approximation\n\u03a6\u0302D(x) = [\u03a6\u0302D(x1), . . . , \u03a6\u0302D(xu+l)] : R d(u+l) \u2192 R2Dr, (73)\nwhich is a matrix of size 2Dr \u00d7 d(u + l). This gives rise to the following system of linear equations, which approximates Eq. (71)\nh\u0302D = ( \u03a6\u0302D(x)[(J u+l l \u2297 C\u2217C) + l\u03b3IM ]\u03a6\u0302D(x)\u2217 + l\u03b3AI2Dk )\u22121 \u03a6\u0302D(x)(I(u+l)\u00d7l \u2297 C\u2217)y. (74)\nEq. (74) is a system of linear equations of size 2Dr \u00d7 2Dr, which is independent of the number of data points (u+ l). This system is more efficient to solve than Eq. (72) when\n2Dr < d(u+ l). (75)"}, {"heading": "5. Numerical Experiments", "text": "We report in this section several experiments to illustrate the numerical properties of the feature maps just constructed. Since the properties of the feature maps for separable kernels follow directly from those of the corresponding scalar kernels, we focus here on the curl-free and div-free kernels.\nApproximate kernel computation. We first checked the quality of the approximation of the kernel values using matrix-valued Fourier feature maps. Using the standard normal distribution, we generated a set of 100 points in R3, which are normalized to\nlie in the cube [\u22121, 1]3. On this set, we first computed the curl-free and div-free kernels induced by the Gaussian kernel, based on Eq. (33), with \u03c3 = 1. We computed the feature maps given by Eq. (25). For the curl-free kernel, in the unbounded map, \u03c8(\u03c9) is given by Eq. (37), with \u03c1(\u03c9) \u223c N (0, (2/\u03c32)I3), and in the bounded map, \u03c8(\u03c9) is given by Eq. (40), with \u03c1(\u03c9) \u223c N (0, (4/\u03c32)I3). Similarly, for the div-free kernel, the \u03c8(\u03c9) maps, bounded and unbounded, are given by Eq. (41). We then computed the relative error ||K\u0302D(x, y)\u2212K(x, y)||F /||K(x, y)||F , with F denoting the Frobenius norm, using D = 100, 500, 1000. The results are reported on Table 1.\nVector field reconstruction by approximate feature maps. Next, we tested the reconstruction of the following curl-free vector field in R2, F (x, y) = sin(4\u03c0x) sin2(2\u03c0y)i + sin2(2\u03c0x) sin(4\u03c0y)j on the rectangle [\u22121,\u22120.4765] \u00d7 [\u22121,\u22120.4765], sampled on a regular grid consisting of 1600 points. The reconstruction is done using 5% of the points on the grid as training data. We first performed the reconstruction with exact kernel least square regression according to Eq. (72), using the curl-free kernel induced by the Gaussian kernel, based on Eq. (33), with \u03c3 = 0.2. With the same kernel, we then performed the approximate feature map least square regression according to Eq. (74 (W = Y = R2, C = I2, \u03b3I = 0, \u03b3A = 10\u22129), with D = 50, 100. The results are reported on Table 2.\nDiscussion of numerical results. As we can see from Tables 1 and 2, the matrixvalued Fourier feature maps can be used both for approximating the kernel values as well as directly in a learning algorithm, with increasing accuracy as the feature dimension increases. Furthermore, while all feature maps associated with a given kernel are essentially equivalent as in Lemma 2, we observe that numerically, on average, the bounded feature maps tend to outperform the unbounded maps."}, {"heading": "6. Conclusion and Future Work", "text": "We have presented a framework for constructing random operator-valued feature maps for operator-valued kernels, using the operator-valued version of Bochner\u2019s Theorem. We have shown that, due to the non-uniqueness of the probability measure in this setting, in general many feature maps can be computed, which can be unbounded or bounded. Under certain conditions, which are satisfied for many common kernels such as curl-free and div-free kernels, bounded feature maps can always be computed. We then showed the uniform convergence, with the bounded maps, of the approximate kernel in the HilbertSchmidt norm, strengthening previous results in the scalar setting. Finally, we showed how a general vector-valued learning formulation can be expressed in terms of feature maps and demonstrated it experimentally. An extensive empirical evaluation of the proposed formulation is left to future work."}, {"heading": "Appendix A. Proofs of Main Mathematical Results", "text": "Proof of Lemma 2 For a function f \u2208 HK of the form f = \u2211N j=1Kxjaj, aj \u2208 W, we have\nf(x) =\nN\u2211\nj=1\nK(x, xj)aj =\nN\u2211\nj=1\n\u03a6K(x) \u2217\u03a6K(xj)aj = \u03a6K(x) \u2217\n  N\u2211\nj=1\n\u03a6K(xj)aj\n  ,\n= \u03a6K(x) \u2217h,\nwhere\nh = N\u2211\nj=1\n\u03a6K(xj)aj \u2208 FK .\nFor the norm, we have\n||f ||2HK = N\u2211\ni,j=1\n\u3008ai,K(xi, xj)aj\u3009W = N\u2211\ni,j=1\n\u3008ai,\u03a6K(xi)\u2217\u03a6K(xj)aj\u3009W\n= N\u2211\ni,j=1\n\u3008\u03a6K(xi)ai,\u03a6K(xj)aj\u3009FK = || N\u2211\ni=1\n\u03a6K(xi)ai||2FK = ||h|| 2 FK .\nBy letting N \u2192 \u221e in the Hilbert space completion for HK , it follows that every f \u2208 HK has the form\nf(x) = \u03a6K(x) \u2217h, h \u2208 FK ,\nand ||f ||HK = ||h||FK .\nThis completes the proof of the lemma.\nProof of Proposition 4 For each pair j, l \u2208 N, we have by Bochner\u2019s Theorem\n\u3008ej , k(x)el\u3009 = \u222b\nRn exp(\u2212i\u3008\u03c9, x\u3009)\u3008ej , d\u00b5(\u03c9)el\u3009.\nThus the proposition follows immediately from the Fourier Inversion Theorem.\nProof of Proposition 8 Let \u03a6K : R n \u2192 FK be a feature map for K, then K(x, t) = \u03a6K(x) \u2217\u03a6K(t). Then for any set of points {xj}Nj=1 and coefficients {bj}Nj=1, we have\nN\u2211\nj,l=1\nbjblKa(xj, xl) =\nN\u2211\nj,l=1\nbjbl\u3008a,\u03a6K(xj)\u2217\u03a6K(xl)a\u3009 = N\u2211\nj,l=1\nbjbl\u3008\u03a6K(xj)a,\u03a6K(xl)a\u3009FK\n=\nN\u2211\nj,l=1\n\u3008bj\u03a6K(xj)a, bl\u03a6K(xl)a\u3009FK = \u2225\u2225\u2225\u2225\u2225\u2225 N\u2211\nj=1\nbj\u03a6K(xj)a \u2225\u2225\u2225\u2225\u2225\u2225 2\nFK\n\u2265 0.\nThis shows that the scalar-valued kernel Ka is positive definite. Thus by the scalar-valued Bochner Theorem, there exists a unique finite positive Borel measure \u00b5a on R n such that\nKa(x, t) =\n\u222b\nRn exp(\u2212i\u3008\u03c9, x\u2212 t\u3009)d\u00b5a(\u03c9).\nFrom the formulas K(x, t) = \u222b Rn\nexp(\u2212i\u3008\u03c9, x \u2212 t\u3009)d\u00b5(\u03c9) and Ka(x, t) = \u3008a,K(x, t)a\u3009, we obtain \u00b5a(\u03c9) = \u3008a, \u00b5(\u03c9)a\u3009 as we claimed.\nProof of Theorem 9 By Proposition 4 and taking into account the fact that \u00b5(\u03c9) is a self-adjoint positive operator on H, we have\ntr[\u00b5(\u03c9)] = \u221e\u2211\nj=1\n\u3008ej , \u00b5(\u03c9)ej\u3009 = \u2223\u2223\u2223\u2223\u2223\u2223 \u221e\u2211\nj=1\n\u3008ej , \u00b5(\u03c9)ej\u3009 \u2223\u2223\u2223\u2223\u2223\u2223\n= 1\n(2\u03c0)n \u2223\u2223\u2223\u2223\u2223\u2223 \u221e\u2211\nj=1\n\u222b\nRn exp(i\u3008\u03c9, x\u3009)\u3008ej , k(x)ej\u3009dx \u2223\u2223\u2223\u2223\u2223\u2223\n= 1\n(2\u03c0)n \u2223\u2223\u2223\u2223\u2223\u2223 \u222b Rn exp(i\u3008\u03c9, x\u3009) \u221e\u2211\nj=1\n\u3008ej , k(x)ej\u3009dx \u2223\u2223\u2223\u2223\u2223\u2223\n= 1\n(2\u03c0)n\n\u2223\u2223\u2223\u2223 \u222b\nRn exp(i\u3008\u03c9, x\u3009tr[k(x)]dx\n\u2223\u2223\u2223\u2223 \u2264 1\n(2\u03c0)n\n\u222b\nRn |tr[k(x)]|dx < \u221e.\nThis shows that \u00b5tr(\u03c9) = tr[\u00b5(\u03c9)] = \u2211\u221e\nj=1 \u00b5jj(\u03c9) \u2208 Tr(H). The positivity of \u00b5tr(\u03c9) follows from the positivity of all the \u00b5jj\u2019s. Furthermore, we have\n\u222b\nRn d\u00b5tr(\u03c9) =\n\u222b\nRn d[\n\u221e\u2211\nj=1\n\u00b5jj(\u03c9)] =\n\u221e\u2211\nj=1\n\u3008ej , k(0)ej\u3009 = tr[k(0)].\nWe note that we must have tr[k(0)] > 0, since tr[k(0)] = 0 \u21d0\u21d2 k(0) = 0 \u21d0\u21d2 K(x, x) = 0 \u2200x \u2208 Rn \u21d0\u21d2 K(x, t) = 0 \u2200(x, t) \u2208 X \u00d7 X . It follows that the normalized measure\n\u00b5tr(\u03c9)\ntr[k(0)]\nis a probability measure on Rn. Since the trace operation is independent of the choice of orthonormal basis {ej}\u221ej=1, it follows that both the normalized and un-normalized measures are independent of the choice of {ej}\u221ej=1. This completes the proof.\nProof of Lemma 7 We make use of the following property of the Fourier transform (see e.g (Jones, 2001)). Assume that f and ||x||f are both integrable on Rn, then the Fourier transform f\u0302 is differentiable and\n\u2202f\u0302\n\u2202\u03c9j (\u03c9) = \u2212i\u0302xjf(\u03c9), 1 \u2264 j \u2264 n.\nAssume further that ||x||2f is integrable on Rn, then this rule can be applied twice to give\n\u22022f\u0302\n\u2202\u03c9j\u2202\u03c9k (\u03c9) =\n\u2202\n\u2202\u03c9j\n( \u2202f\u0302\n\u2202\u03c9k\n) = \u2212 \u2202\n\u2202\u03c9j [i\u0302xkf ] = \u2212x\u0302jxkf.\nFor the curl-free kernel, we have \u03c6(x) = \u03c1\u03020(x) and consequently, under the assumption that \u222b Rn ||\u03c9||2\u03c10(\u03c9) < \u221e, we have\n[kcurl(x)]jk = \u2212 \u22022\u03c6 \u2202xj\u2202xk (x) = \u2212 \u2202 2\u03c1\u03020 \u2202xj\u2202xk (x) = \u03c9\u0302j\u03c9k\u03c10(x), 1 \u2264 j, k \u2264 n.\nIn other words,\n[kcurl(x)]jk =\n\u222b\nRn e\u2212i\u3008\u03c9,x\u3009\u03c9j\u03c9k\u03c10(\u03c9)d\u03c9.\nIt thus follows that\nkcurl(x) =\n\u222b\nRn e\u2212i\u3008\u03c9,x\u3009\u03c9\u03c9T\u03c10(\u03c9)d\u03c9 =\n\u222b\nRn e\u2212i\u3008\u03c9,x\u3009\u00b5(\u03c9)d\u03c9,\nwhere \u00b5(\u03c9) = \u03c9\u03c9T\u03c10(\u03c9). The proof for kdiv is entirely similar.\nTo prove Theorems 11 and 12 , we need the following concentration result for Hilbert space-valued random variables.\nLemma 15 ((Smale and Zhou, 2007)) Let H be a Hilbert space with norm || || and {\u03bej}Dj=1, D \u2208 N, be independent random variables with values in H. Suppose that for each j, ||\u03bej || \u2264 M < \u221e almost surely. Let \u03c32D = \u2211D j=1 E(||\u03bej ||2). Then\nP   \u2225\u2225\u2225\u2225\u2225\u2225 1 D D\u2211\nj=1\n[\u03bej \u2212 E(\u03bej)] \u2225\u2225\u2225\u2225\u2225\u2225 \u2265 \u01eb   \u2264 2 exp ( \u2212 D\u01eb 2M log [ 1 + DM\u01eb \u03c32D ]) \u2200\u01eb > 0. (76)\nProof of Theorem 11 For each x \u2208 Rn fixed, consider the random variable \u03be(x, , .) : (Rn, \u03c1) \u2192 Sym(H) defined by\n\u03be(x, \u03c9) = cos(\u3008\u03c9, x\u3009)\u00b5\u0303(\u03c9).\nWe then have\nk\u0302D(x) = 1\nD\nD\u2211\nj=1\ncos(\u3008\u03c9j , x\u3009)\u00b5\u0303(\u03c9j) = 1\nD\nD\u2211\nj=1\n\u03be(x, \u03c9j),\nk(x) =\n\u222b\nRn cos(\u3008\u03c9, x\u3009)\u00b5\u0303(\u03c9)d\u03c1(\u03c9) = E\u03c1[\u03be(x, \u03c9)].\nUnder the assumption that ||\u00b5\u0303(\u03c9)||HS \u2264 M almost surely, we also have ||\u03be(x, \u03c9)||HS \u2264 M almost surely. Its variance satisfies\n\u03c32(\u03be(x, \u03c9)) = E\u03c1||\u03be(x, \u03c9)||2HS \u2264 E||\u00b5\u0303(\u03c9)||2HS = \u03c32(\u00b5\u0303(\u03c9)).\nIt follows from Lemma 15 that for each fixed x \u2208 Rn, we have\nP[||k\u0302D(x)\u2212 k(x)||HS \u2265 \u01eb] = P   \u2225\u2225\u2225\u2225\u2225\u2225 1 D D\u2211\nj=1\n\u03be(x, \u03c9j)\u2212 E\u03c1[\u03be(x, \u03c9)] \u2225\u2225\u2225\u2225\u2225\u2225 HS \u2265 \u01eb  \n\u2264 2 exp ( \u2212 D\u01eb 2M log [ 1 +\nM\u01eb\n\u03c32(\u03be(x, \u03c9))\n])\n\u2264 2 exp ( \u2212 D\u01eb 2M log [ 1 +\nM\u01eb\n\u03c32(\u00b5\u0303(\u03c9))\n]) .\nThis completes the proof of the theorem.\nTo prove Theorem 12, we first prove the following preliminary results.\nLemma 16 Assume that \u222b Rn\n||\u03c9|| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9) < \u221e. Then the function k : Rn \u2192 Sym(H), with the latter endowed with the Hilbert-Schmidt norm, is Lipschitz, with\n||k(x) \u2212 k(y)||HS \u2264 ||x\u2212 y|| \u222b\nRn ||\u03c9|| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9). (77)\nProof of Lemma 16 Using the fact that the cosine function is Lipschitz with constant 1, that is | cos(x)\u2212 cos(y)| \u2264 |x\u2212 y| for all x, y \u2208 R, we have\n||k(x) \u2212 k(y)||HS = \u2225\u2225\u2225\u2225 \u222b\nRn [cos(\u3008\u03c9, x\u3009) \u2212 cos(\u3008\u03c9, y\u3009)]\u00b5\u0303(\u03c9)d\u03c1(\u03c9) \u2225\u2225\u2225\u2225 HS\n\u2264 \u222b\nRn | cos(\u3008\u03c9, x\u3009) \u2212 cos(\u3008\u03c9, y\u3009)| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9)\n\u2264 \u222b\nRn |\u3008\u03c9, x\u3009 \u2212 \u3008\u03c9, y\u3009| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9)\n\u2264 ||x\u2212 y|| \u222b\nRn ||\u03c9|| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9).\nThis completes the proof.\nLemma 17 The function k\u0302D(x) = 1 D \u2211D j=1 cos(\u03c9j, x\u3009)\u00b5\u0303(\u03c9j) : Rn \u2192 Sym(H), with the latter endowed with the Hilbert-Schmidt norm, is Lipschitz, with\n||k\u0302D(x)\u2212 k\u0302D(y)||HS \u2264 ||x\u2212 y|| 1\nD\nD\u2211\nj=1\n||\u03c9j || ||\u00b5\u0303(\u03c9j)||HS. (78)\nProof of Lemma 17 Similar to the proof of Lemma 16, we utilize the fact that | cos(x)\u2212 cos(y)| \u2264 |x\u2212 y| for all x, y \u2208 R to arrive at\n||k\u0302D(x)\u2212 k\u0302D(y)||HS = 1\nD \u2225\u2225\u2225\u2225\u2225\u2225 D\u2211\nj=1\n[cos(\u3008\u03c9j , x\u3009)\u2212 cos(\u3008\u03c9j , y\u3009)]\u00b5\u0303(\u03c9j) \u2225\u2225\u2225\u2225\u2225\u2225 HS\n\u2264 ||x\u2212 y|| 1 D\nD\u2211\nj=1\n||\u03c9j|| ||\u00b5\u0303(\u03c9j)||HS.\nThis completes the proof.\nCorollary 18 Let f(x) = k\u0302D(x) \u2212 k(x) : Rn \u2192 Sym(H), with the latter endowed with the Hilbert-Schmidt norm, then f is Lipschitz, with\n||f(x)\u2212 f(y)|| \u2264 ||x\u2212 y||\n  \u222b\nRn ||\u03c9|| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9) +\n1\nD\nD\u2211\nj=1\n||\u03c9j|| ||\u00b5\u0303(\u03c9j)||HS   . (79)\nThe Lipschitz constant Lf of f satisfies\nP(Lf \u2265 \u01eb) \u2264 2m1 \u01eb , (80)\nwhere m1 = \u222b Rn ||\u03c9|| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9).\nProof of Corollary 18 By combing the results of Lemmas 16 and 17, we have\n||f(x)\u2212 f(y)||HS = ||(k\u0302D(x)\u2212 k(x)) \u2212 (k\u0302D(y)\u2212 k(y))||HS \u2264 ||k\u0302D(x)\u2212 k\u0302D(y)||HS + ||k(x) \u2212 k(y)||HS\n\u2264 ||x\u2212 y||\n  \u222b\nRn ||\u03c9|| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9) +\n1\nD\nD\u2211\nj=1\n||\u03c9j || ||\u00b5\u0303(\u03c9j)||HS\n \nas we claimed. Thus f is Lipschitz, with the Lipschitz constant Lf satisfying\nLf \u2264 \u222b\nRn ||\u03c9|| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9) +\n1\nD\nD\u2211\nj=1\n||\u03c9j || ||\u00b5\u0303(\u03c9j)||HS,\nwith expectation\nE(Lf ) \u2264 2 \u222b\nRn ||\u03c9|| ||\u00b5\u0303(\u03c9)||HSd\u03c1(\u03c9) = 2m1.\nBy Markov\u2019s inequality, we have for any \u01eb > 0,\nP(Lf \u2265 \u01eb) \u2264 E(Lf ) \u01eb \u2264 2m1 \u01eb .\nThis completes the proof.\nProof of Theorem 12 For each r > 0 fixed, let N = N (\u2126, r) be the covering number for \u2126, that is the minimum number of balls \u2126j, 1 \u2264 j \u2264 N , of radius r covering \u2126. By Proposition 5 in (Cucker and Smale, 2002), the covering number N (\u2126, r) is bounded above by the expression\nN = N (\u2126, r) \u2264 ( 2diam(\u2126)\nr\n)n . (81)\nConsider the function f : Rn \u2192 Sym(H) defined by\nf(x) = k\u0302D(x)\u2212 k(x).\nOn the ball \u2126j, we have\nP( sup x\u2208\u2126j ||k\u0302D(x)\u2212 k(x)||HS \u2265 \u01eb) = P( sup x\u2208\u2126j ||f(x)||HS \u2265 \u01eb).\nBy Corollary 18, f is a Lipschitz function with Lipschitz constant Lf > 0, with Sym(H) being endowed with the Hilbert-Schmidt norm. Let xj be the center of the jth ball \u2126j . For each \u01eb > 0, for any x \u2208 \u2126j, we have\n||f(xj)\u2212 f(x)||HS \u2264 Lf ||xj \u2212 x|| \u2264 rLf < \u01eb\n2 when Lf <\n\u01eb\n2r .\nSince\n||f(x)||HS \u2264 ||f(x)\u2212 f(xj)||HS + ||f(xj)||HS,\nwe have\nsup x\u2208\u2126j\n||f(x)||HS < \u01eb if Lf < \u01eb\n2r and ||f(xj)||HS <\n\u01eb 2 .\nThus over the union of balls \u2126j, 1 \u2264 j \u2264 N , we have\nsup x\u2208\u222aNj=1\u2126j\n||f(x)||HS < \u01eb if Lf < \u01eb\n2r and ||f(xj)||HS <\n\u01eb 2 , 1 \u2264 j \u2264 N .\nThus\nP   sup\nx\u2208\u222aNj=1\u2126j ||f(x)||HS < \u01eb\n  = P ( Lf < \u01eb\n2r and ||f(xj)||HS <\n\u01eb 2 , 1 \u2264 j \u2264 N\n) .\nWe now recall the following properties on an arbitrary probability space (\u03a3,P,F). For any events A,B, let A denote the complement of A in F , then we have A \u2229B = A\u222aB, so that\nP(A \u2229B) = P(A \u222aB) \u2264 P(A) + P(B), (82) P(A \u2229B) = 1\u2212 P(A \u2229B) = 1\u2212 P(A \u222aB) \u2265 1\u2212 P(A)\u2212 P(B). (83)\nApplying property (83) with A = {Lf < \u01eb2r} and B = {||f(xj)||HS < \u01eb2 , 1 \u2264 j \u2264 N}, we obtain\nP   sup\nx\u2208\u222aNj=1\u2126j ||f(x)||HS < \u01eb\n  \u2265 1\u2212 P ( Lf \u2265 \u01eb\n2r\n) \u2212 P ( {||f(xj)||HS < \u01eb\n2 , 1 \u2264 j \u2264 N}\n) .\nApplying property (82) recursively to the set {||f(xj)||HS < \u01eb2 , 1 \u2264 j \u2264 N}, we obtain\nP ( {||f(xj)||HS < \u01eb\n2 , 1 \u2264 j \u2264 N}\n) \u2264 N\u2211\nj=1\nP({||f(xj)||HS < \u01eb\n2 }) =\nN\u2211\nj=1\nP({||f(xj)||HS \u2265 \u01eb\n2 }).\nCombining the last two expressions, we have\nP   sup\nx\u2208\u222aNj=1\u2126j ||f(x)||HS < \u01eb\n  \u2265 1\u2212 P ( Lf \u2265 \u01eb\n2r\n) \u2212 N\u2211\nj=1\nP({||f(xj)||HS \u2265 \u01eb\n2 }).\nEquivalently,\nP   sup\nx\u2208\u222aN j=1 \u2126j\n||f(x)||HS \u2265 \u01eb   \u2264 P ( Lf \u2265 \u01eb\n2r\n) + N\u2211\nj=1\nP({||f(xj)||HS \u2265 \u01eb\n2 }).\nBy Corollary 18, we have\nP ( Lf \u2265 \u01eb\n2r\n) \u2264 4m1r\n\u01eb .\nBy Theorem 11, we have\nP ( ||f(xj)||HS \u2265 \u01eb\n2\n) \u2264 2 exp ( \u2212 D\u01eb 4M log [ 1 +\nM\u01eb\n2\u03c32(\u00b5\u0303(\u03c9))\n]) .\nPutting everything together, we obtain\nP ( sup x\u2208\u2126 ||f(x)||HS \u2265 \u01eb ) \u2264 4m1r \u01eb + 2N exp ( \u2212 D\u01eb 4M log [ 1 +\nM\u01eb\n2\u03c32(\u00b5\u0303(\u03c9))\n])\n\u2264 4m1r \u01eb + 2exp ( \u2212 D\u01eb 4M log [ 1 +\nM\u01eb\n2\u03c32(\u00b5\u0303(\u03c9))\n])( 2diam(\u2126)\nr\n)n\n= ar + b\nrn ,\nwhere a = 4m1\u01eb and b = 2exp ( \u2212 D\u01eb4M log [ 1 + M\u01eb2\u03c32(\u00b5\u0303(\u03c9)) ]) (2diam(\u2126))n.\nLet us find the value r > 0 that minimizes the right hand side in the above expression.\nThe function g(r) = ar + brn for a, b > 0 achieves its minimum on (0,\u221e) at r = ( bn a ) 1 n+1 , with the minimum value given by\ngmin = a n n+1 b 1 n+1 ( n 1 n+1 + n\u2212 n n+1 ) .\nSubstituting the value for a and b, we obtain\nP ( sup x\u2208\u2126 ||f(x)||HS \u2265 \u01eb ) \u2264 a(n) ( m1diam(\u2126) \u01eb ) n n+1 exp ( \u2212 D\u01eb 4(n+ 1)M log [ 1 +\nM\u01eb\n2\u03c32(\u00b5\u0303(\u03c9))\n])\nwhere a(n) = 2 3n+1 n+1 ( n 1 n+1 + n\u2212 n n+1 ) . This completes the proof of the theorem.\nProof of Theorem 13 It is straightforward to show that the optimization problem (67) has a unique solution fz,\u03b3 , which has the form fz,\u03b3(x) = \u2211u+l i=1 K(x, xi)ai for some ai \u2208 W. Under the feature map representation \u03a6K , we have\nfz,\u03b3(x) =\nu+l\u2211\ni=1\nK(x, xi)ai =\nu+l\u2211\ni=1\n\u03a6K(x) \u2217\u03a6K(xi)ai = \u03a6K(x) \u2217h,\nwhere\nh =\nu+l\u2211\ni=1\n\u03a6K(xi)ai = \u03a6K(x)a,\nas we claimed.\nTo prove Theorem 14, we first consider the following operators. The sampling operator Sx : HK \u2192 W l is defined by Sx(f) = (f(xi))li=1, for any\ny = (yi) l i=1 \u2208 W l,\n\u3008Sxf,y\u3009W l = l\u2211\ni=1\n\u3008f(xi), yi\u3009W = l\u2211\ni=1\n\u3008K\u2217xif, yi\u3009HK\n=\nl\u2211\ni=1\n\u3008f,Kxiyi\u3009HK = \u3008f, l\u2211\ni=1\nKxiyi\u3009HK .\nThus the adjoint operator S\u2217 x : W l \u2192 HK is given by\nS\u2217 x y = S\u2217 x (y1, . . . , yl) =\nl\u2211\ni=1\nKxiyi, y \u2208 W l, (84)\nand the operator S\u2217 x Sx : HK \u2192 HK is given by\nS\u2217 x Sxf =\nl\u2211\ni=1\nKxif(xi) =\nl\u2211\ni=1\nKxiK \u2217 xif. (85)\nConsider the operator EC,x : HK \u2192 Y l, defined by\nEC,xf = (CK \u2217 x1f, . . . , CK \u2217 xl f), (86)\nwith CK\u2217xi : HK \u2192 Y and KxiC\u2217 : Y \u2192 HK . For b = (b1, . . . , bl) \u2208 Y l, we have\n\u3008b, EC,xf\u3009Y l = l\u2211\ni=1\n\u3008bi, CK\u2217xif\u3009Y = l\u2211\ni=1\n\u3008KxiC\u2217bi, f\u3009HK . (87)\nThe adjoint operator E\u2217C,x : Y l \u2192 HK is thus\nE\u2217C,x : (b1, . . . , bl) \u2192 l\u2211\ni=1\nKxiC \u2217bi. (88)\nThe operator E\u2217C,xEC,x : HK \u2192 HK is then\nE\u2217C,xEC,xf \u2192 l\u2211\ni=1\nKxiC \u2217CK\u2217xif, (89)\nwith C\u2217C : W \u2192 W. Proof of Theorem 14 Since f(x) = K\u2217x, we have\nfz,\u03b3 = argminf\u2208HK 1\nl\nl\u2211\ni=1\n||yi \u2212 CK\u2217xif ||2Y + \u03b3A||f ||2HK + \u03b3I\u3008f ,M f\u3009Wu+l . (90)\nUsing the operator EC,x, this becomes\nfz,\u03b3 = argminf\u2208HK 1\nl ||EC,xf \u2212 y||2Y l + \u03b3A||f ||2HK + \u03b3I\u3008f ,M f\u3009Wu+l . (91)\nDifferentiating (91) and setting the derivative to zero gives\n(E\u2217C,xEC,x + l\u03b3AI + l\u03b3IS \u2217 x,u+lMSx,u+l)fz,\u03b3 = E \u2217 C,xy, (92)\nwhich is fz,\u03b3 = (E \u2217 C,xEC,x + l\u03b3AIHK + l\u03b3IS \u2217 x,u+lMSx,u+l) \u22121E\u2217C,xy. On the set x = (xi)u+li=1 , the operators Sx,u+l : HK \u2192 Wu+l and S\u2217x,u+l : Wu+l \u2192 HK are given by\nSx,u+lf = (K \u2217 xif) u+l i=1 , f \u2208 HK ,\nS\u2217 x,u+lb =\nu+l\u2211\ni=1\nKxibi, b \u2208 Wu+l.\nBy definition of the operators Sx,u+l and S \u2217 x,u+l, we have\nS\u2217 x,u+l(I(u+l)\u00d7l \u2297 C\u2217)y =\nl\u2211\ni=1\nKxi(C \u2217yi).\nThus the operator E\u2217C,x : Y l \u2192 HK is\nE\u2217C,x = S \u2217 x,u+l(I(u+l)\u00d7l \u2297 C\u2217). (93)\nThe operator E\u2217C,xEC,x : HK \u2192 HK is given by\nE\u2217C,xEC,x = S \u2217 x,u+l(J u+l l \u2297 C\u2217C)Sx,u+l : HK \u2192 HK , (94)\nEquation (92) becomes [ S\u2217 x,u+l(J u+l l \u2297C\u2217C + l\u03b3IM)Sx,u+l + l\u03b3AIHK ] fz,\u03b3 = S \u2217 x,u+l(I(u+l)\u00d7l \u2297 C\u2217)y, (95)\nwhich gives\nfz,\u03b3 = [ S\u2217 x,u+l(J u+l l \u2297 C\u2217C + l\u03b3IM)Sx,u+l + l\u03b3AIHK ]\u22121 S\u2217 x,u+l(I(u+l)\u00d7l \u2297 C\u2217)y. (96)\nFor any x \u2208 X ,\n(S\u2217 x,u+l(I(u+l)\u00d7l \u2297 C\u2217)y)(x) =\nl\u2211\ni=1\nK(x, xi)(C \u2217yi) \u2208 W.\nUsing the feature map \u03a6K , we have for any x \u2208 X ,\n(S\u2217 x,u+l(I(u+l)\u00d7l \u2297 C\u2217)y)(x) =\nl\u2211\ni=1\n\u03a6K(x) \u2217\u03a6K(xi)(C \u2217yi) \u2208 W,\nand for any w \u2208 W,\n\u3008(S\u2217 x,u+l(I(u+l)\u00d7l \u2297 C\u2217)y)(x), w\u3009W =\nl\u2211\ni=1\n\u3008\u03a6K(x)\u2217\u03a6K(xi)(C\u2217yi), w\u3009W\n=\nl\u2211\ni=1\n\u3008\u03a6K(xi)(C\u2217yi),\u03a6K(x)w\u3009FK\n= \u3008\u03a6K(x)(I(u+l)\u00d7l \u2297 C\u2217)y,\u03a6K(x)w\u3009FK .\nFor any f \u2208 HK ,\nS\u2217 x,u+lMSx,u+lf =\nu+l\u2211\ni=1\nKxi(M f)i.\nFor any x \u2208 X ,\n(S\u2217 x,u+lMSx,u+lf)(x) =\nu+l\u2211\ni=1\nK(x, xi)(M f)i =\nu+l\u2211\ni=1\n\u03a6K(x) \u2217\u03a6K(xi)(M f)i \u2208 W,\nand for any w \u2208 W,\n\u3008(S\u2217 x,u+lMSx,u+lf)(x), w\u3009W =\nu+l\u2211\ni=1\n\u3008\u03a6K(xi)(M f)i,\u03a6K(x)w\u3009FK .\nWe have\nu+l\u2211\ni=1\n\u03a6K(xi)(M f)i =\nu+l\u2211\ni=1\n\u03a6K(xi)(M\u03a6K(x) \u2217h)i = \u03a6K(x)M\u03a6K(x) \u2217h \u2208 FK .\nIt follows that\n\u3008(S\u2217 x,u+lMSx,u+lf)(x), w\u3009W = \u3008\u03a6K(x)M\u03a6K(x)\u2217h,\u03a6K(x)w\u3009FK . (97)\nSimilarly, for any f \u2208 HK ,\nS\u2217 x,u+l(J u+l l \u2297 C\u2217C)Sx,u+lf = S\u2217x,u+l(Ju+ll \u2297 C\u2217C)f =\nu+l\u2211\ni=1\nKxi((J u+l l \u2297 C\u2217C)f)i\n=\nu+l\u2211\ni=1\nKxi((J u+l l \u2297 C\u2217C)\u03a6K(x)\u2217h)i.\nFor any x \u2208 X ,\n(S\u2217 x,u+l(J u+l l \u2297 C\u2217C)Sx,u+lf)(x) =\nu+l\u2211\ni=1\nK(x, xi)((J u+l l \u2297 C\u2217C)\u03a6K(x)\u2217h)i\n=\nu+l\u2211\ni=1\n\u03a6K(x) \u2217\u03a6K(xi)((J u+l l \u2297 C\u2217C)\u03a6K(x)\u2217h)i.\nFor any w \u2208 W,\n\u3008(S\u2217 x,u+l(J u+l l \u2297 C\u2217C)Sx,u+lf)(x), w\u3009W = \u3008\nu+l\u2211\ni=1\n\u03a6K(xi)((J u+l l \u2297 C\u2217C)\u03a6K(x)\u2217h)i,\u03a6K(x)w\u3009W\n= \u3008\u03a6K(x)(Ju+ll \u2297 C\u2217C)\u03a6K(x)\u2217h,\u03a6K(x)w\u3009FK .\nEquation (95) is then equivalent to\n\u3008\u03a6K(x)(Ju+ll \u2297 C\u2217C)\u03a6K(x)\u2217h,\u03a6K(x)w\u3009FK + l\u03b3I\u3008\u03a6K(x)M\u03a6K(x)\u2217h,\u03a6K(x)w\u3009FK + l\u03b3A\u3008h,\u03a6K(x)w\u3009FK = \u3008\u03a6K(x)(I(u+l)\u00d7l \u2297 C\u2217)y,\u03a6K(x)w\u3009FK .\nfor all x \u2208 X , w \u2208 W, which is\n\u3008\u03a6K(x)[(Ju+ll \u2297 C\u2217C) + l\u03b3IM ]\u03a6K(x)\u2217h,\u03a6K(x)w\u3009FK + l\u03b3A\u3008h,\u03a6K(x)w\u3009FK = \u3008\u03a6K(x)(I(u+l)\u00d7l \u2297 C\u2217)y,\u03a6K(x)w\u3009FK .\nfor all x \u2208 X , w \u2208 W. This is satisfied if ( \u03a6K(x)[(J u+l l \u2297 C\u2217C) + l\u03b3IM ]\u03a6K(x)\u2217 + l\u03b3AIFK ) h = \u03a6K(x)(I(u+l)\u00d7l \u2297 C\u2217)y.\nThis completes the proof of the theorem."}], "references": [{"title": "Universal multi-task kernels", "author": ["A. Caponnetto", "C. Micchelli", "M. Pontil", "Y. Ying"], "venue": null, "citeRegEx": "Caponnetto et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Caponnetto et al\\.", "year": 2011}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In NIPS,", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "On the error of random Fourier features", "author": ["D. Sutherland", "J. Schneider"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Sutherland and Schneider.,? \\Q2015\\E", "shortCiteRegEx": "Sutherland and Schneider.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "In the scalar setting, one of the most powerful approaches for scaling up kernel methods is Random Fourier Features (Rahimi and Recht, 2007), which applies Bochner\u2019s Theorem and the Inverse Fourier Transform to build random features that approximate a given shift-invariant kernel.", "startOffset": 116, "endOffset": 140}, {"referenceID": 1, "context": "The approach in (Rahimi and Recht, 2007) has been improved both in terms of computational speed (Le et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 2, "context": ", 2013) and rates of convergence (Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015).", "startOffset": 33, "endOffset": 96}, {"referenceID": 1, "context": "Thirdly, when restricting to the scalar setting, our convergence holds for differentiable kernels, which is an improvement over the hypothesis of (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015; Brault et al., 2016), which all require the kernels to be twice-differentiable.", "startOffset": 146, "endOffset": 254}, {"referenceID": 2, "context": "Thirdly, when restricting to the scalar setting, our convergence holds for differentiable kernels, which is an improvement over the hypothesis of (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015; Brault et al., 2016), which all require the kernels to be twice-differentiable.", "startOffset": 146, "endOffset": 254}, {"referenceID": 1, "context": ", 2016) and previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015).", "startOffset": 51, "endOffset": 138}, {"referenceID": 2, "context": ", 2016) and previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015).", "startOffset": 51, "endOffset": 138}, {"referenceID": 1, "context": "Random Fourier features for scalar-valued kernels (Rahimi and Recht, 2007).", "startOffset": 50, "endOffset": 74}, {"referenceID": 1, "context": "This is weaker than the assumptions in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015), which all require that \u222b Rn ||\u03c9||2d\u03c1(\u03c9) < \u221e, that is k is twice-differentiable.", "startOffset": 39, "endOffset": 126}, {"referenceID": 2, "context": "This is weaker than the assumptions in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015), which all require that \u222b Rn ||\u03c9||2d\u03c1(\u03c9) < \u221e, that is k is twice-differentiable.", "startOffset": 39, "endOffset": 126}, {"referenceID": 1, "context": "Furthermore, as with previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015), the convergence in (Brault et al.", "startOffset": 60, "endOffset": 147}, {"referenceID": 2, "context": "Furthermore, as with previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015), the convergence in (Brault et al.", "startOffset": 60, "endOffset": 147}], "year": 2016, "abstractText": "This paper presents a framework for computing random operator-valued feature maps for operator-valued positive definite kernels. This is a generalization of the random Fourier features for scalar-valued kernels to the operator-valued case. Our general setting is that of operator-valued kernels corresponding to RKHS of functions with values in a Hilbert space. We show that in general, for a given kernel, there are potentially infinitely many random feature maps, which can be bounded or unbounded. Most importantly, given a kernel, we present a general, closed form formula for computing a corresponding probability measure, which is required for the construction of the Fourier features, and which, unlike the scalar case, is not uniquely and automatically determined by the kernel. We also show that, under appropriate conditions, random bounded feature maps can always be computed. Furthermore, we show the uniform convergence, under the Hilbert-Schmidt norm, of the resulting approximate kernel to the exact kernel on any compact subset of Euclidean space. Our convergence requires differentiable kernels, an improvement over the twice-differentiability requirement in previous work in the scalar setting. We then show how operator-valued feature maps and their approximations can be employed in a general vector-valued learning framework. The mathematical formulation is illustrated by numerical examples on matrix-valued kernels.", "creator": "LaTeX with hyperref package"}}}