{"id": "1608.06043", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2016", "title": "Context Gates for Neural Machine Translation", "abstract": "In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts on the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to lack of effective control on the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose to use context gates to dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance the adequacy of NMT while keeping the fluency unchanged. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.", "histories": [["v1", "Mon, 22 Aug 2016 03:19:27 GMT  (775kb,D)", "http://arxiv.org/abs/1608.06043v1", "Our code is publicly available atthis https URL"], ["v2", "Tue, 6 Dec 2016 03:09:50 GMT  (821kb,D)", "http://arxiv.org/abs/1608.06043v2", "Accepted by TACL 2016"], ["v3", "Wed, 8 Mar 2017 07:14:27 GMT  (821kb,D)", "http://arxiv.org/abs/1608.06043v3", "Accepted by TACL 2017"]], "COMMENTS": "Our code is publicly available atthis https URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhaopeng tu", "yang liu", "zhengdong lu", "xiaohua liu", "hang li"], "accepted": true, "id": "1608.06043"}, "pdf": {"name": "1608.06043.pdf", "metadata": {"source": "CRF", "title": "Context Gates for Neural Machine Translation", "authors": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li"], "emails": ["tu.zhaopeng@huawei.com", "lu.zhengdong@huawei.com", "liuxiaohua3@huawei.com", "hangli.hl@huawei.com", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years, whose goal is to construct and utilize a single large neural network to accomplish the entire translation task. One great advantage of NMT is that the translation system can be completely constructed by learning from data without human involvement (cf., feature engineering in statistical machine translation (SMT)). The encoderdecoder architecture is widely employed (Cho et\nal., 2014; Sutskever et al., 2014), in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. The representation of source sentence and the representation of partially generated target sentence (translation) at each position are referred to as source context and target context, respectively. The generation of a target word is determined jointly by the source context and target context.\nSeveral techniques in NMT are proved to be very effective, such as gating (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and attention (Bahdanau et al., 2015) which can model long-distance dependencies and complicated alignment relations\nar X\niv :1\n60 8.\n06 04\n3v 1\n[ cs\n.C L\n] 2\n2 A\nug 2\nin the translation process. With the encoder-decoder framework as well as the gating and attention techniques, it is reported that the performance of NMT has surpassed the performance of traditional SMT in terms of BLEU score (Luong et al., 2015).\nDespite the success, we observe that NMT usually yields fluent but inadequate translations.1 We attribute this to a stronger influence of target context on the generation, which corresponds to a stronger language model in SMT. One question naturally arises: what will happen if we change the ratio of influences from the source or target contexts?\nTable 1 shows an example, in which an attentionbased NMT system (Bahdanau et al., 2015) generates a fluent yet inadequate translation (e.g., missing the translation of \u201cgua\u030cngdo\u0304ng\u201d). When we halve the contribution from the source context, the result further loses its adequacy by missing the partial translation \u201cin the first two months of this year\u201d. One possible explanation is that the target context takes a higher weight and thus the system favors a shorter translation. In contrast, when we halve the contribution from the target context, the result completely loses its fluency by generating the translation of \u201cchu\u0304ko\u030cu\u201d (i.e., \u201cthe export of\u201d) multiple times. Therefore, this example indicates that source and target contexts in NMT are highly correlated to translation adequacy and fluency, respectively.\nIn fact, conventional NMT lacks of effective control on influence from the source and target contexts. At each decoding step, NMT treats the source and target contexts equally, and thus ignores the different needs on the contexts. For example, content words in the target sentence are more related to the translation adequacy, and thus should depend more on the source context. In contrast, functional words in the target sentence are more related to the translation fluency (e.g., \u201cof\u201d after \u201cis fond\u201d), and thus should depend more on the target context.\nIn this work, we propose to use context gates to control the contributions of source and target contexts on generation of target words (decoding) in NMT.2 Context gates are non-linear gating units\n1Fluency measures whether the translation is fluent, while adequacy measures whether the translation is faithful to the original sentence (Snover et al., 2009).\n2Our code is publicly available at https://github. com/tuzhaopeng/NMT.\nwhich can dynamically select the amount of context information in the decoding process. Specifically, at each decoding step, the context gate examines both the source and target contexts, and outputs a ratio between zero and one to determine the percentages of information to utilize from the two contexts. In this way, the system can balance the adequacy and fluency of translation with regard to generation of a word at the position.\nExperimental results show that introducing context gates leads to an average improvement of +2.3 BLEU points over a standard attention-based NMT system (Bahdanau et al., 2015). An interesting finding is that we can replace the GRU units in the decoder with conventional RNN units and in the meantime utilize context gates. The translation performance is comparable with the standard NMT system with GRU, but the system enjoys a simpler structure (i.e., uses only a single gate and half of the parameters) and a faster decoding (i.e., requires half of matrix computations in decoding)."}, {"heading": "2 Neural Machine Translation", "text": "Suppose that x = x1, . . . xj , . . . xJ represents a source sentence and y = y1, . . . yi, . . . yI a target sentence. NMT directly models the probability of translation from the source sentence to the target sentence word by word:\nP (y|x) = I\u220f\ni=1\nP (yi|y<i,x) (1)\nwhere y<i = y1, . . . , yi\u22121. The probability of generating the i-th word yi is computed by using a recurrent neural network (RNN) in the decoder:\nP (yi|y<i,x) = g(yi\u22121, ti, si) (2)\nwhere g(\u00b7) is a softmax function, yi\u22121 is the previously generated word, ti is the i-th decoding hidden state, and si is the i-th source representation. The state ti is computed as\nti = f(yi\u22121, ti\u22121, si)\n= f(We(yi\u22121) + Uti\u22121 + Csi) (3)\nwhere\n\u2022 f(\u00b7) is a function to compute the current decoding state given all the related inputs. It can be either a vanilla RNN unit using tanh function, or a sophisticated gated RNN unit such as GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997).\n\u2022 e(yi\u22121) \u2208 Rm is an m-dimensional embedding of the previously generated word yi\u22121.\n\u2022 si is a vector representation extracted from the source sentence by the encoder. The encoder usually uses an RNN to encode the source sentence x into a sequence of hidden states h = h1, . . . hj , . . . hJ , in which hj is the hidden state of the j-th source word xj . si can be either a static vector that summarizes the whole sentence (e.g., si \u2261 hJ ) (Cho et al., 2014; Sutskever et al., 2014), or a dynamic vector that selectively summarizes certain parts of the source sentence at each decoding step (e.g., si = \u2211J j=1 \u03b1i,jhj in which \u03b1i,j\nis alignment probability calculated by an attention model) (Bahdanau et al., 2015).\n\u2022 W \u2208 Rn\u00d7m, U \u2208 Rn\u00d7n, C \u2208 Rn\u00d7n\u2032 are matrices with n and n\u2032 being the numbers of units of decoder hidden state and source representation, respectively.\nThe inputs to the decoder (i.e., si, ti\u22121, and yi\u22121) represent the contexts. Specifically, the source representation si stands for source context, which embeds the information from the source sentence. The previous decoding state ti\u22121 and lastly generated word yi\u22121 stand for target context.3\n3In a latest implementation of NMT (https://github. com/nyu-dl/dl4mt-tutorial), ti\u22121 and yi\u22121 are combined together with a GRU before being fed into the decoder, which can boost translation performance. We follow the practice and treat both of them as target context."}, {"heading": "2.1 Effects of Source and Target Contexts", "text": "We first empirically investigate our hypothesis: whether source and target contexts correlate to translation adequacy and fluency? Figure 2(a) shows the translation lengths with various scaling ratios for source and target contexts. For example, the pair (1.0, 0.5) means fully leveraging the effect of source context while halving the effect of target context. Reducing the effect of target context (i.e., the lines (1.0, 0.8) and (1.0, 0.5)) results in longer translations, while reducing the effect of source context (i.e., the lines (0.8, 1.0) and (0.5, 1.0)) leads to shorter translations. When having the effect of the\ntarget context, most generated translations reach the maximum length, which is three times the length of source sentence in this work.\nFigure 2(b) shows the results of manual evaluation on 200 source sentences randomly sampled from the test sets. Reducing the effect of source context (i.e., (0.8, 1.0) and (0.5, 1.0)) leads to generation of more fluent yet less adequate translations. On the other hand, reducing the effect of target context (i.e., (1.0, 0.5) and (1.0, 0.8)) is expected to yield more adequate but less fluent translations. In such a case, the source words are translated (i.e., higher adequacy) while the translations are in wrong order (i.e., lower fluency). In practice, however, we observe that some source words are translated many times (i.e., lower fluency), while others are left untranslated (i.e., lower adequacy). One possible reason is that NMT lacks of a mechanism to guarantee that each source word is translated. The recently proposed coverage based technique can address this problem (Tu et al., 2016). In this work, we consider another approach, which is complementary to the coverage mechanism.\nThe quantitative (i.e., Figure 2) and qualitative (i.e., Table 1) results confirm that our hypothesis is correct, i.e., source and target contexts are highly correlated to translation adequacy and fluency. We believe that a mechanism that can dynamically select information from source context and target context would be useful for NMT models, and this is exactly the approach which we propose."}, {"heading": "3 Context Gate", "text": ""}, {"heading": "3.1 Architecture", "text": "Inspired by the success of gated units in RNN (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), we propose using a context gate to dynamically control the amount of information flowing from the source and target contexts and thus balance the fluency and adequacy of NMT at each decoding step.\nIntuitively, at each decoding step i, the context gate looks at the input signals from both the source (i.e., si) and target (i.e., ti\u22121 and yi\u22121) sides, and outputs a number between 0 and 1 for each element in the input vectors, where 1 denotes \u201ccompletely transferring this\u201d while 0 denotes \u201ccompletely ig-\nnoring this\u201d. The corresponding input signals are then processed with an element-wise multiplication before being fed to the activation layer to update the decoding state.\nFormally, a context gate consists of a sigmoid neural network layer and an element-wise multiplication operation, as illustrated in Figure 3. The context gate assigns an element-wise weight to the input signals, computed by\nzi = \u03c3(Wze(yi\u22121) + Uzti\u22121 + Czsi) (4)\nHere \u03c3(\u00b7) is a logistic sigmoid function, and Wz \u2208 Rn\u00d7m, Uz \u2208 Rn\u00d7n, Cz \u2208 Rn\u00d7n \u2032 are the weight matrices. Again, m, n and n\u2032 are the dimensions of word embedding, decoding state, and source representation, respectively. Note that zi has the same dimensionality as the transferred input signals (e.g., Csi), and thus each element in the input vectors has a distinct weight."}, {"heading": "3.2 Integrating Context Gate into NMT", "text": "Next, we consider how to integrate context gates into an NMT model.\nThe context gate can decide the amount of context information used in generation of word at each step of decoding. For example, after obtaining the partial translation \u201c. . . new high level technology product\u201d, the gate looks at the translation contexts and decides to leverage the source context more. Accordingly, the gate assigns higher weights to the source context and lower weights to the target context and then feeds them into the decoding activation layer. This could correct the inadequate translation, which misses the translation of \u201cgua\u030cngdo\u0304ng\u201d due to higher influence from the target context.\nWe have three strategies of integrating context gates into NMT, which either affect one of the translation contexts or affect both contexts, as illustrated in Figure 4. The first two strategies are inspired by the concept of output gate in LSTM (Hochreiter and Schmidhuber, 1997), which controls the amount of memory content utilized. In such types of model, zi only affects either source context (i.e., si) or target context (i.e., yi\u22121 and ti\u22121):\n\u2022 Context Gate (source) ti = f ( We(yi\u22121) + Uti\u22121 + zi \u25e6 Csi ) \u2022 Context Gate (target)\nti = f ( zi \u25e6 (We(yi\u22121) + Uti\u22121) + Csi ) where \u25e6 is an element-wise multiplication, and zi is the context gate calculated by Equation 4. This is also essentially similar to the reset gate in GRU, which decides what information to forget from the previous decoding state before transferred to the decoding activation layer. The difference is that here the \u201creset\u201d gate resets the context vector rather than the previous decoding state.\nThe last strategy is inspired by the concept of update gate from GRU, which takes a linear sum between the previous state ti\u22121 and the candidate new state t\u0303i. In our case, we take a linear interpolation between source and target contexts:\n\u2022 Context Gate (both) ti = f ( (1\u2212 zi) \u25e6 (We(yi\u22121) + Uti\u22121) + zi \u25e6 Csi )"}, {"heading": "4 Related Work", "text": "Comparison to (Xu et al., 2015) \u2013 The context gate takes inspiration form the gating scalar model proposed by (Xu et al., 2015) to perform the image caption generation task. The essential difference lies in the task requirement:\n\u2022 In the image caption generation task, the source side (i.e., image) contains more information than the target side (i.e., caption). Therefore, they employ a gating scalar to scale only the source context.\n\u2022 In the machine translation task, both sides contain equivalent information and our model jointly controls the contributions from the source and target contexts. A direct interaction between input signals from both sides is useful for balancing adequacy and fluency of NMT.\nOther differences in the architecture include:\n1 Xu et al. (2015) uses a scalar that is shared by all elements in the source context, while we employ a gate that outputs a distinct weight for each element. The latter offers the gate a more precise control of the context vector, since different elements retain different information.\n2 We add to the architecture peephole connections, by which the source context controls the gate. It has been shown that peephole connections make precise timings easier to learn (Gers and Schmidhuber, 2000).\n3 Our context gate also considers the previously generated word yi\u22121 as input. The lastly generated word would help the gate to better estimate the importance of target context, especially for the generation of functional words in translation that does not exist in the source sentence (e.g., \u201cof\u201d after \u201cis fond\u201d).\nExperimental results (Section 5.4) show that the above modifications consistently improve the translation quality.\nComparison to Gated RNN \u2013 State-of-the-art NMT models (Sutskever et al., 2014; Bahdanau et al., 2015) generally employ a gated unit (e.g., GRU or LSTM) as the activation function in the decoder. One may doubt that the context gate proposed in this work is partially repetitive with a gated unit in terms of controlling the amount of information from the previous decoding state si\u22121 (e.g., reset gate in GRU). We argue that they are in fact complementary to each other: the context gate regulates the contextual information flowing into the decoding state, while the gated unit captures long-term dependencies between decoding states. Our experiments confirm the correctness of our hypothesis: the context gate not only improves over the conventional RNN unit (e.g., an element-wise tanh), but also over the gated unit of GRU, as shown in Section 5.2.\nComparison to Coverage Mechanism \u2013 Recently, Tu et al. (2016) propose adding a coverage mechanism into NMT to alleviate the overtranslation and under-translation problems, which\ndirectly affects the translation adequacy. They maintain a coverage vector to keep track of which source words have been translated. The coverage vector is fed to the attention model to help adjust future attention, which guides NMT to consider more about the un-translated source words. Our approach is complementary: the coverage mechanism produces a better source context representation, while our context gate controls the effect of the source context based on its relative importance. Experiments in Section 5.2 show that combining the two models together can further improve the translation performance. There is one difference as well: coverage mechanism is only applicable to attention-based NMT models, while the context gate is applicable to all NMT models."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Setup", "text": "We carry out experiments on Chinese-English translation. The training dataset consists of 1.25M sentence pairs extracted from LDC corpora4, with 27.9M Chinese words and 34.5M English words respectively. We choose the NIST 2002 (MT02) dataset as development set, and the NIST 2005 (MT05), 2006 (MT06) and 2008 (MT08) datasets as test sets. We use the case-insensitive 4-gram NIST BLEU score (Papineni et al., 2002) as evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test.\nFor efficient training of the neural networks, we limit the source and target vocabularies to the most frequent 30K words in Chinese and English, covering approximately 97.7% and 99.3% of the data in the two languages respectively. All the out-ofvocabulary words are mapped to a special token UNK. We train each model with the sentences of length up to 80 words in the training data. The word embedding dimension is 620 and the size of a hidden layer is 1000. We train our models until the BLEU score on the development set stop improving.\nWe compare our method with state-of-the-art SMT and NMT5 models:\n4The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n5There is some recent progress on aggregating multiple\n\u2022 Moses (Koehn et al., 2007): an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data;\n\u2022 NMT (Bahdanau et al., 2015): an attentionbased NMT model with default setting. We have two variants which differ at the activation function used in the decoder RNN: 1) NMT (vanilla) uses a simple tanh function as the activation function, and 2) NMT (GRU ) uses a sophisticated gate function GRU .\n\u2022 NMT-Coverage (Tu et al., 2016)6: an improved attention-based NMT model with a coverage mechanism.\nWe use the attention-based NMT as baseline, since the attention mechanism is regarded as state of the art NMT."}, {"heading": "5.2 Translation Quality", "text": "Table 2 shows the translation performances in terms of BLEU score. We carry out experiments on multiple NMT variants. For example, \u201c2 + Context Gate (both)\u201d in Row 3 denotes integrating \u201cContext Gate (both)\u201d into the baseline in Row 2 (i.e., models or enlarging the vocabulary(e.g.,, in (Jean et al., 2015)), but here we focus on the generic models.\n6https://github.com/tuzhaopeng/ NMT-Coverage.\nNMT (vanilla)). For baselines, we find that the gated unit (i.e., GRU , Row 4) indeed surpasses its vanilla counterpart (i.e., tanh, Row 2), which is consistent with the results in other work (Chung et al., 2014). Clearly the proposed context gates significantly improve the translation quality in all cases, although there are still considerable differences among different variants. More specifically,\nParameters Context gate introduces few parameters. The newly introduced parameters include Wz \u2208 Rn\u00d7m, Uz \u2208 Rn\u00d7n, Cz \u2208 Rn\u00d7n \u2032 in Equation 4. In this work, the dimensionality of decoding state is n = 1000, the demensionality of word embedding m = 620, and the dimensionality of context representation is n\u2032 = 2000. The context gate only introduces 3.6M additional parameters, which is quite small compared to the number of parameters in the existing models (e.g., 84.3M in the standard NMT model).\nOver NMT (vanilla) We first carry out experiments on a simple decoder without gating function (Rows 2 and 3), which gives a better estimation of context gate. As seen, the proposed context gate significantly improves translation performance by 4.2 BLEU points on average. It is worthy emphasizing that context gate even outperforms a more sophisticated gating function (i.e., GRU in Row 4). This is really encouraging, since our model only has a sin-\ngle gate with half of the parameters (i.e., 3.6M versus 7.2M) and less computations (i.e., half the matrix computations to update the decoding state7).\nOver NMT (GRU) We then investigate the effect of context gate on a standard NMT with GRU as the decoding activation function (Rows 4-7). Several observations can be made. First, context gate also boosts performance on top of GRU in all cases, proving our claim that context gate is complementary to the reset and update gates in GRU. Second, jointly controlling the information from both translation contexts consistently outperforms its singleside counterparts, indicating a direct interaction between input signals from the source and target contexts is useful for NMT models.\nOver NMT-Coverage (GRU) We finally test on a stronger baseline, which employs a coverage mechanism to indicate whether a source word has already been translated or not (Tu et al., 2016). Our context gate still achieves a significant improvement of 1.6 BLEU points on average, reconfirming our claim that the context gate is complimentary to the improved attention model that produces a better source context representation. Finally, our best model (Row 7) outperforms the SMT system using the same data (Row 1) by 3.3 BLEU points with a generic model.\nFrom here on, we refer to \u201cNMT\u201d for \u201cNMT (GRU )\u201d, and \u201cContext Gate\u201d for \u201cContext Gate (both)\u201d if not being explicitly stated.\nSubjective Evaluation We also conduct a subjective evaluation to validate the benefit of incorporating context gate. Two human evaluators are asked\n7We only need to calculate the context gate once via Equation 4 and then apply it to updating the decoding state. In contrast, GRU requires the calculation of an update gate, a reset gate, a proposed updated decoding state and an interpolation between the previous state and the proposed state. Please refer to (Cho et al., 2014) for more details.\nto compare the translations of 200 source sentences randomly sampled from the test sets without knowing from which system a translation is selected. Table 3 shows the results of subjective evaluation. The two human evaluators made close judgments: in adequacy, around 30% of NMT translations are worse, 52% are equal, and 18% are better; while in fluency, around 29% are worse, 52% are equal, and 19% are better."}, {"heading": "5.3 Alignment Quality", "text": "Table 4 lists the alignment performances. Following Tu et al. (2016), we use alignment error rate (AER) (Och and Ney, 2003) and its variant SAER to measure the alignment quality. We find that context gate does not improve alignment quality when it is used alone. When working together with coverage mechanism, however, it produces better alignments, especially one-to-one alignments by selecting the source word with the highest alignment probability per target word (i.e., AER score). One possible reason is that better estimated decoding states (from context gate) and coverage information help to produce more concentrated alignments, as shown in Figure 6."}, {"heading": "5.4 Architecture Analysis", "text": "Table 5 shows a detailed analysis of architecture components measured in BLEU score. Several observations can be made:\n\u2022 Operation Granularity (Rows 2 and 3): Element-wise multiplication (i.e., Context Gate (source)) outperforms the vector-level scalar (i.e., Gating Scalar), indicating that a precise control of each element in the context vector boosts translation performance.\n\u2022 Gate Strategy (Rows 3 and 4): When only fed with previous decoding state si\u22121, Context Gate (both) consistently outperforms Context Gate (source), showing that jointly controlling information from both source and target sides are important for judging the importance of the contexts.\n\u2022 Peephole connections (Rows 4 and 5): Peepholes play an important role in the context gate, which improves the performance by 0.57 in BLEU score.\n\u2022 Previously generated word (Rows 5 and 6): Previously generated word yi\u22121 provides a more explicit signal for the gate to judge the importance of contexts, leading to a further improvement on translation performance."}, {"heading": "5.5 Effects on Long Sentences", "text": "We follow Bahdanau et al. (2015) to group sentences of similar lengths together. Figure 7 shows the BLEU score and the averaged length of translations for each group. As can be seen, NMT favors short translations on long source sentences (i.e., > 40), which may be due to the fact that source context is underutilized. The context gate can alleviate this problem by balancing the source and target contexts, and thus make the decoder favor long translations. In fact, incorporating context gate boosts translation performance on all source sentence groups.\nAs an example, consider this source sentence from the test set:\nzho\u0304uliu\u0300 zhe\u0300ngsh\u0131\u0300 y\u0131\u0304ngguo\u0301 m\u0131\u0301nzho\u0300ng da\u0300o cha\u0304osh\u0131\u0300 ca\u030cigo\u0300u de ga\u0304ofe\u0304ng sh\u0131\u0301ke\u0300, da\u0304ngsh\u0131\u0301 14 jia\u0304 cha\u0304osh\u0131\u0300 de gua\u0304nb\u0131\u0300 l\u0131\u0300ng y\u0131\u0304ngguo\u0301 zhe\u0300 jia\u0304 zu\u0131\u0300 da\u0300 de lia\u0301nsuo\u030c cha\u0304osh\u0131\u0300 su\u030cnsh\u0131\u0304 shu\u0300ba\u030ciwa\u0300n y\u0131\u0304ngba\u0300ng de xia\u0304osho\u0300u sho\u0304uru\u0300 .\nNMT translates it into:\ntwenty - six london supermarkets were closed at a peak hour of the british population in the same period of time .\nwhich almost misses all the information of the source sentence. Integrating context gates can improve the translation adequacy, and the translation is:\nthis is exactly the peak days British people buying the supermarket . the closure of the 14 supermarkets of the 14 supermarkets that the largest chain supermarket in england lost several million pounds of sales income .\nWhen working together with coverage mechanism, the translation is further improved by rectifying the over-translation (e.g., \u201cof the 14 supermarkets\u201d) and under-translation (e.g., \u201csaturday\u201d and \u201cat that time\u201d) problems:\nsaturday is the peak season of british people \u2019s purchases of the supermarket . at that time , the closure of 14 supermarkets made the biggest supermarket of britain lose millions of pounds of sales income ."}, {"heading": "6 Conclusion", "text": "We find that source and target contexts in NMT are highly correlated to translation adequacy and fluency respectively. Based on this observation, we propose using context gates in NMT to dynamically control the contributions from the source and target contexts in generation of target sentence, to enhance the adequacy of NMT. By providing NMT the ability to choose appropriate amount of information from the source and target contexts, one can alleviate the serious inadequate translation problem that NMT suffers from. Experimental results show that NMT with context gates achieves consistent and significant improvements in translation quality over different variants of NMT models.\nThe proposed context gate is in principle applicable to all sequence-to-sequence learning tasks, in which information from the source sequence is transformed to the target sequence (corresponding to adequacy) and the target sequence is generated (corresponding to fluency). In the future, we will investigate the effectiveness of context gate on other tasks, such as dialogue and summarization."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP 2014.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1."], "venue": "ACL 2005.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Recurrent nets that time and count", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber."], "venue": "IJCNN 2000. IEEE.", "citeRegEx": "Gers and Schmidhuber.,? 2000", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural Computation.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL 2015.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP 2013.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "ACL 2007.", "citeRegEx": "Zens et al\\.,? 2007", "shortCiteRegEx": "Zens et al\\.", "year": 2007}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "EMNLP 2015.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz J. Och", "Hermann Ney."], "venue": "Computational Linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "ACL 2002.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Fluency, adequacy, or hter?: exploring different human judgments with a tunable mt metric", "author": ["Matthew Snover", "Nitin Madnani", "Bonnie J Dorr", "Richard Schwartz."], "venue": "Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 259\u2013268.", "citeRegEx": "Snover et al\\.,? 2009", "shortCiteRegEx": "Snover et al\\.", "year": 2009}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "NIPS 2014.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "ACL 2016.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "ICML 2015.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years, whose goal is to construct and utilize a single large neural network to accomplish the entire translation task.", "startOffset": 33, "endOffset": 112}, {"referenceID": 13, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years, whose goal is to construct and utilize a single large neural network to accomplish the entire translation task.", "startOffset": 33, "endOffset": 112}, {"referenceID": 0, "context": "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years, whose goal is to construct and utilize a single large neural network to accomplish the entire translation task.", "startOffset": 33, "endOffset": 112}, {"referenceID": 5, "context": "Several techniques in NMT are proved to be very effective, such as gating (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and attention (Bahdanau et al.", "startOffset": 74, "endOffset": 126}, {"referenceID": 1, "context": "Several techniques in NMT are proved to be very effective, such as gating (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and attention (Bahdanau et al.", "startOffset": 74, "endOffset": 126}, {"referenceID": 0, "context": ", 2014) and attention (Bahdanau et al., 2015) which can model long-distance dependencies and complicated alignment relations ar X iv :1 60 8.", "startOffset": 22, "endOffset": 45}, {"referenceID": 9, "context": "With the encoder-decoder framework as well as the gating and attention techniques, it is reported that the performance of NMT has surpassed the performance of traditional SMT in terms of BLEU score (Luong et al., 2015).", "startOffset": 198, "endOffset": 218}, {"referenceID": 0, "context": "Table 1 shows an example, in which an attentionbased NMT system (Bahdanau et al., 2015) generates a fluent yet inadequate translation (e.", "startOffset": 64, "endOffset": 87}, {"referenceID": 12, "context": "Fluency measures whether the translation is fluent, while adequacy measures whether the translation is faithful to the original sentence (Snover et al., 2009).", "startOffset": 137, "endOffset": 158}, {"referenceID": 0, "context": "3 BLEU points over a standard attention-based NMT system (Bahdanau et al., 2015).", "startOffset": 57, "endOffset": 80}, {"referenceID": 1, "context": "It can be either a vanilla RNN unit using tanh function, or a sophisticated gated RNN unit such as GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997).", "startOffset": 103, "endOffset": 121}, {"referenceID": 5, "context": ", 2014) or LSTM (Hochreiter and Schmidhuber, 1997).", "startOffset": 16, "endOffset": 50}, {"referenceID": 1, "context": ", si \u2261 hJ ) (Cho et al., 2014; Sutskever et al., 2014), or a dynamic vector that selectively summarizes certain parts of the source sentence at each decoding step (e.", "startOffset": 12, "endOffset": 54}, {"referenceID": 13, "context": ", si \u2261 hJ ) (Cho et al., 2014; Sutskever et al., 2014), or a dynamic vector that selectively summarizes certain parts of the source sentence at each decoding step (e.", "startOffset": 12, "endOffset": 54}, {"referenceID": 0, "context": ", si = \u2211J j=1 \u03b1i,jhj in which \u03b1i,j is alignment probability calculated by an attention model) (Bahdanau et al., 2015).", "startOffset": 94, "endOffset": 117}, {"referenceID": 14, "context": "The recently proposed coverage based technique can address this problem (Tu et al., 2016).", "startOffset": 72, "endOffset": 89}, {"referenceID": 5, "context": "Inspired by the success of gated units in RNN (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), we propose using a context gate to dynamically control the amount of information flowing from the source and target contexts and thus balance the fluency and adequacy of NMT at each decoding step.", "startOffset": 46, "endOffset": 98}, {"referenceID": 1, "context": "Inspired by the success of gated units in RNN (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), we propose using a context gate to dynamically control the amount of information flowing from the source and target contexts and thus balance the fluency and adequacy of NMT at each decoding step.", "startOffset": 46, "endOffset": 98}, {"referenceID": 5, "context": "The first two strategies are inspired by the concept of output gate in LSTM (Hochreiter and Schmidhuber, 1997), which controls the amount of memory content utilized.", "startOffset": 76, "endOffset": 110}, {"referenceID": 15, "context": "Comparison to (Xu et al., 2015) \u2013 The context gate takes inspiration form the gating scalar model proposed by (Xu et al.", "startOffset": 14, "endOffset": 31}, {"referenceID": 15, "context": ", 2015) \u2013 The context gate takes inspiration form the gating scalar model proposed by (Xu et al., 2015) to perform the image caption generation task.", "startOffset": 86, "endOffset": 103}, {"referenceID": 15, "context": "1 Xu et al. (2015) uses a scalar that is shared by all elements in the source context, while we employ a gate that outputs a distinct weight for each element.", "startOffset": 2, "endOffset": 19}, {"referenceID": 4, "context": "It has been shown that peephole connections make precise timings easier to learn (Gers and Schmidhuber, 2000).", "startOffset": 81, "endOffset": 109}, {"referenceID": 15, "context": "Figure 5: Comparison to Gating Scalar proposed by (Xu et al., 2015).", "startOffset": 50, "endOffset": 67}, {"referenceID": 13, "context": "Comparison to Gated RNN \u2013 State-of-the-art NMT models (Sutskever et al., 2014; Bahdanau et al., 2015) generally employ a gated unit (e.", "startOffset": 54, "endOffset": 101}, {"referenceID": 0, "context": "Comparison to Gated RNN \u2013 State-of-the-art NMT models (Sutskever et al., 2014; Bahdanau et al., 2015) generally employ a gated unit (e.", "startOffset": 54, "endOffset": 101}, {"referenceID": 14, "context": "Comparison to Coverage Mechanism \u2013 Recently, Tu et al. (2016) propose adding a coverage mechanism into NMT to alleviate the overtranslation and under-translation problems, which directly affects the translation adequacy.", "startOffset": 45, "endOffset": 62}, {"referenceID": 11, "context": "We use the case-insensitive 4-gram NIST BLEU score (Papineni et al., 2002) as evaluation metric, and sign-test (Collins et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 3, "context": ", 2002) as evaluation metric, and sign-test (Collins et al., 2005) for statistical significance test.", "startOffset": 44, "endOffset": 66}, {"referenceID": 14, "context": "\u201cNMT-Coverage\u201d denotes attention-based NMT with a coverage mechanism to indicate whether a source word is translated or not (Tu et al., 2016).", "startOffset": 124, "endOffset": 141}, {"referenceID": 0, "context": "\u2022 NMT (Bahdanau et al., 2015): an attentionbased NMT model with default setting.", "startOffset": 6, "endOffset": 29}, {"referenceID": 14, "context": "\u2022 NMT-Coverage (Tu et al., 2016)6: an improved attention-based NMT model with a coverage mechanism.", "startOffset": 15, "endOffset": 32}, {"referenceID": 6, "context": ",, in (Jean et al., 2015)), but here we focus on the generic models.", "startOffset": 6, "endOffset": 25}, {"referenceID": 2, "context": ", tanh, Row 2), which is consistent with the results in other work (Chung et al., 2014).", "startOffset": 67, "endOffset": 87}, {"referenceID": 14, "context": "Over NMT-Coverage (GRU) We finally test on a stronger baseline, which employs a coverage mechanism to indicate whether a source word has already been translated or not (Tu et al., 2016).", "startOffset": 168, "endOffset": 185}, {"referenceID": 1, "context": "Please refer to (Cho et al., 2014) for more details.", "startOffset": 16, "endOffset": 34}, {"referenceID": 10, "context": "(2016), we use alignment error rate (AER) (Och and Ney, 2003) and its variant SAER to measure the alignment quality.", "startOffset": 42, "endOffset": 61}, {"referenceID": 13, "context": "Following Tu et al. (2016), we use alignment error rate (AER) (Och and Ney, 2003) and its variant SAER to measure the alignment quality.", "startOffset": 10, "endOffset": 27}, {"referenceID": 0, "context": "We follow Bahdanau et al. (2015) to group sentences of similar lengths together.", "startOffset": 10, "endOffset": 33}], "year": 2017, "abstractText": "In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts on the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to lack of effective control on the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose to use context gates to dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance the adequacy of NMT while keeping the fluency unchanged. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.", "creator": "LaTeX with hyperref package"}}}