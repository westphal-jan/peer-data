{"id": "1608.06203", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2016", "title": "Computational and Statistical Tradeoffs in Learning to Rank", "abstract": "For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data.", "histories": [["v1", "Mon, 22 Aug 2016 15:58:31 GMT  (480kb,D)", "http://arxiv.org/abs/1608.06203v1", "30 pages 5 figures"]], "COMMENTS": "30 pages 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["ashish khetan", "sewoong oh"], "accepted": true, "id": "1608.06203"}, "pdf": {"name": "1608.06203.pdf", "metadata": {"source": "CRF", "title": "Computational and Statistical Tradeoffs in Learning to Rank", "authors": ["Ashish Khetan", "Sewoong Oh"], "emails": ["khetan2@illinois.edu", "swoh@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "In classical statistical inference, we are typically interested in characterizing how more data points improve the accuracy, with little restrictions or considerations on computational aspects of solving the inference problem. However, with massive growths of the amount of data available and also the complexity and heterogeneity of the collected data, computational resources, such as time and memory, are major bottlenecks in many modern applications. As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015). Guided by sharp analyses on the sample complexity, these approaches provide theoretically sound guidelines that allow the analyst the flexibility to fall back to simpler algorithms to enjoy the full merit of the improved run-time.\nInspired by these advances, we study the time-data tradeoff in learning to rank. In many applications such as election, policy making, polling, and recommendation systems, we want to aggregate individual preferences to produce a global ranking that best represents the collective social preference. Learning to rank is a rank aggregation approach, which assumes that the data comes from a parametric family of choice models, and learns the parameters that determine the global ranking. Traditionally, each revealed preference is assumed to have one of the following three structures. Pairwise comparison, where one item is preferred over another, is common in sports and chess matches. Best-out-of-\u03ba comparison, where one is chosen among a set of \u03ba alternatives, is common in historical purchase data. \u03ba-way comparison, where we observe a linear ordering of a set of \u03ba candidates, is used in some elections and surveys. We will refer to such structures as traditional in\nar X\niv :1\n60 8.\n06 20\n3v 1\n[ cs\n.L G\n] 2\n2 A\nug 2\ncomparisons to modern datasets with non-traditional structures whose behavior change drastically. For such traditional preferences, efficient schemes for learning to rank have been proposed, such as Ford Jr. (1957); Hunter (2004); Hajek et al. (2014); Chen and Suh (2015), which we explain in detail in Section 1.1. However, modern datasets are unstructured and heterogeneous. As Khetan and Oh (2016) show, this can lead to significant increase in the computational complexity, requiring exponential run-time in the size of the problem in the worst case.\nTo alleviate this computational challenge, we propose a hierarchy of estimators which we call generalized rank-breaking, ordered in increasing computational complexity and achieving increasing accuracy. The key idea is to break down the heterogeneous revealed preferences into simpler pieces of ordinal relations, and apply an estimator tailored for those simple structures treating each piece as independent. Several aspects of rank-breaking makes this problem interesting and challenging. A priori, it is not clear which choices of the simple ordinal relations are rich enough to be statistically efficient and yet lead to tractable estimators. Even if we identify which ordinal relations to extract, the ignored correlations among those pieces can lead to an inconsistent estimate, unless we choose carefully which pieces to include and which to omit in the estimation. We further want sharp analysis on the sample complexity, which reveals how computational and statistical efficiencies trade off. We would like to address all these challenges in providing generalized rank-breaking methods.\nProblem formulation. We study the problem of aggregating ordinal data based on users\u2019 preferences that are expressed in the form of partially ordered sets (poset). A poset is a collection of ordinal relations among items. For example, consider a poset {(i6 \u227a {i5, i4}), (i5 \u227a i3), ({i3, i4} \u227a {i1, i2})} over items {i1, . . . , i6}, where (i6 \u227a {i5, i4}) indicates that item i5 and i4 are both preferred over item i6. Such a relation is extracted from, for example, the user giving a 2-star rating to i5 and i4 and a 1-star to i6. Assuming that the revealed preference is consistent, a poset can be represented as a directed acyclic graph (DAG) Gj as below.\nWe assume that each user j is presented with a subset of items Sj , and independently provides her ordinal preference in the form of a poset, where the ordering is drawn from the Plackett-Luce (PL) model. The PL model is a popular choice model from operations research and psychology, used to model how people make choices under uncertainty. It is a special case of random utility models, where each item i is parametrized by a latent true utility \u03b8i \u2208 R. When offered with Sj , the user samples the perceived utility Ui for each item independently according to Ui = \u03b8i + Zi, where Zi\u2019s are i.i.d. noise. In particular, the PL model assumes Zi\u2019s follow the standard Gumbel distribution. The observed poset is a partial observation of the ordering according to this perceived utilities. We discuss possible extensions to general class of random utility models in Section 1.1.\nThe particular choice of the Gumbel distribution has several merits, largely stemming from the fact that the Gumbel distribution has a log-concave pdf and is inherently memoryless. In our analyses, we use the log-concavity to show that our proposed algorithm is a concave maximization (Remark 2.1) and the memoryless property forms the basis of our rank-breaking idea. Precisely, the PL model is statistically equivalent to the following procedure. Consider a ranking as a mapping from a position in the rank to an item, i.e. \u03c3j : [|Sj |] \u2192 Sj . It can be shown that the PL model is generated by first independently assigning each item i \u2208 Sj an unobserved value Yi, exponentially distributed with mean e\u2212\u03b8i , and the resulting ranking \u03c3j is inversely ordered in Yi\u2019s so that Y\u03c3j(1) \u2264 Y\u03c3j(2) \u2264 \u00b7 \u00b7 \u00b7 \u2264 Y\u03c3j(|Sj |).\nThis inherits the memoryless property of exponential variables, such that P(Y1 < Y2 < Y3) = P(Y1 < {Y2, Y3})P(Y2 < Y3), leading to a simple interpretation of the PL model as sequential choices:\nP(i3 \u227a i2 \u227a i1) = P({i3, i2} \u227a i1)P(i3 \u227a i2) = e\u03b8i1 e\u03b8i1 + e\u03b8i2 + e\u03b8i3 \u00d7 e \u03b8i2 e\u03b8i2 + e\u03b8i3 .\nIn general, we have P[\u03c3j ] = \u220f|Sj |\u22121 i=1 (e \u03b8\u2217 \u03c3j(i))/( \u2211|Sj | i\u2032=i e \u03b8\u2217 \u03c3j(i \u2032)). We assume that the true utility \u03b8\u2217 \u2208 \u2126b where\n\u2126b = { \u03b8 \u2208 Rd \u2223\u2223 \u2211 i\u2208[d] \u03b8i = 0, |\u03b8i| \u2264 b for all i \u2208 [d] } . (1)\nNotice that centering of \u03b8 ensures its uniqueness as PL model is invariant under shifting of \u03b8. The bound b on \u03b8i is written explicitly to capture the dependence in our main results.\nWe denote a set of n users by [n] = {1, . . . , n} and the set of d items by [d]. Let Gj denote the DAG representation of the poset provided by the user j over Sj \u2286 [d] according to the PL model with weights \u03b8\u2217. The maximum likelihood estimate (MLE) maximizes the sum of all possible rankings that are consistent with the observed Gj for each j:\n\u03b8\u0302 \u2208 arg max \u03b8\u2208\u2126b { n\u2211 j=1 log ( \u2211 \u03c3\u2208Gj P\u03b8[\u03c3] )} , (2)\nwhere we slightly abuse the notation Gj to denote the set of all rankings \u03c3 that are consistent with the observation. When Gj has a traditional structure as explained earlier in this section, then the optimization is a simple multinomial logit regression, that can be solved efficiently with off-the-shelf convex optimization tools. Hajek et al. (2014) provides full analysis of the statistical complexity of this MLE under traditional structures. For general posets, it can be shown that the above optimization is a concave maximization, using similar techniques as Remark 2.1. However, the summation over rankings in Gj can involve number of terms super exponential in the size |Sj |, in the worst case. This renders MLE intractable and impractical.\nPairwise rank-breaking. A common remedy to this computational blow-up is to use rankbreaking. Rank-breaking traditionally refers to pairwise rank-breaking, where a bag of all the pairwise comparisons is extracted from observations {Gj}j\u2208[n] and is applied to estimators that are tailored for pairwise comparisons, treating each paired outcome as independent. This is one of the motivations behind the algorithmic advances in the popular topic of learning from pairwise\ncomparisons in (Ford Jr., 1957; Hunter, 2004; Negahban et al., 2014; Shah et al., 2015a; Maystre and Grossglauser, 2015).\nIt is computationally efficient to apply maximum likelihood estimator assuming independent pairwise comparisons, which takes O(d2) operations to evaluate. However, this computational gain comes at the cost of statistical efficiency. Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples. In the example from Figure 1, there are 12 paired relations implied by the DAG: (i6 \u227a i5), (i6 \u227a i4), (i6 \u227a i3), . . . , (i3 \u227a i1), (i4 \u227a i1). In order to get a consistent estimate, Azari Soufiani et al. (2014) provide a rule for choosing which pairs to include, and Khetan and Oh (2016) provide an estimator that optimizes how to weigh each of those chosen pairs to get the best finite sample complexity bound. However, such a consistent pairwise rank-breaking results in throwing away many of the ordered relations, resulting in significant loss in accuracy. For example, Including a paired relation from Gj in the example results in a biased estimator. None of the pairwise orderings can be used from Gj , without making the estimator inconsistent as shown in Azari Soufiani et al. (2013). Whether we include all paired comparisons or only a subset of consistent ones, there is a significant loss in accuracy as illustrated in Figure 3. For the precise condition for consistent rank-breaking we refer to (Azari Soufiani et al., 2013, 2014; Khetan and Oh, 2016).\nThe state-of-the-art approaches operate on either one of the two extreme points on the computational and statistical trade-off. The MLE in (2) requires O( \u2211 j\u2208[n] |Sj |!) summations to just evaluate the objective function, in the worst case. On the other hand, the pairwise rank-breaking requires only O(d2) summations, but suffers from significant loss in the sample complexity. Ideally, we would like to give the analyst the flexibility to choose a target computational complexity she is willing to tolerate, and provide an algorithm that achieves the optimal trade-off at the chosen operating point.\nContribution. We introduce a novel generalized rank-breaking that bridges the gap between MLE and pairwise rank-breaking. Our approach allows the user the freedom to choose the level of computational resources to be used, and provides an estimator tailored for the desired complexity. We prove that the proposed estimator is tractable and consistent, and provide an upper bound on the error rate in the finite sample regime. The analysis explicitly characterizes the dependence on the topology of the data. This in turn provides a guideline for designing surveys and experiments in practice, in order to maximize the sample efficiency. We provide numerical experiments confirming the theoretical guarantees."}, {"heading": "1.1 Related work", "text": "In classical statistics, one is interested in the tradeoff between the sample size and the accuracy, with little considerations to the computational complexity or time. As more computations are typically required with increasing availability of data, the computational resources are often the bottleneck. Recently, a novel idea known as \u201calgorithmic weakening\u201d has been investigated to overcome such a bottleneck, in which a hierarchy of algorithms is proposed to allow for faster algorithms at the expense of decreased accuracy. When guided by sound theoretical analyses, this idea allows the statistician to achieve the same level of accuracy and save time when more data is available. This is radically different from classical setting where processing more data typically requires more computational time.\nDepending on the application, several algorithmic weakenings have been studied. In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available. Various gradient based algorithms are analyzed that show the time-accuracy-sample tradeoff. In a similar context, Shalev-Shwartz and Srebro (2008) analyze a particular implementation of support vector machine and show that the target accuracy can be achieved faster when more data is available, by running the iterative algorithm for shorter amount of time. In the application of de-noising, Chandrasekaran and Jordan (2013) provide a hierarchy of convex relaxations where constraints are defined by convex geometry with increasing complexity. For unsupervised learning, Lucic et al. (2015) introduce a hierarchy of data representations that provide more representative elements when more data is available at no additional computation. Standard clustering algorithms can be applied to thus generated summary of the data, requiring less computational complexity.\nIn the application of learning to rank, we follow the principle of algorithmic weakening and propose a novel rank-breaking to allow the practitioner to navigate gracefully the time-sample trade off as shown in the figure below. We propose a hierarchy of estimators indexed by M \u2208 Z+ indicating how complex the estimator is (defined formally in Section 2). Figure 2 shows the result of a experiment on synthetic datasets on how much time (in seconds) and how many samples are required to achieve a target accuracy. If we are given more samples, then it is possible to achieve the target accuracy, which in this example is MSE\u2264 0.3d2 \u00d7 10\u22126, with fewer operations by using a simpler estimator with smaller M . The details of the experiment is explained in Figure 3.\nLearning to rank from the PL model has been studied extensively under the traditional scenario dating back to Zermelo (1929) who first introduced the PL model for pairwise comparisons. Various approaches for estimating the PL weights from traditional samples have been proposed. The problem can be formulated as a convex optimization that can be solved efficiently using the off-the-shelf solvers. However, tailored algorithms for finding the optimal solution have been pro-\nposed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions.\nOn the theoretical side, when samples consist of pairwise comparisons, Simons and Yao (1999) first established consistency and asymptotic normality of the maximum likelihood estimate when all teams play against each other. For a broader class of scenarios where we allow for sparse observations, where the number of total comparisons grow linearly in the number of teams, Negahban et al. (2014) show that Rank Centrality achieves optimal sample complexity by comparing it to a lower bound on the minimax rate. For a more general class of traditional observations, including pairwise comparisons, Hajek et al. (2014) provide similar optimal guarantee for the maximum likelihood estimator. Chen and Suh (2015) introduced Spectral MLE that applies Rank Centrality followed by MLE, and showed that the resulting estimate is optimal in L\u221e error as well as the previously analyzed L2 error. Shah et al. (2015a) study a new measure of the error induced by the Laplacian of the comparisons graph and prove a sharper upper and lower bounds that match up to a constant factor.\nHowever, in modern applications, the computational complexity of the existing approaches blowup due to the heterogeneity of modern datasets. Although, statistical and computational tradeoffs have been investigated under other popular choice models such as the Mallows models by Betzler et al. (2014) or stochastically transitive models by Shah et al. (2015b), the algorithmic solutions do not apply to random utility models and the analysis techniques do not extend. We provide a novel rank-breaking algorithms and provide finite sample complexity analysis under the PL model. This approach readily generalizes to some RUMs such as the flipped Gumbel distribution. However, it is also known from Azari Soufiani et al. (2014), that for general RUMs there is no consistent rank-breaking, and the proposed approach does not generalize."}, {"heading": "2. Generalized rank-breaking", "text": "Given Gj \u2019s representing the users\u2019 preferences, generalized rank-breaking extracts a set of ordered relations and applies an estimator treating each ordered relation as independent. Concretely, for each Gj , we first extract a maximal ordered partition Pj of Sj that is consistent with Gj . An ordered partition is a partition with a linear ordering among the subsets, e.g. Pj = ({i6} \u227a {i5, i4, i3} \u227a {i2, i1}) for Gj from Figure 1. This is maximal, since we cannot further partition any of the subsets without creating artificial ordered relations that are not present in the original Gj .\nThe extracted ordered partition is represented by a directed hypergraph Gj(Sj , Ej), which we call a rank-breaking graph. Each edge e = (B(e), T (e)) \u2208 Ej is a directed hyper edge from a subset of nodes B(e) \u2286 Sj to another subset T (e) \u2286 Sj . The number of edges in Ej is |Pj |\u22121 where |Pj | is the number of subsets in the partition. For each subset in Pj except for the least preferred subset, there is a corresponding edge whose top-set T (e) is the subset, and the bottom-set B(e) is the set of all items less preferred than T (e). For the example in Figure 1, we have Ej = {e1, e2} where e1 = (B(e1), T (e1)) = ({i6, i5, i4, i3}, {i2, i1}) and e2 = (B(e2), T (e2) = ({i6}, {i5, i4, i3}) extracted\nfrom Gj . Denote the probability that T (e) is preferred over B(e) when T (e) \u222aB(e) is offered as\nP\u03b8(e) = P\u03b8 ( B(e) \u227a T (e) ) = \u2211 \u03c3\u2208\u039bT (e)\nexp (\u2211|T (e)|\nc=1 \u03b8\u03c3(c) ) \u220f|T (e)| u=1 (\u2211|T (e)| c\u2032=u exp ( \u03b8\u03c3(c\u2032) ) + \u2211 i\u2208B(e) exp (\u03b8i) ) (3)\nwhich follows from the definition of the PL model, where \u039bT (e) is the set of all rankings over T (e). The computational complexity of evaluating this probability is determined by the size of the top-set |T (e)|, as it involves (|T (e)|!) summations.\nWe let the analyst choose the order M \u2208 Z+ depending on how much computational resource is available, and only include those edges with |T (e)| \u2264M in the following step. We apply the MLE for comparisons over paired subsets, assuming all rank-breaking graphs are independently drawn. Precisely, we propose order-M rank-breaking estimate, which is the solution that maximizes the log-likelihood under the independence assumption:\n\u03b8\u0302 \u2208 arg max \u03b8\u2208\u2126b LRB(\u03b8) , where LRB(\u03b8) = \u2211 j\u2208[n] \u2211 e\u2208Ej :|T (e)|\u2264M logP\u03b8(e) . (4)\nIn a special case when M = 1, this can be transformed into the traditional pairwise rank-breaking, where (i) this is a concave maximization; (ii) the estimate is (asymptotically) unbiased and consistent as shown in Azari Soufiani et al. (2013, 2014); and (iii) and the finite sample complexity have been analyzed in Khetan and Oh (2016). Although, this order-1 rank-breaking provides a significant gain in computational efficiency, the information contained in higher-order edges are unused, resulting in a significant loss in accuracy.\nWe provide the analyst the freedom to choose the computational complexity he/she is willing to tolerate. However, for general M , it has not been known if the optimization in (4) is tractable and/or if the solution is consistent. Since P\u03b8(B(e) \u227a T (e)) as explicitly written in (3) is a sum of log-concave functions, it is not clear if the sum is also log-concave. Due to the ignored dependency in the formulation (4), it is not clear if the resulting estimate is consistent. We first establish that it is a concave maximization in Remark 2.1, then prove consistency in Remark 2.2, and provide a sharp analysis of the performance in the finite sample regime, characterizing the trade-off between computation and sample size in Section 4. We use the Random Utility Model (RUM) interpretation of the PL model to prove concavity. We refer to Section 5.1 for a proof.\nRemark 2.1. LRB(\u03b8) is concave in \u03b8 \u2208 Rd.\nIn order to discuss consistency of the proposed approach, we need to specify how we sample the set of items to be offered Sj and also which partial ordering over Sj is to be observed. Here, we consider a simple but canonical scenario for sampling ordered relations, and show the proposed method is consistent for all non-degenerate cases. However, we study a more general sampling scenario, when we analyze the order-M estimator in the finite sample regime in Section 4.\nFollowing is the canonical sampling scenario. There is a set of \u02dc\u0300 integers (m\u03031, . . . , m\u0303\u02dc\u0300) whose sum is strictly less than d. A new arriving user is presented with all d items and is asked to provide her top m\u03031 items as an unordered set, and then the next m\u03032 items, and so on. This is sampling from the PL model and observing an ordered partition with (\u02dc\u0300+ 1) subsets of sizes m\u0303a\u2019s, and the last subset includes all remaining items. We apply the generalized rank-breaking to get rank-breaking graphs {Gj} with \u02dc\u0300edges each, and order-M estimate is computed. We show that this is consistent,\ni.e. asymptotically unbiased in the limit of the number of users n. A proof is provided in Section 5.2.\nRemark 2.2. Under the PL model and the above sampling scenario, the order-M rank-breaking estimate \u03b8\u0302 in (4) is consistent for all choices of M \u2265 mina\u2208\u02dc\u0300m\u0303a.\nFigure 3 (left panel) shows the accuracy-sample tradeoff for increasing computation M on the same data. As predicted by the anlaysis, generalized rank-breaking (GRB) is consistent (Remark 2.2) and the error decays at rate (1/n) (Theorem 4.1), and decreases with increase in M , order of GRB. For comparison, we also plot the error achieved by pairwise rank-breaking (PRB) approach where we include all paired relations derived from data, which we call inconsistent PRB. As predicted by Azari Soufiani et al. (2014), this results in an inconsistent estimate, whose error does not vanish as we increase the sample size. Notice that including all paired comparisons increases bias, but also decreases variance of the estimate. Hence, when sample size is limited and variance is dominating the bias, it is actually beneficial to include those biased paired relations to gain in variance at the cost of increased bias. Theoretical analysis of such a bias-variance tradeoff is outside the scope of this paper, but proposes an interesting research direction. We fix d = 256, \u02dc\u0300= 5, m\u0303a = a for a \u2208 {1, 2, 3, 4, 5}, and sample posets from the canonical scenario, except that each user is presented \u03ba = 32 random items. The PL weights are chosen i.i.d. U [\u22122, 2]. On the right panel, we let m\u0303a = 3 for all a \u2208 [\u02dc\u0300] and vary \u02dc\u0300\u2208 {1, 2, 4, 8, 16}. We are providing more observations as \u02dc\u0300 where |Ej | = \u02dc\u0300. The proposed GRB with M = 3 makes the full use of the given observations and achieve decreasing error, whereas for PRB the increased bias dominates the error. For comparisons, we provide the error achieved by an oracle estimator who knows the exact ordering among those items belonging to the top-sets and runs MLE. For example, if \u02dc\u0300= 2, the GRB observes an ordering ({i1, i2, i4, i5, . . .} \u227a {i17, i3, i6} \u227a {i9, i2, i11}) whereas the oracle estimator has extra information on the ordering among those top sets, i.e. ({i1, i2, i4, i5, . . .} \u227a i17 \u227a i3 \u227a i6 \u227a i9 \u227a i2 \u227a i11}).\nPerhaps surprisingly, GRB is able to achieve a similar performance without this significant this extra information, unless |Ej | is large. The performance degradation in large |Ej | regime is precisely captured in our main analysis in Theorem 4.1.\nNotations. We use n to denote the number of users providing partial rankings, indexed by j \u2208 [n] where [n] = {1, 2, . . . , n}. We use d to denote the number of items, indexed by i \u2208 [d]. Given rank-breaking graphs {Gj(Sj , Ej)}j\u2208[n] extracted from the posets {Gj}, we first define the order M rank-breaking graphs {G(M)j (Sj , E (M) j )}, where E (M) j is a subset of Ej that includes only those edges ej \u2208 Ej with |T (ej)| \u2264M . This represents those edges that are included in the estimation for a choice of M . For finite sample analysis, the following quantities capture how the error depends on the topology of the data collected. Let \u03baj \u2261 |Sj | and `j \u2261 |E(M)j |. We index each edge ej in E\n(M) j by a \u2208 [`j ] and define mj,a \u2261 |T (ej,a)| for the a-th edge of the j-th rank-breaking graph and rj,a \u2261 |T (ej,a)|+ |B(ej,a)|. Note that, we use tilde in subscript with mj,a and `j when M is equal to Sj . That is \u02dc\u0300j is the number of edges in Ej and m\u0303j,a is the size of the top-sets in those edges. We\nlet pj \u2261 \u2211 a\u2208[`j ]mj,a denote the effective sample size for the observation G (M) j , such that the total\neffective sample size is \u2211\nj\u2208[n] pj . Notice that although we do not explicitly write the dependence on M , all of the above quantities implicitly depend on the choice of M ."}, {"heading": "3. Comparison graph", "text": "The analysis of the optimization in (4) shows that, with high probability, LRB(\u03b8) is strictly concave with \u03bb2(H(\u03b8)) \u2264 \u2212Cb\u03b31\u03b32\u03b33\u03bb2(L) < 0 for all \u03b8 \u2208 \u2126b (Lemma 5.4), and the gradient is also bounded with \u2016\u2207LRB(\u03b8\u2217)\u2016 \u2264 C \u2032b\u03b3 \u22121/2 2 ( \u2211 j pj log d)\n1/2 (Lemma 5.3). the quantities \u03b31, \u03b32, \u03b33, and \u03bb2(L), to be defined shortly, represent the topology of the data. This leads to Theorem 4.1:\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 2\u2016\u2207LRB(\u03b8\u2217)\u2016 \u2212\u03bb2(H(\u03b8)) \u2264 C \u2032\u2032b\n\u221a\u2211 j pj log d\n\u03b31\u03b3 3/2 2 \u03b33\u03bb2(L)\n, (5)\nwhere Cb, C \u2032 b, and C \u2032\u2032 b are constants that only depend on b, and \u03bb2(H(\u03b8)) is the second largest eigenvalue of a negative semidefinite Hessian matrix H(\u03b8) of LRB(\u03b8). Recall that \u03b8>1 = 0 since we restrict our search in \u2126b. Hence, the error depends on \u03bb2(H(\u03b8)) instead of \u03bb1(H(\u03b8)) whose corresponding eigen vector is the all-ones vector. We define a comparison graph H([d], E) as a weighted undirected graph with weights Aii\u2032 = \u2211 j\u2208[n]:i,i\u2032\u2208Sj pj/(\u03baj(\u03baj \u2212 1)). The corresponding graph Laplacian is defined as:\nL \u2261 n\u2211 j=1 pj \u03baj(\u03baj \u2212 1) \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> . (6)\nIt is immediate that \u03bb1(L) = 0 with 1 as the eigenvector. There are remaining d\u22121 eigenvalues that sum to Tr(L) = \u2211 j pj . The rescaled \u03bb2(L) and \u03bbd(L) capture the dependency on the topology:\n\u03b1 \u2261 \u03bb2(L)(d\u2212 1) Tr(L) , \u03b2 \u2261 Tr(L) \u03bbd(L)(d\u2212 1) . (7)\nIn an ideal case where the graph is well connected, then the spectral gap of the Laplacian is large. This ensures all eigenvalues are of the same order and \u03b1 = \u03b2 = \u0398(1), resulting in a smaller error\nrate. The concavity of LRB(\u03b8) also depends on the following quantities. We discuss the role of the topology in Section 4. Note that the quantities defined in this section implicitly depend on the choice of M , which controls the necessary computational power, via the definition of the rankbreaking {Gj,a}. We define the following quantities that control our upper bound. \u03b31 incorporates asymmetry in probabilities of items being ranked at different positions depending upon their weight \u03b8\u2217i . It is 1 for b = 0 that is when all the items have same weight, and decreases exponentially with increase in b. \u03b32 controls the range of the size of the top-set with respect to the size of the bottomset for which the error decays with the rate of 1/(size of the top-set). The dependence in \u03b33 and \u03bd are due to weakness in the analysis, and ensures that the Hessian matrix is strictly negative definite.\n\u03b31 \u2261 min j,a\n{( rj,a \u2212mj,a\n\u03baj\n)2e2b\u22122} , \u03b32 \u2261 min\nj,a\n{( rj,a \u2212mj,a\nrj,a\n)2} , and (8)\n\u03b33 \u2261 1\u2212max j,a\n{ 4e16b\n\u03b31\nm2j,ar 2 j,a\u03ba 2 j\n(rj,a \u2212mj,a)5\n} , \u03bd \u2261 max\nj,a\n{ mj,a\u03ba 2 j\n(rj,a \u2212mj,a)2\n} . (9)"}, {"heading": "4. Main Results", "text": "We present main theoretical analyses and numerical simulations confirming the theoretical predictions."}, {"heading": "4.1 Upper bound on the achievable error", "text": "We provide an upper bound on the error for the order-M rank-breaking approach, showing the explicit dependence on the topology of the data. We assume each user provides a partial ranking according to his/her ordered partitions. Precisely, we assume that the set of offerings Sj , the number of subsets (\u02dc\u0300j + 1), and their respective sizes (m\u0303j,1, . . . , m\u0303j,\u02dc\u0300j ) are predetermined. Each user randomly draws a ranking of items from the PL model, and provides the partial ranking of the form ({i6} \u227a {i5, i4, i3} \u227a {i2, i1}) in the example in Figure 1. For a choice of M , the order-M rank-breaking graph is extracted from this data. The following theorem provides an upper bound on the achieved error, and a proof is provided in Section 5.\nTheorem 4.1. Suppose there are n users, d items parametrized by \u03b8\u2217 \u2208 \u2126b, and each user j \u2208 [n] is presented with a set of offerings Sj \u2286 [d] and provides a partial ordering under the PL model. For a choice of M \u2208 Z+, if \u03b33 > 0 and the effective sample size \u2211n j=1 pj is large enough such that\nn\u2211 j=1 pj \u2265 214e20b\u03bd2 (\u03b1\u03b31\u03b32\u03b33)2\u03b2 pmax \u03bamin d log d , (10)\nwhere b \u2261 maxi |\u03b8\u2217i | is the dynamic range, pmax = maxj\u2208[n] pj, \u03bamin = minj\u2208[n] \u03baj, \u03b1 is the (rescaled) spectral gap, \u03b2 is the (rescaled) spectral radius in (7), and \u03b31, \u03b32, \u03b33, and \u03bd are defined in (8) and (9), then the generalized rank-breaking estimator in (4) achieves\n1\u221a d \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016 \u2264 40e\n7b\n\u03b1\u03b31\u03b3 3/2 2 \u03b33\n\u221a d log d\u2211n\nj=1 \u2211`j a=1mj,a , (11)\nwith probability at least 1 \u2212 3e3d\u22123. Moreover, for M \u2264 3 the above bound holds with \u03b33 replaced by one, giving a tighter result.\nNote that the dependence on the choice of M is not explicit in the bound, but rather is implicit in the construction of the comparison graph and the number of effective samples N = \u2211 j \u2211 a\u2208[`j ]mj,a. In an ideal case, b = O(1) and mj,a = O(r 1/2 j,a ) for all (j, a) such that \u03b31, \u03b32 are finite. further, if the spectral gap is large such that \u03b1 > 0 and \u03b2 > 0, then Equation (11) implies that we need the effective sample size to scale as O(d log d), which is only a logarithmic factor larger than the number of parameters. In this ideal case, there exist universal constants C1, C2 such that if mj,a < C1 \u221a rj,a and rj,a > C2\u03baj for all {j, a}, then the condition \u03b33 > 0 is met. Further, when rj,a = O(\u03baj,a), max\u03baj,a/\u03baj\u2032,a\u2032 = O(1), and max pj,a/pj\u2032,a\u2032 = O(1), then condition on the effective sample size is met with \u2211 j pj = O(d log d). We believe that dependence in \u03b33 is weakness of our analysis and there is no dependence as long as mj,a < rj,a."}, {"heading": "4.2 Lower bound on computationally unbounded estimators", "text": "Recall that \u02dc\u0300j \u2261 |Ej |, m\u0303j,a = |T (ea)| and r\u0303j,a = |T (ea) \u222a B(ea)| when M = Sj . We prove a fundamental lower bound on the achievable error rate that holds for any unbiased estimator even with no restrictions on the computational complexity. For each (j, a), define \u03b7j,a as\n\u03b7j,a = m\u0303j,a\u22121\u2211 u=0 ( 1 r\u0303j,a \u2212 u + u(m\u0303j,a \u2212 u) m\u0303j,a(r\u0303j,a \u2212 u)2 ) + \u2211 u<u\u2032\u2208[m\u0303j,a\u22121]\n2u m\u0303j,a(r\u0303j,a \u2212 u) m\u0303j,a \u2212 u\u2032 r\u0303j,a \u2212 u\u2032 (12)\n= m\u03032j,a/(3r\u0303j,a) +O(m\u0303 3 j,a/r\u0303 2 j,a) . (13)\nTheorem 4.2. Let U denote the set of all unbiased estimators of \u03b8\u2217 that are centered such that \u03b8\u03021 = 0, and let \u00b5 = maxj\u2208[n],a\u2208[\u02dc\u0300j ]{m\u0303j,a \u2212 \u03b7j,a}. For all b > 0,\ninf \u03b8\u0302\u2208U sup \u03b8\u2217\u2208\u2126b\nE[\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162] \u2265 max  (d\u2212 1)2\u2211n j=1 \u2211\u02dc\u0300 j a=1(m\u0303j,a \u2212 \u03b7j,a) , 1 \u00b5 d\u2211 i=2 1 \u03bbi(L)  . (14) The proof relies on the Cramer-Rao bound and is provided in Section 5. Since \u03b7j,a\u2019s are non-\nnegative, the mean squared error is lower bounded by (d\u22121)2/N , where N = \u2211\nj \u2211 a\u2208\u02dc\u0300j m\u0303j,a is the\neffective sample size. Comparing it to the upper bound in (11), this is tight up to a logarithmic factor when (a) the topology of the data is well-behaved such that all respective quantities are finite; and (b) there is no limit on the computational power and M can be made as large as we need. The bound in Eq. (14) further gives a tighter lower bound, capturing the dependency in \u03b7j,a\u2019s and \u03bbi(L)\u2019s. Considering the first term, \u03b7j,a is larger when m\u0303j,a is close to r\u0303j,a, giving a tighter bound. The second term in (14) implies we get a tighter bound when \u03bb2(L) is smaller.\nIn Figure 4 left and middle panel, we compare performance of our algorithm with pairwise breaking, Cramer Rao lower bound and oracle MLE lower bound. We fix d = 512, n = 105, \u03b8\u2217 chosen i.i.d. uniformly over [\u22122, 2]. Oracle MLE knows relative ordering of items in all the top-sets T (e) and hence is strictly better than the GRB. We fix \u02dc\u0300 = ` = 1 that is r = \u03ba, and vary m . In the left panel, we fix \u03ba = 32 and in the middle panel, we fix \u03ba = 16. Perhaps surprisingly, GRB matches with the oracle MLE which means relative ordering of top-m items among themselves is\nstatistically insignificant when m is sufficiently small in comparison to \u03ba. For \u03ba = 16, as m gets large, the error starts to increase as predicted by our analysis. The reason is that the quantities \u03b31 and \u03b32 gets smaller as m increases, and the upper bound increases consequently. In the right panel, we fix m = 4. When \u03ba is small, \u03b32 is small, and hence error is large; when b is large \u03b31 is exponentially small, and hence error is significantly large. This is different from learning Mallows models in Ali and Meila\u0306 (2012) where peaked distributions are easier to learn, and is related to the fact that we are not only interested in recovering the (ordinal) ranking but also the (cardinal) weight."}, {"heading": "4.3 Computational and statistical tradeoff", "text": "For estimators with limited computational power, however, the above lower bound fails to capture the dependency on the allowed computational power. Understanding such fundamental trade-offs is a challenging problem, which has been studied only in a few special cases, e.g. planted clique problem (Deshpande and Montanari, 2015; Meka et al., 2015). This is outside the scope of this paper, and we instead investigate the trade-off achieved by the proposed rank-breaking approach. When we are limited on computational power, Theorem 4.1 implicitly captures this dependence when order-M rank-breaking is used. The dependence is captured indirectly via the resulting rankbreaking {Gj,a}j\u2208[n],a\u2208[`j ] and the topology of it. We make this trade-off explicit by considering a simple but canonical example. Suppose \u03b8\u2217 \u2208 \u2126b with b = O(1). Each user gives an i.i.d. partial ranking, where all items are offered and the partial ranking is based on an ordered partition with \u02dc\u0300 j = b \u221a 2cd1/4c subsets. The top subset has size m\u0303j,1 = 1, and the a-th subset has size m\u0303j,a = a, up\nto a < \u02dc\u0300j , in order to ensure that they sum at most to c \u221a d for sufficiently small positive constant c and the condition on \u03b33 > 0 is satisfied. The last subset includes all the remaining items in the bottom, ensuring m\u0303j,\u02dc\u0300j \u2265 d/2 and \u03b31, \u03b32 and \u03bd are all finite.\nComputation. For a choice of M such that M \u2264 `j \u2212 1, we consider the computational complexity in evaluating the gradient of LRB, which scales as TM = \u2211 j\u2208[n] \u2211 a\u2208[M ](mj,a!)rj,a = O(M ! \u00d7 dn). Note that we find the MLE by solving a convex optimization problem using first order methods, and detailed analysis of the convergence rate and the complexity of solving general convex optimizations is outside the scope of this paper.\nSample. Under the canonical setting, for M \u2264 `j \u2212 1, we have L = M(M + 1)/(2d(d\u2212 1)) ( I\u2212\n11> ) . This complete graph has the largest possible spectral gap, and hence \u03b1 > 0 and \u03b2 > 0. Since\nthe effective samples size is \u2211\nj,a m\u0303j,aI{m\u0303j,a \u2264 M} = nM(M + 1)/2, it follows from Theorem 4.1\nthat the (rescaled) root mean squared error is O( \u221a\n(d log d)/(nM2)). In order to achieve a target error rate of \u03b5, we need to choose M = \u2126((1/\u03b5) \u221a (d log d)/n). The resulting trade-off between\nrun-time and sample to achieve root mean squared error \u03b5 is T (n) \u221d (d(1/\u03b5) \u221a\n(d log d)/ne)!dn. We show numerical experiment under this canonical setting in Figure 3 (left) with d = 256 and M \u2208 {1, 2, 3, 4, 5}, illustrating the trade-off in practice."}, {"heading": "4.4 Real-world datasets", "text": "On sushi preferences (Kamishima, 2003) and jester dataset (Goldberg et al., 2001), we improve over pairwise breaking and achieves same performance as the oracle MLE. Full rankings over \u03ba = 10 types of sushi are randomly chosen from d = 100 types of sushi are provided by n = 5000 individuals. As the ground truth \u03b8\u2217, we use the ML estimate of PL weights over the entire data. In Figure 5, left panel, for each m \u2208 {3, 4, 5, 6, 7}, we remove the known ordering among the top-m and bottom-(10 \u2212m) sushi in each set, and run our estimator with one breaking edge between top-m and bottom-(10\u2212m) items. We compare our algorithm with inconsistent pairwise breaking (using optimal choice of parameters from Khetan and Oh (2016)) and the oracle MLE. For m \u2264 6, the proposed rank-breaking performs as well as an oracle who knows the hidden ranking among the top m items. Jester dataset consists of continuous ratings between \u221210 to +10 of 100 jokes on sets of size \u03ba, 36 \u2264 \u03ba \u2264 100, by 24, 983 users. We convert ratings into full rankings. The ground truth \u03b8\u2217 is computed similarly. For m \u2208 {2, 3, 4, 5}, we convert each full ranking into a poset that has ` = b\u03ba/mc partitions of size m, by removing known relative ordering from each partition. Figure 5 compares the three algorithms using all samples (middle panel), and by varying the sample size (right panel) for fixed m = 4. All figures are averaged over 50 instances."}, {"heading": "5. Proofs", "text": "We provide the proofs of the main results."}, {"heading": "5.1 Proof of Remark 2.1", "text": "Recall that P\u03b8(B(e) \u227a T (e)) is the probability that an agent ranks the collection of items T (e) above B(e) when offered S = B(e) \u222a T (e). We want to show that P\u03b8(B(e) \u227a T (e)) is log-concave under the PL model. We prove a slightly general result which works for a family of RUMs in the location\nfamily. Random Utility Models (RUM) are defined as a probabilistic model where there is a realvalued utility parameter \u03b8i associated with each items i \u2208 S, and an agent independently samples random utilities {Ui}i\u2208S for each item i with conditional distribution \u00b5i(\u00b7|\u03b8i). Then the ranking is obtained by sorting the items in decreasing order as per the observed random utilities Ui\u2019s. Location family is a subset of RUMs where the shapes of \u00b5i\u2019s are fixed and the only parameters are the means of the distributions. For location family, the noisy utilities can be written as Ui = \u03b8i + Zi for i.i.d. random variable Zi\u2019s. In particular, it is PL model when Zi\u2019s follow the independent standard Gumbel distribution. We will show that for the location family if the probability density function for each Zi\u2019s is log-concave then logP\u03b8(B(e) \u227a T (e)) is concave. The desired claim follows as the pdf of standard Gumbel distribution is log-concave. We use the following Theorem from Pre\u0301kopa (1980). A similar technique was used to prove concavity when |T (e)| = 1 in Azari Soufiani et al. (2012).\nLemma 5.1 (Theorem 9 in Pre\u0301kopa (1980)). Suppose g(\u03b8, Z) is a concave function in R2r, where \u03b8 \u2208 Rr is fixed and Z is a r\u2212component random vector whose probability distribution is logarithmic concave in Rr, then the function\nh(\u03b8) = P[g(\u03b8, Z) \u2265 0], for \u03b8 \u2208 Rr (15)\nis logarithmic concave on Rr.\nTo apply the above lemma to get our result, let r = |S|, g(\u03b8, Z) = mini\u2208T (e){\u03b8i + Zi} \u2212 maxi\u2032\u2208B(e){\u03b8i\u2032 + Zi\u2032}, and observe that P\u03b8(B(e) \u227a T (e)) = P(g(\u03b8, Z) \u2265 0) and g(\u03b8, Z) is concave."}, {"heading": "5.2 Proof of Remark 2.2", "text": "Define event E(e) \u2261 {T (e)\u222aB(e) items are ranked in bottom r positions when the offer set is [d]}. Define P\u03b8,[d](B(e) \u227a T (e)|E(e)) be the conditional probability of T (e) items being ranked higher than B(e) items when the offer set is [d], conditioned on the event E(e). Observe that P\u03b8,[d](B(e) \u227a T (e)|E(e)) is the probability of observing the event B(e) \u227a T (e) under the proposed rank-breaking. First we show that P\u03b8(e) = P\u03b8,[d](B(e) \u227a T (e)|E(e)), where P\u03b8(e) is the probability that T (e) \u227a B(e) when the offer set is {T (e) \u222a B(e)} as defined in (3). This follows from the fact that under PL model for any disjoint set of items {Ci}i\u2208[`] such that \u222a`i=1Ci = [d],\nP ( C` \u227a C`\u22121 \u227a \u00b7 \u00b7 \u00b7 \u227a C1 ) = P ( C` \u227a C`\u22121 ) P ( {C`, C`\u22121} \u227a C`\u22122 ) \u00b7 \u00b7 \u00b7P ( {C`, C`\u22121, \u00b7 \u00b7 \u00b7 , C2} \u227a C1 ) , (16)\nwhere P(Ci1 \u227a Ci2) is the probability that Ci2 items are ranked higher than Ci1 items when the offer set is S = {Ci1 \u222a Ci2}. Under the given sampling scenario, the comparison graph H([d], E) as defined in section 3 is connected and hence the estimate \u03b8\u0302, (4) is unique. Therefore, it follows that maximum likelihood estimate \u03b8\u0302 is consistent. Further, for a general sampling scenario, Theorem 4.1 proves that the estimator is consistent as the error goes to zero in the limit as n increases."}, {"heading": "5.3 Proof of Theorem 4.1", "text": "We define few additional notations. p \u2261 (1/n) \u2211n\nj=1 pj . V (ej,a) \u2261 T (ej,a) \u222a B(ej,a) for all j \u2208 [n] and a \u2208 [`j ]. Note that by definition of rank-breaking edge ej,a, V (ej,a) is a random set of items that are ranked in bottom rj,a positions in a set of Sj items by the user j.\nThe proof sketch is inspired from Khetan and Oh (2016). The main difference and technical challenge is in showing the strict concavity of LRB(\u03b8) when restricted to \u2126b. We want to prove an upper bound on \u2206 = \u03b8\u0302\u2212 \u03b8\u2217, where \u03b8\u0302 is the sample dependent solution of the optimization (4) and \u03b8\u2217 is the true utility parameter from which the samples are drawn. Since \u03b8\u0302, \u03b8\u2217 \u2208 \u2126b, it follows that \u22061 = 0. Since \u03b8\u0302 is the maximizer of LRB(\u03b8), we have the following inequality,\nLRB(\u03b8\u0302)\u2212 LRB(\u03b8\u2217)\u2212 \u3008\u2207LRB(\u03b8\u2217),\u2206\u3009 \u2265 \u2212\u3008\u2207LRB(\u03b8\u2217),\u2206\u3009 \u2265 \u2212\u2016\u2207LRB(\u03b8\u2217)\u20162\u2016\u2206\u20162, (17)\nwhere the last inequality uses the Cauchy-Schwartz inequality. By the mean value theorem, there exists a \u03b8 = c\u03b8\u0302 + (1\u2212 c)\u03b8\u2217 for some c \u2208 [0, 1] such that \u03b8 \u2208 \u2126b and\nLRB(\u03b8\u0302)\u2212 LRB(\u03b8\u2217)\u2212 \u3008\u2207LRB(\u03b8\u2217),\u2206\u3009 = 1 2 \u2206>H(\u03b8)\u2206 \u2264 \u22121 2 \u03bb2(\u2212H(\u03b8))\u2016\u2206\u201622, (18)\nwhere \u03bb2(\u2212H(\u03b8)) is the second smallest eigen value of \u2212H(\u03b8). We will show in Lemma 5.4 that \u2212H(\u03b8) is positive semi definite with one eigen value at zero with a corresponding eigen vector 1 = [1, . . . , 1]>. The last inequality follows since \u2206>1 = 0. Combining Equations (17) and (18),\n\u2016\u2206\u20162 \u2264 2\u2016\u2207LRB(\u03b8\u2217)\u20162 \u03bb2(\u2212H(\u03b8)) , (19)\nwhere we used the fact that \u03bb2(\u2212H(\u03b8)) > 0 from Lemma 5.4. The following technical lemmas prove that the norm of the gradient is upper bounded by \u03b3\n\u22121/2 2 e\nb \u221a\n6np log d with high probability and the second smallest eigen value is lower bounded by (1/8) e\u22126b\u03b1\u03b31\u03b32\u03b33(np/(d \u2212 1)). This finishes the proof of Theorem 4.1.\nThe (random) gradient of the log likelihood in (4) can be written as the following, where the randomness is in which items ended up in the top set T (ej,a) and the bottom set B(ej,a):\n\u2207iLRB(\u03b8) = n\u2211 j=1 `j\u2211 a=1 \u2211 C\u2286Sj ,\n|C|=rj,a\u22121\nI { V (ej,a) = {C, i} }\u2202 logP\u03b8(ej,a) \u2202\u03b8i . (20)\nNote that we are intentionally decomposing each summand as a summation over all C of size rj,a\u22121, such that we can separate the analysis of the expectation in the following lemma. The random variable I{{C, i} = V (ej,a)} indicates that we only include one term for any given instance of the sample. Note that the event I{{C, i} = V (ej,a)} is equivalent to the event that the {C, i} items are ranked in bottom rj,a positions in the set Sj , that is V (ej,a) items are ranked in bottom rj,a positions in the set Sj .\nLemma 5.2. If the j-th poset is drawn from the PL model with weights \u03b8\u2217 then for any given C\u2032 \u2286 Sj with |C\u2032| = rj,a,\nE [ I { C\u2032 = V (ej,a) }\u2202 logP\u03b8\u2217(ej,a) \u2202\u03b8\u2217i \u2223\u2223\u2223\u2223{ej,a\u2032}a\u2032<a] = 0 . (21) First, this lemma implies that E [ I { C\u2032 = V (ej,a) }\u2202 log P\u03b8\u2217 (ej,a) \u2202\u03b8\u2217i ] = 0. Secondly, the above lemma allows us to construct a vector-valued martingale and apply a generalization of Azuma-Hoeffding\u2019s tail bound on the norm to prove the following concentration of measure. This proves the desired bound on the gradient.\nLemma 5.3. If n posets are independently drawn over d items from the PL model with weights \u03b8\u2217 then with probability at least 1\u2212 2e3d\u22123,\n\u2016\u2207LRB(\u03b8\u2217)\u2016 \u2264 \u03b3\u22121/22 e b \u221a 6np log d , (22)\nwhere \u03b32 depend on the choice of the rank-breaking and are defined in Section 3.\nWe will prove in (30) that the Hessian matrix H(\u03b8) \u2208 Sd with Hii\u2032(\u03b8) = \u2202 2LRB(\u03b8) \u2202\u03b8i\u2202\u03b8i\u2032 can be\nexpressed as\n\u2212H(\u03b8) = n\u2211 j=1 `j\u2211 a=1 \u2211 i<i\u2032\u2208Sj I{(i, i\u2032) \u2286 V (ej,a)} ( \u22022 logP\u03b8(ej,a) \u2202\u03b8i\u2202\u03b8i\u2032 (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> ) . (23)\nIt is easy to see that H(\u03b8)1 = 0. The following lemma proves a lower bound on the second smallest eigenvalue \u03bb2(\u2212H(\u03b8)) in terms of re-scaled spectral gap \u03b1 of the comparison graph H defined in Section 3.\nLemma 5.4. Under the hypothesis of Theorem 4.1, if the assumptions in Equation (10) are satisfied then with probability at least 1\u2212 d\u22123, the following holds for any \u03b8 \u2208 \u2126b:\n\u03bb2(\u2212H(\u03b8)) \u2265 e\u22126b\u03b1\u03b31\u03b32\u03b33\n8\nnp\n(d\u2212 1) , (24)\nand \u03bb1(\u2212H(\u03b8)) = 0 with corresponding eigen vector 1.\nThis finishes the proof of the desired claim."}, {"heading": "5.4 Proof of Lemma 5.2", "text": "Recall that ej,a is a random event where randomness is in which items ended up in the top-set T (ej,a) and the bottom-set B(ej,a), and P\u03b8\u2217(ej,a) = P\u03b8\u2217 [B(ej,a) \u227a T (ej,a)] that is the probability of observing B(ej,a) \u227a T (ej,a) when the offer set is B(ej,a) \u222a T (ej,a) as defined in (3). Define, P\u03b8\u2217,Sj [ej,a|V (ej,a) = C\u2032] to be the conditional probability of observing B(ej,a) \u227a T (ej,a), when the offer set is Sj , conditioned on the event that V (ej,a) = C\u2032. Note that we have put subscript Sj in P\u03b8\u2217 to specify that the offer set is Sj . Observe that for any set C\u2032 \u2286 Sj , the event {C\u2032 = V (ej,a)} is equivalent to C\u2032 items being ranked in bottom rj,a positions when the offer set is Sj . In other words, it is conditioned on the event that the subset V (ej,a) items are ranked in bottom rj,a positions when the offer set is Sj . It is easy to check that under PL model\nP\u03b8\u2217,Sj [ej,a|V (ej,a) = C \u2032] = P\u03b8\u2217 [ej,a],\n(see Remark 2.2). Also, by conditioning on any outcome of {ej,a\u2032}a\u2032<a it can be checked that\nP\u03b8\u2217,Sj [ej,a|V (ej,a) = C \u2032, {ej,a\u2032}a\u2032<a] = P\u03b8\u2217,Sj [ej,a|V (ej,a) = C \u2032].\nTherefore, we have E [ \u2202 logP\u03b8\u2217 [ ej,a ]\n\u2202\u03b8\u2217i \u2223\u2223\u2223\u2223V (ej,a) = C\u2032, {ej,a\u2032}a\u2032<a] = E [ \u2202 logP\u03b8\u2217,Sj [ ej,a|V (ej,a) = C\u2032, {ej,a\u2032}a\u2032<a\n] \u2202\u03b8\u2217i \u2223\u2223\u2223\u2223V (ej,a) = C\u2032, {ej,a\u2032}a\u2032<a] =\n\u2211 ej,a:V (ej,a)=C\u2032 {ej,a\u2032}a\u2032<a P\u03b8\u2217,Sj [ ej,a \u2223\u2223V (ej,a) = C\u2032, {ej,a\u2032}a\u2032<a] \u2202 \u2202\u03b8\u2217i logP\u03b8\u2217,Sj [ ej,a \u2223\u2223V (ej,a) = C\u2032, {ej,a\u2032}a\u2032<a]\n= \u2202\n\u2202\u03b8\u2217i \u2211 ej,a:V (ej,a)=C\u2032 P\u03b8\u2217,Sj [ ej,a \u2223\u2223V (ej,a) = C\u2032] = \u2202 \u2202\u03b8\u2217i 1 = 0 ,\nwhere we used {ej,a : V (ej,a) = C\u2032} = {ej,a : V (ej,a) = C\u2032, {ej,a\u2032}a\u2032<a} which follows from the definition of rank-breaking edges ej,a. This proves the desired claim."}, {"heading": "5.5 Proof of Lemma 5.3", "text": "We view \u2207LRB(\u03b8\u2217) as the final value of a discrete time vector-valued martingale with values in Rd. Define \u2207L(ej,a)RB \u2208 Rd as the gradient vector arising out of each rank-breaking edge {ej,a}j\u2208[n],a\u2208[`j ] as\n\u2207iL (ej,a) RB (\u03b8 \u2217) \u2261 \u2211 C\u2286Sj I { V (ej,a) = {C, i} } \u2207i logP\u03b8\u2217(ej,a) , (25)\nsuch that \u2207LRB(\u03b8\u2217) = \u2211 j\u2208[n] \u2211 a\u2208[`j ]\u2207L (ej,a) RB . We take \u2207L (ej,a) RB as the incremental random vector\nin a martingale of \u2211n\nj=1 `j time steps. Let Hj,a denote (the sigma algebra of) the history up to ej,a and define a sequence of random vectors in Rd:\nZj,a \u2261 E[\u2207L (ej,a) RB (\u03b8 \u2217)|Hj,a] ,\nwith the convention that Z1,1 = E[\u2207L (ej,a) RB (\u03b8 \u2217)] = 0 as proved in Lemma 5.2. It also follows from Lemma 5.2 that E[Zj,a+1|Zj,a] = Zj,a for a < `j . Also, from the independence of samples, it follows that E[Zj+1,1|Zj,`j ] = Zj,`j . Applying a generalized version of the vector Azuma-Hoeffding inequality which readily follows from [Theorem 1.8, Hayes (2005)], we have\nP [ \u2016\u2207LRB(\u03b8\u2217)\u2016 \u2265 \u03b4 ] \u2264 2e3 exp ( \u2212 \u03b4\n2\u2211n j=1 \u2211`j a=1mj,a2\u03b3 \u22121 2 e 2b\n) , (26)\nwhere we used \u2016\u2207L(ej,a)RB \u20162 \u2264 mj,a2\u03b3 \u22121 2 e 2b. Choosing \u03b4 = \u03b3\u221212 e b \u221a 6np log d gives the desired bound.\nNow we are left to show that \u2016\u2207L(ej,a)RB \u20162 \u2264 2mj,a\u03b3 \u22121 2 e 2b for any \u03b8 \u2208 \u2126b. Recall that \u03c3 \u2208 \u039bT (ej,a) is the set of all full rankings over T (ej,a) items. In rest of the proof, with a slight abuse of notations, we extend each of these ranking \u03c3 over T (ej,a) \u222a B(ej,a) items in the following way. Consider any full ranking \u03c3\u0303 over B(ej,a) items. Then for each \u03c3 \u2208 \u039bT (ej,a), the extension is such that \u03c3(|T (ej,a)| + c) = \u03c3\u0303(c) for 1 \u2264 c \u2264 |B(ej,a)|. The choice of ranking \u03c3\u0303 will have no impact on\nany of the following mathematical expressions. From the definition of P\u03b8(ej,a) (3), we have, for any i \u2208 V (ej,a),\n\u2202P\u03b8(ej,a) \u2202\u03b8i = I{i \u2208 T (ej,a)}P\u03b8(ej,a) (27)\n\u2212 \u2211\n\u03c3\u2208\u039bT (ej,a)\nexp (\u2211mj,a c=1 \u03b8\u03c3(c) )\u220fmj,a\nu=1 (\u2211rj,a c\u2032=u exp ( \u03b8\u03c3(c\u2032) ))\ufe38 \ufe37\ufe37 \ufe38 \u2261A\u03c3\n(mj,a\u2211 u\u2032=1 I{\u03c3\u22121(i) \u2265 u\u2032} exp(\u03b8i)\u2211rj,a c\u2032=u\u2032 exp ( \u03b8\u03c3(c\u2032) ) )\ufe38 \ufe37\ufe37 \ufe38 \u2261B\u03c3,i\ufe38 \ufe37\ufe37 \ufe38\n\u2261Ei\n.\nNote that A\u03c3, B\u03c3,i and Ei depend on ej,a. Observe that for any 1 \u2264 u\u2032 \u2264 mj,a and any \u03c3 \u2208 \u039bT (ej,a),\n\u2211 i\u2208V (ej,a) I{\u03c3\u22121(i) \u2265 u\u2032} exp(\u03b8i) = rj,a\u2211 c\u2032=u\u2032 exp ( \u03b8\u03c3(c\u2032) ) . (28)\nTherefore, \u2211\ni\u2208V (ej,a)B\u03c3,i = mj,a. It follows that\u2211 i\u2208V (ej,a) Ei = \u2211\n\u03c3\u2208\u039bT (ej,a)\nA\u03c3 ( \u2211 i\u2208V (ej,a) B\u03c3,i ) = mj,a \u2211 \u03c3\u2208\u039bT (ej,a) A\u03c3 = mj,aP\u03b8(ej,a) , (29)\nwhere the last equality follows from the definition of P\u03b8(ej,a) (4). Also, since for any i, i\u2032, e(\u03b8i\u2212\u03b8i\u2032 ) \u2264 e2b; for any i, B\u03c3,i \u2264 e2b \u2211rj,a k=rj,a\u2212mj,a+1(1/k) \u2264 e\n2b(1 + log(rj,a/(rj,a \u2212mj,a + 1))) \u2264 \u03b3\u221212 e2b, where the last inequality follows from the definition of \u03b32 (8) and the fact that x \u2264 \u221a 1 + log x for all\nx \u2265 1. Therefore, Ei \u2264 \u03b3\u221212 e2b \u2211\n\u03c3\u2208\u039bT (ej,a) A\u03c3 = \u03b3\n\u22121 2 e 2bP\u03b8(ej,a). We have \u2202 logP\u03b8(ej,a)/\u2202\u03b8i =\n(1/P\u03b8(ej,a))\u2202P\u03b8(ej,a)/\u2202\u03b8i = I{i \u2208 T (ej,a)} \u2212 Ei/P\u03b8(ej,a). Since |T (ej,a)| = mj,a, \u2016\u2207L (ej,a) RB \u20162 \u2264 mj,a + \u2211 i\u2208V (ej,a)(Ei/P\u03b8(ej,a)) 2 \u2264 2mj,a\u03b3\u221212 e2b, where we used (29) and the fact that \u03b3 \u22121 2 \u2265 1."}, {"heading": "5.5.1 Proof of Lemma 5.4", "text": "First, we prove (23). For brevity, remove {j, a} from P\u03b8(ej,a). From Equations (27) and (29), and |T (ej,a)| = mj,a, we have \u2211 i\u2208V (ej,a) \u2202 \u2202\u03b8i P\u03b8(e) = mj,aP\u03b8(e)\u2212mj,aP\u03b8(e) = 0. It follows that\n\u2211 i\u2208V (ej,a) ( \u22022 logP\u03b8(e) \u2202\u03b8i\u2032\u2202\u03b8i ) =\n1 P\u03b8(e) \u2202 \u2202\u03b8i\u2032 ( \u2211 i\u2208V (ej,a) ( \u2202P\u03b8(e) \u2202\u03b8i )) \u2212 1 (P\u03b8(e))2 \u2202P\u03b8(e) \u2202\u03b8i\u2032 ( \u2211 i\u2208V (ej,a) ( \u2202P\u03b8(e) \u2202\u03b8i )) = 0 . (30)\nSince by definition LRB(\u03b8) = \u2211n\nj=1 \u2211`j a=1 logP\u03b8(ej,a), and Hii\u2032(\u03b8) = \u22022LRB(\u03b8) \u2202\u03b8i\u2202\u03b8i\u2032 which is a symmetric\nmatrix, Equation (30) implies that it can be expressed as given in Equation (23). It follows that all-ones is an eigenvector of H(\u2212\u03b8) with the corresponding eigenvalue being zero.\nTo get a lower bound on \u03bb2(\u2212H(\u03b8)), we apply Weyl\u2019s inequality\n\u03bb2(\u2212H(\u03b8)) \u2265 \u03bb2(E[\u2212H(\u03b8)])\u2212 \u2016H(\u03b8)\u2212 E[H(\u03b8)]\u2016 . (31)\nWe will show in (34) that \u03bb2(E[\u2212H(\u03b8)]) \u2265 e\u22126b\u03b1\u03b31\u03b32\u03b33(np/(4(d \u2212 1))) and in (51) that \u2016H(\u03b8) \u2212 E[H(\u03b8)]\u2016 \u2264 16e4b\u03bd \u221a pmax \u03bamin np \u03b2(d\u22121) log d. Putting these together,\n\u03bb2(\u2212H(\u03b8)) \u2265 e\u22126b\u03b1\u03b31\u03b32\u03b33 np\n4(d\u2212 1) \u2212 16e4b\u03bd \u221a pmax \u03bamin\nnp\n\u03b2(d\u2212 1) log d (32)\n\u2265 e \u22126b\u03b1\u03b31\u03b32\u03b33\n8\nnp\n(d\u2212 1) , (33)\nwhere the last inequality follows from the assumption on n\u03bamin given in (10). To prove a lower bound on \u03bb2(E[\u2212H(\u03b8)]), we claim that for \u03b8 \u2208 \u2126b,\nE [ \u2212H(\u03b8) ] e\u22126b\u03b31\u03b32\u03b33 n\u2211 j=1 pj 4\u03baj(\u03baj \u2212 1) \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> (34)\n= e\u22126b\u03b31\u03b32\u03b33\n4 L ,\nwhere L \u2208 Sd is defined in (6). Using \u03bb2(L) = np\u03b1/(d \u2212 1) from (7), we have \u03bb2(\u2212H(\u03b8)) \u2265 e\u22126b\u03b1\u03b31\u03b32\u03b33(np/(4(d\u2212 1))). To prove (34), notice that\nE[\u2212H(\u03b8)ii\u2032 ] = E [ \u2211 j\u2208[n] \u2211 a\u2208[`j ] I { (i, i\u2032) \u2286 V (ej,a) } \u22022 logP\u03b8(ej,a) \u2202\u03b8i\u2202\u03b8i\u2032 ] , (35)\nwhen i 6= i\u2032. We will show that for any i 6= i\u2032 \u2208 V (ej,a),\n\u22022 logP\u03b8(ej,a) \u2202\u03b8i\u2202\u03b8i\u2032 \u2265  e\u22122bmj,a r2j,a\nif i, i\u2032 \u2208 B(ej,a)\n\u2212 e 4bm2j,a\n(rj,a\u2212mj,a+1)2 otherwise .\n(36)\nWe need to bound the probability of two items appearing in the bottom-set B(ej,a) and in the top-set T (ej,a).\nLemma 5.5. Consider a ranking \u03c3 over a set S \u2286 [d] such that |S| = \u03ba. For any two items i, i\u2032 \u2208 S, \u03b8 \u2208 \u2126b, and 1 \u2264 `, `1, `2 \u2264 \u03ba\u2212 1,\nP\u03b8 [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > ` ] \u2265 e\n\u22124b(\u03ba\u2212 `)(\u03ba\u2212 `\u2212 1) \u03ba(\u03ba\u2212 1)\n( 1\u2212 `\n\u03ba\n)2e2b\u22122 , (37)\nP\u03b8 [ \u03c3\u22121(i) = ` ] \u2264 e 6b\n\u03ba\u2212 ` , (38)\nP\u03b8 [ \u03c3\u22121(i) = `1, \u03c3 \u22121(i\u2032) = `2 ] \u2264 e 10b\n(\u03ba\u2212 `1 \u2212 1)(\u03ba\u2212 `2) . (39)\nwhere the probability P\u03b8 is with respect to the sampled ranking resulting from PL weights \u03b8 \u2208 \u2126b.\nSubstituting ` = \u03baj \u2212 rj,a + mj,a in (37), and `, `1, `2 \u2264 \u03baj \u2212 rj,a + mj,a in (38) and (39), we have,\nP\u03b8 [ (i, i\u2032) \u2286 B(ej,a) ] \u2265 e \u22124b(rj,a \u2212mj,a)2\n4\u03baj(\u03baj \u2212 1)\n(rj,a \u2212mj,a \u03baj )2e2b\u22122 , (40)\nP\u03b8 [ i \u2208 T (ej,a), i\u2032 \u2208 B(ej,a) ] \u2264 mj,a max\n`\u2208[\u03baj\u2212rj,a+mj,a] P(\u03c3\u22121(i) = `)\n\u2264 e 6bmj,a\nrj,a \u2212mj,a , (41) P\u03b8 [ (i, i\u2032) \u2286 T (ej,a) ] \u2264 m2j,a max\n`1,`2\u2208[\u03baj\u2212rj,a+mj,a] P(\u03c3\u22121(i) = `1, \u03c3\u22121(i\u2032) = `2)\n\u2264 e10bm2j,a\n2 (rj,a \u2212mj,a \u2212 1)(rj,a \u2212mj,a) , (42)\nwhere (40) uses rj,a \u2212 mj,a \u2212 1 \u2265 (rj,a \u2212 mj,a)/4, (41) uses P\u03b8[i \u2208 T (ej,a), i\u2032 \u2208 B(ej,a)] \u2264 P\u03b8[i \u2208 T (ej,a)], and (41)-(42) uses counting on the possible choices. The bound in (42) is smaller than the one in (41) as per our assumption that \u03b33 > 0.\nUsing Equations (35)-(36) and (40)-(42), and the definitions of \u03b31, \u03b32, \u03b33 from Section 3, we get\nE[\u2212H(\u03b8)ii\u2032 ] \u2265\u2211 j\u2208[n] \u2211 a\u2208[`j ] {(rj,a \u2212mj,a \u03baj )2e2b\u22122 \ufe38 \ufe37\ufe37 \ufe38\n\u2265\u03b31\n(rj,a \u2212mj,a rj,a )2 \ufe38 \ufe37\ufe37 \ufe38\n\u2265\u03b32\ne\u22126bmj,a 4\u03baj(\u03baj \u2212 1) \u2212 e 6bmj,a rj,a \u2212mj,a e4bm2j,a (rj,a \u2212mj,a + 1)2 }\n\u2265 \u2211 j,a \u03b31\u03b32e \u22126bmj,a 4\u03baj(\u03baj \u2212 1) ( 1 \u2212 4e 16b \u03b31 m2j,ar 2 j,a\u03ba 2 j (rj,a \u2212mj,a)5 )\n\ufe38 \ufe37\ufe37 \ufe38 \u2265\u03b33 . (43)\nThis combined with (23) proves the desired claim (34). Further, in Appendix 5.8, we show that if mj,a \u2264 3 for all {j, a} then \u2202 2 log P\u03b8(ej,a) \u2202\u03b8i\u2202\u03b8i\u2032\nis non-negative even for i 6= i\u2032 \u2208 T (ej,a), and i \u2208 T (ej,a), i\u2032 \u2208 B(ej,a) as opposed to a negative lower-bound given in (36). Therefore, bound on E[\u2212H(\u03b8)] in (34) can be tightened by a factor of \u03b33.\nTo prove claim (36), define the following for \u03c3 \u2208 \u039bT (ej,a),\nA\u03c3 \u2261 exp\n(\u2211mj,a c=1 \u03b8\u03c3(c) )\u220fmj,a u=1 (\u2211rj,a c\u2032=u exp ( \u03b8\u03c3(c\u2032) )) , B\u03c3 \u2261 mj,a\u2211 u\u2032=1 1\u2211rj,a c\u2032=u\u2032 exp ( \u03b8\u03c3(c\u2032)\n) , B\u03c3,i \u2261\nmj,a\u2211 u\u2032=1 I{\u03c3\u22121(i) \u2265 u\u2032}\u2211rj,a c\u2032=u\u2032 exp ( \u03b8\u03c3(c\u2032) ) , C\u03c3 \u2261 mj,a\u2211 u\u2032=1 1(\u2211rj,a c\u2032=u\u2032 exp ( \u03b8\u03c3(c\u2032)\n))2 , C\u03c3,i \u2261\nmj,a\u2211 u\u2032=1 I{\u03c3\u22121(i) \u2265 u\u2032}(\u2211rj,a c\u2032=u\u2032 exp ( \u03b8\u03c3(c\u2032) ))2 , C\u03c3,i,i\u2032 \u2261 mj,a\u2211 u\u2032=1 I{\u03c3\u22121(i), \u03c3\u22121(i\u2032) \u2265 u\u2032}(\u2211rj,a c\u2032=u\u2032 exp ( \u03b8\u03c3(c\u2032)\n))2 . (44) First, a few observations about the expression of A\u03c3. For any \u03c3 \u2208 \u039bT (ej,a) and any i \u2208 V (ej,a), \u03b8i is in the numerator if and only if i \u2208 T (ej,a), since in all the rankings that are consistent with\nthe observation ej,a, T (ej,a) items are ranked in top mj,a positions. For any \u03c3 \u2208 \u039bT (ej,a) and any i \u2208 B(ej,a), \u03b8i is in all the product terms \u220fmj,a u=1 (\u00b7) of the denominator, since in all the consistent rankings these items are ranked below mj,a position. For any i \u2208 T (ej,a), \u03b8i appears in product term corresponding to index u if and only if item i is ranked at position u or lower than u in the ranking \u03c3 \u2208 \u039bT (ej,a). Now, observe that B\u03c3 is defined such that the partial derivative of A\u03c3 with respect to any i \u2208 B(ej,a) is \u2212A\u03c3B\u03c3e\u03b8i , and B\u03c3,i is defined such that the partial derivative of A\u03c3 with respect to any i \u2208 T (ej,a) is A\u03c3 \u2212 A\u03c3B\u03c3e\u03b8i . Further, observe that \u2212C\u03c3e\u03b8i is the partial derivative of B\u03c3 with respect to i \u2208 B(ej,a), \u2212C\u03c3,ie\u03b8i is the partial derivative of B\u03c3,i with respect to i \u2208 T (ej,a), and \u2212C\u03c3,ie\u03b8i\u2032 is the partial derivative of B\u03c3,i with respect to i\u2032 \u2208 B(ej,a). \u2212C\u03c3,i,i\u2032e\u03b8i\u2032 is the partial derivative of B\u03c3,i with respect to i\n\u2032 6= i \u2208 T (ej,a). For ease of notation, we omit subscript (j, a) whenever it is clear from the context. Also, we use\u2211 \u03c3 to denote \u2211 \u03c3\u2208\u039bT (ej,a) . With the above defined notations, from (4), we have, P\u03b8(e) = \u2211 \u03c3 A\u03c3. With the above given observations for the notations in (44), first partial derivative of P\u03b8(e) can be expressed as following:\n\u2202P\u03b8(e) \u2202\u03b8i =\n{\u2211 \u03c3 ( A\u03c3 \u2212A\u03c3B\u03c3,ie\u03b8i ) if i \u2208 T (ej,a)\u2211\n\u03c3\n( \u2212A\u03c3B\u03c3e\u03b8i ) if i \u2208 B(ej,a) .\n(45)\nIt follows that for i 6= i\u2032 \u2208 V (ej,a),\n\u22022P\u03b8(e) \u2202\u03b8i\u2202\u03b8i\u2032\n=  \u2211 \u03c3 ( (A\u03c3(B\u03c3) 2 +A\u03c3C\u03c3)e (\u03b8i+\u03b8i\u2032 ) ) if i, i\u2032 \u2208 B(ej,a)\u2211 \u03c3 ( A\u03c3 \u2212A\u03c3B\u03c3,i\u2032e\u03b8i\u2032 + (A\u03c3B\u03c3,iB\u03c3,i\u2032 +A\u03c3C\u03c3,i,i\u2032)e(\u03b8i+\u03b8i\u2032 ) \u2212A\u03c3B\u03c3,ie\u03b8i ) if i, i\u2032 \u2208 T (ej,a)\u2211\n\u03c3\n( (A\u03c3B\u03c3B\u03c3,i +A\u03c3C\u03c3,i)e (\u03b8i+\u03b8i\u2032 ) \u2212A\u03c3B\u03c3e\u03b8i\u2032 )\notherwise .\n(46)\nUsing \u2202 2 log P\u03b8(e) \u2202\u03b8i\u2202\u03b8i\u2032 = 1P\u03b8(e) \u22022P\u03b8(e) \u2202\u03b8i\u2202\u03b8i\u2032 \u2212 1 (P\u03b8(e))2 \u2202P\u03b8(e) \u2202\u03b8i \u2202P\u03b8(e) \u2202\u03b8i\u2032 , with above derived first and second derivatives, and after following some algebra, we have\n(P\u03b8(e))2 e(\u03b8i+\u03b8i\u2032 ) \u22022 logP\u03b8(e) \u2202\u03b8i\u2202\u03b8i\u2032\n=  ( \u2211 \u03c3 A\u03c3)( \u2211 \u03c3 A\u03c3(B\u03c3) 2)\u2212 ( \u2211 \u03c3 A\u03c3B\u03c3) 2 + ( \u2211 \u03c3 A\u03c3)( \u2211 \u03c3 A\u03c3C\u03c3) if i, i \u2032 \u2208 B(ej,a) ( \u2211 \u03c3 A\u03c3)( \u2211 \u03c3 A\u03c3B\u03c3,iB\u03c3,i\u2032 +A\u03c3C\u03c3,i,i\u2032)\u2212 ( \u2211 \u03c3 A\u03c3B\u03c3,i)( \u2211 \u03c3 A\u03c3B\u03c3,i\u2032) if i, i \u2032 \u2208 T (ej,a) ( \u2211 \u03c3 A\u03c3)( \u2211 \u03c3 A\u03c3B\u03c3B\u03c3,i +A\u03c3C\u03c3,i)\u2212 ( \u2211 \u03c3 A\u03c3B\u03c3)( \u2211 \u03c3 A\u03c3B\u03c3,i) otherwise . (47)\nObserve that from Cauchy-Schwartz inequality ( \u2211 \u03c3 A\u03c3)( \u2211 \u03c3 A\u03c3(B\u03c3) 2) \u2212 ( \u2211 \u03c3 A\u03c3B\u03c3)\n2 \u2265 0. Also, we have e(\u03b8i+\u03b8i\u2032 )C\u03c3 \u2265 e\u22122b(m/r2) and e\u03b8iB\u03c3,i \u2264 e\u03b8iB\u03c3 \u2264 e2b(m/(r \u2212m + 1)) for any i \u2208 V (ej,a). This proves the desired claim (36). Next we need to upper bound deviation of \u2212H(\u03b8) from its expectation. From (47), we have,\u2223\u2223\u22022 log P\u03b8(ej,a) \u2202\u03b8i\u2202\u03b8i\u2032 \u2223\u2223 \u2264 3e4bm2j,a/(rj,a \u2212 mj,a + 1)2 \u2264 3e4b\u03bdmj,a/(\u03baj(\u03baj \u2212 1)), where the last inequality\nfollows from the definition of \u03bd (9). Therefore,\n\u2212H(\u03b8) 3e4b\u03bd n\u2211 j=1 `j\u2211 a=1 \u2211 i<i\u2032\u2208Sj I{(i, i\u2032) \u2286 V (ej,a)} mj,a \u03baj(\u03baj \u2212 1) (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> (48)\n3e4b\u03bd n\u2211 j=1 \u2211 i<i\u2032\u2208Sj \u2211`j a=1mj,a \u03baj(\u03baj \u2212 1) (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> \u2261 n\u2211 j=1 yjLj , (49)\nwhere yj = (3e 4b\u03bdpj)/(\u03baj(\u03baj \u2212 1)) and Lj = \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)\n> = \u03bajdiag(eSj ) \u2212 eSje>Sj for eSj = \u2211 i\u2208Sj ei. Observe that \u2016yjLj\u2016 \u2264 (3e 4b\u03bdpmax)/\u03bamin. Moreover, L 2 j \u03bajLj , and it follows that\nn\u2211 j=1 y2jL 2 j 9e8b\u03bd2 n\u2211 j=1 p2j \u03ba2j (\u03baj \u2212 1)2 \u03bajLj 9e8b\u03bd2pmax \u03bamin L , (50)\nwhere we used the fact that L = (pj/(\u03baj(\u03baj \u2212 1))) \u2211n j=1 Lj , for L defined in (6). Using \u03bbd(L) =\nnp/(\u03b2(d\u22121)) from (7), it follows that \u2016 \u2211n\nj=1 E\u03b8[y2jY 2j ]\u2016 \u2264 9e8b\u03bd2pmax\n\u03bamin\nnp \u03b2(d\u22121) . By the matrix Bernstien\ninequality, with probability at least 1\u2212 d\u22123,\n\u2016H(\u03b8)\u2212 E[H(\u03b8)]\u2016 \u2264 12e4b\u03bd \u221a pmax \u03bamin\nnp\n\u03b2(d\u2212 1) log d+\n8e4b\u03bdpmax log d\n\u03bamin \u2264 16e4b\u03bd \u221a pmax \u03bamin np \u03b2(d\u2212 1) log d , (51)\nwhere the last inequality follows from the assumption on n\u03bamin given in (10)."}, {"heading": "5.6 Proof of Lemma 5.5", "text": "Claim (37): Since providing a lower bound on P\u03b8 [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > ` ] for arbitrary \u03b8 is challenging, we construct a new set of parameters {\u03b8\u0303j}j\u2208[d] from the original \u03b8. These new parameters are constructed such that it is both easy to compute the probability and also provides a lower bound on the original distribution. Define \u03b1\u0303i,i\u2032,`,\u03b8 as\n\u03b1\u0303i,i\u2032,`,\u03b8 \u2261 max `\u2032\u2208[`] max \u2126\u2286S\\{i,i\u2032} :|\u2126|=\u03ba\u2212`\u2032\n{ exp(\u03b8i) + exp(\u03b8i\u2032)(\u2211\nj\u2208\u2126 exp(\u03b8j) ) /|\u2126|\n} , (52)\nand \u03b1i,i\u2032,`,\u03b8 = \u2308 \u03b1\u0303i,i\u2032,`,\u03b8 \u2309 . For ease of notation we remove the subscript from \u03b1 and \u03b1\u0303. We denote\nthe sum of the weights by W \u2261 \u2211\nj\u2208S exp(\u03b8j). We define a new set of parameters {\u03b8\u0303j}j\u2208S :\n\u03b8\u0303j =\n{ log(\u03b1\u0303/2) for j = i or i\u2032 ,\n0 otherwise . (53)\nSimilarly define W\u0303 \u2261 \u2211\nj\u2208S exp(\u03b8\u0303j) = \u03ba\u2212 2 + \u03b1\u0303. We have,\nP\u03b8 [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > ` ] =\n\u2211 j1\u2208S j1 6=i,i\u2032\n( exp(\u03b8j1)\nW\n\u2211 j2\u2208S\nj2 6=i,i\u2032,j1\n( exp(\u03b8j2)\nW \u2212 exp(\u03b8j1) \u00b7 \u00b7 \u00b7 ( \u2211 j`\u2208S j` 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22121\nexp(\u03b8j`) W \u2212 \u2211j`\u22121\nk=j1 exp(\u03b8k)\n) \u00b7 \u00b7 \u00b7 ))\n= \u2211 j1\u2208S j1 6=i,i\u2032\n( exp(\u03b8j1)\nW \u2212 exp(\u03b8j1) \u00b7 \u00b7 \u00b7 \u2211 j`\u22121\u2208S j`\u22121 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22122\n( exp(\u03b8j`\u22121)\nW \u2212 \u2211j`\u22121\nk=j1 exp(\u03b8k) \u2211 j`\u2208S j` 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22121\n( exp(\u03b8j`)\nW\n) \u00b7 \u00b7 \u00b7 ))\n(54)\nConsider the second-last summation term in the above equation and let \u2126` = S \\{i, i\u2032, j1, . . . , j`\u22122}. Observe that, |\u2126`| = \u03ba\u2212 ` and from equation (52),\nexp(\u03b8i)+exp(\u03b8i\u2032 )\u2211 j\u2208\u2126` exp(\u03b8j) \u2264 \u03b1\u0303\u03ba\u2212` . We have,\n\u2211 j`\u22121\u2208\u2126`\nexp(\u03b8j`\u22121) W \u2212 \u2211j`\u22121\nk=j1 exp(\u03b8k) = \u2211\nj`\u22121\u2208\u2126`\nexp(\u03b8j`\u22121) W \u2212 \u2211j`\u22122\nk=j1 exp(\u03b8k)\u2212 exp(\u03b8j`\u22121) \u2265 \u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121)\nW \u2212 \u2211j`\u22122\nk=j1 exp(\u03b8k)\u2212 (\u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121) ) /|\u2126`|\n(55)\n=\n\u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121)\nexp(\u03b8i) + exp(\u03b8i\u2032) + \u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121)\u2212 (\u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121) ) /|\u2126`|\n= ( exp(\u03b8i) + exp(\u03b8i\u2032)\u2211 j`\u22121\u2208\u2126` exp(\u03b8j`\u22121) + 1\u2212 1 \u03ba\u2212 ` )\u22121\n\u2265\n( \u03b1\u0303\n\u03ba\u2212 ` + 1\u2212 1 \u03ba\u2212 `\n)\u22121 (56)\n= \u03ba\u2212 `\n\u03b1\u0303+ \u03ba\u2212 `\u2212 1 = \u2211 j`\u22121\u2208\u2126`\nexp(\u03b8\u0303j`\u22121) W\u0303 \u2212 \u2211j`\u22121\nk=j1 exp(\u03b8\u0303k)\n, (57)\nwhere (55) follows from the Jensen\u2019s inequality and the fact that for any c > 0, 0 < x < c, xc\u2212x is convex in x. Equation (56) follows from the definition of \u03b1\u0303i,i\u2032,`,\u03b8, (52), and the fact that |\u2126`| = \u03ba\u2212`. Equation (57) uses the definition of {\u03b8\u0303j}j\u2208S .\nConsider {\u2126\u02dc\u0300}2\u2264\u02dc\u0300\u2264`\u22121, |\u2126\u02dc\u0300| = \u03ba\u2212 \u02dc\u0300, corresponding to the subsequent summation terms in (54). Observe that\nexp(\u03b8i)+exp(\u03b8i\u2032 )\u2211 j\u2208\u2126\u02dc\u0300 exp(\u03b8j) \u2264 \u03b1/|\u2126\u02dc\u0300|. Therefore, each summation term in equation (54) can be\nlower bounded by the corresponding term where {\u03b8j}j\u2208S is replaced by {\u03b8\u0303j}j\u2208S . Hence, we have\nP\u03b8 [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > ` ] \u2265\n\u2211 j1\u2208S j1 6=i,i\u2032\n( exp(\u03b8\u0303j1)\nW\u0303 \u2212 exp(\u03b8\u0303j1) \u00b7 \u00b7 \u00b7 \u2211 j`\u22121\u2208S j`\u22121 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22122\n( exp(\u03b8\u0303j`\u22121)\nW\u0303 \u2212 \u2211j`\u22121\nk=j1 exp(\u03b8\u0303k) \u2211 j`\u2208S j` 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22121\n( exp(\u03b8j`)\nW\n) \u00b7 \u00b7 \u00b7 ))\n\u2265 e\u22124b \u2211 j1\u2208S j1 6=i,i\u2032\n( exp(\u03b8\u0303j1)\nW\u0303 \u2212 exp(\u03b8\u0303j1) \u00b7 \u00b7 \u00b7 \u2211 j`\u22121\u2208S j`\u22121 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22122\n( exp(\u03b8\u0303j`\u22121)\nW\u0303 \u2212 \u2211j`\u22121\nk=j1 exp(\u03b8\u0303k) \u2211 j`\u2208S j` 6=i,i\u2032, j1,\u00b7\u00b7\u00b7 ,j`\u22121\n( exp(\u03b8\u0303j`)\nW\u0303\n) \u00b7 \u00b7 \u00b7 ))\n= ( e\u22124b ) P \u03b8\u0303 [ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > ` ] . (58)\nThe second inequality uses exp(\u03b8i)W \u2265 e \u22122b/\u03ba and exp(\u03b8\u0303i) W\u0303 \u2264 e2b/\u03ba. Observe that exp(\u03b8\u0303j) = 1 for all j 6= i, i\u2032 and exp(\u03b8\u0303i) + exp(\u03b8\u0303i\u2032) = \u03b1\u0303 \u2264 d\u03b1\u0303e = \u03b1 \u2265 1. Therefore, we have\nP \u03b8\u0303\n[ \u03c3\u22121(i), \u03c3\u22121(i\u2032) > ` ] = ( \u03ba\u2212 2 ` ) ` ! (\u03ba\u2212 2 + \u03b1\u0303)(\u03ba\u2212 2 + \u03b1\u0303\u2212 1) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 2 + \u03b1\u0303\u2212 (`\u2212 1))\n\u2265 (\u03ba\u2212 2)! (\u03ba\u2212 `\u2212 2)!\n1\n(\u03ba+ \u03b1\u2212 2)(\u03ba+ \u03b1\u2212 3) \u00b7 \u00b7 \u00b7 (\u03ba+ \u03b1\u2212 (`+ 1))\n\u2265 (\u03ba\u2212 `+ \u03b1\u2212 2)(\u03ba\u2212 `+ \u03b1\u2212 3) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 `\u2212 1) (\u03ba+ \u03b1\u2212 2)(\u03ba+ \u03b1\u2212 3) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 1)\n\u2265 (\u03ba\u2212 `)(\u03ba\u2212 `\u2212 1) \u03ba(\u03ba\u2212 1)\n( 1\u2212 `\n\u03ba+ 1\n)\u03b1\u22122 . (59)\nClaim (37) follows by combining Equations (58) and (59) and using the fact that \u03b1 \u2264 2e2b. Claim (38): Define,\n\u03b1\u0303`,\u03b8 \u2261 min i\u2208S min `\u2032\u2208[`] min \u2126\u2208S\\{i}\n:|\u2126|=\u03ba\u2212`\u2032+1\n{ exp(\u03b8i)(\u2211\nj\u2208\u2126 exp(\u03b8j) ) /|\u2126|\n} . (60)\nAlso, define \u03b1`,\u03b8 \u2261 b\u03b1\u0303`,\u03b8c. Note that \u03b1`,\u03b8 \u2265 0 and \u03b1\u0303`,\u03b8 \u2264 e2b. We denote the sum of the weights by W \u2261 \u2211 j\u2208S exp(\u03b8j). Analogous to the proof of claim (37), we define the new set of parameters {\u03b8\u0303j}j\u2208S :\n\u03b8\u0303j =\n{ log(\u03b1\u0303`,\u03b8) for j = i ,\n0 otherwise . (61)\nSimilarly define W\u0303 \u2261 \u2211\nj\u2208S exp(\u03b8\u0303j) = \u03ba \u2212 1 + \u03b1\u0303`,\u03b8. Using the techniques similar to the ones used in proof of claim (37), we have,\nP\u03b8 [ \u03c3\u22121(i) = ` ] \u2264 e4bP\n\u03b8\u0303\n[ \u03c3\u22121(i) = ` ] . (62)\nObserve that exp(\u03b8\u0303j) = 1 for all j 6= i and exp(\u03b8\u0303i) = \u03b1\u0303`,\u03b8 \u2265 b\u03b1\u0303`,\u03b8c = \u03b1`,\u03b8 \u2265 0. Therefore, we have\nP \u03b8\u0303\n[ \u03c3\u22121(i) = ` ] = ( \u03ba\u2212 1 `\u2212 1 ) \u03b1\u0303`,\u03b8(`\u2212 1)! (\u03ba\u2212 1 + \u03b1\u0303`,\u03b8)(\u03ba\u2212 2 + \u03b1\u0303`,\u03b8) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 `+ \u03b1\u0303`,\u03b8)\n\u2264 (\u03ba\u2212 1)! (\u03ba\u2212 `)!\ne2b\n(\u03ba\u2212 1 + \u03b1`,\u03b8)(\u03ba\u2212 2 + \u03b1`,\u03b8) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 `+ \u03b1`,\u03b8)\n\u2264 e 2b\n\u03ba\n( 1\u2212 `\n\u03ba+ \u03b1`,\u03b8\n)\u03b1`,\u03b8\u22121 \u2264 e 2b\n\u03ba\u2212 ` . (63)\nClaim 38 follows by combining Equations (62) and (63). Claim (39): Again, we construct a new set of parameters {\u03b8\u0303j}j\u2208[d] from the original \u03b8 using \u03b1\u0303`,\u03b8 defined in (60):\n\u03b8\u0303j =\n{ log(\u03b1\u0303`,\u03b8) for j \u2208 {i, i\u2032} ,\n0 otherwise . (64)\nSimilarly define W\u0303 \u2261 \u2211\nj\u2208S exp(\u03b8\u0303j) = \u03ba\u2212 2 + 2\u03b1\u0303`,\u03b8. Using the techniques similar to the ones used in proof of claim (37), we have,\nP\u03b8 [ \u03c3\u22121(i) = `1, \u03c3 \u22121(i\u2032) = `2 ] \u2264 e8bP\n\u03b8\u0303\n[ \u03c3\u22121(i) = `1, \u03c3 \u22121(i\u2032) = `2 ] (65)\nObserve that exp(\u03b8\u0303j) = 1 for all j 6= i, i\u2032 and exp(\u03b8\u0303i) = exp(\u03b8\u0303i\u2032) = \u03b1\u0303`,\u03b8 \u2265 b\u03b1\u0303c`,\u03b8 = \u03b1`,\u03b8 \u2265 0. Therefore, we have\n= P \u03b8\u0303\n[ \u03c3\u22121(i) = `1, \u03c3 \u22121(i\u2032) = `2 ] = ( ( \u03ba\u22122 `2\u22122 ) \u03b1\u03032`,\u03b8(`2 \u2212 2)!\n(\u03ba\u2212 2 + 2\u03b1\u0303`,\u03b8)(\u03ba\u2212 1 + 2\u03b1\u0303`,\u03b8) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 2 + 2\u03b1\u0303`,\u03b8 \u2212 (`1 \u2212 1))\n1\n(\u03ba\u2212 2 + \u03b1\u0303`,\u03b8 \u2212 (`1 \u2212 1)) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 2 + \u03b1\u0303`,\u03b8 \u2212 (`2 \u2212 2))\n)\n\u2264 (\u03ba\u2212 2)! (\u03ba\u2212 `2)!\ne4b\n(\u03ba\u2212 2)(\u03ba\u2212 1) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 `1 \u2212 1)(\u03ba\u2212 `1 \u2212 1) \u00b7 \u00b7 \u00b7 (\u03ba\u2212 `2)\n\u2264 e 4b\n(\u03ba\u2212 `1 \u2212 1)(\u03ba\u2212 `2) . (66)\nClaim 39 follows by combining Equations (65) and (66)."}, {"heading": "5.7 Proof of Theorem 4.2", "text": "Let H(\u03b8) \u2208 Sd be Hessian matrix such that Hii\u2032(\u03b8) = \u2202 2LRB(\u03b8) \u2202\u03b8i\u2202\u03b8i\u2032 . The Fisher information matrix is defined as I(\u03b8) = \u2212E\u03b8[H(\u03b8)]. From lemma 2.1, LRB(\u03b8) is concave. This implies that I(\u03b8) is positivesemidefinite and from (23) its smallest eigenvalue is zero with all-ones being the corresponding eigenvector. Fix any unbiased estimator \u03b8\u0302 of \u03b8 \u2208 \u2126b. Since, \u03b8\u0302 \u2208 U , \u03b8\u0302 \u2212 \u03b8 is orthogonal to 1. The\nCramer-Rao lower bound then implies that E[\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162] \u2265 \u2211d\ni=2 1 \u03bbi(I(\u03b8)) . Taking supremum over\nboth sides gives\nsup \u03b8 E[\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162] \u2265 sup \u03b8 d\u2211 i=2\n1\n\u03bbi(I(\u03b8)) \u2265 d\u2211 i=2\n1\n\u03bbi(I(0)) . (67)\nIn the following, we will show that\nI(0) = \u2212E\u03b8[H(0)] n\u2211 j=1 `j\u2211 a=1 mj,a \u2212 \u03b7j,a \u03baj(\u03baj \u2212 1) \u2211 i<i\u2032\u2208Sj (ei \u2212 ei\u2032)(ei \u2212 ei\u2032)> (68)\nmax j,a\n{ mj,a \u2212 \u03b7j,a } L . (69)\nUsing Jensen\u2019s inequality, we have \u2211d\ni=2 1 \u03bbi(I(0)) \u2265 (d\u22121) 2\u2211d i=2 \u03bbi(I(0)) = (d\u22121) 2 Tr(I(0)) . From (68), we have\nTr(I(0)) \u2264 \u2211 j,a(mj,a\u2212\u03b7j,a). From (69), we have \u2211d i=2 1/\u03bbi(I(0)) \u2265 (1/max{mj,a\u2212\u03b7j,a}) \u2211d\ni=1 1/\u03bbi(L) . This proves the desired claim.\nNow we are left to show claim (68). Consider a rank-breaking edge ej,a. Using notations defined in lemma 5.4, in particular Equation (44), and omitting subscript {j, a} whenever it is clear from the context, we have, for any i \u2208 V (ej,a),\n\u22022P\u03b8(ej,a) \u22022\u03b8i =\n{\u2211 \u03c3 ( \u2212A\u03c3B\u03c3e\u03b8i +A\u03c3(B\u03c3)2e2\u03b8i +A\u03c3C\u03c3e\u03b8i ) if i \u2208 B(ej,a)\u2211\n\u03c3\n( A\u03c3 \u2212 3A\u03c3B\u03c3,ie\u03b8i +A\u03c3C\u03c3,i)e2\u03b8i +A\u03c3(B\u03c3,i)2e2\u03b8i ) if i \u2208 T (ej,a) ,\n(70)\nand using (45), we have\n\u22022 logP\u03b8(ej,a) \u22022\u03b8i \u2223\u2223\u2223 \u03b8=0 = {( (C\u03c3 \u2212B\u03c3) ) \u03b8=0 if i \u2208 B(ej,a)( 1\nmj,a!\n\u2211 \u03c3 ( C\u03c3,i \u2212B\u03c3,i + (B\u03c3,i)2 ) \u2212 (\u2211 \u03c3 B\u03c3,i mj,a! )2) \u03b8=0 if i \u2208 T (ej,a) , (71)\nwhere \u03c3 \u2208 \u039bT (ej,a) and the subscript \u03b8 = 0 indicates the the respective quantities are evaluated at \u03b8 = 0. From the definitions given in (44), for \u03b8 = 0, we have B\u03c3 \u2212 C\u03c3 = \u2211m\u22121 u=0 (r\u2212u\u22121) (r\u2212u)2\nand, \u2211\n\u03c3(B\u03c3,i \u2212 C\u03c3,i)/(m!) = 1 m \u2211m\u22121 u=0 (m\u2212u)(r\u2212u\u22121) (r\u2212u)2 . Also, \u2211 \u03c3 B\u03c3,i/(m!) = 1 m \u2211m\u22121 u=0\nm\u2212u r\u2212u and\u2211\n\u03c3(B\u03c3,i) 2/(m!) = 1m \u2211m\u22121 u=0 (\u2211u u\u2032=0 1 r\u2212u\u2032 )2 . Combining all these and, using P\u03b8=0[i \u2208 T (ej,a)] = m/\u03ba and P\u03b8=0[i \u2208 B(ej,a)] = (r \u2212m)/\u03ba, and after following some algebra, we have for any i \u2208 Sj ,\n\u2212E [ \u22022 logP\u03b8(ej,a)\n\u22022\u03b8i\n\u2223\u2223\u2223 \u03b8=0 ] = 1\n\u03ba\n( m\u2212 m\u22121\u2211 u=0 1 r \u2212 u \u2212 1 m m\u22121\u2211 u=0 u(m\u2212 u) (r \u2212 u)2 \u2212 1 m m\u22122\u2211 u=0 2u r \u2212 u (m\u22121\u2211 u\u2032>u m\u2212 u\u2032 r \u2212 u\u2032 ))\n= mj,a \u2212 \u03b7j,a\n\u03baj , (72)\nwhere \u03b7j,a is defined in (12). Since row-sums of H(\u03b8) are zeroes, (23), and for \u03b8 = 0, all the items are exchangeable, we have for any i 6= i\u2032 \u2208 Sj ,\nE [ \u22022 logP\u03b8(ej,a)\n\u2202\u03b8i\u2202\u03b8i\u2032\n\u2223\u2223\u2223 \u03b8=0 ] = mj,a \u2212 \u03b7j,a \u03baj(\u03baj \u2212 1) , (73)\nThe claim (68) follows from the expression of H(\u03b8), Equation (23). To verify (72), observe that (r \u2212m)(B\u03c3 \u2212 C\u03c3) +m( \u2211 \u03c3 B\u03c3,i/(m!)) = m\u2212 \u2211m\u22121 u=0 1 r\u2212u . And,\n1\nm (m\u22121\u2211 u=0 m\u2212 u r \u2212 u )2 \u2212 m\u22121\u2211 u=0 ( u\u2211 u\u2032=0 1 r \u2212 u\u2032 )2\n= m\u22121\u2211 u=0 ( (m\u2212 u)2 m(r \u2212 u)2 \u2212 m\u2212 u (r \u2212 u)2 ) + \u2211 0\u2264u<u\u2032\u2264m\u22121 ( 2(m\u2212 u)(m\u2212 u\u2032) m(r \u2212 u)(r \u2212 u\u2032) \u2212 2(m\u2212 u \u2032) (r \u2212 u)(r \u2212 u\u2032) )\n= m\u22121\u2211 u=0 \u2212u(m\u2212 u) m(r \u2212 u)2 + \u2211 0\u2264u<u\u2032\u2264m\u22121 \u22122u(m\u2212 u\u2032) m(r \u2212 u)(r \u2212 u\u2032) ."}, {"heading": "5.8 Tightening of Lemma 5.4", "text": "Recall that P\u03b8(ej,a) is same as probability of P\u03b8[T (ej,a) B(ej,a)] that is the probability that an agent ranks T (ej,a) items above B(ej,a) items when provided with a set comprising V (ej,a) items. As earlier, for brevity of notations, we omit subscript {j, a} whenever it is clear from the context. For m = 1 or 2, it is easy to check that all off-diagonal elements in hessian matrix of logP\u03b8(e) are non-negative. However, since number of terms in summation in P\u03b8(e) grows as m!, for m \u2265 3 the straight-forward approach becomes too complex. Below, we derive expressions for cross-derivatives in hessian, for general m, using alternate definition (sorting of independent exponential r.v.\u2019s in increasing order) of PL model, where the number of terms grow only as 2m. However, we are unable to analytically prove that the cross-derivatives are non-negative for m > 2. Feeding these expressions in MATLAB and using symbolic computation, for m = 3, we can simplify these expressions and it turns out that they are sum of only positive numbers. For m = 4, with limited computational power it becomes intractable. We believe that it should hold for any value of m < r. Using (36), we need to check only for cross-derivatives for the case when i 6= i\u2032 \u2208 T (ej,a) or i \u2208 T (ej,a), i\u2032 \u2208 B(ej,a). Since, minimum of exponential random variables is exponential, we can assume that |B(ej,a)| = 1 that is r = m + 1. Define \u03bbi \u2261 e\u03b8i . Without loss of generality, assume T (ej,a) = {2, \u00b7 \u00b7 \u00b7 ,m + 1} and B(ej,a) = {1}. Define Cx = \u220fm+1 i=3 (1 \u2212 e\u2212\u03bbix). Then, using\nthe alternate definition of the PL model, we have, P\u03b8(e) = \u222b\u221e 0 Cx(1\u2212 e \u2212\u03bb2x)\u03bb1e \u2212\u03bb1xdx. Following some algebra, \u2202 2 log P\u03b8(e) \u2202\u03b81\u2202\u03b82\n\u2265 0 is equivalent to A1 \u2265 0, where A1 \u2261(\u222b Cx ( xe\u2212\u03bb1x \u2212 xe\u2212\u03bbx ) dx )(\u222b Cxxe \u2212\u03bbxdx ) \u2212 (\u222b Cx(e \u03bb1x \u2212 e\u2212\u03bbx)dx )(\u222b Cxx 2e\u2212\u03bbxdx ) ,\nwhere all integrals are from 0 to \u221e and, \u03bb \u2261 \u03bb1 + \u03bb2. Consider A1 as a function of \u03bb1. Since A1(\u03bb1) = 0 for \u03bb1 = \u03bb, showing \u2202A1/\u2202\u03bb1 \u2264 0 for 0 \u2264 \u03bb1 \u2264 \u03bb would suffice. Following some algebra, and using \u03bb1 \u2264 \u03bb, \u2202A1/\u2202\u03bb1 \u2264 0 is equivalent to A2(\u03bb1) \u2261 ( \u222b\u221e 0 Cxxe \u2212\u03bb1x ) / ( \u222b\u221e 0 Cxx 2e\u2212\u03bb1x ) being monotonically non-decreasing in \u03bb1. To further simplify the condition, define f (0)(y) = 1/y2, g(0)(y) = 1/y3 and, f (1)(y) = f (0)(y) \u2212 f (0)(y + \u03bb3), and recursively f (m\u22121)(y) = f (m\u22122)(y) \u2212 f (m\u22122)(y + \u03bbm+1). Similarly define g (0), \u00b7 \u00b7 \u00b7 , g(m\u22121). Using these recursively defined functions,\n2A2(\u03bb1) = f (m\u22121)(\u03bb1)\ng(m\u22121)(\u03bb1) ,\nfor m = 3, 2A2(\u03bb1) = \u03bb\u221221 \u2212 (\u03bb1 + \u03bb3)\u22122 \u2212 (\u03bb1 + \u03bb4)\u22122 + (\u03bb1 + \u03bb3 + \u03bb4)\u22122\n\u03bb\u221231 \u2212 (\u03bb1 + \u03bb3)\u22123 \u2212 (\u03bb1 + \u03bb4)\u22123 + (\u03bb1 + \u03bb3 + \u03bb4)\u22123 .\nTherefore, we need to show that A2(\u03bb1) is monotonically non-decreasing in \u03bb1 \u2265 0 for any nonnegative \u03bb3, \u00b7 \u00b7 \u00b7 , \u03bbm, and that would suffice to prove that the cross-derivatives arising from i \u2208 T (ej,a), i\n\u2032 \u2208 B(ej,a) are non-negative. For cross-derivatives arising from i 6= i\u2032 \u2208 T (ej,a), defineBx = \u220fm+1 i=4 (1\u2212e\u03bbix)e\u2212\u03bb1x. \u22022 log P\u03b8(e) \u2202\u03b82\u2202\u03b83\n\u2265 0 is equivalent to A3 \u2265 0, where A3 \u2261(\u222b\nBx(1\u2212 e\u2212\u03bb2x)(1\u2212 e\u2212\u03bb3x)dx )(\u222b Bxx 2e\u2212(\u03bb2+\u03bb3)xdx ) \u2212 (\u222b Bx(1\u2212 e\u2212\u03bb2x)xe\u2212\u03bb3xdx )(\u222b Bx(1\u2212 e\u2212\u03bb3x)xe\u2212\u03bb2xdx ) ,\nwhere all integrals are from 0 to \u221e. For m = 3, using MATLAB we can show that both types of cross-derivatives are non-negative."}, {"heading": "Acknowledgements", "text": "This work is supported by NSF SaTC award CNS-1527754, and NSF CISE award CCF-1553452."}], "references": [{"title": "Oracle inequalities for computationally adaptive model selection", "author": ["A. Agarwal", "P.L. Bartlett", "J.C. Duchi"], "venue": "arXiv preprint arXiv:1208.0129,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Meil\u0103. Experiments with kemeny ranking: What works when", "author": ["M.A. Ali"], "venue": "Mathematical Social Sciences,", "citeRegEx": "Ali,? \\Q2012\\E", "shortCiteRegEx": "Ali", "year": 2012}, {"title": "Random utility theory for social choice", "author": ["H. Azari Soufiani", "D.C. Parkes", "L. Xia"], "venue": "In NIPS,", "citeRegEx": "Soufiani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soufiani et al\\.", "year": 2012}, {"title": "Generalized method-of-moments for rank aggregation", "author": ["H. Azari Soufiani", "W. Chen", "D. C Parkes", "L. Xia"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Soufiani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Soufiani et al\\.", "year": 2013}, {"title": "Computing parametric ranking models via rank-breaking", "author": ["H. Azari Soufiani", "D. Parkes", "L. Xia"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Soufiani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soufiani et al\\.", "year": 2014}, {"title": "Theoretical and empirical evaluation of data reduction for exact kemeny rank aggregation", "author": ["N. Betzler", "R. Bredereck", "R. Niedermeier"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Betzler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Betzler et al\\.", "year": 2014}, {"title": "The tradeoffs of large scale learning", "author": ["O. Bousquet", "L. Bottou"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Bousquet and Bottou.,? \\Q2008\\E", "shortCiteRegEx": "Bousquet and Bottou.", "year": 2008}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["V. Chandrasekaran", "M.I. Jordan"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Chandrasekaran and Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran and Jordan.", "year": 2013}, {"title": "Spectral mle: Top-k rank aggregation from pairwise comparisons", "author": ["Y. Chen", "C. Suh"], "venue": null, "citeRegEx": "Chen and Suh.,? \\Q2015\\E", "shortCiteRegEx": "Chen and Suh.", "year": 2015}, {"title": "Improved sum-of-squares lower bounds for hidden clique and hidden submatrix problems", "author": ["Y. Deshpande", "A. Montanari"], "venue": "arXiv preprint arXiv:1502.06590,", "citeRegEx": "Deshpande and Montanari.,? \\Q2015\\E", "shortCiteRegEx": "Deshpande and Montanari.", "year": 2015}, {"title": "Solution of a ranking problem from binary comparisons", "author": ["L.R. Ford Jr."], "venue": "The American Mathematical Monthly,", "citeRegEx": "Jr.,? \\Q1957\\E", "shortCiteRegEx": "Jr.", "year": 1957}, {"title": "Eigentaste: A constant time collaborative filtering algorithm", "author": ["K. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins"], "venue": "Information Retrieval,", "citeRegEx": "Goldberg et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2001}, {"title": "Minimax-optimal inference from partial rankings", "author": ["B. Hajek", "S. Oh", "J. Xu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hajek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hajek et al\\.", "year": 2014}, {"title": "A large-deviation inequality for vector-valued martingales", "author": ["T.P. Hayes"], "venue": "Combinatorics, Probability and Computing,", "citeRegEx": "Hayes.,? \\Q2005\\E", "shortCiteRegEx": "Hayes.", "year": 2005}, {"title": "Mm algorithms for generalized bradley-terry models", "author": ["D.R. Hunter"], "venue": "Ann. of Stat., pages 384\u2013406,", "citeRegEx": "Hunter.,? \\Q2004\\E", "shortCiteRegEx": "Hunter.", "year": 2004}, {"title": "Nantonac collaborative filtering: recommendation based on order responses", "author": ["T. Kamishima"], "venue": "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Kamishima.,? \\Q2003\\E", "shortCiteRegEx": "Kamishima.", "year": 2003}, {"title": "Data-driven rank breaking for efficient rank aggregation", "author": ["A. Khetan", "S. Oh"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Khetan and Oh.,? \\Q2016\\E", "shortCiteRegEx": "Khetan and Oh.", "year": 2016}, {"title": "Tradeoffs for space, time, data and risk in unsupervised learning", "author": ["M. Lucic", "M.I. Ohannessian", "A. Karbasi", "A. Krause"], "venue": "In AISTATS,", "citeRegEx": "Lucic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lucic et al\\.", "year": 2015}, {"title": "Fast and accurate inference of plackett-luce models", "author": ["L. Maystre", "M. Grossglauser"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Maystre and Grossglauser.,? \\Q2015\\E", "shortCiteRegEx": "Maystre and Grossglauser.", "year": 2015}, {"title": "Sum-of-squares lower bounds for planted clique", "author": ["R. Meka", "A. Potechin", "A. Wigderson"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Meka et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meka et al\\.", "year": 2015}, {"title": "Rank centrality: Ranking from pair-wise comparisons", "author": ["S. Negahban", "S. Oh", "D. Shah"], "venue": null, "citeRegEx": "Negahban et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2014}, {"title": "Logarithmic concave measures and related topics", "author": ["A. Pr\u00e9kopa"], "venue": "In Stochastic programming,", "citeRegEx": "Pr\u00e9kopa.,? \\Q1980\\E", "shortCiteRegEx": "Pr\u00e9kopa.", "year": 1980}, {"title": "Estimation from pairwise comparisons: Sharp minimax bounds with topology dependence", "author": ["N.B. Shah", "S. Balakrishnan", "J. Bradley", "A. Parekh", "K. Ramchandran", "M.J. Wainwright"], "venue": null, "citeRegEx": "Shah et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2015}, {"title": "Stochastically transitive models for pairwise comparisons: Statistical and computational issues", "author": ["N.B. Shah", "S. Balakrishnan", "A. Guntuboyina", "M.J. Wainright"], "venue": "arXiv preprint arXiv:1510.05610,", "citeRegEx": "Shah et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2015}, {"title": "Svm optimization: inverse dependence on training set size", "author": ["S. Shalev-Shwartz", "N. Srebro"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Shalev.Shwartz and Srebro.,? \\Q2008\\E", "shortCiteRegEx": "Shalev.Shwartz and Srebro.", "year": 2008}, {"title": "Asymptotics when the number of parameters tends to infinity in the bradley-terry model for paired comparisons", "author": ["G. Simons", "Y. Yao"], "venue": "The Annals of Statistics,", "citeRegEx": "Simons and Yao.,? \\Q1999\\E", "shortCiteRegEx": "Simons and Yao.", "year": 1999}], "referenceMentions": [{"referenceID": 6, "context": "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).", "startOffset": 207, "endOffset": 342}, {"referenceID": 24, "context": "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).", "startOffset": 207, "endOffset": 342}, {"referenceID": 7, "context": "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).", "startOffset": 207, "endOffset": 342}, {"referenceID": 0, "context": "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).", "startOffset": 207, "endOffset": 342}, {"referenceID": 17, "context": "As a solution, recent advances in learning theory introduce hierarchies of algorithmic solutions, ordered by the respective computational complexity, for several fundamental machine learning applications in (Bousquet and Bottou, 2008; Shalev-Shwartz and Srebro, 2008; Chandrasekaran and Jordan, 2013; Agarwal et al., 2012; Lucic et al., 2015).", "startOffset": 207, "endOffset": 342}, {"referenceID": 9, "context": "For such traditional preferences, efficient schemes for learning to rank have been proposed, such as Ford Jr. (1957); Hunter (2004); Hajek et al.", "startOffset": 106, "endOffset": 117}, {"referenceID": 9, "context": "For such traditional preferences, efficient schemes for learning to rank have been proposed, such as Ford Jr. (1957); Hunter (2004); Hajek et al.", "startOffset": 106, "endOffset": 132}, {"referenceID": 9, "context": "For such traditional preferences, efficient schemes for learning to rank have been proposed, such as Ford Jr. (1957); Hunter (2004); Hajek et al. (2014); Chen and Suh (2015), which we explain in detail in Section 1.", "startOffset": 106, "endOffset": 153}, {"referenceID": 8, "context": "(2014); Chen and Suh (2015), which we explain in detail in Section 1.", "startOffset": 8, "endOffset": 28}, {"referenceID": 8, "context": "(2014); Chen and Suh (2015), which we explain in detail in Section 1.1. However, modern datasets are unstructured and heterogeneous. As Khetan and Oh (2016) show, this can lead to significant increase in the computational complexity, requiring exponential run-time in the size of the problem in the worst case.", "startOffset": 8, "endOffset": 157}, {"referenceID": 12, "context": "Hajek et al. (2014) provides full analysis of the statistical complexity of this MLE under traditional structures.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "comparisons in (Ford Jr., 1957; Hunter, 2004; Negahban et al., 2014; Shah et al., 2015a; Maystre and Grossglauser, 2015).", "startOffset": 15, "endOffset": 120}, {"referenceID": 20, "context": "comparisons in (Ford Jr., 1957; Hunter, 2004; Negahban et al., 2014; Shah et al., 2015a; Maystre and Grossglauser, 2015).", "startOffset": 15, "endOffset": 120}, {"referenceID": 18, "context": "comparisons in (Ford Jr., 1957; Hunter, 2004; Negahban et al., 2014; Shah et al., 2015a; Maystre and Grossglauser, 2015).", "startOffset": 15, "endOffset": 120}, {"referenceID": 16, "context": "For the precise condition for consistent rank-breaking we refer to (Azari Soufiani et al., 2013, 2014; Khetan and Oh, 2016).", "startOffset": 67, "endOffset": 123}, {"referenceID": 2, "context": "Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples.", "startOffset": 6, "endOffset": 29}, {"referenceID": 2, "context": "Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples. In the example from Figure 1, there are 12 paired relations implied by the DAG: (i6 \u227a i5), (i6 \u227a i4), (i6 \u227a i3), . . . , (i3 \u227a i1), (i4 \u227a i1). In order to get a consistent estimate, Azari Soufiani et al. (2014) provide a rule for choosing which pairs to include, and Khetan and Oh (2016) provide an estimator that optimizes how to weigh each of those chosen pairs to get the best finite sample complexity bound.", "startOffset": 6, "endOffset": 440}, {"referenceID": 2, "context": "Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples. In the example from Figure 1, there are 12 paired relations implied by the DAG: (i6 \u227a i5), (i6 \u227a i4), (i6 \u227a i3), . . . , (i3 \u227a i1), (i4 \u227a i1). In order to get a consistent estimate, Azari Soufiani et al. (2014) provide a rule for choosing which pairs to include, and Khetan and Oh (2016) provide an estimator that optimizes how to weigh each of those chosen pairs to get the best finite sample complexity bound.", "startOffset": 6, "endOffset": 517}, {"referenceID": 2, "context": "Azari Soufiani et al. (2014) showed that if we include all paired comparisons, then the resulting estimate can be statistically inconsistent due to the ignored correlations among the paired orderings, even with infinite samples. In the example from Figure 1, there are 12 paired relations implied by the DAG: (i6 \u227a i5), (i6 \u227a i4), (i6 \u227a i3), . . . , (i3 \u227a i1), (i4 \u227a i1). In order to get a consistent estimate, Azari Soufiani et al. (2014) provide a rule for choosing which pairs to include, and Khetan and Oh (2016) provide an estimator that optimizes how to weigh each of those chosen pairs to get the best finite sample complexity bound. However, such a consistent pairwise rank-breaking results in throwing away many of the ordered relations, resulting in significant loss in accuracy. For example, Including a paired relation from Gj in the example results in a biased estimator. None of the pairwise orderings can be used from Gj , without making the estimator inconsistent as shown in Azari Soufiani et al. (2013). Whether we include all paired comparisons or only a subset of consistent ones, there is a significant loss in accuracy as illustrated in Figure 3.", "startOffset": 6, "endOffset": 1021}, {"referenceID": 6, "context": "In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available.", "startOffset": 43, "endOffset": 70}, {"referenceID": 6, "context": "In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available. Various gradient based algorithms are analyzed that show the time-accuracy-sample tradeoff. In a similar context, Shalev-Shwartz and Srebro (2008) analyze a particular implementation of support vector machine and show that the target accuracy can be achieved faster when more data is available, by running the iterative algorithm for shorter amount of time.", "startOffset": 43, "endOffset": 340}, {"referenceID": 6, "context": "In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available. Various gradient based algorithms are analyzed that show the time-accuracy-sample tradeoff. In a similar context, Shalev-Shwartz and Srebro (2008) analyze a particular implementation of support vector machine and show that the target accuracy can be achieved faster when more data is available, by running the iterative algorithm for shorter amount of time. In the application of de-noising, Chandrasekaran and Jordan (2013) provide a hierarchy of convex relaxations where constraints are defined by convex geometry with increasing complexity.", "startOffset": 43, "endOffset": 618}, {"referenceID": 6, "context": "In the application of supervised learning, Bousquet and Bottou (2008) proposed the idea that weaker approximate optimization algorithms are sufficient for learning when more data is available. Various gradient based algorithms are analyzed that show the time-accuracy-sample tradeoff. In a similar context, Shalev-Shwartz and Srebro (2008) analyze a particular implementation of support vector machine and show that the target accuracy can be achieved faster when more data is available, by running the iterative algorithm for shorter amount of time. In the application of de-noising, Chandrasekaran and Jordan (2013) provide a hierarchy of convex relaxations where constraints are defined by convex geometry with increasing complexity. For unsupervised learning, Lucic et al. (2015) introduce a hierarchy of data representations that provide more representative elements when more data is available at no additional computation.", "startOffset": 43, "endOffset": 784}, {"referenceID": 5, "context": "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition.", "startOffset": 14, "endOffset": 25}, {"referenceID": 5, "context": "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition.", "startOffset": 14, "endOffset": 43}, {"referenceID": 5, "context": "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights.", "startOffset": 14, "endOffset": 129}, {"referenceID": 5, "context": "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions.", "startOffset": 14, "endOffset": 364}, {"referenceID": 5, "context": "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions. On the theoretical side, when samples consist of pairwise comparisons, Simons and Yao (1999) first established consistency and asymptotic normality of the maximum likelihood estimate when all teams play against each other.", "startOffset": 14, "endOffset": 603}, {"referenceID": 5, "context": "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions. On the theoretical side, when samples consist of pairwise comparisons, Simons and Yao (1999) first established consistency and asymptotic normality of the maximum likelihood estimate when all teams play against each other. For a broader class of scenarios where we allow for sparse observations, where the number of total comparisons grow linearly in the number of teams, Negahban et al. (2014) show that Rank Centrality achieves optimal sample complexity by comparing it to a lower bound on the minimax rate.", "startOffset": 14, "endOffset": 905}, {"referenceID": 5, "context": "posed in Ford Jr. (1957) and Hunter (2004), which iteratively finds the fixed point of the KKT condition. Negahban et al. (2014) introduce Rank Centrality, a novel spectral ranking algorithm which formulates a random walk from the given data, and show that the stationary distribution provides accurate estimates of the PL weights. Maystre and Grossglauser (2015) provide a connection between those previous approaches, and give a unified random walk approach that finds the fixed point of the KKT conditions. On the theoretical side, when samples consist of pairwise comparisons, Simons and Yao (1999) first established consistency and asymptotic normality of the maximum likelihood estimate when all teams play against each other. For a broader class of scenarios where we allow for sparse observations, where the number of total comparisons grow linearly in the number of teams, Negahban et al. (2014) show that Rank Centrality achieves optimal sample complexity by comparing it to a lower bound on the minimax rate. For a more general class of traditional observations, including pairwise comparisons, Hajek et al. (2014) provide similar optimal guarantee for the maximum likelihood estimator.", "startOffset": 14, "endOffset": 1126}, {"referenceID": 4, "context": "Chen and Suh (2015) introduced Spectral MLE that applies Rank Centrality followed by MLE, and showed that the resulting estimate is optimal in L\u221e error as well as the previously analyzed L2 error.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Chen and Suh (2015) introduced Spectral MLE that applies Rank Centrality followed by MLE, and showed that the resulting estimate is optimal in L\u221e error as well as the previously analyzed L2 error. Shah et al. (2015a) study a new measure of the error induced by the Laplacian of the comparisons graph and prove a sharper upper and lower bounds that match up to a constant factor.", "startOffset": 0, "endOffset": 217}, {"referenceID": 2, "context": "Although, statistical and computational tradeoffs have been investigated under other popular choice models such as the Mallows models by Betzler et al. (2014) or stochastically transitive models by Shah et al.", "startOffset": 137, "endOffset": 159}, {"referenceID": 2, "context": "Although, statistical and computational tradeoffs have been investigated under other popular choice models such as the Mallows models by Betzler et al. (2014) or stochastically transitive models by Shah et al. (2015b), the algorithmic solutions do not apply to random utility models and the analysis techniques do not extend.", "startOffset": 137, "endOffset": 218}, {"referenceID": 2, "context": "However, it is also known from Azari Soufiani et al. (2014), that for general RUMs there is no consistent rank-breaking, and the proposed approach does not generalize.", "startOffset": 37, "endOffset": 60}, {"referenceID": 2, "context": "In a special case when M = 1, this can be transformed into the traditional pairwise rank-breaking, where (i) this is a concave maximization; (ii) the estimate is (asymptotically) unbiased and consistent as shown in Azari Soufiani et al. (2013, 2014); and (iii) and the finite sample complexity have been analyzed in Khetan and Oh (2016). Although, this order-1 rank-breaking provides a significant gain in computational efficiency, the information contained in higher-order edges are unused, resulting in a significant loss in accuracy.", "startOffset": 221, "endOffset": 337}, {"referenceID": 2, "context": "As predicted by Azari Soufiani et al. (2014), this results in an inconsistent estimate, whose error does not vanish as we increase the sample size.", "startOffset": 22, "endOffset": 45}, {"referenceID": 1, "context": "This is different from learning Mallows models in Ali and Meil\u0103 (2012) where peaked distributions are easier to learn, and is related to the fact that we are not only interested in recovering the (ordinal) ranking but also the (cardinal) weight.", "startOffset": 50, "endOffset": 71}, {"referenceID": 9, "context": "planted clique problem (Deshpande and Montanari, 2015; Meka et al., 2015).", "startOffset": 23, "endOffset": 73}, {"referenceID": 19, "context": "planted clique problem (Deshpande and Montanari, 2015; Meka et al., 2015).", "startOffset": 23, "endOffset": 73}, {"referenceID": 15, "context": "On sushi preferences (Kamishima, 2003) and jester dataset (Goldberg et al.", "startOffset": 21, "endOffset": 38}, {"referenceID": 11, "context": "On sushi preferences (Kamishima, 2003) and jester dataset (Goldberg et al., 2001), we improve over pairwise breaking and achieves same performance as the oracle MLE.", "startOffset": 58, "endOffset": 81}, {"referenceID": 11, "context": "On sushi preferences (Kamishima, 2003) and jester dataset (Goldberg et al., 2001), we improve over pairwise breaking and achieves same performance as the oracle MLE. Full rankings over \u03ba = 10 types of sushi are randomly chosen from d = 100 types of sushi are provided by n = 5000 individuals. As the ground truth \u03b8\u2217, we use the ML estimate of PL weights over the entire data. In Figure 5, left panel, for each m \u2208 {3, 4, 5, 6, 7}, we remove the known ordering among the top-m and bottom-(10 \u2212m) sushi in each set, and run our estimator with one breaking edge between top-m and bottom-(10\u2212m) items. We compare our algorithm with inconsistent pairwise breaking (using optimal choice of parameters from Khetan and Oh (2016)) and the oracle MLE.", "startOffset": 59, "endOffset": 721}, {"referenceID": 18, "context": "We use the following Theorem from Pr\u00e9kopa (1980). A similar technique was used to prove concavity when |T (e)| = 1 in Azari Soufiani et al.", "startOffset": 34, "endOffset": 49}, {"referenceID": 2, "context": "A similar technique was used to prove concavity when |T (e)| = 1 in Azari Soufiani et al. (2012).", "startOffset": 74, "endOffset": 97}, {"referenceID": 21, "context": "1 (Theorem 9 in Pr\u00e9kopa (1980)).", "startOffset": 16, "endOffset": 31}, {"referenceID": 16, "context": "The proof sketch is inspired from Khetan and Oh (2016). The main difference and technical challenge is in showing the strict concavity of LRB(\u03b8) when restricted to \u03a9b.", "startOffset": 34, "endOffset": 55}, {"referenceID": 13, "context": "8, Hayes (2005)], we have", "startOffset": 3, "endOffset": 16}], "year": 2016, "abstractText": "For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data.", "creator": "LaTeX with hyperref package"}}}