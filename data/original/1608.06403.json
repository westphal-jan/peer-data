{"id": "1608.06403", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games", "abstract": "Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary's move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed \\cite{lincombinatorial2014}, where the learner's action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves $O(T^{2/3}\\log T)$ distribution independent and $O(\\log T)$ distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve $O(T^{2/3}\\sqrt{\\log T})$ distribution independent and $O(\\log^2 T)$ distribution dependent regret respectively. Crucially, our framework needs only the simpler \"argmax\" oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an $O(\\log T)$ regret bound, matching the GCB guarantee but removing the dependence on size of the learner's action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top.", "histories": [["v1", "Tue, 23 Aug 2016 07:14:18 GMT  (23kb)", "http://arxiv.org/abs/1608.06403v1", "Appearing in NIPS 2016"]], "COMMENTS": "Appearing in NIPS 2016", "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["sougata chaudhuri", "ambuj tewari"], "accepted": true, "id": "1608.06403"}, "pdf": {"name": "1608.06403.pdf", "metadata": {"source": "CRF", "title": "Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games", "authors": ["Sougata Chaudhur", "Ambuj Tewari"], "emails": ["sougata@umich.edu", "tewaria@umich.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n06 40\n3v 1\n[ cs\n.G T\n] 2\n3 A\nPartial monitoring games are repeated games where the learner receives feedback that might be different from adversary\u2019s move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed [1], where the learner\u2019s action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves O(T 2/3 logT ) distribution independent and O(log T ) distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve O(T 2/3 \u221a logT ) distribution independent and O(log2 T ) distribution dependent regret respectively. Crucially, our framework needs only the simpler \u201cargmax\u201d oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an O(log T ) regret bound, matching the GCB guarantee but removing the dependence on size of the learner\u2019s action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top."}, {"heading": "1 Introduction", "text": "Partial monitoring (PM) games are repeated games played between a learner and an adversary over discrete time points. At every time point, the learner and adversary each simultaneously select an action, from their respective action sets, and the learner gains a reward, which is a function of the two actions. In PM games, the learner receives limited feedback, which might neither be adversary\u2019s move (full information games) nor the reward gained (bandit games). In stochastic PM games, adversary generates actions which are independent and identically distributed according to a distribution fixed before the start of the game and unknown to the learner. The learner\u2019s objective is to develop a learning strategy that incurs low regret over time, based on the feedback received during the course of the game. Regret is defined as the difference between cumulative reward of the learner\u2019s strategy and the best fixed learner\u2019s action in hindsight. The usual learning strategies in online games combine some form of exploration (getting feedback on certain learner\u2019s actions) and exploitation (playing the perceived optimal action based on current estimates).\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nThere has been a substantial advance in our understanding of finite PM games. Starting with early work in the 2000s [2, 3], this body of research reached a culmination point with a comprehensive and complete classification of finite PM games [4]. We refer the reader to these works for more references and also note that newer results continue to appear [5]. Finite PM games restrict both the learner\u2019s and adversary\u2019s action spaces to be finite, with a very general feedback model. All finite partial monitoring games can be classified into one of four categories, with minimax regret \u0398(T ), \u0398(T 2/3), \u0398(T 1/2) and \u0398(1). The classification is governed by global and local observability properties pertaining to a game [4]. Another line of work has extended traditional multi-armed bandit problem (MAB) [6] to include combinatorial action spaces for learner (CMAB) [7, 8]. The combinatorial action space can be exponentially large, rendering traditional MAB algorithms designed for small finite action spaces, impractical with regret bounds scaling with size of action space. The CMAB algorithms exploit a finite subset of base actions, which are specific to the structure of problem at hand, leading to practical algorithms and regret bounds that do not scale with, or scale very mildly with, the size of the learner\u2019s action space.\nWhile finite PM and CMAB problems have witnessed a lot of activity, there is only one paper [1] on combinatorial partial monitoring (CPM) games, to the best of our knowledge. There the authors combined the combinatorial aspect of CMAB with the limited feedback aspect of finite PM games, to develop a CPM model. The model extended PM games to include combinatorial action spaces for learner, which might be exponentially large, and infinite action spaces for the adversary. Neither of these situations can be handled by generic algorithms for finite PM games. Specifically, the model considered an action space X for the learner, that has a small subset of actions defining a global observable set (see Assumption 2 in Section 2). The adversary\u2019s action space is a continuous, bounded vector space with the adversary sampling moves from a fixed distribution over the vector space. The reward function considered is a general non-linear function of learner\u2019s and adversary\u2019s actions, with some restrictions (see Assumptions 1 & 3 in Section 2). The model incorporated a linear feedback mechanism where the feedback received is a linear transformation of adversary\u2019s move. Inspired by the classic confidence bound algorithms for MABs, such as UCB [6], the authors proposed a Global Confidence Bound (GCB) algorithm that enjoyed two types of regret bound. The first one was a distribution independent O(T 2/3 logT ) regret bound and the second one was a distribution dependent O(logT ) regret bound. A distribution dependent regret bound involves factors specific to the adversary\u2019s fixed distribution, while distribution independent means the regret bound holds over all possible distributions in a broad class of distributions. Both bounds also had a logarithmic dependence on |X |. The algorithm combined online estimation with two offline computational oracles. The first oracle finds the action(s) achieving maximum value of reward function over X , for a particular adversary action (argmax oracle), and the second oracle finds the action(s) achieving second maximum value of reward function overX , for a particular adversary action (arg-secondmax oracle). Moreover, the distribution dependent regret bound requires existence of a unique optimal learner action. The inspiration for the CPM model came from various applications like crowdsourcing and matching problems like matching products with customers.\nOur Contributions. We adopt the CPM model proposed earlier [1]. However, instead of using upper confidence bound techniques, our work is motivated by another classic technique developed for MABs, namely that of forced exploration. This technique was already used in the classic paper of Robbins [9] and has also been called \u201cforcing with certainty equivalence\u201d in the control theory literature [10]. We develop a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework (Section 3) borrowing the PEGE terminology from work on linearly parameterized bandits [11]. When the framework is instantiated with different parameters, it achieves O(T 2/3 \u221a logT ) distribution independent and O(log2 T ) distribution dependent regret. Significantly, the framework combines online estimation with only the argmax oracle from GCB, which is a practical advantage over requiring an additional arg-secondmax oracle. Moreover, the distribution dependent regret does not require existence of unique optimal action. Uniqueness of optimal action can be an unreasonable assumption, especially in the presence of a combinatorial action space. Our second contribution is another algorithm PEGE2 (Section 4) that combines a PEGE algorithm with Gap Estimation, to achieve a distribution dependent O(log T ) regret bound, thus matching the GCB regret guarantee in terms of T and gap. Here, gap refers to the difference between expected reward of optimal and second optimal learner\u2019s actions. However, PEGE2 does require access to both the oracles and existence of unique optimal action. On the other hand, like GCB, PEGE2 has the property that its regret is never larger than O(T 2/3 logT ) even when there is no unique optimal action. Another crucial fact\nis that all our regret bounds are independent of |X |, only depending on the size of the small global observable set. Thus, though we have adopted the CPM model [1], our regret bounds are meaningful for countably infinite or even continuous learner\u2019s action space, whereas GCB regret bound has an explicit logarithmic dependence on |X |. We provide a detailed comparison of our work with the GCB algorithm in Section 5. Finally, we discuss how our algorithms can be efficiently applied in the CPM problem of online ranking with feedback restricted to top ranked items (Section 6), a setting already considered [12] but analyzed in a non-stochastic setting."}, {"heading": "2 Preliminaries and Assumptions", "text": "The online game is played between a learner and an adversary, over discrete rounds indexed by t = 1, 2, . . .. The learner\u2019s action set is denoted as X which can be exponentially large. The adversary\u2019s action set is the infinite set [0, 1]n. The adversary fixes a distribution p on [0, 1]n before start of the game (adversary\u2019s strategy), with p unknown to the learner. At each round of the game, adversary samples \u03b8(t) \u2208 [0, 1]n according to p, with E\u03b8(t)\u223cp[\u03b8(t)] = \u03b8\u2217p . The learner chooses x(t) \u2208 X and gets reward r(x(t), \u03b8(t)). However, the learner might not get to know either \u03b8(t) (as in a full information game) or r(x(t), \u03b8(t)) (as in a bandit game). In fact, the learner receives, as feedback, a linear transformation of \u03b8(t).That is, every action x \u2208 X has an associated transformation matrix Mx \u2208 Rmx\u00d7n. On playing action x(t), the learner receives a feedback Mx(t) \u00b7 \u03b8(t) \u2208 Rmx . Note that the game with the defined feedback mechanism subsumes full information and bandit games. Mx = I\nn\u00d7n, \u2200x makes it a full information game since Mx \u00b7 \u03b8 = \u03b8. If r(x, \u03b8) = x \u00b7 \u03b8, then Mx = x \u2208 Rn makes it a bandit game. The dimension n, action space X , reward function r(\u00b7, \u00b7) and transformation matrices Mx, \u2200x \u2208 X are known to the learner. The goal of the learner is to minimize the expected regret, which, for a given time horizon T , is:\nR(T ) = T \u00b7max x\u2208X\nE[r(x, \u03b8)] \u2212 T\u2211\nt=1\nE[r(x(t), \u03b8(t))] (1)\nwhere the expectation in the first term is taken over \u03b8, w.r.t. distribution p, and the second expectation is taken over \u03b8 and possible randomness in the learner\u2019s algorithm.\nFor distribution dependent regret bounds, we define gaps in expected rewards: Let x\u2217 \u2208 S(x) = argmaxx\u2208X r\u0304(x, \u03b8 \u2217 p). Then \u2206x = r\u0304(x\n\u2217, \u03b8\u2217p) \u2212 r\u0304(x, \u03b8\u2217p) , \u2206max = max{\u2206x : x \u2208 X} and \u2206 = min{\u2206x : x \u2208 X ,\u2206x > 0}. Assumption 1. (Restriction on Reward Function) The first assumption is that E\u03b8\u223cp[r(x, \u03b8)] = r\u0304(x, \u03b8\u2217p), for some function r\u0304(\u00b7, \u00b7). That is, the expected reward is a function of x and \u03b8\u2217p , which is always satisfied if r(x, \u03b8) is a linear function of \u03b8, or if distribution p happens to be any distribution with support [0, 1]n and fully parameterized by its mean \u03b8\u2217p . With this assumption, the expected regret becomes:\nR(T ) = T \u00b7 r\u0304(x\u2217, \u03b8\u2217p)\u2212 T\u2211\nt=1\nE[r\u0304(x(t), \u03b8\u2217p)]. (2)\nAssumption 2. (Existence of Global Observable Set) The second assumption is on the existence of a global observable set, which is a subset of learner\u2019s action set and is required for estimating an adversary\u2019s move \u03b8. The global observable set is defined as follows: for a set of actions \u03c3 = {x1, x2, . . . , x|\u03c3|} \u2286 X , let their transformation matrices be stacked in a top down fashion to obtain a R \u2211|\u03c3| i=1 mxi\u00d7n dimensional matrix M\u03c3 . \u03c3 is said to be a global observable set if M\u03c3 has full column rank, i.e., rank(M\u03c3) = n. Then, the Moore-Penrose pseudoinverse M+\u03c3 satisfies M + \u03c3 M\u03c3 = I n\u00d7n. Without the assumption on the existence of global observable set, it might be the case that even if the learner plays all actions in X on same \u03b8, the learner might not be able to recover \u03b8 (as M+\u03c3 M\u03c3 = In\u00d7n will not hold without full rank assumption). In that case, learner might not be able to distinguish between \u03b8\u2217p1 and \u03b8 \u2217 p2 , corresponding to two different adversary\u2019s strategies. Then, with non-zero probability, the learner can suffer \u2126(T ) regret and no learner strategy can guarantee a sub-linear in T regret (the intuition forms the base of the global observability condition in [2]). Note that the size of the global observable set is small, i.e., |\u03c3| \u2264 n. A global observable set can be found by including an action x in \u03c3 if it strictly increases the rank of M\u03c3, till the rank reaches n. There can, of course, be more than one global observable set.\nAssumption 3. (Lipschitz Continuity of Expected Reward Function) The third assumption is on the Lipschitz continuity of expected reward function in its second argument. More precisely, it is assumed that \u2203 R > 0 such that \u2200 x \u2208 X , for any \u03b81 and \u03b82, |r\u0304(x, \u03b81) \u2212 r\u0304(x, \u03b82)| \u2264 R\u2016\u03b81 \u2212 \u03b82\u20162. This assumption is reasonable since otherwise, a small error in estimation of mean reward vector \u03b8\u2217p can introduce a large change in expected reward, leading to difficulty in controlling regret over time. The Lipschitz condition holds trivially for expected reward functions which are linear in second argument. The continuity assumption, along with the fact that adversary\u2019s moves are in [0, 1]n, implies boundedness of expected reward for any learner\u2019s action and any adversary\u2019s action. We denote Rmax = maxx\u2208X ,\u03b8\u2208[0,1]n r\u0304(x, \u03b8).\nThe three assumptions above will be made throughout. However, the fourth assumption will only be made in a subset of our results.\nAssumption 4. (Unique Optimal Action) The optimal action x\u2217 = argmaxx\u2208X r\u0304(x, \u03b8 \u2217 p) is unique. Denote a second best action (which may not be unique) by x\u2217\u2212 = argmaxx\u2208X ,x 6=x\u2217 r\u0304(x, \u03b8 \u2217 p). Note that \u2206 = r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u2217\u2212, \u03b8\u2217p)."}, {"heading": "3 Phased Exploration with Greedy Exploitation", "text": "Algorithm 1 (PEGE) uses the classic idea of doing exploration in phases that are successively further apart from each other. In between exploration phases, we select action greedily by completely trusting the current estimates. The constant \u03b2 controls how much we explore in a given phase and the constant \u03b1 along with the function C(\u00b7) determines how much we exploit. This idea is classic in the bandit literature [9\u201311] but has not been applied to the CPM framework to the best of our knowledge.\nAlgorithm 1 The PEGE Algorithmic Framework 1: Inputs: \u03b1, \u03b2 and function C(\u00b7) (to determine amount of exploration/exploitation in each phase)."}, {"heading": "2: For b = 1, 2, . . . ,", "text": ""}, {"heading": "3: Exploration", "text": "4: For i = 1 to |\u03c3| 5: For j = 1 to b\u03b2 6: Let tj,i = t and \u03b8(tj,i, b) = \u03b8(t) where t is current time point 7: Play xi \u2208 \u03c3 and get feedback Mxi \u00b7 \u03b8(tj,i, b) \u2208 Rmxi . 8: End For 9: End For 10: Estimation 11: \u03b8\u0303j,i = M+\u03c3 (Mx1 \u00b7 \u03b8(tj,1, i), . . . ,Mx|\u03c3| \u00b7 \u03b8(tj,|\u03c3|, i)) \u2208 Rn.\n12: \u03b8\u0302(b) =\n\u2211b i=1 \u2211i\u03b2\nj=1 \u03b8\u0303j,i \u2211b\nj=1 j \u03b2\n\u2208 Rn.\n13: x(b) \u2208 argmaxx\u2208X r\u0304(x, \u03b8\u0302(b)). 14: Exploitation 15: For i = 1 to exp(C(b\u03b1)) 16: Play x(b). 17: End For 18: End For\nIt is easy to see that the estimators in Algorithm 1 have the following properties: Ep[\u03b8\u0303j,i] = M+\u03c3 (Mx1 \u00b7 \u03b8\u2217p, . . . ,Mx|\u03c3| \u00b7 \u03b8\u2217p) = M+\u03c3 M\u03c3 \u00b7 \u03b8\u2217p = \u03b8\u2217p and hence Ep[\u03b8\u0302] = \u03b8\u2217p. Using the fact that\nM+\u03c3 = (M \u22a4 \u03c3 M\u03c3) \u22121M\u22a4\u03c3 , we also have the following bound on estimation error of \u03b8 \u2217 p:\n\u2016\u03b8\u0303j,i \u2212 \u03b8\u2217p\u20162 \u2264 \u2016M+\u03c3 (Mx1 \u00b7 \u03b8(tj,1, i), . . . ,Mx|\u03c3| \u00b7 \u03b8(tj,|\u03c3|, i))\u2212M+\u03c3 M\u03c3\u03b8\u2217p\u20162\n= \u2016(M\u22a4\u03c3 M\u03c3)\u22121 |\u03c3| \u2211\nk=1\nM\u22a4xkMxk \u00b7 (\u03b8(tj,k, i)\u2212 \u03b8\u2217p)\u20162 \u2264 \u221a n |\u03c3| \u2211\nk=1\n\u2016(M\u22a4\u03c3 M\u03c3)\u22121M\u22a4xkMxk\u20162 =: \u03b2\u03c3\n(3) where the constant \u03b2\u03c3 defined above depends only on the structure of the linear transformation matrices of the global observer set and not on adversary strategy p.\nOur first result is about the regret of Algorithm 1 when within phase number b, the exploration part spends |\u03c3| rounds (constant w.r.t. b) and the exploitation part grows polynomially with b. Theorem 1. (Distribution Independent Regret) When Algorithm 1 is initialized with the parameters C(a) = log a, \u03b1 = 1/2 and \u03b2 = 0, and the online game is played over T rounds, we get the following bound on expected regret:\nR(T ) \u2264 Rmax|\u03c3|T 2/3 + 2R\u03b2\u03c3T 2/3 \u221a log 2e2 + 2 logT +Rmax (4)\nwhere \u03b2\u03c3 is the constant as defined in Eq. 3.\nOur next result is about the regret of Algorithm 1 when within phase number b, the exploration part spends |\u03c3| \u00b7 b rounds (linearly increasing with b) and the exploitation part grows exponentially with b.\nTheorem 2. (Distribution Dependent Regret) When Algorithm 1 is initialized with the parameters C(a) = h \u00b7 a, for a tuning parameter h > 0, \u03b1 = 1 and \u03b2 = 1, and the online game is played over T rounds, we get the following bound on expected regret:\nR(T ) \u2264 \u2211\nx\u2208\u03c3\n\u2206x\n( logT\nh\n)2 + 4 \u221a 2\u03c0e2R\u2206max\u03b2\u03c3\n\u2206 e\nh2(2R2\u03b22\u03c3)\n\u22062 . (5)\nSuch an explicit bound for a PEGE algorithm that is polylogarithmic in T and explicitly states the multiplicative and additive constants involved in not known, to the best of our knowledge, even in the bandit literature (e.g., earlier bounds [10] are asymptotic) whereas here we prove it in the CPM setting. Note that the additive constant above, though finite, blows up exponentially fast as \u2206 \u2192 0 for a fixed h. It is well behaved however, if the tuning parameter h is on the same scale as \u2206. This line of thought motivates us to estimate the gap to within constant factors and then feed that estimate into a PEGE algorithm. This is what we will do in the next section."}, {"heading": "4 Combining Gap Estimation with PEGE", "text": "Algorithm 2 tries to estimate the gap\u2206 to within a constant multiplicative factor. However, if there is no unique optimal action or when the true gap is small, gap estimation can take a very large amount of time. To prevent that from happening, the algorithm also takes in a threshold T0 as input and definitely stops if the threshold is reached. The result below assures us that, with high probability, the algorithm behaves as expected. That is, if there is a unique optimal action and the gap is large enough to be estimated with a given confidence before the threshold T0 kicks in, it will output an estimate \u2206\u0302 in the range [0.5\u2206, 1.5\u2206]. On the other hand, if there is no unique optimal action, it does not generate an estimate of \u2206 and instead runs out of the exploration budget T0. Theorem 3. (Gap Estimation within Constant Factors) Let T0 \u2265 1 and \u03b4 \u2208 (0, 1) and define T1(\u03b4) = 256R2\u03b22\u03c3 \u22062 log 512e2R2\u03b22\u03c3 \u22062\u03b4 , T2(\u03b4) = 16R2\u03b22\u03c3 \u22062 log 4e2 \u03b4 . Consider Algorithm 2 run with\nw(b) =\n\u221a\nR2\u03b22\u03c3 log( 4e2b2 \u03b4 )\nb . (6)\nThen, the following 3 claims hold.\n1. Suppose Assumption 4 holds and T1(\u03b4) < T0. Then with probability at least 1 \u2212 \u03b4, Algorithm 2 stops in T1(\u03b4) episodes and outputs an estimate \u2206\u0302 that satisfies 12\u2206 \u2264 \u2206\u0302 \u2264 32\u2206.\nAlgorithm 2 Algorithm for Gap Estimation 1: Inputs: T0 (exploration threshold) and \u03b4 (confidence parameter)\n2: For b = 1, 2, . . . , 3: Exploration 4: For i = 1 to |\u03c3| 5: (Denote) ti = t and \u03b8(ti, b) = \u03b8(t) (t is current time point). 6: Play xi \u2208 \u03c3 and get feedback Mxi \u00b7 \u03b8(ti, b) \u2208 Rmxi . 7: End For 8: Estimation 9: \u03b8\u0303b = M+\u03c3 (Mx1 \u00b7 \u03b8(t1, b), . . . ,Mx|\u03c3| \u00b7 \u03b8(t|\u03c3|, b)) \u2208 Rn.\n10: \u03b8\u0302(b) = \u2211b i=1 \u03b8\u0303i b \u2208 Rn.\n11: Stopping Rule (w(b) is defined as in Eq. (6))"}, {"heading": "12: If argmaxx\u2208X r\u0304(x, \u03b8\u0302(b)) is unique:", "text": ""}, {"heading": "13: x\u0302(b) = argmaxx\u2208X r\u0304(x, \u03b8\u0302(b))", "text": ""}, {"heading": "14: x\u0302\u2212(b) = argmaxx\u2208X ,x 6=x\u0302(b) r\u0304(x, \u03b8\u0302(b)) (need not be unique)", "text": "15: If r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u0302(b)) > 6w(b): 16: STOP and output \u2206\u0302 = r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u0302(b)) 17: End If 18: End If 19: If b > T0: 20: STOP and output \u201cthreshold exceeded\u201d 21: End If 22: End For\n2. Suppose Assumption 4 holds and T0 \u2264 T1(\u03b4). Then with probability at least 1 \u2212 \u03b4, the algorithm either outputs \u201cthreshold exceeded\u201d or outputs an estimate \u2206\u0302 that satisfies 1 2\u2206 \u2264 \u2206\u0302 \u2264 32\u2206. Furthermore, if it outputs \u2206\u0302, it must be the case that the algorithm stopped at an episode b such that T2(\u03b4) < b < T0.\n3. Suppose Assumption 4 fails. Then, with probability at least 1 \u2212 \u03b4, Algorithm 2 stops in T0 episodes and outputs \u201cthreshold exceeded\u201d.\nEquipped with Theorem 3, we are now ready to combine Algorithm 2 with Algorithm 1 to give Algorithm 3. Algorithm 3 first calls Algorithm 2. If Algorithm 2 outputs an estimate \u2206\u0302 it is fed into Algorithm 1. If the threshold T0 is exceeded, then the remaining time is spent in pure exploitation. Note that by choosing T0 to be of order T 2/3 we can guarantee a worst case regret of the same order even when unique optimality assumption fails. For PM games that are globally observable but not locally observable, such a distribution independent O(T 2/3) bound is known to be optimal [4].\nTheorem 4. (Regret Bound for PEGE2) Consider Algorithm 3 run with knowledge of the number T of rounds. Consider the distribution independent bound\nB1(T ) = 2(2R\u03b2\u03c3|\u03c3|2R2maxT )2/3 \u221a log(4e2T 3) +Rmax,\nand the distribution dependent bound\nB2(T ) = 256R2\u03b22\u03c3\n\u22062 log\n512e2R2\u03b22\u03c3T\n\u22062 Rmax|\u03c3|+\n\u2211\nx\u2208\u03c3\n\u2206x 36R2\u03b22\u03c3 logT\n\u22062 + 8e2R2\u03b22\u03c3 \u22062 +Rmax.\nIf Assumption 4 fails, then the expected regret of Algorithm 3 is bounded as R(T ) \u2264 B1(T ). If Assumption 4 holds, then the expected regret of Algorithm 3 is bounded as\nR(T ) \u2264 { B2(T ) if T1(\u03b4) < T0 O(T 2/3 log T ) if T0 \u2264 T1(\u03b4) , (7)\nwhere T1(\u03b4) is as defined in Theorem 3 and \u03b4, T0 are as defined in Algorithm 3.\nIn the above theorem, note that T1(\u03b4) scales as \u0398( 1\u22062 log T \u22062 ) and T0 as \u0398(T 2/3). Thus, the two cases in Eq. (7) correspond to large gap and small gap situations respectively.\nAlgorithm 3 Algorithm Combining PEGE with Gap Estimation (PEGE2) 1: Input: T (total number of rounds)\n2: Call Algorithm 2 with inputs T0 = (\n2R\u03b2\u03c3T |\u03c3|Rmax\n)2/3\nand \u03b4 = 1/T\n3: If Algorithm 2 returns \u201cthreshold exceeded\u201d: 4: Let \u03b8\u0302(T0) be the latest estimate of \u03b8\u2217p maintained by Algorithm 2 5: Play x\u0302(T0) = argmaxx\u2208X r\u0304(x, \u03b8\u0302) for the remaining T \u2212 T0|\u03c3| rounds 6: Else: 7: Let \u2206\u0302 be the gap estimate produced by Algorithm 2 8: For all remaining time steps, run Algorithm 1 with parameters C(a) = ha with\nh = \u2206\u0302 2\n9R2\u03b22\u03c3 , \u03b1 = 1, \u03b2 = 0\n9: End If"}, {"heading": "5 Comparison with GCB Algorithm", "text": "We provide a detailed comparison of our results with those obtained for GCB [1]. (a) While we use the same CPM model, our work is inspired by the forced exploration technique while GCB is inspired by the confidence bound technique, both of which are classic in the bandit literature. (b) One instantiation of our PEGE framework gives an O(T 2/3 \u221a logT ) distribution independent regret bound (Theorem 1), which does not require call to arg-secondmax oracle. This is of substantial practical advantage over GCB since even for linear optimization problems over polyhedra, standard routines usually do not have option of computing action(s) that achieve second maximum value for the objective function. (c) Another instantiation of the PEGE framework gives an O(log2 T ) distribution dependent regret bound (Theorem 2), which neither requires call to arg-secondmax oracle nor the assumption of existence of unique optimal action for learner. This is once again important, since the assumption of existence of unique optimal action might be impractical, especially for exponentially large action space. However, the caveat is that improper setting of the tuning parameter h in Theorem 2 can lead to a large additive component in the regret. (d) A crucial point, which we had highlighted in the beginning, is that the regret bounds achieved by PEGE and PEGE2 do not have dependence on size of learner\u2019s action space, i.e., |X |. The dependence is only on the size of global observable set \u03c3, which is guaranteed to be not more than dimension of adversary\u2019s action space. Thus, though we have adopted the CPM model [1], our algorithms achieve meaningful regret bounds for countably infinite or even continuous learner\u2019s action space. In contrast, the GCB regret bounds have explicit, logarithmic dependence on size of learner\u2019s action space. Thus, their results cannot be extended to problems with infinite learner\u2019s action space (see Section 6 for an example), and are restricted to large, but finite action spaces. (e) The PEGE2 algorithm is a true analogue of the GCB algorithm, matching the regret bounds of GCB in terms of T and gap \u2206 with the advantage that it has no dependence on |X |. The disadvantage, however, is that PEGE2 requires knowledge of time horizon T , while GCB is an anytime algorithm."}, {"heading": "6 Application to Online Ranking", "text": "A recent paper studied an interesting problem of online ranking with feedback restricted to top ranked items [12]. The problem was studied in a non-stochastic setting, i.e., it was assumed that an oblivious adversary generates reward vectors. Moreover, the learner\u2019s action space was exponentially large in number of items to be ranked. The paper made the connection of the problem setting to PM games (but not combinatorial PM games) and proposed an efficient algorithm for the specific problem at hand. However, a careful reading of the paper shows that their algorithmic techniques can handle the CPM model we have discussed so far, but in the non-stochastic setting. The reward function is linear in both learner\u2019s and adversary\u2019s moves, adversary\u2019s move is restricted to a finite space of vectors and feedback is a linear transformation of adversary\u2019s move. In this section, we give\na brief description of the problem setting and show how our algorithms can be used to efficiently solve the problem of online ranking with feedback on top ranked items in the stochastic setting. We also give an example of how the ranking problem setting can be somewhat naturally extended to one which has continuous action space for learner, instead of large but finite action space.\nThe paper considered an online ranking problem, where a learner repeatedly re-ranks a set of n, fixed items, to satisfy diverse users\u2019 preferences, who visit the system sequentially. Each learner action x is a permutation of the n items. Each user has like/dislike preference for each item, varying between users, with each user\u2019s preferences encoded as n length binary relevance vectors \u03b8. Once the ranked list of items is presented to the user, the user scans through the items, but gives relevance feedback only on top ranked item. However, the performance of the learner is judged based on full ranked list and unrevealed, full relevance vector. Thus, we have a PM game, where neither adversary generated relevance vector nor reward is revealed to learner. The paper showed how a number of practical ranking measures, like Discounted Cumulative Gain (DCG), can be expressed as a linear function, i.e., r(x, \u03b8) = f(x) \u00b7 \u03b8. The practical motivation of the work was based on learning a ranking strategy to satisfy diverse user preferences, but with limited feedback received due to user burden constraints and privacy concerns.\nOnline Ranking with Feedback at Top as a Stochastic CPM Game. We show how our algorithms can be applied in online ranking with feedback for top ranked items by showing how it is a specific instance of the CPM model and how our key assumptions are satisfied. The learner\u2019s action space is the finite but exponentially large space of X = n! permutations. Adversary\u2019s move is an n dimensional relevance vector, and thus, is restricted to {0, 1}n (finite space of size 2n) contained in [0, 1]n. In the stochastic setting, we can assume that adversary samples \u03b8 \u2208 {0, 1}n from a fixed distribution on the space. Since the feedback on playing a permutation is the relevance of top ranked item, each move x has an associated transformation matrix Mx \u2208 {0, 1}n, with 1 in the place of the item which is ranked at the top by x and 0 everywhere else. Thus, Mx \u00b7 \u03b8 gives the relevance of item ranked at the top by x. The global observable set \u03c3 is the set of any n actions, where each action, in turn, puts a distinct item on top. Hence, M\u03c3 is the n\u00d7 n dimensional permutation matrix. Assumption 1 is satisfied because the reward function is linear in \u03b8 and r\u0304(x, \u03b8\u2217p) = f(x) \u00b7 \u03b8\u2217p , where Ep[\u03b8] = \u03b8 \u2217 p \u2208 [0, 1]n. Assumption 2 is satisfied since there will always be a global observable set of size n and can be found easily. In fact, there will be multiple global observable sets, with the freedom to choose any one of them. Assumption 3 is satisfied due to the expected reward function being linear in second argument. The Lipschitz constant is maxx\u2208X \u2016f(x)\u20162, which is always less than some small polynomial factor of n, depending on specific f(\u00b7). The value of \u03b2\u03c3 can be easily seen to be n3/2. The argmax oracle returns the permutation which simply sorts items according to their corresponding \u03b8 values. The arg-secondmax oracle is more complicated, though feasible. It requires first sorting the items according to \u03b8 and then compare each pair of consecutive items to see where least drop in reward value occurs.\nLikely Failure of Unique Optimal Action Assumption. Assumption 4 is unlikely to hold in this problem setting. The mean relevance vector \u03b8\u2217p effectively reflects the average preference of all users for each of the n items. It is very likely that at least a few items will not be liked by anyone and which will ultimately be always ranked at the bottom. Equally possible is that two items will have same user preference on average, and can be exchanged without hurting the optimal ranking. Thus, existence of an unique optimal ranking, which indicates that each item will have different average user preference than every other item, is unlikely. Thus, PEGE algorithm can still be applied to get poly-logarithmic regret (Theorem 2), but GCB will only achieve O(T 2/3 logT ) regret.\nA PM Game with Infinite Learner Action Space. We give a simple modification of the ranking problem above to show how the learner can have continuous action space. The learner now ranks the items by producing an n dimensional score vector x \u2208 [0, 1]n and sorting items according to their scores. Thus the learner\u2019s action space is now an uncountably infinite continuous space. As before, the user gets to see the ranked list and gives relevance feedback on top ranked item. The learner\u2019s performance will now be judged by a continuous loss function, instead of a discrete-valued ranking measure, since its moves are in a continuous space. Consider the simplest loss, viz., the squared \u201closs\u201d r(x, \u03b8) = \u2212\u2016x \u2212 \u03b8\u201622 (note -ve sign to keep reward interpetation). It can be easily seen that r\u0304(x, \u03b8\u2217p) = E\u03b8\u223cp[r(x, \u03b8)] = \u2212\u2016x\u201622+2x \u00b7\u03b8\u2217p \u22121 \u00b7\u03b8\u2217p , if the relevance vectors \u03b8 are in {0, 1}n. Thus, the Lipschitz condition is satisfied. The global observable set is still of size n, with the n actions being any n score vectors, whose sorted orders place each of the n items, in turn, on top. \u03b2\u03c3 remains same as before, with argmaxx E\u03b8\u223cpr(x, \u03b8) = E\u03b8\u223cp[\u03b8] = \u03b8 \u2217 p."}, {"heading": "7 Appendix", "text": "We first state the large deviation inequality for vector-valued martingales, which is the generalization of Azuma-Hoeffding inequality for scalar valued martingales.\nTheorem 1.8 of [13]: Let X0, X1, . . . , Xm be a weak martingale sequence taking values in euclidean space Rd, with E[Xi|Xi\u22121] = Xi\u22121. Let X0 = 0 and \u2016Xi \u2212 Xi\u22121\u20162 \u2264 1, for i = 1, 2, . . . ,m. Then, for every \u01eb > 0,\nPr[\u2016Xm\u20162 \u2265 \u01eb] < 2e2e \u2212\u01eb2 m (8)\nWe use the concentration inequality to get a uniform confidence bound, over the space of learner\u2019s action, on the deviation of estimated reward from true reward, after each estimate of mean reward vector is produced.\nLemma 5. At the end of exploration phase within phase b, b = 1, 2, . . ., of Algorithm PEGE, the\nestimator of reward vector \u03b8\u2217p is \u03b8\u0302(b) =\n\u2211b i=1 \u2211i\u03b2\nj=1 \u03b8\u0303j,i \u2211b\nj=1 j \u03b2\n. Then, \u2200 \u03b7 > 0,\nPr[\u2200 x \u2208 X : |r\u0304(x, \u03b8\u0302(b))\u2212 r\u0304(x, \u03b8\u2217p)| \u2264 \u03b7] \u2265 1\u2212 2e2e \u2212(\n\u2211b\u03b2\ni=1 i \u03b2)\u03b72 R2\u03b22\u03c3 (9)\nwhere \u03b2\u03c3 is the constant as defined in Eq. 3 and R is the Lipschitz constant defined in Assumption 3.\nProof. Let {Xi,j}j=1,...,i\u03b2 i=1,...,b be a sequence of random vectors, defined as follows:\nXi,j =\n\u2211i\u22121 i\u2032=1 \u2211i\u2032\u03b2 j\u2032=1 \u03b8 \u2217 p + \u2211j k=1 \u03b8 \u2217 p \u2212 ( \u2211i\u22121 i\u2032=1 \u2211i\u2032\u03b2 j\u2032=1 \u03b8\u0303j\u2032,i\u2032 + \u2211j\nk=1 \u03b8\u0303k,i) \u2211b\ni\u2032\u2032=1\n\u2211(i\u2032\u2032)\u03b2 j\u2032\u2032=1 \u03b2\u03c3 (10)\nIt can be checked that the \u21132 norm of the difference between any two consecutive random vectors is bounded by a constant. That is, \u2016Xi,j \u2212 Xi,j\u22121\u20162 = \u2016\u03b8\u2217p \u2212 \u03b8\u0303j,i\u20162\n\u2211b i\u2032\u2032=1 \u2211(i\u2032\u2032)\u03b2 j\u2032\u2032=1 \u03b2\u03c3 \u2264 1\u2211b i\u2032\u2032=1(i \u2032\u2032)\u03b2 and\n\u2016Xi+1,1 \u2212Xi,i\u03b2\u20162 \u2016\u03b8\u2217p \u2212 \u03b8\u03031,i+1\u20162\n\u2211b i\u2032\u2032=1 \u2211(i\u2032\u2032)\u03b2 j\u2032\u2032=1 \u03b2\u03c3 \u2264 1\u2211b i\u2032\u2032=1(i \u2032\u2032)\u03b2 .\nAlso, \u03b8\u0303j,i is independent of all estimators formed before \u03b8\u0303j,i in Algorithm PEGE. Thus,\nE[Xi,j \u2212Xi,j\u22121|Xi,j\u22121] = E [ \u03b8\u2217p \u2212 \u03b8\u0303j,i \u2211b\ni\u2032\u2032=1\n\u2211(i\u2032\u2032)\u03b2 j\u2032\u2032=1 \u03b2\u03c3 |Xi,j\u22121\n]\n= E\n[\n\u03b8\u2217p \u2212 \u03b8\u0303j,i \u2211b\ni\u2032\u2032=1\n\u2211(i\u2032\u2032)\u03b2\nj\u2032\u2032=1 \u03b2\u03c3\n]\n= 0\n(11)\nThus, {Xi,j}j=1,...,i\u03b2 i=1,...,b satisfy the criteria of weak martingale sequence and hence, by the large deviation inequality of vector valued martingales, we have, \u2200 \u01eb > 0, Pr[\u2016Xb,b\u03b2\u20162 \u2265 \u01eb] <\n2e2e\n\n   \n\u2212\u01eb2\n\u2211b i=1\n\u2211i\u03b2\nj=1 1\n( \u2211b i=1 i\u03b2)2\n\n   \n= 2e2e\u2212\u01eb 2 \u2211b i=1 i \u03b2 .\nNow, it can be clearly seen that \u2016\u03b8\u2217p \u2212 \u03b8\u0302(b)\u20162 = \u03b2\u03c3\u2016Xb,b\u03b2\u20162 and let \u03b7 = \u03b2\u03c3\u01eb. Then, \u2200 \u03b7 > 0, we\nget Pr[\u2016\u03b8\u2217p \u2212 \u03b8\u0302(b)\u20162 \u2265 \u03b7] \u2264 2e2e \u2212(\n\u2211b\u03b2\ni=1 i \u03b2 )\u03b72\n\u03b22\u03c3 .\nUsing the Lipschitz property of expected reward function (Assumption 3), we have\nPr(\u2203 x \u2208 X : |r\u0304(x, \u03b8\u0302(b))\u2212 r\u0304(x, \u03b8\u2217p)| \u2265 \u03b7) \u2264 Pr(R \u00b7 \u2016\u03b8\u2217p \u2212 \u03b8\u0302(b)\u20162 \u2265 \u03b7)\n\u2264 2e2e \u2212(\n\u2211b\u03b2\ni=1 i \u03b2)\u03b72 R2\u03b22\u03c3\n(12)\nTaking complement of the event completes the proof."}, {"heading": "7.1 Proof of Results in Section 3", "text": ""}, {"heading": "7.1.1 Proof of Theorem 1", "text": "We first restate the theorem.\nDistribution Independent Regret: When Algorithm PEGE is initialized with the parameters C(a) = log a, \u03b1 = 1/2 and \u03b2 = 0, and the online game is played over T rounds, we get the following bound on expected regret:\nR(T ) \u2264 Rmax|\u03c3|T 2/3 + 2R\u03b2\u03c3T 2/3 \u221a\nlog 2e2 + 2 logT +Rmax (13) where \u03b2\u03c3 is the constant as defined in Eq. 3.\nProof. Let Algorithm PEGE run forK phases, with parameters initialized as C(a) = log a, \u03b1 = 1/2 and \u03b2 = 0.\nExploration regret: During every exploration phase, the expected regret is bounded by |\u03c3|Rmax, where Rmax is as given in Assumption 3. Thus, total expected regret due to exploration is K|\u03c3|Rmax.\nExploitation regret: Let x\u2217 \u2208 argmaxx\u2208X r\u0304(x, \u03b8\u2217p) and x(b) \u2208 argmaxx\u2208X r\u0304(x, \u03b8\u0302(b)). During every exploitation round within phase b of Algorithm PEGE, the expected regret is |r\u0304(x(b), \u03b8\u2217p) \u2212 r\u0304(x\u2217, \u03b8\u2217p)| . Now, from Lemma 5, with \u03b2 = 0, the following holds w.p. \u2265 1\u2212 \u03b4b,\n\u2200 x, |r\u0304(x, \u03b8\u2217p)\u2212 r\u0304(x, \u03b8\u0302(b)| \u2264\n\u221a\nR2\u03b22\u03c3 log( 2e2\n\u03b4b ) b \ufe38 \ufe37\ufe37 \ufe38\n\u03b7b\n(14)\nThen, w.p. \u2265 1\u2212 \u03b4b, the following event holds true: |r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x(b), \u03b8\u2217p)| \u2264 2\u03b7b, as explained: r\u0304(x\u2217, \u03b8\u2217p) \u2264 r\u0304(x\u2217, \u03b8\u0302(b)) + \u03b7b from Eq. 14\n\u2264 r\u0304(x(b), \u03b8\u0302(b)) + \u03b7b \u2264 r\u0304(x(b), \u03b8\u2217p) + 2\u03b7b from Eq. 14\nThus, the event |r\u0304(x\u2217, \u03b8\u2217p) \u2212 r\u0304(x(b), \u03b8\u2217p)| \u2264 2\u03b7b holds true w.p. \u2265 1 \u2212 \u03b4b, for every fixed phase b. Then, w.p. \u2265 1\u2212\u2211Ki=1 \u03b4i, the following holds true: \u2200 b, |r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x(b), \u03b8\u2217p)| \u2264 2\u03b7b Note that the expected regret per round is always bounded by Rmax (since expected reward is bounded by Rmax) .\nThe number of rounds of exploitation in phase b is b\u03b1. Hence, the total expected regret due to exploitation, over K phases is:\nK\u2211\ni=1\n\n     (1\u2212 K\u2211\nj=1\n\u03b4j) 2R\u03b2\u03c3\n\u221a\nlog(2e2/\u03b4i)\u221a i + (\nK\u2211\nj=1\n\u03b4j)Rmax\n\ufe38 \ufe37\ufe37 \ufe38\nexpected regret per exploitation round\n\n     i\u03b1\nTaking \u03b41 = \u03b42 = . . . = \u03b4K = \u03b4, and summing over exploration and exploitation regret over K phases, we get\nR(T ) \u2264 K|\u03c3|Rmax + K\u2211\ni=1\n(\n(1\u2212K\u03b4)2R\u03b2\u03c3 \u221a\nlog(2e2/\u03b4)\u221a i + (K\u03b4)Rmax\n)\ni\u03b1 (15)\nUsing the inequality \u2211K\ni=1 i y \u2264\n\u222bK\n0 i ydy \u2264 Ky+1, we get expected regret:\nR(T ) \u2264 K|\u03c3|Rmax + (1\u2212K\u03b4)2R\u03b2\u03c3 \u221a log(2e2/\u03b4)K\u03b1+1/2 +K\u03b4RmaxK \u03b1+1 (16)\nNow, we relate K to total time T as: T = |\u03c3|K +\u2211Ki=1 i\u03b1 \u223c K\u03b1+1, for large K .\nHence K \u223c T 11+\u03b1 . Substituting value of K in Eq 16, and taking \u03b1 = 1/2 and \u03b4 = 1KT gives us the required bound on expected regret.\nOur next lemma shows that as the number of phases b grows in Algorithm PEGE, the probability of selecting a sub-optimal arm for greedy exploitation shrinks.\nLemma 6. At the end of exploration phase within phase b, b = 1, 2, . . ., the estimator constructed\nis \u03b8\u0302(b) =\n\u2211b i=1 \u2211i\u03b2\nj=1 \u03b8\u0303j,i \u2211b\nj=1 j \u03b2\n. Then the following holds,\nPr(argmax x\u2208X r\u0304(x, \u03b8\u0302(b)) 6\u2286 argmax x\u2208X\nr\u0304(x, \u03b8\u2217p)) \u2264 2e2e \u2212(\n\u2211b\u03b2\ni=1 i \u03b2)\u22062 4R2\u03b22\u03c3 (17)\nProof. Let us assume x\u2032 \u2208 argmaxx\u2208X r\u0304(x, \u03b8\u0302(b)) such that x\u2032 /\u2208 argmaxx\u2208X r\u0304(x, \u03b8\u2217p). Let x\u2217 \u2208 argmaxx\u2208X r\u0304(x, \u03b8 \u2217 p). Then, by our assumption, r\u0304(x\n\u2032, \u03b8\u0302(b)) \u2265 r\u0304(x\u2217, \u03b8\u0302(b)). By definition of gap \u2206, we also have r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u2032, \u03b8\u2217p) \u2265 \u2206. The two inequalities imply that at least one of the following two inequalities has to hold: either |r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u2217, \u03b8\u0302(b))| \u2265 \u22062 or |r\u0304(x\u2032, \u03b8\u0302(b))\u2212 r\u0304(x\u2032, \u03b8\u2217p)| \u2265 \u22062 .\nThus, argmaxx\u2208X r\u0304(x, \u03b8\u0302(b)) 6\u2286 argmaxx\u2208X r\u0304(x, \u03b8\u2217p) =\u21d2 \u2203 x \u2208 X : |r\u0304(x, \u03b8\u2217p)\u2212 r\u0304(x, \u03b8\u0302(b))| \u2265 \u22062 . By using Lemma 5, and substituting \u03b7 = \u22062 , we get our result."}, {"heading": "7.1.2 Proof of Theorem 2", "text": "We restate the theorem before proving:\nDistribution Dependent Regret: When Algorithm PEGE is initialized with the parameters C(a) = h \u00b7 a, for a tuning parameter h > 0, \u03b1 = 1 and \u03b2 = 1, and the online game is played over T rounds, we get the following bound on expected regret:\nR(T ) \u2264 \u2211\nx\u2208\u03c3\n\u2206x\n( logT\nh\n)2 + 4 \u221a 2\u03c0e2R\u2206max\u03b2\u03c3\n\u2206 e\nh2(2R2\u03b22\u03c3)\n\u22062 . (18)\nProof. Let total number of phases that the algorithm runs for be K . We relate K to total time T as (after substituting parameters C(a) = h \u00b7 a, \u03b1 = 1 and \u03b2 = 1 in Algorithm PEGE):\nT = \u2211K i=1 |\u03c3|i + \u2211K i=1 e hi \u2265 ehK =\u21d2 K \u2264 logT\nh .\nExploration regret: Sine we are in distribution dependent setting now, expected exploration regret in each exploration phase is \u2211\nx\u2208\u03c3 \u2206x. Hence, total expected exploration regret is upper bounded by:\n\u2211K i=1( \u2211 x\u2208\u03c3 \u2206x)i = \u2211 x\u2208\u03c3 \u2206x K(K+1) 2 \u2264 ( \u2211 x\u2208\u03c3 \u2206x) log2 T\nh2 .\nExploitation regret: When a sub-optimal arm is picked in an exploitation round, the expected regret in that round is: \u2264 \u2206max. Using Lemma 6 with \u03b2 = 1, the total expected regret due to exploitation over K phases is upper bounded by:\nK\u2211\ni=1\n2e2\u2206max e hi\u2212 i(i+1)2\n\u22062\n4R2\u03b22\u03c3\n\ufe38 \ufe37\ufe37 \ufe38\nexpected exploitation regret upper bound in phase i\n\u2264 2e2\u2206max \u221e\u2211\ni=1\ne hi\u2212 i(i+1)2\n\u22062\n4R2\u03b22\u03c3\n\u2264 2e2\u2206max \u222b \u221e\n\u2212\u221e\ne hy\u2212 y(y+1)2\n\u22062\n4R2\u03b22\u03c3 dy\n(19)\nThe integral is the moment generating function (adjusting for normalization constant) of a gaussian\nrandom variable Y \u2208 N (0, 4R 2\u03b22\u03c3\n\u22062 ). Thus, the integral is E[e hY ] = e\n2h2R2\u03b22\u03c3 \u22062 and total expected\nregret due to exploitation is upper bounded by: 4e2\u2206max\n\u221a 2\u03c0R\u03b2\u03c3\n\u2206 e\n2h2R2\u03b22\u03c3 \u22062 .\nSumming over exploration and exploitation regrets completes the proof."}, {"heading": "7.2 Proof of Results in Section 4", "text": "The following theorem is about the version of PEGE that Algorithm 3 calls on line 8. It will be needed in the proof of Theorem 4.\nTheorem 7. (Distribution Dependent Regret, version 2) When Algorithm 1 is initialized with the parameters C(a) = h \u00b7 a, for a tuning parameter 0 < h < \u220624R2\u03b22\u03c3 , \u03b1 = 1 and \u03b2 = 0, and the online game is played over T rounds, we get the following bound on expected regret:\nR(T ) \u2264 \u2211\nx\u2208\u03c3\n\u2206x logT\nh +\n2e2\u2206max \u22062 4R2\u03b22\u03c3 \u2212 h\n(20)\nNote: Compared to Theorem 2, the regret bound has better dependence on T \u2014 O(log T ) instead of O(log2 T ) \u2014 but it also has a disadvantage. If the tuning parameter h is incorrectly set, say h \u2265 \u220624R2\u03b22\u03c3 , then the bound does not even apply.\nProof. Key Steps:\nLet total number of phases that the algorithm runs for be K . First: T = \u2211K i=1 |\u03c3|+ \u2211K i=1 e hi \u2265 ehK =\u21d2 K \u2264 logT h .\nExpected regret due to exploration: \u2211K i=1( \u2211 x\u2208\u03c3 \u2206x) = \u2211 x\u2208\u03c3 \u2206xK \u2264 ( \u2211 x\u2208\u03c3 \u2206x) log T\nh .\nExpected regret due to exploitation: When a sub-optimal arm is picked, expected regret \u2264 \u2206max. Using Lemma 6 with \u03b2 = 0, and tuning parameter h < \u2206 2\n4R2\u03b22\u03c3 , we get total expected regret due to\nexploitation\n2e2\u2206max\nK\u2211\ni=1\ne hi\u2212i \u2206\n2 4R2\u03b22\u03c3 \u2264 2e2\u2206max \u221e\u2211\ni=1\ne hi\u2212i \u2206\n2\n4R2\u03b22\u03c3\n= 2e2\u2206max\n\u221e\u2211\ni=1\ne \u2212i( \u2206\n2\n4R2\u03b22\u03c3 \u2212h)\n\u2264 2e2\u2206max \u222b \u221e\n0\ne \u2212y( \u2206\n2\n4R2\u03b22\u03c3 \u2212h)\ndy\n= 2e2\u2206max \u22062\n4R2\u03b22\u03c3 \u2212 h\n(21)"}, {"heading": "7.2.1 Proof of Theorem 3", "text": "Proof. Note that Assumption 1 through Assumption 3 hold. Therefore, from Lemma 5, with \u03b2 = 0 we get, with probability at least 1\u2212 \u03b4b,\n\u2200x, |r\u0304(x, \u03b8\u2217p)\u2212 r\u0304(x, \u03b8\u0302(b)| \u2264\n\u221a\nR2\u03b22\u03c3 log( 2e2\n\u03b4b )\nb\nLet \u03b4b = \u03b4/2b2 which implies \u2211 b\u22651 \u03b4b = \u03c0 2\u03b4/12 < \u03b4. Thus, setting w(b) =\n\u221a\nR2\u03b22\u03c3 log( 4e2b2 \u03b4 )\nb ,\nthe event E defined as\n\u2200b \u2265 1, \u2200x \u2208 X , |r\u0304(x, \u03b8\u0302(b))\u2212 r\u0304(x, \u03b8\u2217p)| \u2264 w(b). (22) holds with probability at least 1\u2212 \u03b4.\n1. Note that b \u2265 T1(\u03b4) implies 8w(b) < \u2206. This is because the latter has the form eLb > Mb with M = 2e/\u03b4 and L = \u22062/(128R2\u03b22\u03c3). Setting b \u2265 2/L log(2M/L) guarantees that eLb/2 \u2265 2M/L which implies that eLb \u2265 Mb since eLb/2 \u2265 1 + Lb/2 \u2265 Lb/2. If 8w(b) < \u2206 then clearly 2w(b) < \u2206. Let x 6= x\u2217 be arbitrary. We have the following chain of implications:\n2w(b) < \u2206\n\u21d2 2w(b) < r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u2217\u2212, \u03b8\u2217p) (def. of \u2206) \u21d2 2w(b) < r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x, \u03b8\u2217p) (Assumption 4) \u21d2 0 < r\u0304(x\u2217, \u03b8\u0302(b))\u2212 r\u0304(x, \u03b8\u0302(b)). (\u2235 E holds)\nThis means that the If condition on line 12 will evaluate to true and x\u0302(b) on line 13 will be set to x\u2217.\nWe also have the following chain of implications:\n8w(b) < \u2206\n\u21d2 8w(b) < r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u2217\u2212, \u03b8\u2217p) (def. of \u2206) \u21d2 8w(b) < r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u2217p) (\u2235 r\u0304(x\u0302\u2212(b), \u03b8\u2217p) \u2264 r\u0304(x\u2217\u2212, \u03b8\u2217p)) \u21d2 8w(b) < r\u0304(x\u0302(b), \u03b8\u2217p)\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u2217p) (\u2235 x\u0302(b) = x\u2217) \u21d2 6w(b) < r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u0302(b)). (\u2235 E holds)\nThis means that the If condition on line 15 will evaluate to true and the algorithm will stop and output an estimate \u2206\u0302.\nNow suppose the algorithm stops and does not output \u201cthreshold exceeded\u201d which means that the If conditions on line 12 and line 15 were both true at some episode b. Let x 6= x\u0302(b) be arbitrary. We have the following chain of implications:\n6w(b) < r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u0302(b)) (line 15) \u21d2 6w(b) < r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x, \u03b8\u0302(b)) (x\u0302(b) unique maximizer by line 12) \u21d2 4w(b) < r\u0304(x\u0302(b), \u03b8\u2217p)\u2212 r\u0304(x, \u03b8\u2217p). (\u2235 E holds)\nThis means, along with Assumption 4, that x\u0302(b) = x\u2217. We also have,\n6w(b) < r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u0302(b)) (line 15) \u21d2 6w(b) < r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x\u2217\u2212, \u03b8\u0302(b)) (\u2235 r\u0304(x\u0302\u2212(b), \u03b8\u0302(b)) \u2265 r\u0304(x\u2217\u2212, \u03b8\u0302(b))) \u21d2 4w(b) < r\u0304(x\u0302(b), \u03b8\u2217p)\u2212 r\u0304(x\u2217\u2212, \u03b8\u2217p) (\u2235 E holds) \u21d2 4w(b) < r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u2217\u2212, \u03b8\u2217p) (\u2235 x\u0302(b) = x\u2217) \u21d2 4w(b) < \u2206. (def. of \u2206)\nNow we prove that the output \u2206\u0302 lies in the right range. We have\n\u2206\u0302 = r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u0302(b)) (line 16) \u2265 r\u0304(x\u0302(b), \u03b8\u2217p)\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u2217p)\u2212 2w(b) (\u2235 E holds) = r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u2217p)\u2212 2w(b) (\u2235 x\u0302(b) = x\u2217) \u2265 r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u2217\u2212, \u03b8\u2217p)\u2212 2w(b) (\u2235 r\u0304(x\u0302\u2212(b), \u03b8\u2217p) \u2264 r\u0304(x\u2217\u2212, \u03b8\u2217p)) \u2265 \u2206\u2212 2w(b) (def. of \u2206)\n\u2265 \u2206 2 . (\u2235 w(b) < \u2206/4)\nSimilarly,\n\u2206\u0302 = r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x\u0302\u2212(b), \u03b8\u0302(b)) (line 16) \u2264 r\u0304(x\u0302(b), \u03b8\u0302(b))\u2212 r\u0304(x\u2217\u2212, \u03b8\u0302(b)) (\u2235 r\u0304(x\u0302\u2212(b), \u03b8\u0302(b)) \u2265 r\u0304(x\u2217\u2212, \u03b8\u0302(b))) = r\u0304(x\u2217, \u03b8\u0302(b))\u2212 r\u0304(x\u2217\u2212, \u03b8\u0302(b)) (\u2235 x\u0302(b) = x\u2217) \u2264 r\u0304(x\u2217, \u03b8\u2217p)\u2212 r\u0304(x\u2217\u2212, \u03b8\u2217p) + 2w(b) (\u2235 E holds) \u2264 \u2206+ 2w(b) (def. of \u2206)\n\u2264 3\u2206 2 . (\u2235 w(b) < \u2206/4)\n2. In this case T0 \u2264 T1(\u03b4) but it could still be that the algorithm stops not because the threshold is exceeded but because line 12 and line 15 were true at some episode b. Clearly b < T0, otherwise we would have output \u201cthreshold exceeded\u201d and not produced an estimate \u2206\u0302. Under the event E, the previous part shows that if stopping occurs with an estimate \u2206\u0302, it must be that 4w(b) < \u2206, i.e.\n4\n\u221a\nR2\u03b22\u03c3 log( 4e2b2 \u03b4 )\nb < \u2206 \u21d2 b > 16R 2\u03b22\u03c3 \u22062 log 4e2 \u03b4 = T2(\u03b4).\nThis means T0 > b > T2(\u03b4).\n3. Finally, suppose Assumptions 1 through 3 hold but Assumption 4 fails. Event E still holds with probability at least 1\u2212\u03b4. However, if there are at least two optimal actions then, under E, their confidence intervals will always overlap and If condition on line 15 will never be true. That means that the algorithm can only stop when the threshold T0 is exceeded."}, {"heading": "7.2.2 Proof of Theorem 4", "text": "Proof. We break the proof into the two cases mentioned in the theorem statement.\nPart 1: Assumption 4 fails. From Theorem 3 we know, that with probability at least 1 \u2212 \u03b4, Algorithm 2 outputs \u201cthreshold exceeded\u201d in this case. Because of Eq. (22), we also have, for an optimal action x\u2217: |r\u0304(x\u0302(T0), \u03b8\u2217p)\u2212 r\u0304(x\u2217, \u03b8\u2217p)| \u2264 2w(T0) which implies a total regret of\n2w(T0)(T \u2212 T0|\u03c3|) \u2264 2w(T0)T in the remainingT\u2212T0|\u03c3| rounds since we execute line 5. The regret when Algorithm 2 was running is bounded by RmaxT0|\u03c3|. On the bad event, which occurs with probability at most 1\u2212 \u03b4, the regret is at most TRmax giving us a total expected regret of\n2w(T0)T + T0|\u03c3|Rmax + \u03b4TRmax = 2\n\u221a\nR2\u03b22\u03c3 log( 4e2T 20 \u03b4 )\nT0 T + T0|\u03c3|Rmax + \u03b4TRmax\nwhich is upper bounded by\n2(2R\u03b2\u03c3|\u03c3|2R2maxT )2/3 \u221a log(4e2T 3) +Rmax\nfor T0 = (\n2R\u03b2\u03c3T |\u03c3|Rmax\n)2/3\nand \u03b4 = 1/T .\nPart 2: Assumption 4 holds. Case A: T1(\u03b4) < T0. In this case, according to Theorem 3, with probability at least 1 \u2212 \u03b4, Algorithm 2 finishes in T1(\u03b4) episodes and outputs 0.5\u2206 \u2264 \u2206 \u2264 1.5\u2206. This means \u2206/36R2\u03b22\u03c3 \u2264 h \u2264 \u22062/4R2\u03b22\u03c3 . Therefore, by Theorem 7, we have, regret due to Algorithm 1 is at most:\n\u2211\nx\u2208\u03c3\n\u2206x 36R2\u03b22\u03c3 logT\n\u22062 + 8e2R2\u03b22\u03c3 \u22062 .\nOverall, the expected regret is bounded by\nT1(\u03b4)Rmax|\u03c3|+ \u2211\nx\u2208\u03c3\n\u2206x 36R2\u03b22\u03c3 logT\n\u22062 + 8e2R2\u03b22\u03c3 \u22062 +RmaxT\u03b4.\nFor \u03b4 = 1/T , this becomes\n256R2\u03b22\u03c3 \u22062 log 512e2R2\u03b22\u03c3T \u22062 Rmax|\u03c3|+ \u2211\nx\u2208\u03c3\n\u2206x 36R2\u03b22\u03c3 logT\n\u22062 + 8e2R2\u03b22\u03c3 \u22062 + Rmax.\nCase B: T2(\u03b4) \u2264 T0 \u2264 T1(\u03b4). In this regime, Algorithm 2 can stop and output \u201cthreshold exceeded\u201d, in which case, expected regret is bounded, as in Part 1, by\n2(2R\u03b2\u03c3|\u03c3|2R2maxT )2/3 \u221a log(4e2T 3) +Rmax.\nHowever, it can also happen that Algorithm 2 stops and outputs 0.5\u2206 \u2264 \u2206 \u2264 1.5\u2206 with probability at least 1\u2212 \u03b4. In that case, total expected regret is bounded, as in Part 2, Case A, by\n256R2\u03b22\u03c3 \u22062 log 512e2R2\u03b22\u03c3T \u22062 Rmax|\u03c3|+ \u2211\nx\u2208\u03c3\n\u2206x 36R2\u03b22\u03c3 logT\n\u22062 + 8e2R2\u03b22\u03c3 \u22062 + Rmax.\nNote that the above bound scales as O(T1(\u03b4)) for \u03b4 = 1/T , which is upper bounded by O(T2(\u03b4) logT2(\u03b4)). But we know that T2(\u03b4) \u2264 T0 which means the bound is no larger than O(T0 logT0) = O(T 2/3 logT ). So no matter what happens, regret is upper bounded by O(T 2/3 logT ) in this case.\nCase C: T0 \u2264 T2(\u03b4). With probability at least 1 \u2212 \u03b4, by Theorem 3, in this case, Algorithm cannot stop and output \u2206\u0302. Instead, it outputs \u201cthreshold exceeded\u201d. When this happens, Algorithm 1 never gets called and only exploitation rounds follow (line 5). Regret is bounded, just as in Part 1, by\n2(2R\u03b2\u03c3|\u03c3|2R2maxT )2/3 \u221a log(4e2T 3) +Rmax."}], "references": [{"title": "Combinatorial partial monitoring game with linear feedback and its applications", "author": ["Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui", "Wei Chen"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["Antonio Piccolboni", "Christian Schindelhauer"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Regret minimization under partial monitoring", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi", "Gilles Stoltz"], "venue": "Mathematics of Operations Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Partial monitoring\u2013classification, regret bounds, and algorithms", "author": ["Gabor Bartok"], "venue": "Mathematics of Operations Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Regret lower bound and optimal algorithm in finite stochastic partial monitoring", "author": ["Junpei Komiyama", "Junya Honda", "Hiroshi Nakagawa"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Combinatorial multi-armed bandit: General framework and applications", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesvari"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "In Herbert Robbins Selected Papers,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1985}, {"title": "Certainty equivalence control with forcing: revisited", "author": ["Rajeev Agrawal", "Demosthenis Teneketzis"], "venue": "Systems & control letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Online ranking with top-1 feedback", "author": ["Sougata Chaudhuri", "Ambuj Tewari"], "venue": "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recently, a general model of combinatorial partial monitoring (CPM) games was proposed [1], where the learner\u2019s action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "Starting with early work in the 2000s [2, 3], this body of research reached a culmination point with a comprehensive and complete classification of finite PM games [4].", "startOffset": 38, "endOffset": 44}, {"referenceID": 2, "context": "Starting with early work in the 2000s [2, 3], this body of research reached a culmination point with a comprehensive and complete classification of finite PM games [4].", "startOffset": 38, "endOffset": 44}, {"referenceID": 3, "context": "Starting with early work in the 2000s [2, 3], this body of research reached a culmination point with a comprehensive and complete classification of finite PM games [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "We refer the reader to these works for more references and also note that newer results continue to appear [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "The classification is governed by global and local observability properties pertaining to a game [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "Another line of work has extended traditional multi-armed bandit problem (MAB) [6] to include combinatorial action spaces for learner (CMAB) [7, 8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "Another line of work has extended traditional multi-armed bandit problem (MAB) [6] to include combinatorial action spaces for learner (CMAB) [7, 8].", "startOffset": 141, "endOffset": 147}, {"referenceID": 7, "context": "Another line of work has extended traditional multi-armed bandit problem (MAB) [6] to include combinatorial action spaces for learner (CMAB) [7, 8].", "startOffset": 141, "endOffset": 147}, {"referenceID": 0, "context": "While finite PM and CMAB problems have witnessed a lot of activity, there is only one paper [1] on combinatorial partial monitoring (CPM) games, to the best of our knowledge.", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "Inspired by the classic confidence bound algorithms for MABs, such as UCB [6], the authors proposed a Global Confidence Bound (GCB) algorithm that enjoyed two types of regret bound.", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "We adopt the CPM model proposed earlier [1].", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "This technique was already used in the classic paper of Robbins [9] and has also been called \u201cforcing with certainty equivalence\u201d in the control theory literature [10].", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "This technique was already used in the classic paper of Robbins [9] and has also been called \u201cforcing with certainty equivalence\u201d in the control theory literature [10].", "startOffset": 163, "endOffset": 167}, {"referenceID": 10, "context": "We develop a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework (Section 3) borrowing the PEGE terminology from work on linearly parameterized bandits [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 0, "context": "Thus, though we have adopted the CPM model [1], our regret bounds are meaningful for countably infinite or even continuous learner\u2019s action space, whereas GCB regret bound has an explicit logarithmic dependence on |X |.", "startOffset": 43, "endOffset": 46}, {"referenceID": 11, "context": "Finally, we discuss how our algorithms can be efficiently applied in the CPM problem of online ranking with feedback restricted to top ranked items (Section 6), a setting already considered [12] but analyzed in a non-stochastic setting.", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "The adversary\u2019s action set is the infinite set [0, 1].", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "The adversary fixes a distribution p on [0, 1] before start of the game (adversary\u2019s strategy), with p unknown to the learner.", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "At each round of the game, adversary samples \u03b8(t) \u2208 [0, 1] according to p, with E\u03b8(t)\u223cp[\u03b8(t)] = \u03b8 p .", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "That is, the expected reward is a function of x and \u03b8 p , which is always satisfied if r(x, \u03b8) is a linear function of \u03b8, or if distribution p happens to be any distribution with support [0, 1] and fully parameterized by its mean \u03b8 p .", "startOffset": 187, "endOffset": 193}, {"referenceID": 1, "context": "Then, with non-zero probability, the learner can suffer \u03a9(T ) regret and no learner strategy can guarantee a sub-linear in T regret (the intuition forms the base of the global observability condition in [2]).", "startOffset": 203, "endOffset": 206}, {"referenceID": 0, "context": "The continuity assumption, along with the fact that adversary\u2019s moves are in [0, 1], implies boundedness of expected reward for any learner\u2019s action and any adversary\u2019s action.", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "We denote Rmax = maxx\u2208X ,\u03b8\u2208[0,1]n r\u0304(x, \u03b8).", "startOffset": 27, "endOffset": 32}, {"referenceID": 8, "context": "This idea is classic in the bandit literature [9\u201311] but has not been applied to the CPM framework to the best of our knowledge.", "startOffset": 46, "endOffset": 52}, {"referenceID": 9, "context": "This idea is classic in the bandit literature [9\u201311] but has not been applied to the CPM framework to the best of our knowledge.", "startOffset": 46, "endOffset": 52}, {"referenceID": 10, "context": "This idea is classic in the bandit literature [9\u201311] but has not been applied to the CPM framework to the best of our knowledge.", "startOffset": 46, "endOffset": 52}, {"referenceID": 9, "context": ", earlier bounds [10] are asymptotic) whereas here we prove it in the CPM setting.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "For PM games that are globally observable but not locally observable, such a distribution independent O(T ) bound is known to be optimal [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": "We provide a detailed comparison of our results with those obtained for GCB [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Thus, though we have adopted the CPM model [1], our algorithms achieve meaningful regret bounds for countably infinite or even continuous learner\u2019s action space.", "startOffset": 43, "endOffset": 46}, {"referenceID": 11, "context": "A recent paper studied an interesting problem of online ranking with feedback restricted to top ranked items [12].", "startOffset": 109, "endOffset": 113}, {"referenceID": 0, "context": "Adversary\u2019s move is an n dimensional relevance vector, and thus, is restricted to {0, 1}n (finite space of size 2) contained in [0, 1].", "startOffset": 128, "endOffset": 134}, {"referenceID": 0, "context": "Assumption 1 is satisfied because the reward function is linear in \u03b8 and r\u0304(x, \u03b8 p) = f(x) \u00b7 \u03b8 p , where Ep[\u03b8] = \u03b8 \u2217 p \u2208 [0, 1].", "startOffset": 121, "endOffset": 127}, {"referenceID": 0, "context": "The learner now ranks the items by producing an n dimensional score vector x \u2208 [0, 1] and sorting items according to their scores.", "startOffset": 79, "endOffset": 85}, {"referenceID": 0, "context": "References [1] Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Antonio Piccolboni and Christian Schindelhauer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Nicolo Cesa-Bianchi, G\u00e1bor Lugosi, and Gilles Stoltz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Gabor Bartok et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Wei Chen, Yajun Wang, and Yang Yuan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Herbert Robbins.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Rajeev Agrawal and Demosthenis Teneketzis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Paat Rusmevichientong and John N Tsitsiklis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Sougata Chaudhuri and Ambuj Tewari.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary\u2019s move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed [1], where the learner\u2019s action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves O(T 2/3 logT ) distribution independent and O(log T ) distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve O(T 2/3 \u221a logT ) distribution independent and O(log T ) distribution dependent regret respectively. Crucially, our framework needs only the simpler \u201cargmax\u201d oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an O(log T ) regret bound, matching the GCB guarantee but removing the dependence on size of the learner\u2019s action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top.", "creator": "LaTeX with hyperref package"}}}