{"id": "1608.06664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Topic Grids for Homogeneous Data Visualization", "abstract": "We propose the topic grids to detect anomaly and analyze the behavior based on the access log content. Content-based behavioral risk is quantified in the high dimensional space where the topics are generated from the log. The topics are being projected homogeneously into a space that is perception- and interaction-friendly to the human experts.", "histories": [["v1", "Tue, 23 Aug 2016 22:44:42 GMT  (30kb,D)", "http://arxiv.org/abs/1608.06664v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["shih-chieh su", "joseph vaughn", "jean-laurent huynh"], "accepted": false, "id": "1608.06664"}, "pdf": {"name": "1608.06664.pdf", "metadata": {"source": "CRF", "title": "Topic Grids for Homogeneous Data Visualization", "authors": ["Shih-Chieh Su", "Joseph Vaughn", "Jean-Laurent Huynh"], "emails": ["shihchie@qualcomm.com,", "jmvaughn@qualcomm.com,", "jeanlaur@qualcomm.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nTo make the data points in the high dimensional space H visible to human, a word embedding (or dimension reduction) technique is employed to map the data points to a lower dimensional space L. The word embedding technique of choice attempts to preserve some relationship among the data points in H after mapping them to L.\nThe output from the dimension reduction algorithm is a set of data points that are non-uniformly scattered around the visualization space. This helps to explain the clustering behavior, including inter-cluster and intra-cluster, among the data points. However, there are also some drawbacks:\n1) The clusters tangle with others; some data points overlap with others. Overlap makes the information less perceivable. 2) The data points are denser in some area. The heterogeneity makes human interaction with the data points more difficult.\nIn order to better utilize the visualization space, we propose to distribute the data points evenly over the visualization space. The cloud of data points is deformed in the same space defined by the dimension reduction algorithm of choice."}, {"heading": "II. METHOD", "text": "The algorithm we propose is called the split-diffuse (SD) algorithm (Algorithm 1). It attempts to realize the strategy above.\nWe keep track of the splitting path in string c. At the end of the recursion, the placement each single point p is resolved. The indexes of the SD-mapped points, S(p), are all integers, and forms a 2h \u00d7 2h array. This means that the mapped data points are equally spaced in a 2h \u00d7 2h square. To achieve this uniformity in the space L, the data points are essentially diffused from the denser area to the coarser area by the SD algorithm \u2014 hence the name split-diffuse.\nIn Figure 1, we generate 64 topics regarding to the content of some repository access logs. A topic is represented by the first three letters of the most descriptive word. The three letters are then encrypted. There are topics sharing the same most\nAlgorithm 1 Split-diffuse algorithm (square of power of 2) Input: data points {p} of length 2h \u00d7 2h, depth d = 0, allocation string c = '' split-diffuse ({p}, d, c) k \u2190 length of {p} if k = 1, then\nresolve S(p) from c return p\nend if a\u2190 mod(depth, 2) m\u2190 median of {p} in the dimension a return ([split-diffuse ({p : p \u2264 m|dim=a}, d+1, c+'L')],\n[split-diffuse ({p : p > m|dim=a}, d+1, c+'R')])\ndescriptive word, but they are different topics, and can further be distinguished by the less descriptive words.\nIn the high dimensional space H (in our case H has 19K+ dimensions) where the topics are created, the topics sharing the same representative word tend to stay close to each other. The 64 topics are mapped from H to a 2D space L, via MDS [1] and t-SNE [2]. The SD algorithm follows to produce the uniform placement. Having the topics sharing the same representative word stay close to each other after the SD mapping is another desirable feature."}, {"heading": "III. PERFORMANCE", "text": "In order to measure the performance of the SD algorithm, we randomly generate the topics in the 2D space L. We define ErrI to be the ratio of violated topology constraints. For example, when pi is to the right of pj , the placement of S(pi)\nar X\niv :1\n60 8.\n06 66\n4v 1\n[ cs\n.L G\n] 2\n3 A\nug 2\n01 6\nnot being to the right of S(pj) is a violation. There are totally \u2016{p}\u2016\u00b7(\u2016{p}\u2016\u22121) such constraints in a 2D space L. A loosen metric ErrII is defined so that the constraint in dimension a is not violated when S(pi)|a = S(pj)|a.\nThe random topics are generated over a 2D space via the uniform approach U, where topics are sampled within a square; and the Gaussian approach G, where topics are sampled from two-variate Gaussian distribution with the magnitude in one variant doubling the other, and then rotated by \u03c0/4. ErrI decreases as the grid set size increases on both U and G samplings. However, ErrII only decreases as the grid set size increases on the U sampling. ErrII increases as the grid set size increases on the G sampling. Moreover, although ErrI decreases as the grid set size increases on both U and G samplings, the decrease rate varies. On the U sampling, ErrI decreases almost 90% when the layout increase from 4 \u00d7 4 to 64\u00d7 64. However, ErrI only decreases around half of its value during the same layout increase for the G sampling."}, {"heading": "IV. APPLICATION AND CONCLUSION", "text": "We apply the SD algorithm to help analyzing behavioral content in the cyber security domain. The goal of the system is to detect behavioral anomaly based on the access logs. A log entry usually has a timestamp, a unique identifier of a user, an action, and the content on which the action was performed. After proper punctuation, the path, the content, and/or any meta data regarding to this access can be viewed as a document, called the content document. We use the latent Dirichlet allocation (LDA) model [3] to decide the topics\namong all these content documents over a benchmark period of time.\nIn our system, 64 topics are generated at a word vector space of 19K+ dimensions. The relevance between a content document and each individual topic is measured. The anomaly, or risk, of an access is quantified by comparing the topics involved in this access with the topics involved in the historical accesses of the same user, as well as the topics involved in the accesses of the peers of this user.\nThe output of the SD algorithm for our system is a set of topic grids, where the 64 topics are placed over a 8\u00d78 layout. The same set of topic grids is used to render the amount of activities on each topic and the risk on each topic. One use case is presented in Figure 2. The human expert can interact with the grids and get detailed explanation about the topic (e.g., mouse over for topic content and click for access detail). Such type of interaction is hard to achieve with the conventional word embedding techniques. like the ones in Figure 1 (a) and (c).\nIn addition to the cyber security domain, the topic grids can be applied to other domains like e-commerce, credit card transaction, customer service, wherever the content document can be derived from the log files."}], "references": [{"title": "Multidimensional scaling: I. theory and method", "author": ["W.S. Torgerson"], "venue": "Psychometrika, vol. 17, no. 4, pp. 401\u2013419, 1952.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1952}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. 2579-2605, p. 85, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "The 64 topics are mapped from H to a 2D space L, via MDS [1] and t-SNE [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "The 64 topics are mapped from H to a 2D space L, via MDS [1] and t-SNE [2].", "startOffset": 71, "endOffset": 74}], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}