{"id": "1608.07836", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2016", "title": "What to do about non-standard (or non-canonical) language in NLP", "abstract": "Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., socio-demographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language.", "histories": [["v1", "Sun, 28 Aug 2016 17:51:41 GMT  (241kb,D)", "http://arxiv.org/abs/1608.07836v1", "KONVENS 2016"]], "COMMENTS": "KONVENS 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["barbara plank"], "accepted": false, "id": "1608.07836"}, "pdf": {"name": "1608.07836.pdf", "metadata": {"source": "CRF", "title": "What to do about non-standard (or non-canonical) language in NLP", "authors": ["Barbara Plank"], "emails": ["b.plank@rug.nl"], "sections": [{"heading": null, "text": "Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., sociodemographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language.\nIn this paper, I review the notion of canonicity, and how it shapes our community\u2019s approach to language. I argue for leveraging what I call fortuitous data, i.e., nonobvious data that is hitherto neglected, hidden in plain sight, or raw data that needs to be refined. If we embrace the variety of this heterogeneous data by combining it with proper algorithms, we will not only produce more robust models, but will also enable adaptive language technology capable of addressing natural language variation."}, {"heading": "1 Introduction", "text": "The publication of the Penn Treebank Wall Street Journal (WSJ) corpus in the late 80s has undoubtedly pushed NLP from symbolic computation to statistical approaches, which dominate our field up to this day. The WSJ has become the NLP benchmark dataset for many tasks (e.g., part-of-speech tagging, parsing, semantic role labeling, discourse\nparsing), and has developed into the de-facto \u201cstandard\u201d in our field.\nHowever, while it has advanced the field in so many ways, it has also introduced almost imperceptible biases: why is newswire considered more standard or more canonical than other text types? Journalists are trained writers who make fewer errors and adhere to a codified norm.1 But let us pause for a minute. If NLP had emerged only in the last decade, would newswire data still be our canon? Or would, say, Wikipedia be considered canonical? User-generated data is less standardized, but is highly available. If we take this thought further and start over today, maybe we would be in an \u2018inverted\u2019 world: social media is standard and newswire with its \u2018headlinese\u2019 is the \u2018bad language\u2019 (Eisenstein, 2013). It is easy to collect large quantities of social media data. Whatever we consider canonical, all data comes with its biases, even more democratic media like Wikipedia carry their own peculiarities.2\nIt seems that what is considered canonical hitherto is mostly a historical coincidence and motivated largely by availability of resources. Newswire has and actually still does dominate our field. For example, in Figure 1, I plot domains versus languages for the treebank data in version 1.3 of the on-going Universal Dependencies3 project (Nivre et al., 2015). Almost all languages include newswire, except ancient languages (for obvious reasons), English (since most data comes from the Web Treebank) and Khazak, Chinese (Wikipedia). While including other domains and languages is highly desirable, it is impossible\n1We do not explicitly concern us here with issues of language prescription, but rather on the assumption-heavy perceptions of some instances of language as \u2018more normal\u2019.\n2For instance, the demographics of Wikipedia shows that mostly young single men aged 18-30 contribute, see https://strategy.wikimedia.org/wiki/ Wikimedia_users#Demographics\n3http://universaldependencies.org/\nar X\niv :1\n60 8.\n07 83\n6v 1\n[ cs\n.C L\n] 2\n8 A\nug 2\n01 6\nto find unbiased data.4 Let\u2019s be aware of this fact and try to collect enough biased data.\nProcessing non-canonical (or non-canonical) data is difficult. A series of papers document large drops in accuracy when moving across domains (McClosky, 2010; Foster et al., 2011, inter alia). There is a large body of work focusing on correcting for domain differences. Typically, in domain adaptation (DA) the task is to adapt a model trained on some source domain to perform better on some new target domain. However, it is less clear what really folds into a domain. In Section 5, I will review the notion of domain and propose what I call variety space.\nIs the annotation of non-canonical also more difficult, just like its processing appears to be? Processing and annotating are two aspects, and the difficulty in one, say processing, does not necessarily propagate the same way to annotation (Plank et al., 2015). However, very little work exists on disentangling the two. The same is true for examining what really constitutes a domain. What remains is clear: the challenge is all about variations of data. Language continuously changes, for various reasons (different social groups, communicative purposes, changes over time), and so we will continuously face interesting challenges, both for processing and annotation.\nIn the remainder I will look at the NLP community\u2019s approach to face these challenges. I will outline one potential way to go about it, arguing for the use of fortuitous data, and end by returning to the question of domain."}, {"heading": "2 What to do about non-standard data", "text": "There are generally three main approaches to go about non-standard data."}, {"heading": "2.1 Annotate more data", "text": "Annotating more data is a first and intuitive solution. However, it is na\u0131\u0308ve, for several reasons.\nDomain (whatever that means) and language (whatever that comprises) are two factors of text variation. Now take the cross-product between the two. We will never be able to create annotated data that spans all possible combinations. This is the problem of training data sparsity, illustrated in Figure 1. The figure only shows a tiny subset of\n4This is related to the problem of overexposure in ethics, e.g., (Hovy and Spruit, 2016).\nthe world\u2019s languages, and a tiny fraction of potential domains out there. The problem is that most of the data that is available out there is unlabeled. Annotation requires time. At the same time, ways of communication change, so what we annotate today might be very distant to what we need to process tomorrow. We cannot just \u201cannotate our way out\u201d (Eisenstein, 2013). Moreover, it might not be trivial to find the right annotators; annotation schemes might need adaptation as well (Zinsmeister et al., 2014) and tradeoffs for doing so need to be defined (Schneider, 2015).\nWhat we need is quick ways to semiautomatically gather annotated data, and use more unsupervised and weakly supervised approaches."}, {"heading": "2.2 Bring training and test data closer to each other", "text": "The second approach is based on the idea of making data resemble each other more. The first strategy here is normalization, that is, preprocess the input to make it closer to what our technology expects, e.g. Han et al. (2013). A less known but similar approach is to artificially corrupt the training data to make it more similar to the expected target do-\nmain (van der Plas et al., 2009). However, normalization implies \u201cnorm\u201d, and as Eisenstein (2013) remarks: whose norm are we targeting? (e.g., labor vs labour). Furthermore, he notes that it is surprisingly difficult to find a precise notion of the normalization task.\nCorrupting the training data is a less explored endeavor. This second strategy though hinges on the assumption that one knows what to expect.\nWhat we need are models that do provide nonsensical predictions on unexpected inputs, i.e., models that include invariant representations. For example, our models should be capable of learning similar representations for the same inherent concept, e.g., kiss vs :* or love vs <3. Recent shifts towards using sub-token level information can be seen as one step in this direction."}, {"heading": "2.3 Domain adaptation", "text": "There is a large body of work on adapting models trained on some source domain to work better on some new target domain. Approaches range from feature augmentation, shared representation learning, instance weighting, to approaches that exploit representation induced from general background corpora. For an overview, see (Plank, 2011; Weiss et al., 2016). However, what all of these approaches have in common is an unrealistic assumption: they know the target domain. That is, researchers typically have a small amount of target data available that they can use to adapt their models.\nAn extreme case of adaptation is cross-lingual learning, whose goal is similar: adapt models trained on some source languages to languages in which few or no resources exist. Also here a large body of work assumes knowledge of the target language and requires some in-domain, typically parallel data. However, most work has focused on a restricted set of languages, only recently approaches emerged that aim to transfer from multiple sources to many target languages (Agic\u0301 et al., 2016).\nWhat we need are methods that can adapt quickly to unknown domains and languages, without much assumptions on what to expect, and use multiple sources, rather than just one. In addition, our models need to detect when to trigger domain adaptation approaches.\nIn the next parts I will outline some possibilities to address these challenges. However, there are other important areas that I will not touch upon here (e.g., evaluation)."}, {"heading": "3 Fortuitous data", "text": "What we need are models that are more robust, work better on unexpected input and can be trained from semi-automatically or weakly annotated data, from a variety of sources. In order to build such models, I argue that the key is to look for signal in non-obvious places, i.e., fortuitous data.5\nFortuitous data is data out there that just waits to be harvested. It might be in plain sight, but is neglected (available but not used), or it is in raw form and first needs to be refined (almost ready but needs refinement). Availability and ease-of-use (or readiness) are therefore two important dimensions that define fortuitous data. Fortuitous data is the unintended yield of a process, a promising by-product or side benefit.\nIn the following I will outline potential sources of fortuitous data. An overview is given in Table 1.\nSide benefit of user-generated content This is data of high availability and high readiness, but it is often not used or \u201cpreprocessed away\u201d. This source of fortuitous data includes user-generated content like webpages, social media posts, communityefforts like Wikipedia or Wiktionary. Concrete examples include hyperlinks that can be used to build more robust named entity and part-of-speech taggers (Plank et al., 2014a), or HTML markup for parsing (Spitkovsky et al., 2010). Similarly, Wiktionary can be used to mine large pools of data for unambiguous instances (Hovy et al., 2015), or can guide constrained inference like in typeconstrained POS tagging (Ta\u0308ckstro\u0308m et al., 2013; Plank et al., 2014b). Broadly speaking, exploiting the web to process the web.\nSide benefit of annotation Another yield that is often disregarded is annotator disagreement. Such data has high readiness, but low availability. It is still rare for annotation efforts to release intermediate or preliminary stages of the annotation project, but such data contains precious signal.\n5Thanks to Anders Johannsen for suggesting fortuitous when I was in search for a name for serendipitous casual data.\nIn fact, instead of adjudicating annotator decisions, we should embrace it. Annotator disagreement contains actual signal informative for a variety of tasks, including tagging, parsing, supersense tagging and relation extraction, e.g., (Plank et al., 2014b; Aroyo et al., 2015).\nSide benefit of behavior When people produce or read texts, they produce loads of by-product in form of behavior data. Examples here include click-through data, but also more distant sources such as cognitive processing data like eye tracking or keystroke dynamics. In a pilot study, I found keystroke logs carry signal that can be used to inform NLP. Such data represents a potentially immense resource (imagine logging devices build into online editors or mobile phones, or eye tracking build into mobile devices). However, only very little work explored this source yet, e.g., (Barrett and S\u00f8gaard, 2015; Klerke et al., 2016). It is also the \u201cmost distant\u201d fortuitous source, having high availability and low readiness, as data often first needs to be refined.\nUsing fortuitous data can thus be seen as a way to quickly obtain semi-automatically labeled data, from a variety of sources. If we pair fortuitous data with appropriate learning algorithms (transfer/multi-task learning), this will enable language technology that can adapt quickly to new language varieties. However, one question remains."}, {"heading": "4 But what\u2019s in a domain?", "text": "As already noted earlier (Plank, 2011), there is no common ground on what constitutes a domain. Blitzer et al. (2006) attribute domain differences mostly to differences in vocabulary, Biber (1988) explores differences between corpora from a sociolinguistics perspective. McClosky (2010) considers it in a broader view: \u201cBy domain, we mean the style, genre, and medium of a document.\u201d Terms such as genre, register, text type, domain, style are often used differently in different communities (Lee, 2002), or interchangeably.\nWhile there exists no definition of domain, work on domain adaptation is plentiful but mostly focused on assuming a dichotomy: source versus target, without much interest in how they differ. In fact, there is surprisingly little work on how texts vary and the consequence for NLP. It is established that out-of-vocabulary (OOV) tokens impact NLP performance. However, what are other factors?\nInterest in this question re-emerged recently. For example, focusing on annotation difficulty, Zeldes and Simonson (2016) remark \u201cthat domain adaptation may be folding in sentence type effects\u201d, motivated by earlier findings by Silveira et al. (2014) who remark that \u201c[t]he most striking difference between the two types of data [Web and newswire] has to do with imperatives, which occur two orders of magnitude more often in the EWT [English Web Treebank].\u201d A very recent paper examines word order properties and their impact on parsing taking a control experiment approach (Gulordava and Merlo, 2016). On another angle, it has been shown that tagging accuracy correlates with demographic factors such as age (Hovy and S\u00f8gaard, 2015).\nI want to propose that \u2018domain\u2019 is an overloaded term. Besides the mathematical definition, in NLP it is typically used to refer to some coherent data with respect to topic or genre. However, there are many other (including yet unknown factors) out there, such as demographic factors, communicational purpose, but also sentence type, style, medium, technology/medium, language, etc. At the same time, these categories are not sharply defined either. Rather than imposing hard categories, let us consider a Wittgensteinian view."}, {"heading": "5 The variety space", "text": "I here propose to see a domain as variety in a highdimensional variety space. Points in the space are the data instances, and regions form domains. A dataset D is a sample from the variety space, conditioned on latent factors V :\nD \u223c P(X ,Y |V )\nThe variety space is a unknown highdimensional space, whose dimensions (latent factors V ) include (fuzzy) aspects such as language (or dialect), topic or genre, and social factors (age, gender, personality, etc.), amongst others. A domain is a variety that forms a region in this complicated network of similarities, with some members more prototypical than others. However, we have neither access to the number of latent factors nor to their types. This vision is inspired by the notion of prototype theory in Cognitive Science and Wittgenstein\u2019s graded notion of categories. Figure 2 shows a hypothetical example of this variety space.\nOur datasets are subspaces of this highdimensional space. Depending on our task, in-\nstances are sentences, documents etc. In the following I will use POS tagging as a running example to analyze what\u2019s in a domain, by referring to the datasets with the typically used categories.\nSome empirical evidence - Taggers and Data Let us examine two POS taggers representative for different tagging approaches and evaluate them on several varieties. We use TNT,6 an HMMbased tagger, and BILTY, a bidirectional LSTM tagger (Plank et al., 2016). Both taggers are trained on the WSJ training portion converted to Universal POS tags (Petrov et al., 2012). As test sets we consider parts of the Web Treebank (emails and answers), two Twitter datasets (FOSTER and GIMPEL/OCT27, Twitter sample 1 and 2 respectively), review data from two different age groups (Hovy and S\u00f8gaard, 2015), above 45 and below 35 years, and data from the CoNLL-X dataset from other Indogermanic languages.7 These datasets were chosen to represent different varieties.\nResults Table 2 shows POS tagging accuracies. First, as is well known, we see that all taggers suffer when applied to other domains. However, models trained on WSJ fare worse on data from the younger age group, thus age is a covariate. This confirms the age bias reported in (Hovy and\n6http://www.coli.uni-saarland.de/ \u02dcthorsten/tnt/\n7http://ilk.uvt.nl/conll/free_data. html except Dutch because of joined MWU units.\nS\u00f8gaard, 2015) for the same data but using different taggers. If we stretch the notion of variety to other languages, we see that performance unsurprisingly drops dramatically. Remember, we just apply an indomain single language tagger to other languages, although only trained on WSJ here.8 BILTY~w+~c performs much better on other languages than TNT. Although the neural network-based tagger that uses both word and character embeddings fares better overall, both taggers suffer similarly, their accuracy variation is highly correlated (\u03c1=0.95, p < 0.01 over all test sets; \u03c1=0.96 if we exclude the other languages, and \u03c1=0.94 if we also include OCT27).\nWhile the two age samples have similar OOV rates, the two Twitter samples differ substantially. Twitter sample 1 (FOSTER) has an OOV rate close to others (28), while sample 2 has the highest OOV (52), every other token is an OOV word. Thus, although both come from the same medium (Twitter), they are very different samples. In general, OOV words are a major cause of performance drop. If we correlate all accuracies with OOV rate, we see a significant correlation (\u03c1 = -0.70, p=0.02274). However, caution is needed here, the high correlation could be influenced by outliers. In fact, if we exclude the other Twitter sample (OCT27, which seems to form an outlier) and other languages, there is no significant correlation (\u03c1=0.23, p-val 0.6584), see Figure 3, explained next.\nRather than just inspecting numbers of single test sets, we will now plot data characteristics versus accuracy. In order to do so we take 10 bootstrap samples (k = 150 sentences) from the original test data, tag it with the best variant of BILTY, which uses word and character features, and evaluate it against gold POS. Figure 3 shows accuracy rates versus OOV rate (above) and accuracy vs KL-divergence between gold and predicted tag bigram distributions (lower plot). Each data point in the plot is a bootstrap sample.\nThe plots show that Twitter sample 2 (dark green, FOSTER) is similar in OOV rate to emails and answers; In fact, it is very close to the original dataset (WSJ), it differs the least from WSJ in terms of POS KL-divergence (lower plot). In contrast, Twitter sample 2 (green, OCT27) has not only high OOV rate, but it also differs highly in KL div from WSJ. The dataset contains many unusual POS sequences that are hard to predict. The same is true for age,\n8Subtoken representations are used train a single tagger for multiple languages (Gillick et al., 2015).\nthe KL plot confirms that the tags of the younger group are harder to predict.\nWe see that performance varies greatly on different samples of Twitter data, as also reported earlier (Hovy et al., 2014). This suggest that Twitter is not a \u2018single domain\u2019. It spans an entire range of varieties (social groups, agents, topics, even languages, etc.). Relating back to variety space, it seems that our two samples span different subspaces. Although the two samples used here do not resemble each other, they still share the commonality of being drawn from the same category (in this case, medium), mirroring Wittgenstein\u2019s theory of family resemblance, cf. (Givo\u0301n, 1986). In fact, if we think about data from Twitter, we will have a prototypical member in mind, but members might vary highly. Whenever we build models for, say, Twitter, we need to be aware of these properties. The more the data varies, the more test samples we will need to achieve higher confidence in our models."}, {"heading": "6 Conclusions", "text": "Current NLP models still suffer dramatically when applied to non-canonical data, where canonicity is a relative notion; in our field, newswire was and still often is the de-facto standard, the canonical data we typically train our models on.\nWhile newswire has advanced the field in so many ways, it has also introduced almost imperceptible biases. What we need is to be aware of such biases, collect enough biased data, and model variety. I argue that if we embrace the variety of this heterogeneous data by combining it with proper algorithms, in addition to including text covariates/latent factors, we will not only produce more robust models, but will also enable adaptive language technology capable of addressing natural language variation."}, {"heading": "Acknowledgments", "text": "I would like to thank the organizers for the invitation to the keynote at KONVENS 2016. I am also grateful to He\u0301ctor Mart\u0131\u0301nez Alonso, Dirk Hovy, Anders Johannsen, Zeljko Agic\u0301 and Gertjan van Noord for valuable discussions and feedback on earlier drafts of this paper."}], "references": [{"title": "Multilingual projection for parsing truly low-resource languages", "author": ["\u017deljko Agi\u0107", "Anders Johannsen", "Barbara Plank", "Hctor Martnez Alonso", "Natalie Schluter", "Anders S\u00f8gaard."], "venue": "TACL, 4:301\u2013312.", "citeRegEx": "Agi\u0107 et al\\.,? 2016", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2016}, {"title": "Truth is a lie: Crowd truth and the seven myths of human annotation", "author": ["Lora Aroyo", "Chris Welty"], "venue": null, "citeRegEx": "Aroyo and Welty,? \\Q2015\\E", "shortCiteRegEx": "Aroyo and Welty", "year": 2015}, {"title": "Using reading behavior to predict grammatical functions", "author": ["Maria Barrett", "Anders S\u00f8gaard."], "venue": "Workshop on Cognitive Aspects of Computational Language Learning.", "citeRegEx": "Barrett and S\u00f8gaard.,? 2015", "shortCiteRegEx": "Barrett and S\u00f8gaard.", "year": 2015}, {"title": "Variation across speech and writing", "author": ["Douglas Biber."], "venue": "Cambridge University Press.", "citeRegEx": "Biber.,? 1988", "shortCiteRegEx": "Biber.", "year": 1988}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120\u2013128. Association for Computa-", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "What to do about bad language on the internet", "author": ["Jacob Eisenstein."], "venue": "NAACL, Stroudsburg, Pennsylvania.", "citeRegEx": "Eisenstein.,? 2013", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "From news to comment: Resources and benchmarks for parsing the language of web", "author": ["Jennifer Foster", "Ozlem Cetinoglu", "Joachim Wagner", "Joseph Le Roux", "Joakim Nivre", "Deirdre Hogan", "Josef VanGenabith"], "venue": "IJCNLP", "citeRegEx": "Foster et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2011}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "arXiv.", "citeRegEx": "Gillick et al\\.,? 2015", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Prototypes: Between plato and wittgenstein", "author": ["Talmy Giv\u00f3n."], "venue": "Noun classes and categorization, pages 77\u2013102.", "citeRegEx": "Giv\u00f3n.,? 1986", "shortCiteRegEx": "Giv\u00f3n.", "year": 1986}, {"title": "Multilingual dependency parsing evaluation: a large-scale analysis of word order properties using artificial data", "author": ["Kristina Gulordava", "Paola Merlo."], "venue": "Transactions of the Association for Computational Linguistics, 4:343\u2013356.", "citeRegEx": "Gulordava and Merlo.,? 2016", "shortCiteRegEx": "Gulordava and Merlo.", "year": 2016}, {"title": "Lexical normalization for social media text", "author": ["Bo Han", "Paul Cook", "Timothy Baldwin."], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 4(1):5.", "citeRegEx": "Han et al\\.,? 2013", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Tagging performance correlates with author age", "author": ["Dirk Hovy", "Anders S\u00f8gaard."], "venue": "ACL.", "citeRegEx": "Hovy and S\u00f8gaard.,? 2015", "shortCiteRegEx": "Hovy and S\u00f8gaard.", "year": 2015}, {"title": "The social impact of natural language processing", "author": ["Dirk Hovy", "Shannon L. Spruit."], "venue": "LREC.", "citeRegEx": "Hovy and Spruit.,? 2016", "shortCiteRegEx": "Hovy and Spruit.", "year": 2016}, {"title": "When POS datasets don\u2019t add up: Combatting sample bias", "author": ["Dirk Hovy", "Barbara Plank", "Anders S\u00f8gaard."], "venue": "LREC.", "citeRegEx": "Hovy et al\\.,? 2014", "shortCiteRegEx": "Hovy et al\\.", "year": 2014}, {"title": "Mining for unambiguous instances to adapt part-of-speech taggers to new domains", "author": ["Dirk Hovy", "Barbara Plank", "H\u00e9ctor Mart\u0131nez Alonso", "Anders S\u00f8gaard."], "venue": "NAACL.", "citeRegEx": "Hovy et al\\.,? 2015", "shortCiteRegEx": "Hovy et al\\.", "year": 2015}, {"title": "Improving sentence compression by learning to predict gaze", "author": ["Sigrid Klerke", "Yoav Goldberg", "Anders S\u00f8gaard."], "venue": "NAACL.", "citeRegEx": "Klerke et al\\.,? 2016", "shortCiteRegEx": "Klerke et al\\.", "year": 2016}, {"title": "Genres, registers, text types, domains and styles: clarifying the concepts and navigating a path through the bnc jungle", "author": ["David Lee."], "venue": "Language and Computers, 42(1):247\u2013292.", "citeRegEx": "Lee.,? 2002", "shortCiteRegEx": "Lee.", "year": 2002}, {"title": "Any domain parsing: automatic domain adaptation for natural language parsing", "author": ["David McClosky."], "venue": "Ph.D. thesis, Brown University.", "citeRegEx": "McClosky.,? 2010", "shortCiteRegEx": "McClosky.", "year": 2010}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "LREC.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Adapting taggers to twitter using not-so-distant supervision", "author": ["Barbara Plank", "Dirk Hovy", "Ryan McDonald", "Anders S\u00f8gaard."], "venue": "COLING.", "citeRegEx": "Plank et al\\.,? 2014a", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Learning part-of-speech taggers with inter-annotator agreement loss", "author": ["Barbara Plank", "Dirk Hovy", "Anders S\u00f8gaard."], "venue": "EACL.", "citeRegEx": "Plank et al\\.,? 2014b", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Non-canonical language is not", "author": ["Barbara Plank", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2015}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "ACL.", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Domain adaptation for parsing", "author": ["Barbara Plank."], "venue": "Ph.D. thesis, University of Groningen.", "citeRegEx": "Plank.,? 2011", "shortCiteRegEx": "Plank.", "year": 2011}, {"title": "What i\u2019ve learned about annotating informal text (and why you shouldnt take my word for it)", "author": ["Nathan Schneider."], "venue": "The 9th Linguistic Annotation Workshop held in conjuncion with NAACL 2015.", "citeRegEx": "Schneider.,? 2015", "shortCiteRegEx": "Schneider.", "year": 2015}, {"title": "A gold standard dependency corpus for english", "author": ["Natalia Silveira", "Timothy Dozat", "Marie-Catherine De Marneffe", "Samuel R Bowman", "Miriam Connor", "John Bauer", "Christopher D Manning."], "venue": "LREC.", "citeRegEx": "Silveira et al\\.,? 2014", "shortCiteRegEx": "Silveira et al\\.", "year": 2014}, {"title": "Profiting from mark-up: Hyper-text annotations for guided parsing", "author": ["Valentin I Spitkovsky", "Daniel Jurafsky", "Hiyan Alshawi."], "venue": "ACL.", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Token and type constraints for cross-lingual part-of-speech tagging", "author": ["Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Slav Petrov", "Ryan McDonald", "Joakim Nivre."], "venue": "Transactions of the Association for Computational Linguistics, 1:1\u201312.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2013", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Domain adaptation with artificial data for semantic parsing of speech", "author": ["Lonneke van der Plas", "James Henderson", "Paola Merlo."], "venue": "NAACL.", "citeRegEx": "Plas et al\\.,? 2009", "shortCiteRegEx": "Plas et al\\.", "year": 2009}, {"title": "A survey of transfer learning", "author": ["Karl Weiss", "Taghi M Khoshgoftaar", "DingDing Wang."], "venue": "Journal of Big Data, 3(1):1\u201340.", "citeRegEx": "Weiss et al\\.,? 2016", "shortCiteRegEx": "Weiss et al\\.", "year": 2016}, {"title": "Different flavors of gum: Evaluating genre and sentence type effects on multilayer corpus annotation quality", "author": ["Amir Zeldes", "Dan Simonson."], "venue": "Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with ACL 2016 (LAW-X", "citeRegEx": "Zeldes and Simonson.,? 2016", "shortCiteRegEx": "Zeldes and Simonson.", "year": 2016}, {"title": "Adapting a part-of-speech tagset to nonstandard text: The case of stts", "author": ["Heike Zinsmeister", "Ulrich Heid", "Kathrin Beck."], "venue": "LREC.", "citeRegEx": "Zinsmeister et al\\.,? 2014", "shortCiteRegEx": "Zinsmeister et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "guage\u2019 (Eisenstein, 2013).", "startOffset": 7, "endOffset": 25}, {"referenceID": 21, "context": "cessing and annotating are two aspects, and the difficulty in one, say processing, does not necessarily propagate the same way to annotation (Plank et al., 2015).", "startOffset": 141, "endOffset": 161}, {"referenceID": 12, "context": ", (Hovy and Spruit, 2016).", "startOffset": 2, "endOffset": 25}, {"referenceID": 5, "context": "We cannot just \u201cannotate our way out\u201d (Eisenstein, 2013).", "startOffset": 38, "endOffset": 56}, {"referenceID": 31, "context": "Moreover, it might not be trivial to find the right annotators; annotation schemes might need adaptation as well (Zinsmeister et al., 2014) and tradeoffs for doing so need to be defined (Schneider, 2015).", "startOffset": 113, "endOffset": 139}, {"referenceID": 24, "context": ", 2014) and tradeoffs for doing so need to be defined (Schneider, 2015).", "startOffset": 54, "endOffset": 71}, {"referenceID": 10, "context": "Han et al. (2013). A less known but similar approach is to artificially corrupt the training data to make it more similar to the expected target do-", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "However, normalization implies \u201cnorm\u201d, and as Eisenstein (2013) remarks: whose norm are we targeting? (e.", "startOffset": 46, "endOffset": 64}, {"referenceID": 23, "context": "For an overview, see (Plank, 2011; Weiss et al., 2016).", "startOffset": 21, "endOffset": 54}, {"referenceID": 29, "context": "For an overview, see (Plank, 2011; Weiss et al., 2016).", "startOffset": 21, "endOffset": 54}, {"referenceID": 0, "context": "However, most work has focused on a restricted set of languages, only recently approaches emerged that aim to transfer from multiple sources to many target languages (Agi\u0107 et al., 2016).", "startOffset": 166, "endOffset": 185}, {"referenceID": 19, "context": "Concrete examples include hyperlinks that can be used to build more robust named entity and part-of-speech taggers (Plank et al., 2014a), or HTML markup for parsing (Spitkovsky et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 26, "context": ", 2014a), or HTML markup for parsing (Spitkovsky et al., 2010).", "startOffset": 37, "endOffset": 62}, {"referenceID": 14, "context": "Similarly, Wiktionary can be used to mine large pools of data for unambiguous instances (Hovy et al., 2015), or can guide constrained inference like in typeconstrained POS tagging (T\u00e4ckstr\u00f6m et al.", "startOffset": 88, "endOffset": 107}, {"referenceID": 27, "context": ", 2015), or can guide constrained inference like in typeconstrained POS tagging (T\u00e4ckstr\u00f6m et al., 2013; Plank et al., 2014b).", "startOffset": 80, "endOffset": 125}, {"referenceID": 20, "context": ", 2015), or can guide constrained inference like in typeconstrained POS tagging (T\u00e4ckstr\u00f6m et al., 2013; Plank et al., 2014b).", "startOffset": 80, "endOffset": 125}, {"referenceID": 20, "context": ", (Plank et al., 2014b; Aroyo et al., 2015).", "startOffset": 2, "endOffset": 43}, {"referenceID": 2, "context": ", (Barrett and S\u00f8gaard, 2015; Klerke et al., 2016).", "startOffset": 2, "endOffset": 50}, {"referenceID": 15, "context": ", (Barrett and S\u00f8gaard, 2015; Klerke et al., 2016).", "startOffset": 2, "endOffset": 50}, {"referenceID": 23, "context": "As already noted earlier (Plank, 2011), there is no common ground on what constitutes a domain.", "startOffset": 25, "endOffset": 38}, {"referenceID": 16, "context": "\u201d Terms such as genre, register, text type, domain, style are often used differently in different communities (Lee, 2002), or interchangeably.", "startOffset": 110, "endOffset": 121}, {"referenceID": 3, "context": "Blitzer et al. (2006) attribute domain differences mostly to differences in vocabulary, Biber (1988) explores differences between corpora from a sociolinguistics perspective.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "(2006) attribute domain differences mostly to differences in vocabulary, Biber (1988) explores differences between corpora from a sociolinguistics perspective.", "startOffset": 73, "endOffset": 86}, {"referenceID": 3, "context": "(2006) attribute domain differences mostly to differences in vocabulary, Biber (1988) explores differences between corpora from a sociolinguistics perspective. McClosky (2010) considers it in a broader view: \u201cBy domain, we mean the style, genre, and medium of a document.", "startOffset": 73, "endOffset": 176}, {"referenceID": 9, "context": "\u201d A very recent paper examines word order properties and their impact on parsing taking a control experiment approach (Gulordava and Merlo, 2016).", "startOffset": 118, "endOffset": 145}, {"referenceID": 11, "context": "On another angle, it has been shown that tagging accuracy correlates with demographic factors such as age (Hovy and S\u00f8gaard, 2015).", "startOffset": 106, "endOffset": 130}, {"referenceID": 27, "context": "For example, focusing on annotation difficulty, Zeldes and Simonson (2016) remark \u201cthat domain adaptation may be folding in sentence type effects\u201d, motivated by earlier findings by Silveira et al.", "startOffset": 48, "endOffset": 75}, {"referenceID": 23, "context": "For example, focusing on annotation difficulty, Zeldes and Simonson (2016) remark \u201cthat domain adaptation may be folding in sentence type effects\u201d, motivated by earlier findings by Silveira et al. (2014) who remark that \u201c[t]he most striking difference between the two types of data [Web and newswire] has to do with imperatives, which occur two orders of magnitude more often in the EWT [English Web Treebank].", "startOffset": 181, "endOffset": 204}, {"referenceID": 22, "context": "We use TNT,6 an HMMbased tagger, and BILTY, a bidirectional LSTM tagger (Plank et al., 2016).", "startOffset": 72, "endOffset": 92}, {"referenceID": 18, "context": "Both taggers are trained on the WSJ training portion converted to Universal POS tags (Petrov et al., 2012).", "startOffset": 85, "endOffset": 106}, {"referenceID": 11, "context": "consider parts of the Web Treebank (emails and answers), two Twitter datasets (FOSTER and GIMPEL/OCT27, Twitter sample 1 and 2 respectively), review data from two different age groups (Hovy and S\u00f8gaard, 2015), above 45 and below 35 years, and data from the CoNLL-X dataset from other Indogermanic languages.", "startOffset": 184, "endOffset": 208}, {"referenceID": 18, "context": "Table 2: Tagging accuracy on various test set varieties (domains, languages and age groups; Tw=Twitter), using coarse POS (Petrov et al., 2012).", "startOffset": 122, "endOffset": 143}, {"referenceID": 7, "context": "8Subtoken representations are used train a single tagger for multiple languages (Gillick et al., 2015).", "startOffset": 80, "endOffset": 102}, {"referenceID": 13, "context": "We see that performance varies greatly on different samples of Twitter data, as also reported earlier (Hovy et al., 2014).", "startOffset": 102, "endOffset": 121}, {"referenceID": 8, "context": "(Giv\u00f3n, 1986).", "startOffset": 0, "endOffset": 13}], "year": 2016, "abstractText": "Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., sociodemographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language. In this paper, I review the notion of canonicity, and how it shapes our community\u2019s approach to language. I argue for leveraging what I call fortuitous data, i.e., nonobvious data that is hitherto neglected, hidden in plain sight, or raw data that needs to be refined. If we embrace the variety of this heterogeneous data by combining it with proper algorithms, we will not only produce more robust models, but will also enable adaptive language technology capable of addressing natural language variation.", "creator": "TeX"}}}