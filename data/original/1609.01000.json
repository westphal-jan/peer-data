{"id": "1609.01000", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2016", "title": "Convexified Convolutional Neural Networks", "abstract": "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.", "histories": [["v1", "Sun, 4 Sep 2016 23:57:43 GMT  (145kb,D)", "http://arxiv.org/abs/1609.01000v1", "29 pages"]], "COMMENTS": "29 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuchen zhang", "percy liang", "martin j wainwright"], "accepted": true, "id": "1609.01000"}, "pdf": {"name": "1609.01000.pdf", "metadata": {"source": "CRF", "title": "Convexified Convolutional Neural Networks", "authors": ["Yuchen Zhang", "Percy Liang", "Martin J. Wainwright"], "emails": ["zhangyuc@cs.stanford.edu.", "pliang@cs.stanford.edu.", "wainwrig@eecs.berkeley.edu."], "sections": [{"heading": "1 Introduction", "text": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37]. There are two principal advantages of a CNN over a fully-connected neural network: (i) sparsity\u2014that each nonlinear convolutional filter acts only on a local patch of the input, and (ii) parameter sharing\u2014that the same filter is applied to each patch.\nHowever, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard [6]. In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation [7]. This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper [19]), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm.\nIn this paper, with the goal of addressing these two challenges, we propose a new model class known as convexified convolutional neural networks (CCNNs). These models have two desirable features. First, training a CCNN corresponds to a convex optimization problem, which can be solved efficiently and optimally via a projected gradient algorithm. Second, the statistical properties of CCNN models can be studied in a precise and rigorous manner. We obtain CCNNs by convexifying\n\u2217Computer Science Department, Stanford University, Stanford, CA 94305. Email: zhangyuc@cs.stanford.edu. \u2020Computer Science Department, Stanford University, Stanford, CA 94305. Email: pliang@cs.stanford.edu. \u2021Department of Electrical Engineering and Computer Science and Department of Statistics, University of California\nBerkeley, Berkeley, CA 94720. Email: wainwrig@eecs.berkeley.edu.\nar X\niv :1\n60 9.\n01 00\n0v 1\n[ cs\n.L G\n] 4\nS ep\n2 01\ntwo-layer CNNs; doing so requires overcoming two challenges. First, the activation function of a CNN is nonlinear. In order to address this issue, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). This approach is inspired by our earlier work [48], involving a subset of the current authors, in which we developed this relaxation step for fully-connected neural networks. Second, the parameter sharing induced by CNNs is crucial to its effectiveness and must be preserved. We show that CNNs with RKHS filters can be parametrized by a low-rank matrix. Further relaxing the low-rank constraint to a nuclear norm constraint leads to our final formulation of CCNNs.\nOn the theoretical front, we prove an oracle inequality on generalization error achieved by our class of CCNNs, showing that it is upper bounded by the best possible performance achievable by a two-layer CNN given infinite data\u2014a quantity to which we refer as the oracle risk\u2014plus a model complexity term that decays to zero polynomially in the sample size. Our results show that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network [48], highlighting the importance of parameter sharing. For models with more than one hidden layer, our theory does not apply, but we provide encouraging empirical results using a greedy layer-wise training heuristic. We then apply CCNNs to the MNIST handwritten digit dataset as well as four variation datasets [43], and find that it achieves the state-of-the-art performance. On the CIFAR-10 dataset, CCNNs outperform CNNs of the same depths, as well as other baseline methods that do not involve nonconvex optimization. We also demonstrate that building CCNNs on top of existing CNN filters improves the performance of CNNs.\nThe remainder of this paper is organized as follows. We begin in Section 2 by introducing convolutional neural networks, and setting up the empirical risk minimization problem studied in this paper. In Section 3, we describe the algorithm for learning two-layer CCNNs, beginning with the simple case of convexifying CNNs with a linear activation function, then proceeding to convexify CNNs with a nonlinear activation. We show that the generalization error of a CCNN converges to that of the best possible CNN. In Section 4, we describe several extensions to the basic CCNN algorithm, including averaging pooling, multi-channel input processing, and the layer-wise learning of multi-layer CNNs. In Section 5, we report the empirical evaluations of CCNNs. We survey related work in Section 6 and conclude the paper in Section 7.\nNotation. For any positive integer n, we use [n] as a shorthand for the discrete set {1, 2, . . . , n}. For a rectangular matrix A, let \u2016A\u2016\u2217 be its nuclear norm, \u2016A\u20162 be its spectral norm (i.e., maximal singular value), and \u2016A\u2016F be its Frobenius norm. We use `2(N) to denote the set of countable dimensional vectors v = (v1, v2, . . . ) such that \u2211\u221e `=1 v 2 ` < \u221e. For any vectors u, v \u2208 `2(N), the\ninner product \u3008u, v\u3009 := \u2211\u221e `=1 uivi and the `2-norm \u2016u\u20162 := \u221a \u3008u, u\u3009 are well defined."}, {"heading": "2 Background and problem set-up", "text": "In this section, we formalize the class of convolutional neural networks to be learned and describe the associated nonconvex optimization problem."}, {"heading": "2.1 Convolutional neural networks.", "text": "At a high level, a two-layer CNN1 is a particular type of function that maps an input vector x \u2208 Rd0 (e.g., an image) to an output vector in y \u2208 Rd2 (e.g., classification scores for the d2 classes). This mapping is formed in the following manner:\n\u2022 First, we extract a collection of P vectors {zp(x)}Pj=1 of the full input vector x. Each vector zp(x) \u2208 Rd1 is referred to as a patch, and these patches may depend on overlapping components of x.\n\u2022 Second, given some choice of activation function \u03c3 : R \u2192 R and a collection of weight vectors {wj}rj=1 in Rd1 , we compute the functions\nhj(z) := \u03c3(w > j z) for each patch z \u2208 Rd1 . (1)\nEach function hj (for j \u2208 [r]) is known as a filter, and note that the same filters are applied to each patch\u2014this corresponds to the parameter sharing of a CNN.\n\u2022 Third, for each patch index p \u2208 [P ], filter index j \u2208 [r], and output coordinate k \u2208 [d2], we introduce a coefficient \u03b1k,j,p \u2208 R that governs the contribution of the filter hj on patch zp(x) to output fk(x). The final form of the CNN is given by f(x) : = (f1(x), . . . , fd2(x)), where the k th\ncomponent is given by\nfk(x) := r\u2211 j=1 P\u2211 p=1 \u03b1k,j,phj(zp(x)). (2)\nTaking the patch functions {zp}Pp=1 and activation function \u03c3 as fixed, the parameters of the CNN are the filter vectors w := {wj \u2208 Rd1 : j \u2208 [r]} along with the collection of coefficient vectors \u03b1 := {\u03b1k,j \u2208 RP : k \u2208 [d2], j \u2208 [r]}. We assume that all patch vectors zp(x) \u2208 Rd1 are contained in the unit `2-ball. This assumption can be satisfied without loss of generality by normalization: By multiplying a constant \u03b3 > 0 to every patch zp(x) and multiplying 1/\u03b3 to the filter vectors w, the assumption will be satisfied without changing the the output of the network.\nGiven some positive radii B1 and B2, we consider the model class Fcnn(B1, B2) := { f of the form (2) : max\nj\u2208[r] \u2016wj\u20162 \u2264 B1 and max k\u2208[d2],j\u2208[r] \u2016\u03b1k,j\u20162 \u2264 B2\n} . (3)\nWhen the radii (B1, B2) are clear from context, we adopt Fcnn as a convenient shorthand."}, {"heading": "2.2 Empirical risk minimization.", "text": "Given an input-output pair (x, y) and a CNN f , we let L(f(x); y) denote the loss incurred when the output y is predicted via f(x). We assume that the loss function L is convex and L-Lipschitz in its first argument given any value of its second argument. As a concrete example, for multiclass classification with d2 classes, the output vector y takes values in the discrete set [d2] = {1, 2, . . . , d2}.\n1Average pooling and multiple channels are also an integral part of CNNs, but these do not present any new technical challenges, so that we defer these extensions to Section 4.\nFor example, given a vector f(x) = (f1(x), . . . , fd2(y)) \u2208 Rd2 of classification scores, the associated multiclass logistic loss for a pair (x, y) is given by L(f(x); y) := \u2212fy(x) + log (\u2211d2 y\u2032=1 exp(fy\u2032(x)) ) .\nGiven n training examples {(xi, yi)}ni=1, we would like to compute an empirical risk minimizer\nf\u0302cnn \u2208 arg min f\u2208Fcnn n\u2211 i=1 L(f(xi); yi). (4)\nRecalling that functions f \u2208 Fcnn depend on the parameters w and \u03b1 in a highly nonlinear way (2), this optimization problem is nonconvex. As mentioned earlier, heuristics based on stochastic gradient methods are used in practice, which makes it challenging to gain a theoretical understanding of their behavior. Thus, in the next section, we describe a relaxation of the class Fcnn that allows us to obtain a convex formulation of the associated empirical risk minimization problem."}, {"heading": "3 Convexifying CNNs", "text": "We now turn to the development of the class of convexified CNNs. We begin in Section 3.1 by illustrating the procedure for the special case of the linear activation function. Although the linear case is not of practical interest, it provides intuition for our more general convexification procedure, described in Section 3.2, which applies to nonlinear activation functions. In particular, we show how embedding the nonlinear problem into an appropriately chosen reproducing kernel Hilbert space (RKHS) allows us to again reduce to the linear setting."}, {"heading": "3.1 Linear activation functions: low rank relaxations", "text": "In order to develop intuition for our approach, let us begin by considering the simple case of the linear activation function \u03c3(t) = t. In this case, the filter hj when applied to the patch vector zp(x) outputs a Euclidean inner product of the form hj(zp(x)) = \u3008zp(x), wj\u3009. For each x \u2208 Rd0 , we first define the P \u00d7 d1-dimensional matrix\nZ(x) := z1(x) >\n... zP (x) >  . (5) We also define the P -dimensional vector \u03b1k,j := (\u03b1k,j,1, . . . , \u03b1k,j,P )\n>. With this notation, we can rewrite equation (2) for the kth output as\nfk(x) = r\u2211 j=1 P\u2211 p=1 \u03b1k,j,p\u3008zp(x), wj\u3009 = r\u2211 j=1 \u03b1>k,jZ(x)wj = tr ( Z(x) ( r\u2211 j=1 wj\u03b1 > k,j )) = tr(Z(x)Ak), (6)\nwhere in the final step, we have defined the d1\u00d7P -dimensional matrix Ak := \u2211r j=1wj\u03b1 > k,j . Observe that fk now depends linearly on the matrix parameter Ak. Moreover, the matrix Ak has rank at most r, due to the parameter sharing of CNNs. See Figure 1 for a graphical illustration of this model structure.\nLetting A := (A1, . . . , Ad2) be a concatenation of these matrices across all d2 output coordinates, we can then define a function fA : Rd1 \u2192 Rd2 of the form\nfA(x) := (tr(Z(x)A1), . . . , tr(Z(x)Ad2)). (7)\nNote that these functions have a linear parameterization in terms of the underlying matrix A. Our model class corresponds to a collection of such functions based on imposing certain constraints on the underlying matrix A: in particular, we define\nFcnn(B1, B2) := { fA : max\nj\u2208[r] \u2016wj\u20162 \u2264 B1 and max\nk\u2208[d2] j\u2208[r]\n\u2016\u03b1k,j\u20162 \u2264 B2\n\ufe38 \ufe37\ufe37 \ufe38 Constraint (C1)\nand rank(A) = r\ufe38 \ufe37\ufe37 \ufe38 Constraint (C2)\n} .\nThis is simply an alternative formulation of our original class of CNNs. Notice that if the filter weights wj are not shared across all patches, then the constraint (C1) still holds, but constraint (C2) no longer holds. Thus, the parameter sharing of CNNs is realized by the low-rank constraint (C2). The matrix A of rank r can be decomposed as A = UV >, where both U and V have r columns. The column space of matrix A contains the convolution parameters {wj}, and the row space of A contains to the output parameters {\u03b1k,j}.\nThe rank-r matrices satisfying constraints (C1) and (C2) form a nonconvex set. A standard convex relaxation of a rank constraint is based on the nuclear norm \u2016A\u2016\u2217 corresponding to the sum of the singular values of A. It is straightforward to verify that any matrix A satisfying the constraints (C1) and (C2) must have nuclear norm bounded as \u2016A\u2016\u2217 \u2264 B1B2r \u221a d2. Consequently, if we define the function class\nFccnn := { fA : \u2016A\u2016\u2217 \u2264 B1B2r \u221a d2 } , (8)\nthen we are guaranteed that Fccnn \u2287 Fcnn. Overall, we propose to minimize the empirical risk (4) over Fccnn instead of Fcnn; doing so\ndefines a convex optimization problem over this richer class of functions\nf\u0302ccnn := arg min fA\u2208Fccnn n\u2211 i=1 L(fA(xi); yi). (9)\nIn Section 3.3, we describe iterative algorithms that can be used to solve this form of convex program in the more general setting of nonlinear activation functions."}, {"heading": "3.2 Nonlinear activations: RKHS filters", "text": "For nonlinear activation functions \u03c3, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS). As we show, this relaxation allows us to reduce the problem to the linear activation case.\nLet K : Rd1 \u00d7 Rd1 \u2192 R be a positive semidefinite kernel function. For particular choices of kernels (e.g., the Gaussian RBF kernel) and some sufficiently smooth activation function \u03c3, we are able to show that the filter h : z 7\u2192 \u03c3(\u3008w, z\u3009) is contained in the RKHS induced by the kernel function K. See Section 3.4 for the choice of the kernel function and the activation function. Let S := {zp(xi) : p \u2208 [P ], i \u2208 [n]} be the set of patches in the training dataset. The representer theorem then implies that for any patch zp(xi) \u2208 S, the function value can be represented by\nh(zp(xi)) = \u2211\n(i\u2032,p\u2032)\u2208[n]\u00d7[P ]\nci\u2032,p\u2032k(zp(xi), zp\u2032(xi\u2032)) (10)\nfor some coefficients {ci\u2032,p\u2032}(i\u2032,p\u2032)\u2208[n]\u00d7[P ]. Filters taking the form (10) are members of the RKHS, because they are linear combinations of basis functions z 7\u2192 k(z, zp\u2032(xi\u2032)). Such filters are parametrized by a finite set of coefficients, which can be estimated via empirical risk minimization.\nLet K \u2208 RnP\u00d7nP be the symmetric kernel matrix, where with rows and columns indexed by the example-patch index pair (i, p) \u2208 [n]\u00d7 [P ]. The entry at row (i, p) and column (i\u2032, p\u2032) of matrix K is equal to K(zp(xi), zp\u2032(xi\u2032)). So as to avoid re-deriving everything in the kernelized setting, we perform a reduction to the linear setting of Section 3.1. Consider a factorization K = QQ> of the kernel matrix, where Q \u2208 RnP\u00d7m; one example is the Cholesky factorization with m = nP . We can interpret each row Q(i,p) \u2208 Rm as a feature vector in place of the original zp(xi) \u2208 Rd1 , and rewrite equation (10) as\nh(zp(xi)) = \u3008Q(i,p), w\u3009 where w := \u2211 (i\u2032,p\u2032) ci\u2032,p\u2032Q(i\u2032,p\u2032).\nIn order to learn the filter h, it suffices to learn the m-dimensional vector w. To do this, define patch matrices Z(xi) \u2208 RP\u00d7m for each i \u2208 [n] so that its p-th row is Q(i,p). Then we carry out all of Section 3.1; solving the ERM gives us a parameter matrix A \u2208 Rm\u00d7Pd2 . The only difference is that the B1 norm constraint needs to be relaxed as well. See Appendix B for details.\nAt test time, given a new input x \u2208 Rd0 , we can compute a patch matrix Z(x) \u2208 RP\u00d7m as follows:\n\u2022 The p-th row of this matrix is the feature vector for patch p, which is equal to Q\u2020v(zp(x)) \u2208 Rm. Here, for any patch z, the vector v(z) is defined as a nP -dimensional vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). We note that if x is an instance xi in the training set, then the vector Q\u2020v(zp(x)) is exactly equal to Q(i,p). Thus the mapping Z(x) applies to both training and testing.\nAlgorithm 1: Learning two-layer CCNNs\nInput: Data {(xi, yi)}ni=1, kernel function K, regularization parameter R > 0, number of filters r.\n1. Construct a kernel matrix K \u2208 RnP\u00d7nP such that the entry at column (i, p) and row (i\u2032, p\u2032) is equal to K(zp(xi), zp\u2032(xi\u2032)). Compute a factorization K = QQ> or an approximation K \u2248 QQ>, where Q \u2208 RnP\u00d7m.\n2. For each xi, construct patch matrix Z(xi) \u2208 RP\u00d7m whose p-th row is the (i, p)-th row of Q, where Z(\u00b7) is defined in Section 3.2.\n3. Solve the following optimization problem to obtain a matrix A\u0302 = (A\u03021, . . . , A\u0302d2):\nA\u0302 \u2208 argmin \u2016A\u2016\u2217\u2264R L\u0303(A) where L\u0303(A) := n\u2211 i=1 L (( tr(Z(xi)A1), . . . , tr(Z(xi)Ad2) ) ; yi ) . (12)\n4. Compute a rank-r approximation A\u0303 \u2248 U\u0302 V\u0302 > where U\u0302 \u2208 Rm\u00d7r and V\u0302 \u2208 RPd2\u00d7r.\nOutput: Return the predictor f\u0302ccnn(x) := ( tr(Z(x)A\u03021), . . . , tr(Z(x)A\u0302d2) ) and the convolutional layer output H(x) := U\u0302>(Z(x))>.\n\u2022 We can then compute the predictor fk(x) = tr(Z(x)Ak) via equation (6). Note that we do not explicitly need to compute the filter values hj(zp(x)) to compute the output under the CCNN.\nRetrieving filters. However, when we learn multi-layer CCNNs, we need to compute the filters explicitly. Recall from Section 3.1 that the column space of matrix A corresponds to parameters of the convolutional layer, and the row space of A corresponds to parameters of the output layer. Thus, once we obtain the parameter matrix A, we compute a rank-r approximation A \u2248 U\u0302 V\u0302 >. Then set the j-th filter hj to the mapping\nz 7\u2192 \u3008U\u0302j , Q\u2020v(z)\u3009 for any patch z \u2208 Rd1 , (11)\nwhere U\u0302j \u2208 Rm is the j-th column of matrix U\u0302 , and Q\u2020v(z) represents the feature vector for patch z. The matrix V\u0302 > encodes parameters of the output layer, thus doesn\u2019t appear in the filter expression (11). It is important to note that the filter retrieval is not unique, because the rankr approximation of the matrix A is not unique. One feasible way is to form the singular value decomposition A = U\u039bV >, then define U\u0302 to be the first r columns of U , and define V\u0302 > to be the first r rows of \u039bV >.\nWhen we apply all of the r filters to all patches of an input x \u2208 Rd0 , the resulting output is H(x) := U\u0302>(Z(x))> \u2014 this is an r \u00d7 P matrix whose element at row j and column p is equal to hj(zp(x))."}, {"heading": "3.3 Algorithm", "text": "The algorithm for learning a two-layer CCNN is summarized in Algorithm 1; it is a formalization of the steps described in Section 3.2. In order to solve the optimization problem (12), the simplest\napproach is to via projected gradient descent: At iteration t, using a step size \u03b7t > 0, it forms the new matrix At+1 based on the previous iterate At according to:\nAt+1 = \u03a0R ( At \u2212 \u03b7t \u2207AL\u0303(At) ) . (13)\nHere \u2207AL\u0303 denotes the gradient of the objective function defined in (12), and \u03a0R denotes the Euclidean projection onto the nuclear norm ball {A : \u2016A\u2016\u2217 \u2264 R}. This nuclear norm projection can be obtained by first computing the singular value decomposition of A, and then projecting the vector of singular values onto the `1-ball. This latter projection step can be carried out efficiently by the algorithm of Duchi et al. [16]. There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46]. All these algorithms can be executed in a stochastic fashion, so that each gradient step processes a mini-batch of examples.\nThe computational complexity of each iteration depends on the width m of the matrix Q. Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nystro\u0308m approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q \u2208 RnP\u00d7m such that K \u2248 QQ>. Typically, the parameter m is chosen to be much smaller than nP . In order to compute the matrix Q, the Nystro\u0308m approximation method takes O(m2nP ) time. The random feature approximation takes O(mnPd1) time, but can be improved to O(mnP log d1) time using the fast Hadamard transform [27]. The complexity of computing a gradient vector on a batch of b images is O(mPd2b). The complexity of projecting the parameter matrix onto the nuclear norm ball is O(min{m2Pd2,mP 2d22}). Thus, the approximate algorithms provide substantial speed-ups on the projected gradient descent steps."}, {"heading": "3.4 Theoretical results", "text": "In this section, we upper bound the generalization error of Algorithm 1, proving that it converges to the best possible generalization error of CNN. We focus on the binary classification case where the output dimension is d2 = 1. 2\nThe learning of CCNN requires a kernel function K. We consider kernel functions whose associated RKHS is large enough to contain any function taking the following form: z 7\u2192 q(\u3008w, z\u3009), where q is an arbitrary polynomial function and w \u2208 Rd1 is an arbitrary vector. As a concrete example, we consider the inverse polynomial kernel:\nK(z, z\u2032) := 1 2\u2212 \u3008z, z\u2032\u3009 , \u2016z\u20162 \u2264 1, \u2016z\u2032\u20162 \u2264 1. (14)\nThis kernel was studied by Shalev-Shwartz et al. [36] for learning halfspaces, and by Zhang et al. [48] for learning fully-connected neural networks. We also consider the Gaussian RBF kernel:\nK(z, z\u2032) := exp(\u2212\u03b3\u2016z \u2212 z\u2032\u201622), \u2016z\u20162 = \u2016z\u2032\u20162 = 1, \u03b3 > 0. (15)\nAs we show in Appendix A, the inverse polynomial kernel and the Gaussian kernel satisfy the above notion of richness. We focus on these two kernels for the theoretical analysis.\n2We can treat the multiclass case by performing a standard one-versus-all reduction to the binary case.\nLet f\u0302ccnn be the CCNN that minimizes the empirical risk (12) using one of the two kernels above. Our main theoretical result is that for suitably chosen activation functions, the generalization error of f\u0302ccnn is comparable to that of the best CNN model. In particular, the following theorem applies to activation functions \u03c3 of the following types:\n(a) arbitrary polynomial functions (e.g., used by [10, 29]).\n(b) sinusoid activation function \u03c3(t) := sin(t) (e.g., used by [39, 22]). (c) erf function \u03c3erf(t) := 2/ \u221a \u03c0 \u222b t 0 e \u2212z2dz, which represents an approximation to the sigmoid\nfunction (See Figure 2(a)). (d) a smoothed hinge loss \u03c3sh(t) := \u222b t \u2212\u221e 1 2(\u03c3erf(z) + 1)dz, which represents an approximation to\nthe ReLU function (See Figure 2(b)).\nTo understand why these activation functions pair with our choice of kernels, we consider polynomial expansions of the above activation functions: \u03c3(t) = \u2211\u221e j=0 ajt\nj , and note that the smoothness of these functions are characterized by the rate of their coefficients {aj}\u221ej=0 converging to zero. If \u03c3 is a polynomial in category (a), then the richness of the RKHS guarantees that it contains the class of filters activated by function \u03c3. If \u03c3 is a non-polynomial function in categories (b),(c),(d), then as Appendix A shows, the RKHS contains the filter only if the coefficients {aj}\u221ej=0 converge quickly enough to zero (the criterion depends on the choice of the kernel). Concretely, the inverse polynomial kernel is shown to capture all of the four categories of activations: they are referred as valid activation functions for the inverse polynomial kernel. The Gaussian kernel induces a smaller RKHS, and is shown to capture categories (a),(b), so that these functions are referred as valid activation functions for the Gaussian kernel. In contrast, the sigmoid function and the ReLU function are not valid for either kernel, because their polynomial expansions fail to converge quickly enough, or more intuitively speaking, because they are not smooth enough functions to be contained in the RKHS.\nWe are ready to state the main theoretical result. In the theorem statement, we use K(X) \u2208 RP\u00d7P to denote the random kernel matrix obtained from an input vector X \u2208 Rd0 drawn randomly from the population. More precisely, the (p, q)-th entry of K(X) is given by K(zp(X), zq(X)).\nTheorem 1. Assume that the loss function L(\u00b7; y) is L-Lipchitz continuous for every y \u2208 [d2] and that K is the inverse polynomial kernel or the Gaussian kernel. For any valid activation function \u03c3, there is a constant C\u03c3(B1) such that with the radius R := C\u03c3(B1)B2r, the expected generalization error is at most\nEX,Y [L(f\u0302ccnn(X);Y )] \u2264 inf f\u2208Fcnn\nEX,Y [L(f(X);Y )] + c LC\u03c3(B1)B2r \u221a log(nP ) EX [\u2016K(X)\u20162]\u221a\nn , (16)\nwhere c > 0 is a universal constant.\nProof sketch The proof of Theorem 1 consists of two parts: First, we consider a larger function class that contains the class of CNNs. This function class is defined as:\nFccnn := { x 7\u2192 r\u2217\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)) : r \u2217 <\u221e and r\u2217\u2211 j=1 \u2016\u03b1j\u20162\u2016hj\u2016H \u2264 C\u03c3(B1)B2d2 } .\nwhere \u2016\u00b7\u2016H is the norm of the RKHS associated with the kernel. This new function class relaxes the class of CNNs in two ways: 1) the filters are relaxed to belong to the RKHS, and 2) the `2-norm bounds on the weight vectors are replaced by a single constraint on \u2016\u03b1j\u20162 and \u2016hj\u2016H. We prove the following property for the predictor f\u0302ccnn: it must be an empirical risk minimizer of Fccnn, even though the algorithm has never explicitly optimized the loss within this nonparametric function class.\nSecond, we characterize the Rademacher complexity of this new function class Fccnn, proving an upper bound for it based on the matrix concentration theory. Combining this bound with the classical Rademacher complexity theory [4], we conclude that the generalization loss of f\u0302ccnn converges to the least possible generalization error of Fccnn. The later loss is bounded by the generalization loss of CNNs (because Fcnn \u2286 Fccnn), which establishes the theorem. See Appendix C for the full proof of Theorem 1.\nRemark on activation functions. It is worth noting that the quantity C\u03c3(B1) depends on the activation function \u03c3, and more precisely, depends on the convergence rate of the polynomial expansion of \u03c3. Appendix A shows that if \u03c3 is a polynomial function of degree `, then C\u03c3(B1) = O(B`1). If \u03c3 is the sinusoid function, the erf function or the smoothed hinge loss, then the quantity C\u03c3(B1) will be exponential in B1. In an algorithmic perspective, we don\u2019t need to know the activation function for executing Algorithm 1. In a theoretical perspective, however, the choice of \u03c3 is relevant from the point of Theorem 1 to compare f\u0302ccnn with the best CNN, whose representation power is characterized by the choice of \u03c3. Therefore, if a CNN with a low-degree polynomial \u03c3 performs well on a given task, then CCNN also enjoys correspondingly strong generalization. Empirically, this is actually borne out: in Section 5, we show that the quadratic activation function performs almost as well as the ReLU function for digit classification.\nRemark on parameter sharing. In order to demonstrate the importance of parameter sharing, consider a CNN without parameter sharing, so that we have filter weights wj,p for each filter index\nj and patch index p. With this change, the new CNN output (2) is\nf(x) = r\u2211 j=1 P\u2211 p=1 \u03b1j,p\u03c3(w > j,pzp(x)), where \u03b1j,p \u2208 R and wj,p \u2208 Rd1 . (17)\nNote that the hidden layer of this new network has P times more parameters than that of the convolutional neural network with parameter sharing. These networks without parameter sharing can be learned by the recursive kernel method proposed by Zhang et al. [48]. This paper shows that under the norm constraints \u2016wj\u20162 \u2264 B\u20321 and \u2211r j=1 \u2211P p=1 |\u03b1j,p| \u2264 B\u20322, the excess risk of the\nrecursive kernel method is at most O(LC\u03c3(B\u20321)B\u20322 \u221a Kmax/n), where Kmax = maxz:\u2016z\u20162\u22641K(z, z) is the maximal value of the kernel function. Plugging in the norm constraints of the function class Fcnn, we have B\u20321 = B1 and B\u20322 = B2r \u221a P . Thus, the expected risk of the estimated f\u0302 is bounded by:\nEX,Y [L(f\u0302(X);Y )] \u2264 inf f\u2208Fcnn\nEX,Y [L(f(X);Y )] + c LC\u03c3(B1)B2r \u221a PKmax\u221a\nn . (18)\nComparing this bound to Theorem 1, we see that (apart from the logarithmic terms) they differ in the multiplicative factors of \u221a P Kmax versus \u221a E[\u2016K(X)\u20162]. Since the matrix K(X) is P -dimensional, we have\n\u2016K(X)\u20162 \u2264 max p\u2208[P ] \u2211 q\u2208[P ] |K(zp(X), zq(X))| \u2264 P Kmax.\nThis demonstrates that \u221a P Kmax is always greater than \u221a E[\u2016K(X)\u20162]. In general, the first term\ncan be up to factor of \u221a P times greater, which implies that the sample complexity of the recursive kernel method is up to P times greater than that of the CCNN. This difference corresponds to the fact that the recursive kernel method learns a model with P times more parameters. Although comparing the upper bounds doesn\u2019t rigorously show that one method is better than the other, it gives the right intuition for understanding the importance of parameter sharing."}, {"heading": "4 Learning multi-layer CCNNs", "text": "In this section, we describe a heuristic method for learning CNNs with more layers. The idea is to estimate the parameters of the convolutional layers incrementally from bottom to the top. Before presenting the multi-layer algorithm, we present two extensions, average pooling and multi-channel inputs.\nAverage pooling. Average pooling is a technique to reduce the output dimension of the convolutional layer from dimensions P \u00d7 r to dimensions P \u2032 \u00d7 r with P \u2032 < P . Suppose that the filter hj applied to all the patch vectors produces the output vector Hj(x) := (hj(z1(x)), \u00b7 \u00b7 \u00b7 , hj(zP (x))) \u2208 RP\u00d7r. Average pooling produces a P \u2032 \u00d7 r matrix, where each row is the average of the rows corresponding to a small subset of the P patches. For example, we might average every pair of adjacent patches, which would produce P \u2032 = P/2 rows. The operation of average pooling can be represented via left-multiplication using a fixed matrix G \u2208 RP \u2032\u00d7P .\nAlgorithm 2: Learning multi-layer CCNNs\nInput:Data {(xi, yi)}ni=1, kernel function K, number of layers m, regularization parameters R1, . . . , Rm, number of filters r1, . . . , rm. Define H1(x) = x. For each layer s = 2, . . . ,m:\n\u2022 Train a two-layer network by Algorithm 1, taking {(Hs\u22121(xi), yi)}ni=1 as training examples and Rs, rs as parameters. Let Hs be the output of the convolutional layer and f\u0302s be the predictor.\nOutput: Predictor f\u0302m and the top convolutional layer output Hm.\nFor the CCNN model, if we apply average pooling after the convolutional layer, then the kth output of the CCNN model becomes tr(GZ(x)Ak) where Ak \u2208 Rm\u00d7P \u2032 is the new (smaller) parameter matrix. Thus, performing a pooling operation requires only replacing every matrix Z(xi) in problem (12) by the pooled matrix GZ(xi). Note that the linearity of the CCNN allows us to effectively pool before convolution, even though for the CNN, pooling must be done after applying the nonlinear filters. The resulting ERM problem is still convex, and the number of parameters have been reduced by P/P \u2032-fold. Although average pooling is straightforward to incorporate in our framework, unfortunately, max pooling does not fit into our framework due to its nonlinearity.\nProcessing multi-channel inputs. If our input has C channels (corresponding to RGB colors, for example), then the input becomes a matrix x \u2208 RC\u00d7d0 . The c-th row of matrix x, denoted by x[c] \u2208 Rd0 , is a vector representing the c-th channel. We define the multi-channel patch vector as a concatenation of patch vectors for each channel:\nzp(x) := (zp(x[1]), . . . , zp(x[C])) \u2208 RCd1 .\nThen we construct the feature matrix Z(x) using the concatenated patch vectors {zp(x)}Pp=1. From here, everything else of Algorithm 1 remains the same. We note that this approach learns a convex relaxation of filters taking the form \u03c3( \u2211C c=1\u3008wc, zp(x[c])\u3009), parametrized by the vectors {wc}Cc=1.\nMulti-layer CCNN. Given these extensions, we are ready to present the algorithm for learning multi-layer CCNNs. The algorithm is summarized in Algorithm 2. For each layer s, we call Algorithm 1 using the output of previous convolutional layers as input\u2014note that this consists of r channels (one from each previous filter) and thus we must use the multi-channel extension. Algorithm 2 outputs a new convolutional layer along with a prediction function, which is kept only at the last layer. We optionally use averaging pooling after each successive layer. to reduce the output dimension of the convolutional layers."}, {"heading": "5 Experiments", "text": "In this section, we compare the CCNN approach with other methods. The results are reported on the MNIST dataset and its variations for digit recognition, and on the CIFAR-10 dataset for object classification."}, {"heading": "5.1 MNIST and variations", "text": "Since the basic MNIST digits are relatively easy to classify, we also consider more challenging variations [43]. These variations are known to be hard for methods without a convolution mechanism (for instance, see the paper [44]). Figure 3 shows a number of sample images from these different datasets. All the images are of size 28 \u00d7 28. For all datasets, we use 10,000 images for training, 2,000 images for validation and 50,000 images for testing. This 10k/2k/50k partitioning is standard for MNIST variations [43].\nImplementation details. For the CCNN method and the baseline CNN method, we train twolayer and three-layer models respectively. The models with k convolutional layers are denoted by CCNN-k and CNN-k. Each convolutional layer is constructed on 5 \u00d7 5 patches with unit stride, followed by 2\u00d7 2 average pooling. The first and the second convolutional layers contains 16 and 32 filters, respectively. The loss function is chosen as the 10-class logistic loss. We use the Gaussian kernel K(z, z\u2032) = exp(\u2212\u03b3\u2016z \u2212 z\u2032\u201622) and set hyperparameters \u03b3 = 0.2 for the first convolutional layer and \u03b3 = 2 for the second. The feature matrix Z(x) is constructed via random feature approximation [33] with dimension m = 500 for the first convolutional layer and m = 1000 for the second. Before training each CCNN layer, we preprocess the input vectors zp(xi) using local contrast normalization and ZCA whitening [12]. The convex optimization problem is solved by projected SGD with mini-batches of size 50.\nAs a baseline approach, the CNN models are activated by the ReLU function \u03c3(t) = max{0, t} or the quadratic function \u03c3(t) = t2. We train them using mini-batch SGD. The input images are preprocessed by global contrast normalization and ZCA whitening [see, e.g. 40]. We compare our method against several alternative baselines. The CCNN-1 model is compared against an SVM with the Gaussian RBF kernel (SVMrbf ) and a fully connected neural network with one hidden layer (NN-1). The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].\nResults. Table 1 shows the classification errors on the test set. The models are grouped with respect to the number of layers that they contain. For models with one convolutional layer, the\nerrors of CNN-1 are significantly lower than that of NN-1, highlighting the benefits of parameter sharing. The CCNN-1 model outperforms CNN-1 on all datasets. For models with two or more hidden layers, the CCNN-2 model outperforms CNN-2 on all datasets, and is competitive against the state-of-the-art. In particular, it achieves the best accuracy on the rand, img and img+rot dataset, and is comparable to the state-of-the-art on the remaining two datasets.\nIn order to understand the key factors that affect the training of CCNN filters, we evaluate five variants:\n(1) replace the Gaussian kernel by a linear kernel; (2) remove the ZCA whitening in preprocessing; (3) use fewer random features (m = 200 rather than m = 500) to approximate the kernel matrix; (4) regularize the parameter matrix by the Frobenius norm instead of the nuclear norm; (5) stop the mini-batch SGD early before it converges.\nWe evaluate the obtained filters by training a second convolutional layer on top of them, then evaluating the classification error on the hardest dataset img+rot. As Table 2 shows, switching to the linear kernel or removing the ZCA whitening significantly degenerates the performance. This is because that both variants equivalently modify the kernel function. Decreasing the number of random features also has a non-negligible effect, as it makes the kernel approximation less accurate. These observations highlight the impact of the kernel function. Interestingly, replacing the nuclear\nnorm by a Frobenius norm or stopping the algorithm early doesn\u2019t hurt the performance. To see their impact on the parameter matrix, we compute the effective rank (ratio between the nuclear norm and the spectral norm, see [18]) of matrix A\u0302. The effective rank obtained by the last two variants are equal to 77 and 24, greater than that of the original CCNN (equal to 12). It reveals that the last two variants have damaged the algorithm\u2019s capability of enforcing a low-rank solution. However, the CCNN filters are retrieved from the top-r singular vectors of the parameter matrix, hence the performance will remain stable as long as the top singular vectors are robust to the variation of the matrix.\nIn Section 3.4, we showed that if the activation function is a polynomial function, then the CCNN requires lower sample complexity to match the performance of the best possible CNN. More precisely, if the activation function is degree-` polynomial, then C\u03c3(B) in the upper bound will be controlled by O(B`). This motivates us to study the performance of low-degree polynomial activations. Table 1 shows that the CNN-2 model with a quadratic activation function achieves error rates comparable to that with a ReLU activation: CNN-2 (Quad) outperforms CNN-2 (ReLU) on the basic and rand datasets, and is only slightly worse on the rot and img dataset. Since the performance of CCNN matches that of the best possible CNN, the good performance of the quadratic activation in part explains why the CCNN is also good."}, {"heading": "5.2 CIFAR-10", "text": "In order to test the capability of CCNN in complex classification tasks, we report its performance on the CIFAR-10 dataset [24]. The dataset consists of 60000 images divided into 10 classes. Each image has 32\u00d732 pixels in RGB colors. We use 50k images for training and 10k images for testing.\nImplementation details. We train CNN and CCNN models with two, three, and four layers Each convolutional layer is constructed on 5\u00d7 5 patches with unit stride, followed by 3\u00d7 3 average pooling with two-pixel stride. We train 32, 32, 64 filters for the three convolutional layers from bottom to the top. For any s\u00d7 s input, zero pixels are padded on its borders so that the input size becomes (s+4)\u00d7 (s+4), and the output size of the convolutional layer is (s/2)\u00d7 (s/2). The CNNs are activated by the ReLU function. For CCNNs, we use the Gaussian kernel with hyperparameter \u03b3 = 1, 2, 2 (for the three convolutional layers). The feature matrix Z(x) is constructed via random feature approximation with dimension m = 2000. The preprocessing steps are the same as in the MNIST experiments. It was known that the generalization performance of the CNN can be improved by training on random crops of the original image [25], so we train the CNN on random 24\u00d724 patches of the image, and test on the central 24\u00d724 patch. We also apply random cropping to training the the first and the second layer of the CCNN.\nWe compare the CCNN against other baseline methods that don\u2019t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].\nResults. We report the classification errors on in Table 3. For models of all depths, the CCNN model outperforms the CNN model. The CCNN-2 and the CCNN-3 model also outperform the three baseline methods. The advantage of the CCNN is substantial for learning two-layer networks, when the optimality guarantee of CCNN holds. The performance improves as more layers are stacked, but as we observe in Table 3, the marginal gain of CCNN diminishes as the network grows deeper. We suspect that this is due to the greedy fashion in which the CCNN layers are constructed. Once\ntrained, the low-level filters of the CCNN are no longer able to adapt to the final classifier. In contrast, the low-level filters of the CNN model are continuously adjusted via backpropagation.\nIt is worth noting that the performance of the CNN can be further improved by adding more layers, switching from average pooling to max pooling, and being regularized by local response normalization and dropout (see, e.g. [25]). The figures in Table 3 are by no means the state-ofthe-art result on CIFAR-10. However, it does demonstrate that the convex relaxation is capable of improving the performance of convolutional neural networks. For future work, we propose to study a better way for convexifying deep CNNs.\nIn Figure 4, we compare the computational efficiency of CNN-3 to its convexified version CCNN3. Both models are trained by mini-batch SGD (with batchsize equal to 50) on a single processor. We optimized the choice of step-size for each algorithm. From the plot, it is easy to identify the three stages of the CCNN-3 curve for training the three convolutional layers from bottom to the top. We also observe that the CCNN converges faster than the CNN. More precisely, the CCNN takes half the runtime of the CNN to reach an error rate of 28%, and one-fifth of the runtime to reach an error rate of 23%. The per-iteration cost for training the first layer of CCNN is about 109% of the per-iteration cost of CNN, but the per-iteration cost for training the remaining two layers are about 18% and 7% of that of CNN. Thus, training CCNN scales well to large datasets.\nTraining a CCNN on top of a CNN. Instead of training a CCNN from scratch, we can also train CCNNs on top of existing CNN layers. More concretely, once a CNN-k model is obtained, we train a two-layer CCNN by taking the (k\u2212 1)-th hidden layer of CNN as input. This approach preserves the low-level features learned by CNN, only convexifying its top convolutional layer. The underlying motivation is that the traditional CNN is good at learning low-level features through\nbackpropagation, while the CCNN is optimal in learning two-layer networks. In this experiment, we convexify the top convolutional layer of CNN-2 and CNN-3 using the CCNN approach, with a smaller Gaussian kernel parameter (i.e. \u03b3 = 0.1) and keeping other hyperparameters the same as in the training of CCNN-2 and CCNN-3. The results are shown in Table 4. The convexified CNN achieves better accuracy on all network depths. It is worth noting that the time for training a convexified layer is only a small fraction of the time for training the original CNN."}, {"heading": "6 Related work", "text": "With the empirical success of deep neural networks, there has been an increasing interest in theoretical understanding. Bengio et al. [5] showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters. This perspective encourages incrementally adding neurons to the network, whose generalization error was studied by Bach [3]. Zhang et al. [47] propose a polynomial-time ensemble method for learning fully-connected neural networks, but their approach handles neither parameter sharing nor the convolutional setting. Other relevant works for learning fully-connected networks include [35, 23, 29]. Aslan et al. [1, 2] propose a method for learning multi-layer latent variable models. They showed that for certain activation functions, the proposed method is a convex relaxation for learning the fully-connected neural network.\nAnother line of work is devoted to understanding the energy landscape of a neural network. Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11]. If this property holds, then gradient descent can find a solution that is \u201cgood enough\u201d. Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20]. However, these results are not applicable to a CNN, since the underlying assumptions are not satisfied by CNNs.\nPast work has studied learning translation invariant features without backpropagation. Mairal et al. [30] present convolutional kernel networks. They propose a translation-invariant kernel whose feature mapping can be approximated by a composition of the convolution, non-linearity and pooling operators, obtained through unsupervised learning. However, this method is not equipped with the optimality guarantees that we have provided for CCNNs in this paper, even for learning one convolution layer. The ScatNet method [8] uses translation and deformation-invariant filters constructed by wavelet analysis; however, these filters are independent of the data, unlike the analysis in this paper. Daniely et al. [13] show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from a random initialization."}, {"heading": "7 Conclusion", "text": "In this paper, we have shown how convex optimization can be used to efficiently optimize CNNs as well as understand them statistically. Our convex relaxation consists of two parts: the nuclear norm relaxation for handling parameter sharing, and the RKHS relaxation for handling non-linearity. For the two-layer CCNN, we proved that its generalization error converges to that of the best possible two-layer CNN. We handled multi-layer CCNNs only heuristically, but observed that adding more\nlayers improves the performance in practice. On real data experiments, we demonstrated that CCNN outperforms the traditional CNN of the same depth, is computationally efficient, and can be combined with the traditional CNN to achieve better performance. A major open problem is to formally study the convex relaxation of deep CNNs."}, {"heading": "Acknowledgements", "text": "This work was partially supported by Office of Naval Research MURI grant DOD-002888, Air Force Office of Scientific Research Grant AFOSR-FA9550-14-1-001, Office of Naval Research grant ONR-N00014, National Science Foundation Grant CIF-31712-23800, as well as a Microsoft Faculty Research Award to the second author.\nA Inverse polynomial kernel and Gaussian kernel\nIn this appendix, we describe the properties of the two types of kernels \u2014 the inverse polynomial kernel (14) and the Gaussian RBF kernel (15). We prove that the associated reproducing kernel Hilbert Spaces (RKHS) of these kernels contain filters taking the form h : z 7\u2192 \u03c3(\u3008w, z\u3009) for particular activation functions \u03c3.\nA.1 Inverse polynomial kernel\nWe first verify that the function (14) is a kernel function. This holds since that we can find a mapping \u03d5 : Rd1 \u2192 `2(N) such that K(z, z\u2032) = \u3008\u03d5(z), \u03d5(z\u2032)\u3009. We use zi to represent the i-th coordinate of an infinite-dimensional vector z. The (k1, . . . , kj)-th coordinate of \u03d5(z), where j \u2208 N and k1, . . . , kj \u2208 [d1], is defined as 2\u2212 j+1 2 xk1 . . . xkj . By this definition, we have\n\u3008\u03d5(x), \u03d5(y)\u3009 = \u221e\u2211 j=0 2\u2212(j+1) \u2211 (k1,...,kj)\u2208[d1]j zk1 . . . zkjz \u2032 k1 . . . z \u2032 kj . (19)\nSince \u2016z\u20162 \u2264 1 and \u2016z\u2032\u20162 \u2264 1, the series on the right-hand side is absolutely convergent. The inner term on the right-hand side of equation (19) can be simplified to\u2211\n(k1,...,kj)\u2208[d1]j zk1 . . . zkjz\n\u2032 k1 . . . z \u2032 kj = (\u3008z, z\u2032\u3009)j . (20)\nCombining equations (19) and (20) and using the fact that |\u3008z, z\u2032\u3009| \u2264 1, we have\n\u3008\u03d5(z), \u03d5(z\u2032)\u3009 = \u221e\u2211 j=0 2\u2212(j+1)(\u3008z, z\u2032\u3009)j (i)= 1 2\u2212 \u3008z, z\u2032\u3009 = K(z, z\u2032),\nwhich verifies that K is a kernel function and \u03d5 is the associated feature map. Next, we prove that the associated RKHS contains the class of nonlinear filters. The lemma was proved by Zhang et al. [48]. We include the proof to make the paper self-contained.\nLemma 1. Assume that the function \u03c3(x) has a polynomial expansion \u03c3(t) = \u2211\u221e\nj=0 ajt j. Let C\u03c3(\u03bb) := \u221a\u2211\u221e j=0 2 j+1a2j\u03bb\n2j. If C\u03c3(\u2016w\u20162) <\u221e, then the RKHS induced by the inverse polynomial kernel contains function h : z 7\u2192 \u03c3(\u3008w, z\u3009) with Hilbert norm \u2016h\u2016H = C\u03c3(\u2016w\u20162).\nProof. Let \u03d5 be the feature map that we have defined for the polynomial inverse kernel. We define vector w \u2208 `2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j \u2208 N and k1, . . . , kj \u2208 [d1], is equal to 2\nj+1 2 ajwk1 . . . wkj . By this definition, we have\n\u03c3(\u3008w, z\u3009) = \u221e\u2211 t=0 aj(\u3008w, z\u3009)j = \u221e\u2211 j=0 aj \u2211 (k1,...,kj)\u2208[d1]j wk1 . . . wkjzk1 . . . zkj = \u3008w,\u03d5(z)\u3009, (21)\nwhere the first equation holds since \u03c3(x) has a polynomial expansion \u03c3(x) = \u2211\u221e\nj=0 ajx j , the second\nby expanding the inner product, and the third by definition of w and \u03d5(z). The `2-norm of w is equal to:\n\u2016w\u201622 = \u221e\u2211 j=0 2j+1a2j \u2211 (k1,...,kj)\u2208[d1]j w2k1w 2 k2 \u00b7 \u00b7 \u00b7w 2 kj = \u221e\u2211 j=0 2j+1a2j\u2016w\u2016 2j 2 = C 2 \u03c3(\u2016w\u20162) <\u221e. (22)\nBy the basic property of the RKHS, the Hilbert norm of h is equal to the `2-norm of w. Combining equations (21) and (22), we conclude that h \u2208 H and \u2016h\u2016H = \u2016w\u20162 = C\u03c3(\u2016w\u20162).\nAccording to Lemma 1, it suffices to upper bound C\u03c3(\u03bb) for a particular activation function \u03c3. To make C\u03c3(\u03bb) < \u221e, the coefficients {aj}\u221ej=0 must quickly converge to zero, meaning that the activation function must be sufficiently smooth. For polynomial functions of degree `, the definition of C\u03c3 implies that C\u03c3(\u03bb) = O(\u03bb`). For the sinusoid activation \u03c3(t) := sin(t), we have\nC\u03c3(\u03bb) = \u221a\u221a\u221a\u221a \u221e\u2211 j=0\n22j+2\n((2j + 1)!)2 \u00b7 (\u03bb2)2j+1 \u2264 2e\u03bb2 .\nFor the erf function and the smoothed hinge loss function defined in Section 3.4, Zhang et al. [48, Proposition 1] proved that C\u03c3(\u03bb) = O(ec\u03bb 2 ) for universal numerical constant c > 0.\nA.2 Gaussian kernel\nThe Gaussian kernel also induces an RKHS that contains a particular class of nonlinear filters. The proof is similar to that of Lemma 1.\nLemma 2. Assume that the function \u03c3(x) has a polynomial expansion \u03c3(t) = \u2211\u221e\nj=0 ajt j. Let C\u03c3(\u03bb) := \u221a\u2211\u221e j=0 j!e2\u03b3 (2\u03b3)j a2j\u03bb\n2j. If C\u03c3(\u2016w\u20162) < \u221e, then the RKHS induced by the Gaussian kernel contains the function h : z 7\u2192 \u03c3(\u3008w, z\u3009) with Hilbert norm \u2016h\u2016H = C\u03c3(\u2016w\u20162).\nProof. When \u2016z\u20162 = \u2016z\u2032\u20162 = 1, It is well-known [see, e.g. 41] the following mapping \u03d5 : Rd1 \u2192 `2(N) is a feature map for the Gaussian RBF kernel: the (k1, . . . , kj)-th coordinate of \u03d5(z), where j \u2208 N and k1, . . . , kj \u2208 [d1], is defined as e\u2212\u03b3((2\u03b3)j/j!)1/2xk1 . . . xkj . Similar to equation (21), we define a vector w \u2208 `2(N) as follow: the (k1, . . . , kj)-th coordinate of w, where j \u2208 N and k1, . . . , kj \u2208 [d1], is equal to e\u03b3((2\u03b3)j/j!)\u22121/2ajwk1 . . . wkj . By this definition, we have\n\u03c3(\u3008w, z\u3009) = \u221e\u2211 t=0 aj(\u3008w, z\u3009)j = \u221e\u2211 j=0 aj \u2211 (k1,...,kj)\u2208[d1]j wk1 . . . wkjzk1 . . . zkj = \u3008w,\u03d5(z)\u3009. (23)\nThe `2-norm of w is equal to:\n\u2016w\u201622 = \u221e\u2211 j=0 j!e2\u03b3 (2\u03b3)j a2j \u2211 (k1,...,kj)\u2208[d1]j w2k1w 2 k2 \u00b7 \u00b7 \u00b7w 2 kj = \u221e\u2211 j=0 j!e2\u03b3 (2\u03b3)j a2j\u2016w\u2016 2j 2 = C 2 \u03c3(\u2016w\u20162) <\u221e. (24)\nCombining equations (21) and (22), we conclude that h \u2208 H and \u2016h\u2016H = \u2016w\u20162 = C\u03c3(\u2016w\u20162).\nComparing Lemma 1 and Lemma 2, we find that the Gaussian kernel imposes a stronger condition on the smoothness of the activation function. For polynomial functions of degree `, we still have C\u03c3(\u03bb) = O(\u03bb`). For the sinusoid activation \u03c3(t) := sin(t), it can be verified that\nC\u03c3(\u03bb) = \u221a\u221a\u221a\u221ae2\u03b3 \u221e\u2211 j=0\n1 (2j + 1)! \u00b7 (\u03bb2 2\u03b3 )2j+1 \u2264 e\u03bb2/(4\u03b3)+\u03b3 .\nHowever, the value of C\u03c3(\u03bb) is infinite when \u03c3 is the erf function or the smoothed hinge loss, meaning that the Gaussian kernel\u2019s RKHS doesn\u2019t contain filters activated by these two functions."}, {"heading": "B Convex relaxation for nonlinear activation", "text": "In this appendix, we provide a detailed derivation of the relaxation for nonlinear activation functions that we previously sketched in Section 3.2. Recall that the filter output is \u03c3(\u3008wj , z\u3009). Appendix A shows that given a sufficiently smooth activation function \u03c3, we can find some kernel function K : Rd1\u00d7Rd1 \u2192 R and a feature map \u03d5 : Rd1 \u2192 `2(N) satisfying K(z, z\u2032) \u2261 \u3008\u03d5(z), \u03d5(z\u2032)\u3009, such that\n\u03c3(\u3008wj , z\u3009) \u2261 \u3008wj , \u03d5(z)\u3009. (25)\nHere wj \u2208 `2(N) is a countable-dimensional vector and \u03d5 := (\u03d51, \u03d52, . . . ) is a countable sequence of functions. Moreover, the `2-norm of wj is bounded as \u2016wj\u20162 \u2264 C\u03c3(\u2016wj\u20162) for a monotonically increasing function C\u03c3 that depends on the kernel (see Lemma 1 and Lemma 2). As a consequence, we may use \u03d5(z) as the vectorized representation of the patch z, and use wj as the linear transformation weights, then the problem is reduced to training a CNN with the identity activation function.\nThe filter is parametrized by an infinite-dimensional vector wj . Our next step is to reduce the original ERM problem to a finite-dimensional one. In order to minimize the empirical risk, one only needs to concern the output on the training data, that is, the output of \u3008wj , \u03d5(zp(xi))\u3009 for all (i, p) \u2208 [n] \u00d7 [P ]. Let T be the orthogonal projector onto the linear subspace spanned by the vectors {\u03d5(zp(xi)) : (i, p) \u2208 [n]\u00d7 [P ]}. Then we have\n\u2200 (i, p) \u2208 [n]\u00d7 [P ] : \u3008wj , \u03d5(zp(xi))\u3009 = \u3008wj , T\u03d5(zp(xi))\u3009 = \u3008Twj , \u03d5(zp(xi))\u3009.\nThe last equation follows since the orthogonal projector T is self-adjoint. Thus, for empirical risk minimization, we can without loss of generality assume that wj belongs to the linear subspace spanned by {\u03d5(zp(xi)) : (i, p) \u2208 [n]\u00d7 [P ]} and reparametrize it by:\nwj = \u2211\n(i,p)\u2208[n]\u00d7[P ]\n\u03b2j,(i,p)\u03d5(zp(xi)). (26)\nLet \u03b2j \u2208 RnP be a vector whose whose (i, p)-th coordinate is \u03b2j,(i,p). In order to estimate wj , it suffices to estimate the vector \u03b2j . By definition, the vector satisfies the relation \u03b2 > j K\u03b2j = \u2016wj\u201622, where K is the nP \u00d7 nP kernel matrix defined in Section 3.2. As a consequence, if we can find a matrix Q such that QQ> = K, then we have the norm constraint\n\u2016Q>\u03b2j\u20162 = \u221a \u03b2>j K\u03b2j = \u2016wj\u20162 \u2264 C\u03c3(\u2016wj\u20162) \u2264 C\u03c3(B). (27)\nLet v(z) \u2208 RnP be a vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). Then by equations (25) and (26), the filter output can be written as\n\u03c3 ( \u3008wj , z\u3009 ) \u2261 \u3008wj , \u03d5(z)\u3009 \u2261 \u3008\u03b2j , v(z)\u3009. (28)\nFor any patch zp(xi) in the training data, the vector v(zp(xi)) belongs to the column space of the kernel matrix K. Therefore, letting Q\u2020 represent the pseudo-inverse of matrix Q, we have\n\u2200 (i, p) \u2208 [n]\u00d7 [P ] : \u3008\u03b2j , v(zp(xi))\u3009 = \u03b2>j QQ\u2020v(zp(xi)) = \u3008(Q>)\u2020Q>\u03b2j , v(zp(xi))\u3009.\nIt means that if we replace the vector \u03b2j on the right-hand side of equation (28) by the vector (Q>)\u2020Q>\u03b2j , then it won\u2019t change the empirical risk. Thus, for ERM we can parametrize the filters by\nhj(z) := \u3008(Q>)\u2020Q>\u03b2j , v(z)\u3009 = \u3008Q\u2020v(z), Q>\u03b2j\u3009. (29)\nLet Z(x) be an P \u00d7 nP matrix whose p-th row is equal to Q\u2020v(zp(x)). Similar to the steps in equation (6), we have\nfk(x) = r\u2211 j=1 \u03b1>k,jZ(x)K 1/2\u03b2j = tr ( Z(x) ( r\u2211 j=1 K1/2\u03b2j\u03b1 > k,j )) = tr(Z(x)Ak),\nwhere Ak := \u2211r j=1Q >\u03b2j\u03b1 > k,j . If we let A := (A1, . . . , Ad2) denote the concatenation of these matrices, then this larger matrix satisfies the constraints:\nConstraint (C1): max j\u2208[r] \u2016Q>\u03b2j\u20162 \u2264 C\u03c3(B1) and max (k,j)\u2208[d2]\u00d7[r] \u2016\u03b1k,j\u20162 \u2264 B2.\nConstraint (C2): The matrix A has rank at most r.\nWe relax these two constraints to the nuclear norm constraint: \u2016A\u2016\u2217 \u2264 C\u03c3(B1)B2r \u221a d2. (30)\nBy comparing constraints (8) and (30), we see that the only difference is that the term B1 in the norm bound has been replaced by C\u03c3(B1). This change is needed because we have used the kernel trick to handle nonlinear activation functions."}, {"heading": "C Proof of Theorem 1", "text": "Since the output is one-dimensional in this case, we can adopt the simplified notation (A,\u03b1j,p) for the matrix (A1, \u03b11,j,p). Letting H be the RKHS associated with the kernel function K, and letting \u2016\u00b7\u2016H be the associated Hilbert norm, consider the function class\nFccnn := { x 7\u2192 r\u2217\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)) : r \u2217 <\u221e and r\u2217\u2211 j=1 \u2016\u03b1j\u20162\u2016hj\u2016H \u2264 C\u03c3(B1)B2d2 } . (31)\nHere \u03b1j,p denotes the p-th entry of vector \u03b1j \u2208 RP , whereas the quantity C\u03c3(B1) only depends on B1 and the activation function \u03c3. The following lemma shows that the function class Fccnn is rich enough so that it contains family of CNN predictors as a subset. The reader should recall the notion of a valid activation function, as defined prior to the statement of Theorem 1.\nLemma 3. For any valid activation function \u03c3, there is a quantity C\u03c3(B1), depending only on B1 and \u03c3, such that Fcnn \u2282 Fccnn.\nSee Appendix C.1 for the proof.\nNext, we connect the function class Fccnn to the CCNN algorithm. Recall that f\u0302ccnn is the predictor trained by the CCNN algorithm. The following lemma shows that f\u0302ccnn is an empirical risk minimizer within Fccnn.\nLemma 4. With the CCNN hyper-parameter R = C\u03c3(B1)B2d2, the predictor f\u0302ccnn is guaranteed to satisfy the inclusion\nf\u0302ccnn \u2208 arg min f\u2208Fccnn n\u2211 i=1 L(f(xi); yi).\nSee Appendix C.2 for the proof.\nOur third lemma shows that the function class Fccnn is not \u201ctoo big\u201d, which we do by upper bounding its Rademacher complexity. The Rademacher complexity of a function class F = {f : X \u2192 R} with respect to n i.i.d. samples {Xi}ni=1 is given by\nRn(F) := EX, [ sup f\u2208F 1 n n\u2211 i=1 if(Xi) ] ,\nwhere { i}ni=1 are an i.i.d. sequence of uniform {\u22121,+1}-valued variables. Rademacher complexity plays an important role in empirical process theory, and in particular can be used to bound the generalization loss of our empirical risk minimization problem. We refer the reader to Bartlett and Mendelson [4] for an introduction to the theoretical properties of Rademacher complexity.\nThe following lemma involves the kernel matrix K(x) \u2208 RP\u00d7P whose (i, j)-th entry is equal to K(zi(x), zj(x)), as well as the expectation E[\u2016K(X)\u20162] of the spectral norm of this matrix when X is drawn randomly.\nLemma 5. There is a universal constant c such that\nRn(Fccnn) \u2264 c C\u03c3(B1)B2r \u221a log(nP )E[\u2016K(X)\u20162]\u221a n . (32)\nSee Appendix C.3 for the proof of this claim.\nCombining Lemmas 3 through 5 allows us to compare the CCNN predictor f\u0302ccnn against the best model in the CNN class. Lemma 4 shows that f\u0302ccnn is the empirical risk minimizer within function class Fccnn. Thus, the theory of Rademacher complexity [4] guarantees that\nE[L(Fccnn(X);Y )] \u2264 inf f\u2208Fccnn E[L(f(x); y)] + 2L \u00b7 Rn(Fccnn) + c\u221a n , (33)\nwhere c is a universal constant. By Lemma 3, we have\ninf f\u2208Fccnn E[L(f(X);Y )] \u2264 inf f\u2208Fcnn E[L(f(X);Y )].\nPlugging this upper bound into inequality (33) and applying Lemma 5 completes the proof.\nC.1 Proof of Lemma 3\nWith the activation functions specified in the lemma statement, Lemma 1 and Lemma 2 show that there is a quantity C\u03c3(B1), such any filter of CNN belongs to the reproducing kernel Hilbert space H and its Hilbert norm is bounded by C\u03c3(B1). As a consequence, any function f \u2208 Fcnn can be represented by\nf(x) := r\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)) where \u2016hj\u2016H \u2264 C\u03c3(B1) and \u2016\u03b1j\u20162 \u2264 B2.\nIt is straightforward to verify that function f satisfies the constraint in equation (31), and consequently belongs to Fccnn.\nC.2 Proof of Lemma 4 Let CR denote the function class { x 7\u2192 tr(Z(x)A) : \u2016A\u2016\u2217 \u2264 R } . We first prove that CR \u2282 Fccnn. Consider an arbitrary function fA(x) : = tr(Z(x)A) belonging to CR. Note that the matrix A has a singular value decomposition (SVD) A = \u2211r\u2217 j=1 \u03bbjwju > j for some r\n\u2217 < \u221e, where wj and uj are unit vectors and \u03bbj are real numbers. Using this notation, the function fA can be represented as the sum\nfA(x) = r\u2217\u2211 j=1 \u03bbju > j Z(x)wj .\nLet v(z) be an nP -dimensional vector whose (i, p)-th coordinate is equal to K(z, zp(xi)). Then Q\u2020v(zp(x)) is the p-th row of matrix Z(x). Letting hj denote the mapping z 7\u2192 \u3008Q\u2020v(z), wj\u3009, we have\nfA(x) = r\u2217\u2211 j=1 P\u2211 p=1 \u03bbjuj,phj(zp(x)).\nThe function hj can also be written as z 7\u2192 \u3008(Q>)\u2020wj , v(z)\u3009. Equation (27) implies that the Hilbert norm of this function is equal to \u2016Q>(Q>)\u2020wj\u20162, which is bounded by \u2016wj\u20162 = 1. Thus we have\nr\u2217\u2211 j=1 \u2016\u03bbjuj\u20162\u2016hj\u2016H = r\u2217\u2211 j=1 |\u03bbj | = \u2016A\u2016\u2217 \u2264 C\u03c3(B1)B2r,\nwhich implies that fA \u2208 Fccnn. Next, it suffices to prove that for some empirical risk minimizer f in function class Fccnn, it also belongs to the function class CR. Recall that any function f \u2208 Fccnn can be represented in the form\nf(x) = r\u2217\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)).\nwhere the filter hj belongs to the RKHS. Let \u03d5 : Rd1 \u2192 `2(N) be a feature map of the kernel function. The function hj(z) can be represented by \u3008w, \u03d5(z)\u3009 for some w \u2208 `2(N). In Appendix B, we have shown that any function taking this form can be replaced by \u3008(Q>)\u2020Q>\u03b2j , v(z)\u3009 for some vector \u03b2j \u2208 RnP without changing the output on the training data. Thus, there exists at least one empirical risk minimizer f of Fccnn such that all of its filters take the form\nhj(z) = \u3008(Q>)\u2020Q>\u03b2j , v(z)\u3009. (34)\nBy equation (27), the Hilbert norm of these filters satisfy:\n\u2016hj\u2016H = \u2016Q>(Q>)\u2020Q>\u03b2j\u20162 = \u2016Q>\u03b2j\u20162.\nAccording to Appendix B, if all filters take the form (34), then the function f can be represented by tr(Z(x)A) for matrix A := \u2211r\u2217 j=1Q >\u03b2j\u03b1 > j . Consequently, the nuclear norm is bounded as\n\u2016A\u2016\u2217 \u2264 r\u2217\u2211 j=1 \u2016\u03b1j\u20162\u2016Q>\u03b2j\u20162 = r\u2217\u2211 j=1 \u2016\u03b1j\u20162\u2016hj\u2016H \u2264 C\u03c3(B1)B2r = R,\nwhich establishes that the function f belongs to the function class CR.\nC.3 Proof of Lemma 5\nThroughout this proof, we use the shorthand notation R := C\u03c3(B1)B2r. Recall that any function f \u2208 Fccnn can be represented in the form\nf(x) = r\u2217\u2211 j=1 P\u2211 p=1 \u03b1j,phj(zp(x)) where hj \u2208 H, (35)\nand the Hilbert space H is induced by the kernel function K. Since any patch z belongs to the compact space {z : \u2016z\u20162 \u2264 1} and K is a continuous kernel satisfying K(z, z) \u2264 1, Mercer\u2019s theorem [41, Theorem 4.49] implies that there is a feature map \u03d5 : Rd1 \u2192 `2(N) such that \u2211\u221e `=1 \u03d5`(z)\u03d5`(z\n\u2032) converges uniformly and absolutely to K(z, z\u2032). Thus, we can write K(z, z\u2032) = \u3008\u03d5(z), \u03d5(z\u2032)\u3009. Since \u03d5 is a feature map, every any function h \u2208 H can be written as h(z) = \u3008\u03b2, \u03d5(z)\u3009 for some \u03b2 \u2208 `2(N), and the Hilbert norm of h is equal to \u2016\u03b2\u20162.\nUsing this notation, we can write the filter hj in equation (35) as hj(z) = \u3008\u03b2j , \u03d5(z)\u3009 for some vector \u03b2j \u2208 `2(N), with Hilbert norm \u2016hj\u2016H = \u2016\u03b2j\u20162. For each x, let \u03a8(x) denote the linear operator that maps any sequence \u03b8 \u2208 `2(N) to the vector in RP with elements[\n\u3008\u03b8, \u03d5(z1(x))\u3009 . . . \u3008\u03b8, \u03d5(zP (x))\u3009 ]T .\nInformally, we can think of \u03a8(x) as a matrix whose p-th row is equal to \u03d5(zp(x)). The function f can then be written as\nf(xi) = r\u2217\u2211 j=1 \u03b1>j \u03a8(xi)\u03b2j = tr \u03a8(xi)( r\u2217\u2211 j=1 \u03b2j\u03b1 > j ) . (36) The matrix \u2211r\u2217 j=1 \u03b2j\u03b1\n> j satisfies the constraint\u2225\u2225\u2225\u2225\u2225\u2225 r\u2217\u2211 j=1 \u03b2j\u03b1 > j \u2225\u2225\u2225\u2225\u2225\u2225 \u2217 \u2264 r\u2217\u2211 j=1 \u2016\u03b1j\u20162 \u00b7 \u2016\u03b2j\u20162 = r\u2217\u2211 j=1 \u2016\u03b1j\u20162 \u00b7 \u2016hj\u2016H \u2264 R. (37)\nCombining equation (36) and inequality (37), we find that the Rademacher complexity is bounded by\nRn(Fccnn) = 1\nn E\n[ sup\nf\u2208Fccnn n\u2211 i=1 if(xi) ] \u2264 1 n E [ sup A:\u2016A\u2016\u2217\u2264R tr (( n\u2211 i=1 i\u03a8(xi) ) A )]\n= R\nn E [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8(xi) \u2225\u2225\u2225\u2225\u2225 2 ] , (38)\nwhere the last equality uses Ho\u0308lder\u2019s inequality\u2014that is, the duality between the nuclear norm and the spectral norm.\nAs noted previously, we may think informally of the quantity \u2211n\ni=1 i\u03a8(xi) as a matrix with P rows and infinitely many columns. Let \u03a8(d)(xi) denote the submatrix consisting of the first d columns of \u03a8(xi) and let \u03a8 (\u2212d)(xi) denote the remaining sub-matrix. We have\nE [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8(xi) \u2225\u2225\u2225\u2225\u2225 2 ] \u2264 E [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8 (d)(xi) \u2225\u2225\u2225\u2225\u2225 2 ] + E \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8 (\u2212d)(xi) \u2225\u2225\u2225\u2225\u2225 2 F 1/2\n\u2264 E [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8 (d)(xi) \u2225\u2225\u2225\u2225\u2225 2 ] + ( nP \u00b7 E [ \u221e\u2211 `=d+1 \u03d52` (z) ])1/2 .\nSince \u2211\u221e\n`=1 \u03d5 2 ` (z) uniformly converges to K(z, z), the second term on the right-hand side converges\nto zero as d \u2192 \u221e. Thus it suffices to bound the first term and take the limit. In order to upper bound the spectral norm \u2225\u2225\u2211n i=1 i\u03a8 (d)(xi) \u2225\u2225 2 , we use a matrix Bernstein inequality due to Minsker [31, Theorem 2.1]. In particular, whenever tr(\u03a8(d)(xi)(\u03a8 (d)(xi))\n>) \u2264 C1, there is a universal constant c such that the expected spectral norm is upper bounded as\nE [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8 (d)(xi) \u2225\u2225\u2225\u2225\u2225 2 ] \u2264 c \u221a log(nC1)E [( n\u2211 i=1 \u2016\u03a8(d)(xi)(\u03a8(d)(xi))>\u20162 )1/2]\n\u2264 c \u221a log(nC1) ( nE[\u2016\u03a8(X)\u03a8>(X)\u20162] )1/2 .\nNote that the uniform kernel expansion K(z, z\u2032) = \u2211\u221e\n`=1 \u03d5`(z)\u03d5`(z \u2032) implies the trace norm bound\ntr(\u03a8(d)(xi)(\u03a8 (d)(xi)) >) \u2264 tr(K(xi)). Since all patches are contained in the unit `2-ball, the kernel function K is uniformly bounded by 1, and hence C1 \u2264 P . Taking the limit d\u2192\u221e, we find that\nE [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 i\u03a8(xi) \u2225\u2225\u2225\u2225\u2225 2 ] \u2264 c \u221a log(nP ) ( nE[\u2016K(X)\u20162] )1/2 .\nFinally, substituting this upper bound into inequality (38) yields the claimed bound (32)."}], "references": [{"title": "Convex two-layer modeling", "author": ["\u00d6. Aslan", "H. Cheng", "X. Zhang", "D. Schuurmans"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Convex deep learning via normalized kernels", "author": ["\u00d6. Aslan", "X. Zhang", "D. Schuurmans"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["F. Bach"], "venue": "arXiv preprint arXiv:1412.8690,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Convex neural networks", "author": ["Y. Bengio", "N.L. Roux", "P. Vincent", "O. Delalleau", "P. Marcotte"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Training a 3-node neural network is NP-complete", "author": ["A.L. Blum", "R.L. Rivest"], "venue": "Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Online learning and stochastic approximations", "author": ["L. Bottou"], "venue": "On-line learning in neural networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Invariant scattering convolution networks. Pattern Analysis and Machine Intelligence", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Pcanet: A simple deep learning baseline for image classification", "author": ["T.-H. Chan", "K. Jia", "S. Gao", "J. Lu", "Z. Zeng", "Y. Ma"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "The loss surface of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "Ann Arbor,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["A. Daniely", "R. Frostig", "Y. Singer"], "venue": "arXiv preprint arXiv:1602.05897,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y.N. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Efficient projections onto the `1ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Compressed sensing: theory and applications", "author": ["Y.C. Eldar", "G. Kutyniok"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "An empirical study of learning speed in back-propagation networks", "author": ["S.E. Fahlman"], "venue": "Journal of Heuristics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1988}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["B.D. Haeffele", "R. Vidal"], "venue": "arXiv preprint arXiv:1506.07540,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Suitable mlp network activation functions for breast cancer and thyroid disease detection", "author": ["I. Isa", "Z. Saad", "S. Omar", "M. Osman", "K. Ahmad", "H.M. Sakim"], "venue": "In 2010 Second International Conference on Computational Intelligence, Modelling and Simulation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Generalization bounds for neural networks through tensor factorization", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master Thesis,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Face recognition: A convolutional neural-network approach", "author": ["S. Lawrence", "C.L. Giles", "A.C. Tsoi", "A.D. Back"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1997}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "On the computational efficiency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "On some extensions of Bernstein\u2019s inequality for self-adjoint operators", "author": ["S. Minsker"], "venue": "arXiv preprint arXiv:1112.5448,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "On the quality of the initial basin in overspecified neural networks", "author": ["I. Safran", "O. Shamir"], "venue": "arXiv preprint arXiv:1511.04210,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Provable methods for training neural networks with sparse connectivity", "author": ["H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Learning kernel-based halfspaces with the 0-1 loss", "author": ["S. Shalev-Shwartz", "O. Shamir", "K. Sridharan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "search. Nature,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Learning invariant representations with local transformations", "author": ["K. Sohn", "H. Lee"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Neural networks with periodic and monotonic activation functions: a comparative study in classification problems", "author": ["J.M. Sopena", "E. Romero", "R. Alquezar"], "venue": "In ICANN", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1999}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1929}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Deep learning using linear support vector machines", "author": ["Y. Tang"], "venue": "arXiv preprint arXiv:1306.0239,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "End-to-end text recognition with convolutional neural networks", "author": ["T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["L. Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Learning halfspaces and neural networks with random initialization", "author": ["Y. Zhang", "J.D. Lee", "M.J. Wainwright", "M.I. Jordan"], "venue": "arXiv preprint arXiv:1511.07948,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "`1-regularized neural networks are improperly learnable in polynomial time", "author": ["Y. Zhang", "J.D. Lee", "M.I. Jordan"], "venue": "In Proceedings on the 33rd International Conference on Machine Learning,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}], "referenceMentions": [{"referenceID": 27, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 163, "endOffset": 171}, {"referenceID": 24, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 163, "endOffset": 171}, {"referenceID": 25, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 190, "endOffset": 194}, {"referenceID": 20, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 215, "endOffset": 219}, {"referenceID": 43, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 241, "endOffset": 245}, {"referenceID": 31, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 264, "endOffset": 272}, {"referenceID": 36, "context": "Convolutional neural networks (CNNs) [28] have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37].", "startOffset": 264, "endOffset": 272}, {"referenceID": 5, "context": "However, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 6, "context": "In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 18, "context": "This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper [19]), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm.", "startOffset": 165, "endOffset": 169}, {"referenceID": 46, "context": "This approach is inspired by our earlier work [48], involving a subset of the current authors, in which we developed this relaxation step for fully-connected neural networks.", "startOffset": 46, "endOffset": 50}, {"referenceID": 46, "context": "Our results show that the sample complexity for CCNNs is significantly lower than that of the convexified fully-connected neural network [48], highlighting the importance of parameter sharing.", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46].", "startOffset": 126, "endOffset": 130}, {"referenceID": 44, "context": "There are other efficient optimization algorithms for solving the problem (12), such as the proximal adaptive gradient method [17] and the proximal SVRG method [46].", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nystr\u00f6m approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q \u2208 RnP\u00d7m such that K \u2248 QQ>.", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nystr\u00f6m approximation [15] or random feature approximation [33]; both are randomized methods to obtain a tall-and-thin matrix Q \u2208 RnP\u00d7m such that K \u2248 QQ>.", "startOffset": 177, "endOffset": 181}, {"referenceID": 26, "context": "The random feature approximation takes O(mnPd1) time, but can be improved to O(mnP log d1) time using the fast Hadamard transform [27].", "startOffset": 130, "endOffset": 134}, {"referenceID": 35, "context": "[36] for learning halfspaces, and by Zhang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[48] for learning fully-connected neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": ", used by [10, 29]).", "startOffset": 10, "endOffset": 18}, {"referenceID": 28, "context": ", used by [10, 29]).", "startOffset": 10, "endOffset": 18}, {"referenceID": 38, "context": ", used by [39, 22]).", "startOffset": 10, "endOffset": 18}, {"referenceID": 21, "context": ", used by [39, 22]).", "startOffset": 10, "endOffset": 18}, {"referenceID": 3, "context": "Combining this bound with the classical Rademacher complexity theory [4], we conclude that the generalization loss of f\u0302ccnn converges to the least possible generalization error of Fccnn.", "startOffset": 69, "endOffset": 72}, {"referenceID": 46, "context": "[48].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "We define the multi-channel patch vector as a concatenation of patch vectors for each channel: zp(x) := (zp(x[1]), .", "startOffset": 109, "endOffset": 112}, {"referenceID": 42, "context": "These variations are known to be hard for methods without a convolution mechanism (for instance, see the paper [44]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 32, "context": "The feature matrix Z(x) is constructed via random feature approximation [33] with dimension m = 500 for the first convolutional layer and m = 1000 for the second.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "Before training each CCNN layer, we preprocess the input vectors zp(xi) using local contrast normalization and ZCA whitening [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 37, "context": "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].", "startOffset": 159, "endOffset": 163}, {"referenceID": 42, "context": "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].", "startOffset": 234, "endOffset": 238}, {"referenceID": 7, "context": "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].", "startOffset": 260, "endOffset": 263}, {"referenceID": 8, "context": "The CCNN-2 model is compared against methods that report the state-of-the-art results on these datasets, including the translation-invariant RBM model (TIRBM) [38], the stacked denoising auto-encoder with three hidden layers (SDAE-3) [44], the ScatNet-2 model [8] and the PCANet-2 model [9].", "startOffset": 287, "endOffset": 290}, {"referenceID": 42, "context": "basic rand rot img img+rot SVMrbf [44] 3.", "startOffset": 34, "endOffset": 38}, {"referenceID": 42, "context": "18% NN-1 [44] 4.", "startOffset": 9, "endOffset": 13}, {"referenceID": 37, "context": "28% TIRBM [38] - 4.", "startOffset": 10, "endOffset": 14}, {"referenceID": 42, "context": "50% SDAE-3 [44] 2.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "76% ScatNet-2 [8] 1.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "48% PCANet-2 [9] 1.", "startOffset": 13, "endOffset": 16}, {"referenceID": 17, "context": "To see their impact on the parameter matrix, we compute the effective rank (ratio between the nuclear norm and the spectral norm, see [18]) of matrix \u00c2.", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "2 CIFAR-10 In order to test the capability of CCNN in complex classification tasks, we report its performance on the CIFAR-10 dataset [24].", "startOffset": 134, "endOffset": 138}, {"referenceID": 24, "context": "It was known that the generalization performance of the CNN can be improved by training on random crops of the original image [25], so we train the CNN on random 24\u00d724 patches of the image, and test on the central 24\u00d724 patch.", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "We compare the CCNN against other baseline methods that don\u2019t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].", "startOffset": 154, "endOffset": 158}, {"referenceID": 8, "context": "We compare the CCNN against other baseline methods that don\u2019t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].", "startOffset": 179, "endOffset": 182}, {"referenceID": 29, "context": "We compare the CCNN against other baseline methods that don\u2019t involve nonconvex optimization: the kernel SVM with Fastfood Fourier features (SVMFastfood) [27], the PCANet-2 model [9] and the convolutional kernel networks (CKN) [30].", "startOffset": 227, "endOffset": 231}, {"referenceID": 26, "context": "52% SVMFastfood [27] 36.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "90% PCANet-2 [9] 22.", "startOffset": 13, "endOffset": 16}, {"referenceID": 29, "context": "86% CKN [30] 21.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "[25]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] showed how to formulate neural network training as a convex optimization problem involving an infinite number of parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "This perspective encourages incrementally adding neurons to the network, whose generalization error was studied by Bach [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 45, "context": "[47] propose a polynomial-time ensemble method for learning fully-connected neural networks, but their approach handles neither parameter sharing nor the convolutional setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Other relevant works for learning fully-connected networks include [35, 23, 29].", "startOffset": 67, "endOffset": 79}, {"referenceID": 22, "context": "Other relevant works for learning fully-connected networks include [35, 23, 29].", "startOffset": 67, "endOffset": 79}, {"referenceID": 28, "context": "Other relevant works for learning fully-connected networks include [35, 23, 29].", "startOffset": 67, "endOffset": 79}, {"referenceID": 0, "context": "[1, 2] propose a method for learning multi-layer latent variable models.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2] propose a method for learning multi-layer latent variable models.", "startOffset": 0, "endOffset": 6}, {"referenceID": 13, "context": "Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11].", "startOffset": 202, "endOffset": 210}, {"referenceID": 10, "context": "Under certain assumptions on the data distribution, it can be shown that any local minimum of a two-layer fully connected neural network has an objective value that is close to the global minimum value [14, 11].", "startOffset": 202, "endOffset": 210}, {"referenceID": 33, "context": "Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20].", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "Similar results have also been established for over-specified neural networks [34], or neural networks that has a certain parallel topology [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 29, "context": "[30] present convolutional kernel networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The ScatNet method [8] uses translation and deformation-invariant filters constructed by wavelet analysis; however, these filters are independent of the data, unlike the analysis in this paper.", "startOffset": 19, "endOffset": 22}, {"referenceID": 12, "context": "[13] show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from a random initialization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[48].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "We refer the reader to Bartlett and Mendelson [4] for an introduction to the theoretical properties of Rademacher complexity.", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "Thus, the theory of Rademacher complexity [4] guarantees that E[L(Fccnn(X);Y )] \u2264 inf f\u2208Fccnn E[L(f(x); y)] + 2L \u00b7 Rn(Fccnn) + c \u221a n , (33)", "startOffset": 42, "endOffset": 45}], "year": 2016, "abstractText": "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.", "creator": "LaTeX with hyperref package"}}}