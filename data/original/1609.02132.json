{"id": "1609.02132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "UberNet: Training a `Universal' Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory", "abstract": "In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a `swiss knife' for vision tasks; we call this architecture an UberNet to indicate its overarching nature.", "histories": [["v1", "Wed, 7 Sep 2016 19:35:30 GMT  (7244kb,D)", "http://arxiv.org/abs/1609.02132v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["iasonas kokkinos"], "accepted": false, "id": "1609.02132"}, "pdf": {"name": "1609.02132.pdf", "metadata": {"source": "CRF", "title": "UberNet : Training a \u2018Universal\u2019 Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory", "authors": ["Iasonas Kokkinos"], "emails": ["iasonas.kokkinos@ecp.fr"], "sections": [{"heading": null, "text": "We address two main technical challenges that emerge when broadening up the range of tasks handled by a single CNN: (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget. Properly addressing these two problems allows us to train accurate predictors for a host of tasks, without compromising accuracy.\nThrough these advances we train in an end-to-end manner a CNN that simultaneously addresses (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all of these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this system can be found at cvn.ecp.fr/ubernet/."}, {"heading": "1. Introduction", "text": "Computer vision involves a host of tasks, such as boundary detection, semantic segmentation, surface estimation, object detection, image classification, to name a few. While Convolutional Neural Networks (CNNs) have been the method of choice for text recognition for more than two decades [56], they have only been recently shown to successful at handling effectively most, if not all, vision tasks.\nWhile only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface\nar X\niv :1\n60 9.\n02 13\n2v 1\n[ cs\n.C V\n] 7\nS ep\n2 01\nnormal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.g. [22,31,32,40,52,85,89,93,94]. Most of these works rely on finetuning a common pretrained CNN, such as the VGG network [93] or others [40, 52], which indicates the broad potential of these CNNs.\nHowever, each of these works ends up with a taskspecific CNN, and potentially a mildly different architecture. If one wanted to perform two tasks, one would need to train and test with separate networks. In our understanding a joint treatment of multiple problems can result not only in simpler, faster, and better systems, but will also be a catalyst for reaching out to other fields. One can expect that such allin-one, \u201cswiss knife\u201d architectures will become indispensable for general AI, involving for instance robots that will be able to recognize the scene they are in, recognize objects, navigate towards them, and manipulate them. Furthermore, having a single visual module to address a multitude of tasks will make it possible to explore methods that improve performance on all of them, rather than developping narrow, problem-specific techniques. Apart from simplicity and efficiency, the problem can also be motivated by arguing that by training a network so as to accomplish multiple tasks one leaves smaller space for \u2018blindspots\u2019 [95], effectively providing a more complete specification of the network duties. Finally, the particular motivation for this research has been the interest in studying the synergy between different visual tasks (e.g. the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.\nThe problem of using a single network to solve multiple tasks has been repeatedly pursued in the context of deep learning for computer vision. In [89] a CNN is used for joint localization detection and classification, [25] propose a net-\nwork that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.\nInspired by these advances, in this work we introduce two techniques that allow us to expand the range of tasks handled by a single deep network, and thereby make it possible to train a single network for multiple, diverse tasks, without sacrificing accuracy.\nOur first contribution consists in exploring how a CNN can be trained from diverse datasets. This problem inevitably shows up once we aim at breadth, since no single dataset currently contains ground-truth for all possible tasks. As shown in Table. 3 high-level annotation (e.g. object positions, landmarks in PASCAL VOC [26]) is often missing from the datasets used for low-level tasks (e.g. BSD [69]), and vice versa. If we consider for instance a network that is supposed to be predicting both human landmarks and surface normals, we have no dataset where an image comes with annotations for both tasks, but rather disjoint datasets (NYU [74], and PASCAL VOC [26], or any other pose estimation dataset for keypoints) providing every image with annotations for only one of the two.\nIn order to handle this challenge we introduce in Sec. 3 a loss function that only relies on the ground truth available per training sample, shunning the losses of tasks for which no ground truth is available at this sample. We combine this loss function with Stochastic Gradient Descent, and end up updating a network parameter only once we have observed a sufficient number of training samples related to the parameter. This results in an asynchronous variant of backpropagation and allows us to train our CNN in an end-to-end manner.\nOur second contribution aims at addressing the limitations of current hardware used for deep learning - in par-\nticular the limited memory available on modern Graphics Processing Units (GPUs). As the number of tasks increases, the memory demand of a naively implemented back-propagation algorithm can increase linearly in the number of tasks, with a factor proportional to the memory requested by task-specific network layers. Instead, we build on recent developments in learning with deep architectures [16, 34] which have shown that it is possible to efficiently train a deep CNN with a memory complexity that is sublinear in the number of layers. We develop a variant that is customized to our multi-task architecture and allows us to perform end-to-end network training for a practically unlimited number of tasks, since the memory complexity is independent of the number of tasks.\nOur current architecture has been systematically evaluated on the following tasks (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) semantic part segmentation (f) semantic boundary detection and (g) proposal generation and object detection. Our present system operates in 0.6-0.7 seconds per frame on a GPU and delivers results that are competititve with the state-of-the-art for these tasks.\nWe start by specifying in Sec. 2 the architecture of our CNN and then turn to our contributions on learning from diverse datasets and dealing with memory constraints in Sec. 3 and Sec. 4 respectively."}, {"heading": "2. UberNet architecture", "text": "We now introduce the architecture of our network, shown in Fig. 2. Aiming at simplicity, we introduce a minimal number of additional, task-specific layers on top of a common CNN trunk that is based on the VGG network. Clearly, we can always include on top additional layers and parameters, e.g. U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.\nThe starting point is that of using a standard \u2018fully\u2019 convolutional network [56,67,79,89], namely a CNN that provides a field of decision variables, rather than a single classificiation at its output; this can be used to accomplish any dense labelling or regression task, such as boundary detection, normal estimation, or semantic segmentation. We now describe our modifications to this basic architecture.\nSkip layers: one first deviation from the most standard architecture is that as in [38, 49, 89, 107] we use skip layers that combine the top-layer neurons with the activations of intermediate neurons to form the network output. Tasks such as boundary detection clearly profit from the smaller degree of spatial abstraction of lower-level neurons [107], while even for high-level tasks, such as semantic segmentation, it has been shown [38,67] that skip layers can improve\nperformance. In particular we pool features from layers conv1 2, conv2 2, conv3 3, conv4 3, conv5 3, fc7 of the VGG-16 network, which show up as C1, . . . ,C6 in Fig. 2.\nSkip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107]. One exception is the last layer (fc7 in VGG, C6 in Fig. 2) which has already been trained so as to be an appropriate argument to a linear classifier (fc8), and therefore seemed to be doing better without normalization.\nCumulative task-specific operations: Scaling up to many tasks requires keeping the task-specific memory and computation budget low, and we therefore choose as in [49, 107] to process the outputs of the skip-pooling with task-specific layers that perform linear operations. In particular if we denote by f1, . . . , f6 the neuron activations at layers 1 up to 6 that are used to obtain the score at a given image position the output for task t is a linear function:\nst = w T t f = 6\u2211 k=1 wTt,kfk (1)\nwith f = [f1| . . . |f6]T ,wt = [wt,1| . . . |wt,6]T . (2)\nRather that explicitly allocating memory for the vector f formed by contactenating the intermediate activations and then forming the matrix product, we instead compute the intermediate results per layer, wTt,kfk and then add them up. This yields the same result, but acts like a low-memory online accumulation of scores across skip layers.\nFusion layers: For the fusion layers, denoted by circular connections in Fig. 2 we observe that instead of simply adding the scores (sum-fusion), one can accelerate training by concatenating the score maps and learning a linear function that operates on top of the concatenated score maps - as originally done in [107]. This scheme is clearly still learning in the end a linear function with the same number of free parameters, but this decomposition, which can be intuitively understood as some form of preconditioning, seems to be more effective. When also back-propagating on the intermediate layers, this typically also results in better performance. We note that for simplicity we assume correspondence across layer positions; these are handled in our network by appropriate interpolation layers which in our diagram are understood to be included in the circular nodes.\nAtrous convolution: We also use convolution with holes (a\u0300 trous) [14,79] which allows us to control the spatial resolution of the output layer. In particular we use a\u0300 trous convolution to obtain an output stride of 8 (rather than 16), which gives us a moderate boost in tasks such as boundary detection or semantic segmentation.\nMulti-resolution CNN: as in [15,46,49,79], rather than processing an image at a single resolution, we form an im-\nage pyramid and pass scaled versions of the same image through CNNs with shared weights. This allows us to deal with the scale variability of image patterns. Even though in [15] a max-fusion scheme is shown to yield higher accuracy than sum-fusion, in our understanding this is particular to the case of semantic segmentation, where a large score at any scale suffices to assign an object label to a pixel. This may not be the case for boundaries where the score should\nbe determined by the accumulation of evidence from multiple scales [105] or for normals, where maximization over scales of the normal vector entries does not make any sense. We therefore use a concatenation of the scores followed by a linear operation, as in the case of fusing the skip-layers described above, and leave the exploration of scale-aware processing [15] for the future.\nThis multi-resolution processing is incorporated in the\nnetwork definition, and as such can be accounted for during end-to-end training. In this pyramid the highest resolution image is set similar to [32] so that the smallest image dimension is 621 pixels and the largest dimension does not exceed 921 (the exact numbers are so that dimensions are of the form 32k + 1, as requested by [15, 49]).\nAs in [15, 49] we use loss layers both at the outputs of the individual scales and the final responses, amounting to a mild form of deep supervision network (DSN) training [107].\nTask-specific deviations: All of these choices have been separately validated on individual tasks, and then integrated in the common architecture shown in Fig. 2. There are still however some task-specific deviations.\nOne exception to the uniform architecture outlined above is for detection, where we follow the work of [85] and learn a convolutional region proposal network, followed by a fully-connected subnetwork that classifies the region proposals into one of 21 labels (20 classes and background). Recent advances however [22,65] may make this exception unnecessary.\nFurthermore, the output of each of the task-specific streams is penalized by a loss function that is adapted to the task at hand. For region labelling tasks (semantic segmentation, human parts, saliency) and object detection we use the softmax loss function, as is common in all recent works on semantic segmentation [13,67] and object detection [32]. For regression tasks (normal estimation, bounding box regression) we use the smooth `1 loss [32]. For normal estimation we apply an `2 normalization prior to penalizing with the `1 loss, since surface normals are unit-norm vectors. For tasks where we want to estimate thin structures (boundaries, semantic boundaries) we use the MIL-based loss function introduced in [49] in order to accommodate imprecision in the placement of the boundary annotations. For these two tasks we also have a class imbalance problem, with many more negatives than positives; we mitigate this by using a weighted cross-entropy loss, as in [107], where we attribute a weight of 0.9 to positives and 0.1 to negatives.\nFurthermore, for low-level tasks such as boundary detection, normal estimation and saliency estimation, as well as semantic boundary detection we set the spatial resolution of the scores to be equal to that of the image - this allows us to train with a loss function that has a higher degree of spatial accuracy and allows us to accurately localize small structures. For region labelling tasks such as semantic segmentation, or human part segmentation we realized that we do not really need this level of spatial accuracy, and instead train score maps that employ a lower spatial resolution, using a downsampling factor of 8 with respect to the original image dimensions. This results in a 64-fold reduction of the task-specific computation and memory demands."}, {"heading": "3. Multi-Task Training using Diverse Datasets", "text": "Having described our network\u2019s architecture, we now turn to parameter estimation. Our objective is to train in an end-to-end manner both the VGG-based CNN trunk that delivers features to all tasks, and the weights of the taskspecific layers.\nAs described in the introduction, the main challenge that we face is the diversity of the tasks that we wish to cover. In order to handle the diversity of the available datasets one needs to handle missing ground truth data during training. Certain recent works such as [20, 77, 78] manage to impute missing data in an EM-type approach, by exploiting domain-specific knowledge - e.g. by requesting that a fixed percentage of the pixels contained in the bounding box of an object obtain the same label as the object. This however may not be possible for arbitrary tasks, e.g. normal estimation.\nInstead, we propose to adapt the loss function to the information that we have per sample, and set to zero the loss of tasks for which we have no ground-truth. While the idea is straightforward, as we describe below some care needs to be taken when optimizing the resulting loss with backpropagation, so as to ensure that the (stochastic) estimates of the parameter gradients accumulate evidence from a sufficient number of training samples.\nOur training objective is expressed as the sum of per-task losses, and regularization terms applied to the parameters of task-specific, as well as shared layers: L(w0,t1,...,tT ) = R(w0) + T\u2211 t=1 \u03b3tk (R(wt) + Lt (w0,wt)) .\n(3)\nIn Eq. 3 we use t to index tasks; w0 denotes the weights of the common CNN trunk, and wt are task-specific weights; \u03b3t is an hyperparameter that determines the relative importance of task t,R(w\u2217) = \u03bb2 \u2016w\u2217\u20162 is an `2 regularization on the relevant network weights, and Lt (w0,wt) is the taskspecific loss function.\nThis task-specific loss is written as follows:\nLt (w0,wt) = 1\nN N\u2211 i=1 \u03b4t,iLt ( f it (w0,wt),y i t ) , (4)\nwhere we use i to index training samples, denote by f it , y i t the task-specific network prediction and ground truth at the i-th example respectively, by wi the task-specific network parameters, and by \u03b4t,i \u2208 {0, 1} we indicate whether example i comes with ground-truth for task t. If \u03b4t,i = 0 we can set yit to an arbitrary value, without affecting the loss - i.e. we do not need to impute the ground truth.\nWe now turn to accounting for the interplay between the term \u03b4t,i and the minimization of Eq. 3 through Stochastic\nSynchronous SGD - backprop for m = 1 to M do {construct minibatch} B \u2190 {i1, . . . , iB} with ii \u223c U [1, N ] {initialize gradient accumulators} dw0 \u2190 0,dw1 \u2190 0, . . . ,dwT \u2190 0 for i \u2208 B do {cnn gradients} dw0 \u2190 dw0 + \u2211 t \u03b4t,i\u03b3t\u2207w0Lt ( f it (w0,wt),y i t\n) {Task gradients, t = 1, . . . , T} dwt \u2190 dwt + \u03b4t,i\u03b3t\u2207w0Lt ( f it (w0,wt),y i t\n) end for for p \u2208 {0, 1, . . . , T} do\nwp \u2190 wp \u2212 ( \u03bbwp + 1 Bdwp ) end for\nend for\nTable 2: Pseudocode for the standard, synchronous stochastic gradient descent algorithm for back-propagation training. We update all parameters at the same time, after observing a fixed number of samples.\nGradient Descent (SGD). Since we want to train our network in an end-to-end manner for all tasks, we consider as our training set the union of different training sets, each of them containing pairs of images and ground-truth for distinct tasks. Images from this set are sampled uniformly at random; as is common practice, we use multiple epochs and within each epoch sample without replacement.\nConsidering that we use a minibatch B of size B, plain SGD for task k would lead to the following update rules:\nw\u2032p = wp \u2212 (\u03bbwp + dwp), p \u2208 {0, 1, . . . , T} (5)\ndw0 = 1\nB \u2211 i\u2208B T\u2211 t=1 \u03b3t\u03b4t,i\u2207w0Lt ( f it (w0,wt),y i t ) , (6)\ndwt = 1\nB \u2211 i\u2208B \u03b3t\u03b4t,i\u2207wtLt ( f it (w0,wt),y i t ) , (7)\nwhere the weight decay term results from `2 regularization and \u2207w\u2217Lt (y\u0302, y) denotes the gradient of the loss for task t with respect to the parameter vector w\u2217. The difference between the two update terms is that the parameters of the common trunk, w0 are affecting all tasks, and as such accumulate the gradients over all tasks, while task-specific parameters wt are only affected by the subset of images for which \u03b4t,i = 1.\nWe observe that this comes with an important flaw: if we have a small batch size, the update rule for wt may use a too noisy gradient if \u2211 i\u2208B \u03b4t,i happens to be small - it may even be that no task-related samples happen to be in the present minibatch. We have empirically observed that this\nAsynchronous SGD - backprop {initialize gradient accumulators} dw0 \u2190 0,dw1 \u2190 0, . . . ,dwT \u2190 0 {initialize counters} c0 \u2190 0, c1 \u2190 0, . . . , cT \u2190 0 for m = 1 to M \u00b7B do\ncan often lead to erratic behaviour, which we originally handled by increasing the minibatch size to quite large numbers (50 images or more, as opposed to 10 or 20). Even though this mitigates the problem partially, firstly it is highly inefficient timewise, and will also not scale up to solving say 10 or 20 tasks simultaneously.\nInstead of this brute-force approach we propose a modified variant of backpropagation that more naturally handles the problem by updating the parameters of a task only once sufficiently many relevant images have been observed. Pseudocode presenting this scheme, in contrast to the standard SGD scheme is provided in Table. 3.\nIn particular we no longer have \u2018a minibatch\u2019, but rather treat images in a streaming mode, keeping one counter per task - as can be seen, rather thanM outer loops, which is the number of minibatches, we useM \u00b7B outer loops, equalling the number of images treated by the original scheme.\nWhenever we process a training sample that contains ground truth for a task, we increment the task counter, and add the current gradient to a cumulative gradient sum. Once\nthe task counter exceeds a threshold we update the task parameters and then reset the counter and cumulative gradient to zero. Clearly, the common CNN parameters are updated regularily, since their counter is incremented for every single training image. This is however not the case for other tasks which may not be affected by a subset of training images.\nThis results in an asynchronous variant of backpropagation, in the sense that any parameter can be updated at a time instance that is independent of the others. We note that apart from implementing the necessary book-keeping, this scheme requires no additional memory or computation. It is also clear that the \u2018asynchronous\u2019 term relates to the manner in which parameters for different tasks are updated, rather than the computation itself, which in our implementation is single-node.\nWe also note that according to the pseudocode, we allow ourselves to use different \u2018effective batchsizes\u2019, Bp, which we have observed to be useful for training. In particular, for detection tasks it is reported in [32] that a batchsize of two suffices, while for dense labelling tasks such as semantic segmentation a batchsize of 10, 20 or even 30 is often used [14,107]. In our training we use an effective batchsizeBp of 2 for detection, 10 for all other task-specific parameters, and 30 for the shared CNN features, w0. The reasoning behind using this larger batch size for the shared CNN features is that we want their updates to absorm information from a larger number of images, containing multiple tasks, so that the task-specific idiosyncracies will cancel out. In this way it becomes more likely that the average gradient will serve all tasks and we avoid having the \u2018moving target\u2019 problem, where every task quickly changes the shared representation of the other tasks, making optimization harder.\nOne subtle difference is that in synchronous SGD the stochastic estimate of the gradient used in the update equals:\ngst = \u03b3t 1\nB \u2211 i\u2208B \u03b4t,i\u2207wtLt ( f it (w0,wt),y i t ) (8)\nwhile for the asynchronous case it will equal:\ngat = \u03b3t 1\nBp \u2211 i\u2208IBp \u2207wtLt ( f it (w0,wt),y i t ) , (9)\nwhere IBp = {it,1, . . . , it,Bp} indicates a subsequence of Bp samples which contain ground-truth for task t. We realize that the first estimate can be expected to have a typically smaller magnitude that the second one, since several of the terms being averaged will equal zero. This implies that we have somehow modified the original cost function, since the stochastic gradient estimates do not match. However this effect can be absorbed in the (empirically set) hyperparameters \u03b3t so that the two estimates will have the same expected magnitude, so we can consider the two algorithms to be optimizing the same quantity.\nI\nC1 C2 C3 C4 C5 C6 C7 C8 L\nA3 A6A1 A2 A4 A5\n(a) Low-memory forward pass\nI\nC1 C2 C3 C4 C5 C6 C7 C8 L\nA3 A6 A7 A8\nG6 G7 G8 y\n(b) Low-memory backpropagation (7-9)"}, {"heading": "4. Memory-Bound Multi-Task Training", "text": "We now turn to handling memory limitations, which turns out to be a major problem when training a network for many tasks. In order to handle these problems we build on recent advances in memory-efficient backpropagation for deep networks [16, 34] and adapt them to the task of multitask learning1. We start by describing the basic idea behind the algorithm of [16], paving the way for the presentation of our extension to multi-task learning.\nThe baseline implementation of the back-propagation algorithm maintains all intermediate layer activations computed during the forward pass. As illustrated in Fig. 3, during the backward pass each layer then combines its stored activations with the back-propagated gradients coming from the layer(s) above, finds the gradients for its own parameters, and then back-propagates gradients to the layer(s) below. While this strategy achieves computational efficiency by reusing the computed activation signals, it is memorydemanding, since it requires storing all intermediate activations. In the popular Caffe [44] library memory is also allocated for all of the gradient signals, since a priori these could feed into multiple layers for a DAG network.\nIf we consider for simplicity that every layer requires N bytes of memory for its activations, and gradient signals, and we have a network with a total of L layers, the memory complexity of a naive implementation would be 2NL - which can become prohibitive for large values of L.\nThe memory-efficient alternative described in [16] is shown in Fig. 4. In a first step, shown in Fig. 4(a), we perform a first forward pass through the network where we store the activations of only a subset of the layers - for a network of depth L, \u221a L activations are stored, lying \u221a L layers apart, while the other intermediate activations, shown in grey, are discarded as soon as they are used. Once this first\n1I thank George Papandreou for suggesting this direction\nI\nC1 C2 C3 C4 C5 C6\nCa7 C a 8 La\nCb7 C b 8 Lb\nA3 A6A1 A2 A4 A5\n(a) Low-memory forward pass\nI\nC1 C2 C3 C4 C5 C6\nCa7 C a 8 La\nCb7 C b 8 Lb\nA3 A6\nAa7 A a 8\nGa7 G a 8 y a\nG6\n(b) Low-memory backpropagation - task a\nI\nC1 C2 C3 C4 C5 C6\nCa7 C a 8 La\nCb7 C b 8 Lb\nA3 A6\nAb7 A b 8\nGb7 G b 8 y b\nG6\n(c) Low-memory backpropagation - task b\nI\nC1 C2 C3 C4 C5 C6\nCa7 C a 8 La\nCb7 C b 8 Lb\nA3 A4 A5\nG3 G4 G5 G6\n(d) Low-memory backpropagation (4-6)\nI\nC1 C2 C3 C4 C5 C6\nCa7 C a 8 La\nCb7 C b 8 Lb\nA1 A2\nG1 G2 G3\n(e) Low-memory backpropagation (1-3)\nstage is accomplished, we run \u221a L times backpropagation\nover sub-networks of length \u221a L, as shown in Fig. 4(b)-(d). The stored activations help us start the backpropagation at a deeper layer of the network, acting like anchor points for the computation: any subnetwork requires the activation of its lowest level, and the gradient signal at its highest layer. It can be seen that through this scheme the total complexity can be reduced from LN to 2 \u221a LN , since we retain \u221a L activation signals, and at any step perform back-propagation over a subnetwork of length \u221a L.\nThis algorithm was originally introduced for chainstructured graphs; having described it, the adaptation to our case is straightforward.\nConsidering that we have LC layers for the shared CNN trunk, T tasks, and LT layers per task, the memory complexity of the naive implementation would be 2N(LC + TLT ), as can also be seen from Fig. 5.\nA naive application of the algorithm presented above would result in a reduction of memory complexity down to 2N \u221a LC + TLT . However, we realize that after the branching point of the different tasks (layer 6 for our figure), the computations are practially decoupled: each taskspecific branch works effectively on its own, and then returns a gradient signal to layer 6. These gradient signals are accumulated over tasks, since our cost is additive over the task-specific losses.\nBased on this observation, we realize that the memory complexity can be reduced to be independent of T : since each task can \u2018clean up\u2019 all of the memory allocated to it, this results in a memory footprint of 2N \u221a LC + LT rather\nthan 2N \u221a LC + TLT .\nThis has allowed us to load an increasing number of tasks on our network without encountering memory issues. Using a 12GB Nvidia card we have been able to use a three-layer pyramid, with the largest image size being 921x621, and using skip-layer connections for all network layers, pyramid levels, and tasks, for seven tasks. The largest dimension that would be possible without the memory-efficient option for our present number of tasks would be 321x321 - and that would only decrease as more tasks are used.\nApart from reducing memory demands, we notice that we can also reduce computation time by performing a lazy evaluation of the gradient signals accumulated at the branching point. In particular, if a training sample does not contain ground-truth for certain tasks, these will not contribute any gradient term to the common CNN trunk; as such the computation over task-specific branches can be avoided for an instance that does not contain ground-truth for the task. This results in a substantial acceleration of training (2- to 4-fold in our case), and would be essential to scale up training for even more tasks."}, {"heading": "5. Experiments", "text": "Our experimental evaluation has two objectives: The first one is to show that the generic UberNet architecture introduced in Sec. 2 successfully addresses a broad range of tasks. In order to examine this we compare primarily to results obtained by methods that rely on the VGG network [93] - more recent works e.g. on detection [22] and semantic segmentation [14] have shown improvements through the use of deeper ResNets [40], but we consider the choice of network to be in a sense orthogonal to the goal of this section.\nThe second objective is to explore how incorporating more tasks affects the performance in the individual tasks. In order to remove erroneous sources of variation we use a common initialization for all single- and multi- task networks, obtained by pretraining a network for joint semantic segmentation and object detection, as detailed in Sec. 5.1. Furthermore, the multi-task network is trained with a union of datasets corresponding to the multiple tasks that we aim at solving. There we have used a particular proportion of images per dataset, so as to moderately favor high-level tasks, as detailed in Sec. 5.1. Even though using a larger task-specific dataset may boost performance for the particular task, the single task networks are only trained with the subset of the multi-task dataset that pertains to the particular task. This may be sacrificing some performance with respect to competing methods, but ensures that the loss term pertaining to a task is unaffected by the single- versus multitask training, and facilitates comparison."}, {"heading": "5.1. Experimental settings", "text": "Optimization: For all of the single-task experiments we use SGD with a momentum of 0.9 and a minibatch size of 10 - with the exception of detection, where we use a minibatch size of 2, following [32]. For the multi-task experiments we use our asynchronous SGD algorithm with effective minibatch sizes of 2 for detection-related parameters, 10 for other task-specific parameters and 30 for the shared CNN features, as justified in Sec. 3. With the exception of the initialization experiment described right below, we always use 5000 iterations, starting with a learning rate of 0.001 and decrease the learning rate by a factor of 10 after 3000 iterations. Other optimization schemes will be explored in a future version of this work.\nInitialization of labelling and detection network: We use a common initialization for all experiments, which requires having at our disposal parameters for both the convolutional labelling tasks, and the region-based detection task. We could use the ImageNet-pretrained VGG network for this, but exploiting pretraining on MS-COCO has been shown to yield boosts in performance e.g. in [14,32]. Leaving a joint pretraining on MS-COCO for a future version of this work, we take a shortcut and instead form a \u2018franken-\nstein\u2019 network where we stitch together two distinct variants of the VGG network, which have both been pretrained on MS-COCO. In particular we use the network of [14] for semantic segmentation (\u2018COCO-S\u2019) and the network of [32] for detection, (\u2018COCO-D\u2019).\nThe two-task network has (i) a common convolutional trunk, up to the fifth convolutional layer, (ii) a detection branch, combining an RPN and an SPP-Pooling layer followed by two fully-connected layers (fc6, fc7), and (iii) a fully-convolutional branch, used for semantic segmentation. The fully-connected branches in (ii) and (iii) are initialized with the parameters of the respective pretrained networks, COCO-D, COCO-S, while for (i) we initialize the parameters of the common layers with the COCO-D parameters. We finetune this network for 10000 iterations on the VOC07++ set [32] stands for the union of PASCAL VOC 2007 trainval and PASCAL VOC 2012 trainval sets; we start with a learning rate of 0.001 and decrease it to 0.0001 after 6000 iterations.\nDatasets: A summary of the datasets used in our experiments is provided in Table. 4. The 5100 images in the BSD dataset correspond to dataset augmentation of the 300 trainval images of BSD with 16 additional rotations. All of these numbers are effectively doubled with flipping-based dataset augmentation, while the VOC-related datasets are used twice, which amounts to placing a higher emphasis on the high-level tasks.\nWe note that the VOC\u201912 validation is used for the evaluation of the human part segmentation, semantic boundary detection and saliency estimation tasks. This means that in general we report numbers for two distinct networks: one where the VOC2012 validation set is included during training, based on which we report results on detection and semantic segmentation; and one where VOC2012 validation is excluded from training, which gives us results on human parts, semantic boundaries, and saliency."}, {"heading": "5.2. Experimental Evaluation", "text": "Object Detection: We start by verifying in the \u2018Ours 1-Task\u2019 row that we can replicate the results of [32]; exceptionally for this experiment, rather than using the ini-\ntialization described above, we start from the MS-COCO pretrained network of [32], finetune on the VOC 2007++ dataset, and test on the VOC 2007 test dataset. The only differences are that we use a minimal image side of 621 rather than 600, a maximal side of 961 rather than 1000, so as to comply with the 32k + 1 restriction on dimensions of [14], and use convolution with holes followed by appropriately modified RPN and ROI-pooling layers to get effectively identical results as [32]. Adding holes to the RPN network did not seem to help (not reported).\nIn the following row we measure the performance of the network obtained by training for the joint segmentation and detection task, which as mentioned in Sec. 5.1 will serve as our starting point for all ensuing experiments. After finetuning on VOC2007++ we observe that we actually get a small boost in performance, which is quite promising, since it is likely to be telling us that the additional supervision signal for semantic segmentation helped the detection subnetwork learn something better about detection.\nHowever when increasing the number of tasks performance drops, but is still comparable to the strong baseline of [32]. As we will see in the Sec. 5.3 this is not necessarily obvious - a difference choice of task weight parameters can adversely influence detection performance while favoring other tasks.\nSemantic Segmentation: The second task that we have tried is semantic segmentation. Even though a really broad range of techniques have been devised for the problem (see e.g. review in [14] for a recent comparison), we only compare to the methods lying closest to our own, which in turns relies on the \u2018Deeplab-Large Field of View (FOV)\u2019 architecture of [13]. We remind that, as detailed in Sec. 2, we deviate from the Deeplab architecture by using linear operations on top of skip layers, by using a multi-scale architecture, and by not using any DenseCRF post-processing.\nWe first observe that thanks to the use of multi-scale processing we get a similar improvement over the singlescale architecture as the one we had obtained in [49]. Understandably this ranks below the latest state-of-the-art results, such as the ones obtained e.g. in [14] with ResNets and Atrous Spatial Pyramid Pooling; but these advances are complementary and easy to include in our network\u2019s archi-\ntecture. Turning to the results of the two-task architecture, we observe that quite surprisingly we get effectively the same performance. This is not obvious at all, given that for this twotask network our starting point has been a VGG-type network that uses the detection network parameters up to the fifth convolutional layer, rather than the segmentation parameters. Apparently, after 10000 iterations of fine-tuning the shared representation was modified to be appropriate for the semantic segmentation task.\nTurning to the multi-task network performance, we observe that performance drops as the number of tasks increases. Still, even without using CRF post-processing, we fare comparably to a strong baseline, such as [78].\nHuman Part Segmentation: This task can be understood as a special case of semantic segemntation, where now we aim at assigning human part labels. Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].\nWe use the dataset introduced in [17] and train a network that is architecturally identical to the one used for semantic segmentation, but is now finetuned for the task of segmenting human parts. As a general comment on this task we can observe that here structured prediction yields quite substantial improvements, apparently due to the highly confined structure of the output space in this labelling task. Since we do not use any such post-processing yet, it is only fair to\ncompare to the first, CRF-free method, and leave an integration with structured prediction for future work.\nAs can be seen in Table. 10 for the single-task case we perform comparably. However, for the multi-task case performance drops substantially (more than 10%). This may be due to the scarcity of data that contain annotation for the task: when training the single task network all of the data contain annotations for human parts, while when training the multi-task network we have human part annotations in only 3432 out of the 59552 images used to train the whole network (referring to Table. 4, we remind the reader that we use twice the VOC-related datasets). One potential remedy is to increase the weight of the task\u2019s loss, or the learning rates of the task-specific parameters, so that the parameter updates are more effective; another alternative is to give the multi-task network more training iterations, so that we pass more times over the part annotations. We are exploring these options.\nSemantic Boundary Detection: We evaluate our method on the Semantic Boundary Detection task defined in [36], where the goal is to find where instances of the 20 PASCAL classes have discontinuities. This can be understood as a combination of semantic segmetnation and boundary detection, but can be tackled head-on with fully convolutional networks. We train on the VOC2012 train and evaluate on VOC2012 val.\nWe compare to the original method of [36], the situational boundary detector of [100], and the High-for-Low method of [7]. The authors of [7] go beyond the individual task of boundary detection and explore what gains can be obtained by providing as inputs to this task the results of a separate semantic segmentation system (\u2018High-for-LowCRF\u2019 row). Even though combining the outputs of different tasks is one of our immediate next goals, we do not consider it yet here. Still, we observe that even applying our architecture out-of-the-box we get reasonably close results, and substantially better than their standalone semantic boundary detection result. Performance deteriorates for the multitask case, but remains quite close to the current \u2018standalone\u2019\nstate-of-the-art. Boundary Detection: We train our network on the union of the (dataset-augmented) BSD trainval set and boundary images from the VOC context dataset [71] and evaluate it on the test set of the Berkeley Segmentation Dataset (BSD) of [69] and. We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].\nA first experiment has been to train our new network with the exact same experimental setup as the one we had used in [49] - including Graduated Deep Supervised Network training, a mix of 30600 images obtained by dataset augmentation from the BSD (300 images augmented by 3 scales, 2 horizontal flips, and 16 rotations) with 20206 images from VOC-context (10103 images with two horizontal flips). The differences are that we now use batch normalization, which allows us to increase the layer-specific learning rates up to 10, and also that we now use the \u2018convolutionalized\u2019 fully-connected layers of the VGG network. The improvement in performance is quite substantial: the maximal F-measure increases from 0.809 to 0.815, surpassing even the performance we would get in [49] by using spectral boundaries on top.\nStill, these settings (graduated DSN, high learning rates) were not as successful on the remaining tasks, while using a mix of data where the images of BSD are three times more than the images of VOC would skew the performance substantially in favor of the low-level task of boundary detection - since the training objective is clearly affected by the number of images containing ground truth for one task\nversus the other. We therefore remove the side losses for the skip layers, reduce the layer-specific learning rate to 1 (which is still quite higher than the 0.001 layer-specific rate used in [49, 107]), and use the particular mix of data used to train the UberNet in the multi-task setup. This means that, after leftright flipping, we use 10200 boundary samples from BSD (i.e. no scale augmentation) and 20206 samples from VOC.\nAs shown in the \u2018Ours, 1 Task\u2019 row of Table. 9 this can substantially affect performance - but we still remain competitive to previous state-of-the-art works, such as [107]. For the multi-task training case performance drops a bit more, but always stays at a reasonably good level when compared to standard strong baselines such as [107].\nSaliency Estimation: We train on the MSRA-10K dataset of [101] and evaluate on the PASCAL-S dataset of [59], where a subset of the Pascal VOC10 validation set is annotated. Additional datasets are typically used to benchmark this task, e.g. in [58], we will explore the performance of our method on those datasets in the future.\nWe only use flipping for dataset augmentation during training. We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning. We note that our method sets a new state-of-the-art for this dataset, and even for the multi-task training case, our method outperforms the previous state-ofthe-art which was the CRF-based variant of [58].\nSurface Normal Estimation: For this task surface normals are typically estimated from point cloud data, rather than directly measured. Both the training, and also the evaluation of a normal estimation algorithm may therefore be affected by this step. We train on the training set if [74] where normals are estimated by [54] and extend it with 20K\nimages of normal ground truth estimated from the raw images in the training scenes of [74]; since the method of [54] is not publicly available, we use as a surrogate the method of [86]. Competing methods, e.g. [2, 25] are using alternative normal estimation methods for the extended data, but we would not expect the differences because of this to be too large.\nWe report multiple results for the single-task training case, obtained by setting values to the weight \u03b3 of the loss term in Eq. 3. We observe that this can have a quite substantial impact on performance. When setting a large weight we can directly compete with the current state-of-the-art, while a low weight can reduce performance substantially. As we will see however in the following subsection, it becomes necessary to set a reasonably low weight, or else this may have adverse effects on the performance of the remaining tasks. When using that lower weight, we witness a further drop in performance for the multi-task case.\nEven though our multi-task network\u2019s performance is not too different from the plain CNN-based result of [103], it is clear that there we have a somewhat unique gap in performance, when compared to what we seen in the remaining tasks.\nOur conjencture is that this may be due to the geometric, and continuous nature of this task, which is quite different from the remaining labelling tasks. It may be that both the intermediate and final features of the VGG network are not appropriate for this task \u2018out-of-the-box\u2019, and it takes substantially large-scale modifications to the inner workings of the network (corresponding to a large weight on the task-specific loss) until the nonlinearities within the VGG network can accommodate the task. It is however interesting that both competing methods (VGG-MLP [2], VGGcascade [25]) address the task by using additional layers on top of the VGG network (a Multi-Layer Perceptron in [2],\na coarse-to-fine cascade in [25]). Even though in this work we have limited ourselves to using linear functions on top of the skip layers for the sake of simplicity and efficiency, these successes suggest that adding instead nonlinearities could be a way of improving performance for this task, as well as potentially also for other tasks."}, {"heading": "5.3. Effect of task weights", "text": "The performance of our network on the multitude of task it adresses depends on the weights assigned to the losses of different tasks in Eq. 3. If the weight of one task is substantially larger, one can expect that this will skew the internal representation of the network in favor of the particular task, while negecting others.\nMotivated by the empirical results in the previous paragraphs we have explored the impact of modifying the weight attributed to the normal estimation task in Eq. 3 in the case of solving multiple, rather than individual tasks. In Table. 12 we report how performance changes when we increase the weight of the normal estimation task (previous experiments relied on the \u03b3 = 1 option).\nWe realize that, at least for our particular experimental settings, there is \u2018no free lunch\u2019, and the performance measures of the different tasks act like communicating vessels. The evaluation may arguably be affected by our optimization choices; using e.g. larger batch sizes, or more iterations and a polynomial schedule as in [14] could help. But the present results indicate that the common CNN trunk has apparently a bounded learning capacity, and suggests that inserting more parameters, potentially through additional nonlinear layers on top of the skip layers, may be needed to maintain high performance across all tasks. We will explore these directions in the future, as well as whether this effect persists when working with deeper networks such as ResNets."}, {"heading": "6. Conclusions and Future Work", "text": "In this work we have introduced two techniques that allow us to train a CNN that tackles a broad set of computer vision problems in a unified architecture. We have shown that one can effectively scale up to many and diverse tasks, since the memory complexity is independent of the number of tasks, and incoherently annotated datasets can be combined during training.\nThere are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113]. Research in these directions is underway, but, more importantly, we consider this work to be a first step in the direction of jointly tackling multiple tasks by exploiting the\nsynergy between them - this has been a recurring theme in computer vision, e.g. for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks. The code for this work will soon be made publicly available from http://cvn.ecp.fr/ubernet/."}, {"heading": "7. Acknowledgements", "text": "This work has been supported by the FP7-RECONFIG, FP7-MOBOT, and H2020-ISUPPORT EU projects, and equipment donated by NVIDIA. I thank George Papandreou for pointing out how low-memory backpropagation can be implemented, Pierre-Andre\u0301 Savalle for showing me how to handle prototxt files, Ross Girshick for making the FasterRCNN system publicly available, and Nikos Paragios for creating the environment where this work took place."}], "references": [{"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "PAMI,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Marr revisited: 2d-3d alignment via surface normal prediction", "author": ["A. Bansal", "B. Russell", "A. Gupta"], "venue": "Proc. CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent human pose estimation", "author": ["V. Belagiannis", "A. Zisserman"], "venue": "CoRR, abs/1605.02914,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Material recognition in the wild with the materials in context database", "author": ["S. Bell", "P. Upchurch", "N. Snavely", "K. Bala"], "venue": "Proc. CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks", "author": ["S. Bell", "C.L. Zitnick", "K. Bala", "R. Girshick"], "venue": "Proc. CVPR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepedge: A multi-scale bifurcated deep network for top-down contour detection", "author": ["G. Bertasius", "J. Shi", "L. Torresani"], "venue": "Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "High-for-low and low-for-high: Efficient boundary detection from deep object features and its applications to high-level vision", "author": ["G. Bertasius", "J. Shi", "L. Torresani"], "venue": "Proc. ICCV,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Integrated perception with recurrent multi-task neural networks", "author": ["H. Bilen", "A. Vedaldi"], "venue": "Proc. NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. Bengio", "Y. LeCun"], "venue": "Proc. CVPR,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Describing people: A poselet-based approach to attribute classification", "author": ["L.D. Bourdev", "S. Maji", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast, exact and multiscale inference for semantic image segmentation with deep gaussian crfs", "author": ["S. Chandra", "I. Kokkinos"], "venue": "Proc. ECCV,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform", "author": ["L. Chen", "J.T. Barron", "G. Papandreou", "K. Murphy", "A.L. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "Proc. ICLR,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "CoRR, abs/1606.00915,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention to scale: Scale-aware semantic image segmentation", "author": ["L. Chen", "Y. Yang", "J. Wang", "W. Xu", "A.L. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Training deep nets with sublinear memory cost", "author": ["T. Chen", "B. Xu", "C. Zhang", "C. Guestrin"], "venue": "CoRR, abs/1604.06174,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Detect what you can: Detecting and representing objects using holistic models and body parts", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Global contrast-based salient region detection", "author": ["M.-M. Cheng", "N.J. Mitra", "X. Huang", "P.H.S. Torr", "S.-M. Hu"], "venue": "PAMI,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep filter banks for texture recognition, description, and segmentation", "author": ["M. Cimpoi", "S. Maji", "I. Kokkinos", "A. Vedaldi"], "venue": "IJCV,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "Proc. ICCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Instance-aware semantic segmentation via multi-task network cascades", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "Proc. CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "R-FCN: object detection via region-based fully convolutional networks", "author": ["J. Dai", "Y. Li", "K. He", "J. Sun"], "venue": "Proc. NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast edge detection using structured forests", "author": ["P. Doll\u00e1r", "C.L. Zitnick"], "venue": "PAMI,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Image super-resolution using deep convolutional networks", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "PAMI,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting depth, surface normals and semantic labels with a common multiscale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "Proc. ICCV,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.M.A. Eslami", "L.J.V. Gool", "C.K.I. Williams", "J.M. Winn", "A. Zisserman"], "venue": "IJCV,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "PAMI,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "N\u02c6 4-fields: Neural network nearest neighbor fields for image transforms", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "Proc. ACCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Laplacian reconstruction and refinement for semantic segmentation", "author": ["G. Ghiasi", "C.C. Fowlkes"], "venue": "Proc. ECCV,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Attend refine repeat: Active box proposal generation via in-out localization", "author": ["S. Gidaris", "N. Komodakis"], "venue": "CoRR, abs/1606.04446,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast R-CNN", "author": ["R.B. Girshick"], "venue": "Proc. ICCV,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Contextual action recognition with r*cnn", "author": ["G. Gkioxari", "R.B. Girshick", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory-efficient backpropagation through time", "author": ["A. Gruslys", "R. Munos", "I. Danihelka", "M. Lanctot", "A. Graves"], "venue": "CoRR, abs/1606.03401,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Matchnet: Unifying feature and metric learning for patch-based matching", "author": ["X. Han", "T. Leung", "Y. Jia", "R. Sukthankar", "A.C. Berg"], "venue": "Proc. CVPR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "Proc. ECCV,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Hypercolumns for object segmentation and finegrained localization", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "Proc. CVPR,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning dense convolutional embeddings for semantic segmentation", "author": ["A.W. Harley", "K.G. Derpanis", "I. Kokkinos"], "venue": "CoRR, abs/1511.04377,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. CVPR,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Pixel-wise deep learning for contour detection", "author": ["J.-J. Hwang", "T.-L. Liu"], "venue": "Proc. ICLR,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepercut: A deeper, stronger, and faster multi-person pose estimation model", "author": ["E. Insafutdinov", "L. Pishchulin", "B. Andres", "M. Andriluka", "B. Schiele"], "venue": "CoRR, abs/1605.03170,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proc. ICML,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R.B. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the ACM,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Salient region detection by UFO: uniqueness, focusness and objectness", "author": ["P. Jiang", "H. Ling", "J. Yu", "J. Peng"], "venue": "Proc. ICCV,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Locally scale-invariant convolutional neural networks", "author": ["A. Kanazawa", "A. Sharma", "D.W. Jacobs"], "venue": "CoRR, abs/1412.5104,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Integrated segmentation and recognition of hand-printed numerals", "author": ["J.D. Keeler", "D.E. Rumelhart", "W.K. Leow"], "venue": "Proc. NIPS,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1990}, {"title": "Visual boundary prediction: A deep neural prediction network and quality dissection", "author": ["J.J. Kivinen", "C.K.I. Williams", "N. Heess"], "venue": "AISTATS,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Pushing the boundaries of boundary detection using deep learning", "author": ["I. Kokkinos"], "venue": "ICLR,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}, {"title": "An expectation maximization approach to the synergy between image segmentation and object categorization", "author": ["I. Kokkinos", "P. Maragos"], "venue": "Proc. ICCV, volume I, pages 617\u2013624,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "NIPS,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Obj-cut", "author": ["M.P. Kumar", "P. Torr", "A. Zisserman"], "venue": "Proc. CVPR,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2005}, {"title": "Discriminatively trained dense surface normal estimation", "author": ["L. Ladicky", "B. Zeisl", "M. Pollefeys"], "venue": "Proc. ECCV,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "CoRR, abs/1603.06668,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1998}, {"title": "Visual saliency based on multiscale deep features", "author": ["G. Li", "Y. Yu"], "venue": "Proc. CVPR,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep contrast learning for salient object detection", "author": ["G. Li", "Y. Yu"], "venue": "Proc. CVPR,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "The secrets of salient object segmentation", "author": ["Y. Li", "X. Hou", "C. Koch", "J.M. Rehg", "A.L. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic object parsing with graph LSTM", "author": ["X. Liang", "X. Shen", "J. Feng", "L. Lin", "S. Yan"], "venue": "Proc. CVPR,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "I.D. Reid", "A. van den Hengel"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2016}, {"title": "Microsoft COCO: common objects in context", "author": ["T. Lin", "M. Maire", "S.J. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Proc. ECCV,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional neural fields for depth estimation from a single image", "author": ["F. Liu", "C. Shen", "G. Lin"], "venue": "Proc. CVPR,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning depth from single monocular images using deep convolutional neural fields", "author": ["F. Liu", "C. Shen", "G. Lin", "I.D. Reid"], "venue": "CoRR, abs/1502.07411,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2015}, {"title": "SSD: single shot multibox detector", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S.E. Reed"], "venue": "CoRR, abs/1512.02325,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2015}, {"title": "Parsenet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "CoRR, abs/1506.04579,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. CVPR,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "Object detection and segmentation from joint embedding of parts and pixels", "author": ["M. Maire", "S.X. Yu", "P. Perona"], "venue": "ICCV,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2001}, {"title": "Cross-stitch networks for multi-task learning", "author": ["I. Misra", "A. Shrivastava", "A. Gupta", "M. Hebert"], "venue": "Proc. CVPR,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2016}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2014}, {"title": "Neuronal Architectures for Pattern Theoretic Problems", "author": ["D. Mumford"], "venue": "Large Scale Theories of the Cortex. MIT Press,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1994}, {"title": "Direct intrinsics: Learning albedo-shading decomposition by convolutional regression", "author": ["T. Narihira", "M. Maire", "S.X. Yu"], "venue": "Proc. ICCV,", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2015}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["P.K. Nathan Silberman", "Derek Hoiem", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2012}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["A. Newell", "K. Yang", "J. Deng"], "venue": "CoRR, abs/1603.06937,", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Proc. ICCV,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2015}, {"title": "Is object localization for free? - weakly-supervised learning with convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "Proc. CVPR,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2015}, {"title": "Weakly- and semi-supervised learning of a DCNN for semantic image segmentation", "author": ["G. Papandreou", "L. Chen", "K. Murphy", "A.L. Yuille"], "venue": "Proc. ICCV,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection", "author": ["G. Papandreou", "I. Kokkinos", "P. Savalle"], "venue": "Proc. CVPR,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2015}, {"title": "Saliency filters: Contrast based filtering for salient region detection", "author": ["F. Perazzi", "P. Kr\u00e4henb\u00fchl", "Y. Pritch", "A. Hornung"], "venue": "Proc. CVPR,", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2012}, {"title": "Flowing convnets for human pose estimation in videos", "author": ["T. Pfister", "J. Charles", "A. Zisserman"], "venue": "Proc. ICCV,", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to segment object candidates", "author": ["P.H.O. Pinheiro", "R. Collobert", "P. Doll\u00e1r"], "venue": "Proc. NIPS,", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2015}, {"title": "Saliency detection via cellular automata", "author": ["Y. Qin", "H. Lu", "Y. Xu", "H. Wang"], "venue": "Proc. CVPR,", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2015}, {"title": "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition", "author": ["R. Ranjan", "V.M. Patel", "R. Chellappa"], "venue": "CoRR, abs/1603.01249,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster R- CNN: towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R.B. Girshick", "J. Sun"], "venue": "Proc. NIPS,", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminatively trained sparse code gradients for contour detection", "author": ["X. Ren", "L. Bo"], "venue": "Proc. NIPS,", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2012}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "Proc. MICCAI,", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei- Fei"], "venue": "IJCV,", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "Proc. ICLR,", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection", "author": ["W. Shen", "X. Wang", "Y. Wang", "X. Bai", "Z. Zhang"], "venue": "Proc. CVPR,", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2015}, {"title": "Object skeleton extraction in natural images by fusing scale-associated deep side outputs", "author": ["W. Shen", "K. Zhao", "Y. Jiang", "Y. Wang", "Z. Zhang", "X. Bai"], "venue": "Proc. CVPR,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative learning of deep convolutional feature point descriptors", "author": ["E. Simo-Serra", "E. Trulls", "L. Ferraz", "I. Kokkinos", "P. Fua", "F. Moreno-Noguer"], "venue": "Proc. ICCV,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Proc. ICLR,", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proc. CVPR,", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I.J. Goodfellow", "R. Fergus"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2013}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "Proc. CVPR,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning-based symmetry detection in natural images", "author": ["S. Tsogkas", "I. Kokkinos"], "venue": "Proc. ECCV,", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning for semantic part segmentation with high-level guidance", "author": ["S. Tsogkas", "I. Kokkinos", "G. Papandreou", "A. Vedaldi"], "venue": "CoRR, abs/1505.02438,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2015}, {"title": "Image Parsing: Unifying Segmentation, Detection, and Recognition", "author": ["Z.W. Tu", "X. Chen", "A. Yuille", "S.C. Zhu"], "venue": "Proc. ICCV,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2003}, {"title": "Situational object boundary detection", "author": ["J.R.R. Uijlings", "V. Ferrari"], "venue": "Proc. CVPR,", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2015}, {"title": "PISA: pixelwise image saliency by aggregating complementary appearance contrast measures with edgepreserving coherence", "author": ["K. Wang", "L. Lin", "J. Lu", "C. Li", "K. Shi"], "venue": "IEEE Trans. Im. Proc.,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep networks for saliency detection via local estimation and global search", "author": ["L. Wang", "H. Lu", "X. Ruan", "M. Yang"], "venue": "Proc. CVPR,", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}, {"title": "Designing deep networks for surface normal estimation", "author": ["X. Wang", "D.F. Fouhey", "A. Gupta"], "venue": "Proc. CVPR,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional pose machines", "author": ["S. Wei", "V. Ramakrishna", "T. Kanade", "Y. Sheikh"], "venue": "Proc. CVPR,", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2016}, {"title": "Scale-space filtering", "author": ["A.P. Witkin"], "venue": "IJCAI,", "citeRegEx": "105", "shortCiteRegEx": null, "year": 1983}, {"title": "Zoom better to see clearer: Human part segmentation with auto zoom net", "author": ["F. Xia", "P. Wang", "L. Chen", "A.L. Yuille"], "venue": "Proc. ECCV,", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2016}, {"title": "Holistically-nested edge detection", "author": ["S. Xie", "Z. Tu"], "venue": "Proc. ICCV,", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2015}, {"title": "Noisy label recovery for shadow detection in unfamiliar domains", "author": ["T.F. Yago Vicente", "M. Hoai", "D. Samaras"], "venue": "Proc. CVPR,", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2016}, {"title": "Lift: Learned invariant feature transform", "author": ["K.M. Yi", "E. Trulls", "V. Lepetit", "P. Fua"], "venue": "Proc. ECCV,", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "Proc. CVPR,", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2015}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "Proc. ECCV,", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2014}, {"title": "Saliency detection by multi-context deep learning", "author": ["R. Zhao", "W. Ouyang", "H. Li", "X. Wang"], "venue": "Proc. CVPR,", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr"], "venue": "Proc. ICCV,", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 55, "context": "While Convolutional Neural Networks (CNNs) have been the method of choice for text recognition for more than two decades [56], they have only been recently shown to successful at handling effectively most, if not all, vision tasks.", "startOffset": 121, "endOffset": 125}, {"referenceID": 23, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 128, "endOffset": 132}, {"referenceID": 54, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 172, "endOffset": 188}, {"referenceID": 27, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 172, "endOffset": 188}, {"referenceID": 48, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 172, "endOffset": 188}, {"referenceID": 106, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 172, "endOffset": 188}, {"referenceID": 90, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 209, "endOffset": 213}, {"referenceID": 108, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 240, "endOffset": 245}, {"referenceID": 34, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 265, "endOffset": 278}, {"referenceID": 91, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 265, "endOffset": 278}, {"referenceID": 109, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 265, "endOffset": 278}, {"referenceID": 87, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 45, "endOffset": 49}, {"referenceID": 61, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 58, "endOffset": 62}, {"referenceID": 73, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 80, "endOffset": 84}, {"referenceID": 68, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 89, "endOffset": 93}, {"referenceID": 35, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 76, "endOffset": 84}, {"referenceID": 70, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 76, "endOffset": 84}, {"referenceID": 35, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 141, "endOffset": 149}, {"referenceID": 70, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 141, "endOffset": 149}, {"referenceID": 16, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 189, "endOffset": 193}, {"referenceID": 9, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 231, "endOffset": 235}, {"referenceID": 70, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 344, "endOffset": 348}, {"referenceID": 90, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 395, "endOffset": 399}, {"referenceID": 96, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 406, "endOffset": 410}, {"referenceID": 9, "context": "[10, 17, 71], but as the number of task grows it becomes impossible to use one dataset for all.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[10, 17, 71], but as the number of task grows it becomes impossible to use one dataset for all.", "startOffset": 0, "endOffset": 12}, {"referenceID": 70, "context": "[10, 17, 71], but as the number of task grows it becomes impossible to use one dataset for all.", "startOffset": 0, "endOffset": 12}, {"referenceID": 1, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 18, "endOffset": 30}, {"referenceID": 24, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 18, "endOffset": 30}, {"referenceID": 102, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 18, "endOffset": 30}, {"referenceID": 24, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 49, "endOffset": 61}, {"referenceID": 62, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 49, "endOffset": 61}, {"referenceID": 63, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 49, "endOffset": 61}, {"referenceID": 72, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 93, "endOffset": 97}, {"referenceID": 107, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 115, "endOffset": 120}, {"referenceID": 18, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 175, "endOffset": 178}, {"referenceID": 57, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 200, "endOffset": 209}, {"referenceID": 111, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 200, "endOffset": 209}, {"referenceID": 13, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 233, "endOffset": 244}, {"referenceID": 26, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 233, "endOffset": 244}, {"referenceID": 66, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 233, "endOffset": 244}, {"referenceID": 29, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 273, "endOffset": 283}, {"referenceID": 81, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 273, "endOffset": 283}, {"referenceID": 84, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 273, "endOffset": 283}, {"referenceID": 20, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 307, "endOffset": 319}, {"referenceID": 36, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 307, "endOffset": 319}, {"referenceID": 81, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 307, "endOffset": 319}, {"referenceID": 2, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 14, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 41, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 74, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 80, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 95, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 97, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 103, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 110, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 21, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 30, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 31, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 39, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 51, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 84, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 88, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 92, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 93, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 92, "context": "Most of these works rely on finetuning a common pretrained CNN, such as the VGG network [93] or others [40, 52], which indicates the broad potential of these CNNs.", "startOffset": 88, "endOffset": 92}, {"referenceID": 39, "context": "Most of these works rely on finetuning a common pretrained CNN, such as the VGG network [93] or others [40, 52], which indicates the broad potential of these CNNs.", "startOffset": 103, "endOffset": 111}, {"referenceID": 51, "context": "Most of these works rely on finetuning a common pretrained CNN, such as the VGG network [93] or others [40, 52], which indicates the broad potential of these CNNs.", "startOffset": 103, "endOffset": 111}, {"referenceID": 94, "context": "Apart from simplicity and efficiency, the problem can also be motivated by arguing that by training a network so as to accomplish multiple tasks one leaves smaller space for \u2018blindspots\u2019 [95], effectively providing a more complete specification of the network duties.", "startOffset": 187, "endOffset": 191}, {"referenceID": 8, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 46, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 49, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 52, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 67, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 71, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 98, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 88, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 75, "endOffset": 79}, {"referenceID": 32, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 195, "endOffset": 199}, {"referenceID": 69, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 284, "endOffset": 288}, {"referenceID": 7, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 399, "endOffset": 402}, {"referenceID": 83, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 510, "endOffset": 514}, {"referenceID": 25, "context": "object positions, landmarks in PASCAL VOC [26]) is often missing from the datasets used for low-level tasks (e.", "startOffset": 42, "endOffset": 46}, {"referenceID": 68, "context": "BSD [69]), and vice versa.", "startOffset": 4, "endOffset": 8}, {"referenceID": 73, "context": "If we consider for instance a network that is supposed to be predicting both human landmarks and surface normals, we have no dataset where an image comes with annotations for both tasks, but rather disjoint datasets (NYU [74], and PASCAL VOC [26], or any other pose estimation dataset for keypoints) providing every image with annotations for only one of the two.", "startOffset": 221, "endOffset": 225}, {"referenceID": 25, "context": "If we consider for instance a network that is supposed to be predicting both human landmarks and surface normals, we have no dataset where an image comes with annotations for both tasks, but rather disjoint datasets (NYU [74], and PASCAL VOC [26], or any other pose estimation dataset for keypoints) providing every image with annotations for only one of the two.", "startOffset": 242, "endOffset": 246}, {"referenceID": 15, "context": "Instead, we build on recent developments in learning with deep architectures [16, 34] which have shown that it is possible to efficiently train a deep CNN with a memory complexity that is sublinear in the number of layers.", "startOffset": 77, "endOffset": 85}, {"referenceID": 33, "context": "Instead, we build on recent developments in learning with deep architectures [16, 34] which have shown that it is possible to efficiently train a deep CNN with a memory complexity that is sublinear in the number of layers.", "startOffset": 77, "endOffset": 85}, {"referenceID": 28, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 25, "endOffset": 41}, {"referenceID": 74, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 25, "endOffset": 41}, {"referenceID": 75, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 25, "endOffset": 41}, {"referenceID": 86, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 25, "endOffset": 41}, {"referenceID": 13, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 69, "endOffset": 82}, {"referenceID": 50, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 69, "endOffset": 82}, {"referenceID": 112, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 69, "endOffset": 82}, {"referenceID": 11, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 117, "endOffset": 125}, {"referenceID": 38, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 117, "endOffset": 125}, {"referenceID": 10, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 203, "endOffset": 210}, {"referenceID": 60, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 203, "endOffset": 210}, {"referenceID": 55, "context": "The starting point is that of using a standard \u2018fully\u2019 convolutional network [56,67,79,89], namely a CNN that provides a field of decision variables, rather than a single classificiation at its output; this can be used to accomplish any dense labelling or regression task, such as boundary detection, normal estimation, or semantic segmentation.", "startOffset": 77, "endOffset": 90}, {"referenceID": 66, "context": "The starting point is that of using a standard \u2018fully\u2019 convolutional network [56,67,79,89], namely a CNN that provides a field of decision variables, rather than a single classificiation at its output; this can be used to accomplish any dense labelling or regression task, such as boundary detection, normal estimation, or semantic segmentation.", "startOffset": 77, "endOffset": 90}, {"referenceID": 78, "context": "The starting point is that of using a standard \u2018fully\u2019 convolutional network [56,67,79,89], namely a CNN that provides a field of decision variables, rather than a single classificiation at its output; this can be used to accomplish any dense labelling or regression task, such as boundary detection, normal estimation, or semantic segmentation.", "startOffset": 77, "endOffset": 90}, {"referenceID": 88, "context": "The starting point is that of using a standard \u2018fully\u2019 convolutional network [56,67,79,89], namely a CNN that provides a field of decision variables, rather than a single classificiation at its output; this can be used to accomplish any dense labelling or regression task, such as boundary detection, normal estimation, or semantic segmentation.", "startOffset": 77, "endOffset": 90}, {"referenceID": 37, "context": "Skip layers: one first deviation from the most standard architecture is that as in [38, 49, 89, 107] we use skip layers that combine the top-layer neurons with the activations of intermediate neurons to form the network output.", "startOffset": 83, "endOffset": 100}, {"referenceID": 48, "context": "Skip layers: one first deviation from the most standard architecture is that as in [38, 49, 89, 107] we use skip layers that combine the top-layer neurons with the activations of intermediate neurons to form the network output.", "startOffset": 83, "endOffset": 100}, {"referenceID": 88, "context": "Skip layers: one first deviation from the most standard architecture is that as in [38, 49, 89, 107] we use skip layers that combine the top-layer neurons with the activations of intermediate neurons to form the network output.", "startOffset": 83, "endOffset": 100}, {"referenceID": 106, "context": "Skip layers: one first deviation from the most standard architecture is that as in [38, 49, 89, 107] we use skip layers that combine the top-layer neurons with the activations of intermediate neurons to form the network output.", "startOffset": 83, "endOffset": 100}, {"referenceID": 106, "context": "Tasks such as boundary detection clearly profit from the smaller degree of spatial abstraction of lower-level neurons [107], while even for high-level tasks, such as semantic segmentation, it has been shown [38,67] that skip layers can improve performance.", "startOffset": 118, "endOffset": 123}, {"referenceID": 37, "context": "Tasks such as boundary detection clearly profit from the smaller degree of spatial abstraction of lower-level neurons [107], while even for high-level tasks, such as semantic segmentation, it has been shown [38,67] that skip layers can improve performance.", "startOffset": 207, "endOffset": 214}, {"referenceID": 66, "context": "Tasks such as boundary detection clearly profit from the smaller degree of spatial abstraction of lower-level neurons [107], while even for high-level tasks, such as semantic segmentation, it has been shown [38,67] that skip layers can improve performance.", "startOffset": 207, "endOffset": 214}, {"referenceID": 4, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 45, "endOffset": 52}, {"referenceID": 65, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 45, "endOffset": 52}, {"referenceID": 42, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 81, "endOffset": 85}, {"referenceID": 48, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 223, "endOffset": 232}, {"referenceID": 106, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 223, "endOffset": 232}, {"referenceID": 48, "context": "Cumulative task-specific operations: Scaling up to many tasks requires keeping the task-specific memory and computation budget low, and we therefore choose as in [49, 107] to process the outputs of the skip-pooling with task-specific layers that perform linear operations.", "startOffset": 162, "endOffset": 171}, {"referenceID": 106, "context": "Cumulative task-specific operations: Scaling up to many tasks requires keeping the task-specific memory and computation budget low, and we therefore choose as in [49, 107] to process the outputs of the skip-pooling with task-specific layers that perform linear operations.", "startOffset": 162, "endOffset": 171}, {"referenceID": 106, "context": "2 we observe that instead of simply adding the scores (sum-fusion), one can accelerate training by concatenating the score maps and learning a linear function that operates on top of the concatenated score maps - as originally done in [107].", "startOffset": 235, "endOffset": 240}, {"referenceID": 13, "context": "Atrous convolution: We also use convolution with holes (\u00e0 trous) [14,79] which allows us to control the spatial resolution of the output layer.", "startOffset": 65, "endOffset": 72}, {"referenceID": 78, "context": "Atrous convolution: We also use convolution with holes (\u00e0 trous) [14,79] which allows us to control the spatial resolution of the output layer.", "startOffset": 65, "endOffset": 72}, {"referenceID": 14, "context": "Multi-resolution CNN: as in [15,46,49,79], rather than processing an image at a single resolution, we form an im-", "startOffset": 28, "endOffset": 41}, {"referenceID": 45, "context": "Multi-resolution CNN: as in [15,46,49,79], rather than processing an image at a single resolution, we form an im-", "startOffset": 28, "endOffset": 41}, {"referenceID": 48, "context": "Multi-resolution CNN: as in [15,46,49,79], rather than processing an image at a single resolution, we form an im-", "startOffset": 28, "endOffset": 41}, {"referenceID": 78, "context": "Multi-resolution CNN: as in [15,46,49,79], rather than processing an image at a single resolution, we form an im-", "startOffset": 28, "endOffset": 41}, {"referenceID": 14, "context": "Even though in [15] a max-fusion scheme is shown to yield higher accuracy than sum-fusion, in our understanding this is particular to the case of semantic segmentation, where a large score at any scale suffices to assign an object label to a pixel.", "startOffset": 15, "endOffset": 19}, {"referenceID": 104, "context": "This may not be the case for boundaries where the score should be determined by the accumulation of evidence from multiple scales [105] or for normals, where maximization over scales of the normal vector entries does not make any sense.", "startOffset": 130, "endOffset": 135}, {"referenceID": 14, "context": "We therefore use a concatenation of the scores followed by a linear operation, as in the case of fusing the skip-layers described above, and leave the exploration of scale-aware processing [15] for the future.", "startOffset": 189, "endOffset": 193}, {"referenceID": 31, "context": "In this pyramid the highest resolution image is set similar to [32] so that the smallest image dimension is 621 pixels and the largest dimension does not exceed 921 (the exact numbers are so that dimensions are of the form 32k + 1, as requested by [15, 49]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "In this pyramid the highest resolution image is set similar to [32] so that the smallest image dimension is 621 pixels and the largest dimension does not exceed 921 (the exact numbers are so that dimensions are of the form 32k + 1, as requested by [15, 49]).", "startOffset": 248, "endOffset": 256}, {"referenceID": 48, "context": "In this pyramid the highest resolution image is set similar to [32] so that the smallest image dimension is 621 pixels and the largest dimension does not exceed 921 (the exact numbers are so that dimensions are of the form 32k + 1, as requested by [15, 49]).", "startOffset": 248, "endOffset": 256}, {"referenceID": 14, "context": "As in [15, 49] we use loss layers both at the outputs of the individual scales and the final responses, amounting to a mild form of deep supervision network (DSN) training [107].", "startOffset": 6, "endOffset": 14}, {"referenceID": 48, "context": "As in [15, 49] we use loss layers both at the outputs of the individual scales and the final responses, amounting to a mild form of deep supervision network (DSN) training [107].", "startOffset": 6, "endOffset": 14}, {"referenceID": 106, "context": "As in [15, 49] we use loss layers both at the outputs of the individual scales and the final responses, amounting to a mild form of deep supervision network (DSN) training [107].", "startOffset": 172, "endOffset": 177}, {"referenceID": 84, "context": "One exception to the uniform architecture outlined above is for detection, where we follow the work of [85] and learn a convolutional region proposal network, followed by a fully-connected subnetwork that classifies the region proposals into one of 21 labels (20 classes and background).", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "Recent advances however [22,65] may make this exception unnecessary.", "startOffset": 24, "endOffset": 31}, {"referenceID": 64, "context": "Recent advances however [22,65] may make this exception unnecessary.", "startOffset": 24, "endOffset": 31}, {"referenceID": 12, "context": "For region labelling tasks (semantic segmentation, human parts, saliency) and object detection we use the softmax loss function, as is common in all recent works on semantic segmentation [13,67] and object detection [32].", "startOffset": 187, "endOffset": 194}, {"referenceID": 66, "context": "For region labelling tasks (semantic segmentation, human parts, saliency) and object detection we use the softmax loss function, as is common in all recent works on semantic segmentation [13,67] and object detection [32].", "startOffset": 187, "endOffset": 194}, {"referenceID": 31, "context": "For region labelling tasks (semantic segmentation, human parts, saliency) and object detection we use the softmax loss function, as is common in all recent works on semantic segmentation [13,67] and object detection [32].", "startOffset": 216, "endOffset": 220}, {"referenceID": 31, "context": "For regression tasks (normal estimation, bounding box regression) we use the smooth `1 loss [32].", "startOffset": 92, "endOffset": 96}, {"referenceID": 48, "context": "For tasks where we want to estimate thin structures (boundaries, semantic boundaries) we use the MIL-based loss function introduced in [49] in order to accommodate imprecision in the placement of the boundary annotations.", "startOffset": 135, "endOffset": 139}, {"referenceID": 106, "context": "For these two tasks we also have a class imbalance problem, with many more negatives than positives; we mitigate this by using a weighted cross-entropy loss, as in [107], where we attribute a weight of 0.", "startOffset": 164, "endOffset": 169}, {"referenceID": 19, "context": "Certain recent works such as [20, 77, 78] manage to impute missing data in an EM-type approach, by exploiting domain-specific knowledge - e.", "startOffset": 29, "endOffset": 41}, {"referenceID": 76, "context": "Certain recent works such as [20, 77, 78] manage to impute missing data in an EM-type approach, by exploiting domain-specific knowledge - e.", "startOffset": 29, "endOffset": 41}, {"referenceID": 77, "context": "Certain recent works such as [20, 77, 78] manage to impute missing data in an EM-type approach, by exploiting domain-specific knowledge - e.", "startOffset": 29, "endOffset": 41}, {"referenceID": 31, "context": "In particular, for detection tasks it is reported in [32] that a batchsize of two suffices, while for dense labelling tasks such as semantic segmentation a batchsize of 10, 20 or even 30 is often used [14,107].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "In particular, for detection tasks it is reported in [32] that a batchsize of two suffices, while for dense labelling tasks such as semantic segmentation a batchsize of 10, 20 or even 30 is often used [14,107].", "startOffset": 201, "endOffset": 209}, {"referenceID": 106, "context": "In particular, for detection tasks it is reported in [32] that a batchsize of two suffices, while for dense labelling tasks such as semantic segmentation a batchsize of 10, 20 or even 30 is often used [14,107].", "startOffset": 201, "endOffset": 209}, {"referenceID": 15, "context": "In order to handle these problems we build on recent advances in memory-efficient backpropagation for deep networks [16, 34] and adapt them to the task of multitask learning1.", "startOffset": 116, "endOffset": 124}, {"referenceID": 33, "context": "In order to handle these problems we build on recent advances in memory-efficient backpropagation for deep networks [16, 34] and adapt them to the task of multitask learning1.", "startOffset": 116, "endOffset": 124}, {"referenceID": 15, "context": "We start by describing the basic idea behind the algorithm of [16], paving the way for the presentation of our extension to multi-task learning.", "startOffset": 62, "endOffset": 66}, {"referenceID": 43, "context": "In the popular Caffe [44] library memory is also allocated for all of the gradient signals, since a priori these could feed into multiple layers for a DAG network.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "The memory-efficient alternative described in [16] is shown in Fig.", "startOffset": 46, "endOffset": 50}, {"referenceID": 92, "context": "In order to examine this we compare primarily to results obtained by methods that rely on the VGG network [93] - more recent works e.", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "on detection [22] and semantic segmentation [14] have shown improvements through the use of deeper ResNets [40], but we consider the choice of network to be in a sense orthogonal to the goal of this section.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "on detection [22] and semantic segmentation [14] have shown improvements through the use of deeper ResNets [40], but we consider the choice of network to be in a sense orthogonal to the goal of this section.", "startOffset": 44, "endOffset": 48}, {"referenceID": 39, "context": "on detection [22] and semantic segmentation [14] have shown improvements through the use of deeper ResNets [40], but we consider the choice of network to be in a sense orthogonal to the goal of this section.", "startOffset": 107, "endOffset": 111}, {"referenceID": 31, "context": "9 and a minibatch size of 10 - with the exception of detection, where we use a minibatch size of 2, following [32].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "in [14,32].", "startOffset": 3, "endOffset": 10}, {"referenceID": 31, "context": "in [14,32].", "startOffset": 3, "endOffset": 10}, {"referenceID": 13, "context": "In particular we use the network of [14] for semantic segmentation (\u2018COCO-S\u2019) and the network of [32] for detection, (\u2018COCO-D\u2019).", "startOffset": 36, "endOffset": 40}, {"referenceID": 31, "context": "In particular we use the network of [14] for semantic segmentation (\u2018COCO-S\u2019) and the network of [32] for detection, (\u2018COCO-D\u2019).", "startOffset": 97, "endOffset": 101}, {"referenceID": 31, "context": "We finetune this network for 10000 iterations on the VOC07++ set [32] stands for the union of PASCAL VOC 2007 trainval and PASCAL VOC 2012 trainval sets; we start with a learning rate of 0.", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "Object Detection: We start by verifying in the \u2018Ours 1-Task\u2019 row that we can replicate the results of [32]; exceptionally for this experiment, rather than using the initialization described above, we start from the MS-COCO pretrained network of [32], finetune on the VOC 2007++ dataset, and test on the VOC 2007 test dataset.", "startOffset": 102, "endOffset": 106}, {"referenceID": 31, "context": "Object Detection: We start by verifying in the \u2018Ours 1-Task\u2019 row that we can replicate the results of [32]; exceptionally for this experiment, rather than using the initialization described above, we start from the MS-COCO pretrained network of [32], finetune on the VOC 2007++ dataset, and test on the VOC 2007 test dataset.", "startOffset": 245, "endOffset": 249}, {"referenceID": 13, "context": "The only differences are that we use a minimal image side of 621 rather than 600, a maximal side of 961 rather than 1000, so as to comply with the 32k + 1 restriction on dimensions of [14], and use convolution with holes followed by appropriately modified RPN and ROI-pooling layers to get effectively identical results as [32].", "startOffset": 184, "endOffset": 188}, {"referenceID": 31, "context": "The only differences are that we use a minimal image side of 621 rather than 600, a maximal side of 961 rather than 1000, so as to comply with the 32k + 1 restriction on dimensions of [14], and use convolution with holes followed by appropriately modified RPN and ROI-pooling layers to get effectively identical results as [32].", "startOffset": 323, "endOffset": 327}, {"referenceID": 31, "context": "However when increasing the number of tasks performance drops, but is still comparable to the strong baseline of [32].", "startOffset": 113, "endOffset": 117}, {"referenceID": 31, "context": "F-RCNN, [32] VOC 2007++ 73.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "2 F-RCNN, [32] MS-COCO + VOC 2007++ 78.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "review in [14] for a recent comparison), we only compare to the methods lying closest to our own, which in turns relies on the \u2018Deeplab-Large Field of View (FOV)\u2019 architecture of [13].", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "review in [14] for a recent comparison), we only compare to the methods lying closest to our own, which in turns relies on the \u2018Deeplab-Large Field of View (FOV)\u2019 architecture of [13].", "startOffset": 179, "endOffset": 183}, {"referenceID": 48, "context": "We first observe that thanks to the use of multi-scale processing we get a similar improvement over the singlescale architecture as the one we had obtained in [49].", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "in [14] with ResNets and Atrous Spatial Pyramid Pooling; but these advances are complementary and easy to include in our network\u2019s archi-", "startOffset": 3, "endOffset": 7}, {"referenceID": 77, "context": "Deeplab -COCO + CRF [78] 70.", "startOffset": 20, "endOffset": 24}, {"referenceID": 48, "context": "4 Deeplab Multi-Scale [49] 72.", "startOffset": 22, "endOffset": 26}, {"referenceID": 48, "context": "1 Deeplab Multi-Scale -CRF [49] 74.", "startOffset": 27, "endOffset": 31}, {"referenceID": 77, "context": "Still, even without using CRF post-processing, we fare comparably to a strong baseline, such as [78].", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 14, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 59, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 97, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 105, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 105, "context": "Deeplab LargeFOV [106] 51.", "startOffset": 17, "endOffset": 22}, {"referenceID": 105, "context": "78 Deeplab LargeFOV-CRF [106] 52.", "startOffset": 24, "endOffset": 29}, {"referenceID": 14, "context": "95 Multi-scale averaging [15] 54.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Attention [15] 55.", "startOffset": 10, "endOffset": 14}, {"referenceID": 105, "context": "Auto Zoom [106] 57.", "startOffset": 10, "endOffset": 15}, {"referenceID": 59, "context": "54 Graph-LSTM [60] 60.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "Table 7: Part segmentation - mean Intersction-Over-Union accuracy on the dataset of [17].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "We use the dataset introduced in [17] and train a network that is architecturally identical to the one used for semantic segmentation, but is now finetuned for the task of segmenting human parts.", "startOffset": 33, "endOffset": 37}, {"referenceID": 35, "context": "Semantic Boundary Detection: We evaluate our method on the Semantic Boundary Detection task defined in [36], where the goal is to find where instances of the 20 PASCAL classes have discontinuities.", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "Semantic Contours [36] 20.", "startOffset": 18, "endOffset": 22}, {"referenceID": 99, "context": "0 Situational Boundary [100] 31.", "startOffset": 23, "endOffset": 28}, {"referenceID": 6, "context": "6 High-for-Low [7] 47.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "7 High-for-Low-CRF [7] 54.", "startOffset": 19, "endOffset": 22}, {"referenceID": 35, "context": "Table 8: Semantic Boundary Detection Results: we report mean Average Precision (AP) performance (%) and Mean Max F-Measure Score on the validation set of PASCAL VOC 2010, provided by [36].", "startOffset": 183, "endOffset": 187}, {"referenceID": 35, "context": "We compare to the original method of [36], the situational boundary detector of [100], and the High-for-Low method of [7].", "startOffset": 37, "endOffset": 41}, {"referenceID": 99, "context": "We compare to the original method of [36], the situational boundary detector of [100], and the High-for-Low method of [7].", "startOffset": 80, "endOffset": 85}, {"referenceID": 6, "context": "We compare to the original method of [36], the situational boundary detector of [100], and the High-for-Low method of [7].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "The authors of [7] go beyond the individual task of boundary detection and explore what gains can be obtained by providing as inputs to this task the results of a separate semantic segmentation system (\u2018High-for-LowCRF\u2019 row).", "startOffset": 15, "endOffset": 18}, {"referenceID": 70, "context": "Boundary Detection: We train our network on the union of the (dataset-augmented) BSD trainval set and boundary images from the VOC context dataset [71] and evaluate it on the test set of the Berkeley Segmentation Dataset (BSD) of [69] and.", "startOffset": 147, "endOffset": 151}, {"referenceID": 68, "context": "Boundary Detection: We train our network on the union of the (dataset-augmented) BSD trainval set and boundary images from the VOC context dataset [71] and evaluate it on the test set of the Berkeley Segmentation Dataset (BSD) of [69] and.", "startOffset": 230, "endOffset": 234}, {"referenceID": 0, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 84, "endOffset": 91}, {"referenceID": 22, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 84, "endOffset": 91}, {"referenceID": 5, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 27, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 40, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 47, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 48, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 89, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 106, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 0, "context": "gPb-owt-ucm [1] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 22, "context": "696 SE-Var [23] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 47, "context": "803 DeepNets [48] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "758 N4-Fields [28] 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 5, "context": "784 DeepEdge [6] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 40, "context": "807 CSCNN [41] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 89, "context": "798 DeepContour [90] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 106, "context": "797 HED-fusion [107] 0.", "startOffset": 15, "endOffset": 20}, {"referenceID": 106, "context": "811 HED-late merging [107] 0.", "startOffset": 21, "endOffset": 26}, {"referenceID": 48, "context": "840 Multi-Scale [49] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 48, "context": "861 Multi-Scale +sPb [49] 0.", "startOffset": 21, "endOffset": 25}, {"referenceID": 48, "context": "866 Ours, training setup of [49] 0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 68, "context": "Table 9: Boundary Detection results: we report the maximal F meaure obtained at the Optimal Dataset Scale, Optimal Image Scale, as well as the Average Precision on the test set of the BSD dataset [69].", "startOffset": 196, "endOffset": 200}, {"referenceID": 48, "context": "A first experiment has been to train our new network with the exact same experimental setup as the one we had used in [49] - including Graduated Deep Supervised Network training, a mix of 30600 images obtained by dataset augmentation from the BSD (300 images augmented by 3 scales, 2 horizontal flips, and 16 rotations) with 20206 images from VOC-context (10103 images with two horizontal flips).", "startOffset": 118, "endOffset": 122}, {"referenceID": 48, "context": "815, surpassing even the performance we would get in [49] by using spectral boundaries on top.", "startOffset": 53, "endOffset": 57}, {"referenceID": 48, "context": "001 layer-specific rate used in [49, 107]), and use the particular mix of data used to train the UberNet in the multi-task setup.", "startOffset": 32, "endOffset": 41}, {"referenceID": 106, "context": "001 layer-specific rate used in [49, 107]), and use the particular mix of data used to train the UberNet in the multi-task setup.", "startOffset": 32, "endOffset": 41}, {"referenceID": 106, "context": "9 this can substantially affect performance - but we still remain competitive to previous state-of-the-art works, such as [107].", "startOffset": 122, "endOffset": 127}, {"referenceID": 106, "context": "For the multi-task training case performance drops a bit more, but always stays at a reasonably good level when compared to standard strong baselines such as [107].", "startOffset": 158, "endOffset": 163}, {"referenceID": 100, "context": "Saliency Estimation: We train on the MSRA-10K dataset of [101] and evaluate on the PASCAL-S dataset of [59], where a subset of the Pascal VOC10 validation set is annotated.", "startOffset": 57, "endOffset": 62}, {"referenceID": 58, "context": "Saliency Estimation: We train on the MSRA-10K dataset of [101] and evaluate on the PASCAL-S dataset of [59], where a subset of the Pascal VOC10 validation set is annotated.", "startOffset": 103, "endOffset": 107}, {"referenceID": 57, "context": "in [58], we will explore the performance of our method on those datasets in the future.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 35, "endOffset": 47}, {"referenceID": 44, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 35, "endOffset": 47}, {"referenceID": 79, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 35, "endOffset": 47}, {"referenceID": 57, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 76, "endOffset": 94}, {"referenceID": 82, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 76, "endOffset": 94}, {"referenceID": 101, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 76, "endOffset": 94}, {"referenceID": 111, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 76, "endOffset": 94}, {"referenceID": 57, "context": "We note that our method sets a new state-of-the-art for this dataset, and even for the multi-task training case, our method outperforms the previous state-ofthe-art which was the CRF-based variant of [58].", "startOffset": 200, "endOffset": 204}, {"referenceID": 79, "context": "SF [80] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "493 GC [18] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "539 DRFI [45] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 100, "context": "690 PISA [101] 0.", "startOffset": 9, "endOffset": 14}, {"referenceID": 82, "context": "660 BSCA [83] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 101, "context": "666 LEGS [102] 0.", "startOffset": 9, "endOffset": 14}, {"referenceID": 111, "context": "752 MC [112] 0.", "startOffset": 7, "endOffset": 12}, {"referenceID": 56, "context": "740 MDF [57] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 57, "context": "FCN [58] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 57, "context": "793 DCL [58] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 57, "context": "815 DCL + CRF [58] 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 58, "context": "Table 10: Saliency estimation results: we report the Maximal F-measure (MF) on the PASCAL Saliency dataset of [59].", "startOffset": 110, "endOffset": 114}, {"referenceID": 73, "context": "We train on the training set if [74] where normals are estimated by [54] and extend it with 20K", "startOffset": 32, "endOffset": 36}, {"referenceID": 53, "context": "We train on the training set if [74] where normals are estimated by [54] and extend it with 20K", "startOffset": 68, "endOffset": 72}, {"referenceID": 73, "context": "images of normal ground truth estimated from the raw images in the training scenes of [74]; since the method of [54] is not publicly available, we use as a surrogate the method of [86].", "startOffset": 86, "endOffset": 90}, {"referenceID": 53, "context": "images of normal ground truth estimated from the raw images in the training scenes of [74]; since the method of [54] is not publicly available, we use as a surrogate the method of [86].", "startOffset": 112, "endOffset": 116}, {"referenceID": 85, "context": "images of normal ground truth estimated from the raw images in the training scenes of [74]; since the method of [54] is not publicly available, we use as a surrogate the method of [86].", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "[2, 25] are using alternative normal estimation methods for the extended data, but we would not expect the differences because of this to be too large.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "[2, 25] are using alternative normal estimation methods for the extended data, but we would not expect the differences because of this to be too large.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "VGG-Cascade [25] 22.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "9 VGG-MLP [2] 19.", "startOffset": 10, "endOffset": 13}, {"referenceID": 102, "context": "8 VGG-Design [103] 26.", "startOffset": 13, "endOffset": 18}, {"referenceID": 102, "context": "2 VGG-fused [103] 27.", "startOffset": 12, "endOffset": 17}, {"referenceID": 53, "context": "Table 11: Normal Estimation on NYU-v2 using the ground truth of [54].", "startOffset": 64, "endOffset": 68}, {"referenceID": 102, "context": "Even though our multi-task network\u2019s performance is not too different from the plain CNN-based result of [103], it is clear that there we have a somewhat unique gap in performance, when compared to what we seen in the remaining tasks.", "startOffset": 105, "endOffset": 110}, {"referenceID": 1, "context": "It is however interesting that both competing methods (VGG-MLP [2], VGGcascade [25]) address the task by using additional layers on top of the VGG network (a Multi-Layer Perceptron in [2], a coarse-to-fine cascade in [25]).", "startOffset": 63, "endOffset": 66}, {"referenceID": 24, "context": "It is however interesting that both competing methods (VGG-MLP [2], VGGcascade [25]) address the task by using additional layers on top of the VGG network (a Multi-Layer Perceptron in [2], a coarse-to-fine cascade in [25]).", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "It is however interesting that both competing methods (VGG-MLP [2], VGGcascade [25]) address the task by using additional layers on top of the VGG network (a Multi-Layer Perceptron in [2], a coarse-to-fine cascade in [25]).", "startOffset": 184, "endOffset": 187}, {"referenceID": 24, "context": "It is however interesting that both competing methods (VGG-MLP [2], VGGcascade [25]) address the task by using additional layers on top of the VGG network (a Multi-Layer Perceptron in [2], a coarse-to-fine cascade in [25]).", "startOffset": 217, "endOffset": 221}, {"referenceID": 13, "context": "larger batch sizes, or more iterations and a polynomial schedule as in [14] could help.", "startOffset": 71, "endOffset": 75}, {"referenceID": 39, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 252, "endOffset": 256}, {"referenceID": 10, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 328, "endOffset": 345}, {"referenceID": 13, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 328, "endOffset": 345}, {"referenceID": 60, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 328, "endOffset": 345}, {"referenceID": 112, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 328, "endOffset": 345}, {"referenceID": 8, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 46, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 49, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 52, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 67, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 71, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 98, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}], "year": 2016, "abstractText": "In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a \u2018swiss knife\u2019 for vision tasks; we call this architecture an UberNet to indicate its overarching nature. We address two main technical challenges that emerge when broadening up the range of tasks handled by a single CNN: (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget. Properly addressing these two problems allows us to train accurate predictors for a host of tasks, without compromising accuracy. Through these advances we train in an end-to-end manner a CNN that simultaneously addresses (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all of these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this system can be found at cvn.ecp.fr/ubernet/.", "creator": "LaTeX with hyperref package"}}}