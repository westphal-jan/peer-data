{"id": "1609.02208", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation", "abstract": "Estimators of information theoretic measures such as entropy and mutual information are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In this paper, we combine both these approaches to design new estimators of entropy and mutual information that outperform state of the art methods. Our estimator uses local bandwidth choices of $k$-NN distances with a finite $k$, independent of the sample size. Such a local and data dependent choice improves performance in practice, but the bandwidth is vanishing at a fast rate, leading to a non-vanishing bias. We show that the asymptotic bias of the proposed estimator is universal; it is independent of the underlying distribution. Hence, it can be pre-computed and subtracted from the estimate. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the asymptotic geometry of nearest neighbors to order statistics is of independent mathematical interest.", "histories": [["v1", "Wed, 7 Sep 2016 22:11:39 GMT  (712kb,D)", "http://arxiv.org/abs/1609.02208v1", "24 pages 8 figures"]], "COMMENTS": "24 pages 8 figures", "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT stat.ML", "authors": ["weihao gao", "sewoong oh", "pramod viswanath"], "accepted": true, "id": "1609.02208"}, "pdf": {"name": "1609.02208.pdf", "metadata": {"source": "CRF", "title": "Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation", "authors": ["Weihao Gao", "Sewoong Oh", "Pramod Viswanath"], "emails": ["wgao9@illinois.edu", "swoh@illinois.edu", "pramodv@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "Unsupervised representation learning is one of the major themes of modern data science; a common theme among the various approaches is to extract maximally \u201cinformative\" features via information-theoretic metrics (entropy, mutual information and their variations) \u2013 the primary reason for the popularity of information theoretic measures is that they are invariant to one-to-one transformations and that they obey natural axioms such as data processing. Such an approach is evident in many applications, as varied as computational biology [17], sociology [30] and information retrieval [23], with the citations representing a mere smattering of recent works. Within mainstream machine learning, a systematic effort at unsupervised clustering and hierarchical information extraction is conducted in recent works of [37, 35]. The basic workhorse in all these methods is the computation of mutual information (pairwise and multivariate) from i.i.d. samples. Indeed, sample-efficient estimation of mutual information emerges as the central scientific question of interest in a variety of applications, and is also of fundamental interest to statistics, machine learning and information theory communities. While these estimation questions have been studied in the past three decades (and summarized in [40]), the renewed importance of estimating information theoretic measures in a sample-efficient manner is persuasively argued in a recent work [6], where the authors note that existing estimators perform poorly in several key scenarios of central interest (especially when the high dimensional random variables are strongly related to each other). The most common estimators (featured in scientific software packages) are nonparametric and involve k nearest neighbor (NN) distances between the samples. The widely used estimator of mutual information is the one by Kraskov and St\u00f6gbauer and Grassberger [16] and christened the KSG estimator (nomenclature based on the authors, cf. [6]) \u2013 while this estimator works well in practice (and performs much better than other approaches such as those based on kernel density estimation procedures), it \u2217Coordinated Science Lab and Department of Electrical and Computer Engineering \u2020Coordinated Science Lab and Department of Industrial and Enterprise Systems Engineering\nar X\niv :1\n60 9.\n02 20\n8v 1\n[ cs\n.I T\n] 7\nS ep\n2 01\nstill suffers in high dimensions. The basic issue is that the KSG estimator (and the underlying differential entropy estimator based on nearest neighbor distances by Kozachenko and Leonenko (KL) [15]) does not take advantage of the fact that the samples could lie in a smaller dimensional subspace (more generally, manifold) despite the high dimensionality of the data itself. Such lower dimensional structures effectively act as boundaries, causing the estimator to suffer from what is known as boundary biases.\nAmeliorating this deficiency is the central theme of recent works [7, 6, 22], each of which aims to improve upon the classical KL (differential) entropy estimator of [15]. A local SVD is used to heuristically improve the density estimate at each sample point in [6], while a local Gaussian density (with empirical mean and covariance weighted by NN distances) is heuristically used for the same purpose in [22]. Both these approaches, while inspired and intuitive, come with no theoretical guarantees (even consistency) and from a practical perspective involve delicate choice of key hyper parameters. An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics. The local density estimation method is a strong generalization of the traditional kernel density estimation methods, but requires a delicate normalization which necessitates the solution of certain integral equations (cf. Equation (9) of [21]). Indeed, such an elaborate numerical effort is one of the key impediments for the entropy estimator of [7] to be practically valuable. A second key impediment is that theoretical guarantees (such as consistency) can only be provided when the bandwidth is chosen globally (leading to poor sample complexity in practice) and consistency requires the bandwidth h to be chosen such that nhd \u2192\u221e and h\u2192 0, where n is the sample size and d is the dimension of the random variable of interest. More generally, it appears that a systematic application of local log-likelihood methods to estimate functionals of the unknown density from i.i.d. samples is missing in the theoretical statistics literature (despite local log-likelihood methods for regression and density estimation being standard textbook fare [41, 20]). We resolve each of these deficiencies in this paper by undertaking a comprehensive study of estimating the (differential) entropy and mutual information from i.i.d. samples using sample dependent bandwidth choices (typically fixed k-NN distances). This effort allows us to connect disparate threads of ideas from seemingly different arenas: NN methods, local log-likelihood methods, asymptotic order statistics and sample-dependent heuristic, but inspired, methods for mutual information estimation suggested in the work of [16].\nMain Results: We make the following contributions.\n1. Density estimation: Parameterizing the log density by a polynomial of degree p, we derive simple closed form expressions for the local log-likelihood maximization problem for the cases of p \u2264 2 for arbitrary dimensions, with Gaussian kernel choices. This derivation, posed as an exercise in [20, Exercise 5.2], significantly improves the computational efficiency upon similar endeavors in the recent efforts of [7, 22, 38].\n2. Entropy estimation: Using resubstitution of the local density estimate, we derive a simple closed form estimator of the entropy using a sample dependent bandwidth choice (of k-NN distance, where k is a fixed small integer independent of the sample size): this estimator outperforms state of the art entropy estimators in a variety of settings. Since the bandwidth is data dependent and vanishes too fast (because k is fixed), the estimator has a bias, which we derive a closed form expression for and show that it is independent of the underlying distribution and hence can be easily corrected: this is our main theoretical contribution, and involves new theorems on asymptotic statistics of nearest neighbors generalizing classical work in probability theory [29], which might be of independent mathematical interest.\n3. Generalized view: We show that seemingly very different approaches to entropy estimation \u2013 recent works of [6, 7, 22] and the classical work of fixed k-NN estimator of Kozachenko and Leonenko [15] \u2013 can all be cast in the local log-likelihood framework as specific kernel and sample dependent bandwidth choices. This allows for a unified view, which we theoretically justify by showing that resubstitution entropy estimation for any kernel choice using fixed k-NN distances as bandwidth involves a bias term that is independent of the underlying distribution (but depends on the specific choice of kernel and parametric density family). Thus our work is a strict mathematical generalization of the classical work of [15].\n4. Mutual Information estimation: The inspired work of [16] constructs a mutual information estimator that subtly altered (in a sample dependent way) the three KL entropy estimation terms, leading to superior empirical performance. We show that the underlying idea behind this change can be incorporated in our framework as well, leading to a novel mutual information estimator that combines the two ideas and outperforms state of the art estimators in a variety of settings.\nIn the rest of this paper we describe these main results, the sections organized in roughly the same order as the enumerated list."}, {"heading": "2 Local likelihood density estimation (LLDE)", "text": "Given n i.i.d. samples X1, . . . , Xn, estimating the unknown density fX(\u00b7) in Rd is a very basic statistical task. Local likelihood density estimators [21, 12] constitute state of the art and are specified by a weight function K : Rd \u2192 R (also called a kernel), a degree p \u2208 Z+ of the polynomial approximation, and the bandwidth h \u2208 R, and maximizes the local log-likelihood:\nLx(f) = n\u2211 j=1 K ( Xj \u2212 x h ) log f(Xj)\u2212 n \u222b K ( u\u2212 x h ) f(u) du , (1)\nwhere maximization is over an exponential polynomial family, locally approximating f(u) near x:\nloge fa,x(u) = a0 + \u3008a1, u\u2212 x\u3009+ \u3008u\u2212 x, a2(u\u2212 x)\u3009+ \u00b7 \u00b7 \u00b7+ ap[u\u2212 x, u\u2212 x, . . . , u\u2212 x] , (2)\nparameterized by a = (a0, . . . , ap) \u2208 R1\u00d7d\u00d7d 2\u00d7\u00b7\u00b7\u00b7\u00d7dp , where \u3008\u00b7, \u00b7\u3009 denotes the inner-product and ap[u, . . . , u] the p-th order tensor projection. The local likelihood density estimate (LLDE) is defined as f\u0302n(x) = fa\u0302(x),x(x) = ea\u03020(x), where a\u0302(x) \u2208 arg maxa Lx(fa,x). The maximizer is represented by a series of nonlinear equations, and does not have a closed form in general. We present below a few choices of the degrees and the weight functions that admit closed form solutions. Concretely, for p = 0, it is known that LDDE reduces to the standard Kernel Density Estimator (KDE) [21]:\nf\u0302n(x) = 1\nn n\u2211 i=1 K ( x\u2212Xi h )/\u222b K ( u\u2212 x h ) du . (3)\nIf we choose the step function K(u) = I(\u2016u\u2016 \u2264 1) with a local and data-dependent choice of the bandwidth h = \u03c1k,x where \u03c1k,x is the k-NN distance from x, then the above estimator recovers the popular k-NN density estimate as a special case, namely, for Cd = \u03c0d/2/\u0393(d/2 + 1),\nf\u0302n(x) = 1 n\n\u2211n i=1 I(\u2016Xi \u2212 x\u2016 \u2264 \u03c1k,x)\nVol{u \u2208 Rd : \u2016u\u2212 x\u2016 \u2264 \u03c1k,x} =\nk\nnCd \u03c1dk,x . (4)\nFor higher degree local likelihood, we provide simple closed form solutions and provide a proof in Section 8.1. Somewhat surprisingly, this result has eluded prior works [22, 38] and [7] which specifically attempted the evaluation for p = 2. Part of the subtlety in the result is to critically use the fact that the parametric family (eg., the polynomial family in (2)) need not be normalized themselves; the local log-likelihood maximization ensures that the resulting density estimate is correctly normalized so that it integrates to 1.\nProposition 2.1. [20, Exercise 5.2] For a degree p \u2208 {1, 2}, the maximizer of local likelihood (1) admits a closed form solution, when using the Gaussian kernel K(u) = e\u2212 \u2016u\u20162 2 . In case of p = 1,\nf\u0302n(x) = S0\nn(2\u03c0)d/2hd exp\n{ \u22121\n2\n1\nS20 \u2016S1\u20162\n} , (5)\nwhere S0 \u2208 R and S1 \u2208 Rd are defined for given x \u2208 Rd and h \u2208 R as\nS0 \u2261 n\u2211 j=1 e\u2212 \u2016Xj\u2212x\u2016 2 2h2 , S1 \u2261 n\u2211 j=1 1 h (Xj \u2212 x) e\u2212 \u2016Xj\u2212x\u2016 2 2h2 . (6)\nIn case of p = 2, for S0 and S1 defined as above,\nf\u0302n(x) = S0\nn(2\u03c0)d/2hd|\u03a3|1/2 exp\n{ \u2212 1\n2\n1\nS20 ST1 \u03a3 \u22121S1\n} , (7)\nwhere |\u03a3| is the determinant and S2 \u2208 Rd\u00d7d and \u03a3 \u2208 Rd\u00d7d are defined as\nS2 \u2261 n\u2211 j=1 1 h2 (Xj \u2212 x)(Xj \u2212 x)T e\u2212 \u2016Xj\u2212x\u2016 2 2h2 , \u03a3 \u2261 S0S2 \u2212 S1S T 1 S20 , (8)\nwhere it follows from Cauchy-Schwarz that \u03a3 is positive semidefinite.\nOne of the major drawbacks of the KDE and k-NN methods is the increased bias near the boundaries. LLDE provides a principled approach to automatically correct for the boundary bias, which takes effect only for p \u2265 2 [12, 31]. This explains the performance improvement for p = 2 in the figure below (left panel), and the gap increases with the correlation as boundary effect becomes more prominent. We use the proposed estimators with p \u2208 {0, 1, 2} to estimate the mutual information between two jointly Gaussian random variables with correlation r, from n = 500 samples, using resubstitution methods explained in the next sections. Each point is averaged over 100 instances.\nIn the right panel, we generate i.i.d. samples from a 2-dimensional Gaussian with correlation 0.9, and found local approximation f\u0302(u\u2212x\u2217) around x\u2217 denoted by the blue \u2217 in the center. Standard k-NN approach fits a uniform distribution over a circle enclosing k = 20 nearest neighbors (red circle). The green lines are the contours of the degree-2 polynomial approximation with bandwidth h = \u03c120,x. The figure illustrates that k-NN method suffers from boundary effect, where it underestimates the probability by over estimating the volume in (4). However, degree-2 LDDE is able to correctly capture the local structure of the pdf, correcting for boundary biases. Despite the advantages of the LLDE, it requires the bandwidth to be data independent and vanishingly small (sublinearly in sample size) for consistency almost everywhere \u2013 both of these are impediments to practical use since there is no obvious systematic way of choosing these hyperparameters. On the other hand, if we restrict our focus to functionals of the density, then both these issues are resolved: this is the focus of the next section where we show that the bandwidth can be chosen to be based on fixed k-NN distances and the resulting universal bias easily corrected.\n3 k-LNN Entropy Estimator We consider resubstitution entropy estimators of the form H\u0302(x) = \u2212(1/n) \u2211n i=1 log f\u0302n(Xi) and propose to use the local likelihood density estimator in (7) and a choice of bandwidth that is local (varying for each point x) and adaptive (based on the data). Concretely, we choose, for each sample point Xi, the bandwidth hXi to be the the distance to its k-th nearest neighbor \u03c1k,i. Precisely, we propose the following k-Local Nearest Neighbor (k-LNN) entropy estimator of degree-2:\nH\u0302 (n) kLNN(X) = \u2212\n1\nn n\u2211 i=1\n{ log\nS0,i n(2\u03c0)d/2\u03c1dk,i|\u03a3i|1/2 \u2212 1 2 1 S20,i ST1,i\u03a3 \u22121 i S1,i\n} \u2212Bk,d , (9)\nwhere subtracting Bk,d defined in Theorem 1 removes the asymptotic bias, and k \u2208 Z+ is the only hyper parameter determining the bandwidth. In practice k is a small integer fixed to be in the range 4 \u223c 8. We only use the dlog ne nearest subset of samples Ti = {j \u2208 [n] : j 6= i and \u2016Xi \u2212Xj\u2016 \u2264 \u03c1dlogne,i} in computing the quantities below:\nS0,i \u2261 \u2211\nj\u2208Ti,m\ne \u2212 \u2016Xj\u2212Xi\u2016\n2\n2\u03c12 k,i , S1,i \u2261 \u2211 j\u2208Ti,m 1 \u03c1k,i (Xj \u2212Xi)e \u2212 \u2016Xj\u2212Xi\u2016 2 2\u03c12 k,i ,\nS2,i \u2261 \u2211\nj\u2208Ti,m\n1\n\u03c12k,i (Xj \u2212Xi)(Xj \u2212Xi)T e\n\u2212 \u2016Xj\u2212Xi\u2016\n2\n2\u03c12 k,i , \u03a3i \u2261 S0,iS2,i \u2212 S1,iST1,i S20,i . (10)\nThe truncation is important for computational efficiency, but the analysis works as long as m = O(n1/(2d)\u2212\u03b5) for any positive \u03b5 that can be arbitrarily small. For a larger m, for example of \u2126(n), those neighbors that are further away have a different asymptotic behavior. We show in Theorem 1 that the asymptotic bias is independent of the underlying distribution and hence can be precomputed and removed, under mild conditions on a twice continuously differentiable pdf f(x) (cf. Lemma 3.1 below).\nTheorem 1. For k \u2265 3 and X1, X2, . . . , Xn \u2208 Rd are i.i.d. samples from a twice continuously differentiable pdf f(x), then\nlim n\u2192\u221e\nE[H\u0302(n)kLNN(X)] = H(X) , (11)\nwhere Bk,d in (9) is a constant that only depends on k and d. Further, if E[(log f(X))2] < \u221e then the variance of the proposed estimator is bounded by Var[H\u0302(n)kLNN(X)] = O((log n) 2/n).\nThis proves the L1 and L2 consistency of the k-LNN estimator; we relegate the proof to Section 10 for ease of reading the main part of the paper. The proof assumes Ansatz 1 (also stated in Section 10, which states that a certain exchange of limit holds. As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made. Our choice of a local adaptive bandwidth hXi = \u03c1k,i is crucial in ensuring that the asymptotic bias Bk,d does not depend on the underlying distribution f(x). This relies on a fundamental connection to the theory of asymptotic order statistics made precise in Lemma 3.1, which also gives the explicit formula for the bias below.\nThe main idea is that the empirical quantities used in the estimate (10) converge in large n limit to similar quantities defined over order statistics. We make this intuition precise in the next section. We define order statistics over i.i.d. standard exponential random variables E1, E2, . . . , Em and i.i.d. random variables \u03be1, \u03be2, . . . , \u03bem drawn uniformly (the Haar measure) over the unit sphere in Rd, for a variable m \u2208 Z+. We define for \u03b1 \u2208 {0, 1, 2},\nS\u0303(m)\u03b1 \u2261 m\u2211 j=1 \u03be (\u03b1) j ( \u2211j `=1E`) \u03b1 ( \u2211k `=1E` ) \u03b1 exp { \u2212 ( \u2211j `=1E` ) 2 2( \u2211k `=1E` ) 2 } , (12)\nwhere \u03be(0)j = 1, \u03be (1) j = \u03bej \u2208 Rd, and \u03be (2) j = \u03bej\u03be T j \u2208 Rd\u00d7d, and let S\u0303\u03b1 = limm\u2192\u221e S\u0303 (m) \u03b1 and \u03a3\u0303 = (1/S\u03030)2(S\u03030S\u03032\u2212 S\u03031S\u0303 T 1 ). We show that the limiting S\u0303\u03b1\u2019s are well-defined (in the proof of Theorem 1) and are directly related to the bias terms in the resubstitution estimator of entropy:\nBk,d = E[ log( k\u2211 `=1 E`) + d 2 log 2\u03c0 \u2212 logCd \u2212 log S\u03030 + 1 2 log \u2223\u2223\u03a3\u0303\u2223\u2223+ ( 1 2S\u030320 S\u0303T1 \u03a3\u0303 \u22121S\u03031) ] . (13)\nIn practice, we propose using a fixed small k such as five. For k \u2264 3 the estimator has a very large variance, and numerical evaluation of the corresponding bias also converges slowly. For some typical choices of k, we provide approximate evaluations below, where 0.0183(\u00b16) indicates empirical mean \u00b5 = 183\u00d7 10\u22124 with confidence interval 6\u00d710\u22124. In these numerical evaluations, we truncated the summation at m = 50, 000. Although we prove that Bk,d converges in m, in practice, one can choose m based on the number of samples and Bk,d can be evaluated for that m. Theoretical contribution: Our key technical innovation is a fundamental connection between nearest neighbor statistics and asymptotic order statistics, stated below as Lemma 3.1: we show that the (normalized) distances \u03c1`,i\u2019s jointly converge to the standardized uniform order statistics and the directions (Xj` \u2212 Xi)/\u2016Xj` \u2212Xi\u2016\u2019s converge to independent uniform distribution (Haar measure) over the unit sphere.\nConditioned on Xi = x, the proposed estimator uses nearest neighbor statistics on Z`,i \u2261 Xj` \u2212 x where Xj` is the `-th nearest neighbor from x such that Z`,i = ((Xj` \u2212 Xi)/\u2016Xj` \u2212 Xi\u2016)\u03c1`,i. Naturally, all the techniques we develop in this paper generalize to any estimators that depend on the nearest neighbor statistics {Z`,i}i,`\u2208[n] \u2013 and the value of such a general result is demonstrated later (in Section 4) when we evaluate the bias in similarly inspired entropy estimators [6, 7, 22, 15].\nLemma 3.1. Let E1, E2, . . . , Em be i.i.d. standard exponential random variables and \u03be1, \u03be2, . . . , \u03bem be i.i.d. random variables drawn uniformly over the unit (d\u2212 1)-dimensional sphere in d dimensions, independent of the Ei\u2019s. Suppose f is twice continuously differentiable and x \u2208 Rd satisfies that there exists \u03b5 > 0 such that f(a) > 0, \u2016\u2207f(a)\u2016 = O(1) and \u2016Hf (a)\u2016 = O(1) for any \u2016a\u2212 x\u2016 < \u03b5. Then for any m = O(log n), we have the following convergence conditioned on Xi = x:\nlim n\u2192\u221e\ndTV((cdnf(x)) 1/d(Z1,i, . . . , Zm,i ) , ( \u03be1E 1/d 1 , . . . , \u03bem( m\u2211 `=1 E`) 1/d )) = 0 . (14)\nwhere dTV(\u00b7, \u00b7) is the total variation and cd is the volume of unit Euclidean ball in Rd.\nEmpirical contribution: Numerical experiments suggest that the proposed estimator outperforms state-of-the-art entropy estimators, and the gap increases with correlation. The idea of using k-NN distance as bandwidth for entropy estimation was originally proposed by Kozachenko and Leonenko in [15], and is a special case of the k-LNN method we propose with degree 0 and a step kernel. We refer to Section 4 for a formal comparison. Another popular resubstitution entropy estimator is to use KDE in (3) [13], which is a special case of the k-LNN method with degree 0, and the Gaussian kernel is used in simulations. As comparison, we also study a new estimator [14] based on von Mises expansion (as opposed to simple re-substitution) which has an improved convergence rate in the large sample regime. In Figure 2 (left), we draw 100 samples i.i.d. from two standard Gaussian random variables with correlation r, and plot resulting mean squared error averaged over 100 instances. The ground truth, in this case is H(X) = log(2\u03c0e) + 0.5 log(1\u2212 r2). On the right, we repeat the same simulation for fixed r = 0.99999 and varying number of samples and m = 7 loge n.\nIn Figure 3, we repeat the same simulation for 6 standard Gaussian random variables with Cov(X1, X2) = Cov(X3, X4) = Cov(X5, X6) = r and Cov(Xi, Xj) = 0 for other pairs (i, j). On the left, we draw 100 i.i.d.\nsamples with various r. We plot resulting mean squared error averaged over 100 instances. The ground truth is H(X) = 3 log(2\u03c0e) + 1.5 log(1\u2212 r2). On the right, we repeat the same simulation for fixed r = 0.99999 and varying number of samples and m = 7 loge n.\nIn Figure 4 (left), we draw 100 samples i.i.d. from a mixture of two joint Gaussian distributions with zero mean and covariance (\n1 r r 1\n) and ( 1 \u2212r \u2212r 1 ) , respectively, and plot resulting average estimate over 100\ninstances. Here we plot an upper bound of the ground truth H(X) \u2264 log(2) + log(2\u03c0e) + 0.5 log(1\u2212 r2) for r \u2265 0.9. On the right, we repeat the same simulation for fixed r = 0.99999 and varying number of samples and m = 7 loge n.\n4 Universality of the k-LNN approach In this section, we show that Theorem 1 holds universally for a general family of entropy estimators, specified by the choice of k \u2208 Z+, degree p \u2208 Z+, and a kernel K : Rd \u2192 R, thus allowing a unified view of several seemingly disparate entropy estimators [15, 6, 7, 22]. The template of the entropy estimator is the following: given n i.i.d. samples, we first compute the local density estimate by maximizing the local likelihood (1) with bandwidth \u03c1k,i, and then resubstitute it to estimate entropy: H\u0302 (n) k,p,K(X) = \u2212(1/n) \u2211n i=1 log f\u0302n(Xi).\nTheorem 2. For the family of estimators described above, under the hypotheses of Theorem 1, if the solution to the maximization a\u0302(x) = arg maxa Lx(fa,x) exists for all x \u2208 {X1, . . . , Xn}, then for any choice of k \u2265 p+1, p \u2208 Z+, and K : Rd \u2192 R, the asymptotic bias is independent of the underlying distribution:\nlim n\u2192\u221e\nE[H\u0302(n)k,p,K(X)] = H(X) + B\u0303k,p,K,d , (15)\nfor some constant B\u0303k,d,p,K that only depends on k, p,K and d.\nWe provide a proof in Section 11. Although in general there is no simple analytical characterization of the asymptotic bias B\u0303k,p,K,d it can be readily numerically computed: since B\u0303k,p,K,d is independent of the underlying distribution, one can run the estimator over i.i.d. samples from any distribution and numerically approximate the bias for any choice of the parameters. However, when the maximization a\u0302(x) = arg maxa Lx(fa,x) admits a closed form solution, as is the case with proposed k-LNN, then B\u0303k,p,K,d can be characterized explicitly in terms of uniform order statistics.\nThis family of estimators is general: for instance, the popular KL estimator is a special case with p = 0 and a step kernel K(u) = I(\u2016u\u2016 \u2264 1). [15] showed (in a remarkable result at the time) that the asymptotic bias is independent of the dimension d and can be computed exactly to be log n\u2212 \u03c8(n) + \u03c8(k)\u2212 log k and \u03c8(k) is the digamma function defined as \u03c8(x) = \u0393\u22121(x)d\u0393(x)/dx. The dimension independent nature of this asymptotic bias term (of O(n\u22121/2) for d = 1 in [36, Theorem 1] and O(n\u22121/d) for general d in [8]) is special to the choice of p = 0 and the step kernel; we explain this in detail in Section 11, later in the paper. Analogously, the estimator in [6] can be viewed as a special case with p = 0 and an ellipsoidal step kernel.\n5 k-LNN Mutual information estimator Given an entropy estimator H\u0302KL, mutual information can be estimated: I\u03023KL = H\u0302KL(X) + H\u0302KL(Y ) \u2212 H\u0302KL(X,Y ). In [16], Kraskov and St\u00f6gbauer and Grassberger introduced I\u0302KSG(X;Y ) by coupling the choices of the bandwidths. The joint entropy is estimated in the usual way, but for the marginal entropy, instead of using kNN distances from {Xj}, the bandwidth hXi = \u03c1k,i(X,Y ) is chosen, which is the k nearest neighbor distance from (Xi, Yi) for the joint data {(Xj , Yj)}. Consider I\u03023LNN(X;Y ) = H\u0302kLNN(X) + H\u0302kLNN(Y )\u2212 H\u0302kLNN(X,Y ). Inspired by [16], we introduce the following novel mutual information estimator we denote by I\u0302LNN\u2212KSG(X;Y ). where for the joint (X,Y ) we use the LNN entropy estimator we proposed in (9), and for the marginal entropy we use the bandwidth hXi = \u03c1k,i(X,Y ) coupled to the joint estimator. Empirically, we observe I\u0302KSG outperforms I\u03023KL everywhere, validating the use of correlated bandwidths. However, the performance of I\u0302LNN\u2212KSG is similar to I\u03023LNN\u2013sometimes better and sometimes worse. In Figure 5 (left), we estimate mutual information under the same setting as in Figure 2 (left). For most regimes of correlation r, both 3LNN and LNN-KSG outperforms other state-of-the-art estimators. The gap\nincreases with correlation r. On the right, we draw i.i.d. samples from two random variables X and Y , where X is uniform over [0, 1] and Y = X + U , where U is uniform over [0, 0.01] independent of X. In the large sample limit, all estimators find the correct mutual information. The plot show how sensitive the estimates are, in the small sample regime. Both LNN and LNN-KSG are significantly more robust compared to other approaches. Mutual information estimators have been recently proposed in [6, 7, 22] based on local likelihood maximization. However, they involve heuristic choices of hyper-parameters or solving elaborate optimization and numerical integrations, which are far from being easy to implement.\nIn Figure 6, we test the mutual information estimators for Y = f(X) + U , where X is uniformly distributed over [0, 1] and U is uniformly distributed over [0, \u03b8], independent of X, for some noise level \u03b8. Similar simulation were studied in [7]. We draw 2500 i.i.d. sample points for each relationship. The plot show that for small noise level \u03b8, i.e., near-functional related random variables, our proposed estimators I\u03023LNN and I\u0302LNN\u2212KSG perform much better than 3KL and KSG estimators. Also our proposed estimators can handle both linear and nonlinear functional relationships.\nIn Figure 7, we test our estimators on linear and nonlinear relationships for both low-dimensional (D = 2) and high-dimensional (D = 5). Here Xi\u2019s are uniformly distributed over [0, 1] and U is uniformly distributed over [\u221238/2, 38/2], independently of Xi\u2019s. Similar simulation were studied in [6]. We can see that our estimators I\u03023LNN and I\u0302LNN\u2212KSG converges much faster than I\u03023KL and I\u0302KSG."}, {"heading": "6 Breaking the bandwidth barrier", "text": "While k-NN distance based bandwidth are routine in practical usage [31], the main finding of this work is that they also turn out to be the \u201ccorrect\" mathematical choice for the purpose of asymptotically unbiased estimation of an integral functional such as the entropy: \u2212 \u222b f(x) log f(x); we briefly discuss the ramifications below. Traditionally, when the goal is to estimate f(x), it is well known that the bandwidth should satisfy h\u2192 0 and nhd \u2192\u221e, for KDEs to be consistent. As a rule of thumb, h = 1.06\u03c3\u0302n\u22121/5 is suggested when d = 1 where \u03c3\u0302 is the sample standard deviation [41, Chapter 6.3]. On the other hand, when estimating entropy, as well as other integral functionals, it is known that resubstitution estimators of the form \u2212(1/n) \u2211n i=1 log f\u0302(Xi) achieve variances scaling as O(1/n) independent of the bandwidth [19]. This allows for a bandwidth as small as O(n\u22121/d). The bottleneck in choosing such a small bandwidth is the bias, scaling as O(h2 + (nhd)\u22121 + En) [19], where the lower order dependence on n, dubbed En, is generally not known. The barrier in choosing a global bandwidth of h = O(n\u22121/d) is the strictly positive bias whose value depends on the unknown distribution and cannot be subtracted off. However, perhaps surprisingly, the proposed local and adaptive choice of the k-NN distance admits an asymptotic bias that is independent of the unknown underlying distribution. Manually subtracting off the non-vanishing bias gives an asymptotically unbiased estimator, with a potentially faster\nconvergence as numerically compared below. Figure 8 illustrates how k-NN based bandwidth significantly improves upon, say a rule-of-thumb choice of O(n\u22121/(d+4)) explained above and another choice of O(n\u22121/(d+2)). In the left figure, we use the setting from Figure 2 (right) but with correlation r = 0.999. On the right, we generate X \u223c N (0, 1) and U from uniform [0, 0.01] and let Y = X + U and estimate I(X;Y ). Following recent advances in [18, 33], the proposed local estimator has a potential to be extended to, for example, Renyi entropy, but with a multiplicative bias as opposed to additive."}, {"heading": "7 Discussion", "text": "The topic of estimation of an integral functional of an unknown density from i.i.d. samples is a classical one in statistics and we tie together a few pertinent topics from the literature in the context of the results of this manuscript."}, {"heading": "7.1 Uniform order statistics and NN distances", "text": "The expression for the asymptotic bias in (13) which is independent of the underlying distribution forms the main result of this paper and crucially depends on Lemma 3.1. Precisely, the lemma implies that the quantities Si\u2019s in (10) converge in distribution to S\u0303i\u2019s in (12). There are two parts to this convergence result: the nearest neighbor distances converge to uniform order statistics and the directions to those nearest neighbors converge independently to Haar measures on the unit sphere. The former has been extensively studied, for example see [29] for a survey of results. The latter is a new result that we state in Lemma 3.1, and proved in Section 9. Intuitively, assuming smoothness, the probability density fX in the neighborhood of a sample Xi (as defined by the distance to the k-th nearest neighbor) converges to a uniform distribution over a ball (of radius decreasing at the rate \u03c1k,i = \u0398(n\u22121/d)), as more samples are collected. The nearest neighbor distances and directions converge to those from the uniform distribution over the ball, and Lemma 3.1 makes this intuition precise for the nearest m neighbors up to m = O(n1/(2d)\u2212 ) with any arbitrarily small but positive \u03b5.\nOnly the convergence analysis of the distances, and not the directions, is required for traditional k-NN\nbased estimators, such as the entropy estimator of [15]. In the seminal paper, [15] introduced resubstitution entropy estimators of the form H\u0302(X) = \u2212(1/n) \u2211n i=1 log f\u0302n(Xi) with f\u0302n(x) = k/(nCd \u03c1 d k,x) (as defined in (4)). This k-NN estimator has a non-vanishing asymptotic bias, which was computed as Bk,d = (\u03c8(k)\u2212 log(k)) with the digamma function \u03c8(\u00b7) and was suggested to be manually removed. For k = 1 this was proved in the original paper of [15], which later was extended in [32, 9] to general k. This mysterious bias term Bk,d = (\u03c8(k)\u2212 log(k)) whose original proofs in [15, 32, 9] provided little explanation for, can be alternatively proved with both rigor and intuition by making connections to uniform order statistics. For a special case of k = 1, with extra assumptions on the support being compact, such an elegant proof is provided in [2, Theorem 7.1] which explicitly applies the convergence of the nearest neighbor distance to uniform order statistics. Namely,\nE[H\u0302(X)] = E [ \u2212 1 n n\u2211 i=1 log ( k nCd \u03c1dk,Xi ) ] \u2192 E [ \u2212 log k f(Xi)\u2211k\nj=1Ej ] = H(X) + \u03c8(k)\u2212 log(k) ,\nwhere the asymptotic expression follows from Cd n f(x)\u03c1dk,x \u2192 \u2211k j=1Ej as shown, for example, in Lemma 3.1\nand we used E[log \u2211k j=1Ej ] = \u03c8(k), where \u03c8(k) = is the digamma function defined as \u03c8(x) = \u0393\n\u22121(x)d\u0393(x)/dx and for large x it is approximately log(x) up to O(1/x), i.e. \u03c8(x) = log x\u2212 1/(2x) + o(1/x). Note that this only requires the convergence of the distance and not the direction. Inspired by this modern approach, we extend such a connection in Lemma 3.1 to prove consistency of our estimator."}, {"heading": "7.2 Convergence rate of the bias", "text": "Establishing the convergence rate of the KL estimator is a challenging problem, and is not quite resolved despite work over the past three decades. The O(1/n) convergence rate of the variance is established in [3, 18, 2, 4] under various assumptions. Establishing the convergence rate of the bias is more challenging. It has been first studied in [10, 11], where root-n consistency is shown in 1-dimension with bounded support and assuming f(x) is bounded below. [36] is the first to prove a root mean squared error convergence rate of O(1/ \u221a n) for general densities with unbounded support in 1-dimension and exponentially decaying tail, such as the Gaussian density. These assumptions are relaxed in [5], where zeroes and fat tails are allowed in f(x). In general d-dimensions, [8, 33] prove bounds on the convergence rate of the bias for finite k = O(1), and [24, 1] for k = \u2126(log n). Establishing the convergence rate for the bias of the proposed local estimator is an interesting open problem \u2013 it is interesting to see if the superior empirical performance of the local estimator is captured in the asymptotics of rate of convergence of the bias.\nIt is intuitive that kernel density estimators can capture the structure in the distribution if the distribution lies on a lower dimensional manifold. This is made precise in [27], which also shows improved convergence rates for distributions whose support is on low dimensional manifolds. However, the estimator in [27] critically uses the geodesic distances between the sample points on the manifold. Given that the proposed estimators fit distributions locally, a concrete question of interest is whether such an improvement can be achieved without such an explicit knowledge of the geodesic distances, i.e., whether the local estimators automatically adapt to underlying lower dimensional structures."}, {"heading": "7.3 Ensemble estimators", "text": "Recent works [34, 25, 26, 1] have proposed ensemble estimators, which use known estimators based on kernel density estimators and k-NN methods and construct a new estimate by taking the weighted linear combination of those methods with varying bandwidth or k, respectively. With a proper choice of the weights, which can be computed analytically by solving a simple linear program, a boosting of the convergence rate can be achieved. The key property that allows the design of such ensemble estimators is that the leading terms (in terms of the sample size n) of the bias have a multiplicative constant that only depends on the unknown distribution. An intuitive explanation for this phenomenon is provided in [1] in the context of k-NN methods; it is interesting to explore if such a phenomenon continues in the k-LNN scenario studied in this paper. Such a study would potentially lead to ensemble-based estimators in the local setting and also naturally allow a careful understanding of the rate of convergence of the bias term."}, {"heading": "8 Proofs", "text": ""}, {"heading": "8.1 Proof of proposition 2.1", "text": "We first prove the derivation of the LLDE with degree p = 2 in Equation (7). The gradient of the local likelihood evaluated at the maximizer is zero [21], which gives a computational tool for finding the maximizer:\n1\nn n\u2211 j=1 K( Xj \u2212 x h ) = \u222b K( u\u2212 x h )ea0+a T 1 (u\u2212x)+(u\u2212x) T a2(u\u2212x)du , (16)\n1\nn n\u2211 j=1 Xj \u2212 x h K( Xj \u2212 x h ) = \u222b u\u2212 x h K( u\u2212 x h )ea0+a T 1 (u\u2212x)+(u\u2212x) T a2(u\u2212x)du , (17)\n1\nn n\u2211 j=1 (Xj \u2212 x)(Xj \u2212 x)T h2 K( Xj \u2212 x h )\n=\n\u222b (u\u2212 x)(u\u2212 x)T\nh2 K( u\u2212 x h )ea0+a T 1 (u\u2212x)+(u\u2212x) T a2(u\u2212x)du , (18)\nwhere K(x) = exp{\u2212\u2016x\u20162/2} is the Gaussian kernel. Notice that the left-hand side of the equations are S0/n, S1/n and S2/n, respectively. The RHS can be written in closed forms as:\n1 n S0 = (2\u03c0) d/2|M |\u22121/2ea0+ 12a T 1 M \u22121a1 , (19) 1 n S1 = 1 nh S0M \u22121a1 , (20) 1 n S2 = 1 nh2 S0(M \u22121 +M\u22121a1a T 1M \u22121) , (21)\nwhere M = h\u22122Id\u00d7d \u2212 2a2 assuming h sufficiently small such that M is positive definite. We want to derive f\u0302(x) = exp{a0} from the equations. From (20) we get M\u22121a1 = S1(h/S0). Together with (21), we get M\u22121 +M\u22121a1a T 1M \u22121 = S2(h 2/S0). Hence, M\u22121 = (S2/S0\u2212 (S1/S0)(S1/S0)T )h2 = h2\u03a3. Plug them in (19), we obtain the desired expression. Analogously, for the derivation of the LLDE with degree p = 1 in Equation (5), we get\n1 n S0 = (2\u03c0) d/2hdea0+ h2 2 a T 1 a1 , (22) 1 n S1 = h n S0a1 . (23)\nThis gives a1 = (1/(hS0))S1, and ea0 = (S0/(n(2\u03c0)d/2hd)) exp{\u22120.5\u2016S1\u20162/S20}."}, {"heading": "9 Proof of Lemma 3.1", "text": "Let us introduce some notations first. Define Sd\u22121 \u2261 {x \u2208 Rd : \u2016x\u2016 = 1} as the unit (d \u2212 1)-dimensional sphere and \u03c3d\u22121 as a normalized spherical measure on Sd\u22121. For any \u03b8 = (\u03b81, . . . , \u03b8m) \u2208 (Sd\u22121)m and x = (x1, . . . , xm) \u2208 Rm+ , define \u03b8x \u2261 (\u03b81x1, . . . , \u03b8mxm) \u2208 Rd\u00d7m. For any set B \u2208 Rd\u00d7m and \u03b8 \u2208 (Sd\u22121)m, define B\u03b8 = {x \u2208 Rm+ : \u03b8x \u2208 B}. Let {\u03bei}mi=1 be i.i.d. random variables uniformly over Sd\u22121. Then for any joint random variables (W1, . . . ,Wm) \u2208 Rm+ which are independent with {\u03bei}mi=1, we have\nP{(\u03be1W1, . . . , \u03bemWm) \u2208 B} = \u222b \u03b8\u2208(Sd\u22121)m P{(W1, . . . ,Wm) \u2208 B\u03b8 | \u03b8} d(\u03c3d\u22121)m(\u03b8) . (24)\nLet Z = (Z1,i, . . . , Zm,i), \u2016Z\u2016 = (\u2016Z1,i\u2016, . . . , \u2016Zm,i\u2016) and let E = (E1/d1 , . . . , ( \u2211m `=1E`)\n1/d), then\u2223\u2223\u2223\u2223\u2223P{ (cdnf(x))1/dZ \u2208 B }\u2212 P {( \u03be1E 1/d 1 , . . . , \u03bem( m\u2211 `=1 E`) 1/d ) \u2208 B }\u2223\u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223P{ (cdnf(x))1/dZ \u2208 B }\u2212 \u222b \u03b8\u2208(Sd\u22121)m P{(E1/d1 , . . . , ( m\u2211 `=1 E`) 1/d) \u2208 B\u03b8 | \u03b8} d(\u03c3d\u22121)m(\u03b8) \u2223\u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223P{ (cdnf(x))1/dZ \u2208 B }\u2212 \u222b \u03b8\u2208(Sd\u22121)m P{(cdnf(x))1/d\u2016Z\u2016 \u2208 B\u03b8 | \u03b8} d(\u03c3d\u22121)m(\u03b8) \u2223\u2223\u2223\u2223\u2223 +\n\u222b \u03b8\u2208(Sd\u22121)m \u2223\u2223\u2223P{(cdnf(x))1/d\u2016Z\u2016 \u2208 B\u03b8 | \u03b8} \u2212 P{E \u2208 B\u03b8 | \u03b8} \u2223\u2223\u2223 d(\u03c3d\u22121)m(\u03b8) . (25) Now consider the first term in (25). We consider two cases separately. Case 1. If \u2016Zm,i\u2016 \u2265 ( \u221a ncdf(x))\n\u22121/d, we show that the tail events happen with a low probability. Denote B(x, r) = {z : \u2016z \u2212 x\u2016 \u2264 r} and let p = P{t \u2208 B(x, \u2016Zm,i\u2016)} = \u222b B(x,\u2016Zm,i\u2016) f(t)dt. Since f is twice continuously differentiable, we can see that p \u2265 0.5cd\u2016Zm,i\u2016df(x) \u2265 0.5/ \u221a n for sufficiently large n. Therefore,\nP{\u2016Zm,i\u2016 \u2265 ( \u221a ncdf(x)) \u22121/d} = m\u22121\u2211 `=0 ( n ` ) p`(1\u2212 p)n\u2212` \u2264 m\u22121\u2211 `=0 n` ( 1\u2212 1 2 \u221a n )(n\u2212`) \u2264\nm\u22121\u2211 `=0 nle\u2212( \u221a n\u2212` \u221a n)/2 \u2264 mnme\u2212( \u221a n\u2212m/ \u221a n)/2 . (26)\nCase 2. If \u2016Zm,i\u2016 < ( \u221a ncdf(x)) \u22121/d, let B = {t : (cdnf(x))1/dt \u2208 B and \u2016tm\u2016 < ( \u221a ncdf(x))\n\u22121/d} and B\u03b8 = {t : (cdnf(x))1/dt \u2208 B\u03b8 and tm < ( \u221a ncdf(x)) \u22121/d}. Note that\nP(Z \u2208 A\u0303) = (n!/(n\u2212 k)!) \u222b t\u2208A\u0303 m\u220f j=1 f(x+ tj)PX(|X \u2212 x| > |tm|)n\u2212mdt , (27)\nwhich gives\u222b \u03b8\u2208(Sd\u22121)m P{(cdnf(x)) 1/d\u2016Z\u2016 \u2208 B\u03b8, \u2016Zm,i\u2016 < ( \u221a ncdf(x))\n\u22121/d | \u03b8} d(\u03c3d\u22121)m(\u03b8) P{(cdnf(x))1/dZ \u2208 B, \u2016Zm,i\u2016 < ( \u221a ncdf(x))\u22121/d}\n=\n\u222b \u03b8\u2208(Sd\u22121)m P{\u2016Z\u2016 \u2208 B\u03b8 | \u03b8} d(\u03c3 d\u22121)m(\u03b8)\nP{Z \u2208 B}\n=\n\u222b \u03b8\u2208(Sd\u22121)m n! (n\u2212k)! ( \u222b t\u2208B\u03b8 (\u220fm j=1 f(x+ \u03b8jtj) ) (P{\u2016X \u2212 x\u2016 > \u2016tm\u2016} )n\u2212m dt ) d(\u03c3d\u22121)m(\u03b8)\nn! (n\u2212k)! \u222b t\u2208B (\u220fm j=1 f(x+ tj) ) (P{\u2016X \u2212 x\u2016 > \u2016tm\u2016} )n\u2212m dt\n\u2264 sup\u03b8\u2208(Sd\u22121)m supt\u2208B\u03b8\n\u220fm j=1 f(x+ \u03b8jtj)\ninft\u2208B \u220fm j=1 f(x+ tj)\n\u2264\n( sup\u2016t\u2016\u2264( \u221a ncdf(x))\u22121/d f(x+ t)\ninf\u2016t\u2016\u2264( \u221a ncdf(x))\u22121/d f(x+ t)\n)m , (28)\nwhere the first inequality follows from the fact that \u222b \u03b8\u2208(Sd\u22121)m( \u222b B\u03b8 g(tm)dt)d(\u03c3 d\u22121)m(\u03b8) = \u222b B g(\u2016tm\u2016)dt.\nSince f is continuously differentiable, by mean value theorem, there exists a, b \u2208 B(x, ( \u221a ncdf(x))\n\u22121/d) such that\nsup\u2016t\u2016\u2264( \u221a ncdf(x))\u22121/d f(x+ t) inf\u2016t\u2016\u2264( \u221a ncdf(x))\u22121/d f(x+ t) = f(b) + (a\u2212 b)T\u2207f(a) f(b) \u2264 1 + 2(\n\u221a ncdf(x))\n\u22121/d\u2016\u2207f(a)\u2016 f(b) , (29)\nBy the assumption, there exists a ball B(x, \u03b5) such that \u2016\u2207f(a)\u2016 = O(1) and f(a) > 0 for all a \u2208 B(x, \u03b5), so for sufficiently large n such that ( \u221a ncdf(x))\n\u22121/d < \u03b5, there exists some constant C such that sup\u2016t\u2016\u2264( \u221a ncdf(x))\u22121/d\nf(x+t) \u2264 (1+Cn\u22121/(2d)) inf\u2016t\u2016\u2264(\u221ancdf(x))\u22121/d f(x+t). Therefore, (28) is upper bounded by (1 + Cn\u22121/(2d))m. Similarly, (28) is lower bounded by (1\u2212 Cn\u22121/(2d))m.\nFor simplicity, let E = {\u2016Zm,i\u2016 < ( \u221a ncdf(x))\n\u22121/d}. Then combining the two cases, the first term in (25) is bounded by:\u2223\u2223\u2223\u2223\u2223P{ (cdnf(x))1/dZ \u2208 B }\u2212 \u222b \u03b8\u2208(Sd\u22121)m P{(cdnf(x))1/d\u2016Z\u2016 \u2208 B\u03b8 | \u03b8} d(\u03c3d\u22121)m(\u03b8)\n\u2223\u2223\u2223\u2223\u2223 \u2264 P { (cdnf(x)) 1/dZ \u2208 B, EC } + \u222b \u03b8\u2208(Sd\u22121)m P{(cdnf(x))1/d\u2016Z\u2016 \u2208 B\u03b8, EC | \u03b8} d(\u03c3d\u22121)m(\u03b8)\n+ \u2223\u2223\u2223\u2223\u2223P{ (cdnf(x))1/dZ \u2208 B, E }\u2212 \u222b \u03b8\u2208(Sd\u22121)m P{(cdnf(x))1/d\u2016Z\u2016 \u2208 B\u03b8, E | \u03b8} d(\u03c3d\u22121)m(\u03b8) \u2223\u2223\u2223\u2223\u2223 \u2264 P{EC}+\n\u222b \u03b8\u2208(Sd\u22121)m P{EC} d(\u03c3d\u22121)m(\u03b8)\n+ P { (cdnf(x)) 1/dZ \u2208 B, E } \u2223\u2223\u2223\u2223\u2223 1\u2212 \u222b \u03b8\u2208(Sd\u22121)m P{(cdnf(x)) 1/d\u2016Z\u2016 \u2208 B\u03b8, E | \u03b8} d(\u03c3d\u22121)m(\u03b8) P { (cdnf(x))1/dZ \u2208 B, E } \u2223\u2223\u2223\u2223\u2223\n\u2264 2P{EC}+ P { (cdnf(x)) 1/dZ \u2208 B, E } max{(1 + Cn\u22121/(2d))m \u2212 1, 1\u2212 (1\u2212 Cn\u22121/(2d))m} \u2264 2mnme\u2212( \u221a n\u2212m/ \u221a n)/2 + max{(1 + Cn\u22121/(2d))m \u2212 1, 1\u2212 (1\u2212 Cn\u22121/(2d))m} . (30)\nNow consider the second term of (25). We will use Corollary 5.5.5 of [29] to show that this term vanishes for m = O(log n) and as n grows.\nLemma 9.1 (Corollary 5.5.5, [29]). Let Y1, Y2, . . . , Yn be i.i.d. samples from unknown distribution with pdf f . Let Y1:n \u2264 Y2:n \u2264 \u00b7 \u00b7 \u00b7 \u2264 Yn:n be the order statistics. Assume the density f satisfies | log f(y)| \u2264 Ly\u03b4 for 0 < y < y0 and f(y) = 0 for y < 0, where L and \u03b4 are constants. Then\ndTV ( n (Y1:n, Y2:n, . . . , Ym:n) , ( E1, E1 + E2, . . . , m\u2211 j=1 Ej ) ) \u2264 C0 ( (m/n)\u03b4m1/2 +m/n ) , (31)\nwhere C0 > 0 is a constant. E1, . . . , Em are i.i.d standard exponential random variables.\nNow for fixed x, consider the distribution of cdf(x)\u2016X \u2212 x\u2016d denoted by P\u0303 . Define Y1, Y2, . . . , Yn drawn i.i.d. from P\u0303 . We can see that cdf(x)\u2016Z\u2016d L = (Y1:n, . . . , Ym:n), where L = denotes equivalence in distribution. The pdf f\u0303 of P\u0303 is given by:\nf\u0303(t) = d\ndt P{cdf(x)\u2016X \u2212 x\u2016d \u2264 t} =\nd\ndt \u222b y\u2208B(x,rt) f(y)dy . (32)\nwhere rt = (t/(cdf(x)))1/d. Here we have:\ndrt dt\n= t1/d\u22121(cdf(x)) \u22121/d\nd =\n1\nf(x)dcdr d\u22121 t\n. (33)\nIf f is twice continuously differentiable, we have:\u2223\u2223\u2223 f\u0303(t)\u2212 1 \u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 ddt \u222b y\u2208B(x,rt) f(y)dy \u2212 1 \u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 drtdt ( ddrt \u222b y\u2208B(x,rt) f(y)dy)\u2212 1 \u2223\u2223\u2223\u2223\u2223 = 1\nf(x)dcdr d\u22121 t \u2223\u2223\u2223\u2223\u2223 ddrt (\u222b y\u2208B(x,rt) f(y)dy ) \u2212 f(x)dcdrd\u22121t \u2223\u2223\u2223\u2223\u2223 = 1\nf(x)dcdr d\u22121 t \u2223\u2223\u2223\u2223\u2223 \u222b y\u2208Sd\u22121(x,rt) (f(y)\u2212 f(x))d\u03c3d\u22121(y) \u2223\u2223\u2223\u2223\u2223 , (34)\nwhere Sd\u22121 is the (d\u22121)-sphere centered at x with radius rt and \u03c3d\u22121 is the spherical measure. By mean value theorem, there exists a(y) \u2208 B(x, rt) such that f(y)\u2212 f(x) = (y\u2212x)T\u2207f(x) + (a(y)\u2212x)THf (a(y))(a(y)\u2212x), where a(y) depends on y. Therefore,\u2223\u2223\u2223\u2223\u2223 \u222b y\u2208Sd\u22121(x,rt) (f(y)\u2212 f(x))d\u03c3d\u22121(y)\n\u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u222b y\u2208Sd\u22121(x,rt) (y \u2212 x)T\u2207f(x)d\u03c3d\u22121(y)\ufe38 \ufe37\ufe37 \ufe38 =0 + \u222b y\u2208Sd\u22121(x,rt) (a(y)\u2212 x)THf (a(y))(a(y)\u2212 x)d\u03c3d\u22121(y) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 ( sup\na\u2208B(x,rt) \u2016Hf (a)\u2016 \u2016a\u2212 x\u20162\n) \u03c3d\u22121(Sd\u22121(x, rt))\n\u2264 dcdrd+1t\n( sup\na\u2208B(x,rt) \u2016Hf (a)\u2016\n) (35)\nSince there exists a ball B(x, \u03b5) such that \u2016Hf (a)\u2016 = O(1) for all a \u2208 B(x, \u03b5). Therefore, for sufficiently small t such that rt < \u03b5, we have:\n\u2223\u2223\u2223 f\u0303(t)\u2212 1 \u2223\u2223\u2223 \u2264 dcdrd+1t ( supa\u2208B(x,rt) \u2016Hf (a)\u2016 )\nf(x)dcdr d\u22121 t\n= r2t\n( supa\u2208B(x,rt) \u2016Hf (a)\u2016 ) f(x) . (36)\nRecall that rt = (t/(cdf(x)))1/d, so there exists L > 0 such that |f\u0303(t) \u2212 1| \u2264 Lt2/d for sufficiently small t. Hence, | log f\u0303(t)| \u2264 L\u2032t2/d for some L\u2032 > 0 and sufficiently small t. So f\u0303 satisfies the condition in Lemma. 9.1 with \u03b4 = 2/d. Therefore, for any B\u03b8 \u2286 Rm+ , we have:\u2223\u2223\u2223P{(cdnf(x))1/d\u2016Z\u2016 \u2208 B\u03b8} \u2212 P{E \u2208 B\u03b8} \u2223\u2223\u2223\n\u2264 dTV  cdnf(x)\u2016Z\u2016d, (E1, E1 + E2, . . . , m\u2211 j=1 Ej )\n\u2264 C0 ( ( m\nn )2/dm1/2 +\nm\nn\n) . (37)\nTherefore, by combing (30) and (37), we have:\u2223\u2223\u2223\u2223\u2223P{ (cdnf(x))1/dZ \u2208 B }\u2212 P {( \u03be1E 1/d 1 , . . . , \u03bem( m\u2211 l=1 E`) 1/d ) \u2208 B }\u2223\u2223\u2223\u2223\u2223 \u2264 2mnme\u2212 \u221a n\u2212m/ \u221a n 2 + max{(1 + Cn \u22121 2d )m \u2212 1, 1\u2212 (1\u2212 Cn \u22121 2d )m}+ C0 ( ( m\nn )\n2 dm 1 2 +\nm\nn\n) , (38)\nfor any setB \u2208 Rd\u00d7m. Therefore, the total variation distance dTV((cdnf(x))1/d(Z1,i, Z2,i, . . . , Zm,i), (\u03be1E1/d1 , \u03be2(E1+ E2) 1/d, . . . , \u03bem( \u2211m `=1E`)\n1/d )) is bounded by the RHS quantity. By taking m = O(log n), the RHS converges to 0 as n goes to infinity. Therefore, we have the desired statement."}, {"heading": "10 Proof of Theorem 1", "text": "We first compute the asymptotic bias. We define new notations to represent the estimate as\nH\u0302 (n) k =\n1\nn n\u2211 i=1 { h ( (cdnf(Xi)) 1/dZk,i, S0,i, S1,i, S2,i) ) \u2212 log f(Xi)\ufe38 \ufe37\ufe37 \ufe38\n\u2261Hi\n} ,\nwhere h : Rd \u00d7 R\u00d7 Rd \u00d7 Rd\u00d7d \u2192 R is defined as\nh(t1, t2, t3, t4) =\nd log \u2016t1\u2016+ d log(2\u03c0)\u2212 log cd \u2212 log t2 + 1\n2 log\n( det ( t4 t2 \u2212 t3t T 3 t22 )) + 1 2 tT3 (t4 \u2212 t3tT3 )\u22121t3 . (39)\nLet Hi \u2261 h((cdnf(Xi))1/dZk,i, S0,i, S1,i, S2,i)) \u2212 log f(Xi). Since the terms H1, H2, . . . ,Hn are identically distributed, the expected value of H\u0302(n)k converges to\nlim n\u2192\u221e E[H\u0302(n)k ] = limn\u2192\u221eE[H1] = limn\u2192\u221eEX1 [ E[H1|X1] ] (40)\nTypical approach of dominated convergence theorem cannot be applied to the above limit, since analyzing E[H1|X1] for finite sample n is challenging. In order to exchange the limit with the (conditional) expectation, we assume the following Ansatz 1 to be true. As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39]. This assumption can be avoided for Renyi entropy as in the proof of consistency in [28] or for sharper results such as the convergence rate of the bias with respect to the sample size but with more assumptions as in [8, 33, 1].\nAnsatz 1. The following exchange of limit holds:\nlim n\u2192\u221e\nE[H1] = EX1 [\nlim n\u2192\u221e\nE[H1|X1] ] , (41)\nUnder this ansatz, perhaps surprisingly, we will show that the expectation inside converges to \u2212 log f(X1) plus some bias that is independent of the underlying distribution. Precisely, for almost every x and given X1 = x,\nE[H1|X1 = x] + log f(x) = E [ h((cdnf(x)) 1/dZk,i, S0,1, S1,i, S2,i) ]\n\u2212\u2192 Bk,d , (42)\nas n\u2192\u221e where Bk,d is a constant that only depends on k and d, defined in (44). This implies that\nEX1 [\nlim n\u2192\u221e\nE[H1|X1] ] = EX1 [\u2212 log f(X1) +Bk,d]\n= H(X) +Bk,d . (43)\nTogether with (40), this finishes the proof of the desired claim. We are now left to prove the convergence of (42). We first give a formal definition of the bias Bk,d by replacing the sample defined quantities by a similar quantities defined from order-statistics, and use Lemma 3.1 to prove the convergence. Recall that our order-statistics is defined by two sequences of m i.i.d. random variables: i.i.d. standard exponential random variables E1, . . . , Em and i.i.d. random variables \u03be1, . . . , \u03bem uniformly distributed over Sd\u22121. We define\nBk,d \u2261 E h  \u03bek( k\u2211\n`=1\nE`\n)1/d , S\u0303\n(\u221e) 0 , S\u0303 (\u221e) 1 , S\u0303 (\u221e) 2  , (44) where, as we will show, S\u0303(\u221e)\u03b1 is the limit of empirical quantity S\u03b1,i defined from samples for each \u03b1 \u2208 {0, 1, 2}, and we know that (cdnf(x))1/dZk,i converges to \u03bek( \u2211k `=1E`)\n1/d for almost every x from Lemma 3.1. S(\u221e) is defined by a convergent random sequence\nS\u0303(m)\u03b1 \u2261 m\u2211 j=1 \u03be (\u03b1) j (\n\u2211j `=1E`) \u03b1/d\n( \u2211k `=1E`) \u03b1/d exp\n{ \u2212 ( \u2211j `=1E` ) 2/d\n2( \u2211k `=1E` ) 2/d\n} , (45)\nwhere \u03be(0)j = 1, \u03be (1) j = \u03bej , \u03be (2) j = \u03bej\u03be T j and S\u0303 (\u221e) \u03b1 = limm\u2192\u221e S\u0303 (m) \u03b1 . This limit exists, since S\u0303 (m) 0 is nondecreasing in m, and the convergence of S\u0303(m)1 and S\u0303 (m) 2 follows from Lemma 10.1. We introduce sim-\npler notations for the joint random variables: S\u0303(m) = (\u03bek( \u2211k `=1E`) 1/d, S\u0303 (m) 0 , S\u0303 (m) 1 , S\u0303 (m) 2 ) and S\u0303 (\u221e) =\n(\u03bek( \u2211k `=1E`) 1/d, S\u0303 (\u221e) 0 , S\u0303 (\u221e) 1 , S\u0303 (\u221e) 2 ). Considering the quantities S (n) = ((cdnf(x)) 1/dZk,i, S0,i, S1,i, S2,i) defined from samples, we show that this converges to S\u0303(\u221e). Precisely, applying triangular inequality,\ndTV(S (n), S\u0303(\u221e)) \u2264 dTV(S(n), S\u0303(m)) + dTV(S\u0303(m), S\u0303(\u221e)) , (46)\nand we show that both terms converge to zero for any m = \u0398(log n). Given that h is continuous and bounded, this implies that\nlim n\u2192\u221e E[H1|X1 = x] = E[ lim n\u2192\u221e h(S(n))\u2212 log f(x)|X1 = x]\n= \u2212 log f(x) + E[h(S\u0303(\u221e))] ,\nfor almost every x, proving (43). The convergence of the first term follows from Lemma 3.1. Precisely, consider the function gm : Rd\u00d7m \u2192 Rd \u00d7 R\u00d7 Rd \u00d7 Rd\u00d7d defined as:\ngm(t1, t2, . . . , tm) =  tk, m\u2211 j=1 exp{\u2212 \u2016tj\u2016 2 2\u2016tk\u20162 }, m\u2211 j=1 tj \u2016tk\u2016 exp{\u2212 \u2016tj\u2016 2 2\u2016tk\u20162 }, m\u2211 j=1 tjt T j \u2016tk\u20162 exp{\u2212 \u2016tj\u2016 2 2\u2016tk\u20162 }  , (47) such that S(n) = gm ( (cdnf(x)) 1/d (Z1,i, Z2,i, . . . , Zm,i ) ) , which follows from the definition of S(n) =\n((cdnf(x)) 1/dZk,i, S0,i, S1,i, S2,i) in (10). Similarly, S\u0303(m) = gm ( \u03be1E 1/d 1 , \u03be2(E1 + E2) 1/d, . . . \u03bem( \u2211m `=1E`) 1/d ) . Since gm is continuous, so for any set A \u2208 Rd \u00d7 R \u00d7 Rd \u00d7 Rd\u00d7d, there exists a set A\u0303 \u2208 Rd\u00d7m such that gm(A\u0303) = A. So for any x such that there exists \u03b5 > 0 such that f(a) > 0, \u2016\u2207f(a)\u2016 = O(1) and \u2016Hf (a)\u2016 = O(1) for any \u2016a\u2212 x\u2016 < \u03b5, we have:\ndTV(S (n), S\u0303(m))\n= sup A \u2223\u2223\u2223\u2223\u2223P{gm ( (cdnf(x)) 1dZ1,i, . . . , (cdnf(x)) 1dZm,i ) \u2208 A}\u2212 P{gm( \u03be1E 1d1 , . . . \u03bem( m\u2211 l=1 E`) 1 d ) \u2208 A} \u2223\u2223\u2223\u2223\u2223 \u2264 sup A\u0303\u2208Rd\u00d7m \u2223\u2223\u2223\u2223\u2223P{( (cdnf(x))1/dZ1,i, . . . , (cdnf(x))1/dZm,i ) \u2208 A\u0303}\u2212 P{( \u03be1E1/d1 , . . . \u03bem( m\u2211 `=1 E`) 1/d ) \u2208 A\u0303}\n\u2223\u2223\u2223\u2223\u2223 = dTV (( (cdnf(x)) 1/dZ1,i, . . . , (cdnf(x)) 1/dZm,i ) , ( \u03be1E 1/d 1 , . . . \u03bem(\nm\u2211 `=1 E`) 1/d )) n\u2192\u221e\u2212\u2192 0 , (48)\nwhere the last inequality follows from Lemma 3.1. By the assumption that f has open support and \u2016\u2207f\u2016 and \u2016Hf\u2016 is bounded almost everywhere, this convergence holds for almost every x.\nFor the second term in (46), let T\u0303 (m)\u03b1 = S\u0303 (\u221e) \u03b1 \u2212 S\u0303(m)\u03b1 and we claim that T\u0303 (m)\u03b1 converges to 0 in distribution\nby the following lemma.\nLemma 10.1. Assume m\u2192\u221e as n\u2192\u221e and k \u2265 3 , then\nlim n\u2192\u221e\nE\u2016 T\u0303 (m)\u03b1 \u2016 = 0 (49)\nfor any \u03b1 \u2208 {0, 1, 2}. Hence (T\u0303 (m)0 , T\u0303 (m) 1 , T\u0303 (m) 2 ) converges to (0, 0, 0) in distribution.\nThis implies that (S\u0303(m)0 , S\u0303 (m) 1 , S\u0303 (m) 2 ) converges to (S\u0303 (\u221e) 0 , S\u0303 (\u221e) 1 , S\u0303 (\u221e) 2 ) in distribution, i.e.,\ndTV(S\u0303 (m), S\u0303(\u221e)) n\u2192\u221e\u2212\u2192 0 , (50)\nCombine (48) and (50) in (46), this implies the desired claim. We next prove the upper bound on the variance, following the technique from [2, Section 7.3]. For the usage of Efron-Stein inequality, we need a second set of i.i.d. samples {X \u20321, X \u20322, . . . , X \u2032n}. For simplicity, denote H\u0302 = H\u0302(n)kLNN (X) be the kLNN estimate base on original sample {X1, . . . , Xn} and H\u0302(i) be the kLNN estimate based on {X1, . . . , Xi\u22121, X \u2032i, Xi+1, . . . Xn}. Then Efron-Stein theorem states that\nVar [ H\u0302 ] \u2264 2 n\u2211 j=1 E [( H\u0302 \u2212 H\u0302(j) )2 ] . (51)\nRecall that\nH\u0302 = 1\nn n\u2211 i=1 { h ( (cdnf(Xi)) 1/dZk,i, S0,i, S1,i, S2,i) ) \u2212 log f(Xi)\ufe38 \ufe37\ufe37 \ufe38\n\u2261Hi\n} ,\nwhere h : Rd \u00d7 R\u00d7 Rd \u00d7 Rd\u00d7d \u2192 R is defined as\nh(t1, t2, t3, t4) =\nd log \u2016t1\u2016+ d log(2\u03c0)\u2212 log cd \u2212 log t2 + 1\n2 log\n( det ( t4 t2 \u2212 t3t T 3 t22 )) + 1 2 tT3 (t4 \u2212 t3tT3 )\u22121t3 . (52)\nSimilarly, we can write H\u0302(j) = 1n \u2211n i=1H (j) i for any j \u2208 {1, . . . , n}. Therefore, the difference of H\u0302 and H\u0302(j) can be bounded by:\nH\u0302 \u2212 H\u0302(j) = 1 n n\u2211 i=1 ( Hi \u2212H(j)i ) . (53)\nNotice that Hi only depends on Xi and its m nearest neighbors, so Hi \u2212H(j)i = 0 if none of Xj and X \u2032j are in m nearest neighbor of Xi. If we denote Zi,j = I{Xj is in m nearest neighbor of Xi}, then Hi = H(j)i if Zi,j +Zi,j\u2032 = 0. According to [2, Lemma 20.6], since X has a density, with probability one, \u2211n i=1 Zi,j \u2264 m\u03b3d,\nwhere \u03b3d is the minimal number of cones of angle \u03c0/6 that can cover Rd, which only depends on d. Similarly,\u2211n i=1 Zi,j\u2032 \u2264 m\u03b3d. If we denote S = {i : Zi,j + Zi,j\u2032 > 0}, the cardinality of S satisfy |S| \u2264 2m\u03b3d. Therefore,\nwe have H\u0302 \u2212 H\u0302(j) = 1n \u2211 i\u2208S ( Hi \u2212H(j)i ) . By Cauchy-Schwarz inequality, we have\nE [( H\u0302 \u2212 H\u0302(j) )2 ] = E  1 n2 (\u2211 i\u2208S ( Hi \u2212H(j)i ))2  \u2264 E [ |S| n2 \u2211 i\u2208S ( Hi \u2212H(j)i )2 ]\n= |S| n2 \u2211 i\u2208S E [( Hi \u2212H(j)i )2 ] \u2264 2|S| n2 \u2211 i\u2208S ( E [ H2i ] + E [ (H (j) i ) 2 ] ) . (54)\nNotice that Hi\u2019s and H (j) i \u2019s are identically distributed, so we are left to compute E [ H21 ] . Conditioning\non X1 = x, similarly to (42), we have\nE [ (H1 + log f(x)) 2|X1 = x ] = E [ h2((cdnf(x)) 1/dZk,i, S0,1, S1,i, S2,i) ]\n\u2212\u2192 B(2)k,d , (55)\nas n\u2192\u221e, where B(2)k,d \u2261 E [ h2 ( \u03bek (\u2211k `=1E` )1/d , S\u0303 (\u221e) 0 , S\u0303 (\u221e) 1 , S\u0303 (\u221e) 2 )] . Therefore,\nE [ H21 |X1 = x ] = B (2) k,d \u2212 2 log f(x)E [H1|X1 = x ]\u2212 (log f(x)) 2\n= B (2) k,d \u2212 2 log f(x)Bk,d + (log f(x)) 2 . (56)\nTake expectation over X1, we obtain: E[H21 ] = EX1 [\nlim n\u2192\u221e\nE [ H21 |X1 ] ] = EX1 [ B (2) k,d \u2212 2 log f(X1)Bk,d + (log f(X1)) 2 ]\n= B (2) k,d + 2H(X)Bk,d +\n\u222b f(x)(log f(x))2dx < +\u221e , (57)\nwhere the last inequality comes from the assumption that \u222b f(x)(log f(x))2dx < +\u221e. Combining with (51) and (54), we have\nVar [ H\u0302 ] \u2264 2 n\u2211 j=1 E [( H\u0302 \u2212 H\u0302(j) )2 ] \u2264 4|S| n \u2211 i\u2208S ( E [ H2i ] + E [ (H (j) i ) 2 ] ) \u2264 8|S| 2C2 n \u2264 32m 2\u03b32dC2 n , (58)\nwhere C2 is the upper bound for E[H21 ]. Take m = O(log n) then the proof is complete."}, {"heading": "10.1 Proof of Lemma 10.1", "text": "Firstly, since |\u03bei| = 1, we can upper bound the expectation of E\u2016 T\u0303 (m)\u03b1,i \u2016 by:\nE\u2016 T\u0303 (m)\u03b1,i \u2016 = E \u2225\u2225\u2225 \u221e\u2211 j=m+1 \u03be (\u03b1) j ( \u2211j `=1E`) \u03b1/d ( \u2211k `=1E`) \u03b1/d exp{\u2212 ( \u2211j `=1E` ) 2/d 2( \u2211k `=1E` ) 2/d } \u2225\u2225\u2225\n\u2264 \u221e\u2211\nj=m+1\nE \u2225\u2225\u2225 \u03be(\u03b1)j (\u2211j`=1E`)\u03b1/d\n( \u2211k `=1E`) \u03b1/d exp{\u2212\n( \u2211j `=1E` ) 2/d\n2( \u2211k `=1E` )\n2/d } \u2225\u2225\u2225\n= \u221e\u2211 j=m+1 E \u2223\u2223\u2223 (\u2211j`=1E`)\u03b1/d ( \u2211k `=1E` ) \u03b1/d exp{\u2212 ( \u2211j `=1E` ) 2/d 2( \u2211k `=1E` ) 2/d } \u2223\u2223\u2223 . (59)\nNotice that the expression is a function of ( \u2211j `=1E`/ \u2211k `=1E`)\n1/d \u2261 Rj for j > m, we will identify the distribution of Rj first. For any fixed j \u2265 k, let Tk = \u2211k `=1E` and Tj\u2212k = \u2211j `=k+1E`, such that Rj = ((Tk +Tj\u2212k)/Tk) 1/d. Notice that Tk is the summation of k i.i.d. standard exponential random variables, so Tk \u223c Erlang (k, 1). Similarly, Tj\u2212k \u223c Erlang (j \u2212 k, 1). Also Tk and Tj\u2212k are independent. Recall that the pdf of Erlang (k, \u03bb) is given by fk,\u03bb(x) = \u03bbkxk\u22121e\u2212\u03bbx/(k \u2212 1)! for x \u2265 0. Therefore, the CDF of Rj is\ngiven by:\nFRj (t) = P{Rj \u2264 t} = P{( Tk + Tj\u2212k Tk )1/d \u2264 t} = P{Tj\u2212k Tk \u2264 td \u2212 1}\n= \u222b x\u22650 xk\u22121e\u2212x (k \u2212 1)! (\u222b (td\u22121)x y=0 yj\u2212k\u22121e\u2212y (j \u2212 k \u2212 1)! dy ) dx\n= \u222b x\u22650 xk\u22121e\u2212x (k \u2212 1)! ( 1\u2212 j\u2212k\u22121\u2211 `=0 1 `! x`(td \u2212 1)`e\u2212x(t d\u22121) ) dx\n= 1\u2212 j\u2212k\u22121\u2211 `=0 (\u222b x\u22650 xk\u22121e\u2212x (k \u2212 1)! 1 `! x`(td \u2212 1)`e\u2212x(t d\u22121)dx )\n= 1\u2212 j\u2212k\u22121\u2211 `=0 ( (td \u2212 1)` (k \u2212 1)!`! \u222b x\u22650 xk\u22121+`e\u2212xt d dx )\n= 1\u2212 j\u2212k\u22121\u2211 `=0 (td \u2212 1)` (k \u2212 1)!`! (k \u2212 1 + `)! t\u2212d(k\u22121+`)\n= 1\u2212 j\u2212k\u22121\u2211 `=0 ( k \u2212 1 + ` ` ) t\u2212d(k\u22121)(1\u2212 t\u2212d)` , (60)\nfor t \u2208 [1,+\u221e). Given the CDF of Rj , each term in (66) is upper bounded by:\nE \u2223\u2223\u2223 (\u2211j`=1E`)\u03b1/d\n( \u2211k `=1E` ) \u03b1/d exp{\u2212\n( \u2211j `=1E` ) 2/d\n2( \u2211k `=1E` )\n2/d } \u2223\u2223\u2223 = ERj \u2223\u2223\u2223 t\u03b1e\u2212t2 \u2223\u2223\u2223 \u2264 ERj [ t2e\u2212t2 ]\n= \u222b \u221e t=1 t2e\u2212t 2 dFRj (t) = t 2e\u2212t 2 FRj (t) \u2223\u2223\u2223\u221e 1 \u2212 \u222b \u221e t=1 FRj (t)d(t 2e\u2212t 2 )\n= \u2212 \u222b \u221e t=1 (2te\u2212t 2 \u2212 2t3e\u2212t 2 )FRj (t)dt = \u222b \u221e t=1 2t(t2 \u2212 1)e\u2212t 2 FRj (t)dt . (61)\nTherefore, in order to establish an upper bound for (66), we need an upper bound for FRj (t). Here we will consider two cases depending on t. If t > (j/2k)1/d, we just use the trivial upper bound FRj (t) < 1. If 1 \u2264 t \u2264 (j/2k)1/d, since td \u2265 1, we have:\nFRj (t) = 1\u2212 j\u2212k\u22121\u2211 `=0 ( k \u2212 1 + ` ` ) t\u2212d(k\u22121)(1\u2212 t\u2212d)` \u2264 1\u2212 j\u2212k\u22121\u2211 `=0 ( k \u2212 1 + ` ` ) t\u2212dk(1\u2212 t\u2212d)` . (62)\nNotice that ( k\u22121+` ` ) t\u2212dk(1\u2212 t\u2212d)` is the pmf of negative binomial distribution NB(k, 1\u2212 t\u2212d). Therefore, FRj (t) \u2264 P{X \u2265 j \u2212 k}, where X \u223c NB(k, 1 \u2212 t\u2212d). The mean and variance of X are given by E[X] = (1\u2212 t\u2212d)k/(1\u2212 (1\u2212 t\u2212d)) = (td \u2212 1)k and Var(X) = (1\u2212 t\u2212d)k/(1\u2212 (1\u2212 t\u2212d))2 = (t2d \u2212 td)k. Therefore, by Chebyshev inequality, the tail probability is upper bounded by:\nP{X \u2265 j \u2212 k} \u2264 Var(X) (j \u2212 k \u2212 E[X])2 = (t2d \u2212 td)k (j \u2212 k \u2212 (td \u2212 1)k)2 = (t2d \u2212 td)k (j \u2212 tdk)2 \u2264 4t2dk/j2 , (63)\nhere we use the fact that t \u2264 (j/2k)1/d so j \u2212 tdk > j/2. Therefore, FRj (t) \u2264 4t2dk/j2 for t > (j/2k)1/d. Combine the two cases and plug into (61), we obtain:\nE \u2223\u2223\u2223 (\u2211j`=1E`)\u03b1/d\n( \u2211k `=1E` ) \u03b1/d exp{\u2212\n( \u2211j `=1E` ) 2/d\n2( \u2211k `=1E` )\n2/d } \u2223\u2223\u2223 = \u222b \u221e\nt=1\n2t(t2 \u2212 1)e\u2212t 2\nFRj (t)dt\n\u2264 \u222b (j/2k)1/d t=1 2t(t2 \u2212 1)e\u2212t 2 4t2dk j2 dt+ \u222b \u221e (j/2k)1/d 2t(t2 \u2212 1)e\u2212t 2 dt\n\u2264 8k j2 \u222b \u221e t=1 t2d+3e\u2212t 2 dt+ 2 \u222b \u221e (j/2k)1/d t3e\u2212t 2 dt\n\u2264 8kCd j2 + 2\n( \u22121\n2 e\u2212t\n2 (t2 + 1) \u2223\u2223\u2223\u221e (j/2k)1/d ) =\n8kCd j2 + e\u2212(j/2k) 2/d (( j 2k )2/d + 1) , (64)\nwhere Cd = \u222b\u221e t=1 t2d+3e\u2212t 2 dt is a constant only depend on d. Therefore, we can see that\nE \u2223\u2223\u2223 (\u2211j`=1E`)\u03b1/d\n( \u2211k `=1E` ) \u03b1/d exp{\u2212\n( \u2211j `=1E` ) 2/d\n2( \u2211k `=1E` )\n2/d } \u2223\u2223\u2223 = O(1/j2). (65)\nSo\nE\u2016 T\u0303 (m)\u03b1,i \u2016 \u2264 \u221e\u2211\nj=m+1\nE \u2223\u2223\u2223 (\u2211j`=1E`)\u03b1/d\n( \u2211k `=1E` ) \u03b1/d exp{\u2212\n( \u2211j `=1E` ) 2/d\n2( \u2211k `=1E` )\n2/d } \u2223\u2223\u2223\u2192 0 . (66)\ngiven m\u2192\u221e as n\u2192\u221e."}, {"heading": "11 Proof of Theorem 2", "text": "The proposed estimator is a solution to a maximization problem a\u0302 = arg maxa LXi(fa,Xi). From [21] we know that the maximizer is a fixed point of a series of non-linear equations of the form\u2211\nj 6=i\n(Xj \u2212Xi)\u2297\u03b1 \u03c1\u03b1k,i K (Xj \u2212Xi \u03c1k,i ) = n\u03c1dk,i e a0 \u222b (u\u2212Xi)\u2297\u03b1 \u03c1\u03b1k,i K (u\u2212Xi \u03c1k,i ) e\u3008u\u2212x,a1\u3009+\u00b7\u00b7\u00b7+ap[(u\u2212x),\u00b7\u00b7\u00b7 ,(u\u2212x)] 1 \u03c1dk,i du\nfor all \u03b1 \u2208 [p] where the superscript \u2297\u03b1 indicates the \u03b1-th order tensor product. From the proof of Theorem 1, specifically (48) and (50), we know that the left-hand side converges to a value that only depends on k, d and K. Let\u2019s denote it by S\u03b1(k) \u2208 Rd \u03b1\n. We make a change of variables a\u03030 = a0 + d log \u03c1k,i + log n and a\u0303\u03b1 = a\u03b1/\u03c1 \u03b1 k,i for \u03b1 6= 0. Then, in the limit of growing n, the above equations can be rewritten as\nS\u03b1(k, d,K) = e a\u03030F\u03b1(d,K, a\u03031, . . . , a\u0303p) , (67)\nfor some function F\u03b1. Notice that the dependence on the underlying distribution vanishes in the limit, and the fixed point a\u0303 only depends on k, p, d, and K. The desired claim follows from the fact that the estimate is limn\u2192\u221e f\u0302n(Xi) = limn\u2192\u221e ea\u03020 = limn\u2192\u221eAk,d,p,K/(n\u03c1dk,i) = f(Xi)Ak,d,p,KCd limn 1/(Cdn\u03c1 d k,if(Xi)) =\nf(Xi)Ak,d,p,KCd/ \u2211k `=1E`, and plugging in the entropy estimator H\u0302(X)\u2192 EXi [\u2212 log f(Xi)] +Bk,d,p,K .\nIn the case of the KL estimator, it happens that S0 = k and F0(d) = Cd such that ea\u03030 = k/Cd, ea\u03020 = f(Xi)k/(Cd\u03c1 d k,if(Xi)n) and Bk,d,p,K = \u2212 log k + E[log( \u2211k `=1E`)] = \u2212 log k + \u03c6(k)."}, {"heading": "Acknowledgement", "text": "This work is supported by NSF SaTC award CNS-1527754, NSF CISE award CCF-1553452, NSF CISE award CCF-1617745. We thank the anonymous reviewers for their constructive feedback."}], "references": [{"title": "Efficient multivariate entropy estimation via k-nearest neighbour distances", "author": ["T.B. Berrett", "R.J. Samworth", "M. Yuan"], "venue": "arXiv preprint arXiv:1606.00304", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Lectures on the Nearest Neighbor Method", "author": ["G. Biau", "L. Devroye"], "venue": "Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Sums of functions of nearest neighbor distances", "author": ["P.J. Bickel", "L. Breiman"], "venue": "moment bounds, limit theorems and a goodness of fit test. The Annals of Probability, pages 185\u2013214", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1983}, {"title": "On the kozachenko\u2013leonenko entropy estimator, 2016", "author": ["S. Delattre", "N. Fournier"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "On entropy estimation by m-spacing method", "author": ["F. El Haje Hussein", "Y. Golubev"], "venue": "Journal of Mathematical Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Efficient estimation of mutual information for strongly dependent variables", "author": ["S. Gao", "G. Ver Steeg", "A. Galstyan"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimating mutual information by local gaussian approximation", "author": ["S. Gao", "G. Ver Steeg", "A. Galstyan"], "venue": "31st Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Demystifying fixed k-nearest neighbor information estimators", "author": ["W. Gao", "S. Oh", "P. Viswanath"], "venue": "arXiv preprint arXiv:1604.03006", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A new class of random vector entropy estimators and its applications in testing statistical hypotheses", "author": ["M.N. Goria", "N.N. Leonenko", "V.V. Mergel", "P.L. Novi Inverardi"], "venue": "Nonparametric Statistics, 17(3):277\u2013297", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Limit theorems for sums of general functions of m-spacings", "author": ["P. Hall"], "venue": "Mathematical Proceedings of the Cambridge Philosophical Society, volume 96, pages 517\u2013532. Cambridge Univ Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1984}, {"title": "On powerful distributional tests based on sample spacings", "author": ["P. Hall"], "venue": "Journal of Multivariate Analysis, 19(2):201\u2013224", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1986}, {"title": "Locally parametric nonparametric density estimation", "author": ["N. Hjort", "M. Jones"], "venue": "The Annals of Statistics, pages 1619\u20131647", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "Estimation of entropy and other functionals of a multivariate density", "author": ["H. Joe"], "venue": "Annals of the Institute of Statistical Mathematics, 41(4):683\u2013697", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "Nonparametric von mises estimators for entropies", "author": ["K. Kandasamy", "A. Krishnamurthy", "B. Poczos", "L. Wasserman"], "venue": "divergences and mutual informations. In NIPS, pages 397\u2013405", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Sample estimate of the entropy of a random vector", "author": ["L.F. Kozachenko", "N.N. Leonenko"], "venue": "Problemy Peredachi Informatsii, 23(2):9\u201316", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Estimating mutual information", "author": ["A. Kraskov", "H. St\u00f6gbauer", "P. Grassberger"], "venue": "Physical review E, 69(6):066138", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Conditional density-based analysis of t cell signaling in single-cell data", "author": ["S. Krishnaswamy", "M. Spitzer", "M. Mingueneau", "S. Bendall", "O. Litvin", "E. Stone", "D. Peer", "G. Nolan"], "venue": "Science, 346:1250689", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A class of r\u00e9nyi information estimators for multidimensional densities", "author": ["N. Leonenko", "L. Pronzato", "V. Savani"], "venue": "The Annals of Statistics, 36(5):2153\u20132182", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Exponential concentration for mutual information estimation with application to forests", "author": ["H. Liu", "L. Wasserman", "J.D. Lafferty"], "venue": "NIPS, pages 2537\u20132545", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Local regression and likelihood", "author": ["C. Loader"], "venue": "Springer Science & Business Media", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Local likelihood density estimation", "author": ["C.R. Loader"], "venue": "The Annals of Statistics, 24(4):1602\u20131618", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Nonparametric k-nearest-neighbor entropy estimator", "author": ["D. Lombardi", "S. Pant"], "venue": "Physical Review E, 93(1):013310", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "volume 1. Cambridge university press Cambridge", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "kn-nearest neighbor estimators of entropy", "author": ["R.M. Mnatsakanov", "N. Misra", "S. Li", "E.J. Harner"], "venue": "Mathematical Methods of Statistics, 17(3):261\u2013277", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Ensemble estimation of multivariate f-divergence", "author": ["K.R. Moon", "A.O. Hero"], "venue": "2014 IEEE International Symposium on Information Theory, pages 356\u2013360. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonparametric ensemble estimation of distributional functionals", "author": ["K.R. Moon", "K. Sricharan", "K. Greenewald", "A.O. Hero III"], "venue": "arXiv preprint arXiv:1601.06884", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Submanifold density estimation", "author": ["A. Ozakin", "A.G. Gray"], "venue": "Advances in Neural Information Processing Systems, pages 1375\u20131382", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimation of r\u00e9nyi entropy and mutual information based on generalized nearest-neighbor graphs", "author": ["D. P\u00e1l", "B. P\u00f3czos", "C. Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems, pages 1849\u20131857", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximate distributions of order statistics: with applications to nonparametric statistics", "author": ["R-D Reiss"], "venue": "Springer Science & Business Media", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting novel associations in large data sets", "author": ["D. Reshef", "Y. Reshef", "H. Finucane", "S. Grossman", "G. McVean", "P. Turnbaugh", "E. Lander", "M. Mitzenmacher", "P. Sabeti"], "venue": "science, 334(6062):1518\u20131524", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Density estimation", "author": ["S.J. Sheather"], "venue": "Statistical Science, 19(4):588\u2013597", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Nearest neighbor estimates of entropy", "author": ["H. Singh", "N. Misra", "V. Hnizdo", "A. Fedorowicz", "E. Demchuk"], "venue": "American journal of mathematical and management sciences, 23(3-4):301\u2013321", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Analysis of k-nearest neighbor distances with application to entropy estimation", "author": ["S. Singh", "B. P\u00f3czos"], "venue": "arXiv preprint arXiv:1603.08578", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Ensemble estimators for multivariate entropy estimation", "author": ["K. Sricharan", "D. Wei", "A.O. Hero"], "venue": "IEEE Transactions on Information Theory, 59(7):4374\u20134388", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "The information sieve", "author": ["G. Ver Steeg", "A. Galstyan"], "venue": "to appear in ICML, arXiv:1507.02284", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Root-n consistent estimators of entropy for densities with unbounded support", "author": ["A.B. Tsybakov", "E.C. Van der Meulen"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1996}, {"title": "Discovering structure in high-dimensional data through correlation explanation", "author": ["G. Ver Steeg", "A. Galstyan"], "venue": "Advances in Neural Information Processing Systems, pages 577\u2013585", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Locally weighted full covariance gaussian density estimation", "author": ["P. Vincent", "Y. Bengio"], "venue": "Technical report, Technical report 1240", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Divergence estimation for multidimensional densities via-nearest-neighbor distances", "author": ["Q. Wang", "S.R. Kulkarni", "S. Verd\u00fa"], "venue": "Information Theory, IEEE Transactions on, 55(5):2392\u20132405", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "Universal estimation of information measures for analog sources", "author": ["Q. Wang", "S.R. Kulkarni", "S. Verd\u00fa"], "venue": "Foundations and Trends in Communications and Information Theory, 5(3):265\u2013353", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "All of nonparametric statistics", "author": ["L. Wasserman"], "venue": "Springer Science & Business Media", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": "Such an approach is evident in many applications, as varied as computational biology [17], sociology [30] and information retrieval [23], with the citations representing a mere smattering of recent works.", "startOffset": 85, "endOffset": 89}, {"referenceID": 29, "context": "Such an approach is evident in many applications, as varied as computational biology [17], sociology [30] and information retrieval [23], with the citations representing a mere smattering of recent works.", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "Such an approach is evident in many applications, as varied as computational biology [17], sociology [30] and information retrieval [23], with the citations representing a mere smattering of recent works.", "startOffset": 132, "endOffset": 136}, {"referenceID": 36, "context": "Within mainstream machine learning, a systematic effort at unsupervised clustering and hierarchical information extraction is conducted in recent works of [37, 35].", "startOffset": 155, "endOffset": 163}, {"referenceID": 34, "context": "Within mainstream machine learning, a systematic effort at unsupervised clustering and hierarchical information extraction is conducted in recent works of [37, 35].", "startOffset": 155, "endOffset": 163}, {"referenceID": 39, "context": "While these estimation questions have been studied in the past three decades (and summarized in [40]), the renewed importance of estimating information theoretic measures in a sample-efficient manner is persuasively argued in a recent work [6], where the authors note that existing estimators perform poorly in several key scenarios of central interest (especially when the high dimensional random variables are strongly related to each other).", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "While these estimation questions have been studied in the past three decades (and summarized in [40]), the renewed importance of estimating information theoretic measures in a sample-efficient manner is persuasively argued in a recent work [6], where the authors note that existing estimators perform poorly in several key scenarios of central interest (especially when the high dimensional random variables are strongly related to each other).", "startOffset": 240, "endOffset": 243}, {"referenceID": 15, "context": "The widely used estimator of mutual information is the one by Kraskov and St\u00f6gbauer and Grassberger [16] and christened the KSG estimator (nomenclature based on the authors, cf.", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "[6]) \u2013 while this estimator works well in practice (and performs much better than other approaches such as those based on kernel density estimation procedures), it \u2217Coordinated Science Lab and Department of Electrical and Computer Engineering \u2020Coordinated Science Lab and Department of Industrial and Enterprise Systems Engineering", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "The basic issue is that the KSG estimator (and the underlying differential entropy estimator based on nearest neighbor distances by Kozachenko and Leonenko (KL) [15]) does not take advantage of the fact that the samples could lie in a smaller dimensional subspace (more generally, manifold) despite the high dimensionality of the data itself.", "startOffset": 161, "endOffset": 165}, {"referenceID": 6, "context": "Ameliorating this deficiency is the central theme of recent works [7, 6, 22], each of which aims to improve upon the classical KL (differential) entropy estimator of [15].", "startOffset": 66, "endOffset": 76}, {"referenceID": 5, "context": "Ameliorating this deficiency is the central theme of recent works [7, 6, 22], each of which aims to improve upon the classical KL (differential) entropy estimator of [15].", "startOffset": 66, "endOffset": 76}, {"referenceID": 21, "context": "Ameliorating this deficiency is the central theme of recent works [7, 6, 22], each of which aims to improve upon the classical KL (differential) entropy estimator of [15].", "startOffset": 66, "endOffset": 76}, {"referenceID": 14, "context": "Ameliorating this deficiency is the central theme of recent works [7, 6, 22], each of which aims to improve upon the classical KL (differential) entropy estimator of [15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 5, "context": "A local SVD is used to heuristically improve the density estimate at each sample point in [6], while a local Gaussian density (with empirical mean and covariance weighted by NN distances) is heuristically used for the same purpose in [22].", "startOffset": 90, "endOffset": 93}, {"referenceID": 21, "context": "A local SVD is used to heuristically improve the density estimate at each sample point in [6], while a local Gaussian density (with empirical mean and covariance weighted by NN distances) is heuristically used for the same purpose in [22].", "startOffset": 234, "endOffset": 238}, {"referenceID": 6, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 112, "endOffset": 119}, {"referenceID": 21, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 112, "endOffset": 119}, {"referenceID": 11, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 175, "endOffset": 183}, {"referenceID": 20, "context": "An effort towards a systematic study is initiated in [7] which connects the aforementioned heuristic efforts of [6, 22] to the local log-likelihood density estimation methods [12, 21] from theoretical statistics.", "startOffset": 175, "endOffset": 183}, {"referenceID": 20, "context": "Equation (9) of [21]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "Indeed, such an elaborate numerical effort is one of the key impediments for the entropy estimator of [7] to be practically valuable.", "startOffset": 102, "endOffset": 105}, {"referenceID": 40, "context": "samples is missing in the theoretical statistics literature (despite local log-likelihood methods for regression and density estimation being standard textbook fare [41, 20]).", "startOffset": 165, "endOffset": 173}, {"referenceID": 19, "context": "samples is missing in the theoretical statistics literature (despite local log-likelihood methods for regression and density estimation being standard textbook fare [41, 20]).", "startOffset": 165, "endOffset": 173}, {"referenceID": 15, "context": "This effort allows us to connect disparate threads of ideas from seemingly different arenas: NN methods, local log-likelihood methods, asymptotic order statistics and sample-dependent heuristic, but inspired, methods for mutual information estimation suggested in the work of [16].", "startOffset": 276, "endOffset": 280}, {"referenceID": 6, "context": "2], significantly improves the computational efficiency upon similar endeavors in the recent efforts of [7, 22, 38].", "startOffset": 104, "endOffset": 115}, {"referenceID": 21, "context": "2], significantly improves the computational efficiency upon similar endeavors in the recent efforts of [7, 22, 38].", "startOffset": 104, "endOffset": 115}, {"referenceID": 37, "context": "2], significantly improves the computational efficiency upon similar endeavors in the recent efforts of [7, 22, 38].", "startOffset": 104, "endOffset": 115}, {"referenceID": 28, "context": "Since the bandwidth is data dependent and vanishes too fast (because k is fixed), the estimator has a bias, which we derive a closed form expression for and show that it is independent of the underlying distribution and hence can be easily corrected: this is our main theoretical contribution, and involves new theorems on asymptotic statistics of nearest neighbors generalizing classical work in probability theory [29], which might be of independent mathematical interest.", "startOffset": 416, "endOffset": 420}, {"referenceID": 5, "context": "Generalized view: We show that seemingly very different approaches to entropy estimation \u2013 recent works of [6, 7, 22] and the classical work of fixed k-NN estimator of Kozachenko and Leonenko [15] \u2013 can all be cast in the local log-likelihood framework as specific kernel and sample dependent bandwidth choices.", "startOffset": 107, "endOffset": 117}, {"referenceID": 6, "context": "Generalized view: We show that seemingly very different approaches to entropy estimation \u2013 recent works of [6, 7, 22] and the classical work of fixed k-NN estimator of Kozachenko and Leonenko [15] \u2013 can all be cast in the local log-likelihood framework as specific kernel and sample dependent bandwidth choices.", "startOffset": 107, "endOffset": 117}, {"referenceID": 21, "context": "Generalized view: We show that seemingly very different approaches to entropy estimation \u2013 recent works of [6, 7, 22] and the classical work of fixed k-NN estimator of Kozachenko and Leonenko [15] \u2013 can all be cast in the local log-likelihood framework as specific kernel and sample dependent bandwidth choices.", "startOffset": 107, "endOffset": 117}, {"referenceID": 14, "context": "Generalized view: We show that seemingly very different approaches to entropy estimation \u2013 recent works of [6, 7, 22] and the classical work of fixed k-NN estimator of Kozachenko and Leonenko [15] \u2013 can all be cast in the local log-likelihood framework as specific kernel and sample dependent bandwidth choices.", "startOffset": 192, "endOffset": 196}, {"referenceID": 14, "context": "Thus our work is a strict mathematical generalization of the classical work of [15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "Mutual Information estimation: The inspired work of [16] constructs a mutual information estimator that subtly altered (in a sample dependent way) the three KL entropy estimation terms, leading to superior empirical performance.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "Local likelihood density estimators [21, 12] constitute state of the art and are specified by a weight function K : R \u2192 R (also called a kernel), a degree p \u2208 Z of the polynomial approximation, and the bandwidth h \u2208 R, and maximizes the local log-likelihood: Lx(f) = n \u2211", "startOffset": 36, "endOffset": 44}, {"referenceID": 11, "context": "Local likelihood density estimators [21, 12] constitute state of the art and are specified by a weight function K : R \u2192 R (also called a kernel), a degree p \u2208 Z of the polynomial approximation, and the bandwidth h \u2208 R, and maximizes the local log-likelihood: Lx(f) = n \u2211", "startOffset": 36, "endOffset": 44}, {"referenceID": 20, "context": "Concretely, for p = 0, it is known that LDDE reduces to the standard Kernel Density Estimator (KDE) [21]:", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Somewhat surprisingly, this result has eluded prior works [22, 38] and [7] which specifically attempted the evaluation for p = 2.", "startOffset": 58, "endOffset": 66}, {"referenceID": 37, "context": "Somewhat surprisingly, this result has eluded prior works [22, 38] and [7] which specifically attempted the evaluation for p = 2.", "startOffset": 58, "endOffset": 66}, {"referenceID": 6, "context": "Somewhat surprisingly, this result has eluded prior works [22, 38] and [7] which specifically attempted the evaluation for p = 2.", "startOffset": 71, "endOffset": 74}, {"referenceID": 11, "context": "LLDE provides a principled approach to automatically correct for the boundary bias, which takes effect only for p \u2265 2 [12, 31].", "startOffset": 118, "endOffset": 126}, {"referenceID": 30, "context": "LLDE provides a principled approach to automatically correct for the boundary bias, which takes effect only for p \u2265 2 [12, 31].", "startOffset": 118, "endOffset": 126}, {"referenceID": 27, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 190, "endOffset": 205}, {"referenceID": 8, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 190, "endOffset": 205}, {"referenceID": 17, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 190, "endOffset": 205}, {"referenceID": 38, "context": "As noted in [28], such an assumption is common in the literature on consistency of k-NN estimators, where it has been implicitly assumed in existing analyses of entropy estimators including [15, 9, 18, 39], without explicitly stating that such assumptions are being made.", "startOffset": 190, "endOffset": 205}, {"referenceID": 5, "context": "Naturally, all the techniques we develop in this paper generalize to any estimators that depend on the nearest neighbor statistics {Z`,i}i,`\u2208[n] \u2013 and the value of such a general result is demonstrated later (in Section 4) when we evaluate the bias in similarly inspired entropy estimators [6, 7, 22, 15].", "startOffset": 290, "endOffset": 304}, {"referenceID": 6, "context": "Naturally, all the techniques we develop in this paper generalize to any estimators that depend on the nearest neighbor statistics {Z`,i}i,`\u2208[n] \u2013 and the value of such a general result is demonstrated later (in Section 4) when we evaluate the bias in similarly inspired entropy estimators [6, 7, 22, 15].", "startOffset": 290, "endOffset": 304}, {"referenceID": 21, "context": "Naturally, all the techniques we develop in this paper generalize to any estimators that depend on the nearest neighbor statistics {Z`,i}i,`\u2208[n] \u2013 and the value of such a general result is demonstrated later (in Section 4) when we evaluate the bias in similarly inspired entropy estimators [6, 7, 22, 15].", "startOffset": 290, "endOffset": 304}, {"referenceID": 14, "context": "Naturally, all the techniques we develop in this paper generalize to any estimators that depend on the nearest neighbor statistics {Z`,i}i,`\u2208[n] \u2013 and the value of such a general result is demonstrated later (in Section 4) when we evaluate the bias in similarly inspired entropy estimators [6, 7, 22, 15].", "startOffset": 290, "endOffset": 304}, {"referenceID": 14, "context": "The idea of using k-NN distance as bandwidth for entropy estimation was originally proposed by Kozachenko and Leonenko in [15], and is a special case of the k-LNN method we propose with degree 0 and a step kernel.", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "Another popular resubstitution entropy estimator is to use KDE in (3) [13], which is a special case of the k-LNN method with degree 0, and the Gaussian kernel is used in simulations.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "As comparison, we also study a new estimator [14] based on von Mises expansion (as opposed to simple re-substitution) which has an improved convergence rate in the large sample regime.", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "4 Universality of the k-LNN approach In this section, we show that Theorem 1 holds universally for a general family of entropy estimators, specified by the choice of k \u2208 Z, degree p \u2208 Z, and a kernel K : R \u2192 R, thus allowing a unified view of several seemingly disparate entropy estimators [15, 6, 7, 22].", "startOffset": 290, "endOffset": 304}, {"referenceID": 5, "context": "4 Universality of the k-LNN approach In this section, we show that Theorem 1 holds universally for a general family of entropy estimators, specified by the choice of k \u2208 Z, degree p \u2208 Z, and a kernel K : R \u2192 R, thus allowing a unified view of several seemingly disparate entropy estimators [15, 6, 7, 22].", "startOffset": 290, "endOffset": 304}, {"referenceID": 6, "context": "4 Universality of the k-LNN approach In this section, we show that Theorem 1 holds universally for a general family of entropy estimators, specified by the choice of k \u2208 Z, degree p \u2208 Z, and a kernel K : R \u2192 R, thus allowing a unified view of several seemingly disparate entropy estimators [15, 6, 7, 22].", "startOffset": 290, "endOffset": 304}, {"referenceID": 21, "context": "4 Universality of the k-LNN approach In this section, we show that Theorem 1 holds universally for a general family of entropy estimators, specified by the choice of k \u2208 Z, degree p \u2208 Z, and a kernel K : R \u2192 R, thus allowing a unified view of several seemingly disparate entropy estimators [15, 6, 7, 22].", "startOffset": 290, "endOffset": 304}, {"referenceID": 14, "context": "[15] showed (in a remarkable result at the time) that the asymptotic bias is independent of the dimension d and can be computed exactly to be log n\u2212 \u03c8(n) + \u03c8(k)\u2212 log k and \u03c8(k) is the digamma function defined as \u03c8(x) = \u0393\u22121(x)d\u0393(x)/dx.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The dimension independent nature of this asymptotic bias term (of O(n\u22121/2) for d = 1 in [36, Theorem 1] and O(n\u22121/d) for general d in [8]) is special to the choice of p = 0 and the step kernel; we explain this in detail in Section 11, later in the paper.", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "Analogously, the estimator in [6] can be viewed as a special case with p = 0 and an ellipsoidal step kernel.", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "In [16], Kraskov and St\u00f6gbauer and Grassberger introduced \u00ceKSG(X;Y ) by coupling the choices of the bandwidths.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Inspired by [16], we introduce the following novel mutual information estimator we denote by \u00ceLNN\u2212KSG(X;Y ).", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "samples from two random variables X and Y , where X is uniform over [0, 1] and Y = X + U , where U is uniform over [0, 0.", "startOffset": 68, "endOffset": 74}, {"referenceID": 5, "context": "Mutual information estimators have been recently proposed in [6, 7, 22] based on local likelihood maximization.", "startOffset": 61, "endOffset": 71}, {"referenceID": 6, "context": "Mutual information estimators have been recently proposed in [6, 7, 22] based on local likelihood maximization.", "startOffset": 61, "endOffset": 71}, {"referenceID": 21, "context": "Mutual information estimators have been recently proposed in [6, 7, 22] based on local likelihood maximization.", "startOffset": 61, "endOffset": 71}, {"referenceID": 0, "context": "In Figure 6, we test the mutual information estimators for Y = f(X) + U , where X is uniformly distributed over [0, 1] and U is uniformly distributed over [0, \u03b8], independent of X, for some noise level \u03b8.", "startOffset": 112, "endOffset": 118}, {"referenceID": 6, "context": "Similar simulation were studied in [7].", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "Here Xi\u2019s are uniformly distributed over [0, 1] and U is uniformly distributed over [\u22123/2, 3/2], independently of Xi\u2019s.", "startOffset": 41, "endOffset": 47}, {"referenceID": 5, "context": "Similar simulation were studied in [6].", "startOffset": 35, "endOffset": 38}, {"referenceID": 30, "context": "6 Breaking the bandwidth barrier While k-NN distance based bandwidth are routine in practical usage [31], the main finding of this work is that they also turn out to be the \u201ccorrect\" mathematical choice for the purpose of asymptotically unbiased estimation of an integral functional such as the entropy: \u2212 \u222b f(x) log f(x); we briefly discuss the ramifications below.", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "On the other hand, when estimating entropy, as well as other integral functionals, it is known that resubstitution estimators of the form \u2212(1/n) \u2211n i=1 log f\u0302(Xi) achieve variances scaling as O(1/n) independent of the bandwidth [19].", "startOffset": 228, "endOffset": 232}, {"referenceID": 18, "context": "The bottleneck in choosing such a small bandwidth is the bias, scaling as O(h + (nhd)\u22121 + En) [19], where the lower order dependence on n, dubbed En, is generally not known.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "Following recent advances in [18, 33], the proposed local estimator has a potential to be extended to, for example, Renyi entropy, but with a multiplicative bias as opposed to additive.", "startOffset": 29, "endOffset": 37}, {"referenceID": 32, "context": "Following recent advances in [18, 33], the proposed local estimator has a potential to be extended to, for example, Renyi entropy, but with a multiplicative bias as opposed to additive.", "startOffset": 29, "endOffset": 37}, {"referenceID": 28, "context": "The former has been extensively studied, for example see [29] for a survey of results.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "based estimators, such as the entropy estimator of [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "In the seminal paper, [15] introduced resubstitution entropy estimators of the form \u0124(X) = \u2212(1/n) \u2211n i=1 log f\u0302n(Xi) with f\u0302n(x) = k/(nCd \u03c1 d k,x) (as defined in (4)).", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "For k = 1 this was proved in the original paper of [15], which later was extended in [32, 9] to general k.", "startOffset": 51, "endOffset": 55}, {"referenceID": 31, "context": "For k = 1 this was proved in the original paper of [15], which later was extended in [32, 9] to general k.", "startOffset": 85, "endOffset": 92}, {"referenceID": 8, "context": "For k = 1 this was proved in the original paper of [15], which later was extended in [32, 9] to general k.", "startOffset": 85, "endOffset": 92}, {"referenceID": 14, "context": "This mysterious bias term Bk,d = (\u03c8(k)\u2212 log(k)) whose original proofs in [15, 32, 9] provided little explanation for, can be alternatively proved with both rigor and intuition by making connections to uniform order statistics.", "startOffset": 73, "endOffset": 84}, {"referenceID": 31, "context": "This mysterious bias term Bk,d = (\u03c8(k)\u2212 log(k)) whose original proofs in [15, 32, 9] provided little explanation for, can be alternatively proved with both rigor and intuition by making connections to uniform order statistics.", "startOffset": 73, "endOffset": 84}, {"referenceID": 8, "context": "This mysterious bias term Bk,d = (\u03c8(k)\u2212 log(k)) whose original proofs in [15, 32, 9] provided little explanation for, can be alternatively proved with both rigor and intuition by making connections to uniform order statistics.", "startOffset": 73, "endOffset": 84}, {"referenceID": 2, "context": "The O(1/n) convergence rate of the variance is established in [3, 18, 2, 4] under various assumptions.", "startOffset": 62, "endOffset": 75}, {"referenceID": 17, "context": "The O(1/n) convergence rate of the variance is established in [3, 18, 2, 4] under various assumptions.", "startOffset": 62, "endOffset": 75}, {"referenceID": 1, "context": "The O(1/n) convergence rate of the variance is established in [3, 18, 2, 4] under various assumptions.", "startOffset": 62, "endOffset": 75}, {"referenceID": 3, "context": "The O(1/n) convergence rate of the variance is established in [3, 18, 2, 4] under various assumptions.", "startOffset": 62, "endOffset": 75}, {"referenceID": 9, "context": "It has been first studied in [10, 11], where root-n consistency is shown in 1-dimension with bounded support and assuming f(x) is bounded below.", "startOffset": 29, "endOffset": 37}, {"referenceID": 10, "context": "It has been first studied in [10, 11], where root-n consistency is shown in 1-dimension with bounded support and assuming f(x) is bounded below.", "startOffset": 29, "endOffset": 37}, {"referenceID": 35, "context": "[36] is the first to prove a root mean squared error convergence rate of O(1/ \u221a n) for general densities with unbounded support in 1-dimension and exponentially decaying tail, such as the Gaussian density.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "These assumptions are relaxed in [5], where zeroes and fat tails are allowed in f(x).", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "In general d-dimensions, [8, 33] prove bounds on the convergence rate of the bias for finite k = O(1), and [24, 1] for k = \u03a9(log n).", "startOffset": 25, "endOffset": 32}, {"referenceID": 32, "context": "In general d-dimensions, [8, 33] prove bounds on the convergence rate of the bias for finite k = O(1), and [24, 1] for k = \u03a9(log n).", "startOffset": 25, "endOffset": 32}, {"referenceID": 23, "context": "In general d-dimensions, [8, 33] prove bounds on the convergence rate of the bias for finite k = O(1), and [24, 1] for k = \u03a9(log n).", "startOffset": 107, "endOffset": 114}, {"referenceID": 0, "context": "In general d-dimensions, [8, 33] prove bounds on the convergence rate of the bias for finite k = O(1), and [24, 1] for k = \u03a9(log n).", "startOffset": 107, "endOffset": 114}, {"referenceID": 26, "context": "This is made precise in [27], which also shows improved convergence rates for distributions whose support is on low dimensional manifolds.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "However, the estimator in [27] critically uses the geodesic distances between the sample points on the manifold.", "startOffset": 26, "endOffset": 30}, {"referenceID": 33, "context": "3 Ensemble estimators Recent works [34, 25, 26, 1] have proposed ensemble estimators, which use known estimators based on kernel density estimators and k-NN methods and construct a new estimate by taking the weighted linear combination of those methods with varying bandwidth or k, respectively.", "startOffset": 35, "endOffset": 50}, {"referenceID": 24, "context": "3 Ensemble estimators Recent works [34, 25, 26, 1] have proposed ensemble estimators, which use known estimators based on kernel density estimators and k-NN methods and construct a new estimate by taking the weighted linear combination of those methods with varying bandwidth or k, respectively.", "startOffset": 35, "endOffset": 50}, {"referenceID": 25, "context": "3 Ensemble estimators Recent works [34, 25, 26, 1] have proposed ensemble estimators, which use known estimators based on kernel density estimators and k-NN methods and construct a new estimate by taking the weighted linear combination of those methods with varying bandwidth or k, respectively.", "startOffset": 35, "endOffset": 50}, {"referenceID": 0, "context": "3 Ensemble estimators Recent works [34, 25, 26, 1] have proposed ensemble estimators, which use known estimators based on kernel density estimators and k-NN methods and construct a new estimate by taking the weighted linear combination of those methods with varying bandwidth or k, respectively.", "startOffset": 35, "endOffset": 50}, {"referenceID": 0, "context": "An intuitive explanation for this phenomenon is provided in [1] in the context of k-NN methods; it is interesting to explore if such a phenomenon continues in the k-LNN scenario studied in this paper.", "startOffset": 60, "endOffset": 63}, {"referenceID": 20, "context": "The gradient of the local likelihood evaluated at the maximizer is zero [21], which gives a computational tool for finding the maximizer:", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "5 of [29] to show that this term vanishes for m = O(log n) and as n grows.", "startOffset": 5, "endOffset": 9}, {"referenceID": 28, "context": "5, [29]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 227, "endOffset": 242}, {"referenceID": 8, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 227, "endOffset": 242}, {"referenceID": 17, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 227, "endOffset": 242}, {"referenceID": 38, "context": "As noted in [28] this is common in the literature on consistency of k-NN estimators, where the same assumptions have been implicitly made without explicitly stating as such, in existing analyses of entropy estimators including [15, 9, 18, 39].", "startOffset": 227, "endOffset": 242}, {"referenceID": 27, "context": "This assumption can be avoided for Renyi entropy as in the proof of consistency in [28] or for sharper results such as the convergence rate of the bias with respect to the sample size but with more assumptions as in [8, 33, 1].", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "This assumption can be avoided for Renyi entropy as in the proof of consistency in [28] or for sharper results such as the convergence rate of the bias with respect to the sample size but with more assumptions as in [8, 33, 1].", "startOffset": 216, "endOffset": 226}, {"referenceID": 32, "context": "This assumption can be avoided for Renyi entropy as in the proof of consistency in [28] or for sharper results such as the convergence rate of the bias with respect to the sample size but with more assumptions as in [8, 33, 1].", "startOffset": 216, "endOffset": 226}, {"referenceID": 0, "context": "This assumption can be avoided for Renyi entropy as in the proof of consistency in [28] or for sharper results such as the convergence rate of the bias with respect to the sample size but with more assumptions as in [8, 33, 1].", "startOffset": 216, "endOffset": 226}, {"referenceID": 20, "context": "From [21] we know that the maximizer is a fixed point of a series of non-linear equations of the form \u2211", "startOffset": 5, "endOffset": 9}], "year": 2016, "abstractText": "Estimators of information theoretic measures such as entropy and mutual information are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In this paper, we combine both these approaches to design new estimators of entropy and mutual information that outperform state of the art methods. Our estimator uses local bandwidth choices of k-NN distances with a finite k, independent of the sample size. Such a local and data dependent choice improves performance in practice, but the bandwidth is vanishing at a fast rate, leading to a non-vanishing bias. We show that the asymptotic bias of the proposed estimator is universal; it is independent of the underlying distribution. Hence, it can be precomputed and subtracted from the estimate. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the asymptotic geometry of nearest neighbors to order statistics is of independent mathematical interest.", "creator": "LaTeX with hyperref package"}}}