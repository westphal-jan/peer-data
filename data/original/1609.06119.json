{"id": "1609.06119", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "FastBDT: A speed-optimized and cache-friendly implementation of stochastic gradient-boosted decision trees for multivariate classification", "abstract": "Stochastic gradient-boosted decision trees are widely employed for multivariate classification and regression tasks. This paper presents a speed-optimized and cache-friendly implementation for multivariate classification called FastBDT. FastBDT is one order of magnitude faster during the fitting-phase and application-phase, in comparison with popular implementations in software frameworks like TMVA, scikit-learn and XGBoost. The concepts used to optimize the execution time and performance studies are discussed in detail in this paper. The key ideas include: An equal-frequency binning on the input data, which allows replacing expensive floating-point with integer operations, while at the same time increasing the quality of the classification; a cache-friendly linear access pattern to the input data, in contrast to usual implementations, which exhibit a random access pattern. FastBDT provides interfaces to C/C++, Python and TMVA. It is extensively used in the field of high energy physics by the Belle II experiment.", "histories": [["v1", "Tue, 20 Sep 2016 11:50:52 GMT  (5861kb,D)", "http://arxiv.org/abs/1609.06119v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["thomas keck"], "accepted": false, "id": "1609.06119"}, "pdf": {"name": "1609.06119.pdf", "metadata": {"source": "CRF", "title": "A speed-optimized and cache-friendly implementation of stochastic gradient-boosted decision trees for multivariate classification", "authors": ["Thomas Keck"], "emails": ["thomas.keck2@kit.edu"], "sections": [{"heading": null, "text": "Keywords: boosted decision trees, multivariate classification, equal-frequency binning, cache-friendly, belle"}, {"heading": "1. Introduction", "text": "In multivariate classification one calculates the probability of a given data-point to be signal, characterised by a set of explanatory features ~x = {x1, . . . , xd} and a class label y (signal y = 1 and background y = \u22121). In supervised machine learning this involves a fittingphase which uses training data-points with known labels and an application-phase, during which the fitted classifier is applied to new data-points with unknown labels. During the fitting-phase, the internal parameters (or model) of a multivariate classifier are adjusted, so that the classifier can statistically distinguish signal and background data-points. The model complexity plays an important role during the fitting-phase and can be controlled by the hyper-parameters of the model. If the model is too simple (too complex) it will be under-fitted (over-fitted) and perform poorly on test data-points with unknown labels. In the following the stochastic gradient-boosted decision tree algorithm (Friedman, 2002) and its hyper-parameters are briefly described.\nar X\niv :1\n60 9.\n06 11\n9v 1\n[ cs\n.L G\n] 2\n0 Se"}, {"heading": "1.1 Decision Tree (DT)", "text": "A DT performs a classification using a number of consecutive cuts (see Figure1). The maximum number of consecutive cuts is a hyper-parameter and is called the depth of the tree D.\nThe cuts are determined during the fitting-phase using a training sample with known labels. At each node only training data-points which passed the preceding cuts are considered. For each feature at each node a cumulative probability histogram (CPH) for signal and background is calculated, respectively. The histograms are used to determine the separation gain for a cut at each position in these histograms. The feature and cut-position (or equivalently bin) with the highest separation gain are used as the cut for the node. Hence each cut locally maximises the separation gain between signal and background on the given training sample.\nThe predictions of a deep DT is often dominated by statistical fluctuations in the training data-points. In consequence, the classifier is over-fitted and performs poorly on new datapoints. There are pruning algorithms which automatically remove cuts prone to over-fitting from the DT (Mingers, 1989). These algorithms are not further discussed here. A detailed description of decision trees is available in Breiman et al. (1984)."}, {"heading": "1.2 Boosted Decision Tree (BDT)", "text": "A BDT constructs a more robust classification model by sequentially constructing shallow DTs during the fitting-phase. The DTs are fitted so that the expectation value of a negative binomial log-likelihood loss-function is minimized. The depth of the individual DT is strongly limited to avoid over-fitting. Therefore a single DT separates signal and background only roughly and is a so-called weak-learner1. By using many weak-learners a well-regularized classifier with large separation power is constructed. The number of trees N (or equivalently the number of boosting steps) and the learning rate \u03b7 are additional\n1. A simple model with few parameters.\nhyper-parameters of this model. The two parameters are anti-correlated meaning decreasing the value of \u03b7 increases the best value for N ."}, {"heading": "1.3 Gradient Boosted Decision Tree (GBDT)", "text": "A GBDT uses gradient-decent in each boosting step to re-weight the original training sample. In consequence, data-points which are hard to classify (often located near the optimal separation hyper-plane) gain influence during the training. A boost-weight calculated for each terminal node during the fitting-phase is used as output of each DT instead of the signal-fraction. The probability of a test data-point (with unknown label) to be signal is the sigmoid-transformed sum of the outputted boost-weights of each tree. The complete algorithm is derived and discussed in detail by Friedman (2000)."}, {"heading": "1.4 Stochastic Gradient Boosted Decision Tree (SGBDT)", "text": "A SGBDT uses a randomly drawn (without replacement) sub-sample instead of the full training sample in each boosting step during the fitting phase. This approach further increases the robustness against over-fitting, because the statistical fluctuations in the training sample are averaged out in the sum over all trees. The incorporation of randomization into the procedure was extensively studied by Friedman (2002). The fraction of samples used in each boosting step is another hyper-parameter called the sub-sampling rate \u03b1."}, {"heading": "1.5 Related Work", "text": "In general there are two approaches to increase the execution speed of an algorithm: Modify the algorithm itself or optimize its implementation.\nIt is easy to see that the first approach has large potential and there are several authors which investigated this approach for the SGBDT algorithm: Already the original paper (Friedman, 2000) on GBDTs showed that 90 % to 95 % of the training data-points can be removed from the fitting-phase after some boosting steps without sacrificing accuracy of the classifier. Another approach was presented in Appel et al. (2013) where a subset of the training data was used to prune underachieving features early during the fitting phase without affecting the final performance. Traditional boosting as discussed above treats the tree learner as a black box, however it is possible to exploit the underlying tree structure to achieve higher accuracy and smaller (hence faster) models (Rie Johnson, 2014).\nFastBDT uses a complementary approach and gains an order of magnitude in execution time by optimizing mainly the implementation of the algorithm. The techniques mentioned above could be implemented in order to get even higher speed-ups."}, {"heading": "2. FastBDT implementation", "text": "On modern hardware it is difficult to predict the execution time, e.g. in terms of spent CPU cycles, because there are many mechanisms built into modern CPUs to exploit parallelisable code execution and memory access patterns. In consequence it is important to benchmark all performance optimisations. In this work perf (de Melo, 2010), valgrind (Nethercote and Seward, 2007) and std::chrono::high_resolution_clock (ISO, 2012) were used to benchmark the execution time and identify critical code sections. The most time consuming code section\nin the SGBDT algorithm is the calculation of the cumulative probability histograms (CPH), which are required in order to calculate the best-cut at each node of the tree. The main concepts used in the implementation of the fitting-phase of FastBDT are described in the following."}, {"heading": "2.1 Memory Access Patterns", "text": "At first we consider the memory layout of the training data. Each training data-point consists of d continuous features, one boolean label (signal or background) and an optional continuous weight. In total there areM training data-points. There are two commonly used memory layouts in this situation: array of structs and struct of arrays (see Figure 2).\nCPU caches assume spatial locality (Kowarschik and Wei\u00df, 2003), which means that if the values in memory are accessed in linear order there is a high probability that they are already cached. In addition, CPU caches assume temporal locality (Kowarschik and Wei\u00df, 2003), which means that frequently accessed values in memory are cached as well. Finally, the CPU assumes branch locality, meaning only a few conditional jumps occure and therefore instructions can be efficiently prefetched and pipelined. The key idea is to use spatial locality (i.e., a linear memory access pattern) for the caching of the large feature input data, temporal locality for the caching of the small cumulative probability histograms (CPH), while at the same time avoiding conditional jumps as much as possible to ensure branch locality.\nFastBDT uses an array of structs memory layout and determines the cuts for all features and all nodes in the same layer of the tree simultaneously. Several sources of conditional jumps and non-uniform memory access are considered:\nSignal and Background The CPHs are calculated separately for signal and background (imagine the red data-points are signal and the blue data-points are background in Figure 2). This leads to additional conditional jumps in both memory layouts. Hence FastBDT stores and processes signal and background data-points separately. Additionally this reduces the amount of CPH data which has to be cached by a factor of two, and saves one conditional jump during the filling of the CPHs.\nMultiple Nodes per Layer In each layer of the decision tree the algorithm has to calculate the optimal cuts for multiple nodes, where each node is optimized with respect to a distinct subset of the training data-points (imagine the blue data-points belong to node A and the red data-points belong to node B in Figure 2). In both memory layouts consecu-\ntive memory access is only possible if the CPHs are calculated for all nodes in the current layer in parallel. Therefore FastBDT calculates the CPHs in different nodes at the same time (the CPH data is likely to be cached due to temporal locality), which allows accessing the values in direct succession (hence the input data is likely to be cached due to spatial locality). In contrast, other popular implementations determine the cuts node-after-node. Hence the data-points which have to be accessed at the current node depend on the preceding nodes and therefore this commonly used approach exhibits random jumps during the memory access regardless of the memory layout.\nStochastic sub-sampling Only a fraction of the training data-points is used during the fitting of each tree (imagine the blue data-points are used and the red data-points are disabled in Figure 2). The array of structs allows consecutive memory access within a data-point without conditional jumps. As a consequence FastBDT uses this memory layout, but has to calculate all the CPHs for the features in parallel as well. In the struct of arrays layout the conditional jumps cannot be avoided without re-arranging the data in the memory."}, {"heading": "2.2 Preprocessing", "text": "Continuous input features are represented by floating point numbers. However DTs only use the order statistic of the features (in contrast to, e.g., artificial neural networks). Hence the algorithm only compares the values to one another and doesn\u2019t use the values themselves. Therefore FastBDT performs an equal-frequency binning on the input features and maps them to integers. This has several advantages: Usually integer operations can be performed faster by the CPU; the integers can directly be used as indices for the CPHs during the calculation of the best-cuts; and the quality of the separation is often improved because the shape of the input feature distribution (which may contain sharp peaks or heavy tails) are mapped to a uniform distribution.\nThis preprocessing is only done during the fitting-phase. Once all cuts are determined the inverse transformation is used to map the integers used in the cuts back to the original floating point numbers. Therefore there is no runtime-overhead during the application-phase due to this preprocessing."}, {"heading": "2.3 Parallelism", "text": "Usually one can distinguish between two types of parallelism: task-level parallelism (multiple processors, multiple cores per processor, multiple threads per core) and instruction-level parallelism (instruction pipelining, multiple execution units and ports, vectorization). It is possible to use task-level parallelism to reduce the execution time during the fittingphase of the SGBDT algorithm. However FastBDT was not designed to do so, since our use-cases typically require fitting many classifiers in parallel and therefore already exploit task-level parallelism effectively, or use a shared infrastructure, where we are only interested in minimizing the total CPU time. Other implementations like XGBoost do take advantage of this type of parallelism.\nThe application-phase is embarrassingly parallel and task-level parallelism can be used by all implementations even if not directly supported2.\nOn the other hand instruction-level parallelism is difficult to exploit in the SGBDT algorithm (in the fitting-phase and the application-phase) due to the large number of conditional jumps intrinsic to the algorithm, which strongly limits the use of instruction pipelining and vectorization. However, due to the chosen memory layout and preprocessing many conditional jumps in the algorithm can be replaced by index operations. Figure 4 show an example for such an optimization (branch locality)"}, {"heading": "3. Comparison", "text": "FastBDT (development version 24.04.2016)3 was compared against other SGBDT implementations used usually in high energy physics:\n\u2022 TMVA (Hoecker et al., 2007) (ROOT version 6.06/00) \u2013 The multivariate analysis package of the ROOT (Brun and Rademakers, 1997) data analysis library developed by CERN (GPLv2 license);\n\u2022 scikit-learn (Pedregosa et al., 2011) (version 0.17.1) \u2013 A machine learning library written in python (BSD license);\n2. E.g. by running the loop over the data-points in parallel using OpenMPI (OpenMP Architecture Review Board, 2015) #pragma omp parallel for like it is done in XGBoost after calling its predict function. 3. git hash ce39fa9ac8cd0e94a5b7d5cdef34300e5d372a63\n\u2022 xgboost (Chen and Guestrin, 2016) (development version 22.04.2016)4 \u2013 A modern implementation of BDTs which performed well in the Higgs Boson challenge (Chen and He, 2015) (Apache license).\nThe used dataset contains 1 million data-points, 35 features and a binary target. It was split into equal parts, used during the fitting and application phase, respectively. The dataset was produced using Monte Carlo simulation of the decay of a D0 meson into one charged pion, one neutral pion and one kaon. A common classification problem in high energy physics. However, the nature of the data has no influence on the execution time of the SGBDT algorithm in the considered implementations, since no optimizations which prune features using their estimated separation power, as described in Appel et al. (2013), are employed.\nIn a first preprocessing step the dataset is converted into the preferred data-format of each implementation:\n\u2022 ROOT TTree for TMVA, \u2022 numpy ndarray for scikit-learn, \u2022 DMatrix for XGBoost \u2022 and FastBDT::EventSample for FastBDT.\nAfterwards, the fitting and application steps are performed for each implementation. Each step is measured individually using std::chrono::high_resolution_clock. The preprocessing time is small5 with respect to the fitting time for most (> 95%) of the investigated hyperparameter configurations. All results stated below are valid as well if one considers the preprocessing as part of the fitting-phase.\nThe execution time of each implementation is measured five times for each considered set of hyper-parameters. If not stated otherwise the following hyper-parameters are chosen:\n\u2022 depth of the trees = 3\n4. git hash b3c9e6a0db0a7eb755949ac6b26e3ef805738350 5. less than < 20% of the training time\n\u2022 number of trees = 100 \u2022 number of features = 35 \u2022 number of training data-points = 500000 \u2022 sampling-rate = 0.5 \u2022 shrinkage = 0.1\nAll implementations have additional hyper-parameters which are not shared by the other implementations. The respective default values are used in these cases.\nTwo versions of XGBoost are considered: The single-core (named just XGBoost in the following) and multi-core version (named XGBoost-i7 in the following). In addition a simple multi-core version of FastBDT is considered in the application-phase measurements (named FastBDT-i7 in the following).\nAll measurements are performed on an Intel(R) Core(TM) i7-4770 CPU (@ 3.40GHz) with a main memory of 32 GigaByte. The code used to perform the measurements can be found in the FastBDT repository."}, {"heading": "3.1 Fitting-Phase", "text": "In general it is expected that the fitting-phase runtime of the algorithms scale linearly in depth of the trees, number of trees, number of features, number of training data-points and sampling-rate. As can be seen from Figure 5 these expectations are not always fulfilled. For each varied hyper-parameter the gradient a and offset c was fitted using an ordinary linear regression.\nFastBDT outperforms all other implementations during the fitting phase, including the multi-core version of XGBoost. scikit-learn is the slowest contestant and also violates the expected linear runtime behaviour for the depth of the trees (see 5b) and the number of training data-points (see 5c). It is not clear why this is the case. TMVA shows a constant runtime for depth of the trees > 6 (see 5b); this can be explained by the default limit on the minimum number of data-points per node of 5%. Hence TMVA stopped growing the trees deeper than six layers. The linear increase in the runtime for the number of features has a constant drop at 20, where TMVA stops filling pairwise histograms for all features, which are used for evaluation and plotting purposes only. FastBDT and XGBoost have a linear runtime behaviour in all considered hyper-parameters, as expected. The multi-core version of XGBoost has an average speedup of 3.83 using 8 cores (including hyper-threading) over the single-core variant and shows a better performance for deep trees. Detailed speedup comparisons between all tested implementations are stated in Table 1."}, {"heading": "3.2 Application-Phase", "text": "During the application-phase the runtime should scale linearly in depth of the trees, number of trees and number of test data-points. The runtime should be independent of the sampling rate and number of features. The results are summarized in Figure 6. Detailed speedup comparisons between all tested implementations are stated in Table 2.\nThe single-core version of FastBDT outperforms all other single-core implementations during the fitting phase. The multi-core version of FastBDT is on average 3.8 times faster\nthan the multi-core version of XGBoost. The runtime behaviour in the number of trees and number of test data-points is as expected. TMVA seems to be faster for small values of the sampling rate and again shows a constant runtime behaviour for depth of the trees > 6. Both can be explained by the limit on the minimum number of data-points per node.\nFastBDT violates the expected linear scaling in the depth of the trees. Probably the caching does not work optimally for deep trees."}, {"heading": "3.3 Classification Quality", "text": "The quality of the classification was evaluated using the area under the receiver operating characteristic (ROC) curve. This quality indicator is independent of the chosen workingpoint (i.e., the desired efficiency or purity) and is widely used to compare the separation power of different algorithms to one another. The higher the value the better the separation between signal and background.\nThe main difference between the implementation arises due to the different regularisation methods. FastBDT uses the equal-frequency binning to prevent over-fitted cuts; XGBoost employs a modified separation gain, which includes the structure of the current tree; and TMVA limits the minimum number of data-points per event to 5%.\nFastBDT outperforms the other implementations in most situations, except for extremely deep trees. In this region the over-fitting effect degrades the performance of the classifier,\nwhereas XGBoost seems to have a superior regularisation method to prevent over-fitting in this situation. The performance of FastBDT and XGBoost is nearly independent of the used sampling rate, whereas TMVA and scikit-learn strongly depend on it. Hence it is likely that TMVA and scikit-learn over-fit in regions with small sampling rate (and therefore low statistic).\nThe classification quality will be different for other datasets, and depends strongly on the concrete classification problem at hand."}, {"heading": "4. Advanced Features", "text": "FastBDT offers more advanced capabilities, three of them are briefly described in the following."}, {"heading": "4.1 Support for Negative Weights", "text": "The boosting algorithm assigns a weight to each data-point in the training dataset. FastBDT supports an additional weight per data-point provided by the user. These individual weights are processed separately from the boosting weights. In particular FastBDT allows for negative individual weights, which are commonly used in data-driven techniques to statistically separate signal and background using a discriminating variable (Martschei et al., 2012). Other frameworks like TMVA, SKLearn and XGBoost support negative weights as well."}, {"heading": "4.2 Support for Missing Values", "text": "There are at least two different kinds of missing values in a dataset. Firstly, missing values which can carry usable information about the target e.g. in particle classification in HEP a feature provided by a detector can be absent because the detector was not activated by the particle. Secondly, missing values which should not be used to infer the target e.g. a feature provided by a detector is absent due to technical reasons.\nFastBDT supports both types of missing values. The first type can be passed as negative or positive infinity, FastBDT will put these values in its underflow or overflow bin. In consequence, a cut can be applied separating the missing from the finite values, and the method can use the information provided by the presence of a missing value. The second type should be passed as NaN (Not a Number) floating point value according to the IEEE 754 floating point standard (iee, 1985). These values are ignored during the fitting and application phase of FastBDT. If the tree tries to cut on a feature, which is NaN, the current node behaves as if it is a terminal-node for the corresponding data-point."}, {"heading": "4.3 Feature Importance Estimation", "text": "May multivariate classification methods offer the possibility to estimate the feature importances, meaning influence of the features on the decision. For BDTs the usual approach to calculate the global feature importance, is to sum up the separation gain of each feature, by looping over all trees and nodes. The individual importance for a single data-point can be calculated similarly by summing up all separation gains along the path of the event through the trees.\nThis approach suffers from the possible correlation and non-linear dependencies between the features, as can be seen by the simple example in Table 3. In a single decision tree, one of the features will have a separation gain of zero, although both carry exactly the same amount of information.\nDue to the fast fitting phase of FastBDT it is viable to use another popular approach, by measuring the decrease in the performance (e.g. the area under the curve (AUC) of the receiver operating characteristic) of the method if a feature is left out. This requires N fit operations, where N is the number of features. We can improve the accuracy of the method further by recursively eliminating the most-important feature, which requires 12N(N +1) fit operations.\nAlthough this approach is independent of the employed classification method, it only becomes a viable option with a fast and robust method."}, {"heading": "5. Conclusion", "text": "FastBDT implements the widely employed Stochastic Gradient-Boosted Decision Tree (SGBDT) algorithm (Friedman, 2002), which exhibits a good out-of-the-box performance and generates an interpretable model. Often, multivariate classifiers are trained once and are applied on big data-sets afterwards. FastBDT performs well in this use-case and offers support for missing values, an equal-frequency preprocessing of the features and a fast application-phase. The main advantage is the fast (in terms of CPU time) fitting-phase compared with popular implementations like TMVA (Hoecker et al., 2007), scikit-learn (Pedregosa et al., 2011) and XGBoost (Chen and Guestrin, 2016). Possible use-cases are: Real-time learning applications, frequent re-fitting of classifiers, fitting a large number of classifiers, and measuring the dependence of many variables to one another.\nFastBDT outperforms other popular implementations in terms of runtime during the fitting-phase and application-phase, as well as in the final classifier quality. The presented runtime measurements are independent of the data-set. The techniques employed by FastBDT could be migrated to other implementations, especially the equal-frequency binning works well with boosted-decision trees.\nFastBDT is licensed under the GPLv3 license and available on github https://github. com/thomaskeck/FastBDT."}], "references": [{"title": "Quickly boosting decision trees \u2013 pruning underachieving features early", "author": ["Ron Appel", "Thomas Fuchs", "Piotr Dollar", "Pietro Perona"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML13),", "citeRegEx": "Appel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Appel et al\\.", "year": 2013}, {"title": "Classification and Regression Trees. Statistics/Probability Series", "author": ["Leo Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": null, "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Root \u2014 an object oriented data analysis framework. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment", "author": ["Rene Brun", "Fons Rademakers"], "venue": "doi: http://dx.doi.org/10.1016/S0168-9002(97)00048-X. URL http://www.sciencedirect. com/science/article/pii/S016890029700048X. New Computing Techniques in Physics", "citeRegEx": "Brun and Rademakers.,? \\Q1997\\E", "shortCiteRegEx": "Brun and Rademakers.", "year": 1997}, {"title": "Xgboost: A scalable tree boosting system, 2016", "author": ["Tianqi Chen", "Carlos Guestrin"], "venue": null, "citeRegEx": "Chen and Guestrin.,? \\Q2016\\E", "shortCiteRegEx": "Chen and Guestrin.", "year": 2016}, {"title": "Higgs boson discovery with boosted trees", "author": ["Tianqi Chen", "Tong He"], "venue": "JMLR Workshop and Conference Proceedings,", "citeRegEx": "Chen and He.,? \\Q2015\\E", "shortCiteRegEx": "Chen and He.", "year": 2015}, {"title": "The new linux \u2019perf", "author": ["Arnaldo Carvalho de Melo"], "venue": "URL http://vger.kernel", "citeRegEx": "Melo.,? \\Q2010\\E", "shortCiteRegEx": "Melo.", "year": 2010}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["Jerome H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman.,? \\Q2000\\E", "shortCiteRegEx": "Friedman.", "year": 2000}, {"title": "Stochastic gradient boosting", "author": ["Jerome H. Friedman"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Friedman.,? \\Q2002\\E", "shortCiteRegEx": "Friedman.", "year": 2002}, {"title": "TMVA: Toolkit for Multivariate Data Analysis", "author": ["Andreas Hoecker", "Peter Speckmayer", "Joerg Stelzer", "Jan Therhaag", "Eckhard von Toerne", "Helge Voss"], "venue": "PoS, ACAT:040,", "citeRegEx": "Hoecker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hoecker et al\\.", "year": 2007}, {"title": "Algorithms for Memory Hierarchies: Advanced Lectures, chapter An Overview of Cache Optimization Techniques and Cache-Aware Numerical Algorithms, pages 213\u2013232", "author": ["Markus Kowarschik", "Christian Wei\u00df"], "venue": null, "citeRegEx": "Kowarschik and Wei\u00df.,? \\Q2003\\E", "shortCiteRegEx": "Kowarschik and Wei\u00df.", "year": 2003}, {"title": "Advanced event reweighting using multivariate analysis", "author": ["D. Martschei", "M. Feindt", "S. Honc", "J. Wagner-Kuhr"], "venue": "J. Phys. Conf. Ser.,", "citeRegEx": "Martschei et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Martschei et al\\.", "year": 2012}, {"title": "An empirical comparison of pruning methods for decision tree induction", "author": ["John Mingers"], "venue": "Machine Learning,", "citeRegEx": "Mingers.,? \\Q1989\\E", "shortCiteRegEx": "Mingers.", "year": 1989}, {"title": "Valgrind: A framework for heavyweight dynamic binary instrumentation", "author": ["Nicholas Nethercote", "Julian Seward"], "venue": "SIGPLAN Not.,", "citeRegEx": "Nethercote and Seward.,? \\Q2007\\E", "shortCiteRegEx": "Nethercote and Seward.", "year": 2007}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Learning nonlinear functions using regularized greedy forest", "author": ["Tong Zhang Rie Johnson"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Johnson.,? \\Q2014\\E", "shortCiteRegEx": "Johnson.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "In the following the stochastic gradient-boosted decision tree algorithm (Friedman, 2002) and its hyper-parameters are briefly described.", "startOffset": 73, "endOffset": 89}, {"referenceID": 11, "context": "There are pruning algorithms which automatically remove cuts prone to over-fitting from the DT (Mingers, 1989).", "startOffset": 95, "endOffset": 110}, {"referenceID": 1, "context": "A detailed description of decision trees is available in Breiman et al. (1984).", "startOffset": 57, "endOffset": 79}, {"referenceID": 6, "context": "The complete algorithm is derived and discussed in detail by Friedman (2000).", "startOffset": 61, "endOffset": 77}, {"referenceID": 6, "context": "The incorporation of randomization into the procedure was extensively studied by Friedman (2002). The fraction of samples used in each boosting step is another hyper-parameter called the sub-sampling rate \u03b1.", "startOffset": 81, "endOffset": 97}, {"referenceID": 6, "context": "It is easy to see that the first approach has large potential and there are several authors which investigated this approach for the SGBDT algorithm: Already the original paper (Friedman, 2000) on GBDTs showed that 90 % to 95 % of the training data-points can be removed from the fitting-phase after some boosting steps without sacrificing accuracy of the classifier.", "startOffset": 177, "endOffset": 193}, {"referenceID": 0, "context": "Another approach was presented in Appel et al. (2013) where a subset of the training data was used to prune underachieving features early during the fitting phase without affecting the final performance.", "startOffset": 34, "endOffset": 54}, {"referenceID": 12, "context": "In this work perf (de Melo, 2010), valgrind (Nethercote and Seward, 2007) and std::chrono::high_resolution_clock (ISO, 2012) were used to benchmark the execution time and identify critical code sections.", "startOffset": 44, "endOffset": 73}, {"referenceID": 9, "context": "CPU caches assume spatial locality (Kowarschik and Wei\u00df, 2003), which means that if the values in memory are accessed in linear order there is a high probability that they are already cached.", "startOffset": 35, "endOffset": 62}, {"referenceID": 9, "context": "In addition, CPU caches assume temporal locality (Kowarschik and Wei\u00df, 2003), which means that frequently accessed values in memory are cached as well.", "startOffset": 49, "endOffset": 76}, {"referenceID": 8, "context": "\u2022 TMVA (Hoecker et al., 2007) (ROOT version 6.", "startOffset": 7, "endOffset": 29}, {"referenceID": 2, "context": "06/00) \u2013 The multivariate analysis package of the ROOT (Brun and Rademakers, 1997) data analysis library developed by CERN (GPLv2 license); \u2022 scikit-learn (Pedregosa et al.", "startOffset": 55, "endOffset": 82}, {"referenceID": 13, "context": "06/00) \u2013 The multivariate analysis package of the ROOT (Brun and Rademakers, 1997) data analysis library developed by CERN (GPLv2 license); \u2022 scikit-learn (Pedregosa et al., 2011) (version 0.", "startOffset": 155, "endOffset": 179}, {"referenceID": 3, "context": "\u2022 xgboost (Chen and Guestrin, 2016) (development version 22.", "startOffset": 10, "endOffset": 35}, {"referenceID": 4, "context": "2016)4 \u2013 A modern implementation of BDTs which performed well in the Higgs Boson challenge (Chen and He, 2015) (Apache license).", "startOffset": 91, "endOffset": 110}, {"referenceID": 0, "context": "However, the nature of the data has no influence on the execution time of the SGBDT algorithm in the considered implementations, since no optimizations which prune features using their estimated separation power, as described in Appel et al. (2013), are employed.", "startOffset": 229, "endOffset": 249}, {"referenceID": 10, "context": "In particular FastBDT allows for negative individual weights, which are commonly used in data-driven techniques to statistically separate signal and background using a discriminating variable (Martschei et al., 2012).", "startOffset": 192, "endOffset": 216}, {"referenceID": 7, "context": "FastBDT implements the widely employed Stochastic Gradient-Boosted Decision Tree (SGBDT) algorithm (Friedman, 2002), which exhibits a good out-of-the-box performance and generates an interpretable model.", "startOffset": 99, "endOffset": 115}, {"referenceID": 8, "context": "The main advantage is the fast (in terms of CPU time) fitting-phase compared with popular implementations like TMVA (Hoecker et al., 2007), scikit-learn (Pedregosa et al.", "startOffset": 116, "endOffset": 138}, {"referenceID": 13, "context": ", 2007), scikit-learn (Pedregosa et al., 2011) and XGBoost (Chen and Guestrin, 2016).", "startOffset": 22, "endOffset": 46}, {"referenceID": 3, "context": ", 2011) and XGBoost (Chen and Guestrin, 2016).", "startOffset": 20, "endOffset": 45}], "year": 2016, "abstractText": "Stochastic gradient-boosted decision trees are widely employed for multivariate classification and regression tasks. This paper presents a speed-optimized and cache-friendly implementation for multivariate classification called FastBDT. FastBDT is one order of magnitude faster during the fitting-phase and application-phase, in comparison with popular implementations in software frameworks like TMVA, scikit-learn and XGBoost. The concepts used to optimize the execution time and performance studies are discussed in detail in this paper. The key ideas include: An equal-frequency binning on the input data, which allows replacing expensive floating-point with integer operations, while at the same time increasing the quality of the classification; a cache-friendly linear access pattern to the input data, in contrast to usual implementations, which exhibit a random access pattern. FastBDT provides interfaces to C/C++, Python and TMVA. It is extensively used in the field of high energy physics by the Belle II experiment.", "creator": "LaTeX with hyperref package"}}}