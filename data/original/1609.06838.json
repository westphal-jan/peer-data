{"id": "1609.06838", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Deep-Learned Collision Avoidance Policy for Distributed Multi-Agent Navigation", "abstract": "High-speed, low-latency obstacle avoidance that is insensitive to sensor noise is essential for enabling multiple decentralized robots to function reliably in cluttered and dynamic environments. While other distributed multi-agent collision avoidance systems exist, these systems require online geometric optimization where tedious parameter tuning and perfect sensing are necessary.", "histories": [["v1", "Thu, 22 Sep 2016 07:05:56 GMT  (2126kb,D)", "https://arxiv.org/abs/1609.06838v1", null], ["v2", "Thu, 6 Jul 2017 07:41:45 GMT  (2244kb,D)", "http://arxiv.org/abs/1609.06838v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CV cs.RO", "authors": ["pinxin long", "wenxi liu", "jia pan"], "accepted": false, "id": "1609.06838"}, "pdf": {"name": "1609.06838.pdf", "metadata": {"source": "CRF", "title": "Deep-Learned Collision Avoidance Policy for Distributed Multi-Agent Navigation", "authors": ["Pinxin Long", "Wenxi Liu", "Jia Pan"], "emails": ["pinxinlong@gmail.com)", "wenxi.liu@hotmail.com)", "pan@cityu.edu.hk)"], "sections": [{"heading": null, "text": "Deep-Learned Collision Avoidance Policy for Distributed Multi-Agent Navigation\nPinxin Long, Wenxi Liu*, and Jia Pan*\nAbstract\u2014High-speed, low-latency obstacle avoidance that is insensitive to sensor noise is essential for enabling multiple decentralized robots to function reliably in cluttered and dynamic environments. While other distributed multi-agent collision avoidance systems exist, these systems require online geometric optimization where tedious parameter tuning and perfect sensing are necessary.\nWe present a novel end-to-end framework to generate reactive collision avoidance policy for efficient distributed multi-agent navigation. Our method formulates an agent\u2019s navigation strategy as a deep neural network mapping from the observed noisy sensor measurements to the agent\u2019s steering commands in terms of movement velocity. We train the network on a large number of frames of collision avoidance data collected by repeatedly running a multi-agent simulator with different parameter settings. We validate the learned deep neural network policy in a set of simulated and real scenarios with noisy measurements and demonstrate that our method is able to generate a robust navigation strategy that is insensitive to imperfect sensing and works reliably in all situations. We also show that our method can be well generalized to scenarios that do not appear in our training data, including scenes with static obstacles and agents with different sizes. Videos are available at https://sites.google.com/view/deepmaca.\nIndex Terms\u2014Collision Avoidance; Distributed Robot Systems; Deep Learning; Multi-Agent Navigation\nI. INTRODUCTION\nSAFE collision avoidance within multi-agent systems isa fundamental problem in robotics, and has many applications including swarm robotics, crowd simulation, AI games, autonomous warehouse and logistics. The problem can generally be defined in the context of an autonomous agent navigating in a scenario with static obstacles and other moving agents. Each agent needs to compute an action at real time and ensure that by executing the action the agent will not collide with the obstacles and other moving agents while making progress towards its goal.\nManuscript received: September, 10, 2016; Revised December, 6, 2016; Accepted December, 29, 2016. This paper was recommended for publication by Editor Nancy Amato upon evaluation of the Associate Editor and Reviewers\u2019 comments. This work was supported by HKSAR Research Grants Council (RGC) General Research Fund (GRF), CityU 17204115, 21203216, and NSFC/RGC Joint Research Scheme CityU103/16 . Asterisk indicates the corresponding author.\nPinxin Long is with Dorabot Inc., Shenzhen, China. This work was done while the author was at the City University of Hong Kong (e-mail: pinxinlong@gmail.com)\n*Wenxi Liu is with the Department of Computer Science, Fuzhou University, Fuzhou, China (e-mail: wenxi.liu@hotmail.com)\n*Jia Pan is with the Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Hong Kong, China (e-mail: jiapan@cityu.edu.hk)\nDigital Object Identifier (DOI): see top of this page.\nPrevious work about multi-agent navigation can be classified into two categories: centralized and decentralized approaches. The centralized approaches focused on computing time-optimal trajectories for all agents to reach their individual goals in a scene with only static obstacles. These methods solve a large optimization problem to compute the timeoptimal plans for all agents simultaneously. For this purpose, they usually have a complete knowledge about all agents\u2019 initial and goal states, and require a perfect communication (i.e., with small error and delay) between the agents and a central coordinate controller, which are difficult to achieve in practice. In addition, the centralized planning system is difficult to scale to handle large numbers of agents and are not robust to motion errors as well as agent failures.\nTo solve the multi-agent navigation in a decentralized manner, we need to replan each agent\u2019s local path at real time to deal with the possible conflict with other agents. Among extensive work addressing this problem, the velocity-based approaches [1]\u2013[5] have gained in popularity due to their robustness and ability to guarantee local collision-free motion for many agents in a cluttered workspace. In the velocity-based framework, each agent employs a continuous cycle of sensing and acting where the action must be computed according to local observations of the environment. These approaches have two main limitations: First, they assume that each agent has perfect sensing about the surrounding environment, while this assumption may be violated due to sensing uncertainty ubiquitous in the real world. This limitation is alleviated in some previous work by using a global localization system (e.g., an overhead motion capture system) to monitor the positions of all agents [3], [5], or using an inter-agent communication protocol for sharing position and velocity information among nearby agents [4], [6], [7]. Second, velocity-based methods usually have many parameters that are sensitive to the scenario settings (e.g., number of agents and shape of ar X\niv :1\n60 9.\n06 83\n8v 2\n[ cs\n.A I]\n6 J\nul 2\n01 7\nobstacles) and thus must be carefully tuned for achieving satisfactory navigation performance. Unfortunately, there is no systematic principle about how to select these parameters, and the manual parameter tuning is tedious.\nThese limitations motivate us to develop a novel decentralized collision avoidance technique for multi-agent navigation, which should not only work in real-world settings without perfect sensing and inter-agent communications but should also provide good collision avoidance performance without tedious parameter tuning. Main results: We present a learning-based collision avoidance framework that provides an end-to-end solution for distributed multi-agent navigation by directly mapping noisy sensor measurements to a steering velocity that is locally collision-free. The end-to-end framework is implemented as Deep Neural Networks (DNNs). To train the network, we collect an extensive dataset consisting of frames showing how an agent should avoid its surrounding agents. Each frame includes both the agent\u2019s observation about other agents and the agent\u2019s reactive collision avoidance strategy in terms of the steering velocity. The dataset is generated by using a state-ofthe-art multi-agent simulator with various parameter settings. We also perform data augmentation on the collected dataset by adding measurement noises and leveraging symmetries to generate more frames, which help to reduce over-fitting and improve the generalization capability of our framework. We train the neural network in an offline manner, and the network learns how to output a collision-avoidance velocity given inputs determined by the agent\u2019s sensor measurements and its goal setting. During the online test, our network will output a local collision avoidance velocity that is then used to update the agent\u2019s position at each sensing-acting cycle until the agent reaches its goal. We evaluate our approach on a variety of simulation scenarios and compare it to the stateof-the-art distributed collision avoidance framework [2]. Our experiments show that our method can effectively generate collision-free motions for multiple agents, and the learned collision avoidance policy is robust against noises in the sensor measurements. Moreover, we also highlight that the learned policy can be well generalized to scenarios that are unseen in the training data, including scenes with static obstacles and with a different number of agents."}, {"heading": "II. RELATED WORK", "text": "In this section, we provide a brief overview of prior work on collision avoidance for multi-agent navigation and machine learning for multi-agent systems."}, {"heading": "A. Collision Avoidance for Multi-Agent Navigation", "text": "Collision avoidance has been studied extensively for safe and efficient multi-agent navigation. Many approaches have been proposed, including techniques based on potential fields [8], local variable-resolution fields [9], dynamic windows [10], and velocity obstacles [1], [2], [11]. Among them is the Optimal Reciprocal Collision Avoidance (ORCA) navigation framework [2], which has been a successful velocitybased approach to avoid collisions with other moving agents\nand obstacles. ORCA has been popular in crowd simulation and multi-agent systems due to its two properties. First, it provides a sufficient condition for multiple robots to avoid collisions with each other, and thus can guarantee collisionfree navigation; second, it is a fully distributed method where robots share no knowledge with each other, and thus can easily be scaled to handle large systems with many robots. However, ORCA and its variants (e.g., [3], [5]) have many parameters that are difficult to tune. More important, these methods are sensitive to the uncertainties ubiquitous in the real-world scenarios. In particular, each robot is assumed to have an accurate observation about the surrounding agents\u2019 positions, velocities and shapes; while in practice such information is extracted from noisy sensing measurement via segmentation and tracking, and thus may have significant uncertainties. To alleviate the requirement of perfect sensing, Hennes et al. [4] and Claes et al. [7] extended the ORCA paradigm with an inter-agent communication protocol for sharing knowledge about agents\u2019 positions and velocities. A one-way communication scheme is also introduced in [6] to coordinate the movement of agents in a crowd scenario. Other approaches [3], [5] avoided the difficulty in sensing uncertainty by using an overhead motion capture system to obtain the global position observation for all agents. Moreover, Yoon et al. [12] used multiple visual sensors to track individuals\u2019 trajectories in crowds. In this paper, we use an end-to-end framework to learn a collision-avoidance policy which is robust to the imperfect sensing and requires no inter-agent communication, and thus is still fully distributed."}, {"heading": "B. Machine Learning for Multi-Agent Systems", "text": "Reinforcement learning has been widely used for the multiagent decision making [13]\u2013[16], which is formulated as a multi-agent Markov Decision Processes (MDP) problem. Multi-agent reinforcement learning allows agents to learn a policy, i.e., a mapping from agent states to actions according to the rewards obtained while interacting with the surrounding environment. In these methods, each independent agent needs to build an MDP model via repeated offline simulation, and\nthen uses the model-based reinforcement learning to compute an optimal policy. An online multi-agent policy adaption approach is proposed by Godoy et al. [17], which originates from multi-arm bandits. They use an online learning framework to plan over the space of preferred velocities and then project these velocities to collision-free ones using ORCA, e.g., their method still holds the perfect sensing assumption and requires parameter-tuning for ORCA. The cooperative approach proposed by Kretzschmar et al. [18] first infers the internal goals of other agents, then plans a set of jointly possible paths for all neighboring agents in the shared environment. However, it is computationally expensive for generating paths for all others agents. Boatright et al. [19] train a set of machinelearned policies by decomposing possible scenarios an agent may encounter into steering contexts. In this paper, we use an end-to-end learning mechanism for supervised training a deep neural network policy from an extensive collection of multi-agent collision avoidance data."}, {"heading": "III. PROBLEM FORMULATION", "text": "The multi-agent navigation problem can be formally defined as follows. We take as given a set of n decision-making agents sharing a 2D environment consisting of obstacles. For simplicity, we assume the geometric shape of each agent ai (1 \u2264 i \u2264 n) is modeled as a disc with a fixed radius rai . In addition, the agent\u2019s dynamics is assumed to be holonomic, i.e., it can move in any direction in the 2D workspace.\nEach agent employs a continual cycle of sensing and acting with a time period \u03c4 . During each cycle, the agent ai computes a local trajectory that starts from its current position pai and has a smallest deviation from a preferred velocity vprefai . The preferred velocity is used to guide the agent in making progress toward its goal gai . In a scenario without static obstacles, vprefai can directly point toward gai ; in a scene with static obstacles, vprefai may point toward a closest node in a precomputed roadmap [1]. The local trajectory should be collision-free with other agents and obstacles, at least within the time horizon \u03c4 .\nThe agent computes the local trajectory by taking into account three factors: its current velocity vai , the observation oai about the surrounding environment, and its preferred velocity vprefai . In previous work such as [1], [2], the observation oai consists of the nearby agents\u2019 positions, velocities and shapes. However, the estimation of these quantities in the real world requires agent-based recognition and tracking, which is difficult to be implemented reliably. In our method, oai only includes the raw sensor measurements about the surrounding environment, and thus is more robust and feasible in practice.\nIn particular, the agent feeds pai , vai , oai and v pref ai into a reactive controller F , whose output will be parsed as a collision avoidance velocity in the next step:\nv+ai = F (pai ,vai ,oai ,v pref ai ), (1)\nwhich is then executed by the agent to update its position as\np+ai = pai + v + ai \u00b7 \u03c4. (2)\nThe agent repeats this cycle until arriving at its goal. During the navigation, agents are not allowed to communicate with\neach other and must make navigation decisions independently, according to the observations collected by their on-board sensors. We do not assume that the agents have perfect sensing about the positions, velocities and shapes of other agents, while such knowledge is usually necessary for previous approaches [1]\u2013[3], [5], [17].\nThe reactive controller F is computed by first converting the observation oai and v pref ai into the local coordinate frame of the agent ai, and then training a deep neural network f using these data as network inputs. The network solves a multi-class classification problem and outputs a probability distribution which is used to determine the agent\u2019s velocity increment \u2206vai for safe collision avoidance and making progress towards the goal. The eventual collision avoidance velocity v+ai is then computed as\nv+ai = vai + \u2206vai . (3)\nWe name this deep neural network as the collision avoidance network (CANet)."}, {"heading": "IV. LEARNING-BASED COLLISION AVOIDANCE", "text": "We begin this section by reviewing the ORCA algorithm, which, with appropriate tuned parameters, is able to produce locally collision-free motions for multiple agents. Next, we describe the details about how to leverage the ORCA algorithm to generate a large training dataset for learning a robust collision avoidance policy in terms of a deep neural network. Finally, we elaborate the network architecture and training details about the collision avoidance policy."}, {"heading": "A. A Recap of ORCA", "text": "In a nutshell, ORCA takes two steps to determine a collision-avoidance velocity v+ai for an agent ai. First, it computes a set of velocities that form the permitted velocity space for the agent, i.e., if choosing a velocity within this space, the agent ai will not collide with other agents in a time horizon \u03c4 . The permitted velocity set is denoted as ORCA\u03c4ai Next, among these permitted velocities, the agent selects the collision avoidance velocity as a velocity that locates inside the permitted velocity space but is closest to its current preferred velocity vprefai , i.e.,\nv+ai = argmin v\u2208ORCA\u03c4ai \u2016v \u2212 vprefai \u2016, (4)\nwhere vprefai has been introduced in Section III. For good performance, the ORCA\u2019s parameters must be tuned carefully during the simulation for different scenarios. A list of the related ORCA parameters is shown in Table I. While varying these parameters, ORCA presents different collision avoidance behaviors, e.g. agents will be more \u201dshy\u201d if NEIGHBORDIST is assigned a larger value. For some highly symmetric scenarios, ORCA agents will get stuck by each other without a carefully chosen TIMEHORIZON. Furthermore, if you change the agent\u2019s PROTECTRADIUS, you will need to select new values for all other parameters."}, {"heading": "B. Dataset", "text": "The training of deep neural networks requires a sizable body of training data. However, directly collecting multi-robot navigation data in the real world could be both challenging and expensive. Thus, in this work we generate training data using the RVO2 simulator1 running the ORCA algorithm with many different configurations in terms of ORCA parameter settings. In this way, the learned policy will behave superior to a simulator with a fixed parameter in term of collision avoidance robustness and efficiency.\n1) Data Generation: Our setup for the data collection of collision avoidance behaviors is shown in Figure 3. Given an agent A whose data is to be recorded, we put it at the origin in the global coordinate space because the agent\u2019s absolute position is not important for the collision avoidance. We then sample its preferred velocity vprefA along a random direction. Next, we generate a few agents randomly placed within the agent A\u2019s neighborhood of radius NEIGHBORDIST. The velocities of all agents are randomly initialized: the velocity magnitude is sampled from a uniform distribution over the interval [0,MAXSPEED], and the direction is also uniformly sampled from [\u2212\u03c0, \u03c0). Note that in this setup we do not add any static obstacles.\nWe repeat the above setup many times with different simulating configurations and generate a large amount of random scenarios. For each scenario, instead of running the simulator many times to generate a sequence, we only execute the simulator one step to generate one frame of collision avoidance data. The reason is the sequence data will have strong correlations with each other, while for training a deep neural network, data items independent with each other are more desirable.\nFor each frame, we record the agent A\u2019s observation oA about the surrounding agents, its preferred velocity vprefA , and the collision avoidance velocity v+A calculated by ORCA. To acquire oA, we mount a simulated 360 degree 2D laser scanner at the center of the agent A. The simulated scanner has an angular resolution of 1 degree and a maximum range of 4 meters. In this way, each scan zA provides 360 distance values (though in Figure 3 we only show 72 scan lines for legibility) ranging from agent radius to the scanner\u2019s maximum range. These distance values imply the shapes and positions of other agents in the agent A\u2019s surrounding environment. We further\n1http://gamma.cs.unc.edu/RVO2/\ninfer these neighboring agents\u2019 velocities by performing a non-rigid point cloud matching between the current scan and the scan in previous time step using the coherent point drift algorithm [20] implemented with the fast Gauss Transform. The matching result estimates the velocity of each point in the current scan, which is denoted as z\u0307A. Two examples of the matched point clouds are shown in Figure 4.\nAfter collecting oA = [zA, z\u0307A], v pref A and v + A , we further convert them from the global coordinate space to the local coordinate space fixed at the center of the agent A, because the collision avoidance behavior should only rely on the agent\u2019s local information. We denote the local observation as o\u0302A, the local preferred velocity as v\u0302prefA = v pref A \u2212 vA, and v\u0302+A = v + A \u2212 vA. In this way, we have prepared the input for the neural network as [o\u0302A, v\u0302 pref A ]. The input o\u0302A has 1080 dimensions consisting of the scan with 360 dimensions and its estimated velocity with 720 dimensions. The output for the neural network is the label for the velocity v\u0302+A in a velocity cluster, as will be discussed later in Section IV-B3.\nAs described in Table I, there are seven parameters to be tuned in ORCA, we fix RADIUS = 0.2m, MAXSPEED = 3.5m/s, MAXNEIGHBORS = 10, NEIGHBORDIST = 3.0m and TIMEHORIZONOBS = 1.0s for all agents; we vary the PROTECTRADIUS from set of {0.2, 0.5}m, and vary TIMEHORIZON from {0.5, 1.0, 2.0}s during the data collection. We do not vary TIMEHORIZONOBS because there is no static obstacle in the training data. Along with the two varied parameters above, another two variables can be changed. The first is the number of A\u2019s neighbors, which can vary from 3 to 10. The other is the sensor measurement noise, which is a Gaussian noise with a standard deviation ranging between 0.01 and 0.05. We generate in total about 310, 000 examples, where each example is a pair in form of ([o\u0302A, v\u0302 pref A ], v\u0302 + A).\n2) Data Cleansing, Augmentation and Preprocessing: Before feeding the saved examples to the CANet, we need to\nfirst perform some data cleansing, augmentation and preprocessing techniques. For data cleansing, we remove the cases that the agent A updates its position with v+A computed by ORCA but still collides with its neighbors. We then delete the unreasonable outliers by checking whether the speed of collision avoidance velocity v+A is close to MAXSPEED. For data augmentation, we generate more examples by first adding measurement noises to the inputs and secondly leveraging symmetries in the collision avoidance scenario along the axis of vA, i.e., if we mirror the positions and velocities of the neighboring agents and the preferred velocity along the axis of vA, the mirrored version of v+A will be a valid collision avoidance velocity. As shown in [21], [22], data augmentation technologies can moderate over-fitting and improve the generalization capability of the learned policy. It is important to note that data augmentation only adds some redundancy and does not rely on any external source of new information.\nFinally, the training data will be performed standardization (or Z-Score normalization) before being fed to the network.\n3) Collision Avoidance Velocity Clustering: As stated in Section III, we formulate the computation of the reactive collision avoidance strategy as a multi-class classification problem. In other words, we divide the space of all possible collision avoidance velocities into several classes; and in the runtime the reactive controller will determine which class the collision avoidance velocity should be chosen from, given the sensor observation and the preferred velocity.\nTo generate a reasonable partition for the space of collision avoidance velocity, we first perform a k-means clustering on the v+A and use the clustering result shown in Figure 5a as a reference, we manually design a partition with 61 classes as shown in Figure 5b.\nWe choose to not model the computation of collision avoidance velocity as a regression problem because the l2 loss function for regression tasks usually is more fragile to outliers [23]. In addition, it is more desirable to output a probability about selecting a collision avoidance velocity given a noisy sensor measurement, but the regression only generates a single velocity output with no indication about the confidence."}, {"heading": "C. Collision Avoidance Network", "text": "The CANet is a two-branch multilayer perceptron (MLP) and its architecture is as summarized in Figure 6. Following is the details about the different components of this network:\n1) Architecture: The CANet has two branches. The input of the main branch is the agent\u2019s observation o\u0302A and the input of the auxiliary branch is v\u0302prefA , which are both described in Section IV-B1. The output of the CANet is a probability distribution over the velocity classes which will be parsed to the collision avoidance velocity v\u0302+ for updating the agent\u2019s position. In the main branch, there are four fully connected hidden layers after the input layer, and these layers consist of 1024, 1024, 512 and 256 rectified linear units (ReLUs) respectively. A dropout layer with probability 0.2 is applied after each of these hidden layers. The auxiliary branch has only one fully connected hidden layer with 256 ReLUs. The two branches are merged by concatenating the fourth layer of the main branch with the hidden layer of the auxiliary branch. This merged layer is then followed by a fully connected layer with one neuron per class, activated by a softmax function. We use the cross-entropy loss function during the training stage.\n2) Training: We randomly split the dataset into ten stratified folds preserving the percentage of samples for each class and report results using 10-fold cross-validation. We train our network for about 5 hours until convergence on a single Nvidia Titan X GPU, using stochastic gradient descent (SGD) with the momentum of 0.9. We use a base learning rate of 0.1 and a decay rate of 0.0002. The network is trained for the maximal 300 epochs with early stopping using a batch size of 64. The classification accuracy on the test dataset is 33.435% (\u00b10.354%) and 64.106% (\u00b10.313%) on the training set.\n3) Collision Avoidance Velocity: CANet will output a distribution over the collision avoidance velocity Pr(l = L(v\u0302+)), where L(v\u0302+) is the class label for each v\u0302+. In this work, we determine the actual collision avoidance velocity using a simple method. We first choose the class l with the highest probability and then perform random sampling inside the class around the class centroid. Next, we compute the safety margins (i.e., closest distance to obstacles) when the agent applies these sampled velocities as the collision avoidance velocity in the time horizon \u03c4 , and choose the velocity with the maximum safety margin as our result. If this velocity will make the agent collide with obstacles (i.e., the minimum safety margin is negative), we will slow down the velocity accordingly."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": "This section presents experiments and results of the proposed framework. We have evaluated this framework in various simulated scenarios and compared it to ORCA. We have also tested our method on a real multi-robot system."}, {"heading": "A. Experiment Setup", "text": "1) Scenarios: We evaluated our learned policy on six different scenarios with different number of agents (as shown in Figure 7). Note that the test scenarios 3 Obstacles and 1 Obstacle have static obstacles, which never appear in any data collection scenarios for training the CANet. In addition, since the trained policy outputs the collision avoidance velocity in a random manner, its performance is averaged over 20 simulations.\nWe compared the performance of learned policy with the ORCA policy. Most parameters of the ORCA policy are set to be the same as the values used in the data generation for the learned policy (as stated in Section IV-B1), but we tuned some parameters to optimize ORCA\u2019s performance. In particular, to obtain the best performance of ORCA, we change TIMEHORIZONOBS to 10.0s for scenarios with static obstacles and tune TIMEHORIZON for different scenarios. In each simulation, the performance of ORCA is evaluated with two different PROTECTRADIUS values 0.2m and 0.5m. The time step size \u03c4 of the sensing-acting cycle is set to 0.1s. The detailed description for each test scenario is as follows:\n\u2022 Crossing: agents are separated in two groups, and their path will intersect in the bottom left corner; \u2022 Circle: agents are initially located along a circle and each agent\u2019s goal is to reach its antipodal position; \u2022 Swap: two groups of agents moving in opposite directions swap their positions; \u2022 Random: agents are randomly initialized in a cluttered environment and are assigned random goals; \u2022 3 Obstacles: six agents move across three obstacles; \u2022 1 Obstacle: four agents initialized on a circle move\ntowards their antipodal positions, and an obstacle is located at the center.\nThe trajectories generated using the learned navigation policy are shown in Figure 1 and Figure 8.\n2) Performance Metrics: To compare the performance of our framework and ORCA quantitatively, we use the following performance metrics: \u2022 Total travel time: the time taken by the last agent to reach\nits goal; \u2022 Total distance traveled: the total distance traveled by all\nagents to reach their goals. \u2022 Safety margin: the agent\u2019s closest distance to other agents\nand static obstacles; \u2022 Completion: if all agents reach their goals within a time\nlimit without any collisions, the scenario is successfully completed."}, {"heading": "B. Quantitative Comparisons", "text": "In Figure 9 and 10, we measure two metrics \u2013 total travel time and total traveled distance \u2013 to evaluate the performance of our approach and ORCA. We can observe that when comparing with the ORCA policy with PROTECTRADIUS = 0.5, the learned policy provides better or comparable performance in terms of navigation duration and trajectory length. In most scenarios, the ORCA policy with PROTECTRADIUS = 0.2 has shorter navigation time and trajectory length than the learned policy. This is because the ORCA policy with PROTECTRADIUS = 0.2 is very aggressive and allows a small safe margin during the navigation, as shown in Table II. Both our learned policy and the ORCA policy with PROTECTRADIUS = 0.5 try to keep a large enough margin with nearby agents/obstacles. The difference is that the ORCA policy with PROTECTRADIUS = 0.5 uses the protect radius parameter to keep a hard margin: no obstacles/agents should get closer to the agent than PROTECTRADIUS \u2212 RADIUS = 0.5\u22120.2 = 0.3m, and this constraint may be too conservative in a cluttered scene.\nInstead, our method learns the preference for margin implicitly from the data and is able to keep the clearance in an adaptive manner: in cluttered situations, the agents can endure a small safety-margin while in an open space, the agents will tend to keep a large safety-margin. For instance, the safety margin in the 3 Obstacles scenario is smaller than in the 1 Obstacle scene as shown in Table II, because the former is more cluttered.\nWe also set up a more challenging scenario with an L-shape static obstacle at the center (shown in Figure 11) and measure the Completion metric. We randomly generate 100 initial states where all agents are randomly placed and they are assigned\nwith appropriate random goals. We then compare our method and ORCA by counting the number of failures, i.e., some agents do not reach their goals or severely collide with other agents/obstacles during the runtime. ORCA has a failure rate of 15% while our learned policy only has 2%. Figure 11 shows a stuck case for ORCA while our learned policy can complete it successfully."}, {"heading": "C. Generalization", "text": "An interesting phenomenon while using the learned policy is that in a highly symmetrical scenario like Circle, the agents will present certain cooperative behaviors since all agents are using the same learned policy. For instance, a cooperative rotation behavior is shown in Figure 1b where agents starts to rotate at the same pace when they are close to each other. While for ORCA (as shown in Figure 1a), each agent passes the central area by itself without any collective behaviors and some agents yields jerky motions.\nThe good generalization capability is another notable feature of our method. The learned policy\u2019s performance in scenarios with static obstacles demonstrates that it generalizes well to handle to previously unseen situations. In addition, we also\nevaluate the learned policy in the Circle scenario with four different-sized agents. In Figure 12a, agents with the same size have identical paths as they take the same strategy to avoid collision with each other. When one agent gets bigger (as shown in Figure 12b), it will deviate more from the original path to generate safe movements and this causes other agents on its path to adjust navigation behaviors accordingly. Please note that this experiment (Figure 12c and 12d) does not reveal the generalization of ORCA since ORCA always knows all agents\u2019 radii before computing the collision-free velocity.\nWe have also demonstrated the proposed method on real robots where each robot is mounted with a Hokuyo URG04LX-UG01 2D lidar sensor. In Figure 13, four robots, three on the right side and one on the left side, are moving to their antipodal positions. As we observe, each robot can effectively avoid collisions with other robots during the navigation in a complete distributed manner. In addition, our system does not require any AR tags and/or additional motion capture systems to offer each agent with the position and/or velocity information about the other agents. In this way, our system can achieve real decentralized multi-agent navigation without any centralized components."}, {"heading": "VI. CONCLUSION AND LIMITATIONS", "text": "This paper is our first step toward learning a reactive collision avoidance policy for efficient and safe multi-agent navigation. By carefully designing the data collection process and leveraging an end-to-end learning framework, our method can learn a deep neural network based collision avoidance policy which demonstrates an advantage over the state-of-theart ORCA policy in terms of ease of use (no parameter tuning), success rate, and navigation performance. In addition, even though being trained over dataset with only identical moving agents, our learned policy generalizes well to various unseen situations, including agents with different sizes and scenarios with static obstacles.\nThe proposed method has some limitations. First, at the current stage, we are training a vanilla multilayer perceptron as the collision avoidance policy. As can be observed from the classification accuracy, the model does not completely fit the training data (the accuracy on training set is around 64%), thus there is still great potential for getting the model improved. Second, we did not add any static obstacles during training data generation, and therefore our model may not perform well in some challenging scenarios with obstacles (e.g., agents pass through a narrow hallway, multiple agents exit a room through a narrow doorway). These challenging tasks can be solved by combining our method with the cutting-edge deep reinforcement learning techniques, which will further improve agents navigation performance.\nBesides the combination with reinforcement learning, there are many other exciting avenues for the future work, such as the extension to vehicles with complex dynamics (e.g., the quadrotors), how to directly leverage 2D/3D camera sensors, and most importantly, to make the entire framework work reliably in real systems (e.g., the automated warehouse) with a large number of agents."}], "references": [{"title": "Reciprocal velocity obstacles for real-time multi-agent navigation", "author": ["J. Van den Berg", "M. Lin", "D. Manocha"], "venue": "IEEE International Conference on Robotics and Automation, 2008, pp. 1928\u20131935.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "The hybrid reciprocal velocity obstacle", "author": ["J. Snape", "J. van den Berg", "S.J. Guy", "D. Manocha"], "venue": "IEEE Transactions on Robotics, vol. 27, no. 4, pp. 696\u2013706, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-robot collision avoidance with localization uncertainty", "author": ["D. Hennes", "D. Claes", "W. Meeussen", "K. Tuyls"], "venue": "International Conference on Autonomous Agents and Multiagent Systems-Volume 1, 2012, pp. 147\u2013 154.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Generalized reciprocal collision avoidance", "author": ["D. Bareiss", "J. van den Berg"], "venue": "The International Journal of Robotics Research, vol. 34, no. 12, pp. 1501\u20131514, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Implicit coordination in crowded multi-agent navigation", "author": ["J. Godoy", "I. Karamouzas", "S.J. Guy", "M. Gini"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Collision avoidance under bounded localization uncertainty", "author": ["D. Claes", "D. Hennes", "K. Tuyls", "W. Meeussen"], "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012, pp. 1192\u2013 1198.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Potential field methods and their inherent limitations for mobile robot navigation", "author": ["Y. Koren", "J. Borenstein"], "venue": "IEEE International Conference on Robotics and Automation, 1991, pp. 1398\u20131404.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1991}, {"title": "Parallelized egocentric fields for autonomous navigation", "author": ["M. Kapadia", "S. Singh", "W. Hewlett", "G. Reinman", "P. Faloutsos"], "venue": "The Visual Computer, vol. 28, no. 12, pp. 1209\u20131227, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "The dynamic window approach to collision avoidance", "author": ["D. Fox", "W. Burgard", "S. Thrun"], "venue": "IEEE Robotics Automation Magazine, vol. 4, no. 1, pp. 23\u201333, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Motion planning in dynamic environments using velocity obstacles", "author": ["P. Fiorini", "Z. Shiller"], "venue": "The International Journal of Robotics Research, vol. 17, no. 7, pp. 760\u2013772, 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Filling in the blanks: reconstructing microscopic crowd motion from multiple disparate noisy sensors", "author": ["S. Yoon", "M. Kapadia", "P. Sahu", "V. Pavlovic"], "venue": "2016 IEEE Winter Applications of Computer Vision Workshops (WACVW). IEEE, 2016, pp. 1\u20139.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning in the multi-robot domain", "author": ["M.J. Matari\u0107"], "venue": "Robot colonies. Springer, 1997, pp. 73\u201383.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiagent systems: A survey from a machine learning perspective", "author": ["P. Stone", "M. Veloso"], "venue": "Autonomous Robots, vol. 8, no. 3, pp. 345\u2013383, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Multiagent reinforcement learning for multi-robot systems: A survey", "author": ["E. Yang", "D. Gu"], "venue": "tech. rep, Tech. Rep., 2004.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["L. Panait", "S. Luke"], "venue": "Autonomous agents and multi-agent systems, vol. 11, no. 3, pp. 387\u2013434, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive learning for multi-agent navigation", "author": ["J.E. Godoy", "I. Karamouzas", "S.J. Guy", "M. Gini"], "venue": "International Conference on Autonomous Agents and Multiagent Systems, 2015, pp. 1577\u20131585.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Socially compliant mobile robot navigation via inverse reinforcement learning", "author": ["H. Kretzschmar", "M. Spies", "C. Sprunk", "W. Burgard"], "venue": "The International Journal of Robotics Research, p. to appear, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating a multiplicity of policies for agent steering in crowd simulation", "author": ["C.D. Boatright", "M. Kapadia", "J.M. Shapira", "N.I. Badler"], "venue": "Computer Animation and Virtual Worlds, vol. 26, no. 5, pp. 483\u2013494, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Point set registration: Coherent point drift", "author": ["A. Myronenko", "X. Song"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 32, no. 12, pp. 2262\u20132275, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv:1512.02595, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust optimization for deep regression", "author": ["V. Belagiannis", "C. Rupprecht", "G. Carneiro", "N. Navab"], "venue": "IEEE International Conference on Computer Vision, 2015, pp. 2830\u20132838.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Among extensive work addressing this problem, the velocity-based approaches [1]\u2013[5] have gained in popularity due to their robustness and ability to guarantee local collision-free motion for many agents in a cluttered workspace.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Among extensive work addressing this problem, the velocity-based approaches [1]\u2013[5] have gained in popularity due to their robustness and ability to guarantee local collision-free motion for many agents in a cluttered workspace.", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": ", an overhead motion capture system) to monitor the positions of all agents [3], [5], or using an inter-agent commu-", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": ", an overhead motion capture system) to monitor the positions of all agents [3], [5], or using an inter-agent commu-", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "nication protocol for sharing position and velocity information among nearby agents [4], [6], [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "nication protocol for sharing position and velocity information among nearby agents [4], [6], [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "nication protocol for sharing position and velocity information among nearby agents [4], [6], [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "fields [8], local variable-resolution fields [9], dynamic windows [10], and velocity obstacles [1], [2], [11].", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "fields [8], local variable-resolution fields [9], dynamic windows [10], and velocity obstacles [1], [2], [11].", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "fields [8], local variable-resolution fields [9], dynamic windows [10], and velocity obstacles [1], [2], [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "fields [8], local variable-resolution fields [9], dynamic windows [10], and velocity obstacles [1], [2], [11].", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "fields [8], local variable-resolution fields [9], dynamic windows [10], and velocity obstacles [1], [2], [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": ", [3], [5]) have many parameters that are difficult to tune.", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": ", [3], [5]) have many parameters that are difficult to tune.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "[4] and Claes et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] extended the ORCA paradigm with an inter-agent communication protocol for sharing knowledge about agents\u2019 positions and velocities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "A one-way communication scheme is also introduced in [6] to coordinate the movement of agents in a crowd scenario.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "Other approaches [3], [5] avoided the difficulty in sensing uncertainty by using an overhead motion capture system to obtain the global position observation for all agents.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "Other approaches [3], [5] avoided the difficulty in sensing uncertainty by using an overhead motion capture system to obtain the global position observation for all agents.", "startOffset": 22, "endOffset": 25}, {"referenceID": 10, "context": "[12] used multiple visual sensors to track individuals\u2019 trajectories in", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Reinforcement learning has been widely used for the multiagent decision making [13]\u2013[16], which is formulated as a multi-agent Markov Decision Processes (MDP) problem.", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "Reinforcement learning has been widely used for the multiagent decision making [13]\u2013[16], which is formulated as a multi-agent Markov Decision Processes (MDP) problem.", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "[17], which originates from multi-arm bandits.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] first infers the internal goals of other agents, then plans a set of jointly possible paths for all neighboring agents in the shared environment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] train a set of machinelearned policies by decomposing possible scenarios an agent may encounter into steering contexts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "In a scenario without static obstacles, v ai can directly point toward gai ; in a scene with static obstacles, v ai may point toward a closest node in a precomputed roadmap [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 0, "context": "In previous work such as [1], [2], the observation oai consists of the nearby agents\u2019 positions, velocities and shapes.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "agents, while such knowledge is usually necessary for previous approaches [1]\u2013[3], [5], [17].", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "agents, while such knowledge is usually necessary for previous approaches [1]\u2013[3], [5], [17].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "agents, while such knowledge is usually necessary for previous approaches [1]\u2013[3], [5], [17].", "startOffset": 83, "endOffset": 86}, {"referenceID": 15, "context": "agents, while such knowledge is usually necessary for previous approaches [1]\u2013[3], [5], [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "infer these neighboring agents\u2019 velocities by performing a non-rigid point cloud matching between the current scan and the scan in previous time step using the coherent point drift algorithm [20] implemented with the fast Gauss Transform.", "startOffset": 191, "endOffset": 195}, {"referenceID": 19, "context": "As shown in [21], [22], data augmentation technologies can moderate over-fitting and improve the generalization capability of the learned policy.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "As shown in [21], [22], data augmentation technologies can moderate over-fitting and improve the generalization capability of the learned policy.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "We choose to not model the computation of collision avoidance velocity as a regression problem because the l2 loss function for regression tasks usually is more fragile to outliers [23].", "startOffset": 181, "endOffset": 185}], "year": 2017, "abstractText": "High-speed, low-latency obstacle avoidance that is insensitive to sensor noise is essential for enabling multiple decentralized robots to function reliably in cluttered and dynamic environments. While other distributed multi-agent collision avoidance systems exist, these systems require online geometric optimization where tedious parameter tuning and perfect sensing are necessary. We present a novel end-to-end framework to generate reactive collision avoidance policy for efficient distributed multi-agent navigation. Our method formulates an agent\u2019s navigation strategy as a deep neural network mapping from the observed noisy sensor measurements to the agent\u2019s steering commands in terms of movement velocity. We train the network on a large number of frames of collision avoidance data collected by repeatedly running a multi-agent simulator with different parameter settings. We validate the learned deep neural network policy in a set of simulated and real scenarios with noisy measurements and demonstrate that our method is able to generate a robust navigation strategy that is insensitive to imperfect sensing and works reliably in all situations. We also show that our method can be well generalized to scenarios that do not appear in our training data, including scenes with static obstacles and agents with different sizes. Videos are available at https://sites.google.com/view/deepmaca.", "creator": "LaTeX with hyperref package"}}}