{"id": "1609.07479", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "Incorporating Relation Paths in Neural Relation Extraction", "abstract": "Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which provide rich and useful information for relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with baselines.", "histories": [["v1", "Fri, 23 Sep 2016 19:59:51 GMT  (287kb,D)", "http://arxiv.org/abs/1609.07479v1", "9 pages, 3 figures, 4 tables"], ["v2", "Tue, 15 Aug 2017 08:56:42 GMT  (287kb,D)", "http://arxiv.org/abs/1609.07479v2", "9 pages, 3 figures, 4 tables. Code and dataset available"], ["v3", "Wed, 13 Sep 2017 19:30:06 GMT  (172kb,D)", "http://arxiv.org/abs/1609.07479v3", "Proceedings of EMNLP 2017. Code and dataset available"]], "COMMENTS": "9 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenyuan zeng", "yankai lin", "zhiyuan liu", "maosong sun"], "accepted": true, "id": "1609.07479"}, "pdf": {"name": "1609.07479.pdf", "metadata": {"source": "CRF", "title": "Incorporating Relation Paths in Neural Relation Extraction", "authors": ["Wenyuan Zeng", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "emails": ["(liuzy@tsinghua.edu.cn)."], "sections": [{"heading": "1 Introduction", "text": "Knowledge Bases (KBs) provide structured information about real world facts and have been used as crucial resources for various natural language processing (NLP) applications including Web search and question answering. Typical KBs such as Freebase (Bollacker et al., 2008), DBpedia (Auer et al., 2007) and YAGO (Suchanek et al., 2007) usually describe knowledge as multi-relational data and represent them as triple facts. As the real-world facts are increasing infinitely, existing KBs are far from complete. Recently, petabytes of natural-language\n\u2217Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn).\ntext containing rich knowledge are readily available, which are important resources for automatically finding novel relational facts. Hence, relation extraction (RE), defined as the task of extracting structured information from plain text, has attracted much interest.\nHowever, supervised RE methods typically suffer from lacking large-scale annotated training data. Manual annotation is time consuming and labor intensive. To address this issue, one of the most promising solutions is distant supervision. (Mintz et al., 2009) automatically generates training data by aligning a KB with plain text. It assumes that, if two target entities have a relation in KB, each sentence that contains these two entities will express that relation, and can be regarded as a positive training instance.\nAlthough existing RE systems have achieved promising results with the help of distant supervision, they still suffer from one major drawback: the models can solely learn from those direct sentences containing both target entities. In fact, there are many indirect sentences containing one of the target entities, which also provide rich information for relation extraction. The basic idea is to build inference chains between two target entities using these sentences. For example, if we know that \u201ch is the father of e\u201d and \u201ce is the father of t\u201d, it will be easy to infer that h is the grandfather of t, with the help of the intermediate entity e.\nTo better utilize text information for relation extraction, we aim to leverage both direct and indirect sentences. We design a path-based method to combine the relational semantics from both direct sen-\nar X\niv :1\n60 9.\n07 47\n9v 1\n[ cs\n.C L\n] 2\n3 Se\np 20\ntences and inference chain. We also take advantages of neural models. Recently, neural models have been successfully applied to classify relations based on plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015). (Zeng et al., 2015) further proposes neural models with distant supervision for relation extraction, and achieved the state-of-the-art performance. Hence, as illustrated in Fig. 1, we propose a path-based neural relation extraction model: (1) We first employ convolutional neural network (CNN) to encode the relational semantics of each direct sentence. (2) We further build a path encoder to extract the relational semantics from multiple relation paths. (3) We finally integrate the information from both direct sentences and relation paths, and then predict the confidence of each relation. To the best of our knowledge, this is the first effort to utilize relation paths for relation extraction from texts.\nThe contributions of this paper can be summarized as follows:\n\u2022 As compared to existing neural relation extraction models, our model is able to utilize the sentences of those containing both two target entities and only one target entity.\n\u2022 Using those sentences containing one of the target entities, our model is more robust and performs well even when the number of noisy instances increases."}, {"heading": "2 Related Work", "text": "Distant supervision for RE is proposed in (Craven et al., 1999). They apply this method in the domain of biology. Afterwards, (Mintz et al., 2009) aligns plain text with Freebase. However, most of these methods heuristically transform distant supervision to traditional supervised learning, simply regarding it as a single-instance single-label problem. To alleviate this issue, (Riedel et al., 2010) regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label, namely a multi-instance single-label problem. Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multi-label learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools, and hence the errors generated by NLP tools will propagate and limit the performances of these methods.\nRecently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural net-\nwork in relation extraction, and (Zeng et al., 2014; dos Santos et al., 2015) adopt CNNs for relation extraction. Although these methods achieve great successes in relation classification, they still rely on manually annotated training data since they regard relation extraction as a supervised learning problem. Hence, (Zeng et al., 2015) combines at-least-one multi-instance learning with neural network model to extract relations on distant supervision data. Further, (Lin et al., 2016) adopts a sentence-level selective attention mechanism to model distant supervised relation extraction. Although above RE models have achieved promising results, they merely learn from those sentences which contain both two target entities. The important information of the relation paths hidden in text is ignored.\nRelation paths have been taken into consideration for relation inference on large-scale KBs. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015) further learn recurrent neural networks to represent unseen relation paths based on all involved relations."}, {"heading": "3 Methodology", "text": "Given a pair of target entities, a set of corresponding direct sentences S = {s1, s2, \u00b7 \u00b7 \u00b7 , sn} which contains this entity pair, and a set of relation paths P = {p1, p2, \u00b7 \u00b7 \u00b7 , pm}, our model measures the confidence of each relation for this entity pair. In this section, we will introduce our model in three parts: (1) Text Encoder. Given the sentence with two corresponding target entities, we use a CNN to embed the sentence into a semantic space, and measure the probability of relations given this sentence. (2) Relation Path Encoder. Given a relation path between the target entities, we measure the probability of relation r, conditioned on the relation path. (3) Joint Model. We integrate the information from direct sentences and relation paths, then predict the confidence of each relation."}, {"heading": "3.1 Text Encoder", "text": "As shown in Fig. 2, we use a CNN to extract information from text. Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism."}, {"heading": "3.1.1 Input Vector", "text": "First, we transform the words {w1, w2, \u00b7 \u00b7 \u00b7 , wl} in sentence s into vectors. For each word wi, we use word embedding to encode its syntactic and semantic meanings, and use position embedding to encode its position information. We then concatenate both word embedding and position embedding to form the input vector of wi for CNN.\nWord Embeddings. Suppose V is the collection of all raw words, and we represent each word by a dw dimensional vector. Thus, the words are encoded by a word embedding matrix WE \u2208 Rdw\u00d7|V |, where each column of WE is the representation of one word .\nPosition Embeddings. The order of words in a sentence is important for understanding that sentence. For instance, those words closer to the target entities are usually more informative for inferring that relation. We specify the position information via position embeddings, which embed the relative positions from current word to head and tail entities in two dp dimensional vectors respectively.\nIn the example shown in Fig. 2, the dimension\ndw of word embeddings is 4, and dimension dp of position embeddings is 1. After transforming all words into vectors, we obtain a sequence of vectors w = {w1,w2, \u00b7 \u00b7 \u00b7 ,wl}, where l is the length of that sentence and wi \u2208 Rd (d = dw + 2\u00d7 dp)."}, {"heading": "3.1.2 Convolution, Max-pooling Layers", "text": "When processing a sentence, it is a great challenge that important information could probably appear in all parts of that sentence. In addition, the length l of a sentence could also vary a lot. Therefore, we apply CNN to encode all local features regardless sentence length. We first apply a convolution layer to extract all possible local features, and then select the most important one via max-pooling layer.\nTo extract local features, the convolution layer first concatenates a sequence of word embeddings within a sliding window to be vector qi of dimension k \u00d7 d:\nqi = wi\u2212k+1:i(1 \u2264 i \u2264 l + k \u2212 1), (1)\nwhere k is the size of the window, and we also set all out-of-index words to be zero vectors. It then multiplies qi by a convolution matrix W \u2208 Rdc\u00d7(k\u00d7d), where dc is the dimension of sentence embeddings. Hence, the output of convolution layer could be expressed as h = {h1,h2, \u00b7 \u00b7 \u00b7 ,hl+k\u22121}:\nhi = Wqi + b, (2)\nwhere b is a bias vector. Finally, the max-pooling layer takes a max operation, followed by a hyperbolic tangent activation, over the sequence of hi to select the most important information, namely,\n[s]j = tanh(max i [hi]j). (3)\nIn fact, other neural RE models such as PCNN (Zeng et al., 2015), RNN (Socher et al., 2012), LSTM (Miwa and Bansal, 2016) can also be easily adopted as text encoder. Due to the space limit, in this paper we only explore the effectiveness of CNN."}, {"heading": "3.1.3 Multi-Instance Learning", "text": "Next, we apply a softmax classifier upon the sentence representation s to predict the corresponding\nrelation. We define the condition probability of relation r as follows,\np(r|\u03b8, s) = exp(er)\u2211nr i=1 exp(ei) , (4)\nwhere ei measures how well this sentence matches relation ri and nr is the number of relations. More specifically, e could be calculated from:\ne = Us+ v, (5)\nwhere U \u2208 Rnr\u00d7dc is the coefficient matrix of relations and v \u2208 Rnr is a bias vector.\nWe use multi-instance learning to alleviate the wrong-labeling issue in distant supervision, choosing one sentence in the set of all direct sentences S = {s1, s2, \u00b7 \u00b7 \u00b7 , sm} which corresponds to the entity pair (h, t). Similar to (Zeng et al., 2015), we define the score function of this entity pair and its corresponding relation r as in max-one setting:\nE(h, r, t|S) = max i p(r|\u03b8, si). (6)\nwhere E reflects the direct information we derive from sentences. Or in random setting:\nE(h, r, t|S) = p(r|\u03b8, si), (7)\nwhere si is randomly selected from S."}, {"heading": "3.2 Relation Path Encoder", "text": "We use relation path encoder to embed the inference information of relation paths. Relation path encoder measures the probability of relation r given a relation path in the text. This will utilize the inference chain structure to help making predictions. More specifically, we define a path p1 between (h, t) as {(h, e), (e, t)}, and the corresponding relations are rA, rB . Each of (h, e) and (e, t) corresponds to at least one sentence in the text. Our model calculates the probability of relation r conditioned on p1 as follows,\np(r|rA, rB) = exp(or)\u2211nr i=1 exp(oi) , (8)\nwhere oi measures how well relation r matches with the relation path (rA, rB). Inspired by work on relation path representation learning (Lin et al., 2015), our model first transforms relation r to its distributed\nrepresentation, i.e. vector r \u2208 RdR , and builds the path embeddings by composition of relation embeddings. Then, the similarity oi is calculated as follows:\noi = \u2212\u2016ri \u2212 (rA + rB)\u2016L1 . (9)\nTherefore, if ri gets more similar to (rA+rB), the conditioned predicting probability of ri will become larger. Finally, for this relation path pi : h\nrA\u2212\u2192 e rB\u2212\u2192 t, we define an relation-path score function,\nG(h, r, t|pi) = E(h, rA, e)E(e, rB, t)p(r|rA, rB), (10) whereE(h, rA, e) andE(e, rB, t) measure the probability of relational facts (h, rA, e) and (e, rB, t) from text, and p(r|rA, rB) measures the probability of relation r given relation path (rA, rB).\nHence, the inferring correlation between relation r and several sentence paths P is defined as,\nG(h, r, t|P ) = max i G(h, r, t|pi), (11)\nwhere we use max operation to filter out those noisy paths."}, {"heading": "3.3 Joint Model", "text": "Given any entity pair (h, t), those sentences S directly mentioning them and relation paths P between them, we define the global score function with respect to a possible relation r as,\nL(h, r, t) = E(h, r, t|S) + \u03b1G(h, r, t|P ), (12)\nwhere E(h, r, t|S) models the correlation between r and (h, t) calculated from direct sentences, G(h, r, t|P ) models the inferring correlation between relation r and several sentence paths P . \u03b1 equals to (1 \u2212 E(h, r, t|S)) times a constant \u03b2. This term serves to depict the relative weight between direct sentences and relation paths, since we don\u2019t need to pay much attention on extra information when CNN has already given a reliable result, namely E(h, r, t|S) is large.\nOne of the advantages of this joint model is to alleviate the issue of error propagation. The uncertainty of information from text encoder and path encoder is characterized by its confidence, and could be integrated and corrected in this joint model step.\nFurthermore, since we treat relation paths in a probabilistic way, our model could fully utilize all relation paths, i.e. those always hold and those likely to hold."}, {"heading": "3.4 Optimization and Implementation Details", "text": "In this section, we introduce optimization details of our model. The overall objective function is defined as:\nJ(\u03b8) = \u2211 (h,r,t) log(L(h, r, t)), (13)\nwhere the summing runs over the log loss of all entity pairs in text and \u03b8 represents the model parameters. To solve this optimization problem, we use stochastic gradient descent (SGD) to maximize our objective function. We first initialize WE with the results from Skip-gram model, and initialize other parameters randomly. Then, we iterate SGD by selecting a mini-batch from the training set, and stop learning when the model\u2019s performance on validation set converges. In the testing mode, we include those relation paths in training set to enrich the path information.\nIn the implementation of CNN, we adopt dropout (Srivastava et al., 2014) upon the output layer. For each element of the CNN output, we set it to zero with a pre-defined probability p. This dropout operation is proved to be effective in practice, especially to alleviate overfitting."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset and Evaluation Metrics", "text": "We build a novel dataset which contains more updated facts and richer structures of relations, e.g. number of relations/relation paths, as compared to existing similar datasets. This makes it more similar to real-world cases, and thus be a more appropriate choice for evaluating RE systems. We build the dataset by aligning Wikidata1 relations with the\n1https://www.wikidata.org/\nNew York Times Corpus (NYT). Wikidata is a large, growing knowledge base, which contains more than 80 million triple facts and 20 million entities. We first sample a subset S from Wikidata by maintaining a subset of entities which also present in Freebase, and restricting relations according to their frequency. This results in 4,574,665 triplets with 1,045,385 entities and 99 relations.\nWe randomly split the subset of knowledge base into three sets including training, validation and testing sets. And then we connect them with NYT corpus by aligning the raw names of entities with those appear in text and following the assumption of distant supervision. To simulate noises in realworld datasets, we also add sentences corresponding to \u201cNo Relation\u201d entity pairs to our dataset. To get those instances with \u201cNo Relation\u201d, we first create a fake knowledge base S\u2212 by randomly replace the head entity or tail entity of triples, i.e., S\u2212 = {(h\u2032, r, t)} \u222a {(h, r, t\u2032)} and then aligning them with NYT corpus. The statistics of the datasets are listed in Table 1.\nFollowing previous work (Mintz et al., 2009), we evaluate our model using held-out evaluation which approximately measures the precision without timeconsuming manual evaluation. We extract relational facts from the test sentences using our model, and compare them with those in Wikidata. We report Precision/Recall curves, Precision@N (P@N) and F1 scores in our experiments."}, {"heading": "4.2 Experimental Settings", "text": "In this paper, we use the word2vec tool 2 to pretrain the word embeddings on NYT corpus. We keep the words which appear more than 100 times in the corpus as vocabulary.\nWe tune our model on the validation set, using grid search to determine the optimal parameters, which are shown in boldface. We select learning rate for SGD \u03bb \u2208 {0.1, 0.01, 0.001}, the sentence embedding size dc \u2208 {50, 60, \u00b7 \u00b7 \u00b7 , 230, \u00b7 \u00b7 \u00b7 , 300}, the window size k \u2208 {1, 2, 3, 4, 5}, and the mini-batch size B \u2208 {40, 160, 640}. Besides, we select the relation embeddings size dR \u2208 {5, 10, \u00b7 \u00b7 \u00b7 , 40, \u00b7 \u00b7 \u00b7 , 60}, and the weight for information from relation paths \u03b2 \u2208 {0.5, 1, \u00b7 \u00b7 \u00b7 , 5}. For\n2https://code.google.com/p/word2vec/\nother parameters which have little effect on the system performance, we follow the settings used in (Zeng et al., 2015): word embedding size dw is 50, position embedding size dp is 5 and dropout rate p is 0.5. For training, we set the iteration number over all training data as 25."}, {"heading": "4.3 Effect of Relation Paths", "text": "To demonstrate the effect of our approach, we empirically compare it with other state-of-the-art methods via held-out evaluation including: (1) CNN+rand represents the CNN model reported in (Zeng et al., 2014). (2) CNN+max represents the CNN model with multi-instances learning used in (Zeng et al., 2015). (3) Path+rand/max is our model with those two multi-instance settings. We implement (1), (2) by ourselves which achieve comparable results as reported in those papers.\nFig. 3 shows the precision/recall curves of all methods. From the figure, we can observe that: (1) Our methods outperform their counterpart methods, achieving higher precision over almost entire range of recall. They also enhance recall by 20% without decrease of precision. These results prove the effectiveness of our approach. (2) As the recall increases, our models exhibit larger improvements compared with CNN in terms of percentage. This is due to the fact that sometimes CNNs cannot extract reliable information from direct sentence, while our methods could alleviate this issue by considering more information from inference chains, and thus still maintain\nhigh precision. (3) Both CNN+max and Path+rand are variations of CNN+rand, aiming to alleviate the problem of noisy data. We see that Path+rand outperforms CNN+max over all range, which indicates that considering path information is a better way to solve this issue. Meanwhile, combining paths information and max operation gives the best performance, Path+max. (4) Path+rand shows a larger improvement over CNN+rand, compared with those of Path+max and CNN+max. This furthermore proves the effectiveness of considering relation path information: CNN+rand has much more severe problem suffering from noises, so using our method to incorporate paths information to alleviate this issue could perform better."}, {"heading": "4.4 Model Robustness under Different Percentages of Noises", "text": "In the task of relation extraction, there are lots of noises in text which may hurt the model\u2019s performance, such as those \u201cNo Relation\u201d entity pairs. Therefore, it is important to verify the robustness of our model in the presence of mass noises. Here, we evaluate those models in three settings, with the same relational facts and different percentages of \u201cNo Relation\u201d sentences in testing set. In each experiments, we extract the first 20000 predicting relational facts according to the model\u2019s predicting scores, and report the precision @top 10%, @top 20%, @top 50% and F1 score in Table 2.\nFrom the table, we can see that: (1) In terms of all evaluations, our models achieve the best performance as compared with other methods in all test settings. It demonstrates the effectiveness of our approach. (2) Even though the scores of all models drop as the increasing of noises, we find that Path+rand/max\u2019s scores decrease much less than their counterparts. This result proves the effectiveness of taking inference chains into consideration. Since we utilize more information to make predic-\ntion, our model is more robust to the presence of mass noises."}, {"heading": "4.5 Effectiveness of Extracted Features in Zero-Shot Situation", "text": "It has been proved that CNN could automatically extract useful features, encoding syntactic and semantic meaning of sentences. These features are sometimes fed to subsequent models to solve other tasks. In this experiment, we demonstrate the effectiveness of the extracted features from our model. Since CNN-based models have already succeeded in extracting relations from single sentences, we set our experiment in a new scenario: predicting the relation between entities which have not appeared in the same sentence.\nA natural approach is to build an relation path between this zero-shot pair. We assume that we can make a prediction about (h, t), once we know the information of (h, e) and (e, t). Therefore, we build the training set by extracting all such relation paths and their sentences from training text, and similar for testing set. To test the effectiveness of features, we encode sentences by CNN+rand/max, Path+rand/max respectively, and then feed the concatenation of the sentence vector to a logistic classifier.\nFrom Table 3, we could observe that: (1) The result using CNN+rand features is comparable to the result using CNN+max features. It shows that using max operation to train the features does not greatly improve the features behavior in this task,\neven though it performs well in previous tasks. The reason is that, both CNN+rand and CNN+max only encode the information from a single sentence, and they are unable to capture the correlations between relations. (2) Feature from Path+rand/max shows its effectiveness over those from other methods. It indicates that our method is able to model the correlations between relations, while also keep the syntactic and semantic meaning of a sentence. Therefore, the features extracted from Path+rand/max are useful for a wider range of applications, especially in those tasks which need the information from relations."}, {"heading": "4.6 Case study", "text": "Table 4 shows some representative inference chains from the testing dataset. These examples can not be predicted correctly by the original CNN model, but are later corrected using our model. We show the test instances and their correct relations, as well as the inference chains the model use. In the first example, test sentence does not directly express the relation spouse, the proof of this relation appears in a further context in NYT. However, using path#1 and path#2, we could easily infer that Rebecca and Issac are spouse. In the second example, the test sentence doesn\u2019t show the relation either. But with the help of intermediate entity, Dijibouti, our model predicts that Somalia shares the border with Ethiopia. Note that this inference chain doesn\u2019t always hold, but our model could capture this uncertainty well via a softmax operation. In general, our model can utilize common sense from inference chains. It helps make correct prediction even if the inference is not explicit."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose a neural relation extraction model which encodes the information of rela-\ntion paths. As compared to existing neural relation extraction models, our model is able to utilize the sentences which contain both two target entities and only one target entity and is more robust for noisy data. Experimental results on real-world datasets show that our model achieves significant and consistent improvements on relation extraction as compared with baselines. The codes and datasets will be released upon accepting.\nIn the future, we will explore the following directions: (1) Our relation extraction model only considers the relation path on plain texts. However, the relation paths in KBs is also important supplements for relation extraction. In future, we will explore the combination of relation paths from both plain texts and KBs for relation extraction. (2) This paper only considers the correlations between inference chains and direct sentences. There may exist more complicated correlations between relation paths. We may take advantages of probabilistic graphical model to encode such patterns for relation extraction."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Learning deep architectures for ai. Foundations and trends R", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of KDD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Craven et al.1999] Mark Craven", "Johan Kumlien"], "venue": "In Proceedings of ISMB,", "citeRegEx": "Craven and Kumlien,? \\Q1999\\E", "shortCiteRegEx": "Craven and Kumlien", "year": 1999}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["dos Santos", "Ma\u0131ra Gatti"], "venue": "In Proceedings of COLING", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of ACL,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Improving learning and inference in a large knowledgebase using latent syntactic cues", "author": ["Gardner et al.2013] Matt Gardner", "Partha Pratim Talukdar", "Bryan Kisiel", "Tom M Mitchell"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Gardner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2013}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of ACL-HLT,", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["Lao", "Cohen2010] Ni Lao", "William W Cohen"], "venue": "Machine learning,", "citeRegEx": "Lao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2010}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W Cohen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Reading the web with learned syntactic-semantic inference rules", "author": ["Lao et al.2012] Ni Lao", "Amarnag Subramanya", "Fernando Pereira", "William W Cohen"], "venue": "In Proceedings of EMNLP-CoNLL,", "citeRegEx": "Lao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2012}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Lin et al.2015] Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "venue": "Proceedings of EMNLP", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Lin et al.2016] Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Luan Huanbo", "Maosong Sun"], "venue": "Proceedings of ACL", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "End-to-end relation extraction using lstms on sequences and tree structures. arXiv preprint arXiv:1601.00770", "author": ["Miwa", "Bansal2016] Makoto Miwa", "Mohit Bansal"], "venue": null, "citeRegEx": "Miwa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miwa et al\\.", "year": 2016}, {"title": "Compositional vector space models for knowledge base inference", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "In 2015 AAAI Spring Symposium Series", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Proceedings of ECML-PKDD,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "Proceedings of EMNLP", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of EMNLP-CoNLL,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of ACL. Citeseer", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of WWW,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Typical KBs such as Freebase (Bollacker et al., 2008), DBpedia (Auer et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 0, "context": ", 2008), DBpedia (Auer et al., 2007) and YAGO (Suchanek et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 21, "context": ", 2007) and YAGO (Suchanek et al., 2007) usually describe knowledge as multi-relational data and represent them as triple facts.", "startOffset": 17, "endOffset": 40}, {"referenceID": 13, "context": "(Mintz et al., 2009) automatically generates training data by aligning a KB with plain text.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "Recently, neural models have been successfully applied to classify relations based on plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 97, "endOffset": 162}, {"referenceID": 24, "context": "Recently, neural models have been successfully applied to classify relations based on plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 97, "endOffset": 162}, {"referenceID": 25, "context": "(Zeng et al., 2015) further proposes neural models with distant supervision for relation extraction, and achieved the state-of-the-art performance.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "Afterwards, (Mintz et al., 2009) aligns plain text with Freebase.", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "To alleviate this issue, (Riedel et al., 2010) regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label, namely a multi-instance single-label problem.", "startOffset": 25, "endOffset": 46}, {"referenceID": 7, "context": "Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multi-label learning in relation extraction.", "startOffset": 9, "endOffset": 55}, {"referenceID": 22, "context": "Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multi-label learning in relation extraction.", "startOffset": 9, "endOffset": 55}, {"referenceID": 1, "context": "Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on.", "startOffset": 24, "endOffset": 38}, {"referenceID": 19, "context": "Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 17, "context": ", 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 23, "context": ", 2015) and machine translation (Sutskever et al., 2014).", "startOffset": 32, "endOffset": 56}, {"referenceID": 18, "context": "(Socher et al., 2012) uses a recursive neural net-", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "work in relation extraction, and (Zeng et al., 2014; dos Santos et al., 2015) adopt CNNs for relation extraction.", "startOffset": 33, "endOffset": 77}, {"referenceID": 25, "context": "Hence, (Zeng et al., 2015) combines at-least-one multi-instance learning with neural network model to extract relations on distant supervision data.", "startOffset": 7, "endOffset": 26}, {"referenceID": 12, "context": "Further, (Lin et al., 2016) adopts a sentence-level selective attention mechanism to model distant supervised relation extraction.", "startOffset": 9, "endOffset": 27}, {"referenceID": 10, "context": "Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012).", "startOffset": 135, "endOffset": 153}, {"referenceID": 9, "context": "PRA has also been used for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013).", "startOffset": 73, "endOffset": 113}, {"referenceID": 6, "context": "PRA has also been used for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013).", "startOffset": 73, "endOffset": 113}, {"referenceID": 15, "context": "(Neelakantan et al., 2015; Lin et al., 2015) further learn recurrent neural networks to represent unseen relation paths based on all involved relations.", "startOffset": 0, "endOffset": 44}, {"referenceID": 11, "context": "(Neelakantan et al., 2015; Lin et al., 2015) further learn recurrent neural networks to represent unseen relation paths based on all involved relations.", "startOffset": 0, "endOffset": 44}, {"referenceID": 25, "context": "In fact, other neural RE models such as PCNN (Zeng et al., 2015), RNN (Socher et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 18, "context": ", 2015), RNN (Socher et al., 2012), LSTM (Miwa and Bansal, 2016) can also be easily adopted as text encoder.", "startOffset": 13, "endOffset": 34}, {"referenceID": 25, "context": "Similar to (Zeng et al., 2015), we define the score function of this entity pair and its corresponding relation r as in max-one setting:", "startOffset": 11, "endOffset": 30}, {"referenceID": 11, "context": "Inspired by work on relation path representation learning (Lin et al., 2015), our model first transforms relation r to its distributed", "startOffset": 58, "endOffset": 76}, {"referenceID": 20, "context": "In the implementation of CNN, we adopt dropout (Srivastava et al., 2014) upon the output layer.", "startOffset": 47, "endOffset": 72}, {"referenceID": 13, "context": "Following previous work (Mintz et al., 2009), we evaluate our model using held-out evaluation which approximately measures the precision without timeconsuming manual evaluation.", "startOffset": 24, "endOffset": 44}, {"referenceID": 25, "context": "com/p/word2vec/ other parameters which have little effect on the system performance, we follow the settings used in (Zeng et al., 2015): word embedding size dw is 50, position embedding size dp is 5 and dropout rate p is 0.", "startOffset": 116, "endOffset": 135}, {"referenceID": 24, "context": "To demonstrate the effect of our approach, we empirically compare it with other state-of-the-art methods via held-out evaluation including: (1) CNN+rand represents the CNN model reported in (Zeng et al., 2014).", "startOffset": 190, "endOffset": 209}, {"referenceID": 25, "context": "(2) CNN+max represents the CNN model with multi-instances learning used in (Zeng et al., 2015).", "startOffset": 75, "endOffset": 94}], "year": 2016, "abstractText": "Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which provide rich and useful information for relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on realworld datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with baselines.", "creator": "LaTeX with hyperref package"}}}