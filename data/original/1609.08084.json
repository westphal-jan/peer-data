{"id": "1609.08084", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Toward Socially-Infused Information Extraction: Embedding Authors, Mentions, and Entities", "abstract": "Entity linking is the task of identifying mentions of entities in text, and linking them to entries in a knowledge base. This task is especially difficult in microblogs, as there is little additional text to provide disambiguating context; rather, authors rely on an implicit common ground of shared knowledge with their readers. In this paper, we attempt to capture some of this implicit context by exploiting the social network structure in microblogs. We build on the theory of homophily, which implies that socially linked individuals share interests, and are therefore likely to mention the same sorts of entities. We implement this idea by encoding authors, mentions, and entities in a continuous vector space, which is constructed so that socially-connected authors have similar vector representations. These vectors are incorporated into a neural structured prediction model, which captures structural constraints that are inherent in the entity linking task. Together, these design decisions yield F1 improvements of 1%-5% on benchmark datasets, as compared to the previous state-of-the-art.", "histories": [["v1", "Mon, 26 Sep 2016 17:19:07 GMT  (10591kb,D)", "http://arxiv.org/abs/1609.08084v1", "Accepted to EMNLP 2016"]], "COMMENTS": "Accepted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yi yang", "ming-wei chang", "jacob eisenstein"], "accepted": true, "id": "1609.08084"}, "pdf": {"name": "1609.08084.pdf", "metadata": {"source": "CRF", "title": "Toward Socially-Infused Information Extraction: Embedding Authors, Mentions, and Entities", "authors": ["Yi Yang", "Ming-Wei Chang"], "emails": ["yiyang@gatech.edu", "minchang@microsoft.com", "jacobe@gatech.edu"], "sections": [{"heading": null, "text": "Entity linking is the task of identifying mentions of entities in text, and linking them to entries in a knowledge base. This task is especially difficult in microblogs, as there is little additional text to provide disambiguating context; rather, authors rely on an implicit common ground of shared knowledge with their readers. In this paper, we attempt to capture some of this implicit context by exploiting the social network structure in microblogs. We build on the theory of homophily, which implies that socially linked individuals share interests, and are therefore likely to mention the same sorts of entities. We implement this idea by encoding authors, mentions, and entities in a continuous vector space, which is constructed so that socially-connected authors have similar vector representations. These vectors are incorporated into a neural structured prediction model, which captures structural constraints that are inherent in the entity linking task. Together, these design decisions yield F1 improvements of 1%-5% on benchmark datasets, as compared to the previous state-of-the-art."}, {"heading": "1 Introduction", "text": "Entity linking on short texts (e.g., Twitter messages) is of increasing interest, as it is an essential step for many downstream applications, such as market research (Asur and Huberman, 2010), topic detection and tracking (Mathioudakis and Koudas, 2010), and question answering (Yih et al., 2015). Tweet entity linking is a particularly difficult problem, because\nthe short context around an entity mention is often insufficient for entity disambiguation. For example, as shown in Figure 1, the entity mention \u2018Giants\u2019 in tweet t1 can refer to the NFL football team New York Giants or the MLB baseball team San Francisco Giants. In this example, it is impossible to disambiguate between these entities solely based on the individual text message.\nWe propose to overcome the difficulty and improve the entity disambiguation capability of the entity linking system by employing social network structures. The sociological theory of homophily asserts that socially connected individuals are more likely to have similar behaviors or share similar interests (McPherson et al., 2001). This property has been used to improve many natural language processing tasks such as sentiment analysis (Tan et al., 2011; Yang and Eisenstein, 2015), topic classification (Hovy, 2015) and user attribute inference (Li et al., 2015). We assume Twitter users will have similar interests in real world entities to their near neighbors \u2014 an assumption of entity homophily \u2014 which ar X\niv :1\n60 9.\n08 08\n4v 1\n[ cs\n.C L\n] 2\n6 Se\np 20\nis demonstrated in Figure 1. The social relation between users u1 and u2 may lead to more coherent topics in tweets t1 and t2. Therefore, by successfully linking the less ambiguous mention \u2018Red Sox\u2019 in tweet t2 to the Boston Red Sox baseball team, the tweet entity linking system will be more confident on linking \u2018Giants\u2019 to the San Francisco Giants football team in tweet t1.\nTo exploit social information, we adopt the recent advance on embedding information networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose NTEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. NTEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distributed representations.\nOur contributions are summarized as follows:\n\u2022 We present a novel model for entity linking that exploits distributed representations of users, mentions, and entities.\n\u2022 We combine this distributed model with a feedforward neural network that learns non-linear combinations of surface features.\n\u2022 We perform message-level inference using a dynamic program to avoid overlapping mentions. The architecture is trained with lossaugmented decoding, a large margin learning technique for structured prediction.\n\u2022 The complete system, NTEL, outperforms the previous state-of-the-art (Yang and Chang, 2015) by 3% average F1 on two benchmark datasets."}, {"heading": "2 Data", "text": "Two publicly available datasets for tweet entity linking are adopted in the work. NEEL is originally collected and annotated for the Named Entity Extraction & Linking Challenge (Cano et al., 2014), and TACL is first used and released by Fang and Chang (2014). The datasets are then cleaned and unified by Yang and Chang (2015). The statistics of the datasets are presented in Table 1."}, {"heading": "3 Testing Entity Homophily", "text": "The hypothesis of entity homophily, as presented in the introduction, is that socially connected individuals are more likely to mention similar entities than disconnected individuals. We now test the hypothesis on real data before we start building our entity linking systems.\nTwitter social networks We test the assumption on the users in the NEEL-train dataset. We construct three author social networks based on the follower, mention and retweet relations between the 1,317 authors in the NEEL-train dataset, which we refer as FOLLOWER, MENTION and RETWEET. Specifically, we use the Twitter API to crawl the friends of the NEEL users (individuals that they follow) and the mention/retweet links are induced from their most recent 3,200 tweets.1 We exploit bi-directed links to create the undirected networks, as bi-directed links result in stronger social network ties than directed links (Kwak et al., 2010; Wu et al., 2011). The numbers of social relations for the networks are 1,604, 379 and 342 respectively.\n1We are able to obtain at most 3,200 tweets for each Twitter user, due to the Twitter API limits.\nMetrics We propose to use the entity-driven similarity between authors to test the hypothesis of entity homophily. For a user ui, we employ a Twitter NER system (Ritter et al., 2011) to detect entity mentions in the timeline, which we use to construct a user entity vector u(ent)i , so that u (ent) i,j = 1 iff user i has mentioned entity j.2 The entity-driven similarity between two users ui and uj is defined as the cosine similarity score between the vectors u (ent) i and u (ent) j . We evaluate the three networks by calculating the average entity-driven similarity of the connected user pairs and that of the disconnected user pairs, which we name as sim(i \u2194 j) and sim(i\u2194/ j).\nResults The entity-driven similarity results of these networks are presented in Table 2. As shown, sim(i\u2194 j) is substantially higher than sim(i\u2194/ j) on all three social networks, indicating that socially connected individuals clearly tend to mention more similar entities than disconnected individuals. Note that sim(i\u2194/ j) is approximately equal to the same base rate defined by the average entity-driven similarity of all pairs of users, because the vast majority of user pairs are disconnected, no matter how to define the network. Among the three networks, RETWEET offers slightly higher sim(i \u2194 j) than FOLLOWER and MENTION. The results verify our hypothesis of entity homophily, which forms the basis for this research. Note that all social relation data was acquired in March 2016; by this time, the authorship information of 22.1% of the tweets in the NEEL-train dataset was no longer available, because the tweets or user accounts had been deleted."}, {"heading": "4 Method", "text": "In this section, we present, NTEL, a novel neural based tweet entity linking framework that is able to\n2We assume each name corresponds to a single entity for this metric, so this metric only approximates entity homophily.\nleverage social information. We first formally define the task of tweet entity linking. Assume we are given an entity database (e.g., Wikipedia or Freebase), and a lexicon that maps a surface form into a set of entity candidates. For each input tweet, we consider any n-grams of the tweet that match the lexicon as mention candidates.3 The entity linking system maps every mention candidate (e.g., \u2018Red Sox\u2019) in the message to an entity (e.g., Boston Red Sox) or to Nil (i.e., not an entity). There are two main challenges in the problem. First, a mention candidate can often potentially link to multiple entities according to the lexicon. Second, as shown in Figure 2, many mention candidates overlap with each other. Therefore, the entity linking system is required to disambiguate entities and produce nonoverlapping entity assignments with respect to the mention candidates in the tweet.\nWe formalize this task as a structured learning problem. Let x be the tweet, u be the author, and y = {yt}Tt=1 be the entity assignments of the T mention candidates in the tweet. The overall scoring function s(x,y, u) can be decomposed as follows,\ns(x,y, u) = T\u2211 t=1 g(x, yt, u, t), (1)\nwhere g(x, yt, u, t) is the scoring function for the tth mention candidate choosing entity yt. Note that the system needs to produce non-overlapping entity assignments, which will be resolved in the inference algorithm.\nThe overview of NTEL is illustrated in Figure 3. We further break down g(x, yt, u, t) into two scoring\n3We adopted the same entity database and lexicon as those used by Yang and Chang (2015).\nfunctions:\ng(x, yt, u, t; \u03981,\u03982) =\ng1(x, yt, t; \u03981) + g2(x, yt, u, t; \u03982), (2)\nwhere g1 is the scoring function for our basic surface features, and g2 is the scoring function for modeling user, mention, entity representations and their compositions. \u03981 and \u03982 are model parameters that will be detailed below. We choose to use a multilayer perceptron (MLP) to model g1(x, yt, t; \u03981), and we employ simple yet efficient bilinear functions to learn the compositions of user, mention, and entity representations g2(x, yt, u, t; \u03982). Finally, we present a training algorithm based on lossaugmented decoding and a non-overlapping inference algorithm."}, {"heading": "4.1 Modeling Surface Features", "text": "We include the 37 features used by Yang and Chang (2015) as our surface feature set. These features are extracted from various sources, including a named entity recognizer, an entity type recognizer, and some statistics of the Wikipedia pages.\nWe exploit a multilayer perceptron (MLP) to transform the surface features to a real-valued score. The output of the MLP is formalized as follows,\ng1(x, yt, t; \u03981) =\u03b2 >h + b\nh =tanh(W\u03c6(x, yt, t) + b), (3)\nwhere \u03c6(x, yt, t) is the feature function, W is an M \u00d7 D matrix, the weights b are bias terms, and h is the output of the hidden layer of the MLP. \u03b2 is an M dimensional vector of weights for the output score, and b is the bias term. The parameters of\nthe MLP are \u03981 = {W,b,\u03b2, b}. Yang and Chang (2015) argue that non-linearity is the key for obtaining good results on the task, as linear models are not expressive enough to capture the high-order relationships between the dense features. They propose a tree-based non-linear model for the task. The MLP forms simple non-linear mappings between the input features and the output score, whose parameters will be jointly learnt with other components in NTEL."}, {"heading": "4.2 Modeling User, Mention, and Entity", "text": "To leverage the social network structure, we first train low-dimensional embeddings for the authors using the social relations. The mention and entity representations are given by word embeddings learnt with a large Twitter corpus and pre-trained Freebase entity embeddings respectively. We will denote the user, word, entity embedding matrices as:\nE(u) = {v(u)u } E(w) = {v(w)w } E(e) = {v(e)e },\nwhere E(u),E(w),E(e) are V (u) \u00d7 D(u), V (w) \u00d7 D(w), V (e) \u00d7 D(e) matrices, and v(u)u , v(w)w , v(e)e are D(u), D(w), D(e) dimensional embedding vectors respectively. V (u), V (w), V (e) are the vocabulary sizes for users, words, and entities. Finally, we present a composition model for learning semantic interactions between user, mention, and entity.\nUser embeddings We obtain low-dimensional Twitter author embeddings E(u) using LINE \u2014 the recently proposed model for embedding information networks (Tang et al., 2015). Specifically, we train LINE with the second-order proximity, which assumes that Twitter users sharing many neighbors are\nclose to each other in the embedding space. According to the original paper, the second-order proximity yields slightly better performances than the firstorder proximity, which assumes connecting users are close to each other, on a variety of downstream tasks.\nMention embeddings The representation of a mention is the average of embeddings of words it contains. As each mention is typically one to three words, the simple representations often perform surprisingly well (Socher et al., 2013). We adopt the structured skip-gram model (Ling et al., 2015) to learn the word embeddings E(w) on a Twitter corpus with 52 million tweets (Owoputi et al., 2013). The mention vector of the t-th mention candidate can be written as:\nv (m) t =\n1\n|x(w)t | \u2211 w\u2208x(w)t v(w)w , (4)\nwhere x(w)t is the set of words in the mention.\nEntity embeddings We use the pre-trained Freebase entity embeddings released by Google to represent entity candidates, which we refer as E(e).4 The embeddings are trained with the skip-gram model (Mikolov et al., 2013) on 100 billion words from various news articles. The entity embeddings can also be learnt from Wikipedia hyperlinks or Freebase entity relations, which we leave as future work.\nCompositions of user, mention, and entity The distributed representations of users, mentions, and entities offer additional information that is useful for improving entity disambiguation capability. In particular, we explore the information by making two assumptions: socially connected users are interested in similar entities (entity homophily), and semantically related mentions are likely to be linked to similar entities.\nWe utilize a simple composition model that takes the form of the summation of two bilinear scoring functions, each of which explicitly leverages one of the assumptions. Given the author representation v (u) u , the mention representation v (m) t , and the entity representation v(e)yt , the output of the model can 4Available at https://code.google.com/archive/p/word2vec/\nbe written as:\ng2(x, yt, u, t; \u03982) =v (u) u > W(u,e)v(e)yt\n+ v (m) t > W(m,e)v(e)yt , (5)\nwhere W(u,e) and W(m,e) are D(u) \u00d7 D(e) and D(w) \u00d7D(e) bilinear transformation matrices. Similar bilinear formulation has been used in the literature of knowledge base completion and inference (Socher et al., 2013; Yang et al., 2014). The parameters of the composition model are \u03982 = {W(u,e),W(m,e),E(u),E(w),E(e)}."}, {"heading": "4.3 Non-overlapping Inference", "text": "The non-overlapping constraint for entity assignments requires inference method that is different from the standard Viterbi algorithm for a linear chain. We now present a variant of the Viterbi algorithm for the non-overlapping structure. Given the overall scoring function g(x, yt, u, t) for the t-th mention candidate choosing an entity yt, we sort the mention candidates by their end indices and define the Viterbi recursion by\ny\u0302t = arg max yt\u2208Yxt ,yt 6=Nil g(x, yt, u, t) (6) a(1) = max(g(x,Nil, u, 1), g(x, y\u03021, u, 1)) (7) a(t) = max (\u03c8t(Nil), \u03c8t(y\u0302t)) (8) \u03c8t(Nil) =g(x,Nil, u, t) + a(t\u2212 1) (9)\n\u03c8t(y\u0302t) =g(x, y\u0302t, u, t) + \u2211\nprev(t)<t\u2032<t\ng(x,Nil, u, t\u2032)\n+ a(prev(t)) (10)\nwhere Yxt is set of entity candidates for the t-th mention candidate, and prev(t) is a function that points out the previous non-overlapping mention candidate for the t-th mention candidate. We exclude any second-order features between entities. Therefore, for each mention candidate, we only need to decide whether it can take the highest scored entity candidate y\u0302t or the special Nil entity based on whether it is overlapped with other mention candidates."}, {"heading": "4.4 Loss-augmented Training", "text": "The parameters need to be learnt during training are \u0398 = [\u03981, {W(u,e),W(m,e)}].5 We train NTEL by minimizing the following loss function for each training tweet:\nL(\u0398) = max y\u2208Yx (\u2206(y,y\u2217) + s(x,y, u))\u2212 s(x,y\u2217, u), (11) where y\u2217 is the gold structure, Yx represents the set of valid output structures for x, and \u2206(y,y\u2217) is the weighted hamming distance between the gold structure y\u2217 and the valid structure y. The hamming loss is decomposable on the mention candidates, which enables efficient inferences. We set the hamming loss weight to 0.2 after a preliminary search. Note that the number of parameters in our composition model is large. Thus, we include an L2 regularizer on these parameters, which is omitted from Equation 11 for brevity. The evaluation of the loss function corresponds to the loss-augmented inference problem:\ny\u0302 = arg max y\u2208Yx\n(\u2206(y,y\u2217) + s(x,y, u)), (12)\nwhich can be solved by the above non-overlapping inference algorithm. We employ vanilla SGD algorithm to optimize all the parameters. The numbers of training epochs are determined by early stopping (at most 1000 epochs). Training takes 6-8 hours on 4 threads."}, {"heading": "5 Experiments", "text": "In this section, we evaluate NTEL on the NEEL and TACL datasets as described in \u00a7 2, focusing on investigating whether social information can improve the task. We also compare NTEL with the previous state-of-the-art system."}, {"heading": "5.1 Social network expansion", "text": "We utilize Twitter follower, mention, and retweet social networks to train user embeddings. We were able to identify 2,312 authors for the tweets of the two datasets in March 2016. We then used the Twitter API to crawl their friend links and timelines, from which we can induce the networks. We find the\n5We fixed the pre-trained embedding matrices during lossaugmented training.\nnumbers of social connections (bidirectional links) between these users are relatively small. In order to learn better user embeddings, we expand the set of author nodes by including nodes that will do the most to densify the author networks. For the follower network, we add additional individuals who are followed by at least twenty authors in the original set. For the mention or retweet networks, we add all users who have mentioned or retweeted by at least ten authors in the original set. The statistics of the resulting networks are presented in Table 3."}, {"heading": "5.2 Experimental Settings", "text": "Following Yang and Chang (2015), we train all the models with the NEEL-train dataset and evaluate different systems on the NEEL-test and TACL datasets. In addition, 800 tweets from the NEELtrain dataset are sampled as our development set to perform parameter tuning. Note that Yang and Chang (2015) also attempt to optimize F1 scores by balancing precision and recall scores on the development set; we do not fine tune our F1 in this way, so that we can apply a single trained system across different test sets.\nMetrics We follow prior work (Guo et al., 2013a; Yang and Chang, 2015) and perform the standard evaluation for an end-to-end entity linking system, computing precision, recall, and F1 score according to the entity references and the system outputs. An output entity is considered as correct if it matches the gold entity and the mention boundary overlaps with the gold mention boundary. More details about the metrics are described by Carmel et al. (2014).\nCompetitive systems Our first baseline system, NTEL-nonstruct, ignores the structure information and makes the entity assignment decision for each mention candidate individually. For NTEL, we start with a baseline system using the surface features, and then incorporate the two bilinear functions\n(user-entity and mention-entity) described in Equation 5 incrementally. Our main evaluation uses the RETWEET+ network, since the retweet network had the greatest entity homophily; an additional evaluation compares across network types.\nParameter tuning We tune all the hyperparameters on the development set, and then re-train the models on the full training data with the best parameters. We choose the number of hidden units for the MLP from {20, 30, 40, 50}, and the regularization penalty for our composition model from {0.001, 0.005, 0.01, 0.05, 0.1}. The sizes of user embeddings and word embeddings are selected from {50, 100} and {200, 400, 600} respectively. The pre-trained Freebase entity embedding size is 1000. The learning rate for the SGD algorithm is set as 0.01. During training, we check the performance on the development set regularly to perform early stopping."}, {"heading": "5.3 Results", "text": "Table 4 summarizes the empirical findings for our approach and S-MART (Yang and Chang, 2015) on the tweet entity linking task. For the systems with user-entity bilinear function, we report results obtained from embeddings trained on RETWEET+ in Table 4, and other results are available in Table 5. The best hyper-parameters are: the number of hidden units for the MLP is 40, the L2 regularization penalty for the composition parameters is 0.005, and the user embedding size is 100. For the word embedding size, we find 600 offers marginal improvements over 400 but requires longer training time. Thus, we choose 400 as the size of word embeddings.\nAs presented in Table 4, NTEL-nonstruct performs 2.7% F1 worse than the NTEL baseline on the two test sets, which indicates the non-overlapping inference improves system performance on the task.\nWith structured inference but without embeddings, NTEL performs roughly the same as S-MART, showing that a feedforward neural network offers similar expressivity to the regression trees employed by Yang and Chang (2015).\nPerformance improves substantially with the incorporation of low-dimensional author, mention, and entity representations. As shown in Table 4, by learning the interactions between mention and entity representations, NTEL with mention-entity bilinear function outperforms the NTEL baseline system by 1.8% F1 on average. Specifically, the bilinear function results in considerable performance gains in recalls, with small compromise in precisions on the datasets.\nSocial information helps to increase about 1% F1 on top of both the NTEL baseline system and the NTEL system with mention-entity bilinear composition. In contrast to the mention-entity composition model, which mainly focuses on improving the baseline system on recall scores, the user-entity composition model increases around 2.5% recalls, without much sacrifice in precisions.\nOur best system achieves the state-of-the-art results on the NEEL-test dataset and the TACL dataset, outperforming S-MART by 0.9% and 5.4% F1 scores respectively. To establish the statistical significance of the results, we obtain 100 bootstrap samples for each test set, and compute the F1 score on each sample for each algorithm. Two-tail paired t-test is then applied to determine if the F1 scores of two algorithms are significantly different. NTEL significantly outperforms S-MART on the NEEL-test dataset and the TACL dataset under p < 0.01 level, with t-statistics equal to 11.5 and 33.6 respectively.\nAs shown in Table 5, MENTION+ and RETWEET+ perform slightly better than FOLLOWER+. Puniyani et al. (2010) show that the mention network has stronger linguistic properties than the follower network, as it gives better correlations on each author\u2019s distribution over latent topics as induced by latent Dirichlet allocation (Blei et al., 2003). Our results suggest that the properties hold with respect to the authors\u2019 interests on real world entities."}, {"heading": "5.4 Error Analysis & Discussion", "text": "We examine the outputs of different systems, focusing on investigating what errors are corrected by the two bilinear functions. The results reveal that the mention-entity composition improves the system ability to tackle mentions that are abbreviations such as \u2018WSJ\u2019 (The Wall Street Journal) and \u2018SJSU\u2019 (San Jose State University), which leads to higher recall scores. The mention-entity model also helps to eliminate errors that incorrectly link non-entities to popular entities. For example, the NTEL baseline system links \u2018sec\u2019 in the tweet \u2018I\u2019m a be in Miami for sec to hit da radio!\u2019 to Southeastern Conference, which is corrected by the mention-entity composition model. The word semantic information encoded in the mention representations alleviates the biased entity information given by the surface features.\nThe user-entity composition model is good at handling highly ambiguous mentions. For example, our full model successfully disambiguates entities for mentions such as \u2018Sox\u2019 (Boston Red Sox vs. Chicago White Sox), \u2018Sanders\u2019 (Bernie Sanders vs. Barry Sanders), and \u2018Memphis\u2019 (Memphis Grizzlies vs. Memphis, Tennessee), which are mistakenly linked to the other entities or Nil by the mentionentity model. Another example is that the social network information helps the system correctly link \u2018Kim\u2019 to Lil\u2019 Kim instead of Kim Kardashian, despite that the latter entity\u2019s wikipedia page is considerably more popular."}, {"heading": "6 Related Work", "text": "Tweet entity linking Previous work on entity linking mainly focuses on well-written docu-\nments (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambiguation is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation. Shen et al. (2013) employ Twitter user account information to improve entity linking, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Fang and Chang (2014) demonstrate that spatial and temporal signals are critical for the task, and they advance the performance by associating entity prior distributions with different timestamps and locations. Our work overcomes the difficulty by leveraging social relations \u2014 socially connected individuals are assumed to share similar interests on entities. As the Twitter post information is often sparse for some users, our assumption enables the utilization of more relevant information that helps to improve the task.\nNLP with social relations Most previous work on incorporating social relations for NLP problems focuses on Twitter sentiment analysis, where the existence of social relations between users is considered as a clue that the sentiment polarities of messages from the users should be similar. Speriosu et al. (2011) construct a heterogeneous network with\ntweets, users, and n-grams as nodes, and the sentiment label distributions associated with the nodes are refined by performing label propagation over social relations. Tan et al. (2011) and Hu et al. (2013) leverage social relations for sentiment analysis by exploiting a factor graph model and the graph Laplacian technique respectively, so that the tweets belonging to social connected users share similar label distributions. We work on entity linking in Twitter messages, where the label space is much larger than that of sentiment classification. The social relations can be more relevant in our problem, as it is challenging to obtain the entity prior distribution for each individual."}, {"heading": "7 Conclusion", "text": "We present a neural based structured learning architecture for tweet entity linking, leveraging the tendency of socially linked individuals to share similar interests on named entities \u2014 the phenomenon of entity homophily. By modeling the compositions of vector representations of author, entity, and mention, our approach is able to exploit the social network as a source of contextual information. This vector-compositional model is combined with nonlinear feature combinations of surface features, via a feedforward neural network. To avoid predicting overlapping entity mentions, we employ a structured prediction algorithm, and train the system with lossaugmented decoding.\nSocial networks arise in other settings besides microblogs, such as webpages and academic research articles; exploiting these networks is a possible direction for future work. We would also like to investigate other metadata attributes that are relevant to the task, such as spatial and temporal signals.\nAcknowledgments This research was supported by the National Science Foundation under awards IIS-1111142 and RI-1452443, by the National Institutes of Health under award number R01GM112697-01, and by the Air Force Office of Scientific Research."}, {"heading": "A Appendix: Additional Results", "text": "In the first version of (Yang and Chang, 2015), the Twitter messages that contain no ground truth entities are excluded in the experiments. For completeness, we now present the evaluation results of NTEL in this setting, which are shown in Table 6. The RETWEET+ network is adopted to train author embeddings. The best hyper-parameters are the same as those described in \u00a7 5, except for the L2 regularization penalty for the composition parameters, which is set as 0.01 here.\nThe results are generally better than those presented in Table 4. As shown, NTEL benefits from the distributed representations of authors, mentions, and entities, which improve the average F1 score by 2.3 points. NTEL also gives the best results on the datasets, outperforming S-MART by about 2% F1 on average."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Using encyclopedic knowledge for named entity disambiguation", "author": ["R. C Bunescu", "M. Pasca."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).", "citeRegEx": "Bunescu and Pasca.,? 2006", "shortCiteRegEx": "Bunescu and Pasca.", "year": 2006}, {"title": "Making sense of microposts (# microposts2014) named entity extraction & linking challenge", "author": ["Amparo E Cano", "Giuseppe Rizzo", "Andrea Varga", "Matthew Rowe", "Milan Stankovic", "Aba-Sah Dadzie."], "venue": "Making Sense of Microposts (# Microposts2014).", "citeRegEx": "Cano et al\\.,? 2014", "shortCiteRegEx": "Cano et al\\.", "year": 2014}, {"title": "Erd\u201914: entity recognition and disambiguation challenge", "author": ["David Carmel", "Ming-Wei Chang", "Evgeniy Gabrilovich", "Bo-June Paul Hsu", "Kuansan Wang."], "venue": "ACM SIGIR Forum, pages 63\u201377.", "citeRegEx": "Carmel et al\\.,? 2014", "shortCiteRegEx": "Carmel et al\\.", "year": 2014}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "author": ["Silviu Cucerzan."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).", "citeRegEx": "Cucerzan.,? 2007", "shortCiteRegEx": "Cucerzan.", "year": 2007}, {"title": "Entity linking on microblogs with spatial and temporal signals", "author": ["Yuan Fang", "Ming-Wei Chang."], "venue": "Transactions of the Association for Computational Linguistics (ACL).", "citeRegEx": "Fang and Chang.,? 2014", "shortCiteRegEx": "Fang and Chang.", "year": 2014}, {"title": "To link or not to link? a study on end-toend tweet entity linking", "author": ["Stephen Guo", "Ming-Wei Chang", "Emre Kiciman."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), Atlanta, GA.", "citeRegEx": "Guo et al\\.,? 2013a", "shortCiteRegEx": "Guo et al\\.", "year": 2013}, {"title": "Microblog entity linking by leveraging extra posts", "author": ["Yuhang Guo", "Bing Qin", "Ting Liu", "Sheng Li."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP), Seattle, WA.", "citeRegEx": "Guo et al\\.,? 2013b", "shortCiteRegEx": "Guo et al\\.", "year": 2013}, {"title": "Demographic factors improve classification performance", "author": ["Dirk Hovy."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), pages 752\u2013762, Beijing, China.", "citeRegEx": "Hovy.,? 2015", "shortCiteRegEx": "Hovy.", "year": 2015}, {"title": "Exploiting social relations for sentiment analysis in microblogging", "author": ["Xia Hu", "Lei Tang", "Jiliang Tang", "Huan Liu."], "venue": "Proceedings of the sixth ACM international conference on Web search and data mining (WSDM), pages 537\u2013546.", "citeRegEx": "Hu et al\\.,? 2013", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Collective tweet wikification based on semi-supervised graph regularization", "author": ["Hongzhao Huang", "Yunbo Cao", "Xiaojiang Huang", "Heng Ji", "Chin-Yew Lin."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Baltimore, MD.", "citeRegEx": "Huang et al\\.,? 2014", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "What is Twitter, a social network or a news media? In Proceedings of the Conference on World-Wide Web (WWW), pages 591\u2013600, New York", "author": ["Haewoon Kwak", "Changhyun Lee", "Hosung Park", "Sue Moon."], "venue": "ACM.", "citeRegEx": "Kwak et al\\.,? 2010", "shortCiteRegEx": "Kwak et al\\.", "year": 2010}, {"title": "Learning multi-faceted representations of individuals from heterogeneous evidence using neural networks", "author": ["Jiwei Li", "Alan Ritter", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1510.05198.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), Denver, CO.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Twittermonitor: trend detection over the twitter stream", "author": ["Michael Mathioudakis", "Nick Koudas."], "venue": "Proceedings of the ACM SIGMOD International Conference on Management of data (SIGMOD), pages 1155\u20131158.", "citeRegEx": "Mathioudakis and Koudas.,? 2010", "shortCiteRegEx": "Mathioudakis and Koudas.", "year": 2010}, {"title": "Birds of a feather: Homophily in social networks", "author": ["Miller McPherson", "Lynn Smith-Lovin", "James M Cook."], "venue": "Annual review of sociology, pages 415\u2013444.", "citeRegEx": "McPherson et al\\.,? 2001", "shortCiteRegEx": "McPherson et al\\.", "year": 2001}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Neural Information Processing Systems (NIPS), pages 3111\u20133119, Lake Tahoe.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning to link with Wikipedia", "author": ["D. Milne", "I.H. Witten."], "venue": "Proceedings of the International Conference on Information and Knowledge Management (CIKM).", "citeRegEx": "Milne and Witten.,? 2008", "shortCiteRegEx": "Milne and Witten.", "year": 2008}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["Olutobi Owoputi", "Brendan O\u2019Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In Proceedings of the North American Chapter of the Association", "citeRegEx": "Owoputi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Owoputi et al\\.", "year": 2013}, {"title": "Social links from latent topics in microblogs", "author": ["Kriti Puniyani", "Jacob Eisenstein", "Shay Cohen", "Eric P. Xing."], "venue": "Proceedings of NAACL Workshop on Social Media, Los Angeles.", "citeRegEx": "Puniyani et al\\.,? 2010", "shortCiteRegEx": "Puniyani et al\\.", "year": 2010}, {"title": "Named entity recognition in tweets: an experimental study", "author": ["Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni."], "venue": "Proceedings of Empirical Methods for Natural Language Processing (EMNLP).", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Linking named entities in tweets with knowledge base via user interest modeling", "author": ["Wei Shen", "Jianyong Wang", "Ping Luo", "Min Wang."], "venue": "Proceedings of Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Shen et al\\.,? 2013", "shortCiteRegEx": "Shen et al\\.", "year": 2013}, {"title": "Reasoning With Neural Tensor Networks For Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Neural Information Processing Systems (NIPS), Lake Tahoe.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Twitter polarity classification", "author": ["Michael Speriosu", "Nikita Sudan", "Sid Upadhyay", "Jason Baldridge"], "venue": null, "citeRegEx": "Speriosu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Speriosu et al\\.", "year": 2011}, {"title": "User-level sentiment analysis incorporating social networks", "author": ["Chenhao Tan", "Lillian Lee", "Jie Tang", "Long Jiang", "Ming Zhou", "Ping Li."], "venue": "Proceedings of Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Tan et al\\.,? 2011", "shortCiteRegEx": "Tan et al\\.", "year": 2011}, {"title": "Line: Large-scale information network embedding", "author": ["Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei."], "venue": "Proceedings of the Conference on World-Wide Web (WWW).", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Who says what to whom on twitter", "author": ["Shaomei Wu", "Jake M Hofman", "Winter A Mason", "Duncan J Watts."], "venue": "Proceedings of the Conference on WorldWide Web (WWW), pages 705\u2013714.", "citeRegEx": "Wu et al\\.,? 2011", "shortCiteRegEx": "Wu et al\\.", "year": 2011}, {"title": "S-mart: Novel tree-based structured learning algorithms applied to tweet entity linking", "author": ["Yi Yang", "Ming-Wei Chang."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Beijing, China.", "citeRegEx": "Yang and Chang.,? 2015", "shortCiteRegEx": "Yang and Chang.", "year": 2015}, {"title": "Putting things in context: Community-specific embedding projections for sentiment analysis", "author": ["Yi Yang", "Jacob Eisenstein."], "venue": "arXiv preprint arXiv:1511.06052.", "citeRegEx": "Yang and Eisenstein.,? 2015", "shortCiteRegEx": "Yang and Eisenstein.", "year": 2015}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "arXiv preprint arXiv:1412.6575.", "citeRegEx": "Yang et al\\.,? 2014", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), Beijing, China.", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": ", Twitter messages) is of increasing interest, as it is an essential step for many downstream applications, such as market research (Asur and Huberman, 2010), topic detection and tracking (Mathioudakis and Koudas, 2010), and question answering (Yih et al.", "startOffset": 188, "endOffset": 219}, {"referenceID": 30, "context": ", Twitter messages) is of increasing interest, as it is an essential step for many downstream applications, such as market research (Asur and Huberman, 2010), topic detection and tracking (Mathioudakis and Koudas, 2010), and question answering (Yih et al., 2015).", "startOffset": 244, "endOffset": 262}, {"referenceID": 15, "context": "The sociological theory of homophily asserts that socially connected individuals are more likely to have similar behaviors or share similar interests (McPherson et al., 2001).", "startOffset": 150, "endOffset": 174}, {"referenceID": 24, "context": "This property has been used to improve many natural language processing tasks such as sentiment analysis (Tan et al., 2011; Yang and Eisenstein, 2015), topic classification (Hovy, 2015) and user attribute inference (Li et al.", "startOffset": 105, "endOffset": 150}, {"referenceID": 28, "context": "This property has been used to improve many natural language processing tasks such as sentiment analysis (Tan et al., 2011; Yang and Eisenstein, 2015), topic classification (Hovy, 2015) and user attribute inference (Li et al.", "startOffset": 105, "endOffset": 150}, {"referenceID": 8, "context": ", 2011; Yang and Eisenstein, 2015), topic classification (Hovy, 2015) and user attribute inference (Li et al.", "startOffset": 57, "endOffset": 69}, {"referenceID": 12, "context": ", 2011; Yang and Eisenstein, 2015), topic classification (Hovy, 2015) and user attribute inference (Li et al., 2015).", "startOffset": 99, "endOffset": 116}, {"referenceID": 25, "context": "To exploit social information, we adopt the recent advance on embedding information networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure.", "startOffset": 93, "endOffset": 112}, {"referenceID": 6, "context": "Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms.", "startOffset": 31, "endOffset": 72}, {"referenceID": 27, "context": "Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms.", "startOffset": 31, "endOffset": 72}, {"referenceID": 27, "context": "\u2022 The complete system, NTEL, outperforms the previous state-of-the-art (Yang and Chang, 2015) by 3% average F1 on two benchmark datasets.", "startOffset": 71, "endOffset": 93}, {"referenceID": 2, "context": "NEEL is originally collected and annotated for the Named Entity Extraction & Linking Challenge (Cano et al., 2014), and TACL is first used and released by Fang and Chang (2014).", "startOffset": 95, "endOffset": 114}, {"referenceID": 2, "context": "NEEL is originally collected and annotated for the Named Entity Extraction & Linking Challenge (Cano et al., 2014), and TACL is first used and released by Fang and Chang (2014). The datasets are then cleaned and unified by Yang and Chang (2015).", "startOffset": 96, "endOffset": 177}, {"referenceID": 2, "context": "NEEL is originally collected and annotated for the Named Entity Extraction & Linking Challenge (Cano et al., 2014), and TACL is first used and released by Fang and Chang (2014). The datasets are then cleaned and unified by Yang and Chang (2015). The statistics of the datasets are presented in Table 1.", "startOffset": 96, "endOffset": 245}, {"referenceID": 11, "context": "1 We exploit bi-directed links to create the undirected networks, as bi-directed links result in stronger social network ties than directed links (Kwak et al., 2010; Wu et al., 2011).", "startOffset": 146, "endOffset": 182}, {"referenceID": 26, "context": "1 We exploit bi-directed links to create the undirected networks, as bi-directed links result in stronger social network ties than directed links (Kwak et al., 2010; Wu et al., 2011).", "startOffset": 146, "endOffset": 182}, {"referenceID": 20, "context": "For a user ui, we employ a Twitter NER system (Ritter et al., 2011) to detect entity mentions in the timeline, which we use to construct a user entity vector u i , so that u (ent) i,j = 1 iff user i has mentioned entity j.", "startOffset": 46, "endOffset": 67}, {"referenceID": 27, "context": "We adopted the same entity database and lexicon as those used by Yang and Chang (2015).", "startOffset": 65, "endOffset": 87}, {"referenceID": 27, "context": "We include the 37 features used by Yang and Chang (2015) as our surface feature set.", "startOffset": 35, "endOffset": 57}, {"referenceID": 27, "context": "Yang and Chang (2015) argue that non-linearity is the key for obtaining good results on the task, as linear models are not expressive enough to capture the high-order relationships between the dense features.", "startOffset": 0, "endOffset": 22}, {"referenceID": 25, "context": "User embeddings We obtain low-dimensional Twitter author embeddings E(u) using LINE \u2014 the recently proposed model for embedding information networks (Tang et al., 2015).", "startOffset": 149, "endOffset": 168}, {"referenceID": 22, "context": "As each mention is typically one to three words, the simple representations often perform surprisingly well (Socher et al., 2013).", "startOffset": 108, "endOffset": 129}, {"referenceID": 13, "context": "We adopt the structured skip-gram model (Ling et al., 2015) to learn the word embeddings E(w) on a Twitter corpus with 52 million tweets (Owoputi et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 18, "context": ", 2015) to learn the word embeddings E(w) on a Twitter corpus with 52 million tweets (Owoputi et al., 2013).", "startOffset": 85, "endOffset": 107}, {"referenceID": 16, "context": "4 The embeddings are trained with the skip-gram model (Mikolov et al., 2013) on 100 billion words from various news articles.", "startOffset": 54, "endOffset": 76}, {"referenceID": 22, "context": "Similar bilinear formulation has been used in the literature of knowledge base completion and inference (Socher et al., 2013; Yang et al., 2014).", "startOffset": 104, "endOffset": 144}, {"referenceID": 29, "context": "Similar bilinear formulation has been used in the literature of knowledge base completion and inference (Socher et al., 2013; Yang et al., 2014).", "startOffset": 104, "endOffset": 144}, {"referenceID": 27, "context": "Following Yang and Chang (2015), we train all the models with the NEEL-train dataset and evaluate different systems on the NEEL-test and TACL datasets.", "startOffset": 10, "endOffset": 32}, {"referenceID": 27, "context": "Following Yang and Chang (2015), we train all the models with the NEEL-train dataset and evaluate different systems on the NEEL-test and TACL datasets. In addition, 800 tweets from the NEELtrain dataset are sampled as our development set to perform parameter tuning. Note that Yang and Chang (2015) also attempt to optimize F1 scores by balancing precision and recall scores on the development set; we do not fine tune our F1 in this way, so that we can apply a single trained system across different test sets.", "startOffset": 10, "endOffset": 299}, {"referenceID": 6, "context": "Metrics We follow prior work (Guo et al., 2013a; Yang and Chang, 2015) and perform the standard evaluation for an end-to-end entity linking system, computing precision, recall, and F1 score according to the entity references and the system outputs.", "startOffset": 29, "endOffset": 70}, {"referenceID": 27, "context": "Metrics We follow prior work (Guo et al., 2013a; Yang and Chang, 2015) and perform the standard evaluation for an end-to-end entity linking system, computing precision, recall, and F1 score according to the entity references and the system outputs.", "startOffset": 29, "endOffset": 70}, {"referenceID": 3, "context": "More details about the metrics are described by Carmel et al. (2014).", "startOffset": 48, "endOffset": 69}, {"referenceID": 27, "context": "Table 4 summarizes the empirical findings for our approach and S-MART (Yang and Chang, 2015) on the tweet entity linking task.", "startOffset": 70, "endOffset": 92}, {"referenceID": 27, "context": "With structured inference but without embeddings, NTEL performs roughly the same as S-MART, showing that a feedforward neural network offers similar expressivity to the regression trees employed by Yang and Chang (2015).", "startOffset": 198, "endOffset": 220}, {"referenceID": 0, "context": "(2010) show that the mention network has stronger linguistic properties than the follower network, as it gives better correlations on each author\u2019s distribution over latent topics as induced by latent Dirichlet allocation (Blei et al., 2003).", "startOffset": 222, "endOffset": 241}, {"referenceID": 18, "context": "Puniyani et al. (2010) show that the mention network has stronger linguistic properties than the follower network, as it gives better correlations on each author\u2019s distribution over latent topics as induced by latent Dirichlet allocation (Blei et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambiguation is usually performed by maximizing the global topical coherence between entities.", "startOffset": 94, "endOffset": 159}, {"referenceID": 4, "context": "Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambiguation is usually performed by maximizing the global topical coherence between entities.", "startOffset": 94, "endOffset": 159}, {"referenceID": 17, "context": "Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambiguation is usually performed by maximizing the global topical coherence between entities.", "startOffset": 94, "endOffset": 159}, {"referenceID": 10, "context": "To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014).", "startOffset": 139, "endOffset": 159}, {"referenceID": 1, "context": "Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambiguation is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation.", "startOffset": 95, "endOffset": 586}, {"referenceID": 1, "context": "Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambiguation is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation. Shen et al. (2013) employ Twitter user account information to improve entity linking, based on the intuition that all tweets posted by the same user share an underlying topic distribution.", "startOffset": 95, "endOffset": 753}, {"referenceID": 1, "context": "Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambiguation is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation. Shen et al. (2013) employ Twitter user account information to improve entity linking, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Fang and Chang (2014) demonstrate that spatial and temporal signals are critical for the task, and they advance the performance by associating entity prior distributions with different timestamps and locations.", "startOffset": 95, "endOffset": 945}, {"referenceID": 23, "context": "Speriosu et al. (2011) construct a heterogeneous network with", "startOffset": 0, "endOffset": 23}, {"referenceID": 23, "context": "Tan et al. (2011) and Hu et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "(2011) and Hu et al. (2013) leverage social relations for sentiment analysis by exploiting a factor graph model and the graph Laplacian technique respectively, so that the tweets belonging to social connected users share similar label distributions.", "startOffset": 11, "endOffset": 28}], "year": 2016, "abstractText": "Entity linking is the task of identifying mentions of entities in text, and linking them to entries in a knowledge base. This task is especially difficult in microblogs, as there is little additional text to provide disambiguating context; rather, authors rely on an implicit common ground of shared knowledge with their readers. In this paper, we attempt to capture some of this implicit context by exploiting the social network structure in microblogs. We build on the theory of homophily, which implies that socially linked individuals share interests, and are therefore likely to mention the same sorts of entities. We implement this idea by encoding authors, mentions, and entities in a continuous vector space, which is constructed so that socially-connected authors have similar vector representations. These vectors are incorporated into a neural structured prediction model, which captures structural constraints that are inherent in the entity linking task. Together, these design decisions yield F1 improvements of 1%-5% on benchmark datasets, as compared to the previous state-of-the-art.", "creator": "TeX"}}}