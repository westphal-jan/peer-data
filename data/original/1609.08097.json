{"id": "1609.08097", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Creating Causal Embeddings for Question Answering with Minimal Supervision", "abstract": "A common model for question answering (QA) is that a good answer is one that is closely related to the question, where relatedness is often determined using general-purpose lexical models such as word embeddings. We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-specific embeddings. With causality as a use case, we implement this insight in three steps. First, we generate causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns. Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect. Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embeddings. We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo! Answers. We show that explicitly modeling causality improves performance in both tasks. In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative).", "histories": [["v1", "Mon, 26 Sep 2016 17:50:15 GMT  (97kb,D)", "http://arxiv.org/abs/1609.08097v1", "To appear in EMNLP 2016"]], "COMMENTS": "To appear in EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rebecca sharp", "mihai surdeanu", "peter jansen", "peter clark", "michael hammond"], "accepted": true, "id": "1609.08097"}, "pdf": {"name": "1609.08097.pdf", "metadata": {"source": "CRF", "title": "Creating Causal Embeddings for Question Answering with Minimal Supervision", "authors": ["Rebecca Sharp", "Mihai Surdeanu", "Peter Jansen", "Peter Clark", "Michael Hammond"], "emails": ["hammond}@email.arizona.edu,", "PeterC@allenai.org", "P@1,"], "sections": [{"heading": "1 Introduction", "text": "Question answering (QA), i.e., finding short answers to natural language questions, is one of the most important but challenging tasks on the road towards natural language understanding (Etzioni, 2011). A\ncommon approach for QA is to prefer answers that are closely related to the question, where relatedness is often determined using lexical semantic models such as word embeddings (Yih et al., 2013; Jansen et al., 2014; Fried et al., 2015). While appealing for its robustness to natural language variation, this onesize-fits-all approach does not take into account the wide range of distinct question types that can appear in any given question set, and that are best addressed individually (Chu-Carroll et al., 2004; Ferrucci et al., 2010; Clark et al., 2013).\nGiven the variety of question types, we suggest that a better approach is to look for answers that are related to the question through the appropriate relation, e.g., a causal question should have a causeeffect relation with its answer. If we adopt this view, and continue to work with embeddings as a mechanism for assessing relationship, this raises a key question: how do we train and use task-specific embeddings cost-effectively? Using causality as a use case, we answer this question with a framework for producing causal word embeddings with minimal supervision, and a demonstration that such taskspecific embeddings significantly benefit causal QA.\nIn particular, the contributions of this work are:\n(1) A methodology for generating causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns, e.g., X causes Y. We then train dedicated embedding (as well as two other distributional similarity) models over this data. Levy and Goldberg (2014) have modified the algorithm of Mikolov et al. (2013) to use an arbitrary, rather than linear, context. Here we make this context task-specific,\nar X\niv :1\n60 9.\n08 09\n7v 1\n[ cs\n.C L\n] 2\n6 Se\np 20\ni.e., the context of a cause is its effect. Further, to mitigate sparsity and noise, our models are bidirectional, and noise aware (by incorporating the likelihood of noise in the training process).\n(2) The insight that QA benefits from task-specific embeddings. We implement a QA system that uses the above causal embeddings to answer questions and demonstrate that they significantly improve performance over a strong baseline. Further, we show that causal embeddings encode complementary information to vanilla embeddings, even when trained from the same knowledge resources.\n(3) An analysis of direct vs. indirect evaluations for task-specific word embeddings. We evaluate our causal models both directly, in terms of measuring their capacity to rank causally-related word pairs over word pairs of other relations, as well as indirectly in the downstream causal QA task. In both tasks, our analysis indicates that including causal models significantly improves performance. However, from the direct evaluation, it is difficult to estimate which models will perform best in realworld tasks. Our analysis re-enforces recent observations about the limitations of word similarity evaluations (Faruqui et al., 2016): we show that they have limited coverage and may align poorly with real-world tasks."}, {"heading": "2 Related Work", "text": "Addressing the need for specialized solving methods in QA, Oh et. al (2013) incorporate a dedicated causal component into their system, and note that it improves the overall performance. However, their model is limited by the need for lexical overlap between a causal construction found in their knowledge base and the question itself. Here, we develop a causal QA component that exploits specialized word embeddings to gain robustness to lexical variation.\nThere has been a vast body of work which demonstrates that word embeddings derived from distributional similarity are useful in many tasks, including question answering \u2013 see inter alia (Fried et al., 2015; Yih et al., 2013). However, Levy and Goldberg (2015) note that there are limitations on the type of semantic knowledge which is encoded in these general-purpose similarity\nembeddings. Therefore, here we build customized task-specific embeddings for causal QA.\nCustomized embeddings have been created for a variety of tasks, including semantic role labeling (FitzGerald et al., 2015; Woodsend and Lapata, 2015), and binary relation extraction (Riedel et al., 2013). Similar to Riedel et al., we train embeddings customized for specific relations, but we bootstrap training data using minimal supervision (i.e., a small set of patterns) rather than relying on distant supervision and large existing knowledge bases. Additionally, while Riedel et al. represent all relations in a general embedding space, here we train a dedicated embedding space for just the causal relations.\nIn QA, embeddings have been customized to have question words that are close to either their answer words (Bordes et al., 2014), or to structured knowledge base entries (Yang et al., 2014). While these methods are useful for QA, they do not distinguish between different types of questions, and as such their embeddings are not specific to a given question type.\nAdditionally, embeddings have been customized to distinguish functional similarity from relatedness (Levy and Goldberg, 2014; Kiela et al., 2015). In particular, Levy and Goldberg train their embeddings by replacing the standard linear context of the target word with context derived from the syntactic dependency graph of the sentence. In this work, we make use of this extension to arbitrary context in order to train our embeddings with contexts derived from binary causal relations. We extract cause-effect text pairs such that the cause text becomes the target text and the effect text serves as the context.\nRecently, Faruqui et al.(2016) discussed issues surrounding the evaluation of similarity word embeddings, including the lack of correlation between their performance on word-similarity tasks and \u201cdownstream\u201d or real-world tasks like QA, text classification, etc. As they advocate, in addition to a direct evaluation of our causal embeddings, we also evaluate them independently in a downstream QA task. We provide the same comparison for two alternative approaches (an alignment model and a convolutional neural network model), confirming that the direct evaluation performance can be misleading without the task-specific, downstream evaluation.\nWith respect to extracting causal relations from text, Girju et al. (2002) use modified Hearst patterns (Hearst, 1992) to extract a large number of potential cause-effect tuples, where both causes and effects must be nouns. However, Cole et al. (2005) show that these nominal-based causal relations account for a relatively small percentage of all causal relations, and for this reason, (Yang and Mao, 2014) allow for more elaborate argument structures in their causal extraction by identifying verbs, and then following the syntactic subtree of the verbal arguments to construct their candidate causes and effects. Additionally, Do et al. (2011) observe that nouns as well as verbs can signal causality. We follow these intuitions in developing our causal patterns by using both nouns and verbs to signal potential participants in causal relations, and then allowing for the entire dominated structures to serve as the cause and/or effect arguments."}, {"heading": "3 Approach", "text": "Our focus is on reranking answers to causal questions using using task-specific distributional similarity methods. Our approach operates in three steps:\n(1) We start by bootstrapping a large number of cause-effect pairs from free text using a small number of syntactic and surface patterns (Section 4).\n(2) We then use these bootstrapped pairs to build several task-specific embedding (and other distributional similarity) models (Section 5). We evaluate these models directly on a causal-relation identification task (Section 6).\n(3) Finally, we incorporate these models into a reranking framework for causal QA and demonstrate that the resulting approach performs better than the reranker without these task-specific models, even if trained on the same data (Section 7)."}, {"heading": "4 Extracting Cause-Effect Tuples", "text": "Because the success of embedding models depends on large training datasets (Sharp et al., 2015), and such datasets do not exist for open-domain causality, we opted to bootstrap a large number of cause-effect pairs from a small set of patterns. We wrote these patterns using Odin (Valenzuela-Esca\u0301rcega et al.,\n2016), a rule-based information extraction framework which has the distinct advantage of being able to operate over multiple representations of content (i.e., surface and syntax). For this work, we make use of rules that operate over both surface sequences as well as dependency syntax in the grammars introduced in steps (2) and (3) below.\nOdin operates as a cascade, allowing us to implement a two-stage approach. First, we identify potential participants in causal relations, i.e., the potential causes and effects, which we term causal mentions (CM). A second grammar then identifies actual causal relations that take these CMs as arguments.\nWe consider both noun phrases (NP) as well as entire clauses to be potential CMs, since causal patterns form around participants that are syntactically more complex than flat NPs. For example, in the sentence The collapse of the housing bubble caused stock prices to fall, both the cause (the collapse of the housing bubble) and effect (stock prices to fall) are more complicated nested structures. Reducing these arguments to non-recursive NPs (e.g., The collapse and stock prices) is clearly insufficient to capture the relevant context.\nFormally, we extract our causal relations using the following algorithm:\n(1) Pre-processing: Much of the text we use to extract causal relation tuples comes from the Annotated Gigaword (Napoles et al., 2012). This text is already fully annotated and no further processing is necessary. We additionally use text from the Simple English Wikipedia1, which we processed using the Stanford CoreNLP toolkit (Manning et al., 2014) and the dependency parser of Chen and Manning (2014).\n(2) CM identification: We extract causal mentions (which are able to serve as arguments in our causal patterns) using a set of rules designed to be robust to the variety that exists in natural language. Namely, to find CMs that are noun phrases, we first find words that are tagged as nouns, then follow outgoing dependency links for modifiers and attached prepo-\n1https://simple.wikipedia.org/wiki/Main_Page. The Simple English version was preferred over the full version due to its simpler sentence structures, which make extracting cause-effect tuples more straightforward.\nsitional phrases2, to a maximum depth of two links. To find CMs that are clauses, we first find words that are tagged as verbs (excluding verbs which themselves were considered to signal causation3), then again follow outgoing dependency links for modifiers and arguments. We used a total of four rules to label CMs.\n(3) Causal tuple extraction: After CMs are identified, a grammar scans the text for causal relations that have CMs as arguments. Different patterns have varying probabilities of signaling causation (Khoo et al., 1998). To minimize the noise in the extracted pairs, we restrict ourselves to a set of 13 rules designed to find unambiguously causal patterns, such as CAUSE led to EFFECT, where CAUSE and EFFECT are CMs. The rules operate by looking for a trigger phrase, e.g., led, and then following the dependency paths to and/or from the trigger phrase to see if all required CM arguments exist.\nApplying this causal grammar over Gigaword and Simple English Wikipedia produced 815,233 causal tuples, as summarized in Table 1. As bootstrapping methods are typically noisy, we manually evaluated the quality of approximately 250 of these pairs selected at random. Of the tuples evaluated, approximately 44% contained some amount of noise. For example, from the sentence Except for Springer\u2019s show, which still relies heavily on confrontational topics that lead to fistfights virtually every day..., while ideally we would only extract (confrontational topics \u2192 fistfights), instead we extract the tuple (show which still relies heavily on confrontational topics \u2192 fistfights virtually every day), which contains a large amount of noise: show, relies, heavily, etc. This finding prompted our noise-aware model described at the end of Section 5.\n2The outgoing dependency links from the nouns which we followed were: nn, amod, advmod, ccmod, dobj, prep of, prep with, prep for, prep into, prep on, prep to, and prep in.\n3The verbs we excluded were: cause, result, lead, create."}, {"heading": "5 Models", "text": "We use the extracted causal tuples to train three distinct distributional similarity models that explicitly capture causality.\nCausal Embedding Model (cEmbed): The first distributional similarity model we use is based on the skip-gram word-embedding algorithm of Mikolov et al. (2013), which has been shown to improve a variety of language processing tasks including QA (Yih et al., 2013; Fried et al., 2015). In particular, we use the variant implemented by Levy and Goldberg (2014) which modifies the original algorithm to use an arbitrary, rather than linear, context. Our novel contribution is to make this context taskspecific: intuitively, the context of a cause is its effect. Further, these contexts are generated from tuples that are themselves bootstrapped, which minimizes the amount of supervision necessary.\nThe Levy and Goldberg model trains using singleword pairs, while our CMs could be composed of multiple words. For this reason, we decompose each cause\u2013effect tuple, (CMc, CMe), such that each word wc \u2208 CMc is paired with each word we \u2208 CMe.\nAfter filtering the extracted cause-effect tuples for stop words and retaining only nouns, verbs, and adjectives, we generated over 3.6M (wc, we) wordpairs4 from the approximately 800K causal tuples.\nThe model learns two embedding vectors for each word, one for when the word serves as a target word and another for when the word serves as a context word. Here, since the relation of interest is inherently directional, both sets of embeddings are meaningful, and so we make use of both \u2013 the target vectors encode the effects of given causes, whereas the context vectors capture the causes of the corresponding effects.\nCausal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al., 2015).\n4For all models proposed in this section we used lemmas rather than words.\nTo verify these observations in our context, we train an alignment model that \u201ctranslates\u201d causes (i.e., the \u201csource language\u201d) into effects (i.e., the \u201cdestination language\u201d), using our cause\u2013effect tuples. This is done using IBM Model 1 (Brown et al., 1993) and GIZA++ (Och and Ney, 2003).\nCausal Convolutional Neural Network Model (cCNN): Each of the previous models have at their root a bag-of-words representation, which is a simplification of the causality task. To address this potential limitation, we additionally trained a convolutional neural network (CNN) which operates over variable-length texts, and maintains distinct embeddings for causes and effects. The architecture of this approach is shown in Figure 1, and consists of two sub-networks (one for cause text and one for effect text), each of which begins by converting the corresponding text into 50-dimensional embeddings. These are then fed to a convolutional layer,5 which is followed by a max-pooling layer of equal length. Then, these top sub-network layers, which can be thought of as a type of phrasal embedding, are merged by taking their cosine similarity. Finally, this cosine similarity is normalized by feeding it into a dense layer with a single node which has a softplus activation. In designing our CNN, we attempted to minimize architectural and hyperparameter tuning by taking inspiration from Iyyer et al. (2015), preferring simpler architectures. We train the network using a binary cross entropy objective function and the Adam optimizer (Kingma and Ba, 2014), using the Keras library (Chollet, 2015) operating over Theano (Theano Development Team, 2016), a popular deep-learning framework.6\n5The convolutional layer contained 100 filters, had a filter length of 2 (i.e., capturing bigram information), and an inner ReLU activation.\n6We also experimented with an equivalent architecture where the sub-networks are implemented using long short-\nNoise-aware Causal Embedding Model (cEmbedNoise): We designed a variant of our cEmbed approach to address the potential impact of the noise introduced by our bootstrapping method. While training, we weigh the causal tuples by the likelihood that they are truly causal, which we approximate with pointwise mutual information (PMI). For this, we first score the tuples by their causal PMI and then scale these scores by the overall frequency of the tuple (Riloff, 1996), to account for the PMI bias toward low-frequency items. That is, the score S of a tuple, t, is computed as:\nS(t) = log p(t|causal)\np(t) \u2217 log(freq(t)) (1)\nWe then discretize these scores into five quantiles, ascribing a linearly decreasing weight during training to datums in lower scoring quantiles."}, {"heading": "6 Direct Evaluation: Ranking Word Pairs", "text": "We begin the assessment of our models with a direct evaluation to determine whether or not the proposed approaches capture causality better than generalpurpose word embeddings and whether their robustness improves upon a simple database look-up. For this evaluation, we follow the protocol of Levy and Goldberg (2014). In particular, we create a collection of word pairs, half of which are causally related, with the other half consisting of other relations. These pairs are then ranked by our models and several baselines, with the goal of ranking the causal pairs above the others. The embedding models rank the pairs using the cosine similarity between the target vector for the causal word and the context vector of the effect word. The alignment model ranks pairs using the probability P (Effect|Cause) given by IBM Model 1, and the CNN ranks pairs by the value of the output returned by the network."}, {"heading": "6.1 Data", "text": "In order to avoid bias towards our extraction methods, we evaluate our models on an external set of\nterm memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), and found that they consistently under-perform this CNN architecture. Our conjecture is that CNNs perform better because LSTMs are more sensitive to overall word order than CNNs, which capture only local contexts, and we have relatively little training data, which prevents the LSTMs from generalizing well.\nword pairs drawn from the SemEval 2010 Task 8 (Hendrickx et al., 2009), originally a multi-way classification of semantic relations between nominals. We used a total of 1730 nominal pairs, 865 of which were from the Cause-Effect relation (e.g., (dancing \u2192 happiness)) and an equal number which were randomly selected from the other eight relations (e.g., (juice \u2192 grapefruit), from the Entity-Origin relation). This set was then randomly divided into equally-sized development and test partitions."}, {"heading": "6.2 Baselines", "text": "We compared our distributional similarity models against three baselines:\nVanilla Embeddings Model (vEmbed): a standard word2vec model trained with the skip-gram algorithm and a sliding window of 5, using the original texts from which our causal pairs were extracted.7 As with the cEmbed model, SemEval pairs were ranked using the cosine similarity between the vector representations of their arguments.\nLook-up Baseline: a given SemEval pair was ranked by the number of times it appeared in our extracted cause-effect tuples.\nRandom: pairs were randomly shuffled."}, {"heading": "6.3 Results", "text": "Figure 2 shows the precision-recall (PR) curve for each of the models and baselines. As expected, the causal models are better able to rank causal\n7All embedding models analyzed here, including this baseline and our causal variants, produced embedding vectors of 200 dimensions.\npairs than the vanilla embedding baseline (vEmbed), which, in turn, outperforms the random baseline. Our look-up baseline, which ranks pairs by their frequency in our causal database, shows a high precision for this task, but has coverage for only 35% of the causal SemEval pairs.\nSome models perform better on the low-recall portion of the curve (e.g., the look-up baseline and cCNN), while the embedding and alignment models have a higher and more consistent performance across the PR curve. We hypothesize that models that better balance precision and recall will perform better in a real-world QA task, which may need to access a given causal relation through a variety of lexical patterns or variations. We empirically validate this observation in Section 7.\nThe PR curve for the causal embeddings shows an atypical dip at low-recall. To examine this, we analyzed its top-ranked 15% of SemEval pairs, and found that incorrectly ranked pairs were not found in the database of causal tuples. Instead, these incorrect rankings were largely driven by low frequency words whose embeddings could not be robustly estimated due to lack of direct evidence. Because this sparsity is partially driven by directionality, we implemented a bidirectional embedding model (cEmbedBi) that (a) trains a second embedding model by reversing the input (effects as targets, causes as contexts), and (b) ranks pairs by the average of the scores returned by these two unidirectional causal embedding models. Specifically, the final bidirectional score of the pair, (e1, e2), where e1 is the can-\ndidate cause and e2 is the candidate effect, is:\nsbi(e1, e2) = 1 2(sc\u2192e(e1, e2) + se\u2192c(e2, e1)) (2)\nwhere sc\u2192e is the score given by the original causal embeddings, i.e., from cause to effect, and se\u2192c is the score given by the reversed-input causal embeddings, i.e., from effect to cause.\nAs Figure 2 shows, the bidirectional embedding variants consistently outperform their unidirectional counterparts. All in all, the best overall model is cEmbedBiNoise, which is both bidirectional and incorporates the noise handling approach from Section 5. This model substantially improves performance in the low-recall portion of the curve, while also showing strong performance across the curve."}, {"heading": "7 Indirect Evaluation: QA Task", "text": "The main objective of our work is to investigate the impact of a customized causal embedding model for QA. Following our direct evaluation, which solely evaluated the degree to which our models directly encode causality, here we evaluate each of our proposed causal models in terms of their contribution to a downstream real-world QA task.\nOur QA system uses a standard reranking approach (Jansen et al., 2014). In this architecture, the candidate answers are initially extracted and ranked using a shallow candidate retrieval (CR) component that uses solely information retrieval techniques, then they are re-ranked using a \u201clearning to rank\u201d approach. In particular, we used SVM rank8, a Support Vector Machines classifier adapted for ranking, and re-ranked the candidate answers with a set of features derived from both the initial CR score and the models we have introduced. For our model combinations (see Table 2), the feature set includes the CR score and the features from each of the models in the combination."}, {"heading": "7.1 Data", "text": "We evaluate on a set of causal questions extracted from the Yahoo! Answers corpus9 with simple surface patterns such as What causes ... and What\n8 http://www.cs.cornell.edu/people/tj/ svm_light/svm_rank.html\n9Freely available through Yahoo!\u2019s Webscope program (research-data-requests@yahoo-inc.com)\nis the result of ...10. We extracted a total of 3031 questions, each with at least four candidate answers, and we evaluated performance using five-fold crossvalidation, with three folds for training, one for development, and one for testing."}, {"heading": "7.2 Models and Features", "text": "We evaluate the contribution of the bidirectional and noise-aware causal embedding models (cEmbedBi, and cEmbedBiNoise) as well as the causal alignment model (cAlign) and the causal CNN (cCNN). These models are compared against three baselines: the vanilla embeddings (vEmbed), the lookup baseline (LU), and additionally a vanilla alignment model (vAlign) which is trained over 65k question-answer pairs from Yahoo! Answers.\nThe features11 we use for the various models are:\nEmbedding model features: For both our vanilla and causal embedding models, we use the same set of features as Fried et al. (2015): the maximum, minimum, and average pairwise cosine similarity between question and answer words, as well as the overall similarity between the composite question and answer vectors. When using the causal embeddings, since the relation is directed, we first determine whether the question text is the cause or the effect12, which in turn determines which embeddings to use for the question text and which to use for the candidate answer texts. For example, in a question such as \u201dWhat causes X?\u201d, since X is the effect, all cosine similarities would be found using the effect vectors for the question words and the cause vectors for the answer candidate words.\nAlignment model features: We use the same global alignment probability, p(Q|A) of Surdeanu et al. (2011). In our causal alignment model, we adapt this to causality as p(Effect|Cause), and again we first determine the direction of the causal relation implied in the question. We include the additional undirected alignment features based on\n10We lightly filtered these with stop words to remove noncausal questions, such as those based on math problems and the results of sporting events. Our dataset will be freely available, conditioned on users having obtained the Webscope license.\n11Due to the variety of features used, each feature described here is independently normalized to lie between 0.0 and 1.0.\n12We do this through the use of simple regular expressions, e.g., \u201d\u02c6 [Ww]hat ([a-z]+ ){0,3}cause.+\u201d\nJensen-Shannon distance, proposed more recently by Fried et al. (2015), in our vanilla alignment model. However, due to the directionality inherent in causality, they do not apply to our causal model so there we omit them.\nLook-up feature: For the look-up baseline we count the number of times words from the question and answer appear together in our database of extracted causal pairs, once again after determining the directionality of the questions. If the total number of matches is over a threshold13, we consider the causal relation to be established and give the candidate answer a score of 1; or a score of 0, otherwise."}, {"heading": "7.3 Results", "text": "The overall results are summarized in Table 2. Lines 1\u20135 in the table show that each of our baselines performed better than CR by itself, except for vAlign, suggesting that the vanilla alignment model does not generate accurate predictions for causal questions.\n13Empirically determined to be 100 matches. Note that using this threshold performed better than simply using the total number of matches.\nThe strongest baseline was CR + vEmbed (line 3), the vanilla embeddings trained over Gigaword, at 34.6% P@1. For this reason, we consider this to be the baseline to \u201cbeat\u201d, and perform statistical significance of all proposed models with respect to it.\nIndividually, the cEmbedBi model is the best performing of the causal models. While the performance of cAlign in the direct evaluation was comparable to that of cEmbedBi, here it performs far worse (line 6 vs 8), suggesting that the robustness of embeddings is helpful in QA. Notably, despite the strong performance of the cCNN in the low-recall portion of the PR curve in the direct evaluation, here the model performs poorly (line 9).\nNo individual causal model outperforms the strong vanilla embedding baseline (line 3), likely owing to the reduction in generality inherent to building task-specific QA models. However, comparing lines 6\u20139 vs. 10\u201314 shows that the vanilla and causal models are capturing different and complementary kinds of knowledge (i.e., causality vs. association through distributional similarity), and are able to be combined to increase overall task performance (lines 10\u201312). These results highlight that QA is a complex task, where solving methods need to address the many distinct information needs in question sets, including both causal and direct association relations. This contrasts with the direct evaluation, which focuses strictly on causality, and where the vanilla embedding baseline performs near chance. This observation highlights one weakness of word similarity tasks: their narrow focus may not directly translate to estimating their utility in realworld NLP applications.\nAdding in the lookup baseline (LU) to the bestperforming causal model does not improve performance (compare lines 10 and 12), suggesting that the bidirectional causal embeddings subsume the contribution of the LU model. cEmbedBi (line 10) also performs better than cEmbedBiNoise (line 11). We conjecture that the \u201cnoise\u201d filtered out by cEmbedBiNoise contains distributional similarity information, which is useful for the QA task. cEmbedBi vastly outperforms cCNN (line 14), suggesting that strong overall performance across the precisionrecall curve better translates to the QA task. We hypothesize that the low cCNN performance is caused by insufficient training data, preventing the CNN ar-\nchitecture from generalizing well. Our best performing overall model combines both variants of the causal embedding model (cEmbedBi and cEmbedBiNoise), reaching a P@1 of 37.3%, which shows a 7.7% relative improvement over the strong CR + vEmbed baseline."}, {"heading": "7.4 Error Analysis", "text": "We performed an error analysis to gain more insight into our model as well as the source of the remaining errors. For simplicity, we used the combination model CR + vEmbed + cEmbedBi. Examining the model\u2019s learned feature weights, we found that the vanilla overall similarity feature had the highest weight, followed by the causal overall similarity and causal maximum similarity features. This indicates that even in causal question answering, the overall topical similarity between question and answer is still useful and complementary to the causal similarity features.\nTo determine sources of error, we randomly selected 20 questions that were incorrectly answered and analyzed them according to the categories shown in Table 3. We found that for 70% of the questions, the answer chosen by our system was as good as or better than the gold answer, often the case with community question answering datasets.\nAdditionally, while the maximum causal similarity feature is useful, it can be misleading due to embedding noise, low-frequency words, and even the bag-of-words nature of the model (35% of the incorrect questions). For example, in the question What are the effects of growing up with an older sibling who is better than you at everything?, the model chose the answer ...You are you and they are them - you will be better and different at other things... largely because of the high causal similarity between (grow\u2192 better). While this could arguably be help-\nful in another context, here it is irrelevant, suggesting that future improvement could come from models that better incorporate textual dependencies."}, {"heading": "8 Conclusion", "text": "We presented a framework for creating customized embeddings tailored to the information need of causal questions. We trained three popular models (embedding, alignment, and CNN) using causal tuples extracted with minimal supervision by bootstrapping cause-effect pairs from free text, and evaluated their performance both directly (i.e., the degree to which they capture causality), and indirectly (i.e., their real-world utility on a high-level question answering task).\nWe showed that models that incorporate a knowledge of causality perform best for both tasks. Our analysis suggests that the models that perform best in the real-world QA task are those that have consistent performance across the precision-recall curve in the direct evaluation. In QA, where the vocabulary is much larger, precision must be balanced with highrecall, and this is best achieved by our causal embedding model. Additionally, we showed that vanilla and causal embedding models address different information needs of questions, and can be combined to improve performance.\nExtending this work beyond causality, we hypothesize that additional embedding spaces customized to the different information needs of questions would allow for robust performance over a larger variety of questions, and that these customized embedding models should be evaluated both directly and indirectly to accurately characterize their performance.\nResources\nAll code and resources needed to reproduce this work are available at http://clulab.cs. arizona.edu/data/emnlp2016-causal/."}, {"heading": "Acknowledgments", "text": "We thank the Allen Institute for Artificial Intelligence for funding this work. Additionally, this work was partially funded by the Defense Advanced Research Projects Agency (DARPA) Big Mechanism program under ARO contract W911NF-14-1-0395."}], "references": [{"title": "Bridging the lexical chasm: Statistical approaches to answer finding", "author": ["Berger et al.2000] A. Berger", "R. Caruana", "D. Cohn", "D. Freytag", "V. Mittal"], "venue": "In Proc. of the 23rd Annual International ACM SIGIR Conference on Research & Development on Informa-", "citeRegEx": "Berger et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Berger et al\\.", "year": 2000}, {"title": "Question answering with subgraph embeddings", "author": ["Bordes et al.2014] A. Bordes", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1406.3676", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] P.F. Brown", "S.A. Della Pietra", "V.J. Della Pietra", "R.L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] D. Chen", "C.D. Manning"], "venue": "In Proc. of the Conferenc on Empirical Methods for Natural Language Processing (EMNLP)", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Keras. https:// github.com/fchollet/keras", "author": ["F. Chollet"], "venue": null, "citeRegEx": "Chollet.,? \\Q2015\\E", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "A study of the knowledge base requirements for passing an elementary science test", "author": ["Clark et al.2013] P. Clark", "P. Harrison", "N. Balasubramanian"], "venue": "In Proc. of the 2013 workshop on Automated Knowledge Base Construction (AKBC),", "citeRegEx": "Clark et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2013}, {"title": "A lightweight tool for automatically extracting causal relationships from text", "author": ["Cole et al.2005] S.V. Cole", "M.D. Royal", "M.G. Valtorta", "M.N. Huhns", "J.B. Bowles"], "venue": "In SoutheastCon,", "citeRegEx": "Cole et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cole et al\\.", "year": 2005}, {"title": "Minimally supervised event causality identification", "author": ["Do et al.2011] Q.X. Do", "Y.S. Chan", "D. Roth"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Do et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Do et al\\.", "year": 2011}, {"title": "A noisy-channel approach to question answering", "author": ["Echihabi", "Marcu2003] A. Echihabi", "D. Marcu"], "venue": "In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Echihabi et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Echihabi et al\\.", "year": 2003}, {"title": "Search needs a shake-up", "author": ["O. Etzioni"], "venue": "Nature,", "citeRegEx": "Etzioni.,? \\Q2011\\E", "shortCiteRegEx": "Etzioni.", "year": 2011}, {"title": "Problems with evaluation of word embeddings using word similarity tasks. arXiv preprint arXiv:1605.02276", "author": ["Faruqui et al.2016] M. Faruqui", "Y. Tsvetkov", "R. Rastogi", "C. Dyer"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Building Watson: An overview of the DeepQA project", "author": ["J.W. Murdock", "E. Nyberg", "J. Prager"], "venue": "AI magazine,", "citeRegEx": "Murdock et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Murdock et al\\.", "year": 2010}, {"title": "Semantic role labeling with neural network factors", "author": ["O. T\u00e4ckstr\u00f6m", "K. Ganchev", "D. Das"], "venue": "In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "FitzGerald et al\\.,? \\Q2015\\E", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Higher-order lexical semantic models for non-factoid answer reranking", "author": ["Fried et al.2015] D. Fried", "P. Jansen", "G. Hahn-Powell", "M. Surdeanu", "P. Clark"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Fried et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fried et al\\.", "year": 2015}, {"title": "Text mining for causal relations", "author": ["Girju", "Moldovan2002] R. Girju", "D.I. Moldovan"], "venue": "In FLAIRS Conference,", "citeRegEx": "Girju et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Girju et al\\.", "year": 2002}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["M.A. Hearst"], "venue": "In Proc. of the 14th conference on Computational linguistics (COLING),", "citeRegEx": "Hearst.,? \\Q1992\\E", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals", "author": ["S.N. Kim", "Z. Kozareva", "P. Nakov", "D. \u00d3 S\u00e9aghdha", "S. Pad\u00f3", "M. Pennacchiotti", "L. Romano", "S. Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Discourse complements lexical semantics for non-factoid answer reranking", "author": ["Jansen et al.2014] P. Jansen", "M. Surdeanu", "P. Clark"], "venue": "In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Jansen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jansen et al\\.", "year": 2014}, {"title": "Automatic extraction of cause-effect information from newspaper text without knowledge-based inferencing", "author": ["Khoo et al.1998] C.S.G. Khoo", "J. Kornfilt", "R.N. Oddy", "S.H. Myaeng"], "venue": "Literary and Linguistic Computing,", "citeRegEx": "Khoo et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Khoo et al\\.", "year": 1998}, {"title": "Specializing word embeddings for similarity or relatedness", "author": ["Kiela et al.2015] D. Kiela", "F. Hill", "S. Clark"], "venue": "In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Goldberg2014] O. Levy", "Y. Goldberg"], "venue": "In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Levy et al.2015] O. Levy", "S. Remus", "C. Biemann", "I. Dagan", "I. Ramat-Gan"], "venue": "In Proc. of the Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013] T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Annotated gigaword", "author": ["Napoles et al.2012] C. Napoles", "M. Gormley", "B. Van Durme"], "venue": "In Proc. of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,", "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] F.J. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Whyquestion answering using intra-and inter-sentential causal relations", "author": ["Oh et al.2013] J.-H. Oh", "K. Torisawa", "C. Hashimoto", "M. Sano", "S. De Saeger", "K. Ohtake"], "venue": "In The 51st Annual Meeting of the Association", "citeRegEx": "Oh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2013}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Riedel et al.2013] S. Riedel", "L. Yao", "A. McCallum", "B.M. Marlin"], "venue": "In Proc. of Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Statistical machine translation for query expansion in answer retrieval", "author": ["Riezler et al.2007] S. Riezler", "A. Vasserman", "I. Tsochantaridis", "V. Mittal", "Y. Liu"], "venue": "In Proc. of the 45th Annual Meeting of the Association", "citeRegEx": "Riezler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2007}, {"title": "Automatically generating extraction patterns from untagged text", "author": ["E. Riloff"], "venue": "In Proc. of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Riloff.,? \\Q1996\\E", "shortCiteRegEx": "Riloff.", "year": 1996}, {"title": "Spinning straw into gold", "author": ["Sharp et al.2015] R. Sharp", "P. Jansen", "M. Surdeanu", "P. Clark"], "venue": "In Proc. of the Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies", "citeRegEx": "Sharp et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sharp et al\\.", "year": 2015}, {"title": "Automatic question answering using the web: Beyond the factoid", "author": ["Soricut", "Brill2006] R. Soricut", "E. Brill"], "venue": "Journal of Information Retrieval - Special Issue on Web Information Retrieval,", "citeRegEx": "Soricut et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2006}, {"title": "Learning to rank answers to non-factoid questions from web collections", "author": ["Surdeanu et al.2011] M. Surdeanu", "M. Ciaramita", "H. Zaragoza"], "venue": null, "citeRegEx": "Surdeanu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2011}, {"title": "Odin\u2019s runes: A rule language for information extraction", "author": ["G. Hahn-Powell", "M. Surdeanu"], "venue": "In Proc. of the 10th International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "ValenzuelaEsc\u00e1rcega et al\\.,? \\Q2016\\E", "shortCiteRegEx": "ValenzuelaEsc\u00e1rcega et al\\.", "year": 2016}, {"title": "Distributed representations for unsupervised semantic role labeling", "author": ["Woodsend", "Lapata2015] K. Woodsend", "M. Lapata"], "venue": "In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Woodsend et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2015}, {"title": "Multi level causal relation identification using extended features", "author": ["Yang", "Mao2014] X. Yang", "K. Mao"], "venue": "Expert Systems with Applications,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Joint relational embeddings for knowledge-based question answering", "author": ["Yang et al.2014] M.-C. Yang", "N. Duan", "M. Zhou", "H.-C. Rim"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Semi-markov phrasebased monolingual alignment", "author": ["Yao et al.2013] X. Yao", "B. Van Durme", "C. CallisonBurch", "P. Clark"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih et al.2013] W. Yih", "M. Chang", "C. Meek", "A. Pastusiak"], "venue": "In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": ", finding short answers to natural language questions, is one of the most important but challenging tasks on the road towards natural language understanding (Etzioni, 2011).", "startOffset": 157, "endOffset": 172}, {"referenceID": 41, "context": "such as word embeddings (Yih et al., 2013; Jansen et al., 2014; Fried et al., 2015).", "startOffset": 24, "endOffset": 83}, {"referenceID": 19, "context": "such as word embeddings (Yih et al., 2013; Jansen et al., 2014; Fried et al., 2015).", "startOffset": 24, "endOffset": 83}, {"referenceID": 13, "context": "such as word embeddings (Yih et al., 2013; Jansen et al., 2014; Fried et al., 2015).", "startOffset": 24, "endOffset": 83}, {"referenceID": 5, "context": "in any given question set, and that are best addressed individually (Chu-Carroll et al., 2004; Ferrucci et al., 2010; Clark et al., 2013).", "startOffset": 68, "endOffset": 137}, {"referenceID": 26, "context": "Levy and Goldberg (2014) have modified the algorithm of Mikolov et al. (2013) to use an arbitrary, rather than linear, context.", "startOffset": 56, "endOffset": 78}, {"referenceID": 10, "context": "uations (Faruqui et al., 2016): we show that they have limited coverage and may align poorly with real-world tasks.", "startOffset": 8, "endOffset": 30}, {"referenceID": 13, "context": "There has been a vast body of work which demonstrates that word embeddings derived from distributional similarity are useful in many tasks, including question answering \u2013 see inter alia (Fried et al., 2015; Yih et al., 2013).", "startOffset": 186, "endOffset": 224}, {"referenceID": 41, "context": "There has been a vast body of work which demonstrates that word embeddings derived from distributional similarity are useful in many tasks, including question answering \u2013 see inter alia (Fried et al., 2015; Yih et al., 2013).", "startOffset": 186, "endOffset": 224}, {"referenceID": 13, "context": "There has been a vast body of work which demonstrates that word embeddings derived from distributional similarity are useful in many tasks, including question answering \u2013 see inter alia (Fried et al., 2015; Yih et al., 2013). However, Levy and Goldberg (2015) note that there are limitations on the type of semantic knowledge which is encoded in these general-purpose similarity embeddings.", "startOffset": 187, "endOffset": 260}, {"referenceID": 12, "context": "Customized embeddings have been created for a variety of tasks, including semantic role labeling (FitzGerald et al., 2015; Woodsend and Lapata, 2015), and binary relation extraction (Riedel et al.", "startOffset": 97, "endOffset": 149}, {"referenceID": 30, "context": ", 2015; Woodsend and Lapata, 2015), and binary relation extraction (Riedel et al., 2013).", "startOffset": 67, "endOffset": 88}, {"referenceID": 1, "context": "In QA, embeddings have been customized to have question words that are close to either their answer words (Bordes et al., 2014), or to structured knowl-", "startOffset": 106, "endOffset": 127}, {"referenceID": 38, "context": "edge base entries (Yang et al., 2014).", "startOffset": 18, "endOffset": 37}, {"referenceID": 21, "context": "Additionally, embeddings have been customized to distinguish functional similarity from relatedness (Levy and Goldberg, 2014; Kiela et al., 2015).", "startOffset": 100, "endOffset": 145}, {"referenceID": 10, "context": "Recently, Faruqui et al.(2016) discussed issues surrounding the evaluation of similarity word embeddings, including the lack of correlation between their performance on word-similarity tasks and \u201cdownstream\u201d or real-world tasks like QA, text classification, etc.", "startOffset": 10, "endOffset": 31}, {"referenceID": 15, "context": "(2002) use modified Hearst patterns (Hearst, 1992) to extract a large number of potential cause-effect tuples, where both causes and effects must be nouns.", "startOffset": 36, "endOffset": 50}, {"referenceID": 12, "context": "With respect to extracting causal relations from text, Girju et al. (2002) use modified Hearst patterns (Hearst, 1992) to extract a large number of potential cause-effect tuples, where both causes and effects must be nouns.", "startOffset": 55, "endOffset": 75}, {"referenceID": 6, "context": "However, Cole et al. (2005) show that these nominal-based causal relations account for a relatively small percentage of all causal relations, and for this reason, (Yang and Mao, 2014) allow for more elaborate argument structures in their causal extraction by identifying verbs, and then following the syntactic subtree of the verbal arguments to construct their candidate causes and effects.", "startOffset": 9, "endOffset": 28}, {"referenceID": 6, "context": "However, Cole et al. (2005) show that these nominal-based causal relations account for a relatively small percentage of all causal relations, and for this reason, (Yang and Mao, 2014) allow for more elaborate argument structures in their causal extraction by identifying verbs, and then following the syntactic subtree of the verbal arguments to construct their candidate causes and effects. Additionally, Do et al. (2011) observe that nouns as well as verbs can signal causality.", "startOffset": 9, "endOffset": 423}, {"referenceID": 33, "context": "Because the success of embedding models depends on large training datasets (Sharp et al., 2015), and such datasets do not exist for open-domain causality, we opted to bootstrap a large number of cause-effect pairs from a small set of patterns.", "startOffset": 75, "endOffset": 95}, {"referenceID": 27, "context": "(1) Pre-processing: Much of the text we use to extract causal relation tuples comes from the Annotated Gigaword (Napoles et al., 2012).", "startOffset": 112, "endOffset": 134}, {"referenceID": 25, "context": "We additionally use text from the Simple English Wikipedia1, which we processed using the Stanford CoreNLP toolkit (Manning et al., 2014) and the dependency parser of Chen and Manning (2014).", "startOffset": 115, "endOffset": 137}, {"referenceID": 25, "context": "We additionally use text from the Simple English Wikipedia1, which we processed using the Stanford CoreNLP toolkit (Manning et al., 2014) and the dependency parser of Chen and Manning (2014).", "startOffset": 116, "endOffset": 191}, {"referenceID": 20, "context": "Different patterns have varying probabilities of signaling causation (Khoo et al., 1998).", "startOffset": 69, "endOffset": 88}, {"referenceID": 41, "context": "(2013), which has been shown to improve a variety of language processing tasks including QA (Yih et al., 2013; Fried et al., 2015).", "startOffset": 92, "endOffset": 130}, {"referenceID": 13, "context": "(2013), which has been shown to improve a variety of language processing tasks including QA (Yih et al., 2013; Fried et al., 2015).", "startOffset": 92, "endOffset": 130}, {"referenceID": 25, "context": "Causal Embedding Model (cEmbed): The first distributional similarity model we use is based on the skip-gram word-embedding algorithm of Mikolov et al. (2013), which has been shown to improve a variety of language processing tasks including QA (Yih et al.", "startOffset": 136, "endOffset": 158}, {"referenceID": 13, "context": ", 2013; Fried et al., 2015). In particular, we use the variant implemented by Levy and Goldberg (2014) which modifies the original algorithm to use an arbitrary, rather than linear, context.", "startOffset": 8, "endOffset": 103}, {"referenceID": 0, "context": "Causal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al.", "startOffset": 118, "endOffset": 253}, {"referenceID": 31, "context": "Causal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al.", "startOffset": 118, "endOffset": 253}, {"referenceID": 35, "context": "Causal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al.", "startOffset": 118, "endOffset": 253}, {"referenceID": 40, "context": "Causal Alignment Model (cAlign): Monolingual alignment (or translation) models have been shown to be successful in QA (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2011; Yao et al., 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al.", "startOffset": 118, "endOffset": 253}, {"referenceID": 33, "context": ", 2013), and recent work has shown that they can be successfully trained with less data than embedding models (Sharp et al., 2015).", "startOffset": 110, "endOffset": 130}, {"referenceID": 4, "context": "We train the network using a binary cross entropy objective function and the Adam optimizer (Kingma and Ba, 2014), using the Keras library (Chollet, 2015) operating over Theano (Theano Development Team, 2016), a popular deep-learning framework.", "startOffset": 139, "endOffset": 154}, {"referenceID": 17, "context": "In designing our CNN, we attempted to minimize architectural and hyperparameter tuning by taking inspiration from Iyyer et al. (2015), preferring simpler architectures.", "startOffset": 114, "endOffset": 134}, {"referenceID": 32, "context": "For this, we first score the tuples by their causal PMI and then scale these scores by the overall frequency of the tuple (Riloff, 1996), to account for the PMI bias toward low-frequency items.", "startOffset": 122, "endOffset": 136}, {"referenceID": 16, "context": "word pairs drawn from the SemEval 2010 Task 8 (Hendrickx et al., 2009), originally a multi-way clas-", "startOffset": 46, "endOffset": 70}, {"referenceID": 19, "context": "Our QA system uses a standard reranking approach (Jansen et al., 2014).", "startOffset": 49, "endOffset": 70}, {"referenceID": 13, "context": "Embedding model features: For both our vanilla and causal embedding models, we use the same set of features as Fried et al. (2015): the maximum,", "startOffset": 111, "endOffset": 131}, {"referenceID": 35, "context": "Alignment model features: We use the same global alignment probability, p(Q|A) of Surdeanu et al. (2011). In our causal alignment model, we adapt this to causality as p(Effect|Cause), and again we first determine the direction of the causal relation implied in the question.", "startOffset": 82, "endOffset": 105}, {"referenceID": 13, "context": "Jensen-Shannon distance, proposed more recently by Fried et al. (2015), in our vanilla alignment", "startOffset": 51, "endOffset": 71}], "year": 2016, "abstractText": "A common model for question answering (QA) is that a good answer is one that is closely related to the question, where relatedness is often determined using generalpurpose lexical models such as word embeddings. We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-specific embeddings. With causality as a use case, we implement this insight in three steps. First, we generate causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns. Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect. Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embeddings. We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo! Answers. We show that explicitly modeling causality improves performance in both tasks. In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative).", "creator": "LaTeX with hyperref package"}}}