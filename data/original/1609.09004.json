{"id": "1609.09004", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Byte-based Language Identification with Deep Convolutional Networks", "abstract": "We report on our system for the shared task on discriminating between similar languages (DSL 2016). The system uses only byte representations in a deep residual network (ResNet). The system, named ResIdent, is trained only on the data released with the task (closed training). We obtain 84.88% accuracy on subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A large difference in accuracy on development data can be observed with relatively minor changes in our network's architecture and hyperparameters. We therefore expect fine-tuning of these parameters to yield higher accuracies.", "histories": [["v1", "Wed, 28 Sep 2016 16:51:56 GMT  (146kb,D)", "https://arxiv.org/abs/1609.09004v1", "6 pages. arXiv admin note: text overlap witharXiv:1609.07053"], ["v2", "Fri, 28 Oct 2016 15:28:29 GMT  (74kb,D)", "http://arxiv.org/abs/1609.09004v2", "7 pages. Adapted reviewer comments. arXiv admin note: text overlap witharXiv:1609.07053"]], "COMMENTS": "6 pages. arXiv admin note: text overlap witharXiv:1609.07053", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["johannes bjerva"], "accepted": false, "id": "1609.09004"}, "pdf": {"name": "1609.09004.pdf", "metadata": {"source": "CRF", "title": "Byte-based Language Identification with Deep Convolutional Networks", "authors": ["Johannes Bjerva"], "emails": ["j.bjerva@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "Language identification is an unsolved problem, certainly in the context of discriminating between very similar languages (Baldwin and Lui, 2010). This problem is tackled in the Discriminating between Similar Languages (DSL) series of shared tasks (Zampieri et al., 2014; Zampieri et al., 2015). Most successful approaches to the DSL shared task in previous years have relied on settings containing ensembles of classifiers (Goutte et al., 2016). These classifiers often use various combinations of features, mostly based on word, character, and/or byte n-grams (see, e.g., Cavnar et al. (1994), Lui and Baldwin (2012)).\nWe are interested in exploring a single methodological aspect in the current edition of this shared task (Malmasi et al., 2016). We aim to investigate whether reasonable results for this task could be obtained by applying recently emerged neural network architectures, coupled with sub-token input representations. To address this question, we explore convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Deep residual networks (ResNets) are a recent building block for CNNs which have yielded promising results in, e.g., image classification tasks (He et al., 2015; He et al., 2016). ResNets are constructed by stacking so-called residual units. These units can be viewed as a series of convolutional layers with a \u2018shortcut\u2019 which facilitates signal propagation in the neural network. This, in turn, allows for training deeper networks more easily (He et al., 2016). In Natural Language Processing (NLP), ResNets have shown state-of-the-art performance for Semantic and Part-of-Speech tagging (Bjerva et al., 2016). However, no previous work has attempted to apply ResNets to language identification."}, {"heading": "2 Method", "text": "Several previous approaches in the DSL shared tasks have formulated the task as a two-step classification, first identifying the language group, and then the specific language (Zampieri et al., 2015). Instead of taking this approach, we formulate the task as a multi-class classification problem, with each language / dialect representing a separate class. Our system is a deep neural network consisting of a bidirectional Gated Recurrent Unit (GRU) network at the upper level, and a Deep Residual Network (ResNet) at the lower level (Figure 1). The inputs of our system are byte-level representations of each input sentence, with byte embeddings which are learnt during training. Using byte-level representations differs from character-level representations in that UTF-8 encodes non-ascii symbols with more than one byte, which potentially allows for more disambiguating power. A concrete example can be found when considering the relatively similar languages Norwegian and Swedish. Here, there are two pairs of letters which are interchangeable: where Swedish uses \u2018a\u0308\u2019 (C3 A4) and \u2018o\u0308\u2019 (C3 B6), Norwegian uses \u2018\u00e6\u2019 (C3 A6) and\nar X\niv :1\n60 9.\n09 00\n4v 2\n[ cs\n.C L\n] 2\n8 O\nct 2\n01 6\n\u2018\u00f8\u2019 (C3 B8). Hence, using the lower-level byte representation, we allow the model to take advantage of the first shared byte between these characters. The architecture used in this work is based on the sequence-to-sequence labelling architecture used in Bjerva et al. (2016), modified for the task of language identification. Our system is implemented in Keras using the Tensorflow backend (Chollet, 2015; Abadi et al., 2016)."}, {"heading": "2.1 Gated Recurrent Unit Networks", "text": "GRUs (Cho et al., 2014) are a recently introduced variant of RNNs, and are designed to prevent vanishing gradients, thus being able to cope with longer input sequences than vanilla RNNs. GRUs are similar to the more commonly-used Long Short-Term Memory networks (LSTMs), both in purpose and implementation (Chung et al., 2014). A bi-directional GRU makes both forward and backward passes over sequences, and can therefore use both preceding and succeeding contexts to predict a tag (Graves and Schmidhuber, 2005; Goldberg, 2015). Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS and semantic tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016; Bjerva et al., 2016)."}, {"heading": "2.2 Deep Residual Networks", "text": "Deep Residual Networks (ResNets) are built up by stacking residual units. A residual unit can be expressed as:\nyl = h(xl) + F(xl,Wl), xl+1 = f(yl),\n(1)\nwhere xl and xl+1 are the input and output of the l-th layer, Wl is the weights for the l-th layer, and F is a residual function (He et al., 2016), e.g., the identity function (He et al., 2015), which we also use in our experiments. ResNets can be intuitively understood by thinking of residual functions as paths through which information can propagate easily. This means that, in every layer, a ResNet learns more complex feature combinations, which it combines with the shallower representation from the previous\nlayer. This architecture allows for the construction of much deeper networks. ResNets have recently been found to yield impressive performance in both image recognition and NLP tasks (He et al., 2015; He et al., 2016; O\u0308stling, 2016; Conneau et al., 2016), and are an interesting and effective alternative to simply stacking layers. In this paper we use the assymetric variant of ResNets as described in Equation 9 in He et al. (2016):\nxl+1 = xl + F(f\u0302(xl),Wl). (2) Our residual block, using dropout and batch normalization (Srivastava et al., 2014; Ioffe and Szegedy, 2015), is defined in Table 1. In the table, merge indicates the concatenation of the input of the residual block, with the output of the final convolutional layer."}, {"heading": "2.3 System Description", "text": "We represent each sentence using a byte-based representation (Sb). This representation is a 2- dimensional matrix Sb \u2208 Rs\u00d7db , where s is the zero-padded sentence length and db is the dimensionality of the byte embeddings. Byte embeddings are first passed through a ResNet in order to obtain a representation which captures something akin to byte n-gram features.1 The size of n is determined by the convolutional window size used. We use a convolutional window size with length 8, meaning that for each byte in the input, the ResNet can learn a suitable representation incorporating up to 8 bytes of context information. These overlapping byte-based n-gram features are then passed through to the bi-GRU, which yields a sentence level representation. The softmax layer applied to the bi-GRU output is then used in order to obtain the network\u2019s predicted class per input."}, {"heading": "2.3.1 Hyperparameters", "text": "The hyperparameters used by the system were tuned on an altogether different task (semantic tagging), and adapted for the current task. The dimensionality of our byte embeddings, db, is set to 64. Our residual block is defined in Section 2.2. We use rectified linear units (ReLUs) for all activation functions (Nair and Hinton, 2010), and apply dropout with p = 0.1 to both input weights and recurrent weights in the bi-GRU. All GRU layers have 100 hidden units.\nAll experiments were run with early stopping monitoring validation set loss, using a maximum of 50 epochs, and a batch size of 100. Optimisation is done using the ADAM algorithm (Kingma and Ba, 2015), with the categorical cross-entropy loss function as training objective.\nFor the B tasks, we train the model in the same way as for the A tasks. Only a handful of instances (n \u2248 5) per B run are classified as belonging to a language which the B group does not contain. These cases are automatically converted to being in the class hr. For the B tasks, we also perform a simple clean-up of the data. We first remove all hyperlinks, hashtags and usernames from the text with a simple regex-based script. We then remove all tweets classified as English. We submitted three runs for each subtask. The system used for runs 1, 2 and 3 contain five, four and three residual blocks respectively."}, {"heading": "3 Results", "text": "We evaluate our system in subtasks A, B1 and B2. Subtask A contains data for five language groups, with two to three languages in each group (Tan et al., 2014). Subtask B1 and B2 contain data for a subset of\n1Note that bytes are passed through the ResNet one by one, yielding one representation per byte, rather than as a whole sequence, which would yield a single representation per sentence.\nTest Set Track Run Accuracy F1 (micro) F1 (macro) F1 (weighted)\nthe languages in subtask A, compiled from Twitter. Subtask B1 contains the amount of tweets necessary for a human annotator to make reliable judgements, whereas B2 contains the maximum amount of data available per tweet.\nFor subtasks A and B1, run 3 results in the best accuracy on test, whereas run 2 results in the best accuracy on B2. The results are shown in Table 2. Table 3 and Table 4 contain confusion matrices for the results in subtask A and B respectively."}, {"heading": "4 Discussion", "text": "Judging from the confusion matrices in Section 3, our system has very low confusion between language groups. However, confusion can be observed within all groups. Although the system achieves reasonable performance, there is a large gap between our system and the best performing systems (e.g. C\u0327o\u0308ltekin and Rama (2016), who obtain 89.38% accuracy on task A, 86.2% on B1, and 82.2% on B2). This can to some extent be explained by limitations caused by our implementation.\nThe largest limiting factor can be found in the fact that we only allowed our system to use the first ca. 384 bytes of each training/testing instance. For the training and development set, and subtask A, this was no major limitation, as this allowed us to use more than 90% of the available data. However, for subtasks B1 and B2, this may have seriously affected the system\u2019s performance. Additionally, we restricted our system to using only byte embeddings as input. Adding word-level representations into the mix, would likely increase system performance.\nWe also observed considerable differences in development accuracy when changing hyperparameters of our network in relatively minor ways. For instance, altering the patch sizes used in our CNNs had a noticeable impact on validation loss. However, altering the amount of residual blocks used, did not have a large effect on results. The neural network architecture, as well as most of the hyperparameters, were tuned on an altogether different task (semantic tagging), and adapted for the current task. Further fine tuning of the network architecture and hyperparameters for this task would therefore likely lead to narrowing the performance gap."}, {"heading": "5 Conclusions", "text": "We implemented a language identification system using deep residual networks (ResNets) coupled with a bidirectional Gated Recurrent Unit network (bi-GRU), using only byte-level representations. In the DSL 2016 shared task, we achieved reasonable performance, with 84.88% accuracy on subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. Although acceptable performance was achieved, further fine tuning of input representations and system architecture would likely improve performance."}, {"heading": "Acknowledgements", "text": "We would like to thank the Center for Information Technology of the University of Groningen for their support and for providing access to the Peregrine high performance computing cluster, as well as the anonymous reviewers for their valuable feedback."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "author": ["Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vasudevan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vasudevan et al\\.", "year": 2016}, {"title": "Language identification: The long and the short of the matter", "author": ["Baldwin", "Lui2010] Timothy Baldwin", "Marco Lui"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Baldwin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baldwin et al\\.", "year": 2010}, {"title": "Barbara Plank", "author": ["Johannes Bjerva"], "venue": "and Johan Bos.", "citeRegEx": "Bjerva et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "John M Trenkle", "author": ["William B Cavnar"], "venue": "et al.", "citeRegEx": "Cavnar et al.1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Discriminating similar languages: experiments with linear SVMs and neural networks", "author": ["\u00c7\u00f6ltekin", "Rama2016] \u00c7a\u011fr\u0131 \u00c7\u00f6ltekin", "Taraka Rama"], "venue": "In Proceedings of the 3rd Workshop on Language Technology for Closely Related Languages, Varieties and Dialects (VarDial),", "citeRegEx": "\u00c7\u00f6ltekin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "\u00c7\u00f6ltekin et al\\.", "year": 2016}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "KyungHyun Cho", "author": ["Junyoung Chung", "Caglar Gulcehre"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chung et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lo\u0131\u0308c Barrault", "author": ["Alexis Conneau", "Holger Schwenk"], "venue": "and Yann Lecun.", "citeRegEx": "Conneau et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "A primer on neural network models for natural language processing. arXiv preprint arXiv:1510.00726", "author": ["Yoav Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Shervin Malmasi", "author": ["Cyril Goutte", "Serge L\u00e9ger"], "venue": "and Marcos Zampieri.", "citeRegEx": "Goutte et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Shaoqing Ren", "author": ["Kaiming He", "Xiangyu Zhang"], "venue": "and Jian Sun.", "citeRegEx": "He et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Shaoqing Ren", "author": ["Kaiming He", "Xiangyu Zhang"], "venue": "and Jian Sun.", "citeRegEx": "He et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Ioffe", "Szegedy2015] Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "langid.py: An off-the-shelf language identification tool", "author": ["Lui", "Baldwin2012] Marco Lui", "Timothy Baldwin"], "venue": "In Proceedings of the ACL 2012 system demonstrations,", "citeRegEx": "Lui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2012}, {"title": "Ahmed Ali", "author": ["Shervin Malmasi", "Marcos Zampieri", "Nikola Ljube\u0161i\u0107", "Preslav Nakov"], "venue": "and J\u00f6rg Tiedemann.", "citeRegEx": "Malmasi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Morphological reinflection with convolutional neural networks", "author": ["Robert \u00d6stling"], "venue": "In Proceedings of the 2016 Meeting of SIGMORPHON,", "citeRegEx": "\u00d6stling.,? \\Q2016\\E", "shortCiteRegEx": "\u00d6stling.", "year": 2016}, {"title": "2016", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg"], "venue": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of ACL", "citeRegEx": "Plank et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Ilya Sutskever", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky"], "venue": "and Ruslan Salakhutdinov.", "citeRegEx": "Srivastava et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Nikola Ljube\u0161ic", "author": ["Liling Tan", "Marcos Zampieri"], "venue": "and J\u00f6rg Tiedemann.", "citeRegEx": "Tan et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lei He", "author": ["Peilu Wang", "Yao Qian", "Frank K Soong"], "venue": "and Hai Zhao.", "citeRegEx": "Wang et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ruslan Salakhutdinov", "author": ["Zhilin Yang"], "venue": "and William Cohen.", "citeRegEx": "Yang et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "2014", "author": ["Marcos Zampieri", "Liling Tan", "Nikola Ljube\u0161i\u0107", "J\u00f6rg Tiedemann"], "venue": "A Report on the DSL Shared Task", "citeRegEx": "Zampieri et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "2015", "author": ["Marcos Zampieri", "Liling Tan", "Nikola Ljube\u0161i\u0107", "J\u00f6rg Tiedemann", "Preslav Nakov"], "venue": "Overview of the DSL Shared Task", "citeRegEx": "Zampieri et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We report on our system for the shared task on discrimination of similar languages (DSL 2016). The system uses only byte representations in a deep residual network (ResNet). The system, named ResIdent, is trained only on the data released with the task (closed training). We obtain 84.88% accuracy on subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A large difference in accuracy on development data can be observed with relatively minor changes in our network\u2019s architecture and hyperparameters. We therefore expect fine-tuning of these parameters to yield higher accuracies.", "creator": "LaTeX with hyperref package"}}}