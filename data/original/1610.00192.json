{"id": "1610.00192", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2016", "title": "A large scale study of SVM based methods for abstract screening in systematic reviews", "abstract": "A major task in systematic reviews is abstract screening, i.e., excluding, often hundreds or thousand of, irrelevant citations returned from a database search based on titles and abstracts. Thus, a systematic review platform that can automate the abstract screening process is of huge importance. Several methods have been proposed for this task. However, it is very hard to clearly understand the applicability of these methods in a systematic review platform because of the following challenges: (1) the use of non-overlapping metrics for the evaluation of the proposed methods, (2) usage of features that are very hard to collect, (3) using a small set of reviews for the evaluation, and (4) no solid statistical testing or equivalence grouping of the methods. In this paper, we use feature representation that can be extracted per citation. We evaluate SVM-based methods (commonly used) on a large set of reviews ($61$) and metrics ($11$) to provide equivalence grouping of methods based on a solid statistical test. Our analysis also includes a strong variability of the metrics using $500$x$2$ cross validation. While some methods shine for different metrics and for different datasets, there is no single method that dominates the pack. Furthermore, we observe that in some cases relevant (included) citations can be found after screening only 15-20% of them via a certainty based sampling. A few included citations present outlying characteristics and can only be found after a very large number of screening steps. Finally, we present an ensemble algorithm for producing a $5$-star rating of citations based on their relevance. Such algorithm combines the best methods from our evaluation and through its $5$-star rating outputs a more easy-to-consume prediction.", "histories": [["v1", "Sat, 1 Oct 2016 21:11:38 GMT  (122kb)", "https://arxiv.org/abs/1610.00192v1", null], ["v2", "Thu, 13 Oct 2016 12:56:11 GMT  (122kb)", "http://arxiv.org/abs/1610.00192v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["tanay kumar saha", "mourad ouzzani", "hossam m hammady", "ahmed k elmagarmid"], "accepted": false, "id": "1610.00192"}, "pdf": {"name": "1610.00192.pdf", "metadata": {"source": "CRF", "title": "A large scale study of SVM based methods for abstract screening in systematic reviews*", "authors": ["Tanay Kumar Saha"], "emails": ["tksaha@iupui.edu", "aelmagarmid}@qf.org.qa"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n00 19\n2v 2\n[ cs\n.I R\n] 1\n3 O\nct 2\n01 6\nI. INTRODUCTION\nRandomized controlled trials (RCTs) are a key component of medical research and by far the best way of achieving results that can genuinely increase our knowledge about treatment effectiveness [1]. Because of the huge explosion of RCTs, it is very hard for individuals to glean evidence from all of them. Systematic reviews synthesize the results of more than one RCT, summarize the effects of their individual outcomes with a certain degree of confidence and provide answers about the effectiveness of a particular intervention [2]. Thus, systematic reviews provide current best evidence for policy and clinical decision-making.\nSystematic review involves formulating a research question, searching in multiple biomedical databases, identifying relevant studies based on abstracts and titles (abstract screening)\nThis work was done while the first author was doing internship at QCRI, HBKU, Qatar. All private reviews/datasets used in this paper was collected from the existing abstract screening platform Rayyan(http://rayyan.qcri.org/).\nand then based on full texts of a subset thereof, assessing their methodological qualities, data extraction and synthesis, and finally reporting the conclusions on the review question [3]. However, the search precision is quite low and it usually returns a large number of citations, from hundreds to thousands. Systematic review authors have to go through the tedious and time-consuming task of screening these citations. Therefore, a systematic review platform that can automate the abstract screening process is fundamental in expediting the process of processing systematic reviews.\nThere are several challenges in building a systematic review platform for abstract screening including: (1) feature extraction should be fast, (2) the features should be readily available, (3) the learning and prediction algorithms should be very efficient, and (4) the algorithms should also be able to handle the problem of extreme data imbalance (due to the low search precision). In addition, the presentation of the citations should be based on their relevance, i.e. all the relevant citations should be ranked higher than the irrelevant ones. This is an instance of the bipartite ranking problem. Generating a clear bipartition is a quite difficult task [4] and a common measure of success in such scenario is the area under the ROC curve (AUC). However, the optimization techniques based on the AUC can be reduced to binary classification [5] methods under specific settings. In this paper, we evaluate several SVM based methods that either use a loss function such as 0 \u2212 1 (Classification), AUC, KLD, or quadratic mean error.\nTo address the first two constraints, quick feature extraction and availability, we focus in this paper on two classes of features, namely uni-bigram and Word2Vec features. Other features may be hard to obtain such as co-citations or are not always readily available such as MeSH terms. Uni-Bigram features do not have such issues. Another popular choice is LDA based topic features. However, LDA features need to be generated per review and with each addition of a new batch of citations, one needs to regenerate the topics. There are several practical concerns about how many topics should be generated and the efficiency of generating the LDA model. Therefore, we use Word2Vec based features as it captures the semantic similarity between words similarly to LDA and once the model is built (offline), the features are readily available. On the other hand, for the data-imbalance constraint, i.e., irrelevant\ncitations are more common than the relevant ones, we evaluate algorithms that have the potentials to address this problem, namely SVM cost (cost-sensitive SVM) and SVMperf (SVM with multivariate performance measure). This will allow us to get more insights on the applicability of these algorithmic approaches to tackle the data imbalance issue. We should also note that SVM-based approaches have less parameters to tune compared to others such as random forests.\nIt is worth mentioning that a recent systematic review of current approaches in abstract screening [6] reported that among the 69 publications it surveyed, only 22 report about Recall, 18 report about Precision, and 10 about AUC. The non-overlapping usage of these metrics makes it difficult to draw a conclusion about the different methods. For example, if a particular work only reports Recall but not Precision, it is hard to understand its applicability. Moreover, recent works [7], [8] suggest studying the variability of the reported metrics and advocate using a large number of repetitions (500\u00d7 2) to make the conclusions more reproducible. Table I characterizes most of the existing approaches in terms of their most important aspects. In particular, we observe that most of the studies have been performed on a small number of reviews, the evaluation is reported with different metrics, the cross validation uses a very small number of repetitions, and most of the approaches did not perform proper statistical significant tests.\nIn this paper, using a large set of systematic reviews, we conduct an extensive evaluation of several classification methods, along with different feature representations, which have the potential to address the above constraints. This would give us insights on the best approach to adopt for abstract screening in a practical systematic review platform. We also present such an approach as a consequence of our evaluation. More specifically, we make the following contributions:\n1) We use a large sample of reviews (61, Most of them were collected from an existing abstract screening platform: Rayyan1) 2) We evaluate 18 different methods and report on 11 different metrics 3) We perform a 500x2 cross validation 4) We apply a 2-factor ANOVA analysis with a paired t-test\nand group the equivalent methods. 5) We present an ensemble method that present prediction\nresults through a 5-star rating method.\nThe remainder of this paper is organized as follows. In Section II, we discuss related work. In Section III, we detail the SVM based methods used in the evaluation. Section IV presents the experimental results. Section V details the proposed 5-star rating method. We conclude in Section VI."}, {"heading": "II. RELATED WORK", "text": "We divide the related work into four groups: 1) work on \u201cAbstract Screening\u201d methods, 2) discussion of \u201cData Imbalance\u201d as it is one of the main issues in the abstract\n1 http://rayyan.qcri.org/\nAspect Related Studies\nFeature Space Representation Uni, Bigram, MeSH [9], LDA [10], Uni + Cite [11]\nAlgorithm SVM Based [12], [13], Others [14]\nCross Validation 5\u00d72 [9], [11], [13], [15], [16], 10-fold [17]\nNo. of Reviews used 15 [11], [15], [16], 6 [12], 18 [9], 3 [18], 1 [17]\nStatistical Testing Post-hoc paired Wilcoxon test [9], Rank Group [11]\nReported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]\nTABLE I A BRIEF SUMMARY OF RECENT CONTRIBUTIONS AND THEIR CHARACTERISTICS. FOR MORE DETAILS ABOUT ALGORITHMS USED IN SYSTEMATIC REVIEWS PLEASE CONSULT [6], [14], AND FOR METRIC USAGE [6]\nscreening process, 3) \u201cActive Learning\u201d a a popular method to address data imbalance issue, and 4) \u201cLinear Review\u201d, from the legal domain, which bears some similarity with abstract screening in systematic reviews."}, {"heading": "A. Abstract Screening", "text": "A large body of past research has focused on automating the abstract screening process [9], [11]\u2013[13], [15], [16]. In terms of feature representation, most of the existing approaches [15], [16] use unigram, bigram, and MeSH (Medical Subject Headings). An alternative to the MeSH terms is extracting LDA based latent topics from the titles along with the abstracts and using them as features [12]. Other methods [11], [15] utilize external information as features, such as citations and cocitations. In terms of methods, SVM-based learning algorithms are commonly used [9], [13], [15]. According to a recent study [14], 13 different types of algorithms have been proposed in systematic reviews including SVM, Decision Trees, Naive Bayes, k-nearest neighbor, and neural networks. To the best of our knowledge, for abstract screening, past works did not use the structured output version of SVM (SVMperf ) with different loss functions. SVMperf [19] can learn faster than SVM. For prediction, it only keeps feature weights and thus the prediction module becomes very fast. On the other hand, as Transductive learning [20] takes both the labeled and unlabeled citations into account, the corresponding learning algorithm is slower than SVMperf [19]."}, {"heading": "B. Data imbalance", "text": "Data imbalance in supervised classification is a well studied problem [19]\u2013[23]. Two different kinds of approaches have been proposed to solve it. The first focuses on designing loss functions: KLD [24], Quadratic Mean [25], Cost sensitive\nclassification [26], Mean Average Precision [27], Random Forest [28] with meta-cost, and AUC [19]. The second kind generates synthetic data for artificially balancing the ratio of labeled relevant and irrelevant citations. The example of such methods are Borderline-SMOTE [29], Safe-levelSMOTE [30], and oversampling of the minority class along with undersampling the majority class. The authors in [31]\u2013 [33] use probability calibration techniques. In this paper, we evaluate the algorithmic approaches rather than the data centric approaches or methods based on probability calibration since synthetic data generation is computationally intensive\u2014 generating data in extreme imbalance becomes quite difficult. Besides, for probability calibration we need a validation set that might not be always available in such kind of abstract screening process."}, {"heading": "C. Active Learning", "text": "As systematic reviews are done in batches, the problem of abstract screening can be modeled as an instance of batch mode active learning. Online learning trains the classifier after adding every citation whereas Batch-mode active learning (BAL) does the same after adding batches of citations. However, BAL does not have any theoretical guarantee compared to online learning [34], [35]. The task of BAL is to select batches with informative samples (citations) that would help learn a better classification model. There are two popular methods to select samples: (1) Certainty based and (2) Uncertainty based. In certainty based methods, \u201cmost certain\u201d samples are selected to train the classifier, while in uncertainty based method ,\u201cmost uncertain\u201d ones are selected. In the uncertainty sampling-based methodologies, a large number of uncertainty metrics have been proposed; examples include entropy [36], smallest-margin [37], least confidence [38], committee disagreement [39], and version space reduction [34], [40]. Using \u201cmost uncertain\u201d samples based on these metrics improve the quality of the classifier to find the best separating hyperplane, and thus to improve its accuracy in classifying new instances [12]. On the other hand, certainty based methods have also been shown to be effective to carry out active learning on imbalanced data sets, as demonstrated in [41]."}, {"heading": "D. Linear Review", "text": "A very similar review system that is popular among law firms is Technology assisted Linear Review (TAR) [42]\u2013[46]. The main objectives of both review systems are very similar. TAR is used to save time of the attorneys to screen relevant documents rather than on the irrelevant ones. Generally the number of documents are huge (millions) compared to the same in systemtic review, and, so active learning becomes a popular method. However, unlike systematic reviews, TAR researchers are interested in finding a good stabilization point\u2014 Finding a point, where the training of the classifier should be stopped. Moreover, in TAR, achieving at least 75% recall is considered as acceptable, whereas in Systematic reviews 95%- 100% recall is desirable."}, {"heading": "III. METHOD", "text": "For abstract screening, we have title (Ti) and abstract (Ai) for a set of n citations, C = {CTi,Ai}1\u2264i\u2264n. We represent each citation, CTi,Ai as a tuple \u3008xCi , yCi\u3009: xCi is a d-dimensional feature space representation of the citation and yCi \u2208 {+1,\u22121} is a binary label denoting whether Ci is relevant or not. Additionally, we use the following notations throughout the paper. We use L and U for the labeled and unlabeled set of citations, respectively; F() to represent features for a set of citations; h for the hypothesis or hyperplane learned by training on L; and l(w) for the latent space representation for a particular word, w. In this section, we describe the feature space representation and the methods for evaluation of automated abstract screening system."}, {"heading": "A. Feature Space Representation", "text": "We use two types of feature space representation: (1) UniBigram and (2) Word2Vec. For Word2Vec [48], we train the model on the entire set of citations (abstracts and titles) available in an existing systematic review platform. We use Gensim [49] package with the following parameters: the number of context words as 5, the min word count as 15 and the number of dimensions in the latent space as 500. Thus, for each word in the set of all available words (wi \u2208 W), we learn a 500-dimensional latent space representation. Some query results from the trained model are shown in the Table II. We can conclude that the model captures the latent similarities among various bio-medical terms. For example, the cosine similarity between \u201cliver\u201d and \u201ccirrhosis\u201d is 0.63, and for the query \u201cThe way breast and cancer is related, which words are related to \u201ccirrhosis\u201d in the same way\u201d, we can see \u201cliver\u201d is listed as one of the top-5 answers. To represent a citation Ci in the latent space, we use a simple averaging technique to generate a 500 dimensional feature vector, xCi .\nxCi =\n\u2211\nw\u2208WCi l(w)\n|WCi |\nAfter averaging, we apply two kinds of normalization: (1) instance normalization (row normalization) and (2) feature based normalization (column normalization). These normalizations give statistically significantly better results than using raw features for threshold agnostic metrics such as AUC and AUPRC. In instance normalization, we z-normalize extracted feature for each citation, xCi . For column normalization, we z-normalize in each of the 500 dimensions. After both normalizations, we keep the feature values up to 2-decimal places to minimize the memory requirement. Apache Lucene A standard analyzer 2 is applied to tokenize titles and abstracts and also to generate uni-bigrams. Subsequently, the word counts are binarized. After this step, we obtain a feature space representation for both the labeled and unlabeled citations, i.e. F(L) and F(U). Note that the UniBigram is a sparse feature representation whereas Word2Vec\n2https://lucene.apache.org/core/"}, {"heading": "B. Algorithms", "text": "In Table III, we outline the different algorithms we used in our comparison along with their parameters, loss functions, and the identifier to represent each algorithm. Table VI uses the identifiers as opposed to the name of the methods. For example, the first row in Table III refers to a method, identified by Id 1, that uses SVMperf with b = 0 and AUC as the loss function. We use three types of SVM: (1) Inductive (2) Transductive and (3) SVM for Multivariate Performance. Inductive SVM learns a hypothesis, h induced by F(L). Transductive SVM answers the most general question: What information do we get from studying F(U) and how can we use it? It also uses the idea of equivalence classes: two functions from the hypothesis space H belong to the same\nequivalence class if they both classify the instances from F(U) and F(L) in the same way. This reduces the learning problem of finding a h \u2208 H to a different learning problem where the goal is to find one equivalence class from infinitely many induced by all the instances in F(U) and F(L). On the other hand, SVMperf is an implementation of the Support Vector Machine (SVM) formulation for optimizing multivariate performance measures [19]. It exploits the alternative structural formulation of the SVM optimization problem for conventional binary classification with error rate [50]. We use three different loss functions for the SVMperf implementation:\nAUC = # of swapped pairs\nKLD(p, p\u0302) = \u2211\nl\u2208L\np(l) log p(l)\np\u0302(l) (1)\nQuadMean Error = \u221a FN TP + FN FN TP + FN + FP FP + TN FP FP+TN\n2\nFor KLD (Kullback-Leibler Divergence) loss function, p(l) and p\u0302(l) are estimated as follows:\np(l) = TP + FNTP + FP + FN + TN p\u0302(l) = TP + FPTP + FP + FN + TN"}, {"heading": "C. Tuning the cost parameter and setting the threshold", "text": "For the inductive and Transductive SVM methods, the default cost parameter, denoted as c, is computed as follows:\nb =\n\u2211\n1\u2264i\u2264|L|\n\u2016xCi\u2016\n|L|\nc = 1.0 b\u2217b\nThe summation of all the 2-norm of the feature vectors are divided by the number of instances in F(L) to generate b. Finally, the fraction 1.0\nb\u2217b gives c. J in SVM cost is set to the\nfollowing ratio: # of irrelevant# of relevant . The ratio biases the hypothesis learner (train algorithm) to penalize mistakes on the minority class J (relevant class) times more than the majority class. Furthermore, we follow the recommendation for error loss function from [19] to set the cost parameter for SVMperf :\nSVMperf (c) = SVM (c)\u00d7# of instances100.0\nThe motivation of using these values is to make our results more reproducible. To evaluate the reproducibility of each method is one of the main goals of this paper. The way we\ncalculate the parameter values can also avoid the need for a representative validation set which is very hard to obtain in a systematic review platform. Note, for all the methods, we set the classification threshold to 0.0 (the threshold is very important to separate the relevent citations from the irrelevant ones)."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "We run all of our experiments on a computer with a 24- core Intel XEON X5650 2.6Ghz processor running CentOS 6.7 operating system. For each dataset (described in the next section), we perform a 500\u00d7 2 fold cross validation for each method to analyze the variability. In n\u00d7k fold cross validation, we split the data in k blocks; k-th block becomes the test block and the rest becomes the training data and we repeat this process n times. The split is carried out through stratification. So, for 500 \u00d7 2, we split each dataset into two blocks and then use each block once as a training and once as a test. We repeat this process for 500 times."}, {"heading": "A. Datasets", "text": "We use 61 reviews for our experiments. Among the 61 reviews, 15 reviews are publicly available from [16] and the rest of the 46 reviews are collected from a deployed platform for producing systematic reviews. In Table III-C, all publicly available reviews start with C (Cohen) whereas reviews from our system starts with P (Private). In the table, we give three statistics about the review\u2014the total number of citations, the total number of relevant citations, and prevalence (ratio of relevant and total number of citations). We divide the reviews into three prevalence groups: (1) Low (0.22% to 5.92%) (2) Mid (6.79% to 13.07%) and (3) High (13.45% to 40.08%). This allows us to study the behavior of the performance metrics, discussed later, within each group. This also helps us present our findings3 in a detailed fashion.\nThe datasets of each group are sorted by their prevalence. For example, in dataset P18, the prevalence is 0.22% as 5 (no. of relevant citations) out of 2241 (total citations) is 0.0022."}, {"heading": "B. Performance Metrics", "text": "To be self contained, we reproduce the metric table from [6]. Table V gives the definition and formula for all the metrics used in the evaluation. The first four measures (Recall, Precision, F-measure and Accuracy) depend on a threshold and are widely used for evaluating automated abstract screening methods. ROC and AUPRC do not depend on thresholds and are common in binary classification problems with data imbalance. We also add two more metrics: (1) Arithmetic mean error and (2) Quadratic mean error. Both of these errors depend on the loss in Recall of the relevant (LRp ) and irrelevant class (LRn). The arithmetic mean error is the arithmetic mean of LRp and LRn . The quadratic mean error is the square root of the arithmetic mean of the squares of LRp and LRn . In active learning settings, Burden, Utility, and Yield are frequently used.\n3For more detailed results: https://drive.google.com/open?id=0B1RWfbRmk3gVc0ViYW5wX2VSQ28"}, {"heading": "C. Statistical Test", "text": "We have 18 different methods (Table III) and three prevalence groups: (1) Low-prevalence, (2) Mid-prevalence, and (3) High prevalence. We want to compare the methods based on the datasets of a particular prevalence group on a specific metric. Our goal is to generalize the findings on a larger population of possible datasets which fall within a defined prevalence group. So, we model the problem similar to [51]:\nMETRIC \u223c DATA + METHOD (2)\nwhere DATA has been modeled as random effects and METHOD as a fixed effect. To be more specific, the model resembles y = mx + c, where x is a variable and m, c are the constants. In this model, DATA and METHOD mimic x and c respectively. We fit the model in a linear regression framework and perform a two-factor (DATA and METHOD) analysis of variance. This helps us identify whether there is any statistically significant difference among the methods, the datasets and the interactions between the methods and the data. If the test shows statistically significant performance difference among the methods, we again compare them pairwise using paired t-tests (post-hoc testing). Post-hoc testing with 18 methods leads to 18*17 possible post-hoc comparisons. Each of these 18*17 tests needs correction for multiple testing.\nTo avoid the large number of testing, we follow these steps: 1) We aim to find the best method and compare it with the\nrest via paired t-tests. 2) All the methods (say, 4) that are not statistically signif-\nicant (with \u03b1 = 0.5) with respect to the best method falls in the same group. So, we can skip these methods (4) along with the best one for the next comparison. 3) Go to 1. (We repeat the same steps with the remaining 18 (# of methods) - 5 (4 + the best method) =13.)"}, {"heading": "D. Performance comparison", "text": "In Table VI, we present our results (in top three ranked tiers/groups), in terms of the metrics that we define in Table V, for all methods on all datasets. Datasets are grouped by their prevalence, as we explained earlier. The results, for each metric and prevalence, are presented as equivalence groups based on the statistical test (\u03b1=0.05). All methods that use the UniBi, WORD2VEC ROW, and WORD2VEC COLUMN features have an identifier as 1\u221211 (11 is specifically for SVM Transduction), 21\u2212 25 and 31\u2212 35 respectively (Table III).\nWe first analyze the metrics which depend on a threshold. For Precision (Prec), cost sensitive SVM (7, where 7 has been used as an identifier for cost sensitive SVM in Table III) outperforms others in the low and mid prevalence groups. But, for the high prevalence group, it falls in the second tier (Table VI). SVM Default (5) also performs well. For Recall, WORD2VEC ROW with SVMperf (AUC) (21) is in the top position in all three prevalence groups. SVM Transduction with Uni-Bi (11) and cost-sensitive SVM with WORD2VEC ROW (25) are the top performers for the F1 metric. For ACC, cost-sensitive SVM(B1) with UniBI (7) performs the best followed by SVM Transduction with UniBi (11).\nNow, we analyze the performance of the methods on two threshold agnostic metrics. In AUC, WORD2VEC ROW with SVMperf (AUC) (21) is the one with the best performance. Cost sensitive SVM(B1) with WORD2VEC ROW normalized feature (25) also performs well. In AUCPR, SVMperf with (B0, AUC) and (B1, AUC) along with the Uni-BI feature are the top performing methods.\nCost sensitive SVM with WORD2VEC ROW normalized feature, 24 and 25, rank first in the AM and QUADMEAN ERRORS metrics, respectively. Again WORD2VEC ROW with SVMperf (AUC) (21) ranks first in the YIELD and UTILITY metrics while the SVM Default (5) tops the list in the BURDEN metric.\nIn conclusion, we observe that that there is almost always a method that ranks first in the three prevalence groups. However, there is no single dominant method across all metrics. In summary, there is no single \u201cwinner\u201d or best method\u2014various methods perform well on different prevalence groups and for different metrics. For instance, WORD2VEC ROW with SVMperf (AUC) (21) seems to be a good choice, outperforming the other methods in five metrics. However it is not present in any of the equivalence groups for a few other metrics. This is because the method has a very high recall, but very low precision. The low precision might come from the averaging technique (Section III-A) we use to represent the feature space. It follows that a holistic \u201ccomposite\u201d strategy must be adopted in any practical system. We show such a method in Section V. This is not surprising since each method has been designed to meet a certain objective. However, abstract screening is a complicated process, in which, a \u201cgood\u201d method\nshould be able to optimize several metrics such as precision, recall, AUC, utility and possibly many others simultaneously."}, {"heading": "E. Experiment in an active learning setting", "text": "In a practical system, users will usually label citations by batches. Hence, an active learning approach has the potential to achieve better results [12]. We use the SVMperf (AUC) (21) method for this experiment as it is the top performing method in terms of the AUC metric in all three prevalence groups (Table VI). Initially for training, we choose 5 relevant and 45 irrelevant citations uniformly at random from the entire set and then learn a hyperplane h. We calculate the distance (score) from the hyperplane for each of the unlabeled instances and rank them based on this score. We choose the top-50 from the ranked citations to retrain the model along with the existing labeled citations. We repeat this experiment 500 times and take the average. The goal of this experiment is to see the following: if a particular user labels 50 top ranked citations per batch, what is the percentage of total citations that the user has to screen to get all the relevant ones?\nFigure 1 shows the results of the proposed experiment. The results are based on three groups of prevalence values. For the low prevalence group, out of 20 reviews, 7 reviews (the first four values on the top histogram) need 40% of the total citations to be screened to get all the relevant citations, 12 need around 50 to 70% citations, and 1 needs more than 90%. For the mid and high prevalence groups, 9 out of 20 and 11 out of 21 reviews need around 80% to 90% citations to be screened to get all the relevant citations. In the bottom row of Figure 1, we show the spread of standard deviations for each of the prevalence groups. As expected, the low prevalence\ngroup has a very high standard deviation. For the mid and high prevalence groups, the standard deviation is very small (around \u223c 0.01-0.05) in most of the cases.\nIn Figure 2, we further analyze the inclusion behavior of a particular random run of review 1. As the results are similar for some of the other reviews, we omit the plots to avoid repetitions. For a random run, around 58%, 1500 out of 2544 documents have to be screened before it gets its final relevant citation. However, we clearly see from the figure that, it gets almost all but the final one after screening only 400 out of 2544 which is around 15% of the total citations. The last citations surprisingly takes 22 more iterations. With the help of a domain expert, it would be interesting to see whether the final citation has any outlying characteristics.\nAlgorithm 1: RelRank: A Five Star rating algorithm using ensemble of max-margin based methods\nInput : L, Labeled dataset; U , Unlabeled dataset Output: Score, S1\u2264i\u2264|U|\n1 FL, FU \u2190 GenerateFeature (L, U , feature = W) 2 h1 \u2190 Train ( SVMperf , FL) 3 h2 \u2190 Train ( SVM cost, FL) 4 Sh1 \u2190 Predict (h1, FU ) 5 Sh2 \u2190 Predict (h2, FU ) 6 FL, FU \u2190 GenerateFeature (L, U , feature = U) 7 h3 \u2190 Train ( SVM cost, FL) 8 Sh3 \u2190 Predict (h3, FU ) 9 S \u2190 GenerateCombinedScore (U , Sh1, Sh2, Sh3)\n10 return S\nV. PROPOSED 5-STAR RATING ALGORITHM\nIn our systematic review platform, we wanted to rank citations based on their graded relevance. The intuition behind this proposal is to help reviewers better manage their time. For this purpose, we rate the citations from 1 \u2212 5 using RelRank (described later in Algorithm 1 ). The citations with 5 star might be relevant (with high probability) and need more attention whereas 1 starred documents might be irrelevant (with high probability) and may need less attention. Among the 5 stars, we conceptualize 3 \u2212 5 star to indicate relevant citations and 1\u2212 2 star for irrelevant ones.\nWe present our 5-star rating algorithm, RelRank using an ensemble of SVM based methods. We choose method 21, 25, and 7 (See Table III) because of their special characteristics. For instance, Method 21 outperforms others based on the AUC and RECALL metric. Moreover, the Method 21 has the lowest standard deviation for the AUC metric (See Table VI). While evaluated on the F1 metric which is a harmonic mean of Precision and Recall, Method 25 produces the highest F1 measure. Method 11 has similar effect as Method 25 but with much higher computation time. On the other hand, Method 7 has the highest Precision. So, in RelRank , if all of these three methods (Methods 21, 25 and 7) agree on the relevance of a citation, then the citation gets a 5 star; if two of them agrees then it gets a 4 star. Within a particular star, citations are ranked based on their average ranks (described in Algorithm 2).\nIn RelRank , we first generate WORD2VEC ROW features for both the L and U . Then, we train SVMperf and SVM cost (Methods 21 and 25) to generate first (h1) and second hyperplane (h2) respectively. Step 4 and 5 generate scores for U based on the distances from h1 and h2. Using the Uni-Bi gram features (Step 6), we compute a third score Sh3 using hyperplane h3 (Step 7). In Step 9, we combine the three scores to generate a final score for U (which we formally present as Algorithm 2. Please see the appendix for more details).\nWe evaluate our 5-star algorithm on each of the citations in different cumulative star-groups. The results are provided in Table VII. For instance, we evaluate the citations which get 3 or more stars, 4 or more stars and 5 stars. We observe that our 5-star algorithm performs similarly to Method 21 for the citations receiving 3 stars or above. However, for the precision metric, the 5-star algorithm performs better than Method 21 as it falls in the rank group 7 whereas Method 21 falls in the group 9. This is because the 5-star algorithm is a combination of Method 21 with two other methods that have better precision. However, for the recall metric, RelRank (3-star) are in the top group whereas RelRank (4-star) and RelRank (5-star) are in the second top group. This is obvious as increasing recall might lose precision. We observe a similar behavior in the remaining metrics."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "Automating the production of systematic reviews will be crucial in delivering the promise of evidence-based medicine. An key task in this process is sifting through hundreds to thousands of citations and identifying relevant studies for further analysis. In this paper, we studied the most popular classification methods employed in this task. We focus on the methods that better fit the constraints of a practical system for abstract screening. In all, we report on 18 methods on a\nvery large number of reviews (61) using 11 metrics. There is no single \u201cwinner\u201d or best method\u2014various methods perform well on different prevalence groups and for different metrics. For instance, WORD2VEC ROW with SVMperf (AUC) seems to be a good choice, outperforming the other methods in five metrics but is not doing well for a few other metrics. We also observe that in an active learning setting, a very large portion of included citations can be easily found with few iteration. However, one or two citations have some outlying behavior and requires much more iterations to be found.\nWe also presented an ensemble method that combines three of the methods we evaluated and convert their scores into a 5-star rating system through a voting mechanism and rank citations based on their graded relevance. The goal is to help\nreviewers better manage their time; A 5-star citation might be relevant with high probability and need more attention whereas a 1-star citation might be irrelevant with high probability and thus need less attention.\nAs a future work, we plan to look at different approaches to combine methods as well look more carefully at the outlying citations and how they can be screened with less iterations."}], "references": [{"title": "A method for assessing the quality of a randomized control trial", "author": ["T.C. Chalmers", "H. Smith", "B. Blackburn", "B. Silverman", "B. Schroeder", "D. Reitman", "A. Ambroz"], "venue": "Controlled clinical trials, vol. 2, no. 1, pp. 31\u201349, 1981.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1981}, {"title": "Systematic reviews and meta-analyses", "author": ["L.S. Uman"], "venue": "Journal of the Canadian Academy of Child and Adolescent Psychiatry, vol. 20, no. 1, p. 57, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Learnability of bipartite ranking functions", "author": ["S. Agarwal", "D. Roth"], "venue": "Learning Theory. Springer, 2005, pp. 16\u201331.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust reductions from ranking to classification", "author": ["M.-F. Balcan", "N. Bansal", "A. Beygelzimer", "D. Coppersmith", "J. Langford", "G.B. Sorkin"], "venue": "Machine learning, vol. 72, no. 1-2, pp. 139\u2013153, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Using text mining for study identification in systematic reviews: a systematic review of current approaches", "author": ["A. OMara-Eves", "J. Thomas", "J. McNaught", "M. Miwa", "S. Ananiadou"], "venue": "Systematic reviews, vol. 4, no. 1, p. 1, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating and maintaining classification algorithms", "author": ["T. Raeder"], "venue": "University of Notre Dame,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Consequences of variability in classifier performance estimates", "author": ["T. Raeder", "T.R. Hoens", "N.V. Chawla"], "venue": "2010 IEEE 10th International Conference on Data Mining (ICDM), 2010, pp. 421\u2013430.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A prospective evaluation of an automated classification system to support evidence-based medicine and systematic review", "author": ["K. Ambert"], "venue": "vol. 2010, p. 121, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Supporting systematic reviews using lda-based document representations", "author": ["Y. Mo", "G. Kontonatsios", "S. Ananiadou"], "venue": "Systematic reviews, vol. 4, no. 1, p. 1, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to identify relevant studies for systematic reviews using random forest and external information", "author": ["M. Khabsa", "A. Elmagarmid", "I. Ilyas", "H. Hammady", "M. Ouzzani"], "venue": "Machine Learning, pp. 1\u201318, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing systematic review workload through certainty-based screening", "author": ["M. Miwa", "J. Thomas", "A. OMara-Eves", "S. Ananiadou"], "venue": "Journal of biomedical informatics, vol. 51, pp. 242\u2013253, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Studying the potential impact of automated document classification on scheduling a systematic review update", "author": ["A.M. Cohen", "K. Ambert", "M. McDonagh"], "venue": "BMC medical informatics and decision making, vol. 12, no. 1, p. 1, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "A critical analysis of studies that address the use of text mining for citation screening in systematic reviews", "author": ["B.K. Olorisade", "E. De Quincey", "O. Brereton", "P. Andras"], "venue": "2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Optimizing feature representation for automated systematic review work prioritization", "author": ["A.M. Cohen"], "venue": "AMIA annual symposium proceedings, vol. 2008, 2008, p. 121.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing workload in systematic review preparation using automated citation classification", "author": ["A.M. Cohen", "W.R. Hersh", "K. Peterson", "P.-Y. Yen"], "venue": "Journal of the American Medical Informatics Association, vol. 13, no. 2, pp. 206\u2013219, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine learning methods in systematic reviews: identifying quality improvement intervention evaluations", "author": ["S. Hempel", "K.D. Shetty", "P.G. Shekelle", "L.V. Rubenstein", "M.S. Danz", "B. Johnsen", "S.R. Dalal"], "venue": "2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Active learning for biomedical citation screening", "author": ["B.C. Wallace", "K. Small", "C.E. Brodley", "T.A. Trikalinos"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, 2010, pp. 173\u2013182.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "Proceedings of the 22nd international conference on Machine learning, 2005, pp. 377\u2013384.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Transductive inference for text classification using support vector machines", "author": ["\u2014\u2014"], "venue": "ICML, vol. 99, 1999, pp. 200\u2013209.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "PEGASOS: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming, vol. 127, no. 1, pp. 3\u201330, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Data classification: algorithms and applications", "author": ["C.C. Aggarwal"], "venue": "CRC Press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Probabilistic models for text mining", "author": ["Y. Sun", "H. Deng", "J. Han"], "venue": "Mining Text Data. Springer, 2012, pp. 259\u2013295.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimizing text quantifiers for multivariate loss functions", "author": ["A. Esuli", "F. Sebastiani"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), vol. 9, no. 4, p. 27, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "A quadratic mean based supervised learning model for managing data skewness.", "author": ["W. Liu", "S. Chawla"], "venue": "in SDM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "The foundations of cost-sensitive learning", "author": ["C. Elkan"], "venue": "International joint conference on artificial intelligence, vol. 17, no. 1, 2001, pp. 973\u2013 978.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "A support vector method for optimizing average precision", "author": ["Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, 2007, pp. 271\u2013278.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Using random forest to learn imbalanced data", "author": ["C. Chen", "A. Liaw", "L. Breiman"], "venue": "University of California, Berkeley, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Borderline-SMOTE: a new oversampling method in imbalanced data sets learning", "author": ["H. Han", "W.-Y. Wang", "B.-H. Mao"], "venue": "Advances in intelligent computing, 2005, pp. 878\u2013887.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Safelevel-SMOTE: Safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem", "author": ["C. Bunkhumpornpat", "K. Sinapiromsaran", "C. Lursinsap"], "venue": "Advances in knowledge discovery and data mining, 2009, pp. 475\u2013482.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving class probability estimates for imbalanced data", "author": ["B.C. Wallace", "I.J. Dahabreh"], "venue": "Knowledge and Information Systems, vol. 41, no. 1, pp. 33\u201352, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Class probability estimates are unreliable for imbalanced data (and how to fix them)", "author": ["\u2014\u2014"], "venue": "12th International Conference on Data Mining, 2012, pp. 695\u2013704.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Predicting good probabilities with supervised learning", "author": ["A. Niculescu-Mizil", "R. Caruana"], "venue": "Proceedings of the 22nd international conference on Machine learning, 2005, pp. 625\u2013632.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Support vector machine active learning with application to text classification", "author": ["S. Tong", "D. Koller"], "venue": "vol. 2, 2001, pp. 45\u201366.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "Theory of active learning", "author": ["S. Hanneke"], "venue": "2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Committee-based sampling for training probabilistic classifiers", "author": ["I. Dagan", "S.P. Engelson"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning, 1995, pp. 150\u2013157.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1995}, {"title": "Active hidden markov models for information extraction", "author": ["T. Scheffer", "C. Decomain", "S. Wrobel"], "venue": "Advances in Intelligent Data Analysis. Springer, 2001, pp. 309\u2013318.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2001}, {"title": "Reducing labeling effort for structured prediction tasks", "author": ["A. Culotta", "A. McCallum"], "venue": "AAAI, 2005, pp. 746\u2013751.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "Proceedings of the fifth annual workshop on Computational learning theory. ACM, 1992, pp. 287\u2013294.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1992}, {"title": "Noisy generalized binary search", "author": ["R. Nowak"], "venue": "Advances in neural information processing systems, 2009, pp. 1366\u20131374.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Certainty-enhanced active learning for improving imbalanced data classification", "author": ["J. Fu", "S. Lee"], "venue": "Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on. IEEE, 2011, pp. 405\u2013 412.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Document categorization in legal electronic discovery: computer classification vs. manual review", "author": ["H.L. Roitblat", "A. Kershaw", "P. Oot"], "venue": "Journal of the American Society for Information Science and Technology, vol. 61, no. 1, pp. 70\u201380, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "The challenge and promise of predictive coding for privilege", "author": ["M. Gabriel", "C. Paskach", "D. Sharpe"], "venue": "ICAIL 2013 DESI V Workshop, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Predictive coding: Explanation and analysis of judicial impact and acceptance compared to established e-commerce methodology", "author": ["D.W. Henry"], "venue": "http://www.dwhenry.com/files/Predictive%20Coding.pdf, [Online;Accessed 23-June-2015].", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review", "author": ["M.R. Grossman", "G.V. Cormack"], "venue": "Rich. JL & Tech., vol. 17, p. 1, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "The grossman-cormack glossary of technologyassisted review", "author": ["C. GLOSSARY"], "venue": "Federal Courts Law Review, vol. 7, no. 1, 2013.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Batch-mode active learning for technology-assisted review", "author": ["T.K. Saha", "M. Al Hasan", "C. Burgess", "M.A. Habib", "J. Johnson"], "venue": "Big Data (Big Data), 2015 IEEE International Conference on. IEEE, 2015, pp. 1134\u20131143.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA, 2010, pp. 45\u201350.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 217\u2013226.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "RMEQ: A tool for computing equivalence groups in repeated measures studies. in: Linking literature", "author": ["AM M.S. Cohen"], "venue": "Information and Knowledge for Biology: Proceedings of the BioLINK2008 Workshop, 2008.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Randomized controlled trials (RCTs) are a key component of medical research and by far the best way of achieving results that can genuinely increase our knowledge about treatment effectiveness [1].", "startOffset": 193, "endOffset": 196}, {"referenceID": 1, "context": "and then based on full texts of a subset thereof, assessing their methodological qualities, data extraction and synthesis, and finally reporting the conclusions on the review question [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 2, "context": "Generating a clear bipartition is a quite difficult task [4] and a common measure of success in such scenario is the area under the ROC curve (AUC).", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "However, the optimization techniques based on the AUC can be reduced to binary classification [5] methods under specific settings.", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "It is worth mentioning that a recent systematic review of current approaches in abstract screening [6] reported that among the 69 publications it surveyed, only 22 report about Recall, 18 report about Precision, and 10 about AUC.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "Moreover, recent works [7], [8] suggest studying the variability of the reported metrics and advocate using a large number of repetitions (500\u00d7 2) to make the conclusions more reproducible.", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "Moreover, recent works [7], [8] suggest studying the variability of the reported metrics and advocate using a large number of repetitions (500\u00d7 2) to make the conclusions more reproducible.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "Feature Space Representation Uni, Bigram, MeSH [9], LDA [10], Uni + Cite [11]", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "Feature Space Representation Uni, Bigram, MeSH [9], LDA [10], Uni + Cite [11]", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "Feature Space Representation Uni, Bigram, MeSH [9], LDA [10], Uni + Cite [11]", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "Algorithm SVM Based [12], [13], Others [14]", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "Algorithm SVM Based [12], [13], Others [14]", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "Algorithm SVM Based [12], [13], Others [14]", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "Cross Validation 5\u00d72 [9], [11], [13], [15], [16], 10-fold [17]", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "Cross Validation 5\u00d72 [9], [11], [13], [15], [16], 10-fold [17]", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "Cross Validation 5\u00d72 [9], [11], [13], [15], [16], 10-fold [17]", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "Cross Validation 5\u00d72 [9], [11], [13], [15], [16], 10-fold [17]", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "Cross Validation 5\u00d72 [9], [11], [13], [15], [16], 10-fold [17]", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "Cross Validation 5\u00d72 [9], [11], [13], [15], [16], 10-fold [17]", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "of Reviews used 15 [11], [15], [16], 6 [12], 18 [9], 3 [18], 1 [17]", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "of Reviews used 15 [11], [15], [16], 6 [12], 18 [9], 3 [18], 1 [17]", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "of Reviews used 15 [11], [15], [16], 6 [12], 18 [9], 3 [18], 1 [17]", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "of Reviews used 15 [11], [15], [16], 6 [12], 18 [9], 3 [18], 1 [17]", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "of Reviews used 15 [11], [15], [16], 6 [12], 18 [9], 3 [18], 1 [17]", "startOffset": 48, "endOffset": 51}, {"referenceID": 16, "context": "of Reviews used 15 [11], [15], [16], 6 [12], 18 [9], 3 [18], 1 [17]", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "of Reviews used 15 [11], [15], [16], 6 [12], 18 [9], 3 [18], 1 [17]", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "Statistical Testing Post-hoc paired Wilcoxon test [9], Rank Group [11]", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "Statistical Testing Post-hoc paired Wilcoxon test [9], Rank Group [11]", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "Reported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]", "startOffset": 20, "endOffset": 23}, {"referenceID": 9, "context": "Reported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "Reported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "Reported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "Reported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "Reported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "Reported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Reported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "Reported Metric AUC [9], [11], Utility [12], Burden [12], Yield [12], WSS [11], Precision [13], Recall [11], F1 [13]", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "FOR MORE DETAILS ABOUT ALGORITHMS USED IN SYSTEMATIC REVIEWS PLEASE CONSULT [6], [14], AND FOR METRIC USAGE [6]", "startOffset": 76, "endOffset": 79}, {"referenceID": 12, "context": "FOR MORE DETAILS ABOUT ALGORITHMS USED IN SYSTEMATIC REVIEWS PLEASE CONSULT [6], [14], AND FOR METRIC USAGE [6]", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "FOR MORE DETAILS ABOUT ALGORITHMS USED IN SYSTEMATIC REVIEWS PLEASE CONSULT [6], [14], AND FOR METRIC USAGE [6]", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "A large body of past research has focused on automating the abstract screening process [9], [11]\u2013[13], [15], [16].", "startOffset": 87, "endOffset": 90}, {"referenceID": 9, "context": "A large body of past research has focused on automating the abstract screening process [9], [11]\u2013[13], [15], [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "A large body of past research has focused on automating the abstract screening process [9], [11]\u2013[13], [15], [16].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "A large body of past research has focused on automating the abstract screening process [9], [11]\u2013[13], [15], [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "A large body of past research has focused on automating the abstract screening process [9], [11]\u2013[13], [15], [16].", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "In terms of feature representation, most of the existing approaches [15], [16] use unigram, bigram, and MeSH (Medical Subject Headings).", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "In terms of feature representation, most of the existing approaches [15], [16] use unigram, bigram, and MeSH (Medical Subject Headings).", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "An alternative to the MeSH terms is extracting LDA based latent topics from the titles along with the abstracts and using them as features [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 9, "context": "Other methods [11], [15] utilize external information as features, such as citations and cocitations.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "Other methods [11], [15] utilize external information as features, such as citations and cocitations.", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": "In terms of methods, SVM-based learning algorithms are commonly used [9], [13], [15].", "startOffset": 69, "endOffset": 72}, {"referenceID": 11, "context": "In terms of methods, SVM-based learning algorithms are commonly used [9], [13], [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "In terms of methods, SVM-based learning algorithms are commonly used [9], [13], [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "According to a recent study [14], 13 different types of algorithms have been proposed in systematic reviews including SVM, Decision Trees, Naive Bayes, k-nearest neighbor, and neural networks.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "SVM [19] can learn faster than SVM.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "On the other hand, as Transductive learning [20] takes both the labeled and unlabeled citations into account, the corresponding learning algorithm is slower than SVM [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "On the other hand, as Transductive learning [20] takes both the labeled and unlabeled citations into account, the corresponding learning algorithm is slower than SVM [19].", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "Data imbalance in supervised classification is a well studied problem [19]\u2013[23].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "Data imbalance in supervised classification is a well studied problem [19]\u2013[23].", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "The first focuses on designing loss functions: KLD [24], Quadratic Mean [25], Cost sensitive", "startOffset": 51, "endOffset": 55}, {"referenceID": 23, "context": "The first focuses on designing loss functions: KLD [24], Quadratic Mean [25], Cost sensitive", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "classification [26], Mean Average Precision [27], Random Forest [28] with meta-cost, and AUC [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "classification [26], Mean Average Precision [27], Random Forest [28] with meta-cost, and AUC [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 26, "context": "classification [26], Mean Average Precision [27], Random Forest [28] with meta-cost, and AUC [19].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "classification [26], Mean Average Precision [27], Random Forest [28] with meta-cost, and AUC [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "The example of such methods are Borderline-SMOTE [29], Safe-levelSMOTE [30], and oversampling of the minority class along with undersampling the majority class.", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": "The example of such methods are Borderline-SMOTE [29], Safe-levelSMOTE [30], and oversampling of the minority class along with undersampling the majority class.", "startOffset": 71, "endOffset": 75}, {"referenceID": 29, "context": "The authors in [31]\u2013 [33] use probability calibration techniques.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "The authors in [31]\u2013 [33] use probability calibration techniques.", "startOffset": 21, "endOffset": 25}, {"referenceID": 32, "context": "However, BAL does not have any theoretical guarantee compared to online learning [34], [35].", "startOffset": 81, "endOffset": 85}, {"referenceID": 33, "context": "However, BAL does not have any theoretical guarantee compared to online learning [34], [35].", "startOffset": 87, "endOffset": 91}, {"referenceID": 34, "context": "In the uncertainty sampling-based methodologies, a large number of uncertainty metrics have been proposed; examples include entropy [36], smallest-margin [37], least confidence [38], committee disagreement [39], and version space reduction [34], [40].", "startOffset": 132, "endOffset": 136}, {"referenceID": 35, "context": "In the uncertainty sampling-based methodologies, a large number of uncertainty metrics have been proposed; examples include entropy [36], smallest-margin [37], least confidence [38], committee disagreement [39], and version space reduction [34], [40].", "startOffset": 154, "endOffset": 158}, {"referenceID": 36, "context": "In the uncertainty sampling-based methodologies, a large number of uncertainty metrics have been proposed; examples include entropy [36], smallest-margin [37], least confidence [38], committee disagreement [39], and version space reduction [34], [40].", "startOffset": 177, "endOffset": 181}, {"referenceID": 37, "context": "In the uncertainty sampling-based methodologies, a large number of uncertainty metrics have been proposed; examples include entropy [36], smallest-margin [37], least confidence [38], committee disagreement [39], and version space reduction [34], [40].", "startOffset": 206, "endOffset": 210}, {"referenceID": 32, "context": "In the uncertainty sampling-based methodologies, a large number of uncertainty metrics have been proposed; examples include entropy [36], smallest-margin [37], least confidence [38], committee disagreement [39], and version space reduction [34], [40].", "startOffset": 240, "endOffset": 244}, {"referenceID": 38, "context": "In the uncertainty sampling-based methodologies, a large number of uncertainty metrics have been proposed; examples include entropy [36], smallest-margin [37], least confidence [38], committee disagreement [39], and version space reduction [34], [40].", "startOffset": 246, "endOffset": 250}, {"referenceID": 10, "context": "Using \u201cmost uncertain\u201d samples based on these metrics improve the quality of the classifier to find the best separating hyperplane, and thus to improve its accuracy in classifying new instances [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 39, "context": "On the other hand, certainty based methods have also been shown to be effective to carry out active learning on imbalanced data sets, as demonstrated in [41].", "startOffset": 153, "endOffset": 157}, {"referenceID": 40, "context": "A very similar review system that is popular among law firms is Technology assisted Linear Review (TAR) [42]\u2013[46].", "startOffset": 104, "endOffset": 108}, {"referenceID": 44, "context": "A very similar review system that is popular among law firms is Technology assisted Linear Review (TAR) [42]\u2013[46].", "startOffset": 109, "endOffset": 113}, {"referenceID": 46, "context": "For Word2Vec [48], we train the model on the entire set of citations (abstracts and titles) available in an existing systematic review platform.", "startOffset": 13, "endOffset": 17}, {"referenceID": 47, "context": "We use Gensim [49] package with the following parameters: the number of context words as 5, the min word count as 15 and the number of dimensions in the latent space as 500.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "On the other hand, SVM is an implementation of the Support Vector Machine (SVM) formulation for optimizing multivariate performance measures [19].", "startOffset": 141, "endOffset": 145}, {"referenceID": 48, "context": "It exploits the alternative structural formulation of the SVM optimization problem for conventional binary classification with error rate [50].", "startOffset": 138, "endOffset": 142}, {"referenceID": 17, "context": "Furthermore, we follow the recommendation for error loss function from [19] to set the cost parameter for SVM :", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "Among the 61 reviews, 15 reviews are publicly available from [16] and the rest of the 46 reviews are collected from a deployed platform for producing systematic reviews.", "startOffset": 61, "endOffset": 65}, {"referenceID": 4, "context": "To be self contained, we reproduce the metric table from [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 49, "context": "So, we model the problem similar to [51]:", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Hence, an active learning approach has the potential to achieve better results [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "We use \u03b2 = 19 in our experimental evaluations following the suggestion from [18].", "startOffset": 76, "endOffset": 80}], "year": 2016, "abstractText": "A major task in systematic reviews is abstract screening, i.e., excluding, often hundreds or thousand of, irrelevant citations returned from a database search based on titles and abstracts. Thus, a systematic review platform that can automate the abstract screening process is of huge importance. Several methods have been proposed for this task. However, it is very hard to clearly understand the applicability of these methods in a systematic review platform because of the following challenges: (1) the use of non-overlapping metrics for the evaluation of the proposed methods, (2) usage of features that are very hard to collect, (3) using a small set of reviews for the evaluation, and (4) no solid statistical testing or equivalence grouping of the methods. In this paper, we use feature representation that can be extracted per citation. We evaluate SVM based methods (commonly used) on a large set of reviews (61) and metrics (11) to provide equivalence grouping of methods based on a solid statistical test. Our analysis also includes a strong variability of the metrics using 500x2 cross validation. While some methods shine for different metrics and for different datasets, there is no single method that dominates the pack. Furthermore, we observe that in some cases relevant (included) citations can be found after screening only 15-20% of them via a certainty based sampling. A few included citations present outlying characteristics and can only be found after a very large number of screening steps. Finally, we present an ensemble algorithm for producing a 5star rating of citations based on their relevance. Such algorithm combines the best methods from our evaluation and through its 5-star rating outputs a more easy-to-consume prediction.", "creator": "gnuplot 5.0 patchlevel 1"}}}