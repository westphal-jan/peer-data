{"id": "1610.01382", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC and Random Forest", "abstract": "Besides spoken words, speech signals also carry information about speaker gender, age, and emotional state which can be used in a variety of speech analysis applications. In this paper, a divide and conquer strategy for ensemble classification has been proposed to recognize emotions in speech. Intrinsic hierarchy in emotions has been utilized to construct an emotions tree, which assisted in breaking down the emotion recognition task into smaller sub tasks. The proposed framework generates predictions in three phases. Firstly, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the speech is classified as emotional, then in the second phase, it is further classified into positive and negative classes. Finally, individual positive or negative emotions are identified based on the outcomes of the previous stages. Several experiments have been performed on a widely used benchmark dataset. The proposed method was able to achieve improved recognition rates as compared to several other approaches.", "histories": [["v1", "Wed, 5 Oct 2016 12:16:35 GMT  (516kb)", "http://arxiv.org/abs/1610.01382v1", "8 pages, conference paper, The 2nd International Integrated Conference &amp; Concert on Convergence (2016)"]], "COMMENTS": "8 pages, conference paper, The 2nd International Integrated Conference &amp; Concert on Convergence (2016)", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["abdul malik badshah", "jamil ahmad", "mi young lee", "sung wook baik"], "accepted": false, "id": "1610.01382"}, "pdf": {"name": "1610.01382.pdf", "metadata": {"source": "CRF", "title": "Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC and Random Forest", "authors": ["Abdul Malik Badshah", "Jamil Ahmad", "Mi Young Lee", "Sung Wook Baik"], "emails": ["sbaik@sejong.ac.kr"], "sections": [{"heading": null, "text": "Besides spoken words, speech signals also carry information about speaker gender, age, and emotional state which can be used in a variety of speech analysis applications. In this paper, a divide and conquer strategy for ensemble classification has been proposed to recognize emotions in speech. Intrinsic hierarchy in emotions has been utilized to construct an emotions tree, which assisted in breaking down the emotion recognition task into smaller sub tasks. The proposed framework generates predictions in three phases. Firstly, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the speech is classified as emotional, then in the second phase, it is further classified into positive and negative classes. Finally, individual positive or negative emotions are identified based on the outcomes of the previous stages. Several experiments have been performed on a widely used benchmark dataset. The proposed method was able to achieve improved recognition rates as compared to several other approaches.\nKeywords: speech emotions, divide-and-conquer, emotion classification, random forest"}, {"heading": "1. Introduction", "text": "Body poses, facial expressions, and speech are the most common ways to express emotions [1]. However, certain emotions like disgust and boredom cannot be identified from gestures or facial expressions but could be effectively recognized from speech tone due to the difference in energy, speaking rate, speech linguistic and semantic information [2]. Speech emotion recognition (SER) has attracted increasing attention in the past few decades due to the increasing use of emotions in affective computing and human computer interaction. It has played a tremendous role in changing the way we interact with computers over the last few years [3]. Affective computing techniques use emotions to interact in more natural ways. Similarly, automatic voice response (AVR) systems can become more adaptive and user-friendly if affect states of users could be identified during interactions. The performance of such systems highly depend on the ability to accurately recognize emotions. SER is a challenging task due to the inherent complexity in emotional expression.\nInformation regarding the emotions of a person can help in several speech analysis applications such as human computer interaction, humanoid robots, mobile communication, and call centers [4]. Emergency call centers all around the world deal with fake calls on a daily basis. It has become necessary to avoid wastage of precious resources in responding to fake calls. Utilizing information like age, gender, emotional state, environmental sounds, and speech transcript, the situational seriousness could be assessed effectively. If a caller report an abnormal situation by calling an emergency call center, the SER can be used to assess whether the person is under stress, fear or not which can increase the truth rate of person and can help the call centers in making an effective decision. The emotion detection work presented here is a portion of a large project for lie detection from speech in emergency calls.\nManuscript received: MM/DD/YYYY / revised: MM/DD/YYYY / Corresponding Author: sbaik@sejong.ac.kr Tel:+82-02-3408-3797 Sejong University\nacoustic features. Three classifiers namely support vector machine (SVM) [5], Random Forest (RF) [6] and Decision Tree (DT) [7] were evaluated. Four models for each classifier were used at four stages to recognize the speech signals at each stage of the proposed method."}, {"heading": "2. Related Work", "text": "Speech emotion recognition has been studied vigorously in the past decade. Several different strategies involving a variety of feature extraction methods, and classification schemes have been proposed. Some of the recent works in this area are being presented here.\nVogt and Andre [8] proposed a gender-dependent emotion recognition model for emotion detection from speech. They used 20 different features consisting of 17 MFCC, 2 energy coefficients, and 1 pitch value for gender-independent and 30 features consisting of 22 MFCC, 3 energy and 5 pitch value for male and female emotions to train Na\u00efve Bayes classifiers. They evaluated their framework on two different datasets including Berlin and SmartKom. They achieved an improvement of 2% in overall recognition rates for Berlin dataset and 4% for SmartKom datasets, respectively. Lalitha et al. [9] developed a speaker emotion recognition system to recognize different emotions with a generalized feature set. They used and compared the result of two different classifiers including continuous hidden markov model (HMM) and support vector machine. Their proposed algorithm was able to achieve higher recognition rates for sad (83.33%), anger (91.66%), and fear (100%) on the validation set. However, emotions like boredom, disgust and happy were not recognized accurately and achieved 50%, 23.07 % and 26.31% accuracy rates, respectively. Shah et al. [10] conducted an experiment using Discrete Wavelet Transforms (DWTs) and Mel Frequency Cepstral Coefficients (MFCCs) for speaker independent automatic emotion recognition from speech. They achieved 68.5% recognition accuracy using DWT and 55% accuracy using MFCC. These accuracy rates were not satisfactory. Moreover, they only used four emotions namely neutral, happy, sad, and anger. Krishna et al. [11] also used MFCC and wavelet features. They used Gaussian Mixture Model (GMM) as classifier for emotion recognition. They concluded that using Sub-band based Cepstral Parameter (SBC) method provides higher recognition rates compared to MFCC. They achieved 70% recognition rate using SBC as compared to 51% using MFCC on their dataset.\nDespite the efforts in constructing efficient SER systems using various features and classification frameworks, there is still significant room for improvement. Implementing SER for real world applications is a very challenging task which require careful design strategies [12]."}, {"heading": "3. Proposed Method", "text": "This section presents the overall architecture of the proposed framework, explains feature extraction\nmethod, and highlights key design aspects of the classification scheme."}, {"heading": "3.1 Feature Extraction", "text": "Feature extraction is an important task in recognizing emotions from speech. The entire classification framework depends on the representational capability of the features being used. The proposed method uses acoustic features to recognize emotions. Mel Frequency Cepstral Coefficients (MFCCs) are robust to noise, possesses significant representational capability, and performs best with several speech analysis applications. Speech signals are divided into small frames of several milliseconds and 16 MFCC coefficients are extracted from each frame. These features are then used to train various classifiers for emotion modeling. MFCC features are well known and the readers are referred to [13, 14] for more details.\nThe Proposed method use DC rule to detect and classify seven different types of emotions in speech. The emotion recognition system is divided into 3 stages. The first stage detects whether the sound signal contains any emotion or not (neutral speech). The second stage classify emotional signal into positive and negative emotions. The third stage further classify positive emotions into happy or surprise, and the negative emotions into angry, disgust, fear or sad shown in figure 1. This break down of emotions into several stages offer several advantages. Firstly, it simplifies the emotion recognition task by dividing in to several phases allowing us to effectively train efficient classifiers for each phase. Secondly, it reduces the amount of effort needed to detect emotions because if no emotions are spotted in the first stage, no further processing is required. Thirdly, heterogeneous classification schemes could be combined to form a high performance ensemble. Finally, the confidence of outcomes at the different phases can be used as evidence to compute the final results.\nFor each stage in the classification framework, three different classifiers were evaluated. In phase 1, the objective is to detect emotions in speech. The training data was divided into neutral and emotional sets and the various classifiers were trained with it. Speech signals identified as neutral is not processed any further and the classification process ends immediately after phase 1. Furthermore, the classification schemes returned almost perfect recognition rate for neutral speech. The second phase attempts to further determine the nature of emotions being expressed by categorizing the emotional speech into positive or negative. Positive emotions include happy and surprise, whereas negative emotions include fear, anger, disgust, and sad. Finally, the positive or negative emotions into their respective base emotions. The DC based approach achieved better results as compared to using a single classifier for SER."}, {"heading": "4. Experiments and Results", "text": "This section presents results of various experiments performed with the speech dataset for emotion\nrecognition. It also highlights the major strengths and weaknesses observed during the evaluation process."}, {"heading": "4.1 Database Description", "text": "Surrey Audio-Visual Expressed Emotion (SAVEE) Database [15] is used to evaluate the performance of the proposed method. The SAVEE database was recorded from four native English male speakers at the University of Surrey. It consist of 7 different type of emotions namely anger, disgust, fear, happiness, sadness, surprise and neutral. The sound files vary from 2 to 7 seconds in duration depending on the sentence uttered by the speaker. Total of 120 utterance per speaker were recorded.\nThe experiment was performed on a Desktop PC with Intel Core i7-3770 CPU @ 3.40GHz, 16.0 GB RAM and 64-bit Windows 7 Professional. The feature extraction and classification modules were implemented using C++. Experimental results analysis were performed in Matlab 2015a. Mainly two types of experiments were performed. The first experiment was carried out to compare the performance of DC based approach and single multiclass classifier based approach. The second experiment was to evaluate the proposed method through different classifier including SVM, Decision Tree and Random Forest. Results of these experiments are described in detail in the following sections."}, {"heading": "4.3 Stage-wise Classification Performance", "text": "Stage wise classification performance is evaluated and discussed in this section. Fig. 2 shows the classification performance of DT, SVM, and RF. All these schemes achieved almost perfect recognition rates for neutral speech. However, recognition rates of emotional speech identification were relatively low with DT and SVM, achieving 81.11% and 80% accuracy, respectively. RF classifier achieved 92.2% accuracy for identifying emotional speech. Overall accuracy achieved by RF was the highest 96.11% at this stage.\nAt Stage 2, classifiers are evaluated for identifying emotional speech obtained from previous phase as negative or positive. Accuracy rates are shown in Fig. 3. SVM Classifier has a lowest accuracy among the classifiers used at this stage for both negative and positive emotions, achieving accuracies around 70% and 76.67%. Decision Tree achieved better performance in identifying positive emotions. However, its\nclasses achieving 90% overall accuracy at this stage.\nPositive emotions from stage 2 are then identified as happy or surprise. Similarly, negative emotions are classified into angry, disgust, fear, and sad. Results in Fig. 4 reveal that RF classifier achieves higher emotion recognition accuracy as compared to DT and SVM. RF achieved 87% accuracy for identifying positive emotions. Though, DT and SVM offers higher accuracy in recognizing happy emotions but their performance in recognizing surprise emotion was relatively low, 73.33% accuracy rate for SVM and 46.67% for DT.\nA multi-class emotion recognition model was trained to classify negative emotions from stage 2 into angry, disgust, fear and sad. Results in Fig. 5 show that SVM classifier failed to classify fear emotions achieving only 37% accuracy, while its accuracy for angry, disgust and sad was 80%, 33% and 86.6%, respectively. DT achieved high accuracy in recognition of sad emotion, whereas its performance for angry, disgust and fear were 93%, 60% and 46.6%, respectively. In case of RF, angry, disgust, and sad emotions were successfully recognized with respective accuracies of 100%, 93.33%, 73.3%. However, the accuracy for fear emotion was relatively low at 46.67 %. Most of the test samples having fear emotions were missclassified as disgust, as shown in Table 2. Comparatively, RF achieved high accuracy in classifying 4\nwas able to achieve accuracy of 60% only.\nOverall classification results for using RF as a single classifier is provided in Table 1, and results of the proposed DC scheme is given in Table 2. By using RF as a single multi-class classifier for SER, accuracy rates were very low for disgust, happy, and sad emotions where each of these emotions were classified with less than 50% accuracy. Similarly, other emotions like angry, fear, and surprise achieved accuracy of about 66% each. The proposed DC based scheme was able to improve overall recognition accuracy for almost all emotions. From the performance comparisons of different classifiers, it can be easily concluded that RF is suited best among the classifiers for the proposed method."}, {"heading": "4.4 Comparison with other SER approaches", "text": "Performance of the proposed scheme is compared with two similar approaches in Table 3. Results suggest that the proposed classification approach outperforms the other methods in identifying anger, disgust, neutral, and sad emotions. In case of fear, the proposed method performed better than [11], however, it was lower than the method described in [9]. Similarly, in case of happy emotion, our method achieved significantly better performance than [9] and slightly lower accuracy than [11]. Overall, the proposed method outperformed both methods [9] and [11] by 23% and 17%, respectively."}, {"heading": "5. Conclusion", "text": "In this paper, we presented a divide and conquer based approach to detect and classify emotions from speech signals. MFCC features are extracted from the speech signals which are used to train classifiers at three different stages of the proposed scheme. At the first stage, generic emotions are detected in speech by using a simple binary classifier which separates neutral speech from the emotional speech. At the second stage, emotions are further identified as positive or negative. Finally, individual emotions are identified at the last stage of the classification scheme. Three different classification schemes including DT, SVM, and RF\nbased emotions recognition scheme achieves high accuracy with RF classifier at each stage.\nIn future, we plan to incorporate Dempster-Shafer theory of fusion to combine the decisions of intermediate classifiers before making a final decision. We hope that this fusion will help to improve accuracy even further."}, {"heading": "Acknowledgements", "text": "a solution for situation-awareness based on the analysis of speech and environmental sounds)."}], "references": [{"title": "Facial and vocal expressions of emotion", "author": ["J.A. Russell", "J.-A. Bachorowski", "J.-M. Fernandez-Dols"], "venue": "Annual review of psychology, vol. 54, pp. 329-349, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Emotion-specific effects of facial expressions and postures on emotional experience", "author": ["S.E. Duclos", "J.D. Laird", "E. Schneider", "M. Sexter", "L. Stern", "O. Van Lighten"], "venue": "Journal of Personality and Social Psychology, vol. 57, p. 100, 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Emotion recognition in human-computer interaction", "author": ["R. Cowie", "E. Douglas-Cowie", "N. Tsapatsoulis", "G. Votsis", "S. Kollias", "W. Fellenz"], "venue": "Signal Processing Magazine, IEEE, vol. 18, pp. 32-80, 2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Dempster-Shafer Fusion Based Gender Recognition for Speech Analysis Applications", "author": ["J. Ahmad", "K. Muhammad", "S. i. Kwon", "S.W. Baik", "S. Rho"], "venue": "2016 International Conference on Platform Technology and Service (PlatCon), 2016, pp. 1-4.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "The nature of statistical learning theory: Springer", "author": ["V. Vapnik"], "venue": "Science & Business Media,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning, vol. 45, pp. 5-32, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Emotion recognition using a hierarchical binary decision tree approach", "author": ["C.-C. Lee", "E. Mower", "C. Busso", "S. Lee", "S. Narayanan"], "venue": "Speech Communication, vol. 53, pp. 1162-1171, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving automatic emotion recognition from speech via gender differentiation", "author": ["T. Vogt", "E. Andr\u00e9"], "venue": "Proc. Language Resources and Evaluation Conference (LREC 2006), Genoa, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Emotion Recognition through Speech Signal for Human-Computer Interaction", "author": ["S. Lalitha", "S. Patnaik", "T. Arvind", "V. Madhusudhan", "S. Tripathi"], "venue": "Electronic System Design (ISED), 2014 Fifth International Symposium on, 2014, pp. 217-218.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker independent automatic emotion recognition from speech: a comparison of MFCCs and discrete  8  A.M. Badshah, J Ahmad, MY Lee, S.W. Baik wavelet transforms", "author": ["A. Firoz Shah", "V. Vimal Krishnan", "A. Raji Sukumar", "A. Jayakumar", "P. Babu Anto"], "venue": "Advances in Recent Technologies in Communication and Computing, 2009. ARTCom'09. International Conference on, 2009, pp. 528-531.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Emotion recognition in speech using MFCC and wavelet features", "author": ["K. Krishna Kishore", "P. Krishna Satish"], "venue": "Advance Computing Conference (IACC), 2013 IEEE 3rd International, 2013, pp. 842- 847.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Survey on speech emotion recognition: Features, classification schemes, and databases", "author": ["M. El Ayadi", "M.S. Kamel", "F. Karray"], "venue": "Pattern Recognition, vol. 44, pp. 572-587, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparison of different implementations of MFCC", "author": ["F. Zheng", "G. Zhang", "Z. Song"], "venue": "Journal of Computer Science and Technology, vol. 16, pp. 582-589, 2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Gender Identification using MFCC for Telephone Applications-A Comparative Study", "author": ["J. Ahmad", "M. Fiaz", "S.-i. Kwon", "M. Sodanil", "B. Vo", "S.W. Baik"], "venue": "arXiv preprint arXiv:1601.01577, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Body poses, facial expressions, and speech are the most common ways to express emotions [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "However, certain emotions like disgust and boredom cannot be identified from gestures or facial expressions but could be effectively recognized from speech tone due to the difference in energy, speaking rate, speech linguistic and semantic information [2].", "startOffset": 252, "endOffset": 255}, {"referenceID": 2, "context": "It has played a tremendous role in changing the way we interact with computers over the last few years [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "Information regarding the emotions of a person can help in several speech analysis applications such as human computer interaction, humanoid robots, mobile communication, and call centers [4].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "Three classifiers namely support vector machine (SVM) [5], Random Forest (RF) [6] and Decision Tree (DT) [7] were evaluated.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "Three classifiers namely support vector machine (SVM) [5], Random Forest (RF) [6] and Decision Tree (DT) [7] were evaluated.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Three classifiers namely support vector machine (SVM) [5], Random Forest (RF) [6] and Decision Tree (DT) [7] were evaluated.", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "Vogt and Andre [8] proposed a gender-dependent emotion recognition model for emotion detection from speech.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "[9] developed a speaker emotion recognition system to recognize different emotions with a generalized feature set.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] conducted an experiment using Discrete Wavelet Transforms (DWTs) and Mel Frequency Cepstral Coefficients (MFCCs) for speaker independent automatic emotion recognition from speech.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] also used MFCC and wavelet features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Implementing SER for real world applications is a very challenging task which require careful design strategies [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "MFCC features are well known and the readers are referred to [13, 14] for more details.", "startOffset": 61, "endOffset": 69}, {"referenceID": 13, "context": "MFCC features are well known and the readers are referred to [13, 14] for more details.", "startOffset": 61, "endOffset": 69}, {"referenceID": 10, "context": "In case of fear, the proposed method performed better than [11], however, it was lower than the method described in [9].", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "In case of fear, the proposed method performed better than [11], however, it was lower than the method described in [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "Similarly, in case of happy emotion, our method achieved significantly better performance than [9] and slightly lower accuracy than [11].", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "Similarly, in case of happy emotion, our method achieved significantly better performance than [9] and slightly lower accuracy than [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "Overall, the proposed method outperformed both methods [9] and [11] by 23% and 17%, respectively.", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "Overall, the proposed method outperformed both methods [9] and [11] by 23% and 17%, respectively.", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "Emotions Emotion Recognition Accuracy Proposed Method Method [9] Method [11] Anger 100 91 90 Boredom 50 Disgust 73 23.", "startOffset": 61, "endOffset": 64}, {"referenceID": 10, "context": "Emotions Emotion Recognition Accuracy Proposed Method Method [9] Method [11] Anger 100 91 90 Boredom 50 Disgust 73 23.", "startOffset": 72, "endOffset": 76}], "year": 2016, "abstractText": "Besides spoken words, speech signals also carry information about speaker gender, age, and emotional state which can be used in a variety of speech analysis applications. In this paper, a divide and conquer strategy for ensemble classification has been proposed to recognize emotions in speech. Intrinsic hierarchy in emotions has been utilized to construct an emotions tree, which assisted in breaking down the emotion recognition task into smaller sub tasks. The proposed framework generates predictions in three phases. Firstly, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the speech is classified as emotional, then in the second phase, it is further classified into positive and negative classes. Finally, individual positive or negative emotions are identified based on the outcomes of the previous stages. Several experiments have been performed on a widely used benchmark dataset. The proposed method was able to achieve improved recognition rates as compared to several other approaches.", "creator": "Microsoft\u00ae Word 2013"}}}