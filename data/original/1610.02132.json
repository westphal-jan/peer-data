{"id": "1610.02132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks", "abstract": "Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal.", "histories": [["v1", "Fri, 7 Oct 2016 03:44:34 GMT  (701kb,D)", "http://arxiv.org/abs/1610.02132v1", null], ["v2", "Mon, 10 Apr 2017 22:25:06 GMT  (1597kb,D)", "http://arxiv.org/abs/1610.02132v2", null], ["v3", "Thu, 25 May 2017 08:05:19 GMT  (1819kb,D)", "http://arxiv.org/abs/1610.02132v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["dan alistarh", "demjan grubic", "jerry li", "ryota tomioka", "milan vojnovic"], "accepted": false, "id": "1610.02132"}, "pdf": {"name": "1610.02132.pdf", "metadata": {"source": "CRF", "title": "QSGD: Randomized Quantization for Communication-Optimal Stochastic Gradient Descent", "authors": ["Dan Alistarh", "Jerry Z. Li", "Ryota Tomioka", "Milan Vojnovic"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks."}, {"heading": "1 Introduction", "text": "The surge of massive data has led to significant interest in distributed algorithms for scaling computations in the context of machine learning and optimization. Such methods are based on distributing computation over multiple CPU threads, GPUs, or machines in large-scale computing clusters. Much attention has been devoted to scaling large-scale stochastic gradient descent (SGD) algorithms, which arise in many applications, including computer vision, speech processing, and various classification and regression tasks, such as product recommendation or click-through-rate prediction.\nIn brief, SGD can be defined as follows. Let f : Rn \u2192 R be a function which we want to minimize. We have access to stochastic gradients g\u0303 such that E[g\u0303(x)] = \u2207f(x). A standard instance of SGD will converge towards the minimum by iterating the procedure\nxt+1 = xt \u2212 \u03b7tg\u0303(xt),\nwhere xt is the current candidate, and \u03b7t is a variable step-size parameter. Notably, this arises in the setting where we are given i.i.d. data points X1, . . . , Xm generated from an unknown distribution D, and a loss function `(X, \u03b8), which measures the loss of the model \u03b8 at data point X . We wish to find a model \u03b8\u2217 which approximately minimizes f(\u03b8) = EX\u223cD[`(X, \u03b8)], the expected loss to the data. Since for each i, the function\u2207`(Xi, \u03b8) is a stochastic gradient for f , we can use SGD to find \u03b8\u2217. This framework captures many fundamental tasks, such as neural network training. Stochastic optimization techniques for these tasks have a long history, starting with Robbins and Munro [16].\nIn this paper, we focus on parallel SGD methods, which have received considerable attention recently due to their high scalability potential [3, 5, 15, 8]. Specifically, we consider a setting where a large dataset is partitioned among K processors, which collectively minimize a function f . Each processor maintains a local copy of the parameter vector xt; in each iteration, it obtains a new stochastic gradient update (corresponding to its local data). Processors then broadcast their gradient updates to their peers, and aggregate the gradients to compute the new iterate xt+1. This simple framework can be used to model various instances of parallel SGD, e.g. [15, 12, 8].\n\u2217Work performed in part while the author was an intern at Microsoft Research, Cambridge, UK.\nar X\niv :1\n61 0.\n02 13\n2v 1\n[ cs\n.L G\n] 7\nO ct\n2 01\nIn most current implementations of parallel SGD, in each iteration, each processor must communicate their entire gradient update to all other processors. If the gradient vector is dense, each processor will need to send and receive n floating-point numbers per iteration to/from each peer to communicate the gradients and maintain the parameter vector x. In practical applications, communicating the gradients in each iteration has been observed to be a significant performance bottleneck [18, 20, 5].\nOne way to reduce this cost has been to reduce the precision of floating-point operations. While popular in practice [7, 1], this procedure can only reduce communication by a small constant factor before diverging [6]. A more radical approach, and the original motivation behind our work, has been a family of compression heuristics known as 1-Bit SGD [18]. Roughly, 1-Bit SGD proposes to reduce each component of the gradient vector to just its sign (one bit), scaled by the average over the components of g\u0303 (a constant number of floats). This lossy compression method was experimentally observed to still allow convergence of SGD [18], under certain conditions; thanks to the reduction in communication, it enabled state-of-the-art scaling of deep neural networks (DNNs) for acoustic modelling [20]. However, it is currently not known if 1-Bit SGD converges, even under strong assumptions, and it is not clear if higher compression is achievable. Contributions. The goal of this paper is to understand the trade-offs between the communication cost of distributed stochastic gradient descent, and its convergence guarantees. We propose a family of lossy compression schemes called Quantized SGD (QSGD), by which processors can trade-off the number of bits communicated per iteration with the number of iterations until convergence.\nInformally, the most basic variant of QSGD ensures the following: for well-behaved functions f : Rn \u2192 R, QSGD converges to a minimum, ensuring that each processor transmits at most \u221a n(log n+ 1 + log e) +O(1) expected bits per iteration, and requires at most \u221a n times more iterations than unquantized SGD. It is perhaps surprising that a sublinear number of bits per iteration is sufficient for convergence, although theoretically this variant requires more iterations to converge.\nWe generalize this scheme to allow a smooth trade-off between the number of bits communicated per iteration and the number of iterations. At the other extreme of this trade-off, we can achieve the following guarantee: there exists a QSGD variant in which each processor transmits \u2264 2.8n+ 32 bits per iteration in expectation, and which converges at most 2\u00d7 slower than unquantized SGD. Figure 2.1 shows the total communication required to achieve some target error by our schemes. We see that that not only do we lower the communication required per iteration, but we also decrease it overall.\nThe key technique behind QSGD is a new random quantization scheme: given the vector of gradient updates at a processor for an iteration, we quantize each component by randomized rounding to a discrete set of values, in a principled way which preserves some statistical properties of the original. More precisely, the quantized vector is an unbiased estimator of the original, and the noise introduced by quantization is bounded and can be controlled by a tuning parameter. The resulting quantized vector is then encoded by an efficient lossless coding scheme.\nQSGD is a local, efficiently-computable encoding-decoding procedure, and can be plugged in directly into most variants of SGD. In addition to data-parallel SGD, we also show that quantization works for (non-stochastic) gradient descent, as well as variance-reduced SGD (SVRG) [10]. In conjunction with SVRG, QSGD becomes optimal in terms of total communication, matching a lower bound by Tsitsiklis and Luo [22]. Under additional assumptions, QSGD allows convergence to local minima for non-convex objectives, and convergence under asynchronous iterations.\nWe explore the practicality of QSGD via empirical validation on real datasets. In general, results confirm the analytical trade-off between communication and convergence time. Experiments using real-world data, such MNIST and CIFAR-10 datasets to train DNNs, show that QSGD does not significantly affect convergence rate. In fact, perhaps surprisingly, it was observed that for some non-convex objectives, QSGD may even improve accuracy for the same number of iterations. Related Work. One line of related research studies the communication complexity of convex optimization. In particular, Tsitsiklis and Luo [22] studied two-processor convex minimization, and provided a lower bound of \u2126(n(log n + log(1/ ))) bits on the communication cost of n-dimensional convex problems. On the positive side, they gave a non-stochastic algorithm by which the gradient components are rounded to a bounded set of integers before being transmitted. The total cost of their protocol for strongly convex problems is O(n log n(log n + log(1/ ))) bits. By contrast, our focus is on stochastic gradient methods. QSGD matches their lower bound when used in conjunction with SVRG, closing an open problem posed in their paper. Recent work by Arjevani and Shamir [2] focused on round\ncomplexity lower bounds on the number of communication rounds necessary for convex learning. A parallel line of research studied trade-offs between the communication budget and the achievable minimax risk for distributed statistical estimation [23].\nThere is an extremely rich area studying algorithms and systems for efficient distributed large-scale learning, e.g. [3, 7, 1, 18, 20, 15, 6]. In this area, the research closest to ours is that on 1-Bit SGD [18, 20], a heuristic for reducing the communication cost of SGD inspired by delta-sigma modulation [17], described above, using n bits and two floats per iteration. QSGD can achieve higher (sublinear) compression rates, and provably converges under standard assumptions. Another related scheme is Buckwild! [6], which provides convergence guarantees for asynchronous SGD under quantization errors, such as those arising from lower-precision arithmetic. Buckwild! can tolerate up to 8-bit integer precision, but direct one-bit quantization is observed to diverge. Roadmap. The remainder of the paper is structured as follows. Section 2 reviews some background results. Our main theoretical results are described in Section 3. Section 4 presents the results of our experiments. All proofs are available in the Appendix. In particular, in Appendix A and B, we present the encoding schemes, and prove their properties. In Appendix C, we show how to use QSGD for smooth non-convex objectives. In Appendix D we provide guarantees for quantized gradient descent, and in Appendix E we also provide guarantees for quantized SVRG methods."}, {"heading": "2 Preliminaries", "text": "We consider stochastic gradient descent (SGD), a family of algorithms for finding minima of a function f , given access to random gradients of f . There are many variants of SGD in the literature, with different preconditions and guarantees. Our techniques are rather portable, and can usually be applied in a black-box fashion on top of SGD. Therefore, for conciseness, we will focus on a basic setup for SGD. The following assumptions are standard; see e.g. [4] for a more thorough treatment.\nLet X \u2286 Rn be a known convex set, and let f : X \u2192 R be differentiable, convex, and unknown. We will assume the following smoothness condition on f :\nDefinition 2.1 (Smoothness). Let f : Rn \u2192 R be differentiable and convex. We say that it is L-smooth if for all x, y \u2208 Rn, we have\n0 \u2264 f(x)\u2212 f(y)\u2212\u2207f(y)T (x\u2212 y) \u2264 L 2 \u2016x\u2212 y\u201622 .\nWe assume repeated access to stochastic gradients, which on (possibly random) input x, outputs a direction which is in expectation the correct direction to move in. Formally:\nDefinition 2.2. Fix f : X \u2192 R. A stochastic gradient for f is a random function g\u0303(x) so that E[g\u0303(x)] = \u2207f(x). We say the stochastic gradient has second moment at most B if E[\u2016g\u0303\u201622] \u2264 B for all x \u2208 X . We say it has variance at most \u03c32 if E[\u2016g\u0303(x)\u2212\u2207f(x)\u201622] \u2264 \u03c32 for all x \u2208 X .\nWe pause to make a couple of remarks about these definitions. First, observe that any stochastic gradient with second moment boundB is automatically also a stochastic gradient with variance bound \u03c32 = B, since E[\u2016g\u0303(x)\u2212\u2207f(x)\u20162] \u2264 E[\u2016g\u0303(x)\u20162] as long as E[g\u0303(x)] = \u2207f(x). Second, in convex optimization, one often assumes a second moment bound when dealing with non-smooth convex optimization, and a variance bound when dealing with smooth convex optimization. However, for us it will be convenient to assume a second moment bound, though we deal primarily with smooth convex optimization. This does not seem to be a major distinction in theory or in practice, for instance, [4] often uses them interchangeably whenever it is convenient.\nGiven access to stochastic gradients, and a starting point x0, SGD builds iterates xt given by\nxt+1 = \u03a0X (xt \u2212 \u03b7tg\u0303t(xt)) .\nHere \u03a0X is projection onto X , and (\u03b7t)t\u22650 is a sequence of step sizes. In this setting, one can show:\ns 0 20 40 60 80 100\nT ot\nal c\nom m\nun ic\nat io\nn ef\nfic ie\nnc y\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nGeneralized QSGD Optimized QSGD Unquantized SGD\nTheorem 2.1 ([4], Theorem 6.3). Let X \u2286 Rn be convex, and let f : X \u2192 R be an unknown, convex, and L-smooth. Let x0 \u2208 X be given, and let R2 = supx\u2208X \u2016x\u2212 x0\u20162. Let T > 0 be fixed. Given repeated, independent access to stochastic gradients with variance bound \u03c32 for f , SGD with initial point x0 and constant step sizes \u03b7t = 1L+1/\u03b3 ,\nwhere \u03b3 = R\u03c3 \u221a 2 T , achieves\nE [ f ( 1\nT T\u2211 t=0 xt\n)] \u2212 min\nx\u2208X f(x) \u2264 R\n\u221a 2\u03c32\nT + LR2 T . (1)\nMinibatched SGD. A modification to the SGD scheme presented above often observed in practice is a technique known as minibatching. In minibatched SGD, updates are of the form xt+1 = \u03a0X (xt \u2212 \u03b7tG\u0303t(xt)), where G\u0303t(xt) = 1 m \u2211m i=1 g\u0303t,i, and where each g\u0303t,i is an independent stochastic gradient for f at xt. It is not hard to see that if g\u0303t,i are stochastic gradients with variance bound \u03c32, then the G\u0303t is a stochastic gradient with variance bound \u03c32/m. By inspection of Theorem 2.1, as long as the first term in (1) dominates, minibatched SGD requires 1/m fewer iterations to converge."}, {"heading": "2.1 Parallel Stochastic Gradient Descent", "text": "We consider synchronous data-parallel SGD, motivated by modelling real-world multi-GPU systems, and focus on the communication cost of SGD in this setting. We have a set of K processors p1, p2, . . . , pK who proceed in synchronous steps, and communicate using point-to-point messages. Each processor maintains a local copy of a vector x of dimension n, representing the current estimate of the minimizer. Each processor also has access to many, private, independent stochastic gradients for f . The algorithm proceeds in synchronous iterations, described in Algorithm 1.\nIn particular, each processor aggregates the value of x, then obtains random gradient updates for each component of x, then communicates these updates to all peers, and finally aggregates the received updates and applies them to its local model. Importantly, we add encoding and decoding steps for the gradients before and after send/receive in lines 3 and 7, respectively. In the following, whenever describing a variant of SGD, we assume the above general pattern, and only specify the encode/decode functions. It is also important to notice that the decoding step does not necessarily recover the original gradient g\u0303`; instead, we usually apply an approximate version of the gradient.\nWhen the encoding and decoding steps are the identity (i.e., no encoding / decoding), we shall refer to this algorithm as parallel SGD. In this case, it is a simple calculation to see that at each processor, if xt was the value of x that the processors held before iteration t, then the updated value of x by the end of this iteration is xt+1 = xt \u2212 (\u03b7t/K) \u2211K `=1 g\u0303\n`(xt), where each g\u0303` is a stochatic gradient. In particular, this update is merely a minibatched update of size K. Thus, by the discussion above, and by rephrasing Theorem 2.1, we have the following corollary:\nCorollary 2.2. Let X , f, L,x0, and R be as in Theorem 2.1. Fix > 0. Suppose we run parallel SGD on K processors, each with access to independent stochastic gradients with second moment bound B, with step size \u03b7t = 1/(L+ \u221a K/\u03b3),\nwhere \u03b3 is as in Theorem 2.1. Then if\nT = O ( R2 \u00b7max ( 2B\nK 2 , L\n)) , (2)\nthen E [ f (\n1 T \u2211T t=0 xt )] \u2212minx\u2208X f(x) \u2264 .\nIn most reasonable regimes, the first term of the max in the RHS of (2) will dominate the number of iterations necessary. In particular, we clearly see that in these regimes, the number of iterations depends linearly on the second moment bound B. This simple observation will be crucial for us later."}, {"heading": "3 Random Quantization SGD Schemes", "text": "In this section, we present our main results on random quantization SGD schemes. Each scheme is a lossy compression coding defined by a random quantization function, applied to each input stochastic gradient vector, followed by a lossless coding scheme used to communicate a quantized stochastic gradient vector. We first present such a loss-compression scheme that encodes a stochastic gradient vector of dimension nwith \u0398\u0303( \u221a n) bits. We then present a strict generalization of this scheme with a tuning parameter that allows to smoothly control the number of information bits used to encode a stochastic gradient vector between \u0398\u0303( \u221a n) and \u0398(n) bits. The special scheme is presented first because it is simpler to describe and thus serves as a gentle introduction for the more general scheme. For each lossy-compression scheme, we present two main results: (1) showing that a quantized stochastic gradient vector is unbiased and that it has a bounded second-moment of the `2 norm, which implies a bound on the number of iterates for the quantized SGD system, and (2) an upper bound on the expected number of information bits used to encode each input stochastic gradient vector, i.e., a bound on the expected number of bits communicated in each iteration round. Notation. Throughout, log denotes the base-2 logarithm. For any vector v \u2208 Rn, we let \u2016v\u20160 denote the number of nonzeros of v. We let F be the number of bits used to encode a float variable. (If F = O(log 1/ ), then our convergence results will hold up to (1 + poly( )) multiplicative error, which is usually negligible. In practice, 32 or 64 bits always suffice, therefore in the following F is considered constant.) For any string \u03c9 \u2208 {0, 1}\u2217, we will let |\u03c9| denote its length. For any scalar x \u2208 R, we let sgn (x) \u2208 {\u22121,+1} denote its sign, with the convention that sgn (0) = 1."}, {"heading": "3.1 A Simple Random Quantization SGD Scheme", "text": "We define a random quantization function, which for any vector v \u2208 Rn such that v 6= 0 is defined as a random vector Q(v) whose coordinates are given by\nQi(v) = \u2016v\u20162 \u00b7 sgn (vi) \u03bei(v) , (3)\nwhere \u03bei(v)\u2019s are independent random variables such that \u03bei(v) = 1 with probability |vi|/\u2016v\u20162, and \u03bei(v) = 0, otherwise. If v = 0, we define Q(v) = 0.\nThe key properties of Q[g\u0303(x)] are sparsity, unbiasedness, and bounded second moment as shown in the following lemma: Lemma 3.1. For any v \u2208 Rn, we have E[\u2016Q(v)\u20160] \u2264 \u221a n (sparsity), E[Q(v)] = v (unbiasedness), and E[\u2016Q(v)\u20162] \u2264\u221a\nn\u2016v\u201622 (second moment bound).\nProof. The first claim of the lemma follows from the following inequality\nE[\u2016\u03be(v)\u20160] = n\u2211 i=1 |vi| \u2016v\u20162 \u2264 \u221a n.\nThe second claim follows by the following series of relations\nE[Qi(v)] = \u2016v\u20162 \u00b7 sgn (vi)E [\u03bei(v)]\n= \u2016v\u20162 \u00b7 sgn (vi) |vi| \u2016v\u20162\n= sgn (vi) \u00b7 |vi| = vi .\nThe last claim of the lemma is established as follows:\nE[\u2016Q(v)\u20162] = n\u2211 i=1 \u2016v\u201622 E [ \u03bei(v) 2 ]\n= \u2016v\u20162 n\u2211 i=1 |vi| \u2264 \u221a n\u2016v\u201622 .\nThe sparsity allows us to succinctly encode Q(x), for any x, in expectation. The information contained in Q(v) can be expressed by (1) a float variable that encodes the value of \u2016v\u20162, (2) identities of the vector coordinates i for which \u03bei(v) = 1, and (3) the values of signs sgn (vi) for these coordinates. Let Code(Q(v)) denote a binary representation of such a tuple representation of Q(v). Then, one can show the following bound, whose proof is deferred to the end of this section. Lemma 3.2. For every vector v \u2208 Rn, we have E[|Code(Q(v))|] \u2264 \u221a n(log(n) + log(2e)) + F , where F is the number of bits for representing one floating point number.\nThese two lemmas together imply the following theorem.\nTheorem 3.3. Let f : Rn \u2192 R be fixed, and let x \u2208 Rn be arbitrary. If g\u0303(x) is a stochastic gradient for f at x with second moment bound B, then Q(g\u0303(x)) is a stochastic gradient for f at x with second moment bound \u221a nB. Moreover, in expectation Q(g\u0303(x)) can be communicated using \u221a n(log n+ log 2e) + F bits.\nIn particular, by Corollary 2.2, this means that in comparison to vanilla SGD, we require at most \u221a n times as many iterations to converge to the same error, but we communicate only O\u0303( \u221a n) bits per iteration, as compared to F \u00b7 n bits.\nFinally Lemma 3.2 can be formally shown as follows.\nProof of Lemma 3.2. We first note that conditional on \u2016\u03be(v)\u20160 = x, for some 0 \u2264 x \u2264 n, the number of information bits to encode the tuple representation of Q(v) is at most the sum of F bits to encode the value of \u2016v\u20162, log( ( n x ) ) bits to encode identities of coordinates i for which \u03bei(v) = 1, and x bits to encode the value of sgn (vi) for coordinates for which \u03bei(v) = 1. Moreover, note that\nE [ log (( n\n\u2016\u03be(v)\u20160\n))] = E [ log (( n\n\u2016\u03be(v)\u20160\n)) 1\u2016\u03be(v)\u20160>0 ] a \u2264 E [ \u2016\u03be(v)\u201601\u2016\u03be(v)\u20160>0 ] log(ne) = E [\u2016\u03be(v)\u20160] log(ne) \u2264 \u221a n log(ne)\nwhere (a) holds by the fact ( n k ) \u2264 (en/k)k \u2264 (en)k, for any 0 \u2264 k \u2264 n.\nHence, it follows that E[|Code(Q(v))|] = F + E [ log (( n\n\u2016\u03be(v)\u20160\n))] + E[\u2016\u03be(v)\u20160]\n\u2264 F + \u221a n log(ne) + \u221a n."}, {"heading": "3.2 A Generalized Random Quantization SGD Scheme", "text": "In this section, we consider a more general, parametrizable lossy-compression scheme for stochastic gradient vectors. As previously mentioned, this uses a random quantization function that is a strict generalization of that presented earlier, and that allows to tradeoff the number of information bits used to encode a stochastic gradient vector with the convergence rate. This random quantization function is denoted with Q(v, s), where s \u2265 1 is the tuning parameter, and for any v \u2208 Rn such that v 6= 0 is defined as\nQi(v, s) = \u2016v\u20162 \u00b7 sgn (vi) \u03bei(v, s) , (4)\nwhere \u03bei(v, s)\u2019s are independent random variables with distributions defined as follows. Let 0 \u2264 ` < s be an integer such that |vi|/\u2016v\u20162 \u2208 [`/s, (`+ 1)/s]. Then\n\u03bei(v, s) =\n{ `/s with probability 1\u2212 p ( |vi| \u2016v\u20162 , s ) ;\n(`+ 1)/s otherwise.\nHere, p(a, s) = as\u2212 ` for any a \u2208 [0, 1]. If v = 0, then we define Q(v, s) = 0. The random quantization function (3) corresponds to the special case s = 1. We obtain the three key properties as we show in the following lemma. Lemma 3.4. For any v \u2208 Rn, we have that E[\u2016Q(v, s)\u20160] \u2264 s2 + \u221a n (sparsity), E[Q(v, s)] = v (unbiasedness), and E[\u2016Q(v, s)\u201622] \u2264 (1 + min(n/s2, \u221a n/s))\u2016v\u201622 (second moment bound).\nProof. We show the proof of the first claim in Lemma A.5 in Appendix A. The second claim holds because E[\u03bei(v, s)] = ( `+ 1\u2212 s |vi|\n\u2016v\u20162 ) \u00b7 ` s + ( s |vi| \u2016v\u20162 \u2212 ` ) \u00b7 `+ 1 s = |vi| \u2016v\u20162 .\nWe thus turn our attention to the third claim of the lemma. We first note the following bound: E[\u03bei(v, s)2] = E[\u03bei(v, s)]2 + E [ (\u03bei(v, s)\u2212 E[\u03bei(v, s)])2 ] =\nv2i \u2016v\u201622 + 1 s2 p ( |vi| \u2016v\u20162 , s )( 1\u2212 p ( |vi| \u2016v\u20162 , s )) \u2264 v 2 i\n\u2016v\u201622 +\n1\ns2 p ( |vi| \u2016v\u20162 , s ) .\nUsing this bound, we have\nE[\u2016Q(v, s)\u20162] = n\u2211 i=1 E [ \u2016v\u201622\u03bei ( |vi| \u2016v\u20162 , s )2]\n\u2264 \u2016v\u201622 n\u2211 i=1 [ |vi|2 \u2016v\u201622 + 1 s2 p ( |vi| \u2016v\u20162 , s )]\n= ( 1 + 1\ns2 n\u2211 i=1 p ( |vi| \u2016v\u20162 , s )) \u2016v\u201622\na \u2264 ( 1 + min ( n\ns2 , \u2016v\u20161 s\u2016v\u20162\n)) \u2016v\u201622\n\u2264 ( 1 + min ( n\ns2 ,\n\u221a n\ns\n)) \u2016v\u201622 .\nwhere (a) follows from the fact that p(a, s) \u2264 1 and p(a, s) \u2264 as.\nNote that the factor cn,s := 1+min(n/s2, \u221a n/s) in the second-moment bound is parameterized with the dimension n and the tuning parameter s. For the special case s = 1, we have cn,s = \u0398( \u221a n), which is consistent with the result in Lemma 3.1. By varying the value of the parameter s between 1 and \u221a n, we can smoothly vary cn,s between \u0398( \u221a n) and \u0398(1). We can also see that as we increase s, the quantized gradient becomes less sparse. The sparsity bound is O( \u221a n) at s = 1 and O(n) at s = \u221a n. We also note that the distribution of \u03bei(v, s) is a unique distribution that has minimal variance over distributions that have support {0, 1/s, . . . , 1} and unbiased. Given a quantized vector, we then use a coding scheme which is a variant of Elias coding, to compress/decode the quantized vector. This scheme is given in Appendix A. In the sparse regime where we expect the quantized gradient to contain at most n/2 non-zero coordinates, we have the following theorem.\nTheorem 3.5. Let f : Rn \u2192 R be fixed, and let x \u2208 Rn be arbitrary. If g\u0303(x) is a stochastic gradient for f at x with second moment bound B, then Qs(g\u0303(x)) is a stochastic gradient for f at x with second moment bound(\n1 + min ( n s2 , \u221a n s )) B. Moreover, if s2 + \u221a n \u2264 n/2, there is an encoding scheme so that in expectation, the number\nof bits needed to communicate Qs(g\u0303(x)) is upper bounded by\nF + ( 3 + 3\n2 \u00b7 (1 + o(1)) log\n( 2(s2 + n)\ns2 + \u221a n\n)) (s2 + \u221a n) .\nThe communication cost can be, roughly speaking, broken down into one float number representing the norm and s2 + \u221a n (in expectation) bits and integers representing the signs, magnitudes, and positions of the non-zero coordinates.\nFor large s, the quantized gradient becomes dense, and we no longer need to communicate the positions of the non-zero coordinates and we obtain the following theorem.\nTheorem 3.6. Let f,x, and g\u0303(x) be as in Theorem 3.5. There is an encoding scheme forQs(g\u0303(x)) which in expectation has length\nF +\n( 1 + o(1)\n2\n( log ( 1 + s2 + min(n, s \u221a n)\nn\n) + 1 ) + 2 ) n .\nIn particular, if s = \u221a n, then this encoding requires \u2264 F + 2.8n bits in expectation.\nThe description of the quantization schemes in Theorems 3.5 and 3.6, their proofs, and their use in the context of SVRG [10], are deferred to the Appendix. These encoding schemes are not entirely obvious. Naively, one would expect that to encode \u03bei(v, s), one would need log s bits because it may take one of s integer values. However, we may improve upon this by observing that the vector of \u03bei(v, s) can only have a few coordinates which are large. Hence, by using an encoding scheme known as recursive Elias coding, which is more efficient at encoding small integers, we may decrease the number of bits needed to encode Qs. Indeed, using such arguments, we are able to show analogs of Lemma 3.2 for Qs, which achieve the compression rates claimed in Theorems 3.5 and 3.6."}, {"heading": "4 Experiments", "text": "We now empirically validate our approach, using experiments aimed at data-parallel and model-parallel settings. We have implemented QSGD on GPUs using the Chainer deep learning framework [21] , and on CPUs on top of the Hogwild! framework for parallel SGD [15]. Since the conclusions are roughly similar, we only report on GPU experiments below. Quantization vs. Accuracy. In the first set of experiments, we explore the relation between performance and the granularity at which quantization is applied to the gradient vector. In particular, we apply quantization to buckets of d consecutive vector components, using the basic random quantization (3). Setting d = 1 corresponds to no quantization (vanilla SGD), and d = n corresponds to full quantization, which sends O( \u221a n log n) bits per iteration. A simple extension of Theorem 3.3 predicts that the second moment bound then becomes \u221a dB. Here, our experiments deviate from the theory, as we use a deep network, with non-convex objective.\nMNIST dataset. The first dataset is the MNIST dataset of handwritten digits. The training set consists of 60,000 28 x 28 single digit images. The test set consists of 10,000 images. We train a two-layer perceptron with 4096 hidden units and ReLU activation with a minibatch size of 256 and step size of 0.1. Results are shown in Figure 2(a). Rather surprisingly, in terms of both training negative log-likelihood loss and the test accuracy, QSGD improves performance. This is consistent with recent work [14] suggesting benefits of added noise in training deep networks. We observed no such improvement for a linear model on the same dataset.\nThe total number of parameters of this model is 3.3 million, most of them lying in the first layer. Using Theorem 3.2, we can approximate the effective number of floats communicated by QSGD. Assuming F = 32, we get roughly 88k, 49k, and 29k effective floats for bucket sizes d = 256, 1024, and 4096, respectively. There is a massive reduction in communication since for each bucket we only need to communicate one float and the positions and signs of O\u0303( \u221a d) entries, each of which only requires O(log d) bits, which is typically much smaller than 32 (e.g., 11 bits for d = 256). CIFAR-10 dataset. Next, we consider the CIFAR-10 object classification dataset [11]. The original training set consists of 50,000 32 \u00d7 32 color images, augmented by translating, cropping with window size 28 \u00d7 28, and horizontal flipping. The augmented training set contains 1.8 million images.\nWe use a small VGG model [19] consisting of nine 2D convolution layers and three fully connected layers. The\ntotal number of parameters is roughly 22 million. All methods used momentum of 0.9. See the full paper for the details. When we only quantized the fully connected layers, we have found that the bucket size can be increased without much loss in accuracy (see Fig. 2(b)). The effective number of floats to be communicated are 1.5 million, 1.3 million, and 1.2 million for bucket sizes d = 256, 1024, and 4096, respectively. On the other hand, when we also applied the quantization to the convolutional layers, we observed a noticeable increase in the training objective as well as reduction in the test accuracy. The effective number of floats to be communicated are 580k, 312k, 176k, respectively. Parallelization. In Figure 3 (a) and (b), we show preliminary scalability experiments on MNIST, using up to 4 GPUs, compared with vanilla SGD and 1-Bit SGD [18]. The setup is the same as in the previous section, and we use double buffering [18] to perform communication and quantization concurrently with the computation. Experiments are preliminary in the sense that we did not fully optimize either 1-Bit SGD or QSGD to their full potential; in particular, quantized gradients are communicated in raw floats instead of using more efficient encoding. Model parallel setting. It is natural to ask if we can apply the same quantization function in the model parallel setting. More precisely, assume that different parts of the network are computed on different machines. Then we need to communicate the activations in the forward pass, as well as the gradients in the backward pass. The effect of introducing such a communication bottleneck in the network can be simulated on a single machine by introducing a \u201cquantization layer\u201d in the network. The most naive version of such a layer would randomly quantize forward and backward messages independently. This was a bad idea: we suspect that the independent sampling of the sparsity pattern broke the correlation between the activation in the forward pass and the update received in the backward pass. Therefore, we implemented a quantization layer that memorizes the sparsity pattern that was sampled in the forward pass and uses it in the backward pass. More precisely, in the forward pass, the layer receives the activation h = (hi) and each coordinate is sampled with probability |hi|/\u2016h\u20162 (see (3)). We denote the sampled binary mask by \u03be = (\u03bei); \u03bei = 1 if the coordinate is chosen, \u03bei = 0, otherwise. In the backward pass, the layer applies the binary mask \u03be to the gradient \u2206 with a debiasing term, which can be written as \u2206\u0303 = \u2206 \u25e6 \u03be \u25e6 (\u2016h\u20162/|hi|), where \u25e6 denotes the Hadamard (elementwise) product. The last term cancels the bias in the choice of the sparsity pattern. This can be implemented in a distributed setting by storing the sampling probability |hi|/\u2016h\u20162 and the binary mask \u03be on the sender and the receiver, respectively so that in the backward pass the receiver only needs to know the binary mask.\nFor this experiment, we use a three-layer perceptron with 4096 hidden units and ReLU activation in between. Each ReLU activation is followed by a quatization layer we described above. The result on the MNIST dataset is shown in Figure 3(c). We can see that the network tolerates quantization up to bucket size d = 4096, although there is a clear slow-down in convergence."}, {"heading": "5 Conclusions and Future Work", "text": "We have presented QSGD, a family of lossy compression techniques which allow a smooth trade off between the amount of communication per iteration and the running time. QSGD can communicate sublinear bits per iteration, and is communication-optimal for convex optimization. Experimental results suggest that QSGD can be practical, as it significantly reduces communication cost, and is competitive with standard uncompressed techniques. In future work, we plan to investigate optimized implementations for QSGD, and its potential for scaling in large-scale applications. On the theoretical side, it is interesting to consider settings where both gradients and the model are transmitted in quantized form, and applications of randomized quantization beyond SGD."}, {"heading": "A A Compression Scheme for Qs Matching Theorem 3.5", "text": "In this section, we describe a scheme for coding Qs and provide an upper bound for the expected number of information bits that it uses, which gives the bound in Theorem 3.5.\nObserve that for any vector v, the output of Q(v, s) is naturally expressible by a tuple (\u2016v\u20162,\u03c3, \u03b6), where \u03c3 is the vector of signs of the vi\u2019s and \u03b6 is the vector of \u03bei(v, s) values. With a slight abuse of notation, let us consider Q(v, s) as a function from R \\ {0} to Bs, where\nBs = {(A,\u03c3, z) \u2208 R\u00d7Rn\u00d7Rn : A \u2208 R\u22650,\u03c3i \u2208 {\u22121,+1}, zi \u2208 {0, 1/s, . . . , 1}} .\nWe define a coding scheme that represents each tuple in Bs with a codeword in {0, 1}\u2217 according to a mapping Codes : Bs \u2192 {0, 1}\u2217.\nTo encode a single coordinate, we utilize a lossless encoding scheme for positive integers known as recursive Elias coding or Elias omega coding.\nDefinition A.1. Let k be a positive integer. The recursive Elias coding of k, denoted Elias(k), is defined to be the {0, 1} string constructed as follows. First, place a 0 at the end of the string. If k = 0, then terminate. Otherwise, prepend the binary representation of k to the beginning of the code. Let k\u2032 be the number of bits so prepended minus 1, and recursively encode k\u2032 in the same fashion. To decode an recursive Elias coded integer, start with N = 1. Recursively, if the next bit is 0, stop, and output N . Otherwise, if the next bit is 1, then read that bit and N additional bits, and let that number in binary be the new N , and repeat.\nThe following are well-known properties of the recursive Elias code which are not too hard to prove.\nLemma A.1. For any positive integer k, we have\n1. |Elias(k)| \u2264 log k + log log k + log log log k . . .+ 1 = (1 + o(1)) log k + 1.\n2. The recursive Elias code of k can be encoded and decoded in time O(|Elias(k)|).\n3. Moreover, the decoding can be done without previously knowing a bound on the size of k.\nGiven a tuple (A,\u03c3, z) \u2208 Bs, our coding outputs a string S defined as follows. First, it uses F bits to encode A. It proceeds to encode using Elias recursive coding the position of the first nonzero entry of z. It then appends a bit denoting \u03c3i and follows that with Elias(szi). Iteratively, it proceeds to encode the distance from the current coordinate of z to the next nonzero using c, and encodes the \u03c3i and zi for that coordinate in the same way. The decoding scheme is also straightforward: we first read off F bits to construct A, then iteratively use the decoding scheme for Elias recursive coding to read off the positions and values of the nonzeros of z and \u03c3.\nWe can now present a full description of our lossy-compression scheme. For any input vector v, we first compute quantization Q(v, s), and then encode using Codes. In our notation, this is expressed as v \u2192 Codes(Q(v, s)). Lemma A.2. For any v \u2208 Rn and s2 + \u221a n \u2264 n/2, we have\nE[|Codes(Q(v, s))|] \u2264 ( 3 + 3\n2 \u00b7 (1 + o(1)) log\n( 2(s2 + n)\ns2 + \u221a n\n)) (s2 + \u221a n) .\nThis lemma together with Lemma 3.4 suffices to prove Theorem 3.5. We first show a technical lemma about the behavior of the coordinate-wise coding function c on a vector with\nbounded `p norm.\nLemma A.3. Let q \u2208 Rd be a vector so that for all i, we have that qi is a positive integer, and moreover, \u2016q\u2016pp \u2264 \u03c1. Then\nd\u2211 i=1 |Elias(qi)| \u2264 ( 1 + o(1) p log ( \u03c1 n ) + 1 ) n .\nProof. Recall that for any positive integer k, the length of Elias(k) is at most (1 + o(1)) log k + 1. Hence, we have\nd\u2211 i=1 |Elias(qi)| \u2264 (1 + o(1)) d\u2211 i=1 (log qi) + d\n\u2264 1 + o(1) p n\u2211 i=1 (log(qpi )) + d\n(a) \u2264 1 + o(1) p n log\n( 1\nn n\u2211 i=1 qpi\n) + +d\n\u2264 1 + o(1) p n log ( \u03c1 n ) + n\nwhere (a) follows from Jensen\u2019s inequality.\nWe can bound the number of information bits needed for our coding scheme in terms of the number of non-zeroes of our vector.\nLemma A.4. For any tuple (A,\u03c3, z) \u2208 Bs, the string Codes(A,\u03c3, z) has length of at most this many bits:\nF + ( (1 + o(1)) \u00b7 log ( n\n\u2016z\u20160\n) + 1 + o(1)\n2 log ( s2\u2016z\u201622 \u2016z\u20160 ) + 3 ) \u00b7 \u2016z\u20160.\nProof. First, the float A takes F bits to communicate. Let us now consider the rest of the string. We break up the string into a couple of parts. First, there is the subsequence S1 dedicated to pointing to the next nonzero coordinate of z. Second, there is the subsequence S2 dedicated to communicating the sign and c(zi) for each nonzero coordinate i. While these two sets of bits are not consecutive within the string, it is clear that they partition the remaining bits in the string. We bound the length of these two substrings separately.\nWe first bound the length of S1. Let i1, . . . , i\u2016z\u20160 be the nonzero coordinates of z. Then, from the definition of Codes, it is not hard to see that S1 consists of the encoding of the vector\nq(1) = (i1, i2 \u2212 i1, . . . , i\u2016z\u20160 \u2212 i\u2016z\u20160\u22121) ,\nwhere each coordinate of this vector is encoded using c. By Lemma A.3, since this vector has length \u2016z\u20160 and has `1 norm at most n, we have that\n|S1| \u2264 ( (1 + o(1)) log n\n\u2016z\u20160 + 1\n) \u2016z\u20160 . (5)\nWe now bound the length of S2. Per non-zero coordinate of z, we need to communicate a sign (which takes one bit), and c(szi). Thus by Lemma A.3, we have that\n|S2| = \u2016z\u20160\u2211 j=1 (1 + |Elias(szi)|)\n\u2264 \u2016z\u20160 + ( (1 + o(1))\n2 log s2\u2016z\u201622 \u2016z\u20160 + 1\n) \u2016z\u20160 . (6)\nPutting together (5) and (6) yields the desired conclusion.\nWe first need the following technical lemma about the number of nonzeros of Q(v, s) that we have in expectation.\nLemma A.5. Let v \u2208 Rn such that \u2016v\u20162 6= 0. Then\nE[\u2016Q(v, s)\u20160] \u2264 s2 + \u221a n.\nProof. Let u = v/\u2016v\u20162. Let I(u) denote the set of coordinates i of u so that ui \u2264 1/s. Since\n1 \u2265 \u2211 i 6\u2208I(u) u2i \u2265 (n\u2212 |I(u)|)/s2 ,\nwe must have that s2 \u2265 n\u2212 |I(u)|. Moreover, for each i \u2208 I(u), we have that Qi(v, s) is nonzero with probability ui, and zero otherwise. Hence\nE[L(v)] \u2264 n\u2212 |I(u)|+ \u2211 i\u2208I(u) ui \u2264 s2 + \u2016u\u20161 \u2264 s2 + \u221a n .\nProof of Lemma A.2 Let Q(v, s) = (\u2016v\u20162,\u03c3, \u03b6), and let u = v/\u2016v\u20162. Observe that we always have that\n\u2016\u03b6\u201622 \u2264 n\u2211 i=1 ( ui + 1 s )2 (a) \u2264 2 n\u2211 i=1 u2i + 2 n\u2211 i=1 1 s2 = 2 ( 1 + n s2 ) , (7)\nwhere (a) follows since (a+ b)2 \u2264 2(a2 + b2) for all a, b \u2208 R. By Lemma A.4, we now have that\nE[|Codes(Q(v, s)|] \u2264F + (1 + o(1))E [ \u2016\u03b6\u20160 log ( n\n\u2016\u03b6\u20160 )] + 1 + o(1) 2 E [ \u2016\u03b6\u20160 log ( s2R(\u03b6) \u2016\u03b6\u20160 )] + 3E[\u2016\u03b6\u20160]\n\u2264F + (1 + o(1))E [ \u2016\u03b6\u20160 log ( n\n\u2016\u03b6\u20160\n)] +\n1 + o(1)\n2 E\n[ \u2016\u03b6\u20160 log ( 2 ( s2 + n ) \u2016\u03b6\u20160 )] + 3 ( s2 + \u221a n ) ,\nby (7) and Lemma A.5.\nIt is a straightforward verification that the function f(x) = x log ( C x ) is concave for all C > 0. Moreover, it is increasing up until x = C/2, and decreasing afterwards. Hence, by Jensen\u2019s inequality, Lemma A.5, and the assumption that s2 + \u221a n \u2264 n/2, we have that\nE [ \u2016\u03b6\u20160 log ( n\n\u2016\u03b6\u20160\n)] \u2264 (s2 + \u221a n) log ( n\ns2 + \u221a n\n) , and\nE [ \u2016\u03b6\u20160 log ( 2 ( s2 + n ) \u2016\u03b6\u20160 )] \u2264 (s2 + \u221a n) log ( 2(s2 + n) s2 + \u221a n ) .\nSimplifying yields the expression in the Lemma."}, {"heading": "B A Compression Scheme for Qs Matching Theorem 3.6", "text": "For the case of the quantized SGD scheme that requires \u0398(n) bits per iteration, we can improve the constant factor in the bit length bound in Theorem A.2 by using a different encoding of Q(v, s). This corresponds to the regime where s = \u221a n, i.e., where the quantized update is not expected to be sparse. In this case, there is no advantage gained by transmitting the location of the next nonzero, since generally that will simply be the next coordinate of the vector. Therefore, we may as well simply transmit the value of each coordinate in sequence.\nMotivated by the above remark, we define the following alternative compression function. Define Elias\u2032(k) = Elias(k + 1) to be a compression function on all nonnegative natural numbers. It is easy to see that this is uniquely decodable. Let Code\u2032s be the compression function which, on input (A,\u03c3, z), simply encodes every coordinate of z in the same way as before, even if it is zero, using Elias\u2032. It is straightforward to show that this compression function is still uniquely decodable. Then, just as before, our full quantization scheme is as follows. For any arbitrary vector v, we first compute Q(v, s), and then encode using Code\u2032s. In our notation, this is expressed as v \u2192 Code \u2032 s(Q(v, s)). For this compression scheme, we show:\nLemma B.1. For any v \u2208 Rn, we have\nE[|Code\u2032s(Q(v, s))|] \u2264 F + ( 1 + o(1)\n2\n( log ( 1 + s2 + min(n, s \u221a n)\nn\n) + 1 ) + 2 ) n .\nIn particular, if s = \u221a n, then E[|Code\u2032s(Q(v, s))|] \u2264 F + 2.8n.\nIt is not hard to see that this is equivalent to the bound stated in Theorem 3.6. We start by showing the following lemma.\nLemma B.2. For any tuple (A,\u03c3, z) \u2208 Bs, the string Code\u2032s(A,\u03c3, z) has length of at most this many bits:\nF +\n( 1 + o(1)\n2\n( log ( 1 +\ns2\u2016z\u201622 n\n) + 1 ) + 2 ) n.\nProof. The proof of this lemma follows by similar arguments as that of Lemma A.4. The main differences are that (1) we do not need to encode the position of the nonzeros, and (2) we always encode Elias(k + 1) instead of Elias(k). Hence, for coordinate i, we require 1 + Elias(szi + 1) bits, since in addition to encoding zi we must also encode the\nsign. Thus the total number of bits may be bounded by\nF + n\u2211 i=1 (Elias(zi + 1) + 1) = F + n+ n\u2211 i=1 Elias(szi + 1)\n\u2264 F + n+ n\u2211 i=1 [(1 + o(1)) log(szi + 1) + 1]\n\u2264 F + 2n+ (1 + o(1)) n\u2211 i=1 log(szi + 1)\n\u2264 F + 2n+ 1 + o(1) 2 n\u2211 i=1 log((szi + 1) 2)\n(a) \u2264 F + 2n+ 1 + o(1) 2 n\u2211 i=1 (log(1 + s2z2i ) + log (2))\n(b) \u2264 F + 2n+ 1 + o(1) 2 n\n( log ( 1 + 1\nn n\u2211 i=1 s2z2i\n) + 1 )\nwhere (a) follows from basic properties of logarithms and (b) follows from the concavity of the function x 7\u2192 log(1 +x) and Jensen\u2019s inequality. Simplifying yields the desired statement.\nProof of Lemma B.1 As in the proof of Lemma A.2, let Q(v, s) = (\u2016v\u20162,\u03c3, \u03b6), and let u = v/\u2016v\u20162. By Lemma B.2, we have\nE[|Code\u2032s(Q(v, s)|] \u2264 F + ( 1 + o(1)\n2\n( E [ log ( 1 + s2R(\u03b6)\nn\n)] + 1 ) + 2 ) n\n(a) \u2264 F +\n( 1 + o(1)\n2\n( log ( 1 + E [ s2R(\u03b6) ] n ) + 1 ) + 2 ) n\n(b) \u2264 F + ( 1 + o(1)\n2\n( log ( 1 + s2(1 + min(n/s2, \u221a n/s)\nn\n) + 1 ) + 2 ) n\nwhere (a) follows from Jensen\u2019s inequality, and (b) follows from the proof of Lemma 3.4."}, {"heading": "C Quantization for Non-convex SGD", "text": "As stated previously, our techniques are portable, and apply easily to a variety of settings where SGD is applied. As a demonstration of this, we show here how we may use quantization on top of recent results which show that SGD converges to local minima when applied on smooth, non-convex functions.\nThroughout this paper, our theory only considers the case when f is a convex function. In many interesting applications such as neural network training, however, the objective is non-convex, where much less is known. However, there has been an interesting line of recent work which shows that SGD at least always provably converges to a local minima, when f is smooth. For instance, by applying Theorem 2.1 in [9], we immediately obtain the following convergence result for quantized SGD. Let Qs be the quantization function defined in Section 3.2. Here we will only state the convergence bound; the communication complexity per iteration is the same as in 3.2.\nTheorem C.1. Let f : Rn \u2192 R be a L-smooth (possibly nonconvex) function, and let x1 be an arbitrary initial point. Let T > 0 be fixed, and s > 0. Then there is a random stopping time R supported on {1, . . . , N} so that QSGD with\nData: Parameter vector x 1 procedure GradientDescent 2 for each iteration t do 3 Q(\u2207f(x))\u2190 Quantize(\u2207f(x))) //quantize gradient 4 x\u2190 x\u2212 \u03b7tQ(\u2207f(x)) //apply gradient 5 end\nAlgorithm 2: The gradient descent algorithm with gradient encoding.\nquantization function Qs, and constant stepsizes \u03b7 = O(1/L) and access to stochastic gradients of f with second moment bound B satisfies\n1 L E [ \u2016\u2207f(x)\u201622 ] \u2264 O\n(\u221a L(f(x1)\u2212 f\u2217)\nN +\n(1 + min(n/s2, \u221a n/s))B\nL\n) .\nObserve that the only difference in the assumptions in [9] from what we generally assume is that they assume a variance bound on the stochastic gradients, whereas we prefer a second moment bound. Hence our result applies immediately to their setting.\nAnother recent result [13] demonstrates local convergence for SGD for smooth non-convex functions in asynchronous settings. The formulas there are more complicated, so for simplicity we will not reproduce them here. However, it is not hard to see that quantization affects the convergence bounds there in a manner which is parallel to Theorem 3.5."}, {"heading": "D Quantized Gradient Descent: Description and Analysis", "text": "In this section, we consider the effect of lossy compression on standard (non-stochastic) gradient descent. Since this procedure is not data-parallel, we will first have to modify the blueprint for the iterative procedure, as described in Algorithm 2. In particular, we assume that, instead of directly applying the gradient to the iterate xt+1, the procedure first quantizes the gradient, before applying it. This setting models a scenario where the model and the computation are performed by different machines, and we wish to reduce the communication cost of the gradient updates.\nWe now give a quantization function tailored for gradient descent, prove convergence of gradient descent with quantization, and then finally bound the length of the encoding. The Quantization Function. We consider the following deterministic quantization function, inspired by [18]. For any vector v \u2208 Rn, let I(v) be the smallest set of indices of v such that\u2211\ni\u2208I(v)\n|vi| \u2265 \u2016v\u2016.\nFurther, define Q(v) to be the vector\nQ(v)i =  \u2016v\u2016 if x \u2265 0 and i \u2208 I(v);\u2212\u2016v\u2016 if x < 0 and i \u2208 I(v); 0 otherwise.\nPractically, we preserve the sign for each index in I(v), the 2-norm of v, and cancel out all remaining components of v. Note that we can use the same encoding as in Section 3.1. Convergence Bound. We begin by proving some properties of our quantization function. We have the following:\nLemma D.1. For all v \u2208 Rn, we have\n1. vTQ(v) \u2265 \u2016v\u20162,\n2. |I(v)| \u2264 \u221a n, and 3. \u2016Q(v)\u20162 \u2264 \u221a n\u2016v\u20162.\nProof. For the first claim, observe that vTQ(v) = \u2016v\u2016 \u2211 i\u2208I(v) |vi| \u2265 \u2016v\u20162.\nWe now prove the second claim. Let v = (v1, . . . , vn), and without loss of generality, assume that |vi| \u2265 |vi+1| for all i = 1, . . . , n\u2212 1, so that the coordinates are in decreasing order. Then I(v) = {1, . . . , D} for some D.\nWe show that if D \u2265 \u221a n then \u2211D i=1 |vi| \u2265 \u2016v\u2016, which shows that |I(v)| \u2264\n\u221a n. Indeed, we have that(\nD\u2211 i=1 |vi|\n)2 =\nD\u2211 i=1 v2i + \u2211 i6=j i,j\u2264D |vi||vj |\n\u2265 D\u2211 i=1 v2i + (D 2 \u2212D)v2D+1 .\nOn the other hand, we have\n\u2016v\u20162 = D\u2211 i=1 v2i + n\u2211 D+1 v2i\n\u2264 D\u2211 i=1 v2i + (n\u2212D)v2D+1\nand so we see that if D = \u221a n, we must have (\u2211D i=1 |vi| )2 \u2265 \u2016v\u20162, as claimed.\nFor the third claim, observe that \u2016Q(v)\u20162 = \u2016v\u20162 \u00b7 |I(v)|; thus the claim follows from the previous upper bound on the cardinality of I(v).\nTo establish convergence of the quantized method, we prove the following theorem.\nTheorem D.2. Let f : Rn \u2192 R be a `-strongly convex, L-smooth function, with global minimizer x\u2217, and condition number \u03ba = L/`. Then, for all step sizes \u03b7 satisfying \u03b7 \u2264 O ( `\nL2 \u221a n\n) , for all T \u2265 1, and all initial points x0, we have\nf (xT )\u2212 f (x\u2217) \u2264 exp ( \u2212\u2126 ( 1\n\u03ba2 \u221a n\n) T ) (f (x0)\u2212 f (x\u2217)) .\nProof. We first establish the following two properties:\nLemma D.3. Let f be `-strongly convex and L-smooth. Then,\n1. for all x \u2208 Rn,\n` 2 \u2016x\u2212 x\u2217\u20162 \u2264 f(x)\u2212 f(x\u2217) \u2264 L 2 \u2016x\u2212 x\u2217\u20162 .\n2. for all x \u2208 Rn,\n\u2207f(x)TQ(\u2207f(x)) \u2265 `(f(x)\u2212 f(x\u2217)) .\nProof. The first property follows directly from the definitions of strong convexity and smoothness. We now show the second property. If x = x\u2217 the property trivially holds so assume that this does not happen. By Lemma D.1, we have\n\u2207f(x)TQ(\u2207f(x)) \u2265 \u2016\u2207f(x)\u20162 .\nWe then have\nf(x)\u2212 f(x\u2217) \u2264 \u2207f(x)T (x\u2212 x\u2217) \u2264 \u2016\u2207f(x)\u2016 \u2016x\u2212 x\u2217\u2016 ,\nwhere the first inequality follows from convexity, and the second from Cauchy-Schwartz. From strong convexity we then have that `2\u2016x\u2212 x \u2217\u20162 \u2264 \u2207fT (x)(x\u2212 x\u2217) from which we get that\n` 2 \u2016x\u2212 x\u2217\u2016 \u2264 \u2207f(x)T x\u2212 x\n\u2217\n\u2016x\u2212 x\u2217\u2016 \u2264 \u2016\u2207f(x)\u2016 ,\nwhere the last line follows since from self-duality of the 2-norm, we know that for all vectors v \u2208 Rn, we have \u2016v\u20162 = sup\u2016u\u2016=1 vTu. Putting these two things together yields that\nf(x)\u2212 f(x\u2217) \u2264 2 ` \u2016\u2207f(x)\u20162 \u2264 2 ` \u2207f(x)TQ(\u2207f(x)) ,\nas claimed.\nWith all this in place, we can now complete the proof of the theorem. Fix t \u2265 0. By applying the lemma, we have:\n\u2207f(xt)T (xt+1 \u2212 xt) = \u2212\u03b7\u2207f(xt)TQ(\u2207f(xt)) \u2264 \u2212\u03b7 `\n2 (f(x)\u2212 f(x\u2217)) . (8)\nMoreover, observe that, from standard properties of smooth functions [4], we have\n1\n2L \u2016\u2207f(x)\u20162 \u2264 f(x)\u2212 f(x\u2217) . (9)\nThus, we obtain the following chain of inequalities:\nf(xt+1)\u2212 f(xt) (a) \u2264 \u2207f(xt+1)T (xt+1 \u2212 xt)\n= \u2207f(xt)T (xt+1 \u2212 xt) + (\u2207f(xt+1)\u2212\u2207f(xt))T (xt+1 \u2212 xt) (b) \u2264 \u2212\u03b7 ` 2 (f(xt)\u2212 f(x\u2217)) + \u2016\u2207f(xt+1)\u2212\u2207f(xt)\u2016 \u2016xt+1 \u2212 xt\u2016\n(c) \u2264 \u2212\u03b7 ` 2 (f(xt)\u2212 f(x\u2217)) + L\u2016xt+1 \u2212 xt\u20162 = \u2212\u03b7 ` 2 (f(xt)\u2212 f(x\u2217)) + \u03b72L\u2016Q(\u2207f(xt))\u20162\n(d) \u2264 \u2212\u03b7 ` 2 (f(xt)\u2212 f(x\u2217)) + \u03b72L \u221a n\u2016\u2207f(xt)\u201622\n(e) \u2264 \u2212\u03b7 ` 2 (f(xt)\u2212 f(x\u2217)) + \u03b722L2 \u221a n(f(xt)\u2212 f(x\u2217))\n= ( \u2212\u03b7 `\n2 + 2\u03b72L2\n\u221a n ) (f(xt)\u2212 f(x\u2217)) ,\nwhere (a) follows from the convexity of f , (b) follows from Equation 8 and the Cauchy-Schwarz inequality, (c) follows from the L-smoothness of f , (d) follows from Lemma D.1, and (e) follows from Equation 9. By our choice of \u03b7, we know that the RHS of Equation 10 is negative. Hence, by Lemma D.3 and the definition of \u03b7, we have\nf(xt+1)\u2212 f(xt) \u2264 \u2212\u2126 ( 1\n\u03ba2 \u221a n\n) (f (xT )\u2212 f (x\u2217)) . (10)\nLetting \u03b4t = f(xt)\u2212f(x\u2217), and observing that f(xt+1)\u2212f(xt) = \u03b4t+1\u2212\u03b4t, we see that Equation 10 is equivalent to the statement that\n\u03b4t+1 \u2264 ( 1\u2212 \u2126 ( 1\n\u03ba2 \u221a n\n)) \u03b4t .\nThus altogether we have\n\u03b4T \u2264 ( 1\u2212 \u2126 ( 1\n\u03ba2 \u221a n\n))T \u03b40\n\u2264 exp ( \u2212\u2126 ( 1\n\u03ba2 \u221a n\n) T ) \u03b40 ,\nas claimed.\nEncoding Length. By an argument similar to Lemma 3.2, we obtain the following:\nTheorem D.4. Let v \u2208 Rn. Then\n|Code(Q(v))| \u2264 \u221a n(log(n) + 1 + log(e)) + F."}, {"heading": "E Quantized SVRG", "text": "Variance Reduction for Sums of Smooth Functions. One common setting in which SGD sees application in machine learning is when f can be naturally expressed as a sum of smooth functions. Formally, we assume that f(x) = 1m \u2211m i=1 fi(x). When f can be expressed as a sum of smooth functions, this lends itself naturally to SGD. This is because a natural stochastic gradient for f in this setting is, on input x, to sample a uniformly random index i, and output \u2207fi(x). We will also impose somewhat stronger assumptions on the f and fi, namely, that f is strongly convex, and that the fi are convex and smooth.\nDefinition E.1 (Strong Convexity). Let f : Rn \u2192 R be a differentiable function. We say that f is `-strongly convex if for all x, y \u2208 Rn, we have\nf(x)\u2212 f(y) \u2264 \u2207f(x)T (x\u2212 y)\u2212 ` 2 \u2016x\u2212 y\u201622 .\nObserve that when ` = 0 this is the normal definition of convexity. Note that it is well-known that even if we impose these stronger assumptions on f and fi, then by only applying SGD one still cannot achieve exponential convergence rates, i.e. error rates which improve as exp(\u2212T ) at iteration T . (Such a rate is known in the literature as linear convergence.) However, an epoch-based modification of SGD, known as stochastic variance reduced gradient descent (SVRG) [10], is able to give such rates in this specific setting. We describe the method below, following the presentation of Bubeck [4].\nLet y(1) \u2208 Rn be an arbitrary point. For p = 1, 2, . . . , P , we let x(p)1 = y(p). Each p is called an epoch. Then, within epoch p, for t = 1, . . . , T , we let i(p)t be a uniformly random integer from [m] completely independent from everything else, and we set:\nx (p) t+1 = x (p) t \u2212 \u03b7\n( \u2207f\ni (p) t (x (p) t )\u2212\u2207fi(p)t (y\n(p)) +\u2207f(y(p)) ) .\nWe then set\ny(p+1) = 1\nk k\u2211 i=1 x (p) t .\nWith this iterative scheme, we have the following guarantee: Theorem E.1 ([10]). Let f(x) = 1m \u2211m i=1 fi(x), where f is `-strongly convex, and fi are convex and L-smooth, for all i. Let x\u2217 be the unique minimizer of f over Rn. Then, if \u03b7 = O(1/L) and T = O(L/`), we have\nE [ f(y(p+1)) ] \u2212 f(x\u2217) \u2264 0.9p ( f(y(1))\u2212 f(x\u2217) ) . (11)\nQuantized SVRG In parallel SVRG, we are givenK processors, each processor i having access to fim/K , . . . , f(i+1)m/K\u22121. The goal is the same as before: to approximately minimize f = 1m \u2211m i=1 fi. For processor i, let hi = 1 m \u2211(i+1)m/K\u22121 j=im/K fi\nbe the portion of f that it knows, so that f = \u2211K i=1 hi.\nA natural question is whether we can apply randomized quantization to reduce communication for parallel SVRG. Whenever one applies our quantization functions to the gradient updates in SVRG, the resulting update is no longer an update of the form used in SVRG, and hence the analysis for SVRG does not immediately give any results in black-box fashion. Instead, we prove that despite this technical issue, one can quantize SVRG updates using our techniques and still obtain the same convergence bounds.\nLet Q\u0303(v) = Q(v, \u221a n), where Q(v, s) is defined as in Section 3.2. Our quantized SVRG updates are as follows. Given arbitrary starting point x0, we let y(1) = x0. At the beginning of epoch p, processor i broadcasts Hp,i = Q\u0303(\u2207hi(y(p))). Each processor then computes Hp = \u2211 i=1Hp,i, and sets Within each epoch, for t = 1, . . . , T , and for i = 1, . . . ,K, we let j(p)i,t be a uniformly random integer from [m] completely independent from everything else. Then, in iteration t in epoch s, processor i broadcasts the update vector\nu (p) t,i = Q\u0303\n( \u2207f\nj (p) i,t (x (p) t )\u2212\u2207fj(p)i,t (y (p)) +Hs\n) .\nEach processor then computes the total update for that iteration u(p)t = 1 K \u2211K i=1 ut,i, and sets x (p) t+1 = x (p) t \u2212 \u03b7ut. At\nthe end of epoch p, each processor sets y(p+1) = 1T \u2211T t=1 x (p) t .\nLet us first consider the communication cost of quantized SGD. By Theorem B.1, each processor transmits at most F + 2.8n bits per iteration, and then an additional F + 2.8n bits per epoch to communicate the Hs,i. We have just proven the following theorem:\nCorollary E.2. Quantized SGD with R epochs and T iterations per epoch requires \u2264 R(F + 2.8n)(T + 1) bits of communication per processor.\nAnalysis of Quantized SVRG. As with the case of quantized SGD, it is not hard to see that the parallel updates are equivalent to minibatched updates, and serve only to decrease the variance of the random gradient estimate. Hence, as before, for simplicity of presentation, we will consider the effect of quantization on convergence rates on a single processor. In this case, the updates can be written down somewhat more simply. Namely, in iteration t of epoch p, we have that\nx (p) t+1 = x (p) t \u2212 \u03b7Q\u0303 (p) t\n( \u2207f\nj (p) t (x (p) t )\u2212\u2207fj(p)t (y\n(p)) + Q\u0303(p)(y) ) ,\nwhere j(p)t is a random index of [m], and Q\u0303 (p) t and Q\u0303 (p) are all different, independent instances of Q\u0303. In this setting, we show:\nTheorem E.3. Let f, fi, `, L,x\u2217, \u03b7, and T be as in Theorem E.1. Then, for all r \u2265 1, quantized SVRG with initial point y(1) satisfies Equation (11).\nProof. We follow the presentation in [4]. Fix an epoch p \u2265 1, and let E denote the expectation taken with respect to the randomness within that epoch. We will show that\nE [ f ( y(p+1) )] \u2212 f(x\u2217) = E\n[ 1\nT T\u2211 i=1 x (p) t\n] \u2212 f(x\u2217) \u2264 0.9 ( f ( y(p) ) \u2212 f(x\u2217) ) .\nThis clearly suffices to show the theorem. Because we only deal with a fixed epoch, for simplicity of notation, we shall proceed to drop the dependence on p in the notation. For t = 1, . . . , T , let vt = Q\u0303t ( \u2207fjt(xt)\u2212\u2207fjt(y)\u2212 Q\u0303(y) ) be the update in iteration t. Following the proof in [4], one sees that it suffices to show the following two equations:\nEjt,Q\u0303t,Q\u0303 [vt] = \u2207f(xt) , and Ejt,Q\u0303t,Q\u0303 [ \u2016vt\u20162 ] \u2264 C \u00b7 L (f(xt)\u2212 f(x\u2217) + f(y)\u2212 f(x\u2217)) ,\nwhere C is some universal constant. That the first equation is true follows from the unbiasedness of Q\u0303. We now show the second. We have:\nEjt,Q\u0303t,Q\u0303 [ \u2016vt\u20162 ] = Ejt,Q\u0303 EQ\u0303s [ \u2016vt\u20162 ] (a)\n\u2264 2Ejt,Q\u0303 [\u2225\u2225\u2225\u2207fjt(xt)\u2212\u2207fjt(y) + Q\u0303(y)\u2225\u2225\u22252] (b)\n\u2264 4Ejt [ \u2016\u2207fjt(xt)\u2212\u2207fjt(x\u2217)\u2016 2 ] + 4Ejt,Q\u0303 [\u2225\u2225\u2225\u2207fjt(x\u2217)\u2212\u2207fjt(y) + Q\u0303(y)\u2225\u2225\u22252] (c)\n\u2264 4Ejt [ \u2016\u2207fjt(xt)\u2212\u2207fjt(x\u2217)\u2016 2 ] + 8Ejt,Q\u0303 [ \u2016\u2207fjt(x\u2217)\u2212\u2207fjt(y) +\u2207f(y)\u2016 2 ]\n+ 8Ejt,Q\u0303 [\u2225\u2225\u2225\u2207f(y)\u2212 Q\u0303(y)\u2225\u2225\u22252] (d)\n\u2264 4Ejt [ \u2016\u2207fjt(xt)\u2212\u2207fjt(x\u2217)\u2016 2 ] + 8Ejt,Q\u0303 [ \u2016\u2207fjt(x\u2217)\u2212\u2207fjt(y) +\u2207f(y)\u2016 2 ]\n+ 16 \u2016\u2207f(y)\u20162\n(e) \u2264 4L (f(xt)\u2212 f(x\u2217)) + 8L (f(y)\u2212 f(x\u2217)) + 32L(f(y)\u2212 f(x\u2217)) \u2264 C \u00b7 L (f(xt)\u2212 f(x\u2217) + f(y)\u2212 f(x\u2217)) ,\nas claimed, for some (admittedly large) constant C, although we have not tried to optimize the constant in this section. Here (a) follows from Lemma 3.4, (b) and (c) follows from the fact that (a+ b)2 \u2264 2a2 + 2b2 for all scalars a, b, (d) follows from Lemma 3.4, and (e) follows from Lemma 6.4 in [4] and the standard fact that \u2016\u2207f(y)\u20162 \u2264 2L(f(y \u2212 f(x\u2217)) if f is `-strongly convex.\nIn particular, observe that when L/` is a constant, this implies that for all epochs r, we may communicate O(r(F + n)) bits and get an error rate of the form (11). Up to constant factors, this matches the lower bound given in [22]. This achieves the upper bound given in the same paper, but with a more efficient algorithm, when f is expressible as a sum of smooth functions."}, {"heading": "F Details of the experiment", "text": "CIFAR-10) for this task:\nx\u2192 Conv2D*(5, 64)\u2192 MaxPool\u2192 Dropout \u2192 Conv2D*(7, 128)\u2192 MaxPool\u2192 Dropout \u2192 Conv2D*(9, 128)\u2192 MaxPool\u2192 Dropout \u2192 Linear(4096)\u2192 ReLU\u2192 Dropout \u2192 Linear(4096)\u2192 ReLU\u2192 Dropout \u2192 Linear(10).\nHere each Conv2D*(s, c) is realized as a composition of (s\u2212 1)/2 3\u00d7 3 2D convolutional layers with c filters followed by batch normalization and rectified linear activation, which can be expressed as\nx\u2192 Conv2D(3, c)\u2192 BN\u2192 RL \u2192 \u00b7 \u00b7 \u00b7 \u2192 Conv2D(3, c)\u2192 BN\u2192 RL\ufe38 \ufe37\ufe37 \ufe38 (s\u2212 1)/2 times"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks<lb>to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks.<lb>A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates<lb>between nodes can be very large. Consequently, lossy compresion heuristics have been proposed, by which nodes only<lb>communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge,<lb>and it is not clear whether they are optimal.<lb>In this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression<lb>of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user<lb>to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the<lb>model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results<lb>with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with<lb>standard uncompressed techniques on a variety of real tasks.", "creator": "LaTeX with hyperref package"}}}