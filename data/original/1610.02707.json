{"id": "1610.02707", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2016", "title": "Multi-Objective Deep Reinforcement Learning", "abstract": "We propose Deep Optimistic Linear Support Learning (DOL) to solve high-dimensional multi-objective decision problems where the relative importances of the objectives are not known a priori. Using features from the high-dimensional inputs, DOL computes the convex coverage set containing all potential optimal solutions of the convex combinations of the objectives. To our knowledge, this is the first time that deep reinforcement learning has succeeded in learning multi-objective policies. In addition, we provide a testbed with two experiments to be used as a benchmark for deep multi-objective reinforcement learning.", "histories": [["v1", "Sun, 9 Oct 2016 19:08:36 GMT  (177kb,D)", "http://arxiv.org/abs/1610.02707v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hossam mossalam", "yannis m assael", "diederik m roijers", "shimon whiteson"], "accepted": false, "id": "1610.02707"}, "pdf": {"name": "1610.02707.pdf", "metadata": {"source": "CRF", "title": "Multi-Objective Deep Reinforcement Learning", "authors": ["Hossam Mossalam", "Yannis M. Assael", "Diederik M. Roijers", "Shimon Whiteson"], "emails": ["shimon.whiteson}@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].\nWhile the aforementioned approaches have focused on single-objective settings, many real-world problems have multiple possibly conflicting objectives. For example, an agent that may want to maximise the performance of a web application server, while minimising its power consumption [17]. Such problems can be modelled as multi-objective Markov decision processes (MOMDPs), and solved with multi-objective reinforcement learning (MORL) [18]. Because it is typically not clear how to evaluate available trade-offs between different objectives a priori, there is no single optimal policy. Hence, it is desirable to produce a coverage set (CS) which contains at least one optimal policy (and associated value vector) for each possible utility function that a user might have.\nSo far, deep learning methods for Markov decision processes (MDPs) have not been extended to MOMDPs. One reason is that it is not clear how neural networks can account for unknown preferences and the resulting sets of value vectors. In this paper, we circumvent this issue by taking an outer loop approach [19] to multi-objective reinforcement learning, i.e., we aim to learn an approximate coverage set of policies, each represented by a neural network, by evaluating a sequence of scalarised single-objective problems. In order to enable the use of deep Q-Networks [7] for learning in MOMDPs, we build off the state-of-the-art optimistic linear support (OLS) framework [19, 20]. OLS is a generic outer loop method for solving multi-objective decision problems, i.e., it repeatedly calls a single-objective solver as a subroutine. OLS terminates after a finite number of calls to that subroutine and produces an approximate CS. In principle any single-objective solver can be used, as long as it is OLS-compliant, i.e., produces policy value vectors rather than scalar values. Making a single-objective solver OLS-compliant typically requires little effort.\nWe present three new deep multi-objective RL algorithms. First, we investigate how the learning setting effects OLS, and how deep RL can be made OLS-compliant. Using an OLS-compliant neural network combined with the OLS framework results in Deep OLS Learning (DOL). Our empirical evaluation shows that DOL can tackle multi-objective problems with much larger inputs than classical multi-objective RL algorithms. We improve upon DOL by leveraging the fact that\nar X\niv :1\n61 0.\n02 70\n7v 1\n[ cs\n.A I]\n9 O\nct 2\n01 6\nthe OLS framework solves a series of single-objective problems that become increasingly similar as the series progresses [21], which results in increasingly similar optimal value vectors. Deep Q-networks produce latent embeddings of the features of a problem w.r.t. the function value. Hence, we hypothesise that we can reuse parts of the network used to solve the previous single-objective problem, in order to speed up learning on the next one. This results in two new algorithms that we call Deep OLS Learning with Full Reuse (DOL-FR), which reuses all parameter values of neural networks, and Deep OLS Learning with Partial Reuse (DOL-PR) which reuses all parameter values of neural networks, except those for the last layer of the network. We show empirically that reusing only part of the network (DOL-PR) is more effective than reusing the entire network (DOL-FR) and drastically improves the performance compared to DOL without reuse."}, {"heading": "2 Background", "text": "In a single-objective RL setting [22], an agent observes the current state st \u2208 S at each discrete time step t, chooses an action at \u2208 A according to a potentially stochastic policy \u03c0, observes a reward signal R(st, at) = rt \u2208 R, and transitions to a new state st+1. Its objective is to maximise an expectation over the discounted return, Rt = rt + \u03b3rt+1 + \u03b32rt+2 + \u00b7 \u00b7 \u00b7 , where rt is the reward received at time t and \u03b3 \u2208 [0, 1] is a discount factor. Markov Decision Process (MDP). Such sequential decision problems are commonly modelled as a finite single-objective Markov decision process (MDP), a tuple of \u3008S,A, R, T, \u03b3\u3009. The Qfunction of a policy \u03c0 is Q\u03c0(s, a) = E [Rt|st = s, at = a]. The optimal action-value function Q\u2217(s, a) = max\u03c0 Q \u03c0(s, a) obeys the Bellman optimality equation:\nQ\u2217(s, a) = Es\u2032 [ R(s, a) + \u03b3max\na\u2032 Q\u2217(s\u2032, a\u2032) | s, a\n] . (1)\nDeep Q-Networks (DQN). DeepQ-learning [7] uses neural networks parameterised by \u03b8 to represent Q(s, a; \u03b8). DQNs are optimised by minimising:\nLi(\u03b8i) = Es,a,r,s\u2032 [ (yDQNi \u2212Q(s, a; \u03b8i)) 2 ] , (2)\nat each iteration i, with target yDQNi = r + \u03b3maxa\u2032 Q(s \u2032, a\u2032; \u03b8\u2212i ). Here, \u03b8 \u2212 i are the parameters of a target network that is frozen for a number of iterations while updating the online network Q(s, a; \u03b8i) by gradient descent. The action a is chosen from Q(s, a; \u03b8i) by an action selector, which typically implements an -greedy policy that selects the action that maximises the Q-value with a probability of 1\u2212 and chooses randomly with a probability of . DQN uses experience replay [23]: during learning, the agent builds a dataset of episodic experiences and is then trained by sampling mini-batches of experiences. Experience replay is used in [7] to reduce variance by breaking correlation among the samples, whilst, it enables re-use of past experiences for learning.\nMulti-Objective MDPs (MOMDP). An MOMDP, is an MDP in which the reward function R(st, at) = rt \u2208 Rn describes a vector of n rewards, one for each objective [18]. We use bold variables to denote vectors. The solution to an MOMDP is a set of policies called a coverage set, that contains at least one optimal policy for each possible preference, i.e., utility or scalarisation function, f , that a user might have. This scalarisation function maps each possible policy value vector, V\u03c0 onto a scalar value. In this paper, we focus on the highly prevalent case where the scalarisation function, is linear, i.e., f(V\u03c0,w) = w \u00b7V\u03c0 , where w is a vector that determines the relative importance of the objectives, such that f(V\u03c0,w) is a convex combination of the objectives. The corresponding coverage set is called the convex coverage set (CCS) [18].\nOptimistic Linear Support (OLS). OLS takes an outer loop approach in which the CCS is incrementally constructed by solving a series of scalarised, i.e., single-objective, MDPs for different linear scalarisation vectors w. This enables the use of DQNs as a single-objective MDP solver. In each\niteration, OLS finds one policy by solving a scalarised MDP, and its value vector V\u03c0 is added to an intermediate approximate coverage set, S.\nUnlike other outer loop methods, OLS uses the concept of corner weights to pick the weights to use for creating scalarised instances and the concept of estimated improvement to prioritise those corner weights. To define corner weights, we first define the scalarised value function V \u2217S (w) = maxV \u03c0\u2208S w \u00b7V\u03c0, as a function of the linear scalarisation vector w, for a set of value vectors S. V \u2217S (w) for an S containing three value vectors is depicted in Figure 1. V \u2217 S (w) forms a piecewise linear and convex function that comprise the upper surface of the scalarised values of each value vector. The corner weights are the weights at the corners of the convex upper surface [24], marked with crosses in the figure. OLS always selects the corner weight w that maximises an optimistic upper bound on the difference between V \u2217S (w) and the optimal scalarised value function, i.e., V \u2217 CCS(w)\u2212 V \u2217S (w), and solves the single-objective MDP scalarized by the selected w.\nIn the planning setting for which OLS was devised, such an upper bound can typically be computed using upper bounds on the error with respect to the optimal value of the scalarised policy values at each previous w in the series, in combination with linear programs. The error bounds at the previous w stem from the approximation quality of the single-objective planning methods that OLS uses. However, in reinforcement learning, the true CCS is fundamentally unknown and no upper bounds can be given on the approximation quality of deep Q-learning. Therefore, we use V \u2217\nCCS (w)\u2212V \u2217S (w)\nas a heuristic to determine the priority, where V \u2217 CCS (w) is defined as maximal attainable scalarised value if we assume that the values found for previous w in the series were optimal for those w."}, {"heading": "3 Methodology", "text": "In this section, we propose our algorithms for MORL that employ deep Q-learning. Firstly, we propose our basic deep OLS learning (DOL) algorithm; we build off the OLS framework for multiobjective learning and integrate DQN. Then, we improve on this algorithm by introducing Deep OLS Learning with Partial (DOL-PR) and Full Reuse (DOL-FR). DOL, DOL-PR, and DOL-FR make use of a single-objective subroutine, which is defined together with DOL in Section 3.1."}, {"heading": "3.1 Deep OLS Learning (DOL)", "text": "There are two requirements to make use of the OLS framework. We first need a scalarized, i.e., single-objective learning algorithm that is OLS compliant. OLS compliance entails that rather than learning a single value per Q(s, a), we need a vector-valued Q-value Q(s, a). The estimates of Q(s, a) need to be accurate enough to determine the next corner weight in the series of linear scalarisation weights, w, that OLS is going to generate. To satisfy those requirements we adjust our neural network architectures to output a matrix of |A| \u00d7 n (where n is the number of objectives) instead of just |A|, and we train for an extended number of episodes. We define scalarised deep Q-learning, which uses this network architecture, and optimises the parameters to maximise the inner product of w and the Q-values for a given w instead of the scalar Q-values as in standard deep Q-learning. Using scalarised deep Q-learning as a subroutine in OLS results in our first algorithm: deep OLS learning (DOL)."}, {"heading": "3.2 Deep OLS Learning with Full (DOL-FR) and Partial Reuse (DOL-PR)", "text": "While DOL can already tackle very large MOMDPs, re-learning the parameters for the entire network when we move to the next w in the sequence is rather inefficient. Fortunately, we can exploit the following observation: the optimal value vectors (and thus optimal policies) for a scalarised MOMDP with a w and a w\u2032 that are close together, are typically close as well [21]. Because deep Q-networks learn to extract the features of a problem that are relevant to the rewards of an MOMDP, we can speed up computation by reusing the neural network parameters that were trained earlier in the sequence.\nIn Algorithm 1, we present an umbrella version of three novel algorithms, which we denote DOL. The different algorithms are obtained by setting the reuse parameter (i.e., the type of reuse) to one of three values: DOL (without reuse) is obtained by setting reuse to \u2018none\u2019, DOL with full reuse (DOL-FR) is obtained by setting reuse to \u2018full\u2019, and DOL with partial reuse (DOL-PR) is obtained by setting reuse to \u2018partial\u2019.\nAlgorithm 1 Deep OLS Learning (with different types of reuse)\n1: function DOL(m, \u03c4, template, reuse) 2: . Where, m \u2013 the (MOMDP) environment, \u03c4 \u2013 improvement threshold, 3: . template \u2013 specification of DQN architecture, reuse \u2013 the type of reuse 4: S = empty partial CSS 5: W = empty list of explored corner weights 6: Q = priority queue initialised with the extrema weights simplex with infinite priority 7: DQN_Models = empty table of DQNs, indexed by the weight, w, for which it was learnt 8: while Q is not empty \u2227it \u2264 max_it do 9: w = Q.pop()\n10: if reuse = \u2018none\u2019 \u2228 DQN_Models is empty then 11: model = a randomly initialised DQN, from a pre-specified architecture template 12: else 13: model = copyNearestModel(w,DQN_Models) 14: if reuse = \u2018partial\u2019 then reinitialise the last layer of model with random weights 15: V, new_model = scalarisedDeepQLearning(m,w, model) 16: W =W \u222aw 17: if (\u2203w\u2032) w\u2032\u00b7V > max\nU\u2208S w\u2032\u00b7U then\n18: Wdel =Wdel\u222a corner weights made obsolete by V from Q 19: Wdel =Wdel \u222a {w} 20: Remove Wdel from Q 21: Remove vectors from S that are no longer optimal for any w after adding V 22: WV = newCornerWeights(S, V) 23: S = S \u222a {V} 24: DQN_Models[w] = new_model 25: for each w\u2032 \u2208WV do 26: if estimateImprovement(w\u2032,W, S) > \u03c4 then 27: Q.add(w\u2032) 28: it++ 29: return S, DQN_Models\nDOL-FR applies full deep Q-network reuse; we start learning for a new scalarisation weight w\u2032, using the complete network we optimised for the previous w that is closest to w\u2032 in the sequence of scalarisation weights that OLS generated so far. DOL-PR applies partial deep Q-network reuse; we take the same network as for full reuse, but we reinitialise the last layer of the network randomly, in order to escape local optima. DOL (without reuse) does no reuse whatsoever, i.e., all network parameters are initialised randomly at the start of each iteration.\nDOL keeps track of the partial CCS, S, to which at most one value vector will be added at each iteration (line 4). To find these vectors, scalarised deep Q-learning (Section 3.1) is run for different corner weights. The corner weights that are not yet explored are kept in a priority queue, Q, and after they have been explored, are stored in a list W (line 5 and 6). Q is initialised with the extrema weights and keeps track of the scalarisation weights ordered by estimated improvement. In order to reuse the learnt parameters in DOL-PR/FR, DOL keeps track of them along with the corner weight w for which they were found in DQN_Models.\nFollowing the OLS framework, at each iteration of DOL, the weight with the highest improvement is popped (line 9). After selecting w, DOL now reuses the DQNs it learnt in previous iterations (depending on the parameter reuse). The function copyNearestModel finds the network learnt for the closest weight to the current corner weight on line 13. In the case of full reuse (reuse = \u2018full\u2032), all parameter values are copied. In the case of partial reuse (reuse = \u2018partial\u2032), the last layer is reinitialised with random parameter values (line 14), and in the case of no reuse (reuse = \u2018none\u2032) all the network parameters are reset (line 11).\nFollowing the different types of reuse, scalarised deep Q-learning, as described in Section 3.1 is invoked for the w popped off of Q on line 9. Scalarised deep Q-learning returns a value vector, V, corresponding to the learnt policy represented by a DQN, which is also returned (line 15). The current corner weight is added to the list of explored weights (line 16), which is used to determine\nthe priorities for subsequently discovered corner weights in the current and future iterations. If there is a weight vector w in the weight simplex for which the scalarised value is higher than for any of the vectors in S, the value vector is added to S, and new corner weights are determined and stored (lines 18-27). The DQN that corresponds to V is stored in DQN_models[w]. If V does not improve upon S for any w, it is discarded.\nExtending S with V leads to new corner weights. These new corner weights and their estimated improvement are calculated using the newCornerWeights and estimateImprovement methods of OLS [20]. The new corner weights are added to Q if their improvement value is greater than the threshold \u03c4 (lines 25-27). Also, corner weights in Q which are made obsolete (i.e. are no longer on the convex upper surface) by the new value vector are removed (line 18-19). This is repeated until there are no more corner weights in Q, at which point DOL terminates."}, {"heading": "4 Experimental Evaluation", "text": "In this section, we evaluate the performance of DOL and DOL-PR/FR. We make use of two multiobjective reinforcement learning problems called mountain car (MC) and deep sea treasure (DST) . We first show, how DOL and DOL-PR/FR are able to learn the correct CSS, using direct access to the state st of the problems. Then, we explore the scalability of our proposed methods, and evaluate the performance of weight reuse, we create an image version of the DST problem, in which we use a bitmap as input for scalarised deep Q-learning."}, {"heading": "4.1 Setup", "text": "For both the raw and image problems we follow the DQN setup of [7], employing experience replay and a target network to stabilise learning. We use an -greedy exploration policy with annealing from = 1 to 0.05, for the first 2000 and 3000 episodes, respectively, and learning continues for an equal number of episodes. The discount factor is \u03b3 = 0.97, and the target network is reset every 100 episodes. To stabilise learning, we execute parallel episodes in batches of 32. The parameters are optimised using Adam and a learning rate of 10\u22123. In each experiment we average over 5 runs.\nFor the raw state model we used an MLP architecture with 1 hidden layer of 100 neurons, and rectified linear unit activations. To process the 3\u00d711\u00d710 image inputs of Deep Sea we employed two convolutional layers of 16\u00d7 3\u00d7 3 and 32\u00d7 3\u00d7 3 and a fully connected layer on top. Finally, to facilitate future research we publish the source-code to replicate our experiments 1."}, {"heading": "4.2 Multi-Objective Mountain Car", "text": "In order to show that DOL, DOL-FR, and DOL-PR can learn a CCS, we first test on the multi-objective mountain car problem (MC). MC is a variant of the famous mountain car problem introduced in [22]. In single-objective mountain car problem, the agent controls a car located in a valley between two hills and it tries to get the car to reach the top of the hill on the right side. The car has a limited engine power, thus, the agent needs to oscillate the car between both hills until the car has gathered enough inertia that would let it reach the goal.\nThe reward in the single-objective variant is \u22121 for all time steps and 0 when the goal is reached. Our multi-objective variant adds another reward which is the fuel consumption for each time step, which is proportional to the force exerted by the car. In MC, there are only 2 value vectors in the CCS, and is thus a small problem.\nRaw version. We evaluate our proposed methods within the MC environment with the agent having direct access to the st. We employ the same neural network architecture as for DST. However, for MC, we used the CCS obtained by q-table algorithm as the true CCS which was then used for Max CCS error calculations as\nthe true CCS. As it can be seen in Figure 3, the three algorithms achieve very similar results on the\n1https://github.com/hossam-mossalam/multi-objective-deep-rl\nMC problem with DOL-PR achieving the least error. The algorithms learn a good approximation to the CCS in 2 iterations. After that, they continue making tiny improvements to these vectors that are not visible on the graph. The different algorithms behave equally well, which is due to the fact that for the extrema of the weight space, i.e., the first two iterations, the optimal policies are very different, and reuse does not contribute significantly.\n4.3 Deep Sea Treasure\nTo test the performance of our algorithms on a problem with a larger CCS, we adapt the well-known deep sea treasure (DST) [25] benchmark for MORL. In DST, the agent controls a submarine searching for treasures in a 10 \u00d7 11 grid. The state st consists of the current agent\u2019s coordinates (x, y). The grid contains 10 treasures that their rewards increase in proportion to the distance from the starting point s0 = (0, 0). The agent\u2019s action spaces is formed by navigation in four directions, and the map is depicted in Figure 4.\nAt each time-step the agent gets rewarded for the two different objectives. The first is zero unless a treasure value was received, and the second is a time penalty of \u22121 for each time-step. To be able to learn a CCS instead of a Pareto front, as it was in the original work [25], we\nhave adapted the values of the treasures such that the value of the most efficient policy for reaching each treasure is in the CCS. The rewards for both objectives were normalised between [0 \u2212 1] to facilitate the learning.\nRaw version. We first evaluate our proposed methods, in a simple scenario, where the agent has direct access to the st. Hence, we employ a simple neural network architecture, to measure the maximum error in scalarised value with respect to the true CCS. The true CSS is obtained by planning with an exact algorithm on the underlying MOMDP. We refer to this error as Max CCS Error. An analytical visualisation of measuring the true CSS and the discovered CSS difference, is illustrated in Figure 5. As it can be seen in Figure 6a, DOL exhibits the highest error. Contrary to the preliminary expectations, having access to the raw state information st does not make the feature extraction and reuse\nredundant. Furthermore, we discovered that when DOL-FR was used and the initialisation model already corresponded to an optimal policy, the miss-approximation error increased significantly, and less so for DOL-PR. We therefore conclude that our algorithms can efficiently approximate a CCS, and that reuse enables more accurate learning.\nImage version. Similar to the raw version, our deep convolutional architectures for the image version, are still able to approximate the CCS with a high accuracy. As seen in Figure 6b, the reuse methods show higher performance than DOL, and DOL-PR exhibits the highest stability as well. This is attributed to the fact that the network has learned to encode the state-space, which paves the way\ntowards efficient learning of the Q-values. DOL-PR exhibits the highest performance, as by resetting the last layer, we keep this encoded state-space, but we still allow DOL to train a new set of Q-values from scratch. We therefore conclude that DOL-PR is the preferred algorithm.\nAccuracy vs Episodes. We further investigated the effects of the number of training episodes on the max CCS error. As can be seen in Figure 6c, the error is highly affected by the number of training episodes. Specifically, for a small number of episodes DOL-PR is unable to providine sufficient accuracy to build the CCS. It is interesting to note that though the error decreases up to 4000 episodes, at 10000 episodes the network is overfitting which results in lower performance."}, {"heading": "5 Related Work", "text": "Multi-objective reinforcement learning [18, 25] has recently seen a renewed interest. Most algorithms in the literature [26\u201328] are however based on an inner loop approach, i.e., replacing the inner workings of single-objective solvers to work with sets of value vectors in the innermost workings of the algorithm. This is a fundamentally different approach, of which it is not clear how it could be applied to DQN, i.e., back-propagation cannot be transformed into a multi-objective algorithm in such a way. Other work does apply an outer loop approach but does not employ Deep RL [29\u201331]. We argue that enabling deep RL is essential for scaling up to larger problems.\nAnother popular class of MORL algorithms are heuristic policy search methods that find a set of alternative policies. These are for example based on multi-objective evolutionary algorithms (MOEAs) [32, 33] or Pareto local search (PLS) [34]. Especially MOEAs are compatible with neural networks, but evolutionary optimisation of NNs is typically rather slow compared to back-propagation (which is what the deep Q-learning algorithm that we employ in this paper as a single-objective subroutine uses).\nOutside of MORL, there are algorithms that are based on OLS but apply to different problem settings. Notably, the OLSAR algorithm [21] does planning in multi-objective partially observable MDPs (POMDPs), and applies reuse to the alpha matrices that it makes use of to represent the multi-objective value function. Unlike in our work, however, these alpha matrices form a guaranteed lower bound on the value function and can be reused fully without affecting the necessary exploration for learning in later iterations. Furthermore, the variational OLS (VOLS) algorithm [35], applies OLS to multiobjective coordination graphs and reuses reparameterisations of these graphs that are returned by the single-objective variational inference methods that VOLS uses as a subroutine. These variational subroutines are not made OLS compliant, like the DQNs in this paper, but the value vectors are retrieved by a separate policy evaluation step (which would be suboptimal in the context of deep RL)."}, {"heading": "6 Discussion", "text": "In this work, we proposed three new algorithms that enable the usage of deep Q-learning for multiobjective reinforcement learning. Our algorithms build off the recent optimistic linear support framework, and as such tackle the problem by learning one policy and corresponding value vector per iteration. Further, we extend the main deep OLS learning (DOL), to take advantage of the nature of neural networks, and introduce full (DOL-FR) and partial (DOL-PR) parameter reuse, in between the iterations, to pave the way towards faster learning.\nWe showed empirically that in problems with large inputs, our algorithms can learn CCS with high accuracy. For these problems DOL-PR outperforms DOL and DOL-FR, indicating that a) reuse is useful, and b) doing partial reuse rather than full reuse effectively prevents the model from getting stuck in a policy that was optimal for a previous w. In future work, we are planning to incorporate early stopping technique, and optimise our model for the accuracy requirements of OLS, while lowering the number of episodes required."}, {"heading": "Acknowledgements", "text": "This work is in part supported by the TERESA project (EC-FP7 grant #611153)."}], "references": [{"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Data-efficient learning of feedback policies from image pixels using deep dynamical models", "author": ["Y.M. Assael", "N. Wahlstr\u00f6m", "T.B. Sch\u00f6n", "M.P. Deisenroth"], "venue": "NIPS Deep Reinforcement Learning Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "In ICLR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["J.N. Foerster", "Y.M. Assael", "N. de Freitas", "S. Whiteson"], "venue": "arXiv preprint arXiv:1605.06676,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning", "author": ["X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["B.C. Stadie", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "arXiv preprint 1511.06581,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "In AAAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Action-conditional video prediction using deep networks in Atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["M.G. Bellemare", "G. Ostrovski", "A. Guez", "P.S. Thomas", "R. Munos"], "venue": "In AAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A.D. Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": "In Deep Learning Workshop,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Move Evaluation in Go Using Deep Convolutional Neural Networks", "author": ["C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": "search. Nature,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Managing power consumption and performance of computing systems using reinforcement learning", "author": ["G. Tesauro", "R. Das", "H. Chan", "J.O. Kephart", "C. Lefurgy", "D.W. Levine", "F. Rawson"], "venue": "In NIPS 2007: Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "A survey of multi-objective sequential decisionmaking", "author": ["D.M. Roijers", "P. Vamplew", "S. Whiteson", "R. Dazeley"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Computing convex coverage sets for faster multi-objective coordination", "author": ["D.M. Roijers", "S. Whiteson", "F.A. Oliehoek"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Multi-Objective Decision-Theoretic Planning", "author": ["D.M. Roijers"], "venue": "PhD thesis, University of Amsterdam,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Point-based planning for multi-objective POMDPs", "author": ["D.M. Roijers", "S. Whiteson", "F.A. Oliehoek"], "venue": "In IJCAI 2015: Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Reinforcement Learning for Robots Using Neural Networks", "author": ["L. Lin"], "venue": "PhD thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Algorithms for partially observable Markov decision processes", "author": ["H.-T. Cheng"], "venue": "PhD thesis, University of British Columbia,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}, {"title": "Empirical evaluation methods for multiobjective reinforcement learning algorithms", "author": ["P. Vamplew", "R. Dazeley", "A. Berry", "E. Dekker", "R. Issabekov"], "venue": "Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Learning all optimal policies with multiple criteria", "author": ["L. Barrett", "S. Narayanan"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Multi-objective reinforcement learning using sets of Pareto dominating policies", "author": ["K.V. Moffaert", "A. Now\u00e9"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Model-based multi-objective reinforcement learning", "author": ["M.A. Wiering", "M. Withagen", "M.M. Drugan"], "venue": "ADPRL", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "The scalarized multi-objective multi-armed bandit problem: an empirical study of its exploration vs. exploitation tradeoff", "author": ["S.Q. Yahyaa", "M.M. Drugan", "B. Manderick"], "venue": "In IJCNN 2014: Proceedings of the 2014 International Joint Conference on Neural Networks,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "A novel adaptive weight selection algorithm for multi-objective multi-agent reinforcement learning", "author": ["K. Van Moffaert", "T. Brys", "A. Chandra", "L. Esterle", "P.R. Lewis", "A. Now\u00e9"], "venue": "In IJCNN 2014: Proceedings of the 2013 International Joint Conference on Neural Networks,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Dynamic preferences in multi-criteria reinforcement learning", "author": ["S. Natarajan", "P. Tadepalli"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Evolutionary algorithms for solving multi-objective problems", "author": ["C.C. Coello", "G.B. Lamont", "D.A. Van Veldhuizen"], "venue": "Springer Science & Business Media,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Solving multi-objective reinforcement learning problems by EDA-RL - acquisition of various strategies", "author": ["H. Handa"], "venue": "ISDA", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Pareto local policy search for MOMDP planning", "author": ["C. Kooijman", "M. de Waard", "M. Inja", "D.M. Roijers", "S. Whiteson"], "venue": "ESANN", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Variational multi-objective coordination", "author": ["D.M. Roijers", "S. Whiteson", "A.T. Ihler", "F.A. Oliehoek"], "venue": "In MALIC 2015: NIPS Workshop on Learning, Inference and Control of Multi-Agent Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 180, "endOffset": 185}, {"referenceID": 1, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 180, "endOffset": 185}, {"referenceID": 2, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 180, "endOffset": 185}, {"referenceID": 3, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 204, "endOffset": 207}, {"referenceID": 4, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 225, "endOffset": 228}, {"referenceID": 5, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 6, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 7, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 8, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 9, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 10, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 11, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 12, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 13, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 14, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 281, "endOffset": 289}, {"referenceID": 15, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 281, "endOffset": 289}, {"referenceID": 16, "context": "For example, an agent that may want to maximise the performance of a web application server, while minimising its power consumption [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "Such problems can be modelled as multi-objective Markov decision processes (MOMDPs), and solved with multi-objective reinforcement learning (MORL) [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "In this paper, we circumvent this issue by taking an outer loop approach [19] to multi-objective reinforcement learning, i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 6, "context": "In order to enable the use of deep Q-Networks [7] for learning in MOMDPs, we build off the state-of-the-art optimistic linear support (OLS) framework [19, 20].", "startOffset": 46, "endOffset": 49}, {"referenceID": 18, "context": "In order to enable the use of deep Q-Networks [7] for learning in MOMDPs, we build off the state-of-the-art optimistic linear support (OLS) framework [19, 20].", "startOffset": 150, "endOffset": 158}, {"referenceID": 19, "context": "In order to enable the use of deep Q-Networks [7] for learning in MOMDPs, we build off the state-of-the-art optimistic linear support (OLS) framework [19, 20].", "startOffset": 150, "endOffset": 158}, {"referenceID": 20, "context": "the OLS framework solves a series of single-objective problems that become increasingly similar as the series progresses [21], which results in increasingly similar optimal value vectors.", "startOffset": 121, "endOffset": 125}, {"referenceID": 21, "context": "In a single-objective RL setting [22], an agent observes the current state st \u2208 S at each discrete time step t, chooses an action at \u2208 A according to a potentially stochastic policy \u03c0, observes a reward signal R(st, at) = rt \u2208 R, and transitions to a new state st+1.", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "Its objective is to maximise an expectation over the discounted return, Rt = rt + \u03b3rt+1 + \u03b3rt+2 + \u00b7 \u00b7 \u00b7 , where rt is the reward received at time t and \u03b3 \u2208 [0, 1] is a discount factor.", "startOffset": 156, "endOffset": 162}, {"referenceID": 6, "context": "DeepQ-learning [7] uses neural networks parameterised by \u03b8 to represent Q(s, a; \u03b8).", "startOffset": 15, "endOffset": 18}, {"referenceID": 22, "context": "DQN uses experience replay [23]: during learning, the agent builds a dataset of episodic experiences and is then trained by sampling mini-batches of experiences.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Experience replay is used in [7] to reduce variance by breaking correlation among the samples, whilst, it enables re-use of past experiences for learning.", "startOffset": 29, "endOffset": 32}, {"referenceID": 17, "context": "An MOMDP, is an MDP in which the reward function R(st, at) = rt \u2208 R describes a vector of n rewards, one for each objective [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "The corresponding coverage set is called the convex coverage set (CCS) [18].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "The corner weights are the weights at the corners of the convex upper surface [24], marked with crosses in the figure.", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "Fortunately, we can exploit the following observation: the optimal value vectors (and thus optimal policies) for a scalarised MOMDP with a w and a w\u2032 that are close together, are typically close as well [21].", "startOffset": 203, "endOffset": 207}, {"referenceID": 19, "context": "These new corner weights and their estimated improvement are calculated using the newCornerWeights and estimateImprovement methods of OLS [20].", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "For both the raw and image problems we follow the DQN setup of [7], employing experience replay and a target network to stabilise learning.", "startOffset": 63, "endOffset": 66}, {"referenceID": 21, "context": "MC is a variant of the famous mountain car problem introduced in [22].", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "To test the performance of our algorithms on a problem with a larger CCS, we adapt the well-known deep sea treasure (DST) [25] benchmark for MORL.", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "To be able to learn a CCS instead of a Pareto front, as it was in the original work [25], we have adapted the values of the treasures such that the value of the most efficient policy for reaching each treasure is in the CCS.", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "Multi-objective reinforcement learning [18, 25] has recently seen a renewed interest.", "startOffset": 39, "endOffset": 47}, {"referenceID": 24, "context": "Multi-objective reinforcement learning [18, 25] has recently seen a renewed interest.", "startOffset": 39, "endOffset": 47}, {"referenceID": 25, "context": "Most algorithms in the literature [26\u201328] are however based on an inner loop approach, i.", "startOffset": 34, "endOffset": 41}, {"referenceID": 26, "context": "Most algorithms in the literature [26\u201328] are however based on an inner loop approach, i.", "startOffset": 34, "endOffset": 41}, {"referenceID": 27, "context": "Most algorithms in the literature [26\u201328] are however based on an inner loop approach, i.", "startOffset": 34, "endOffset": 41}, {"referenceID": 28, "context": "Other work does apply an outer loop approach but does not employ Deep RL [29\u201331].", "startOffset": 73, "endOffset": 80}, {"referenceID": 29, "context": "Other work does apply an outer loop approach but does not employ Deep RL [29\u201331].", "startOffset": 73, "endOffset": 80}, {"referenceID": 30, "context": "Other work does apply an outer loop approach but does not employ Deep RL [29\u201331].", "startOffset": 73, "endOffset": 80}, {"referenceID": 31, "context": "These are for example based on multi-objective evolutionary algorithms (MOEAs) [32, 33] or Pareto local search (PLS) [34].", "startOffset": 79, "endOffset": 87}, {"referenceID": 32, "context": "These are for example based on multi-objective evolutionary algorithms (MOEAs) [32, 33] or Pareto local search (PLS) [34].", "startOffset": 79, "endOffset": 87}, {"referenceID": 33, "context": "These are for example based on multi-objective evolutionary algorithms (MOEAs) [32, 33] or Pareto local search (PLS) [34].", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "Notably, the OLSAR algorithm [21] does planning in multi-objective partially observable MDPs (POMDPs), and applies reuse to the alpha matrices that it makes use of to represent the multi-objective value function.", "startOffset": 29, "endOffset": 33}, {"referenceID": 34, "context": "Furthermore, the variational OLS (VOLS) algorithm [35], applies OLS to multiobjective coordination graphs and reuses reparameterisations of these graphs that are returned by the single-objective variational inference methods that VOLS uses as a subroutine.", "startOffset": 50, "endOffset": 54}], "year": 2016, "abstractText": "We propose Deep Optimistic Linear Support Learning (DOL) to solve highdimensional multi-objective decision problems where the relative importances of the objectives are not known a priori. Using features from the high-dimensional inputs, DOL computes the convex coverage set containing all potential optimal solutions of the convex combinations of the objectives. To our knowledge, this is the first time that deep reinforcement learning has succeeded in learning multiobjective policies. In addition, we provide a testbed with two experiments to be used as a benchmark for deep multi-objective reinforcement learning.", "creator": "LaTeX with hyperref package"}}}