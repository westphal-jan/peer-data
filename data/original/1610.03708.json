{"id": "1610.03708", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "Generating captions without looking beyond objects", "abstract": "This paper explores new evaluation perspectives for image captioning and introduces a noun translation task that achieves comparative image caption generation performance by translating from a set of nouns to captions. This implies that in image captioning, all word categories other than nouns can be evoked by a powerful language model without sacrificing performance on the precision-oriented metric BLEU. The paper also investigates lower and upper bounds of how much individual word categories in the captions contribute to the final BLEU score. A large possible improvement exists for nouns, verbs, and prepositions.", "histories": [["v1", "Wed, 12 Oct 2016 13:42:03 GMT  (3074kb,D)", "https://arxiv.org/abs/1610.03708v1", "This paper is accepted to the ECCV2016 2nd Workshop on Storytelling with Images and Videos (VisStory)"], ["v2", "Tue, 18 Oct 2016 09:35:03 GMT  (3076kb,D)", "http://arxiv.org/abs/1610.03708v2", "This paper was presented at the ECCV2016 2nd Workshop on Storytelling with Images and Videos (VisStory)"]], "COMMENTS": "This paper is accepted to the ECCV2016 2nd Workshop on Storytelling with Images and Videos (VisStory)", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["hendrik heuer", "christof monz", "arnold w m smeulders"], "accepted": false, "id": "1610.03708"}, "pdf": {"name": "1610.03708.pdf", "metadata": {"source": "CRF", "title": "Generating captions without looking beyond objects", "authors": ["Hendrik Heuer", "Christof Monz", "Arnold W.M. Smeulders"], "emails": ["h.heuer@uva.nl", "c.monz@uva.nl", "a.w.m.smeulders@uva.nl"], "sections": [{"heading": null, "text": "Keywords: Image captioning, evaluation, machine translation"}, {"heading": "1 Introduction", "text": "The objective of image captioning is to automatically generate grammatical descriptions of images that represent the meaning of a single image. Figure 1 compares different captions for an image from the MSCOCO [2] dataset. Describing Figure 1 as \u201cwoman and dog with frisbee on grass near fence.\u201d is merely turning the results of an object detection into a fluent English sentences. Generating a caption such as \u201ca woman playing tug of war with a dog over a white frisbee.\u201d exhibits an understanding that the dog is biting a frisbee, that the woman is playing with the dog and, more specifically, that the woman and the dog are playing a game called tug of war. Captions like this are more informative and relevant. They could also be beneficial in an assistive technology context where captions help visually impaired people, e.g. on Facebook.\nThis paper motivates why it is important to dissociate the captioning task from the object detection task a\u0300 la ImageNet and why new evaluation perspectives are needed. For this, it introduces a blind noun translation task that compares an image captioning system to the intrinsic language modeling capabilities of state-of-the-art recurrent neural networks. The paper also analyzes the image captions generated by three state-of-the-art systems: the Karpathy et al. [3] system, the Vinyals et al. [7] system, and the Xu et al. [9] system. The systems are analyzed in regards to the contribution of different word categories, i.e., part of speech tags, to the BLEU score.\nar X\niv :1\n61 0.\n03 70\n8v 2\n[ cs\n.C V"}, {"heading": "2 Background", "text": "Recent approaches in automated caption creation have addressed the problem of generating grammatical descriptions of images as a sequence modeling problem using recurrent neural network (RNN) language models [3,7,9,8].\nBoth Karpathy et al. [3] and Vinyals et al. [7] combine a vision CNN with a language generating RNN. For each image, a feature vector of the last fullyconnected layer of a CNN pretrained on ImageNet is fed into an RNN. Xu et al. [9] extended this architecture by adding an attention mechanism, which learns not only a distribution over the words in the vocabulary but also a distribution over the locations in the image based on the last convolutional layer of a CNN pretrained on ImageNet. Wu et al. [8] introduced a method of incorporating explicit high-level concepts such as bag, eating, and red, which is remarkable as it covers a noun, a verb, and an adjective. Their attribute-based V2L framework consists of an image analysis module that learns a mapping between an image and the semantic attributes through a CNN, as well as a language module, that learns a mapping from the attributes vector to a sequence of words using an LSTM. The suggested broad coverage over language is misleading, however. Nouns are well covered these days by concept detectors. \u201cEating\u201d is a visually well-defined state descriptor which can generally be captured in an attribute. This does not hold for many other actions. \u201cRed\u201d is likewise well described. It is one of the rare adjectives to have that property. In the sequel, we will argue that current error measures like BLEU are too insensitive to capture this. This work is complementary to previous work that disentangled the contribution of visual model and language model [10].\nBLEU is a precision-oriented machine translation evaluation metric that measures the N-gram overlap between sentences in the output of a machine translation system and one or more reference translations [4]. The clipped n-gram counts for all the candidate sentences are divided by the number of candidate n-grams in the test corpus to compute a precision score for the entire test corpus. BLEU ignores the relevance of words and only operates on a local level without taking the overall grammaticality of the sentence or the sentence meaning into account.\nGenerating captions without looking beyond objects 3\nTable 1\nCategory Karpathy et al., 2015 Vinyals et al., 2014 Xu et al.,2015 Wu et al., 2016 BLEU-1 63 67 72 73 BLEU-2 45 46 50 56 BLEU-3 32 33 36 41 BLEU-4 23 25 25 31\n1\nFigure 2 compares four state-of-the-art models regarding their BLEU-1-4 performances. While a significant improvement over the last years can be observed, it is not clear how much individual word categories, i.e., part-of-speech tags in the captions, contribute to the final BLEU score."}, {"heading": "3 Methodology", "text": "We introduce a blind noun translation task that demonstrates the language modeling capabilities of state-of-the-art language models like LSTMs and practically shows how much a language model can infer from a set of nouns. For the blind noun translation task, a state-of-the-art neural machine translation model [1] is trained to translate nouns into full captions. These captions are compared to the Karpathy et al. [3] image captioning system. The nouns fed to the translation model are not taken from the ground truth of the dataset but are automatically extracted from the captions generated by the Karpathy et al. system to increase comparability of the results. For this, the Stanford log-linear part-of-speech tagger [6,5] is used to extract all noun tags.\nThe model for the blind noun translation task is trained on the MSCOCO 2014 [2] training set, which includes over 80,000 images with 5 captions per image. The neural machine translation model was trained on 500,000 training pairs, which each consists of a caption on the target side and a random permutation of the nouns in the caption on the source side. Pairs of nouns such as \u201cplate food table woman\u201d are translated into target sentences such as \u201cA woman sitting at a table eating a plate of food\u201d. For the evaluation of the blind noun translation experiment, the MSCOCO 2014 [2] validation set is used, which consists of more than 40,000 captions.\nTo compare the contribution of individual word categories, the generated output of three state-of-the-art systems from Karpathy et al. [3], Vinyals et al. [7], and Xu et al. system [9] was obtained and analyzed. Since different subsets\nof the datasets were provided, the results are not directly comparable with one another. That said, they give an indication of the contribution of specific word categories.\nFor each system, a set of generated captions was selected. Using the Stanford log-linear part-of-speech tagger [6,5], for each word in the caption, the word category based on its syntactic function was assigned (noun, verb, adjective). Using this, the lower and the upper bound were determined. For both approaches, a certain word category was replaced by a special token. For the upper bound, only the verbs in the generated captions are replaced by this token, meaning that, effectively, no verb is correct, since they will always be different from the references. For the lower bound, all verbs in both the references and the generated captions are replaced by this token, which means that every verb is detected correctly."}, {"heading": "4 Results of the blind noun (theoretical) experiment", "text": "Table 1 shows the performance on the blind noun translation task in comparison to the n-gram precision scores of the Karpathy et al. image captioning system. The difference in performance according to the n-gram precision is relatively small, ranging from only 1.1% for 4-grams up to 3.2% for bigrams. This indicates that with respect to n-gram precision, the captions generated by the blind noun translation system are very comparative, even though the system does not do any image processing and merely turns a set of one-hot vectors into the most likely sentence according to the distribution of the training data. Since the commonly used BLEU metric combines the geometric mean of the four precision scores with a brevity penality, this also manifests a shortcoming of the BLEU precision metric. It shows that the evaluation of image captioning systems has to take into account that powerful language models are very good at reproducing the probability distribution of the training data and that the main power of vision is in recognizing basic noun-concepts. The added value of language modeling currently is mostly limited to concept detection as is demonstrated in this otherwise blind experiment.\nThe comparison of the contributions of different word categories can be seen in Figures 3 and 4. Each figure visualizes the performance difference regarding (a) unigram precision (BLEU-1) and (b) 4-gram precision (BLEU-4). The dotted line in the middle marks the system performance, i.e., how well the system performs on the MSCOCO Captioning Challenge without any modifications. A line above the system performance highlights the upper bound and indicates how much room for improvement is left, a line below the system performance shows how much worse a system would be performing given that it wouldn\u2019t be able to detect any member of the respective category. Figures 3 and 4 show that overall, nouns are currently the most important category where vision makes the most out of the image. Next to nouns, verbs and prepositions have the biggest room for improvement, closely followed by determiners. BLEU-1 An analysis of the lower bound shows that the systems already manage to capture a lot of crucial information in the captions. All systems would lose almost 19% or more in unigram precision (BLEU-1) if they would fail to generate any nouns. Failing to generate determiners would yield a similarly drastic\ndrop in performance. Interestingly, prepositions would also lead to losing 15% unigram precision performance. The pattern of performance drop is consistent with the three different systems analyzed. At the upper bound, despite the already high performance regarding nouns, there is still a possible improvement of 11,9% for the Vinyals et al. system (up to BLEU-1 86,8), 11,1% for the Xu et al. system (up to BLEU-1 88,6), and 14,0% for the Karpathy et al. system (up to BLEU-1 82,4). Moreover, improvements in capturing and generating verbs, adjectives and prepositions could yield an improvement of up to 3% for the BLEU-1 score metric alone. Interestingly, the possible performance gain by improving determiners is low (0.6%) for the attention-based Xu et al. system and the Vinyals et al. (1.1%) system and high for the Karpathy system (5%). Like with the lower bound, the shape of the improvement distribution is consistent between the three systems. BLEU-4 For the lower bound of the 4-gram precision score, failing to generate any nouns eradicates the entire performance of all three systems. A similar pattern can be observed for prepositions and determiners. Adjectives and adverbs are not affected regarding the precision score, which is likely connected to them being largely absent. At the upper bound, an improvement on the generation of nouns could effectively double the performance of the system for the 4-gram precision score. Improving prepositions has considerable potential for all systems, especially for the attention-based system that could improve by 3.3% through this. Interestingly, even though the number of verbs, and thus 4-gram, per sentence is limited, improving on verbs would yield a similar improvement in the range of 1.7 - 2.6%."}, {"heading": "5 Discussion", "text": "The results show that a blind noun translation system can generate captions that are comparable to state-of-the-art image captioning systems. This highlights how strong the language modeling capabilities of the LSTM are. It also shows how important it is to critically evaluate the contribution of the LSTM and its intrinsic language modeling capabilities, which should motivate a more rigorous evaluation of image captioning results.\nWhile we acknowledge the limitations of the BLEU metric for the overall evaluation task, as a precision-based metric, it is still suited to study how much individual word categories contribute to image captioning performance. The results show that it is possible to perform a more qualitative analysis of the contribution of specific linguistic phenomena on the image captioning task performance. The analysis indicates that a considerable improvement in regards to the BLEU precision metric and certain word categories is possible, especially nouns, verbs, and prepositions.\nImage captioning, much like machine translation, needs a reliable automatic metric which can be run quickly at no cost and which correlates with human judgment while taking task-specific challenges into account. To advance from turning the results of an object detection task into a fluent and adequate sen-\ntence, a reliable, automatically testable way of evaluating captions is needed. The goal should be to generate meaningful, sharp sentences that convey the semantic content of an image to a variety of stakeholders."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR abs/1409.0473 (2014),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Microsoft COCO captions: Data collection and evaluation", "author": ["X. Chen", "H. Fang", "T. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3128\u20133137", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. pp. 311\u2013318", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "In Proceedings of HLT-NAACL 2003. pp. 252\u2013259", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "EMNLP \u201900,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CoRR abs/1411.4555", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Image captioning with an intermediate attributes layer", "author": ["Q. Wu", "C. Shen", "A. van den Hengel", "L. Liu", "A.R. Dick"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Trainable performance upper bounds for image and video captioning", "author": ["L. Yao", "N. Ballas", "K. Cho", "J.R. Smith", "Y. Bengio"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Figure 1 compares different captions for an image from the MSCOCO [2] dataset.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "[3] system, the Vinyals et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] system, and the Xu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Recent approaches in automated caption creation have addressed the problem of generating grammatical descriptions of images as a sequence modeling problem using recurrent neural network (RNN) language models [3,7,9,8].", "startOffset": 208, "endOffset": 217}, {"referenceID": 6, "context": "Recent approaches in automated caption creation have addressed the problem of generating grammatical descriptions of images as a sequence modeling problem using recurrent neural network (RNN) language models [3,7,9,8].", "startOffset": 208, "endOffset": 217}, {"referenceID": 8, "context": "Recent approaches in automated caption creation have addressed the problem of generating grammatical descriptions of images as a sequence modeling problem using recurrent neural network (RNN) language models [3,7,9,8].", "startOffset": 208, "endOffset": 217}, {"referenceID": 7, "context": "Recent approaches in automated caption creation have addressed the problem of generating grammatical descriptions of images as a sequence modeling problem using recurrent neural network (RNN) language models [3,7,9,8].", "startOffset": 208, "endOffset": 217}, {"referenceID": 2, "context": "[3] and Vinyals et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] combine a vision CNN with a language generating RNN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] extended this architecture by adding an attention mechanism, which learns not only a distribution over the words in the vocabulary but also a distribution over the locations in the image based on the last convolutional layer of a CNN pretrained on ImageNet.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] introduced a method of incorporating explicit high-level concepts such as bag, eating, and red, which is remarkable as it covers a noun, a verb, and an adjective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "This work is complementary to previous work that disentangled the contribution of visual model and language model [10].", "startOffset": 114, "endOffset": 118}, {"referenceID": 3, "context": "BLEU is a precision-oriented machine translation evaluation metric that measures the N-gram overlap between sentences in the output of a machine translation system and one or more reference translations [4].", "startOffset": 203, "endOffset": 206}, {"referenceID": 0, "context": "For the blind noun translation task, a state-of-the-art neural machine translation model [1] is trained to translate nouns into full captions.", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "[3] image captioning system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For this, the Stanford log-linear part-of-speech tagger [6,5] is used to extract all noun tags.", "startOffset": 56, "endOffset": 61}, {"referenceID": 4, "context": "For this, the Stanford log-linear part-of-speech tagger [6,5] is used to extract all noun tags.", "startOffset": 56, "endOffset": 61}, {"referenceID": 1, "context": "The model for the blind noun translation task is trained on the MSCOCO 2014 [2] training set, which includes over 80,000 images with 5 captions per image.", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "For the evaluation of the blind noun translation experiment, the MSCOCO 2014 [2] validation set is used, which consists of more than 40,000 captions.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "[3], Vinyals et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7], and Xu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "system [9] was obtained and analyzed.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "Using the Stanford log-linear part-of-speech tagger [6,5], for each word in the caption, the word category based on its syntactic function was assigned (noun, verb, adjective).", "startOffset": 52, "endOffset": 57}, {"referenceID": 4, "context": "Using the Stanford log-linear part-of-speech tagger [6,5], for each word in the caption, the word category based on its syntactic function was assigned (noun, verb, adjective).", "startOffset": 52, "endOffset": 57}, {"referenceID": 2, "context": "[3] compared to a noun translation system, that translates the nouns extracted from the Karpathy et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "image captioning system into captions using a state-of-the-art machine translation system [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "[3]) 63.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] system on the MSCOCO dataset showing possible improvement respectively loss for different word categories.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] system on the MSCOCO dataset showing possible improvement respectively loss for different word categories.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "This paper explores new evaluation perspectives for image captioning and introduces a noun translation task that achieves comparative image caption generation performance by translating from a set of nouns to captions. This implies that in image captioning, all word categories other than nouns can be evoked by a powerful language model without sacrificing performance on n-gram precision. The paper also investigates lower and upper bounds of how much individual word categories in the captions contribute to the final BLEU score. A large possible improvement exists for nouns, verbs, and prepositions.", "creator": "LaTeX with hyperref package"}}}