{"id": "1610.04416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2016", "title": "Distributional Inclusion Hypothesis for Tensor-based Composition", "abstract": "According to the distributional inclusion hypothesis, entailment between words can be measured via the feature inclusions of their distributional vectors. In recent work, we showed how this hypothesis can be extended from words to phrases and sentences in the setting of compositional distributional semantics. This paper focuses on inclusion properties of tensors; its main contribution is a theoretical and experimental analysis of how feature inclusion works in different concrete models of verb tensors. We present results for relational, Frobenius, projective, and holistic methods and compare them to the simple vector addition, multiplication, min, and max models. The degrees of entailment thus obtained are evaluated via a variety of existing word-based measures, such as Weed's and Clarke's, KL-divergence, APinc, balAPinc, and two of our previously proposed metrics at the phrase/sentence level. We perform experiments on three entailment datasets, investigating which version of tensor-based composition achieves the highest performance when combined with the sentence-level measures.", "histories": [["v1", "Fri, 14 Oct 2016 11:52:19 GMT  (28kb)", "http://arxiv.org/abs/1610.04416v1", "To appear in COLING 2016"]], "COMMENTS": "To appear in COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["dimitri kartsaklis", "mehrnoosh sadrzadeh"], "accepted": false, "id": "1610.04416"}, "pdf": {"name": "1610.04416.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["d.kartsaklis@qmul.ac.uk", "mehrnoosh.sadrzadeh@qmul.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n04 41\n6v 1\n[ cs\n.C L\n] 1\n4 O\nct 2\n01 6"}, {"heading": "1 Introduction", "text": "Distributional hypothesis asserts that words that often occur in the same contexts have similar meanings (Firth, 1957). Naturally these models are used extensively to measure the semantic similarity of words (Turney and Pantel, 2010). Similarity is an a-directional relationship and computational linguists are also interested in measuring degrees of directional relationships between words. Distributional inclusion hypothesis is exactly about such relationships, and particularly, about entailment (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013). It states that a word u entails a word v if in any context that word u is used so can be word v. For example, in a corpus of sentences \u2018a boy runs\u2019, \u2018a person runs\u2019, \u2018a person sleeps\u2019, according to this hypothesis, boy \u22a2 person, since wherever \u2018boy\u2019 is used, so is \u2018person\u2019. Formally, we have that u entails v if features of u are included in features of v, where features are non-zero contexts. In this example, boy \u22a2 person, since {run} \u2282 {run, sleep}.\nFor the same reasons that the distributional hypothesis is not directly applicable to phrases and sentences, the distributional inclusion hypothesis does not scale up from words to larger language constituents. In a nutshell, this is because the majority of phrases and sentences of language do not frequently occur in corpora of text, thus reliable statistics cannot be collected for them. In distributional models, this problem is addressed with the provision of composition operators, the purpose of which is to combine the statistics of words and obtain statistics for phrases and sentences. In recent work, we have applied the same compositionality principles to lift the entailment relation from word level to phrase/sentence level (Kartsaklis and Sadrzadeh, 2016; Balk\u0131r et al., 2016a; Balk\u0131r et al., 2016b; Balk\u0131r, 2014). The work in (Balk\u0131r, 2014; Balk\u0131r et al., 2016b) was focused on the use of entropic measures on density matrices and compositional operators thereof, but no experimental results were considered; similarly, Bankova et al. (2016) use a specific form of density matrices to represent words for entailment purposes, focusing only on theory. In (Balk\u0131r et al., 2016a), we showed how entropic and other measures can be used on vectors as well as on density matrices and supported this claim with experimental results. In (Kartsaklis and Sadrzadeh, 2016), we focused on making the distributional inclusion hypothesis compositional, worked\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/\nout how feature inclusion lifts from words to compositional operators on them, and based on experimental results showed that intersective composition operators result in more reliable entailments. This paper takes a more concrete perspective and focuses on the feature inclusion properties of tensors constructed in different ways and composition operators applied to the tensors and vectors.\nOne can broadly classify the compositional distributional models to three categories: ones that are based on simple element-wise operations between vectors, such as addition and multiplication (Mitchell and Lapata, 2010); tensor-based models in which relational words such as verbs and adjectives are tensors and matrices contracting and multiplying with noun (and noun-phrase) vectors (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al., 2012; Kalchbrenner et al., 2014) and is usually optimized against a specific objective.\nTensor-based models stand in between the two extremes of element-wise vector mixing and neural net-based methods, offering a sufficiently powerful alternative that allows for theoretical reasoning at a level deeper than it is usually possible with black-box statistical approaches. Models of this form have been used in the past with success in a number of NLP tasks, such as head-verb disambiguation (Grefenstette and Sadrzadeh, 2011), term-definition classification (Kartsaklis et al., 2012), and generic sentence similarity (Kartsaklis and Sadrzadeh, 2014). In this paper we extend this work to entailment, by investigating (theoretically and experimentally) the properties of feature inclusion on the phrase and sentence vectors produced in four different tensor-based compositional distributional models: the relational models of (Grefenstette and Sadrzadeh, 2011), the Frobenius models of (Kartsaklis et al., 2012; Milajevs et al., 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010).\nContrary to formal semantic models, and customary to distributional models, our entailments are nonboolean and come equipped with degrees. We review a number of measures that have been developed for evaluating degrees of entailment at the lexical level, such as the APinc measure and its newer version, balAPinc of Kotlerman et al. (2010), which is considered as state-of-the-art for word-level entailment. A newly proposed adaptation of these metrics, recently introduced by the authors in (Kartsaklis and Sadrzadeh, 2016), is also detailed. This measure takes into account the specificities introduced by the use of a compositional operator and lifts the measures from words to phrase/sentence level.\nWe experiment with these models and evaluate them on entailment relations between simple intransitive sentences, verb phrases, and transitive sentences on the datasets of (Kartsaklis and Sadrzadeh, 2016). Our findings suggest that the Frobenius models provide the highest performance, especially when combined with our sentence-level measures. On a more general note, the experimental results of this paper support that of previous work (Kartsaklis and Sadrzadeh, 2016) and strongly indicate that compositional models employing some form of intersective feature selection, i.e. point-wise vector multiplication or tensor-based models with an element of element-wise mixing (such as the Frobenius constructions), are more appropriate for entailment tasks in distributional settings."}, {"heading": "2 Compositional distributional semantics", "text": "The purpose of a compositional distributional model is to produce a vector representing the meaning of a phrase or a sentence by combining the vectors of its words. In the simplest case, this is done by element-wise operations on the vectors of the words (Mitchell and Lapata, 2010). Specifically, the vector representation \u2212\u2192w of a sequence of words w1, . . . , wn is defined to be:\n\u2212\u2192w := \u2212\u2192w1 + \u2212\u2192w2 + \u00b7 \u00b7 \u00b7 + \u2212\u2192wn \u2212\u2192w := \u2212\u2192w1 \u2299 \u2212\u2192w2 \u2299 \u00b7 \u00b7 \u00b7 \u2299 \u2212\u2192wn\nIn a more linguistically motivated approach, relational words such as verbs and adjectives are treated as linear or multi-linear maps. These are then applied to the vectors of their arguments by following the rules of the grammar (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010). For example, an adjective is a map N \u2192 N , for N a basic noun space of the model. Equivalently, this map can be represented as a matrix living in the space N \u2297N . In a similar way, a transitive verb is a map N \u00d7N \u2192 S, or equivalently, a \u201ccube\u201d or a tensor of order 3 in the space N \u2297N \u2297S, for S a basic sentence space of the model. Composition takes place by tensor contraction, which is a generalization of\nmatrix multiplication to higher order tensors. For the case of an adjective-noun compound, this simplifies to matrix multiplication between the adjective matrix and the vector of its noun, while for a transitive sentence it takes the following form, where verb is a tensor of order 3 and \u00d7 is tensor contraction:\n\u2212\u2192svo = (verb \u00d7 \u2212\u2192 obj)\u00d7 \u2212\u2212\u2192 subj\nFinally, phrase and sentence vectors have been also produced by the application of neural architectures, such as recursive or recurrent neural networks (Socher et al., 2012; Cheng and Kartsaklis, 2015) and convolutional neural networks (Kalchbrenner et al., 2014). These models have been shown to perform well on large scale entailment tasks such as the ones introduced by the RTE challenge."}, {"heading": "3 Distributional inclusion hypothesis", "text": "The distributional inclusion hypothesis (DIH) (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that whenever a word u entails a word v, then it makes sense to replace instances of u with v. For example, \u2018cat\u2019 entails \u2018animal\u2019, hence in the sentence \u2018a cat is asleep\u2019, it makes sense to replace \u2018cat\u2019 with \u2018animal\u2019 and obtain \u2018an animal is asleep\u2019. On the other hand, \u2018cat\u2019 does not entail \u2018butterfly\u2019, and indeed it does not make sense to do a similar substitution and obtain the sentence \u2018a butterfly is asleep\u2019. This hypothesis has limitations, the main one being that it only makes sense in upward monotone contexts. For instance, the substitution of u for v would not work for sentences that have negations or quantifiers such as \u2018all\u2019 and \u2018none\u2019. As a result, one cannot replace \u2018cat\u2019 with \u2018animal\u2019 in sentences such as \u2018all cats are asleep\u2019 or \u2018a cat is not asleep\u2019. Despite this and other limitations, the DIH has been subject to a good amount of study in the distributional semantics community and its predictions have been validated (Geffet and Dagan, 2005; Kotlerman et al., 2010).\nFormally, the DIH says that if word u entails word v, then the set of features of u are included in the set of features of v. In the context of a distributional model of meaning, the term feature refers to a non-zero dimension of the distributional vector of a word. By denoting the features of a distributional vector \u2212\u2192v by F(\u2212\u2192v ), one can symbolically express the DIH as follows:\nu \u22a2 v whenever F(\u2212\u2192u ) \u2286 F(\u2212\u2192v ) (1)\nThe research on the DIH can be categorised into two classes. In the first class, the degree of entailment between two words is based on the distance between the vector representations of the words. This distance must be measured by asymmetric means, since entailment is directional. Examples of measures used here are entropy-based measures such as KL-divergence (Chen and Goodman, 1996). KL-divergence is only defined when the support of \u2212\u2192v is included in the support of \u2212\u2192u . To overcome this restriction, a variant referred to by \u03b1-skew (Lee, 1999) has been proposed (for \u03b1 \u2208 (0, 1] a smoothing parameter). Representativeness provides another way of smoothing the KL-divergence. The formulae for these are as follows, where abusing the notation we take \u2212\u2192u and \u2212\u2192v to also denote the probability distributions of u and v:\nDKL( \u2212\u2192v \u2016\u2212\u2192u ) =\n\u2211\ni\nvi(ln vi \u2212 lnui) s\u03b1( \u2212\u2192u ,\u2212\u2192v ) = DKL( \u2212\u2192v \u2016\u03b1\u2212\u2192u + (1\u2212 \u03b1)\u2212\u2192v )\nRD( \u2212\u2192v \u2016\u2212\u2192u ) =\n1\n1 +DKL( \u2212\u2192v ||\u2212\u2192u )\nRepresentativeness turns KL-divergence into a number in the unit interval [0, 1]. As a result we obtain 0 \u2264 RD( \u2212\u2192v \u2016\u2212\u2192u ) \u2264 1, with RD( \u2212\u2192v \u2016\u2212\u2192u ) = 0 when the support of \u2212\u2192v is not included in the support of \u2212\u2192u and RD( \u2212\u2192v \u2016\u2212\u2192u ) = 1, when \u2212\u2192u and \u2212\u2192v represent the same distribution.\nThe research done in the second class attempts a more direct measurement of the inclusion of features, with the simplest possible case returning a binary value for inclusion or lack thereof. Measures developed by Weeds et al. (2004) and Clarke (2009) advance this simple methods by arguing that not all features play an equal role in representing words and hence they should not be treated equally when it comes to measuring entailment. Some features are more \u201cpertinent\u201d than others and these features have to be given a higher weight when computing inclusion. For example, \u2018cat\u2019 can have a non-zero coordinate on\nall of the features \u2018mammal, miaow, eat, drink, sleep\u2019. But the amount of these coordinates differ, and one can say that, for example, the higher the coordinate the more pertinent the feature. Pertinence is computed by various different measures, the most recent of which is balAPinc (Kotlerman et al., 2010), where LIN is Lin\u2019s similarity (Lin, 1998) and APinc is an asymmetric measure:\nbalAPinc(u, v) = \u221a LIN(u, v) \u00b7 APinc(u, v) APinc(u, v) =\n\u2211\nr [P (r) \u00b7 rel \u2032(fr)]\n|F(\u2212\u2192u )|\nAPinc applies the DIH via the idea that features with high values in F(\u2212\u2192u ) must also have high values in F(\u2212\u2192v ). In the above formula, fr is the feature in F(\n\u2212\u2192u ) with rank r; P (r) is the precision at rank r; and rel\u2032(fr) is a weight computed as follows:\nrel\u2032(f) =\n{ 1\u2212 rank(f,F( \u2212\u2192v ))\n|F(\u2212\u2192v )|+1 f \u2208 F(\u2212\u2192v )\n0 o.w. (2)\nwhere rank(f,F(\u2212\u2192v )) shows the rank of feature f within the entailed vector. In general, APinc can be seen as a version of average precision that reflects lexical inclusion."}, {"heading": "4 Measuring feature inclusion at the phrase/sentence level", "text": "In recent work, the authors of this paper introduced a variation of the APinc and balAPinc measures aiming to address the extra complications imposed when evaluating entailment at the phrase/sentence level (Kartsaklis and Sadrzadeh, 2016). The modified measures differ from the original ones in two aspects. Firstly, in a compositional distributional model, the practice of considering only non-zero elements of the vectors as features becomes too restrictive and thus suboptimal for evaluating entailment; indeed, depending on the form of the vector space and the applied compositional operator (especially in intersective models, see Sections 5 and 6), an element can get very low values without however ever reaching zero. The new measures exploit this blurring of the notion of \u201cfeatureness\u201d to the limit, by letting F(\u2212\u2192w ) to include all the dimensions of \u2212\u2192w .\nSecondly, the continuous nature of distributional models is further exploited by providing a stronger realization of the idea that u \u22a2 v whenever v occurs in all the contexts of u. Let f (u)r be a feature in F(\u2212\u2192u ) with rank r and f (v)r the corresponding feature in F( \u2212\u2192v ), we remind that Kotlerman et al. consider that feature inclusion holds at rank r whenever f (u)r > 0 and f (v) r > 0; the new measures strengthen this assumption by requiring that f (u)r \u2264 f (v) r . Incorporating these modifications in the APinc measure, P (r) and rel\u2032(fr) are redefined as follows:\nP (r) =\n\u2223 \u2223{f (u) r |f (u) r \u2264 f (v) r , 0 < r \u2264 | \u2212\u2192u |} \u2223 \u2223\nr rel\u2032(fr) =\n{\n1 f (u) r \u2264 f (v) r 0 o.w. (3)\nThe new relevance function subsumes the old one, as by definition high-valued features in F(\u2212\u2192u ) must be even higher in F(\u2212\u2192v ). The new APinc at the phrase/sentence level thus becomes as follows:\nSAPinc(u, v) =\n\u2211\nr [P (r) \u00b7 rel \u2032(fr)]\n|\u2212\u2192u | (4)\nwhere |\u2212\u2192u | is the number of dimensions of \u2212\u2192u . Further, we notice that balAPinc is the geometric average of APinc with Lin\u2019s similarity measure, which is symmetric. According to (Kotlerman et al., 2010), the rationale of including a symmetric measure was that APinc tends to return unjustifyingly high scores when the entailing word is infrequent, that is, when the feature vector of the entailing word is very short; the purpose of the symmetric measure was to penalize the result, since in this case the similarity of the narrower term with the broader one is usually low. However, now that all feature vectors have the same length, such a balancing action is unnecessary; more importantly, it introduces a strong element of symmetry in a measure that is intended to be strongly asymmetric. We cope with this issue by replacing Lin\u2019s measure with representativeness on KL-divergence,1 obtaining the following new version of balAPinc:\n1Using other asymmetric measures is also possible; the choice of representativeness on KL-divergence was based on informal experimentation which showed that this combination works better than other options in practice.\nSBalAPinc(u, v) = \u221a RD( \u2212\u2192u \u2016\u2212\u2192v ) \u00b7 SAPinc(\u2212\u2192u ,\u2212\u2192v ) (5)\nRecall that RD(p\u2016q) is asymmetric, measuring the extent to which q represents (i.e. is similar to) p. So the term RD(\n\u2212\u2192u \u2016\u2212\u2192v ) in the above formula measures how well the broader term v represents the narrower one u; as an example, we can think that the term \u2018animal\u2019 is representative of \u2018cat\u2019, while the reverse is not true. The new measure aims at: (i) retaining a strongly asymmetric nature; and (ii) providing a more fine-grained element of entailment evaluation."}, {"heading": "5 Generic feature inclusion in compositional models", "text": "In the presence of a compositional operator, features of phrases or sentences adhere to set-theoretic properties. For simple additive and multiplicative models, the set of features of a phrase/sentence is derived from the set of features of their words using union and intersection. It is slightly less apparent (and for reasons of space we will not give details here) that the features of point-wise minimum and maximum of vectors are also derived from the intersection and union of their features, respectively. That is:\nF(\u2212\u2192v1 + \u00b7 \u00b7 \u00b7+ \u2212\u2192vn) = F(max( \u2212\u2192v1 , \u00b7 \u00b7 \u00b7 , \u2212\u2192vn)) = F( \u2212\u2192v1) \u222a \u00b7 \u00b7 \u00b7 \u222a F( \u2212\u2192vn) F(\u2212\u2192v1 \u2299 \u00b7 \u00b7 \u00b7 \u2299 \u2212\u2192vn) = F(min( \u2212\u2192v1 , \u00b7 \u00b7 \u00b7 , \u2212\u2192vn)) = F( \u2212\u2192v1) \u2229 \u00b7 \u00b7 \u00b7 \u2229 F( \u2212\u2192vn)\nAs shown in (Kartsaklis and Sadrzadeh, 2016), element-wise composition of this form lifts naturally from the word level to phrase/sentence level; specifically, for two sentences s1 = u1 . . . un and s2 = v1 . . . vn for which ui \u22a2 vi \u2200i \u2208 [1, n], it is always the case that s1 \u22a2 s2. This kind of lifting of the entailment relationship from words to the phrase/sentence level also holds for tensor-based models (Balk\u0131r et al., 2016a).\nIn general, feature inclusion is a more complicated process for tensor-based settings, since in this case the composition operation is matrix multiplication and tensor contraction. As an example, consider the simple case of a matrix multiplication between a m\u00d7 n matrix M and a n\u00d7 1 vector \u2212\u2192v . Matrix M can be seen as a list of column vectors (\u2212\u2192w1, \u2212\u2192w2, \u00b7 \u00b7 \u00b7 , \u2212\u2192wn), where \u2212\u2192wi = (w1i, \u00b7 \u00b7 \u00b7 , wmi) T. The result of the matrix multiplication is a combination of scalar multiplications of each element vi of the vector \u2212\u2192v with the corresponding column vector \u2212\u2192wi of the matrix M. That is, we have:\n\n   \nw11 \u00b7 \u00b7 \u00b7 w1n w21 \u00b7 \u00b7 \u00b7 w2n\n... ...\nwm1 \u00b7 \u00b7 \u00b7 wmn\n\n    \u00d7\n\n  v1 ... vn\n\n  = v1 \u2212\u2192w1 + v2 \u2212\u2192w2 + \u00b7 \u00b7 \u00b7+ vn \u2212\u2192wn\nLooking at M \u00d7 \u2212\u2192v in this way enables us to describe F(M \u00d7 \u2212\u2192v ) in terms of the union of F(\u2212\u2192wi)\u2019s where vi is non zero, that is, we have:\nF(M\u00d7\u2212\u2192v ) = \u22c3\nvi 6=0\nF(\u2212\u2192wi)\nBy denoting vi a feature whenever it is non-zero, we obtain an equivalent form as follows: \u22c3\ni\nF(\u2212\u2192wi) |F(vi) (6)\nThe above notation says that we collect features of each \u2212\u2192wi vector but only up to \u201cfeatureness\u201d of vi, that is up to vi being non-zero. This can be extended to tensors of higher order; a tensor of order 3, for example, can be seen as a list of matrices, a tensor of order 4 as a list of \u201ccubes\u201d and so on. For the case of this paper, we will not go beyond matrix multiplication and cube contraction."}, {"heading": "6 Feature inclusion in concrete constructions of tensor-based models", "text": "While the previous section provided a generic analysis of the feature inclusion behaviour of tensorbased models, the exact feature inclusion properties of these models depend on the specific concrete constructions, and in principle get a form more refined than that of simple intersective or union-based composition. In this section we investigate a number of tensor-based models with regard to feature inclusion, and derive their properties."}, {"heading": "6.1 Relational model", "text": "As a starting point we will use the model of Grefenstette and Sadrzadeh (2011), which adopts an extensional approach and builds the tensor of a relational word from the vectors of its arguments. More specifically, the tensors for adjectives, intransitive verbs, and transitive verbs are defined as below, respectively:\nadj = \u2211\ni\n\u2212\u2212\u2212\u2192 Nouni verbIN = \u2211\ni\n\u2212\u2212\u2192 Sbji verbTR = \u2211\ni\n\u2212\u2212\u2192 Sbji \u2297 \u2212\u2212\u2192 Obji (7)\nwhere \u2212\u2212\u2212\u2212\u2192 Nouni, \u2212\u2212\u2192 Sbji and \u2212\u2212\u2192 Obji refer to the distributional vectors of the nouns, subjects and objects that occurred as arguments for the adjective and the verb across the training corpus. For the case of a subjectverb sentence and a verb-object phrase, composition reduces to element-wise multiplication of the two vectors, and the features of the resulting sentence/phrase vector get the following form (with \u2212\u2192s and \u2212\u2192o to denote the vectors of the subject/verb of the phrase/sentence):\nF(\u2212\u2192sv) = \u22c3\ni\nF( \u2212\u2212\u2192 Sbji) \u2229 F( \u2212\u2192s ) F(\u2212\u2192vo) = \u22c3\ni\nF( \u2212\u2212\u2192 Obji) \u2229 F( \u2212\u2192o ) (8)\nFor a transitive sentence, the model of Grefenstette and Sadrzadeh (2011) returns a matrix, computed in the following way:\nsvoRel = verb\u2299 ( \u2212\u2192s \u2297\u2212\u2192o )\nwhere verb is defined as in Equation 7. By noticing that F(\u2212\u2192u \u2297\u2212\u2192v ) = F(\u2212\u2192u )\u00d7F(\u2212\u2192v ), with symbol \u00d7 to denote in this case the cartesian product of the two feature sets, we define the feature set of a transitive sentence as follows:\nF(svoRel) = \u22c3\ni\nF( \u2212\u2212\u2192 Sbji)\u00d7F( \u2212\u2212\u2192 Obji) \u2229 F( \u2212\u2192s )\u00d7F(\u2212\u2192o ) (9)\nEquation 9 shows that the features of this model are pairs (fsbj, fobj), with fsbj a subject-related feature and fobj an object-related feature, providing a fine-grained representation of the sentence. Throughout this paper, we refer to this model as relational."}, {"heading": "6.2 Frobenius models", "text": "As pointed out in (Kartsaklis et al., 2012), the disadvantage of the relational model is that their resulting representations of verbs have one dimension less than what their types dictate. According to the type assignments, an intransitive verb has to be a matrix and a transitive verb a cube, where as in the above we have a vector and a matrix. A solution presented in (Kartsaklis et al., 2012) suggested the use of Frobenius operators in order to expand vectors and matrices into higher order tensors by embedding them into the the diagonals. For example, a vector is embedded into a matrix by putting it in the diagonal of a matrix and padding the off-diagonal elements with zeros. Similarly, one can embed a matrix into a cube by putting it into the main diagonal and pad the rest with zeros. Using this method, for example, one could transform a simple intersective model in tensor form by embedding the context vector of a verb \u2212\u2192v first into a matrix and then into a cube. For a transitive sentence, one could use the matrix defined in Equation 7 and derive a vector for the meaning of the sentence in two ways, each one corresponding to a different embedding of the matrix into a tensor of order 3:\n\u2212\u2192svoCpSbj = \u2212\u2192s \u2299 (verb\u00d7\u2212\u2192o ) \u2212\u2192svoCpObj = ( \u2212\u2192s T \u00d7 verb)\u2299\u2212\u2192o (10)\nWe refer to these models as Copy-Subject and Copy-Object, correspondingly. In order to derive their feature inclusion properties, we first examine the form of the sentence vector produced when the verb is composed with a new subject/object pair:\n\u2212\u2192svoCpSbj = \u2212\u2192s \u2299 (verb\u00d7\u2212\u2192o ) = \u2212\u2192s \u2299\n\u2211\ni\n\u2212\u2212\u2192 Sbji\u3008 \u2212\u2212\u2192 Obji| \u2212\u2192o \u3009\n\u2212\u2192svoCpObj = ( \u2212\u2192s T \u00d7 verb)\u2299\u2212\u2192o = \u2212\u2192o \u2299\n\u2211\ni\n\u2212\u2212\u2192 Obji\u3008 \u2212\u2192s | \u2212\u2212\u2192 Sbji\u3009\nWe can now define the feature sets of the two models using notation similar to that of Equation 6:\nF(\u2212\u2192svoCpSbj) = F( \u2212\u2192s ) \u2229\n\u22c3\ni\nF( \u2212\u2212\u2192 Sbji) |F ( \u3008 \u2212\u2212\u2192 Obji| \u2212\u2192o \u3009 )\nF(\u2212\u2192svoCpObj) = F( \u2212\u2192o ) \u2229\n\u22c3\ni\nF( \u2212\u2212\u2192 Obji) |F ( \u3008\u2212\u2192s | \u2212\u2192 Sbji\u3009 )\n(11)\nThe symbol | defines a restriction on feature inclusion based on how well the arguments of the sentence fit to the arguments of the verb. For a subject-object pair (Sbj,Obj) that has occured with the verb in the corpus, this translates to the following:\n\u2022 Copy-Subject: Include the features of Sbj up to similarity of Obj with the sentence object\n\u2022 Copy-Object: Include the features of Obj up to similarity of Sbj with the sentence subject\nNote that each of the Frobenius models puts emphasis on a different argument of a sentence; the CopySubject model collects features of the subjects that occured with the verb, while the Copy-Object model collects features from the verb objects. It is reasonable then to further combine the two models in order to get a more complete representation of the sentence meaning, and hence its feature inclusion properties. Below we define the feature sets of two variations, where this combination is achieved via addition (we refer to this model as Frobenius additive) and element-wise multiplication (Frobenius multiplicative) of the vectors produced by the individual models (Kartsaklis and Sadrzadeh, 2014):\nF(\u2212\u2192svoFrAdd) = F( \u2212\u2192svoCpSbj) \u222a F( \u2212\u2192svoCpObj) F(\u2212\u2192svoFrMul) = F( \u2212\u2192svoCpSbj) \u2229 F( \u2212\u2192svoCpObj) (12)\nwhere F(\u2212\u2192svoCpSbj) and F( \u2212\u2192svoCpObj) are defined as in Equation 11."}, {"heading": "6.3 Projective models", "text": "In this section we provide an alternative solution and remedy the problem of having lower dimensions than the required by arguing that the sentence/phrase space should be spanned by the vectors of the arguments of the verb across the corpus. Thus we create verb matrices for intransitive sentences and verb phrases by summing up projectors of the argument vectors, in the following way:\nvitv := \u2211\ni\n\u2212\u2212\u2192 Sbji \u2297 \u2212\u2212\u2192 Sbji vvp := \u2211\ni\n\u2212\u2212\u2192 Obji \u2297 \u2212\u2212\u2192 Obji (13)\nWhen these verbs are composed with some subject/object to form a phrase/sentence, each vector in the spanning space is weighted by its similarity (assuming normalized vectors) with the vector of that subject/object, that is:\n\u2212\u2192svPrj = \u2212\u2192s T \u00d7 vitv =\n\u2211\ni\n\u3008 \u2212\u2212\u2192 Sbji| \u2212\u2192s \u3009 \u2212\u2212\u2192 Sbji\n\u2212\u2192voPrj = vvp \u00d7 \u2212\u2192o =\n\u2211\ni\n\u3008 \u2212\u2212\u2192 Obji| \u2212\u2192o \u3009 \u2212\u2212\u2192 Obji (14)\nTranslating the above equations to feature inclusion representations will give:\nF(\u2212\u2192svPrj) = \u22c3\ni\nF( \u2212\u2212\u2192 Sbji) |F ( \u3008 \u2212\u2212\u2192 Sbji| \u2212\u2192s \u3009 ) F(\u2212\u2192voPrj) = \u22c3\ni\nF( \u2212\u2212\u2192 Obji) |F ( \u3008 \u2212\u2212\u2192 Obji| \u2212\u2192o \u3009 ) (15)\nwith symbol | to define again a restriction on feature inclusion based on the similarity of the arguments with the subject or object of the sentence/phrase. For the subject-verb case, this reads: \u201cinclude a subject that occured with the verb, up to its similarity with the subject of the sentence\u201d. For the case of a transitive verb (a function of two arguments), we define the sentence space to be spanned by the average of the argument vectors, obtaining:\nvtrv := \u2211\ni\n\u2212\u2212\u2192 Sbji \u2297\n(\u2212\u2212\u2192 Sbji + \u2212\u2212\u2192 Obji\n2\n)\n\u2297 \u2212\u2212\u2192 Obji\nThe meaning of a transitive sentence then is computed as:\n\u2212\u2192svoPrj = \u2212\u2192s T \u00d7 vtrv \u00d7 \u2212\u2192o = \u2211\ni\n[\n\u3008\u2212\u2192s | \u2212\u2212\u2192 Sbji\u3009\u3008 \u2212\u2212\u2192 Obji | \u2212\u2192o \u3009\n(\u2212\u2212\u2192 Sbji + \u2212\u2212\u2192 Obji\n2\n)]\n(16)\nFeature-wise, the above translates to the following:\nF(\u2212\u2192svoPrj) = \u22c3\ni\n( F( \u2212\u2212\u2192 Sbji) \u222a F( \u2212\u2212\u2192 Obji) )\n| F ( \u3008\u2212\u2192s | \u2212\u2192 Sbji\u3009 ) F ( \u3008 \u2212\u2192 Obji| \u2212\u2192o \u3009 ) (17)\nNote that in contrast with the relational and Frobenius models, which all employ an element of intersective feature selection, the projective models presented in this section are purely union-based."}, {"heading": "6.4 Inclusion of verb vectors", "text": "The models of the previous sections provide a variety of options for representing the meaning of a verb from its arguments. However, none of these constructions takes into account the distributional vector of the verb itself, which includes valuable information that could further help in entailment tasks. We remedy this problem by embedding the missing information into the existing tensors; for example, we can amend the tensors of the projective model as follows:\nvitv = \u2211\ni\n\u2212\u2212\u2192 Sbji \u2297 (\u2212\u2212\u2192 Sbji \u2299 \u2212\u2192v )\nvvp = \u2211\ni\n(\u2212\u2212\u2192 Obji \u2299 \u2212\u2192v ) \u2297 \u2212\u2212\u2192 Obji (18)\nwith \u2212\u2192v denoting the distributional vector of the verb. In the context of an intransitive sentence, now we have the following interaction:\n\u2212\u2192sv = \u2212\u2192s T \u00d7 vitv = \u2212\u2192s T \u00d7\n\u2211\ni\n\u2212\u2212\u2192 Sbji \u2297 (\u2212\u2212\u2192 Sbji \u2299 \u2212\u2192v ) = \u2212\u2192v \u2299 \u2211\ni\n\u3008\u2212\u2192s | \u2212\u2212\u2192 Sbji\u3009 \u2212\u2212\u2192 Sbji (19)\nWe see that the result of the standard projective model (Equation 14) is now enchanced with an additional step of interesective feature selection. In feature inclusion terms, we get:\nF(\u2212\u2192sv) = F(\u2212\u2192v ) \u2229 F(\u2212\u2192svPrj) F( \u2212\u2192vo) = F(\u2212\u2192v ) \u2229 F(\u2212\u2192voPrj) (20)\nIt is easy to show that similar formulae hold for the relational and Frobenius models."}, {"heading": "7 Experimental setting", "text": "We evaluate the feature inclusion behaviour of the tensor-based models of Section 6 in three different tasks; specifically, we measure upward-monotone entailment between (a) intransitive sentences; (b) verb phrases; and (c) transitive sentences. We use the entailment datasets introduced in (Kartsaklis and Sadrzadeh, 2016), which consist of 135 subject-verb pairs, 218 verb-object pairs, and 70 subject-verbobject pairs, the phrases/sentences of which stand in a fairly clear entailment relationship. Each dataset has been created using hypernym-hyponym relationships from WordNet, and it was extended with the reverse direction of the entailments as negative examples, creating three strictly directional entailment\ndatasets of 270 (subject-verb), 436 (verb-object) and 140 (subject-verb-object) entries. Some examples of positive entailments from all three categories include:2\nSubject-verb Verb-object Subject-verb-object evidence suggests \u22a2 information expresses sign contract \u22a2 write agreement book presents account \u22a2 work shows evidence\nsurvey reveals \u22a2 work shows publish book \u22a2 produce publication woman marries man \u22a2 female joins male player plays \u22a2 contestant compete sing song \u22a2 perform music author retains house \u22a2 person holds property\nstudy demonstrates \u22a2 examination shows reduce number \u22a2 decrease amount study demonstrates importance \u22a2 work shows value summer finishes \u22a2 season ends promote development \u22a2 support event experiment tests hypothesis \u22a2 research evaluates proposal\nIn all cases, we first apply a compositional model to the phrases/sentences of each pair in order to create vectors representing their meaning, and then we evaluate the entailment relation between the phrases/sentences by using these composite vectors as input to a number of entailment measures. The goal is to see which combination of compositional model/entailment measure is capable of better recognizing strictly directional entailment relationships between phrases and sentences.\nWe experimented with a variety of entailment measures, including SAPinc and SBalAPinc as in (Kartsaklis and Sadrzadeh, 2016), their word-level counterparts (Kotlerman et al., 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.99 as in Kotlerman et al. (2010), WeedsPrec as in Weeds et al. (2004), and ClarkeDE as in Clarke (2009). We use strict feature inclusion as a baseline; in this case, entailment holds only when F( \u2212\u2212\u2212\u2212\u2192 phrase1) \u2286 F( \u2212\u2212\u2212\u2212\u2192 phrase2). For compositional operators, we experimented with element-wise vector multiplication and MIN, vector addition and MAX, and the tensor models presented in Section 6. Informal experimentation showed that directly embedding distributional information from verb vectors in the tensors (Section 6.4) works considerably better than the simple versions, so the results we report here are based on this approach. We also present results for a least squares fitting model, which approximates the distributional behaviour of holistic phrase/sentence vectors along the lines of (Baroni and Zamparelli, 2010). Specifically, for each verb, we compute an estimator that predicts the ith element of the resulting vector as follows:\n\u2212\u2192wi = (X T X)\u22121XT\u2212\u2192yi\nHere, the rows of matrix X are the vectors of the subjects (or objects) that occur with our verb, and \u2212\u2192yi is a vector containing the ith elements of the holistic phrase vectors across all training instances; the resulting \u2212\u2192wi\u2019s form the rows of our verb matrix. This model could be only implemented for verb-object and subject-verb phrases due to data sparsity problems. As our focus is on analytic properties of features, we did not experiment with any neural model.\nRegarding evaluation, since the tasks follow a binary classification objective and our models return a continuous value, we report area under curve (AUC). This reflects the generic discriminating power of a binary classifier by evaluating the task at every possible threshold. In all the experiments, we used a 300-dimensional PPMI vector space trained on the concatenation of UKWAC and Wikipedia corpora. The context was defined as a 5-word window around the target word."}, {"heading": "8 Results and discussion", "text": "The results are presented in Table 1 (subject-verb and verb-object task) and Table 2 (subject-verb-object task). In all cases, a combination of a Frobenius tensor model with one of the sentence-level measures (SAPinc) gives the highest performance. In general, SAPinc and SBalAPinc work very well with all the tested compositional models, achieving a cross-model performance higher than that of any other metric, for all three tasks. From a feature inclusion perspective, we see that models employing an element of interesective composition (vector multiplication, MIN, relational and Frobenius tensor models) have consistent high performances across all the tested measures. The reason may be that the intersective filtering avoids generation of very dense vectors and thus facilitates entailment judgements based on the DIH. On the other hand, union-based compositional models, such as vector addition, MAX, and the projective tensor models, produce dense vectors for even very short sentences, which affects negatively the evaluation of entailment. The non-compositional verb-only baseline was worse than any compositional model other than the least-squares model, which is the only tensor model that did not perform well; this indicates that our algebraic tensor-based constructions are more robust against data sparsity problems than statistical models based on holistic vectors of phrases and sentences.\n2The datasets are available at http://compling.eecs.qmul.ac.uk/resources/."}, {"heading": "9 Conclusion and future work", "text": "In this paper we investigated the application of the distributional inclusion hypothesis on evaluating entailment between phrase and sentence vectors produced by compositional operators with a focus on tensor-based models. Our results showed that intersective composition in general, and the Frobenius tensor models in particular, achieve the best performance when evaluating upward monotone entailment, especially when combined with the sentence-level measures of (Kartsaklis and Sadrzadeh, 2016). Experimenting with different versions of tensor models for entailment is an interesting topic that we plan to pursue further in a future paper. Furthermore, the extension of word-level entailment to phrases and sentences provides connections with natural logic (MacCartney and Manning, 2007), a topic that is worth a separate treatment and constitutes a future direction."}, {"heading": "Acknowledgments", "text": "The authors gratefully acknowledge support by EPSRC for Career Acceleration Fellowship EP/J002607/1 and AFOSR International Scientific Collaboration Grant FA9550-14-1-0079."}], "references": [{"title": "Sentence Entailment in Compositional Distributional Semantics", "author": ["E. Balk\u0131r", "D. Kartsaklis", "M. Sadrzadeh."], "venue": "Proceedings of the International Symposium on Artificial Intelligence and Mathematics (ISAIM), Fort Lauderdale, FL, January.", "citeRegEx": "Balk\u0131r et al\\.,? 2016a", "shortCiteRegEx": "Balk\u0131r et al\\.", "year": 2016}, {"title": "2016b. Topics in Theoretical Computer Science: The First IFIP WG", "author": ["Esma Balk\u0131r", "Mehrnoosh Sadrzadeh", "Bob Coecke"], "venue": "TTCS", "citeRegEx": "Balk\u0131r et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balk\u0131r et al\\.", "year": 2015}, {"title": "Using density matrices in a compositional distributional model of meaning", "author": ["Esma Balk\u0131r."], "venue": "Master\u2019s thesis, University of Oxford.", "citeRegEx": "Balk\u0131r.,? 2014", "shortCiteRegEx": "Balk\u0131r.", "year": 2014}, {"title": "Graded entailment for compositional distributional semantics", "author": ["Desislava Bankova", "Bob Coecke", "Martha Lewis", "Daniel Marsden."], "venue": "arXiv preprint arXiv:1601.04908.", "citeRegEx": "Bankova et al\\.,? 2016", "shortCiteRegEx": "Bankova et al\\.", "year": 2016}, {"title": "Nouns are Vectors, Adjectives are Matrices", "author": ["M. Baroni", "R. Zamparelli."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman."], "venue": "Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL \u201996, pages 310\u2013318, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Chen and Goodman.,? 1996", "shortCiteRegEx": "Chen and Goodman.", "year": 1996}, {"title": "Syntax-aware multi-sense word embeddings for deep compositional models of meaning", "author": ["Jianpeng Cheng", "Dimitri Kartsaklis."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1531\u20131542, Lisbon, Portugal, September. Association for Computational Linguistics.", "citeRegEx": "Cheng and Kartsaklis.,? 2015", "shortCiteRegEx": "Cheng and Kartsaklis.", "year": 2015}, {"title": "Context-theoretic semantics for natural language: an overview", "author": ["Daoud Clarke."], "venue": "Proceedings of the workshop on geometrical models of natural language semantics, pages 112\u2013119. Association for Computational Linguistics.", "citeRegEx": "Clarke.,? 2009", "shortCiteRegEx": "Clarke.", "year": 2009}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "Linguistic Analysis, 36.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Similarity-based models of word cooccurrence probabilities", "author": ["Ido Dagan", "Lillian Lee", "Fernando C.N. Pereira."], "venue": "Mach. Learn., 34(1-3):43\u201369.", "citeRegEx": "Dagan et al\\.,? 1999", "shortCiteRegEx": "Dagan et al\\.", "year": 1999}, {"title": "A Synopsis of Linguistic Theory, 1930-1955", "author": ["John R. Firth."], "venue": "Studies in Linguistic Analysis, pages 1\u201332.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["Maayan Geffet", "Ido Dagan."], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 107\u2013114, Ann Arbor, Michigan, June. Association for Computational Linguistics.", "citeRegEx": "Geffet and Dagan.,? 2005", "shortCiteRegEx": "Geffet and Dagan.", "year": 2005}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394\u20131404. Association for Computational Linguistics.", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Measuring semantic content in distributional vectors", "author": ["Aur\u00e9lie Herbelot", "Mohan Ganesalingam."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, volume 2, pages 440\u2013445. Association for Computational Linguistics.", "citeRegEx": "Herbelot and Ganesalingam.,? 2013", "shortCiteRegEx": "Herbelot and Ganesalingam.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655\u2013665, Baltimore, Maryland, June. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "B. Coecke, I. Hasuo, and P. Panangaden, editors, Quantum Physics and Logic 2014 (QPL 2014). EPTSC 172, pages 249\u2013261.", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2014", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2014}, {"title": "A Compositional Distributional Inclusion Hypothesis", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 9th Conference on Logical Aspects of Computational Linguistics (LACL), Lecture Notes in Computer Science. Springer. To appear.", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2016", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2016}, {"title": "A unified sentence space for categorical distributional-compositional semantics: Theory and experiments", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman."], "venue": "COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Posters, 8-15 December 2012, Mumbai, India, pages 549\u2013558.", "citeRegEx": "Kartsaklis et al\\.,? 2012", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "Directional distributional similarity for lexical inference", "author": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet."], "venue": "Natural Language Engineering, 16(04):359\u2013389.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Measures of distributional similarity", "author": ["Lillian Lee."], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, pages 25\u201332.", "citeRegEx": "Lee.,? 1999", "shortCiteRegEx": "Lee.", "year": 1999}, {"title": "An information-theoretic definition of similarity", "author": ["Dekang Lin."], "venue": "Proceedings of the International Conference on Machine Learning, pages 296\u2013304.", "citeRegEx": "Lin.,? 1998", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Natural logic for textual inference", "author": ["Bill MacCartney", "Christopher D. Manning."], "venue": "ACL Workshop on Textual Entailment and Paraphrasing. Association for Computational Linguistics.", "citeRegEx": "MacCartney and Manning.,? 2007", "shortCiteRegEx": "MacCartney and Manning.", "year": 2007}, {"title": "Evaluating neural word representations in tensor-based compositional settings", "author": ["Dmitrijs Milajevs", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708\u2013719, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Milajevs et al\\.,? 2014", "shortCiteRegEx": "Milajevs et al\\.", "year": 2014}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131439.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "Ng. A."], "venue": "Conference on Empirical Methods in Natural Language Processing 2012.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of artificial intelligence research, 37(1):141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Julie Weeds", "David Weir", "Diana McCarthy."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, number 1015. Association for Computational Linguistics.", "citeRegEx": "Weeds et al\\.,? 2004", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "1 Introduction Distributional hypothesis asserts that words that often occur in the same contexts have similar meanings (Firth, 1957).", "startOffset": 120, "endOffset": 133}, {"referenceID": 25, "context": "Naturally these models are used extensively to measure the semantic similarity of words (Turney and Pantel, 2010).", "startOffset": 88, "endOffset": 113}, {"referenceID": 9, "context": "Distributional inclusion hypothesis is exactly about such relationships, and particularly, about entailment (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013).", "startOffset": 108, "endOffset": 185}, {"referenceID": 11, "context": "Distributional inclusion hypothesis is exactly about such relationships, and particularly, about entailment (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013).", "startOffset": 108, "endOffset": 185}, {"referenceID": 13, "context": "Distributional inclusion hypothesis is exactly about such relationships, and particularly, about entailment (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013).", "startOffset": 108, "endOffset": 185}, {"referenceID": 16, "context": "In recent work, we have applied the same compositionality principles to lift the entailment relation from word level to phrase/sentence level (Kartsaklis and Sadrzadeh, 2016; Balk\u0131r et al., 2016a; Balk\u0131r et al., 2016b; Balk\u0131r, 2014).", "startOffset": 142, "endOffset": 232}, {"referenceID": 0, "context": "In recent work, we have applied the same compositionality principles to lift the entailment relation from word level to phrase/sentence level (Kartsaklis and Sadrzadeh, 2016; Balk\u0131r et al., 2016a; Balk\u0131r et al., 2016b; Balk\u0131r, 2014).", "startOffset": 142, "endOffset": 232}, {"referenceID": 2, "context": "In recent work, we have applied the same compositionality principles to lift the entailment relation from word level to phrase/sentence level (Kartsaklis and Sadrzadeh, 2016; Balk\u0131r et al., 2016a; Balk\u0131r et al., 2016b; Balk\u0131r, 2014).", "startOffset": 142, "endOffset": 232}, {"referenceID": 2, "context": "The work in (Balk\u0131r, 2014; Balk\u0131r et al., 2016b) was focused on the use of entropic measures on density matrices and compositional operators thereof, but no experimental results were considered; similarly, Bankova et al.", "startOffset": 12, "endOffset": 48}, {"referenceID": 0, "context": "In (Balk\u0131r et al., 2016a), we showed how entropic and other measures can be used on vectors as well as on density matrices and supported this claim with experimental results.", "startOffset": 3, "endOffset": 25}, {"referenceID": 16, "context": "In (Kartsaklis and Sadrzadeh, 2016), we focused on making the distributional inclusion hypothesis compositional, worked", "startOffset": 3, "endOffset": 35}, {"referenceID": 0, "context": "In recent work, we have applied the same compositionality principles to lift the entailment relation from word level to phrase/sentence level (Kartsaklis and Sadrzadeh, 2016; Balk\u0131r et al., 2016a; Balk\u0131r et al., 2016b; Balk\u0131r, 2014). The work in (Balk\u0131r, 2014; Balk\u0131r et al., 2016b) was focused on the use of entropic measures on density matrices and compositional operators thereof, but no experimental results were considered; similarly, Bankova et al. (2016) use a specific form of density matrices to represent words for entailment purposes, focusing only on theory.", "startOffset": 175, "endOffset": 462}, {"referenceID": 23, "context": "One can broadly classify the compositional distributional models to three categories: ones that are based on simple element-wise operations between vectors, such as addition and multiplication (Mitchell and Lapata, 2010); tensor-based models in which relational words such as verbs and adjectives are tensors and matrices contracting and multiplying with noun (and noun-phrase) vectors (Coecke et al.", "startOffset": 193, "endOffset": 220}, {"referenceID": 8, "context": "One can broadly classify the compositional distributional models to three categories: ones that are based on simple element-wise operations between vectors, such as addition and multiplication (Mitchell and Lapata, 2010); tensor-based models in which relational words such as verbs and adjectives are tensors and matrices contracting and multiplying with noun (and noun-phrase) vectors (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al.", "startOffset": 386, "endOffset": 470}, {"referenceID": 12, "context": "One can broadly classify the compositional distributional models to three categories: ones that are based on simple element-wise operations between vectors, such as addition and multiplication (Mitchell and Lapata, 2010); tensor-based models in which relational words such as verbs and adjectives are tensors and matrices contracting and multiplying with noun (and noun-phrase) vectors (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al.", "startOffset": 386, "endOffset": 470}, {"referenceID": 4, "context": "One can broadly classify the compositional distributional models to three categories: ones that are based on simple element-wise operations between vectors, such as addition and multiplication (Mitchell and Lapata, 2010); tensor-based models in which relational words such as verbs and adjectives are tensors and matrices contracting and multiplying with noun (and noun-phrase) vectors (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al.", "startOffset": 386, "endOffset": 470}, {"referenceID": 24, "context": ", 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al., 2012; Kalchbrenner et al., 2014) and is usually optimized against a specific objective.", "startOffset": 147, "endOffset": 195}, {"referenceID": 14, "context": ", 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al., 2012; Kalchbrenner et al., 2014) and is usually optimized against a specific objective.", "startOffset": 147, "endOffset": 195}, {"referenceID": 12, "context": "Models of this form have been used in the past with success in a number of NLP tasks, such as head-verb disambiguation (Grefenstette and Sadrzadeh, 2011), term-definition classification (Kartsaklis et al.", "startOffset": 119, "endOffset": 153}, {"referenceID": 17, "context": "Models of this form have been used in the past with success in a number of NLP tasks, such as head-verb disambiguation (Grefenstette and Sadrzadeh, 2011), term-definition classification (Kartsaklis et al., 2012), and generic sentence similarity (Kartsaklis and Sadrzadeh, 2014).", "startOffset": 186, "endOffset": 211}, {"referenceID": 15, "context": ", 2012), and generic sentence similarity (Kartsaklis and Sadrzadeh, 2014).", "startOffset": 41, "endOffset": 73}, {"referenceID": 12, "context": "In this paper we extend this work to entailment, by investigating (theoretically and experimentally) the properties of feature inclusion on the phrase and sentence vectors produced in four different tensor-based compositional distributional models: the relational models of (Grefenstette and Sadrzadeh, 2011), the Frobenius models of (Kartsaklis et al.", "startOffset": 274, "endOffset": 308}, {"referenceID": 17, "context": "In this paper we extend this work to entailment, by investigating (theoretically and experimentally) the properties of feature inclusion on the phrase and sentence vectors produced in four different tensor-based compositional distributional models: the relational models of (Grefenstette and Sadrzadeh, 2011), the Frobenius models of (Kartsaklis et al., 2012; Milajevs et al., 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010).", "startOffset": 334, "endOffset": 382}, {"referenceID": 22, "context": "In this paper we extend this work to entailment, by investigating (theoretically and experimentally) the properties of feature inclusion on the phrase and sentence vectors produced in four different tensor-based compositional distributional models: the relational models of (Grefenstette and Sadrzadeh, 2011), the Frobenius models of (Kartsaklis et al., 2012; Milajevs et al., 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010).", "startOffset": 334, "endOffset": 382}, {"referenceID": 16, "context": ", 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010).", "startOffset": 34, "endOffset": 66}, {"referenceID": 4, "context": ", 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010).", "startOffset": 112, "endOffset": 141}, {"referenceID": 16, "context": "A newly proposed adaptation of these metrics, recently introduced by the authors in (Kartsaklis and Sadrzadeh, 2016), is also detailed.", "startOffset": 84, "endOffset": 116}, {"referenceID": 16, "context": "We experiment with these models and evaluate them on entailment relations between simple intransitive sentences, verb phrases, and transitive sentences on the datasets of (Kartsaklis and Sadrzadeh, 2016).", "startOffset": 171, "endOffset": 203}, {"referenceID": 16, "context": "On a more general note, the experimental results of this paper support that of previous work (Kartsaklis and Sadrzadeh, 2016) and strongly indicate that compositional models employing some form of intersective feature selection, i.", "startOffset": 93, "endOffset": 125}, {"referenceID": 4, "context": ", 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al., 2012; Kalchbrenner et al., 2014) and is usually optimized against a specific objective. Tensor-based models stand in between the two extremes of element-wise vector mixing and neural net-based methods, offering a sufficiently powerful alternative that allows for theoretical reasoning at a level deeper than it is usually possible with black-box statistical approaches. Models of this form have been used in the past with success in a number of NLP tasks, such as head-verb disambiguation (Grefenstette and Sadrzadeh, 2011), term-definition classification (Kartsaklis et al., 2012), and generic sentence similarity (Kartsaklis and Sadrzadeh, 2014). In this paper we extend this work to entailment, by investigating (theoretically and experimentally) the properties of feature inclusion on the phrase and sentence vectors produced in four different tensor-based compositional distributional models: the relational models of (Grefenstette and Sadrzadeh, 2011), the Frobenius models of (Kartsaklis et al., 2012; Milajevs et al., 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010). Contrary to formal semantic models, and customary to distributional models, our entailments are nonboolean and come equipped with degrees. We review a number of measures that have been developed for evaluating degrees of entailment at the lexical level, such as the APinc measure and its newer version, balAPinc of Kotlerman et al. (2010), which is considered as state-of-the-art for word-level entailment.", "startOffset": 42, "endOffset": 1669}, {"referenceID": 23, "context": "In the simplest case, this is done by element-wise operations on the vectors of the words (Mitchell and Lapata, 2010).", "startOffset": 90, "endOffset": 117}, {"referenceID": 8, "context": "These are then applied to the vectors of their arguments by following the rules of the grammar (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010).", "startOffset": 95, "endOffset": 179}, {"referenceID": 12, "context": "These are then applied to the vectors of their arguments by following the rules of the grammar (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010).", "startOffset": 95, "endOffset": 179}, {"referenceID": 4, "context": "These are then applied to the vectors of their arguments by following the rules of the grammar (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010).", "startOffset": 95, "endOffset": 179}, {"referenceID": 24, "context": "\u2212\u2192 svo = (verb \u00d7 \u2212\u2192 obj)\u00d7 \u2212\u2212\u2192 subj Finally, phrase and sentence vectors have been also produced by the application of neural architectures, such as recursive or recurrent neural networks (Socher et al., 2012; Cheng and Kartsaklis, 2015) and convolutional neural networks (Kalchbrenner et al.", "startOffset": 187, "endOffset": 236}, {"referenceID": 6, "context": "\u2212\u2192 svo = (verb \u00d7 \u2212\u2192 obj)\u00d7 \u2212\u2212\u2192 subj Finally, phrase and sentence vectors have been also produced by the application of neural architectures, such as recursive or recurrent neural networks (Socher et al., 2012; Cheng and Kartsaklis, 2015) and convolutional neural networks (Kalchbrenner et al.", "startOffset": 187, "endOffset": 236}, {"referenceID": 14, "context": ", 2012; Cheng and Kartsaklis, 2015) and convolutional neural networks (Kalchbrenner et al., 2014).", "startOffset": 70, "endOffset": 97}, {"referenceID": 9, "context": "3 Distributional inclusion hypothesis The distributional inclusion hypothesis (DIH) (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that whenever a word u entails a word v, then it makes sense to replace instances of u with v.", "startOffset": 84, "endOffset": 161}, {"referenceID": 11, "context": "3 Distributional inclusion hypothesis The distributional inclusion hypothesis (DIH) (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that whenever a word u entails a word v, then it makes sense to replace instances of u with v.", "startOffset": 84, "endOffset": 161}, {"referenceID": 13, "context": "3 Distributional inclusion hypothesis The distributional inclusion hypothesis (DIH) (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that whenever a word u entails a word v, then it makes sense to replace instances of u with v.", "startOffset": 84, "endOffset": 161}, {"referenceID": 11, "context": "Despite this and other limitations, the DIH has been subject to a good amount of study in the distributional semantics community and its predictions have been validated (Geffet and Dagan, 2005; Kotlerman et al., 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 18, "context": "Despite this and other limitations, the DIH has been subject to a good amount of study in the distributional semantics community and its predictions have been validated (Geffet and Dagan, 2005; Kotlerman et al., 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 5, "context": "Examples of measures used here are entropy-based measures such as KL-divergence (Chen and Goodman, 1996).", "startOffset": 80, "endOffset": 104}, {"referenceID": 19, "context": "To overcome this restriction, a variant referred to by \u03b1-skew (Lee, 1999) has been proposed (for \u03b1 \u2208 (0, 1] a smoothing parameter).", "startOffset": 62, "endOffset": 73}, {"referenceID": 25, "context": "Measures developed by Weeds et al. (2004) and Clarke (2009) advance this simple methods by arguing that not all features play an equal role in representing words and hence they should not be treated equally when it comes to measuring entailment.", "startOffset": 22, "endOffset": 42}, {"referenceID": 7, "context": "(2004) and Clarke (2009) advance this simple methods by arguing that not all features play an equal role in representing words and hence they should not be treated equally when it comes to measuring entailment.", "startOffset": 11, "endOffset": 25}, {"referenceID": 18, "context": "Pertinence is computed by various different measures, the most recent of which is balAPinc (Kotlerman et al., 2010), where LIN is Lin\u2019s similarity (Lin, 1998) and APinc is an asymmetric measure:", "startOffset": 91, "endOffset": 115}, {"referenceID": 20, "context": ", 2010), where LIN is Lin\u2019s similarity (Lin, 1998) and APinc is an asymmetric measure:", "startOffset": 39, "endOffset": 50}, {"referenceID": 16, "context": "4 Measuring feature inclusion at the phrase/sentence level In recent work, the authors of this paper introduced a variation of the APinc and balAPinc measures aiming to address the extra complications imposed when evaluating entailment at the phrase/sentence level (Kartsaklis and Sadrzadeh, 2016).", "startOffset": 265, "endOffset": 297}, {"referenceID": 18, "context": "According to (Kotlerman et al., 2010), the rationale of including a symmetric measure was that APinc tends to return unjustifyingly high scores when the entailing word is infrequent, that is, when the feature vector of the entailing word is very short; the purpose of the symmetric measure was to penalize the result, since in this case the similarity of the narrower term with the broader one is usually low.", "startOffset": 13, "endOffset": 37}, {"referenceID": 16, "context": "As shown in (Kartsaklis and Sadrzadeh, 2016), element-wise composition of this form lifts naturally from the word level to phrase/sentence level; specifically, for two sentences s1 = u1 .", "startOffset": 12, "endOffset": 44}, {"referenceID": 0, "context": "This kind of lifting of the entailment relationship from words to the phrase/sentence level also holds for tensor-based models (Balk\u0131r et al., 2016a).", "startOffset": 127, "endOffset": 149}, {"referenceID": 12, "context": "1 Relational model As a starting point we will use the model of Grefenstette and Sadrzadeh (2011), which adopts an extensional approach and builds the tensor of a relational word from the vectors of its arguments.", "startOffset": 64, "endOffset": 98}, {"referenceID": 12, "context": "For a transitive sentence, the model of Grefenstette and Sadrzadeh (2011) returns a matrix, computed in the following way:", "startOffset": 40, "endOffset": 74}, {"referenceID": 17, "context": "2 Frobenius models As pointed out in (Kartsaklis et al., 2012), the disadvantage of the relational model is that their resulting representations of verbs have one dimension less than what their types dictate.", "startOffset": 37, "endOffset": 62}, {"referenceID": 17, "context": "A solution presented in (Kartsaklis et al., 2012) suggested the use of Frobenius operators in order to expand vectors and matrices into higher order tensors by embedding them into the the diagonals.", "startOffset": 24, "endOffset": 49}, {"referenceID": 15, "context": "Below we define the feature sets of two variations, where this combination is achieved via addition (we refer to this model as Frobenius additive) and element-wise multiplication (Frobenius multiplicative) of the vectors produced by the individual models (Kartsaklis and Sadrzadeh, 2014):", "startOffset": 255, "endOffset": 287}, {"referenceID": 16, "context": "We use the entailment datasets introduced in (Kartsaklis and Sadrzadeh, 2016), which consist of 135 subject-verb pairs, 218 verb-object pairs, and 70 subject-verbobject pairs, the phrases/sentences of which stand in a fairly clear entailment relationship.", "startOffset": 45, "endOffset": 77}, {"referenceID": 16, "context": "We experimented with a variety of entailment measures, including SAPinc and SBalAPinc as in (Kartsaklis and Sadrzadeh, 2016), their word-level counterparts (Kotlerman et al.", "startOffset": 92, "endOffset": 124}, {"referenceID": 18, "context": "We experimented with a variety of entailment measures, including SAPinc and SBalAPinc as in (Kartsaklis and Sadrzadeh, 2016), their word-level counterparts (Kotlerman et al., 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.", "startOffset": 156, "endOffset": 180}, {"referenceID": 4, "context": "We also present results for a least squares fitting model, which approximates the distributional behaviour of holistic phrase/sentence vectors along the lines of (Baroni and Zamparelli, 2010).", "startOffset": 162, "endOffset": 191}, {"referenceID": 4, "context": ", 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.", "startOffset": 58, "endOffset": 82}, {"referenceID": 4, "context": ", 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.99 as in Kotlerman et al. (2010), WeedsPrec as in Weeds et al.", "startOffset": 58, "endOffset": 135}, {"referenceID": 4, "context": ", 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.99 as in Kotlerman et al. (2010), WeedsPrec as in Weeds et al. (2004), and ClarkeDE as in Clarke (2009).", "startOffset": 58, "endOffset": 172}, {"referenceID": 4, "context": ", 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.99 as in Kotlerman et al. (2010), WeedsPrec as in Weeds et al. (2004), and ClarkeDE as in Clarke (2009). We use strict feature inclusion as a baseline; in this case, entailment holds only when F( \u2212\u2212\u2212\u2212\u2192 phrase1) \u2286 F( \u2212\u2212\u2212\u2212\u2192 phrase2).", "startOffset": 58, "endOffset": 206}, {"referenceID": 16, "context": "Our results showed that intersective composition in general, and the Frobenius tensor models in particular, achieve the best performance when evaluating upward monotone entailment, especially when combined with the sentence-level measures of (Kartsaklis and Sadrzadeh, 2016).", "startOffset": 242, "endOffset": 274}, {"referenceID": 21, "context": "Furthermore, the extension of word-level entailment to phrases and sentences provides connections with natural logic (MacCartney and Manning, 2007), a topic that is worth a separate treatment and constitutes a future direction.", "startOffset": 117, "endOffset": 147}], "year": 2016, "abstractText": "According to the distributional inclusion hypothesis, entailment between words can be measured via the feature inclusions of their distributional vectors. In recent work, we showed how this hypothesis can be extended from words to phrases and sentences in the setting of compositional distributional semantics. This paper focuses on inclusion properties of tensors; its main contribution is a theoretical and experimental analysis of how feature inclusion works in different concrete models of verb tensors. We present results for relational, Frobenius, projective, and holistic methods and compare them to the simple vector addition, multiplication, min, and max models. The degrees of entailment thus obtained are evaluated via a variety of existing wordbased measures, such as Weed\u2019s and Clarke\u2019s, KL-divergence, APinc, balAPinc, and two of our previously proposed metrics at the phrase/sentence level. We perform experiments on three entailment datasets, investigating which version of tensor-based composition achieves the highest performance when combined with the sentence-level measures.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}