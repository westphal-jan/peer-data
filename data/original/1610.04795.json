{"id": "1610.04795", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2016", "title": "Sample Efficient Optimization for Learning Controllers for Bipedal Locomotion", "abstract": "Learning policies for bipedal locomotion can be difficult, as experiments are expensive and simulation does not usually transfer well to hardware. To counter this, we need al- gorithms that are sample efficient and inherently safe. Bayesian Optimization is a powerful sample-efficient tool for optimizing non-convex black-box functions. However, its performance can degrade in higher dimensions. We develop a distance metric for bipedal locomotion that enhances the sample-efficiency of Bayesian Optimization and use it to train a 16 dimensional neuromuscular model for planar walking. This distance metric reflects some basic gait features of healthy walking and helps us quickly eliminate a majority of unstable controllers. With our approach we can learn policies for walking in less than 100 trials for a range of challenging settings. In simulation, we show results on two different costs and on various terrains including rough ground and ramps, sloping upwards and downwards. We also perturb our models with unknown inertial disturbances analogous with differences between simulation and hardware. These results are promising, as they indicate that this method can potentially be used to learn control policies on hardware.", "histories": [["v1", "Sat, 15 Oct 2016 22:53:04 GMT  (3097kb,D)", "http://arxiv.org/abs/1610.04795v1", "To appear in International Conference on Humanoid Robots (Humanoids '2016), IEEE-RAS. (Rika Antonova and Akshara Rai contributed equally)"]], "COMMENTS": "To appear in International Conference on Humanoid Robots (Humanoids '2016), IEEE-RAS. (Rika Antonova and Akshara Rai contributed equally)", "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["rika antonova", "akshara rai", "christopher g atkeson"], "accepted": false, "id": "1610.04795"}, "pdf": {"name": "1610.04795.pdf", "metadata": {"source": "CRF", "title": "Sample Efficient Optimization for Learning Controllers for Bipedal Locomotion", "authors": ["Rika Antonova", "Akshara Rai", "Christopher G. Atkeson"], "emails": ["arai}@andrew.cmu.edu,", "cga@cs.cmu.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nDesigning and learning policies for bipedal locomotion is a challenging problem, as it is extremely expensive to do such experiments on an actual robot. We typically do not have robots that can take a fall, and it is cumbersome to perform these experiments. On top of this, most objective functions are non-convex, non-differentiable and noisy. With these considerations in mind, it is important to find optimization methods that are sample efficient, robust to noise and non-convexity, and try to minimize the number of bad policies sampled. Bayesian Optimization is one such gradient-free black-box global optimization method, that is sample efficient and robust to noise.\nOne common way of learning controllers is to come up with a control policy parameterizations and a cost function [4], [23], [6]. This cost is now minimised with respect to the policy parameters. In general, a variety of optimization approaches could be applied to this problem, for example gradient descent, evolutionary algorithms, and random search. Approaches like grid search, pure random search, and various evolutionary algorithms usually make the least re-\n* Both of these authors contributed equally. 1 Robotics Institute, School of Computer Science, Carnegie Mellon\nUniversity, USA. 2 Autonomous Motion Department, MPI-IS, Tu\u0308bingen, Germany. {rantonov, arai}@andrew.cmu.edu, cga@cs.cmu.edu This research was supported in part by the Max-Planck-Society. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding organization.\nstrictive assumptions, but are not sample-efficient. Gradientbased algorithms can be very effective when an analytical gradient is available or can be approximated effectively. However, random restarts are usually necessary to optimize non-convex functions to avoid bad local optima, reducing sample efficiency. Bayesian optimization is well suited for such optimization problems as it constructs a global representation of the cost function, while only reducing uncertainty in promising regions of the search space.\nIn this work, we learn optimal reflex parameters for neuromuscular models described in [9]. These models are human-inspired control pathways that are capable of producing locomotion behaviour in a variety of scenarios as demonstrated in [18]. We start with a 2 dimensional 7-link simulated robot with hip, knee and ankle actuation. We formulate a cost function which incorporates components like distance and time walked and optimize it over a set of 16 parameters of the neuromuscular model.\nPromising results were reported in prior work for optimizing an 8-dimensional control policy for a small bipedal robot using Bayesian Optimization [4]. However, our 16-dimensional search space proved to be challenging for standard Bayesian Optimization, with performance not much better than uniform random search. Bayesian Optimization can take advantage of a domain-specific kernel, which gives an informed similarity between different policies. In this paper, we achieve this by developing a Determinants of Gait (DoG) kernel for the domain of bipedal ocomotion. Our kernel uses gait characteristics described in [17] to create an appropriate similarity metric between different parameter sets of the neuromuscular model. Under this metric, policies that generate successful walking gaits are closer together and policies that result in a fall are more distant from successful ones. This helps the optimization to effectively separate good regions of the parameter space from bad regions, making it more sample efficient.\nWe demonstrate that our kernel can substantially reduce the number of function evaluations (or robot trials) needed for optimization on different terrains with modeling disturbances. This leads us to believe that our kernel helps improve sample efficiency in conditions different from those in which it was generated. Potentially, we can generate this kernel in simulation and use it for optimization on the actual robots."}, {"heading": "II. BACKGROUND", "text": ""}, {"heading": "A. Overview of Bayesian Optimization", "text": "Bayesian Optimization is a framework for sequential global search to find a vector x\u2217 that minimizes a cost\nar X\niv :1\n61 0.\n04 79\n5v 1\n[ cs\n.R O\n] 1\n5 O\nct 2\n01 6\nfunction f(x), while evaluating f as few times as possible ([3] give an overview).\nx\u2217 = arg min x f(x)\nThe optimization starts with initializing a prior to capture uncertainty over the value of f(x) for each x in the domain. At iteration t an auxiliary function u, called an acquisition function, is used to sequentially select the next parameter vector to test, xt. f(xt) is then evaluated by doing an experiment, and used to update our estimate of f . The aim of the acquisition function is to achieve an effective tradeoff between exploration and exploitation. The use of an acquisition function is illustrated in Figure 1 (1D example).\nA common way to model the prior and posterior for f is by using a Gaussian Process f(x) \u223c GP(\u00b5(x), k(xi,xj)), with mean function \u00b5 and kernel k. The mean of the prior can be set to 0 if no relevant domain-specific information is available. The kernel k(xi,xj) encodes how similar f is expected to be for two inputs xi,xj : points close together are expected to influence each other strongly, while points far apart would have almost no influence. The most widely used kernel is Squared Exponential kernel of the form kSE(xi,xj) = exp ( \u2212 12\u2016xi \u2212 xj\u2016 2 )\n. A Gaussian Process conditioned on cost evaluations represents a posterior distribution for f . Update equations for the posterior mean and covariance conditioned on evaluations can be found in [16] for both noisy and noiseless settings. An example posterior is illustrated in Figure 2."}, {"heading": "B. Optimization for Bipedal Locomotion", "text": "Bayesian Optimization (BO) with Gaussian Processes and closely related methods have been recently applied to several robotics domains. Krause et al. [12] developed an approach utilizing Gaussian Processes and the principle of optimizing mutual information for solving sensor placement problems. Martinez-Cantin et al. [14] used BO for online path planning\nfor optimal sensing with a mobile robot. Lizotte et al. [13] used a closely related approach of Gaussian Process Regression to optimize the gait on a quadruped robot and showed that this approach required substantially fewer evaluations than state-of-the-art local gradient approaches.\nMore specific to the domain of bipedal locomotion, Calandra et al. used BO to efficiently find gait parameters that optimize a desired cost [4]. They optimized eight parameters - four threshold values of a finite state machine of a walking controller and four control signals applied during extension and flexion of knees and hips - for a small biped.\nWhile these previous results are encouraging, it is not immediately clear whether BO would be as successful in finding good policies for higher-dimensional controllers. Calandra et al. mentioned that only around 1% of the parameter space they considered led to walking gaits, and we have observed similar difficulties in our experiments in 16 dimensions. Hence two questions arise : would BO be effective if the dimensionality is increased from 8 to 16? And if it does, how does it compare to previously used approaches, like CMA-ES [18]?"}, {"heading": "III. REVIEW OF NEUROMUSCULAR MODELS", "text": "1) Neuromuscular Stance Control: In stance, each leg is actuated by 7 Hill-type muscles [15], consisting of the soleus (SOL), gastrocnemius (GAS), vastus (VAS), hamstring (HAM), tibialis anterior (TA), hip flexors (HFL)\nand gluteus (GLU), illustrated in Figure 3. Together, these muscles produce torques about the hip, knee and ankle. The muscle force F is a non-linear function of the muscle state sm and stimulus Sm, which when multiplied by the moment arm r(\u03b8i) gives the resultant torque on joint i:\n\u03c4mi = F (S m, sm)r(\u03b8i),\nwhere \u03c4mi is the torque applied by muscle m on joint i and \u03b8i is the joint angle.\nMost of the muscle reflexes in stance are positive length or force feedbacks on the muscle stimulus. In general, the stimulus Sm(t) for muscle m is a function of the time delayed length or force signal Pm times a feedback gain Km:\nSm(t) = Sm0 +K m \u00b7 Pm(t\u2212\u2206t),\nwhere Sm0 is the pre-stimulus, K m is the feedback gain and Pm is the time-delayed feedback signal of length or force. Some muscles can be co-activated and have multiple feedback signals from more than one muscle. The feedback gains Km described above are a subset of the parameters that we aim to tune in our optimization. The details of these feedback pathways can be found in [18].\nThis feedback structure generates compliant leg behaviour and prevents the knee from overextending in stance. To balance the trunk, feedback on the torso angle is added to the GLU stimulus:\nSGLUtorso(t) = K stance p (\u03b8des \u2212 \u03b8)\u2212Kstanced \u03b8\u0307,\nwhere Kstancep is the position gain on the torso angle \u03b8 and \u03b8des is the desired angle. Kstanced is the velocity gain and \u03b8\u0307 is the angular velocity. Specifically, here are the stance parameters we optimize over, and their roles in the neuromuscular model:\n1) KGAS : Positive force feedback gain on GAS 2) KGLU : Positive force feedback gain on GLU 3) KHAM : Positive force feedback gain on HAM 4) KSOL : Positive force feedback gain on SOL 5) KTASOL : Negative force feedback from SOL on TA 6) KTA : Positive length feedback on TA 7) KV AS : Positive force feedback on VAS 8) Kstancep : Position gain on feedback on torso angle 9) Kstanced : Velocity gain on feedback on torso velocity 10) KGLUmix : Gain for mixing force feedback and feedback on angle for GLU\n2) Swing Leg Placement Control: The swing control is controlled by three main components \u2013 target leg angle, leg clearance and hip control. Target leg angle is a direct result of the foot placement strategy which is a function of the velocity of the center of mass (CoM) v, and the as distance between the stance leg the CoM, and presented in [23]:\n\u03b1tgt = \u03b10 + Cdd+ Cvv,\nwhere \u03b1tgt is the target leg angle, \u03b10 is the nominal leg angle, \u03b10, Cd and Cv are parameters optimized by our control.\nLeg clearance is a function of the desired leg retraction during swing. The knee is actively flexed until the leg reaches\nthe desired leg clearance height, lclr and then held at this height, until the leg reaches a threshold leg angle. At this point, the knee is extended and allowed to reach the target leg angle \u03b1tgt. Details of this control can be found in [7]. As was noted in [18], and observed in our experiments, the control is relatively insensitive to the individual gains of the set-up in swing. It is sufficient to control the higher level parameters such as the desired leg clearance and target leg angle.\nThe third part of the control involves maintaining the desired leg angle \u03b1tgt by applying a hip torque \u03c4\u03b1hip:\n\u03c4\u03b1hip = K swing p (\u03b1tgt \u2212 \u03b1)\u2212K swing d (\u03b1\u0307),\nwhere Kswingp is the position gain on the leg angle, K swing d is the velocity gain, \u03b1 is the leg angle and \u03b1\u0307 is the leg angular velocity (see Figure 3).\nMore concisely, the swing parameters that we focus on in our optimization are the following:\n1) Kswingp : Position gain on feedback on leg angle 2) Kswingd : Velocity gain on feedback on leg velocity 3) \u03b10 : Nominal leg angle 4) Cd : Gain on the horizontal distance between the stance\nfoot and CoM 5) Cv : Gain on the horizontal velocity of the CoM 6) lclr : Desired leg clearance Though originally developed for explaining human neural control pathways, these controllers have recently been applied to robots and prosthetics, for example in [19] and [20]. As demonstrated in [18], these models are indeed capable of generating a variety of locomotion behaviours for a humanoid model - for example, walking on flat, rough ground, turning, running, walking upstairs and on ramps. However, a full study of using these models to control biped robots still needs to be done. Whether these models will transfer well to robots with significantly different dynamics and inertial properties than humans needs to be explored. It is difficult to transfer these models to robots because of a large number of interdependent gains that need to be tuned. Typically, this is done using Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [10], an evolutionary algorithm for difficult non-linear non-convex blackbox optimization problems. Even though CMA-ES is useful for optimizing non-convex problems in high dimensions, it is not sample efficient and depends on the initial starting point. An optimization for 16 neuromuscular parameters takes 400 generations, around a day on a standard i7 processor and about 5,000 trials, as reported in [18].\nThe large number of trials make it impossible to implement CMA-ES on a real robot. This is a shortcoming because often we find that after training the policies in simulation, they do not transfer well to the real robot, due to differences between simulation and real hardware."}, {"heading": "IV. DETERMINANTS OF GAIT KERNEL", "text": ""}, {"heading": "A. Kernels for Sequential Decision Making", "text": "As described in section II-A, the kernel k(xi,xj) captures how similar the cost function f is expected to be for\nparameter vectors xi and xj , and in the case of using Squared Exponential kernel, the similarity is a function of the Euclidean distance between xi,xj \u2208 Rd.\nA more informed alternative is to use a kernel that specifically leverages the structure of the problem at hand, for example the resulting trajectories or behavior. Intuitively, a kernel that can better encode similarity among policies will be more sample-efficient, since it will be better able to generalize across policies with similar performance. The Behavior-Based Kernel (BBK) of [21] is one kernel that leverages structure in the trajectories generated by the evaluated policies. To determine the similarity between policies given by vectors xi,xj , BBK uses the similarity of the trajectory distributions induced by the corresponding policies (instead of the default approach of utilizing Euclidean distance |xi\u2212xj |2). While this could help in settings where computing a trajectory for a set of policy parameters is inexpensive, in the setting of robotic locomotion this requires running a simulation or executing the policy on the real hardware. This amounts to being as expensive as a cost function evaluation which makes BBK infeasible for such problems. Nonetheless, the idea of using auxiliary information in the kernel is promising, if this information can be pre-computed for a large portion of the policy space and made available during online optimization. We describe our approach for constructing such a kernel in the next section. Our kernel effectively incorporates domain knowledge available in bipedal locomotion and eliminates the need for computing full trajectories during online optimization. Instead, it uses behavior information from only a short part of the trajectories pre-generated during an offline phase."}, {"heading": "B. Determinants of Gait Kernel", "text": "Bipedal walking can be characterized with some basic metrics, called gait determinants, as described in [17]. The six determinants of gait deal with the conservation of energy and maintaining forward momentum during human walking. For developing our kernel, we focussed on the knee flexion in swing, ankle movement and center of mass trajectory.\nTo compute the gait determinants of a given set of parameters, we run a short simulation for 5 seconds (as compared to 100 seconds for a complete trial). Next, we compute the score of the parameters on the following metrics M1\u22125:\n1) Is the knee flexed in swing?\nM1 = (\u03b8 thr high > \u03b8 swing knee ) \u2227 (\u03b8 swing knee > \u03b8 thr low) (1)\nHere, \u03b8swingknee is the knee joint angle in swing, \u03b8 thr high and \u03b8thrlow are the high and low thresholds on knee angle, similar to human data as described in [22].\n2) Is there heel-strike and toe-off?\nM2 = (\u03b8 strike ankle < 0) \u2227 (\u03b8t.o.ankle > 0) (2)\n\u03b8strikeankle is the ankle joint angle at heel-strike (start of stance) and \u03b8t.o.ankle is the ankle joint angle at take-off (end of stance). The two conditions ensure heel-strike and toe-off respectively.\n3) Is the center of mass movement approximately oscillatory between steps?\nM3 = (Y strike CoM < Y midst CoM ) \u2227 (Y t.o.CoM < Y midstCoM ) (3)\nY strikeCoM , Y midst CoM and Y t.o. CoM are center of mass heights\nat heel strike, mid stance and take-off. 4) Is the torso leaning forward?\nM4 = \u03b8 mean torso > 0 (4)\n\u03b8meantorso is the mean torso angle which should be leaning forward for a energy efficient forward movement.\n5) Deviation from average human walking speed\nM5 = ||vavg \u2212 vhuman|| (5)\nvavg and vhuman are the average simulator speed and the average human walking speed, 1.3m/s [22].\nM1\u22124 are binary \u2208 {0, 1} and M5 is continuous per step. These are then summed up to form the total score for step i:\nscorei = 5\u2211 j=1 M ij (6)\nThe final metric is then computed as a sum of scores over all the steps:\n\u03c6(x) = N\u2211 i=1 scorei (7)\nwhere N is the total number of steps in the first 5 seconds of simulation.\nWith this, a 16D point x in the original parameter space now corresponds to a 1D point \u03c6(x) in this new feature space and we obtain our Determinants of Gait kernel:\nk(xi,xj)\u2192 k(\u03c6(xi), \u03c6(xj))\n\u03c6(x) is a very coarse measurement of the chances of the policy induced by x resulting in stable walking movements over longer simulation periods. More importantly, points that lead to obviously unstable movements obtain a similar score of near zero, and are therefore grouped together. This kernel has no explicit information of the specific cost we are trying to optimize. It can very easily be used across multiple costs for walking behaviours, over slightly disturbed models as well as across multiple optimization methods."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we describe our experiments with the DoG kernel on two cost functions. First, we introduce our experimental settings and then move on to the results."}, {"heading": "A. Details of Experimental Setup: Cost Function and Algorithms Compared", "text": "To ensure that our approach can perform well across various cost functions, we conduct experiments on two different costs, constructed such that parameter sets achieving low cost also achieve stable and robust walking gaits. The first cost function varies smoothly over the parameter space:\ncost = 1\n1 + t +\n0.3\n1 + d + 0.01(s\u2212 stgt), (8)\nwhere t is seconds walked, d is the final hip position, s is mean speed and stgt is the desired walking speed (from human data). This cost encourages walking further and for longer through the first two terms, and penalizes deviating from the target speed with the last.\nThe second cost function is a slightly modified version of the cost used in [18] for experiments with CMA-ES. It penalizes policies that lead to falls in a non-smooth manner:\ncostCMA = { 300\u2212 xfall, if fall 100||vavg \u2212 vtgt||+ ctr, if walk\n(9)\nHere xfall is the distance travelled before falling, vavg is the average speed in simulation, vtgt is the target speed and ctr is the cost of transport. The first term directly penalizes policies that result in a fall, inversely to the distance walked. If the model walks for the simulation time, the cost is lower, ensured by the constants, and encourages policies that result in lower cost of transport and walk at target velocity. Since we have the same set of gains for left and right legs, the steadiness cost of the original cost [18] was unimportant.\nIn the following sections we compare the performance of several baseline and state-of-the-art optimization algorithms in simulation. Motivated by the discussion in [5], we include the baseline of uniform random search. While this search is uninformed and not sample-efficient, it could (perhaps surprisingly) serve as a competitive baseline in non-convex high-dimensional optimization problems. We also provide comparisons with CMA-ES [10] and Bayesian Optimization with a Euclidean kernel (basic BO). Since we were optimizing a non-convex function in a 16D space, it was not feasible to calculate the global minimum exactly. To estimate the global minimum for the costs we used, we ran CMA-ES (until convergence) and BO with our domain kernel (for 100 trials) for 50 runs without model disturbances on flat ground. When reporting results, we plot the best results found in this easier setting as the estimated optimum for comparison.\nFor experiments with Bayesian Optimization we explored using two libraries: MOE developed by Yelp [11] and Matlab implementation from [8]."}, {"heading": "B. Model Disturbances", "text": "Most real robots have poor dynamic models, as well as unmodeled disturbances, like friction, non-rigid dynamics, etc which make simulations a poor representation of the real robot. There has been a lot of work done in identifying dynamic models of robots reliably, for example in [1]. However, while such methods can definitely help bring simulators close to the real robot, there are other discrepancies like non-rigid dynamics and friction which are still very hard to model. As a result, often controllers that work well in simulation lead to poor performance on the real robot. In such cases, ideally, we would like to have optimization techniques that quickly adapt to this slightly different setting and find a new solution in a few cost function evaluations.\nTo test if our approach is capable of generalizing to unforeseen disturbances, modeling and environmental perturbations, we conduct our experiments on models with mass\nand inertial disturbances and on different ground profiles. We perturb the mass of each link, inertia and center of mass location randomly by up to 15% of the original value. For mass/inertia we randomly pick a variable from a uniform distribution between [\u22120.15, 0.15] \u00b7 M , where M is the original mass/inertia of the segment. Similarly we change the location of the center of mass by [\u22120.15, 0.15] \u00b7L/2, where L is the length of the link. These disturbances are different for each run of our algorithm, hence we test a wide range of possible modelling disturbances. For the ground profiles, we generate random ground height disturbances of upto \u00b18cm per step."}, {"heading": "C. Experiments with DoG Kernel", "text": "We pre-compute Determinants of Gait (DoG) kernel scores for 100,000 parameter sets, which takes 7-10 hours on a modern desktop to speed up our computations. These samples are generated using a Sobol sequence [2] on an undisturbed model on flat ground. Thereafter, the same kernel is used for all the experiments described below.\nIn experiments with Bayesian Optimization, we were directly able to replace the Euclidean distances of a squared exponential kernel with the distance in the DoG kernel space:\nkDoG(xi,xj) = exp ( \u2212 1\n2 \u2016\u03c6(xi)\u2212 \u03c6(xj)\u20162\n) (10)\nwith \u03c6(xi) as described in Section IV-B. We used a Matlab implementation of Bayesian Optimization from [8] and used a pre-sampled grid when considering next candidates for optimization. This allowed us to reuse our pre-computed scores to speed up kernel computations, but restricted us to only use these pre-sampled parameter sets. This can be harmful if an optimal set was not sampled; we sampled a dense grid to decrease the probability of this happening.\nOur DoG scores were obtained from an unperturbed model of our system on flat ground. Our experimental results, however, were obtained on settings with different ground profiles and model disturbances (as discussed in Section V-B). These perturbed settings were designed such that originally optimal set of policy parameters would likely become suboptimal. This is illustrated in the top and middle rows of Figure 4, where the policy performing well on flat ground falls on rough ground. This shows that our perturbations were indeed significant. After using the kernel for the optimization in these perturbed settings, we observed that best policies found were able to walk on rough ground (the lower part of Figure 4). This suggests that our kernel can be used to find optimum in settings significantly different than those it was created on.\nAll the experiments described below are done for 50 independent runs, each with a unique set of modeling disturbances and a different ground profile for rough ground walking. Each run consists of 100 trials or cost function evaluations, in which the optimization algorithm evaluates a parameter set for 100 seconds of simulation. Note that the disturbances and ground profiles remain constant across each run (and 100 trials).\n1) Experiments on the smooth cost function: Figure 5 shows results of our experiments using the DoG kernel on the smooth cost. For BO with DoG kernel, 25-30 cost function evaluations were sufficient to find points that corresponded to robot model walking on a randomly generated rough ground with \u00b18cm disturbance. This is in contrast to basic BO that did not find such results in under 100 trials.\nTo let CMA-ES also benefit from the kernel, we started each run from one of the best 100 points for the DoG kernel. After tuning the \u03c3 parameter of CMA-ES to make it exploit more around the starting point, we were able to find policies that resulted in walking on rough ground after 65-70 cost function evaluations on most runs. On the other hand, CMA-ES starting from a random initial point was not able to find walking policies in 100 evaluations.\nThese results suggest that DoG scores successfully captured useful information about the parameter space and were able to effectively focus BO and CMA-ES on the promising regions of the policy search space.\n2) Experiments with the non-smooth cost: We observed good performance on the non-smooth cost function (Figure 6), though it was not as remarkable as the smooth cost. BO with kernel still outperformed all other methods by a margin, but this different cost seems to hurt BO and CMA-ES alike. Since this cost is discontinuous, there is a huge discrepancy between costs for parameters that walk and those that don\u2019t. If no walking policies are sampled, BO learns little about the domain and samples randomly, which makes it difficult to find good parameters. Hence not all runs find a walking solution. BO was able to find successful walking in 74% of cases on rough ground with \u00b16cm disturbance in less than 60 trials/evaluations. CMA-ES starting from a good kernel point was able to do it in 40% of runs.\nThis showed that our kernel was indeed independent of the cost function to an extent, and worked well on two very different costs. We believe that the slightly worse performance on the second cost is because of the cost structure, rather than a kernel limitation, as it still finds walking solutions for a significant portion of runs.\n3) Experiments on different terrains: We also optimized on ramps \u2013 sloping upwards, as well as downwards. The ramp up and down ground slopes were gradually increased every 20m, until the maximum slope was reached. The\nmaximum slopes for going down and going up were 20% (tan(\u03b8) = 0.2). BO with DoG kernel was able to find parameters that walked for 100 seconds in 50% of cases in ramp up and 90% in ramp down. Example optimized policies walking up and down slope are shown in Figure 7.\nWe believe the reason we could not find walking policies on ramps in all runs, was that we are not optimizing the hip lean, which was noted to be crucial for this profile in [18]. Since we did not consider this variable when generating our 16 dimensional kernel, it was not trivial to optimize over it without re-generating the grid. Similarly, we found that we could not find any policies that climbed up stairs. Perhaps this could be achieved when optimizing over a much larger set of parameters, as in [18].\nIn the future, we would like to include more variables for optimizing over different terrains, and include them as part of the kernel."}, {"heading": "VI. CONCLUSIONS", "text": "In this work we focused on sample-efficiently finding walking policies for a bipedal neuromuscular model. This high dimensional optimization problem proved challenging for standard Bayesian Optimization. So, we introduced the Determinants of Gait (DoG) metric and constructed the corresponding DoG kernel to effectively incorporate domain knowledge into the kernel. For our experiments we precomputed the kernel on flat ground with an unperturbed model, and then tested in more challenging settings. We demonstrated that our approach offers improved performance for learning walking patterns on different ground profiles, like rough ground, ramp up and ramp down, all with various unknown inertial disturbances to the original model.\nOur results motivated us to consider several directions of future work. One of the next steps would be to experiment\nwith learning more parameters of the neuromuscular model. Adjusting more parameters would allow us to fine-tune walking behaviors for more challenging settings like stairs and steeper ramps. This would also make the problem more challenging because of the increase in the dimensionality of the search space. An informed kernel could provide robust performance by simplifying the search. We also would like to experiment with different ways of computing final DoG scores, perhaps by constructing a k-dimensional vector of individual metrics instead of collapsing them into a scalar score. And most importantly, we want to experiment with our approach on real hardware. We developed our experimental setup with future hardware experiments in mind, so we hope our approach would offer the needed sample efficiency to enable learning control policy parameters on the real hardware efficiently and adaptively."}], "references": [{"title": "Model-based Control of a Robot Manipulator", "author": ["An", "Chae H", "Atkeson", "Christopher G", "Hollerbach", "John M"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "Algorithm 659: Implementing Sobol\u2019s Quasirandom Sequence Generator", "author": ["Paul Bratley", "Bennett L Fox"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning", "author": ["Eric Brochu", "Vlad M Cora", "Nando De Freitas"], "venue": "arXiv preprint arXiv:1012.2599,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Bayesian Gait Optimization for Bipedal Locomotion", "author": ["Roberto Calandra", "Nakul Gopalan", "Andr\u00e9 Seyfarth", "Jan Peters", "Marc Peter Deisenroth"], "venue": "In Learning and Intelligent Optimization,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Bayesian Optimization for Learning Gaits Under Uncertainty", "author": ["Roberto Calandra", "Andr\u00e9 Seyfarth", "Jan Peters", "Marc Peter Deisenroth"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Generalized Biped Walking Control", "author": ["Stelian Coros", "Philippe Beaudoin", "Michiel van de Panne"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Robust Swing Leg Placement Under Large Disturbances", "author": ["Ruta Desai", "Hartmut Geyer"], "venue": "ROBIO", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Bayesian Optimization with Inequality Constraints", "author": ["Jacob R Gardner", "Matt J Kusner", "Zhixiang Eddie Xu", "Kilian Q Weinberger", "John Cunningham"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "A Muscle-reflex Model that Encodes Principles of Legged Mechanics Produces Human Walking Dynamics and Muscle Activities", "author": ["Hartmut Geyer", "Hugh Herr"], "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "The CMA Evolution Strategy: A Comparing Review", "author": ["Nikolaus Hansen"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Yelp Inc)", "author": ["Scott Clark"], "venue": "Introducing MOE: Metric Optimization Engine", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Near-optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies", "author": ["Andreas Krause", "Ajit Singh", "Carlos Guestrin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Automatic Gait Optimization with Gaussian Process Regression", "author": ["Daniel J Lizotte", "Tao Wang", "Michael H Bowling", "Dale Schuurmans"], "venue": "In IJCAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "A Bayesian Exploration-Exploitation Approach for Optimal Online Sensing and Planning with a Visually Guided Mobile Robot", "author": ["Ruben Martinez-Cantin", "Nando de Freitas", "Eric Brochu", "Jos\u00e9 Castellanos", "Arnaud Doucet"], "venue": "Autonomous Robots,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "The Mechanics of Muscle Function in Locomotion", "author": ["JB Morrison"], "venue": "Journal of Biomechanics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1970}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["Carl Edward Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "The Major Determinants in Normal and Pathological Gait", "author": ["J.B. dec. M. Saunders", "Verne T. Inman", "Howard D. Eberhart"], "venue": "The Journal of Bone & Joint Surgery,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1953}, {"title": "A Neural Circuitry that Emphasizes Spinal Feedback Generates Diverse Behaviours of Human Locomotion", "author": ["Seungmoon Song", "Hartmut Geyer"], "venue": "The Journal of Physiology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Toward Balance Recovery with Leg Prostheses Using Neuromuscular Model Control", "author": ["Nitish Thatte", "Hartmut Geyer"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Biped Gait Controller for Large Speed Variations, Combining Reflexes and a Central Pattern Generator in a Neuromuscular Model", "author": ["Nicolas Van der Noot", "Auke J Ijspeert", "Renaud Ronsse"], "venue": "ICRA", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning", "author": ["Aaron Wilson", "Alan Fern", "Prasad Tadepalli"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "EMG Profiles During Normal Human Walking: Stride-to-Stride and Inter-subject Variability", "author": ["DA Winter", "HJ Yack"], "venue": "Electroencephalography and Clinical Neurophysiology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1987}, {"title": "Simbicon: Simple Biped Locomotion Control", "author": ["KangKang Yin", "Kevin Loken", "Michiel van de Panne"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "One common way of learning controllers is to come up with a control policy parameterizations and a cost function [4], [23], [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 22, "context": "One common way of learning controllers is to come up with a control policy parameterizations and a cost function [4], [23], [6].", "startOffset": 118, "endOffset": 122}, {"referenceID": 5, "context": "One common way of learning controllers is to come up with a control policy parameterizations and a cost function [4], [23], [6].", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "In this work, we learn optimal reflex parameters for neuromuscular models described in [9].", "startOffset": 87, "endOffset": 90}, {"referenceID": 17, "context": "These models are human-inspired control pathways that are capable of producing locomotion behaviour in a variety of scenarios as demonstrated in [18].", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "Promising results were reported in prior work for optimizing an 8-dimensional control policy for a small bipedal robot using Bayesian Optimization [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 16, "context": "Our kernel uses gait characteristics described in [17] to create an appropriate similarity metric between different parameter sets of the neuromuscular model.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "function f(x), while evaluating f as few times as possible ([3] give an overview).", "startOffset": 60, "endOffset": 63}, {"referenceID": 15, "context": "Update equations for the posterior mean and covariance conditioned on evaluations can be found in [16] for both noisy and noiseless settings.", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "[12] developed an approach utilizing Gaussian Processes and the principle of optimizing", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] used BO for online path planning Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] used a closely related approach of Gaussian Process Regression to optimize the gait on a quadruped robot and showed that this approach required substantially fewer evaluations than state-of-the-art local gradient approaches.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "used BO to efficiently find gait parameters that optimize a desired cost [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 17, "context": "Hence two questions arise : would BO be effective if the dimensionality is increased from 8 to 16? And if it does, how does it compare to previously used approaches, like CMA-ES [18]?", "startOffset": 178, "endOffset": 182}, {"referenceID": 8, "context": "We use neuromuscular model policies, as introduced in [9], as our controller for a 7-link planar human-like model.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "[7] designed reflex laws for swing that enabled target foot-placement and leg clearance, by analyzing the double pendulum dynamics of the human leg.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "the model to overcome disturbances in the range of up to \u00b110 cm [18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "is actuated by 7 Hill-type muscles [15], consisting of the soleus (SOL), gastrocnemius (GAS), vastus (VAS), hamstring (HAM), tibialis anterior (TA), hip flexors (HFL)", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "The details of these feedback pathways can be found in [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "between the stance leg the CoM, and presented in [23]:", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "Details of this control can be found in [7].", "startOffset": 40, "endOffset": 43}, {"referenceID": 17, "context": "As was noted in [18], and observed in our experiments, the control is relatively insensitive to the individual gains of the", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "More concisely, the swing parameters that we focus on in our optimization are the following: 1) K p : Position gain on feedback on leg angle 2) K d : Velocity gain on feedback on leg velocity 3) \u03b10 : Nominal leg angle 4) Cd : Gain on the horizontal distance between the stance foot and CoM 5) Cv : Gain on the horizontal velocity of the CoM 6) lclr : Desired leg clearance Though originally developed for explaining human neural control pathways, these controllers have recently been applied to robots and prosthetics, for example in [19] and [20].", "startOffset": 534, "endOffset": 538}, {"referenceID": 19, "context": "More concisely, the swing parameters that we focus on in our optimization are the following: 1) K p : Position gain on feedback on leg angle 2) K d : Velocity gain on feedback on leg velocity 3) \u03b10 : Nominal leg angle 4) Cd : Gain on the horizontal distance between the stance foot and CoM 5) Cv : Gain on the horizontal velocity of the CoM 6) lclr : Desired leg clearance Though originally developed for explaining human neural control pathways, these controllers have recently been applied to robots and prosthetics, for example in [19] and [20].", "startOffset": 543, "endOffset": 547}, {"referenceID": 17, "context": "As demonstrated in [18], these models are indeed capable of generating a variety of locomotion behaviours for a humanoid model - for example, walking on flat, rough ground, turning, running, walking upstairs and on ramps.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "Typically, this is done using Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [10], an evolutionary algorithm for difficult non-linear non-convex blackbox optimization problems.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "An optimization for 16 neuromuscular parameters takes 400 generations, around a day on a standard i7 processor and about 5,000 trials, as reported in [18].", "startOffset": 150, "endOffset": 154}, {"referenceID": 20, "context": "The Behavior-Based Kernel (BBK) of [21] is one kernel that leverages structure in the trajectories generated by the evaluated policies.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "Bipedal walking can be characterized with some basic metrics, called gait determinants, as described in [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "Here, \u03b8 knee is the knee joint angle in swing, \u03b8 thr high and \u03b8 low are the high and low thresholds on knee angle, similar to human data as described in [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "3m/s [22].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "The second cost function is a slightly modified version of the cost used in [18] for experiments with CMA-ES.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Since we have the same set of gains for left and right legs, the steadiness cost of the original cost [18] was unimportant.", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "Motivated by the discussion in [5], we include the baseline of uniform random search.", "startOffset": 31, "endOffset": 34}, {"referenceID": 9, "context": "We also provide comparisons with CMA-ES [10] and Bayesian Optimization with a Euclidean kernel (basic BO).", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "For experiments with Bayesian Optimization we explored using two libraries: MOE developed by Yelp [11] and Matlab implementation from [8].", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": "For experiments with Bayesian Optimization we explored using two libraries: MOE developed by Yelp [11] and Matlab implementation from [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "There has been a lot of work done in identifying dynamic models of robots reliably, for example in [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "These samples are generated using a Sobol sequence [2] on an undisturbed model on flat ground.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "We used a Matlab implementation of Bayesian Optimization from [8] and used a pre-sampled grid when considering next candidates for optimization.", "startOffset": 62, "endOffset": 65}, {"referenceID": 17, "context": "We believe the reason we could not find walking policies on ramps in all runs, was that we are not optimizing the hip lean, which was noted to be crucial for this profile in [18].", "startOffset": 174, "endOffset": 178}, {"referenceID": 17, "context": "Perhaps this could be achieved when optimizing over a much larger set of parameters, as in [18].", "startOffset": 91, "endOffset": 95}], "year": 2016, "abstractText": "Learning policies for bipedal locomotion can be difficult, as experiments are expensive and simulation does not usually transfer well to hardware. To counter this, we need algorithms that are sample efficient and inherently safe. Bayesian Optimization is a powerful sample-efficient tool for optimizing non-convex black-box functions. However, its performance can degrade in higher dimensions. We develop a distance metric for bipedal locomotion that enhances the sample-efficiency of Bayesian Optimization and use it to train a 16 dimensional neuromuscular model for planar walking. This distance metric reflects some basic gait features of healthy walking and helps us quickly eliminate a majority of unstable controllers. With our approach we can learn policies for walking in less than 100 trials for a range of challenging settings. In simulation, we show results on two different costs and on various terrains including rough ground and ramps, sloping upwards and downwards. We also perturb our models with unknown inertial disturbances analogous with differences between simulation and hardware. These results are promising, as they indicate that this method can potentially be used to learn control policies on hardware.", "creator": "LaTeX with hyperref package"}}}