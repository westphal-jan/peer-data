{"id": "1610.06251", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "DeepGraph: Graph Structure Predicts Network Growth", "abstract": "The topological (or graph) structures of real-world networks are known to be predictive of multiple dynamic properties of the networks. Conventionally, a graph structure is represented using an adjacency matrix or a set of hand-crafted structural features. These representations either fail to highlight local and global properties of the graph or suffer from a severe loss of structural information. There lacks an effective graph representation, which hinges the realization of the predictive power of network structures.", "histories": [["v1", "Thu, 20 Oct 2016 00:16:05 GMT  (2925kb,D)", "http://arxiv.org/abs/1610.06251v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["cheng li", "xiaoxiao guo", "qiaozhu mei"], "accepted": false, "id": "1610.06251"}, "pdf": {"name": "1610.06251.pdf", "metadata": {"source": "CRF", "title": "DeepGraph: Graph Structure Predicts Network Growth", "authors": ["Cheng Li", "Xiaoxiao Guo", "Qiaozhu Mei"], "emails": ["qmei}@umich.edu"], "sections": [{"heading": null, "text": "In this study, we propose to learn the represention of a graph, or the topological structure of a network, through a deep learning model. This end-to-end prediction model, named DeepGraph, takes the input of the raw adjacency matrix of a real-world network and outputs a prediction of the growth of the network. The adjacency matrix is first represented using a graph descriptor based on the heat kernel signature, which is then passed through a multicolumn, multi-resolution convolutional neural network. Extensive experiments on five large collections of real-world networks demonstrate that the proposed prediction model significantly improves the effectiveness of existing methods, including linear or nonlinear regressors that use hand-crafted features, graph kernels, and competing deep learning methods.\nCategories and Subject Descriptors H.4 [Information Systems Applications]: Miscellaneous\nGeneral Terms Algorithms, Experimentation\nKeywords Graph Representation, Network Growth, Deep Learning"}, {"heading": "1. INTRODUCTION", "text": "Today we are surrounded by real-world networks of people, information, and technology. These heterogeneous, large scale, and\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nfast evolving networks have provided a new perspective of scientific research, which has resulted in a rapid development of new theories, algorithms, and applications.\nHow to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8]. These properties may include the size of the network or a subgraph (e.g., size of a community), the influence of cascades or contagions in the network (e.g., number of adopters), metrics of individual nodes or structures (e.g., degree or diameter), or even external properties that are not directly observed from the network structure (e.g., prestige, productivity or revenue of a node or a community). All these properties change over time, and their dynamics can be generally referred to as the growth of a network1. Indeed, the prestige of an individual node grows with the size of its ego-network. The influence range of a diffusion grows with the size of the diffusion network, subgraph of people who have adopted the diffusion. Accurate prediction of network growth has many valuable applications. For example, predicting the growth of research communities helps scientists to identify promising research directions; predicting the growth of social groups helps social network vendors optimize their marketing strategies; predicting the growth of the diffusion of a rumor helps analysts to estimate its potential damage and apply intervention in time.\nTaking a typical data mining perspective, most existing methods extract features from both the network itself and any external information sources available. A function is learned that takes these features as input and outputs a predicted value of the network property in the future [1]. From many explorations on different genres of networks, there has been a consensus in literature that features extracted from the topological structure of the network (a.k.a., the graph) are generally very informative in these prediction tasks [1, 8]. As a comparison, other types of information, e.g., content or demographics, are only useful in certain scenarios. For example, the content of a hashtag is predictive to its diffusion [43] and homophily (e.g., similar demongraphics) is predictive to the growth of social groups [7], but these effects are not generalizable to other networks and other dynamic properties. In this study, we focus on investigating the predictive power of the graph structure of a network on its growth.\nExisting structural features are typically hand-crafted based on theoretical and empirical findings in the social network literature. For example, open triads with two strong ties are likely to be closed in the near future [11]; dense communities are resistant to novel information and they grow slower than others [16]; nodes spanning structural holes are likely to gain social capital and experience a rapid growth of its prestige and other properties [6]. Features\n1The growth refers to both the increment and decrement of the dynamic properties of the networks, i.e., positive or negative growth.\nar X\niv :1\n61 0.\n06 25\n1v 1\n[ cs\n.S I]\n2 0\nO ct\n2 01\n6\nsuch as network density, clustering coefficients, triadic profiles, and structural holes are therefore designed to implement these intuitions and represent the graph structure.\nDespite the success in predicting network growth, there are observable issues of representing the topological structure of a network using these hand-crafted features. Some of them only describe a global property of the network, such as network density or degree distribution; some of them provide a fine-grained description of local structures but fail to capture global information, such as triads and other substructures; others lie between the two extremes, such as structural holes. None of these features is able to fully represent both the local and the global structure of a graph and the complex interaction between local and global properties. On the other hand, these heuristic features usually have a limited characterization power of networks, as many networks may share the same feature representation. For example, most real-world networks at scale may have a similar (power-law) degree distribution, and two very different networks may happen to have the same ratio of closed triangles. Taking a machine-learning point of view, we are intrigued by the following questions: what is a suitable representation of network structure and how effective is such a representation when used to predict network growth?\nOur answers to the two questions are inspired by the recent developments in deep learning and graph representation. We introduce a graph descriptor that is based on the Heat Kernel Signature (HKS) [31], which serves as a universal low-level representation of the topological structures of networks. HKS has been successfully employed in representing the surface of 3D objects [13, 39]. By modeling the amount of heat flow over nodes of a network over time, HKS successfully stores both the global and the local structural information of the entire network. Using a histogram to describe the probability distribution of heat values at a series of time points [13, 39], isomorphic networks (networks with the same topological structure) can be mapped to a unique representation at little loss of structural information. However, unlike 3D objects which are composed of polygon meshes, the structures of networks vary in shape, size, and complex local structures. To address this issue, some computations of HKS need to be approximated carefully. Inspired by the semantics of the HKS-based graph descriptors, we propose a multicolumn, multiresolution neural network that learns latent hierarchical representations of graphs on top of the HKSbased graph descriptor. The proposed deep neural network, named DeepGraph, predicts network growth in an end-to-end process.\nWe conduct extensive experiments to evaluate the effectiveness of DeepGraph. Different growing properties are predicted for five genres of real-world networks, including cascade networks and egonetworks. Empirical results show that our method outperforms baseline approaches that use alternative graph representations, handcrafted features, or existing deep learning architectures. High-level representations learned by DeepGraph well connect to existing findings in the social network literature.\nThe rest of the paper starts with Section 2, which summarizes the related literature. In Section 3, we formulate the data mining problem and describe how HKS is used to represent the network structure and how the deep neural network is architected. We present the design and the results of empirical experiments in Section 4 and Section 5, and then conclude the paper in Section 6."}, {"heading": "2. RELATED WORK", "text": "Predicting the growth of networks or the evolution of certain properties of networks has been widely studied. People attempt to predict the dynamics of various network metrics or aggregated activities in a network, e.g., the number of up-votes on Digg sto-\nries [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8]. In these studies, a set of problem-specific features are usually manually designed based on the network structure, textual content, user demographics, historical statistics, and other sources of information. Among them, the features extracted from the network structure are both effective in individual tasks and robust across different tasks. In this work, we limit our focus on information purely from the network structure.\nFinding a suitable representation of the topological structure of a network has always been a critical preliminary step of network analysis. Conventionally, a network is represented as an adjacency matrix or a sparse list of edges. However, these lossless representations do not effectively present the structural characteristics of the network. Moreover, they are sensitive to the manipulation of node orders, making networks with the same topological structure mapped to different representations. Other approaches represent the network structure with a series of network metrics and/or a set of structural patterns (e.g., triads [19], quads [37], or metapaths [32]). Arbitrary higher-order substructures can be included, such as communities and structural holes. These bag-of-substructures better capture local patterns of the network structure.The major problem of this approach is that it is computationally infeasible to enumerate high-order substructures, and low-level substructures have limited representation power of the global structure of the network. As a result, many different networks may share the same or similar bag-of-substructures.\nIn graph classification, a myriad of graph kernel methods are proposed which compute pairwise similarities between graphs [17, 2, 30, 29]. For example, graphlets [29, 36] computes the graph similarity based on the distribution of induced, non-isomorphic subgraphs. Some other graph kernels integrate frequent graph mining into the model training process [28, 26]. Graph kernels provide an indirect representation of networks so that similar structured networks yield a high value through the graph kernel function. The burden of graph kernels is the design of effective kernels. In the paper, we compare existing graph kernels to highlight the flexibility of our model.\nRecently, researchers have started to apply deep learning to network structure representation learning. Several proposals have been made to learn a low-dimensional vector representation of individual nodes by considering their neighborhood [34, 25, 15]. Deep learning techniques have also improved graph kernels for graph structure learning[40, 41, 23]. Recently, Niepert et al. [24] applied convolution over receptive fields constructed by sequence of neighboring nodes. These methods focus only on the local structure of a graph and graph kernels require expensive pairwise comparisons. In the paper, we compare our model to these alternative deep learning approaches and show the performance advantage of our model.\nHeat kernels have been studied for the task of graph clustering [3], graph partitioning [12], and modeling social network marketing processes [21]. These applications rely on the raw output of heat kernels for a variety of tasks, rather than developing a signature, nor do they abstract representation of graphs base on heat kernels. In the community of computer vision, Heat kernel signature has been successfully used to model 3D objects [31, 13, 39], whose surfaces are defined by polygon meshes, a network composed of simple convex polygons. In contrast, real-world networks are consist of various shapes, sizes, and local structures. How to represent arbitary networks with heat kernel signatures and how to predict network growth using such a signature remain a challenging question to be studied."}, {"heading": "3. DEEPGRAPH FOR NETWORK GROWTH PREDICTION", "text": "We propose a unified predictive neural network model to learn graph structure representation for network growth prediction problem. The proposed predictive model, named DeepGraph, combines heat kernel signature and deep neural networks. Below we describe the two key components of our model, (1) a heat kernel signature based graph descriptor and (2) a deep multi-column, multiresolution convolutional neural network, in turn, following a brief definition of the network growth prediction problem."}, {"heading": "3.1 Problem Formulation and Notations", "text": "Given a real-world network snapshot at time t, denote its graph structure as G(t) = (V,E), with a set of nodes V and a set of edges E. A node i \u2208 V represents an entity (e.g., an actor in a social network or a paper in a citation network), an edge (i, j) \u2208 E represents a relationship (e.g., friendship, citation, or influence) between node i and node j. An adjacency matrix W \u2208 R|V |\u00d7|V | encodes the topological structure of the graph G. In this work, we consider the binary adjacency matrix. Its element wij is 1 if and only if (i, j) \u2208 E and 0 otherwise.\nA network property is a function that maps a graph structure G(t) to a property value y(t) \u2208 R. For example, a network property could be the number of friends given a user\u2019s Facebook egonetwork. A network growth predictor is a function that maps a graph structure G(t) to a property value y(t\n\u2032) at time t\u2032, satisfying t\u2032 > t. For example, a network growth predictor could map a user\u2019s Facebook ego-network of this year to the number of friends next year.\nThe network growth prediction can be naturally formulated as a supervised learning problem. Specifically, the problem is to derive a network growth predictor f given a training set of tuples {(G(ti)i , y (t\u2032i) i )} M i=1 to minimize the prediction error over a test set of tuples {(G(tj)j , y (t\u2032j) j )} N j=1 satisfying \u2200i t\u2032i > ti, \u2200j t\u2032j > tj , minj(tj) >maxi(ti), and minj(t\u2032j) >maxi(t \u2032 i). The time ordering constraints highlights the practical motivation that we are interested in using historical data to predict future properties of current networks.2 To apply a machine learning algorithm, it is critical to first represent the graph G(t) computationally, such as using a vector of features.\nNotice that such a representation can consider different sources of information when a particular type of real-world network is considered: the graph structure, the profiles of the nodes, the semantics of the edges, the activities of the nodes, and the information content being circled within the network. We focus our investigation onto solely the graph (topological) structure, as the structural information is ubiquitously available in all types of networks and it is known to be predictive to the growth of the network. We leave a multi-modal representation of the network for future work."}, {"heading": "3.2 Heat Kernel Signature based Graph Descriptors", "text": "The motivation in adopting Heat Kernel Signature (HKS) is its theoretical proven properties in representing graphs: HKS is an intrinsic and informative representation for graphs [31]. Intrinsicness means that isomorphic graphs map to the same HKS representation, and informativeness means if two graphs have the same HKS representation, then they must be isomorphic graphs. Our\n2In practice, researchers focus on a specially case of the network growth prediction problem with the equal interval increment constraint, t\u2032j \u2212 tj = t\u2032i \u2212 ti = C > 0 [20, 35].\nHKS-based graph descriptor builds on the theoretical properties of HKS and further provides universal representations for graph with different sizes in network growth prediction.\nHeat kernel function. Formally, the heat kernel hz(i, j), a function of two nodes i, j at any given diffusion step z, denotes the amount of aggregated heat flow through all edges among two nodes after diffusion step z3. In computer vision, graphs are stored as meshed networks and heat kernels are computed by finding eigenfunctions of the Laplace-Beltrami operator [31]. However, meshed networks are not available for most real-world networks. Instead, we use eigenfunction expansion of a graph Laplacian [31, 3] to compute the heat kernel for information networks. Given a graph G = (V,E,W ), the graph Laplacian is defined as:\nL = D\u2212W (1)\nwhere D is a diagonal degree matrix with diagonal entries being the summation of rows of W : Dii = \u2211 j wij . The normalized Laplacian of the graph is given by\nLN = D \u2212 1 2LD\u2212 1 2 (2)\nThe heat kernel is then defined as\nhz(i, j) = |V |\u2211 k=1 e\u2212\u03bbkz\u03c6k(i)\u03c6k(j) (3)\nwhere \u03bbk is the k-th eigenvalue of the normalized Laplacian LN and \u03c6k is the k-th eigenfunction s.t. \u2211 i |\u03c6k(i)|\n2 = 1. Note that the eigenvalues might be unreal in the case of directed graphs. There has been studies on how to tackle this problem [9]. In this work, for simplicity, we convert directed graphs to undirected ones by applying W = (W +W\u1d40)/2.\nHeat kernel signature. Heat kernel signature was introduced to mitigate the computation bottleneck of using heat kernel functions in representing graphs. Both heat kernel and heat kernel signature are proven to be intrinsic and stable against noises. However, the computation complexity of using heat kernel as a point signature is overwhelming since the point signature, {kt(v, .)}t>0 , is defined on the product of temporal and spatial domain. Heat kernel signature simplifies the computation by considering only a subset of product of temporal and spatial domain while keeping as much information as possible. Specifically, heat kernel signature reduces the computation complexity by only requiring hz(v, v) over a finite set of N diffusion steps z \u2208 {z1, z2, ..., zN} for \u2200v \u2208 V without losing the intrinsic and informative properties.\nFormally, a heat kernel signature (HKS) is a matrix H \u2208 R|V |\u00d7N satisfying\nHij = hzj (i, i) (4)\nThese time points are sampled with equal difference after logarithm [31], such that log zn \u2212 log zn\u22121 = log zn+1 \u2212 log zn.\nGraph descriptor. The practical issues in combining HKS and deep neural networks are that we need a global vertex indexing to guarantee the uniqueness and that the size depends on |V |. We further process heat kernel signature H into a universal representation independent of |V | using a histogram conversion. Specifically, 3The diffusion is simulated for a given graph snapshot. The heat kernel computation does not require graph snapshot at other timestamps. The diffusion step z should not be confused with the network timestamp t.\nwe use histograms to estimate the distribution of HKS values in each column4. By denoting NB the number of bins used in the histogram, we obtain a universal descriptor S \u2208 RNB\u00d7N . Unlike HKS, the new descriptor is independent of vertex ordering and vertex number. We call this final matrix graph descriptor, S(G), as it is adapted to describe information networks. Figure 1 shows four examples of our graph descriptors for real world graph structures.\nGraph descriptor vs. adjacency matrix. We have described the process in converting an adjacency matrix into our graph descriptor, which is then passed through a deep neural network for further feature extraction. All computation in this process is to obtain a more effective low-level representation of the topological structure information than the original adjacency matrix.\nFirst, isometric graphs could be represented by many different adjacency matrices, while our graph descriptor would provide a unique representation for those isometric graphs. The unique representation simplifies the neural network structures for network growth prediction.\nSecond, our graph descriptor provides similar representations for graphs with similar structures. The similarity of graphs is less preserved in adjacency matrix representation. Such information loss could cause great burden for deep neural networks in growth prediction tasks.\nThird, our graph descriptor is a universal graph structure representation which does not depend on vertex ordering or the number of vertexes, while the adjacency matrix is not.\nTime complexity. The major overhead of computing graph descriptors lies in the calculation of eigenvectors. The time complexity of computing eigenvectors is O(K|V |2) where K is the number of eigenvectors. Our graph descriptors finish in acceptable time frame for real world network data. The data description and time complexity analysis are in Section 4.\nSemantics of graph descriptor. The rows and columns in our graph descriptor reflect the network topology from different perspectives. The rows express the heat density dynamics over diffusion steps, and the columns capture the static heat density patterns for a given diffusion step. Successive rows or columns express higher-order properties of the topology structure information. Such representational properties motivate the adoption of row-wise and column-wise convolution networks for feature learning.\n4The bin ranges are aligned column-wise on the training data."}, {"heading": "3.3 Deep Graph Descriptor", "text": "As information abounds in the raw representation extracted by the HKS-based graph descriptor, applying a simple regressor, e.g., linear regression, could fail to fully extract useful information from it. In contrast, deep neural networks (DNN) have achieved tremendous success in learning latent representations from raw inputs in a compositional hierarchy. Combining DNN and HKS-based graph descriptor together thus offers an opportunity to address the graph structure representation challenges in predicting network growth. Inspired by the semantics of the graph descriptors, we propose a deep multicolumn, multiresolution convolutional neural networks for the network growth prediction task.\nMultiresolution convolutions. Our model builds on the multiresolution 1-D convolution (MrConv) which maps an input matrix into a feature map matrix. Specifically, let xi \u2208 Rk denote the i-th row of the input matrix. The input is then represented as x1:n = \u2295ni=1xi where \u2295 is the concatenation operator and n is the number of rows. The 1-D convolution with a filter sizem apply a filter w \u2208 Rmk to each possible window of m rows to produce a new feature vector c = [c1, c2, ..., cn\u2212m+1]. The feature ci is generated from a window of m rows xi:i+m\u22121 by:\nci = g(w \u1d40xi:i+m\u22121 + b) (5)\nwhere b \u2208 R is a bias term and g is a non-linear function such as a hyperbolic tangent function or a rectified non-linearity function.\nWe have described the process by which one feature vector c is extracted from one filter. Our multiresolution convolution (MrConv) layer uses multiple filters with varying filter sizes to obtain multiple resolution features. Specifically, one MrConv layer has l different convolution filter sizes {m1,m2, ...,ml}. The filter of sizem generates a corresponding feature vector c(m). Feature vectors generated by different filter sizes are then concatenated into one vector c\u2217 = \u2295li=1c(mi). Moreover, we extend each filter size to have d different filters. The final output feature map is a matrix O where each column is a feature vector c\u2217 and there are d columns.\nO = ( c\u22171, c\u22172, ..., c\u2217d ) (6)\nAn example of our MrConv is shown in Figure 2(a). The example MrConv layer has two different filter sizes {1, 2}. Each filter size has three different filters, whose feature vectors form different columns in the final feature map. Multiple multiresolution convolution layers are stacked to form our model.\nMulticolumn model. Inspired by the different semantics of rows and columns in the HKS-based graph descriptor, our model deploys a two network-column structure, as shown in Figure 2(b). One column uses multiresolution 1-D convolution (MrConv) operations over the graph descriptor bins and the other one uses MrConv over diffusion times. The two columns extract different features from the graph descriptors at multiple resolution scales. Intuitively, the first column extracts statistical features of the density dynamics in diffusion. The second column extracts features on static density pattern for different diffusion steps. Both kinds of features reflect the topology of the underlying graph structure, but explain the structure topology from different perspectives. A single column convolutional neural network can hardly extract such two kinds of features successfully.\nThe feature maps from the two columns are then concatenated and passed through multiple dense (i.e. fully-connected) layers with non-linear activation functions. The output from the multiple\ndense layers are then passed through a final linear fully-connected layer with only one output unit. The output unit y\u0302 is thus the network growth prediction of our model."}, {"heading": "3.4 End-to-End Training", "text": "Let McMrConv(., \u03b8) denote the multicolumn multiresolution convolutional neural network with parameters \u03b8. The final output of our neural network given a graph Gk is represented as:\ny\u0302k = McMrConv(S(Gk), \u03b8) (7)\nGiven a training data set {(Gk, yk)}Kk=1, the deep neural network is trained to minimize the average squared error.\nL(\u03b8) = 1 K K\u2211 k=1 ( McMrConv(S(Gk), \u03b8)\u2212 yk )2 (8)\nThe HKS-based graph descriptor and the deep neural network assembles DeepGraph, an end-to-end deep architecture to predict network growth based on graph structure."}, {"heading": "4. EXPERIMENT SETUP", "text": "We compare our model with existing approaches, including handcrated feature based linear or nonlinear regression, graph kernels and alternative deep learning approaches, on the network growth prediction problem. We then evaluate variants of our model to assign the credit of the two key components, the HKS-based graph descriptor and the deep neural network."}, {"heading": "4.1 Data sets", "text": "When selecting real-world data sets for evaluation, we consider both popularity and diversity of the application scenarios. The five data sets we choose include social networks, scientific collaboration networks, information diffusion networks, and entertainment networks. Note that to train and test the network growth prediction algorithm, it is desirable to obtain a large number of time-variant networks, which is not directly available (e.g., there is one global Facebook network). So following [40], from each large network we extract the ego-networks (subgraph consisting of the neighbors of an \u201cego\u201d node) of many individual nodes. The statistics of these data sets are presented in Table 1. Please note that due to the diverse nature of the data sets and the various precision of timestamps available, it is hard to apply an unified time frame for all data sets.\nViewed from another perspective, this helps us evaluate the flexibility and generality of our methods, verifying whether it can be applied to any length and granularity of time frames.\nWe follow the procedure described in [40] to construct ego-nets. The Facebook data set is collected from the New Orleans networks [38], where nodes are Facebook users and edges are friendships. We derive the snapshot of ego-networks for each user according to the timestamps listed in Table 1, which is used to predict the number of new friends this user made in the next four months.\nAs the YouTube [22] data set also describes user friendships, it\nfollows the same setting as Facebook. The AAN data set [10] is built upon scientific publications from the ACL Anthology5, where nodes are authors and edges are collaboration. Each author\u2019s ego-nets are extracted to predict her hindex in the next year.\nIMDB is a movie co-star data set6, where nodes are actors or actresses, and an edge is formed if they appear in the same movie. The ego-nets of each actor/actress is used to predict the number of new movies the actor/actress produced in the next year.\nThe Weibo data set [44] contains a set of Sina Weibo users with their complete following-followee relationships, as well as 300,000 retweeting paths among these users. For an original tweet we construct its diffusion network, which is the subgraph (of the followingfollowee graph) with users who retweeted the tweet within the first 5 hours of the original tweet. We then predict the growth of this diffusion network in the next 3 hours (i.e., the number of new users who retweet this tweet). For simplicity, we ignore the direction of the edges and treat all graphs as undirected.\nTo examine whether we can truly predict future growth, we make sure of two important points: (1) the period to compute growth for test set is always later than that for training set; (2) one graph can only appear in one of the training, validation and test set. For Weibo data set, we sort all diffusion graphs by time of occurrence, and take the earliest 80 percent graphs for training, the next 5 percent for validation, and the last 15 percent for testing. For other data sets, for each node in the global network, they are randomly assigned to the training/validation/test set with probability of 0.8/0.05/0.15. Based on which set they are in, their ego-nets and growth are computed according to the time listed in Table 1. If a node has not yet been created for the given time, it is simply removed.\nWe notice that the growth of all the networks in general follows a power-law distribution, where a large number of networks did not grow at all. Therefore we downsampled 50% graphs of each train/val/test set with zero growth (to the numbers shown in Table 1) and applied a logarithm transformation of the outcome variable (network growth), following [20, 35]. The network growth are scaled logarithmically for two reasons. First, baseline methods with linear regression are sensitive to extremely large outcomes. Second, when a network grows to a considerably large scale, we care more about its scale rather than the exact number.\nPlease note that our method is not limited to ego-networks \u2013 data set Weibo is a set of diffusion graphs of tweets."}, {"heading": "4.2 Evaluation Metric", "text": "We use mean squared error (MSE) as our evaluation metric, which is a common choice for regression tasks. Specifically, denote y\u0302 a prediction value, and y the ground truth value, the MSE is:\nMSE = 1\nn n\u2211 i=1 (y\u0302i \u2212 yi)2 (9)\nAs noted before, y in above equation is a scaled version of the original value yo, that is y = log2(y o + 1)."}, {"heading": "4.3 Baseline methods", "text": "We compare DeepGraph with methods from two categories: featurebased methods used for network prediction tasks, and alternative graph representation methods.\nFeature based. Many structural features have been designed for various network prediction tasks [1, 27, 35, 8]. We select from them those that could be generalized across data sets, including: 5http://aclweb.org/anthology/ 6http://www.imdb.com/\nFrequencies of k-node substructures (k \u2264 4)[37]. This counts the number of nodes (k = 1), edges (k = 2), triads (e.g., the number of closed and open triangles) and quads.\nOther network properties: average degree, the length of the shortest path, edge density, the number of leaf nodes (nodes with degree 1), the number of leaf edges, the average closeness of all nodes, clustering coefficient, diameter, and the number of communities obtained by a community detection algorithm [4].\nGraph kernels. Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30]. In our experiment, the RW kernel does not finish after 10 days for a single data set, so we exclude it for comparison. This exclusion is also observed for the same reason in [24, 41].\n-linear and -deep. Feature based methods and graph kernels are usually trained on SVMs. We report linear regression instead, as SVM empirically generates poor results for our regression tasks. We append -linear to each method to indicate usage of linear regression. To obtain even stronger baselines, we apply deep learning to both feature vectors and graph kernels, indicated by -deep.\nSmoothed graph kernels. Yanardag et al. [41] apply smoothing to graph kernels, which extends their method of deep graph kernels [40] by considering structural similarity between sub-structures. We report smoothed results only on deep neural networks as it outperforms alternatives empirically.\nPSCN, which applies convolutional neural networks (CNN) to locally connected regions from graphs [24], achieving better results over graph kernels on some of the classification data sets.\nHyper-parameters. All hyper-parameters are tuned to obtain the best results on validation set. For linear regression, we chose the L2-coefficient from {100, 10\u22121, ..., 10\u22127}. For neural network regression, the initial learning rate is selected from {0.1, 0.05, 0.01, ..., 10\u22124}, the number of hidden layers from {1, 2, ..., 4}, and the hidden layer size from {32, 64, ..., 1024}. The size of the graphlets for GK is chosen from {3, 4} (higher than 4 is extremely slow), the height parameter of WL from {2, 3, 4}, the discount parameter for smoothed graph kernels from {1, 0.8, ..., 0}. Following [24] for PSCN, the width is set to the average number of nodes, and the receptive field size is chosen between 5 and 10.\nNotes. Please notice that in our experiments we are not identifying the nodes in the networks or using the information of the nodes outside the network itself. Of course, knowing the president of United States is in the network provides more confidence on its growth. We choose not to identify nodes because (1) this study focuses on investigating the predictive power of the topological structure of networks, and (2) in practice information about individual nodes may not be available for privacy reasons. For the same reasons, we do not include any information other than the network structure (e.g., content of tweets, or historical metrics of the network) in the prediction task, even though including more information may improve the prediction accuracy.\nIt is also worth mentioning that even though most networks in our data sets are subgraphs of a much larger network (e.g., the Facebook friendship network), we only use the structure within the subgraphs and do not touch the outside structure of the global network. This is because partitioning a large network into subgraphs is just a way to create abundant networks for training and testing the model. In reality we may make predictions of an entire network, or the global structure outside a network may not be observable.\n4.4 DeepGraph Model Parameters\nHyper-parameters and Preprocessing. For parameters included in HKS, we set them to default values across all data sets without further tuning. In Equation 4, we set t1 = 0.1, tN = 25, and N = 64. Number of bins NB is set to 64. In order to compute histograms, HKS values above +1.2 and below \u22121.2 standard deviation are respectively put to the first and last bins. Values in between are assigned to the remaining equally divided 62 bins.\nWe perform standard normalization for the histograms of graphs. Each histogram is preprocessed by pixel-wise normalization. We compute the mean and standard deviation for each pixel over the training data set. Then each pixel is normalized by subtracting the corresponding mean value and being divided by sd7 .\nWe initialize the parameters of the neural networks using a Gaussian distribution with zero mean and unit standard deviation. An adaptive optimizer, Adam, is used to optimize the parameters of the neural networks. Default hyper-parameters of Adam are used [18].\nStructure related hyper-parameters of DeepGraph is set to be the same across datasets. There are two multiresolution convolution layers for each network column, with number of filters 32 and 16. For each convolution layer, we apply three sizes of filters, which are 2, 4, and 6. TanH is used as the activation function. There are two fully connected layers both of size 256. Dropout is applied to the last two dense layers with probability of 0.5. Other learning parameters are listed in Table 2."}, {"heading": "4.5 Variants of DeepGraph", "text": "To assign the credit of each key component in our DeepGraph model, we also experiment with some of its variants, by feeding our graph descriptor (GD) to a linear regressor (GD-linear), a standard convolutional neural network (GD-CNN), and a multilayer perceptron (GD-MLP). Hyper-parameters for these models are tuned similarly as baselines."}, {"heading": "5. EXPERIMENT RESULTS", "text": ""}, {"heading": "5.1 Overall performance", "text": "The overall performance of all competing methods across data sets are displayed in 3. We make the following observations. First, integrating graph descriptor with deep learning, our method DeepGraph outperforms all competing methods significantly. This empirically confirms that graph descriptor could preserve more information of the network structure than bag-of-substructures, both globally and locally. In contrast, utilizing manually designed features could lead to loss of information.\nGD-MLP and GD-CNN have already gain improvement over the strongest baseline on most of the data sets, while DeepGraph can further improve the performance by utilizing the semantics of HKS-based graph descriptor. This shows that we can indeed extract more useful features by applying column-wise and row-wise convolution over graph descriptors.\nComparing with GD-linear, which applies linear regression on top of the HKS-based graph descriptor, DeepGraph, GD-MLP, and GD-CNN performs significantly better. This indicates that the effectiveness of the HKS-based graph descriptor has to be utlized by 7 = 10\u22128 is added to the denominator to avoid numeric issues.\na \u201cdeeper\u201d model which explores the convolutions and non-linear transformations of the low-level representation.\nComparing feature based methods with other baselines, the former exhibit strong prediction power. Incorporating both local and global information of the networks, the hand-crafted features are very indicative of the network growth, which is hard for automatic methods to compete.\nWhen trained on deep networks, the performance of graph kernels could be improved over their linear version. Smoothing kernels can further bring in some improvement. By applying convolution over locally connected regions of the graphs, PSCN can beat many graph kernels on most data sets. These results are consistent with previous studies [24, 41]."}, {"heading": "5.2 Computational Cost of DeepGraph", "text": "Training of DeepGraph is very fast. The models are converged in less than 10 minutes on a Titan X GPU. The major overhead of DeepGraph is the computation of the HKS-based graph descriptors. We empirically measure the computation time for all data sets on a server with 2.40 GHz CPU and 120G RAM. The graphs in our data sets have size as large as 5,000 nodes and 200,000 edges, which is enough for most network prediction problems [20, 27, 42]. The generation of graph descriptors takes an average of 0.86 hour per data set. In contrast, the strongest baseline, feature based method, takes 7.9 hours on average to generate all features. While the strongest graph kernel, SP, takes nearly 5 days."}, {"heading": "5.3 Feature Analysis", "text": "It has been shown empirically that DeepGraph could well abstract high-level features to represent graphs. It is intriguing to know whether these learned features correspond to well-known structural patterns in network literature. To this end, we select some of the network properties manually computed for the feature based method. Note that we work only on test set, as we care more about the prediction performance. These properties characterize either global or local aspects of networks, and are listed in Figure 3.\nThe feature vectors output by the last hidden layer of DeepGraph are fed to t-SNE [4], a dimensionality reduction algorithm for visualizing high-dimensional data sets. The visualizations of data sets Weibo and AAN are displayed in Figure 3. We obtain similar results on other data sets, which are omitted to conserve space.\nTo connect the hand-crafted structural properties with the learned high-level features, we color individual graphs by the values of these properties (e.g., network density). Patterns on the distribution of colors could suggest a connection between learned features and the network property.\nAs we could observe, properties on left column of Figure 3 exhibit some related patterns. For example, in the Figure 5(e), the graphs clustered to the top have the fewest communities, while graphs in the bottom right corner have the most. This is interesting as an ego-net with a larger number of communities implies that the graph center lies in between bigger communities, which are likely to be structural holes in the global network. According to social network literature [6], nodes spanning structural holes are likely to gain social capital, promoting the growth of its ego-net. Indeed, when we compare the color scheme of 5(e) with 5(g), which plots the actual growth of the network sizes, we can see that the number of communities in a diffusion network is indeed positively correlated with the growth of the diffusion network.\nTop three figures on the right column also show some common characteristics: the left cluster forms a band, with lower values on the bottom and higher ones on the top. Graphs with higher values mostly cluster to the right area.\nThese common patterns suggest that the high-level features output by deep learning have indeed captured these network proper-\nties. As we include both local and global properties, we demonstrate from another perspective that DeepGraph could learn globalto-local structural information from the network topology. Comparing to 5(h), which plots the actual growth of the h-index of the ego-nodes, we can see the correlation of the features are much weaker than those in the left column (Weibo network). This is not surprising, as h-index is a property that can not be directly derived from the network itself, and thus the prediction task is much harder.\nSome additional observations can be made from Figure 3. First, as the number of open and closed triangles are actually features of graphlets [29, 36], we can see that DeepGraph has automatically learned these useful features without human input. Second, since edge density is a function of the number of edges and nodes, DeepGraph not only learns the number of edges and nodes (we do not show the node property in Figure 3, but this is true), but also their none-linear relationship that involves division."}, {"heading": "5.4 Error Analysis", "text": "Graphs in our data sets typically have hundreds of nodes, which is hard for humans to directly generalize useful information from a set of graphs. As a compromise, we characterize graphs by a set of simple network properties, e.g., the number of nodes, edges, and edge density.\nWe first want to investigate graphs for which DeepGraph makes more mistakes than baseline, and also the other way around. Here we use the strongest baseline, feature-based method as our reference. The procedure is as follows: among graphs where DeepGraph has smaller MSE than the baseline, we select the top 100 with the largest MSE differences between the two methods. For these top graphs, we compute the average of the properties mentioned above. Similar procedure is also applied to the baseline.\nThe statistics of graphs where either DeepGraph or the baseline significantly outperforms the other are higher than the average statistics of each data set. This could result form the skewed distribution of the data set \u2013 a large number of graphs are of smaller size, leading to more training instances of small graphs. We also observe that both methods perform reasonably well on denser networks.\nOn the other hand, graphs on which DeepGraph performed better have relatively larger sizes than those where the baseline performed better. This indicates that the HKS representation has an advantage on larger graphs, the structures of which are more difficult to be represented by a bag of local substructures."}, {"heading": "6. CONCLUSION", "text": "We present a novel neural network model that predicts the growth of network properties based on its graph structure. This model, DeepGraph, computes a new representation of the graph structure based on heat kernel signatures. A multi-column, multi-resolution convolution neural network is designed to further learn the highlevel representations and predict the network growth in an end-toend fashion. Experiments on large collections of real-world networks prove that DeepGraph significantly outperforms methods based on hand-crafted features, graph kernels, and competing deep learning methods. The higher-level representations learned by DeepGraph well correlate with findings and theories in social network literature, showing that a deep learning model can automatically discover meaningful and predictive structural patterns in networks.\nOur study reassures the predictive power of network structures and suggests a way to effectively utilize this power. A meaningful future direction is to integrate network structure with other types of information, such as the content of information cascades in the network. A joint representation of multi-modal information may maximize the performance of particular prediction tasks."}, {"heading": "7. REFERENCES", "text": "[1] L. Backstrom, D. Huttenlocher, J. Kleinberg, and X. Lan.\nGroup formation in large social networks: membership, growth, and evolution. In Proc. of SIGKDD, 2006.\n[2] L. Bai, L. Rossi, A. Torsello, and E. R. Hancock. A quantum jensen\u2013shannon graph kernel for unattributed graphs. Pattern Recognition, 2015.\n[3] X. Bai and E. R. Hancock. Heat kernels, manifolds and graph embedding. In Structural, Syntactic, and Statistical Pattern Recognition. Springer, 2004.\n[4] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large networks. JSTAT, 2008.\n[5] K. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. In Proc. of ICDM, 2005.\n[6] R. S. Burt. The network structure of social capital. Research in organizational behavior, 2000.\n[7] R. Chen, Y. Chen, Y. Liu, and Q. Mei. Does team competition increase pro-social lending? evidence from online microfinance. Games and Economic Behavior, 2015.\n[8] J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014.\n[9] F. Chung. Laplacians and the cheeger inequality for directed graphs. Annals of Combinatorics, 2005.\n[10] B. G. P. M. Dragomir R. Radev, Mark Thomas Joseph. A Bibliometric and Network Analysis of the field of Computational Linguistics. JASIST, 2009.\n[11] D. Easley and J. Kleinberg. Networks, crowds, and markets: Reasoning about a highly connected world. Cambridge University Press, 2010.\n[12] Y. Fang, M. Sun, and K. Ramani. Heat-passing framework for robust interpretation of data in networks. PloS one, 2015.\n[13] Y. Fang, J. Xie, G. Dai, M. Wang, F. Zhu, T. Xu, and E. Wong. 3d deep shape descriptor. In Proc. of CVPR, 2015.\n[14] T. G\u00e4rtner, P. Flach, and S. Wrobel. On graph kernels: Hardness results and efficient alternatives. In Learning Theory and Kernel Machines. 2003.\n[15] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proc. of SIGKDD, 2016.\n[16] R. Guimera, B. Uzzi, J. Spiro, and L. A. N. Amaral. Team assembly mechanisms determine collaboration network structure and team performance. Science, 2005.\n[17] H. Kashima, K. Tsuda, and A. Inokuchi. Kernels for graphs. Kernel methods in computational biology, 2004.\n[18] D. Kingma and J. Ba. Adam: A method for stochastic optimization. Proc. of ICLR, 2015.\n[19] G. Kossinets and D. J. Watts. Empirical analysis of an evolving social network. science, 2006.\n[20] A. Kupavskii, L. Ostroumova, A. Umnov, S. Usachev, P. Serdyukov, G. Gusev, and A. Kustarev. Prediction of retweet cascade size over time. In Proc. of CIKM, 2012.\n[21] H. Ma, H. Yang, M. R. Lyu, and I. King. Mining social networks using heat diffusion processes for marketing candidates selection. In Proc. of CIKM, 2008.\n[22] A. Mislove. Online Social Networks: Measurement, Analysis, and Applications to Distributed Information Systems. PhD thesis, 2009.\n[23] A. Narayanan, M. Chandramohan, L. Chen, Y. Liu, and S. Saminathan. subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs. In\nWorkshop on Mining and Learning with Graphs, 2016. [24] M. Niepert, M. Ahmed, and K. Kutzkov. Learning\nconvolutional neural networks for graphs. 2016. [25] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online\nlearning of social representations. In Proc. of SIGKDD, 2014.\n[26] S. Ranu and A. K. Singh. Graphsig: A scalable approach to mining significant subgraphs in large graph databases. In Proc. of ICDE, 2009.\n[27] D. M. Romero, C. Tan, and J. Ugander. On the interplay between social and topical structure. Proc. of ICWSM, 2013.\n[28] H. Saigo, S. Nowozin, T. Kadowaki, T. Kudo, and K. Tsuda. gboost: a mathematical programming approach to graph classification and regression. Machine Learning, 2009.\n[29] N. Shervashidze, T. Petri, K. Mehlhorn, K. M. Borgwardt, and S. Vishwanathan. Efficient graphlet kernels for large graph comparison. In International conference on artificial intelligence and statistics, pages 488\u2013495, 2009.\n[30] N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-lehman graph kernels. JMLR, 2011.\n[31] J. Sun, M. Ovsjanikov, and L. Guibas. A concise and provably informative multi-scale signature based on heat diffusion. In Computer graphics forum, 2009.\n[32] Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. In Proc. of VLDB, 2011.\n[33] G. Szabo and B. A. Huberman. Predicting the popularity of online content. Communications of the ACM, 2010.\n[34] J. Tang, M. Qu, and Q. Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In Proc. of SIGKDD, 2015.\n[35] O. Tsur and A. Rappoport. What\u2019s in a hashtag?: content based prediction of the spread of ideas in microblogging communities. In Proc. of WSDM, 2012.\n[36] J. Ugander, L. Backstrom, and J. Kleinberg. Subgraph frequencies: Mapping the empirical and extremal geography of large graph collections. In Proc. of WWW, 2013.\n[37] J. Ugander, L. Backstrom, C. Marlow, and J. Kleinberg. Structural diversity in social contagion. Proc. of the National Academy of Sciences, 2012.\n[38] B. Viswanath, A. Mislove, M. Cha, and K. P. Gummadi. On the evolution of user interaction in facebook. In Proc. of WOSN, 2009.\n[39] J. Xie, Y. Fang, F. Zhu, and E. Wong. Deepshape: Deep learned shape descriptor for 3d shape matching and retrieval. In Proc. of CVPR, 2015.\n[40] P. Yanardag and S. Vishwanathan. Deep graph kernels. In Proc. of SIGKDD, 2015.\n[41] P. Yanardag and S. Vishwanathan. A structural smoothing framework for robust graph comparison. In NIPS, 2015.\n[42] J. Yang and J. Leskovec. Modeling information diffusion in implicit networks. In Proc. of ICDM, 2010.\n[43] L. Yang, T. Sun, M. Zhang, and Q. Mei. We know what@ you# tag: does the dual role affect hashtag adoption? In Proc. of WWW, 2012.\n[44] J. Zhang, B. Liu, J. Tang, T. Chen, and J. Li. Social influence locality for modeling retweeting behaviors. In IJCAI, 2013."}], "references": [{"title": "Group formation in large social networks: membership, growth, and evolution", "author": ["L. Backstrom", "D. Huttenlocher", "J. Kleinberg", "X. Lan"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "A quantum jensen\u2013shannon graph kernel for unattributed graphs", "author": ["L. Bai", "L. Rossi", "A. Torsello", "E.R. Hancock"], "venue": "Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Heat kernels, manifolds and graph embedding", "author": ["X. Bai", "E.R. Hancock"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "JSTAT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Shortest-path kernels on graphs", "author": ["K.M. Borgwardt", "H.-P. Kriegel"], "venue": "In Proc. of ICDM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "The network structure of social capital", "author": ["R.S. Burt"], "venue": "Research in organizational behavior,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Does team competition increase pro-social lending? evidence from online microfinance", "author": ["R. Chen", "Y. Chen", "Y. Liu", "Q. Mei"], "venue": "Games and Economic Behavior,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Can cascades be predicted", "author": ["J. Cheng", "L. Adamic", "P.A. Dow", "J.M. Kleinberg", "J. Leskovec"], "venue": "In Proc. of WWW,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Laplacians and the cheeger inequality for directed graphs", "author": ["F. Chung"], "venue": "Annals of Combinatorics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "A Bibliometric and Network Analysis of the field of Computational Linguistics", "author": ["B.G.P.M. Dragomir R. Radev", "Mark Thomas Joseph"], "venue": "JASIST,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Networks, crowds, and markets: Reasoning about a highly connected world", "author": ["D. Easley", "J. Kleinberg"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Heat-passing framework for robust interpretation of data in networks", "author": ["Y. Fang", "M. Sun", "K. Ramani"], "venue": "PloS one,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "3d deep shape descriptor", "author": ["Y. Fang", "J. Xie", "G. Dai", "M. Wang", "F. Zhu", "T. Xu", "E. Wong"], "venue": "In Proc. of CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "On graph kernels: Hardness results and efficient alternatives", "author": ["T. G\u00e4rtner", "P. Flach", "S. Wrobel"], "venue": "In Learning Theory and Kernel Machines", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "node2vec: Scalable feature learning for networks", "author": ["A. Grover", "J. Leskovec"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Team assembly mechanisms determine collaboration network structure and team", "author": ["R. Guimera", "B. Uzzi", "J. Spiro", "L.A.N. Amaral"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Kernels for graphs", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": "Kernel methods in computational biology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proc. of ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Empirical analysis of an evolving social", "author": ["G. Kossinets", "D.J. Watts"], "venue": "network. science,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Prediction of retweet cascade size over time", "author": ["A. Kupavskii", "L. Ostroumova", "A. Umnov", "S. Usachev", "P. Serdyukov", "G. Gusev", "A. Kustarev"], "venue": "In Proc. of CIKM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Mining social networks using heat diffusion processes for marketing candidates selection", "author": ["H. Ma", "H. Yang", "M.R. Lyu", "I. King"], "venue": "In Proc. of CIKM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Online Social Networks: Measurement, Analysis, and Applications to Distributed Information Systems", "author": ["A. Mislove"], "venue": "PhD thesis,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs", "author": ["A. Narayanan", "M. Chandramohan", "L. Chen", "Y. Liu", "S. Saminathan"], "venue": "Workshop on Mining and Learning with Graphs,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Learning convolutional neural networks for graphs. 2016", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Graphsig: A scalable approach to mining significant subgraphs in large graph databases", "author": ["S. Ranu", "A.K. Singh"], "venue": "In Proc. of ICDE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "On the interplay between social and topical structure", "author": ["D.M. Romero", "C. Tan", "J. Ugander"], "venue": "Proc. of ICWSM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "gboost: a mathematical programming approach to graph classification and regression", "author": ["H. Saigo", "S. Nowozin", "T. Kadowaki", "T. Kudo", "K. Tsuda"], "venue": "Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Efficient graphlet kernels for large graph comparison", "author": ["N. Shervashidze", "T. Petri", "K. Mehlhorn", "K.M. Borgwardt", "S. Vishwanathan"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Weisfeiler-lehman graph kernels", "author": ["N. Shervashidze", "P. Schweitzer", "E.J. Van Leeuwen", "K. Mehlhorn", "K.M. Borgwardt"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "A concise and provably informative multi-scale signature based on heat diffusion", "author": ["J. Sun", "M. Ovsjanikov", "L. Guibas"], "venue": "In Computer graphics forum,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "X. Yan", "P.S. Yu", "T. Wu"], "venue": "In Proc. of VLDB,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Predicting the popularity of online content", "author": ["G. Szabo", "B.A. Huberman"], "venue": "Communications of the ACM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Pte: Predictive text embedding through large-scale heterogeneous text networks", "author": ["J. Tang", "M. Qu", "Q. Mei"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "What\u2019s in a hashtag?: content based prediction of the spread of ideas in microblogging communities", "author": ["O. Tsur", "A. Rappoport"], "venue": "In Proc. of WSDM,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Subgraph frequencies: Mapping the empirical and extremal geography of large graph collections", "author": ["J. Ugander", "L. Backstrom", "J. Kleinberg"], "venue": "In Proc. of WWW,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Structural diversity in social contagion", "author": ["J. Ugander", "L. Backstrom", "C. Marlow", "J. Kleinberg"], "venue": "Proc. of the National Academy of Sciences,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "On the evolution of user interaction in facebook", "author": ["B. Viswanath", "A. Mislove", "M. Cha", "K.P. Gummadi"], "venue": "In Proc. of WOSN,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Deepshape: Deep learned shape descriptor for 3d shape matching and retrieval", "author": ["J. Xie", "Y. Fang", "F. Zhu", "E. Wong"], "venue": "In Proc. of CVPR,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Deep graph kernels", "author": ["P. Yanardag", "S. Vishwanathan"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "A structural smoothing framework for robust graph comparison", "author": ["P. Yanardag", "S. Vishwanathan"], "venue": "In NIPS,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Modeling information diffusion in implicit networks", "author": ["J. Yang", "J. Leskovec"], "venue": "In Proc. of ICDM,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "We know what@ you# tag: does the dual role affect hashtag adoption", "author": ["L. Yang", "T. Sun", "M. Zhang", "Q. Mei"], "venue": "In Proc. of WWW,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Social influence locality for modeling retweeting behaviors", "author": ["J. Zhang", "B. Liu", "J. Tang", "T. Chen", "J. Li"], "venue": "In IJCAI,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}], "referenceMentions": [{"referenceID": 32, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 41, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 0, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 26, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 19, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 34, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 7, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 0, "context": "A function is learned that takes these features as input and outputs a predicted value of the network property in the future [1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 42, "context": "For example, the content of a hashtag is predictive to its diffusion [43] and homophily (e.", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": ", similar demongraphics) is predictive to the growth of social groups [7], but these effects are not generalizable to other networks and other dynamic properties.", "startOffset": 70, "endOffset": 73}, {"referenceID": 10, "context": "For example, open triads with two strong ties are likely to be closed in the near future [11]; dense communities are resistant to novel information and they grow slower than others [16]; nodes spanning structural holes are likely to gain social capital and experience a rapid growth of its prestige and other properties [6].", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "For example, open triads with two strong ties are likely to be closed in the near future [11]; dense communities are resistant to novel information and they grow slower than others [16]; nodes spanning structural holes are likely to gain social capital and experience a rapid growth of its prestige and other properties [6].", "startOffset": 181, "endOffset": 185}, {"referenceID": 5, "context": "For example, open triads with two strong ties are likely to be closed in the near future [11]; dense communities are resistant to novel information and they grow slower than others [16]; nodes spanning structural holes are likely to gain social capital and experience a rapid growth of its prestige and other properties [6].", "startOffset": 320, "endOffset": 323}, {"referenceID": 30, "context": "We introduce a graph descriptor that is based on the Heat Kernel Signature (HKS) [31], which serves as a universal low-level representation of the topological structures of networks.", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "HKS has been successfully employed in representing the surface of 3D objects [13, 39].", "startOffset": 77, "endOffset": 85}, {"referenceID": 38, "context": "HKS has been successfully employed in representing the surface of 3D objects [13, 39].", "startOffset": 77, "endOffset": 85}, {"referenceID": 12, "context": "Using a histogram to describe the probability distribution of heat values at a series of time points [13, 39], isomorphic networks (networks with the same topological structure) can be mapped to a unique representation at little loss of structural information.", "startOffset": 101, "endOffset": 109}, {"referenceID": 38, "context": "Using a histogram to describe the probability distribution of heat values at a series of time points [13, 39], isomorphic networks (networks with the same topological structure) can be mapped to a unique representation at little loss of structural information.", "startOffset": 101, "endOffset": 109}, {"referenceID": 32, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 41, "endOffset": 45}, {"referenceID": 41, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 127, "endOffset": 134}, {"referenceID": 26, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 127, "endOffset": 134}, {"referenceID": 19, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 165, "endOffset": 176}, {"referenceID": 34, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 165, "endOffset": 176}, {"referenceID": 7, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 165, "endOffset": 176}, {"referenceID": 18, "context": ", triads [19], quads [37], or metapaths [32]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": ", triads [19], quads [37], or metapaths [32]).", "startOffset": 21, "endOffset": 25}, {"referenceID": 31, "context": ", triads [19], quads [37], or metapaths [32]).", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "In graph classification, a myriad of graph kernel methods are proposed which compute pairwise similarities between graphs [17, 2, 30, 29].", "startOffset": 122, "endOffset": 137}, {"referenceID": 1, "context": "In graph classification, a myriad of graph kernel methods are proposed which compute pairwise similarities between graphs [17, 2, 30, 29].", "startOffset": 122, "endOffset": 137}, {"referenceID": 29, "context": "In graph classification, a myriad of graph kernel methods are proposed which compute pairwise similarities between graphs [17, 2, 30, 29].", "startOffset": 122, "endOffset": 137}, {"referenceID": 28, "context": "In graph classification, a myriad of graph kernel methods are proposed which compute pairwise similarities between graphs [17, 2, 30, 29].", "startOffset": 122, "endOffset": 137}, {"referenceID": 28, "context": "For example, graphlets [29, 36] computes the graph similarity based on the distribution of induced, non-isomorphic subgraphs.", "startOffset": 23, "endOffset": 31}, {"referenceID": 35, "context": "For example, graphlets [29, 36] computes the graph similarity based on the distribution of induced, non-isomorphic subgraphs.", "startOffset": 23, "endOffset": 31}, {"referenceID": 27, "context": "Some other graph kernels integrate frequent graph mining into the model training process [28, 26].", "startOffset": 89, "endOffset": 97}, {"referenceID": 25, "context": "Some other graph kernels integrate frequent graph mining into the model training process [28, 26].", "startOffset": 89, "endOffset": 97}, {"referenceID": 33, "context": "Several proposals have been made to learn a low-dimensional vector representation of individual nodes by considering their neighborhood [34, 25, 15].", "startOffset": 136, "endOffset": 148}, {"referenceID": 24, "context": "Several proposals have been made to learn a low-dimensional vector representation of individual nodes by considering their neighborhood [34, 25, 15].", "startOffset": 136, "endOffset": 148}, {"referenceID": 14, "context": "Several proposals have been made to learn a low-dimensional vector representation of individual nodes by considering their neighborhood [34, 25, 15].", "startOffset": 136, "endOffset": 148}, {"referenceID": 39, "context": "Deep learning techniques have also improved graph kernels for graph structure learning[40, 41, 23].", "startOffset": 86, "endOffset": 98}, {"referenceID": 40, "context": "Deep learning techniques have also improved graph kernels for graph structure learning[40, 41, 23].", "startOffset": 86, "endOffset": 98}, {"referenceID": 22, "context": "Deep learning techniques have also improved graph kernels for graph structure learning[40, 41, 23].", "startOffset": 86, "endOffset": 98}, {"referenceID": 23, "context": "[24] applied convolution over receptive fields constructed by sequence of neighboring nodes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Heat kernels have been studied for the task of graph clustering [3], graph partitioning [12], and modeling social network marketing processes [21].", "startOffset": 64, "endOffset": 67}, {"referenceID": 11, "context": "Heat kernels have been studied for the task of graph clustering [3], graph partitioning [12], and modeling social network marketing processes [21].", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "Heat kernels have been studied for the task of graph clustering [3], graph partitioning [12], and modeling social network marketing processes [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 30, "context": "ture has been successfully used to model 3D objects [31, 13, 39], whose surfaces are defined by polygon meshes, a network com-", "startOffset": 52, "endOffset": 64}, {"referenceID": 12, "context": "ture has been successfully used to model 3D objects [31, 13, 39], whose surfaces are defined by polygon meshes, a network com-", "startOffset": 52, "endOffset": 64}, {"referenceID": 38, "context": "ture has been successfully used to model 3D objects [31, 13, 39], whose surfaces are defined by polygon meshes, a network com-", "startOffset": 52, "endOffset": 64}, {"referenceID": 30, "context": "The motivation in adopting Heat Kernel Signature (HKS) is its theoretical proven properties in representing graphs: HKS is an intrinsic and informative representation for graphs [31].", "startOffset": 178, "endOffset": 182}, {"referenceID": 19, "context": "In practice, researchers focus on a specially case of the network growth prediction problem with the equal interval increment constraint, tj \u2212 tj = ti \u2212 ti = C > 0 [20, 35].", "startOffset": 164, "endOffset": 172}, {"referenceID": 34, "context": "In practice, researchers focus on a specially case of the network growth prediction problem with the equal interval increment constraint, tj \u2212 tj = ti \u2212 ti = C > 0 [20, 35].", "startOffset": 164, "endOffset": 172}, {"referenceID": 30, "context": "In computer vision, graphs are stored as meshed networks and heat kernels are computed by finding eigenfunctions of the Laplace-Beltrami operator [31].", "startOffset": 146, "endOffset": 150}, {"referenceID": 30, "context": "Instead, we use eigenfunction expansion of a graph Laplacian [31, 3] to compute the heat kernel for information networks.", "startOffset": 61, "endOffset": 68}, {"referenceID": 2, "context": "Instead, we use eigenfunction expansion of a graph Laplacian [31, 3] to compute the heat kernel for information networks.", "startOffset": 61, "endOffset": 68}, {"referenceID": 8, "context": "There has been studies on how to tackle this problem [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 30, "context": "These time points are sampled with equal difference after logarithm [31], such that log zn \u2212 log zn\u22121 = log zn+1 \u2212 log zn.", "startOffset": 68, "endOffset": 72}, {"referenceID": 37, "context": "Figure (a) and (b) are subnetworks from Facebook [38].", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "Figure (c) and (d) are some authors\u2019 collaboration networks built from ACL Anthology [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 39, "context": "So following [40], from each large network we extract the ego-networks (subgraph consisting of the neighbors", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "scaled growth scales label y to log2(y + 1) [20, 35].", "startOffset": 44, "endOffset": 52}, {"referenceID": 34, "context": "scaled growth scales label y to log2(y + 1) [20, 35].", "startOffset": 44, "endOffset": 52}, {"referenceID": 39, "context": "We follow the procedure described in [40] to construct ego-nets.", "startOffset": 37, "endOffset": 41}, {"referenceID": 37, "context": "The Facebook data set is collected from the New Orleans networks [38],", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "As the YouTube [22] data set also describes user friendships, it", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "The AAN data set [10] is built upon scientific publications from the ACL Anthology, where nodes are authors and edges are collaboration.", "startOffset": 17, "endOffset": 21}, {"referenceID": 43, "context": "The Weibo data set [44] contains a set of Sina Weibo users with their complete following-followee relationships, as well as 300,000 retweeting paths among these users.", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "Therefore we downsampled 50% graphs of each train/val/test set with zero growth (to the numbers shown in Table 1) and applied a logarithm transformation of the outcome variable (network growth), following [20, 35].", "startOffset": 205, "endOffset": 213}, {"referenceID": 34, "context": "Therefore we downsampled 50% graphs of each train/val/test set with zero growth (to the numbers shown in Table 1) and applied a logarithm transformation of the outcome variable (network growth), following [20, 35].", "startOffset": 205, "endOffset": 213}, {"referenceID": 0, "context": "Many structural features have been designed for various network prediction tasks [1, 27, 35, 8].", "startOffset": 81, "endOffset": 95}, {"referenceID": 26, "context": "Many structural features have been designed for various network prediction tasks [1, 27, 35, 8].", "startOffset": 81, "endOffset": 95}, {"referenceID": 34, "context": "Many structural features have been designed for various network prediction tasks [1, 27, 35, 8].", "startOffset": 81, "endOffset": 95}, {"referenceID": 7, "context": "Many structural features have been designed for various network prediction tasks [1, 27, 35, 8].", "startOffset": 81, "endOffset": 95}, {"referenceID": 36, "context": "com/ Frequencies of k-node substructures (k \u2264 4)[37].", "startOffset": 48, "endOffset": 52}, {"referenceID": 3, "context": "Other network properties: average degree, the length of the shortest path, edge density, the number of leaf nodes (nodes with degree 1), the number of leaf edges, the average closeness of all nodes, clustering coefficient, diameter, and the number of communities obtained by a community detection algorithm [4].", "startOffset": 307, "endOffset": 310}, {"referenceID": 23, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 98, "endOffset": 101}, {"referenceID": 13, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 131, "endOffset": 135}, {"referenceID": 28, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 168, "endOffset": 172}, {"referenceID": 29, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 220, "endOffset": 224}, {"referenceID": 23, "context": "This exclusion is also observed for the same reason in [24, 41].", "startOffset": 55, "endOffset": 63}, {"referenceID": 40, "context": "This exclusion is also observed for the same reason in [24, 41].", "startOffset": 55, "endOffset": 63}, {"referenceID": 40, "context": "[41] apply smoothing to graph kernels, which extends their method of deep graph kernels [40] by considering structural similarity between sub-structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[41] apply smoothing to graph kernels, which extends their method of deep graph kernels [40] by considering structural similarity between sub-structures.", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "PSCN, which applies convolutional neural networks (CNN) to locally connected regions from graphs [24], achieving better results over graph kernels on some of the classification data sets.", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "Following [24] for PSCN, the width is set to the average number of nodes, and the receptive field size is chosen between 5 and 10.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "Default hyper-parameters of Adam are used [18].", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "These results are consistent with previous studies [24, 41].", "startOffset": 51, "endOffset": 59}, {"referenceID": 40, "context": "These results are consistent with previous studies [24, 41].", "startOffset": 51, "endOffset": 59}, {"referenceID": 19, "context": "The graphs in our data sets have size as large as 5,000 nodes and 200,000 edges, which is enough for most network prediction problems [20, 27, 42].", "startOffset": 134, "endOffset": 146}, {"referenceID": 26, "context": "The graphs in our data sets have size as large as 5,000 nodes and 200,000 edges, which is enough for most network prediction problems [20, 27, 42].", "startOffset": 134, "endOffset": 146}, {"referenceID": 41, "context": "The graphs in our data sets have size as large as 5,000 nodes and 200,000 edges, which is enough for most network prediction problems [20, 27, 42].", "startOffset": 134, "endOffset": 146}, {"referenceID": 3, "context": "The feature vectors output by the last hidden layer of DeepGraph are fed to t-SNE [4], a dimensionality reduction algorithm for visualizing high-dimensional data sets.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "According to social network literature [6], nodes spanning structural holes are likely to gain social capital, promoting the growth of its ego-net.", "startOffset": 39, "endOffset": 42}, {"referenceID": 28, "context": "First, as the number of open and closed triangles are actually features of graphlets [29, 36], we can see that DeepGraph has automatically learned these useful features without human input.", "startOffset": 85, "endOffset": 93}, {"referenceID": 35, "context": "First, as the number of open and closed triangles are actually features of graphlets [29, 36], we can see that DeepGraph has automatically learned these useful features without human input.", "startOffset": 85, "endOffset": 93}], "year": 2016, "abstractText": "The topological (or graph) structures of real-world networks are known to be predictive of multiple dynamic properties of the networks. Conventionally, a graph structure is represented using an adjacency matrix or a set of hand-crafted structural features. These representations either fail to highlight local and global properties of the graph or suffer from a severe loss of structural information. There lacks an effective graph representation, which hinges the realization of the predictive power of network structures. In this study, we propose to learn the represention of a graph, or the topological structure of a network, through a deep learning model. This end-to-end prediction model, named DeepGraph, takes the input of the raw adjacency matrix of a real-world network and outputs a prediction of the growth of the network. The adjacency matrix is first represented using a graph descriptor based on the heat kernel signature, which is then passed through a multicolumn, multi-resolution convolutional neural network. Extensive experiments on five large collections of real-world networks demonstrate that the proposed prediction model significantly improves the effectiveness of existing methods, including linear or nonlinear regressors that use hand-crafted features, graph kernels, and competing deep learning methods.", "creator": "LaTeX with hyperref package"}}}