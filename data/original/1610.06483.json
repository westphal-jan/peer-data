{"id": "1610.06483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "An Extended Neo-Fuzzy Neuron and its Adaptive Learning Algorithm", "abstract": "A modification of the neo-fuzzy neuron is proposed (an extended neo-fuzzy neuron (ENFN)) that is characterized by improved approximating properties. An adaptive learning algorithm is proposed that has both tracking and smoothing properties. An ENFN distinctive feature is its computational simplicity compared to other artificial neural networks and neuro-fuzzy systems.", "histories": [["v1", "Thu, 20 Oct 2016 16:25:38 GMT  (540kb)", "http://arxiv.org/abs/1610.06483v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["yevgeniy v bodyanskiy", "oleksii k tyshchenko", "daria s kopaliani"], "accepted": false, "id": "1610.06483"}, "pdf": {"name": "1610.06483.pdf", "metadata": {"source": "CRF", "title": "An Extended Neo-Fuzzy Neuron and its Adaptive Learning Algorithm", "authors": ["Yevgeniy V. Bodyanskiy", "Oleksii K. Tyshchenko", "Daria S. Kopaliani"], "emails": ["bodya@kture.kharkov.ua", "}@gmail.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nArtificial neural networks (ANNs) are currently widely used for solving different Data Mining tasks due to their approximating capabilities and their ability to learn from experimental data [1-3]. However, when the data come sequentially in real time, many neural networks lose their effectiveness because of the multiepoch learning (which is used in many ANNs and designated only for a batch mode). Of course, radial-basis-function networks (RBFN) could be used in such situations. These ANNs are characterized by a high speed of learning processes, but, first of all, these networks suffer from the so-called \u00abcurse of dimensionality\u00bb and, secondly, even a trained neural network is a \u00abblack box\u00bb, and its results can not be interpreted. Hybrid systems of computational intelligence [5-7], and above all neuro-fuzzy systems (NFSs), combining the advantages of ANNs and fuzzy inference systems (FISs), do not suffer from the \u201ccurse of dimensionality\u201d and provide linguistic interpretability and transparency of the results. However, since most of the well-known NFSs are trained with the help of the error backpropagation concept, they are ill-equipped to work in an online mode.\nDue to the above mentioned problems, we would like to develop hybrid systems of computational intelligence that deal with processing the incoming data in an online mode and have advantages of both ANNs and NFSs.\nThe remainder of this paper is organized as follows: Section 2 gives a neo-fuzzy neuron architecture. Section 3 describes an extended neo-fuzzy neuron architecture. Section 4 presents experiments and evaluation. Conclusions and future work are given in the final section.\nII. THE NEO-FUZZY NEURON ARCHITECTURE\nTo overcome the above mentioned problems, a neuro-fuzzy system called by the authors a \u201cneo-fuzzy neuron (NFN)\u201d was introduced in [8 \u2013 10]. The architecture of the neo-fuzzy neuron is in fig.1.\nFig.1. A neo-fuzzy neuron\nThe neo-fuzzy neuron is a nonlinear learning system with multiple inputs and one output that implements the mapping\n  1\n\u02c6 n\ni i i y f x    (1)\nwhere ix is the i-th component of the n-dimensional input signal vector  1,..., ,..., T n\ni nx x x x R  , y\u0302 is a scalar NFN output. The NFN structural blocks are nonlinear synapses iNS that carry out a nonlinear transformation of the i-th component of ix in the form\n    1\nh\ni i li li i l f x w x    (2)\nwhere liw is the l-th synaptic weight of the i-th nonlinear synapse, 1, 2,...,l h , 1, 2,...,i n ;  li ix is the l-th membership function in the i-th nonlinear synapse performing fuzzification of a crisp component ix . Thus, transformation implemented by the NFN can be written as\n  1 1\n\u02c6 n h\nli li i i l y w x     . (3) Fuzzy inference implemented by the same NFN has the form\nIF ix IS liX THEN THE OUTPUT IS , 1, 2,...,liw l h , (4) which means that a nonlinear synapse actually implements the zero-order Takagi-Sugeno fuzzy inference [11, 12]. The NFN authors [8\u201310] used traditional triangular constructions as membership functions that meet the unity partition conditions\n \n1, 1, 1,\n1, 1, 1,\n, ,\n, ,\n0\ni l i i l i li li l i\nl i i li i i li l i\nl i li\nx c if x c c c c c x\nx if x c c c c\notherwise\n\n  \n  \n              \n(5)\nwhere lic stands for arbitrarily selected (usually uniformly distributed) centers of the membership functions in the interval [0, 1] , thus, naturally 0 1ix  .\nSuch a choice of the membership functions leads to the fact that the i-th component of the input signal ix activates only two adjacent functions, thus their sum is equal to 1 which means that    1, 1li i l i ix x    (6) and\n     1, 1,i i li li i l i l i if x w x w x    . (7) Just exactly this circumstance allows to synthesize simple and effective adaptive controllers for nonlinear control objects [13, 14]. One can use other membership functions except triangular constructions, first of all, B-splines [15] that proved their effectiveness in the neo-fuzzy neurons [16]. A general case of the membership functions based on the q-th degree B-spline can be presented in the form\n   \n \n1,\n1,\n, 1,\n, 1,\n1 , 1,\n0\n, 1,\n, 1 1,\n1, 2,..., - .\ni li l i\nBi li B li i li i l q i li\nl q i i B l i i\nl q i l i\nif x c c for q\notherwise x c\nx qx q c c c x\nx q for q c c\nl h q\n\n\n\n \n \n \n          \n    \n     \n(8)\nWhen 2q  , we obtain the traditional triangular functions. It should be mentioned that B-splines also provide the unity partition in the form\n  1\n, 1 h\nB li i l x q   ,\nthey are non-negative which means that  , 0Bli ix q  and have a local support  , 0Bli ix q  for ,,i li l q ix c c     .\nWhen the vector signal         1 ,..., ,..., T\ni nx k x k x k x k ( 1,2,...k  \u2013 the current discrete time) is fed to the NFN input, a scalar value is calculated at the NFN output\n       1 1\n\u02c6 1 n h\nli li i i l y k w k x k     (9)\nwhere  1liw k  is the current value of adjusted synaptic weights that were obtained from a learning procedure of the previous  1k  observations.\nIntroducing a  1nh \u2013 membership functions vector            11 1 1 1 12 2,..., , ,...,hx k x k x k x k         ,..., Tli i hn nx k x k  and a corresponding synaptic weights vector\n          11 1 121 1 ,..., 1 , 1 ,..., 1 ,h liw k w k w k w k w k       , 1 T\nhnw k  , we can write the transformation (9) implemented by the NFN in a compact form\n      \u02c6 1Ty k w k x k  . (10) To adjust the neo-fuzzy neuron parameters, the authors used a gradient procedure that minimizes the learning\ncriterion\n        \n    \n2 2\n2\n1 1\n1 1\u02c6 2 2\n1 2\nn h\nli li i i l\nE k y k y k e k\ny k w x k  \n   \n     \n (11)\nand has the form                     \n          1 1\n1\n\u02c61\n1\n1\nli li li i\nli li i\nli\nn h\nli li i li i i l\nw k w k e k x k\nw k y k y k x k\nw k\ny k w k x k x k\n \n \n    \n   \n    \n  \n       \n(12)\nwhere  y k is an external reference signal,  e k is a learning error,  is a learning rate parameter. A special algorithm was proposed in [17] to accelerate the NFN learning procedure which has both tracking (for non-stationary signal processing) and filtering properties (for \"noisy\" data processing)                   1 2 1 , 1 ,0 1. w k w k r k e k x k r k r k x k               (13)\nWhen 0  , the algorithm (13) is identical to the one-step Kaczmarz-Widrow-Hoff learning algorithm [18] and when 1  \u2013 to the Goodwin-Ramage-Caines stochastic approximation algorithm [19].\nIt should be mentioned that one can use many other learning and identification algorithms including the traditional least-squares method with all modifications to train the NFN synaptic weights.\nIII. AN EXTENDED NEO-FUZZY NEURON\nAs mentioned above, the NFN nonlinear synapse iNS implements the zero-order Takagi-Sugeno inference thus being the elementary Wang-Mendel neuro-fuzzy system [20\u201322]. It is possible to improve approximating properties of such a system by using a special structural unit which we called an \u201cextended nonlinear synapse\u201d ( ,iENS fig.2) and to synthesize an \u201cextended neo-fuzzy neuron\u201d (ENFN) that contains iENS elements instead of usual nonlinear synapses iNS (fig.3).\nFig.2. An extended neo-fuzzy synapse\nFig.3. An extended neo-fuzzy neuron\nIntroducing additional variables\n    0 1 2 2 ... ,p pli i li i li li i li i li ix x w w x w x w x      (14)\n                 0 1 2 2 1 0 1 1 1 1 1 1 1 0 2 2 2 2 ... ... ... ... ,\nh p p\ni i li i li li i li i li i l\np p i i i i i i i i i i i p p p p i i i i i i i hi i hi i\nf x x w w x w x w x\nw x w x x w x x\nw x w x x w x x\n\n  \n  \n\n     \n    \n    \n (15)\n 0 1 01 1 1 2 2, ,..., , ,..., ,..., , Tp p p i i i i i i hiw w w w w w w (16)\n              1 1 1\n2 2\n, ,..., ,\n,..., ,..., ,\np i i i i i i i i i i\nTp p i i i i i i hi i\nx x x x x x\nx x x x x\n   \n  \n (17)\nit can be written\n    ,Ti i i i if x w x  (18)\n      1 1\n\u02c6 n n\nT T i i i i i i y f x w x w x           (19)\nwhere  1 ,..., ,..., TT T T Ti nw w w w ,         1 1 ,..., ,..., TT T Ti i n nx x x x       . It\u2019s easy to see that the ENFN contains  1p hn adjusted synaptic weights and the fuzzy inference implemented\nby each iENS has the form\n0 1 ... , 1, 2,..., i li\np p li li i li i\nIF x IS X THEN THE OUTPUT IS w w x w x l h   \n(20)\nwhich coincides with the p-order Takagi-Sugeno inference. The ENFN has a much simpler architecture than a traditional neuro-fuzzy system that simplifies its numerical implementation. When the vector signal  x k is fed to the ENFN input, a scalar value is calculated at the ENFN output\n      \u02c6 1Ty k w k x k   (21) wherein this expression differs from the expression (10) by the fact that it contains  1p  times more adjusted parameters than the conventional NFN. It is clear that the algorithm (13) may be used for training ENFN parameters obtaining the form in this case\n                  1 2 1 , 1 ,0 1. w k w k r k e k x k r k r k x k         \n    \n   \n   (22)\nIV. EXPERIMENT AND ANALYSIS\nTo demonstrate the efficiency of the proposed adaptive neuro-fuzzy system and its learning procedure (22), we have implemented a simulation test based on forecasting of a chaotic process defined by the Mackey-Glass equation [23]\n      10 0.2 0.1 . 1 t t y t y t y t         (23)\nThe signal defined by (22) was quantized with a step 0.1. We took a fragment containing 12000 points. The goal was to predict a time-series value on the next step 1k  using its values on steps 3k  , 2k  , 1k  , and k .\nFirst 7000 points were used as a training set (for adjusting weight coefficients of the architecture), next 5000 points were used as a test set (7001-12000) without adjusting weight coefficients. p is a fuzzy inference order, a number of membership functions h is 3, a smoothing parameter  is 0.9 during the weight adaptation procedure in (21).\nWe implement one step prediction in all our experiments. Symmetric mean absolute percentage error (SMAPE), root mean square error (RMSE) and mean square error (MSE), used for result evaluation, are shown in Tab.1-5. Fig.4-8 present time series outputs, prediction values and prediction errors (a time series value is marked with a blue color, a prediction value is marked with a green color, a prediction error is marked with a red color). The proposed algorithm gives the close approximation and the high prediction quality of sufficiently nonstationary processes in an online mode.\n        3sin / 250 500,\n0.8sin / 250 0.2sin / 25 .\nk if k\nf k k k otherwise\n\n \n  \n   \nRMSEtest MSEtest SMAPEtest p=0 0.0080359 0.0214440 25.1994989 p=1 0.0114304 0.0194163 25.3300074\np=2 0.0017572 0.0104677 20.2466940 p=3 0.0010388 0.0106037 20.2904977 p=5 8.6533285 0.0109404 20.4180131\nFig.8. The Narendra time series prediction (p = 5, h = 3,  = 0.9).\nAs it can be seen from the fig.4-8, the ENFN approximating properties are better when compared to the traditional NFN architecture which is in fact an ENFN special case (the NFN implements the zero-order TakagiSugeno fuzzy inference, p=0).\nVI. CONCLUSION\nThe architecture of an extended neo-fuzzy neuron is proposed in the paper which is a generalization of the standard neo-fuzzy neuron in a case of the \u201cabove zero\u201d-order fuzzy inference. The learning algorithm is proposed that is characterized by both tracking and filtering properties. The extended NFN has improved approximating properties, it\u2019s characterized by a high learning rate, it also has simple numerical implementation.\nACKNOWLEDGMENT\nThe authors would like to thank anonymous reviewers for their careful reading of this paper and for their helpful comments.\nREFERENCES\n[1] D. Graupe, Principles of Artificial Neural Networks (Advanced Series in Circuits and Systems). Singapore: World Scientific Publishing Co. Pte. Ltd., 2007. [2] K. Suzuki, Artificial Neural Networks: Architectures and Applications. NY: InTech, 2013. [3] G. Hanrahan, Artificial Neural Networks in Biological and Environmental Analysis. NW: CRC Press, 2011. [4] L. Rutkowski, Computational Intelligence. Methods and Techniques. Berlin: Springer-Verlag, 2008. [5] C.L. Mumford and L.C. Jain, Computational Intelligence. Berlin: Springer-Verlag, 2009. [6] R. Kruse, C. Borgelt, F. Klawonn, C. Moewes, M. Steinbrecher, and P. Held, Computational Intelligence. A Methodological\nIntroduction. Berlin: Springer-Verlag, 2013. [7] K.-L. Du and M.N.S. Swamy, Neural Networks and Statistical Learning. London: Springer-Verlag, 2014. [8] T. Yamakawa, E. Uchino, T. Miki and H. Kusanagi, \u201cA neo fuzzy neuron and its applications to system identification and\nprediction of the system behavior,\u201d Proc. 2nd Int. Conf. on Fuzzy Logic and Neural Networks, pp. 477-483, 1992. [9] E. Uchino and T. Yamakawa, \u201cSoft computing based signal prediction, restoration and filtering,\u201d in Intelligent Hybrid\nSystems: Fuzzy Logic, Neural Networks and Genetic Algorithms, Boston: Kluwer Academic Publisher, 1997, pp. 331-349. [10] T. Miki and T. Yamakawa, \u201cAnalog implementation of neo-fuzzy neuron and its on-board learning,\u201d in Computational\nIntelligence and Applications, Piraeus: WSES Press, 1999, pp. 144-149. [11] T. Takagi T. and M. Sugeno, \u201cFuzzy identification of systems and its application to modeling and control\u201d, in IEEE Trans.\non Systems, Man, and Cybernetics, no. 15, 1985, pp. 116 \u2013 132. [12] J-S.R. Jang, C.T. Sun and E. Mizutani, Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and\nMachine Intelligence, New Jersey: Prentice Hall, 1997. [13] Ye. Bodyanskiy and V. Kolodyazhniy, \u201cAdaptive nonlinear control using neo-fuzzy model\u201d, in Sinergies Between\nInformation Processing and Automation, pp. 122 \u2013 127, 2004. [14] Ye. Bodyanskiy, I. Kokshenev, V. Kolodyazhniy, and P. Otto, \u201cA self-training robust neo-fuzzy controller with constraints\non control actions\u201d, 50th Int. Wiss. Koll. Tagungsband, ss. 125 \u2013 126, 2005. [15] J. Zhang and H. Knoll, \u201cConstructing fuzzy-controllers with B-spline models \u2013 Principles and Applications\u201d, in Int. J. of\nIntelligent Systems, no. 13, 1998, pp. 257 \u2013 285. [16] V. Kolodyazhniy and Ye. Bodyanskiy, \u201cCascaded multiresolution spline-based fuzzy neural network\u201d, Proc. Int. Symp. on\nEvolving Intelligent Systems, pp. 26 \u2013 29, 2010.\n[17] Ye. Bodyanskiy, I. Kokshenev and V. Kolodyazhniy, \u201cAn adaptive learning algorithm for a neo-fuzzy neuron\u201d, Proc. 3rd Int. Conf. of European Union Soc. for Fuzzy Logic and Technology, pp. 375-379, 2003. [18] S. Haykin, Neural Networks: A Comprehensive Foundation. Upper Saddle River, New Jersey: Prentice Hall, 1999. [19] G.C. Goodwin, P.J. Ramage, and P.E. Caines, \u201cDiscrete time stochastic adaptive control\u201d, in SIAM J. Control and\nOptimization, no. 19, 1981, pp. 829 \u2013 853. [20] L.X. Wang and J.M. Mendel, \u201cFuzzy basis functions, universal approximation and orthogonal least squares learning\u201d, in\nIEEE Trans. on Neural Networks, no. 3, 1993, pp. 807 \u2013 814. [21] L.X. Wang, Adaptive Fuzzy Systems and Control. Design and Stability Analysis. Upper Saddle River, New Jersey: Prentice\nHall, 1994. [22] S. Osowski, Sieci neuronowe do przetwarzania informacji. Warszawa: Oficijna Wydawnicza Politechniki Warszawskiej,\n2006. [23] M.C. Mackey and L. Glass, \u201cOscillation and chaos in physiological control systems\u201d, in Science, no. 197, 1977, pp. 238-\n289. [24] K.S. Narendra and K. Parthasarathy, \u201cIdentification and control of dynamic systems using neural networks\u201d, in IEEE Trans.\non Neural Networks, no. 1, 1990, pp. 4-26."}], "references": [{"title": "Principles of Artificial Neural Networks (Advanced Series in Circuits and Systems)", "author": ["D. Graupe"], "venue": "Singapore: World Scientific Publishing Co. Pte. Ltd.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Artificial Neural Networks: Architectures and Applications", "author": ["K. Suzuki"], "venue": "NY: InTech", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Artificial Neural Networks in Biological and Environmental Analysis", "author": ["G. Hanrahan"], "venue": "NW: CRC Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Computational Intelligence", "author": ["L. Rutkowski"], "venue": "Methods and Techniques. Berlin: Springer-Verlag", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Computational Intelligence", "author": ["C.L. Mumford", "L.C. Jain"], "venue": "Berlin: Springer-Verlag", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Computational Intelligence", "author": ["R. Kruse", "C. Borgelt", "F. Klawonn", "C. Moewes", "M. Steinbrecher", "P. Held"], "venue": "A Methodological Introduction. Berlin: Springer-Verlag", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural Networks and Statistical Learning", "author": ["K.-L. Du", "M.N.S. Swamy"], "venue": "London: Springer-Verlag", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "T", "author": ["T. Yamakawa", "E. Uchino"], "venue": "Miki and H. Kusanagi, \u201cA neo fuzzy neuron and its applications to system identification and prediction of the system behavior,\u201d Proc. 2nd Int. Conf. on Fuzzy Logic and Neural Networks, pp. 477-483", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Soft computing based signal prediction", "author": ["E. Uchino", "T. Yamakawa"], "venue": "restoration and filtering,\u201d in Intelligent Hybrid Systems: Fuzzy Logic, Neural Networks and Genetic Algorithms, Boston: Kluwer Academic Publisher", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Analog implementation of neo-fuzzy neuron and its on-board learning,", "author": ["T. Miki", "T. Yamakawa"], "venue": "Computational Intelligence and Applications, Piraeus: WSES Press,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Fuzzy identification of systems and its application to modeling and control", "author": ["T. Takagi T", "M. Sugeno"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics, no", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1985}, {"title": "Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and Machine Intelligence", "author": ["J-S.R. Jang", "C.T. Sun", "E. Mizutani"], "venue": "New Jersey: Prentice Hall", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Adaptive nonlinear control using neo-fuzzy model\u201d, in Sinergies Between Information Processing and Automation, pp", "author": ["Ye. Bodyanskiy", "V. Kolodyazhniy"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "A self-training robust neo-fuzzy controller with constraints on control actions", "author": ["Ye. Bodyanskiy", "I. Kokshenev", "V. Kolodyazhniy", "P. Otto"], "venue": "50th Int. Wiss. Koll. Tagungsband, ss", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Constructing fuzzy-controllers with B-spline models \u2013 Principles and Applications", "author": ["J. Zhang", "H. Knoll"], "venue": "Int. J. of Intelligent Systems, no. 13", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Bodyanskiy, \u201cCascaded multiresolution spline-based fuzzy neural network", "author": ["V. Kolodyazhniy", "Ye"], "venue": "Proc. Int. Symp. on Evolving Intelligent Systems, pp", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "An adaptive learning algorithm for a neo-fuzzy neuron", "author": ["Ye. Bodyanskiy", "I. Kokshenev", "V. Kolodyazhniy"], "venue": "Proc. 3rd Int. Conf. of European Union Soc. for Fuzzy Logic and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Neural Networks: A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "Upper Saddle River, New Jersey: Prentice Hall", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Discrete time stochastic adaptive control", "author": ["G.C. Goodwin", "P.J. Ramage", "P.E. Caines"], "venue": "SIAM J. Control and Optimization, no. 19", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1981}, {"title": "Fuzzy basis functions", "author": ["L.X. Wang", "J.M. Mendel"], "venue": "universal approximation and orthogonal least squares learning\u201d, in IEEE Trans. on Neural Networks, no. 3", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Adaptive Fuzzy Systems and Control", "author": ["L.X. Wang"], "venue": "Design and Stability Analysis. Upper Saddle River, New Jersey: Prentice Hall", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Sieci neuronowe do przetwarzania informacji", "author": ["S. Osowski"], "venue": "Warszawa: Oficijna Wydawnicza Politechniki Warszawskiej", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Oscillation and chaos in physiological control systems", "author": ["M.C. Mackey", "L. Glass"], "venue": "Science, no. 197", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1977}, {"title": "Identification and control of dynamic systems using neural networks", "author": ["K.S. Narendra", "K. Parthasarathy"], "venue": "IEEE Trans. on Neural Networks, no. 1", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Artificial neural networks (ANNs) are currently widely used for solving different Data Mining tasks due to their approximating capabilities and their ability to learn from experimental data [1-3].", "startOffset": 203, "endOffset": 208}, {"referenceID": 1, "context": "INTRODUCTION Artificial neural networks (ANNs) are currently widely used for solving different Data Mining tasks due to their approximating capabilities and their ability to learn from experimental data [1-3].", "startOffset": 203, "endOffset": 208}, {"referenceID": 2, "context": "INTRODUCTION Artificial neural networks (ANNs) are currently widely used for solving different Data Mining tasks due to their approximating capabilities and their ability to learn from experimental data [1-3].", "startOffset": 203, "endOffset": 208}, {"referenceID": 4, "context": "Hybrid systems of computational intelligence [5-7], and above all neuro-fuzzy systems (NFSs), combining the advantages of ANNs and fuzzy inference systems (FISs), do not suffer from the \u201ccurse of dimensionality\u201d and provide linguistic interpretability and transparency of the results.", "startOffset": 45, "endOffset": 50}, {"referenceID": 5, "context": "Hybrid systems of computational intelligence [5-7], and above all neuro-fuzzy systems (NFSs), combining the advantages of ANNs and fuzzy inference systems (FISs), do not suffer from the \u201ccurse of dimensionality\u201d and provide linguistic interpretability and transparency of the results.", "startOffset": 45, "endOffset": 50}, {"referenceID": 6, "context": "Hybrid systems of computational intelligence [5-7], and above all neuro-fuzzy systems (NFSs), combining the advantages of ANNs and fuzzy inference systems (FISs), do not suffer from the \u201ccurse of dimensionality\u201d and provide linguistic interpretability and transparency of the results.", "startOffset": 45, "endOffset": 50}, {"referenceID": 7, "context": "THE NEO-FUZZY NEURON ARCHITECTURE To overcome the above mentioned problems, a neuro-fuzzy system called by the authors a \u201cneo-fuzzy neuron (NFN)\u201d was introduced in [8 \u2013 10].", "startOffset": 164, "endOffset": 172}, {"referenceID": 9, "context": "THE NEO-FUZZY NEURON ARCHITECTURE To overcome the above mentioned problems, a neuro-fuzzy system called by the authors a \u201cneo-fuzzy neuron (NFN)\u201d was introduced in [8 \u2013 10].", "startOffset": 164, "endOffset": 172}, {"referenceID": 10, "context": ", li w l h \uf03d , (4) which means that a nonlinear synapse actually implements the zero-order Takagi-Sugeno fuzzy inference [11, 12].", "startOffset": 121, "endOffset": 129}, {"referenceID": 11, "context": ", li w l h \uf03d , (4) which means that a nonlinear synapse actually implements the zero-order Takagi-Sugeno fuzzy inference [11, 12].", "startOffset": 121, "endOffset": 129}, {"referenceID": 7, "context": "The NFN authors [8\u201310] used traditional triangular constructions as membership functions that meet the unity partition conditions", "startOffset": 16, "endOffset": 22}, {"referenceID": 8, "context": "The NFN authors [8\u201310] used traditional triangular constructions as membership functions that meet the unity partition conditions", "startOffset": 16, "endOffset": 22}, {"referenceID": 9, "context": "The NFN authors [8\u201310] used traditional triangular constructions as membership functions that meet the unity partition conditions", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "where li c stands for arbitrarily selected (usually uniformly distributed) centers of the membership functions in the interval [0, 1] , thus, naturally 0 1 i x \uf0a3 \uf0a3 .", "startOffset": 127, "endOffset": 133}, {"referenceID": 12, "context": "(7) Just exactly this circumstance allows to synthesize simple and effective adaptive controllers for nonlinear control objects [13, 14].", "startOffset": 128, "endOffset": 136}, {"referenceID": 13, "context": "(7) Just exactly this circumstance allows to synthesize simple and effective adaptive controllers for nonlinear control objects [13, 14].", "startOffset": 128, "endOffset": 136}, {"referenceID": 14, "context": "One can use other membership functions except triangular constructions, first of all, B-splines [15] that proved their effectiveness in the neo-fuzzy neurons [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "One can use other membership functions except triangular constructions, first of all, B-splines [15] that proved their effectiveness in the neo-fuzzy neurons [16].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "A special algorithm was proposed in [17] to accelerate the NFN learning procedure which has both tracking (for non-stationary signal processing) and filtering properties (for \"noisy\" data processing) \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "When 0 \uf061 \uf03d , the algorithm (13) is identical to the one-step Kaczmarz-Widrow-Hoff learning algorithm [18] and when 1 \uf061 \uf03d \u2013 to the Goodwin-Ramage-Caines stochastic approximation algorithm [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "When 0 \uf061 \uf03d , the algorithm (13) is identical to the one-step Kaczmarz-Widrow-Hoff learning algorithm [18] and when 1 \uf061 \uf03d \u2013 to the Goodwin-Ramage-Caines stochastic approximation algorithm [19].", "startOffset": 187, "endOffset": 191}, {"referenceID": 19, "context": "AN EXTENDED NEO-FUZZY NEURON As mentioned above, the NFN nonlinear synapse i NS implements the zero-order Takagi-Sugeno inference thus being the elementary Wang-Mendel neuro-fuzzy system [20\u201322].", "startOffset": 187, "endOffset": 194}, {"referenceID": 20, "context": "AN EXTENDED NEO-FUZZY NEURON As mentioned above, the NFN nonlinear synapse i NS implements the zero-order Takagi-Sugeno inference thus being the elementary Wang-Mendel neuro-fuzzy system [20\u201322].", "startOffset": 187, "endOffset": 194}, {"referenceID": 21, "context": "AN EXTENDED NEO-FUZZY NEURON As mentioned above, the NFN nonlinear synapse i NS implements the zero-order Takagi-Sugeno inference thus being the elementary Wang-Mendel neuro-fuzzy system [20\u201322].", "startOffset": 187, "endOffset": 194}, {"referenceID": 22, "context": "EXPERIMENT AND ANALYSIS To demonstrate the efficiency of the proposed adaptive neuro-fuzzy system and its learning procedure (22), we have implemented a simulation test based on forecasting of a chaotic process defined by the Mackey-Glass equation [23] \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 10 0.", "startOffset": 248, "endOffset": 252}, {"referenceID": 23, "context": "The first Narendra object [24] is the dynamic plant identification problem and is described by the equation \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 2 1 / 1 y k y k y k f k \uf02b \uf03d \uf02b \uf02b (24) where", "startOffset": 26, "endOffset": 30}], "year": 2016, "abstractText": "A modification of the neo-fuzzy neuron is proposed (an extended neo-fuzzy neuron (ENFN)) that is characterized by improved approximating properties. An adaptive learning algorithm is proposed that has both tracking and smoothing properties and solves prediction, filtering and smoothing tasks of non-stationary \u201cnoisy\u201d stochastic and chaotic signals. An ENFN distinctive feature is its computational simplicity compared to other artificial neural networks and neuro-fuzzy systems.", "creator": "PScript5.dll Version 5.2.2"}}}