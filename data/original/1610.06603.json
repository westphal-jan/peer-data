{"id": "1610.06603", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Combinatorial Multi-Armed Bandit with General Reward Functions", "abstract": "In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the $\\max()$ function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve $O(\\log T)$ distribution-dependent regret and $\\tilde{O}(\\sqrt{T})$ distribution-independent regret, where $T$ is the time horizon. We apply our results to the $K$-MAX problem and expected utility maximization problems. In particular, for $K$-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first $\\tilde{O}(\\sqrt T)$ bound on the $(1-\\epsilon)$-approximation regret of its online problem, for any $\\epsilon&gt;0$.", "histories": [["v1", "Thu, 20 Oct 2016 20:54:41 GMT  (402kb)", "https://arxiv.org/abs/1610.06603v1", "to appear in NIPS 2016"], ["v2", "Tue, 31 Jan 2017 18:18:05 GMT  (490kb)", "http://arxiv.org/abs/1610.06603v2", "published in Neural Information Processing Systems (NIPS) 2016"], ["v3", "Wed, 1 Feb 2017 04:49:22 GMT  (490kb)", "http://arxiv.org/abs/1610.06603v3", "published in Neural Information Processing Systems (NIPS) 2016"]], "COMMENTS": "to appear in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["wei chen", "wei hu", "fu li", "jian li", "yu liu", "pinyan lu"], "accepted": true, "id": "1610.06603"}, "pdf": {"name": "1610.06603.pdf", "metadata": {"source": "CRF", "title": "Combinatorial Multi-Armed Bandit with General Reward Functions", "authors": ["Wei Chen", "Wei Hu", "Fu Li", "Jian Li", "Yu Liu", "Pinyan Lu"], "emails": ["weic@microsoft.com.", "huwei@cs.princeton.edu.", "fuli.theory.research@gmail.com.", "lapordge@gmail.com."], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n06 60\n3v 3\n[ cs\n.L G\n] 1\nF eb\n2 01\n\u221a T )\ndistribution-independent regret, where T is the time horizon. We apply our results to the K-MAX problem and expected utility maximization problems. In particular, for K-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first O\u0303( \u221a T ) bound on the (1 \u2212 \u01eb)approximation regret of its online problem, for any \u01eb > 0."}, {"heading": "1 Introduction", "text": "Stochastic multi-armed bandit (MAB) is a classical online learning problem typically specified as a player againstm machines or arms. Each arm, when pulled, generates a random reward following an unknown distribution. The task of the player is to select one arm to pull in each round based on the historical rewards she collected, and the goal is to collect cumulative reward over multiple rounds as much as possible. In this paper, unless otherwise specified, we use MAB to refer to stochastic MAB.\nMAB problem demonstrates the key tradeoff between exploration and exploitation: whether the player should stick to the choice that performs the best so far, or should try some less explored alternatives that may provide better rewards. The performance measure of an MAB strategy is its cumulative regret, which is defined as the difference between the cumulative reward obtained by always playing the arm with the largest expected reward and the cumulative reward achieved by the learning strategy. MAB and its variants have been extensively studied in the literature, with classical results such as tight \u0398(logT ) distribution-dependent and \u0398( \u221a T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].\nAn important extension to the classical MAB problem is combinatorial multi-armed bandit (CMAB). In CMAB, the player selects not just one arm in each round, but a subset of arms or a combinatorial\n\u2217Microsoft Research, email: weic@microsoft.com. The authors are listed in alphabetical order. \u2020Princeton University, email: huwei@cs.princeton.edu. \u2021The University of Texas at Austin, email: fuli.theory.research@gmail.com. \u00a7Tsinghua University, email: lapordge@gmail.com. \u00b6Tsinghua University, email: liuyujyyz@gmail.com. \u2016Shanghai University of Finance and Economics, email: lu.pinyan@mail.shufe.edu.cn.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nobject in general, referred to as a super arm, which collectively provides a random reward to the player. The reward depends on the outcomes from the selected arms. The player may observe partial feedbacks from the selected arms to help her in decision making. CMAB has wide applications in online advertising, online recommendation, wireless routing, dynamic channel allocations, etc., because in all these settings the action unit is a combinatorial object (e.g. a set of advertisements, a set of recommended items, a route in a wireless network, and an allocation between channels and users), and the reward depends on unknown stochastic behaviors (e.g. users\u2019 click through behaviors, wireless transmission quality, etc.). Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].\nMost of these studies focus on linear reward functions, for which the expected reward for playing a super arm is a linear combination of the expected outcomes from the constituent base arms. Even for studies that do generalize to non-linear reward functions, they typically still assume that the expected reward for choosing a super arm is a function of the expected outcomes from the constituent base arms in this super arm [8, 17]. However, many natural reward functions do not satisfy this property. For example, for the functionmax(), which takes a group of variables and outputs the maximum one among them, its expectation depends on the full distributions of the input random variables, not just their means. Function max() and its variants underly many applications. As an illustrative example, we consider the following scenario in auctions: the auctioneer is repeatedly selling an item to m bidders; in each round the auctioneer selects K bidders to bid; each of the K bidders independently draws her bid from her private valuation distribution and submits the bid; the auctioneer uses the first-price auction to determine the winner and collects the largest bid as the payment.1 The goal of the auctioneer is to gain as high cumulative payments as possible. We refer to this problem as the K-MAX bandit problem, which cannot be effectively solved in the existing CMAB framework.\nBeyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4]. The problem can be formulated as maximizing E[u( \u2211\ni\u2208S Xi)] among all feasible sets S, where Xi\u2019s are independent random variables and u(\u00b7) is a utility function. For example, Xi could be the random delay of edge ei in a routing graph, S is a routing path in the graph, and the objective is maximizing the utility obtained from any routing path, and typically the shorter the delay, the larger the utility. The utility function u(\u00b7) is typically nonlinear to model risk-averse or risk-prone behaviors of users (e.g. a concave utility function is often used to model risk-averse behaviors). The non-linear utility function makes the objective function much more complicated: in particular, it is no longer a function of the means of the underlying random variables Xi\u2019s. When the distributions of Xi\u2019s are unknown, we can turn EUM into an online learning problem where the distributions of Xi\u2019s need to be learned over time from online feedbacks, and we want to maximize the cumulative reward in the learning process. Again, this is not covered by the existing CMAB framework since only learning the means of Xi\u2019s is not enough.\nIn this paper, we generalize the existing CMAB framework with semi-bandit feedbacks to handle general reward functions, where the expected reward for playing a super arm may depend more than just the means of the base arms, and the outcome distribution of a base arm can be arbitrary. This generalization is non-trivial, because almost all previous works on CMAB rely on estimating the expected outcomes from base arms, while in our case, we need an estimation method and an analytical tool to deal with the whole distribution, not just its mean. To this end, we turn the problem into estimating the cumulative distribution function (CDF) of each arm\u2019s outcome distribution. We use stochastically dominant confidence bound (SDCB) to obtain a distribution that stochastically dominates the true distribution with high probability, and hence we also name our algorithm SDCB. We are able to show O(log T ) distribution-dependent and O\u0303( \u221a T ) distribution-independent regret bounds in T rounds. Furthermore, we propose a more efficient algorithm called Lazy-SDCB, which first executes a discretization step and then applies SDCB on the discretized problem. We show that Lazy-SDCB also achieves O\u0303( \u221a T ) distribution-independent regret bound. Our regret bounds are tight with respect to their dependencies on T (up to a logarithmic factor for distribution-independent bounds). To make our scheme work, we make a few reasonable assumptions, including boundedness, monotonicity and Lipschitz-continuity2 of the reward function, and independence among base arms. We apply our algorithms to the K-MAX and EUM problems, and provide efficient solutions with\n1We understand that the first-price auction is not truthful, but this example is only for illustrative purpose for the max() function.\n2The Lipschitz-continuity assumption is only made for Lazy-SDCB. See Section 4.\nconcrete regret bounds. Along the way, we also provide the first polynomial time approximation scheme (PTAS) for the offline K-MAX problem, which is formulated as maximizing E[maxi\u2208S Xi] subject to a cardinality constraint |S| \u2264 K , where Xi\u2019s are independent nonnegative random variables.\nTo summarize, our contributions include: (a) generalizing the CMAB framework to allow a general reward function whose expectation may depend on the entire distributions of the input random variables; (b) proposing the SDCB algorithm to achieve efficient learning in this framework with near-optimal regret bounds, even for arbitrary outcome distributions; (c) giving the first PTAS for the offline K-MAX problem. Our general framework treats any offline stochastic optimization algorithm as an oracle, and effectively integrates it into the online learning framework.\nRelated Work. As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions. In particular, Chen et al. [8] look at general non-linear reward functions and Kveton et al. [17] consider specific non-linear reward functions in a conjunctive or disjunctive form, but both papers require that the expected reward of playing a super arm is determined by the expected outcomes from base arms.\nThe only work in combinatorial bandits we are aware of that does not require the above assumption on the expected reward is [15], which is based on a general Thompson sampling framework. However, they assume that the joint distribution of base arm outcomes is from a known parametric family within known likelihood function and only the parameters are unknown. They also assume the parameter space to be finite. In contrast, our general case is non-parametric, where we allow arbitrary bounded distributions. Although in our known finite support case the distribution can be parametrized by probabilities on all supported points, our parameter space is continuous. Moreover, it is unclear how to efficiently compute posteriors in their algorithm, and their regret bounds depend on complicated problem-dependent coefficients which may be very large for many combinatorial problems. They also provide a result on the K-MAX problem, but they only consider Bernoulli outcomes from base arms, much simpler than our case where general distributions are allowed.\nThere are extensive studies on the classical MAB problem, for which we refer to a survey by Bubeck and Cesa-Bianchi [5]. There are also some studies on adversarial combinatorial bandits, e.g. [26, 6]. Although it bears conceptual similarities with stochastic CMAB, the techniques used are different.\nExpected utility maximization (EUM) encompasses a large class of stochastic optimization problems and has been well studied (e.g. [27, 20, 21, 4]). To the best of our knowledge, we are the first to study the online learning version of these problems, and we provide a general solution to systematically address all these problems as long as there is an available offline (approximation) algorithm. The K-MAX problem may be traced back to [13], where Goel et al. provide a constant approximation algorithm to a generalized version in which the objective is to choose a subset S of cost at most K and maximize the expectation of a certain knapsack profit."}, {"heading": "2 Setup and Notation", "text": "Problem Formulation. We model a combinatorial multi-armed bandit (CMAB) problem as a tuple (E,F , D,R), where E = [m] = {1, 2, . . . ,m} is a set of m (base) arms, F \u2286 2E is a set of subsets of E, D is a probability distribution over [0, 1]m, and R is a reward function defined on [0, 1]m \u00d7 F . The arms produce stochastic outcomes X = (X1, X2, . . . , Xm) drawn from distribution D, where the i-th entry Xi is the outcome from the i-th arm. Each feasible subset of arms S \u2208 F is called a super arm. Under a realization of outcomes x = (x1, . . . , xm), the player receives a reward R(x, S) when she chooses the super arm S to play. Without loss of generality, we assume the reward value to be nonnegative. Let K = maxS\u2208F |S| be the maximum size of any super arm. Let X(1), X(2), . . . be an i.i.d. sequence of random vectors drawn from D, where X(t) = (X\n(t) 1 , . . . , X (t) m ) is the outcome vector generated in the t-th round. In the t-th round, the player chooses a super arm St \u2208 F to play, and then the outcomes from all arms in St, i.e., {X(t)i | i \u2208 St}, are revealed to the player. According to the definition of the reward function, the reward value in the t-th round is R(X(t), St). The expected reward for choosing a super arm S in any round is denoted by rD(S) = EX\u223cD[R(X,S)].\nWe also assume that for a fixed super arm S \u2208 F , the reward R(x, S) only depends on the revealed outcomes xS = (xi)i\u2208S . Therefore, we can alternatively express R(x, S) as RS(xS), where RS is a function defined on [0, 1]S .3\nA learning algorithm A for the CMAB problem selects which super arm to play in each round based on the revealed outcomes in all previous rounds. Let SAt be the super arm selected by A in the t-th round.4 The goal is to maximize the expected cumulative reward in T rounds, which is E [\n\u2211T t=1 R(X (t), SAt ) ] = \u2211T t=1 E [ rD(S A t ) ] . Note that when the underlying distribution D is\nknown, the optimal algorithm A\u2217 chooses the optimal super arm S\u2217 = argmaxS\u2208F{rD(S)} in every round. The quality of an algorithm A is measured by its regret in T rounds, which is the difference between the expected cumulative reward of the optimal algorithm A\u2217 and that of A:\nRegAD(T ) = T \u00b7 rD(S\u2217)\u2212 T \u2211\nt=1\nE [ rD(S A t ) ] .\nFor some CMAB problem instances, the optimal super arm S\u2217 may be computationally hard to find even when the distribution D is known, but efficient approximation algorithms may exist, i.e., an \u03b1-approximate (0 < \u03b1 \u2264 1) solution S\u2032 \u2208 F which satisfies rD(S\u2032) \u2265 \u03b1 \u00b7 maxS\u2208F{rD(S)} can be efficiently found given D as input. We will provide the exact formulation of our requirement on such an \u03b1-approximation computation oracle shortly. In such cases, it is not fair to compare a CMAB algorithm A with the optimal algorithm A\u2217 which always chooses the optimal super arm S\u2217. Instead, we define the \u03b1-approximation regret of an algorithm A as\nRegAD,\u03b1(T ) = T \u00b7 \u03b1 \u00b7 rD(S\u2217)\u2212 T \u2211\nt=1\nE [ rD(S A t ) ] .\nAs mentioned, almost all previous work on CMAB requires that the expected reward rD(S) of a super arm S depends only on the expectation vector \u00b5 = (\u00b51, . . . , \u00b5m) of outcomes, where \u00b5i = EX\u223cD[Xi]. This is a strong restriction that cannot be satisfied by a general nonlinear function RS and a general distribution D. The main motivation of this work is to remove this restriction.\nAssumptions. Throughout this paper, we make several assumptions on the outcome distribution D and the reward function R.\nAssumption 1 (Independent outcomes from arms). The outcomes from all m arms are mutually independent, i.e., for X \u223c D, X1, X2, . . . , Xm are mutually independent. We write D as D = D1 \u00d7D2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm, where Di is the distribution of Xi.\nWe remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.\nAssumption 2 (Bounded reward value). There exists M > 0 such that for any x \u2208 [0, 1]m and any S \u2208 F , we have 0 \u2264 R(x, S) \u2264 M . Assumption 3 (Monotone reward function). If two vectors x, x\u2032 \u2208 [0, 1]m satisfy xi \u2264 x\u2032i (\u2200i \u2208 [m]), then for any S \u2208 F , we have R(x, S) \u2264 R(x\u2032, S).\nComputation Oracle for Discrete Distributions with Finite Supports. We require that there exists an \u03b1-approximation computation oracle (0 < \u03b1 \u2264 1) for maximizing rD(S), when each Di (i \u2208 [m]) has a finite support. In this case, Di can be fully described by a finite set of numbers (i.e., its support {vi,1, vi,2, . . . , vi,si} and the values of its cumulative distribution function (CDF) Fi on the supported points: Fi(vi,j) = PrXi\u223cDi [Xi \u2264 vi,j ] (j \u2208 [si])). The oracle takes such a representation of D as input, and can output a super arm S\u2032 = Oracle(D) \u2208 F such that rD(S\u2032) \u2265 \u03b1 \u00b7maxS\u2208F{rD(S)}."}, {"heading": "3 SDCB Algorithm", "text": "3[0, 1]S is isomorphic to [0, 1]|S|; the coordinates in [0, 1]S are indexed by elements in S. 4Note that SAt may be random due to the random outcomes in previous rounds and the possible randomness\nused by A.\nAlgorithm 1 SDCB (Stochastically dominant confidence bound) 1: Throughout the algorithm, for each arm i \u2208 [m], maintain: (i) a counter Ti which stores the\nnumber of times arm i has been played so far, and (ii) the empirical distribution D\u0302i of the observed outcomes from arm i so far, which is represented by its CDF F\u0302i\n2: // Initialization 3: for i = 1 to m do 4: // Action in the i-th round 5: Play a super arm Si that contains arm i 6: Update Tj and F\u0302j for each j \u2208 Si 7: end for\n8: for t = m+ 1,m+ 2, . . . do 9: // Action in the t-th round\n10: For each i \u2208 [m], let Di be a distribution whose CDF Fi is\nFi(x) =\n{ max{F\u0302i(x)\u2212 \u221a\n3 ln t 2Ti , 0}, 0 \u2264 x < 1 1, x = 1\n11: Play the super arm St \u2190 Oracle(D), where D = D1 \u00d7D2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm 12: Update Tj and F\u0302j for each j \u2208 St 13: end for\nWe present our algorithm stochastically dominant confidence bound (SDCB) in Algorithm 1. Throughout the algorithm, we store, in a variable Ti, the number of times the outcomes from arm i are observed so far. We also maintain the empirical distribution D\u0302i of the observed outcomes from arm i so far, which can be represented by its CDF F\u0302i: for x \u2208 [0, 1], the value of F\u0302i(x) is just the fraction of the observed outcomes from arm i that are no larger than x. Note that F\u0302i is always a step function which has \u201cjumps\u201d at the points that are observed outcomes from arm i. Therefore it suffices to store these discrete points as well as the values of F\u0302i at these points in order to store the whole function F\u0302i. Similarly, the later computation of stochastically dominant CDF Fi (line 10) only requires computation at these points, and the input to the offline oracle only needs to provide these points and corresponding CDF values (line 11).\nThe algorithm starts with m initialization rounds in which each arm is played at least once5 (lines 2- 7). In the t-th round (t > m), the algorithm consists of three steps. First, it calculates for each i \u2208 [m] a distribution Di whose CDF Fi is obtained by lowering the CDF F\u0302i (line 10). The second step is to call the \u03b1-approximation oracle with the newly constructed distribution D = D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm as input (line 11), and thus the super arm St output by the oracle satisfies rD(St) \u2265 \u03b1\u00b7maxS\u2208F{rD(S)}. Finally, the algorithm chooses the super arm St to play, observes the outcomes from all arms in St, and updates Tj\u2019s and F\u0302j \u2019s accordingly for each j \u2208 St. The idea behind our algorithm is the optimism in the face of uncertainty principle, which is the key principle behind UCB-type algorithms. Our algorithm ensures that with high probability we have Fi(x) \u2264 Fi(x) simultaneously for all i \u2208 [m] and all x \u2208 [0, 1], where Fi is the CDF of the outcome distribution Di. This means that each Di has first-order stochastic dominance over Di.6 Then from the monotonicity property of R(x, S) (Assumption 3) we know that rD(S) \u2265 rD(S) holds for all S \u2208 F with high probability. Therefore D provides an \u201coptimistic\u201d estimation on the expected reward from each super arm.\nRegret Bounds. We prove O(log T ) distribution-dependent and O( \u221a T log T ) distributionindependent upper bounds on the regret of SDCB (Algorithm 1).\n5Without loss of generality, we assume that each arm i \u2208 [m] is contained in at least one super arm. 6We remark that while Fi(x) is a numerical lower confidence bound on Fi(x) for all x \u2208 [0, 1], at the\ndistribution level, Di serves as a \u201cstochastically dominant (upper) confidence bound\u201d on Di.\nWe call a super arm S bad if rD(S) < \u03b1 \u00b7 rD(S\u2217). For each super arm S, we define \u2206S = max{\u03b1 \u00b7 rD(S\u2217)\u2212 rD(S), 0}.\nLet FB = {S \u2208 F | \u2206S > 0}, which is the set of all bad super arms. Let EB \u2286 [m] be the set of arms that are contained in at least one bad super arm. For each i \u2208 EB, we define\n\u2206i,min = min{\u2206S | S \u2208 FB, i \u2208 S}. Recall that M is an upper bound on the reward value (Assumption 2) and K = maxS\u2208F |S|. Theorem 1. A distribution-dependent upper bound on the \u03b1-approximation regret of SDCB (Algorithm 1) in T rounds is\nM2K \u2211\ni\u2208EB\n2136\n\u2206i,min lnT +\n(\n\u03c02\n3 + 1\n)\n\u03b1Mm,\nand a distribution-independent upper bound is\n93M \u221a mKT lnT + ( \u03c02\n3 + 1\n)\n\u03b1Mm.\nThe proof of Theorem 1 is given in Appendix A.1. The main idea is to reduce our analysis on general reward functions satisfying Assumptions 1-3 to the one in [18] that deals with the summation reward functionR(x, S) = \u2211\ni\u2208S xi. Our analysis relies on the Dvoretzky-Kiefer-Wolfowitz inequality [10, 24], which gives a uniform concentration bound on the empirical CDF of a distribution.\nApplying Our Algorithm to the Previous CMAB Framework. Although our focus is on general reward functions, we note that when SDCB is applied to the previous CMAB framework where the expected reward depends only on the means of the random variables, it can achieve the same regret bounds as the previous combinatorial upper confidence bound (CUCB) algorithm in [8, 18].\nLet \u00b5i = EX\u223cD[Xi] be arm i\u2019s mean outcome. In each round CUCB calculates (for each arm i) an upper confidence bound \u00b5\u0304i on \u00b5i, with the essential property that \u00b5i \u2264 \u00b5\u0304i \u2264 \u00b5i+\u039bi holds with high probability, for some \u039bi > 0. In SDCB, we use Di as a stochastically dominant confidence bound of Di. We can show that \u00b5i \u2264 EYi\u223cDi [Yi] \u2264 \u00b5i + \u039bi holds with high probability, with the same interval length \u039bi as in CUCB. (The proof is given in Appendix A.2.) Hence, the analysis in [8, 18] can be applied to SDCB, resulting in the same regret bounds.We further remark that in this case we do not need the three assumptions stated in Section 2 (in particular the independence assumption on Xi\u2019s): the summation reward case just works as in [18] and the nonlinear reward case relies on the properties of monotonicity and bounded smoothness used in [8]."}, {"heading": "4 Improved SDCB Algorithm by Discretization", "text": "In Section 3, we have shown that our algorithm SDCB achieves near-optimal regret bounds. However, that algorithm might suffer from large running time and memory usage. Note that, in the t-th round, an arm i might have been observed t\u2212 1 times already, and it is possible that all the observed values from arm i are different (e.g., when arm i\u2019s outcome distribution Di is continuous). In such case, it takes \u0398(t) space to store the empirical CDF F\u0302i of the observed outcomes from arm i, and both calculating the stochastically dominant CDF Fi and updating F\u0302i take \u0398(t) time. Therefore, the worst-case space usage of SDCB in T rounds is \u0398(T ), and the worst-case running time is \u0398(T 2) (ignoring the dependence on m and K); here we do not count the time and space used by the offline computation oracle.\nIn this section, we propose an improved algorithm Lazy-SDCB which reduces the worst-case memory usage and running time to O( \u221a T ) andO(T 3/2), respectively, while preserving the O( \u221a T logT ) distribution-independent regret bound. To this end, we need an additional assumption on the reward function:\nAssumption 4 (Lipschitz-continuous reward function). There exists C > 0 such that for any S \u2208 F and any x, x\u2032 \u2208 [0, 1]m, we have |R(x, S)\u2212R(x\u2032, S)| \u2264 C\u2016xS \u2212 x\u2032S\u20161, where \u2016xS \u2212 x\u2032S\u20161 = \u2211\ni\u2208S |xi \u2212 x\u2032i|.\nAlgorithm 2 Lazy-SDCB with known time horizon Input: time horizon T\n1: s \u2190 \u2308 \u221a T \u2309 2: Ij \u2190 { [0, 1s ], j = 1\n( j\u22121s , j s ], j = 2, . . . , s\n3: Invoke SDCB (Algorithm 1) for T rounds, with the following change: whenever observing an outcome x (from any arm), find j \u2208 [s] such that x \u2208 Ij , and regard this outcome as js\nAlgorithm 3 Lazy-SDCB without knowing the time horizon 1: q \u2190 \u2308log2 m\u2309 2: In rounds 1, 2, . . . , 2q, invoke Algorithm 2 with input T = 2q\n3: for k = q, q + 1, q + 2, . . . do 4: In rounds 2k + 1, 2k + 2, . . . , 2k+1, invoke Algorithm 2 with input T = 2k 5: end for\nWe first describe the algorithm when the time horizon T is known in advance. The algorithm is summarized in Algorithm 2. We perform a discretization on the distribution D = D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm to obtain a discrete distribution D\u0303 = D\u03031 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 D\u0303m such that (i) for X\u0303 \u223c D\u0303, X\u03031, . . . , X\u0303m are also mutually independent, and (ii) every D\u0303i is supported on a set of equally-spaced values { 1s , 2s , . . . , 1}, where s is set to be \u2308 \u221a T \u2309. Specifically, we partition [0, 1] into s intervals: I1 = [0, 1s ], I2 = ( 1 s , 2 s ], . . . , Is\u22121 = ( s\u22122 s , s\u22121 s ], Is = ( s\u22121 s , 1], and define D\u0303i as\nPr X\u0303i\u223cD\u0303i [X\u0303i = j/s] = Pr Xi\u223cDi\n[Xi \u2208 Ij ] , j = 1, . . . , s.\nFor the CMAB problem ([m],F , D,R), our algorithm \u201cpretends\u201d that the outcomes are drawn from D\u0303 instead of D, by replacing any outcome x \u2208 Ij by js (\u2200j \u2208 [s]), and then applies SDCB to the problem ([m],F , D\u0303, R). Since each D\u0303i has a known support { 1s , 2s , . . . , 1}, the algorithm only needs to maintain the number of occurrences of each support value in order to obtain the empirical CDF of all the observed outcomes from arm i. Therefore, all the operations in a round can be done using O(s) = O( \u221a T ) time and space, and the total time and space used by Lazy-SDCB are O(T 3/2) and O( \u221a T ), respectively.\nThe discretization parameter s in Algorithm 2 depends on the time horizon T , which is why Algorithm 2 has to know T in advance. We can use the doubling trick to avoid the dependency on T . We present such an algorithm (without knowing T ) in Algorithm 3. It is easy to see that Algorithm 3 has the same asymptotic time and space usages as Algorithm 2.\nRegret Bounds. We show that both Algorithm 2 and Algorithm 3 achieve O( \u221a T logT ) distribution-independent regret bounds. The full proofs are given in Appendix B. Recall that C is the coefficient in the Lipschitz condition in Assumption 4.\nTheorem 2. Suppose the time horizon T is known in advance. Then the \u03b1-approximation regret of Algorithm 2 in T rounds is at most\n93M \u221a mKT lnT + 2CK \u221a T + ( \u03c02\n3 + 1\n)\n\u03b1Mm.\nProof Sketch. The regret consists of two parts: (i) the regret for the discretized CMAB problem ([m],F , D\u0303, R), and (ii) the error due to discretization. We directly apply Theorem 1 for the first part. For the second part, a key step is to show |rD(S)\u2212 rD\u0303(S)| \u2264 CK/s for all S \u2208 F (see Appendix B.1).\nTheorem 3. For any time horizon T \u2265 2, the \u03b1-approximation regret of Algorithm 3 in T rounds is at most\n318M \u221a mKT lnT + 7CK \u221a T + 10\u03b1Mm lnT."}, {"heading": "5 Applications", "text": "We describe the K-MAX problem and the class of expected utility maximization problems as applications of our general CMAB framework.\nThe K-MAX Problem. In this problem, the player is allowed to select at most K arms from the set of m arms in each round, and the reward is the maximum one among the outcomes from the selected arms. In other words, the set of feasible super arms is F = { S \u2286 [m] \u2223 \u2223 |S| \u2264 K }\n, and the reward function is R(x, S) = maxi\u2208S xi. It is easy to verify that this reward function satisfies Assumptions 2, 3 and 4 with M = C = 1.\nNow we consider the corresponding offline K-MAX problem of selecting at most K arms from m independent arms, with the largest expected reward. It can be implied by a result in [14] that finding the exact optimal solution is NP-hard, so we resort to approximation algorithms. We can show, using submodularity, that a simple greedy algorithm can achieve a (1 \u2212 1/e)-approximation. Furthermore, we give the first PTAS for this problem. Our PTAS can be generalized to constraints other than the cardinality constraint |S| \u2264 K , including s-t simple paths, matchings, knapsacks, etc. The algorithms and corresponding proofs are given in Appendix C.\nTheorem 4. There exists a PTAS for the offline K-MAX problem. In other words, for any constant \u01eb > 0, there is a polynomial-time (1\u2212 \u01eb)-approximation algorithm for the offline K-MAX problem.\nWe thus can apply our SDCB algorithm to the K-MAX bandit problem and obtain O(log T ) distribution-dependent and O\u0303( \u221a T ) distribution-independent regret bounds according to Theorem 1, or can apply Lazy-SDCB to get O\u0303( \u221a T ) distribution-independent bound according to Theorem 2 or 3.\nStreeter and Golovin [26] study an online submodular maximization problem in the oblivious adversary model. In particular, their result can cover the stochastic K-MAX bandit problem as a special case, and an O(K \u221a mT logm) upper bound on the (1 \u2212 1/e)-regret can be shown. While the techniques in [26] can only give a bound on the (1 \u2212 1/e)-approximation regret for K-MAX, we can obtain the first O\u0303( \u221a T ) bound on the (1\u2212 \u01eb)-approximation regret for any constant \u01eb > 0, using our PTAS as the offline oracle. Even when we use the simple greedy algorithm as the oracle, our experiments show that SDCB performs significantly better than the algorithm in [26] (see Appendix D).\nExpected Utility Maximization. Our framework can also be applied to reward functions of the form R(x, S) = u( \u2211\ni\u2208S xi), where u(\u00b7) is an increasing utility function. The corresponding offline problem is to maximize the expected utility E[u( \u2211\ni\u2208S xi)] subject to a feasibility constraint S \u2208 F . Note that if u is nonlinear, the expected utility may not be a function of the means of the arms in S. Following the celebrated von Neumann-Morgenstern expected utility theorem, nonlinear utility functions have been extensively used to capture risk-averse or risk-prone behaviors in economics (see e.g., [11]), while linear utility functions correspond to risk-neutrality.\nLi and Deshpande [20] obtain a PTAS for the expected utility maximization (EUM) problem for several classes of utility functions (including for example increasing concave functions which typically indicate risk-averseness), and a large class of feasibility constraints (including cardinality constraint, s-t simple paths, matchings, and knapsacks). Similar results for other utility functions and feasibility constraints can be found in [27, 21, 4]. In the online problem, we can apply our algorithms, using their PTASs as the offline oracle. Again, we can obtain the first tight regret bounds on the (1 \u2212 \u01eb)-approximation regret for any \u01eb > 0, for the class of online EUM problems."}, {"heading": "Acknowledgments", "text": "Wei Chen was supported in part by the National Natural Science Foundation of China (Grant No. 61433014). Jian Li and Yu Liu were supported in part by the National Basic Research Program of China grants 2015CB358700, 2011CBA00300, 2011CBA00301, and the National NSFC grants 61033001, 61361136003. The authors would like to thank Tor Lattimore for referring to us the DKW inequality."}, {"heading": "Appendix", "text": ""}, {"heading": "A Missing Proofs from Section 3", "text": ""}, {"heading": "A.1 Proof of Theorem 1", "text": "We present the proof of Theorem 1 in four steps. In Section A.1.1, we review the L1 distance between two distributions and present a property of it. In Section A.1.2, we review the DvoretzkyKiefer-Wolfowitz (DKW) inequality, which is a strong concentration result for empirical CDFs. In Section A.1.3, we prove some key technical lemmas. Then we complete the proof of Theorem 1 in Section A.1.4."}, {"heading": "A.1.1 The L1 Distance between Two Probability Distributions", "text": "For simplicity, we only consider discrete distributions with finite supports \u2013 this will be enough for our purpose.\nLet P be a probability distribution. For any x, let P (x) = PrX\u223cP [X = x]. We write P = P1\u00d7P2\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Pn if the (multivariate) random variable X \u223c P can be written as X = (X1, X2, . . . , Xn), where X1, . . . , Xn are mutually independent and Xi \u223c Pi (\u2200i \u2208 [n]). For two distributions P and Q, their L1 distance is defined as\nL1(P,Q) = \u2211\nx\n|P (x)\u2212Q(x)|,\nwhere the summation is taken over x \u2208 supp(P ) \u222a supp(Q). The L1 distance has the following property. It is a folklore result and we provide a proof for completeness.\nLemma 1. Let P = P1\u00d7P2\u00d7\u00b7 \u00b7 \u00b7\u00d7Pn andQ = Q1\u00d7Q2\u00d7\u00b7 \u00b7 \u00b7\u00d7Qn be two probability distributions. Then we have\nL1(P,Q) \u2264 n \u2211\ni=1\nL1(Pi, Qi). (1)\nProof. We prove (1) by induction on n.\nWhen n = 2, we have\nL1(P,Q) = \u2211\nx\n\u2211\ny\n|P (x, y)\u2212Q(x, y)|\n= \u2211\nx\n\u2211\ny\n|P1(x)P2(y)\u2212Q1(x)Q2(y)|\n\u2264 \u2211\nx\n\u2211\ny\n(|P1(x)P2(y)\u2212 P1(x)Q2(y)|+ |P1(x)Q2(y)\u2212Q1(x)Q2(y)|)\n= \u2211\nx\nP1(x) \u2211\ny\n|P2(y)\u2212Q2(y)|+ \u2211\ny\nQ2(y) \u2211\nx\n|P1(x)\u2212Q1(x)|\n= 1 \u00b7 L1(P2, Q2) + 1 \u00b7 L1(P1, Q1)\n=\n2 \u2211\ni=1\nL1(Pi, Qi).\nHere the summation is taken over x \u2208 supp(P1) \u222a supp(Q1) and y \u2208 supp(P2) \u222a supp(Q2). Suppose (1) is proved for n = k \u2212 1 (k \u2265 3). When n = k, using the results for n = k \u2212 1 and n = 2, we get\nL1(P,Q) \u2264 k\u22122 \u2211\ni=1\nL1(Pi, Qi) + L1(Pk\u22121 \u00d7 Pk, Qk\u22121 \u00d7Qk)\n\u2264 k\u22122 \u2211\ni=1\nL1(Pi, Qi) + L1(Pk\u22121, Qk\u22121) + L1(Pk, Qk)\n= k \u2211\ni=1\nL1(Pi, Qi).\nThis completes the proof."}, {"heading": "A.1.2 The DKW Inequality", "text": "Consider a distribution D with CDF F (x). Let F\u0302n(x) be the empirical CDF of n i.i.d. samples X1, . . . , Xn drawn from D, i.e., F\u0302n(x) = 1n \u2211n i=1 1{Xi \u2264 x} (x \u2208 R).7 Then we have: Lemma 2 (Dvoretzky-Kiefer-Wolfowitz inequality [10, 24]). For any \u01eb > 0 and any n \u2208 Z+, we have\nPr\n[\nsup x\u2208R\n\u2223 \u2223 \u2223 F\u0302n(x)\u2212 F (x) \u2223 \u2223 \u2223 \u2265 \u01eb ] \u2264 2e\u22122n\u01eb2.\nNote that for any fixed x \u2208 R, from the Chernoff bound we have Pr [\u2223 \u2223 \u2223 F\u0302n(x) \u2212 F (x) \u2223 \u2223 \u2223 \u2265 \u01eb ]\n\u2264 2e\u22122n\u01eb 2\n. The DKW inequality states a stronger guarantee that the Chernoff concentration holds simultaneously for all x \u2208 R."}, {"heading": "A.1.3 Technical Lemmas", "text": "The following lemma describes some properties of the expected reward rP (S) = EX\u223cP [R(X,S)].\nLemma 3. Let P = P1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Pm and P \u2032 = P \u20321 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 P \u2032m be two probability distributions over [0, 1]m. Let Fi and F \u2032i be the CDFs of Pi and P \u2032 i , respectively (i = 1, . . . ,m). Suppose each Pi (i \u2208 [m]) is a discrete distribution with finite support.\n(i) If for any i \u2208 [m], x \u2208 [0, 1] we have F \u2032i (x) \u2264 Fi(x), then for any super arm S \u2208 F , we have rP \u2032(S) \u2265 rP (S).\n(ii) If for any i \u2208 [m], x \u2208 [0, 1] we have Fi(x) \u2212 F \u2032i (x) \u2264 \u039bi (\u039bi > 0), then for any super arm S \u2208 F , we have\nrP \u2032(S)\u2212 rP (S) \u2264 2M \u2211\ni\u2208S\n\u039bi.\nProof. It is easy to see why (i) is true. If we have F \u2032i (x) \u2264 Fi(x) for all i \u2208 [m] and x \u2208 [0, 1], then for all i, P \u2032i has first-order stochastic dominance over Pi. When we change the distribution from Pi into P \u2032i , we are moving some probability mass from smaller values to larger values. Recall that the reward function R(x, S) has a monotonicity property (Assumption 3): if x and x\u2032 are two vectors in [0, 1]m such that xi \u2264 x\u2032i for all i \u2208 [m], then R(x, S) \u2264 R(x\u2032, S) for all S \u2208 F . Therefore we have rP (S) \u2264 rP \u2032 (S) for all S \u2208 F .\nNow we prove (ii). Without loss of generality, we assume S = {1, 2, . . . , n} (n \u2264 m). Let P \u2032\u2032 = P \u2032\u20321 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 P \u2032\u2032m be a distribution over [0, 1]m such that the CDF of P \u2032\u2032i is the following:\nF \u2032\u2032i (x) =\n{\nmax{Fi(x)\u2212 \u039bi, 0}, 0 \u2264 x < 1, 1, x = 1.\n(2)\nIt is easy to see that F \u2032\u2032i (x) \u2264 F \u2032i (x) for all i \u2208 [m] and x \u2208 [0, 1]. Thus from the result in (i) we have\nrP \u2032 (S) \u2264 rP \u2032\u2032(S). (3) 7We use 1{\u00b7} to denote the indicator function, i.e., 1{H} = 1 if an event H happens, and 1{H} = 0 if it\ndoes not happen.\nLet supp(Pi) = {vi,1, vi,2, . . . , vi,si} where 0 \u2264 vi,1 < \u00b7 \u00b7 \u00b7 < vi,si \u2264 1. Define PS = P1 \u00d7 P2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Pn, and define P \u2032S and P \u2032\u2032S similarly. Recall that the reward function R(x, S) can be written as RS(xS) = RS(x1, . . . , xn). Then we have\nrP \u2032\u2032 (S)\u2212 rP (S) = \u2211\nx1,...,xn\nRS(x1, . . . , xn)P \u2032\u2032 S (x1, . . . , xn)\u2212\n\u2211\nx1,...,xn\nRS(x1, . . . , xn)PS(x1, . . . , xn)\n= \u2211\nx1,...,xn\nRS(x1, . . . , xn) \u00b7 (P \u2032\u2032S (x1, . . . , xn)\u2212 PS(x1, . . . , xn))\n\u2264 \u2211\nx1,...,xn\nM \u00b7 |P \u2032\u2032S (x1, . . . , xn)\u2212 PS(x1, . . . , xn)|\n=M \u00b7 L1(P \u2032\u2032S , PS), where the summation is taken over xi \u2208 {vi,1, . . . , vi,si} (\u2200i \u2208 S). Then using Lemma 1 we obtain\nrP \u2032\u2032(S)\u2212 rP (S) \u2264 M \u00b7 \u2211\ni\u2208S\nL1(P \u2032\u2032 i , Pi). (4)\nNow we give an upper bound on L1(P \u2032\u2032i , Pi) for each i. Let Fi,j = Fi(vi,j), F \u2032\u2032 i,j = F \u2032\u2032 i (vi,j), and Fi,0 = F \u2032\u2032 i,0 = 0. We have\nL1(P \u2032\u2032 i , Pi) =\nsi \u2211\nj=1\n|P \u2032\u2032i (vi,j)\u2212 Pi(vi,j)|\n=\nsi \u2211\nj=1\n\u2223 \u2223(F \u2032\u2032i,j \u2212 F \u2032\u2032i,j\u22121)\u2212 (Fi,j \u2212 Fi,j\u22121) \u2223 \u2223\n=\nsi \u2211\nj=1\n\u2223 \u2223(Fi,j \u2212 F \u2032\u2032i,j)\u2212 (Fi,j\u22121 \u2212 F \u2032\u2032i,j\u22121) \u2223 \u2223 .\n(5)\nIn fact, for all 1 \u2264 j < si, we have Fi,j \u2212 F \u2032\u2032i,j \u2265 Fi,j\u22121 \u2212 F \u2032\u2032i,j\u22121. To see this, consider two cases:\n\u2022 If Fi,j < \u039bi, then we have Fi,j\u22121 \u2264 Fi,j < \u039bi. By definition (2) we have F \u2032\u2032i,j = F \u2032\u2032i,j\u22121 = 0. Thus Fi,j \u2212 F \u2032\u2032i,j = Fi,j \u2265 Fi,j\u22121 = Fi,j\u22121 \u2212 F \u2032\u2032i,j\u22121.\n\u2022 If Fi,j \u2265 \u039bi, then by definition (2) we have Fi,j \u2212 F \u2032\u2032i,j = \u039bi \u2265 Fi,j\u22121 \u2212 F \u2032\u2032i,j\u22121.\nTherefore (5) becomes\nL1(P \u2032\u2032 i , Pi) =\nsi\u22121 \u2211\nj=1\n( (Fi,j \u2212 F \u2032\u2032i,j)\u2212 (Fi,j\u22121 \u2212 F \u2032\u2032i,j\u22121) ) + \u2223 \u2223(1\u2212 1)\u2212 (Fi,si\u22121 \u2212 F \u2032\u2032i,si\u22121) \u2223 \u2223\n= Fi,si\u22121 \u2212 F \u2032\u2032i,si\u22121 + \u2223 \u2223Fi,si\u22121 \u2212 F \u2032\u2032i,si\u22121 \u2223 \u2223 = 2 (\nFi,si\u22121 \u2212 F \u2032\u2032i,si\u22121 )\n\u2264 2\u039bi,\n(6)\nwhere the last inequality is due to (2).\nWe complete the proof of the lemma by combining (3), (4) and (6):\nrP \u2032(S)\u2212 rP (S) \u2264 rP \u2032\u2032(S)\u2212 rP (S) \u2264 M \u00b7 \u2211\ni\u2208S\nL1(P \u2032\u2032 i , Pi) \u2264 2M\n\u2211\ni\u2208S\n\u039bi.\nThe following lemma is similar to Lemma 1 in [18]. We will use some additional notation:\n\u2022 For t \u2265 m + 1 and i \u2208 [m], let Ti,t be the value of counter Ti right after the t-th round of SDCB. In other words, Ti,t is the number of observed outcomes from arm i in the first t rounds.\n\u2022 Let St be the super arm selected by SDCB in the t-th round. Lemma 4. Define an event in each round t (m+ 1 \u2264 t \u2264 T ):\nHt = { 0 < \u2206St \u2264 4M \u00b7 \u2211\ni\u2208St\n\u221a\n3 ln t\n2Ti,t\u22121\n}\n. (7)\nThen the \u03b1-approximation regret of SDCB in T rounds is at most\nE\n[\nT \u2211\nt=m+1\n1{Ht}\u2206St\n]\n+\n(\n\u03c02\n3 + 1\n)\n\u03b1Mm.\nProof. Let Fi be the CDF of Di. Let F\u0302i,l be the empirical CDF of the first l observations from arm i. For m+ 1 \u2264 t \u2264 T , define an event\nEt = {\nthere exists i \u2208 [m] such that sup x\u2208[0,1]\n\u2223 \u2223 \u2223 F\u0302i,Ti,t\u22121 (x) \u2212 Fi(x) \u2223 \u2223 \u2223 \u2265 \u221a 3 ln t\n2Ti,t\u22121\n}\n,\nwhich means that the empirical CDF F\u0302i is not close enough to the true CDF Fi at the beginning of the t-th round.\nRecall that we have S\u2217 = argmaxS\u2208F{rD(S)} and \u2206S = max{\u03b1 \u00b7 rD(S\u2217)\u2212 rD(S), 0} (S \u2208 F ). We bound the \u03b1-approximation regret of SDCB as\nRegSDCBD,\u03b1(T ) = T \u2211\nt=1\nE [\u03b1 \u00b7 rD(S\u2217)\u2212 rD(St)] \u2264 T \u2211\nt=1\nE [\u2206St ]\n= E\n[\nm \u2211\nt=1\n\u2206St\n]\n+ E\n[\nT \u2211\nt=m+1\n1{Et}\u2206St\n]\n+ E\n[\nT \u2211\nt=m+1\n1{\u00acEt}\u2206St\n]\n,\n(8)\nwhere \u00acEt is the complement of event Et. We separately bound each term in (8).\n(a) the first term\nThe first term in (8) can be trivially bounded as\nE\n[\nm \u2211\nt=1\n\u2206St\n]\n\u2264 m \u2211\nt=1\n\u03b1 \u00b7 rD(S\u2217) \u2264 m \u00b7 \u03b1M. (9)\n(b) the second term\nBy the DKW inequality we know that for any i \u2208 [m], l \u2265 1, t \u2265 m+ 1 we have\nPr\n[\nsup x\u2208[0,1]\n\u2223 \u2223 \u2223 F\u0302i,l(x)\u2212 Fi(x) \u2223 \u2223 \u2223 \u2265 \u221a 3 ln t\n2l\n]\n\u2264 2e\u22122l\u00b7 3 ln t2l = 2e\u22123 ln t = 2t\u22123.\nTherefore\nE\n[\nT \u2211\nt=m+1\n1{Et} ] \u2264 T \u2211\nt=m+1\nm \u2211\ni=1\nt\u22121 \u2211\nl=1\nPr\n[\n\u2223 \u2223 \u2223 F\u0302i,j,l \u2212 Fi,j \u2223 \u2223 \u2223 \u2265 \u221a 3 ln t\n2l\n]\n\u2264 T \u2211\nt=m+1\nm \u2211\ni=1\nt\u22121 \u2211\nl=1\n2t\u22123\n\u2264 2m T \u2211\nt=m+1\nt\u22122\n\u2264 \u03c0 2\n3 m,\nand then the second term in (8) can be bounded as\nE\n[\nT \u2211\nt=m+1\n1{Et}\u2206St\n]\n\u2264 \u03c0 2\n3 m \u00b7 (\u03b1 \u00b7 rD(S\u2217)) \u2264\n\u03c02\n3 \u03b1Mm. (10)\n(c) the third term\nWe fix t > m and first assume \u00acEt happens. Let ci = \u221a\n3 ln t 2Ti,t\u22121 for each i \u2208 [m]. Since \u00acEt happens, we have\n\u2223 \u2223 \u2223 F\u0302i,Ti,t\u22121(x) \u2212 Fi(x) \u2223 \u2223 \u2223 < ci \u2200i \u2208 [m], x \u2208 [0, 1]. (11)\nRecall that in round t of SDCB (Algorithm 1), the input to the oracle is D = D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm, where the CDF Fi of Di is\nFi(x) =\n{\nmax{F\u0302i,Ti,t\u22121(x) \u2212 ci, 0}, 0 \u2264 x < 1, 1, x = 1.\n(12)\nFrom (11) and (12) we know that Fi(x) \u2264 Fi(x) \u2264 Fi(x) + 2ci for all i \u2208 [m], x \u2208 [0, 1]. Thus, from Lemma 3 (i) we have rD(S) \u2264 rD(S) \u2200S \u2208 F , (13) and from Lemma 3 (ii) we have\nrD(S) \u2264 rD(S) + 2M \u2211\ni\u2208S\n2ci \u2200S \u2208 F . (14)\nAlso, from the fact that the algorithm chooses St in the t-th round, we have\nrD(St) \u2265 \u03b1 \u00b7max S\u2208F {rD(S)} \u2265 \u03b1 \u00b7 rD(S\u2217). (15)\nFrom (13), (14) and (15) we have\n\u03b1 \u00b7 rD(S\u2217) \u2264 \u03b1 \u00b7 rD(S\u2217) \u2264 rD(St) \u2264 rD(St) + 2M \u2211\ni\u2208St\n2ci,\nwhich implies \u2206St \u2264 4M \u2211\ni\u2208St\nci.\nTherefore, when \u00acEt happens, we always have \u2206St \u2264 4M \u2211 i\u2208St ci. In other words,\n\u00acEt =\u21d2 { \u2206St \u2264 4M \u2211\ni\u2208St\n\u221a\n3 ln t\n2Ti,t\u22121\n}\n.\nThis implies\n{\u00acEt,\u2206St > 0} =\u21d2 { 0 < \u2206St \u2264 4M \u2211\ni\u2208St\n\u221a\n3 ln t\n2Ti,t\u22121\n}\n= Ht.\nHence, the third term in (8) can be bounded as\nE\n[\nT \u2211\nt=m+1\n1{\u00acEt}\u2206St\n]\n= E\n[\nT \u2211\nt=m+1\n1{\u00acEt,\u2206St > 0}\u2206St\n] \u2264 E [ T \u2211\nt=m+1\n1{Ht}\u2206St\n]\n. (16)\nFinally, by combining (8), (9), (10) and (16) we have\nRegSDCBD,\u03b1(T ) \u2264 E [ T \u2211\nt=m+1\n1{Ht}\u2206St\n]\n+\n(\n\u03c02\n3 + 1\n)\n\u03b1Mm,\ncompleting the proof of the lemma."}, {"heading": "A.1.4 Finishing the Proof of Theorem 1", "text": "Lemma 4 is very similar to Lemma 1 in [18]. We now apply the counting argument in [18] to finish the proof of Theorem 1.\nFrom Lemma 4 we know that it remains to bound E [\n\u2211T t=m+1 1{Ht}\u2206St\n]\n, where Ht is defined in (7).\nDefine two decreasing sequences of positive constants\n1 = \u03b20 >\u03b21 > \u03b22 > . . .\n\u03b11 > \u03b12 > . . .\nsuch that limk\u2192\u221e \u03b1k = limk\u2192\u221e \u03b2k = 0. We choose {\u03b1k} and {\u03b2k} as in Theorem 4 of [18], which satisfy\n\u221a 6 \u221e \u2211\nk=1\n\u03b2k\u22121 \u2212 \u03b2k\u221a \u03b1k \u2264 1 (17)\nand \u221e \u2211\nk=1\n\u03b1k \u03b2k < 267. (18)\nFor t \u2208 {m+ 1, . . . , T } and k \u2208 Z+, let\nmk,t =\n{\n\u03b1k\n(\n2MK \u2206St\n)2\nlnT \u2206St > 0,\n+\u221e \u2206St = 0,\nand Ak,t = {i \u2208 St | Ti,t\u22121 \u2264 mk,t}.\nThen we define an event Gk,t = {|Ak,t| \u2265 \u03b2kK},\nwhich means \u201cin the t-th round, at least \u03b2kK arms in St had been observed at most mk,t times.\u201d\nLemma 5. In the t-th round (m+ 1 \u2264 t \u2264 T ), if event Ht happens, then there exists k \u2208 Z+ such that event Gk,t happens.\nProof. Assume that Ht happens and that none of G1,t,G2,t, . . . happens. Then |Ak,t| < \u03b2kK for all k \u2208 Z+. Let A0,t = St and A\u0304k,t = St \\Ak,t for k \u2208 Z+\u222a{0}. It is easy to see A\u0304k\u22121,t \u2286 A\u0304k,t for all k \u2208 Z+. Note that limk\u2192\u221e mk,t = 0. Thus there exists N \u2208 Z+ such that A\u0304k,t = St for all k \u2265 N , and then we have St = \u22c3\u221e k=1 ( A\u0304k,t \\ A\u0304k\u22121,t )\n. Finally, note that for all i \u2208 A\u0304k,t, we have Ti,t\u22121 > mk,t. Therefore\n\u2211\ni\u2208St\n1 \u221a\nTi,t\u22121 =\n\u221e \u2211\nk=1\n\u2211\ni\u2208A\u0304k,t\\A\u0304k\u22121,t\n1 \u221a\nTi,t\u22121 \u2264\n\u221e \u2211\nk=1\n\u2211\ni\u2208A\u0304k,t\\A\u0304k\u22121,t\n1 \u221a mk,t\n= \u221e \u2211\nk=1\n\u2223 \u2223A\u0304k,t \\ A\u0304k\u22121,t \u2223 \u2223\n\u221a mk,t\n= \u221e \u2211\nk=1\n|Ak\u22121,t \\Ak,t|\u221a mk,t = \u221e \u2211\nk=1\n|Ak\u22121,t| \u2212 |Ak,t|\u221a mk,t\n= |St|\u221a m1,t +\n\u221e \u2211\nk=1\n|Ak,t| ( 1 \u221a mk+1,t \u2212 1\u221a mk,t )\n< K\n\u221a m1,t\n+ \u221e \u2211\nk=1\n\u03b2kK\n(\n1 \u221a mk+1,t \u2212 1\u221a mk,t\n)\n=\n\u221e \u2211\nk=1\n(\u03b2k\u22121 \u2212 \u03b2k)K\u221a mk,t .\nNote that we assume Ht happens. Then we have\n\u2206St \u2264 4M \u00b7 \u2211\ni\u2208St\n\u221a\n3 ln t\n2Ti,t\u22121 \u2264 2M\n\u221a 6 lnT \u00b7 \u2211\ni\u2208St\n1 \u221a\nTi,t\u22121\n< 2M \u221a 6 lnT \u00b7 \u221e \u2211\nk=1\n(\u03b2k\u22121 \u2212 \u03b2k)K\u221a mk,t = \u221a 6 \u221e \u2211\nk=1\n\u03b2k\u22121 \u2212 \u03b2k\u221a \u03b1k \u00b7\u2206St \u2264 \u2206St ,\nwhere the last inequality is due to (17). We reach a contradiction here. The proof of the lemma is completed.\nBy Lemma 5 we have\nT \u2211\nt=m+1\n1{Ht}\u2206St \u2264 \u221e \u2211\nk=1\nT \u2211\nt=m+1\n1{Gk,t,\u2206St > 0}\u2206St .\nFor i \u2208 [m], k \u2208 Z+, t \u2208 {m+ 1, . . . , T }, define an event Gi,k,t = Gk,t \u2227 {i \u2208 St, Ti,t\u22121 \u2264 mk,t}.\nThen by the definitions of Gk,t and Gi,k,t we have\n1{Gk,t,\u2206St > 0} \u2264 1\n\u03b2kK\n\u2211\ni\u2208EB\n1{Gi,k,t,\u2206St > 0}.\nTherefore T \u2211\nt=m+1\n1{Ht}\u2206St \u2264 \u2211\ni\u2208EB\n\u221e \u2211\nk=1\nT \u2211\nt=m+1\n1{Gi,k,t,\u2206St > 0} \u2206St \u03b2kK .\nFor each arm i \u2208 EB, suppose i is contained in Ni bad super arms SBi,1, SBi,2, . . . , SBi,Ni . Let \u2206i,l = \u2206SB i,l (l \u2208 [Ni]). Without loss of generality, we assume \u2206i,1 \u2265 \u2206i,2 \u2265 . . . \u2265 \u2206i,Ni . Note that \u2206i,Ni = \u2206i,min. For convenience, we also define \u2206i,0 = +\u221e, i.e., \u03b1k ( 2MK \u2206i,0 )2 = 0. Then we have T \u2211\nt=m+1\n1{Ht}\u2206St\n\u2264 \u2211\ni\u2208EB\n\u221e \u2211\nk=1\nT \u2211\nt=m+1\nNi \u2211\nl=1\n1{Gi,k,t, St = SBi,l} \u2206St \u03b2kK\n\u2264 \u2211\ni\u2208EB\n\u221e \u2211\nk=1\nT \u2211\nt=m+1\nNi \u2211\nl=1\n1{Ti,t\u22121 \u2264 mk,t, St = SBi,l} \u2206i,l \u03b2kK\n= \u2211\ni\u2208EB\n\u221e \u2211\nk=1\nT \u2211\nt=m+1\nNi \u2211\nl=1\n1\n{\nTi,t\u22121 \u2264 \u03b1k ( 2MK\n\u2206i,l\n)2\nlnT, St = S B i,l\n}\n\u2206i,l \u03b2kK\n= \u2211\ni\u2208EB\n\u221e \u2211\nk=1\nT \u2211\nt=m+1\nNi \u2211\nl=1\nl \u2211\nj=1\n1\n{\n\u03b1k\n(\n2MK\n\u2206i,j\u22121\n)2\nlnT < Ti,t\u22121 \u2264 \u03b1k ( 2MK\n\u2206i,j\n)2\nlnT, St = S B i,l\n}\n\u2206i,l \u03b2kK\n\u2264 \u2211\ni\u2208EB\n\u221e \u2211\nk=1\nT \u2211\nt=m+1\nNi \u2211\nl=1\nl \u2211\nj=1\n1\n{\n\u03b1k\n(\n2MK\n\u2206i,j\u22121\n)2\nlnT < Ti,t\u22121 \u2264 \u03b1k ( 2MK\n\u2206i,j\n)2\nlnT, St = S B i,l\n}\n\u2206i,j \u03b2kK\n\u2264 \u2211\ni\u2208EB\n\u221e \u2211\nk=1\nT \u2211\nt=m+1\nNi \u2211\nl=1\nNi \u2211\nj=1\n1\n{\n\u03b1k\n(\n2MK\n\u2206i,j\u22121\n)2\nlnT < Ti,t\u22121 \u2264 \u03b1k ( 2MK\n\u2206i,j\n)2\nlnT, St = S B i,l\n}\n\u2206i,j \u03b2kK\n\u2264 \u2211\ni\u2208EB\n\u221e \u2211\nk=1\nT \u2211\nt=m+1\nNi \u2211\nj=1\n1\n{\n\u03b1k\n(\n2MK\n\u2206i,j\u22121\n)2\nlnT < Ti,t\u22121 \u2264 \u03b1k ( 2MK\n\u2206i,j\n)2\nlnT\n}\n\u2206i,j \u03b2kK\n\u2264 \u2211\ni\u2208EB\n\u221e \u2211\nk=1\nNi \u2211\nj=1\n(\n\u03b1k\n(\n2MK\n\u2206i,j\n)2\nlnT \u2212 \u03b1k ( 2MK\n\u2206i,j\u22121\n)2\nlnT\n)\n\u2206i,j \u03b2kK\n=4M2K\n(\n\u221e \u2211\nk=1\n\u03b1k \u03b2k\n)\nlnT \u00b7 \u2211\ni\u2208EB\nNi \u2211\nj=1\n(\n1 \u22062i,j \u2212 1 \u22062i,j\u22121\n)\n\u2206i,j\n\u22641068M2K lnT \u00b7 \u2211\ni\u2208EB\nNi \u2211\nj=1\n(\n1 \u22062i,j \u2212 1 \u22062i,j\u22121\n)\n\u2206i,j ,\nwhere the last inequality is due to (18).\nFinally, for each i \u2208 EB we have Ni \u2211\nj=1\n(\n1 \u22062i,j \u2212 1 \u22062i,j\u22121\n)\n\u2206i,j = 1\n\u2206i,Ni +\nNi\u22121 \u2211\nj=1\n1\n\u22062i,j (\u2206i,j \u2212\u2206i,j+1)\n\u2264 1 \u2206i,Ni +\n\u222b \u2206i,1\n\u2206i,Ni\n1\nx2 dx\n= 2 \u2206i,Ni \u2212 1 \u2206i,1\n< 2\n\u2206i,min .\nIt follows that\nT \u2211\nt=m+1\n1{Ht}\u2206St \u2264 1068M2K lnT \u00b7 \u2211\ni\u2208EB\n2\n\u2206i,min = M2K\n\u2211\ni\u2208EB\n2136\n\u2206i,min lnT. (19)\nCombining (19) with Lemma 4, the distribution-dependent regret bound in Theorem 1 is proved.\nTo prove the distribution-independent bound, we decompose \u2211T t=m+1 1{Ht}\u2206St into two parts: T \u2211\nt=m+1\n1{Ht}\u2206St = T \u2211\nt=m+1\n1{Ht,\u2206St \u2264 \u01eb}\u2206St + T \u2211\nt=m+1\n1{Ht,\u2206St > \u01eb}\u2206St\n\u2264 \u01ebT + T \u2211\nt=m+1\n1{Ht,\u2206St > \u01eb}\u2206St , (20)\nwhere \u01eb > 0 is a constant to be determined. The second term can be bounded in the same way as in the proof of the distribution-dependent regret bound, except that we only consider the case \u2206St > \u01eb. Thus we can replace (19) by\nT \u2211\nt=m+1\n1{Ht,\u2206St > \u01eb}\u2206St \u2264 M2K \u2211\ni\u2208EB,\u2206i,min>\u01eb\n2136 \u2206i,min lnT \u2264 M2Km2136 \u01eb lnT. (21)\nIt follows that T \u2211\nt=m+1\n1{Ht}\u2206St \u2264 \u01ebT +M2Km 2136\n\u01eb lnT.\nFinally, letting \u01eb = \u221a\n2136M2Km lnT T , we get\nT \u2211\nt=m+1\n1{Ht}\u2206St \u2264 2 \u221a 2136M2KmT lnT < 93M \u221a mKT lnT .\nCombining this with Lemma 4, we conclude the proof of the distribution-independent regret bound in Theorem 1.\nAlgorithm 4 CUCB [8, 18] 1: For each arm i, maintain: (i) \u00b5\u0302i, the average of all observed outcomes from arm i so far, and (ii)\nTi, the number of observed outcomes from arm i so far.\n2: // Initialization 3: for i = 1 to m do 4: // Action in the i-th round 5: Play a super arm Si that contains arm i, and update \u00b5\u0302i and Ti. 6: end for\n7: for t = m+ 1,m+ 2, . . . do 8: // Action in the t-th round 9: \u00b5\u0304i \u2190 min{\u00b5\u0302i + \u221a\n3 ln t 2Ti , 1} \u2200i \u2208 [m] 10: Play the super arm St \u2190 Oracle(\u00b5\u0304), where \u00b5\u0304 = (\u00b5\u03041, . . . , \u00b5\u0304m). 11: Update \u00b5\u0302i and Ti for all i \u2208 St. 12: end for"}, {"heading": "A.2 Analysis of Our Algorithm in the Previous CMAB Framework", "text": "We now give an analysis of SDCB in the previous CMAB framework, following our discussion in Section 3. We consider the case in which the expected reward only depends on the means of the random variables. Namely, rD(S) only depends on \u00b5i\u2019s (i \u2208 S), where \u00b5i is arm i\u2019s mean outcome. In this case, we can rewrite rD(S) as r\u00b5(S), where \u00b5 = (\u00b51, . . . , \u00b5m) is the vector of means. Note that the offline computation oracle only needs a mean vector as input.\nWe no longer need the three assumptions (Assumptions 1-3) given in Section 2. In particular, we do not require independence among outcome distributions of all arms (Assumption 1). Although we cannot write D as D = D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm, we still let Di be the outcome distribution of arm i. In this case, Di is the marginal distribution of D in the i-th component.\nWe summarize the CUCB algorithm [8, 18] in Algorithm 4. It maintains the empirical mean \u00b5\u0302i of the outcomes from each arm i, and stores the number of observed outcomes from arm i in a variable Ti. In each round, it calculates an upper confidence bound (UCB) \u00b5\u0304i of \u00b5i, Then it uses the UCB vector \u00b5\u0304 as the input to the oracle, and plays the super arm output by the oracle. In the t-th round (t > m), each UCB \u00b5\u0304i has the key property that\n\u00b5i \u2264 \u00b5\u0304i \u2264 \u00b5i + 2 \u221a 3 ln t\n2Ti,t\u22121 (22)\nholds with high probability. (Recall that Ti,t\u22121 is the value of Ti after t \u2212 1 rounds.) To see this, note that we have |\u00b5i \u2212 \u00b5\u0302i| \u2264 \u221a\n3 ln t 2Ti,t\u22121 with high probability (by Chernoff bound), and then (22)\nfollows from the definition of \u00b5\u0304i in line 9 of Algorithm 4.\nWe prove that the same property as (22) also holds for SDCB. Consider a fixed t > m, and let D = D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm be the input to the oracle in the t-th round of SDCB. Let \u03bdi = EYi\u223cDi [Yi]. We can think that SDCB uses the mean vector \u03bd = (\u03bd1, . . . , \u03bdm) as the input to the oracle used by CUCB. We now show that for each i, we have\n\u00b5i \u2264 \u03bdi \u2264 \u00b5i + 2 \u221a 3 ln t\n2Ti,t\u22121 (23)\nwith high probability.\nTo show (23), we first prove the following lemma.\nLemma 6. Let P andP \u2032 be two distributions over [0, 1]with CDFs F andF \u2032, respectively. Consider two random variables Y \u223c P and Y \u2032 \u223c P \u2032.\n(i) If for all x \u2208 [0, 1] we have F \u2032(x) \u2264 F (x), then we have E[Y ] \u2264 E[Y \u2032]. (ii) If for all x \u2208 [0, 1] we have F (x)\u2212 F \u2032(x) \u2264 \u039b (\u039b > 0), then we have E[Y \u2032] \u2264 E[Y ] + \u039b.\nProof. We have\nE[Y ] =\n\u222b 1\n0\nxdF (x) = (xF (x)) \u2223 \u2223 1 0 \u2212 \u222b 1\n0\nF (x) dx = 1\u2212 \u222b 1\n0\nF (x) dx.\nSimilarly, we have\nE[Y \u2032] = 1\u2212 \u222b 1\n0\nF \u2032(x) dx.\nThen the lemma holds trivially.\nNow we prove (23). According to the DKW inequality, with high probability we have\nFi(x) \u2212 2 \u221a 3 ln t\n2Ti,t\u22121 \u2264 Fi(x) \u2264 Fi(x) (24)\nfor all i \u2208 [m] and x \u2208 [0, 1], where Fi is the CDF of Di used in round t of SDCB, and Fi is the CDF of Di. Suppose (24) holds for all i, x, then for any i, the two distributions Di and Di satisfy the two conditions in Lemma 6, with \u039b = 2 \u221a\n3 ln t 2Ti,t\u22121 ; then from Lemma 6 we know that\n\u00b5i \u2264 \u03bdi \u2264 \u00b5i + 2 \u221a\n3 ln t 2Ti,t\u22121 . Hence we have shown that (23) holds with high probability.\nThe fact that (23) holds with high probability means that the mean of Di is also a UCB of \u00b5i with the same confidence as in CUCB. With this property, the analysis in [8, 18] can also be applied to SDCB, resulting in exactly the same regret bounds."}, {"heading": "B Missing Proofs from Section 4", "text": ""}, {"heading": "B.1 Analysis of the Discretization Error", "text": "The following lemma gives an upper bound on the error due to discretization. Refer to Section 4 for the definition of the discretized distribution D\u0303.\nLemma 7. For any S \u2208 F , we have\n|rD(S)\u2212 rD\u0303(S)| \u2264 CK\ns .\nTo prove Lemma 7, we show a slightly more general lemma which gives an upper bound on the discretization error of the expectation of a Lipschitz continuous function.\nLemma 8. Let g(x) be a Lipschitz continuous function on [0, 1]n such that for any x, x\u2032 \u2208 [0, 1]n, we have |g(x)\u2212 g(x\u2032)| \u2264 C\u2016x\u2212 x\u2032\u20161, where \u2016x\u2212 x\u2032\u20161 = \u2211n i=1 |xi \u2212 x\u2032i|. Let P = P1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Pn be a probability distribution over [0, 1]n. Define another distribution P\u0303 = P\u03031\u00d7\u00b7 \u00b7 \u00b7\u00d7 P\u0303n over [0, 1]n as follows: each P\u0303i (i \u2208 [n]) takes values in { 1s , 2s , . . . , 1}, and\nPr X\u0303i\u223cP\u0303i [X\u0303i = j/s] = Pr Xi\u223cPi\n[Xi \u2208 Ij ] , j \u2208 [s],\nwhere I1 = [0, 1s ], I2 = ( 1 s , 2 s ], . . . , Is\u22121 = ( s\u22122 s , s\u22121 s ], Is = ( s\u22121 s , 1]. Then\n\u2223 \u2223 \u2223 EX\u223cP [g(X)]\u2212 EX\u0303\u223cP\u0303 [g(X\u0303)] \u2223 \u2223 \u2223 \u2264 C \u00b7 n\ns . (25)\nProof. Throughout the proof, we consider X = (X1, . . . , Xn) \u223c P and X\u0303 = (X\u03031, . . . , X\u0303n) \u223c P\u0303 . Let vj = j s (j = 0, 1, . . . , s) and\npi,j = Pr[X\u0303i = vj ] = Pr[Xi \u2208 Ij ] i \u2208 [n], j \u2208 [s].\nWe prove (25) by induction on n.\n(1) When n = 1, we have\nE[g(X1)] = \u2211\nj\u2208[s],p1,j>0\np1,j \u00b7 E [ g(X1) \u2223 \u2223X1 \u2208 Ij ] . (26)\nSince g is continuous, for each j \u2208 [s] such that p1,j > 0, there exists \u03bej \u2208 [vj\u22121, vj ] such that\nE [g(X1)|X1 \u2208 Ij ] = g(\u03bej)\nFrom the Lipschitz continuity of g we have\n|g(vj)\u2212 g(\u03bej)| \u2264 C|vj \u2212 \u03bej | \u2264 C|vj \u2212 vj\u22121| = C\ns .\nHence\n\u2223 \u2223 \u2223 E[g(X1)]\u2212 E[g(X\u03031)] \u2223 \u2223 \u2223 =\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2211\nj\u2208[s],p1,j>0\np1,j \u00b7 E[g(X1)|X1 \u2208 Ij ]\u2212 \u2211\nj\u2208[s],p1,j>0\np1,j \u00b7 g(vj)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2211\nj\u2208[s],p1,j>0\np1,j \u00b7 g(\u03bej)\u2212 \u2211\nj\u2208[s],p1,j>0\np1,j \u00b7 g(vj)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u2211\nj\u2208[s],p1,j>0\np1,j \u00b7 |g(\u03bej)\u2212 g(vj)|\n\u2264 \u2211\nj\u2208[s],p1,j>0\np1,j \u00b7 C\ns\n= C\ns .\nThis proves (25) for n = 1.\n(ii) Suppose (25) is correct for n = 1, 2, . . . , k \u2212 1. Now we prove it for n = k (k \u2265 2). We define two functions on [0, 1]k\u22121:\nh(x1, . . . , xk\u22121) = EXk [g(x1, . . . , xk\u22121, Xk)]\nand\nh\u0303(x1, . . . , xk\u22121) = EX\u0303k [g(x1, . . . , xk\u22121, X\u0303k)].\nFor any fixed x1, . . . , xk\u22121 \u2208 [0, 1], the function g(x1, . . . , xk\u22121, x) on x \u2208 [0, 1] is Lipschitz continuous. Therefore from the result for n = 1 we have\n\u2223 \u2223 \u2223 h(x1, . . . , xk\u22121)\u2212 h\u0303(x1, . . . , xk\u22121) \u2223 \u2223 \u2223 \u2264 C\ns \u2200x1, . . . , xk\u22121 \u2208 [0, 1].\nThen we have \u2223\n\u2223 \u2223 E[g(X)]\u2212 E[g(X\u0303)]\n\u2223 \u2223 \u2223\n= \u2223 \u2223\n\u2223 EX1,...,Xk\u22121 [E[g(X)|X1, . . . , Xk\u22121]]\u2212 E[g(X\u0303)]\n\u2223 \u2223 \u2223\n= \u2223 \u2223\n\u2223 EX1,...,Xk\u22121 [h(X1, . . . , Xk\u22121)]\u2212 E[g(X\u0303)]\n\u2223 \u2223 \u2223\n\u2264 \u2223 \u2223\n\u2223 EX1,...,Xk\u22121 [h(X1, . . . , Xk\u22121)]\u2212 EX1,...,Xk\u22121 [h\u0303(X1, . . . , Xk\u22121)]\n\u2223 \u2223 \u2223\n+ \u2223 \u2223\n\u2223 EX1,...,Xk\u22121 [h\u0303(X1, . . . , Xk\u22121)]\u2212 E[g(X\u0303)]\n\u2223 \u2223 \u2223\n\u2264EX1,...,Xk\u22121 [\u2223 \u2223 \u2223 h(X1, . . . , Xk\u22121)\u2212 h\u0303(X1, . . . , Xk\u22121) \u2223 \u2223 \u2223 ]\n+ \u2223 \u2223\n\u2223 EX1,...,Xk\u22121,X\u0303k\n[g(X1, . . . , Xk\u22121, X\u0303k)]\u2212 E[g(X\u0303)] \u2223 \u2223 \u2223\n\u2264EX1,...,Xk\u22121 [ C\ns\n]\n+ \u2223 \u2223\n\u2223 EX\u0303k\n[ E[g(X1, . . . , Xk\u22121, X\u0303k)|X\u0303k]\u2212 E[g(X\u03031, . . . , X\u0303k\u22121, X\u0303k)|X\u0303k] ]\u2223 \u2223 \u2223\n\u2264 C s + EX\u0303k [\u2223 \u2223 \u2223 E[g(X1, . . . , Xk\u22121, X\u0303k)|X\u0303k]\u2212 E[g(X\u03031, . . . , X\u0303k\u22121, X\u0303k)|X\u0303k] \u2223 \u2223 \u2223 ] = C\ns +\n\u2211\nj\u2208[s],pk,j>0\npk,j \u00b7 \u2223 \u2223 \u2223 E[g(X1, . . . , Xk\u22121, vj)]\u2212 E[g(X\u03031, . . . , X\u0303k\u22121, vj)] \u2223 \u2223 \u2223 .\n(27)\nFor any j \u2208 [s], the function g(x1, . . . , xk\u22121, vj) on (x1, . . . , xk\u22121) \u2208 [0, 1]k\u22121 is Lipschitz continuous. Then from the induction hypothesis at n = k \u2212 1, we have\n\u2223 \u2223 \u2223 E[g(X1, . . . , Xk\u22121, vj)]\u2212 E[g(X\u03031, . . . , X\u0303k\u22121, vj)] \u2223 \u2223 \u2223 \u2264 C(k \u2212 1)\ns \u2200j \u2208 [s]. (28)\nFrom (27) and (28) we have\n\u2223 \u2223 \u2223 E[g(X)]\u2212 E[g(X\u0303)] \u2223 \u2223 \u2223 \u2264 C\ns +\n\u2211\nj\u2208[s],pk,j>0\npk,j \u00b7 C(k \u2212 1)\ns\n= C\ns + C(k \u2212 1) s\n= Ck\ns .\nThis concludes the proof for n = k.\nNow we prove Lemma 7.\nProof of Lemma 7. We have\nrD(S) = EX\u223cD[R(X,S)] = EX\u223cD[RS(XS)] = EXS\u223cDS [RS(XS)],\nwhere XS = (Xi)i\u2208S and DS = (Di)i\u2208S . Similarly, we have\nrD\u0303(S) = EX\u0303S\u223cD\u0303S [RS(X\u0303S)].\nAccording to Assumption 4, the function RS defined on [0, 1]S is Lipschitz continuous. Then from Lemma 8 we have\n|rD(S)\u2212 rD\u0303(S)| = \u2223 \u2223 \u2223 EXS\u223cDS [RS(XS)]\u2212 EX\u0303S\u223cD\u0303S [RS(X\u0303S)] \u2223 \u2223 \u2223 \u2264 C \u00b7 |S| s \u2264 C \u00b7K s .\nThis completes the proof."}, {"heading": "B.2 Proof of Theorem 2", "text": "Proof of Theorem 2. Let S\u2217 = argmaxS\u2208F{rD(S)} and S\u0303\u2217 = argmaxS\u2208F{rD\u0303(S)} be the optimal super arms in problems ([m],F , D,R) and ([m],F , D\u0303, R), respectively. Suppose Algorithm 2 selects super arm St in the t-th round (1 \u2264 t \u2264 T ). Then its \u03b1-approximation regret is bounded as\nReg Alg. 2 D,\u03b1 (T )\n=T \u00b7 \u03b1 \u00b7 rD(S\u2217)\u2212 T \u2211\nt=1\nE [rD(St)]\n=T \u00b7 \u03b1 (\nrD(S \u2217)\u2212 rD\u0303(S\u0303\u2217)\n) + T \u2211\nt=1\nE [rD\u0303(St)\u2212 rD(St)] + ( T \u00b7 \u03b1 \u00b7 rD\u0303(S\u0303\u2217)\u2212 T \u2211\nt=1\nE [rD\u0303(St)]\n)\n\u2264T \u00b7 \u03b1 (rD(S\u2217)\u2212 rD\u0303(S\u2217)) + T \u2211\nt=1\nE [rD\u0303(St)\u2212 rD(St)] + Reg Alg. 1 D\u0303,\u03b1 (T ).\nwhere the inequality is due to rD\u0303(S\u0303 \u2217) \u2265 rD\u0303(S\u2217).\nThen from Lemma 7 and the distribution-independent bound in Theorem 1 we have\nReg Alg. 2 D,\u03b1 (T ) \u2264 T \u00b7 \u03b1 \u00b7\nCK s + T \u00b7 CK s + 93M\n\u221a mKT lnT + ( \u03c02\n3 + 1\n)\n\u03b1Mm\n\u2264 2 \u00b7 CKT s\n+ 93M \u221a mKT lnT + ( \u03c02\n3 + 1\n)\n\u03b1Mm\n\u2264 93M \u221a mKT lnT + 2CK \u221a T + ( \u03c02\n3 + 1\n)\n\u03b1Mm.\n(29)\nHere in the last two inequalities we have used \u03b1 \u2264 1 and s = \u2308 \u221a T \u2309 \u2265 \u221a T . The proof is completed."}, {"heading": "B.3 Proof of Theorem 3", "text": "Proof of Theorem 3. Let n = \u2308log2 T \u2309. Then we have 2n\u22121 < T \u2264 2n. If n \u2264 q = \u2308log2 m\u2309, then T \u2264 2m and the regret in T rounds is at most 2m \u00b7 \u03b1M . The regret bound holds trivially.\nNow we assume n \u2265 q + 1. Using Theorem 2, we have Reg\nAlg. 3 D,\u03b1 (T )\n\u2264RegAlg. 3D,\u03b1 (2n)\n=RegAlg. 2D,\u03b1 (2 q) +\nn\u22121 \u2211\nk=q\nReg Alg. 2 D,\u03b1 (2 k)\n\u2264RegAlg. 2D,\u03b1 (2m) + n\u22121 \u2211\nk=q\nReg Alg. 2 D,\u03b1 (2 k)\n\u2264 2m \u00b7 \u03b1M + n\u22121 \u2211\nk=q\n( 93M \u221a mK \u00b7 2k ln 2k + 2CK \u221a 2k + ( \u03c02\n3 + 1\n)\n\u03b1Mm\n)\n\u2264 2\u03b1Mm+ ( 93M \u221a mK ln 2n\u22121 + 2CK ) \u00b7 n\u22121 \u2211\nk=1\n\u221a 2k + (n\u2212 1) \u00b7 ( \u03c02\n3 + 1\n)\n\u03b1Mm\n\u2264 ( 93M \u221a mK ln 2n\u22121 + 2CK ) \u00b7 \u221a 2n\u221a\n2\u2212 1 +\n(\n\u03c02\n3 + 3\n)\n(n\u2212 1) \u00b7 \u03b1Mm\n\u2264 ( 93M \u221a mK lnT + 2CK ) \u00b7 \u221a 2T\u221a 2\u2212 1 + ( \u03c02 3 + 3 ) log2 T \u00b7 \u03b1Mm\nAlgorithm 5 Greedy-K-MAX 1: S \u2190 \u2205 2: for i = 1 to K do 3: k \u2190 argmaxj\u2208[m]\\S rD(S \u222a {j}) 4: S \u2190 S \u222a {k} 5: end for Output: S\n\u2264 318M \u221a mKT lnT + 7CK \u221a T + 10\u03b1Mm lnT.\nC The Offline K-MAX Problem\nIn this section, we consider the offline K-MAX problem. Recall that we have m independent random variables {Xi}i\u2208[m]. Xi follows the discrete distribution Di with support {vi,1, . . . , vi,si} \u2282 [0, 1], and D = D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm is the joint distribution of X = (X1, . . . , Xm). Let pi,j = Pr[Xi = vi,j ]. Define rD(S) = EX\u223cD[maxi\u2208S Xi] and OPT = maxS:|S|=K rD(S). Our goal is to find (in polynomial time) a subset S \u2286 [m] of cardinality K such that rD(S) \u2265 \u03b1 \u00b7 OPT (for certain constant \u03b1).\nFirst, we show that rD(S) can be calculated in polynomial time given any S \u2286 [m]. Let S = {i1, i2, . . . , in}. Note that for X \u223c D, maxi\u2208S Xi can only take values in the set V (S) = \u22c3\ni\u2208S supp(Di). For any v \u2208 V (S), we have\nPr X\u223cD\n[\nmax i\u2208S Xi = v\n]\n= Pr X\u223cD [Xi1 = v,Xi2 \u2264 v, . . . , Xin \u2264 v] + Pr\nX\u223cD [Xi1 < v,Xi2 = v,Xi3 \u2264 v, . . . , Xin \u2264 v]\n+ \u00b7 \u00b7 \u00b7 + Pr\nX\u223cD [Xi1 < v, . . . , Xin\u22121 < v,Xin = v].\n(30)\nSince Xi1 , . . . , Xin are mutually independent, each probability appearing in (30) can be calculated in polynomial time. Hence for any v \u2208 V (S), PrX\u223cD [maxi\u2208S Xi = v] can be calculated in polynomial time using (30). Then rD(S) can be calculated by\nrD(S) = \u2211\nv\u2208V (S)\nv \u00b7 Pr X\u223cD\n[\nmax i\u2208S Xi = v\n]\nin polynomial time.\nC.1 (1\u2212 1/e)-Approximation\nWe now show that a simple greedy algorithm (Algorithm 5) can find a (1 \u2212 1/e)-approximate solution, by proving the submodularity of rD(S). In fact, this is implied by a slightly more general result [13, Lemma 3.2]. We provide a simple and direct proof for completeness.\nLemma 9. Algorithm 5 can output a subset S such that rD(S) \u2265 (1\u2212 1/e) \u00b7OPT.\nProof. For any x \u2208 [0, 1]m, let fx(S) = maxi\u2208S xi be a set function defined on 2[m]. (Define fx(\u2205) = 0.) We can verify that fx(S) is monotone and submodular:\n\u2022 Monotonicity. For any A \u2286 B \u2286 [m], we have fx(A) = maxi\u2208A xi \u2264 maxi\u2208B xi = fx(B).\n\u2022 Submodularity. For any A \u2286 B \u2286 [m] and any k \u2208 [m] \\ B, there are three cases (note that maxi\u2208A xi \u2264 maxi\u2208B xi):\n(i) If xk \u2264 maxi\u2208A xi, then fx(A \u222a {k})\u2212 fx(A) = 0 = fx(B \u222a {k})\u2212 fx(B).\n(ii) If maxi\u2208A xi < xk \u2264 maxi\u2208B xi, then fx(A \u222a {k})\u2212 fx(A) = xk \u2212maxi\u2208A xi > 0 = fx(B \u222a {k})\u2212 fx(B).\n(iii) If xk > maxi\u2208B xi, then fx(A \u222a {k}) \u2212 fx(A) = xk \u2212 maxi\u2208A xi \u2265 xk \u2212 maxi\u2208B xi = fx(B \u222a {k})\u2212 fx(B).\nTherefore, we always have fx(A \u222a {k})\u2212 fx(A) \u2265 fx(B \u222a {i})\u2212 fx(B). The function fx(S) is submodular.\nFor any S \u2286 [m] we have\nrD(S) =\ns1 \u2211\nj1=1\ns2 \u2211\nj2=1\n\u00b7 \u00b7 \u00b7 sm \u2211\njm=1\nf(v1,j1 ,...,vm,jm )(S)\nm \u220f\ni=1\npi,ji .\nSince each set function f(v1,j1 ,...,vm,jm )(S) is monotone and submodular, rD(S) is a convex combination of monotone submodular functions on 2[m]. Therefore, rD(S) is also a monotone submodular function. According to the classical result on submodular maximization [25], the greedy algorithm can find a (1\u2212 1/e)-approximate solution to maxS\u2286[m],|S|\u2264K{rD(S)}."}, {"heading": "C.2 PTAS", "text": "Now we provide a PTAS for the K-MAX problem. In other words, we give an algorithm which, given any fixed constant 0 < \u03b5 < 1/2, can find a solution S of cardinality |K| such that rD(S) \u2265 (1 \u2212 \u03b5) \u00b7 OPT in polynomial time. We first provide an overview of our approach, and then spell out the details later.\n1. (Discretization) We first transform each Xi to another discrete distribution X\u0303i, such that all X\u0303i\u2019s are supported on a set of size O(1/\u03b52).\n2. (Computing signatures) For each Xi, we can compute from X\u0303i a signature Sig(Xi) which is a vector of size O(1/\u03b52). For a set S, we define its signature Sig(S) to be \u2211\ni\u2208S Sig(Xi). We show that if two sets S1 and S2 have the same signature, their objective values are close (Lemma 12).\n3. (Enumerating signatures) We enumerate all possible signatures (there are polynomial number of them when treating \u03b5 as a constant) and try to find the one which is the signature of a set of size K , and the objective value is maximized."}, {"heading": "C.2.1 Discretization", "text": "We first describe the discretization step. We say that a random variable X follows the Bernoulli distribution B(v, q) if X takes value v with probability q and value 0 with probability 1\u2212 q. For any discrete distribution, we can rewrite it as the maximum of a set of Bernoulli distributions.\nDefinition 1. Let X be a discrete random variable with support {v1, v2, . . . , vs}(v1 < v2 < \u00b7 \u00b7 \u00b7 < vs) and Pr[X = vj ] = pj . We define a set of independent Bernoulli random variables {Zj}j\u2208[s] as\nZj \u223c B ( vj , pj \u2211\nj\u2032\u2264j pj\u2032\n)\n.\nWe call {Zj} the Bernoulli decomposition of Xi. Lemma 10. For a discrete distribution X and its Bernoulli decomposition {Zj}, maxj{Zj} has the same distribution with X .\nProof. We can easily see the following:\nPr[max j\n{Zj} = vi] = Pr[Zi = vi] \u220f\ni\u2032>i\nPr[Zi\u2032 = 0]\n= pi \u2211\ni\u2032\u2264i pi\u2032\n\u220f\nh>i\n(\n1\u2212 ph\u2211 h\u2032\u2264h ph\u2032\n)\nAlgorithm 6 Discretization 1: We first run Greedy-K-MAX to obtain a solution SG and let W = rD(SG). 2: for i = 1 to m do 3: Compute the Bernoulli decomposition {Zi,j}j of Xi. 4: for all Zi,j do 5: Create another Bernoulli variable Z\u0303i,j as follows: 6: if vi,j > W/\u03b5 then 7: Let Z\u0303i,j \u223c B ( W \u03b5 ,E[Zi,j ] \u03b5 W )\n(Case 1) 8: else 9: Let Z\u0303i,j = \u230aZi,j\u03b5W \u230b\u03b5W (Case 2)\n10: end if 11: end for 12: Let X\u0303i = maxj{Z\u0303ij} 13: end for\n= pi \u2211\ni\u2032\u2264i pi\u2032\n\u220f\nh>i\n\u2211\nh\u2032\u2264h\u22121 ph\u2032 \u2211\nh\u2032\u2264h ph\u2032 = pi.\nHence, Pr[maxj{Zj} = vi] = Pr[X = vi] for all i \u2208 [s].\nNow, we describe how to construct the discretization X\u0303i of Xi for all i \u2208 [m]. The pseudocode can be found in Algorithm 6. We first run Greedy-K-MAX to obtain a solution SG. Let W = rD(SG). By Lemma 9, we know that W \u2265 (1 \u2212 1/e)OPT. Then we compute the Bernoulli decomposition {Zi,j}j of Xi. For each Zi,j , we create another Bernoulli variable Z\u0303i,j as follows: Recall that vi,j is the nonzero possible value of Zij . We distinguish two cases. Case 1: If vi,j > W/\u03b5, then we let Z\u0303i,j \u223c B ( W \u03b5 ,E[Zi,j ] \u03b5 W )\n. It is easy to see that E[Z\u0303ij ] = E[Zij ]. Case 2: If vi,j \u2264 W/\u03b5, then we let Z\u0303i,j = \u230aZi,j\u03b5W \u230b\u03b5W. We note that more than one Z\u0303ij\u2019s may have the same support, and all Z\u0303ij \u2019s are supported on DS = {0, \u03b5W, 2\u03b5W, . . . ,W/\u03b5}. Finally, we let X\u0303i = maxj{Z\u0303ij}, which is the discretization of Xi. Since X\u0303i is the maximum of a set of Bernoulli distributions, it is also a discrete distribution supported on DS. We can easily compute Pr[X\u0303i = v] for any v \u2208 DS. Now, we show that the discretization only incurs a small loss in the objective value. The key is to show that we do not lose much in the transformation from Zi,j\u2019s to Z\u0303i,j\u2019s. We prove a slightly more general lemma as follows.\nLemma 11. Consider any set of Bernoulli variables {Zi \u223c B(ai, pi)}1\u2264i\u2264n. Assume that E[maxi\u2208[n] Zi] < cW, where c is a constant such that c\u03b5 < 1/2. For each Zi, we create a Bernoulli variable Z\u0303i in the same way as Algorithm 6. Then the following holds:\nE[maxZi] \u2265 E[max Z\u0303i] \u2265 E[maxZi]\u2212 (2c+ 1)\u03b5W.\nProof. Assume a1 is the largest among all ai\u2019s.\nIf a1 < W/\u03b5, all Z\u0303i are created in Case 2. In this case, it is obvious to have that\nE[maxZi] \u2265 E[max Z\u0303i] \u2265 E[maxZi]\u2212 \u03b5W.\nIf a1 \u2265 W/\u03b5, the proof is slightly more complicated. Let L = {i | ai \u2265 W/\u03b5}. We prove by induction on n (i.e., the number of the variables) the following more general claim:\nE[maxZi] \u2265 E[max Z\u0303i] \u2265 E[maxZi]\u2212 \u03b5W\u2212 c \u2211\ni\u2208L\n\u03b5aipi. (31)\nConsider the base case n = 1. The lemma holds immediately in Case 1 as E[Z1] = E[Z\u03031].\nAssuming the lemma is true for n = k, we show it also holds for n = k + 1. Recall we have Z\u03031 \u223c B(W\u03b5 , \u03b5E[Z1]/W). Thus\nE[max i\u22651 Zi]\u2212 E[max i\u22651 Z\u0303i] =a1p1 + (1\u2212 p1)E[max i\u22652 Zi]\u2212 a1p1 \u2212 (1\u2212 \u03b5E[Z1]/W)E[max i\u22652 Z\u0303i]\n\u2265(1\u2212 p1)E[max i\u22652 Z\u0303i]\u2212 (1\u2212 \u03b5E[Z1]/W)E[max i\u22652 Z\u0303i]\n=(\u03b5a1p1/W\u2212 p1)E[max i\u22652 Z\u0303i] \u2265 0,\nwhere the first inequality follows from the induction hypothesis and the last from a1 \u2265 W/\u03b5. The other direction can be seen as follows:\nE[max i\u22651 Z\u0303i]\u2212 E[max i\u22651 Zi] =a1p1 + (1\u2212 \u03b5E[Z1]/W)E[max i\u22652 Z\u0303i]\u2212 (a1p1 + (1\u2212 p1)E[max i\u22652 Zi])\n\u2265(1 \u2212 \u03b5E[Z1]/W)E[max i\u22652 Zi]\u2212 (1\u2212 p1)E[max i\u22652 Zi]\u2212 \u03b5W \u2212 c \u2211\ni\u2208L\\{1}\n\u03b5aipi\n\u2265(\u2212\u03b5E[Z1]/W)E[max i\u22652 Zi]\u2212 \u03b5W \u2212 c \u2211\ni\u2208L\\{1}\n\u03b5aipi\n\u2265\u2212 \u03b5W \u2212 c \u2211\ni\u2208L\n\u03b5aipi,\nwhere the last inequality holds since E[maxi\u22652 Zi] \u2264 cW. This finishes the proof of (31). Now, we show that \u2211\ni\u2208L aipi \u2264 2W. This can be seen as follows. First, we can see from Markov inequality that Pr[maxZi > W/\u03b5] \u2264 c\u03b5. Equivalently, we have \u220f\ni\u2208L(1\u2212 pi) \u2265 1\u2212 c\u03b5. Then, we can see that\nW \u2265 \u2211\ni\u2208L\nai \u220f\nj<i\n(1 \u2212 pj)pi \u2265 (1\u2212 c\u03b5) \u2211\ni\u2208L\naipi \u2265 1\n2\n\u2211\ni\u2208L\naipi.\nPlugging this into (31), we prove the lemma.\nCorollary 1. For any set S \u2286 [m], suppose E[maxi\u2208S Xi] < cW, where c is a constant such that c\u03b5 < 1/2. Then the following holds:\nE[max i\u2208S Xi] \u2265 E[max i\u2208S X\u0303i] \u2265 E[max i\u2208S Xi]\u2212 (2c+ 1)\u03b5W."}, {"heading": "C.2.2 Signatures", "text": "For each Xi, we have created its discretization X\u0303i = maxj{Z\u0303ij}. Since X\u0303i is a discrete distribution, we can define its Bernoulli decomposition {Yij}j\u2208[h] where h = |DS|. Suppose Yij \u223c B(j\u03b5W, qij). Now, we define the signature of Xi to be the vector Sig(Xi) = (Sig(Xi)1, . . . , Sig(Xi)h) where\nSig(Xi)j = min (\u230a\u2212 ln (1 \u2212 qij) \u03b54/m \u230b , \u230a ln(1/\u03b54) \u03b54/m \u230b) \u00b7 \u03b5 4 m j \u2208 [h].\nFor any set S, define its signature to be\nSig(S) = \u2211\ni\u2208S\nSig(Xi).\nDefine the set SG of signature vectors to be all nonnegative h-dimensional vectors, where each coordinate is an integer multiple of \u03b54/m and at most m ln(1/\u03b54). Clearly, the size of SG is O ( ( m\u03b5\u22124 log(h/\u03b52) )h\u22121 ) = O\u0303(mO(1/\u03b5 2)), which is polynomial for any fixed constant \u03b5 > 0\n(recall h = |DS| = O(1/\u03b52)). Now, we prove the following crucial lemma.\nLemma 12. Consider two sets S1 and S2. If Sig(S1) = Sig(S2), the following holds: \u2223\n\u2223 \u2223 \u2223\nE[max i\u2208S1 X\u0303i]\u2212 E[max i\u2208S2 X\u0303i]\n\u2223 \u2223 \u2223 \u2223 \u2264 O(\u03b5)W.\nAlgorithm 7 PTAS-K-MAX 1: U \u2190 \u2205 2: for all signature vector sg \u2208 SG do 3: Find a set S such that |S| = K and Sig(S) = sg 4: if rD(S) > rD(U) then 5: U \u2190 S 6: end if 7: end for Output: U\nProof. Suppose {Yij}j\u2208[h] is the Bernoulli decomposition of X\u0303i. For any set S, we define Yk(S) = maxi\u2208S Yik (it is the max of a set of Bernoulli distributions). It is not hard to see that Yk(S) has a Bernoulli distribution B(k\u03b5W, pk(S)) with pk(S) = 1 \u2212 \u220f\ni\u2208S(1 \u2212 qik). As Sig(S1) = Sig(S2), we have that\n|pk(S1)\u2212 pk(S2)| = | \u220f\ni\u2208S1\n(1\u2212 qik)\u2212 \u220f\ni\u2208S2\n(1\u2212 qik)|\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 exp ( \u2211\ni\u2208S1\nln(1\u2212 qik) ) \u2212 exp ( \u2211\ni\u2208S2\nln(1\u2212 qik) )\u2223 \u2223 \u2223 \u2223\n\u2223\n\u2264 2\u03b54 \u2200k \u2208 [h].\nNoticing maxi\u2208S X\u0303i = maxk Yk(S), we have that \u2223\n\u2223 \u2223 \u2223\nE[max i\u2208S1 X\u0303i]\u2212 E[max i\u2208S2 X\u0303i]\n\u2223 \u2223 \u2223 \u2223 = \u2223 \u2223 \u2223 \u2223\nE[max k Yk(S1)]\u2212 E[max k Yk(S2)]\n\u2223 \u2223 \u2223 \u2223\n\u2264W \u03b5\n(\n\u2211\nk\n|pk(S1)\u2212 pk(S2)| )\n\u22644h\u03b53W = O(\u03b5)W where the first inequality follows from Lemma 1.\nFor any signature vector sg, we associate to it a set of random variables {Bk \u223c B(k\u03b5W, 1 \u2212 e\u2212sgk)}hk=1.8 Define the value of sg to be Val(sg) = E[maxk\u2208[h] Bk]. Corollary 2. For any feasible set S with Sig(S) = sg, |E[maxi\u2208S X\u0303i] \u2212 Val(sg)| \u2264 O(\u03b5)W. Moreover, combining with Corollary 1, we have that |E[maxi\u2208S Xi]\u2212 Val(sg)| \u2264 O(\u03b5)W."}, {"heading": "C.2.3 Enumerating Signatures", "text": "Our algorithm enumerates all signature vectors sg in SG. For each sg, we check if we can find a set S of size K such that Sig(S) = sg. This can be done by a standard dynamic program in O\u0303(mO(1/\u03b5\n2)) time as follows: We use Boolean variable R[i][j][sg\u2032] to represent whether signature vector sg\u2032 \u2208 SG can be dominated by i variables in set {X1, . . . , Xj}. The dynamic programming recursion is\nR[i][j][sg\u2032] = R[i][j \u2212 1][sg\u2032] \u2227R[i\u2212 1][j \u2212 1][sg\u2032 \u2212 Sig(Xj)].\nIf the answer is yes (i.e., we can find such S), we say sg is a feasible signature vector and S is a candidate set. Finally, we pick the candidate set with maximum rD(S) and output the set. The pseudocode can be found in Algorithm 7.\nNow, we are ready to prove Theorem 4 by showing Algorithm 7 is a PTAS for the K-MAX problem.\nProof of Theorem 4. Suppose S\u2217 is the optimal solution and sg\u2217 is the signature of S\u2217. By Corollary 2, we have that |OPT\u2212 Val(sg\u2217)| \u2264 O(\u03b5)W.\n8 It is not hard to see the signature of maxk\u2208[h] Bk is exactly sg.\nAlgorithm 8 Online Submodular Maximization [26] 1: Let A1,A2, . . . ,AK be K instances of Exp3 2: for t = 1, 2, . . . do 3: // Action in the t-th round 4: for i = 1 to K do 5: Use Ai to select an arm at,i \u2208 [m] 6: end for 7: Play the super arm St \u2190 \u22c3K i=1{at,i}\n8: for i = 1 to K do 9: Feed back ft( \u22c3i j=1{at,j})\u2212 ft( \u22c3i\u22121 j=1{at,j}) as the payoff Ai receives for choosing at,i\n10: end for 11: end for\nWhen Algorithm 7 is enumerating sg\u2217, it can find a set S such that Sig(S) = sg\u2217 (there exists at least one such set since S\u2217 is one). Therefore, we can see that\n|E[max i\u2208S Xi]\u2212 E[max i\u2208S\u2217 Xi]| \u2264 |Val(sg\u2217)\u2212max i\u2208S Xi|+ |Val(sg\u2217)\u2212 E[max i\u2208S\u2217 Xi]| \u2264 O(\u03b5)W.\nLet U be the output of Algorithm 7. Since W \u2265 (1 \u2212 1/e)OPT, we have rD(U) \u2265 rD(S) = E[maxi\u2208S Xi] \u2265 (1 \u2212O(\u03b5))OPT. The running time of the algorithm is polynomial for a fixed constant \u03b5 > 0, since the number of signature vectors is polynomial and the dynamic program in each iteration also runs in polynomial time. Hence, we have a PTAS for the K-MAX problem.\nRemark. In fact, Theorem 4 can be generalized in the following way: instead of the cardinality constraint |S| \u2264 K , we can have more general combinatorial constraint on the feasible set S. As long as we can execute line 3 in Algorithm 7 in polynomial time, the analysis wound be the same. Using the same trick as in [20], we can extend the dynamic program to a more general class of combinatorial constraints where there is a pseudo-polynomial time for the exact version9 of the deterministic version of the corresponding problem. The class of constraints includes s-t simple paths, knapsacks, spanning trees, matchings, etc."}, {"heading": "D Empirical Comparison between the SDCB Algorithm and Online", "text": "Submodular Maximization on the K-MAX Problem\nWe perform experiments to compare the SDCB algorithm with the online submodular maximization algorithm in [26], on the K-MAX problem.\nOnline Submodular Maximization. First we briefly describe the online submodular maximization problem considered in [26] and the algorithm therein. At the beginning, an oblivious adversary sets a sequence of submodular functions f1, f2, . . . , fT on 2[m], where ft will be used to determine the reward in the t-th round. In the t-th round, if the player selects a feasible super arm St, the reward will be ft(St). This model covers the K-MAX problem as an instance: suppose X(t) = (X\n(t) 1 , . . . , X (t) m ) \u223c D is the outcome vector sampled in the t-th round, then the func-\ntion ft(S) = maxi\u2208S X (t) i is submodular and will determine the reward in the t-th round. We summarize the algorithm in Algorithm 8. It uses K copies of the Exp3 algorithm (see [3] for an introduction). For the K-MAX problem, Algorithm 8 achieves an O(K \u221a mT logm) upper bound on the (1\u2212 1/e)-approximation regret.\nSetup. We set m = 9 and K = 3, i.e., there are 9 arms in total and it is allowed to select at most 3 arms in each round. We compare the performance of SDCB/Lazy-SDCB and the online submodular maximization algorithm on four different distributions. Here we use the greedy algorithm Greedy-K-MAX (Algorithm 5) as the offline oracle.\n9 In the exact version of a problem, we ask for a feasible set S such that total weight of S is exactly a given target value B. For example, in the exact spanning tree problem where each edge has an integer weight, we would like to find a spanning tree of weight exactly B.\nLet Xi \u223c Di (i = 1, . . . , 9). We consider the following distributions. For all of them, the optimal super arm is S\u2217 = {1, 2, 3}.\n\u2022 Distribution 1: All Di\u2019s have the same support {0, 0.2, 0.4, 0.6, 0.8, 1}. For i \u2208 {1, 2, 3}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.1 and Pr[Xi = 1] = 0.5. For i \u2208 {4, 5, 6, . . . , 9}, Pr[Xi = 0] = 0.5 and Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = Pr[Xi = 1] = 0.1. \u2022 Distribution 2: All Di\u2019s have the same support {0, 0.2, 0.4, 0.6, 0.8, 1}. For i \u2208 {1, 2, 3}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.1 and Pr[Xi = 1] = 0.5. For i \u2208 {4, 5, 6, . . . , 9}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.12 and Pr[Xi = 1] = 0.4. \u2022 Distribution 3: All Di\u2019s have the same support {0, 0.2, 0.4, 0.6, 0.8, 1}. For i \u2208 {1, 2, 3}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.1 and Pr[Xi = 1] = 0.5. For i \u2208 {4, 5, 6}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.12 and Pr[Xi = 1] = 0.4. For i \u2208 {7, 8, 9}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.16 and Pr[Xi = 1] = 0.2. \u2022 Distribution 4: All Di\u2019s are continuous distributions on [0, 1]. For i \u2208 {1, 2, 3}, Di is the uniform distribution on [0, 1]. For i \u2208 {4, 5, 6, . . . , 9}, the probability density function (PDF) of Xi is\nf(x) =\n{\n1.2 x \u2208 [0, 0.5], 0.8 x \u2208 (0.5, 1].\nThese distributions represent several different scenarios. Distribution 1 is relatively \u201ceasy\u201d because the suboptimal arms 4-9\u2019s distribution is far away from arms 1-3\u2019s distribution, whereas distribution 2 is \u201chard\u201d since the distribution of arms 4-9 is close to the distribution of arms 1-3. In distribution 3, the distribution of arms 4-6 is close to the distribution of arms 1-3\u2019s, while arms 7-9\u2019s distribution is further away. Distribution 4 is an example of a group of continuous distributions for which Lazy-SDCB is more efficient than SDCB.\nWe use SDCB for distributions 1-3, and Lazy-SDCB (with known time horizon) for distribution 4. Figure 1 shows the regrets of both SDCB and the online submodular maximization algorithm. We plot the 1-approximation regrets instead of the (1 \u2212 1/e)-approximation regrets, since the greedy oracle usually performs much better than its (1 \u2212 1/e)-approximation guarantee. We can see from Figure 1 that our algorithms achieve much lower regrets in all examples."}], "references": [{"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In COLT,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "A utility equivalence theorem for concave functions", "author": ["Anand Bhalgat", "Sanjeev Khanna"], "venue": "In IPCO,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Combinatorial bandits", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Combinatorial pure exploration of multi-armed bandits", "author": ["Shouyuan Chen", "Tian Lin", "Irwin King", "Michael R. Lyu", "Wei Chen"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Combinatorial multi-armed bandit and its extension to probabilistically triggered arms", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan", "Qinshi Wang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Combinatorial bandits revisited", "author": ["Richard Combes", "M. Sadegh Talebi", "Alexandre Proutiere", "Marc Lelarge"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["Aryeh Dvoretzky", "Jack Kiefer", "Jacob Wolfowitz"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1956}, {"title": "The foundations of expected utility", "author": ["P.C. Fishburn"], "venue": "Dordrecht: Reidel,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1982}, {"title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations", "author": ["Yi Gai", "Bhaskar Krishnamachari", "Rahul Jain"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Asking the right questions: Model-driven optimization using probes", "author": ["Ashish Goel", "Sudipto Guha", "Kamesh Munagala"], "venue": "In PODS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "How to probe for an extreme value", "author": ["Ashish Goel", "Sudipto Guha", "Kamesh Munagala"], "venue": "ACM Transactions on Algorithms (TALG),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Thompson sampling for complex online problems", "author": ["Aditya Gopalan", "Shie Mannor", "Yishay mansour"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Matroid bandits: Fast combinatorial optimization with learning", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Hoda Eydgahi", "Brian Eriksson"], "venue": "In UAI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Combinatorial cascading bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesv\u00e1ri"], "venue": "In AISTATS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1985}, {"title": "Maximizing expected utility for stochastic combinatorial optimization problems", "author": ["Jian Li", "Amol Deshpande"], "venue": "In FOCS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Stochastic combinatorial optimization via poisson approximation", "author": ["Jian Li", "Wen Yuan"], "venue": "In STOC,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Combinatorial partial monitoring game with linear feedback and its applications", "author": ["Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui", "Wei Chen"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Stochastic online greedy learning with semi-bandit feedbacks", "author": ["Tian Lin", "Jian Li", "Wei Chen"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "The tight constant in the dvoretzky-kiefer-wolfowitz inequality", "author": ["Pascal Massart"], "venue": "The Annals of Probability,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1990}, {"title": "An analysis of approximations for maximizing submodular set functions \u2013 I", "author": ["George L. Nemhauser", "Laurence A. Wolsey", "Marshall L. Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1978}, {"title": "An online algorithm for maximizing submodular functions", "author": ["Matthew Streeter", "Daniel Golovin"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}], "referenceMentions": [{"referenceID": 18, "context": "MAB and its variants have been extensively studied in the literature, with classical results such as tight \u0398(logT ) distribution-dependent and \u0398( \u221a T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].", "startOffset": 226, "endOffset": 236}, {"referenceID": 1, "context": "MAB and its variants have been extensively studied in the literature, with classical results such as tight \u0398(logT ) distribution-dependent and \u0398( \u221a T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].", "startOffset": 226, "endOffset": 236}, {"referenceID": 0, "context": "MAB and its variants have been extensively studied in the literature, with classical results such as tight \u0398(logT ) distribution-dependent and \u0398( \u221a T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].", "startOffset": 226, "endOffset": 236}, {"referenceID": 11, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 7, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 21, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 14, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 6, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 15, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 17, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 16, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 22, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 8, "context": "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].", "startOffset": 92, "endOffset": 129}, {"referenceID": 7, "context": "Even for studies that do generalize to non-linear reward functions, they typically still assume that the expected reward for choosing a super arm is a function of the expected outcomes from the constituent base arms in this super arm [8, 17].", "startOffset": 234, "endOffset": 241}, {"referenceID": 16, "context": "Even for studies that do generalize to non-linear reward functions, they typically still assume that the expected reward for choosing a super arm is a function of the expected outcomes from the constituent base arms in this super arm [8, 17].", "startOffset": 234, "endOffset": 241}, {"referenceID": 19, "context": "Beyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4].", "startOffset": 126, "endOffset": 141}, {"referenceID": 20, "context": "Beyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4].", "startOffset": 126, "endOffset": 141}, {"referenceID": 3, "context": "Beyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4].", "startOffset": 126, "endOffset": 141}, {"referenceID": 11, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 92, "endOffset": 107}, {"referenceID": 15, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 92, "endOffset": 107}, {"referenceID": 17, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 92, "endOffset": 107}, {"referenceID": 8, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 92, "endOffset": 107}, {"referenceID": 7, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 147, "endOffset": 154}, {"referenceID": 16, "context": "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.", "startOffset": 147, "endOffset": 154}, {"referenceID": 7, "context": "[8] look at general non-linear reward functions and Kveton et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] consider specific non-linear reward functions in a conjunctive or disjunctive form, but both papers require that the expected reward of playing a super arm is determined by the expected outcomes from base arms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The only work in combinatorial bandits we are aware of that does not require the above assumption on the expected reward is [15], which is based on a general Thompson sampling framework.", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "There are extensive studies on the classical MAB problem, for which we refer to a survey by Bubeck and Cesa-Bianchi [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 25, "context": "[26, 6].", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "[26, 6].", "startOffset": 0, "endOffset": 7}, {"referenceID": 19, "context": "[27, 20, 21, 4]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 20, "context": "[27, 20, 21, 4]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 3, "context": "[27, 20, 21, 4]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 12, "context": "The K-MAX problem may be traced back to [13], where Goel et al.", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": ",m} is a set of m (base) arms, F \u2286 2 is a set of subsets of E, D is a probability distribution over [0, 1], and R is a reward function defined on [0, 1] \u00d7 F .", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": ",m} is a set of m (base) arms, F \u2286 2 is a set of subsets of E, D is a probability distribution over [0, 1], and R is a reward function defined on [0, 1] \u00d7 F .", "startOffset": 146, "endOffset": 152}, {"referenceID": 0, "context": "Therefore, we can alternatively express R(x, S) as RS(xS), where RS is a function defined on [0, 1] .", "startOffset": 93, "endOffset": 99}, {"referenceID": 19, "context": "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.", "startOffset": 117, "endOffset": 136}, {"referenceID": 20, "context": "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.", "startOffset": 117, "endOffset": 136}, {"referenceID": 3, "context": "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.", "startOffset": 117, "endOffset": 136}, {"referenceID": 12, "context": "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.", "startOffset": 117, "endOffset": 136}, {"referenceID": 0, "context": "There exists M > 0 such that for any x \u2208 [0, 1] and any S \u2208 F , we have 0 \u2264 R(x, S) \u2264 M .", "startOffset": 41, "endOffset": 47}, {"referenceID": 0, "context": "If two vectors x, x \u2208 [0, 1] satisfy xi \u2264 xi (\u2200i \u2208 [m]), then for any S \u2208 F , we have R(x, S) \u2264 R(x, S).", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "3 SDCB Algorithm [0, 1] is isomorphic to [0, 1]; the coordinates in [0, 1] are indexed by elements in S.", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "3 SDCB Algorithm [0, 1] is isomorphic to [0, 1]; the coordinates in [0, 1] are indexed by elements in S.", "startOffset": 41, "endOffset": 47}, {"referenceID": 0, "context": "3 SDCB Algorithm [0, 1] is isomorphic to [0, 1]; the coordinates in [0, 1] are indexed by elements in S.", "startOffset": 68, "endOffset": 74}, {"referenceID": 0, "context": "We also maintain the empirical distribution D\u0302i of the observed outcomes from arm i so far, which can be represented by its CDF F\u0302i: for x \u2208 [0, 1], the value of F\u0302i(x) is just the fraction of the observed outcomes from arm i that are no larger than x.", "startOffset": 141, "endOffset": 147}, {"referenceID": 0, "context": "Our algorithm ensures that with high probability we have Fi(x) \u2264 Fi(x) simultaneously for all i \u2208 [m] and all x \u2208 [0, 1], where Fi is the CDF of the outcome distribution Di.", "startOffset": 114, "endOffset": 120}, {"referenceID": 0, "context": "We remark that while Fi(x) is a numerical lower confidence bound on Fi(x) for all x \u2208 [0, 1], at the distribution level, Di serves as a \u201cstochastically dominant (upper) confidence bound\u201d on Di.", "startOffset": 86, "endOffset": 92}, {"referenceID": 17, "context": "The main idea is to reduce our analysis on general reward functions satisfying Assumptions 1-3 to the one in [18] that deals with the summation reward functionR(x, S) = \u2211 i\u2208S xi.", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "Our analysis relies on the Dvoretzky-Kiefer-Wolfowitz inequality [10, 24], which gives a uniform concentration bound on the empirical CDF of a distribution.", "startOffset": 65, "endOffset": 73}, {"referenceID": 23, "context": "Our analysis relies on the Dvoretzky-Kiefer-Wolfowitz inequality [10, 24], which gives a uniform concentration bound on the empirical CDF of a distribution.", "startOffset": 65, "endOffset": 73}, {"referenceID": 7, "context": "Although our focus is on general reward functions, we note that when SDCB is applied to the previous CMAB framework where the expected reward depends only on the means of the random variables, it can achieve the same regret bounds as the previous combinatorial upper confidence bound (CUCB) algorithm in [8, 18].", "startOffset": 304, "endOffset": 311}, {"referenceID": 17, "context": "Although our focus is on general reward functions, we note that when SDCB is applied to the previous CMAB framework where the expected reward depends only on the means of the random variables, it can achieve the same regret bounds as the previous combinatorial upper confidence bound (CUCB) algorithm in [8, 18].", "startOffset": 304, "endOffset": 311}, {"referenceID": 7, "context": ") Hence, the analysis in [8, 18] can be applied to SDCB, resulting in the same regret bounds.", "startOffset": 25, "endOffset": 32}, {"referenceID": 17, "context": ") Hence, the analysis in [8, 18] can be applied to SDCB, resulting in the same regret bounds.", "startOffset": 25, "endOffset": 32}, {"referenceID": 17, "context": "We further remark that in this case we do not need the three assumptions stated in Section 2 (in particular the independence assumption on Xi\u2019s): the summation reward case just works as in [18] and the nonlinear reward case relies on the properties of monotonicity and bounded smoothness used in [8].", "startOffset": 189, "endOffset": 193}, {"referenceID": 7, "context": "We further remark that in this case we do not need the three assumptions stated in Section 2 (in particular the independence assumption on Xi\u2019s): the summation reward case just works as in [18] and the nonlinear reward case relies on the properties of monotonicity and bounded smoothness used in [8].", "startOffset": 296, "endOffset": 299}, {"referenceID": 0, "context": "There exists C > 0 such that for any S \u2208 F and any x, x \u2208 [0, 1], we have |R(x, S)\u2212R(x\u2032, S)| \u2264 C\u2016xS \u2212 xS\u20161, where \u2016xS \u2212 xS\u20161 = \u2211 i\u2208S |xi \u2212 xi|.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Specifically, we partition [0, 1] into s intervals: I1 = [0, 1s ], I2 = ( 1 s , 2 s ], .", "startOffset": 27, "endOffset": 33}, {"referenceID": 13, "context": "It can be implied by a result in [14] that finding the exact optimal solution is NP-hard, so we resort to approximation algorithms.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "Streeter and Golovin [26] study an online submodular maximization problem in the oblivious adversary model.", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "While the techniques in [26] can only give a bound on the (1 \u2212 1/e)-approximation regret for K-MAX, we can obtain the first \u00d5( \u221a T ) bound on the (1\u2212 \u01eb)-approximation regret for any constant \u01eb > 0, using our PTAS as the offline oracle.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "Even when we use the simple greedy algorithm as the oracle, our experiments show that SDCB performs significantly better than the algorithm in [26] (see Appendix D).", "startOffset": 143, "endOffset": 147}, {"referenceID": 10, "context": ", [11]), while linear utility functions correspond to risk-neutrality.", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "Li and Deshpande [20] obtain a PTAS for the expected utility maximization (EUM) problem for several classes of utility functions (including for example increasing concave functions which typically indicate risk-averseness), and a large class of feasibility constraints (including cardinality constraint, s-t simple paths, matchings, and knapsacks).", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "Similar results for other utility functions and feasibility constraints can be found in [27, 21, 4].", "startOffset": 88, "endOffset": 99}, {"referenceID": 3, "context": "Similar results for other utility functions and feasibility constraints can be found in [27, 21, 4].", "startOffset": 88, "endOffset": 99}, {"referenceID": 0, "context": "References [1] Jean-Yves Audibert and S\u00e9bastien Bubeck.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Anand Bhalgat and Sanjeev Khanna.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] S\u00e9bastien Bubeck and Nicol\u00f2 Cesa-Bianchi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Shouyuan Chen, Tian Lin, Irwin King, Michael R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Richard Combes, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Ashish Goel, Sudipto Guha, and Kamesh Munagala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Ashish Goel, Sudipto Guha, and Kamesh Munagala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Aditya Gopalan, Shie Mannor, and Yishay mansour.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Tze Leung Lai and Herbert Robbins.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Jian Li and Amol Deshpande.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Jian Li and Wen Yuan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Tian Lin, Jian Li, and Wei Chen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Pascal Massart.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] George L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Matthew Streeter and Daniel Golovin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "7 Then we have: Lemma 2 (Dvoretzky-Kiefer-Wolfowitz inequality [10, 24]).", "startOffset": 63, "endOffset": 71}, {"referenceID": 23, "context": "7 Then we have: Lemma 2 (Dvoretzky-Kiefer-Wolfowitz inequality [10, 24]).", "startOffset": 63, "endOffset": 71}, {"referenceID": 0, "context": "Let P = P1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Pm and P \u2032 = P \u2032 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 P \u2032 m be two probability distributions over [0, 1].", "startOffset": 94, "endOffset": 100}, {"referenceID": 0, "context": "(i) If for any i \u2208 [m], x \u2208 [0, 1] we have F \u2032 i (x) \u2264 Fi(x), then for any super arm S \u2208 F , we have rP \u2032(S) \u2265 rP (S).", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "(ii) If for any i \u2208 [m], x \u2208 [0, 1] we have Fi(x) \u2212 F \u2032 i (x) \u2264 \u039bi (\u039bi > 0), then for any super arm S \u2208 F , we have rP \u2032(S)\u2212 rP (S) \u2264 2M \u2211", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "If we have F \u2032 i (x) \u2264 Fi(x) for all i \u2208 [m] and x \u2208 [0, 1], then for all i, P \u2032 i has first-order stochastic dominance over Pi.", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "Recall that the reward function R(x, S) has a monotonicity property (Assumption 3): if x and x are two vectors in [0, 1] such that xi \u2264 xi for all i \u2208 [m], then R(x, S) \u2264 R(x, S) for all S \u2208 F .", "startOffset": 114, "endOffset": 120}, {"referenceID": 0, "context": "Let P \u2032\u2032 = P \u2032\u2032 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 P \u2032\u2032 m be a distribution over [0, 1] such that the CDF of P \u2032\u2032 i is the following: F \u2032\u2032 i (x) = { max{Fi(x)\u2212 \u039bi, 0}, 0 \u2264 x < 1, 1, x = 1.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "(2) It is easy to see that F \u2032\u2032 i (x) \u2264 F \u2032 i (x) for all i \u2208 [m] and x \u2208 [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 17, "context": "The following lemma is similar to Lemma 1 in [18].", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "there exists i \u2208 [m] such that sup x\u2208[0,1] \u2223", "startOffset": 37, "endOffset": 42}, {"referenceID": 0, "context": "sup x\u2208[0,1] \u2223", "startOffset": 6, "endOffset": 11}, {"referenceID": 0, "context": "\u2223 < ci \u2200i \u2208 [m], x \u2208 [0, 1].", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "(12) From (11) and (12) we know that Fi(x) \u2264 Fi(x) \u2264 Fi(x) + 2ci for all i \u2208 [m], x \u2208 [0, 1].", "startOffset": 86, "endOffset": 92}, {"referenceID": 17, "context": "4 Finishing the Proof of Theorem 1 Lemma 4 is very similar to Lemma 1 in [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "We now apply the counting argument in [18] to finish the proof of Theorem 1.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "We choose {\u03b1k} and {\u03b2k} as in Theorem 4 of [18], which satisfy \u221a 6 \u221e \u2211", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "Algorithm 4 CUCB [8, 18] 1: For each arm i, maintain: (i) \u03bc\u0302i, the average of all observed outcomes from arm i so far, and (ii) Ti, the number of observed outcomes from arm i so far.", "startOffset": 17, "endOffset": 24}, {"referenceID": 17, "context": "Algorithm 4 CUCB [8, 18] 1: For each arm i, maintain: (i) \u03bc\u0302i, the average of all observed outcomes from arm i so far, and (ii) Ti, the number of observed outcomes from arm i so far.", "startOffset": 17, "endOffset": 24}, {"referenceID": 7, "context": "We summarize the CUCB algorithm [8, 18] in Algorithm 4.", "startOffset": 32, "endOffset": 39}, {"referenceID": 17, "context": "We summarize the CUCB algorithm [8, 18] in Algorithm 4.", "startOffset": 32, "endOffset": 39}, {"referenceID": 0, "context": "Let P andP \u2032 be two distributions over [0, 1]with CDFs F andF , respectively.", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "(i) If for all x \u2208 [0, 1] we have F (x) \u2264 F (x), then we have E[Y ] \u2264 E[Y ].", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "(ii) If for all x \u2208 [0, 1] we have F (x)\u2212 F (x) \u2264 \u039b (\u039b > 0), then we have E[Y ] \u2264 E[Y ] + \u039b.", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "3 ln t 2Ti,t\u22121 \u2264 Fi(x) \u2264 Fi(x) (24) for all i \u2208 [m] and x \u2208 [0, 1], where Fi is the CDF of Di used in round t of SDCB, and Fi is the CDF of Di.", "startOffset": 60, "endOffset": 66}, {"referenceID": 7, "context": "With this property, the analysis in [8, 18] can also be applied to SDCB, resulting in exactly the same regret bounds.", "startOffset": 36, "endOffset": 43}, {"referenceID": 17, "context": "With this property, the analysis in [8, 18] can also be applied to SDCB, resulting in exactly the same regret bounds.", "startOffset": 36, "endOffset": 43}, {"referenceID": 0, "context": "Let g(x) be a Lipschitz continuous function on [0, 1] such that for any x, x \u2208 [0, 1], we have |g(x)\u2212 g(x\u2032)| \u2264 C\u2016x\u2212 x\u20161, where \u2016x\u2212 x\u20161 = \u2211n i=1 |xi \u2212 xi|.", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "Let g(x) be a Lipschitz continuous function on [0, 1] such that for any x, x \u2208 [0, 1], we have |g(x)\u2212 g(x\u2032)| \u2264 C\u2016x\u2212 x\u20161, where \u2016x\u2212 x\u20161 = \u2211n i=1 |xi \u2212 xi|.", "startOffset": 79, "endOffset": 85}, {"referenceID": 0, "context": "Let P = P1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Pn be a probability distribution over [0, 1].", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Define another distribution P\u0303 = P\u03031\u00d7\u00b7 \u00b7 \u00b7\u00d7 P\u0303n over [0, 1] as follows: each P\u0303i (i \u2208 [n]) takes values in { 1s , 2s , .", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "We define two functions on [0, 1]: h(x1, .", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": ", xk\u22121 \u2208 [0, 1], the function g(x1, .", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": ", xk\u22121, x) on x \u2208 [0, 1] is Lipschitz continuous.", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": ", xk\u22121 \u2208 [0, 1].", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": ", xk\u22121) \u2208 [0, 1] is Lipschitz continuous.", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "According to Assumption 4, the function RS defined on [0, 1] is Lipschitz continuous.", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": ", vi,si} \u2282 [0, 1], and D = D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7Dm is the joint distribution of X = (X1, .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "For any x \u2208 [0, 1], let fx(S) = maxi\u2208S xi be a set function defined on 2.", "startOffset": 12, "endOffset": 18}, {"referenceID": 24, "context": "According to the classical result on submodular maximization [25], the greedy algorithm can find a (1\u2212 1/e)-approximate solution to maxS\u2286[m],|S|\u2264K{rD(S)}.", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "Algorithm 8 Online Submodular Maximization [26] 1: Let A1,A2, .", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "Using the same trick as in [20], we can extend the dynamic program to a more general class of combinatorial constraints where there is a pseudo-polynomial time for the exact version9 of the deterministic version of the corresponding problem.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "D Empirical Comparison between the SDCB Algorithm and Online Submodular Maximization on the K-MAX Problem We perform experiments to compare the SDCB algorithm with the online submodular maximization algorithm in [26], on the K-MAX problem.", "startOffset": 212, "endOffset": 216}, {"referenceID": 25, "context": "First we briefly describe the online submodular maximization problem considered in [26] and the algorithm therein.", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "It uses K copies of the Exp3 algorithm (see [3] for an introduction).", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "\u2022 Distribution 4: All Di\u2019s are continuous distributions on [0, 1].", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "For i \u2208 {1, 2, 3}, Di is the uniform distribution on [0, 1].", "startOffset": 53, "endOffset": 59}], "year": 2017, "abstractText": "In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the max() function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve O(log T ) distribution-dependent regret and \u00d5( \u221a T ) distribution-independent regret, where T is the time horizon. We apply our results to the K-MAX problem and expected utility maximization problems. In particular, for K-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first \u00d5( \u221a T ) bound on the (1 \u2212 \u01eb)approximation regret of its online problem, for any \u01eb > 0.", "creator": "LaTeX with hyperref package"}}}