{"id": "1610.06620", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Proposing Plausible Answers for Open-ended Visual Question Answering", "abstract": "Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system's visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of `Answer Proposal': for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA.", "histories": [["v1", "Thu, 20 Oct 2016 22:01:36 GMT  (3332kb,D)", "https://arxiv.org/abs/1610.06620v1", null], ["v2", "Mon, 24 Oct 2016 00:12:29 GMT  (3332kb,D)", "http://arxiv.org/abs/1610.06620v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["omid bakhshandeh", "trung bui", "zhe lin", "walter chang"], "accepted": false, "id": "1610.06620"}, "pdf": {"name": "1610.06620.pdf", "metadata": {"source": "CRF", "title": "Proposing Plausible Answers for Open-ended Visual Question Answering", "authors": ["Omid Bakhshandeh", "Trung Bui", "Zhe Lin", "Walter Chang"], "emails": ["omidb@cs.rochester.edu", "bui@adobe.com", "zlin@adobe.com", "wachang@adobe.com"], "sections": [{"heading": "1 Introduction", "text": "With the recent progress made in AI, there is a renewed interest in building AI systems which are capable of reasoning in addition to perception. As humans, we often use our commonsense reasoning for interpreting complex visual and auditory input. Question Answering (QA) is a crucial ability for any intelligent system and requires many degrees of complex reasoning. Imagine that you are blindfolded and asked \u2018What is parked next to that tree?\u2019. Although you cannot see the referent objects, you\ncan still propose a set of plausible answers using your common sense. Your set of proposed answers is most probably within {car, bike, bus, motorcycle, scooter} or other objects that can be parked. As humans, given the semantic interpretation of a question, we have a set of default presumptions about the semantic type of the plausible answer. Each question seems to reflect some semantic \u2018frame\u2019 which then naturally activates certain \u2018slots\u2019 that can only be instantiated with certain types of entities. Hence, one could link the surface task of \u2018finding plausible answers to a question\u2019 to a deeper theory of structured commonsense knowledge. As Minsky (1974) points out, a question also includes suggestions and recommendations about its set of answers (assignment proposal). Minsky notes that \u201c \u2018default\u2019 assignments become the simplest special cases of recommendations, ... one has a hierarchy in which such proposals depend on features of the situation\u201d.\nIn this paper we focus on developing the capability to propose relevant and plausible answers to a given question, which is a key intelligent behavior that an AI system should demonstrate. We introduce the novel task of \u2018Answer Proposal\u2019, in which a system seeks to generate a ranked list of meaningful candidate answers associated with the semantic features of a given question. Having a prior commonsense knowledge about the scope of the plausible answers can not only narrow down the search space for the final prediction, but also, in case of an incorrect prediction, help the system appear more intelligent from the user-experience point of view.\nA great framework for showcasing the potential of Answer Proposal is multimodal QA. Visual Question Answering (VQA) (Antol et al., 2015; Ren et al., 2015) is one of the most interesting multimodal\nar X\niv :1\n61 0.\n06 62\n0v 2\n[ cs\n.C L\n] 2\n4 O\nct 2\nQA tasks for evaluating visual understanding of images through questions posed in natural language. This task is set up as follows: given an image and a natural language question about it, a system should provide an accurate natural language answer. The answer is either selected from a list of choices (for a multiple-choice question), or is generated (for an open-ended question), where the open-ended task is more challenging than the multiple-choice one. In both cases, VQA is automatically evaluated given that many open-ended answers contain only a few words.\nThere have been many recent approaches for tackling the task of VQA. The state-of-the-art openended systems mainly train a multi-class classifier which uses the space of all possible answers for classification. If we look under the hood of such systems, it is evident that they do not have any deep understanding of whether their top answers are even plausible candidates for the given question. For example, consider the example images along with their corresponding questions in Table 1. The Answer row of this table shows the top answers from one of the state-of-the-art systems (hereinafter, SOTAQA)1 (Antol et al., 2015). Looking at these top answers, it is clear that this system does not exhibit basic understanding of e.g., the kind of things that you can drink out of, or the kind of things that can be blown out.\nIn this paper we mainly leverage the power of Answer Proposal for proposing plausible answers for open-ended questions in the context of the VQA problem. The contributions of this paper can be sum-\n1Demo available through http://cloudcv.org/vqa/\nmarized as follows: (1) We introduce the task of \u2018Answer Proposal\u2019 together with an intrinsic metric for stand-alone evaluation (Section 2). (2) We present various Answer Proposal models, ranging from a neural generative model to a semantic graph matching one. We tackle the task of VQA by feeding the Answer Proposal list into a deep binary classifier which determines the correctness of a proposed answer (Section 3-4). (3) We show that our best answer proposal model achieves a high recall score and generates highly plausible answer proposal sets. Furthermore, our approach for open-ended VQA is competitive with some other models and can also unveil some of the biases of multiple-choice VQA (Section 5). We hope that the results in this paper ignite interest in the community to leverage semantic understanding and commonsense knowledge for tackling VQA."}, {"heading": "2 The Task of Answer Proposal", "text": "We define the task of \u2018Answer Proposal\u2019 as follows.\nDefinition 2.1. Given the questionQ, create a list of all plausible answers, P , which is ranked according to their prior probabilities.\nFor example, given the question q =\u2018What is parked in front of the tree?\u2019, the set p ={\u2018car\u2019, \u2018bike\u2019, \u2018motorcycle\u2019}. The objective of a system is to generate the list P in a way that the actual correct answer appears higher in the ranked list. Hence, we define the intrinsic evaluation of the task as follows.\nDefinition 2.2. Given a list of M triplets of questions, answers, and the plausible answer proposal list, such as (qi, ai, pi), we define Recall@N as:\nRecall@N = \u2211 i=1toM IiN M\nIiN = 1 if ai \u2208 pi[: N ], else 0. (1)\nwhere Pi[:N] is the answer proposal list with cutoff=N and IiN is the success indicator variable. Recall@N evaluation metric has been used in information retrieval for evaluating the quality of the retrieved ranked lists, hence, is a great fit for the task of Answer Proposal.\nThe problem of finding plausible answer set given a question can be viewed as a many-to-many mapping. Figure 1 draws an example such mapping. It is important to note that learning such a mapping accurately is challenging, e.g., \u2018Apple\u2019 is a plausible answer for both \u2018What is he eating?\u2019 and \u2018What is on the tree?\u2019 questions despite the fact that these questions are not semantically similar. On the other hand, \u2018Hot dog\u2019 is an edible entity but cannot be found on top of a tree."}, {"heading": "3 Approach for Tackling VQA", "text": "We extrinsically evaluate Answer Proposal through the VQA task. Our approach for tackling VQA consists of two main modules: answer proposal generator, and a simple deep binary classifier. The answer proposal module takes in the question and then generates a list of plausible proposal answers (p) using the semantic features of the question (q). Then each item from the proposal list (p), together with the question (q) and the image (i), gets fed into the deep binary classifier which then predicts the probability of the triple (q, i, p) being correct. Figure 2 shows this pipeline. In Section 4, we will introduce various models for Answer Proposal module.\nDeep Binary Classifier\nThis module classifies whether or not a given triplet (q, i, p) is correct. Our deep binary classifier is a simple multilayer perceptron which works as follows: it takes in the concatenated feature vectors representing each of the three inputs, and outputs a prediction of \u2018yes\u2019 or \u2018no\u2019 indicating the correctness of the triplet. Hence, our model is \u03c3(W TFqip), where Fqip refers to the concatenated feature vectors and \u03c3 is the sigmoid function. Our image features are 2,048-dimensional vectors computed using the penultimate layer of the state-of-the-art convolutional neural network for image recognition, Resnet101 (He et al., 2016). Both the question and plausible answer are represented using 300-dimensional average Word2Vec word embeddings (Mikolov et al., 2013) which is a bag-of-words (BOW) model2. The classifier is trained end-to-end with the objective of minimizing the binary logistic loss of the prediction by using stochastic gradient descent. The model has three layers, where each layer has 8,192 hidden units, with dropout after the first layer."}, {"heading": "4 Answer Proposal Models", "text": "In this Section we present various models for generating Answer Proposal lists. We mainly devise two classes of approaches: generative and retrieval.\n2We also experimented with using a Recurrent Neural Network for encoding the question, which yielded worse results (Jabri et al., 2016; Zhou et al., 2015)."}, {"heading": "4.1 Generative Model", "text": "The generative model is an encoder-decoder Recurrent Neural Network (RNN) architecture (Sutskever et al., 2014; Cho et al., 2014), which generates the answer proposal being conditioned on the question. The encoder RNN processes the question and the decoder3 generates the proposed answer one token at a time until hitting the end-of-sentence (EOS) token. Every question is encoded into a state vector of size 512, which is then set as the initial recurrent state of the decoder. We tune the model parameters on the val set, where we set the number of layers to 2. The model is trained end-to-end, using Stochastic Gradient Descent with early stopping. For decoding, we use beam search with beam-width = 25. The main advantage of this model is that it can compose novel multi-word answers which have not been seen in the training set before. Figure 3 outlines this model by an example."}, {"heading": "4.2 Retrieval Models", "text": "A retrieval model uses similarity metrics to retrieve train-set questions similar to the test question. There are various ways for capturing semantic similarity between a pair of questions. In this Section we present two different retrieval approaches.\nAverage Word2Vec Model We experimented with various textual similarity metrics, among which BLEU and Word2Vec are notable4. BLEU is the widely used Machine Translation (MT) metric (Papineni et al., 2002) which\n3We got worse results using attention for decoder RNN. 4We also tried other sentence-level embedding models, such as the Skip-thoughts model (Kiros et al., 2015), all of which performed weaker than Word2Vec in capturing generic textual similarity\nscores a hypothesis against a gold reference by computing the geometric mean of precision scores for different n-grams. BLEU can only capture exact ngram matches, hence, for instance, it does not account for the similarity between \u2018eating\u2019 and \u2018devouring\u2019 for comparing \u2018What is she eating?\u2019 and \u2018What is he devouring?\u2019. We obtained the most promising results when using Word2Vec (W2V) as the similarity metric. We use Word2Vec (Mikolov et al., 2013) as a sentence-level vector representation where we average the word-level embeddings to obtain the sentence-level vector. Then, the similarity between two sentences equals the cosine similarity between their corresponding vectors. This simple model is effective in capturing generic similarity of many questions such as \u2018What is filled with water?\u2019 and \u2018What is she filling with water?\u2019.\nSemantic Graph Matching Model\nThe generative model represents the natural language question as a stream of tokens and the Word2Vec model represents it as a bag of words. As discussed in Section 1, as humans, given the rich semantic features of a question, we can infer a set of default presumptions about the semantic types of plausible answers. Our semantic graph matching model attempts to get closer to that premise by encoding the semantic structure of each question. There are various approaches for representing the structure of questions, including dependency parse trees or semantic parses. Although the dependency structure provides a lot of information regarding how the individual words relate grammatically, it does not provide much information regarding what a sentence actually means and what the ontology types of different words are5. This makes deep semantic parsing a more suitable choice for our task. Semantic parse graph of a sentence maps natural language input to a formal meaning representation. A broadcoverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) operates at the generic natural language level, mapping surface level words into their underlying meaning representation.\n5Beyond these shortcomings, the state-of-the-art dependency parsers (e.g., CoreNLP (Manning et al., 2014)) very often fail at parsing questions altogether, mainly confusing copular and auxiliary constructions. This is due to their training corpora which often lacks questions.\nHere we use the TRIPS6 (Allen et al., 2008) broad-coverage semantic parser which produces the state-of-the-art logical form from natural text (Allen et al., 2008). The TRIPS logical form language is an encoding of the semantic content of a sentence that can be mapped to a formal knowledge representation. TRIPS provides a richer semantic structure than other off-the-shelf text processing systems, mainly it provides sense disambiguated deep structures augmented with semantic ontology types7. Figure 4 shows the TRIPS semantic parse for the question \u2018What is parked in front of the tree?\u2019. In this graph representation, each node specifies a word 8 in bold along with its corresponding ontology type on its left. The edges in the graph are semantic roles9 between the nodes. The root of each graph is an speech act node. SPEECHACT indicates the communicative function of an utterance, such as \u2018tell\u2019 act or an \u2018acknowledgment\u2019 one. The two speech act types that we are interested in are the following: \u2013 WH-QUESTION: Indicates a \u2018wh\u2019 question node, which has two semantic roles: (1) a \u2018content\u2019 role which points to the main semantic type in the question and (2) a \u2018focus\u2019 role which refers to the \u2018wh\u2019 question word itself.\n\u2013 YES/NO-QUESTION: Indicates a Yes/No question, which only has the semantic role \u2018content\u2019.\nAs shown in Figure 4, such a semantic parse graph\n6http://trips.ihmc.us/parser/cgi/parse 7More importantly, TRIPS parser was originally developed as a part of a conversational assistant, tailored to parsing natural language questions.\n8What is not shown in this graph is that words are also sense disambiguated according to WordNet (Miller, 1995) senses.\n9For the full list of semantic roles in TRIPS parser please refer to http://trips.ihmc.us/parser/ LFDocumentation.pdf.\nprovides a semantically rich representation for the question.\nAs a retrieval model, our semantic graph matching approach aims at using the semantic parse graph representations to retrieve similar train-set parse graphs given a test parse graph. This approach consists of the following two main stages: \u2022 Stage 1, question type categorization: As mentioned earlier, the semantic features of the questions play a major role in our understanding of the plausible answers. The question word, represented by the SPEECHACT node, identifies the subcategory of answer types. Mainly, the only plausible answer for YES/NO-QUESTIONs are {yes, no}10, or the answer to WH-QUESTION of type \u2018How many\u2019 is always a number. At this stage, given a test question, we filter a part of the train-set which share the same question type as with the test set.\n\u2022 Stage 2, semantic graph matching: At this stage, given a set of train-set graphs which share the same question type category from stage 1, we semantically match the test graph with the train-set graphs. Consider the two questions \u2018What is she eating?\u2019 and \u2018What is he consuming in the kitchen?\u2019.The graph of these sentences would not exactly match, however, they are indeed similar and share the same plausible answer set. This brings up the idea of implementing different graph mutation patterns to mutate the test graph through performing a few actions, each with a different priority. The mutations are a combination of the following two actions: (1) replacing a node with its ontology type (one or more levels up), or (2) deleting a node11. At the end, this stage generates a ranked list of plausible answers, where a matching between a train-set graph and less mutated test graph appears higher in the list."}, {"heading": "5 Experiments", "text": "In this Section we summarize our experiments on intrinsic and extrinsic evaluation of various Answer Proposal models. For all the experiments, we use the COCO trainval2014 dataset with the same train\n10Our semantic graph matching model does not need to proceed to further stages for the YES/NO-QUESTIONS as the ultimate plausible answer set is {yes, no}.\n11Linguistic knowledge about core semantic role of verbs enables us to make an informed decision about deleting the nodes.\nand val set split as with Zhou et al. (2015), containing 339,482 training and 30,377 test instances. We use the test2015-standard blind set as the test set. In addition to the models described in Section 4, we include the model W2V+Sem in the experiments. Given the ranked proposal lists from the Semantic and the W2V model, the W2V+Sem model simply generates an aggregated ranked list by alternating between the ranked items of the two lists."}, {"heading": "5.1 Intrinsic Evaluation", "text": "We intrinsically evaluate various models according to their Recall@N score. Each open-ended VQA question come with a gold answer list of size 10. Here we compute Recall@N scores according to two measures of correctness: (1) it exactly matches the majority answer, (2) it matches any of the answers in the answer list. Given that the blind test set\ndoes not provide the answers, we perform the intrinsic evaluation on the trainval2014-val set.\nFigure 5a shows the Recall@N scores evaluated according the majority answer and Figure 5b shows the evaluation against any of the answers. These results show that our best model perform very well (with the highest recall of 87%@100 according to majority and 93%@100 according to any) on generating relevant answers to a given question. The model traces become almost constant after N=100. As this graph shows, the W2V+Sem model achieves the highest Recall and the Semantic approach and W2V are competitively close. The high recall of W2V retrieval model shows the effectiveness of word embeddings for measuring the similarity among VQA questions. This further suggests that there are many structurally similar questions in the VQA dataset which makes capturing word order in meaning representation less crucial. The MT model comes short in generating long hypotheses list and is the weakest performing system. It is interesting to see that our best models obtain a high recall on wh-questions (Figure 5c) as well.\nAlthough the MT model often fails at including the correct answer within its proposal list, whenever the correct answer is included in the proposal (i.e., there is a hit), it is at the very top of the list. Figure 6 visualizes the rank distribution of all the hits across all the models, where the MT model shows to have more than 60% of its answers at rank 1. Moreover, as expected, the MT model is capable of generating novel answers (e.g., \u2018wooden bottle\u2019), however, it suffers from generating generic and commonplace answers (e.g., \u20180 feet\u2019, as an answer for a \u2018how long\u2019 question). As a result, we did not include the MT\nmodel as an answer proposal module in our upcoming VQA experiments.\nAs Figure 6 shows, the Semantic model also comes close to having about 50% of its answers ranked at position 1. This further showcases the strength of our Semantic model, suggesting it has higher precision in including relevant answers in the proposal list. Table 2 shows example top-5 ranked answer proposals for various models. As you can see in this table\u2019s examples, W2V model does not have deep understanding about the SPEECHACT of the questions and sometimes proposes \u2018yes\u2019 or \u2018no\u2019 to wh-questions12. Analyzing the proposal lists of our highest recall model, W2V+Sem, we have identified three main sources of error. As shown in Table 3, the errors are sometimes due to the missing tokens in multi-word answers or the errors in the human annotations of the VQA dataset13. Another source of er-\n12The Semantic approach has a precision of 99.8% and a recall of 99.2% for detecting YES/NO questions. The error margin is due to occasional parsing errors.\n13We believe that the human performance of 83.3% on\nror is the inherent characteristic of our model, where it only includes the commonsense plausible answers. This results in missing answers such as \u2018surfing\u2019 for the question \u2018what is unusual about what this dog is doing?\u2019.\nIn order to further assess the plausibility of the answer proposals, we conducted a human evaluation as follows.\nHuman Evaluation on Plausibility We conducted human evaluation on a subset of the val set, asking three human judges to rate various systems according to the following prompt.\nDefinition 5.1. You are shown a question about an image along with a proposed answer. Without seeing the image, do you think the proposed answer can be the actual correct answer to the question?\nFor comparison, we also include the top answer list on val set from the SOTA-QA model (example answer list in Table 1). We set the proposal list\ntest2015-standar set is partly due to such annotation errors.\ncutoff=5 across all models. Table 4 shows the answer proposals generated by our Semantic approach on the same questions presented in Table 1. Table 5 shows the human evaluation results. As the results show, our Semantic approach comes the closest to our premise of only including truly plausible answers in the answer proposal set. The gap between our answer proposal approaches and the SOTA-QA VQA system is very significant."}, {"heading": "5.2 Extrinsic Evaluation on VQA", "text": "We evaluate our approach for tackling VQA (described in Section 3 and depicted in Figure 2) using various answer proposal modules. We train all the models on trainval2014-train. Table 2 shows the final predicted answers to a few example questions. Table 6 shows the test results on the test2015standard set. In this table we also include three stateof-the-art models (including the current leader on leaderboard14) to be introduced in Section 6. Toprank is a baseline which naively predicts the answer to be the highest-ranked answer in W2V+Sem proposal list. It is interesting to see that this model can actually predict Yes/No with 70.7% accuracy, which shows the bias of the test dataset. As the results show, W2V+Sem is our best performing system, also reflecting on its higher recall. Although our model (which employs a very simple classification module) outperforms some of the state-of-the-art models, it is performing weaker than the best performing systems. Apart from the errors propagated from the Answer Proposal module, outlined in the previous subsection, we hypothesize that QA being constrained by not generating implausible answers can be a challenging task.\n14https://competitions.codalab.org/ competitions/6961#results"}, {"heading": "5.3 Revisiting Mutliple-choice VQA", "text": "The state-of-the-art accuracy on multiple-choice VQA is higher than open-ended, being a less challenging task. A recent work (Jabri et al., 2016) which studies the biases of the VQA dataset along with the best performing systems shows that their simple binary classification approach (a multilayer perceptron, which takes in triplet of (question,image,answer)) outperforms many of the other complex systems. They suggest two explanations for this observation: (1) the best performing systems are the ones which can best exploit biases in VQA dataset (2) the current VQA models all come short in modeling the problem effectively and reach the same ceiling in accuracy.\nOur answer proposal model enables us to shed more light on this matter. We trained the same binary classifier on multiple-choice questions, which achieves 65.2% accuracy on test2015-standard set. Then at test time we swap the multiple-choice list with our pre-generated W2V+Sem answer proposal list. We see that the accuracy of the system drops to 49%. This can be partly due to the estimated 8% of the cases in which the correct answer is missing from the answer proposal list. However, as another experiment, we swaped the multiple-choice list with our answer proposal list at train time as well as test time. Then the accuracy of the system increased to 55.1%. This makes the explanation (2) more probable: the current high performing VQA systems are indeed learning the biases of the dataset, where they come short in solving the same task only if provided with a set of more challenging and semantically similar choices15. We conclude that training multiple-\n15The VQA multiple-choice list includes many random and irrelevant options, e.g., the choices in the set {3, no, toothpaste and toothbrush, robin, no idea, red} are all provided for the\nchoice VQA models on a list of plausible choices enables a system to better learn the important features of the question, image, and the answer, however, this will clearly make the multiple-choice VQA task more challenging."}, {"heading": "6 Related Work", "text": "There has been a renewed interest in combining vision and language. VQA is one of the most interesting recent challenges, mainly facilitated by the release of the VQA dataset (Antol et al., 2015), the Toronto COCO-QA (CQA) dataset (Ren et al., 2015), and the Visual7W dataset (Zhu et al., 2016). The VQA dataset is a collection of free-form questions, with both the questions and the set of answers being crowd-sourced. The VQA questions were collected by asking the crowd workers to compose a visually verifiable question which will \u2018stump a smart robot\u2019. VQA contains 204,721 real images\nquestion \u2018what is on the other side of the train?\u2019. This makes the classification task easier.\nand 50,000 abstract images, with various multiplechoice and open-ended questions. Visual7W (Zhu et al., 2016) is another recent dataset, which establishes a grounding link between a textual answer and the regions of the image. This enables answering a question with not only text but also with visual regions. Visual7w contains 327,939 7W multiplechoice QA pairs (but not open-ended questions), including various \u2018wh\u2019 questions. In this paper we base our work on the main VQA challenge dataset16, specifically open-ended question answering, which is shown to be a more challenging task.\nThere are various approaches for tackling the task of VQA. The majority of these approaches predict the answer by training a multi-class classifier on image and question features. The classification is performed on the set of unique answers observed in the training set. For this classification there are various neural network architectures combining complex attention mechanisms and memory networks (Lu et al., 2016; Yang et al., 2015). Zhou et al. (Zhu et al., 2016) use deep convolutional features for representing images and averaging word embeddings as question features. The concatenated feature vectors are then fed into a multi-class logistic regression model. Another work (Ma et al., 2016) uses a one-dimensional convolutional network instead of an LSTM encoder for getting the question-level embedding from word-level embeddings.\nAnother model (iBOWImg) (Zhou et al., 2015), is a bag-of-words baseline which concatenates the word features from the question and convolutional features from the image to predict the answer, which shows results competitive with many recent more complex approaches using recurrent neural networks (LSTMImg). The Dependency Neural Module Network (D-NMN) approach (Andreas et al., 2016) performs dynamic image processing via a compositional network which dynamically restructures it-\n16http://visualqa.org/download.html\nself using the syntactic parse tree of the question. Fukui et al. (2016) use Multimodal Compact Bilinear pooling (MCB) for combining multi-modal (textual and visual) information. They show that multiple MCBs with their architecture with attention achieves the state-of-the-art results on the VQA task. Another recent work (Lu et al., 2016) introduced coattention of image and question, where they jointly learn a hierarchical attention mechanism based on parsing the question and the image. Recently it has been shown that the attentions generated by neural attention mechanisms are either negatively correlated with where a human looks in the image or if they have positive correlation it is worse than taskindependent saliency (Das et al., 2016). Furthermore, many simpler classification approaches (Jabri et al., 2016) are shown to outperform the complex attention architectures that are expected to perform some complex reasoning. This brings up questions regarding the effectiveness of the current complex approaches to VQA and further reveals the biases of the VQA multiple-choice question set (Jabri et al., 2016).\nThere are also approaches that look at the problem not as a classification task but as a generative one. Notable among the generative approaches, Malinowski et al. (2014) generate the answer using an LSTM which is conditioned on the deep convolutional image features and the question. Although generative models are a promising way to generalize the production of unseen answers during training, the earlier work showed that joint learning of the encoding and decoding models from the VQA datasets has not been successful. More importantly, it is not clear how to automatically evaluate the novel generated text.\nOur deep binary classification module (Section 3) is more closely related to Shih et al. (Shih et al., 2016) and Jabri et al. (Jabri et al., 2016) which also take the answer as an input variable to a classifier that then assigns a probability to the (question, image, answer) triplet as a whole. While we use deep convolutional features as our visual features, Shih et al. use a more complex image processing module where they select image regions for answering. However, none of the earlier work propose an effective approach for open-ended question answering, where the \u2018answer\u2019 in the (question, image,\nanswer) triplet is not given. Our work introduces a novel perspective for tackling open-ended VQA questions which has not been explored by any of the previous work: answer proposals. We provide various methods for proposing plausible answers, where the semantic-driven approach outperforms others.\nThe impact of semantics as opposed to surface ngram wording of textual content has also been studied in SPICE captioning evaluation (Anderson et al., 2016). SPICE emphasizes on the importance of semantic propositional content of captions, as captured by dependency parse trees, which correlates well with how human evaluates captions. We also note the work of (Xu et al., 2015) that while focused on caption generation and retrieval tasks for video using a joint language and vision model, proposed a compositional semantics language model that enforced semantic compatibility between essential concepts, similar to our goal of using question semantics to constrain our answer proposals."}, {"heading": "7 Conclusion", "text": "We introduced the novel task of proposing plausible answers for a given open-ended question, where a system should generate a ranked list of plausible answers given a question. We use the VQA task as a multimodal test framework for training and testing answer proposal models. We provide various answer proposal models, ranging from vectorbased to deep semantic ones. We show that our best performing model which combines our two retrieval models achieves a high recall. Furthermore, we show that our Semantic Graph Matching approach generates truly plausible answers, unlike the state-of-the-art models. Our full VQA model outperforms some other solutions to VQA, however, performs weaker than the current best performing systems. We hypothesize that answering questions with the condition of generating only plausible answers can be more challenging that only answering questions. Our next step is to employ better answer proposal models, possibly by injecting external world knowledge from other resources. Although we have mainly experimented with the VQA task, similar Answer Proposal models can be potentially used in other QA tasks."}], "references": [{"title": "Deep semantic analysis of text", "author": ["James F. Allen", "Mary Swift", "Will de Beaumont."], "venue": "Proceedings of the 2008 Conference on Semantics in Text Processing, STEP \u201908, pages 343\u2013354, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Allen et al\\.,? 2008", "shortCiteRegEx": "Allen et al\\.", "year": 2008}, {"title": "Spice: Semantic propositional image caption evaluation", "author": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould."], "venue": "ECCV.", "citeRegEx": "Anderson et al\\.,? 2016", "shortCiteRegEx": "Anderson et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "NAACL.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "International Conference on Computer Vision (ICCV).", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proceedings of the 7th Linguistic", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Wide-coverage semantic analysis with boxer", "author": ["Johan Bos."], "venue": "Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277\u2013286. College Publications.", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions? CoRR, abs/1606.03556", "author": ["Abhishek Das", "Harsh Agrawal", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach."], "venue": "Proceedings of the 2016 Conference on Empirical Methods", "citeRegEx": "Fukui et al\\.,? 2016", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "CVPR.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["Allan Jabri", "Armand Joulin", "Laurens van der Maaten."], "venue": "CoRR, abs/1606.08390.", "citeRegEx": "Jabri et al\\.,? 2016", "shortCiteRegEx": "Jabri et al\\.", "year": 2016}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "NIPS.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."], "venue": "CoRR, abs/1606.00061.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lin Ma", "Zhengdong Lu", "Hang Li."], "venue": "AAAI, pages 3567\u20133573. AAAI Press.", "citeRegEx": "Ma et al\\.,? 2016", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "A multiworld approach to question answering about realworld scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz."], "venue": "Advances in Neural Information Processing Systems 27, pages 1682\u20131690.", "citeRegEx": "Malinowski and Fritz.,? 2014", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neu-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller."], "venue": "Commun. ACM, 38(11):39\u201341, November.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "A framework for representing knowledge", "author": ["Marvin Minsky."], "venue": "Technical report, Cambridge, MA, USA.", "citeRegEx": "Minsky.,? 1974", "shortCiteRegEx": "Minsky.", "year": 1974}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Strouds-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Question answering about images using visual semantic embeddings", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel."], "venue": "Deep Learning Workshop, ICML 2015.", "citeRegEx": "Ren et al\\.,? 2015", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Learning to localize little landmarks", "author": ["Kevin J. Shih", "Saurabh Singh", "Derek Hoiem."], "venue": "Computer Vision and Pattern Recognition.", "citeRegEx": "Shih et al\\.,? 2016", "shortCiteRegEx": "Shih et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso."], "venue": "Proceedings of AAAI Conference on Artificial Intelligence.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola."], "venue": "CoRR, abs/1511.02274.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus."], "venue": "CoRR, abs/1512.02167.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Yuke Zhu", "Oliver Groth", "Michael S. Bernstein", "Li Fei-Fei."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, IEEE.", "citeRegEx": "Zhu et al\\.,? 2016", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "As Minsky (1974) points out, a question also includes suggestions and recommendations about its set of answers (assignment proposal).", "startOffset": 3, "endOffset": 17}, {"referenceID": 3, "context": "Visual Question Answering (VQA) (Antol et al., 2015; Ren et al., 2015) is one of the most interesting multimodal ar X iv :1 61 0.", "startOffset": 32, "endOffset": 70}, {"referenceID": 20, "context": "Visual Question Answering (VQA) (Antol et al., 2015; Ren et al., 2015) is one of the most interesting multimodal ar X iv :1 61 0.", "startOffset": 32, "endOffset": 70}, {"referenceID": 3, "context": "The Answer row of this table shows the top answers from one of the state-of-the-art systems (hereinafter, SOTAQA)1 (Antol et al., 2015).", "startOffset": 115, "endOffset": 135}, {"referenceID": 9, "context": "Our image features are 2,048-dimensional vectors computed using the penultimate layer of the state-of-the-art convolutional neural network for image recognition, Resnet101 (He et al., 2016).", "startOffset": 172, "endOffset": 189}, {"referenceID": 16, "context": "Both the question and plausible answer are represented using 300-dimensional average Word2Vec word embeddings (Mikolov et al., 2013) which is a bag-of-words (BOW) model2.", "startOffset": 110, "endOffset": 132}, {"referenceID": 10, "context": "We also experimented with using a Recurrent Neural Network for encoding the question, which yielded worse results (Jabri et al., 2016; Zhou et al., 2015).", "startOffset": 114, "endOffset": 153}, {"referenceID": 25, "context": "We also experimented with using a Recurrent Neural Network for encoding the question, which yielded worse results (Jabri et al., 2016; Zhou et al., 2015).", "startOffset": 114, "endOffset": 153}, {"referenceID": 22, "context": "The generative model is an encoder-decoder Recurrent Neural Network (RNN) architecture (Sutskever et al., 2014; Cho et al., 2014), which generates the answer proposal being conditioned on the question.", "startOffset": 87, "endOffset": 129}, {"referenceID": 6, "context": "The generative model is an encoder-decoder Recurrent Neural Network (RNN) architecture (Sutskever et al., 2014; Cho et al., 2014), which generates the answer proposal being conditioned on the question.", "startOffset": 87, "endOffset": 129}, {"referenceID": 19, "context": "BLEU is the widely used Machine Translation (MT) metric (Papineni et al., 2002) which", "startOffset": 56, "endOffset": 79}, {"referenceID": 11, "context": "We also tried other sentence-level embedding models, such as the Skip-thoughts model (Kiros et al., 2015), all of which performed weaker than Word2Vec in capturing generic textual similarity scores a hypothesis against a gold reference by computing the geometric mean of precision scores for different n-grams.", "startOffset": 85, "endOffset": 105}, {"referenceID": 16, "context": "We use Word2Vec (Mikolov et al., 2013) as a sentence-level vector representation where we average the word-level embeddings to obtain the sentence-level vector.", "startOffset": 16, "endOffset": 38}, {"referenceID": 4, "context": "A broadcoverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) operates at the generic natural language level, mapping surface level words into their underlying meaning representation.", "startOffset": 32, "endOffset": 87}, {"referenceID": 5, "context": "A broadcoverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) operates at the generic natural language level, mapping surface level words into their underlying meaning representation.", "startOffset": 32, "endOffset": 87}, {"referenceID": 0, "context": "A broadcoverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) operates at the generic natural language level, mapping surface level words into their underlying meaning representation.", "startOffset": 32, "endOffset": 87}, {"referenceID": 15, "context": ", CoreNLP (Manning et al., 2014)) very often fail at parsing questions altogether, mainly confusing copular and auxiliary constructions.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "Here we use the TRIPS6 (Allen et al., 2008) broad-coverage semantic parser which produces the state-of-the-art logical form from natural text (Allen et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 0, "context": ", 2008) broad-coverage semantic parser which produces the state-of-the-art logical form from natural text (Allen et al., 2008).", "startOffset": 106, "endOffset": 126}, {"referenceID": 17, "context": "What is not shown in this graph is that words are also sense disambiguated according to WordNet (Miller, 1995) senses.", "startOffset": 96, "endOffset": 110}, {"referenceID": 25, "context": "and val set split as with Zhou et al. (2015), containing 339,482 training and 30,377 test instances.", "startOffset": 26, "endOffset": 45}, {"referenceID": 10, "context": "A recent work (Jabri et al., 2016) which studies the biases of the VQA dataset along with the best performing systems shows that their simple binary classification approach (a multilayer perceptron, which takes in triplet of (question,image,answer)) outperforms many of the other complex systems.", "startOffset": 14, "endOffset": 34}, {"referenceID": 3, "context": "VQA is one of the most interesting recent challenges, mainly facilitated by the release of the VQA dataset (Antol et al., 2015), the Toronto COCO-QA (CQA) dataset (Ren et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 20, "context": ", 2015), the Toronto COCO-QA (CQA) dataset (Ren et al., 2015), and the Visual7W dataset (Zhu et al.", "startOffset": 43, "endOffset": 61}, {"referenceID": 26, "context": ", 2015), and the Visual7W dataset (Zhu et al., 2016).", "startOffset": 34, "endOffset": 52}, {"referenceID": 26, "context": "Visual7W (Zhu et al., 2016) is another recent dataset, which establishes a grounding link between a textual answer and the regions of the image.", "startOffset": 9, "endOffset": 27}, {"referenceID": 12, "context": "For this classification there are various neural network architectures combining complex attention mechanisms and memory networks (Lu et al., 2016; Yang et al., 2015).", "startOffset": 130, "endOffset": 166}, {"referenceID": 24, "context": "For this classification there are various neural network architectures combining complex attention mechanisms and memory networks (Lu et al., 2016; Yang et al., 2015).", "startOffset": 130, "endOffset": 166}, {"referenceID": 26, "context": "(Zhu et al., 2016) use deep convolutional features for representing images and averaging word embeddings as question features.", "startOffset": 0, "endOffset": 18}, {"referenceID": 13, "context": "Another work (Ma et al., 2016) uses a one-dimensional convolutional network instead of an LSTM encoder for getting the question-level embedding from word-level embeddings.", "startOffset": 13, "endOffset": 30}, {"referenceID": 25, "context": "Another model (iBOWImg) (Zhou et al., 2015), is a bag-of-words baseline which concatenates the word features from the question and convolutional features from the image to predict the answer, which shows results competitive with many recent more complex approaches using recurrent neural networks (LSTMImg).", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": "The Dependency Neural Module Network (D-NMN) approach (Andreas et al., 2016) performs dynamic image processing via a compositional network which dynamically restructures it-", "startOffset": 54, "endOffset": 76}, {"referenceID": 12, "context": "Another recent work (Lu et al., 2016) introduced coattention of image and question, where they jointly learn a hierarchical attention mechanism based on parsing the question and the image.", "startOffset": 20, "endOffset": 37}, {"referenceID": 7, "context": "Recently it has been shown that the attentions generated by neural attention mechanisms are either negatively correlated with where a human looks in the image or if they have positive correlation it is worse than taskindependent saliency (Das et al., 2016).", "startOffset": 238, "endOffset": 256}, {"referenceID": 10, "context": "Furthermore, many simpler classification approaches (Jabri et al., 2016) are shown to outperform the complex attention architectures that are expected to perform some complex reasoning.", "startOffset": 52, "endOffset": 72}, {"referenceID": 10, "context": "This brings up questions regarding the effectiveness of the current complex approaches to VQA and further reveals the biases of the VQA multiple-choice question set (Jabri et al., 2016).", "startOffset": 165, "endOffset": 185}, {"referenceID": 7, "context": "Fukui et al. (2016) use Multimodal Compact Bilinear pooling (MCB) for combining multi-modal (textual and visual) information.", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": "(Shih et al., 2016) and Jabri et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "(Jabri et al., 2016) which also take the answer as an input variable to a classifier that then assigns a probability to the (question, image, answer) triplet as a whole.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "The impact of semantics as opposed to surface ngram wording of textual content has also been studied in SPICE captioning evaluation (Anderson et al., 2016).", "startOffset": 132, "endOffset": 155}, {"referenceID": 23, "context": "We also note the work of (Xu et al., 2015) that while focused on caption generation and retrieval tasks for video using a joint language and vision model, proposed a compositional semantics language model that enforced semantic compatibility between essential concepts, similar to our goal of using question semantics to constrain our answer proposals.", "startOffset": 25, "endOffset": 42}], "year": 2016, "abstractText": "Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system\u2019s visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of \u2018Answer Proposal\u2019: for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA.", "creator": "LaTeX with hyperref package"}}}