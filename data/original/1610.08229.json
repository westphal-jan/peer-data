{"id": "1610.08229", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2016", "title": "Word Embeddings and Their Use In Sentence Classification Tasks", "abstract": "This paper have two parts. In the first part we discuss word embeddings. We discuss the need for them, some of the methods to create them, and some of their interesting properties. We also compare them to image embeddings and see how word embedding and image embedding can be combined to perform different tasks. In the second part we implement a convolutional neural network trained on top of pre-trained word vectors. The network is used for several sentence-level classification tasks, and achieves state-of-art (or comparable) results, demonstrating the great power of pre-trainted word embeddings over random ones.", "histories": [["v1", "Wed, 26 Oct 2016 08:48:10 GMT  (1582kb,D)", "http://arxiv.org/abs/1610.08229v1", "13 pages, 10 figures"]], "COMMENTS": "13 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["amit mandelbaum", "adi shalev"], "accepted": false, "id": "1610.08229"}, "pdf": {"name": "1610.08229.pdf", "metadata": {"source": "CRF", "title": "Word Embeddings and Their Use In Sentence Classification Tasks", "authors": ["Amit Mandelbaum", "Adi Shalev"], "emails": ["amit.mandelbaum@mail.huji.ac.il", "bitan.adi@gmail.com"], "sections": [{"heading": null, "text": "I. Introduction\nThere are some definitions for what WordEmbeddings are, but in the most generalnotion, word embeddings are the numerical representation of words, usually in a shape of a vector in <d. Being more specific, word embeddings are unsupervisedly learned word representation vectors whose relative similarities correlate with semantic similarity. In computational linguistics they are often referred as distributional semantic model or distributed representations.\nThe theoretical foundations of word embeddings can be traced back to the early 1950\u2019s and in particular in the works of Zellig Harris, John Firth, and Ludwig Wittgenstein. The earliest attempts at using feature representations to quantify (semantic) similarity used handcrafted features. A good example is the work on semantic differentials [Osgood, 1964]. The early 1990\u2019s saw the rise of automatically generated contextual features, and the rise of Deep Learning methods for Natural Language Processing (NLP) in the early 2010\u2019s helped to increase their popularity, to the point that, these days, word embeddings are the most popular\nresearch area in NLP 1. This work will be divided into two parts. In the first part we will discuss the need for word embeddings, some of the methods to create them, and some interesting features of those embeddings. We also compare them to image embeddings (usually referred as image features) and see how word embedding and image embedding can be combined to perform different tasks.\nIn the second part of this paper we will present our implementation of Convolutional Neural Networks for Sentence Classification [Kim ,2014]. This work which became very popular is a very good demonstration of the power of pre-trained word embeddings. Using a relatively simple model, the authors were able to achieve state-of-art (or comparable) results, for several sentence-level classification tasks. In this part we will present the model, discuss the results and compare them to those of the original article. We will also extend and test the model on some datasets that were not used in the original article. Finally, we will\n1In 2015 the dominating subject at EMNLP (\"Empirical Methods in NLP\") conference was word embeddings, source: http://sebastianruder.com/word-embeddings-1/\nar X\niv :1\n61 0.\n08 22\n9v 1\n[ cs\n.L G\n] 2\n6 O\nct 2\n01 6\npropose some extensions for the model which might be a good proposition for future work.\nII. Word Embeddings\ni. Motivation\nIt is obvious that every mathematical system or algorithm needs some sort of numeric input to work with. However, while images and audio naturally come in the form of rich, highdimensional vectors (i.e. pixel intensity for images and power spectral density coefficients for audio data), words are treated as discrete atomic symbols.\nThe naive way of converting words to vectors might assign each word a one-hot vector in <|V| where |V| being vocabulary size. This vector will be all zeros except one unique index for each word. Representing words in this way leads to substantial data sparsity and usually means that we may need more data in order to successfully train statistical models.\nWhat mentioned above raise the need for continuous, vector space representations of words that contain data that can be leveraged by models. To be more specific we want semantically similar words to be mapped to nearby points, thus making the representation carry useful information about the word actual meaning.\nii. Word Embeddings Methods\nWord embeddings models can be divided into main categories:\n\u2022 Count-based methods \u2022 Predictive methods\nModels in both categories share, in at least some way, the assumption that words that appear in the same contexts share semantic meaning.\nOne of the most influential early works in count-based methods is the LSI/LSA (Latent Semantic Indexing/Analysis) [Deerwester et al., 1990] method. This method is based on the Firth\u2019s hypothesis from 1957 [Firth, 1957] that the meaning of a word is defined \"by the company it keeps\". This hypothesis leads to a very simple albeit a very high-dimensional word embedding. Formally, each word can be represented as a vector in <N where N is the unique number of words in a given dictionary (in practice N=100,000). Then, by taking a very large corpus (e.g. Wikipedia), let Count5(w1, w2) be the number of times w1 and w2 occur within a distance 5 of each other in the corpus. Then the word embedding for a word w is a vector of dimension N, with one coordinate for each dictionary word. The coordinate corresponding to word w2 is Count5(w, w2).\nThe problem with the resulting embedding is that it uses extremely high-dimensional vectors. In the LSA article, is was empirically discovered that these embeddings can be reduced to vectors R300 by doing a rank-300 SVD on the NxN original embeddings matrix.\nThis method was later refined with reweighting heuristics, such as taking the logarithm, or Pointwise Mutual Information (PMI) [Kenneth et al., 1990] on the count, which is a very popular method.\nThe second family of methods, sometimes also referred as neural probabilistic language models, had theoretical and some practical appearance as early as 1986 [Hinton, 1986], but first to show the utility of pre-trained word embeddings were arguably Collobert and Weston in 2008 [Collobert and Weston, 2008]. Unlike count-based models, predictive models try to predict a word from its neighbors in terms of learned small, dense embedding vectors.\nTwo of the most popular methods which appeared recently are the Glove (Global Vectors for Word Representation) method\n[Pennington et. al., 2014], which is an unsupervised learning method, although not predictive in the common sense, and Word2Vec, a family of energy based predictive models, presented by [Mikolov et. al., 2013]. As Word2Vec is the embedding method used in our work it shall be briefly discussed here.\niii. Word2Vec\nWord2vec is a particularly computationallyefficient predictive model for learning word embeddings from raw text. It comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. Algorithmically, these models are similar, except that CBOW predicts target words (e.g. \u2019mat\u2019) from source context words (\u2019the cat sits on the\u2019), while the skip-gram does the inverse and predicts source context-words from the target words. In the skip-gram model (see fig-\nure 2) a neural network is trained over a large corpus in where the training objective is to learn word vector representations that are good at predicting the nearby words. The method\nis also using a simplified version of NCE [Gutmann and Hyv\u00c3d\u2019rinen, 2012] called Negative sampling where the objective function is defined as follows:\nlog\u03c3(v\u2032TwO vwI ) + k\n\u2211 i=1\nEwi\u223cPn(w)[\u03c3(\u2212v \u2032T wi vwI )]\n(1) where v\u2032w and vw are the \"input\" and \"output\" vector representations of w, \u03c3 is the sigmoid function but can also be seen as the network parameters function, and Pn is some noise probability used to sample random words. In the article they recommend k to be between 5 to 20, while the context of predicted words should be 5 or 10. This above objective is later put in the Skip-Gram objective (equtaion 2) to produce optimal word embeddings.\n1 T\nT\n\u2211 t=1 \u2211 \u2212c\u2264j\u2264c,j 6=0 logp(wt+j|wt) (2)\nThis objective enables the model to differentiate data from noise by means of logistic regression, thus learning high-quality vector representations.\nThe CBOW does exactly the same but the direction is inverted. In other words the CBOW trains a binary logistic classifier where, given a window of context words, gives a higher probability to \"correct\" if the next word is correct and a higher probability to \"incorrect\" if the next word is a random sampled one. Notice that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets.\nFinally the vector we used in our work had a dimension of 300. The Network was trained on the Google News dataset which contains 30 billion training words, with negative sampling as mentioned above. These embeddings can be found online2.\nA lot of follow-up work was done on the Word2Vec method. One interesting work was\n2code.google.com/p/word2vec\ndone by [Goldberg and Levy, 2014] where experiments and theory were used to suggest that these newer methods are related to the older PMI based models, but with new hyperparameters and/or term reweightings. In this project appendix you can find a simplified version of Word2Vec we implemented in TensorFlow architecture using the text8 dataset3 and the Skip-Gram model. See figure 3 for visualized results.\niv. Word Embeddings Properties\nSimilarity: The simplest property of embeddings obtained by all the methods described above is that similar words tend to have similar vectors. More formally, the similarity between two words (as rated by humans on a [-1,1] scale) correlates with the cosine similarity between those words\u2019 vectors. The fact that\nwords embedding are related to their contextwords stand behind the similarity property\n3http://mattmahoney.net/dc/textdata\nas naturally, similar words tend to appear in similar context. This, however creates the problem that antonyms (e.g. cold and hot etc.) also appear with the same context while they are, by definition, have opposite meaning. In [Mikolov et. al., 2013] the score of the (accept,reject) pair is 0.73, and the score of (long,short) is 0.71.\nThe problem of antonyms was tackled directly by [Schwartz et al., 2015]. In this article, the authors introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. Symmetric patterns are a special type of patterns that contain exactly two wildcards and that tend to be instantiated by wildcard pairs such that each member of the pair can take the X or the Y position. For example, the symmetry of the pattern \"X or Y\" is exemplified by the semantically plausible expressions \"cats or dogs\" and \"dogs or cats\". Specifically it was found that two patterns are particularly indicative of antonymy - \"from X to Y\" and \"either X or Y\".\nUsing their model the authors were able to achieve a \u03c1 score of 0.56 on the simlex999 dataset [Hill et al., 2016], improving state-ofthe-art word2vec skip-gram model results by as much as 5.5-16.7%. Furthermore, the authors demonstrated the adaptability of their model to antonym judgment specifications.\nLinear analogy relationships: A more interesting property of recent embeddings [Mikolov et. al., 2013] is that they can solve\nanalogy relationships via linear algebra. This is despite the fact that those embeddings are being produced via nonlinear methods. For example, vqueen is the most similar answer to the vking\u2212 vmen + vwomen equation. It turns out, though, that much more sophisticated relationships are also encoded in this way as we can see in figure 5 below.\nAn interesting theoretical work on non-linear embeddings (especially PMI) was done by [Arora et al., 2015]. In their article they suggest that the creation of a textual corpus is driven by the random walk of a discourse vector ct \u2208 <d, which is a unit vector whose direction in space represents what is being talked about. Each word has a (time-invariant) latent vector vw \u2208 <d that captures its correlations with the discourse vector. Using a word production model they predict that words occurring at successive time steps will also tend to have vectors that are close together, thus explaining why similar words have similar vectors.\nUsing the above model the authors introduce the \"RELATIONS = DIRECTIONS\" notion for linear analogies. The authors claim that for each relation R, some direction \u00b5R can be found which satisfies some equation. This leads to the finding that given enough examples of a relationship R, it is possible to compute \u00b5R using SVD and then given a pair of words with a realtion R and a word c, find the best analogy with word d by finding the pair c and d such that vc\u2212 vd has highest possible projection over \u00b5R. In this way, thay also explain that low dimension of the vectors has a \"purifying\" effect\nthat reduces the effect of the overfitting coming from the PMI approximation, thus achieving much better results than higher dimensional vectors.\nv. Word Embeddings Extensions\nIn this last subsection we will review two interesting works that extend the word embedding concept to phrases and sentences using different approaches.\nIn [Mitchell and Lapata, 2008] the authors address the problem that vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature. The authors suggests the use of two composition operations, multiplication and addition (and their combination). This way the authors are able to combine word embeddings into phrase or sentences embeddings while taking into account important properties like word order and semantic relationship between words (i.e. semantic composition types).\nIn MIL (Multi Instance Transfer Learning) [Kotzias et al., 2014] the authors propose a neural network model which learns embedding at increasing level of hierarchy, starting from word embeddings, going to sentences and ending with entire document embeddings. The authors then use transfer learning by pulling the sentence or word embedding that were trained as part of the document embeddings and use them for sentence or word review classification or similarity tasks (See figure 6 below).\nIII. Word Embeddings vs. Image Embeddings\ni. Image Embeddings\nImage embeddings, or image features, were wildly used for most image processing and classification tasks until the early 2010\u2019s. The features ranged from simple histograms or edge maps to the more sophisticated and very popular SIFT [Lowe, 1999] and HOG\n[Dalal and Triggs, 2005]. However, recent years have seen the rise of Deep Learning for image classification, especially since 2012 when the AlexNet [Krizhevsky et al., 2012] article was published. As those Convolutional Neural Networks (CNN) operated directly on the images, it was suggested that these networks learn the best image features for the specific task that they are trained for, thus obviating the need for specific hand-crafted features.\nIn recent years though, an extensive research was done on the nature and usage of the kernels and features learned by CNN\u2019s. Extensive study of CNN feature layers was done in [Zeiler and Fergus, 2014] where they empirically confirmed that each convolutional layer of the CNN learns a set of filters. Their experiments also confirm that filters complexity and expressive power is rising from layer to layer (i.e. as the network goes deeper) starting from simple edge detectors to complex objects detectors like eyes, flowers, faces and more.\nThe authors also suggest using the pre-trained one before last layer as a feature map, or Image Embeddings input for simpler SVM classifiers.\nAnother popular work was done a bit earlier in [Yangqing et al., 2014] where they also used a pre-trained CNN features as a base for visual recognition tasks. This work was followed by several works with one of them being considered the philosophical father of the algorithm we implement later. In [Razavian et al., 2014] the authors used the one before last layer of a network similar to AlexNet that was pre-trained on ImageNet [Russakovsky et al., 2015] as image embeddings. The authors were able to acheive stateof-art results on several recognition tasks, using simple classifiers like SVM. The result was surprising due to the fact that the CNN model was originally optimized for the task of object classification in ILSVRC 2013 dataset. Nevertheless, it showed itself to be a strong competitor to the more sophisticated and highly tuned state-of-the-art methods.\nThese works and others suggested that given a large enough database of images, a CNN can learn an image embedding which captures the \"essence\" of the picture and can be used later as an input to different tasks, similar to what is done with word embeddings.\nii. Similarities and Differences\nAs we saw earlier Word embedding and Image embeddings are similar in the sense that while they are being learned as part of a specific task, they can be successfully used later for a variety of other tasks. Also, in both cases, similar images or words will usually have similar embeddings. However Word embeddings and image embeddings differ in some aspects.\nThe first difference is that while word embeddings depends mostly on the words surrounding the given word, image embeddings usually rely on the specific image itself. This might explain the fact that linear analogies does not appear naturally in images. An interesting work was done in [Reed et al., 2015] where a neural network is trained to make visual analogies\nand learn to make them based on appearance, rotation, 3D pose, and various object attributes.\nAnother difference is that while word embeddings are usually low-ranked, image embeddings might have same or even higher dimension then the original image. Those embeddings are still useful as they contain a lot of information that is extracted from the image and can be used easily.\nLastly, we notice that word embeddings are trained on a specific corpus where the final embedding results come as the form of wordvectors. This limits the embedding to be valid only for words that were found in the original corpus while other words will need to be initialized as random vectors (as also done in our work). In images on the other hand, the embeddings come as a pre-trained model where features or embeddings can be pulled for any sort of image by feeding the model with the image, making image embeddings models a bit more robust (although they might subjected to other constraints like size and image type).\niii. Joint Word-Image Embeddings\nTo conclude this part we will review some of the recent work done in the exciting area of joint word-image embeddings. The first immediate usage of joint word-image embeddings is image annotations or image labeling. An early notable work was done by\n[Weston, et al., 2010] where a representation of images and representation of annotations where both mapped to a joint feature space by learning a mapping which optimizes top-ofthe-list ranking measures for images and annotations. This method, however, learns linear mappings from image features to the embedding space, and the available labels were only those provided in the image training set. It could thus not generalize to new classes.\nIn 2014 DeVise (A Deep Visual-Semantic Embedding Model) model was shown by [Frome et al., 2013]. This work which continued earlier work [Socher et al., 2013], combined image embedding and word embedding trained separately into joint similarity metric (see figure 8). This enabled them to give performance comparable to a state-of-the-art softmax based model on a flat object classification metric, while simultaneously making more semantically reasonable errors. Their model was also able to make correct predictions across thousands of previously unseen classes by leveraging semantic knowledge elicited only from un-annotated text.\nAnother line of works which combines image and words embeddings is the image captioning area. In this area the embeddings are usually not combined into a joint space but rather used together to create captions for images. In [Karpathy and Fei Fei, 2015] an image\nfeatures pulled from a pre-trained CNN are fed into a Recurrent Neural Network (RNN) which uses word embeddings in order to generate a captioning for the image, based on the image features and previous words (see figure 9). This sort of combination appears in most image captioning works or video action recognition tasks.\nFinally, a slightly more sophisticated method combining RNN\u2019s and Fisher Vectors can be found in [Lev et al., 2015] where the authors were able to achieve state-of-art results on both image captioning and video action recognition tasks, using transfer learning on the embeddings learned for the image captioning tasks.\nIV. CNN for Sentence Classification Model\nIn this section and the following we are going to represent our implementation of The Convolutional Neural Networks for Sentence Classification model [Kim ,2014] and our results. This model gained much popularity since it was first introduced in late 2014, mainly because it provides a very strong demonstration for the power of pre-trained word embeddings.\nThe model and results were examined in detail in [Zhang and Wallace, 2015] where they test many types of configurations for the model, including different sizes and number of filters, different activation units and different word embbeddings.\nA partial implementation of the model was\ndone in Theano framework by the authors4 and another simplified version of the model was done in TensorFlow5. In our work we used small parts of the mentioned codes, however most of the code had to be re-written and expanded in order to perform a true implmentation of the article\u2019s model.\ni. Model details\nThe model architecture, shown in figure 10, is a slight variant of the CNN architecture of [Collobert et al., 2011]. Formally, let xi \u2208 <k be the k-dimensional word vector corresponding to the i-th word in the sentence. Let n be the length (in number of words) of the longest sentence in the dataset, and let lh be the width of the widest filter in the network. Then, the input to the network is a k\u00d7 (n + lh \u2212 1) matrix, which is a concatenation of the word embeddings vectors of each sentences, padded by lh \u2212 1 zero vectors in the beginning and some more zero vectors in the end so there are n + lh \u2212 1 vectors eventually.\nThe input of the network is convolved with filters of different widths (i.e. number of words in the window) and different sizes (i.e. number of features). For example, a feature ci is generated from a window of words xi:i+h\u22121 by a filter with width h is:\nci = f (wxi:i+h\u22121 + b) (3)\nwhere w are the filter weights, b is a bias term\n4https://github.com/yoonkim/CNN_sentence 5https://github.com/dennybritz/cnn-text-\nclassification-tf\nand f is a non-linear function like ReLU. This process is done for all filters and for all words to create a number of feature maps for each filter. Next, those features maps are then maxpooled (so we can deal with different sentence sizes) and finally connected to a soft-max classification layer.\nFor regularization we employ dropout [Hinton et al., 2014] on the penultimate layer. This entails randomly (under some probability) setting values in the weight vector to 0. In the original article they also employed constraint on l2 norms of this layer, however [Zhang and Wallace, 2015] found that it had negligible contribution to results and therefore was not used here.\nTraining of the network is done by minimizing the corss-entropy loss between predicted labels (soft-max layer) and correct ones. The parameters to be estimated include the weight vector(s), of the filter(s), the bias term in the activation function, the weight vector of the softmax function and (optionally) the word embeddings. Optimization is performed using SGD [Rumelhart et al., 1988] and back-propagation, with a small mini-batch size specified later.\nV. Datasets\nWe test our model on various benchmarks. Some of them used in the original article while others are extension we do to the original work.\nYou can see the dataset statistics summary in table 1 below.\n\u2022 MR: Movie reviews with one sentence per review. Classification involves detecting positive/negative reviews [Pang and Lee, 2005]6\n\u2022 SST-1: Stanford Sentiment Treebank-an extension of MR but with train/dev/test splits provided and fine-grained labels (very positive, positive, neutral, negative, very negative), re-labeled by [Socher et al., 2013] 7\n\u2022 SST-2: Same as SST-1 but with neutral reviews removed and binary labels.\n\u2022 Subj: Subjectivity dataset where the task is to classify a sentence as being subjective or objective [Pang and Lee, 2004].\n\u2022 TREC: TREC question dataset-task involves classifying a question into 6 question types (whether the question is about person, location, numeric information,\n6https://www.cs.cornell.edu/people/pabo/moviereview-data/\n7http://nlp.stanford.edu/sentiment/ Data is actually provided at the phrase-level and hence we train the model on both phrases and sentences but only score on sentences at test time, as in [Socher et al., 2013] Thus the training set is an order of magnitude larger than listed in table 1.\netc.) [Li and Roth, 2002]8.\n\u2022 Irony: [Wallace et al., 2014] This contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal. This dataset was not used in the original article but was tested in [Zhang and Wallace, 2015].\n\u2022 Opi: Opinions dataset, which comprises sentences extracted from user reviews on a given topic, e.g. \"sound quality of ipod nano\". There are 51 such topics and each topic contains approximately 100 sentences. The test is to classify which opinion belongs to which topic [Ganesan et al., 2010]. This dataset was not used in the original article but was tested in [Zhang and Wallace, 2015].\n\u2022 Tweet: Tweets from 10 different authors. Classification involves classifying which Tweet belongs to which author9. This dataset was not used in the original article.\n\u2022 Polite: Sentnces taken Wikipedia editors\u2019 logs which have 25 ranges of politeness [Danescu-Niculescu-Mizil et al., 2013]. We narrowed it to 2 binary classes (polite/inpolite). This dataset was not used in the original article.\nVI. Experimental Setup\ni. Hyperparameters and Training\nIn our implementation of the model we experimented with a lof of different configurations. Eventually, since results differences were minor, we decided to use the same architecture and parameters mentioned in the original article for all experiments, with some changes\n8http://cogcomp.cs.illinois.edu/Data/QA/QC/ 9http://www.cs.huji.ac.il/ nogazas/pages/projects.html,\nThanks to Noga Zaslavsky\nmentioned below. Below is a list of parameters and specifications that were used for all experiments:\n\u2022 Word embeddings: We used the pretrained Word2Vec [Mikolov et. al., 2013] mentioned earlier. Each word embedding is in <300. For words that are not found in Word2Vec we randomly initialized them with a uniform distribution in the range of [-0.5,0.5].\n\u2022 Filters: We used filters with window sizes of [3,4,5] with 100 features each. For activation we used ReLU.\n\u2022 Dropout rate: 0.5.\n\u2022 Mini-Batch size: 50.\n\u2022 Optimizer: While AdaDelte optimizer [Zeiler, 2012] was used in the original article. We decided to use the more recent ADAM optimizer [Kingma and Ba, 2014], as it seemed to converge much faster (i.e. needed less epochs on training) and in some cases improved the results.\n\u2022 Learning rate: 0.001. We lower it to\n0.0005 after 8 epchs and to 0.00005 after 16 epochs. Notice that the original article didn\u2019t mention the learning rate.\n\u2022 Number of epochs: This was also not mentioned in the original article but can be found in the authors code10. We used 25 epochs for the static version (see Model Variations below). For non static we used either 4 (MR, SST1, SST2, Subj), 10 (Polite), 16 (Twitter, Opi), or 25 (TREC). For the random version we used 25 except for Tweet where we used 10, and MR and SST-1 where we used 4.\n\u2022 l2-loss: We added l2-loss with \u03bb = 0.15 on the weights and biases of the final layer. Altough this was not done in the original artcle, we found it to slightly improve the results.\nAs mentioned earlier, we decided not to use l2 constrains on the norms due to their negligible contribution.\nii. Model Variations\nWe experiment with several variants of the model like in the original article.\n\u2022 CNN-rand: Our baseline model where all words are randomly initialized and then modified during training.\n10We note here that in the original article they used early stopping with a dev set. However, the early stopping parameters are not mentioned and experiments demanded a lot of coding which is behind the scope of this project. We do assume that the 25 number used in the code might be close enough to the actual number used in the article.\n\u2022 CNN-static: model with pre-trained vectors from word2vec. All words-including the unknown ones that are randomly initialized-are kept static and only the other parameters of the model are learned.\n\u2022 CNN-non-static: Same as above but the pretrained vectors are fine-tuned for each task.\nThe authors also used a multi-channel model where one channel is static and the other is not. However, experiments showed that on most datasets, this did not improve the results. As implementing this would have required a lot more coding, we decided to drop it.\nVII. Results and discussion\nIn this section we will compare the results we got in our implementation to the ones achieved in the original article. Full results can be found in the original article, and we do note that most of them are state-of-art results, or comparable. For datasets that were not present in the original article we shall compare with other achieved results, whether ours or others\u2019.\nIn table 2 above we can see a comparison between our results and the ones in the original article [Kim ,2014]. We can see that overall, our results are comparable (and sometimes better) to the ones in the original article. We also see that like in the original article, our baseline model with all randomly initialized words (CNN-rand) does not perform well on its own (on most cases). These results suggest that the pre-trained vectors are good, \u2019universal\u2019 feature extractors and can be utilized across datasets. Finetuning the pre-trained vectors\nfor each task gives still further improvements (CNN-non-static).\nThe differences in some of our results can be related to the different optimizer we used, and the fact that we did not use early stopping. We do note that our results (at least on the non-static version) were achieved with much less training than the original article11. We also note that on the TREC dataset we were able to achieve a new state-of-art results, improving the current one (95%) by 3.6%. Both these benefits can be related to the use of ADAM [Kingma and Ba, 2014] optimizer.\nOn table 3 we can see our results for datasets that were not used in the original article. We also compare them to other results where applicable.\nOn the Opi and Irony dataset we note that the general line of improved results with pretrained vectors is maintained. On the Opi dataset we were also able to achieve a new state-of-art result. We were also able to achieve comparable results on the Irony dataset. Notice that the other reported result is AUC and not accuracy.\nThe other two results are interesting. On the Tweet dataset we notice that random vectors actually perform a lot better than pre-trained static ones. The reason is that on this dataset, almost half of the vocabulary was not found in the Word2Vec embeddings. This makes sense, as tweets usually contain a lot of marks (for example :-) ) and hashtags which would naturally will not be available in embeddings that\n11That, if we take the 25 epochs in the code we mentioned earlier as an indication to the nubmer of epochs training used in the original article\nwere trained on news. This makes the static version a bad choice as it keep the embeddings random during training.\nOn this dataset we also applied a simple SVM classifier on the TF-IDF features of each tweet. This simple classifier produced much better results, as TF-IDF features are sensitive to uniqe words in a tweet (like hashtags), that usually indicates which is the author, thus making classification easier.\nOn the Poilte dataset we notice that results does not matter on the choice of model. The results themselves are also not very good. This results needs further inspection but they might suggests that this model is not fitted for this task or that politeness is a complicated task for automatic classification.\nVIII. Conclusions and Future Directions\nIn this work we reviewed word embeddings. We saw their origins, discussed the different methods for creating them, and saw some of their interesting properties. We think that word embeddings is a very exciting topic for both research and applications, and expect that a lot research is going to be carried for better understanding of their properties and better creation methods.\nIn this work we also compared Image features and word embeddings and saw how they can be combined to build learning algorithms that can finally gain a good understanding of pictures and scenes. This area is just in its beginning and we expect a lot of work to be carried towards creating a hybrid system which gains understanding of both vision and language, and that combines those understandings together to the benefit of both fields.\nFinally, we saw that despite little tuning of hyperparameters, a simple CNN with one layer of convolution, trained on top of Word2Vec embeddings, performs remarkably well on sentence classification tasks. These results add to the well-established evidence that unsupervised pre-training of word vectors is an important ingredient in deep learning for NLP.\nTo conclude this work we propose here two lines for future work that we think might be interesting to check. First, in the spirit of [Kotzias et al., 2014], we notice that in our network, the one before last layer is actually learning sentence embeddings. It might be interesting to train the network on some classification task with a relatively large dataset, and then use the learned sentence embeddings in the same fashion word embeddings are used in our work. For example we can train the network on the MR task and then take the learned sentence embeddings and use them as an embedding input to some document classification task. We can then check if this method achieves improvement over models that try to classify documents using only pre-trained word embeddings.\nThe second line of research is in the spirit of [Zeiler and Fergus, 2014]. ConvNets visualization helped to gain a lot of insights about image sturctre and how features in increasing level of complexity are combined to create images. It might be interesting to apply those same method of visualization to the filters used in our, or similar works and see if the ConvNet filters learn some interesting semantic properties or compositions that can give insights on the structure of language and how computers (or even humans) percept them.\nReferences\n[Arora et al., 2015] Arora, Sanjeev, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. \"Rand-walk: A latent variable model approach to word embeddings.\" arXiv preprint arXiv:1502.03520 (2015).\n[Collobert and Weston, 2008] Collobert, Ronan, and Jason Weston. \"A unified architecture for natural language processing: Deep neural networks with multitask learning.\" Proceedings of the 25th international conference on Machine learning. ACM. (2008).\n[Collobert et al., 2011] Collobert, Ronan, Jason Weston, L\u00c3l\u2019on Bottou, Michael Karlen,\nKoray Kavukcuoglu, and Pavel Kuksa. \"Natural language processing (almost) from scratch.\" Journal of Machine Learning Research 12, no. Aug (2011): 2493- 2537.\n[Dalal and Triggs, 2005] Dalal, Navneet, and Bill Triggs. \"Histograms of oriented gradients for human detection.\" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905). Vol. 1. IEEE, 2005.\n[Danescu-Niculescu-Mizil et al., 2013] Danescu-Niculescu-Mizil, Cristian, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. \"A computational approach to politeness with application to social factors.\" arXiv preprint arXiv:1306.6078 (2013).\n[Deerwester et al., 1990] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. \"Indexing by latent semantic analysis.\" Journal of the American society for information science 41, no. 6 (1990): 391.\n[Firth, 1957] Firth, J.R. (1957). \"A synopsis of linguistic theory 1930-1955\". Studies in Linguistic Analysis (Oxford: Philological Society): 1-32. Reprinted in F.R. Palmer, ed. (1968). Selected Papers of J.R. Firth 1952-1959. London: Longman.\n[Frome et al., 2013] Frome, Andrea, Greg S. Corrado, Jon Shlens, Samy Bengio, Jeff Dean, and Tomas Mikolov. \"Devise: A deep visual-semantic embedding model.\" In Advances in neural information processing systems, pp. 2121-2129. 2013.\n[Ganesan et al., 2010] Ganesan, Kavita, ChengXiang Zhai, and Jiawei Han. \"Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions.\" Proceedings of the 23rd international conference on computational linguistics. Association for Computational Linguistics, 2010.\n[Goldberg and Levy, 2014] Goldberg, Yoav, and Omer Levy. \"word2vec Explained: deriving Mikolov et al.\u2019s negative-sampling word-embedding method.\" arXiv preprint arXiv:1402.3722 (2014).\n[Gutmann and Hyv\u00c3d\u2019rinen, 2012] Gutmann, Michael U., and Aapo Hyv\u00c3d\u2019rinen. \"Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics.\" Journal of Machine Learning Research 13.Feb (2012): 307-361.\n[Hill et al., 2016] Hill, Felix, Roi Reichart, and Anna Korhonen. \"Simlex-999: Evaluating semantic models with (genuine) similarity estimation.\" Computational Linguistics (2016).\n[Hinton, 1986] Hinton, Geoffrey E. \"Distributed representations.\" Parallel Distributed Processing: Explorations in the Microstructure of Cognition (1986).\n[Hinton et al., 2014] Srivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. \"Dropout: a simple way to prevent neural networks from overfitting.\" Journal of Machine Learning Research 15, no. 1 (2014): 1929-1958.\n[Karpathy and Fei Fei, 2015] Karpathy, Andrej, and Li Fei-Fei. \"Deep visual-semantic alignments for generating image descriptions.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.\n[Kenneth et al., 1990] Church, Kenneth Ward, and Patrick Hanks. \"Word association norms, mutual information, and lexicography.\" Computational linguistics 16.1 (1990): 22-29.\n[Kim ,2014] Kim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\n[Kingma and Ba, 2014] Kingma, Diederik, and Jimmy Ba. \"Adam: A method for\nstochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n[Kotzias et al., 2014] Kotzias, Dimitrios, Misha Denil, Phil Blunsom, and Nando de Freitas. \"Deep multi-instance transfer learning.\" arXiv preprint arXiv:1411.3128 (2014).\n[Krizhevsky et al., 2012] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.\n[Lev et al., 2015] Lev, Guy, Gil Sadeh, Benjamin Klein, and Lior Wolf. \"RNN Fisher Vectors for Action Recognition and Image Annotation.\" arXiv preprint arXiv:1512.03958 (2015).\n[Li and Roth, 2002] Li, Xin, and Dan Roth. \"Learning question classifiers.\" Proceedings of the 19th international conference on Computational linguistics-Volume 1. Association for Computational Linguistics, 2002.\n[Lowe, 1999] Lowe, David G. \"Object recognition from local scale-invariant features.\" Computer vision, 1999. The proceedings of the seventh IEEE international conference on. Vol. 2. Ieee, 1999.\n[Maaten and Hinton, 2008] Maaten, Laurens van der, and Geoffrey Hinton. \"Visualizing data using t-SNE.\" Journal of Machine Learning Research 9.Nov (2008): 2579- 2605.\n[Mikolov et. al., 2013] Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781 (2013).\n[Mitchell et al., 2008] Mitchell, Tom M., Svetlana V. Shinkareva, Andrew Carlson, KaiMin Chang, Vicente L. Malave, Robert A. Mason, and Marcel Adam Just. \"Predicting human brain activity associated with\nthe meanings of nouns.\" science 320, no. 5880 (2008): 1191-1195.\n[Mitchell and Lapata, 2008] Mitchell, Jeff, and Mirella Lapata. \"Vector-based Models of Semantic Composition.\" ACL. 2008.\n[Osgood, 1964] Osgood, Charles E. \"Semantic differential technique in the comparative study of cultures.\" American Anthropologist 66.3 (1964): 171-200.\n[Pang and Lee, 2004] Pang, B., Lee, L. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics (p. 271). Association for Computational Linguistics. 2004.\n[Pang and Lee, 2005] Pang, Bo, and Lillian Lee. \"Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.\" Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics, 2005.\n[Pennington et. al., 2014] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. \"Glove: Global Vectors for Word Representation.\" EMNLP. Vol. 14. (2014).\n[Razavian et al., 2014] Sharif Razavian, Ali, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. \"CNN features offthe-shelf: an astounding baseline for recognition.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 806-813. 2014.\n[Reed et al., 2015] Reed, Scott E., Yi Zhang, Yuting Zhang, and Honglak Lee. \"Deep visual analogy-making.\" In Advances in Neural Information Processing Systems, pp. 1252-1260. 2015.\n[Rumelhart et al., 1988] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J.\nWilliams. \"Learning representations by back-propagating errors.\" Cognitive modeling 5.3 (1988): 1.\n[Russakovsky et al., 2015] Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang et al. \"Imagenet large scale visual recognition challenge.\" International Journal of Computer Vision 115, no. 3 (2015): 211- 252.\n[Salton et al., 1975] Salton, Gerard, Anita Wong, and Chung-Shu Yang. \"A vector space model for automatic indexing.\" Communications of the ACM 18.11 (1975): 613-620.\n[Schwartz et al., 2015] Schwartz, Roy, Roi Reichart, and Ari Rappoport. \"Symmetric pattern based word embeddings for improved word similarity prediction.\" Proc. of CoNLL. 2015.\n[Socher et al., 2013] Socher, Richard, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. \"Recursive deep models for semantic compositionality over a sentiment treebank.\" In Proceedings of the conference on empirical methods in natural language processing (EMNLP), vol. 1631, p. 1642. 2013.\n[Socher et al., 2013] Socher, Richard, Milind Ganjoo, Christopher D. Manning, and Andrew Ng. \"Zero-shot learning through cross-modal transfer.\" In Advances in neural information processing systems, pp. 935-943. 2013.\n[Wallace et al., 2014] Byron C Wallace, Laura Kertz Do Kook Choe, and Eugene Charniak. Humans require context to infer ironic intent (so computers probably do, too). In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2014, pages 512-516.\n[Weston, et al., 2010] Weston, Jason, Samy Bengio, and Nicolas Usunier. \"Large scale\nimage annotation: learning to rank with joint word-image embeddings.\" Machine learning 81.1 (2010): 21-35.\n[Yangqing et al., 2014] Jia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. \"Caffe: Convolutional architecture for fast feature embedding.\" In Proceedings of the 22nd ACM international conference on Multimedia, pp. 675-678. ACM, 2014.\n[Zhang and Wallace, 2015] Zhang, Ye, and Byron Wallace. \"A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification.\"arXiv preprint arXiv:1510.03820 (2015).\n[Zeiler, 2012] Zeiler, Matthew D. \"ADADELTA: an adaptive learning rate method.\" arXiv preprint arXiv:1212.5701 (2012).\n[Zeiler and Fergus, 2014] Zeiler, Matthew D., and Rob Fergus. \"Visualizing and understanding convolutional networks.\" European Conference on Computer Vision. Springer International Publishing, 2014."}], "references": [{"title": "Rand-walk: A latent variable model approach to word embeddings.", "author": ["Arora et al", "2015] Arora", "Sanjeev", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning.", "author": ["Collobert", "Weston", "2008] Collobert", "Ronan", "Jason Weston"], "venue": "Proceedings of the 25th inter-", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch.", "author": ["Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research 12,", "citeRegEx": "Kavukcuoglu and Kuksa.,? \\Q2011\\E", "shortCiteRegEx": "Kavukcuoglu and Kuksa.", "year": 2011}, {"title": "Histograms of oriented gradients for human detection.", "author": ["Dalal", "Triggs", "2005] Dalal", "Navneet", "Bill Triggs"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Dalal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dalal et al\\.", "year": 2005}, {"title": "A computational approach to politeness", "author": ["Danescu-Niculescu-Mizil et al", "2013] Danescu-Niculescu-Mizil", "Cristian", "Moritz Sudhof", "Dan Jurafsky", "Jure Leskovec", "Christopher Potts"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Indexing by latent semantic analysis.", "author": ["Deerwester et al", "1990] Deerwester", "Scott", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1990\\E", "shortCiteRegEx": "al. et al\\.", "year": 1990}, {"title": "Devise: A deep visual-semantic embedding model.\" In Advances in neural information", "author": ["Frome et al", "2013] Frome", "Andrea", "Greg S. Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions.", "author": ["Ganesan et al", "2010] Ganesan", "Kavita", "ChengXiang Zhai", "Jiawei Han"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "word2vec Explained: deriving Mikolov et al.\u2019s negative-sampling word-embedding method.", "author": ["Goldberg", "Levy", "2014] Goldberg", "Yoav", "Omer Levy"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Hyv\u00c3d\u2019rinen. \"Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics.", "author": ["Gutmann", "Hyv\u00c3d\u2019rinen", "2012] Gutmann", "Michael U", "Aapo"], "venue": null, "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation.", "author": ["Hill et al", "2016] Hill", "Felix", "Roi Reichart", "Anna Korhonen"], "venue": "Computational Linguistics", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["Hinton et al", "2014] Srivastava", "Nitish", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions.", "author": ["Karpathy", "Fei Fei", "2015] Karpathy", "Andrej", "Li Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Word association norms, mutual information, and lexicography.\" Computational linguistics", "author": ["Kenneth et al", "1990] Church", "Kenneth Ward", "Patrick Hanks"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1990\\E", "shortCiteRegEx": "al. et al\\.", "year": 1990}, {"title": "Convolutional neural networks for sentence classification.", "author": ["Kim", "Yoon"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "Kim and Yoon.,? \\Q2014\\E", "shortCiteRegEx": "Kim and Yoon.", "year": 2014}, {"title": "Adam: A method", "author": ["Kingma", "Ba", "2014] Kingma", "Diederik", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Deep multi-instance transfer learning.", "author": ["Kotzias et al", "2014] Kotzias", "Dimitrios", "Misha Denil", "Phil Blunsom", "Nando de Freitas"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems", "author": ["Krizhevsky et al", "2012] Krizhevsky", "Alex", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "RNN Fisher Vectors for Action Recognition and Image Annotation.", "author": ["Lev et al", "2015] Lev", "Guy", "Gil Sadeh", "Benjamin Klein", "Lior Wolf"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Learning question classifiers.", "author": ["Li", "Roth", "2002] Li", "Xin", "Dan Roth"], "venue": "Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "Object recognition from local scale-invariant features.", "author": ["Lowe", "1999] Lowe", "David G"], "venue": "Computer vision,", "citeRegEx": "Lowe et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 1999}, {"title": "Visualizing data using t-SNE.", "author": ["Maaten", "Hinton", "2008] Maaten", "Laurens van der", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space.", "author": ["Mikolov et. al", "2013] Mikolov", "Tomas", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Vector-based Models of Semantic Composition.", "author": ["Mitchell", "Lapata", "2008] Mitchell", "Jeff", "Mirella Lapata"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Semantic differential technique in the comparative study of cultures.\" American Anthropologist", "author": ["Osgood", "1964] Osgood", "Charles E"], "venue": null, "citeRegEx": "Osgood et al\\.,? \\Q1964\\E", "shortCiteRegEx": "Osgood et al\\.", "year": 1964}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee", "B. 2004] Pang", "L. Lee"], "venue": "In Proceedings of the 42nd annual meeting on Association", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.", "author": ["Pang", "Lee", "2005] Pang", "Bo", "Lillian Lee"], "venue": "Proceedings of the 43rd annual meeting on association", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Glove: Global Vectors for Word Representation.", "author": ["Pennington et. al", "2014] Pennington", "Jeffrey", "Richard Socher", "Christopher D. Manning"], "venue": "EMNLP. Vol", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "CNN features offthe-shelf: an astounding baseline for recognition.", "author": ["Razavian et al", "2014] Sharif Razavian", "Ali", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In Proceedings of the IEEE", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Deep visual analogy-making.", "author": ["Reed et al", "2015] Reed", "Scott E", "Yi Zhang", "Yuting Zhang", "Honglak Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition", "author": ["Russakovsky et al", "2015] Russakovsky", "Olga", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "A vector space model for automatic indexing.", "author": ["Salton et al", "1975] Salton", "Gerard", "Anita Wong", "Chung-Shu Yang"], "venue": "Communications of the ACM", "citeRegEx": "al. et al\\.,? \\Q1975\\E", "shortCiteRegEx": "al. et al\\.", "year": 1975}, {"title": "Symmetric pattern based word embeddings for improved word similarity prediction.", "author": ["Schwartz et al", "2015] Schwartz", "Roy", "Roi Reichart", "Ari Rappoport"], "venue": "Proc. of CoNLL", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over", "author": ["Socher et al", "2013] Socher", "Richard", "Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Zero-shot learning through cross-modal transfer.\" In Advances in neural information processing", "author": ["Socher et al", "2013] Socher", "Richard", "Milind Ganjoo", "Christopher D. Manning", "Andrew Ng"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Humans require context to infer ironic intent (so computers probably do, too)", "author": ["Wallace et al", "2014] Byron C Wallace", "Laura Kertz Do Kook Choe", "Eugene Charniak"], "venue": "In Proceedings of the Annual Meet-", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature", "author": ["Yangqing et al", "2014] Jia", "Yangqing", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification.\"arXiv preprint arXiv:1510.03820", "author": ["Zhang", "Wallace", "2015] Zhang", "Ye", "Byron Wallace"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method.", "author": ["Zeiler", "2012] Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2012}, {"title": "Visualizing and understanding convolutional networks.", "author": ["Zeiler", "Fergus", "2014] Zeiler", "Matthew D", "Rob Fergus"], "venue": "European Conference on Computer Vision. Springer International Publishing,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "This paper has two parts. In the first part we discuss word embeddings. We discuss the need for them, some of the methods to create them, and some of their interesting properties. We also compare them to image embeddings and see how word embedding and image embedding can be combined to perform different tasks. In the second part we implement a convolutional neural network trained on top of pre-trained word vectors. The network is used for several sentence-level classification tasks, and achieves state-of-art (or comparable) results, demonstrating the great power of pre-trainted word embeddings over random ones.", "creator": "LaTeX with hyperref package"}}}