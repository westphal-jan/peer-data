{"id": "1610.09513", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences", "abstract": "Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.", "histories": [["v1", "Sat, 29 Oct 2016 14:05:10 GMT  (1016kb,D)", "http://arxiv.org/abs/1610.09513v1", "Selected for an oral presentation at NIPS, 2016"]], "COMMENTS": "Selected for an oral presentation at NIPS, 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel neil", "michael pfeiffer", "shih-chii liu"], "accepted": true, "id": "1610.09513"}, "pdf": {"name": "1610.09513.pdf", "metadata": {"source": "CRF", "title": "Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences", "authors": ["Daniel Neil", "Michael Pfeiffer"], "emails": ["shih}@ini.uzh.ch"], "sections": [{"heading": "1 Introduction", "text": "Interest in recurrent neural networks (RNNs) has greatly increased in recent years, since larger training databases, more powerful computing resources, and better training algorithms have enabled breakthroughs in both processing and modeling of temporal sequences. Applications include speech recognition [13], natural language processing [1, 20], and attention-based models for structured prediction [5, 29]. RNNs are attractive because they equip neural networks with memories, and the introduction of gating units such as LSTM and GRU [16, 6] has greatly helped in making the learning of these networks manageable. RNNs are typically modeled as discrete-time dynamical systems, thereby implicitly assuming a constant sampling rate of input signals, which also becomes the update frequency of recurrent and feed-forward units. Although early work such as [25, 10, 4] has realized the resulting limitations and suggested continuous-time dynamical systems approaches towards RNNs, the great majority of modern RNN implementations uses fixed time steps.\nAlthough fixed time steps are perfectly suitable for many RNN applications, there are several important scenarios in which constant update rates impose constraints that affect the precision and efficiency of RNNs. Many real-world tasks for autonomous vehicles or robots need to integrate input from a variety of sensors, e.g. for vision, audition, distance measurements, or gyroscopes. Each sensor may have its own data sampling rate, and short time steps are necessary to deal with sensors with high sampling frequencies. However, this leads to an unnecessarily higher computational load and\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n09 51\n3v 1\n[ cs\n.L G\n] 2\n9 O\nct 2\npower consumption so that all units in the network can be updated with one time step. An interesting new application area is processing of event-based sensors, which are data-driven, and record stimulus changes in the world with short latencies and accurate timing. Processing the asynchronous outputs of such sensors with time-stepped models would require high update frequencies, thereby counteracting the potential power savings of event-based sensors. And finally there is an interest coming from computational neuroscience, since brains can be viewed loosely as very large RNNs. However, biological neurons communicate with spikes, and therefore perform asynchronous, event-triggered updates in continuous time. This work presents a novel RNN model which can process inputs sampled at asynchronous times and is described further in the following sections."}, {"heading": "2 Model Description", "text": "Long short-term memory (LSTM) units [16] (Fig. 1(a)) are an important ingredient for modern deep RNN architectures. We first define their update equations in the commonly-used version from [12]:\nit = \u03c3i(xtWxi + ht\u22121Whi + wci ct\u22121 + bi) (1) ft = \u03c3f (xtWxf + ht\u22121Whf + wcf ct\u22121 + bf ) (2) ct = ft ct\u22121 + it \u03c3c(xtWxc + ht\u22121Whc + bc) (3) ot = \u03c3o(xtWxo + ht\u22121Who + wco ct + bo) (4) ht = ot \u03c3h(ct) (5)\nThe main difference to classical RNNs is the use of the gating functions it, ft, ot, which represent the input, forget, and output gate at time t respectively. ct is the cell activation vector, whereas xt and ht represent the input feature vector and the hidden output vector respectively. The gates use the typical sigmoidal nonlinearities \u03c3i, \u03c3f , \u03c3o and tanh nonlinearities \u03c3c, and \u03c3h with weight parameters Whi, Whf , Who, Wxi, Wxf , and Wxo, which connect the different inputs and gates with the memory cells and outputs, as well as biases bi, bf , and bo. The cell state ct itself is updated with a fraction of the previous cell state that is controlled by ft, and a new input state created from the element-wise (Hadamard) product, denoted by , of it and the output of the cell state nonlinearity \u03c3c. Optional peephole [11] connection weights wci, wcf , wco further influence the operation of the input, forget, and output gates.\nThe Phased LSTM model extends the LSTM model by adding a new time gate, kt (Fig. 1(b)). The opening and closing of this gate is controlled by an independent rhythmic oscillation specified by three parameters; updates to the cell state ct and ht are permitted only when the gate is open. The first parameter, \u03c4 , controls the real-time period of the oscillation. The second, ron, controls the ratio of the duration of the \u201copen\u201d phase to the full period. The third, s, controls the phase shift of the oscillation to each Phased LSTM cell. All parameters can be learned during the training process. Though other variants are possible, we propose here a particularly successful linearized formulation\nof the time gate, with analogy to the rectified linear unit that propagates gradients well:\n\u03c6t = (t\u2212 s) mod \u03c4\n\u03c4 , kt =  2\u03c6t ron , if \u03c6t < 1 2 ron 2\u2212 2\u03c6t ron , if 1 2 ron < \u03c6t < ron\n\u03b1\u03c6t, otherwise\n(6)\n\u03c6t is an auxiliary variable, which represents the phase inside the rhythmic cycle. The gate kt has three phases (see Fig. 2a): in the first two phases, the \"openness\" of the gate rises from 0 to 1 (first phase) and drops from 1 to 0 (second phase). During the third phase, the gate is closed and the previous cell state is maintained. The leak with rate \u03b1 is active in the closed phase, and plays a similar role as the leak in a parametric \u201cleaky\u201d rectified linear unit [15] by propagating important gradient information even when the gate is closed. Note that the linear slopes of kt during the open phases of the time gate allow effective transmission of error gradients.\nIn contrast to traditional RNNs, and even sparser variants of RNNs [19], updates in Phased LSTM can optionally be performed at irregularly sampled time points tj . This allows the RNNs to work with event-driven, asynchronously sampled input data. We use the shorthand notation cj = ctj for cell states at time tj (analogously for other gates and units), and let cj\u22121 denote the state at the previous update time tj\u22121. We can then rewrite the regular LSTM cell update equations for cj and hj (from Eq. 3 and Eq. 5), using proposed cell updates c\u0303j and h\u0303j mediated by the time gate kj :\nc\u0303j = fj cj\u22121 + ij \u03c3c(xjWxc + hj\u22121Whc + bc) (7) cj = kj c\u0303j + (1\u2212 kj) cj\u22121 (8)\nh\u0303j = oj \u03c3h(c\u0303j) (9)\nhj = kj h\u0303j + (1\u2212 kj) hj\u22121 (10)\nA schematic of Phased LSTM with its parameters can be found in Fig. 2a, accompanied by an illustration of the relationship between the time, the input, the time gate kt, and the state ct in Fig. 2b.\nOne key advantage of this Phased LSTM formulation lies in the rate of memory decay. For the simple task of keeping an initial memory state c0 as long as possible without receiving additional inputs (i.e. ij = 0 at all time steps tj), a standard LSTM with a nearly fully-opened forget gate (i.e. fj = 1\u2212 ) after n update steps would contain\ncn = fn cn\u22121 = (1\u2212 ) (fn\u22121 cn\u22122) = . . . = (1\u2212 )n c0 . (11)\nThis means the memory for < 1 decays exponentially with every time step. Conversely, the Phased LSTM state only decays during the open periods of the time gate, but maintains a perfect memory during its closed phase, i.e. cj = cj\u2212\u2206 if kt = 0 for tj\u2212\u2206 \u2264 t \u2264 tj . Thus, during a single oscillation period of length \u03c4 , the units only update during a duration of ron \u00b7 \u03c4 , which will result in substantially fewer than n update steps. Because of this cyclic memory, Phased LSTM can have much longer and adjustable memory length via the parameter \u03c4 .\nThe oscillations impose sparse updates of the units, therefore substantially decreasing the total number of updates during network operation. During training, this sparseness ensures that the gradient is required to backpropagate through fewer updating timesteps, allowing an undecayed gradient to be backpropagated through time and allowing faster learning convergence. Similar to the shielding of the cell state ct (and its gradient) by the input gates and forget gates of the LSTM, the time gate prevents external inputs and time steps from dispersing and mixing the gradient of the cell state."}, {"heading": "3 Results", "text": "In the following sections, we investigate the advantages of the Phased LSTM model in a variety of scenarios that require either precise timing of updates or learning from a long sequence. For all the results presented here, the networks were trained with Adam [18] set to default learning rate parameters, using Theano [2] with Lasagne [9]. Unless otherwise specified, the leak rate was set to \u03b1 = 0.001 during training and \u03b1 = 0 during test. The phase shift, s, for each neuron was uniformly chosen from the interval [0, \u03c4 ]. The parameters \u03c4 and s were learned during training, while the open ratio ron was fixed at 0.05 and not adjusted during training, except in the first task to demonstrate that the model can train successfully while learning all parameters."}, {"heading": "3.1 Frequency Discrimination Task", "text": "In this first experiment, the network is trained to distinguish two classes of sine waves from different frequency sets: those with a period in a target range T \u223c U(5, 6), and those outside the range, i.e. T \u223c {U(1, 5) \u222a U(6, 100)}, using U(a, b) for the uniform distribution on the interval (a, b). This task illustrates the advantages of Phased LSTM, since it involves a periodic stimulus and requires fine timing discrimination. The inputs are presented as pairs \u3008y, t\u3009, where y is the amplitude and t the timestamp of the sample from the input sine wave.\nFigure 3 illustrates the task: the blue curves must be separated from the lighter curves based on the samples shown as circles. We evaluate three conditions for sampling the input signals: In the standard condition (Fig. 3a), the sine waves are regularly sampled every 1 ms; in the oversampled\ncondition (Fig. 3b), the sine waves are regularly sampled every 0.1 ms, resulting in ten times as many data points. Finally, in the asynchronously sampled condition (Fig. 3c), samples are collected at asynchronous times over the duration of the input. Additionally, the sine waves have a uniformly drawn random phase shift from all possible shifts, random numbers of samples drawn from U(15, 125), a random duration drawn from U(15, 125), and a start time drawn from U(0, 125\u2212 duration). The number of samples in the asynchronous and standard sampling condition is equal. The classes were approximately balanced, yielding a 50% chance success rate.\nSingle-layer RNNs are trained on this data, each repeated with five random initial seeds. We compare our Phased LSTM configuration to regular LSTM, and batch-normalized (BN) LSTM which has found success in certain applications [14]. For the regular LSTM and the BN-LSTM, the timestamp is used as an additional input feature dimension; for the Phased LSTM, the time input controls the time gates kt. The architecture consists of 2-110-2 neurons for the LSTM and BN-LSTM, and 1-110-2 for the Phased LSTM. The oscillation periods of the Phased LSTMs are drawn uniformly in the exponential space to give a wide variety of applicable frequencies, i.e., \u03c4 \u223c exp(U(0, 3)). All other parameters match between models where applicable. The default LSTM parameters are given in the Lasagne Theano implementation, and were kept for LSTM, BN-LSTM, and Phased LSTM. Appropriate gate biasing was investigated but did not resolve the discrepancies between the models.\nAll three networks excel under standard sampling conditions as expected, as seen in Fig. 3d (left). However, for the same number of epochs, increasing the data sampling by a factor of ten has devastating effects for both LSTM and BN-LSTM, dropping their accuracy down to near chance (Fig. 3d, middle). Presumably, if given enough training iterations, their accuracies would return to the normal baseline. However, for the oversampled condition, Phased LSTM actually increases in accuracy, as it receives more information about the underlying waveform. Finally, if the updates are not evenly spaced and are instead sampled at asynchronous times, even when controlled to have the same number of points as the standard sampling condition, it appears to make the problem rather challenging for traditional state-of-the-art models (Fig. 3d, right). However, the Phased LSTM has no difficulty with the asynchronously sampled data, because the time gates kt do not need regular updates and can be correctly sampled at any continuous time within the period.\nWe extend the previous task by training the same RNN architectures on signals composed of two sine waves. The goal is to distinguish signals composed of sine waves with periods T1 \u223c U(5, 6) and T2 \u223c U(13, 15), each with independent phase, from signals composed of sine waves with periods T1 \u223c {U(1, 5) \u222a U(6, 100)} and T2 \u223c {U(1, 13) \u222a U(15, 100)}, again with independent phase. Despite being significantly more challenging, Fig. 4a demonstrates how quickly the Phased LSTM converges to the correct solution compared to the standard approaches, using exactly the same parameters. Additionally, the Phased LSTM appears to exhibit very low variance during training."}, {"heading": "3.2 Adding Task", "text": "To investigate how introducing time gates helps learning when long memory is required, we revisit an original LSTM task called the adding task [16]. In this task, a sequence of random numbers is presented along with an indicator input stream. When there is a 0 in the indicator input stream, the presented value should be ignored; a 1 indicates that the value should be added. At the end of presentation the network produces a sum of all indicated values. Unlike the previous tasks, there is no inherent periodicity in the input, and it is one of the original tasks that LSTM was designed to solve well. This would seem to work against the advantages of Phased LSTM, but using a longer period for the time gate kt could allow more effective training as a unit opens only a for a few timesteps during training.\nIn this task, a sequence of numbers (of length 490 to 510) was drawn from U(\u22120.5, 0.5). Two numbers in this stream of numbers are marked for addition: one from the first 10% of numbers (drawn with uniform probability) and one in the last half (drawn with uniform probability), producing a model of a long and noisy stream of data with only few significant points. Importantly, this should challenge the Phased LSTM model because there is no inherent periodicity and every timestep could contain the important marked points.\nThe same network architecture is used as before. The period \u03c4 was drawn uniformly in the exponential domain, comparing four sampling intervals exp(U(0, 2)), exp(U(2, 4)), exp(U(4, 6)), and exp(U(6, 8)). Note that despite different \u03c4 values, the total number of LSTM updates remains approximately the same, since the overall sparseness is set by ron. However, a longer period \u03c4 provides a longer jump through the past timesteps for the gradient during backpropagation-through-time.\nMoreover, we investigate whether the model can learn longer sequences more effectively when longer periods are used. By varying the period \u03c4 , the results in Fig. 4b show longer \u03c4 accelerates training of the network to learn much longer sequences faster."}, {"heading": "3.3 N-MNIST Event-Based Visual Recognition", "text": "To test performance on real-world asynchronously sampled data, we make use of the publiclyavailable N-MNIST [24] dataset for neuromorphic vision. The recordings come from an event-based vision sensor that is sensitive to local temporal contrast changes [26]. An event is generated from a pixel when its local contrast change exceeds a threshold. Every event is encoded as a 4-tuple \u3008x, y, p, t\u3009 with position x, y of the pixel, a polarity bit p (indicating a contrast increase or decrease), and a timestamp t indicating the time when the event is generated. The recordings consist of events generated by the vision sensor while the sensor undergoes three saccadic movements facing a static digit from the MNIST dataset (Fig. 5a). An example of the event responses can be seen in Fig. 5c).\nIn previous work using event-based input data [21, 23], the timing information was sometimes removed and instead a frame-based representation was generated by computing the pixel-wise event-rate over some time period (as shown in Fig. 5(b)). Note that the spatio-temporal surface of\nevents in Fig. 5(c) reveals details of the digit much more clearly than in the blurred frame-based representation.The Phased LSTM allows us to operate directly on such spatio-temporal event streams.\nTable 1 summarizes classification results for three different network types: a CNN trained on framebased representations of N-MNIST digits and two RNNs, a BN-LSTM and a Phased LSTM, trained directly on the event streams. Regular LSTM is not shown, as it was found to perform worse. The CNN was comprised of three alternating layers of 8 kernels of 5x5 convolution with a leaky ReLU nonlinearity and 2x2 max-pooling, which were then fully-connected to 256 neurons, and finally fullyconnected to the 10 output classes. The event pixel address was used to produce a 40-dimensional embedding via a learned embedding matrix [9], and combined with the polarity to produce the input. Therefore, the network architecture was 41-110-10 for the Phased LSTM and 42-110-10 for the BN-LSTM, with the time given as an extra input dimension to the BN-LSTM.\nTable 1 shows that Phased LSTM trains faster than alternative models and achieves much higher accuracy with a lower variance even within the first epoch of training. We further define a factor, \u03c1, which represents the probability that an event is included, i.e. \u03c1 = 1.0 means all events are included. The RNN models are trained with \u03c1 = 0.75, and again the Phased LSTM achieves slightly higher performance than the BN-LSTM model. When testing with \u03c1 = 0.4 (fewer events) and \u03c1 = 1.0 (more events) without retraining, both RNN models perform well and greatly outperform the CNN. This is because the accumulated statistics of the frame-based input to the CNN change drastically when the overall spike rates are altered. The Phased LSTM RNNs seem to have learned a stable spatio-temporal surface on the input and are only slightly altered by sampling it more or less frequently.\nFinally, as each neuron of the Phased LSTM only updates about 5% of the time, on average, 159 updates are needed in comparison to the 3153 updates needed per neuron of the BN-LSTM, leading to an approximate twenty-fold reduction in run time compute cost. It is also worth noting that these results form a new state-of-the-art accuracy for this dataset [24, 7]."}, {"heading": "3.4 Visual-Auditory Sensor Fusion for Lip Reading", "text": "Finally, we demonstrate the use of Phased LSTM on a task involving sensors with different sampling rates. Few RNN models ever attempt to merge sensors of different input frequencies, although the sampling rates can vary substantially. For this task, we use the GRID dataset [8]. This corpus contains video and audio of 30 speakers each uttering 1000 sentences composed of a fixed grammar and a constrained vocabulary of 51 words. The data was randomly divided into a 90%/10% train-test set. An OpenCV [17] implementation of a face detector was used on the video stream to extract the face which was then resized to grayscale 48x48 pixels. The goal here is to obtain a model that can use audio alone, video alone, or both inputs to robustly classify the sentence. However, since the audio alone is sufficient to achieve greater than 99% accuracy, sensor modalities were randomly masked to zero during training to encourage robustness towards sensory noise and loss.\nThe network architecture first separately processes video and audio data before merging them in two RNN layers that receive both modalities. The video stream uses three alternating layers of 16 kernels of 5x5 convolution and 2x2 subsampling to reduce the input of 1x48x48 to 16x2x2, which is then used as the input to 110 recurrent units. The audio stream connects the 39-dimensional MFCCs (13 MFCCs with first and second derivatives) to 150 recurrent units. Both streams converge into the Merged-1 layer with 250 recurrent units, and is connected to a second hidden layer with 250 recurrent units named Merged-2. The output of the Merged-2 layer is fully-connected to 51 output nodes, which represent the vocabulary of GRID. For the Phased LSTM network, all recurrent units are Phased LSTM units.\nIn the audio and video Phased LSTM layers, we manually align the open periods of the time gates to the sampling times of the inputs and disable learning of the \u03c4 and s parameters (see Fig. 6a). This prevents presenting zeros or artificial interpolations to the network when data is not present. In the merged layers, however, the parameters of the time gate are learned, with the period \u03c4 of the first merged layer drawn from U(10, 1000) and the second from U(500, 3000). Fig. 6b shows a visualization of one frame of video and the complete duration of an audio sample.\nDuring evaluation, all networks achieve greater than 98% accuracy on audio-only and combined audio-video inputs. However, video-only evaluation with an audio-video capable network proved the most challenging, so the results in Fig. 6c focus on these results (though result rankings are representative of all conditions). Two differently-sampled versions of the data were used: In the first \u201clow resolution\u201d version (Fig. 6c, top), the sampling rate of the MFCCs was matched to the sampling rate of the 25 fps video. In the second \u201chigh-resolution\u201d condition, the sampling rate was set to the more common value of 100 Hz sampling frequency (Fig. 6c, bottom and shown in Fig. 6a). The higher audio sampling rate did not increase accuracy, but allows for a faster latency (10ms instead of 40ms). The Phased LSTM again converges substantially faster than both LSTM and batch-normalized LSTM. The peak accuracy of 81.15% compares favorably against lipreading-focused state-of-the-art approaches [28] while avoiding manually-crafted features."}, {"heading": "4 Discussion", "text": "The Phased LSTM has many surprising advantages. With its rhythmic periodicity, it acts like a learnable, gated Fourier transform on its input, permitting very fine timing discrimination. Alternatively, the rhythmic periodicity can be viewed as a kind of persistent dropout that preserves state [27], enhancing model diversity. The rhythmic inactivation can even be viewed as a shortcut to the past for gradient backpropagation, accelerating training. The presented results support these interpretations, demonstrating the ability to discriminate rhythmic signals and to learn long memory traces. Importantly, in all experiments, Phased LSTM converges more quickly and theoretically requires only 5% of the computes at runtime, while often improving in accuracy compared to standard LSTM. The presented methods can also easily be extended to GRUs [6], and it is likely that even simpler models, such as ones that use a square-wave-like oscillation, will perform well, thereby making even more efficient and encouraging alternative Phased LSTM formulations. An inspiration for using oscillations in recurrent networks comes from computational neuroscience [3], where rhythms have been shown to play important roles for synchronization and plasticity [22]. Phased LSTMs were not designed as biologically plausible models, but may help explain some of the advantages and robustness of learning in large spiking recurrent networks."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Rhythms of the Brain", "author": ["G. Buzsaki"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "An analog VLSI recurrent neural network learning a continuous-time trajectory", "author": ["G. Cauwenberghs"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["K. Cho", "A. Courville", "Y. Bengio"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Skimming digits: Neuromorphic classification of spike-encoded images", "author": ["G.K. Cohen", "G. Orchard", "S.H. Ieng", "J. Tapson", "R.B. Benosman", "A. van Schaik"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Approximation of dynamical systems by continuous time recurrent neural networks", "author": ["K.-I. Funahashi", "Y. Nakamura"], "venue": "Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Recurrent nets that time and count", "author": ["F.A. Gers", "J. Schmidhuber"], "venue": "In Neural Networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In The IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A clockwork rnn", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1402.3511,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Effective sensor fusion with event-based sensors and deep network architectures", "author": ["D. Neil", "S.-C. Liu"], "venue": "In IEEE Int. Symposium on Circuits and Systems (ISCAS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity", "author": ["B. Nessler", "M. Pfeiffer", "L. Buesing", "W. Maass"], "venue": "PLoS Comput Biol,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Real-time classification and sensor fusion with a spiking Deep Belief Network", "author": ["P. O\u2019Connor", "D. Neil", "S.-C. Liu", "T. Delbruck", "M. Pfeiffer"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Converting static image datasets to spiking neuromorphic datasets using saccades", "author": ["G. Orchard", "A. Jayawant", "G. Cohen", "N. Thakor"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Learning state space trajectories in recurrent neural networks", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1989}, {"title": "Retinomorphic event-based vision sensors: bioinspired cameras with spiking outputs", "author": ["C. Posch", "T. Serrano-Gotarredona", "B. Linares-Barranco", "T. Delbruck"], "venue": "Proceedings of the IEEE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Recurrent dropout without memory", "author": ["S. Semeniuta", "A. Severyn", "E. Barth"], "venue": "loss. arXiv,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Lipreading with long short-term memory", "author": ["M. Wand", "J. Koutn\u00edk", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1601.08188,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Applications include speech recognition [13], natural language processing [1, 20], and attention-based models for structured prediction [5, 29].", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "Applications include speech recognition [13], natural language processing [1, 20], and attention-based models for structured prediction [5, 29].", "startOffset": 74, "endOffset": 81}, {"referenceID": 17, "context": "Applications include speech recognition [13], natural language processing [1, 20], and attention-based models for structured prediction [5, 29].", "startOffset": 74, "endOffset": 81}, {"referenceID": 4, "context": "Applications include speech recognition [13], natural language processing [1, 20], and attention-based models for structured prediction [5, 29].", "startOffset": 136, "endOffset": 143}, {"referenceID": 26, "context": "Applications include speech recognition [13], natural language processing [1, 20], and attention-based models for structured prediction [5, 29].", "startOffset": 136, "endOffset": 143}, {"referenceID": 14, "context": "RNNs are attractive because they equip neural networks with memories, and the introduction of gating units such as LSTM and GRU [16, 6] has greatly helped in making the learning of these networks manageable.", "startOffset": 128, "endOffset": 135}, {"referenceID": 5, "context": "RNNs are attractive because they equip neural networks with memories, and the introduction of gating units such as LSTM and GRU [16, 6] has greatly helped in making the learning of these networks manageable.", "startOffset": 128, "endOffset": 135}, {"referenceID": 22, "context": "Although early work such as [25, 10, 4] has realized the resulting limitations and suggested continuous-time dynamical systems approaches towards RNNs, the great majority of modern RNN implementations uses fixed time steps.", "startOffset": 28, "endOffset": 39}, {"referenceID": 8, "context": "Although early work such as [25, 10, 4] has realized the resulting limitations and suggested continuous-time dynamical systems approaches towards RNNs, the great majority of modern RNN implementations uses fixed time steps.", "startOffset": 28, "endOffset": 39}, {"referenceID": 3, "context": "Although early work such as [25, 10, 4] has realized the resulting limitations and suggested continuous-time dynamical systems approaches towards RNNs, the great majority of modern RNN implementations uses fixed time steps.", "startOffset": 28, "endOffset": 39}, {"referenceID": 14, "context": "Long short-term memory (LSTM) units [16] (Fig.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "We first define their update equations in the commonly-used version from [12]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "Optional peephole [11] connection weights wci, wcf , wco further influence the operation of the input, forget, and output gates.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "The leak with rate \u03b1 is active in the closed phase, and plays a similar role as the leak in a parametric \u201cleaky\u201d rectified linear unit [15] by propagating important gradient information even when the gate is closed.", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "In contrast to traditional RNNs, and even sparser variants of RNNs [19], updates in Phased LSTM can optionally be performed at irregularly sampled time points tj .", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "For all the results presented here, the networks were trained with Adam [18] set to default learning rate parameters, using Theano [2] with Lasagne [9].", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "For all the results presented here, the networks were trained with Adam [18] set to default learning rate parameters, using Theano [2] with Lasagne [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 12, "context": "We compare our Phased LSTM configuration to regular LSTM, and batch-normalized (BN) LSTM which has found success in certain applications [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "(b) Frame-based representation of an \u20188\u2019 digit from the N-MNIST dataset [24] obtained by integrating all input spikes for each pixel.", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "To investigate how introducing time gates helps learning when long memory is required, we revisit an original LSTM task called the adding task [16].", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "To test performance on real-world asynchronously sampled data, we make use of the publiclyavailable N-MNIST [24] dataset for neuromorphic vision.", "startOffset": 108, "endOffset": 112}, {"referenceID": 23, "context": "The recordings come from an event-based vision sensor that is sensitive to local temporal contrast changes [26].", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "In previous work using event-based input data [21, 23], the timing information was sometimes removed and instead a frame-based representation was generated by computing the pixel-wise event-rate over some time period (as shown in Fig.", "startOffset": 46, "endOffset": 54}, {"referenceID": 20, "context": "In previous work using event-based input data [21, 23], the timing information was sometimes removed and instead a frame-based representation was generated by computing the pixel-wise event-rate over some time period (as shown in Fig.", "startOffset": 46, "endOffset": 54}, {"referenceID": 21, "context": "It is also worth noting that these results form a new state-of-the-art accuracy for this dataset [24, 7].", "startOffset": 97, "endOffset": 104}, {"referenceID": 6, "context": "It is also worth noting that these results form a new state-of-the-art accuracy for this dataset [24, 7].", "startOffset": 97, "endOffset": 104}, {"referenceID": 7, "context": "For this task, we use the GRID dataset [8].", "startOffset": 39, "endOffset": 42}, {"referenceID": 25, "context": "15% compares favorably against lipreading-focused state-of-the-art approaches [28] while avoiding manually-crafted features.", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "Alternatively, the rhythmic periodicity can be viewed as a kind of persistent dropout that preserves state [27], enhancing model diversity.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "The presented methods can also easily be extended to GRUs [6], and it is likely that even simpler models, such as ones that use a square-wave-like oscillation, will perform well, thereby making even more efficient and encouraging alternative Phased LSTM formulations.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "An inspiration for using oscillations in recurrent networks comes from computational neuroscience [3], where rhythms have been shown to play important roles for synchronization and plasticity [22].", "startOffset": 98, "endOffset": 101}, {"referenceID": 19, "context": "An inspiration for using oscillations in recurrent networks comes from computational neuroscience [3], where rhythms have been shown to play important roles for synchronization and plasticity [22].", "startOffset": 192, "endOffset": 196}], "year": 2016, "abstractText": "Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.", "creator": "LaTeX with hyperref package"}}}