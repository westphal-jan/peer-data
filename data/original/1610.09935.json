{"id": "1610.09935", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Knowledge Questions from Knowledge Graphs", "abstract": "We address the novel problem of automatically generating quiz-style knowledge questions from a knowledge graph such as DBpedia. Questions of this kind have ample applications, for instance, to educate users about or to evaluate their knowledge in a specific domain. To solve the problem, we propose an end-to-end approach. The approach first selects a named entity from the knowledge graph as an answer. It then generates a structured triple-pattern query, which yields the answer as its sole result. If a multiple-choice question is desired, the approach selects alternative answer options. Finally, our approach uses a template-based method to verbalize the structured query and yield a natural language question. A key challenge is estimating how difficult the generated question is to human users. To do this, we make use of historical data from the Jeopardy! quiz show and a semantically annotated Web-scale document collection, engineer suitable features, and train a logistic regression classifier to predict question difficulty. Experiments demonstrate the viability of our overall approach.", "histories": [["v1", "Mon, 31 Oct 2016 14:27:07 GMT  (440kb,D)", "http://arxiv.org/abs/1610.09935v1", null], ["v2", "Tue, 1 Nov 2016 05:39:25 GMT  (382kb,D)", "http://arxiv.org/abs/1610.09935v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dominic seyler", "mohamed yahya", "klaus berberich"], "accepted": false, "id": "1610.09935"}, "pdf": {"name": "1610.09935.pdf", "metadata": {"source": "CRF", "title": "Knowledge Questions from Knowledge Graphs", "authors": ["Dominic Seyler", "Mohamed Yahya", "Klaus Berberich"], "emails": ["dseyler2@illinois.edu", "myahya@mpi-inf.mpg.de", "klaus.berberich@htwsaar.de"], "sections": [{"heading": "1. INTRODUCTION", "text": "Knowledge graphs (KGs) such as YAGO [42] and DBpedia [4] contain facts about real-world named entities. They provide taxonomic knowledge, for instance, that BarackObama is a person as well as a formerSenator. They also contain factual knowledge, for instance, that BarackObama is married to MichelleObama and was born on August 4, 1961. Textual knowledge captures how named entities and their relationships are referred to in natural language, for example, BarackObama as \u2018Barack H. Obama\u2019.\nEasily extensible data formats such as RDF are commonly used to store KGs, which makes it easy to complement them with additional facts without having to worry about a predefined schema. RDF stores facts as (subject, predicate, object) triples, which can then be queried using SPARQL as a simple-yet-powerful structured query language.\nIn this work, we address the problem of generating quiz-\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nc\u00a9 2017 Copyright held by the owner/author(s).\nBarackObamaGrammyAward\nlawyer president\ntypetype\npersonentity\naward\nisAisAisA isA\nwon\ntype\nHonolulu\nbornIn\ncity typeisA\n?x president type Honolulu bornIn\n?x president type GrammyAward won\nBillClinton\ntype\nQ1\nQ2\nIllinois state\nTUS Presidents = {BarackObama, RonaldRegan, ...}\nBarackObamaGrammyAward\nwon\n?x president type Honolu u bornIn\n?x president type GrammyAward won\nBillClinton\nQ1\nQ2\nIllinois state\nIllinois\nstate\n\u201cThis president from Illinois won a Grammy.\u201d diff (Q1, BarackObama) = hard\ndisthardQ1 = RonaldRegan dist easy Q1 = HarryTruman\nFigure 1: A fragment of a KG, a topic, and a hard question generated from it. Two distractors for turning it into a multiple choice question are shown, one easy to rule out and one hard (Regan had a Hollywood career before becoming president).\nstyle knowledge questions from KGs. As shown in Figure 1, starting from a KG and a topic such as US Presidents, we generate a quiz question whose unique answer is an entity from that topic. The question starts its life as an automatically generated triple-pattern query, which our system verbalizes. Each generated question is adorned with a difficulty level, providing an estimate for how hard it is to answer, and optionally a set of distractors, which can be listed alongside the correct answer to obtain a multiple-choice question. Our system is able to judge the impact of the distractors on the difficulty of the resulting multiple-choice question.\nApplications of automatically generated knowledge questions include education and evaluation. One way to educate users about a specific domain (e.g., Sports or Politics) is to prompt them with questions, so that they pick up facts as they try to answer \u2013 reminiscent of flash cards used by pupils. When qualification for a task needs to be ensured, such as knowledge about a specific domain, automatically generated knowledge questions can serve as a qualification test. Crowdsourcing is one concrete use case as outlined in [39]. Likewise, knowledge questions can serve as a form of CAPTCHA to exclude likely bots.\nChallenges. To discriminate how much people know about a domain, it is typical to ask progressively more difficult questions. In our setting, this means that we need to automatically quantify the difficulty of a question. This is\nar X\niv :1\n61 0.\n09 93\n5v 1\n[ cs\n.C L\n] 3\n1 O\nct 2\n01 6\nnot a trivial task as it requires to take into consideration multiple signals and their interaction. One might, for example, consider all questions whose answer is BarackObama to be easy, as he is a prominent entity. However, very few people would know that he won a GrammyAward. It is therefore important to identify signals that predict question difficulty and to combine them in a meaningful manner.\nAnswers provided by the user should be easy to verify automatically. In our setting, we want to ensure that disputes about the correctness of an answer are minimal, since we envision a setting with minimal human involvement (on the asking side). One important way to achieve this is by ensuring that each question has exactly one correct answer. Complementary to having questions with unique correct answers is dealing with possible variation in user input (e.g., \u2018Barack Obama\u2019 vs \u2018Barack H. Obama\u2019 ). One way of overcoming this is by turning fill-in-the-blank questions to multiple-choice questions. Here, one needs to carefully consider the impact distractors have on question difficulty.\nA final challenge is the production of well-formed natural language questions. We are interested not only in correct language, but also in generating questions that do not look artificial. Such questions are desirable not only for their aesthetic appeal, but also minimize the chance of humans discovering that such questions were generated automatically. An important consideration here is how to ensure that coherent questions have sufficient variety. For example, while a KG may classify BarackObama as both an entity and a formerSenator, we would like to use the latter in asking about him, as the first is unnatural. Similarly, while the relation connecting TobeyMaguire to Spider-Man might be called actedIn, we would like to have some variety in how this is expressed (e.g., \u2018acted in\u2019 or \u2018starred in\u2019 )\nContributions. We propose an end-to-end approach to the novel problem of generating quiz-style knowledge questions from knowledge graphs. Our approach has three major components: query generation, difficulty estimation, and query verbalization to generate a question. In a setting where multiple-choice questions are desired, a fourth component takes care of both generating the distractors and quantifying their impact on question difficulty. Figure 2 depicts our pipeline for generating questions and multiple choice questions.\nThe query generation component generates a structured query that will serve as the basis of the final question shown to a human. By starting from a structured query, we are able to generate questions that are certain to have exactly one unique, correct answer in our knowledge graph. In query generation, several challenges need to be addressed so that the resulting cues are meaningful.\nDifficulty estimation is one of the challenges that needs to be addressed. To estimate the difficulty of a structured query, we leverage different signals about contained named entities, which we derive from a Web-scale document collection annotated with named entities from the KG. To learn weighting those signals, we make use of more than thirty years\u2019 worth of data from the Jeopardy! quiz show.\nSince our questions start their life as structured queries over the KG, we also verbalize them by generating a corresponding natural language question. Following earlier work on query verbalization and natural language generation, we adopt a template-based approach. However, we extend this approach with automatically mined paraphrases for rela-\ntions and classes in the KG, ensuring diversity in the resulting natural language questions.\nOutline. The rest of this paper unfolds as follows. Section 2 introduces preliminaries and provides a formal statement of the problem addressed in this work. Following that, we provide details on each stage shown in Figure 2. Section 3 describes how a SPARQL query can be generated that has a unique answer in the KG. Our approach for estimating the difficulty of the generated query is subject to Section 4. Section 5 describes how the query can be verbalized into natural language. Extensions for multiple-choice questions are described in Section 6. Section 7 lays out the setup and results of our experiments. We put our work in context with existing prior research in Section 8, before concluding in Section 9."}, {"heading": "2. PRELIMINARIES AND PROBLEM STATEMENT", "text": "We now lay out preliminaries and formally state the problem addressed in this work.\nKnowledge Graphs (KGs) such as as Freebase [10], Yago [42], and DBpedia [4] describe entities E (e.g., BarackObama) by connecting them to other entities, types T \u2014 also called classes (e.g., president, leader), and literals L (e.g., \u20181985-02-05\u2019 ) using predicates P (e.g., bornIn, birthdate, type). A KG is thus a set of facts (or triples), {f | f \u2208 E \u222a T \u00d7 P \u00d7 E \u222a T \u222a L}. A triple can also be seen as an instance of a binary predicate, with the first argument called the subject and the second called the object, hence the name subject-predicate-object (SPO). Figure 1 shows a KG fragment.\nPattern-matching is used to query a KG. Given a set of variables V that are always prefixed with a question mark (e.g., ?x), a triple-pattern-query is a set of triple patterns Q = {q | q \u2208 V \u222a E \u222a T \u00d7 V \u222a P \u00d7 V \u222a E \u222a T \u222a L}. An answer a to a query is a total mapping of variables to items in the KG such that the application of a to each q results in a fact in the KG. In our setting, inspired by Jeopardy!, we restrict ourselves to queries having a single variable for which a unique answer exists in the KG. Put differently, there exists only one binding of the single variable to a named entity, so that all triple patterns have corresponding facts in the KG.\nMore specifically, we use Yago2s [43] as our reference knowledge graph in this work. Yago2s is automatically constructed by combining information extraction over Wikipedia infoboxes and categories with the lexical database WordNet [15]. In total, Yago2s contains 2.6m entities, 300k types organized into a type hierarchy, and more than a hundred predicates which are used to form more than 48m facts. Yago entities are associated with Wikipedia entries, whereas a Yago type corresponds to a WordNet synset or Wikipedia category. To compute signals necessary for estimating question difficulty, we make use of the ClueWeb09/12 document collections and the FACC annotations provided by Google [20]. The latter\nprovide semantic annotations of disambiguated named entities from Freebase, which we can easily map to Yago2s via their corresponding Wikipedia article. An annotated sentence in this corpus looks as follows:\n\u201c [Obama|BarackObama] endorsed [Clinton|HillaryClinton] earlier today.\u201d\nJeopardy! is a popular U.S. TV quiz show that features comprehensive natural language questions that are referred to as clues. Clues are usually posed as a statement and the required answer is in turn posed as a question. For instance, in Jeopardy! the question: This fictional private investigator was created by Arthur Conan Doyle. has the answer: Who is Sherlock Holmes? Clues come with monetary values, corresponding to the amount added to a contestant\u2019s balance when answering correctly. We reckon that monetary values correlate with human performance and thus question difficulty \u2013 a hypothesis which we investigate in Section 4.\nProblem Statement. Put formally, our objective in this work is to automatically generate a question Q whose unique answer is an entity e \u2208 T which can be supported by facts in the KG. T is a thematic set of entities called a topic topic, which allows us to control the domain from which knowledge questions are generated (e.g., American Politics). Moreover, we assume a predefined set of difficulty levels D = {d1, ..., dn} with a strict total order < defined over its elements, and we want to estimate the difficulty of providing the answer a to Q, denoted diff (Q, a). An extension of the above problem which we also deal with in this work is the generation of multiple choice questions (MCQ\u2019s), where the task is to extend a question Q into a MCQ by generating a set of incorrect answers, called distractors, and quantifying their difficulty.\nIn our concrete instantiation of the above problem, we use Wikipedia categories as topics and Yago2s as our KG. As a first attempt to address the above problem, we consider a setting with two difficulty levels, D = {easy, hard}, where easy < hard. For our purposes, a question is any natural language sentence that requires an answer. It can look like what we think of as a question, or as a declarative sentence in the same style as Jeopardy! clues."}, {"heading": "3. QUERY GENERATION", "text": "The first stage in our pipeline is the generation of a query that has a unique answer in the KG. This query serves as the basis for generating a question that will be shown to human contestants. The unique answer will be the one a contestant needs to provide in order to correctly answer the question. As is common practice in quiz-games, ensuring that a question has a single answer simplifies answer verification.\nThe input to the query generation step is a topic T. The unique answer to the generated query will be an entity e \u2208 T randomly drawn from the KG. Query generation is guided by the following desiderata: i) the query should contain at least one type triple pattern, which is crucial when verbalizing the query to generate a question (e.g., \u201cWhich president . . . \u201d), and ii) entities mentioned in the query should not give any obvious clues about the answer entity. In what follows we present the challenges in achieving each of these desiderata, and our solutions to these challenges."}, {"heading": "3.1 Answer Type Selection", "text": "Questions asking for entities always require a type that is either specified implicitly (e.g., \u2018who\u2019 for person and \u2018where\u2019 for location) or explicitly (e.g., \u201cWhich president . . . \u201d). Here we address the problem of selecting a type to refer to the answer entity in the question. KGs tend to contain a large number of types and typically associate an entity with multiple types. Some of these types are easy for an average human to understand and typically appear in text talking about an entity (e.g., president, lawyer). Other types, however, are artifacts of attempts to have an ontologically complete and formally sound type system. Such types are meaningful only in the context of a type system, but not on their own (e.g., the type entity or thing).\nWe use our entity-annotated corpus to capture the salience of a semantic type t for an entity e, denoted s(t, e). We start by collecting occurrences of an entity e along with textual types to which it belongs ttext in our entity-annotated corpus. We use the following patterns to collect (ttext, e) pairs: Pattern #1: ENTITY (\u2018is a\u2019 |\u2018is an\u2019 |\u2018, a\u2019 |\u2018and other\u2019 |\u2018or other\u2019 ) TYPE \u201cBarackObama and other presidents attended the ceremony.\u201d\nPattern #2: TYPE (\u2018like\u2019 |\u2018such as\u2019 |\u2018including\u2019 |\u2018especially\u2019 |) ENTITY \u201c...several attorneys including BarackObama\u201d These patterns are inspired by Hearst [23].\nThe next step before computing semantic type salience is to disambiguate (ttext, e) pairs to (t, e) pairs \u2014 note that entities are already disambiguated in the corpus, so we only need to disambiguate ttext to a semantic type t in the KG. Relying on the fact that our semantic types are WordNet synsets [15], we use the lexicon that comes with WordNet (e.g., {lawyer, attorney} \u2192 lawyer) for generating a set of semantic type candidates for a given textual type. We then use a simple yet effective heuristic where a textual type ttext paired with an entity e is disambiguated to a semantic type t if i) t is in the set of candidates for ttext and ii) e \u2208 t.\nWe compute salience s(t, e) as the relative frequency with which the disambiguated (t, e) pair was observed in our corpus. To select a type for the answer entity e, we draw one of the types to which it belongs randomly based on s(t, e)."}, {"heading": "3.2 Triple Pattern Generation", "text": "We now have an answer entity e and one of its semantic types t that will be used to refer to e in the question. We now need to create a query (which includes the type constraint t) whose unique answer over the KG is e. We focus here on questions with unknown entities as these are the ones we can use Jeopardy! data to train our difficulty classifier on [17]. In principle, we can allow for unknown relations or types as well if we had the right training data. Creating a query means selecting facts where e is either the subject or object and turning these into triple patterns by replacing e with a variable (?x). Not all facts can be used here, as some reveal too much about the answer and render the question too trivial. Other facts will be redundant given the facts already used.\nElimination of Textual Overlap with the Answer. The first restriction we impose on a fact is that the surface forms of entities that appear in it cannot have any textual overlap with surface forms of the answer entity. The question \u201cThis president is married to Michelle Obama.\u201d reveals\ntoo much about the answer entity. For overlap, we look at the set of words in the surface forms, excluding common stop words. We discuss our approach to collecting surface forms for entities in Section 5.2 below.\nElimination of Redundant Facts. Given a set of facts that has been chosen, a new fact does not always add new information. Keeping this new fact in a query will result in an awkwardly phrased question that can be clearly identified by a human as having been automatically generated. In our example from Figure 1 we decided to use the type president to ask about BarackObama. Using the fact (BarackObama type politician) or the fact (BarackObama type person) to extend the question is clearly redundant and adds no extra information. To eliminate this issue, we check each new type fact against all existing ones. If the new type is a supertype (e.g., person) of an existing one (e.g., president), we discard it."}, {"heading": "4. DIFFICULTY ESTIMATION", "text": "We now describe our approach to estimating the difficulty of answering the knowledge query generated in Section 3. There are several, seemingly contradictory, signals that affect the difficulty of a question. As discussed earlier, one might expect any question asking for a popular entity such as BarackObama to be an easy one. However, if we were to ask \u201cThis president from Illinois won a Grammy Award.\u201d, few people are likely to think of BarackObama. We use a classification model trained on a corpus of questions paired with their difficulties to predict question difficulty.\nNote that the difficulty is computed based on the query and not its verbalization, which we generate in the next section. Our goal here is to create questions that measure factual knowledge rather than linguistic ability. We elaborate on this point further in Section 5.\nSince we rely on supervised training for difficulty estimation, we make the natural assumption that the difficulty labels in the training and \u2018testing\u2019 questions are drawn from the same underlying distribution for some target audience. We also assume that for this population, it is possibly to capture the difficulty of a question. As evidence for this, in the Jeopardy! dataset [1] we find a positive correlation between the attempted questions for a certain difficulty-level and the number of times a question of this difficulty-level could not be answered. For the five difficulty-levels ($200, $400, $600, $800, $1000), 4.46%, 8.35%, 12.69%, 17.82% and 25.69% of the questions could not be answered, respectively."}, {"heading": "4.1 Data Preparation", "text": "We use the Jeopardy! quiz-game show data described in Section 2 for training and testing our difficulty estimation classifier. The larger goal is to estimate the difficulty of answering queries generated from a knowledge graph, so we restrict ourselves to a subset of the Jeopardy! questions answerable from Yago [42], which we collected as described below. However, all methods and tools are general enough to apply to a setting other than ours of Jeopardy!/Yago.\nWe say a question is answerable form Yago if i) all entities mentioned in the question and its answer are in Yago, and ii) all relations connecting these entities are captured by Yago. To find these questions, we automatically annotate the questions with Yago entities using the Stanford CoreNLP named entity recognizer (NER) [18] in conjunction with the AIDA tool for named entity disambiguation [25]. We concatenate\nthe output of the NER system with the answer entity, which we annotate as an entity mention as well, and pass it to AIDA for an improved disambiguation context. An example of an input to AIDA looks as follows:\n[Shah Jahan] built this complex in [Agra, India] to immortalize [Mumtaz], his favorite wife. [Taj Mahal]\nand the corresponding disambiguated output is:\nShahJahan built this complex in Agra to immortalize MumtazMahal, his favorite wife. TajMahal\nWe retain an entity-annotated question if i) its answer can be mapped to a Yago entity, ii) its body has at least one entity (the one that will be given in the question, not the answer), and iii) considering all entities in the question and the answer entity, each entity can be paired with another entity to which it has a direct relation in Yago. The last condition ensures that we have questions that can be captured by the relationships in Yago. However, it does not identify this relation, and such a match may be spurious. Since this is hard to establish automatically, we invoke humans at this point.\nWe run a crowdsourcing task on the questions that survive the above automated annotation and filtering procedure. The task is to assign one of two labels to an entityannotated question/answer pair. A question/answer pair is to be labeled Good if i) all entities in the question have been captured and disambiguated correctly, ii) the question can be captured by relations in Yago, and iii) the answer is a unique one. The crowdsourcing task ran until we obtained a total of 500 questions that we use in our experiments."}, {"heading": "4.2 Difficulty Classifier", "text": "After obtaining the data needed for training and testing a difficulty classifier, we turn our attention to building this classifier and the features used to do so. Formally, our goal is to learn a function diff (Q, e) \u2208 {easy, hard} that learns the difficulty of providing the answer e to the query Q.\nWe use logistic regression as our model of choice. We chose this specific model due to the ease with which it can be trained and because it allows easy inspection of feature weights, which proved helpful during development. As we are dealing with a binary classification case (easy, hard classification), we train our model to learn the probability of the question being an easy one, P (diff (Q, e) = easy) , and set a decision boundary at 0.5. We judge a question to be easy if P (diff (Q, e) = easy) > 0.5 and hard otherwise.\nThe model, however, only works if provided with the right features. Table 1 provides a summary of our features and a brief description of each. The key ingredients in our feature repertoire are entity salience, per coarse semantic type salience, and coherence of entity pairs.\nEntity Salience (\u03c6) is a normalized score that is used as a proxy for an entity\u2019s popularity. As our entities come from Wikipedia, we use the Wikipedia link structure to compute entity salience as the relative frequency with which the Wikipedia entry for an entity is linked to from all other entries. We also consider salience on a per-coarse-semantictype basis. The second group of Table 1 defines a set of templates. We consider the coarse semantic types person, location, and organization and define a fourth coarse semantic type other that collects entities not in any of the three aforementioned coarse types (e.g., movies, inventions). Having specialized features for individual coarse-grained types allows us to take into consideration some particularities of\nthese coarse types. For example, locations tend to have disproportionately high salience. By having a feature that accounts for this specific semantic type, we can mitigate this. Without this feature, having a location in a question would result in our classifier always labeling the question as easy.\nCoherence of entity pairs (\u03d5) captures the relative tendency of two entities to appear in the same context. This feature essentially informs us about how much the presence of one entity indicates the presence of the other entity. For example, we would expect that:\n\u03d5(BarackObama, WhiteHouse) > \u03d5(BarackObama, GrammyAward).\nThe reason is that the first pair is more likely to co-occur together than the second one. All else being equal, we would expect a question asking for BarackObama using the WhiteHouse in the question to be easier than one asking for him using GrammyAward. Intuitively, coherence counteracts the effect of salience. Since BarackObama is a salient entity, we would expect questions asking for him to be relatively easy. However, asking for him using GrammyAward is likely to make the question difficult, as people are unlikely to make a connection between the two entities.\nWe capture coherence using Wikipedia\u2019s link structure. Given two entities e1 and e2, we define their coherence as the Jaccard coefficient of the sets of Wikipedia entries that link to their respective entries in Wikipedia. The intuition here is that any overlap corresponds to a mention of the relation between these two entities. For the above measures, we take their maximum, minimum, average, and sum over the question as features as detailed in Table 1."}, {"heading": "5. QUERY VERBALIZATION", "text": "We now turn to the problem of query verbalization, whereby we transform a query constructed in Section 3 into a natural language question. A human can digest this question without the technical expertise required to understand a triple pattern query. Our final goal is to construct well-formed questions that are easy to understand.\nThe goal of our questions is to test factual knowledge as opposed to linguistic ability. The way that a question is formulated is not a factor in predicting its difficulty. This guides our approach to query verbalization, which ensures uniformity in how questions are phrased.\nWe rely on a hand crafted verbalization template and automatically generated lexicons for transforming a query into a question. The verbalization template specifies where the different components of the query appear in the question. The lexicon serves as a bridge between knowledge graph entries and natural language. We start by describing our template and then move to our lexicon generation process."}, {"heading": "5.1 Verbalization Template", "text": "Our approach to verbalizing queries is based on templates. Such approaches are standard in the natural language generation literature [26, 34]. We adopt a template inspired by the Jeopardy! quiz game show given in Figure 3. Most of the work is done in the function verbalize.\nInput: Query, Q = {q1, ..., qn}\nQtype := {qi \u2208 Q | has the predicate type} = {qt1 , . . . , qtm} Qinstance := Q \\Qtype = {qi1 , . . . , qil}\nThis verbalize(qt1), . . . , and verbalize(qtm) verbalize(qi1), . . . , and verbalize(qil) .\nFigure 3: Verbalization Template\nThe function verbalize takes a triple pattern and produces its verbalization. How this verbalization is performed depends on the nature of the triple pattern. More concretely, there are three distinct patterns possible in our setting (see Section 3):\n\u2022 Type: if the predicate is type, then this results in verbalizing the object, which is a semantic type.\n\u2022 PO: where the triple pattern is of the form ?var p o and p is not type.\n\u2022 SP: where the triple pattern is of the form s p ?var and p is not type.\nBy considering these cases individually we ensure that linguistically well-formed verbalizations are created. Figure 4 shows an example of each of the three cases above. Verbalizing a triple pattern requires that we are able to verbalize its constituent semantic items (entities, types, and predicates) in a manner that is considerate of the specific pattern. We present our solution to this next."}, {"heading": "5.2 Verbalization Lexicons", "text": "Semantic items in the knowledge graph are simply identifiers that are not meant for direct human consumption. It is therefore important that we map each semantic item to\nphrases that can be used to represent it in a natural language string such as a question.\nEntities. To verbalize entities we follow the approach of Hoffart et al. [25] and rely on the fact that our entities come from Wikipedia. We resort to Wikipedia for extracting surface forms of our entities. For each entity e, we collect the surface forms of all links to e\u2019s Wikipedia entry. We consider this text to be a possible verbalization of e.\nThe above process extracts many spurious verbalizations of an entity e. To overcome this issue, we associate with each candidate verbalization the number of times it was used to link to e\u2019s Wikipedia entry and restrict ourselves to the five most frequent ones, which we add to the lexicon for the entry corresponding to e.\nPredicates. As Figure 4 shows, predicate verbalization depends on the the pattern in which it is observed (SP or PO). We rely on our large entity-annotated corpus described in Section 2 for mining predicate verbalizations sensitive to the SP and PO patterns. For each triple (e1 p e2) \u2208 KG, we collect all sentences in our corpus that match the patterns PatSP =\u201ce1 w1...wn e2\u201d (e.g., \u201cBarackObama was born in Hawaii\u201d) and PatPO =\u201ce2 w1...wn e1\u201d (e.g., \u201cHawaii is the birthplace of BarackOmaba\u201d) . Following the distant supervision assumption [31], we hypothesize that \u2018w1...wn\u2019 is expressing p. The above hypothesis does not always hold. To filter out possible noise we resort to a combination of heuristic filtering and scoring. We remove from the above verbalization candidate set any phrases that are longer than 50 characters or contain a third entity e3. We subsequently score how good of a fit a phrase \u2018w1...wn\u2019 is for a predicate p using normalized pointwise mutual information (npmi). For each predicate p, we retain the 5 highest scoring verbalizations for each of the two patterns, PatSP and PatPO, which are used for verbalizing SP and PO triple patterns, respectively.\nTypes. As explained in Section 2, our types are WordNet synsets. We therefore rely on the lexicon distributed as part of WordNet for type paraphrasing.\nEach of the three lexicons provides several ways to verbalize a semantic item. When verbalizing a specific semantic item, we choose a verbalization uniformly at random to ensure variety."}, {"heading": "6. MULTIPLE-CHOICE QUESTIONS", "text": "The final component in our question generation framework turns a question into a multiple-choice question. This has several advantages: in general, it is easier to administer a multiple-choice question as the problem of answer verification can be completely mechanized. This is particularly true in cases where questions are not administered though a computer, where such things as completion suggestion can ensure canonical answers. In general, where knowledge questions are involved (as opposed to free response questions that might involve opinion), the use of multiple-choice questions is widespread as observed in such tests as the GRE.\nTurning a question into multiple-choice requires distractors: entities that are presented to the user as answer candidates, but are in fact incorrect answers. Of course, not all entities constitute reasonable distractors. A negative example would be entities that are completely unrelated to the question. In addition to being related to the question, distractors should ideally be related to the correct answer entity. It should generally be possible to confuse a distractor\nwith the correct answer to make a multiple-choice question interesting. We call this the confusability of a distractor. The more confusable a distractor is with the correct answer, the more likely a test taker is to choose it as an answer, making the multiple-choice question more challenging.\nIn what follows we take a look at the problems of generating distractors in our framework and quantifying the confusability of these distractors."}, {"heading": "6.1 Distractor Generation", "text": "Our starting point for generating distractors is the query Q = {q1, ..., qn} generated in Section 3, which formed the basis of the question verbalized in Section 5. By starting with a query, we have a fairly simple but powerful scheme for generating distractors. By removing one or more triple patterns from Q we obtain a query Q\u2032 \u2282 Q that has more than one answer entity. All but one of these entities are an incorrect answer to Q.\nThe relaxation scheme described above can generate a large number of candidate distractors. However, not all relaxations stay close to the original query. If a relaxation deviates too much from Q, the obtained distractors become meaningless. We address this by imposing two restrictions on relaxed queries used to generate distractors: (i) a semantic type restriction, and (ii) a relaxation distance restriction.\nSemantic type restriction ensures that the answer and distractor are type-compatible. For example, a multiple-choice question asking for a location should not have a person as one of its distractors. The semantic type restriction requires that a semantic type triple pattern Q is relaxed to the corresponding coarse type.\nThe relaxation distance restriction refers to relaxations involving instance triple patterns. We define the distance between a query Q and a query Q\u2032 \u2286 Q as follows:\ndist(Q,Q\u2032) = |answers(Q\u2032)| \u2212 |answers(Q)|,\nwhere answers(Q\u2032) is the set of answers of Q\u2032 (|answers(Q)| is always 1). We restrict relaxed queries to have a distance of no more than \u03b1, which we set to 10. By pooling the results of all relaxed queries, we form a set of candidate distractors. The choice of distractor is based on how much difficulty we want the distractors to introduce using our notion of distractor confusability."}, {"heading": "6.2 Distractor Confusability", "text": "All things equal, a multiple-choice question can be made more or less difficult by the choice of distractors. If one of the distractors is highly confusable with the answer entity, the multiple-choice question is difficult. If none of the distractors is easy to confuse with the answer entity, the multiple-choice question is easy.\nBased on this observation we regard a distractor as confusable if it is likely to be the answer to the original question based on our difficulty model. This implies that if an entity is very likely to be the answer to a question asking about a different entity, this entity pair must be similar. We can therefore define confusability between the question\u2019s answer ea and a distractor entity edist as follows:\nconf (Q, ea, edist) =\n1\u2212 |P (diff (Q, ea) = easy)\u2212 P (diff (Q, edist) = easy)|.\nSince we can have more than one distractor in a multiplechoice question, we capture the above intuition regarding\nhow multiple distractors affect the overall difficulty of the question. We observe that a multiple-choice question is as confusing as its most confusing distractor and define the confusability of a distractor set Dist = {edist1, edist2, ...} as:\nconf (Q, ea, Dist) = max edist\u2208Dist conf (Q, ea, edist).\nLooking at the big picture, we relate the notion of confusability in a multiple-choice question with our earlier notion of difficulty by combining diff (Q, ea) \u2208 {easy, hard} and conf (Q, ea, Dist) \u2208 [0, 1] as shown in Table 2. We see that an easy question can be turned in two a hard one when a very confusable distractor is added, since the user has to distinguish between two very similar entities. However, adding an easy distractor to a hard question will not change its difficulty because even when both entities are not similar to each other, the user still has to know which entitiy is the correct answer."}, {"heading": "7. EXPERIMENTAL EVALUATION", "text": "In the following section we evaluate our approach to knowledge question generation from knowledge graphs. We perform two user studies which focus on evaluating the difficulty model and our distractor generation framework."}, {"heading": "7.1 Human Assessment of Difficulty", "text": "An important motivation for automating difficulty assessment of questions is the fact that it is difficulty to judge for the average human what constitutes an easy or hard question. Beinborn et al. [7] has already shown this result for language proficiency tests, where language teachers were shown to be bad at predicting the difficulty of questions when considering the actual performance of students. We would like to observe if the same applies to our setting. To create fair and informative tests, it is crucial that we are able to correctly assess the difficulty of a question.\nWe start with the assumption that the creators of Jeopardy! are good at automatically assessing question difficulty. Evidence for this was discussed in Section 4, where we showed that there exists a correlation between the monetary value of a question and the likelihood of it being incorrectly answered by Jeopardy! contestants.\nIn our experiment we want to show how well the average human can predict the difficulty of a question. To do so, we randomly sampled 100 easy ($200) and 100 hard ($1000) questions from the 500 questions generated in Section 4 to maximize the discrepancy in question difficulty. We then asked three human evaluators (eval1, eval2, eval3) to annotate each of the 200 questions as easy or hard. We then compared their answers with each other and with the ground truth according to Jeopardy!.\nTable 3 shows the agreement between each pair of human evaluators and the majority vote difficulty assessment using Fleiss\u2019 Kappa [19]. When looking at pairwise agreement between evaluators, it ranges from fair to moderate [28].\nThis leads us to conclude that it is hard for non-experts to properly judge the difficulty of questions.\nWe also compared the majority vote of the evaluators on the difficulty of the questions with the ground truth provided by Jeopardy!. The result was agreement on 62.5% of questions. This suggests that there is a need to automate the task."}, {"heading": "7.2 Question Difficulty Classification", "text": "We start by looking at the quality of our scheme for assigning difficulty levels to questions. The scheme is described in Section 4, where the possible difficulty levels are D = {easy, hard}. We train our logistic regression classifier on 500 Jeopardy! questions annotated as described in Section 4. Using ten-fold cross validation, our classifier was able to correctly identify the difficulty levels of questions with an accuracy of 66.4%.\nTo gain insight into how informative our features are, we performed a feature ablation study where we look at the results for all combinations of our features. For this part, we grouped our features into three classes:\n\u2022 SAL: \u201cSalience\u201d features as in Table 1, with additional log-transformation of salience values to deal with longtail entities.\n\u2022 COH: \u201cCoherence\u201d features in Table 1. \u2022 TYPE: \u201cPer-coarse-semantic-type Salience\u201d and \u201cAn-\nswer Type\u201d features in Table 1.\nTable 4 shows the results of this experiment. Each row corresponds to a certain combination of features enabled or disabled. Rows are shown in descending order of ten-fold cross validation accuracy. It can be seen that best performance is achieved when all of our features are integrated. From this observation it can be reasoned that all features are necessary and give complementary signals. The bottom row corresponds to a random classifier."}, {"heading": "7.3 User Study on Difficulty Estimation", "text": "In the following we perform an experiment on how well our classifier agrees with relative difficulty assessments of\nhumans for questions generated by our system. It is important to note that we ask humans for relative difficulty assessments as opposed to absolute difficulties, since we have shown in Section 7.1 that humans are not very proficient in judging absolute difficulties.\nFor the user study we sampled a set of 50 entities with at least 5 non-type facts in Yago. For each entity, we generated a set of three questions and presented them with the answer entity to human annotators. The annotators were asked order these questions by their relative difficulty and were allowed to skip a set of questions about an entity if they were not familiar with the entity.\nWe then compared the correlation between the ranking given by each of the human annotators and the output of our logistic regression classifier. For this we used Kendall\u2019s \u03c4 , which ranges from -1, in the case of perfect disagreement, to 1, in the case of perfect agreement.\nA total of 13 evaluators took part in the study and evaluated 92.5 questions on average. Rankings produced by the difficulty classifier moderately agree with the human annotators with \u03c4 = 0.563. When the \u03c4 -values for users are weighted by study participation, the average rises to \u03c4 = 0.593. Here, each user\u2019s contribution to the final average depends on how many questions she evaluated to avoid overly representing users that evaluated only few questions."}, {"heading": "7.4 Distractors Confusability", "text": "We now turn to the evaluation of distractor generation for multiple-choice questions. Our goal is to accurately predict the confusability of a distractor given a question\u2019s correct answer. In Section 6.2 we presented our scheme for quantifying distractor confusability and how it fits into a multiple-choice question setting. We evaluate our approach here.\nFor this experiment we automatically generate 10,000 multiple-choice questions. Each question has three answer choices, which are the correct answer and two distractors. We then restricted ourselves to 400 multiple-choice questions whose distractor pair has the largest difference in confusability. This was done to maximize the probability that study participants can actually discriminate the more confusable from the less confusable distractor.\nWe ran each multiple-choice question through a crowdsourcing platform and asked workers to judge which distractor is more confusing. Each multiple-choice question was judged by 5 workers so we could take the majority vote in case the judgments where not unanimous. We then compare this majority vote with the result of our confusability estimator. Our estimator agreed with the human annotations on 76% of the 400 multiple-choice questions. This translates to a Cohen\u2019s \u03ba of 0.521, indicating moderate agreement [12]."}, {"heading": "8. RELATED WORK", "text": "There has been work on knowledge question generation for testing linguistic knowledge and reading comprehension. The generation of language proficiency tests has been tackled in several works [21, 32, 35]. Here, the focus is on generating cloze (fill-in-the-blank) tests. Beinborn et al. [7] presents an approach for predicting the difficulty of answering such questions with multiple blanks using SVMs trained on four classes of features that look at individual blanks, their candidate answers, their dependence on other blanks, and the overall question difficulty.\nQuestion generation for reading comprehension is aimed\nat evaluating knowledge from text corpora. This includes including general Wikipedia knowledge [8, 24] and specialized domain such as medical texts [2, 46]. While the above works focus on generating a question from a single document, Questimator [22] generates multiple choice questions from the textual Wikipedia corpus by considering multiple documents related to a single topic to produce a question. Work in this area has mostly taken the approach of overgeneration and ranking [24, 46]. Multiple questions are generated for a given passage using rules. A learned model ranks the questions in terms of \u201cacceptability\u201d. In this setting, acceptable answers should be sensical, grammatical, and their answers should not be obvious.\nRecent work has started to look at the problem of generating questions, including multiple choice ones, from KGs and ontologies [3, 38, 41, 37]. Strong motivations for studying this problem, compared to question generation from text, are scenarios where structured data is what is available at hand, and the ability to generate deeper, structurally more complex questions. Our system is an end-to-end solution for this problem over a large KG.\nIn Section 5 we presented a simple approach for query verbalization that sits our needs. The query verbalization problem has been tackled by Ngomo et al. for SPARQL [33, 14], and Koutrika et al. for SQL [27], with a focus on usability. Similar to our approach, these earlier works take a template-based approach to verbalization, which are very widely used on the natural language generation from logical form such as SPARQL queries [26, 34].\nMuch recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48]. The value of knowledge graphs is that they return crisp answers and allow for complex constraint to answer structurally complex questions. Of course, question answering has a long history, with one of the major highlights being IBM\u2019s Watson [16], which won the Jeopardy! game show combining both structured and unstructured sources for answering.\nOne important contribution of our work is an approach to compute the difficulty of questions generated. This topic has received attention lately in community question answering [29, 45], by using a competition-based approach that tries to capture how much skill a question requires for answering. There has also been work on estimating query difficulty in the context of information retrieval [11, 49] to learn an estimator that predicts the expected precision of the query by analyzing the overlap between the results of the full query and the results of its sub-queries."}, {"heading": "9. CONCLUSION", "text": "We proposed an end-to-end approach to the novel problem of generating quiz-style knowledge questions from knowledge graphs. Our approach addresses the challenges inherent to this problem, most importantly estimating the difficulty of generated questions. To this end, we engineer suitable features and train a model of question difficulty on historical data from the Jeopardy! quiz show, which is shown to outperform humans on this difficult task. A working prototype implementing our approach is accessible at:\nhttps://gate.d5.mpi-inf.mpg.de/q2g"}, {"heading": "10. REFERENCES", "text": "[1] J! Archive. http://j-archive.com.\n[2] M. Agarwal and P. Mannem. Automatic gap-fill question generation from text books. In BEA, 2011.\n[3] T. Alsubait et al. Generating multiple choice questions from ontologies: Lessons learnt. In OWLED, 2014.\n[4] S. Auer et al. DBpedia: A Nucleus for a Web of Open Data. In ISWC/ASWC, 2007.\n[5] H. Bast et al. Semantic Search on Text and Knowledge Bases. Foundations and Trends in IR, 10(2-3), 2016.\n[6] H. Bast and E. Haussmann. More Accurate Question Answering on Freebase. In CIKM, 2015.\n[7] L. Beinborn et al. Predicting the Difficulty of Language Proficiency Tests. TACL, 2, 2014.\n[8] A. S. Bhatia et al. Automatic generation of multiple choice questions using wikipedia. In PReMI, 2013.\n[9] R. Blanco et al. Effective and efficient entity search in RDF data. In ISWC, 2011.\n[10] K. D. Bollacker et al. Freebase: a Collaboratively Created Graph Database for Structuring Human Knowledge. In SIGMOD, 2008.\n[11] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Morgan & Claypool Publishers, 2010.\n[12] J. Cohen. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37, 1960.\n[13] W. Cui et al. KBQA: an Online Template Based Question Answering System over Freebase. In IJCAI, 2016.\n[14] B. Ell et al. Spartiqulation \u2013 Verbalizing SPARQL Queries. In ILD Workshop, ESWC, 2012.\n[15] C. Fellbaum, editor. WordNet: an Electronic Lexical Database. MIT Press, 1998.\n[16] D. A. Ferrucci. Introduction to \u201dthis is watson\u201d. IBM Journal of Research and Development, 2012.\n[17] D. A. Ferrucci et al. Building Watson: An Overview of the DeepQA Project. AI Magazine, 31(3), 2010.\n[18] J. R. Finkel et al. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In ACL, 2005.\n[19] J. L. Fleiss. Measuring Nominal Scale Agreement among Many Raters. Psychological Bulletin, 1971.\n[20] E. Gabrilovich et al. FACC1: Freebase annotation of ClueWeb corpora, Version 1, 2013.\n[21] D. M. Gates. How to Generate Cloze Questions from Definitions: A Syntactic Approach. In AAAI, 2011.\n[22] Q. Guo et al. Questimator: Generating Knowledge Assessments for Arbitrary Topics. In IJCAI, 2016.\n[23] M. A. Hearst. Automatic Acquisition of Hyponyms from Large Text Corpora. In COLING, 1992.\n[24] M. Heilman and N. A. Smith. Question Generation via Overgenerating Transformations and Ranking. Technical report, 2009.\n[25] J. Hoffart et al. Robust Disambiguation of Named Entities in Text. In EMNLP, 2011.\n[26] N. Indurkhya and F. J. Damerau, editors. Handbook of Natural Language Processing. Chapman and Hall/CRC, 2010.\n[27] G. Koutrika et al. Explaining Structured Queries in Natural Language. In ICDE, 2010.\n[28] J. R. Landis and G. G. Koch. The Measurement of Observer Agreement for Categorical Data. Biometrics, Vol. 33, 1977.\n[29] J. Liu et al. Question difficulty estimation in community question answering services. In EMNLP, 2013.\n[30] V. Lo\u0301pez et al. Scaling up question-answering to linked data. In EKAW, 2010.\n[31] M. Mintz et al. Distant supervision for relation extraction without labeled data. In ACL, 2009.\n[32] A. Narendra et al. Automatic Cloze-Questions Generation. In RANLP, 2013.\n[33] A.-C. Ngonga Ngomo et al. Sorry, I Don\u2019T Speak SPARQL: Translating SPARQL Queries into Natural Language. In WWW, 2013.\n[34] E. Reiter and R. Dale. Building Natural Language Generation Systems. Cambridge University Press, 2000.\n[35] K. Sakaguchi et al. Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners. In ACL, 2013.\n[36] D. Savenkov and E. Agichtein. When a knowledge base is not enough: Question answering over knowledge bases with external text data. In SIGIR, 2016.\n[37] I. V. Serban et al. Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus. In ACL, 2016.\n[38] D. Seyler et al. Generating quiz questions from knowledge graphs. In WWW, 2015.\n[39] D. Seyler et al. Automated question generation for quality control in human computation tasks. In WebSci, 2016.\n[40] S. Shekarpour et al. Question answering on interlinked data. In WWW, 2013.\n[41] L. Song and L. Zhao. Domain-specific question generation from a knowledge base. arXiv, 2016.\n[42] F. M. Suchanek et al. Yago: A Core of Semantic Knowledge. In WWW, 2007.\n[43] F. M. Suchanek et al. Yago2s: Modular high-quality information extraction with an application to flight planning. In BTW, volume 214, 2013.\n[44] C. Unger et al. Template-based question answering over RDF data. In WWW, 2012.\n[45] Q. Wang et al. A regularized competition model for question difficulty estimation in community question answering services. In EMNLP, 2014.\n[46] W. Wang et al. Automatic question generation for learning evaluation in medicine. In ICWL, 2007.\n[47] K. Xu et al. What Is the Longest River in the USA? Semantic Parsing for Aggregation Questions. In AAAI, 2015.\n[48] P. Yin et al. Answering Questions with Complex Semantic Constraints on Open Knowledge Bases. In CIKM, 2015.\n[49] E. Yom-Tov et al. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In SIGIR, 2005.\n[50] L. Zou et al. Natural language question answering over RDF: a graph data driven approach. In SIGMOD, 2014."}], "references": [{"title": "Automatic gap-fill question generation from text books", "author": ["M. Agarwal", "P. Mannem"], "venue": "BEA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating multiple choice questions from ontologies: Lessons learnt", "author": ["T. Alsubait"], "venue": "In OWLED,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "DBpedia: A Nucleus for a Web of Open Data", "author": ["S. Auer"], "venue": "In ISWC/ASWC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Semantic Search on Text and Knowledge Bases", "author": ["H. Bast"], "venue": "Foundations and Trends in IR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "More Accurate Question Answering on Freebase", "author": ["H. Bast", "E. Haussmann"], "venue": "CIKM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting the Difficulty of Language Proficiency", "author": ["L. Beinborn"], "venue": "Tests. TACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Automatic generation of multiple choice questions using wikipedia", "author": ["A.S. Bhatia"], "venue": "In PReMI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Effective and efficient entity search in RDF data", "author": ["R. Blanco"], "venue": "In ISWC,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Freebase: a Collaboratively Created Graph Database for Structuring Human Knowledge", "author": ["K.D. Bollacker"], "venue": "In SIGMOD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Estimating the Query Difficulty for Information Retrieval", "author": ["D. Carmel", "E. Yom-Tov"], "venue": "Morgan & Claypool Publishers", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A Coefficient of Agreement for Nominal Scales", "author": ["J. Cohen"], "venue": "Educational and Psychological Measurement, 20(1):37", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1960}, {"title": "KBQA: an Online Template Based Question Answering System over Freebase", "author": ["W. Cui"], "venue": "In IJCAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Spartiqulation \u2013 Verbalizing SPARQL Queries", "author": ["B. Ell"], "venue": "In ILD Workshop,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "editor", "author": ["C. Fellbaum"], "venue": "WordNet: an Electronic Lexical Database. MIT Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Introduction to \u201dthis is watson", "author": ["D.A. Ferrucci"], "venue": "IBM Journal of Research and Development", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Building Watson: An Overview of the DeepQA Project", "author": ["D.A. Ferrucci"], "venue": "AI Magazine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling", "author": ["J.R. Finkel"], "venue": "In ACL,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Measuring Nominal Scale Agreement among Many Raters", "author": ["J.L. Fleiss"], "venue": "Psychological Bulletin", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1971}, {"title": "FACC1: Freebase annotation of ClueWeb", "author": ["E. Gabrilovich"], "venue": "corpora, Version", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "How to Generate Cloze Questions from Definitions: A Syntactic Approach", "author": ["D.M. Gates"], "venue": "AAAI", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Questimator: Generating Knowledge Assessments for Arbitrary Topics", "author": ["Q. Guo"], "venue": "In IJCAI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Automatic Acquisition of Hyponyms from Large Text Corpora", "author": ["M.A. Hearst"], "venue": "COLING", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "Question Generation via Overgenerating Transformations and Ranking", "author": ["M. Heilman", "N.A. Smith"], "venue": "Technical report", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust Disambiguation of Named Entities in Text", "author": ["J. Hoffart"], "venue": "In EMNLP,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "editors", "author": ["N. Indurkhya", "F.J. Damerau"], "venue": "Handbook of Natural Language Processing. Chapman and Hall/CRC", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Explaining Structured Queries in Natural Language", "author": ["G. Koutrika"], "venue": "In ICDE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "The Measurement of Observer Agreement for Categorical Data", "author": ["J.R. Landis", "G.G. Koch"], "venue": "Biometrics, Vol. 33", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1977}, {"title": "Question difficulty estimation in community question answering services", "author": ["J. Liu"], "venue": "In EMNLP,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Scaling up question-answering to linked data", "author": ["V. L\u00f3pez"], "venue": "In EKAW,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz"], "venue": "In ACL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Automatic Cloze-Questions Generation", "author": ["A. Narendra"], "venue": "In RANLP,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Sorry, I Don\u2019T Speak SPARQL: Translating SPARQL Queries into Natural Language", "author": ["A.-C. Ngonga Ngomo"], "venue": "In WWW,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Building Natural Language Generation Systems", "author": ["E. Reiter", "R. Dale"], "venue": "Cambridge University Press", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners", "author": ["K. Sakaguchi"], "venue": "In ACL,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "When a knowledge base is not enough: Question answering over knowledge bases with external text data", "author": ["D. Savenkov", "E. Agichtein"], "venue": "SIGIR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["I.V. Serban"], "venue": "In ACL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Generating quiz questions from knowledge graphs", "author": ["D. Seyler"], "venue": "In WWW,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Automated question generation for quality control in human computation tasks", "author": ["D. Seyler"], "venue": "In WebSci,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Question answering on interlinked data", "author": ["S. Shekarpour"], "venue": "In WWW,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Domain-specific question generation from a knowledge base", "author": ["L. Song", "L. Zhao"], "venue": "arXiv", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Yago: A Core of Semantic Knowledge", "author": ["F.M. Suchanek"], "venue": "In WWW,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Yago2s: Modular high-quality information extraction with an application to flight planning", "author": ["F.M. Suchanek"], "venue": "In BTW,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Template-based question answering over RDF data", "author": ["C. Unger"], "venue": "In WWW,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A regularized competition model for question difficulty estimation in community question answering services", "author": ["Q. Wang"], "venue": "In EMNLP,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Automatic question generation for learning evaluation in medicine", "author": ["W. Wang"], "venue": "In ICWL,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2007}, {"title": "What Is the Longest River in the USA? Semantic Parsing for Aggregation Questions", "author": ["K. Xu"], "venue": "In AAAI,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Answering Questions with Complex Semantic Constraints on Open Knowledge Bases", "author": ["P. Yin"], "venue": "In CIKM,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval", "author": ["E. Yom-Tov"], "venue": "In SIGIR,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2005}, {"title": "Natural language question answering over RDF: a graph data driven approach", "author": ["L. Zou"], "venue": "In SIGMOD,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}], "referenceMentions": [{"referenceID": 40, "context": "Knowledge graphs (KGs) such as YAGO [42] and DBpedia [4] contain facts about real-world named entities.", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "Knowledge graphs (KGs) such as YAGO [42] and DBpedia [4] contain facts about real-world named entities.", "startOffset": 53, "endOffset": 56}, {"referenceID": 37, "context": "Crowdsourcing is one concrete use case as outlined in [39].", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "Knowledge Graphs (KGs) such as as Freebase [10], Yago [42], and DBpedia [4] describe entities E (e.", "startOffset": 43, "endOffset": 47}, {"referenceID": 40, "context": "Knowledge Graphs (KGs) such as as Freebase [10], Yago [42], and DBpedia [4] describe entities E (e.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "Knowledge Graphs (KGs) such as as Freebase [10], Yago [42], and DBpedia [4] describe entities E (e.", "startOffset": 72, "endOffset": 75}, {"referenceID": 41, "context": "More specifically, we use Yago2s [43] as our reference knowledge graph in this work.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "Yago2s is automatically constructed by combining information extraction over Wikipedia infoboxes and categories with the lexical database WordNet [15].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "To compute signals necessary for estimating question difficulty, we make use of the ClueWeb09/12 document collections and the FACC annotations provided by Google [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 21, "context": "several attorneys including BarackObama\u201d These patterns are inspired by Hearst [23].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Relying on the fact that our semantic types are WordNet synsets [15], we use the lexicon that comes with WordNet (e.", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "We focus here on questions with unknown entities as these are the ones we can use Jeopardy! data to train our difficulty classifier on [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 40, "context": "The larger goal is to estimate the difficulty of answering queries generated from a knowledge graph, so we restrict ourselves to a subset of the Jeopardy! questions answerable from Yago [42], which we collected as described below.", "startOffset": 186, "endOffset": 190}, {"referenceID": 16, "context": "To find these questions, we automatically annotate the questions with Yago entities using the Stanford CoreNLP named entity recognizer (NER) [18] in conjunction with the AIDA tool for named entity disambiguation [25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "To find these questions, we automatically annotate the questions with Yago entities using the Stanford CoreNLP named entity recognizer (NER) [18] in conjunction with the AIDA tool for named entity disambiguation [25].", "startOffset": 212, "endOffset": 216}, {"referenceID": 24, "context": "Such approaches are standard in the natural language generation literature [26, 34].", "startOffset": 75, "endOffset": 83}, {"referenceID": 32, "context": "Such approaches are standard in the natural language generation literature [26, 34].", "startOffset": 75, "endOffset": 83}, {"referenceID": 23, "context": "[25] and rely on the fact that our entities come from Wikipedia.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Following the distant supervision assumption [31], we hypothesize that \u2018w1.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "[7] has already shown this result for language proficiency tests, where language teachers were shown to be bad at predicting the difficulty of questions when considering the actual performance of students.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Table 3 shows the agreement between each pair of human evaluators and the majority vote difficulty assessment using Fleiss\u2019 Kappa [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "When looking at pairwise agreement between evaluators, it ranges from fair to moderate [28].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "521, indicating moderate agreement [12].", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "The generation of language proficiency tests has been tackled in several works [21, 32, 35].", "startOffset": 79, "endOffset": 91}, {"referenceID": 30, "context": "The generation of language proficiency tests has been tackled in several works [21, 32, 35].", "startOffset": 79, "endOffset": 91}, {"referenceID": 33, "context": "The generation of language proficiency tests has been tackled in several works [21, 32, 35].", "startOffset": 79, "endOffset": 91}, {"referenceID": 5, "context": "[7] presents an approach for predicting the difficulty of answering such questions with multiple blanks using SVMs trained on four classes of features that look at individual blanks, their candidate answers, their dependence on other blanks, and the overall question difficulty.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This includes including general Wikipedia knowledge [8, 24] and specialized domain such as medical texts [2, 46].", "startOffset": 52, "endOffset": 59}, {"referenceID": 22, "context": "This includes including general Wikipedia knowledge [8, 24] and specialized domain such as medical texts [2, 46].", "startOffset": 52, "endOffset": 59}, {"referenceID": 0, "context": "This includes including general Wikipedia knowledge [8, 24] and specialized domain such as medical texts [2, 46].", "startOffset": 105, "endOffset": 112}, {"referenceID": 44, "context": "This includes including general Wikipedia knowledge [8, 24] and specialized domain such as medical texts [2, 46].", "startOffset": 105, "endOffset": 112}, {"referenceID": 20, "context": "While the above works focus on generating a question from a single document, Questimator [22] generates multiple choice questions from the textual Wikipedia corpus by considering multiple documents related to a single topic to produce a question.", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "Work in this area has mostly taken the approach of overgeneration and ranking [24, 46].", "startOffset": 78, "endOffset": 86}, {"referenceID": 44, "context": "Work in this area has mostly taken the approach of overgeneration and ranking [24, 46].", "startOffset": 78, "endOffset": 86}, {"referenceID": 1, "context": "Recent work has started to look at the problem of generating questions, including multiple choice ones, from KGs and ontologies [3, 38, 41, 37].", "startOffset": 128, "endOffset": 143}, {"referenceID": 36, "context": "Recent work has started to look at the problem of generating questions, including multiple choice ones, from KGs and ontologies [3, 38, 41, 37].", "startOffset": 128, "endOffset": 143}, {"referenceID": 39, "context": "Recent work has started to look at the problem of generating questions, including multiple choice ones, from KGs and ontologies [3, 38, 41, 37].", "startOffset": 128, "endOffset": 143}, {"referenceID": 35, "context": "Recent work has started to look at the problem of generating questions, including multiple choice ones, from KGs and ontologies [3, 38, 41, 37].", "startOffset": 128, "endOffset": 143}, {"referenceID": 31, "context": "for SPARQL [33, 14], and Koutrika et al.", "startOffset": 11, "endOffset": 19}, {"referenceID": 12, "context": "for SPARQL [33, 14], and Koutrika et al.", "startOffset": 11, "endOffset": 19}, {"referenceID": 25, "context": "for SQL [27], with a focus on usability.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "Similar to our approach, these earlier works take a template-based approach to verbalization, which are very widely used on the natural language generation from logical form such as SPARQL queries [26, 34].", "startOffset": 197, "endOffset": 205}, {"referenceID": 32, "context": "Similar to our approach, these earlier works take a template-based approach to verbalization, which are very widely used on the natural language generation from logical form such as SPARQL queries [26, 34].", "startOffset": 197, "endOffset": 205}, {"referenceID": 7, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 11, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 28, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 38, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 42, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 45, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 48, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 3, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 192, "endOffset": 203}, {"referenceID": 34, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 192, "endOffset": 203}, {"referenceID": 46, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 192, "endOffset": 203}, {"referenceID": 14, "context": "Of course, question answering has a long history, with one of the major highlights being IBM\u2019s Watson [16], which won the Jeopardy! game show combining both structured and unstructured sources for answering.", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "This topic has received attention lately in community question answering [29, 45], by using a competition-based approach that tries to capture how much skill a question requires for answering.", "startOffset": 73, "endOffset": 81}, {"referenceID": 43, "context": "This topic has received attention lately in community question answering [29, 45], by using a competition-based approach that tries to capture how much skill a question requires for answering.", "startOffset": 73, "endOffset": 81}, {"referenceID": 9, "context": "There has also been work on estimating query difficulty in the context of information retrieval [11, 49] to learn an estimator that predicts the expected precision of the query by analyzing the overlap between the results of the full query and the results of its sub-queries.", "startOffset": 96, "endOffset": 104}, {"referenceID": 47, "context": "There has also been work on estimating query difficulty in the context of information retrieval [11, 49] to learn an estimator that predicts the expected precision of the query by analyzing the overlap between the results of the full query and the results of its sub-queries.", "startOffset": 96, "endOffset": 104}], "year": 2017, "abstractText": "We address the novel problem of automatically generating quiz-style knowledge questions from a knowledge graph such as DBpedia. Questions of this kind have ample applications, for instance, to educate users about or to evaluate their knowledge in a specific domain. To solve the problem, we propose an end-to-end approach. The approach first selects a named entity from the knowledge graph as an answer. It then generates a structured triple-pattern query, which yields the answer as its sole result. If a multiplechoice question is desired, the approach selects alternative answer options. Finally, our approach uses a template-based method to verbalize the structured query and yield a natural language question. A key challenge is estimating how difficult the generated question is to human users. To do this, we make use of historical data from the Jeopardy! quiz show and a semantically annotated Web-scale document collection, engineer suitable features, and train a logistic regression classifier to predict question difficulty. Experiments demonstrate the viability of our overall approach.", "creator": "LaTeX with hyperref package"}}}