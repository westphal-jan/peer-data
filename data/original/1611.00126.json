{"id": "1611.00126", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Improving Twitter Sentiment Classification via Multi-Level Sentiment-Enriched Word Embeddings", "abstract": "Most of existing work learn sentiment-specific word representation for improving Twitter sentiment classification, which encoded both n-gram and distant supervised tweet sentiment information in learning process. They assume all words within a tweet have the same sentiment polarity as the whole tweet, which ignores the word its own sentiment polarity. To address this problem, we propose to learn sentiment-specific word embedding by exploiting both lexicon resource and distant supervised information. We develop a multi-level sentiment-enriched word embedding learning method, which uses parallel asymmetric neural network to model n-gram, word level sentiment and tweet level sentiment in learning process. Experiments on standard benchmarks show our approach outperforms state-of-the-art methods.", "histories": [["v1", "Tue, 1 Nov 2016 04:48:09 GMT  (118kb)", "http://arxiv.org/abs/1611.00126v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shufeng xiong"], "accepted": false, "id": "1611.00126"}, "pdf": {"name": "1611.00126.pdf", "metadata": {"source": "CRF", "title": "Improving Twitter Sentiment Classification via Multi-Level Sentiment-Enriched Word Embeddings", "authors": ["Shufeng Xiong"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n00 12\n6v 1\n[ cs\n.C L\n] 1\nN ov\n2 01"}, {"heading": "Introduction", "text": "Twitter, which is one of the biggest micro-blog site on the internet, has emerged as an important source for online opinions and sentiment indexes. As a result of its massive, diverse, and rising user base, the containing opinion information in Twitter has been successfully used to many tasks, such as stock market movement prediction (Si et al. 2013), political monitoring (Pla and Hurtado 2014) and inferring public mood about social events (Bollen, Mao, and Pepe 2011). Therefore, excellent sentiment classification performance, the ability to identify positive, negative, and neutral opinion, is fundamental.\nResearchers have proposed many approaches to improve Twitter classification performance (Davidov, Tsur, and Rappoport 2010; Barbosa and Feng 2010; Mukherjee and Bhattacharyya 2012). Especially, recent advance in deep neural network demonstrate the importance of representation learning of text, e.g., word level and document(sentence) level, for natural language processing tasks (Le and Mikolov 2014; Collobert et al. 2011; Tang, Qin, and Liu 2015). Traditional word embedding methods (Collobert et al. 2011; Mikolov et al. 2013) model the syntactic context information of words. Based on them, (Tang et al. 2014) proposed Sentiment-Specific Word Embedding (SSWE) learning method for Twitter sentiment classification, which aims to tackle the problem that two word with opposite polarity\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nand similar syntactic role for sentiment classification task. Following SSWE, (Ren et al. 2016b) further proposed Topic and Sentiment-enriched Word Embedding (TSWE) model to learn topic-enriched word embedding, which considered the polysemous phenomenon of sentiment-baring words.\nHowever, existing work only exploit Twitter overall sentiment label for learning sentiment-specific word embedding, although there are many state-of-the-art sentiment lexicons (Hu and Liu 2004; Wilson, Wiebe, and Hoffmann 2005), which list common sentiment-baring words with its polarity. Existing work learn sentiment-specific word embedding by using distant supervised tweet polarity label based on traditional learning model. Actually, traditional methods learn word embedding based on a local context model. But, Twitter sentiment label belongs to global document level. For exploiting tweet sentiment label, (Tang 2015) and (Ren et al. 2016b) assume that each word in an opinioned context window indicates the sentiment polarity of the context, and local context has the same polarity as the global sentiment of tweet. In other words, they assigned the tweet level global polarity to its local context window without any adjustment. On the other hand, word sentiment polarity from lexicon is still another useful information for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013). Accordingly, a uniform framework to exploit multi-level sentiment label for learning word embedding is necessary. How to effectively exploit both word and tweet sentiment label for learning word embedding, is still a challenge problem.\nIn fact, SSWE model combines multiple objectives, which are syntactic and sentiment learning, into one function. Moreover, TSWE model further encode topic, sentiment and syntactic into optimizing objective of neural network. However, most of the time, multiple objectives can not be directly optimized by a unified framework, i.e. word sentiment and tweet sentiment. Inspired by recent work for Multi-task deep Learning (Collobert and Weston 2008), we propose to learn word embedding by exploiting multi-level sentiment representation objective optimization approach. Although multi-level sentiment representation can be seen as multiple tasks, they actually have two different inputs 1) word with its context and 2) the whole tweet, which correspond to word sentiment and tweet sentiment, while multitask deep learning commonly has the same input. Therefore,\nit is unable to directly use multi-task deep learning framework on this issue.\nFor tackle this problem, we develop a Multi-level Sentiment-enriched Word Embedding (MSWE) model to learn word representation. MSWE consists of two two-parts asymmetric sub networks, which share a common linear layer and word representation layer. The two-parts sub networks are 1) several Multi-Layer Perceptron (MLP) networks and 2) one Convolution Neural Network (CNN), which are used to model n-gram and sentiment information of word level and sentiment information of tweet level, respectively. Our model is under the assumption that each word vector encodes its own sentiment and word composition encodes tweet sentiment, while SSWE/TSWE assumes each word vector indicate the tweet overall sentiment. Specifically, we feed words (with its context window) into MLP to model word level sentiment, which encodes the word its own sentiment. At the same time, the whole tweet is fed into CNN to model tweet level sentiment.\nThe contributions of this paper can be summarized as follows.\n\u2022 We propose to encode both word level and tweet level sentiment information when learning sentiment-specific word embedding, which makes full use of existing sentiment lexicons and distant supervised Twitter corpus.\n\u2022 To address the multi-level sentiment representation objective optimization, we develop a novel word embedding learning framework, which employs two asymmetric sub networks to model two level sentiment information, respectively.\n\u2022 We conduct experiments on standard Twitter sentiment classification benchmarks. The experiments results demonstrate that our method outperforms previous stateof-the-art approaches."}, {"heading": "Multi-Level Sentiment-Enriched Word Embedding for Twitter Sentiment Classification", "text": "In this paper, we argue that it is important to jointly model both word level and tweet level sentiment information to learn good sentiment-specific word embedding. In this way, it not only made full use of existing sentiment lexicon resource, but also distant supervised Twitter corpus in a unified representation framework. In other words, n-gram and multi-level sentiment information are both encoded in word embedding, which can improve the sentiment classification performance.\nFigure 1(c) describes the architecture of the representation learning network MSWE, which are used for encoding sentiment information in word embedding. The network takes tweet as input. First, MSWE model splits input tweet into several windows, i.e. n windows. Then, n window is the actual input of n left sub networks and all the windows are the actual input of one right sub network. The left sub network outputs word level sentiment and n-gram information, and the right sub network output tweet level sentiment sentiment. We proceed to describe the network in detail."}, {"heading": "Sentiment-Specific Word Embedding", "text": "Our model aims to learn sentiment-specific word embedding, which is more beneficial for twitter sentiment classification than common used word embedding. Sentimentspecific word embedding model stems from C&W model (Collobert et al. 2011), which learns word representation from n-gram contexts by using negative sampling. When learning the representation of a word w, they chose its contextual words c in a window t as positive sample, in which w is in the center position of c. For getting negative sample, they alternate the word w with a different word w\u0303 to form mutational context c\u0303. Its training objective is that the context c is expected to obtain a higher language model score than the mutational context c\u0303 by a margin of 1. The optimization\nfunction is\nlossngm(c) = max(0, 1\u2212 f ngm(c) + fngm(c\u0303)), (1)\nwhere fngm(\u00b7) is the output of C&W model, which represents the n-gram score of a context through out neural network. The C&W model architecture is shown in Figure 1(a). The bottom layer is lookup layer for represent L ddimension word vectors in vocabulary. All the words in context window are concatenated to form [L1, ..., Lt], which is fed into a hidden linear layer. A hTanh activation layer is used on top of the linear layer, its output is\na = hTanh(W1 \u2217 [L1, ..., Lt] + b1), (2)\nwhere W1 \u2208 Rh\u00d7(t\u2217d) and b1 \u2208 Rh is the model parameter, h is the length hidden unit. The n-gram score is computed by a linear transformation applied in a,\nfngm(c) = W2 \u2217 a, (3)\nwhere W1 \u2208 R1\u00d7h is the model parameter. Although C&W model is successfully applied in many NLP task, it is not effective enough for sentiment classification. To address this problem, (Tang et al. 2014) proposed a sentiment-specific word embedding SSWE model base on C&W model. They added tweet sentiment information loss into the objective function, which demonstrates the effectiveness of encoding sentiment information into word embedding for Twitter sentiment classification. The network structure is shown in Figure 1(b). There are some other variations, for example, TSWE (Topic-Enriched Word Embedding) encodes topic distribution into loss function of model and TEWE (Topic and Sentiment-Enriched Word Embedding) considers both topic and sentiment information (Ren et al. 2016b). Overall, all of them use multiple optimization objectives in their loss function."}, {"heading": "Multi-Level Sentiment-Enriched Word Embedding", "text": "Existing work demonstrate that sentiment lexicon is an important resource for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013). In particular, (Socher et al. 2013) developed Sentiment Treebank, which is based on word sentiment and further annotates the sentiment label of syntactically plausible phrase of sentences. By using this corpus, they train a RNTN (Recursive Neural Tensor Network) model for sentiment classification. However, the labelled phrase of Sentiment Treebank is still limited, which can not contain all the available word combinations. Nevertheless, comparing with word combination, the basic sentiment-bared words are changed relatively less over time. Therefore, we propose to consider word level sentiment from lexicon for encoding the individual sentiment information, and simultaneously model tweet level sentiment for considering the composition sentiment information of words.\nAn intuitively approach is adding a new optimization objective in the loss function as practice in SSWE and TEWE. But, there are n word sentiment optimization objectives corresponding to n input words in tweet and only one optimization objective for tweet level sentiment. Existing structure\ncan not optimize these multi-level sentiment scoring objective. As shown in Figure 1(c), we exploit two sub networks for modelling two level optimization objectives by using similar framework as multi-task learning (Chu et al. 2015; Collobert and Weston 2008; Dong et al. 2015).\nShared Units Two sub networks have shared units, each unit contains one embedding layer and one linear layer. Assuming there is an input tweet D consists of n context windows, embedding dimension is d and hidden layer length is h. Then the shared unit number is n, each unit connects one left sub network, and the whole n units connect to right sub network after a pooling operation. For each shared unit, the input of embedding layer is t words in a window and the output is represented as:\nx1:t = x1 \u2295 x2 \u2295 ...\u2295 xt, (4)\nwhere xi, xi+1, ..., xi+t is the t word in window i. A linear transformation is applied to xi:i+t to produce a new feature\nei = f(W 1 1 \u2217 xi:i+t + b 1 1), (5)\nwhere W 11 \u2208 R (t\u2217d)\u00d7h and b11 \u2208 R h are the parameter of linear layer.\nWord level-Specific Layers For left sub network (modelling word level sentiment and n-gram), a following tanh activation layer outputs a1 = hTanh(ei) and two linear transformations output n-gram and word level sentiment predicted scores\nfngm = W 12 \u2217 a1, (6)\nand fws = W 13 \u2217 a1 (7)\nWhen training the model, we input window c and its mutation c\u0303 into the left sub network, the loss function is calculated by Equation (8)\nloss1(c, c\u0303) = \u03b1 \u2217 lossngm(c, c\u0303)+ (1\u2212\u03b1) \u2217 lossws(c), (8)\nlossngm(c, c\u0303) = max(0, 1\u2212 f ngm(c) + fngm(c\u0303)), (9)\nlossws(c) = max(0, 1\u2212\u03c6(0)f ws 0 (c)+\u03c6(1)f ws 1 (c)), (10)\nwhere \u03b1 is linear interpolation weight and \u03c6(\u00b7) is an indicator of the sentiment polarity of the center word in c,\n\u03c6(j) =\n{\n1 if y[j] == 1, \u22121 if y[j] == 0. , (11)\nwhere y is the gold label of a word, while we use 2- dimension vector to represent y, i.e. the negative polarity as [1,0] and the positive polarity as [0,1]. The sentiment polarity is from existing sentiment lexicon, in our experiments, we use the lexicon from (Hu and Liu 2004). Remarkably, if the center word is not sentiment word, we only optimize the n-gram score.\nTwitter Level-Specific Layers For right sub network (modelling Twitter level sentiment), each linear transformation in shared unit is seen as a convolution operation on tweet text sequence. Subsequently, we use three pooling methods, max-pooling, average-pooling and min-pooling on e1, e2, ..., en to get fixed dimensional features max(e),\navg(e) and min(e). Then, we concatenate these features and feed them into a linear layer to get\na2 = W 2 1 \u2217 [max(e)\u2295 avg(e)\u2295min(e)] + b 2 2, (12)\nwhere W 21 \u2208 R t\u2217h\u00d7h and b21 \u2208 R h are the parameter of linear layer. Finally, the top softmax layer predicts the tweet sentiment\nfds = softmax(a2) (13)\nThe loss function of tweet level is\nloss2(D) = \u2212 \u2211\nk={0,1}\ngk(D)log f ds k , (14)\nwhere g(\u00b7) is the gold sentiment distribution of tweet on [positive, negative].\nSince Equation (8) and Equation (14) optimize word level and tweet level information loss, respectively. Our final optimization objective is to get an overall score, as follow\nloss = \u03b2 \u2217 loss1(c, c\u0303) + (1\u2212 \u03b2) \u2217 loss2(D) (15)\nwhere \u03b2 is the trade-off coefficient between two levels. Model Training We train word embedding from both lexicon and massive distant-supervised tweets that is collected with positive (e.g. #happy, #joy, #happyness) and negative (e.g. #sadness, #angry, #frustrated) hashtag and emoticons (e.g. :( :-( : ( :) :-) : ) :D). We crawl tweets from March 1st, 2015 to April 31th, 2015. We tokenize each tweet with NLTK package, remove the @user, URLs, duplicates, quotes, spams and tweets written in language other than English. Finally, we collect 5M positive tweets and 5M negative tweets.\nWe use Stochastic Gradient Descent (SGD) to optimize the training target. For speeding up the training process, mini-batch training is commonly used in SGD. But, for word level training, there are a number of valid words1 in a tweet need to compute n-gram and sentiment loss. And there is one loss value in tweet level sentiment prediction. Moreover, for calculating tweet level sentiment prediction score, it must first compute the linear transformation a1. Therefore, general mini-batch can not be used in our model. Here we use\n1The word has enough left and right context words among the window settings.\none trick which uses two batch sizes in training. The main batch size is for tweet level, and the second batch size is set as the window number in a tweet for word level training. In other words, the batch size is changeable for word level while batch size is fixed for tweet level through the training process. During training, we empirically set the window size as 3, the embedding dimension as 50, the length of hidden layer as 20, the main batch size as 32 and the learning rate as 0.01."}, {"heading": "Sentiment Classification with Sentiment-Enriched Word Embedding", "text": "After learning sentiment-specific word embedding, it can be used for sentiment classification by using existing supervised learning framework (e.g. SVM). Here, we use a neural network model to perform the classification task. The architecture of the model is showed in Figure 2. Firstly, a convolutional layer with multiple filters applied in the input tweet, which have been represented by using learned sentiment-specific word embedding. Then, a Max pooling layer takes the maximum value as the feature of each convolutional filter. The next hidden layer is a full connected layer with ReLU activation, which is used for learning hidden feature representation. The final layer is a full connected layer with 2-dimension output that is used for predicting the positive/negative polarity distribution with softmax. We use dropout on input layer and hidden layer for regularization. The classifier is trained by using back-propagation with AdaGrad to update parameters."}, {"heading": "Experimental Evaluation", "text": ""}, {"heading": "Datasets and Settings", "text": "Datasets For demonstrating the effectiveness of the proposed method, we perform experiments on the following two datasets: 1) SemEval2013, which is a standard Twitter sentiment classification benchmark (Nakov and Sara 2013); 2) CST (Context-Sensitive Twitter), which is the latest benchmark for Twitter sentiment classification task (Ren et al. 2016a). (Ren et al. 2016a) crawled basic opinion tweet and its context for evaluating their model, which utilized the contextual tweets as auxiliary data for Twitter sentiment classifi-\ncation. In our experiments, we only use the basic data rather than both basic and contextual tweets, because our model does not consider the contextual tweets at present. Table 1 provides detailed information about each dataset. Evaluation metric is Macro-F1 of positive and negative categories.\nHyper-parameters The hyper-parameter for specific task should carefully tune, for easily comparison of the experimental result, we use the unified setting that is chosen via a grid search on SemEval2013 developing dataset. There are seven hyper-parameters in the final model, including the network structure parameters (i.e. embedding dimension D, the length of hidden layer H , the convolutional filter size S and filter number N ) and the parameters for supervised training (i.e. the dropout rate of input layer d1, the dropout rate of hiddenReLU layer d2 and the learning rate \u03b7 of AdaGrad). Table 2 reports all the hyper-parameters."}, {"heading": "Results of Comparison Experiments", "text": "We compare our model with a number of state-of-the-art methods. The results of our models against other methods are listed in Table 3. All the methods fall into two categories: traditional classifier with various features and neural network classifier. In first category, SSWE achieves the best performance, which uses word embedding features that encode both n-gram and sentiment information. Because not explicitly exploiting sentiment information, C&W and Word2vec features are relatively weak. NRC system performs better than other partners except SSWE, because of using sentiment lexicons and many manually designed features. For second category, neural network classifier can naturally use word embedding for classification. Both TSWE and CNNM-Local, which exploited other extra information rather than sentiment, achieved the current best performance in SemEval2013 and SST, respectively. Under the same condition that only using sentiment information, our model performs better than them. Both our method and NRC utilize sentiment lexicons and achieve better performance, it demonstrates sentiment lexicon is still a strong resource for informal Twitter text sentiment classification."}, {"heading": "Effect of parameter \u03b2", "text": "As given in Equation (15), \u03b2 is the trade-off between word level and tweet level information. We tune \u03b2 on the development set of SemEval2013. For another coefficient \u03b1, we follow (Tang et al. 2014) to set it as 0.5. Figure 3 shows the macro-F1 scores of MSWE with different \u03b2 on SemEval2013 development set. It shows that MSWE gives better performance when \u03b2 = 0.8, which means giving more consideration to n-gram and word sentiment from lexicon. The model with \u03b2 = 1 stands for SSWE but the training sentiment label of word is from lexicon and \u03b2 = 0 stands for only using tweet level sentiment information. The model with \u03b2 = 0 gives lower performance, which shows the ngram information is an indispensable evidence for Twitter sentiment classification. As a result, we set \u03b2 as 0.8 in our final experiments."}, {"heading": "Related Work", "text": "For Twitter sentiment classification, many work follow traditional sentiment classification methods (Pang, Lee, and Vaithyanathan 2002), which used machine learning methods to train a classifier for Twitter sentiment. Except common used text features, there are some distant supervised features can be utilized(Go, Bhayani, and Huang 2009). Many studies use these massive noisy labelled tweets as training data or auxiliary source for Twitter sentiment classification (Hu et al. 2013). Unlike previous studies, our approach uses distant supervised information as well as lexicon knowledge for training word embedding, which is a combination of noisylabelled resource and knowledge base.\nThere is a large body of work on word embedding learning (Mikolov et al. 2013; Collobert et al. 2011; Pennington, Socher, and Manning 2014). These models are based on word correlations within context windows. Recently, several methods integrate other information into word representation learning for specific tasks, which are more bene-\nficial than common embedding (Liu, Qiu, and Huang 2015; Ren et al. 2016b; Tang et al. 2014; Zhou et al. 2015).\nFor sentiment classification, (Tang et al. 2014) proposed to integrate the sentiment information of tweets into neural network to learn sentiment specific word embedding. (Tang et al. 2014) added a new optimization objective on the top layer of C&W model (Collobert et al. 2011), and it is able to add more optimization objective, such as topic distribution (Ren et al. 2016b). However, it is unable to simultaneously integrate both word level and tweet level sentiment information into its optimization function. Therefore, we propose multi-level sentiment specific word embedding learning model to tackle this problem. The difference is 1) our method uses lexicon knowledge as supervised information in word level sentiment, while they use twitter overall sentiment as the label of all the words in that twitter; 2) we treat each context window in word level learning as a convolution operation, and then pool all the context windows into neural network to predict tweet level sentiment polarity, while they assigned the tweet overall sentiment label to each word instead of modelling twitter sentiment.\nOur method uses similar schema as multi-task learning, which is firstly proposed in (Caruana 1997). For natural language processing community, a notable work about multitask learning was proposed by (Collobert and Weston 2008). In their model, each considered task shared the lookup tables as well as the first hidden layer, and the task was randomly selected for training in each round. (Liu et al. 2015) proposed to jointly train semantic classification and information retrieval, which have more shared layers between two tasks. Most of multi-task learning frameworks can be seen as a parameter sharing approach. However, these work aim to train the model for multi-tasks themselves, while our method\naims to learn the shared embedding. Therefore, our method uses context window as the only convolution filter for Twitter level sentiment modelling. Our main target is to bring Twitter level sentiment information to word embedding but not to predict Twitter sentiment in MSWE. In our model, a shared unit not only represents a context composition window in word level but also a convolution window for tweet level."}, {"heading": "Conclusion", "text": "This paper proposes to utilize both word sentiment label from lexicon and tweet sentiment label from distant supervised information for training sentiment-enriched word embedding. Because these two information are crossing word and tweet level, we develop a multi-level sentiment-enriched word embedding learning method. Our method naturally integrates word level information into tweet level as a convolution result, while simultaneously modelling word level ngram and sentiment information. When using learned word embedding to Twitter sentiment classification, it achieves the best results in standard benchmarks. For our future work, we have plans to integrate more information that may enhance Twitter sentiment classification."}], "references": [{"title": "Robust sentiment detection on twitter from biased and noisy data", "author": ["L. Barbosa", "J. Feng"], "venue": "Proceedings of COLING, COLING \u201910, 36\u201344. Bollen, J.; Mao, H.; and Pepe, A. 2011. Modeling public mood and emotion: Twitter sentiment and socio-economic", "citeRegEx": "Barbosa and Feng,? 2010", "shortCiteRegEx": "Barbosa and Feng", "year": 2010}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning", "citeRegEx": "Caruana,? 1997", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "Multi-task recurrent neural network for immediacy prediction", "author": ["X. Chu", "W. Ouyang", "W. Yang", "X. Wang"], "venue": "The IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Chu et al\\.,? 2015", "shortCiteRegEx": "Chu et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of ICML, ICML \u201908, 160\u2013167.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L.E.O. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res. 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Enhanced sentiment learning using twitter hashtags and smileys", "author": ["D. Davidov", "O. Tsur", "A. Rappoport"], "venue": "Coling 2010: Posters, 241\u2013249.", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Multi-task learning for multiple language translation", "author": ["D. Dong", "H. Wu", "W. He", "D. Yu", "H. Wang"], "venue": "Proceedings of ACL, 1723\u20131732.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Techniques and applications for sentiment analysis", "author": ["R. Feldman"], "venue": "Commun. ACM 56(4):82\u201389.", "citeRegEx": "Feldman,? 2013", "shortCiteRegEx": "Feldman", "year": 2013}, {"title": "Twitter sentiment classification using distant supervision", "author": ["A. Go", "R. Bhayani", "L. Huang"], "venue": "CS224N Project Report, Stanford 1:12.", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of KDD, KDD \u201904, 168\u2013177.", "citeRegEx": "Hu and Liu,? 2004", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Unsupervised sentiment analysis with emotional signals", "author": ["X. Hu", "J. Tang", "H. Gao", "H. Liu"], "venue": "Proceedings of WWW, 607\u2013618. ACM.", "citeRegEx": "Hu et al\\.,? 2013", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "Mikolov."], "venue": "ICML.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["X. Liu", "J. Gao", "X. He", "L. Deng", "K. Duh", "Y.-Y. Wang"], "venue": "Proceedings of NAACL, 912\u2013921.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning contextsensitive word embeddings with neural tensor skip-gram model", "author": ["P. Liu", "X. Qiu", "X. Huang"], "venue": "IJCAI.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Burges, C. J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.; and Weinberger, K. Q., eds., Proc. NIPS, Proc. NIPS, 3111\u20133119. Curran", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets", "author": ["S. Mohammad", "K. Svetlana"], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Mohammad and Svetlana,? 2013", "shortCiteRegEx": "Mohammad and Svetlana", "year": 2013}, {"title": "Sentiment analysis in Twitter with lightweight discourse analysis", "author": ["S. Mukherjee", "P. Bhattacharyya"], "venue": "Proceedings of COLING 2012, 1847\u20131864.", "citeRegEx": "Mukherjee and Bhattacharyya,? 2012", "shortCiteRegEx": "Mukherjee and Bhattacharyya", "year": 2012}, {"title": "Semeval-2013 task 2: Sentiment analysis in twitter", "author": ["P. Nakov", "R. Sara"], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), 312\u2013320.", "citeRegEx": "Nakov and Sara,? 2013", "shortCiteRegEx": "Nakov and Sara", "year": 2013}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval 2(1-2):1\u2013135.", "citeRegEx": "Pang and Lee,? 2008", "shortCiteRegEx": "Pang and Lee", "year": 2008}, {"title": "Thumbs up? sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Proceedings of EMNLP, EMNLP, 79\u201386.", "citeRegEx": "Pang et al\\.,? 2002", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "Proc. EMNLP, Proc. EMNLP, 1532\u20131543. Association for Computational Linguistics.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Political tendency identification in twitter using sentiment analysis techniques", "author": ["F. Pla", "Hurtado", "L.I.S.-F."], "venue": "Proceedings of COLING, 183\u2013192.", "citeRegEx": "Pla et al\\.,? 2014", "shortCiteRegEx": "Pla et al\\.", "year": 2014}, {"title": "Contextsensitive twitter sentiment classification using neural network", "author": ["Y. Ren", "Y. Zhang", "M. Zhang", "D. Ji"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2016}, {"title": "Improving twitter sentiment classification using topic-enriched multiprototype word embeddings", "author": ["Y. Ren", "Y. Zhang", "M. Zhang", "D. Ji"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2016}, {"title": "Exploiting topic based twitter sentiment for stock prediction", "author": ["J. Si", "A. Mukherjee", "B. Liu", "Q. Li", "H. Li", "X. Deng"], "venue": "Proceedings of ACL, 24\u201329.", "citeRegEx": "Si et al\\.,? 2013", "shortCiteRegEx": "Si et al\\.", "year": 2013}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of EMNLP, 151\u2013161.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of EMNLP, 1642. Citeseer.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["D. Tang", "F. Wei", "M. Zhou", "T. Liu", "B. Qin"], "venue": "ACL, ACL.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Learning semantic representations of users and products for document level sentiment classification", "author": ["D. Tang", "B. Qin", "T. Liu"], "venue": "Proceedings of ACL, 1014\u20131023.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Sentiment-specific representation learning for document-level sentiment analysis", "author": ["D. Tang"], "venue": "Proceedings of WSDM, WSDM, 447\u2013452. ACM.", "citeRegEx": "Tang,? 2015", "shortCiteRegEx": "Tang", "year": 2015}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S. Wang", "C.D. Manning"], "venue": "Proceedings ofACL, 90\u201394. Association for Computational Linguistics.", "citeRegEx": "Wang and Manning,? 2012", "shortCiteRegEx": "Wang and Manning", "year": 2012}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proceedings of EMNLP, HLT \u201905, 347\u2013354.", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Learning continuous word embedding with metadata for question retrieval in community question answering", "author": ["G. Zhou", "T. He", "J. Zhao", "P. Hu"], "venue": "Proceedings of ACL, 250\u2013259.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 24, "context": "As a result of its massive, diverse, and rising user base, the containing opinion information in Twitter has been successfully used to many tasks, such as stock market movement prediction (Si et al. 2013), political monitoring (Pla and Hurtado 2014) and inferring public mood about social events (Bollen, Mao, and Pepe 2011).", "startOffset": 188, "endOffset": 204}, {"referenceID": 0, "context": "Researchers have proposed many approaches to improve Twitter classification performance (Davidov, Tsur, and Rappoport 2010; Barbosa and Feng 2010; Mukherjee and Bhattacharyya 2012).", "startOffset": 88, "endOffset": 180}, {"referenceID": 16, "context": "Researchers have proposed many approaches to improve Twitter classification performance (Davidov, Tsur, and Rappoport 2010; Barbosa and Feng 2010; Mukherjee and Bhattacharyya 2012).", "startOffset": 88, "endOffset": 180}, {"referenceID": 4, "context": ", word level and document(sentence) level, for natural language processing tasks (Le and Mikolov 2014; Collobert et al. 2011; Tang, Qin, and Liu 2015).", "startOffset": 81, "endOffset": 150}, {"referenceID": 4, "context": "Traditional word embedding methods (Collobert et al. 2011; Mikolov et al. 2013) model the syntactic context information of words.", "startOffset": 35, "endOffset": 79}, {"referenceID": 14, "context": "Traditional word embedding methods (Collobert et al. 2011; Mikolov et al. 2013) model the syntactic context information of words.", "startOffset": 35, "endOffset": 79}, {"referenceID": 27, "context": "Based on them, (Tang et al. 2014) proposed Sentiment-Specific Word Embedding (SSWE) learning method for Twitter sentiment classification, which aims to tackle the problem that two word with opposite polarity", "startOffset": 15, "endOffset": 33}, {"referenceID": 9, "context": "However, existing work only exploit Twitter overall sentiment label for learning sentiment-specific word embedding, although there are many state-of-the-art sentiment lexicons (Hu and Liu 2004; Wilson, Wiebe, and Hoffmann 2005), which list common sentiment-baring words with its polarity.", "startOffset": 176, "endOffset": 227}, {"referenceID": 29, "context": "For exploiting tweet sentiment label, (Tang 2015) and (Ren et al.", "startOffset": 38, "endOffset": 49}, {"referenceID": 18, "context": "On the other hand, word sentiment polarity from lexicon is still another useful information for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 121, "endOffset": 174}, {"referenceID": 7, "context": "On the other hand, word sentiment polarity from lexicon is still another useful information for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 121, "endOffset": 174}, {"referenceID": 26, "context": "On the other hand, word sentiment polarity from lexicon is still another useful information for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 121, "endOffset": 174}, {"referenceID": 3, "context": "Inspired by recent work for Multi-task deep Learning (Collobert and Weston 2008), we propose to learn word embedding by exploiting multi-level sentiment representation objective optimization approach.", "startOffset": 53, "endOffset": 80}, {"referenceID": 4, "context": "Sentimentspecific word embedding model stems from C&W model (Collobert et al. 2011), which learns word representation from n-gram contexts by using negative sampling.", "startOffset": 60, "endOffset": 83}, {"referenceID": 27, "context": "To address this problem, (Tang et al. 2014) proposed a sentiment-specific word embedding SSWE model base on C&W model.", "startOffset": 25, "endOffset": 43}, {"referenceID": 18, "context": "Existing work demonstrate that sentiment lexicon is an important resource for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 103, "endOffset": 156}, {"referenceID": 7, "context": "Existing work demonstrate that sentiment lexicon is an important resource for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 103, "endOffset": 156}, {"referenceID": 26, "context": "Existing work demonstrate that sentiment lexicon is an important resource for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 103, "endOffset": 156}, {"referenceID": 26, "context": "In particular, (Socher et al. 2013) developed Sentiment Treebank, which is based on word sentiment and further annotates the sentiment label of syntactically plausible phrase of sentences.", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": "As shown in Figure 1(c), we exploit two sub networks for modelling two level optimization objectives by using similar framework as multi-task learning (Chu et al. 2015; Collobert and Weston 2008; Dong et al. 2015).", "startOffset": 151, "endOffset": 213}, {"referenceID": 3, "context": "As shown in Figure 1(c), we exploit two sub networks for modelling two level optimization objectives by using similar framework as multi-task learning (Chu et al. 2015; Collobert and Weston 2008; Dong et al. 2015).", "startOffset": 151, "endOffset": 213}, {"referenceID": 6, "context": "As shown in Figure 1(c), we exploit two sub networks for modelling two level optimization objectives by using similar framework as multi-task learning (Chu et al. 2015; Collobert and Weston 2008; Dong et al. 2015).", "startOffset": 151, "endOffset": 213}, {"referenceID": 9, "context": "The sentiment polarity is from existing sentiment lexicon, in our experiments, we use the lexicon from (Hu and Liu 2004).", "startOffset": 103, "endOffset": 120}, {"referenceID": 17, "context": "Datasets For demonstrating the effectiveness of the proposed method, we perform experiments on the following two datasets: 1) SemEval2013, which is a standard Twitter sentiment classification benchmark (Nakov and Sara 2013); 2) CST (Context-Sensitive Twitter), which is the latest benchmark for Twitter sentiment classification task (Ren et al.", "startOffset": 202, "endOffset": 223}, {"referenceID": 27, "context": "For another coefficient \u03b1, we follow (Tang et al. 2014) to set it as 0.", "startOffset": 37, "endOffset": 55}, {"referenceID": 10, "context": "Many studies use these massive noisy labelled tweets as training data or auxiliary source for Twitter sentiment classification (Hu et al. 2013).", "startOffset": 127, "endOffset": 143}, {"referenceID": 14, "context": "There is a large body of work on word embedding learning (Mikolov et al. 2013; Collobert et al. 2011; Pennington, Socher, and Manning 2014).", "startOffset": 57, "endOffset": 139}, {"referenceID": 4, "context": "There is a large body of work on word embedding learning (Mikolov et al. 2013; Collobert et al. 2011; Pennington, Socher, and Manning 2014).", "startOffset": 57, "endOffset": 139}, {"referenceID": 27, "context": "42 SVM + C&W (Tang et al. 2014) 75.", "startOffset": 13, "endOffset": 31}, {"referenceID": 27, "context": "89 SVM + Word2vec (Tang et al. 2014) 76.", "startOffset": 18, "endOffset": 36}, {"referenceID": 27, "context": "31 NBSVM (Tang et al. 2014) 75.", "startOffset": 9, "endOffset": 27}, {"referenceID": 27, "context": "28 RAE (Tang et al. 2014) 75.", "startOffset": 7, "endOffset": 25}, {"referenceID": 15, "context": "12 NRC (Top System in SemEval) (Mohammad and Svetlana 2013) 84.", "startOffset": 31, "endOffset": 59}, {"referenceID": 27, "context": "24 SSWE (Tang et al. 2014) 84.", "startOffset": 8, "endOffset": 26}, {"referenceID": 30, "context": "NBSVM: a state-of-the-art performer which trades-off between Naive Bayes and NB-enhanced SVM (Wang and Manning 2012).", "startOffset": 93, "endOffset": 116}, {"referenceID": 25, "context": "RAE: Recursive Autoencoders with pre-trained word vectors from Wikipedia (Socher et al. 2011).", "startOffset": 73, "endOffset": 93}, {"referenceID": 15, "context": "NRC: the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features (Mohammad and Svetlana 2013).", "startOffset": 167, "endOffset": 195}, {"referenceID": 27, "context": "SSWE: SVM classifier with SSWE word embedding features (Tang et al. 2014).", "startOffset": 55, "endOffset": 73}, {"referenceID": 27, "context": "ficial than common embedding (Liu, Qiu, and Huang 2015; Ren et al. 2016b; Tang et al. 2014; Zhou et al. 2015).", "startOffset": 29, "endOffset": 109}, {"referenceID": 32, "context": "ficial than common embedding (Liu, Qiu, and Huang 2015; Ren et al. 2016b; Tang et al. 2014; Zhou et al. 2015).", "startOffset": 29, "endOffset": 109}, {"referenceID": 27, "context": "For sentiment classification, (Tang et al. 2014) proposed to integrate the sentiment information of tweets into neural network to learn sentiment specific word embedding.", "startOffset": 30, "endOffset": 48}, {"referenceID": 27, "context": "(Tang et al. 2014) added a new optimization objective on the top layer of C&W model (Collobert et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "2014) added a new optimization objective on the top layer of C&W model (Collobert et al. 2011), and it is able to add more optimization objective, such as topic distribution (Ren et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 1, "context": "Our method uses similar schema as multi-task learning, which is firstly proposed in (Caruana 1997).", "startOffset": 84, "endOffset": 98}, {"referenceID": 3, "context": "For natural language processing community, a notable work about multitask learning was proposed by (Collobert and Weston 2008).", "startOffset": 99, "endOffset": 126}, {"referenceID": 12, "context": "(Liu et al. 2015) proposed to jointly train semantic classification and information retrieval, which have more shared layers between two tasks.", "startOffset": 0, "endOffset": 17}], "year": 2016, "abstractText": "Most of existing work learn sentiment-specific word representation for improving Twitter sentiment classification, which encoded both n-gram and distant supervised tweet sentiment information in learning process. They assume all words within a tweet have the same sentiment polarity as the whole tweet, which ignores the word its own sentiment polarity. To address this problem, we propose to learn sentimentspecific word embedding by exploiting both lexicon resource and distant supervised information. We develop a multi-level sentiment-enriched word embedding learning method, which uses parallel asymmetric neural network to model n-gram, word level sentiment and tweet level sentiment in learning process. Experiments on standard benchmarks show our approach outperforms state-of-the-art methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}