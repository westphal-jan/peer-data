{"id": "1611.00179", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Dual Learning for Machine Translation", "abstract": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation \\emph{dual-NMT}. Experiments show that dual-NMT works very well on English$\\leftrightarrow$French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.", "histories": [["v1", "Tue, 1 Nov 2016 10:38:29 GMT  (46kb,D)", "http://arxiv.org/abs/1611.00179v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["di he", "yingce xia", "tao qin", "liwei wang 0001", "nenghai yu", "tie-yan liu", "wei-ying ma"], "accepted": true, "id": "1611.00179"}, "pdf": {"name": "1611.00179.pdf", "metadata": {"source": "CRF", "title": "Dual Learning for Machine Translation", "authors": ["Yingce Xia", "Di He", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma"], "emails": ["yingce.xia@gmail.com;", "ynh@ustc.edu.cn", "dih@cis.pku.edu.cn;", "wanglw@cis.pku.edu.cn;", "taoqin@microsoft.com", "tie-yan.liu@microsoft.com", "wyma@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora. However, such parallel data are costly to collect in practice and thus are usually limited in scale, which may constrain the related research and applications.\nGiven that there exist almost unlimited monolingual data in the Web, it is very natural to leverage them to boost the performance of MT systems. Actually different methods have been proposed for this purpose, which can be roughly classified into two categories. In the first category [2, 5], monolingual corpora in the target language are used to train a language model, which is then integrated with the MT models trained from parallel bilingual corpora to improve the translation quality. In the second category [16, 12], pseudo bilingual sentence pairs are generated from monolingual data by using the translation model trained from aligned parallel corpora, and then these pseudo bilingual sentence\n\u2217The first two authors contributed equally to this work.\nar X\niv :1\n61 1.\n00 17\n9v 1\n[ cs\n.C L\n] 1\nN ov\npairs are used to enlarge the training data for subsequent learning. While the above methods could improve the MT performance to some extent, they still suffer from certain limitations. The methods in the first category only use the monolingual data to train language models, but do not fundamentally address the shortage of parallel training data. Although the methods in the second category can enlarge the parallel training data, there is no guarantee/control on the quality of the pseudo bilingual sentence pairs.\nIn this paper, we propose a dual-learning mechanism that can leverage monolingual data (in both the source and target languages) in a more effective way. By using our proposed mechanism, these monolingual data can play a similar role to the parallel bilingual data, and significantly reduce the requirement on parallel bilingual data during the training process. Specifically, the dual-learning mechanism for MT can be described as the following two-agent communication game.\n1. The first agent, who only understands language A, sends a message in language A to the second agent through a noisy channel, which converts the message from language A to language B using a translation model.\n2. The second agent, who only understands language B, receives the translated message in language B. She checks the message and notifies the first agent whether it is a natural sentence in language B (note that the second agent may not be able to verify the correctness of the translation since the original message is invisible to her). Then she sends the received message back to the first agent through another noisy channel, which converts the received message from language B back to language A using another translation model.\n3. After receiving the message from the second agent, the first agent checks it and notifies the second agent whether the message she receives is consistent with her original message. Through the feedback, both agents will know whether the two communication channels (and thus the two translation models) perform well and can improve them accordingly.\n4. The game can also be started from the second agent with an original message in language B, and then the two agents will go through a symmetric process and improve the two channels (translation models) according to the feedback.\nIt is easy to see from the above descriptions, although the two agents may not have aligned bilingual corpora, they can still get feedback about the quality of the two translation models and collectively improve the models based on the feedback. This game can be played for an arbitrary number of rounds, and the two translation models will get improved through this reinforcement procedure (e.g., by means of the policy gradient methods). In this way, we develop a general learning framework for training machine translation models through a dual-learning game.\nThe dual learning mechanism has several distinguishing features. First, we train translation models from unlabeled data through reinforcement learning. Our work significantly reduces the requirement on the aligned bilingual data, and it opens a new window to learn to translate from scratch (i.e., even without using any parallel data). Experimental results show that our method is very promising.\nSecond, we demonstrate the power of deep reinforcement learning (DRL) for complex real-world applications, rather than just games. Deep reinforcement learning has drawn great attention in recent years. However, most of them today focus on video or board games, and it remains a challenge to enable DRL for more complicated applications whose rules are not pre-defined and where there is no explicit reward signals. Dual learning provides a promising way to extract reward signals for reinforcement learning in real-world applications like machine translation.\nThe remaining parts of the paper are organized as follows. In Section 2, we briefly review the literature of neural machine translation. After that, we introduce our dual-learning algorithm for neural machine translation. The experimental results are provided and discussed in Section 4. We extend the breadth and depth of dual learning and discuss future directions in the last section."}, {"heading": "2 Background: Neural Machine Translation", "text": "In principle, our dual-learning framework can be applied to both phrase-based statistical machine translation and neural machine translation. In this paper, we focus on the latter one, i.e., neural machine translation (NMT), due to its simplicity as an end-to-end system, without suffering from human crafted engineering [6].\nNeural machine translation systems are typically implemented with a Recurrent Neural Network (RNN) based encoder-decoder framework. Such a framework learns a probabilistic mapping P (y|x) from a source language sentence x = {x1, x2, ..., xTx} to a target language sentence y = {y1, y2, ..., yTy} , in which xi and yt are the i-th and t-th words for sentences x and y respectively.\nTo be more concrete, the encoder of NMT reads the source sentence x and generates Tx hidden states by an RNN:\nhi = f(hi\u22121, xi) (1)\nin which hi is the hidden state at time i, and function f is the recurrent unit such as Long Short-Term Memory (LSTM) unit [14] or Gated Recurrent Unit (GRU) [4]. Afterwards, the decoder of NMT computes the conditional probability of each target word yt given its proceeding words y<t, as well as the source sentence, i.e., P (yt|y<t, x), which is then used to specify P (y|x) according to the probability chain rule. P (yt|y<t, x) is given as:\nP (yt|y<t, x) \u221d exp(yt; rt, ct) (2) rt = g(rt\u22121, yt\u22121, ct) (3)\nct = q(rt\u22121, h1, \u00b7 \u00b7 \u00b7 , hTx) (4)\nwhere rt is the decoder RNN hidden state at time t, similarly computed by an LSTM or GRU, and ct denotes the contextual information in generating word yt according to different encoder hidden states. ct can be a \u2018global\u2019 signal summarizing sentence x [4, 14], e.g., c1 = \u00b7 \u00b7 \u00b7 = cTy = hTx , or \u2018local\u2019 signal implemented by an attention mechanism [1], e.g., ct = \u2211Tx i=1 \u03b1ihi, \u03b1i = exp{a(hi,rt\u22121)}\u2211 j exp{a(hj ,rt\u22121)} , where a(\u00b7, \u00b7) is a feed-forward neural network. We denote all the parameters to be optimized in the neural network as \u0398 and denote D as the dataset that contains source-target sentence pairs for training. Then the learning objective is to seek the optimal parameters \u0398\u2217:\n\u0398\u2217 = argmax \u0398 \u2211 (x,y)\u2208D Ty\u2211 t=1 logP (yt|y<t, x; \u0398) (5)"}, {"heading": "3 Dual Learning for Neural Machine Translation", "text": "In this section, we present the dual-learning mechanism for neural machine translation. Noticing that MT can (always) happen in dual directions, we first design a two-agent game with a forward translation step and a backward translation step, which can provide quality feedback to the dual translation models even using monolingual data only. Then we propose a dual-learning algorithm, called dual-NMT, to improve the two translation models based on the quality feedback provided in the game.\nConsider two monolingual corpora DA and DB which contain sentences from language A and B respectively. Please note these two corpora are not necessarily aligned with each other, and they may even have no topical relationship with each other at all. Suppose we have two (weak) translation models that can translate sentences from A to B and verse visa. Our goal is to improve the accuracy of the two models by using monolingual corpora instead of parallel corpora. Our basic idea is to leverage the duality of the two translation models. Starting from a sentence in any monolingual data, we first translate it forward to the other language and then further translate backward to the original language. By evaluating this two-hop translation results, we will get a sense about the quality of the two translation models, and be able to improve them accordingly. This process can be iterated for many rounds until both translation models converge.\nSuppose corpus DA contains NA sentences, and DB contains NB sentences. Denote P (.|s; \u0398AB) and P (.|s; \u0398BA) as two neural translation models, where \u0398AB and \u0398BA are their parameters (as described in Section 2).\nAssume we already have two well-trained language models LMA(.) and LMB(.) (which are easy to obtain since they only require monolingual data), each of which takes a sentence as input and outputs a real value to indicate how confident the sentence is a natural sentence in its own language. Here the\nAlgorithm 1 The dual-learning algorithm 1: Input: Monolingual corpora DA and DB , initial translation models \u0398AB and \u0398BA, language\nmodels LMA and LMB , hyper-parameter \u03b1, beam search size K, learning rates \u03b31,t, \u03b32,t . 2: repeat 3: t = t+ 1. 4: Sample sentence sA and sB from DA and DB respectively. 5: Set s = sA. . Model update for the game beginning from A. 6: Generate K sentences smid,1, . . . , smid,K using beam search according to translation model P (.|s; \u0398AB). 7: for k = 1, . . . ,K do 8: Set the language-model reward for the kth sampled sentence as r1,k = LMB(smid,k). 9: Set the communication reward for the kth sampled sentence as r2,k =\nlogP (s|smid,k; \u0398BA). 10: Set the total reward of the kth sample as rk = \u03b1r1,k + (1\u2212 \u03b1)r2,k. 11: end for 12: Compute the stochastic gradient of \u0398AB :\n\u2207\u0398AB E\u0302[r] = 1\nK K\u2211 k=1 [rk\u2207\u0398AB logP (smid,k|s; \u0398AB)].\n13: Compute the stochastic gradient of \u0398BA:\n\u2207\u0398BAE\u0302[r] = 1\nK K\u2211 k=1 [(1\u2212 \u03b1)\u2207\u0398BA logP (s|smid,k; \u0398BA)].\n14: Model updates:\n\u0398AB \u2190 \u0398AB + \u03b31,t\u2207\u0398AB E\u0302[r],\u0398BA \u2190 \u0398BA + \u03b32,t\u2207\u0398BAE\u0302[r].\n15: Set s = sB . . Model update for the game beginning from B. 16: Go through line 6 to line 14 symmetrically. 17: until convergence\nlanguage models can be trained either using other resources, or just using the monolingual data DA and DB .\nFor a game beginning with sentence s in DA, denote smid as the middle translation output. This middle step has an immediate reward r1 = LMB(smid), indicating how natural the output sentence is in language B. Given the middle translation output smid, we use the log probability of s recovered from smid as the reward of the communication (we will use reconstruction and communication interchangeably). Mathematically, reward r2 = logP (s|smid; \u0398BA). We simply adopt a linear combination of the LM reward and communication reward as the total reward, e.g., r = \u03b1r1 + (1 \u2212 \u03b1)r2, where \u03b1 is a hyper-parameter. As the reward of the game can be considered as a function of s, smid and translation models \u0398AB and \u0398BA, we can optimize the parameters in the translation models through policy gradient methods for reward maximization, as widely used in reinforcement learning [15].\nWe sample smid according to the translation model P (.|s; \u0398AB). Then we compute the gradient of the expected reward E[r] with respect to parameters \u0398AB and \u0398BA. According to the policy gradient theorem [15], it is easy to verify that\n\u2207\u0398BAE[r] = E[(1\u2212 \u03b1)\u2207\u0398BA logP (s|smid; \u0398BA)] (6)\n\u2207\u0398ABE[r] = E[r\u2207\u0398AB logP (smid|s; \u0398AB)] (7)\nin which the expectation is taken over smid.\nBased on Eqn.(6) and (7), we can adopt any sampling approach to estimate the expected gradient. Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation [10, 14, 11], we use beam search [14] to obtain more meaningful results (more\nreasonable middle translation outputs) for gradient computation, i.e., we greedily generate top-K high-probability middle translation outputs, and use the averaged value on the beam search results to approximate the true gradient. If the game begins with sentence s in DB , the computation of the gradient is just symmetric and we omit it here.\nThe game can be repeated for many rounds. In each round, one sentence is sampled from DA and one from DB , and we update the two models according to the game beginning with the two sentences respectively. The details of this process are given in Algorithm 1."}, {"heading": "4 Experiments", "text": "We conducted a set of experiments to test the proposed dual-learning mechanism for neural machine translation."}, {"heading": "4.1 Settings", "text": "We compared our dual-NMT approach with two baselines: the standard neural machine translation [1] (NMT for short), and a recent NMT-based method [12] which generates pseudo bilingual sentence pairs from monolingual corpora to assist training (pseudo-NMT for short). We leverage a tutorial NMT system implemented by Theano for all the experiments. 2\nWe evaluated our algorithm on the translation task of a pair of languages: English\u2192French (En\u2192Fr) and French\u2192English (Fr\u2192En). In detail, we used the same bilingual corpora from WMT\u201914 as used in [1, 6], which contains 12M sentence pairs extracting from five datasets: Europarl v7, Common Crawl corpus, UN corpus, News Commentary, and 109French-English corpus. Following common practices, we concatenated newstest2012 and newstest2013 as the validation set, and used newstest2014 as the testing set. We used the \u201cNews Crawl: articles from 2012\u201d provided by WMT\u201914 as monolingual data.\nWe used the GRU networks and followed the practice in [1] to set experimental parameters. For each language, we constructed the vocabulary with the most common 30K words in the parallel corpora, and out-of-vocabulary words were replaced with a special token <UNK>. For monolingual corpora, we removed the sentences containing at least one out-of-vocabulary words. Each word was projected into a continuous vector space of 620 dimensions, and the dimension of the recurrent unit was 1000. We removed sentences with more than 50 words from the training set. Batch size was set as 80 with 20 batches pre-fetched and sorted by sentence lengths.\nFor the baseline NMT model, we exactly followed the settings reported in [1]. For the baseline pseudo-NMT [12], we used the trained NMT model to generate pseudo bilingual sentence pairs from monolingual data, removed the sentences with more than 50 words, merged the generated data with the original parallel training data, and then trained the model for testing. Each of the baseline models was trained with AdaDelta [17] on K40m GPU until their performances stopped to improve on the validation set.\nOur method needs a language model for each language. We trained an RNN based language model [8] for each language using its corresponding monolingual corpus. Then the language model was fixed and the log likelihood of a received message was used to reward the communication channel (i.e., the translation model) in our experiments.\nWhile playing the game, we initialized the channels using warm-start translation models (e.g., trained from bilingual data corpora), and see whether dual-NMT can effectively improve the machine translation accuracy. In our experiments, in order to smoothly transit from the initial model trained from bilingual data to the model training purely from monolingual data, we adopted the following soft-landing strategy. At the very beginning of the dual learning process, for each mini batch, we used half sentences from monolingual data and half sentences from bilingual data (sampled from the dataset used to train the initial model). The objective was to maximize the reward based on monolingual data defined in Section 3 together the likelihood on bilingual data defined in Section 2. When the training process went on, we gradually increased the percentage of monolingual sentences in the mini batch, until no bilingual data were used at all. Specifically, we tested two settings in our experiments:\n2dl4mt-tutorial: https://github.com/nyu-dl\n\u2022 In the first setting (referred to Large), we used all the 12M bilingual sentences pairs during the soft-landing process. That is, the warm start model was learnt based on full bilingual data.\n\u2022 In the second setting (referred to Small), we randomly sampled 10% of the 12M bilingual sentences pairs and used them during the soft-landing process.\nFor each of the settings we trained our dual-NMT algorithm for one week. We set the beam search size to be 2 in the middle translation process, and \u03b1 = 0.005. We found that using stochastic gradient decent performs very well for our algorithm, and we chose the \u03b31,t to be 0.0002 and chose the \u03b32,t to be 0.02. All the hyperparameters in the experiments are set by cross validation.\nWe used the BLEU score [9] as the evaluation metric, which are computed by the multi-bleu.perl script3. Following the common practice, during testing we used beam search [14] with beam size of 12 for all the algorithms as in many previous works."}, {"heading": "4.2 Results and Analysis", "text": "We report the experimental results in this section. Recall that the two baselines for English\u2192French and French\u2192English are trained separately while our dual-NMT conducts joint training. We summarize the overall performances in Table 1 and plot the BLEU scores with respect to the length of source sentences in Figure 1.\nFrom Table 1 we can see that our dual-NMT algorithm outperforms the baseline algorithms in all the settings. For the translation from English to French, dual-NMT outperforms the baseline NMT by about 2.1/3.4 points for the first/second warm start setting, and outperforms pseudo-NMT by about 1.7/3.1 points for both settings. For the translation from French to English, the improvement is more significant: our dual-NMT outperforms NMT by about 2.3/5.2 points for the first/second warm start setting, and outperforms pseudo-NMT by about 2.1/4.3 points for both settings. Surprisingly,\n3https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nwith only 10% bilingual data, dual-NMT achieves comparable translation accuracy as vanilla NMT using 100% bilingual data for the Fr\u2192En task. These results demonstrate the effectiveness of our dual-NMT algorithm. Furthermore, we have the following observations:\n\u2022 Although pseudo-NMT outperforms NMT, its improvements are not very significant. Our hypothesis is that the quality of pseudo bilingual sentence pairs generated from the monolingual data is not very good, which limits the performance gain of pseudo-NMT. One might need to carefully select and filter the generated pseudo bilingual sentence pairs to get better performance for pseudo-NMT.\n\u2022 When the parallel bilingual data are small, dual-NMT makes larger improvement. This shows that the dual-learning mechanism makes very good utilization of monolingual data. Thus we expect dual-NMT will be more helpful for language pairs with smaller labeled parallel data. Dual-NMT opens a new window to learn to translate from scratch.\nWe plot BLEU scores with respect to the length of source sentences in Figure 1. From the figure, we can see that our dual-NMT algorithm outperforms the baseline algorithms in all the ranges of length.\nWe make some deep studies on our dual-NMT algorithm in Table 2. We study the self-reconstruction performance of the algorithms: For each sentence in the test set, we translated it forth and back using the models and then checked how close the back translated sentence is to the original sentence using the BLEU score. We also used beam search to generate all the translation results. It can be easily seen from Table 2 that the self-reconstruction BLEU scores of our dual-NMT are much higher than NMT and pseudo-NMT. In particular, our proposed method outperforms NMT by about 11.9/9.6 points when using warm-start model trained on large parallel data, and outperforms NMT for about 20.7/17.8 points when using the warm-start model trained on 10% parallel data.\nWe list several example sentences in Table 4 to compare the self-reconstruction results of models before and after dual learning. It is quite clear that after dual learning, the reconstruction is largely improved for both directions, i.e., English\u2192French\u2192English and French\u2192English\u2192French.\nWe summarize the En\u2192Fr BLEU scores (carried out by multi-bleu.pl) on tokenized newstest2014 of several 1-layer NMT based machine translation systems in Table 3. The numbers of the second column are the BLEU scores before/after postprocessing the <UNK>. We use the method proposed by [6] to deal with unknown words. We can see that our dual-NMT outperforms the above baselines.\nTo summarize, all the results show that the dual-learning mechanism is promising and better utilizes the monolingual data."}, {"heading": "5 Discussions", "text": "In this section, we discuss the possible extensions of our proposed dual learning mechanism and list several future works for machine translation.\nFirst, although we have focused on machine translation in this work, the basic idea of dual learning is generally applicable: as long as two tasks are in dual form, we can apply the dual-learning mechanism to simultaneously learn both tasks from unlabeled data using reinforcement learning algorithms. Actually, many AI tasks are naturally in dual form, for example, speech recognition versus text to speech, image caption versus image generation, question answering versus question generation (e.g., Jeopardy!), search (matching queries to documents) versus keyword extraction (extracting keywords/queries for documents), so on and so forth. It would very be interesting to design and test dual-learning algorithms for more dual tasks beyond machine translation.\nSecond, although we have focused on dual learning on two tasks, our technology is not restricted to two tasks only. Actually, our key idea is to form a closed loop so that we can extract feedback signals by comparing the original input data with the final output data. Therefore, if more than two associated tasks can form a closed loop, we can apply our technology to improve the model in each task from unlabeled data. For example, for an English sentence x, we can first translate it to a Chinese sentence y, then translate y to a French sentence z, and finally translate z back to an English sentence x\u2032. The similarity between x and x\u2032 can indicate the effectiveness of the three translation models in the loop, and we can once again apply the policy gradient methods to update and improve these models based on the feedback signals during the loop. We would like to name this generalized dual learning as close-loop learning, and will test its effectiveness in the future.\nWe plan to explore the following directions in the future. First, in the experiments we used bilingual data to warm start the training of dual-NMT. A more exciting direction is to learn from scratch, i.e., to learn translations directly from monolingual data of two languages (maybe plus lexical dictionary). Second, our dual-NMT was based on NMT systems in this work. Our basic idea can also be applied to phrase-based SMT systems and we will look into this direction. Third, we only considered a pair of languages in this paper. We will extend our approach to jointly train multiple translation models for a tuple of 3+ languages using monolingual data."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Large language models in machine translation", "author": ["T. Brants", "A.C. Popat", "P. Xu", "F.J. Och", "J. Dean"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Citeseer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Y. Cheng", "S. Shen", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "IJCAI", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "On using monolingual corpora in neural machine translation", "author": ["C. Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "H.-C. Lin", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1503.03535", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48\u201354. Association for Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, volume 2, page 3", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": "arXiv preprint arXiv:1511.06732", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Improving neural machine translation models with monolingual data", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "ACL", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "ACL", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, pages 1057\u20131063", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Semi-supervised model adaptation for statistical machine translation", "author": ["N. Ueffing", "G. Haffari", "A. Sarkar"], "venue": "Machine Translation Journal", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 118, "endOffset": 128}, {"referenceID": 3, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 118, "endOffset": 128}, {"referenceID": 13, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 118, "endOffset": 128}, {"referenceID": 0, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 199, "endOffset": 205}, {"referenceID": 5, "context": "State-of-the-art machine translation (MT) systems, including both the phrase-based statistical translation approaches [7, 4, 14] and the recently emerged neural networks based translation approaches [1, 6], heavily rely on aligned parallel training corpora.", "startOffset": 199, "endOffset": 205}, {"referenceID": 1, "context": "In the first category [2, 5], monolingual corpora in the target language are used to train a language model, which is then integrated with the MT models trained from parallel bilingual corpora to improve the translation quality.", "startOffset": 22, "endOffset": 28}, {"referenceID": 4, "context": "In the first category [2, 5], monolingual corpora in the target language are used to train a language model, which is then integrated with the MT models trained from parallel bilingual corpora to improve the translation quality.", "startOffset": 22, "endOffset": 28}, {"referenceID": 15, "context": "In the second category [16, 12], pseudo bilingual sentence pairs are generated from monolingual data by using the translation model trained from aligned parallel corpora, and then these pseudo bilingual sentence \u2217The first two authors contributed equally to this work.", "startOffset": 23, "endOffset": 31}, {"referenceID": 11, "context": "In the second category [16, 12], pseudo bilingual sentence pairs are generated from monolingual data by using the translation model trained from aligned parallel corpora, and then these pseudo bilingual sentence \u2217The first two authors contributed equally to this work.", "startOffset": 23, "endOffset": 31}, {"referenceID": 5, "context": ", neural machine translation (NMT), due to its simplicity as an end-to-end system, without suffering from human crafted engineering [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 13, "context": "in which hi is the hidden state at time i, and function f is the recurrent unit such as Long Short-Term Memory (LSTM) unit [14] or Gated Recurrent Unit (GRU) [4].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "in which hi is the hidden state at time i, and function f is the recurrent unit such as Long Short-Term Memory (LSTM) unit [14] or Gated Recurrent Unit (GRU) [4].", "startOffset": 158, "endOffset": 161}, {"referenceID": 3, "context": "ct can be a \u2018global\u2019 signal summarizing sentence x [4, 14], e.", "startOffset": 51, "endOffset": 58}, {"referenceID": 13, "context": "ct can be a \u2018global\u2019 signal summarizing sentence x [4, 14], e.", "startOffset": 51, "endOffset": 58}, {"referenceID": 0, "context": ", c1 = \u00b7 \u00b7 \u00b7 = cTy = hTx , or \u2018local\u2019 signal implemented by an attention mechanism [1], e.", "startOffset": 83, "endOffset": 86}, {"referenceID": 14, "context": "As the reward of the game can be considered as a function of s, smid and translation models \u0398AB and \u0398BA, we can optimize the parameters in the translation models through policy gradient methods for reward maximization, as widely used in reinforcement learning [15].", "startOffset": 260, "endOffset": 264}, {"referenceID": 14, "context": "According to the policy gradient theorem [15], it is easy to verify that \u2207\u0398BAE[r] = E[(1\u2212 \u03b1)\u2207\u0398BA logP (s|smid; \u0398BA)] (6)", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation [10, 14, 11], we use beam search [14] to obtain more meaningful results (more", "startOffset": 118, "endOffset": 130}, {"referenceID": 13, "context": "Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation [10, 14, 11], we use beam search [14] to obtain more meaningful results (more", "startOffset": 118, "endOffset": 130}, {"referenceID": 10, "context": "Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation [10, 14, 11], we use beam search [14] to obtain more meaningful results (more", "startOffset": 118, "endOffset": 130}, {"referenceID": 13, "context": "Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation [10, 14, 11], we use beam search [14] to obtain more meaningful results (more", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "We compared our dual-NMT approach with two baselines: the standard neural machine translation [1] (NMT for short), and a recent NMT-based method [12] which generates pseudo bilingual sentence pairs from monolingual corpora to assist training (pseudo-NMT for short).", "startOffset": 94, "endOffset": 97}, {"referenceID": 11, "context": "We compared our dual-NMT approach with two baselines: the standard neural machine translation [1] (NMT for short), and a recent NMT-based method [12] which generates pseudo bilingual sentence pairs from monolingual corpora to assist training (pseudo-NMT for short).", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "In detail, we used the same bilingual corpora from WMT\u201914 as used in [1, 6], which contains 12M sentence pairs extracting from five datasets: Europarl v7, Common Crawl corpus, UN corpus, News Commentary, and 10French-English corpus.", "startOffset": 69, "endOffset": 75}, {"referenceID": 5, "context": "In detail, we used the same bilingual corpora from WMT\u201914 as used in [1, 6], which contains 12M sentence pairs extracting from five datasets: Europarl v7, Common Crawl corpus, UN corpus, News Commentary, and 10French-English corpus.", "startOffset": 69, "endOffset": 75}, {"referenceID": 0, "context": "We used the GRU networks and followed the practice in [1] to set experimental parameters.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "For the baseline NMT model, we exactly followed the settings reported in [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "For the baseline pseudo-NMT [12], we used the trained NMT model to generate pseudo bilingual sentence pairs from monolingual data, removed the sentences with more than 50 words, merged the generated data with the original parallel training data, and then trained the model for testing.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "Each of the baseline models was trained with AdaDelta [17] on K40m GPU until their performances stopped to improve on the validation set.", "startOffset": 54, "endOffset": 58}, {"referenceID": 7, "context": "We trained an RNN based language model [8] for each language using its corresponding monolingual corpus.", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "We used the BLEU score [9] as the evaluation metric, which are computed by the multi-bleu.", "startOffset": 23, "endOffset": 26}, {"referenceID": 13, "context": "Following the common practice, during testing we used beam search [14] with beam size of 12 for all the algorithms as in many previous works.", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "System BLEU RNNSearch[6] 29.", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "08 RNNSearch-LV with 500k source/target words[6] 32.", "startOffset": 45, "endOffset": 48}, {"referenceID": 12, "context": "11 MRT [13] 31.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "We use the method proposed by [6] to deal with unknown words.", "startOffset": 30, "endOffset": 33}], "year": 2016, "abstractText": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the languagemodel likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English\u2194French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.", "creator": "LaTeX with hyperref package"}}}