{"id": "1611.01576", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Quasi-Recurrent Neural Networks", "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.", "histories": [["v1", "Sat, 5 Nov 2016 00:31:25 GMT  (353kb,D)", "http://arxiv.org/abs/1611.01576v1", "Submitted to conference track at ICLR 2017"], ["v2", "Mon, 21 Nov 2016 20:52:34 GMT  (353kb,D)", "http://arxiv.org/abs/1611.01576v2", "Submitted to conference track at ICLR 2017"]], "COMMENTS": "Submitted to conference track at ICLR 2017", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.LG", "authors": ["james bradbury", "stephen merity", "caiming xiong", "richard socher"], "accepted": true, "id": "1611.01576"}, "pdf": {"name": "1611.01576.pdf", "metadata": {"source": "CRF", "title": "QUASI-RECURRENT NEURAL NETWORKS", "authors": ["James Bradbury", "Stephen Merity", "Caiming Xiong"], "emails": ["james.bradbury@salesforce.com", "smerity@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recurrent neural networks (RNNs), including gated variants such as the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) have become the standard model architecture for deep learning approaches to sequence modeling tasks. RNNs repeatedly apply a function with trainable parameters to a hidden state. Recurrent layers can also be stacked, increasing network depth, representational power and often accuracy. RNN applications in the natural language domain range from sentence classification (Wang et al., 2015) to word- and character-level language modeling (Zaremba et al., 2014). RNNs are also commonly the basic building block for more complex models for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015; Bradbury & Socher, 2016) or question answering (Kumar et al., 2016; Xiong et al., 2016). Unfortunately standard RNNs, including LSTMs, are limited in their capability to handle tasks involving very long sequences, such as document classification or character-level machine translation, as the computation of features or states for different parts of the document cannot occur in parallel.\nConvolutional neural networks (CNNs) (Krizhevsky et al., 2012), though more popular on tasks involving image data, have also been applied to sequence encoding tasks (Zhang et al., 2015). Such models apply time-invariant filter functions in parallel to windows along the input sequence. CNNs possess several advantages over recurrent models, including increased parallelism and better scaling to long sequences such as those often seen with character-level language data. Convolutional models for sequence processing have been more successful when combined with RNN layers in a hybrid architecture (Lee et al., 2016), because traditional max- and average-pooling approaches to combining convolutional features across timesteps assume time invariance and hence cannot make full use of large-scale sequence order information.\nWe present quasi-recurrent neural networks for neural sequence modeling. QRNNs address both drawbacks of standard models: like CNNs, QRNNs allow for parallel computation across both timestep and minibatch dimensions, enabling high throughput and good scaling to long sequences. Like RNNs, QRNNs allow the output to depend on the overall order of elements in the sequence. We describe QRNN variants tailored to several natural language tasks, including document-level sentiment classification, language modeling, and character-level machine translation. These models outperform strong LSTM baselines on all three tasks while dramatically reducing computation time.\n\u2217Equal contribution\nar X\niv :1\n61 1.\n01 57\n6v 1\n[ cs\n.N E\n] 5\nN ov\n2 01\n6"}, {"heading": "2 MODEL", "text": "Each layer of a quasi-recurrent neural network consists of two kinds of subcomponents, analogous to convolution and pooling layers in CNNs. The convolutional component, like convolutional layers in CNNs, allows fully parallel computation across both minibatches and spatial dimensions, in this case the sequence dimension. The pooling component, like pooling layers in CNNs, lacks trainable parameters and allows fully parallel computation across minibatch and feature dimensions.\nGiven an input sequence X \u2208 RT\u00d7n of T n-dimensional vectors x1 . . .xT , the convolutional subcomponent of a QRNN performs convolutions in the timestep dimension with a bank of m filters, producing a sequence Z \u2208 RT\u00d7m of m-dimensional candidate vectors zt. In order to be useful for tasks that include prediction of the next token, the filters must not allow the computation for any given timestep to access information from future timesteps. That is, with filters of width k, each zt depends only on xt\u2212k+1 through xt. This concept, known as a masked convolution (van den Oord et al., 2016), is implemented by padding the input to the left by the convolution\u2019s filter size minus one.\nWe apply additional convolutions with separate filter banks to obtain sequences of vectors for the elementwise gates that are needed for the pooling function. While the candidate vectors are passed through a tanh nonlinearity, the gates use an elementwise sigmoid. If the pooling function requires a forget gate ft and an output gate ot at each timestep, the full set of computations in the convolutional component is then:\nZ = tanh(Wz \u2217X) F = \u03c3(Wf \u2217X) O = \u03c3(Wo \u2217X),\n(1)\nwhere Wz ,Wf , and Wo, each in Rk\u00d7n\u00d7m, are the convolutional filter banks and \u2217 denotes a masked convolution along the timestep dimension. Note that if the filter width is 2, these equations reduce to the LSTM-like\nzt = tanh(W 1 zxt\u22121 +W 2 zxt)\nft = \u03c3(W 1 fxt\u22121 +W 2 fxt)\not = \u03c3(W 1 oxt\u22121 +W 2 oxt).\n(2)\nConvolution filters of larger width effectively compute higher n-gram features at each timestep; thus larger widths are especially important for character-level tasks.\nSuitable functions for the pooling subcomponent can be constructed from the familiar elementwise gates of the traditional LSTM cell. We seek a function controlled by gates that can mix states across timesteps, but which acts independently on each channel of the state vector. The simplest option, which Balduzzi & Ghifary (2016) term \u201cdynamic average pooling\u201d, uses only a forget gate:\nht = ft ht\u22121 + (1\u2212 ft) zt, (3)\nwhere denotes elementwise multiplication. The function may also include an output gate:\nct = ft ct\u22121 + (1\u2212 ft) zt ht = ot ct.\n(4)\nOr the recurrence relation may include an independent input and forget gate:\nct = ft ct\u22121 + it zt ht = ot ct.\n(5)\nWe term these three options f -pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize h or c to zero. Although the recurrent parts of these functions must be calculated for each timestep in sequence, their simplicity and parallelism along feature dimensions means that, in practice, evaluating them over even long sequences requires a negligible amount of computation time.\nA single QRNN layer thus performs an input-dependent pooling, followed by a gated linear combination of convolutional features. As with convolutional neural networks, two or more QRNN layers should be stacked to create a model with the capacity to approximate more complex functions."}, {"heading": "2.1 VARIANTS", "text": "Motivated by several common natural language tasks, and the long history of work on related architectures, we introduce several extensions to the stacked QRNN described above. Notably, many extensions to both recurrent and convolutional models can be applied directly to the QRNN as it combines elements of both model types.\nRegularization An important extension to the stacked QRNN is a robust regularization scheme inspired by recent work in regularizing LSTMs.\nThe need for an effective regularization method for LSTMs, and dropout\u2019s relative lack of efficacy when applied to recurrent connections, led to the development of recurrent dropout schemes, including variational inference\u2013based dropout (Gal & Ghahramani, 2016) and zoneout (Krueger et al., 2016). These schemes extend dropout to the recurrent setting by taking advantage of the repeating structure of recurrent networks, providing more powerful and less destructive regularization.\nVariational inference\u2013based dropout locks the dropout mask used for the recurrent connections across timesteps, so a single RNN pass uses a single stochastic subset of the recurrent weights. Zoneout stochastically chooses a new subset of channels to \u201czone out\u201d at each timestep; for these channels the network copies states from one timestep to the next without modification.\nAs QRNNs lack recurrent weights, the variational inference approach does not apply. Thus we extended zoneout to the QRNN architecture by modifying the pooling function to keep the previous pooling state for a stochastic subset of channels. Conveniently, this is equivalent to stochastically setting a subset of the QRNN\u2019s f gate channels to 1, or applying dropout on 1\u2212 f :\nF = 1\u2212 \u03c3(dropout(1\u2212Wf \u2217X)) (6) Thus the pooling function itself need not be modified at all. We note that when using an off-theshelf dropout layer in this context, it is important to remove automatic rescaling functionality from the implementation if it is present. In many experiments, we also apply ordinary dropout between layers, including between word embeddings and the first QRNN layer.\nDensely-Connected Layers We can also extend the QRNN architecture using techniques introduced for convolutional networks. For sequence classification tasks, we found it helpful to use skip-connections between every QRNN layer, a technique termed \u201cdense convolution\u201d by Huang et al. (2016). Where traditional feed-forward or convolutional networks have connections only between subsequent layers, a \u201cDenseNet\u201d with L layers has feed-forward or convolutional connections between every pair of layers, for a total ofL(L\u22121). This can improve gradient flow and convergence properties, especially in deeper networks, although it requires a parameter count that is quadratic in the number of layers.\nWhen applying this technique to the QRNN, we include connections between the input embeddings and every QRNN layer and between every pair of QRNN layers. This is equivalent to concatenating\neach QRNN layer\u2019s input to its output along the channel dimension before feeding the state into the next layer. The output of the last layer alone is then used as the overall encoding result.\nEncoder\u2013Decoder Models To demonstrate the generality of QRNNs, we extend the model architecture to sequence-to-sequence tasks, such as machine translation, by using a QRNN as encoder and a modified QRNN, enhanced with attention, as decoder. The motivation for modifying the decoder is that simply feeding the last encoder hidden state (the output of the encoder\u2019s pooling layer) into the decoder\u2019s recurrent pooling layer, analogously to conventional recurrent encoder\u2013decoder architectures, would not allow the encoder state to affect the gate or update values that are provided to the decoder\u2019s pooling layer. This would substantially limit the representational power of the decoder.\nInstead, the output of each decoder QRNN layer\u2019s convolution functions is supplemented at every timestep with the final encoder hidden state. This is accomplished by adding the result of the convolution for layer ` (e.g., W`z \u2217X`, in RT\u00d7m) with broadcasting to a linearly projected copy of layer `\u2019s last encoder state (e.g., V`zh\u0303 ` T , in Rm):\nZ` = tanh(W`z \u2217X` +V`zh\u0303`T ) F` = \u03c3(W`f \u2217X` +V`f h\u0303`T )\nO` = \u03c3(W`o \u2217X` +V`oh\u0303`T ),\n(7)\nwhere the tilde denotes that h\u0303 is an encoder variable. Encoder\u2013decoder models which operate on long sequences are made significantly more powerful with the addition of soft attention (Bahdanau et al., 2015), which removes the need for the entire input representation to fit into a fixed-length encoding vector. In our experiments, we computed an attentional sum of the encoder\u2019s last layer\u2019s hidden states. We used the dot products of these encoder hidden states with the decoder\u2019s last layer\u2019s un-gated hidden states, applying a softmax along the encoder timesteps, to weight the encoder states into an attentional sum kt for each decoder timestep. This context, and the decoder state, are then fed into a linear layer followed by the output gate:\n\u03b1st = softmax all s\n(cLt \u00b7 h\u0303Ls ) kt = \u2211 s \u03b1sth\u0303 L s\nhLt = ot (Wkkt +WccLt ),\n(8)\nwhere L is the last layer.\nWhile the first step of this attention procedure is quadratic in the sequence length, in practice it takes significantly less computation time than the model\u2019s linear and convolutional layers due to the simple and highly parallel dot-product scoring function."}, {"heading": "Model Time / Epoch (s) Test Acc (%)", "text": ""}, {"heading": "3 EXPERIMENTS", "text": "We evaluate the performance of the QRNN on three different natural language tasks: document-level sentiment classification, language modeling, and character-based neural machine translation. Our QRNN models outperform LSTM-based models of equal hidden size on all three tasks while dramatically improving computation speed. Experiments were implemented in Chainer (Tokui et al.)."}, {"heading": "3.1 SENTIMENT CLASSIFICATION", "text": "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset (Maas et al., 2011). The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words (Wang & Manning, 2012). We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., Miyato et al. (2016)).\nOur best performance on a held-out development set was achieved using a four-layer denselyconnected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings (Pennington et al., 2014). Dropout of 0.3 was applied between layers, and we used L2 regularization of 4 \u00d7 10\u22126. Optimization was performed on minibatches of 24 examples using RMSprop (Tieleman & Hinton, 2012) with learning rate of 0.001, \u03b1 = 0.9, and = 10\u22128.\nSmall batch sizes and long sequence lengths provide an ideal situation for demonstrating the QRNN\u2019s performance advantages over traditional recurrent architectures. We observed a speedup of 3.2x on IMDb train time per epoch compared to the optimized LSTM implementation provided in NVIDIA\u2019s cuDNN library. For specific batch sizes and sequence lengths, a 16x speed gain is possible. Figure 4 provides extensive speed comparisons.\nIn Figure 3, we visualize the hidden state vectors cLt of the final QRNN layer on part of an example from the IMDb dataset. Even without any post-processing, changes in the hidden state are visible and interpretable in regards to the input. This is a consequence of the elementwise nature of the recurrent pooling function, which delays direct interaction between different channels of the hidden state until the computation of the next QRNN layer."}, {"heading": "3.2 LANGUAGE MODELING", "text": "We replicate the language modeling experiment of Zaremba et al. (2014) and Gal & Ghahramani (2016) to benchmark the QRNN architecture for natural language sequence prediction. The experiment uses a standard preprocessed version of the Penn Treebank (PTB) by Mikolov et al. (2010).\nWe implemented a gated QRNN model with medium hidden size: 2 layers with 640 units in each layer. Both QRNN layers use a convolutional filter width k of two timesteps. While the \u201cmedium\u201d models used in other work (Zaremba et al., 2014; Gal & Ghahramani, 2016) consist of 650 units in\neach layer, it was more computationally convenient to use a multiple of 32. As the Penn Treebank is a relatively small dataset, preventing overfitting is of considerable importance and a major focus of recent research. It is not obvious in advance which of the many RNN regularization schemes would perform well when applied to the QRNN. Our tests showed encouraging results from zoneout applied to the QRNN\u2019s recurrent pooling layer, implemented as described in Section 2.1.\nThe experimental settings largely followed the \u201cmedium\u201d setup of Zaremba et al. (2014). Optimization was performed by stochastic gradient descent (SGD) without momentum. The learning rate was set at 1 for six epochs, then decayed by 0.95 for each subsequent epoch, for a total of 72 epochs. We additionally used L2 regularization of 2 \u00d7 10\u22124 and rescaled gradients with norm above 10. Zoneout was applied by performing dropout with ratio 0.1 on the forget gates of the QRNN, without rescaling the output of the dropout function. Batches consist of 20 examples, each 105 timesteps.\nComparing our results on the gated QRNN with zoneout to the results of LSTMs with both ordinary and variational dropout in Table 2, we see that the QRNN is highly competitive. The QRNN without zoneout strongly outperforms both our medium LSTM and the medium LSTM of Zaremba et al. (2014) which do not use recurrent dropout and is even competitive with variational LSTMs. This may be due to the limited computational capacity that the QRNN\u2019s pooling layer has relative to the LSTM\u2019s recurrent weights, providing structural regularization over the recurrence.\nWithout zoneout, early stopping based upon validation loss was required as the QRNN would begin overfitting. By applying a small amount of zoneout (p = 0.1), no early stopping is required and the QRNN achieves competitive levels of perplexity to the variational LSTM of Gal & Ghahramani"}, {"heading": "Model Parameters Validation Test", "text": "(2016), which had variational inference based dropout of 0.2 applied recurrently. The best performing variation also used Monte Carlo (MC) dropout averaging at test time of 1000 different masks, making it computationally expensive to run.\nWhen training on the PTB dataset with an NVIDIA K40 GPU, we found that the QRNN is substantially faster than a standard LSTM, even when comparing against the optimized cuDNN LSTM. In Figure 4 we provide a breakdown of the time taken for Chainer\u2019s default LSTM, the cuDNN LSTM, and QRNN to perform a full forward and backward pass on a single batch during training of the RNN LM on PTB. For both LSTM implementations, running time was dominated by the RNN computations, even with the highly optimized cuDNN implementation. For the QRNN implementation, however, the \u201cRNN\u201d layers are no longer the bottleneck. Indeed, there are diminishing returns from further optimization of the QRNN itself as the softmax and optimization overhead take equal or greater time. Note that the softmax, over a vocabulary size of only 10,000 words, is relatively small; for tasks with larger vocabularies, the softmax would likely dominate computation time.\nIt is also important to note that the cuDNN library\u2019s RNN primitives do not natively support any form of recurrent dropout. That is, running an LSTM that uses a state-of-the-art regularization scheme at cuDNN-like speeds would likely require an entirely custom kernel."}, {"heading": "3.3 CHARACTER-LEVEL NEURAL MACHINE TRANSLATION", "text": "We evaluate the sequence-to-sequence QRNN architecture described in 2.1 on a challenging neural machine translation task, IWSLT German\u2013English spoken-domain translation, applying fully character-level segmentation. This dataset consists of 209,772 sentence pairs of parallel training data from transcribed TED and TEDx presentations, with a mean sentence length of 103 characters for German and 93 for English. We remove training sentences with more than 300 characters in English or German, and use a unified vocabulary of 187 Unicode code points.\nOur best performance on a development set (TED.tst2013) was achieved using a four-layer encoder\u2013 decoder QRNN with 320 units per layer, no dropout or L2 regularization, and gradient rescaling to a maximum magnitude of 5. Inputs were supplied to the encoder reversed. The first encoder layer used convolutional filter width k = 6, while the other encoder layers used k = 2. Optimization was performed for 10 epochs on minibatches of 16 examples using Adam (Kingma & Ba, 2014) with \u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999, and = 10\u22128. Decoding was performed using beam search with beam width 8 and length normalization \u03b1 = 0.6. The modified log-probability ranking criterion is provided in the appendix.\nResults using this architecture were compared to an equal-sized four-layer encoder\u2013decoder LSTM with attention, applying dropout of 0.2. We again optimized using Adam; other hyperparameters were equal to their values for the QRNN and the same beam search procedure was applied. Table 3 shows that the QRNN outperformed the character-level LSTM, almost matching the performance of a word-level attentional baseline."}, {"heading": "Model Train Time BLEU (TED.tst2014)", "text": ""}, {"heading": "4 RELATED WORK", "text": "Exploring alternatives to traditional RNNs for sequence tasks is a major area of current research. Quasi-recurrent neural networks are related to several such recently described models, especially the strongly-typed recurrent neural networks (T-RNN) introduced by Balduzzi & Ghifary (2016). While the motivation and constraints described in that work are different, Balduzzi & Ghifary (2016)\u2019s concepts of \u201clearnware\u201d and \u201cfirmware\u201d parallel our discussion of convolution-like and pooling-like subcomponents. As the use of a fully connected layer for recurrent connections violates the constraint of \u201cstrong typing\u201d, all strongly-typed RNN architectures (including the T-RNN, T-GRU, and T-LSTM) are also quasi-recurrent. However, some QRNN models (including those with attention or skip-connections) are not \u201cstrongly typed\u201d. In particular, a T-RNN differs from a QRNN as described in this paper with filter size 1 and f -pooling only in the absence of an activation function on z. Similarly, T-GRUs and T-LSTMs differ from QRNNs with filter size 2 and fo- or ifo-pooling respectively in that they lack tanh on z and use tanh rather than sigmoid on o.\nThe QRNN is also related to work in hybrid convolutional\u2013recurrent models. Zhou et al. (2015) apply CNNs at the word level to generate n-gram features used by an LSTM for text classification. Xiao & Cho (2016) also tackle text classification by applying convolutions at the character level, with a stride to reduce sequence length, then feeding these features into a bidirectional LSTM. A similar approach was taken by Lee et al. (2016) for character-level machine translation. Their model\u2019s encoder uses a convolutional layer followed by max-pooling to reduce sequence length, a four-layer highway network, and a bidirectional GRU. The parallelism of the convolutional, pooling, and highway layers allows training speed comparable to subword-level models without hard-coded text segmentation.\nThe QRNN encoder\u2013decoder model shares the favorable parallelism and path-length properties exhibited by the ByteNet (Kalchbrenner et al., 2016), an architecture for character-level machine translation based on residual convolutions over binary trees. Their model was constructed to achieve three desired properties: parallelism, linear-time computational complexity, and short paths between any pair of words in order to better propagate gradient signals."}, {"heading": "5 CONCLUSION", "text": "Intuitively, many aspects of the semantics of long sequences are context-invariant and can be computed in parallel (e.g., convolutionally), but some aspects require long-distance context and must be computed recurrently. Many existing neural network architectures either fail to take advantage of the contextual information or fail to take advantage of the parallelism. QRNNs exploit both parallelism and context, exhibiting advantages from both convolutional and recurrent neural networks. QRNNs have better predictive accuracy than LSTM-based models of equal hidden size, even though they use fewer parameters and run substantially faster. Our experiments show that the speed and accuracy advantages remain consistent across tasks and at both word and character levels.\nExtensions to both CNNs and RNNs are often directly applicable to the QRNN, while the model\u2019s hidden states are more interpretable than those of other recurrent architectures as its channels maintain their independence across timesteps. We believe that QRNNs can serve as a building block for long-sequence tasks that were previously impractical with traditional RNNs."}, {"heading": "APPENDIX", "text": ""}, {"heading": "BEAM SEARCH RANKING CRITERION", "text": "The modified log-probability ranking criterion we used in beam search for translation experiments is:\nlog(Pcand) = T + \u03b1\nT . . .\nTtrg + \u03b1\nTtrg T\u2211 i=1 log(p(wi|w1 . . . wi\u22121)), (9)\nwhere \u03b1 is a length normalization parameter (Wu et al., 2016), wi is the ith output character, and Ttrg is a \u201ctarget length\u201d equal to the source sentence length plus five characters. This reduces at \u03b1 = 0 to ordinary beam search with probabilities:\nlog(Pcand) = T\u2211 i=1 log(p(wi|w1 . . . wi\u22121)), (10)\nand at \u03b1 = 1 to beam search with probabilities normalized by length (up to the target length):\nlog(Pcand) \u223c 1\nT T\u2211 i=1 log(p(wi|w1 . . . wi\u22121)). (11)\nConveniently, this ranking criterion can be computed at intermediate beam-search timesteps, obviating the need to apply a separate reranking on complete hypotheses."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Strongly-typed recurrent neural networks", "author": ["David Balduzzi", "Muhammad Ghifary"], "venue": "In ICML,", "citeRegEx": "Balduzzi and Ghifary.,? \\Q2016\\E", "shortCiteRegEx": "Balduzzi and Ghifary.", "year": 2016}, {"title": "In Proceedings of the First Conference on Machine Translation, Berlin, Germany", "author": ["James Bradbury", "Richard Socher. MetaMind neural machine translation system for WMT"], "venue": "Association for Computational Linguistics, 2016.", "citeRegEx": "Bradbury and WMT,? 2016", "shortCiteRegEx": "Bradbury and WMT", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Gal and Ghahramani.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang"], "venue": "arXiv preprint arXiv:1412.1058,", "citeRegEx": "Johnson and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2014}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1610.10099,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.01305,", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann"], "venue": "arXiv preprint arXiv:1610.03017,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "A way out of the odyssey: Analyzing and combining recent insights for LSTMs", "author": ["Shayne Longpre", "Sabeek Pradhan", "Caiming Xiong", "Richard Socher"], "venue": null, "citeRegEx": "Longpre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Longpre et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.T. Luong", "H. Pham", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Multi-dimensional sentiment analysis with learned representations", "author": ["Andrew L Maas", "Andrew Y Ng", "Christopher Potts"], "venue": "Technical report,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Gr\u00e9goire Mesnil", "Tomas Mikolov", "Marc\u2019Aurelio Ranzato", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.5335,", "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Virtual adversarial training for semi-supervised text classification", "author": ["Takeru Miyato", "Andrew M Dai", "Ian Goodfellow"], "venue": "arXiv preprint arXiv:1605.07725,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "In ICLR,", "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning"], "venue": "In ACL,", "citeRegEx": "Wang and Manning.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Predicting polarities of tweets by composing word embeddings with long short-term memory", "author": ["Xin Wang", "Yuanchao Liu", "Chengjie Sun", "Baoxun Wang", "Xiaolong Wang"], "venue": "In ACL,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1606.02960,", "citeRegEx": "Wiseman and Rush.,? \\Q2016\\E", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers", "author": ["Yijun Xiao", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1602.00367,", "citeRegEx": "Xiao and Cho.,? \\Q2016\\E", "shortCiteRegEx": "Xiao and Cho.", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In NIPS,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A C-LSTM neural network for text classification", "author": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis Lau"], "venue": "arXiv preprint arXiv:1511.08630,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "RNN applications in the natural language domain range from sentence classification (Wang et al., 2015) to word- and character-level language modeling (Zaremba et al.", "startOffset": 83, "endOffset": 102}, {"referenceID": 31, "context": ", 2015) to word- and character-level language modeling (Zaremba et al., 2014).", "startOffset": 55, "endOffset": 77}, {"referenceID": 0, "context": "RNNs are also commonly the basic building block for more complex models for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015; Bradbury & Socher, 2016) or question answering (Kumar et al.", "startOffset": 110, "endOffset": 178}, {"referenceID": 15, "context": "RNNs are also commonly the basic building block for more complex models for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015; Bradbury & Socher, 2016) or question answering (Kumar et al.", "startOffset": 110, "endOffset": 178}, {"referenceID": 12, "context": ", 2015; Bradbury & Socher, 2016) or question answering (Kumar et al., 2016; Xiong et al., 2016).", "startOffset": 55, "endOffset": 95}, {"referenceID": 30, "context": ", 2015; Bradbury & Socher, 2016) or question answering (Kumar et al., 2016; Xiong et al., 2016).", "startOffset": 55, "endOffset": 95}, {"referenceID": 10, "context": "Convolutional neural networks (CNNs) (Krizhevsky et al., 2012), though more popular on tasks involving image data, have also been applied to sequence encoding tasks (Zhang et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 32, "context": ", 2012), though more popular on tasks involving image data, have also been applied to sequence encoding tasks (Zhang et al., 2015).", "startOffset": 110, "endOffset": 130}, {"referenceID": 13, "context": "Convolutional models for sequence processing have been more successful when combined with RNN layers in a hybrid architecture (Lee et al., 2016), because traditional max- and average-pooling approaches to combining convolutional features across timesteps assume time invariance and hence cannot make full use of large-scale sequence order information.", "startOffset": 126, "endOffset": 144}, {"referenceID": 11, "context": "The need for an effective regularization method for LSTMs, and dropout\u2019s relative lack of efficacy when applied to recurrent connections, led to the development of recurrent dropout schemes, including variational inference\u2013based dropout (Gal & Ghahramani, 2016) and zoneout (Krueger et al., 2016).", "startOffset": 274, "endOffset": 296}, {"referenceID": 5, "context": "For sequence classification tasks, we found it helpful to use skip-connections between every QRNN layer, a technique termed \u201cdense convolution\u201d by Huang et al. (2016). Where traditional feed-forward or convolutional networks have connections only between subsequent layers, a \u201cDenseNet\u201d with L layers has feed-forward or convolutional connections between every pair of layers, for a total ofL(L\u22121).", "startOffset": 147, "endOffset": 167}, {"referenceID": 0, "context": "Encoder\u2013decoder models which operate on long sequences are made significantly more powerful with the addition of soft attention (Bahdanau et al., 2015), which removes the need for the entire input representation to fit into a fixed-length encoding vector.", "startOffset": 128, "endOffset": 151}, {"referenceID": 18, "context": "3 Ensemble of RNNs and NB-SVM (Mesnil et al., 2014) \u2212 92.", "startOffset": 30, "endOffset": 51}, {"referenceID": 14, "context": "6 2-layer LSTM (Longpre et al., 2016) \u2212 87.", "startOffset": 15, "endOffset": 37}, {"referenceID": 14, "context": "6 Residual 2-layer bi-LSTM (Longpre et al., 2016) \u2212 90.", "startOffset": 27, "endOffset": 49}, {"referenceID": 16, "context": "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset (Maas et al., 2011).", "startOffset": 128, "endOffset": 147}, {"referenceID": 21, "context": "Our best performance on a held-out development set was achieved using a four-layer denselyconnected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings (Pennington et al., 2014).", "startOffset": 204, "endOffset": 229}, {"referenceID": 16, "context": "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset (Maas et al., 2011). The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words (Wang & Manning, 2012). We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., Miyato et al. (2016)).", "startOffset": 129, "endOffset": 479}, {"referenceID": 31, "context": "While the \u201cmedium\u201d models used in other work (Zaremba et al., 2014; Gal & Ghahramani, 2016) consist of 650 units in", "startOffset": 45, "endOffset": 91}, {"referenceID": 30, "context": "We replicate the language modeling experiment of Zaremba et al. (2014) and Gal & Ghahramani (2016) to benchmark the QRNN architecture for natural language sequence prediction.", "startOffset": 49, "endOffset": 71}, {"referenceID": 30, "context": "We replicate the language modeling experiment of Zaremba et al. (2014) and Gal & Ghahramani (2016) to benchmark the QRNN architecture for natural language sequence prediction.", "startOffset": 49, "endOffset": 99}, {"referenceID": 19, "context": "The experiment uses a standard preprocessed version of the Penn Treebank (PTB) by Mikolov et al. (2010). We implemented a gated QRNN model with medium hidden size: 2 layers with 640 units in each layer.", "startOffset": 82, "endOffset": 104}, {"referenceID": 31, "context": "The experimental settings largely followed the \u201cmedium\u201d setup of Zaremba et al. (2014). Optimization was performed by stochastic gradient descent (SGD) without momentum.", "startOffset": 65, "endOffset": 87}, {"referenceID": 31, "context": "The experimental settings largely followed the \u201cmedium\u201d setup of Zaremba et al. (2014). Optimization was performed by stochastic gradient descent (SGD) without momentum. The learning rate was set at 1 for six epochs, then decayed by 0.95 for each subsequent epoch, for a total of 72 epochs. We additionally used L regularization of 2 \u00d7 10\u22124 and rescaled gradients with norm above 10. Zoneout was applied by performing dropout with ratio 0.1 on the forget gates of the QRNN, without rescaling the output of the dropout function. Batches consist of 20 examples, each 105 timesteps. Comparing our results on the gated QRNN with zoneout to the results of LSTMs with both ordinary and variational dropout in Table 2, we see that the QRNN is highly competitive. The QRNN without zoneout strongly outperforms both our medium LSTM and the medium LSTM of Zaremba et al. (2014) which do not use recurrent dropout and is even competitive with variational LSTMs.", "startOffset": 65, "endOffset": 868}, {"referenceID": 31, "context": "Model Parameters Validation Test LSTM (medium) (Zaremba et al., 2014) 20M 86.", "startOffset": 47, "endOffset": 69}, {"referenceID": 8, "context": "7 LSTM with CharCNN embeddings (Kim et al., 2016) 19M \u2212 78.", "startOffset": 31, "endOffset": 49}, {"referenceID": 17, "context": "9 Zoneout + Variational LSTM (medium) (Merity et al., 2016) 20M 84.", "startOffset": 38, "endOffset": 59}, {"referenceID": 22, "context": "tst2014) Word-level LSTM w/attn (Ranzato et al., 2016) \u2212 20.", "startOffset": 32, "endOffset": 54}, {"referenceID": 7, "context": "The QRNN encoder\u2013decoder model shares the favorable parallelism and path-length properties exhibited by the ByteNet (Kalchbrenner et al., 2016), an architecture for character-level machine translation based on residual convolutions over binary trees.", "startOffset": 116, "endOffset": 143}, {"referenceID": 31, "context": "Zhou et al. (2015) apply CNNs at the word level to generate n-gram features used by an LSTM for text classification.", "startOffset": 0, "endOffset": 19}, {"referenceID": 31, "context": "Zhou et al. (2015) apply CNNs at the word level to generate n-gram features used by an LSTM for text classification. Xiao & Cho (2016) also tackle text classification by applying convolutions at the character level, with a stride to reduce sequence length, then feeding these features into a bidirectional LSTM.", "startOffset": 0, "endOffset": 135}, {"referenceID": 12, "context": "A similar approach was taken by Lee et al. (2016) for character-level machine translation.", "startOffset": 32, "endOffset": 50}], "year": 2016, "abstractText": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep\u2019s computation on the previous timestep\u2019s output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.", "creator": "LaTeX with hyperref package"}}}