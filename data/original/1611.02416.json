{"id": "1611.02416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "An Efficient Approach to Boosting Performance of Deep Spiking Network Training", "abstract": "Nowadays deep learning is dominating the field of machine learning with state-of-the-art performance in various application areas. Recently, spiking neural networks (SNNs) have been attracting a great deal of attention, notably owning to their power efficiency, which can potentially allow us to implement a low-power deep learning engine suitable for real-time/mobile applications. However, implementing SNN-based deep learning remains challenging, especially gradient-based training of SNNs by error backpropagation. We cannot simply propagate errors through SNNs in conventional way because of the property of SNNs that process discrete data in the form of a series. Consequently, most of the previous studies employ a workaround technique, which first trains a conventional weighted-sum deep neural network and then maps the learning weights to the SNN under training, instead of training SNN parameters directly. In order to eliminate this workaround, recently proposed is a new class of SNN named deep spiking networks (DSNs), which can be trained directly (without a mapping from conventional deep networks) by error backpropagation with stochastic gradient descent. In this paper, we show that the initialization of the membrane potential on the backward path is an important step in DSN training, through diverse experiments performed under various conditions. Furthermore, we propose a simple and efficient method that can improve DSN training by controlling the initial membrane potential on the backward path. In our experiments, adopting the proposed approach allowed us to boost the performance of DSN training in terms of converging time and accuracy.", "histories": [["v1", "Tue, 8 Nov 2016 07:41:54 GMT  (1341kb,D)", "http://arxiv.org/abs/1611.02416v1", "9 pages, 5 figures"]], "COMMENTS": "9 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["seongsik park", "sang-gil lee", "hyunha nam", "sungroh yoon"], "accepted": false, "id": "1611.02416"}, "pdf": {"name": "1611.02416.pdf", "metadata": {"source": "CRF", "title": "An Efficient Approach to Boosting Performance of Deep Spiking Network Training", "authors": ["Seongsik Park", "Sang-gil Lee", "Hyunha Nam", "Sungroh Yoon"], "emails": ["sryoon@snu.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].\nWhile the conventional deep neural networks (DNNs) excel at the problems on many different areas, one of their major drawbacks is that they are computationally expensive: A single neuron in the layer requires all values of the neurons from the previous layer to calculate a weighted sum, which needs a huge amount of multiplicative operations. Thus, the current deep learning frameworks rely heavily on GPU-based parallel computation, making it infeasible, without some compromises, to apply them to mobile devices or (even power-efficient) chips.\nIn addition, the method of backpropagation, the most popular method for training DNNs, requires a differentiable activation function in each neuron, flowing information between neurons using real\n\u2217To whom correspondence should be addressed.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n02 41\n6v 1\n[ cs\n.L G\n] 8\nN ov\nnumbers. However, from a neurological viewpoint, this backpropagation technique is not an efficient mechanism. Information is encoded with \u201call-or-none\u201d action potentials, such as the \u201cspike train,\u201d and only a small subset of neurons propagate information by firing spikes.\nIn this regard, spiking neural networks (SNNs) can provide a much more power-efficient way of implementing artificial neurons than conventional DNNs [20, 19] leveraged by their compact and sparse activity profiles. Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.\nMany of existing approaches train and convert a conventional DNN to its SNN counterpart by replacing the activation functions (mostly ReLU) in the trained DNN with spiking neurons, producing the SNN. For instance, O\u2019Connor et al. [23] mapped a trained deep belief network to an SNN by using the Siegert mean-firing-rate approximation model for integrate-and-fire spiking neurons. Diehl et al. [5] trained a DNN with the stochastic gradient descent (SGD) and then converted it to an SNN based on parameter optimization. Zambrano and Bohte [26] used the asynchronous pulsed sigma-delta coding for spike trains with adaptive dynamic range in the membrane potential.\nWhile these pioneering methods closed the gap of performance between conventional DNNs and SNNs, they could not train SNNs with the spikes by backpropagation with SGD. Although Lee et al. [18] reported that they could train SNNs with a differentiable activation function, their work incurred additional computation overhead for handling activation and gradients, thus discouraging the deployment of their method for neuromorphic hardware with limited resources.\nTo address the limitation of these techniques, O\u2019Connor and Welling [22] recently proposed deep spiking networks (DSNs), one of the first multi-layer SNNs that can be trained with spikes by backpropagation with SGD. They reported that their DSN-based deep neural network delivered performance comparable to conventional weighted-sum DNNs with similar capacity. In addition, they proposed the method of fractional stochastic gradient descent (F-SGD), which is a training algorithm to improve SNN performance. Furthermore, most parts of their method require only additive operations rather than multiplicative ones, making their network power-efficient and hardwarefriendly for implementation. The authors also made an important discovery that adjusting the initial value of the membrane potential on the backward path of an SNN could significantly affect the quality of training.\nInspired by this intriguing discovery, in this paper, we thoroughly analyze the impact of initializing the membrane potential on the overall performance of training an SNN. Based on this analysis, we propose a new, efficient approach that can boost the performance of training SNNs. Our approach is grounded on the method of precharging membrane potentials in the previous work, effectively advancing it by reducing the side effects of precharged potentials as training progresses. Furthermore, we share our neurological insight gained by the observations of network behavior: We can interpret the performance boosts as originating from the long-term potentiation of synapses, while we can represent the negative impact of the initial membrane potentials as a trace of background activities interfering with memory maintenance in synapses."}, {"heading": "2 Methods and Experiments", "text": "As baseline training algorithm, we present the original version of the training methodology for DSNs [22] in Algorithm 1.\nAs outlined in this algorithm, the training procedure of DSNs is different from that of conventional DNNs. For each training example, we extract a set of spike trains (series of spikes) t times and use each spike train to train a DSN. In this paper, we call the quantity t the time period for training (in other papers, the term \u201ctime steps\u201d is also frequently used). The spikes in a spike train are propagated forward and backward through the network, while the gradient information for weight updates is accumulated. After each time period is over, the parameters in the DSN are updated using the accumulated gradient information.\nThe properties of information encoding and propagation in SNNs make the time period for training data have a critical role. The neurons in an SNN would receive more stimuli as the time period increases since the input spike train is proportional to it. The neurons can thus have more chances to fire a spike, which would eventually improve the performance of the SNN.\nAlgorithm 1: Training in Deep Spiking Networks [22] E : # of epochs, D : # of training data, T : Time period phibwd : Membrane potentials on the backward path\nfor e in E do for d in D do\nphibwd \u2190 INITIALIZATIONFUNCTION; del\u2190 0; for t in T do\nst \u2190 SAMPLINGINPUTDATA(d); errt \u2190 FORWARDPATHOPERATION(st); delt \u2190 BACKWARDPATHOPERATION(errt); del\u2190 del + delt;\nend UPDATEPARAMETERS(del);\nend end"}, {"heading": "2.1 Effects of Time Periods and Initial Potential on SNN Training", "text": "To verify the previous work and validate the effectiveness of the proposed approach, we performed a diverse set of experiments using the code under the same default hyper-parameters and conditions from the original work (available at https://github.com/petered/spiking-mlp).\nFigure 1a shows the training results (error rates versus epochs) obtained from training an SNN implemented as a DSN using SGD with the MNIST data [15] for three different time periods. Evidently, we can observe that the time period used for training significantly affects training error rates. As expected, the longer a time period used for training the SNN, the more accurate result we could get. In the extreme case with an (vastly insufficient) time period of 5, the SNN could not be trained at all because of the lack of spikes, as shown in Figure 1b. Especially, in that case, the number of backward spiking instances was insufficient to stimulate a neuron on the backward path to fire a spike. As a result, there was no change in terms of the amount of spiking regardless of the number of epochs executed. Obviously, we should use a time period that is long enough to avoid this unsatisfactory result. On the other hand, as the time period is strongly related to the total training time, it is important to make the time period as short as possible for assuring efficient training.\nThe main problem encountered when training an SNN with insufficient time periods is that the neurons in the SNN cannot properly propagate error signal backwards. Each neuron in an SNN integrates the spikes from other neurons and fires a spike only when its membrane potential is greater than a certain threshold. Thus, if the neuron could not obtain a sufficient number of spikes during the\ntime period of a sampled input spike train, it will not fire. Even worse, as an SNN is getting deeper, this phenomenon of disappearing spikes between neurons would be more serious just as the vanishing gradient problem in gradient descent.\nTo resolve this problem and to improve training with relatively short time periods, the method of backwards quantization, a technique of precharging membrane potentials on the backward path, was introduced in the original DSN paper [22]. The main idea is that we should precharge each membrane potential on the backward path at the beginning of each training, in order to make it easily exceed the threshold with fewer spikes. By applying this method, the spikes for error backpropagation will be fired more easily. In other words, errors will be be propagated more effectively through the backward path. Consequently, we can potentially boost the effectiveness of training in terms of convergence time and accuracy through this precharge method.\nO\u2019Connor and Welling [22] empirically suggested three types of initialization methods: \u201cno reset,\u201d \u201cuniform random reset,\u201d and \u201czero reset.\u201d In the \u201cno reset\u201d approach, the membrane potential from the previous training sample is maintained at the beginning of training using the current sample without resetting the potential. The \u201cuniform random reset\u201d and \u201czero reset\u201d approaches refer to resetting the membrane potentials at the onset of each training with uniformly sampled random values and all zeros, respectively. Although the \u201czero reset\u201d approach is the one that is biologically most plausible, this option makes the training of an SNN inefficient (as seen before, a DSN needs an adequate amount of time periods to deliver satisfactory performance). Consequently, the \u201cno reset\u201d or \u201cuniform random reset\u201d approaches have been commonly applied to DSN training for precharging membrane potentials on the backward path."}, {"heading": "2.2 Our Proposal: Step-Decay Precharge Method", "text": "In a typical SNN, the precharged membrane potential before the beginning of training would be considered as noise, which causes faulty learning results for the current training sample. The way of precharging the membrane potential therefore critically affects the training result.\nTo put our proposal in a proper context, we show the SNN training results (error rates and the number of spikes versus training epochs) obtained by using each of the three aforementioned precharging methods, as shown in Figure 2. In the figure, the curve labeled \u201cDNN w/ReLU\u201d represents the error rate of the conventional DNN with ReLU activations, and three types of dotted lines indicate the measurements obtained from using the three precharging approaches (note that two types of solid lines represent the results from using our proposed approach, which will be elaborated shortly).\nInitially in the training, we noticed a similar pattern of the effects of precharged membrane potential on training quality as in the DSN paper [22]. That is, the result using the \u201cno reset\u201d option gave the best result as shown in the figure. However, as the training continued, the precharged membrane potential started to become acting as noise, which hindered further training.\nAlgorithm 2: Training in Deep Spiking Networks with Proposed Step-Decay Method E : # of epochs, D : # of training data, T : Time period phibwd : Membrane potentials on the backward path, Cd : Linear decay constant\nfor e in E do for d in D do\nphibwd \u2190 INITIALIZATIONFUNCTION/(Cd \u00d7 e); // step-wise linear decay del\u2190 0; for t in T do\nst \u2190 SAMPLINGINPUTDATA(d); errt \u2190 FORWARDPATHOPERATION(st); delt \u2190 BACKWARDPATHOPERATION(errt); del\u2190 del + delt;\nend UPDATEPARAMETERS(del);\nend end\nTo address this issue, we propose a new step-wise decay method for precharging the membrane potentials. Our method can boost the effectiveness of SNN training and reduce the negative effect of precharging when an SNN completes its training. Algorithm 2 outlines our approach, in which the amount of each precharged membrane potential decreases as training epochs proceed, effectively reducing its noisy effect on the final result. As proof of concept, in this paper, we use a linear function for implementing a step-wise decay (with a linear decay constant Cd); other types of (linear and non-linear) functions would work as well.\nThe solid lines in Figure 2a represent the measurement results obtained by applying the proposed decay approach to SGD-based training of the same DSN shown previously (Cd = 1). In this experiment, we could get better training results by adopting the proposed approach. For both of the \u201cno reset\u201d and \u201cuniform random reset\u201d options, our approach enhanced the training results. Compared with the original DSN, we were also able to reduce the number of backward spikes, owing to the decreased membrane potentials.\nAs shown in Figure 3a, we further tested our approach with F-SGD, an SNN training algorithm proposed in the context DSNs, which is different from SGD in that it updates the parameters whenever a spike is generated. In contrast to the SGD case, the improvement delivered by applying our method to F-SGD was marginal. This is because the number of spikes on the backward path for F-SGD was significantly smaller than that for SGD, as seen in Figure 3b. Based on this observation, we deduced that F-SGD could be more sensitive to the level of precharged membrane potentials and that we could get better training results if we reduce the effect of the decay at each epoch.\nFinally, to find an (empirically optimal) linear decay constant that can reduce the precharged potential effectively, we tried to sweep the values of Cd from 0.1 to 0.9 (in the unit of 0.1), as shown in Figure 4a. As expected, the training accuracy varied by changing the linear decay constant, and using Cd = 0.3 gave the best result in this specific experiment. Figure 4b shows the error rates versus the number of training epochs obtained by applying our approach with this best value of Cd = 0.3 to F-SGD training of the same DSN used in the previous experiments. Table 1 lists additional experimental results training obtained by using various configurations."}, {"heading": "3 Discussion and Conclusion", "text": "In this paper, we have proposed a new approach to boosting the effectiveness of DSN training with SGD and backpropagation. Our proposal is based on the fact that the initialization of membrane\npotentials on the backward path of a DSN significantly affects the training accuracy as previously discovered. The main idea of the proposed method is that decreasing the effect of initialization potentials can improve training accuracy as training progresses. We validated the effectiveness of our method through a diverse sets of experiments under various conditions.\nAs long as we train an SNN with SGD and backpropagation (both are widely used in DNNs), the precharge method is expected to be an inevitable prerequisite for boosting the effectiveness of SNN training. In this context, we believe that our approach makes a meaningful contribution to the field, providing a simple and effective means of improving SNN training quality. This paper presents our experimental results only with one type of SNN (i.e., DSN) and one type of data (i.e., MNIST). We expect that our method will be able to find applications in other types of SNNs and datasets, which will be a part of our future work.\nIt is possible to interpret the precharge effect as a type of transfer learning, especially when we do not reset the membrane potential at the beginning of each training. In this case, the maintained membrane potential acts to compensate the lack of spiking caused by insufficient time periods. However, depending on the training data and/or the degree of network training, this could merely indicate noise, even though the maintained membrane potential led to satisfactory results in our work and in the original DSN paper.\nTo scrutinize this interpretation further, we hypothesized that the randomized membrane potential with a step-wise decay could be the best combination among many initialization options. We then tried to empirically verify our hypothesis by carrying out experiments with sufficient time periods, as shown in Figure 5a. We also found out that the firing rate of the forward path on the network increased further when we applied the step-wise decay method as depicted in Figure 5b.\nFrom a neuroscience point of view, these phenomena correspond to the long-term potentiation where a persistent strengthening of synapses occurs between neurons [4], resulting in higher spikes. In addition, if we interpret the initialized values as a trace of background activities of the network, they interfere with memory maintenance in synapses by leading a synaptic efficacy to faster decay [9], inhibiting the long-term potentiation, i.e., successful learning."}, {"heading": "Acknowledgments", "text": "This work was supported in by the BK21 Plus Project in 2016 (Electrical and Computer Engineering, Seoul National University), in part by a grant from Samsung Electronics, and in part by a grant from Naver."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv.org, page arXiv:1409.0473, Sept.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep autoencoder neural networks for gene ontology annotation predictions", "author": ["D. Chicco", "P.J. Sadowski", "P. Baldi"], "venue": "BCB, pages 533\u2013540,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Plasticity in the human central nervous system", "author": ["S. Cooke", "T. Bliss"], "venue": "Brain, 129(7):1659\u20131673,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast-classifying, highaccuracy spiking deep networks through weight and threshold balancing", "author": ["P.U. Diehl", "D. Neil", "J. Binas", "M. Cook", "S.-C. Liu", "M. Pfeiffer"], "venue": "2015 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Functional requirements for reward-modulated spike-timing-dependent plasticity", "author": ["N. Fr\u00e9maux", "H. Sprekeler", "W. Gerstner"], "venue": "The Journal of Neuroscience, 30(40):13326\u201313337,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Spiking neuron models: Single neurons, populations, plasticity", "author": ["W. Gerstner", "W.M. Kistler"], "venue": "Cambridge university press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "The tempotron: a neuron that learns spike timing\u2013based decisions", "author": ["R. G\u00fctig", "H. Sompolinsky"], "venue": "Nature neuroscience, 9(3):420\u2013428,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Memory maintenance in synapses with calcium-based plasticity in the presence of background activity", "author": ["D. Higgins", "M. Graupner", "N. Brunel"], "venue": "PLOS Comput Biol, 10(10):e1003834,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A quantitative description of membrane current and its application to conduction and excitation in nerve", "author": ["A.L. Hodgkin", "A.F. Huxley"], "venue": "The Journal of physiology, 117(4):500,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1952}, {"title": "Polychronization: computation with spikes", "author": ["E.M. Izhikevich"], "venue": "Neural computation, 18(2): 245\u2013282,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Modeling synaptic plasticity in conjunction with the timing of pre-and postsynaptic action potentials", "author": ["W.M. Kistler", "J.L. Van Hemmen"], "venue": "Neural Computation, 12(2):385\u2013405,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the Ieee, 86(11):2278\u20132324, Nov.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Deep learning", "author": ["Y. Lecun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444, May", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "deeptarget: End-to-end learning framework for microrna target prediction using deep recurrent neural networks", "author": ["B. Lee", "J. Baek", "S. Park", "S. Yoon"], "venue": "BCB, pages 434\u2013442,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Training Deep Spiking Neural Networks using Backpropagation", "author": ["J. Lee", "T. Delbruck", "M. Pfeiffer"], "venue": "arXiv.org, page arXiv:1608.08782, Aug.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Networks of spiking neurons - The third generation of neural network models", "author": ["W. Maass"], "venue": "Neural Networks, 10(9):1659\u20131671,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Deep learning in bioinformatics", "author": ["S. Min", "B. Lee", "S. Yoon"], "venue": "Briefings in Bioinformatics,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-time classification and sensor fusion with a spiking deep belief network", "author": ["P. O\u2019Connor", "D. Neil", "S.-C. Liu", "T. Delbruck", "M. Pfeiffer"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "deepmirgene: Deep neural network based precursor microrna prediction", "author": ["S. Park", "S. Min", "H. Choi", "S. Yoon"], "venue": "arXiv preprint arXiv:1605.00017,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and Efficient Asynchronous Neural Computation with Adapting Spiking Neural Networks", "author": ["D. Zambrano", "S.M. Bohte"], "venue": "arXiv.org, Sept.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 42, "endOffset": 50}, {"referenceID": 13, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 42, "endOffset": 50}, {"referenceID": 12, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 293, "endOffset": 297}, {"referenceID": 2, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 327, "endOffset": 337}, {"referenceID": 23, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 327, "endOffset": 337}, {"referenceID": 0, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 327, "endOffset": 337}, {"referenceID": 1, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 358, "endOffset": 373}, {"referenceID": 16, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 358, "endOffset": 373}, {"referenceID": 22, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 358, "endOffset": 373}, {"referenceID": 20, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 358, "endOffset": 373}, {"referenceID": 19, "context": "In this regard, spiking neural networks (SNNs) can provide a much more power-efficient way of implementing artificial neurons than conventional DNNs [20, 19] leveraged by their compact and sparse activity profiles.", "startOffset": 149, "endOffset": 157}, {"referenceID": 18, "context": "In this regard, spiking neural networks (SNNs) can provide a much more power-efficient way of implementing artificial neurons than conventional DNNs [20, 19] leveraged by their compact and sparse activity profiles.", "startOffset": 149, "endOffset": 157}, {"referenceID": 9, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 129, "endOffset": 136}, {"referenceID": 6, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 129, "endOffset": 136}, {"referenceID": 7, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 161, "endOffset": 168}, {"referenceID": 10, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 161, "endOffset": 168}, {"referenceID": 11, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 189, "endOffset": 196}, {"referenceID": 5, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 189, "endOffset": 196}, {"referenceID": 21, "context": "[23] mapped a trained deep belief network to an SNN by using the Siegert mean-firing-rate approximation model for integrate-and-fire spiking neurons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] trained a DNN with the stochastic gradient descent (SGD) and then converted it to an SNN based on parameter optimization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Zambrano and Bohte [26] used the asynchronous pulsed sigma-delta coding for spike trains with adaptive dynamic range in the membrane potential.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "[18] reported that they could train SNNs with a differentiable activation function, their work incurred additional computation overhead for handling activation and gradients, thus discouraging the deployment of their method for neuromorphic hardware with limited resources.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Figure 1a shows the training results (error rates versus epochs) obtained from training an SNN implemented as a DSN using SGD with the MNIST data [15] for three different time periods.", "startOffset": 146, "endOffset": 150}, {"referenceID": 3, "context": "From a neuroscience point of view, these phenomena correspond to the long-term potentiation where a persistent strengthening of synapses occurs between neurons [4], resulting in higher spikes.", "startOffset": 160, "endOffset": 163}, {"referenceID": 8, "context": "In addition, if we interpret the initialized values as a trace of background activities of the network, they interfere with memory maintenance in synapses by leading a synaptic efficacy to faster decay [9], inhibiting the long-term potentiation, i.", "startOffset": 202, "endOffset": 205}], "year": 2016, "abstractText": "Nowadays deep learning is dominating the field of machine learning with state-ofthe-art performance in various application areas. Recently, spiking neural networks (SNNs) have been attracting a great deal of attention, notably owning to their power efficiency, which can potentially allow us to implement a low-power deep learning engine suitable for real-time/mobile applications. However, implementing SNN-based deep learning remains challenging, especially gradient-based training of SNNs by error backpropagation. We cannot simply propagate errors through SNNs in conventional way because of the property of SNNs that process discrete data in the form of a series. Consequently, most of the previous studies employ a workaround technique, which first trains a conventional weighted-sum deep neural network and then maps the learning weights to the SNN under training, instead of training SNN parameters directly. In order to eliminate this workaround, recently proposed is a new class of SNN named deep spiking networks (DSNs), which can be trained directly (without a mapping from conventional deep networks) by error backpropagation with stochastic gradient descent. In this paper, we show that the initialization of the membrane potential on the backward path is an important step in DSN training, through diverse experiments performed under various conditions. Furthermore, we propose a simple and efficient method that can improve DSN training by controlling the initial membrane potential on the backward path. In our experiments, adopting the proposed approach allowed us to boost the performance of DSN training in terms of converging time and accuracy.", "creator": "LaTeX with hyperref package"}}}