{"id": "1611.04021", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Leveraging Video Descriptions to Learn Video Question Answering", "abstract": "We propose a scalable approach to learn video-based question answering (QA): answer a \"free-form natural language question\" about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.", "histories": [["v1", "Sat, 12 Nov 2016 17:15:57 GMT  (1315kb,D)", "http://arxiv.org/abs/1611.04021v1", "7 pages, 5 figures. Accepted to AAAI 2017"], ["v2", "Mon, 19 Dec 2016 16:07:33 GMT  (2585kb,D)", "http://arxiv.org/abs/1611.04021v2", "7 pages, 5 figures. Accepted to AAAI 2017. Camera-ready version"]], "COMMENTS": "7 pages, 5 figures. Accepted to AAAI 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["kuo-hao zeng", "tseng-hung chen", "ching-yao chuang", "yuan-hong liao", "juan carlos niebles", "min sun"], "accepted": true, "id": "1611.04021"}, "pdf": {"name": "1611.04021.pdf", "metadata": {"source": "CRF", "title": "Leveraging Video Descriptions to Learn Video Question Answering", "authors": ["Kuo-Hao Zeng", "Tseng-Hung Chen", "Ching-Yao Chuang", "Yuan-Hong Liao", "Juan Carlos Niebles", "Min Sun"], "emails": ["jniebles}@cs.stanford.edu", "{s104061544@m104,", "s102061145@m102,", "s102061137@m102,", "sunmin@ee}.nthu.edu.tw"], "sections": [{"heading": "Introduction", "text": "Understanding video contents at human-level is a holy grail in visual intelligence. Towards this goal, researchers have studied intermediate tasks such as detection of object and events, semantic segmentation and video summarization. Recently, there has been increased interest in a number of tasks that bridge language and vision, which are aimed at demonstrating abilities closer to human-level understanding. For example, a number of researchers have worked on video captioning and generated natural language descriptions of videos. Despite the great progress, video captioning suffers from similar issues as in image captioning: (1) it is fairly easy to generate a relevant, but non-specific, natural language description (Vinyals et al. 2015); (2) it is hard to evaluate the quality of the generated open-ended natural language description.\nAn alternative task that addresses these issues is visual question answering (QA) (Antol et al. 2015), which brings two important properties: (1) specific parts of a visual observation need to be understood to answer a question; (2) the space of relevant answers to a specific question is greatly reduced. Thanks to these properties, the visual QA task has gradually become popular for demonstrating human-level\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nvisual understanding at a finer-level of detail. Moreover, with the reduced answer space, simple metrics such as standard accuracy (i.e., percentage of correct answers) can be used to evaluate performance.\nThe biggest drawback of visual QA comes from large human efforts required to build benchmarking datasets. Most current collection techniques (Antol et al. 2015; Malinowski and Fritz 2014) require humans to view the visual data and then to manually create QA pairs for both training and testing. Furthermore, the situation becomes even worse when using \u201cvideos\u201d (rather than images) as the visual data. One of the earliest attempts to create a QA benchmark for videos is the MovieQA dataset collected by Tapaswi et al. (Tapaswi et al. 2016). Since it is expensive to hire annotators to watch entire movies, plot synopses are used as a proxy for the movie at the first step. Human annotators may form any number and type of questions for each plot paragraph. Given the initial set of questions, annotators are asked to localize context in the movie to answer the question. At this point, annotators may correct the questions if they cannot localize context in the movie. Finally, annotators are asked to provide one correct answer and four wrong answers. In total, MovieQA consists of 14,944 QA pairs from 408 movies.\nThe MovieQA dataset and the approach to collect data have the following limitations. First, it is unknown how to create a large-scale QA dataset consists of videos in the wild with no plot as a proxy. Second, the task of picking one\nar X\niv :1\n61 1.\n04 02\n1v 1\n[ cs\n.C V\n] 1\n2 N\nov 2\n01 6\ncorrect answer out of five candidate answers is less challenging than the task with 1K answer space in VQA (Antol et al. 2015). In this paper, we aim at building a video QA dataset that does not require manual construction of QA pairs for training (see Fig. 1 for our workflow). We propose to leverage the fact that Internet videos with user curated descriptions can be easily harvested at a large-scale. Moreover, a state-of-the-art question generation method (Heilman and Smith 2010) can be utilized to generate candidate QA pairs automatically from descriptions. In this way, we have collected a large-scale video QA dataset with 18,100 videos and 175, 076 candidate QA pairs.\nWhile automatic generation of QA pairs can scale very well, it is not perfect. In fact, we observe that 10% of the automatically generated pairs are irrelevant/inconsistent to the visual content in the corresponding video. As we will show, current supervised learning frameworks for video QA can be harmed by non-perfect training QA pairs. To tackle this challenge, we introduce a novel ratio test to automatically identify non-perfect candidate QA pairs and a selfpaced learning procedure to iteratively train a better model. Furthermore, we demonstrate that this strategy is widely applicable by extending several existing models that bridge vision and language to tackle the problem of video-based QA.\nWe extend four methods for our video-based QA task. They include MN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). We empirically evaluate their performance on 2, 000 videos associated with about 2500 manually generated ground truth QA pairs using crowdsourcing. Our results show that self-paced learning is effective and the extended SS method outperforms other baselines."}, {"heading": "Related Work", "text": "Image-QA. There has a significant recent interest in imagebased visual question answering (Bigham et al. 2010; Geman et al. 2014; Malinowski and Fritz 2014; Malinowski, Rohrbach, and Fritz 2015; Antol et al. 2015; Gao et al. 2015; Noh, Seo, and Han 2016; Andreas et al. 2016; Ma, Lu, and Li 2016), where the goal is to answer questions given a \u201csingle image\u201d as visual observation. In the following, we discuss a few of them which have collected their own Image-QA dataset. Bigham et al. (Bigham et al. 2010) use crowdsourced workers to complete Image-QA task asked by visually-impaired users in near real-time. (Geman et al. 2014; Malinowski and Fritz 2014) are the pioneer works on automatic visual question answering, but only consider question answer pairs related to a limited number of objects, attributes, etc. Mateusz et al. (Malinowski and Fritz 2014) also propose a new evaluation metric (WUPS) accounts for word-level ambiguities in the answer words. In our experiments, we also adopt this metric. Mateusz et al. (Malinowski, Rohrbach, and Fritz 2015) further propose a sequence-to-sequence-like model for Image-QA and extend their previous dataset (Malinowski and Fritz 2014). Antol et al. (Antol et al. 2015) manually collected a large-scale free-form and open-ended Image-QA dataset. They also propose a model which embeds question and image into a joint representation space. Gao et al. (Gao et al. 2015) collected\na Freestyle Multilingual Image Question Answering (FMIQA) dataset consisting of Chinese question-answer pairs and their English translation. They also propose a sequenceto-sequence-like model with two set of LSTMs: one for question and one for answer. Most methods require to be trained with manually collected visual QA data, which must be correct. In contrast, we propose a novel way to harvest and automatically generate our own video QA dataset, which scales to a very large number of QA pairs with the cost of potentially containing non-perfect QA pairs. This creates a challenge to existing methods, for which leveraging the large number of examples is risky due to potentially non-perfect training examples. We tackle this issue\nby introducing a self-paced learning procedure to handle non-perfect QA pairs during training.\nQuestion generation. Automatic question generation is an active research topic by itself. Most existing question generation methods (Rus and Lester 2009; Rus and Graessar 2009; Gates 2008) focus on generating questions in specific domains such as English as a Second Language (ESL). For our purposes, it is important to generate a diverse set of QA pairs that can match the open nature of the user-generated video domain. In particular, we adopt the method from Heilman and Smith (Heilman and Smith 2010) to generate our candidate QA pairs from video description sentences. Their method consists of a statistical ranking-based framework for generation of QA pairs in open domains. In a similar spirit, Ren et al. (Ren, Kiros, and Zemel 2015) propose to automatically generate QA pairs from image description for the image-based QA task. However, they focus on generating high-quality questions by constraining their structure to four types of questions: objects, numbers, color, and locationrelated questions. In contrast, our goal is to generate a very large number of open-domain questions that can be used to train data-demanding models such as deep learning models.\nVideo-QA. In contrast to the Image-QA task, video-based QA is a much less explored task. Tu et al. (Tu et al. 2014) have studied joint parsing of videos and corresponding text to answer queries. Tapaswi et al. (Tapaswi et al. 2016) recently collect a Multimodal QA dataset consisting movie clips, plot, subtitle, script, and Described Video Service (DVS). Similar to most Image-QA datasets, they ask human annotators to generate \u201cmultiple choice\u201d type of QA pairs. This requires a huge amount of human efforts, since human annotators need to verify that the context of the answer to the question can be localized in the movie. Zhu et al. (Zhu et al. 2015) collect a larger video-based QA dataset consists of 390, 744 \u201cfill-in-the-blank\u201d type of questions automatically generated from other manually created video caption datasets. Our proposed method focus on answering \u201cfreeform natural language questions\u201d rather than a \u201cfill-in-theblank\u201d types of questions. Moreover, our videos and descriptions are harvested from online video repository without any additional efforts to generate descriptions manually. Hence, we believe our proposed method further advance towards large-scale video-based QA task."}, {"heading": "Video Question Answering Dataset", "text": "We describe our new Video Question Answering (VideoQA) dataset. Since we aim for collecting videos with high quality description, we create a crawler to harvest data from an online curated video repository1. Our harvested data include the following types."}, {"heading": "Harvested Data", "text": "Internet videos. We have collected 18,100 open-domain videos with 1.5 minutes duration on average (45 seconds median duration). Our videos are typically captured from a handheld camera such as cellphone, GoPro, etc. Hence, the video quality and amount of camera motion are fairly different from movie clips. Descriptions. Originally, each video is associated with a few description sentences submitted by the video owner. Then, the staffs of the video repository curates these sentences by removing abnormal ones. As a result, there are typically 3-5 description sentences for each video. One typical description is shown in Fig. 1. It contains detail description of the scene (e.g., backyard), actor (e.g., the girl), action (e.g., score), and possibly non-visual information (e.g., practice for her World Cup debut)."}, {"heading": "Questions Generation (QG)", "text": "Candidate QA pairs. We apply state-of-the-art question generation method (Heilman and Smith 2010) to automatically generate candidate QA pairs (auto-QG) for each description sentence. We expect that some candidate QA pairs are not perfect. In our method section, we will describe our strategy to handle these non-perfect QA pairs. Generating questions with the answer \u201cNo\u201d. The stateof-the-art question generation method (Heilman and Smith 2010) can only generate \u201cYes/No\u201d questions with the answer \u201cYes\u201d. In order to make half of the \u201cYes/No\u201d questions with answer \u201cNo\u201d, we use existing \u201cYes/No\u201d questions to retrieve similar \u201cYes/No\u201d questions associated to other videos. Since the retrieved \u201cYes/No\u201d questions are most likely irrelevant/inconsistent with respect to the video content, we assign \u201cNo\u201d as the answer of these retrieved \u201cYes/No\u201d questions. In total, we have 174, 775 candidate QA pairs. Examples are shown in Fig. 2. Among them 151, 062 QA pairs from 14100 videos are used for training and 21252 QA pairs from 2000 videos are used for validation. The remaining 2000 videos are used for testing.\n1http://jukinmedia.com/videos\nVerified QA pairs. For realizing the quality of QA pairs generated by auto-QG, we ask users on Amazon Mechanical Turk to manually clean the a subset of candidate QA pairs in two steps. First, each turker is given five QA pairs corresponding to one video. The turker decides whether each QA pair is correct, irrelevant, or can-be-corrected. QA pairs which are selected as \u201ccan-be-corrected\u201d are moved into the second step, where we ask turkers to correct each QA pair. There are much fewer (about 10%) QA pairs required the second step. Human-generated QA pairs. For evaluating the video-QA performance, we collect 2461 human generated QA pairs associated to the testing videos. First, we have in-house annotators to remove descriptions which are irrelevant to the video content. Then, we ask workers on Amazon Mechanical Turk (AMT) to generate a QA pairs according to the titles and cleaned descriptions. This is time-consuming similar to the procedure used in MovieQA (Tapaswi et al. 2016). For diversity of the generated QA pairs, we make sure that each video is assigned to two different workers. Finally, we keep the QA pairs which have answers within the union set of the answers in training."}, {"heading": "Questions and Answers Analysis", "text": "Questions. We categorize questions into different types based on the words that start the question. Fig.3 (a) is the question types distribution. Our video-QA dataset contain diverse questions, including 5W1H questions. Moreover, because our QA task is based on video content, several questions aim at characters\u2019 actions or motions. Specifically, auxiliary verbs such as \u201cDoes\u201d, \u201cDid\u201d, and \u201cDo\u201d imply that many of our questions is about the main verbs in the event description. It is quite different from the image-based QA\ndatasets (Antol et al. 2015; Ren, Kiros, and Zemel 2015; Malinowski and Fritz 2014), which is mainly about objects, colors, and numbers. Among all questions, the maximum, minimum, mean, standard deviation, and median lengths are 36, 2, 10.8, 5.3, 9, respectively. Besides, we report more analysis on Human-generated QA pairs and comparison between Automatically-generated QA pairs and Humangenerated QA pairs in supplementary material. Answers. In order to have a better sense of our Video-QA dataset, we show the answer (\u201cYes/No\u201d answers excluded) distribution on eight manually defined categories (see Fig.3 (b)). Two typical answers in each category are shown. Instead of objects, colors, and numbers in most Image-QA dataset, our answers contains a large portion of human roles and actions. Note that \u201cYes\u201d and \u201cNo\u201d account for 32.5% and 32.5% of the whole set, respectively."}, {"heading": "Our Method", "text": "Video-QA is to predict an answer a given a question q and video observation v. We define a video as a sequence of image observations v = [ v1, v2, . . . ] , and both answer and question as a natural language sentence (i.e., a sequence of word) a = [ a1, a2, . . . ] and q = [ q1, q2, . . . ] , respectively. To achieve Video-QA, we propose to learn a function a = f(v,q), where v,q are the inputs and a is the desired output. Given a loss L(a, f(v,q)) measuring the difference between a truth answer a and a predicted answer f(v,q), we can train function f(\u00b7) using a set of (vi,qi, ai)i2 triplets automatically generated from videos and their description sentences. As mentioned in video question answering dataset section, the automatically generated QA pairs inevitably includes some non-perfect pairs which are irrelevant or inconsistent with respect to the video content. We propose a novel test ratio and a self-pace learning procedure to mitigate the effect of training with non-perfect QA pairs."}, {"heading": "Mitigating Effect of Non-perfect Candidate QAs", "text": "The key to mitigate effect of non-perfect pairs is to automatically identify them. We follow our intuition below to design a test to identify non-perfect pairs. Intuitively, if a training question answer pair is \u201crelevant/consistent\u201d with respect to a video content, the loss L(a, f(v,q)) should be small. If we keep the same QA pair, but change the video content to a dummy video vD with all zero observation, the loss L(a, f(vD,q)) should increase significantly. In contrast, if another training question answer pair is \u201cirrelevant/inconsistent\u201d with respect to a video content, the loss L(a, f(v,q)) should be large. Moreover, if we keep the same QA pair, but change the video content to a dummy video vD, the loss L(a, f(vD,q)) should not change much. Our intuition suggests that the loss of a non-perfect triplet (vi,qi, ai)i is less sensitive to the change of video content, compared to the loss of an ideal triplet. Ratio test. Following the intuition, we calculate the ratio r as the dummy loss L(a, f(vD,q)) divided by the original loss L(a, f(v,q)). If the ratio r is small, it implies the training triplet is non-perfect.\n2subscript denotes triplet index.\nSelf-paced learning. Firstly, we use all the training triplets to learn a reasonable function f(\u00b7). Once we have the initial function f(\u00b7), we can calculate ratio for every training triplet. For a video with a ratio smaller than a threshold \u03b3 (i.e., satisfied the ratio test), we change its training video into the dummy video vD. Then, we re-train the function f(\u00b7). Given a new function, the same steps can be repetitively applied. The whole self-paced procedure stops after no addition videos satisfied the ratio test."}, {"heading": "Extened Methods", "text": "We extend the following methods for our video-QA task. Extended End-to-End Memory Network (MN) (Sukhbaatar et al. 2015). The QA task in MN consists of a set of statements, followed by a question whose answer is typically a single word. We change the set of statements into a video - a sequence of frames. In order to capture the temporal relation among actions in consecutive frames, we first use a bi-directional LSTM to encode the sequence of frame representations. The bi-directional LSTM and the MN are jointly trained in an end-to-end fashion. Fig. 4(a) shows the model visualization similar to the one in (Sukhbaatar et al. 2015). Extended VQA (Antol et al. 2015). The VQA model is designed for question answering given a single image observation. We extend the model to handle video observation using an one-layer LSTM to encode a sequence of frames. The extended E-VQA (Fig. 4(b)) encodes both video and question using two LSTMs separately into a joint representation space, where an AND-like operation (i.e., elementwise multiplication) is used to fuse two representations. Extended Soft Attention (SA) (Yao et al. 2015). The SA model learns to dynamically applying soft-attention on different frames in order to generate caption. We modified ESA to encode questions while paying attention on different frames to generate answer. This model (Fig. 4(c)) mimics how human understand a question while paying attention to different frames; finally, answer the question. Extended Sequence-to-sequence (SS) (Venugopalan et al. 2015). The SS model learns to encode a video; then, decode a sentence. We modified E-SS to first, encode a video; then, encode a question; finally, decode an answer. This model (Fig. 4(d)) mimics how human first watch a video; then, listen to question; finally, answer the question.\nAll extended QA methods consist of various combinations of sequence-encoding, embedding, and soft-attention mechanism. They are all trained in an end-to-end fashion following our self-paced learning procedure (see the mitigating effect of non-perfect candidate QAs section). We report their video-QA performance in the experiments section."}, {"heading": "Experiments and Results", "text": "We evaluate all methods on the Video-QA dataset described in the video question answering dataset section. In all experiments, we use 14, 100 videos and 151, 263 candidate QA pairs for training, 2, 000 videos and 21, 352 candidate QA\npairs for validation, and 2, 000 videos and 2461 ground truth QA pairs for testing."}, {"heading": "Implementation Details", "text": "QA pairs data preprocessing. For questions, we do not explicitly stem, spellcheck or normalize any of the question for simplicity. Basically, we use an one-hot vector to represent a word in the question except MN (use the bag-of-words same as (Sukhbaatar et al. 2015)). We remove punctuations and replace digits as <NUMBER>. For answers, we only remove stop words. We choose the top K = 1000 most frequent answers as possible candidates, which is the same as (Antol et al. 2015). This set of answers covers 81% of the training and validation answers. Video data preprocessing. Similar to existing video understanding approaches, we utilize both appearance and local motion features. For appearance, we extract VGG (Simonyan and Zisserman 2015) feature for each frame. For local motion, we extract C3D (Tran et al. 2015) feature for 16 consecutive frames. We divide a video into maximum 45-50 clips by considering GPU memory limit. Then, we averagepool all the VGG and C3D features in each clip to obtain a video observation v. Self-paced learning implementation. According to the results of data cleaning by Amazon Mechanical Turk, we found that about 10% question answer pairs are removed by human annotators. Thus, at the first iteration of self-paced learning, we set \u03b3 to remove 10% QA pairs with small loss ratio in the training data. Then, the same \u03b3 is used in all following iterations. Our iterative self-paced method typically ends in 2 iterations."}, {"heading": "Training details", "text": "We implement and train all the extended methods using TensorFlow (et al. 2015) with the batch size of 100 and selected the final model according to the best validation accuracy. Other model-specific training details are described below. E-MN. We use stochastic gradient descent with an initial learning rate of 0.001, the same learning rate decay and gradient clipping scheme in (Sukhbaatar et al. 2015). Inspired by several memory based models, we set 500 as the num-\nber of memories and the LSTM hidden dimension. Training runs up to 50 epochs. E-VQA. Except the total number of iterations, the training settings are all the same as (Antol et al. 2015). Training runs up to 15000 iterations. E-SA. Except the optimization algorithm and the total number of epochs, the training settings are all the same as (Yao et al. 2015). We use Adam optimizer (Kingma and Ba 2014) with an initial learning rate of 0.0001. Training runs up to 30 epochs. E-SS. Except the optimization algorithm and the total number of epochs, the training settings are all the same as (Venugopalan et al. 2015). We use Adam optimizer (Kingma and Ba 2014) with an initial learning rate of 0.0001. Training runs up to 30 epochs."}, {"heading": "Evaluation Metrics", "text": "Inspired by Image-QA (Malinowski and Fritz 2014; Antol et al. 2015), we evaluate Video-QA using both classification accuracy and WUPS \u2013 its relaxed version based on word similarity. Notice that our answer space is 1K and classification accuracy is so strict that it will consider \u201ccat\u201d a wrong class for a ground truth \u201ckitten\u201d. This motivates us to report WUPS, where we follow (Malinowski and Fritz 2014) to use 0.0 and 0.9 as threshold values. Moreover, we separately report performance on \u201cYes/No\u201d type of questions and \u201cOthers\u201d types of questions. Since \u201cYes/No\u201d questions are considered to be less challenging. Finally, we report the average accuracy over \u201cYes/No\u201d and \u201cOthers\u201d (see Table. 1)."}, {"heading": "Results", "text": "Baseline method. We use Skip-Thought (ST) (Kiros et al. 2015) to directly learn the sentence semantic and syntactic properties in the framework of Recurrent Neural Network. Using the above methods as the representation for questions, we can capture the similarity between question sentences. Given a test question, we retrieve the top 10 nearest (using cosine similarity) training questions and their answers. The final answer is chosen by the majority votes of the top ten answer list. We compare the extended methods with the question retrieval baseline in the Baseline section of Table. 1. We found that baseline performs significantly\nworse than our extended methods on \u201cOthers\u201d questions, but performs on a par with extended methods on \u201cYes/No\u201d questions. Hence, we suspect the baseline makes many false positive \u201cYes/No\u201d predictions. For \u201cYes/No\u201d, we further report true-positive true-positive+false-positive+false-negative as Acc \u2020, which penalizes false positive predictions. As measured by Acc\u2020, the baseline is inferior to most extended methods. Extended methods. Self-paced E-SS (31.0%. avg. Acc) outperforms other extended methods since it jointly encodes both videos and questions sequentially.\nOn the other hand, self-paced E-VQA performs the worst among all extended methods, since it only use an elementwise multiplication operation to combine visual observation and questions. Importance of video observation. We also train all extended methods with dummy video observations such that they are forced to answer the only given question. In the\nNon-visual section of Table. 1, we show that all extended methods suffer when not observing videos. Effectiveness of self-paced learning. In the Self-paced section of Table. 1, we show that all extended methods achieve performance gain after self-paced learning.\nE-SA achieves the smallest gain, since soft-attention (SA) can select different visual observations to handle noisy QA training pairs. Among them, E-SS achieves a 2.6% improvement in avg. accuracy over its Train-all version.\nFinally, we show typical Video-QA results of our best method (E-SS) in Fig. 5 and more examples in supplementary material."}, {"heading": "Conclusions", "text": "Our scalable approach has generated a large scale videobased question answering dataset (e.g., 18, 100 videos and 175, 076 QA pairs) with minimal human efforts. Moreover, our extended models and self-paced learning procedure are shown to be effective. In the future, we will further increase\nthe scale of the Video-QA dataset and improve the procedure to handle a larger amount of non-perfect training examples."}], "references": [{"title": "Deep compositional question answering with neural module", "author": ["Andreas"], "venue": null, "citeRegEx": "Andreas,? \\Q2016\\E", "shortCiteRegEx": "Andreas", "year": 2016}, {"title": "C", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "Zitnick"], "venue": "L.; and Parikh, D.", "citeRegEx": "Antol et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "R", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "Miller"], "venue": "C.; Miller, R.; Tatarowicz, A.; White, B.; White, S.; and Yeh, T.", "citeRegEx": "Bigham et al. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["Gao"], "venue": null, "citeRegEx": "Gao,? \\Q2015\\E", "shortCiteRegEx": "Gao", "year": 2015}, {"title": "D", "author": ["Gates"], "venue": "M.", "citeRegEx": "Gates 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Visual turing test for computer vision systems", "author": ["Geman"], "venue": null, "citeRegEx": "Geman,? \\Q2014\\E", "shortCiteRegEx": "Geman", "year": 2014}, {"title": "N", "author": ["M. Heilman", "Smith"], "venue": "A.", "citeRegEx": "Heilman and Smith 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Ba", "author": ["D.P. Kingma"], "venue": "J.", "citeRegEx": "Kingma and Ba 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R", "author": ["R. Kiros", "Y. Zhu", "Salakhutdinov"], "venue": "R.; Zemel, R.; Urtasun, R.; Torralba, A.; and Fidler, S.", "citeRegEx": "Kiros et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lu Ma", "L. Li 2016] Ma", "Z. Lu", "H. Li"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "and Fritz", "author": ["M. Malinowski"], "venue": "M.", "citeRegEx": "Malinowski and Fritz 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Rohrbach Malinowski", "M. Fritz 2015] Malinowski", "M. Rohrbach", "M. Fritz"], "venue": null, "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "P", "author": ["Noh, H.", "Seo"], "venue": "H.; and Han, B.", "citeRegEx": "Noh. Seo. and Han 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring models and data for image question answering", "author": ["Kiros Ren", "M. Zemel 2015] Ren", "R. Kiros", "R. Zemel"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Question generation shared task and evaluation challenge v status", "author": ["Rus", "V. Graessar 2009] Rus", "Graessar"], "venue": null, "citeRegEx": "Rus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2009}, {"title": "and Lester", "author": ["V. Rus"], "venue": "J.", "citeRegEx": "Rus and Lester 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Zisserman", "author": ["K. Simonyan"], "venue": "A.", "citeRegEx": "Simonyan and Zisserman 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "MovieQA: Understanding stories in movies through question-answering", "author": ["Tapaswi"], "venue": null, "citeRegEx": "Tapaswi,? \\Q2016\\E", "shortCiteRegEx": "Tapaswi", "year": 2016}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["Tran"], "venue": null, "citeRegEx": "Tran,? \\Q2015\\E", "shortCiteRegEx": "Tran", "year": 2015}, {"title": "S", "author": ["K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "Zhu"], "venue": "C.", "citeRegEx": "Tu et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence - video", "author": ["Venugopalan"], "venue": null, "citeRegEx": "Venugopalan,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals"], "venue": null, "citeRegEx": "Vinyals,? \\Q2015\\E", "shortCiteRegEx": "Vinyals", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Yao"], "venue": null, "citeRegEx": "Yao,? \\Q2015\\E", "shortCiteRegEx": "Yao", "year": 2015}, {"title": "A", "author": ["L. Zhu", "Z. Xu", "Y. Yang", "Hauptmann"], "venue": "G.", "citeRegEx": "Zhu et al. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We propose a scalable approach to learn video-based question answering (QA): answer a \u201cfree-form natural language question\u201d about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended from MN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.", "creator": "LaTeX with hyperref package"}}}