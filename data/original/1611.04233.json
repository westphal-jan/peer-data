{"id": "1611.04233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "A New Recurrent Neural CRF for Learning Non-linear Edge Features", "abstract": "Conditional Random Field (CRF) and recurrent neural models have achieved success in structured prediction. More recently, there is a marriage of CRF and recurrent neural models, so that we can gain from both non-linear dense features and globally normalized CRF objective. These recurrent neural CRF models mainly focus on encode node features in CRF undirected graphs. However, edge features prove important to CRF in structured prediction. In this work, we introduce a new recurrent neural CRF model, which learns non-linear edge features, and thus makes non-linear features encoded completely. We compare our model with different neural models in well-known structured prediction tasks. Experiments show that our model outperforms state-of-the-art methods in NP chunking, shallow parsing, Chinese word segmentation and POS tagging.", "histories": [["v1", "Mon, 14 Nov 2016 02:48:46 GMT  (94kb)", "http://arxiv.org/abs/1611.04233v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shuming ma", "xu sun"], "accepted": false, "id": "1611.04233"}, "pdf": {"name": "1611.04233.pdf", "metadata": {"source": "CRF", "title": "A New Recurrent Neural CRF for Learning Non-linear Edge Features", "authors": ["Shuming Ma", "Xu Sun"], "emails": ["xusun}@pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n04 23\n3v 1\n[ cs\n.C L\n] 1\n4 N\nov 2"}, {"heading": "Introduction", "text": "Conditional Random Field (CRF) is a widely used algorithm for structured prediction. It is an undirected graphical model trained to maximize a conditional probability. The undirected graph can be encoded with a set of features (node features and edge features). Usually, these features are sparse and well manual designed.\nFor minimizing the effort in feature engineering, neural network models are used to automatically extract features (Chen and Manning 2014; Collobert et al. 2011). These models learn dense features, which have better representation of both syntax and semantic information. Because of the success of CRF and neural networks, many models take advantage of both of them. Collobert et al. (2011) used CRF objective to compute sentence-level probability of convolutional neural networks. Durrett and Klein (2015) introduced a neural CRF model to join sparse features and dense features for parsing. Andor et al. (2016) proposed a transitionbased neural model with a globally normalized CRF objective, and they use feedforward neural networks to learn neural features.\nThe marriage of feedforward neural network and CRF is natural because feedforward neural network scores local unstructured decisions while CRF makes global structured decisions. It is harder to combine recurrent neural model with\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nCRF because both of them use structural inference. Huang et al. (2015) provided a solution to combine recurrent structure with CRF structure, and gained good performance in sequence labelling. However, their model only encode node features while both node features and edge features are important to CRF.\nIn order to completely encode non-linear features for CRF, we propose a new recurrent neural CRF model. Our model uses LSTM to learn edge information of input words, and takes LSTM output as CRF energy function. We do not change the internal structure of both LSTM and CRF, so it easily decodes via standard recurrent propogation and CRF dynamic programming inference, without any extra effort. In our model, we use edge embedding to capture connections inside input structure. LSTM is used to learn hidden edge features from edge embedding. After that, CRF globally normalizes the scores of LSTM output. Andor et al. (2016) proved that globally normalized CRF objective solved label bias problem for neural models.\nThe contribution of our paper can be listed as follow:\n\u2022 We propose a neural model which can learn non-linear edge features. We find that learning non-linear edge features is even more important than node features due to the ability of modelling non-linear structure dependence.\n\u2022 We experiment our model in several well-known sequence labelling tasks, including shallow parsing, NP chunking, POS tagging and Chinese word segmentation. It shows that our model can outperform state-of-the-art methods in these tasks."}, {"heading": "Background", "text": "In structured prediction, our goal is to predict structure y given the observations x. The ith label in structure y is denoted as yi, and the ith observation is xi.\nCRF (Lafferty, McCallum, and Pereira 2001) is a popular and effective algorithm for structured prediction. It has a log-linear conditional probability with respect to energy functions over local cliques and transition cliques:\nlog(p(y|x)) \u221d \u2211\ni\nElocal(yi, x, i)\n+ \u2211\ni\nEtrans(yi\u22121, yi, x, i) (1)\nwhere Elocal(yi, x, i) is energy function over local clique at position i, and Etrans(yi\u22121, yi, x, i) is energy function over transition clique.\nEnergy functions are used to learn features. Since conventional CRF is log-linear model, both local clique and transition clique have linear energy functions:\nElocal(yi, x, i) = \u00b5lgl(yi, x, i)) (2)\nEtrans(yi\u22121, yi, x, i) = \u03bbkfk(yi\u22121, yi, x, i) (3)\nwhere fk is the indicator function of the kth feature for the transition clique (yi\u22121, yi, x), gl is the indicator function of lth feature for the local clique (yi, x), and \u03bbk and \u00b5l are parameters of CRF.\nTherefore, conventional CRF can only learn linear features. To learn high-order features, LSTM is combined with CRF model (Huang, Xu, and Yu 2015). At each time step, LSTM recurrently inputs a word and outputs scores of each predicted labels. The output function of LSTM can be used as energy function over local cliques:\nElocal(yi, x, i) = \u2211\ni\nsi[yi]) (4)\nsi = W (s)hi (5)\nwhere hi is the hidden state of LSTM at the ith time step, si[yi] is the ythi element of vector si, and A[k, l] is a transition score for jumping from jth tag to kth tag.\nAs for transition cliques, energy function is a transition matrix of variables Ai,j for jumping from ith tag to jth tag:\nEtrans(yi\u22121, yi, x, i) = Ai,j , (6)\nso energy function over transition cliques is linear as conventional CRF. Therefore, LSTM-CRF learns non-linear node features (over local cliques) and linear edge features (over transition cliques).\nFor further contain more context information, LSTM layer can be replaced with bidirectional LSTM (BiLSTM) layer. BiLSTM contains both forward information and backward information, so that BiLSTM-CRF performs better than LSTM-CRF."}, {"heading": "Proposal", "text": "Current LSTM-CRF only learns linear edge features in that it has linear energy function over transition cliques. Do and Artieres (2010) show that non-linear energy function performs better in extracting features for structured prediction. For a non-linear energy function, we propose a new recurrent neural CRF, which uses LSTM as energy function over transition cliques. Therefore, our model is able to learn nonlinear edge features."}, {"heading": "Edge Embedding", "text": "For learning non-linear edge features, we use edge embedding to provide raw edge information. In natural language processing, input structure is usually a sequence of words, so edges of input structure is connections of neighboring words. We have three methods to produce edge embedding from input structure. Bigram: Bigram embedding is an intuition way to contain neighboring words features. We can build a bigram dictionary and assign a vector to each key. It proves to be efficient in several model (Pei, Ge, and Chang 2014; Chen et al. 2015), but it may suffer from sparsity and low training speed. Concatenation: Concatenation is a useful way to join two words\u2019 information. It is simple and widely used in previous work (Collobert et al. 2011; Huang, Xu, and Yu 2015). Feedforward layer: Feedforward layer is another method to learn information from input words. It inputs two word embedding and outputs edge embedding after a single neural network layer."}, {"heading": "Layers", "text": "Figure 1 shows our proposed Recurrent Neural CRF model. Our model contains three layers: input layer, LSTM layer and CRF layer. Input Layer: Input layer is used to input words and provide edge embedding for LSTM layer. Edge embedding is from the concatenation of neighboring word vectors, and it provides raw primary edge features.\nLSTM Layer: LSTM layer recurrently inputs edge embedding from input layer and computes output as energy function over transition cliques for CRF layer. Our LSTM layer does not normalize energy output (using softmax function) until it does in CRF layer. Thus, our model is gobally normalized, which can solve label bias problem (Andor et al. 2016). CRF layer: CRF layer is to predict output structure given energy function from LSTM layer. Since we do not change CRF internal structure, viterbi algorithm is still suitable to find out the structure with highest conditional probability efficiently."}, {"heading": "Objective function", "text": "In our model, objective function is similar to CRF objective, allowing computing gradients via dynamic programming. For learning non-linear features, we replace the linear energy function with LSTM output function:\np(y|x) \u221d exp( \u2211\ni\nti[yi\u22121, yi] + \u2211\ni\nElocal(yi, x, i)) (7)\nti = W (s)hi (8)\nwhere ti[yi\u22121, yi] is LSTM energy output which contains hidden edge information.\nThe objective has a non-linear transition energy function which neither conventional CRF nor LSTM-CRF has. The local energy function can be either linear or non-linear. We call our model with linear local energy function Edge-based1 and the model with non-linear energy function Edgebased-2. Edge-based-1: Edge-based-1 model has a linear local energy function, which captures simplest linear node features. We use it to stress the importance of learning non-linear edge features. Our experiments show that model learning only non-linear edge features outperforms model learning only non-linear node features. Local energy function in Edgebased-1 is:\nElocal(yi, x, i) = \u00b5lgl(yi, x, i) (9)\ngl(yi, x, i) =\n{\n1, if yi = l 0, otherwise.\n(10)\nEdge-based-2: Edge-based-2 model has a non-linear local energy function. It is proposed to show the combination of learning non-linear node features and edge features. Local energy function in Edge-based-2 is:\nElocal(yi, x, i) = \u2211\ni\nsi[yi]) (11)\nsi = W (s)hi (12)\nwhere si[yi] is computed by another LSTM, and contains hidden node information.\nTable 1 shows the different objective function of these recurrent nerual CRF models."}, {"heading": "Training", "text": "We have two kinds of criteria to train our models: probabilistic criteria and large margin criteria (Do and Artie\u0300res 2010). Probabilistic Criteria: Probabilistic Criteria was first proposed in (Lafferty, McCallum, and Pereira 2001). The regularized objective function of recurrent neural CRF can be described as:\nL(\u03b8) =\nm \u2211\nj=1\nRj(\u03b8) + \u03bb\n2 \u2016\u03b8\u20162 (13)\nwhere m is the number of samples in the corpus. We denote the unnormalized score of a sample for Edge-Node Recurrent Neural CRF as:\nF (xj , yj , \u03b8) = \u2211\ni\nti[yi\u22121, yi] + \u2211\ni\nsi[yi] (14)\nAnd this score for Edge-based model is:\nF (xj , yj , \u03b8) = \u2211\ni\nti[yi\u22121, yi] + \u2211\ni\nq[yi] (15)\nThen Rj(\u03b8) in Equation 13 can be written as:\nRj(\u03b8) = log \u2211\ny\u2032\nexp(F (xj , y \u2032, \u03b8))\u2212 F (xj , yj, \u03b8) (16)\nLarge Margin Criteria: Large margin criteria is first introduced by Taskar et al. (2005). In large margin criteria, the margin between the scores of correct tag sequence and incorrect sequence will be larger than a given large margin:\nF (xj , yj , \u03b8) \u2265 F (xj , y \u2032, \u03b8) + \u2206(yj , y \u2032) (17)\nwhere \u2206(yj , y\u2032) is the number of incorrect tags in y\u2032. So the Rj(\u03b8) in objective function is:\nRj(\u03b8) = max y\u2032 F (xj , y \u2032, \u03b8)\u2212F (xj , yj, \u03b8)+\u2206(yj , y \u2032) (18)"}, {"heading": "Optimization", "text": "To minimize the objective function, we use AdaGrad (Duchi, Hazan, and Singer 2011), which is a widely used algorithm recently. The parameter \u03b8i for the tth update can be calculated as:\n\u03b8t,i = \u03b8t\u22121,i \u2212 \u03b1 \u221a\n\u2211t\n\u03c4=1 g 2 \u03c4,i\ngt,i (19)\nwhere \u03b1 is the initial learning rate, and gt,i is the gradient of parameter \u03b8i for the tth update."}, {"heading": "Related Work", "text": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information.\nCRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN. Durrett and Klein (2015) uses feed forward neural networks with CRF for parsing. Huang et al. (2015) use recurrent neural networks to learn non-linear node features. They show that BiLSTM-CRF is more robust than neural models without CRF. Do and Artieres (2010) suggest feedforward neural networks to learn neural features. Zhou et al. (2015) proposes a transition based neural model with CRF for parsing. Finally, Andor et al. (2016) proves that a globally normalized CRF objective helps deal with label bias problem in neural models.\nCompared with these neural CRF models, our recurrent neural CRF has a recurrent structure with the ability to learn non-linear edge features. Recurrent structure helps capture long distant information, and non-linear edge features provide more non-linear structure dependence. Table 2 shows the correlation between our proposed recurrent neural CRF model and other existing neural CRF models."}, {"heading": "Experiments", "text": "We perform some experiments to analyze our proposed models. We choose well-known sequence labelling tasks, including NP chunking, shallow parsing, POS tagging and Chinese word segmentation as our benchmark so that our experiment results are comparable. We compare our model with other popular neural models, and analyze the effect of non-linear edge features."}, {"heading": "Tasks", "text": "We introduce our benchmark tasks as follows: NP Chunking: NP Chunking is short for Noun Phrase Chunking, that the non-recursive cores of noun phrases called based NPs are identified. Our datasets are from CoNLL-2000 shallow-parsing shared task, which consists of 8936 sentences in training set and 2012 sentences in test set. We further split the training set and extract 90% sentences as development set. Following previous work, we label the sentences with BIO2 format, including 3 tags (B-NP,I-NP,O). Our evaluation metric is F-score. Shallow Parsing: Shallow parsing is a task similar to NP Chunking, but it needs to identify all chunk types(VP,PP,DT...). The dataset is also from CoNLL-2000, and it contains 23 tags. We use F-score as the evaluation metric. POS tagging: POS tagging is short for Part-of-Speech Tagging, that each word is annotated with a particular part-ofspeech. We use the standard benchmark dataset from the Penn Treebank. We use Sections 0-18 of the treebank as the training set, Sections 19-21 as the development set, and Sections 22-24 as the test set. We use tag accuracy as evaluation metric. Chinese word segmentation for social media text: Word segmentation is a fundamental task for Chinese language processing (Sun, Wang, and Li 2012; Xu and Sun 2016). Although current models perform well in formal text, many of them do badly in informal text like social media text. Our corpus is from NLPCC2016 shared task. Since we have no access to test set, we split training set and extract 10% samples as test set. We use F-score as our evaluation metric."}, {"heading": "Embeddings", "text": "Embeddings are distributed vectors to represent the semantic of words (Bengio et al. 2003; Mikolov et al. 2013). It proves that embeddings can influence the performance of neural models. In our models, we use random initialized word embeddings as well as Senna embeddings (Collobert et al. 2011). Our experiments show that Senna Embeddings can slightly improve the performance of our models. We also incorporate the feature embeddings as suggested by previous work (Collobert et al. 2011). The features include a window of last 2 words and next 2 words, as well as the word suffixes up to 2 characters. Besides, we make use of part-of-speech tags in NP chunking and shallow parsing. To alleviate heavy feature engineering, we do not use other features like bigram or trigram, though they may increase the accuracy as shown in (Pei, Ge, and Chang 2014) and (Chen et al. 2015). All these feature embeddings are random initialized.\nWe also try three methods to learn edge embedding, including concatenate current words embeddings with feature embeddings as our edge embedding in our model."}, {"heading": "Settings", "text": "We tune our hyper-parameters on the development sets. Our model is not sensitive to the dimension of hidden states when it is large enough. For the balance of accuracy and time cost, we set this number to 300 for NP chunking and shallow parsing, and the number is 200 for POS tagging and Chinese word segmentation. The dimension of input embeddings is set to be 100. The initial learning rate of AdaGrad algorithm is 0.1, and the regularization parameter is 10\u22126. The dropout method proves to avoid overfitting in neural models (Srivastava et al. 2014), but we find it has limited impact in our models. Besides, we select probabilistic criteria to train our model for its steady convergence and robust performance."}, {"heading": "Baselines", "text": "We choose current popular neural models as our baselines, including RNN, LSTM, BiLSTM and BiLSTM-CRF. RNN and LSTM are basic recurrent neural models. For further learn bidirectional context information, we also implement Bi-LSTM for our tasks. We compare our model with these model to show the gain from combining neural model with CRF objective. Finally, BiLSTM-CRF is our strong baseline. We compare our model with BiLSTM-CRF to show\nthat learning non-linear edge features is more important than single non-linear node features."}, {"heading": "Results", "text": "We analyze the performance of our models in the above benchmark tasks. Our baselines include popular neural models. We train each model for 40 passes through the training sets. The performance curves of these models in test sets are provided as showed in Figure 2. It shows that our Edge-based model outperforms the baseline neural models, including RNN, LSTM, BiLSTM and BiLSTM-CRF.\nAccording to Table 3, our models significantly outperform recurrent models without edge information in three tasks. It concludes that globally normalized objective can bring better performance in that it can model more structure dependence. Besides, our models also have higher accuracy than models with linear edge features, which shows that modelling non-linear edge features is very important for neural models. It seems that Edge-based-2 achieves better result than Edge-based-1 in NP chunking and shallow parsing, so combining non-linear edge features with node features is helpful in these two tasks.\nWe also compare our models with some existing systems as shown in Table 4. NP Chunking: In NP Chunking, a popular algorithm is second-order CRF (Sha and Pereira 2003), which can achieve a score of 94.30%. McDonald et al. (2005) implemented a multilabel learning algorithm, with a score of\n94.29%. Sun et al. (2008) proposed a latent variable CRF model, improving the score up to 94.34%. Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing. They combined the convolutional neural networks with CRF, and reached 94.32% F-score. Huang et al. (2015) combined BiLSTM with a CRF layer, raising the score up to 94.46%. Our Edge-based model can beat all of these models in performance, and obtain state-of-art result with a score of 94.80%. POS tagging: As an important task in natural language processing, there are lots of work on POS tagging. We make a comparison of our models with some recent work. Sun (2014) introduced a structure regularization method for CRF, which reached 97.36% accuracy. Collobert et al. (2011) used a Convolution-CRF model, and obtained 97.29%. Andor et al. (2016) proposed a globally normalized transition based neural model, which made use of feedfor-\nward neural networks and achieved 97.44% accuracy. Our Edge-based model can outperform the above models with 97.56% accuracy. Chinese word segmentaion for social media text: Our corpus is latest so we do not find comparable result. Instead, we implement some state-of-the-art models, and compare with our model. We find that LSTM achieves 90.50% Fscore while BiLSTM is slightly better with 90.81%. BiLSTM gains from CRF objective, and achieves 91.16% Fscore. Our model can beat all of these model, with a 91.27% F-score."}, {"heading": "Significance Tests", "text": "We conduct significance tests based on t-test to show the improvement of our models over the baselines. The significance tests suggest that our Edge-based-1 model has a very significant improvement over baseline, with p \u2264 0.004 in NP chunking, p \u2264 0.007 in shallow parsing and p \u2264 0.0001 in POS tagging and Chinese word segmentation. The Edgebased-2 model also has high statistically significance, with p \u2264 0.0001 in all tasks. The significance tests support theoretical analysis that our models can outperform the baselines in accuracy."}, {"heading": "Conclusions", "text": "We propose a new recurrent neural CRF model for learning non-linear edge features. Our model is capable to completely encoding non-linear features for CRF. Experiments show that our model outperforms state-of-the-art methods in several structured prediction tasks, including NP chunking, shallow parsing, Chinese word segmentation and POS tagging."}, {"heading": "Acknowledgements", "text": "This work was supported in part by National Natural Science Foundation of China (No. 61300063). Xu Sun is the corresponding author of this paper."}], "references": [{"title": "A high-performance semisupervised learning method for text chunking", "author": ["R.K. Ando", "T. Zhang"], "venue": "ACL 2005.", "citeRegEx": "Ando and Zhang,? 2005", "shortCiteRegEx": "Ando and Zhang", "year": 2005}, {"title": "Globally normalized transition-based neural networks", "author": ["D. Andor", "C. Alberti", "D. Weiss", "A. Severyn", "A. Presta", "K. Ganchev", "S. Petrov", "M. Collins"], "venue": "arXiv preprint arXiv:1603.06042.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of Machine Learning Research 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 740\u2013750.", "citeRegEx": "Chen and Manning,? 2014", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["X. Chen", "X. Qiu", "C. Zhu", "P. Liu", "X. Huang"], "venue": "EMNLP 2015, 1197\u20131206.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P.P. Kuksa"], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Neural conditional random fields", "author": ["T.M.T. Do", "T. Arti\u00e8res"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, 177\u2013184.", "citeRegEx": "Do and Arti\u00e8res,? 2010", "shortCiteRegEx": "Do and Arti\u00e8res", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12:2121\u2013 2159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Neural CRF parsing", "author": ["G. Durrett", "D. Klein"], "venue": "ACL 2015, 302\u2013312.", "citeRegEx": "Durrett and Klein,? 2015", "shortCiteRegEx": "Durrett and Klein", "year": 2015}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science 14(2):179\u2013211.", "citeRegEx": "Elman,? 1990", "shortCiteRegEx": "Elman", "year": 1990}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks 18(5-6):602\u2013610.", "citeRegEx": "Graves and Schmidhuber,? 2005", "shortCiteRegEx": "Graves and Schmidhuber", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "ICASSP 2013, 6645\u20136649.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Z. Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), 282\u2013289.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Flexible text segmentation with structured multilabel classification", "author": ["R.T. McDonald", "K. Crammer", "F. Pereira"], "venue": "HLT/EMNLP 2005.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "INTERSPEECH 2010, 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013,", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Max-margin tensor neural network for chinese word segmentation", "author": ["W. Pei", "T. Ge", "B. Chang"], "venue": "ACL 2014, 293\u2013303.", "citeRegEx": "Pei et al\\.,? 2014", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Conditional neural fields", "author": ["J. Peng", "L. Bo", "J. Xu"], "venue": "Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009, 1419\u20131427.", "citeRegEx": "Peng et al\\.,? 2009", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "Shallow parsing with conditional random fields", "author": ["F. Sha", "F.C.N. Pereira"], "venue": "HLT-NAACL 2003.", "citeRegEx": "Sha and Pereira,? 2003", "shortCiteRegEx": "Sha and Pereira", "year": 2003}, {"title": "Voting between multiple data representations for text chunking", "author": ["H. Shen", "A. Sarkar"], "venue": "Advances in Artificial Intelligence, 18th Conference of the Canadian Society for Computational Studies of Intelligence, 389\u2013400.", "citeRegEx": "Shen and Sarkar,? 2005", "shortCiteRegEx": "Shen and Sarkar", "year": 2005}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A.K. Joshi"], "venue": "ACL 2007.", "citeRegEx": "Shen et al\\.,? 2007", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Parsing with compositional vector grammars", "author": ["R. Socher", "J. Bauer", "C.D. Manning", "A.Y. Ng"], "venue": "ACL 2013, 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Modeling latent-dynamic in shallow parsing: A latent conditional model with improved inference", "author": ["X. Sun", "L. Morency", "D. Okanohara", "Y. Tsuruoka", "J. Tsujii"], "venue": "COLING 2008, 841\u2013848.", "citeRegEx": "Sun et al\\.,? 2008", "shortCiteRegEx": "Sun et al\\.", "year": 2008}, {"title": "Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection", "author": ["X. Sun", "H. Wang", "W. Li"], "venue": "ACL 2012, 253\u2013262.", "citeRegEx": "Sun et al\\.,? 2012", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "Structure regularization for structured prediction", "author": ["X. Sun"], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, 2402\u20132410.", "citeRegEx": "Sun,? 2014", "shortCiteRegEx": "Sun", "year": 2014}, {"title": "Asynchronous parallel learning for neural networks and structured models with dense features", "author": ["X. Sun"], "venue": "COLING 2016.", "citeRegEx": "Sun,? 2016", "shortCiteRegEx": "Sun", "year": 2016}, {"title": "Learning structured prediction models: a large margin approach", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "(ICML 2005), 896\u2013903.", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Dependency-based gated recursive neural network for chinese word segmentation", "author": ["J. Xu", "X. Sun"], "venue": "ACL 2016.", "citeRegEx": "Xu and Sun,? 2016", "shortCiteRegEx": "Xu and Sun", "year": 2016}, {"title": "Text chunking based on a generalization of winnow", "author": ["T. Zhang", "F. Damerau", "D. Johnson"], "venue": "Journal of Machine Learning Research 2:615\u2013637.", "citeRegEx": "Zhang et al\\.,? 2002", "shortCiteRegEx": "Zhang et al\\.", "year": 2002}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H.S. Torr"], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, 1529\u20131537.", "citeRegEx": "Zheng et al\\.,? 2015", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "A neural probabilistic structured-prediction model for transitionbased dependency parsing", "author": ["H. Zhou", "Y. Zhang", "S. Huang", "J. Chen"], "venue": "ACL 2015, 1213\u20131222.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "For minimizing the effort in feature engineering, neural network models are used to automatically extract features (Chen and Manning 2014; Collobert et al. 2011).", "startOffset": 115, "endOffset": 161}, {"referenceID": 5, "context": "For minimizing the effort in feature engineering, neural network models are used to automatically extract features (Chen and Manning 2014; Collobert et al. 2011).", "startOffset": 115, "endOffset": 161}, {"referenceID": 2, "context": "For minimizing the effort in feature engineering, neural network models are used to automatically extract features (Chen and Manning 2014; Collobert et al. 2011). These models learn dense features, which have better representation of both syntax and semantic information. Because of the success of CRF and neural networks, many models take advantage of both of them. Collobert et al. (2011) used CRF objective to compute sentence-level probability of convolutional neural networks.", "startOffset": 116, "endOffset": 391}, {"referenceID": 2, "context": "For minimizing the effort in feature engineering, neural network models are used to automatically extract features (Chen and Manning 2014; Collobert et al. 2011). These models learn dense features, which have better representation of both syntax and semantic information. Because of the success of CRF and neural networks, many models take advantage of both of them. Collobert et al. (2011) used CRF objective to compute sentence-level probability of convolutional neural networks. Durrett and Klein (2015) introduced a neural CRF model to join sparse features and dense features for parsing.", "startOffset": 116, "endOffset": 507}, {"referenceID": 1, "context": "Andor et al. (2016) proposed a transitionbased neural model with a globally normalized CRF objective, and they use feedforward neural networks to learn neural features.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Huang et al. (2015) provided a solution to combine recurrent structure with CRF structure, and gained good performance in sequence labelling.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Andor et al. (2016) proved that globally normalized CRF objective solved label bias problem for neural models.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "It proves to be efficient in several model (Pei, Ge, and Chang 2014; Chen et al. 2015), but it may suffer from sparsity and low training speed.", "startOffset": 43, "endOffset": 86}, {"referenceID": 5, "context": "It is simple and widely used in previous work (Collobert et al. 2011; Huang, Xu, and Yu 2015).", "startOffset": 46, "endOffset": 93}, {"referenceID": 5, "context": "Feedforward networks Convolution model (Collobert et al. 2011) Neural CRF networks (Do and Arti\u00e8res 2010) and Transition-based neural networks (Andor et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 6, "context": "2011) Neural CRF networks (Do and Arti\u00e8res 2010) and Transition-based neural networks (Andor et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 1, "context": "2011) Neural CRF networks (Do and Arti\u00e8res 2010) and Transition-based neural networks (Andor et al. 2016) Recurrent networks LSTM-CRF model (Huang, Xu, and Yu 2015) This work", "startOffset": 86, "endOffset": 105}, {"referenceID": 1, "context": "Thus, our model is gobally normalized, which can solve label bias problem (Andor et al. 2016).", "startOffset": 74, "endOffset": 93}, {"referenceID": 6, "context": "Training We have two kinds of criteria to train our models: probabilistic criteria and large margin criteria (Do and Arti\u00e8res 2010).", "startOffset": 109, "endOffset": 131}, {"referenceID": 29, "context": "Large Margin Criteria: Large margin criteria is first introduced by Taskar et al. (2005). In large margin criteria, the margin between the scores of correct tag sequence and incorrect sequence will be larger than a given large margin:", "startOffset": 68, "endOffset": 89}, {"referenceID": 2, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 16, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 23, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 4, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 28, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 9, "context": "Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks.", "startOffset": 55, "endOffset": 67}, {"referenceID": 12, "context": "LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem.", "startOffset": 5, "endOffset": 67}, {"referenceID": 10, "context": "LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem.", "startOffset": 5, "endOffset": 67}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model.", "startOffset": 87, "endOffset": 784}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.", "startOffset": 87, "endOffset": 854}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN.", "startOffset": 87, "endOffset": 944}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN. Durrett and Klein (2015) uses feed forward neural networks with CRF for parsing.", "startOffset": 87, "endOffset": 994}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN. Durrett and Klein (2015) uses feed forward neural networks with CRF for parsing. Huang et al. (2015) use recurrent neural networks to learn non-linear node features.", "startOffset": 87, "endOffset": 1070}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN. Durrett and Klein (2015) uses feed forward neural networks with CRF for parsing. Huang et al. (2015) use recurrent neural networks to learn non-linear node features. They show that BiLSTM-CRF is more robust than neural models without CRF. Do and Artieres (2010) suggest feedforward neural networks to learn neural features.", "startOffset": 87, "endOffset": 1231}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN. Durrett and Klein (2015) uses feed forward neural networks with CRF for parsing. Huang et al. (2015) use recurrent neural networks to learn non-linear node features. They show that BiLSTM-CRF is more robust than neural models without CRF. Do and Artieres (2010) suggest feedforward neural networks to learn neural features. Zhou et al. (2015) proposes a transition based neural model with CRF for parsing.", "startOffset": 87, "endOffset": 1312}, {"referenceID": 1, "context": "Finally, Andor et al. (2016) proves that a globally normalized CRF objective helps deal with label bias problem in neural models.", "startOffset": 9, "endOffset": 29}, {"referenceID": 30, "context": "Chinese word segmentation for social media text: Word segmentation is a fundamental task for Chinese language processing (Sun, Wang, and Li 2012; Xu and Sun 2016).", "startOffset": 121, "endOffset": 162}, {"referenceID": 2, "context": "Embeddings are distributed vectors to represent the semantic of words (Bengio et al. 2003; Mikolov et al. 2013).", "startOffset": 70, "endOffset": 111}, {"referenceID": 17, "context": "Embeddings are distributed vectors to represent the semantic of words (Bengio et al. 2003; Mikolov et al. 2013).", "startOffset": 70, "endOffset": 111}, {"referenceID": 5, "context": "In our models, we use random initialized word embeddings as well as Senna embeddings (Collobert et al. 2011).", "startOffset": 85, "endOffset": 108}, {"referenceID": 5, "context": "We also incorporate the feature embeddings as suggested by previous work (Collobert et al. 2011).", "startOffset": 73, "endOffset": 96}, {"referenceID": 4, "context": "To alleviate heavy feature engineering, we do not use other features like bigram or trigram, though they may increase the accuracy as shown in (Pei, Ge, and Chang 2014) and (Chen et al. 2015).", "startOffset": 173, "endOffset": 191}, {"referenceID": 24, "context": "The dropout method proves to avoid overfitting in neural models (Srivastava et al. 2014), but we find it has limited impact in our models.", "startOffset": 64, "endOffset": 88}, {"referenceID": 20, "context": "NP Chunking: In NP Chunking, a popular algorithm is second-order CRF (Sha and Pereira 2003), which can achieve a score of 94.", "startOffset": 69, "endOffset": 91}, {"referenceID": 15, "context": "McDonald et al. (2005) implemented a multilabel learning algorithm, with a score of", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "NP chunking F1 Shallow parsing F1 POS tagging Acc Sha and Pereira (2003) 94.", "startOffset": 50, "endOffset": 73}, {"referenceID": 15, "context": "NP chunking F1 Shallow parsing F1 POS tagging Acc Sha and Pereira (2003) 94.30 Zhang et al. (2002) 94.", "startOffset": 50, "endOffset": 99}, {"referenceID": 15, "context": "NP chunking F1 Shallow parsing F1 POS tagging Acc Sha and Pereira (2003) 94.30 Zhang et al. (2002) 94.17 Collobert (2011) 97.", "startOffset": 50, "endOffset": 122}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.", "startOffset": 3, "endOffset": 53}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.", "startOffset": 3, "endOffset": 70}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.", "startOffset": 3, "endOffset": 99}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.", "startOffset": 3, "endOffset": 128}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.", "startOffset": 3, "endOffset": 154}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.", "startOffset": 3, "endOffset": 183}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.", "startOffset": 3, "endOffset": 213}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.32 Andor et al. (2016) 97.", "startOffset": 3, "endOffset": 239}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.32 Andor et al. (2016) 97.44 Sun et al. (2008) 94.", "startOffset": 3, "endOffset": 263}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.32 Andor et al. (2016) 97.44 Sun et al. (2008) 94.34 Huang et al. (2015) 94.", "startOffset": 3, "endOffset": 289}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.32 Andor et al. (2016) 97.44 Sun et al. (2008) 94.34 Huang et al. (2015) 94.46 Shen et al. (2007) 97.", "startOffset": 3, "endOffset": 314}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM).", "startOffset": 18, "endOffset": 61}, {"referenceID": 21, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM).", "startOffset": 18, "endOffset": 61}, {"referenceID": 20, "context": "Sun et al. (2008) proposed a latent variable CRF model, improving the score up to 94.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.", "startOffset": 19, "endOffset": 640}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.", "startOffset": 19, "endOffset": 735}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing.", "startOffset": 19, "endOffset": 862}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing. They combined the convolutional neural networks with CRF, and reached 94.32% F-score. Huang et al. (2015) combined BiLSTM with a CRF layer, raising the score up to 94.", "startOffset": 19, "endOffset": 1030}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing. They combined the convolutional neural networks with CRF, and reached 94.32% F-score. Huang et al. (2015) combined BiLSTM with a CRF layer, raising the score up to 94.46%. Our Edge-based model can beat all of these models in performance, and obtain state-of-art result with a score of 94.80%. POS tagging: As an important task in natural language processing, there are lots of work on POS tagging. We make a comparison of our models with some recent work. Sun (2014) introduced a structure regularization method for CRF, which reached 97.", "startOffset": 19, "endOffset": 1391}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing. They combined the convolutional neural networks with CRF, and reached 94.32% F-score. Huang et al. (2015) combined BiLSTM with a CRF layer, raising the score up to 94.46%. Our Edge-based model can beat all of these models in performance, and obtain state-of-art result with a score of 94.80%. POS tagging: As an important task in natural language processing, there are lots of work on POS tagging. We make a comparison of our models with some recent work. Sun (2014) introduced a structure regularization method for CRF, which reached 97.36% accuracy. Collobert et al. (2011) used a Convolution-CRF model, and obtained 97.", "startOffset": 19, "endOffset": 1500}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing. They combined the convolutional neural networks with CRF, and reached 94.32% F-score. Huang et al. (2015) combined BiLSTM with a CRF layer, raising the score up to 94.46%. Our Edge-based model can beat all of these models in performance, and obtain state-of-art result with a score of 94.80%. POS tagging: As an important task in natural language processing, there are lots of work on POS tagging. We make a comparison of our models with some recent work. Sun (2014) introduced a structure regularization method for CRF, which reached 97.36% accuracy. Collobert et al. (2011) used a Convolution-CRF model, and obtained 97.29%. Andor et al. (2016) proposed a globally normalized transition based neural model, which made use of feedforward neural networks and achieved 97.", "startOffset": 19, "endOffset": 1571}], "year": 2016, "abstractText": "Conditional Random Field (CRF) and recurrent neural models have achieved success in structured prediction. More recently, there is a marriage of CRF and recurrent neural models, so that we can gain from both non-linear dense features and globally normalized CRF objective. These recurrent neural CRF models mainly focus on encode node features in CRF undirected graphs. However, edge features prove important to CRF in structured prediction. In this work, we introduce a new recurrent neural CRF model, which learns non-linear edge features, and thus makes non-linear features encoded completely. We compare our model with different neural models in well-known structured prediction tasks. Experiments show that our model outperforms state-of-the-art methods in NP chunking, shallow parsing, Chinese word segmentation and POS tagging.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}