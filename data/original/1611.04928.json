{"id": "1611.04928", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Neural Machine Translation with Pivot Languages", "abstract": "Neural machine translation systems typically rely on the size of parallel corpora. Nevertheless, high-quality parallel corpora are scarce resources for specific language pairs and domains. For a source-to-target language pair with a small parallel corpus, we introduce the pivot language to \"bridge\" source language and target language under the existence of large source-to-pivot and pivot-to-target parallel corpora. We propose three kinds of connection terms to jointly train source-to-pivot and pivot-to-target translation models in order to enhance the interaction between two sets of model parameters. Experiments on German-French and Spanish-French translation tasks with English as the pivot language show that our joint training approach improves the translation quality significantly than independent training on source-to-pivot, pivot-to-target and source-to-target directions.", "histories": [["v1", "Tue, 15 Nov 2016 16:44:54 GMT  (188kb,D)", "https://arxiv.org/abs/1611.04928v1", null], ["v2", "Tue, 21 Feb 2017 04:13:38 GMT  (264kb,D)", "http://arxiv.org/abs/1611.04928v2", "fix experiments and revise the paper"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yong cheng", "yang liu", "qian yang", "maosong sun", "wei xu"], "accepted": false, "id": "1611.04928"}, "pdf": {"name": "1611.04928.pdf", "metadata": {"source": "CRF", "title": "Joint Training for Pivot-based Neural Machine Translation", "authors": ["Yong Cheng", "Yang Liu", "Qian Yang", "Maosong Sun", "Wei Xu"], "emails": ["sms}@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Recent several years have witnessed the rapid development of neural machine translation (NMT) [Sutskever et al., 2014; Bahdanau et al., 2015], which advocates the use of neural networks to directly model the translation process in an endto-end way. Thanks to the capability of learning representations from training data, NMT systems have achieved significant improvements over conventional statistical machine translation (SMT) across a variety of language pairs [JunczysDowmunt et al., 2016; Johnson et al., 2016].\nHowever, there still remains a major challenge for NMT: large-scale parallel corpora are usually non-existent for most language pairs. This is unfortunate because NMT is a datahungry approach and requires a large amount of data to fully train model parameters. Without sufficient training data, NMT tends to learn poor estimates on low-count events. Zoph et al. [2016] indicate that NMT obtains much worse translation quality than SMT when only small-scale parallel corpora are available.\nAs a result, improving neural machine translation on resource-scarce language pairs has attracted much attention in the community [Firat et al., 2016; Zoph et al., 2016; Johnson et al., 2016]. Most existing methods focus on leveraging data of multiple resource-rich language pairs to im-\nprove NMT for resource-scarce language pairs. Firat et al. [2016] propose multi-way, multilingual neural machine translation to achieve direct source-to-target translation even without parallel data available. Zoph et al. [2016] present a transfer learning method that transfers the model parameters trained for resource-rich language pairs to initialize and constrain the translation model training of resource-scarce language pairs. Johnson et al. [2016] introduce a universal NMT model for all language pairs, which takes advantage of multilingual data to improve NMT for all languages involved.\nBridging source and target languages with a pivot language is another important direction, which has been intensively studied in conventional SMT [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013]. Pivot-based approaches assume that there exist source-pivot and pivot-target parallel corpora, which can be used to train source-to-pivot and pivot-to-target translation models, respectively. One of the most representative approaches, triangulation approach, is to construct a source-to-target phrase table through combining source-to-pivot and pivot-to-target phrase tables. Another representative approach adopts a pivot-based translation strategy. As a result, source-to-target translation can be divided into two steps: the source sentence is first translated into a pivot sentence using the source-topivot model, which is then translated to a target sentence using the pivot-to-target model. Pivot-based approaches have been widely used in SMT due to its simplicity, effectiveness, and minimum requirement of multilingual data. Recently, Johnson et al. [2016] adapt pivot-based approaches to NMT and show that their universal model without incremental training achieves much worse translation performance than pivot-based NMT.\nHowever, pivot-based approaches often suffer from the error propagation problem: the errors made in the sourceto-pivot translation will be propagated to the pivot-to-target translation. This can be partly attributed to the discrepancy between source-pivot and pivot-target parallel corpora since they are usually loosely-related or even unrelated. To aggregate the situation, source-to-pivot and pivot-to-target translation models are trained independently, which further enlarges the gap between source and target languages.\nIn this work, we propose an approach to joint training for pivot-based neural machine translation. The basic idea is to\nar X\niv :1\n61 1.\n04 92\n8v 2\n[ cs\n.C L\n] 2\n1 Fe\nb 20\n17\nconnect the source-to-pivot and pivot-to-target NMT models and enable them to interact with each other during training. This can be done either by encouraging the sharing of word embeddings on the pivot language or by maximizing the likelihood of the cascaded model on a small source-target parallel corpus. Experiments on the Europarl and WMT corpora show that joint training of source-to-pivot and pivot-to-target models obtains significant improvements over independent training."}, {"heading": "2 Background", "text": "Given a source language sentence x and a target language sentence y, we use P (y|x;\u03b8x\u2192y) to denote a standard attention-based neural machine translation model [Bahdanau et al., 2015], where \u03b8x\u2192y is a set of model parameters.\nIdeally, the source-to-target model can be trained on a source-target parallel corpus Dx,y = {\u3008x(s),y(s)\u3009}Ss=1 using maximum likelihood estimation:\n\u03b8\u0302x\u2192y = argmax \u03b8x\u2192y\n{ L(\u03b8x\u2192y) } (1)\nwhere the log-likelihood is defined as\nL(\u03b8x\u2192y) = S\u2211\ns=1\nlogP (y(s)|x(s);\u03b8x\u2192y) (2)\nUnfortunately, parallel corpora are usually not readily available for low-resource language pairs. Instead, one can assume that there exist a third language called pivot with source-pivot and pivot-target parallel corpora available. As a result, it is possible to bridge the source and target languages with the pivot [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].\nLet z be a pivot language sentence. The source-to-target model can be decomposed into two sub-models by treating the pivot sentence as a latent variable:\nP (y|x;\u03b8x\u2192z,\u03b8z\u2192y) = \u2211 z P (z|x;\u03b8x\u2192z)P (y|z;\u03b8z\u2192y) (3)\nLet Dx,z = {\u3008x(m), z(m)\u3009}Mm=1 be a source-pivot parallel corpus, and Dz,y = {\u3008z(n),y(n)\u3009}Nn=1 be a pivot-target parallel corpus. The source-to-pivot and pivot-to-target models can be independently trained on the two parallel corpora, respectively:\n\u03b8\u0302x\u2192z = argmax \u03b8x\u2192z\n{ L(\u03b8x\u2192z) } (4)\n\u03b8\u0302z\u2192y = argmax \u03b8z\u2192y\n{ L(\u03b8z\u2192y) } (5)\nwhere the log-likelihoods are defined as:\nL(\u03b8x\u2192z) = M\u2211\nm=1\nlogP (z(m)|x(m);\u03b8x\u2192z) (6)\nL(\u03b8z\u2192y) = N\u2211\nn=1\nlogP (y(n)|z(n);\u03b8z\u2192y) (7)\nAs Figure 1 shows, a pivot-based translation strategy is usually adopted. Given an unseen source sentence to be translated x, the decision rule is given by:\ny\u0302 = argmax y {\u2211 z P (z|x; \u03b8\u0302x\u2192z)P (y|z; \u03b8\u0302z\u2192y) } (8)\nDue to the exponential search space of the pivot language, the decoding process is usually approximated with two steps. The first step translates the source sentence x into a pivot sentence:\nz\u0302 = argmax z\n{ P (z|x; \u03b8\u0302x\u2192z) } (9)\nThen, the pivot sentence is translated to a target sentence:\ny\u0302 = argmax y\n{ P (y|z\u0302; \u03b8\u0302z\u2192y) } (10)\nAlthough pivot-based approaches are widely for addressing the data scarcity problem in machine translation, they suffer from cascaded translation errors: the mistakes made in the source-to-pivot translation as shown in Eq. (9) will be propagated to the pivot-to-target translation as shown in Eq. (10). This can be partly attributed to the model discrepancy problem: the source-to-pivot and pivot-to-target models are quite different in terms of vocabulary and parameter space because the source-pivot and pivot-target parallel corpora are usually loosely-related or even unrelated. To make things worse, the source-to-pivot model P (z|x;\u03b8x\u2192z) and the pivot-to-target model P (y|z;\u03b8z\u2192y) are trained on the two parallel corpora independently, which further increases the discrepancy between two models.\nTherefore, it is important to reduce the discrepancy between source-to-pivot and pivot-to-target models to further improve pivot-based neural machine translation."}, {"heading": "3 Joint Training for Pivot-based NMT", "text": ""}, {"heading": "3.1 Training Objective", "text": "To alleviate the model discrepancy problem, we propose an approach to joint training for pivot-based neural machine translation. The basic idea is to connect source-to-pivot and pivot-to-target models and enable them to interact with each other during training. Our new training objective is given by:\nJ (\u03b8x\u2192z,\u03b8z\u2192y) = L(\u03b8x\u2192z) + L(\u03b8z\u2192y) + \u03bbR(\u03b8x\u2192z,\u03b8z\u2192y) (11)\nNote that the training objective consists of three parts: the source-to-pivot likelihoodL(\u03b8x\u2192z), the pivot-to-target likelihood L(\u03b8z\u2192y), and a connection term R(\u03b8x\u2192z,\u03b8z\u2192y). The hyper-parameter \u03bb is used to balance the preference between likelihoods and the connection term.\nWe expect that the connection term associates the sourceto-pivot model \u03b8x\u2192z with the pivot-to-target model \u03b8z\u2192y and enables the interaction between two models during training. In the following subsection, we will introduce the three connection terms used in our experiments."}, {"heading": "3.2 Connection Terms", "text": "It is difficult to connect the source-to-pivot and pivot-to-target models during training because the source-to-pivot and pivotto-target models are distantly-related by definition. More importantly, NMT lacks linguistically interpretable language structures such as phrases in SMT to achieve a direct connection at the parameter level [Wu and Wang, 2007].\nFortunately, both the source-to-pivot and pivot-to-target models include the word embeddings of the pivot language as parameters. It is possible to connect the two models via pivot word embeddings.\nMore formally, let Vzx\u2192z be the pivot vocabulary of the source-to-pivot model and Vzz\u2192y be the pivot vocabulary of the pivot-to-target model. We use w to denote a word in the pivot language and \u03b8wx\u2192z \u2208 Rd to denote the vector representation of w in the source-to-pivot model. \u03b8wz\u2192y \u2208 Rd is defined in a similar way.\nOur first connection term encourages the two models to generate the same vector representations for pivot words in the intersection of two vocabularies:\nRhard(\u03b8x\u2192z,\u03b8z\u2192y) = \u220f\nw\u2208Vzx\u2192z\u2229Vzz\u2192y\n\u03b4(\u03b8wx\u2192z,\u03b8 w z\u2192y) (12)\nwhere \u03b4(\u03b8wx\u2192z,\u03b8 w z\u2192y) = 1 if the two vectors \u03b8 w x\u2192z and \u03b8 w z\u2192y are identical. Otherwise, \u03b4(\u03b8wx\u2192z,\u03b8 w z\u2192y) = 0.\nAs word embeddings seem hardly to be exactly identical due to the divergence of natural languages, an alternative is to soften the above hard matching constraint by penalizing the Euclidean distance between two vectors:\nRsoft(\u03b8x\u2192z,\u03b8z\u2192y) = \u2212 \u2211\nw\u2208Vzx\u2192z\u2229Vzz\u2192y\n||\u03b8wx\u2192z \u2212 \u03b8wz\u2192y||2 (13)\nThe third connection term assumes that there is a small bridging source-target parallel corpus Dx,y =\n{\u3008x(s),y(s)\u3009}Ss=1 (Bridging Corpus) available. The connection term is defined as the log-likelihood of the bridging data:\nRlikelihood(\u03b8x\u2192z,\u03b8z\u2192y)\n= S\u2211 s=1 logP (y(s)|x(s);\u03b8x\u2192z,\u03b8z\u2192y) (14)\n= S\u2211 s=1 log \u2211 z P (z|x(s);\u03b8x\u2192z)P (y(s)|z;\u03b8z\u2192y)(15)"}, {"heading": "3.3 Training", "text": "In training, our goal is to find the optimal source-to-pivot and pivot-to-target model parameters that maximize the training objective:\n\u03b8\u0302x\u2192z, \u03b8\u0302z\u2192y = argmax \u03b8x\u2192z,\u03b8z\u2192y\n{ J (\u03b8x\u2192z,\u03b8z\u2192y) } (16)\nThe partial derivative of J (\u03b8x\u2192z,\u03b8z\u2192y) with respect to the parameters \u03b8x\u2192z of the source-to-pivot model can be calculated as:\n\u2202J (\u03b8x\u2192z,\u03b8z\u2192y) \u2202\u03b8x\u2192z\n= M\u2211 m=1 \u2202 logP (z(m)|x(m);\u03b8x\u2192z) \u2202\u03b8x\u2192z +\n\u03bb \u2202R(\u03b8x\u2192z,\u03b8z\u2192y)\n\u2202\u03b8x\u2192z (17)\nThe partial derivative with respect to the parameters \u03b8z\u2192y can be calculated similarly.\nThe gradients of the first and second connection terms Rhard(\u03b8x\u2192z,\u03b8z\u2192y) and Rsoft(\u03b8x\u2192z,\u03b8z\u2192y) with respect to model parameters are easy to calculate. However, calculating the gradients of the third connection term Rlikelihood(\u03b8x\u2192z,\u03b8z\u2192y) involves enumerating all possible pivot sentences in an exponential search space (see Eq. (15)).\nTo alleviate this problem, we follow standard practice to use a subset to approximate the full space [Shen et al., 2016; Cheng et al., 2016]. Two methods can be used to generate a subset: sampling k translations from the full space [Shen et al., 2016] or generating a top-k list of candidate translations [Cheng et al., 2016]. We find that using top-k lists leads to better results than sampling in our experiments.\nWe use standard mini-batched stochastic gradient descent algorithms to optimize model parameters. In each iteration, three mini-batches are constructed by randomly selecting sentence pairs from the source-pivot parallel corpus Dx,z , the pivot-target parallel corpus Dz,y , and the bridging sourcetarget parallel corpus Dx,y (only available for the third connection term), respectively. After separate gradient calculation in each mini-batch, the gradients are collected to update model parameters."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "We evaluated our approach on two translation tasks:\n1. Spanish-English-French: Spanish as the source language, English as the pivot language, and French as the target language,\n2. German-English-French: German as the source language, English as the pivot language, and French as the target language.\nTable 1 shows the statistics of the Europarl and WMT corpora used in our experiments. We use tokenize.perl script for tokenization. For each language pair, we remove the empty lines and retain sentence pairs with no more than 50 words. To avoid the intersection of the source-pivot and pivot-target corpora, we split the overlapped pivot-language sentences of source-to-pivot and pivot-to-target corpora into two separate parts with equal size and merge them separately with the non-overlapping parts for each language pair.\nThe Europarl corpus consists of 850K Spanish-English sentence pairs with 22.32M Spanish words and 21.44M English words, 840K German-English sentence pairs with 20.88M German words and 21.91M English words, and 900K English-French sentence pairs with 22.56M English words and 25.00M French words. The WMT 2006 shared task datasets are used as the development and test sets. The evaluation metric is case-insensitive BLEU [Papineni et al., 2002] as calculated by the multi-bleu.perl script.\nThe WMT corpus is composed of the Common Crawl, News Commentary, Europarl v7 and UN corpora. The Spanish-English parallel corpus consists of 6.78M sentence pairs with 183.01M Spanish words and 166.28M English words. The English-French parallel corpus comprises 9.29M sentence pairs with 227.06M English words and 258.95M French words. The newstest2011 and newstest2012 datasets serve as development and test sets. We use case-sensitive BLEU as the evaluation metric.\nWe use the attention-based neural machine translation system RNNSEARCH [Bahdanau et al., 2015] in our experiments. For the Europarl corpus in Table 1, we set the vo-\ncabulary size of all the languages to 30K which covers over 99% of words for English, Spanish and French and over 97 % for German. We follow Jean et al. [2015] to address rare words. For Spanish-English and English-French corpora from the WMT corpus, due to large vocabulary size, we adopt byte pair encoding [Sennrich et al., 2016b] to split rare words into sub-words. The size of sub-words is set to 43K, 33K, 43K respectively for Spanish, English, and French. These sub-words cover 100% of the text.\nWe set the hyper-parameter \u03bb for balancing between likelihood and the connection term to 1.0. The threshold of gradients is set to 0.1. The bridging source-target parallel corpus contains 100K sentence pairs that do not overlap with the training data. We set k to 10 for calculating top-k lists to approximate the full search space. The parameters for the source-to-pivot and pivot-to-target translation models in the likelihood connection term are initialized by pre-trained model parameters."}, {"heading": "4.2 Results on the Europarl Corpus", "text": "Table 2 shows the comparison results between our joint training on three connection terms and independent training on the Europarl Corpus. For the source-to-target translation task, we present source-to-pivot, pivot-to-target and source-to-target translation results compared with independent training. In Spanish-to-French translation task, soft connection achieves significant improvements in Spanish-to-French and Spanishto-English directions although hard connection still performs comparably with independent training. In German-to-French translation task, soft and hard connections also achieve comparable performances with independent training.\nIn contrast, we find that likelihood connection dramatically improves translation performance on both Spanish-to-French and German-to-French corpora (up to +2.80 BLEU scores in Spanish-to-French and up to 2.23 BLEU scores in Germanto-French). The significant improvements for source-to-pivot and pivot-to-target directions are also observed. This sug-\ngests that introducing source-to-target parallel corpus to maximize P (y|x;\u03b8x\u2192z,\u03b8z\u2192y) with z as latent variables makes the source-to-pivot and pivot-to-target translation models improved collaboratively.\nTable 3 shows pivot and target translation examples of in-\ndependent training and our approaches. Apparently, our approaches improve translation quality of both pivot sentences and target sentences.\nAccording to Eq. (3), the cost of the source-to-target model can be decomposed into the cost of source-to-pivot and pivotto-target models. Because we have a small test trilingual corpus, (Spanish, English, French), we use the English sentence to approximate the latent variables in Eq. (3). Then we calculate the cost of Spanish-to-French on the trilingual corpus."}, {"heading": "100K 33.35 31.63 32.45", "text": "Figure 2 shows the learning curves of the test cost of independent training and joint training on three connection terms. We can find that hard and soft connections learn slower than the independent training. Likelihood connection drives its cost lower after fine-tuning based on pre-trained parameters in just 10K iterations."}, {"heading": "4.3 Results on the WMT Corpus", "text": "Likelihood connection obtains the best performance in our three proposed connection terms according to experiments on the Europarl corpus. To further verify its practicability, Table 4 shows results on the WMT corpus which is a much larger corpus. We find that likelihood connection still outperforms independent training significantly on Spanish-toEnglish, English-to-French and Spanish-to-French directions (up to +1.18 BLEU scores in Spanish-to-French).\nWe also compare our approach with Firat et al. [2016]. They propose a multi-way, multilingual NMT model to build a source-to-target translation model. Although our parallel training corpus is much smaller than theirs, Table 5 shows that our approach achieves substantial improvements over them (up to +4.32 BLEU)."}, {"heading": "4.4 Effect of Bridging Corpora", "text": "As bridging corpora are used in likelihood connection term for \u201cbridging\u201d the source-to-pivot and pivot-to-target translation models, why do not we directly build NMT systems with these corpora?\nWe train source-to-target models using bridging corpora and show translation results in Table 6 . We observe that performance is much worse than that in Table 2 and Table 4 using the pivot-based translation strategy. It indicates that NMT yields poor performance on low-resource languages and the pivot-based translation strategy remedies the drawback to alleviate data scarcity effectively.\nWe also investigate the effect of the data size of bridging corpora on the likelihood connection. Table 7 shows that using a small parallel corpus (1K sentence pairs) has made a measurable improvement. When more than 50K sentence pairs are added, the further improvements become modest.\nThis finding suggests that a small corpus suffices to enable the likelihood connection to reach the reasonable performance."}, {"heading": "5 Related Work", "text": "Our work is inspired by two lines of research: (1) machine translation with pivot languages and (2) incorporating additional data resource for NMT."}, {"heading": "5.1 Machine Translation with Pivot Languages", "text": "Machine translation suffers from the scarcity of parallel corpora. For low-resource language pairs, a pivot language is introduced to \u201cbridge\u201d source and target languages in statical machine translation [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Zahabi et al., 2013; El Kholy et al., 2013].\nIn NMT, Firat et al. [2016] and Johnson et al [2016] propose multi-way, multilingual NMT models that enable zeroresource machine translation. They also need to apply pivotbased approaches into NMT to ameliorate the performance of zero-resource machine translation. Zoph et al. [2016] adopt transfer learning to fine-tune parameters of the low-resource language pairs using trained parameters on the high-resource language pairs. However, our approach aims to jointly train source-to-pivot and pivot-to-target NMT models, which can alleviate the error propagation of pivot-based approaches. We use connection terms to \u201cbridge\u201d these two models and make them benefit each other."}, {"heading": "5.2 Incorporating Additional Data Resources for NMT", "text": "Due to the limit in quantity, quality and coverage for parallel corpora, additional data resources have raised attention recently. Gulccehre et al [2015] propose to incorporate target-side monolingual corpora as a language model for NMT. Sennrich, Haddow, and Birch [2016a] pair the target monolingual corpora with its corresponding translations, then merge them with parallel corpora for retraining source-to-target model. Zhang and Zong [2016] propose two approaches, self-training algorithm and multi-task learning framework, to incorporate source-side monolingual corpora. Cheng et al. [2016] introduce an autoencoder framework to reconstruct monolingual sentences using source-to-target and target-to-source NMT models. The proposed model can exploit both source and target monolingual corpora. In contrast to Cheng et al. [2016], the objective of our likelihood connection is to maximize the probability of target-language sentences through pivot sentences given source sentences. We use a small source-to-target parallel corpus to train source-topivot and pivot-to-target NMT models jointly."}, {"heading": "6 Conclusion", "text": "We present joint training for pivot-based neural machine translation. Experiments on different language pairs confirm that our approach achieves significant improvements. It is appealing to combine source and pivot sentences for decoding target sentences [Firat et al., 2016] or train a multi-source model directly [Zoph and Knight, 2016]. We also plan to study better connection terms for our joint training."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio"], "venue": "Proceedings of ICLR,", "citeRegEx": "Bahdanau et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Phrase-based statistical machine translation with pivot languages", "author": ["Nicola Bertoldi", "Madalina Barbaiani", "Marcello Federico", "Roldano Cattoni"], "venue": "IWSLT,", "citeRegEx": "Bertoldi et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "Proceedings of ACL,", "citeRegEx": "Cheng et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Machine translation by triangulation: Making effective use of multi-parallel corpora", "author": ["Trevor Cohn", "Mirella Lapata"], "venue": "Proceedings of ACL,", "citeRegEx": "Cohn and Lapata. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Language independent connectivity strength features for phrase pivot statistical machine translation", "author": ["Ahmed El Kholy", "Nizar Habash", "Gregor Leusch", "Evgeny Matusov", "Hassan Sawaf"], "venue": "Proceedings of ACL,", "citeRegEx": "El Kholy et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T Yarman Vural", "Kyunghyun Cho"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Firat et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Lo\u0131\u0308c Barrault", "author": ["Caglar Gulccehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho"], "venue": "Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. On using monolingual corpora in neural machine translation. arXiv:1503.03535 [cs.CL],", "citeRegEx": "Gulccehre et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Sebastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "Proceedings of ACL,", "citeRegEx": "Jean et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Johnson et al", "2016] Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang"], "venue": "arXiv:1610.01108v2,", "citeRegEx": "Junczys.Dowmunt et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Koehn. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Bleu: a methof for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Proceedings of ACL,", "citeRegEx": "Papineni et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving nerual machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "Proceedings of ACL,", "citeRegEx": "Sennrich et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "Proceedings of ACL,", "citeRegEx": "Sennrich et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "Proceedings of ACL,", "citeRegEx": "Shen et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "Proceedings of NIPS,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["Masao Utiyama", "Hitoshi Isahara"], "venue": "HLT-NAACL,", "citeRegEx": "Utiyama and Isahara. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Pivot language approach for phrase-based statistical machine translation", "author": ["Hua Wu", "Haifeng Wang"], "venue": "Machine Translation,", "citeRegEx": "Wu and Wang. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Using context vectors in improving a machine translation system with bridge language", "author": ["Samira Tofighi Zahabi", "Somayeh Bakhshaei", "Shahram Khadivi"], "venue": "Proceedings of ACL,", "citeRegEx": "Zahabi et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Jiajun Zhang", "Chengqing Zong"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Zhang and Zong. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight"], "venue": "Proceedings of NAACL,", "citeRegEx": "Zoph and Knight. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Zoph et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Recent several years have witnessed the rapid development of neural machine translation (NMT) [Sutskever et al., 2014; Bahdanau et al., 2015], which advocates the use of neural networks to directly model the translation process in an endto-end way.", "startOffset": 94, "endOffset": 141}, {"referenceID": 0, "context": "Recent several years have witnessed the rapid development of neural machine translation (NMT) [Sutskever et al., 2014; Bahdanau et al., 2015], which advocates the use of neural networks to directly model the translation process in an endto-end way.", "startOffset": 94, "endOffset": 141}, {"referenceID": 5, "context": "As a result, improving neural machine translation on resource-scarce language pairs has attracted much attention in the community [Firat et al., 2016; Zoph et al., 2016; Johnson et al., 2016].", "startOffset": 130, "endOffset": 191}, {"referenceID": 21, "context": "As a result, improving neural machine translation on resource-scarce language pairs has attracted much attention in the community [Firat et al., 2016; Zoph et al., 2016; Johnson et al., 2016].", "startOffset": 130, "endOffset": 191}, {"referenceID": 3, "context": "Bridging source and target languages with a pivot language is another important direction, which has been intensively studied in conventional SMT [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 146, "endOffset": 282}, {"referenceID": 17, "context": "Bridging source and target languages with a pivot language is another important direction, which has been intensively studied in conventional SMT [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 146, "endOffset": 282}, {"referenceID": 16, "context": "Bridging source and target languages with a pivot language is another important direction, which has been intensively studied in conventional SMT [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 146, "endOffset": 282}, {"referenceID": 1, "context": "Bridging source and target languages with a pivot language is another important direction, which has been intensively studied in conventional SMT [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 146, "endOffset": 282}, {"referenceID": 18, "context": "Bridging source and target languages with a pivot language is another important direction, which has been intensively studied in conventional SMT [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 146, "endOffset": 282}, {"referenceID": 4, "context": "Bridging source and target languages with a pivot language is another important direction, which has been intensively studied in conventional SMT [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 146, "endOffset": 282}, {"referenceID": 0, "context": "Given a source language sentence x and a target language sentence y, we use P (y|x;\u03b8x\u2192y) to denote a standard attention-based neural machine translation model [Bahdanau et al., 2015], where \u03b8x\u2192y is a set of model parameters.", "startOffset": 159, "endOffset": 182}, {"referenceID": 3, "context": "As a result, it is possible to bridge the source and target languages with the pivot [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 85, "endOffset": 221}, {"referenceID": 17, "context": "As a result, it is possible to bridge the source and target languages with the pivot [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 85, "endOffset": 221}, {"referenceID": 16, "context": "As a result, it is possible to bridge the source and target languages with the pivot [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 85, "endOffset": 221}, {"referenceID": 1, "context": "As a result, it is possible to bridge the source and target languages with the pivot [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 85, "endOffset": 221}, {"referenceID": 18, "context": "As a result, it is possible to bridge the source and target languages with the pivot [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 85, "endOffset": 221}, {"referenceID": 4, "context": "As a result, it is possible to bridge the source and target languages with the pivot [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 85, "endOffset": 221}, {"referenceID": 17, "context": "More importantly, NMT lacks linguistically interpretable language structures such as phrases in SMT to achieve a direct connection at the parameter level [Wu and Wang, 2007].", "startOffset": 154, "endOffset": 173}, {"referenceID": 14, "context": "To alleviate this problem, we follow standard practice to use a subset to approximate the full space [Shen et al., 2016; Cheng et al., 2016].", "startOffset": 101, "endOffset": 140}, {"referenceID": 2, "context": "To alleviate this problem, we follow standard practice to use a subset to approximate the full space [Shen et al., 2016; Cheng et al., 2016].", "startOffset": 101, "endOffset": 140}, {"referenceID": 14, "context": "Two methods can be used to generate a subset: sampling k translations from the full space [Shen et al., 2016] or generating a top-k list of candidate translations [Cheng et al.", "startOffset": 90, "endOffset": 109}, {"referenceID": 2, "context": ", 2016] or generating a top-k list of candidate translations [Cheng et al., 2016].", "startOffset": 61, "endOffset": 81}, {"referenceID": 11, "context": "The evaluation metric is case-insensitive BLEU [Papineni et al., 2002] as calculated by the multi-bleu.", "startOffset": 47, "endOffset": 70}, {"referenceID": 0, "context": "We use the attention-based neural machine translation system RNNSEARCH [Bahdanau et al., 2015] in our experiments.", "startOffset": 71, "endOffset": 94}, {"referenceID": 13, "context": "For Spanish-English and English-French corpora from the WMT corpus, due to large vocabulary size, we adopt byte pair encoding [Sennrich et al., 2016b] to split rare words into sub-words.", "startOffset": 126, "endOffset": 150}, {"referenceID": 10, "context": "We use the statistical significance test with paired bootstrap resampling [Koehn, 2004].", "startOffset": 74, "endOffset": 87}, {"referenceID": 3, "context": "For low-resource language pairs, a pivot language is introduced to \u201cbridge\u201d source and target languages in statical machine translation [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 136, "endOffset": 249}, {"referenceID": 17, "context": "For low-resource language pairs, a pivot language is introduced to \u201cbridge\u201d source and target languages in statical machine translation [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 136, "endOffset": 249}, {"referenceID": 16, "context": "For low-resource language pairs, a pivot language is introduced to \u201cbridge\u201d source and target languages in statical machine translation [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 136, "endOffset": 249}, {"referenceID": 18, "context": "For low-resource language pairs, a pivot language is introduced to \u201cbridge\u201d source and target languages in statical machine translation [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 136, "endOffset": 249}, {"referenceID": 4, "context": "For low-resource language pairs, a pivot language is introduced to \u201cbridge\u201d source and target languages in statical machine translation [Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007; Zahabi et al., 2013; El Kholy et al., 2013].", "startOffset": 136, "endOffset": 249}], "year": 2017, "abstractText": "While recent neural machine translation approaches have delivered state-of-the-art performance for resource-rich language pairs, they suffer from the data scarcity problem for resource-scarce language pairs. Although this problem can be alleviated by exploiting a pivot language to bridge the source and target languages, the source-to-pivot and pivot-to-target translation models are usually independently trained. In this work, we introduce a joint training algorithm for pivot-based neural machine translation. We propose three methods to connect the two models and enable them to interact with each other during training. Experiments on Europarl and WMT corpora show that joint training of source-to-pivot and pivot-to-target models leads to significant improvements over independent training across various languages.", "creator": "LaTeX with hyperref package"}}}